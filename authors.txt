2021 ||| the attention-hesitation model. a non-intrusive intervention strategy for incremental smart home dialogue management. ||| birte richter ||| 
2020 ||| visual attention in primates and for machines - neuronal mechanisms. ||| frederik beuth ||| 
2017 ||| deployment of processing resources to concurrent stimulation by sustained spatial attention in touch. ||| cheuk yee pang ||| 
2018 ||| improvement of the correlation between alse and bci by adjusting the feeding and positioning conditions based on indirect measurement of termination impedances using current transformers. ||| seyyed ali hassanpour razavi ||| 
2021 ||| periodicity, surprisal, attention: skip conditions for recurrent neural networks. ||| tayfun alpay ||| 
2019 ||| breakdown mechanisms of the coreless transformer in data couplers at high voltages. ||| julie paye ||| 
2018 ||| effective influences in neuronal networks: attentional modulation of effective influences underlying flexible processing and how to measure them. ||| daniel harnack ||| 
2018 ||| object completion effects in attention and memory. ||| siyi chen ||| 
2021 ||| transformer-based nmt: modeling, training and implementation. ||| hongfei xu ||| 
2017 ||| a computational model of visual attention. ||| jayachandra chilukamari ||| 
2019 ||| sensing physiological arousal and visual attention during user interaction. ||| oludamilare matthews ||| 
2017 ||| visual attention for high-fidelity imaging. ||| timothy bradley ||| 
2017 ||| trade-offs between focused and distributed temporal attention. ||| raluca david ||| 
2018 ||| visual attention mechanism in deep learning and its applications. ||| shiyang yan ||| 
2019 ||| perceptual issues of visual attention and depth perception in augmented reality. ||| michael l. long ||| 
2019 ||| topology and attention in computational pathology. ||| talha qaiser ||| 
2017 ||| ratives. (study and prediction of visual attention with deep learning networks in view of assessment of patients with neurodegenerative diseases). ||| souad chaabouni ||| 
2018 ||| e sur l'attention). ||| remi paulin ||| 
2019 ||| les d'attention). ||| thiziri belkacem ||| 
2021 ||| e sur l'attention). ||| marcely zanon boito ||| 
2017 ||| computational methods in the study of individuals' attention online. ||| nir grinberg ||| 
2018 ||| limited attention, the use of accounting information and its impacts on individual investment decision making. ||| bianca quirantes checon ||| 
2021 ||| computational methods for measurement of visual attention from videos towards large-scale behavioral analysis. ||| eunji chong ||| 
2018 ||| uncertainty estimation of visual attention models using spatiotemporal analysis. ||| tariq alshawi ||| 
2018 ||| attentional shapecontextnet for point cloud recognition. ||| sainan liu ||| 
2017 ||| visual search does not fully characterize feature-based selective attention: evidence from the centroid paradigm. ||| amy winter ||| 
2017 ||| using the centroid method to study feature based selective attention. ||| matthew inverso ||| 
2020 ||| computational modeling for visual attention analysis. ||| yingyue xu ||| 
2020 ||| adapting interaction based on users' visual attention. ||| baris serim ||| 
2019 ||| memory for problem solving: comparative studies in attention, working and long-term memory. ||| katarzyna bobrowicz ||| 
2017 ||| active attention for target detection and recognition in robot vision. ||| wentao luan ||| 
2020 ||| assisting students with attention deficit disorder through technology. ||| eleni didaskalou ||| 
2018 ||| a web application for reading and attentional assessments. ||| sara bertoni ||| andrea facoetti ||| sandro franceschini ||| claudio e. palazzi ||| daniele ronzani ||| 
2019 ||| investigating the variation of mental fatigue and attention control of obstructive sleep apnea patients. ||| farhad nassehi ||| galip  ||| zdemir ||| osman erogul ||| nart bedin atalay ||| melike yuceege ||| hikmet firat ||| 
2020 ||| feature extraction with bidirectional encoder representations from transformers in hyperspectral images. ||| ibrahim onur sigirci ||| hakan ozgur ||| gokhan bilgin ||| 
2019 ||| using morpheme-level attention mechanism for turkish sequence labelling. ||| yasin esref ||| burcu can ||| 
2020 ||| using adaptive locally connected layer in attention based deep neural network for speech command recognition. ||| yasemin turkan ||| f. boray tek ||| 
2017 ||| bit allocation with visual attention and visual distortion sensitivity. ||| mesut pak ||| ulug bayazit ||| 
2017 ||| maximum overlap discrete wavelet based transformer differential protection. ||| okan ozgonenel ||| serap karagol ||| 
2021 ||| twitter dataset and evaluation of transformers for turkish sentiment analysis. ||| abdullatif k ||| ksal ||| arzucan  ||| zg ||| r ||| 
2021 ||| surgical activity recognition with transformer networks. ||| simge nur kabatas ||| duygu sarikayas ||| 
2018 ||| a passive brain-computer interface for monitoring mental attention state. ||| murat kaya ||| igdem inan aci ||| yuriy mishchenko ||| 
2020 ||| attention model for extracting saliency map in driving videos. ||| ekrem aksoy ||| ahmet yazici ||| 
2021 ||| horizontal attention convolution layer for stereo matching. ||| alper emlek ||| murat pe ||| ker ||| 
2020 ||| transformer protection algorithm based on s-transform. ||| kubra nur akpinar ||| okan ozgonenel ||| unal kurt ||| 
2018 ||| hierarchical neural model with attention mechanisms for the classification of social media text related to mental health. ||| julia ive ||| george gkotsis ||| rina dutta ||| robert stewart ||| sumithra velupillai ||| 
2019 ||| fast domain adaptation of semantic parsers via paraphrase attention. ||| avik ray ||| yilin shen ||| hongxia jin ||| 
2019 ||| weakly supervised attentional model for low resource ad-hoc cross-lingual information retrieval. ||| lingjun zhao ||| rabih zbib ||| zhuolin jiang ||| damianos g. karakos ||| zhongqiang huang ||| 
2019 ||| deep bidirectional transformers for relation extraction without supervision. ||| yannis papanikolaou ||| ian roberts ||| andrea pierleoni ||| 
2017 ||| what lies above: alternative user experiences produced through focussing attention on gnss infrastructure. ||| christopher wood ||| stefan poslad ||| antonios kaniadakis ||| jennifer gabrys ||| 
2017 ||| attention from afar: simulating the gazes of remote participants in hybrid meetings. ||| bin xu ||| jason b. ellis ||| thomas erickson ||| 
2018 ||| attending to breath: exploring how the cues in a virtual environment guide the attention to breath and shape the quality of experience to support mindfulness. ||| mirjana prpa ||| kivan |||  tatar ||| jules fran ||| oise ||| bernhard e. riecke ||| thecla schiphorst ||| philippe pasquier ||| 
2021 ||| nomen est omen - the role of signatures in ascribing email author identity with transformer neural networks. ||| sudarshan srinivasan ||| edmon begoli ||| maria mahbub ||| kathryn knight ||| 
2021 ||| adversarial watermarking transformer: towards tracing text provenance with data hiding. ||| sahar abdelnabi ||| mario fritz ||| 
2021 ||| automatic modulation classification based on improved r-transformer. ||| xueyuan liu ||| 
2020 ||| a novel image classification model jointing attention and resnet for scratch. ||| shuaifei zhao ||| lichen qiu ||| peng qi ||| yan sun ||| 
2019 ||| an attention-mechanism-based traffic flow prediction scheme for smart city. ||| xiao hu ||| xin wei ||| yun gao ||| wenqin zhuang ||| mingzi chen ||| haibing lv ||| 
2021 ||| network intrusion detection based on dense dilated convolutions and attention mechanism. ||| ke cao ||| jinqi zhu ||| weiiia feng ||| chunmei ma ||| ming liu ||| tian du ||| 
2021 ||| a graph attention mechanism based multi-agent reinforcement learning method for efficient traffic light control. ||| changqing su ||| yan yan ||| tao wang ||| baoxian zhang ||| cheng li ||| 
2020 ||| ksf-st: video captioning based on key semantic frames extraction and spatio-temporal attention mechanism. ||| zhaowei qu ||| luhan zhang ||| xiaoru wang ||| bingyu cao ||| yueli li ||| fu li ||| 
2021 ||| residual transformer network for 3d objects classification. ||| shan meng ||| daoyuan liang ||| yumei li ||| 
2021 ||| topic enhanced multi-head co-attention: generating distractors for reading comprehension. ||| pengju shuai ||| zixi wei ||| sishun liu ||| xiaofei xu ||| li li ||| 
2020 ||| cascading top-down attention for visual question answering. ||| weidong tian ||| rencai zhou ||| zhongqiu zhao ||| 
2021 ||| learning dynamic coherence with graph attention network for biomedical entity linking. ||| mumeng bo ||| meihui zhang ||| 
2019 ||| urban area vehicle re-identification with self-attention stair feature fusion and temporal bayesian re-ranking. ||| chenghuan liu ||| du q. huynh ||| mark reynolds ||| 
2021 ||| attention-guided progressive partition network for human parsing. ||| xi huang ||| chengkun he ||| jie shao ||| 
2021 ||| annealing attention networks for user feature-based rumor early detection on weibo. ||| zhengwu luo ||| lina wang ||| wenqi wang ||| aoshuang ye ||| 
2019 ||| ta-blstm: tag attention-based bidirectional long short-term memory for service recommendation in mashup creation. ||| min shi ||| yufei tang ||| jianxun liu ||| 
2021 ||| multi-aspect controlled response generation in a multimodal dialogue system using hierarchical transformer network. ||| mauajama firdaus ||| nidhi thakur ||| asif ekbal ||| 
2020 ||| a cascaded step-temporal attention network for ecg arrhythmia classification. ||| yanyun tao ||| guoqi yue ||| kaixin wang ||| yuzhen zhang ||| bin jiang ||| 
2019 ||| a novel two-factor attention encoder-decoder network through combining temporal and prior knowledge for weather forecasting. ||| minglei yuan ||| xiaozhong ji ||| tong lu ||| pengfei chen ||| hualu zhang ||| 
2021 ||| pay attention: improving classification of pe malware using attention mechanisms based on system call analysis. ||| ori or-meir ||| aviad cohen ||| yuval elovici ||| lior rokach ||| nir nissim ||| 
2020 ||| sasrm: a semantic and attention spatio-temporal recurrent model for next location prediction. ||| xu zhang ||| boming li ||| chao song ||| zhengwen huang ||| yan li ||| 
2019 ||| condensed convolution neural network by attention over self-attention for stance detection in twitter. ||| shengping zhou ||| junjie lin ||| lianzhi tan ||| xin liu ||| 
2020 ||| penalty-based sequence generative adversarial networks with enhanced transformer for text generation. ||| mingjun duan ||| yubai li ||| 
2020 ||| an hardware-aware image polarity detector enhanced with visual attention. ||| edoardo ragusa ||| tommaso apicella ||| christian gianoglio ||| rodolfo zunino ||| paolo gastaldo ||| 
2020 ||| poem generation using transformers and doc2vec embeddings. ||| marvin c. santillan ||| arnulfo p. azcarraga ||| 
2021 ||| mask region-oriented diabetic retinopathies detection in ophthalmic medical images via non-local attention. ||| hongtian zhao ||| haoyue peng ||| zhongpai gao ||| shibao zheng ||| 
2021 ||| sequential recommendation with context-aware collaborative graph attention networks. ||| mengfei zhang ||| cheng guo ||| jiaqi jin ||| mao pan ||| jinyun fang ||| 
2021 ||| a relation-guided attention mechanism for relational triple extraction. ||| yi yang ||| xueming li ||| xu li ||| 
2021 ||| a spatial-temporal graph attention network for multi-intersection traffic light control. ||| liu he ||| qing'an li ||| libing wu ||| min wang ||| jianxin li ||| dan wu ||| 
2021 ||| ccgl-gan: criss-cross attention and global-local discriminator generative adversarial networks for text-to-image synthesis. ||| xihong ye ||| lu lu ||| 
2021 ||| macro discourse relation recogniztion based on micro discourse structure and self-interactive attention network. ||| yaxin fan ||| feng jiang ||| peifeng li ||| qiaoming zhu ||| 
2021 ||| a structural transformer with relative positions in trees for code-to-sequence tasks. ||| johannes villmow ||| adrian ulges ||| ulrich schwanecke ||| 
2019 ||| attention-based adversarial training for seamless nudity censorship. ||| gabriel s. sim ||| es ||| jonatas wehrmann ||| rodrigo c. barros ||| 
2019 ||| ensemble attention for text recognition in natural images. ||| hongchao gao ||| yujia li ||| xi wang ||| jizhong han ||| ruixuan li ||| 
2020 ||| using self-attention lstms to enhance observations in goal recognition. ||| leonardo amado ||| gabriel paludo licks ||| matheus marcon ||| ramon fraga pereira ||| felipe meneguzzi ||| 
2019 ||| multi-perspective feature generation based on attention mechanism. ||| longxuan ma ||| lei zhang ||| 
2018 ||| a novel document classification algorithm based on statistical features and attention mechanism. ||| chao li ||| yanfen cheng ||| hongxia wang ||| 
2020 ||| object detection with extended attention and spatial information. ||| yingda guan ||| zuochang ye ||| yan wang ||| 
2021 ||| region attention network for single image super-resolution. ||| xiaobiao du ||| chongjin liu ||| xiaoling yang ||| 
2019 ||| hierarchical recurrent attention networks for context-aware education chatbots. ||| jean-baptiste aujogue ||| alex aussem ||| 
2021 ||| deep recurrent neural networks with attention mechanisms for respiratory anomaly classification. ||| conor wall ||| li zhang ||| yonghong yu ||| kamlesh mistry ||| 
2021 ||| a novel joint model with second-order features and matching attention for aspect-based sentiment analysis. ||| guozheng rao ||| xinru gu ||| zhiyong feng ||| qing cong ||| li zhang ||| 
2021 ||| attentional social recommendation system with graph convolutional network. ||| yanbin jiang ||| huifang ma ||| yuhang liu ||| zhixin li ||| 
2020 ||| attention-based 3d object reconstruction from a single image. ||| andrey de aguiar salvi ||| nathan gavenski ||| eduardo h. p. pooch ||| felipe tasoniero ||| rodrigo c. barros ||| 
2019 ||| bci and multimodal feedback based attention regulation for lower limb rehabilitation. ||| jiaxing wang ||| weiqun wang ||| zeng-guang hou ||| weiguo shi ||| xu liang ||| shixin ren ||| liang peng ||| yan-jie zhou ||| 
2019 ||| improving universal language model fine-tuning using attention mechanism. ||| fl ||| vio arthur o. santos ||| karina l. ponce-guevara ||| david mac ||| do ||| cleber zanchettin ||| 
2020 ||| unleashing the potential of attention model for news headline generation. ||| yong liao ||| kui meng ||| jianshen zhang ||| gongshen liu ||| 
2019 ||| a gan model with self-attention mechanism to generate multi-instruments symbolic music. ||| faqian guan ||| chunyan yu ||| suqiong yang ||| 
2020 ||| dynamic graph attention-aware networks for session-based recommendation. ||| ahed abugabah ||| xiaochun cheng ||| jianfeng wang ||| 
2021 ||| attention based double layer lstm for chinese image captioning. ||| wei wu ||| deshuai sun ||| 
2019 ||| an attention-based hybrid lstm-cnn model for arrhythmias classification. ||| fan liu ||| xingshe zhou ||| tianben wang ||| jinli cao ||| zhu wang ||| hua wang ||| yanchun zhang ||| 
2021 ||| a dual-questioning attention network for emotion-cause pair extraction with context awareness. ||| qixuan sun ||| yaqi yin ||| hong yu ||| 
2019 ||| attention-driven driving maneuver detection system. ||| xishuai peng ||| ava zhao ||| song wang ||| yi lu murphey ||| yuanxiang li ||| 
2019 ||| convolutional lstm network with hierarchical attention for relation classification in clinical texts. ||| li tang ||| fei teng ||| zheng ma ||| lufei huang ||| ming xiao ||| xuan li ||| 
2020 ||| automatic lyrics transcription using dilated convolutional neural networks with self-attention. ||| emir demirel ||| sven ahlb ||| ck ||| simon dixon ||| 
2021 ||| main: multihead-attention imputation networks. ||| spyridon mouselinos ||| kyriakos polymenakos ||| antonis nikitakis ||| konstantinos kyriakopoulos ||| 
2019 ||| self-attention based network for medical query expansion. ||| su chen ||| qinmin vivian hu ||| yang song ||| yun he ||| huaying wu ||| liang he ||| 
2019 ||| dynamic fusion of convolutional features based on spatial and temporal attention for visual tracking. ||| dongcheng zhao ||| yi zeng ||| 
2021 ||| attention-based multi-proximity preserved attributed network embedding. ||| xing-xing liu ||| kai wang ||| chang-dong wang ||| ling huang ||| 
2020 ||| neighborhood-aware attention network for semi-supervised face recognition. ||| qi zhang ||| zhen lei ||| stan z. li ||| 
2020 ||| m3la: a novel approach based on encoder-decoder with attention framework for multi-modal multi-label learning. ||| yinlong zhu ||| yi zhang ||| 
2020 ||| double attention for pathology image diagnosis network with visual interpretability. ||| hao cheng ||| kaijie wu ||| kai ma ||| jie tian ||| rui xu ||| chaochen gu ||| xinping guan ||| 
2018 ||| a neural generation-based conversation model using fine-grained emotion-guide attention. ||| zhiheng zhou ||| man lan ||| yuanbin wu ||| 
2019 ||| character-aware convolutional recurrent networks with self-attention for emotion detection on twitter. ||| jiangping huang ||| chunli xiang ||| shuwei yuan ||| desen yuan ||| xiaorui huang ||| 
2020 ||| federated multi-task learning with hierarchical attention for sensor data analytics. ||| yujing chen ||| yue ning ||| zheng chai ||| huzefa rangwala ||| 
2021 ||| multi-actor-attention-critic reinforcement learning for central place foraging swarms. ||| ning yang ||| qi lu ||| kele xu ||| bo ding ||| zijian gao ||| 
2019 ||| directional attention based video frame prediction using graph convolutional networks. ||| prateep bhattacharjee ||| sukhendu das ||| 
2021 ||| multiplicative attention mechanism for multi-horizon time series forecasting. ||| runpeng cui ||| jianqiang wang ||| zheng wang ||| 
2020 ||| multi-channel co-attention network for visual question answering. ||| weidong tian ||| bin he ||| nanxun wang ||| zhongqiu zhao ||| 
2017 ||| classification of radiology reports using neural attention models. ||| bonggun shin ||| falgun h. chokshi ||| timothy lee ||| jinho d. choi ||| 
2020 ||| heterogeneous graph attention networks for early detection of rumors on twitter. ||| qi huang ||| junshuai yu ||| jia wu ||| bin wang ||| 
2019 ||| pyramid attention dense network for image super-resolution. ||| sibao chen ||| chao hu ||| bin luo ||| chris h. q. ding ||| shilei huang ||| 
2021 ||| a method of relation extraction: integrating graph convolutional networks, relative entity position attention and back-multi-head-attention mechanism. ||| ci liu ||| xuetong zhao ||| yawei zhao ||| 
2021 ||| parallel scale-wise attention network for effective scene text recognition. ||| usman sajid ||| michael chow ||| jin zhang ||| taejoon kim ||| guanghui wang ||| 
2020 ||| heterogeneous multi-modal sensor fusion with hybrid attention for exercise recognition. ||| anjana wijekoon ||| nirmalie wiratunga ||| kay cooper ||| 
2020 ||| scene attention mechanism for remote sensing image caption generation. ||| shiqi wu ||| xiangrong zhang ||| xin wang ||| chen li ||| licheng jiao ||| 
2020 ||| wavelet denoising and attention-based rnn- arima model to predict forex price. ||| zhiwen zeng ||| matloob khushi ||| 
2021 ||| a multimodal classification of noisy hate speech using character level embedding and attention. ||| nishchal prasad ||| sriparna saha ||| pushpak bhattacharyya ||| 
2020 ||| attention and graph matching network for retrieval-based dialogue system with domain knowledge. ||| xu li ||| jinghua zhu ||| 
2021 ||| local frequency domain transformer networks for video prediction. ||| hafez farazi ||| jan nogga ||| sven behnke ||| 
2018 ||| distant supervision for relation extraction with hierarchical attention and entity descriptions. ||| heng she ||| bin wu ||| bai wang ||| renjun chi ||| 
2021 ||| universal transformer hawkes process. ||| lu-ning zhang ||| jian-wei liu ||| zhi-yan song ||| xin zuo ||| wei-min li ||| ze-yu liu ||| 
2021 ||| edge prior and spatial attention fusion enhanced hierarchical multi-patch network for image deblurring. ||| yafeng zhao ||| hui cui ||| binyu zhao ||| jiquan ma ||| 
2021 ||| automatic topic labeling model with paired-attention based on pre-trained deep neural network. ||| dongbin he ||| yan-zhao ren ||| abdul mateen khattak ||| xinliang liu ||| sha tao ||| wanlin gao ||| 
2020 ||| dynamic global-local attention network based on capsules for text classification. ||| ji wang ||| qiaohong chen ||| haolei pei ||| qi sun ||| yubo jia ||| 
2019 ||| attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| hao tang ||| dan xu ||| nicu sebe ||| yan yan ||| 
2021 ||| multiple self-attention network for intracranial vessel segmentation. ||| yang li ||| jiajia ni ||| ahmed elazab ||| jianhuang wu ||| 
2021 ||| embedding extra knowledge and a dependency tree based on a graph attention network for aspect-based sentiment analysis. ||| yuanlin li ||| xiao sun ||| meng wang ||| 
2019 ||| an end-to-end location and regression tracker with attention-based fused features. ||| qinyi zhang ||| shishuai du ||| huihua yang ||| 
2020 ||| knowledge-based context-aware multi-turn conversational model with hierarchical attention. ||| chunquan chen ||| si li ||| 
2020 ||| nasabn: a neural architecture search framework for attention-based networks. ||| kun jing ||| jungang xu ||| hui xu ||| 
2021 ||| a multi -role graph attention network for knowledge graph alignment. ||| linyi ding ||| weijie yuan ||| kui meng ||| gongshen liu ||| 
2020 ||| heterogeneous information network embedding with convolutional graph attention networks. ||| meng cao ||| xiying ma ||| kai zhu ||| ming xu ||| chongjun wang ||| 
2021 ||| generating fake cyber threat intelligence using transformer-based models. ||| priyanka ranade ||| aritran piplai ||| sudip mittal ||| anupam joshi ||| tim finin ||| 
2021 ||| spatial generation of molecules with transformers. ||| tim cofala ||| thomas teusch ||| oliver kramer ||| 
2021 ||| novel soft sensor model based on spatio-temporal attention. ||| xuan hu ||| zhiqiang geng ||| yongming han ||| wei huang ||| kai chen ||| feng xie ||| 
2019 ||| improving sentence representations with local and global attention for classification. ||| zesheng liu ||| xu bai ||| tian cai ||| chanjuan chen ||| wang zhang ||| lei jiang ||| 
2021 ||| transformer with local-feature extractor for relation extraction. ||| lihan liu ||| pengfei li ||| 
2021 ||| deep personalized glucose level forecasting using attention-based recurrent neural networks. ||| mohammadreza armandpour ||| brian kidd ||| yu du ||| jianhua z. huang ||| 
2020 ||| vocoder-free end-to-end voice conversion with transformer network. ||| june-woo kim ||| ho-young jung ||| minho lee ||| 
2021 ||| dense video captioning with hierarchical attention-based encoder-decoder networks. ||| mingjing yu ||| huicheng zheng ||| zehua liu ||| 
2018 ||| classify sentence from multiple perspectives with category expert attention network. ||| shiyun chen ||| maoquan wang ||| jiacheng zhang ||| liang he ||| 
2021 ||| link prediction with multiple structural attentions in multiplex networks. ||| shangrong huang ||| quanyu ma ||| chao yang ||| yazhou yao ||| 
2021 ||| attention-based lstm for motion switching detection of particles in living cells. ||| bo you ||| ge yang ||| 
2018 ||| a fully attention-based information retriever. ||| alvaro henrique chaim correia ||| jorge luiz moreira silva ||| thiago de castro martins ||| f ||| bio gagliardi cozman ||| 
2020 ||| dsmith: compiler fuzzing through generative deep learning model with attention. ||| haoran xu ||| yongjun wang ||| shuhui fan ||| peidai xie ||| aizhi liu ||| 
2021 ||| pgmanet: pose-guided mixed attention network for occluded person re-identification. ||| you zhai ||| xianfeng han ||| wenzhuo ma ||| xinye gou ||| guoqiang xiao ||| 
2020 ||| grammatical error detection with self attention by pairwise training. ||| quanbin wang ||| ying tan ||| 
2019 ||| text classification using gated and transposed attention networks. ||| kang he ||| min zhu ||| 
2019 ||| abstractive summarization with keyword and generated word attention. ||| qianlong wang ||| jiangtao ren ||| 
2021 ||| concept-based topic attention for a convolutional sequence document summarization model. ||| shirin akther khanam ||| fei liu ||| yi-ping phoebe chen ||| 
2021 ||| sintactical distance attention guided graph convolutional network for aspect-based sentiment analisis. ||| luwei xiao ||| donghong gu ||| yun xue ||| xiaohui hu ||| yongsheng zhu ||| 
2020 ||| a dual transformer model for intelligent decision support for maintenance of wind turbines. ||| joyjit chatterjee ||| nina dethlefs ||| 
2021 ||| dual multi-task network with bridge-temporal-attention for student emotion recognition via classroom video. ||| jun he ||| li peng ||| bo sun ||| lejun yu ||| meng guo ||| 
2021 ||| fake news detection on news-oriented heterogeneous information networks through hierarchical graph attention. ||| yuxiang ren ||| jiawei zhang ||| 
2021 ||| attention-based spatio-temporal graphic lstm for eeg emotion recognition. ||| xiaoxu li ||| wenming zheng ||| yuan zong ||| hongli chang ||| cheng lu ||| 
2021 ||| weakly labeled semi-supervised sound event detection with multi-scale residual attention. ||| maolin tang ||| qijun zhao ||| zhengxi liu ||| 
2019 ||| focalnet - foveal attention for post-processing dnn outputs. ||| burhan ahmad mudassar ||| saibal mukhopadhyay ||| 
2020 ||| transkp: transformer based key-phrase extraction. ||| mukund rungta ||| rishabh kumar ||| mehak preet dhaliwal ||| hemant tiwari ||| vanraj vala ||| 
2021 ||| personalized session-based recommendation using graph attention networks. ||| yongquan xie ||| zhengru li ||| tian qin ||| finn tseng ||| johannes kristinsson ||| shiqi qiu ||| yi lu murphey ||| 
2017 ||| mipal: multiple-instance passive aggressive learning for identification of attention deficit hyperactive disorder from fmri. ||| prabhash kumarasinghe ||| suresh sundaram ||| vigneshwaran subbaraju ||| 
2021 ||| prototypical inception network with cross branch attention for time series classification. ||| jingyu sun ||| susumu takeuchi ||| ikuo yamasaki ||| 
2021 ||| curvilinear collaborative metric learning with macro-micro attentions. ||| hangbin zhang ||| raymond k. wong ||| victor w. chu ||| 
2021 ||| multi-scale attention network based on multi-feature fusion for person re-identification. ||| minghao li ||| liming yuan ||| xianbin wen ||| jianchen wang ||| gengsheng xie ||| yansong jia ||| 
2021 ||| ssrnas: search space reduced one-shot nas by a recursive attention-based predictor with cell tensor-flow diagram. ||| yue liu ||| kai zhu ||| zitu liu ||| 
2020 ||| a dynamic-attention on crowd region with physical optical flow features for crowd counting. ||| qian wang ||| wenxi li ||| songjian chen ||| rui feng ||| 
2021 ||| simdet: cross similarity attention for one-shot object detection. ||| rujia cai ||| yingjie qin ||| lizhe qi ||| yunquan sun ||| 
2018 ||| multi-granularity hierarchical attention siamese network for visual tracking. ||| xing chen ||| xiang zhang ||| huibin tan ||| long lan ||| zhigang luo ||| xuhui huang ||| 
2021 ||| ada: adaptive depth attention model for click - through rate prediction. ||| shujin liu ||| derong chen ||| jie shao ||| 
2021 ||| discrete auto-regressive variational attention models for text modeling. ||| xianghong fang ||| haoli bai ||| jian li ||| zenglin xu ||| michael r. lyu ||| irwin king ||| 
2021 ||| attention-based multi-filter convolutional neural network for inappropriate speech detection. ||| shu-yu lin ||| yi-ling chen ||| 
2019 ||| visual relationship attention for image captioning. ||| zongjian zhang ||| yang wang ||| qiang wu ||| fang chen ||| 
2018 ||| automatic chromosome classification using deep attention based sequence learning of chromosome bands. ||| monika sharma ||| swati ||| lovekesh vig ||| 
2021 ||| 3d multi-branch encoder-decoder networks with attentional feature fusion for pulmonary nodule detection in ct scans. ||| chenjiao zhang ||| lulu wang ||| xing wu ||| zhongshi he ||| 
2019 ||| image captioning based on sentence-level and word-level attention. ||| haiyang wei ||| zhixin li ||| canlong zhang ||| tao zhou ||| yu quan ||| 
2021 ||| hierarchical attention transformer networks for long document classification. ||| yongli hu ||| puman chen ||| tengfei liu ||| junbin gao ||| yanfeng sun ||| baocai yin ||| 
2018 ||| attention-based bilstm network with lexical feature for emotion classification. ||| kai gao ||| hua xu ||| chengliang gao ||| hanyong hao ||| junhui deng ||| xiaomin sun ||| 
2021 ||| transformer-based relation detect model for aspect-based sentiment analysis. ||| zixi wei ||| xiaofei xu ||| lijian li ||| kaixin qin ||| li li ||| 
2020 ||| adaptive graph convolutional networks with attention mechanism for relation extraction. ||| zhixin li ||| yara sun ||| suqin tang ||| canlong zhang ||| huifang ma ||| 
2020 ||| toward tag-free aspect based sentiment analysis: a multiple attention network approach. ||| yao qiang ||| xin li ||| dongxiao zhu ||| 
2020 ||| cccnet: an attention based deep learning framework for categorized counting of crowd in different body states. ||| sarkar snigdha sarathi das ||| syed md. mukit rashid ||| mohammed eunus ali ||| 
2018 ||| fine-grained air quality prediction using attention based neural network. ||| tianyu liu ||| yongzhi ying ||| yanyan xu ||| dengfeng ke ||| kaile su ||| 
2020 ||| hierarchical component-attention based speaker turn embedding for emotion recognition. ||| shuo liu ||| jinlong jiao ||| ziping zhao ||| judith dineley ||| nicholas cummins ||| bj ||| rn w. schuller ||| 
2020 ||| a bert-based approach with relation-aware attention for knowledge base question answering. ||| da luo ||| jindian su ||| shanshan yu ||| 
2019 ||| distant supervised why-question generation with passage self-matching attention. ||| jiaxin hu ||| zhixu li ||| renshou wu ||| hongling wang ||| an liu ||| jiajie xu ||| pengpeng zhao ||| lei zhao ||| 
2020 ||| forecasting photovoltaic power production using a deep learning sequence to sequence model with attention. ||| elizaveta kharlova ||| daniel may ||| petr mus ||| lek ||| 
2021 ||| predicting conversation outcomes using multimodal transformer. ||| can li ||| wenbo wang ||| bitty balducci ||| detelina marinova ||| yi shang ||| 
2021 ||| tag-aware attentional graph neural networks for personalized tag recommendation. ||| ruoran huang ||| chuanqi han ||| li cui ||| 
2018 ||| ta4rec: recurrent neural networks with time attention factors for session-based recommendations. ||| yu sun ||| peize zhao ||| honggang zhang ||| 
2021 ||| deep-attention model to analyze reliable customers via federated learning. ||| usman ahmed ||| jerry chun-wei lin ||| gautam srivastava ||| 
2020 ||| transformer decoder based reinforcement learning approach for conversational response generation. ||| farshid faal ||| jia yuan yu ||| ketra schmitt ||| 
2020 ||| multimodal emotion recognition using deep generalized canonical correlation analysis with an attention mechanism. ||| yu-ting lan ||| wei liu ||| bao-liang lu ||| 
2020 ||| dynamic attention aggregation with bert for neural machine translation. ||| jia-rui zhang ||| hongzheng li ||| shumin shi ||| heyan huang ||| yue hu ||| xiangpeng wei ||| 
2019 ||| neural networks applied in the prediction of top oil temperature of transformer. ||| wenxia pan ||| kun zhao ||| tianao gao ||| congchuang gao ||| 
2020 ||| cascade modeling with multihead self-attention. ||| chaochao liu ||| wenjun wang ||| pengfei jiao ||| xue chen ||| yueheng sun ||| 
2021 ||| cacnet: cube attentional cnn for automatic speech recognition. ||| nan zhang ||| jianzong wang ||| wenqi wei ||| xiaoyang qu ||| ning cheng ||| jing xiao ||| 
2019 ||| spatial-temporal attention network for malware detection using micro-architecture features. ||| fang li ||| jinrong han ||| ziyuan zhu ||| dan meng ||| 
2021 ||| fa-gal-resnet: lightweight residual network using focused attention mechanism and generative adversarial learning via knowledge distillation. ||| hequn yang ||| ting lu ||| wenjing guo ||| shan chang ||| guohua liu ||| yiyang luo ||| 
2021 ||| a relation-aware attention neural network for modeling the usage of scientific online resources. ||| yongxiu xu ||| heyan huang ||| chong feng ||| chuan zhou ||| jiarui zhang ||| yue hu ||| 
2020 ||| robust semi-supervised semantic segmentation based on self-attention and spectral normalization. ||| jia zhang ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2019 ||| a transformer-based variational autoencoder for sentence generation. ||| danyang liu ||| gongshen liu ||| 
2021 ||| visual explanation using attention mechanism in actor-critic-based deep reinforcement learning. ||| hidenori itaya ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| komei sugiura ||| 
2019 ||| text attention and focal negative loss for scene text detection. ||| randong huang ||| bo xu ||| 
2021 ||| eacoupledcf: an enhanced attention-based coupled collaborative filtering approach for recommendation. ||| feng zhang ||| xiangfu meng ||| ruimin chai ||| quangui zhang ||| 
2021 ||| nvnet: an enhanced attention network for segmenting neck vascular from ultrasound images. ||| bohao zhang ||| changbo wang ||| chenhui li ||| 
2020 ||| dual semantic relationship attention network for image-text matching. ||| keyu wen ||| xiaodong gu ||| 
2020 ||| convolutional transformer with sentiment-aware attention for sentiment analysis. ||| pengfei li ||| peixiang zhong ||| jiaheng zhang ||| kezhi mao ||| 
2019 ||| stochastic imputation and uncertainty-aware attention to ehr for mortality prediction. ||| eunji jun ||| ahmad wisnu mulyadi ||| heung-il suk ||| 
2021 ||| driving maneuver detection using features of driver's attention and face shift through deeping learning. ||| song wang ||| yi lu murphey ||| 
2021 ||| gap-wf: graph attention pooling network for fine-grained ssl/tls website fingerprinting. ||| jie lu ||| gaopeng gou ||| majing su ||| dong song ||| chang liu ||| chen yang ||| yangyang guan ||| 
2021 ||| cap-gan: towards adversarial robustness with cycle-consistent attentional purification. ||| mingu kang ||| trung quang tran ||| seung ju cho ||| daeyoung kim ||| 
2020 ||| big-transformer: integrating hierarchical features for transformer via bipartite graph. ||| xiaobo shu ||| mengge xue ||| yanzeng li ||| zhenyu zhang ||| tingwen liu ||| 
2021 ||| integrating argument-level attention with multi-level scores to predict what happen next. ||| zhenyu huang ||| yongjun wang ||| hongzuo xu ||| songlei jian ||| 
2021 ||| fa-iati: a framework of frequency adaptive and iterative attention interaction for image-text matching. ||| youxuan qin ||| jing zhao ||| ming li ||| chao sun ||| 
2021 ||| cans: coupled-attention networks for sarcasm detection on social media. ||| xuan zhao ||| jimmy huang ||| haitian yang ||| 
2018 ||| interpretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling. ||| kaixuan chen ||| lina yao ||| xianzhi wang ||| dalin zhang ||| tao gu ||| zhiwen yu ||| zheng yang ||| 
2021 ||| heterogeneous graph gated attention network. ||| shuai ma ||| jian-wei liu ||| xin zuo ||| wei-min li ||| 
2020 ||| scene text recognition by attention network with gated embedding. ||| cong wang ||| cheng-lin liu ||| 
2019 ||| data-to-text generation with attention recurrent unit. ||| hechong wang ||| wei zhang ||| yuesheng zhu ||| zhiqiang bai ||| 
2017 ||| designing an adaptive attention mechanism for relation classification. ||| pengda qin ||| weiran xu ||| jun guo ||| 
2021 ||| mcdalnet: multi-scale contextual dual attention learning network for medical image segmentation. ||| pengcheng guo ||| xiangdong su ||| haoran zhang ||| feilong bao ||| 
2021 ||| bilingual self-attention network: generating headlines for online linguistic questions. ||| mingchao lit ||| haozhuang liu ||| yang wang ||| hai-tao zheng ||| 
2021 ||| spatio-temporal action detector with self-attention. ||| xurui ma ||| zhigang luo ||| xiang zhang ||| qing liao ||| xingyu shen ||| mengzhu wang ||| 
2019 ||| dagcn: dual attention graph convolutional networks. ||| fengwen chen ||| shirui pan ||| jing jiang ||| huan huo ||| guodong long ||| 
2021 ||| attention-based seq2seq regularisation for relation extraction. ||| haojie huang ||| raymond k. wong ||| 
2020 ||| predicting outcomes of chemical reactions: a seq2seq approach with multi-view attention and edge embedding. ||| xia xiao ||| chao shang ||| jinbo bi ||| sanguthevar rajasekaran ||| 
2021 ||| boost transformer with bert and copying mechanism for asr error correction. ||| wenkun li ||| hui di ||| lina wang ||| kazushige ouchi ||| jing lu ||| 
2021 ||| information reuse attention in convolutional neural networks for facial expression recognition in the wild. ||| chuang wang ||| ruimin hu ||| 
2021 ||| a contextual attention network for multimodal emotion recognition in conversation. ||| tana wang ||| yaqing hou ||| dongsheng zhou ||| qiang zhang ||| 
2021 ||| a visual self-attention network for facial expression recognition. ||| naigong yu ||| deguo bai ||| 
2021 ||| out-of-distribution detection with uncertainty enhanced attention maps. ||| yue gao ||| qinliang su ||| 
2019 ||| hierarchical multi-dimensional attention model for answer selection. ||| wei liu ||| lei zhang ||| longxuan ma ||| pengfei wang ||| feng zhang ||| 
2020 ||| gcn-lrp explanation: exploring latent attention of graph convolutional networks. ||| jinlong hu ||| tenghui li ||| shoubin dong ||| 
2019 ||| attention-driven multi-sensor selection. ||| stefan braun ||| daniel neil ||| jithendar anumula ||| enea ceolini ||| shih-chii liu ||| 
2021 ||| multi-scale spatial transformer network for lidar-camera 3d object detection. ||| zhifan wang ||| xiaohong zhang ||| shidong wang ||| tong xin ||| haofeng zhang ||| jianfeng lu ||| 
2021 ||| hcov: a target attention-based filter pruning with retaining high-covariance feature map. ||| chenrui zhang ||| yinan ma ||| jing wu ||| chengnian long ||| 
2020 ||| an improved template representation-based transformer for abstractive text summarization. ||| jiaming sun ||| yunli wang ||| zhoujun li ||| 
2021 ||| multi-attention based spatial-temporal graph convolution networks for traffic flow forecasting. ||| jun hu ||| liyin chen ||| 
2019 ||| attention-based multi-instance neural network for medical diagnosis from incomplete and low quality data. ||| zeyuan wang ||| josiah poon ||| shiding sun ||| simon k. poon ||| 
2017 ||| fusing attention with visual question answering. ||| ryan burt ||| mihael cudic ||| jos |||  c. pr ||| ncipe ||| 
2021 ||| func-esim: a dual pairwise attention network for cross-version binary function matching. ||| degang sun ||| yunting guo ||| min yu ||| gang li ||| chao liu ||| jianguo jiang ||| weiqing huang ||| 
2021 ||| dtgan: dual attention generative adversarial networks for text-to-image generation. ||| zhenxing zhang ||| lambert schomaker ||| 
2021 ||| a transformer based multi-task model for domain classification, intent detection and slot-filling. ||| tulika saha ||| neeti priya ||| sriparna saha ||| pushpak bhattacharyya ||| 
2021 ||| context and knowledge enriched transformer framework for emotion recognition in conversations. ||| soumitra ghosh ||| deeksha varshney ||| asif ekbal ||| pushpak bhattacharyya ||| 
2021 ||| attention-based graph neural network for news recommendation. ||| zhenyan ji ||| mengdan wu ||| jirui liu ||| jos |||  enrique armend ||| riz-i ||| igo ||| 
2020 ||| personalized destination prediction using transformers in a contextless data setting. ||| athanasios tsiligkaridis ||| jing zhang ||| hiroshi taguchi ||| daniel nikovski ||| 
2021 ||| ptwa: pre-training with word attention for chinese named entity recognition. ||| kaixin ma ||| meiling liu ||| tiejun zhao ||| jiyun zhou ||| yang yu ||| 
2021 ||| author name disambiguation using multiple graph attention networks. ||| zhiqiang zhang ||| chunqi wu ||| zhao li ||| juanjuan peng ||| haiyan wu ||| haiyu song ||| shengchun deng ||| biao wang ||| 
2019 ||| an attention-based model for learning dynamic interaction networks. ||| sandro cavallari ||| soujanya poria ||| erik cambria ||| vincent w. zheng ||| hongyun cai ||| 
2020 ||| toward improving the evaluation of visual attention models: a crowdsourcing approach. ||| dario zanca ||| stefano melacci ||| marco gori ||| 
2021 ||| transfake: multi-task transformer for multimodal enhanced fake news detection. ||| quanliang jing ||| di yao ||| xinxin fan ||| baoli wang ||| haining tan ||| xiangpeng bu ||| jingping bi ||| 
2020 ||| attention-based multi-model ensemble for automatic cataract detection in b-scan eye ultrasound images. ||| xiaofei zhang ||| jiancheng lv ||| heng zheng ||| yongsheng sang ||| 
2021 ||| a generative bayesian graph attention network for semi-supervised classification on scarce data. ||| zhongtian sun ||| anoushka harit ||| jialin yu ||| alexandra i. cristea ||| noura al moubayed ||| 
2020 ||| attention-based deep learning model for text readability evaluation. ||| yuxuan sun ||| keying chen ||| lin sun ||| chenlu hu ||| 
2019 ||| adpr: an attention-based deep learning point-of-interest recommendation framework. ||| junjie yin ||| yun li ||| zheng liu ||| jian xu ||| bin xia ||| qianmu li ||| 
2021 ||| on the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition. ||| negar heidari ||| alexandros iosifidis ||| 
2019 ||| graph convolutional networks with structural attention model for aspect based sentiment analysis. ||| junjie chen ||| hongxu hou ||| yatu ji ||| jing gao ||| 
2021 ||| dual sequential recommendation integrating high-order collaborative relations via graph attention networks. ||| yuehong wu ||| juan yang ||| 
2021 ||| revisiting the onsets and frames model with additive attention. ||| kin wai cheuk ||| yin-jyun luo ||| emmanouil benetos ||| dorien herremans ||| 
2018 ||| robust human action recognition using global spatial-temporal attention for human skeleton data. ||| yun han ||| sheng-luen chung ||| arul-murugan ambikapathi ||| jui-shan chan ||| wei-you lin ||| shun-feng su ||| 
2021 ||| attention and adaptive bilinear matching network for cross-domain few-shot defect classification of industrial parts. ||| liangbing sa ||| chongchong yu ||| ziyan chen ||| xia zhao ||| yafeng yang ||| 
2020 ||| bilinear semi-tensor product attention (bstpa) model for visual question answering. ||| zongwen bai ||| ying li ||| meili zhou ||| di li ||| dong wang ||| dawid polap ||| marcin wozniak ||| 
2021 ||| multitask learning using bert with task-embedded attention. ||| lukasz maziarka ||| tomasz danel ||| 
2020 ||| att: attention-based timbre transfer. ||| deepak kumar jain ||| akshi kumar ||| linqin cai ||| siddharth singhal ||| vaibhav kumar ||| 
2020 ||| a semantic subgraphs based link prediction method for heterogeneous social networks with graph attention networks. ||| kai zhu ||| meng cao ||| 
2020 ||| event recognition with automatic album detection based on sequential grouping of confidence scores and neural attention. ||| andrey v. savchenko ||| 
2021 ||| paying attention: using a siamese pyramid network for the prediction of protein-protein interactions with folding and self-binding primary sequences. ||| junzheng wu ||| eric paquet ||| herna l. viktor ||| wojtek michalowski ||| 
2020 ||| rad: reinforced attention decoder model on question generation. ||| xin li ||| zhen huang ||| feng liu ||| changjian wang ||| minghao hu ||| shiyi xu ||| yuxing peng ||| 
2021 ||| tt2inet: text to photo-realistic image synthesis with transformer as text encoder. ||| jianwei zhu ||| zhixin li ||| huifang ma ||| 
2020 ||| multi-object tracking via multi-attention. ||| xianrui wang ||| hefei ling ||| jiazhong chen ||| ping li ||| 
2019 ||| ta-stan: a deep spatial-temporal attention learning framework for regional traffic accident risk prediction. ||| lei zhu ||| tianrui li ||| shengdong du ||| 
2020 ||| att-darts: differentiable neural architecture search for attention. ||| kohei nakai ||| takashi matsubara ||| kuniaki uehara ||| 
2019 ||| abstractive text summarization with multi-head attention. ||| jinpeng li ||| chuang zhang ||| xiaojun chen ||| yanan cao ||| pengcheng liao ||| peng zhang ||| 
2021 ||| an image captioning approach using dynamical attention. ||| changzhi wang ||| xiaodong gu ||| 
2020 ||| multivariate time series classification with an attention-based multivariate convolutional neural network. ||| achyut mani tripathi ||| rashmi dutta baruah ||| 
2019 ||| spinal stenosis detection in mri using modular coordinate convolutional attention networks. ||| uddeshya upadhyay ||| badrinath singhal ||| meenakshi singh ||| 
2021 ||| temporal convolutional attention neural networks for time series forecasting. ||| yang lin ||| irena koprinska ||| mashud rana ||| 
2018 ||| semantic image segmentation based on attentions to intra scales and inner channels. ||| hongchao lu ||| zhidong deng ||| xiaolong liu ||| 
2020 ||| a transformer based approach for identification of tweet acts. ||| tulika saha ||| aditya prakash patra ||| sriparna saha ||| pushpak bhattacharyya ||| 
2021 ||| critic boosting attention network on local descriptor for few-shot learning. ||| chengzhang shi ||| chung-ming own ||| ching-chih chou ||| bailu guo ||| 
2021 ||| extended attention mechanism for tsp problem. ||| hua yang ||| 
2019 ||| deep fusion: an attention guided factorized bilinear pooling for audio-video emotion recognition. ||| yuanyuan zhang ||| zi-rui wang ||| jun du ||| 
2021 ||| ia-cnn: a generalised interpretable convolutional neural network with attention mechanism. ||| zhisong zhang ||| yaran chen ||| haoran li ||| qichao zhang ||| 
2019 ||| question answering with hierarchical attention networks. ||| tayfun alpay ||| stefan heinrich ||| michael nelskamp ||| stefan wermter ||| 
2020 ||| pointing direction estimation for attention target extraction using body-mounted camera. ||| yusei oozono ||| hirotake yamazoe ||| joo-ho lee ||| 
2018 ||| transformer-darwin: a hybrid locomotion humanoid designed to walk or roll. ||| jean chagas vaz ||| paul y. oh ||| 
2020 ||| attention-model guided image enhancement for robotic vision applications. ||| ming yi ||| wanxiang li ||| armagan elibol ||| nak young chong ||| 
2021 ||| syntactic knowledge-infused transformer and bert models. ||| dhanasekar sundararaman ||| vivek subramanian ||| guoyin wang ||| shijing si ||| dinghan shen ||| dong wang ||| lawrence carin ||| 
2021 ||| review-aware neural recommendation with cross-modality mutual attention. ||| songyin luo ||| xiangkui lu ||| jun wu ||| jianbo yuan ||| 
2017 ||| movie fill in the blank with adaptive temporal attention and description update. ||| jie chen ||| jie shao ||| fumin shen ||| chengkun he ||| lianli gao ||| heng tao shen ||| 
2020 ||| beyond 512 tokens: siamese multi-depth transformer-based hierarchical encoder for long-form document matching. ||| liu yang ||| mingyang zhang ||| cheng li ||| michael bendersky ||| marc najork ||| 
2019 ||| a dynamic co-attention network for session-based recommendation. ||| wanyu chen ||| fei cai ||| honghui chen ||| maarten de rijke ||| 
2021 ||| litegt: efficient and lightweight graph transformers. ||| cong chen ||| chaofan tao ||| ngai wong ||| 
2021 ||| modeling heterogeneous graph network on fraud detection: a community-based framework with attention mechanism. ||| li wang ||| peipei li ||| kai xiong ||| jiashu zhao ||| rui lin ||| 
2021 ||| disenkgat: knowledge graph embedding with disentangled graph attention network. ||| junkang wu ||| wentao shi ||| xuezhi cao ||| jiawei chen ||| wenqiang lei ||| fuzheng zhang ||| wei wu ||| xiangnan he ||| 
2019 ||| attention-residual network with cnn for rumor detection. ||| yixuan chen ||| jie sui ||| liang hu ||| wei gong ||| 
2020 ||| lsan: modeling long-term dependencies and short-term correlations with hierarchical attention for risk prediction. ||| muchao ye ||| junyu luo ||| cao xiao ||| fenglong ma ||| 
2019 |||  attention-based feature extractor. ||| jingyi wang ||| qiang liu ||| zhaocheng liu ||| shu wu ||| 
2021 ||| attention based dynamic graph learning framework for asset pricing. ||| ajim uddin ||| xinyuan tao ||| dantong yu ||| 
2019 ||| knowledge-aware textual entailment with graph attention network. ||| daoyuan chen ||| yaliang li ||| min yang ||| hai-tao zheng ||| ying shen ||| 
2021 ||| conditional graph attention networks for distilling and refining knowledge graphs in recommendation. ||| ke tu ||| peng cui ||| daixin wang ||| zhiqiang zhang ||| jun zhou ||| yuan qi ||| wenwu zhu ||| 
2021 ||| improving chinese character representation with formation graph attention network. ||| xiaosu wang ||| yun xiong ||| hao niu ||| jingwen yue ||| yangyong zhu ||| philip s. yu ||| 
2020 ||| rkt: relation-aware self-attention for knowledge tracing. ||| shalini pandey ||| jaideep srivastava ||| 
2021 ||| trilateral spatiotemporal attention network for user behavior modeling in location-based search. ||| yi qi ||| ke hu ||| bo zhang ||| jia cheng ||| jun lei ||| 
2021 ||| determining subjective bias in text through linguistically informed transformer based multi-task network. ||| manjira sinha ||| tirthankar dasgupta ||| 
2021 ||| bert-qpp: contextualized pre-trained transformers for query performance prediction. ||| negar arabzadeh ||| maryam khodabakhsh ||| ebrahim bagheri ||| 
2019 ||| query-based interactive recommendation by meta-path and adapted attention-gru. ||| yu zhu ||| yu gong ||| qingwen liu ||| yingcai ma ||| wenwu ou ||| junxiong zhu ||| beidou wang ||| ziyu guan ||| deng cai ||| 
2021 ||| agcnt: adaptive graph convolutional network for transformer-based long sequence time-series forecasting. ||| hongyang su ||| xiaolong wang ||| yang qin ||| 
2018 ||| aqupr: attention based query passage retrieval. ||| parth pathak ||| mithun das gupta ||| niranjan nayak ||| harsh kohli ||| 
2020 ||| spatial-temporal convolutional graph attention networks for citywide traffic flow forecasting. ||| xiyue zhang ||| chao huang ||| yong xu ||| lianghao xia ||| 
2020 ||| time-aware graph relational attention network for stock recommendation. ||| xiaoting ying ||| cong xu ||| jianliang gao ||| jianxin wang ||| zhao li ||| 
2017 ||| interacting attention-gated recurrent networks for recommendation. ||| wenjie pei ||| jie yang ||| zhu sun ||| jie zhang ||| alessandro bozzon ||| david m. j. tax ||| 
2020 ||| do people and neural nets pay attention to the same words: studying eye-tracking data for non-factoid qa evaluation. ||| valeria bolotova ||| vladislav blinov ||| yukun zheng ||| w. bruce croft ||| falk scholer ||| mark sanderson ||| 
2020 ||| deep multifaceted transformers for multi-objective ranking in large-scale e-commerce recommender systems. ||| yulong gu ||| zhuoye ding ||| shuaiqiang wang ||| lixin zou ||| yiding liu ||| dawei yin ||| 
2021 ||| multi-factors aware dual-attentional knowledge tracing. ||| moyu zhang ||| xinning zhu ||| chunhong zhang ||| yang ji ||| feng pan ||| changchuan yin ||| 
2021 ||| you are what and where you are: graph enhanced attention network for explainable poi recommendation. ||| zeyu li ||| wei cheng ||| haiqi xiao ||| wenchao yu ||| haifeng chen ||| wei wang ||| 
2019 ||| dsanet: dual self-attention network for multivariate time series forecasting. ||| siteng huang ||| donglin wang ||| xuehan wu ||| ao tang ||| 
2019 ||| regularized adversarial sampling and deep time-aware attention for click-through rate prediction. ||| yikai wang ||| liang zhang ||| quanyu dai ||| fuchun sun ||| bo zhang ||| yang he ||| weipeng yan ||| yongjun bao ||| 
2020 ||| gaeat: graph auto-encoder attention networks for knowledge graph completion. ||| yanfei han ||| quan fang ||| jun hu ||| shengsheng qian ||| changsheng xu ||| 
2018 ||| a sequential neural information diffusion model with structure attention. ||| zhitao wang ||| chengyao chen ||| wenjie li ||| 
2019 ||| recommender system using sequential and global preference via attention mechanism and topic modeling. ||| kyeongpil kang ||| junwoo park ||| wooyoung kim ||| hojung choe ||| jaegul choo ||| 
2018 ||| hram: a hybrid recurrent attention machine for news recommendation. ||| dhruv khattar ||| vaibhav kumar ||| vasudeva varma ||| manish gupta ||| 
2021 ||| dcap: deep cross attentional product network for user response prediction. ||| zekai chen ||| fangtian zhong ||| zhumin chen ||| xiao zhang ||| robert pless ||| xiuzhen cheng ||| 
2019 ||| graph convolutional networks with motif-based attention. ||| john boaz lee ||| ryan a. rossi ||| xiangnan kong ||| sungchul kim ||| eunyee koh ||| anup rao ||| 
2021 ||| a knowledge-aware recommender with attention-enhanced dynamic convolutional network. ||| yi liu ||| bohan li ||| yalei zang ||| aoran li ||| hongzhi yin ||| 
2021 ||| block access pattern discovery via compressed full tensor transformer. ||| xing li ||| qiquan shi ||| gang hu ||| lei chen ||| hui mao ||| yiyuan yang ||| mingxuan yuan ||| jia zeng ||| zhuo cheng ||| 
2021 ||| understanding the property of long term memory for the lstm with attention mechanism. ||| wendong zheng ||| putian zhao ||| kai huang ||| gang chen ||| 
2018 ||| learning multi-touch conversion attribution with dual-attention mechanisms for online advertising. ||| kan ren ||| yuchen fang ||| weinan zhang ||| shuhao liu ||| jiajun li ||| ya zhang ||| yong yu ||| jun wang ||| 
2021 ||| how to leverage a multi-layered transformer language model for text clustering: an ensemble approach. ||| mira ait saada ||| fran ||| ois role ||| mohamed nadif ||| 
2021 ||| mixed attention transformer for leveraging word-level knowledge to neural cross-lingual information retrieval. ||| zhiqi huang ||| hamed r. bonab ||| sheikh muhammad sarwar ||| razieh rahimi ||| james allan ||| 
2019 ||| bert4rec: sequential recommendation with bidirectional encoder representations from transformer. ||| fei sun ||| jun liu ||| jian wu ||| changhua pei ||| xiao lin ||| wenwu ou ||| peng jiang ||| 
2019 ||| cross-domain attention network with wasserstein regularizers for e-commerce search. ||| minghui qiu ||| bo wang ||| cen chen ||| xiaoyi zeng ||| jun huang ||| deng cai ||| jingren zhou ||| forrest sheng bao ||| 
2017 ||| an attention-based collaboration framework for multi-view network representation learning. ||| meng qu ||| jian tang ||| jingbo shang ||| xiang ren ||| ming zhang ||| jiawei han ||| 
2018 ||| attention-based adaptive model to unify warm and cold starts recommendation. ||| shaoyun shi ||| min zhang ||| yiqun liu ||| shaoping ma ||| 
2020 ||| cola-gnn: cross-location attention based graph neural networks for long-term ili prediction. ||| songgaojun deng ||| shusen wang ||| huzefa rangwala ||| lijing wang ||| yue ning ||| 
2021 ||| spectral graph attention network with fast eigen-approximation. ||| heng chang ||| yu rong ||| tingyang xu ||| wenbing huang ||| somayeh sojoudi ||| junzhou huang ||| wenwu zhu ||| 
2020 ||| disenhan: disentangled heterogeneous graph attention network for recommendation. ||| yifan wang ||| suyao tang ||| yuntong lei ||| weiping song ||| sheng wang ||| ming zhang ||| 
2019 ||| ehr coding with multi-scale feature attention and structured knowledge graph propagation. ||| xiancheng xie ||| yun xiong ||| philip s. yu ||| yangyong zhu ||| 
2019 ||| hican: hierarchical convolutional attention network for sequence modeling. ||| yi cao ||| weifeng zhang ||| bo song ||| congfu xu ||| 
2019 ||| a zero attention model for personalized product search. ||| qingyao ai ||| daniel n. hill ||| s. v. n. vishwanathan ||| w. bruce croft ||| 
2020 ||| logic enhanced commonsense inference with chain transformer. ||| chenxi yuan ||| chun yuan ||| yang bai ||| ziran li ||| 
2019 ||| attributed multi-relational attention network for fact-checking url recommendation. ||| di you ||| nguyen vo ||| kyumin lee ||| qiang liu ||| 
2018 ||| hierarchical complementary attention network for predicting stock price movements with news. ||| qikai liu ||| xiang cheng ||| sen su ||| shuguang zhu ||| 
2021 ||| tabular data concept type detection using star-transformers. ||| yiwei zhou ||| siffi singh ||| christos christodoulopoulos ||| 
2021 ||| dynstgat: dynamic spatial-temporal graph attention network for traffic signal control. ||| libing wu ||| min wang ||| dan wu ||| jia wu ||| 
2019 ||| towards explainable representation of time-evolving graphs via spatial-temporal graph attention networks. ||| zhining liu ||| dawei zhou ||| jingrui he ||| 
2018 ||| kame: knowledge-based attention model for diagnosis prediction in healthcare. ||| fenglong ma ||| quanzeng you ||| houping xiao ||| radha chitta ||| jing zhou ||| jing gao ||| 
2018 ||| ready for use: subject-independent movement intention recognition via a convolutional attention model. ||| dalin zhang ||| lina yao ||| kaixuan chen ||| sen wang ||| 
2019 ||| hierarchical multi-label text classification: an attention-based recurrent network approach. ||| wei huang ||| enhong chen ||| qi liu ||| yuying chen ||| zai huang ||| yang liu ||| zhou zhao ||| dan zhang ||| shijin wang ||| 
2021 ||| span-level emotion cause analysis by bert-based graph attention network. ||| xiangju li ||| wei gao ||| shi feng ||| daling wang ||| shafiq r. joty ||| 
2021 ||| poshan: cardinal pos pattern guided attention for news headline incongruence. ||| rahul mishra ||| shuo zhang ||| 
2018 ||| rumor detection with hierarchical social attention network. ||| han guo ||| juan cao ||| yazi zhang ||| junbo guo ||| jintao li ||| 
2020 ||| qsan: a quantum-probability based signed attention network for explainable false information detection. ||| tian tian ||| yudong liu ||| xiaoyu yang ||| yuefei lyu ||| xi zhang ||| binxing fang ||| 
2021 ||| assorted attention network for cross-lingual language-to-vision retrieval. ||| tan yu ||| yi yang ||| hongliang fei ||| yi li ||| xiaodong chen ||| ping li ||| 
2019 ||| how does bert answer questions?: a layer-wise analysis of transformer representations. ||| betty van aken ||| benjamin winter ||| alexander l ||| ser ||| felix a. gers ||| 
2019 ||| collective link prediction oriented network embedding with hierarchical graph attention. ||| yizhu jiao ||| yun xiong ||| jiawei zhang ||| yangyong zhu ||| 
2017 ||| aspect-level sentiment classification with heat (hierarchical attention) network. ||| jiajun cheng ||| shenglin zhao ||| jiani zhang ||| irwin king ||| xin zhang ||| hui wang ||| 
2020 ||| stp-udgat: spatial-temporal-preference user dimensional graph attention network for next poi recommendation. ||| nicholas lim ||| bryan hooi ||| see-kiong ng ||| xueou wang ||| yong liang goh ||| renrong weng ||| jagannadan varadarajan ||| 
2018 ||| multiresolution graph attention networks for relevance matching. ||| ting zhang ||| bang liu ||| di niu ||| kunfeng lai ||| yu xu ||| 
2021 ||| attention based subgraph classification for link prediction by network re-weighting. ||| darong lai ||| zheyi liu ||| junyao huang ||| zhihong chong ||| weiwei wu ||| christine nardini ||| 
2020 ||| agatha: automatic graph mining and transformer based hypothesis generation approach. ||| justin sybrandt ||| ilya tyagin ||| michael shtutman ||| ilya safro ||| 
2021 ||| grad-sam: explaining transformers via gradient self-attention maps. ||| oren barkan ||| edan hauon ||| avi caciularu ||| ori katz ||| itzik malkiel ||| omri armstrong ||| noam koenigstein ||| 
2021 ||| neural information diffusion prediction with topic-aware attention network. ||| hao wang ||| cheng yang ||| chuan shi ||| 
2018 ||| personalizing search results using hierarchical rnn with query-aware attention. ||| songwei ge ||| zhicheng dou ||| zhengbao jiang ||| jian-yun nie ||| ji-rong wen ||| 
2019 ||| neighborhood interaction attention network for link prediction. ||| zhitao wang ||| yu lei ||| wenjie li ||| 
2017 ||| a compare-aggregate model with dynamic-clip attention for answer selection. ||| weijie bian ||| si li ||| zhao yang ||| guang chen ||| zhiqing lin ||| 
2021 ||| multivariate and propagation graph attention network for spatial-temporal prediction with outdoor cellular traffic. ||| chung-yi lin ||| hung-ting su ||| shen-lung tung ||| winston h. hsu ||| 
2017 ||| hierarchical rnn with static sentence-level attention for text-based speaker change detection. ||| zhao meng ||| lili mou ||| zhi jin ||| 
2021 ||| gdfm: gene vectors embodied deep attentional factorization machines for interaction prediction. ||| sameen mansha ||| tayyab khalid ||| faisal kamiran ||| masroor hussain ||| syed fawad hussain ||| hongzhi yin ||| 
2017 ||| neupl: attention-based semantic matching and pair-linking for entity disambiguation. ||| minh c. phan ||| aixin sun ||| yi tay ||| jialong han ||| chenliang li ||| 
2021 ||| spatiotemporal swin-transformer network for short time weather forecasting. ||| alabi bojesomo ||| hasan al-marzouqi ||| panos liatsis ||| 
2017 ||| a temporal attentional model for rumor stance classification. ||| amir pouran ben veyseh ||| javid ebrahimi ||| dejing dou ||| daniel lowd ||| 
2020 ||| st-grat: a novel spatio-temporal graph attention networks for accurately forecasting dynamically changing road speed. ||| cheonbok park ||| chunggi lee ||| hyojin bahng ||| yunwon tae ||| seungmin jin ||| kihwan kim ||| sungahn ko ||| jaegul choo ||| 
2020 ||| dual head-wise coattention network for machine comprehension with multiple-choice questions. ||| zhuang liu ||| kaiyu huang ||| degen huang ||| zhuang liu ||| jun zhao ||| 
2021 ||| improving irregularly sampled time series learning with time-aware dual-attention memory-augmented networks. ||| zhen wang ||| yang zhang ||| ai jiang ||| ji zhang ||| zhao li ||| jun gao ||| ke li ||| chenhao lu ||| zujie ren ||| 
2021 ||| continuous-time sequential recommendation with temporal graph collaborative transformer. ||| ziwei fan ||| zhiwei liu ||| jiawei zhang ||| yun xiong ||| lei zheng ||| philip s. yu ||| 
2018 ||| understanding reading attention distribution during relevance judgement. ||| xiangsheng li ||| yiqun liu ||| jiaxin mao ||| zexue he ||| min zhang ||| shaoping ma ||| 
2020 ||| transformer models for recommending related questions in web search. ||| rajarshee mitra ||| manish gupta ||| sandipan dandapat ||| 
2021 ||| age inference using a hierarchical attention neural network. ||| yaguang liu ||| lisa singh ||| 
2019 ||| interactive variance attention based online spoiler detection for time-sync comments. ||| wenmian yang ||| weijia jia ||| wenyuan gao ||| xiaojie zhou ||| yutao luo ||| 
2020 ||| diversifying search results using self-attention network. ||| xubo qin ||| zhicheng dou ||| ji-rong wen ||| 
2021 ||| match-ignition: plugging pagerank into transformer for long-form text matching. ||| liang pang ||| yanyan lan ||| xueqi cheng ||| 
2021 ||| extractive-abstractive summarization of judgment documents using multiple attention networks. ||| yan gao ||| zhengtao liu ||| juan li ||| fan guo ||| fei xiao ||| 
2017 ||| looking away and catching up: dealing with brief attentional disconnection in synchronous groupware. ||| carl gutwin ||| scott bateman ||| gaurav arora ||| ashley coveney ||| 
2017 ||| coattention based bilstm for answer selection. ||| lei zhang ||| longxuan ma ||| 
2021 ||| entity relation extraction based on multi-attention mechanism and bigru network. ||| lingyun wang ||| caiquan xiong ||| wenxiang xu ||| song lin ||| 
2021 ||| attention - based non-profiled side-channel attack. ||| xiangjun lu ||| chi zhang ||| dawu gu ||| 
2020 ||| malware classification through attention residual network based visualization. ||| diangarti bhalang tariang ||| sri charan birudaraju ||| ruchira naskar ||| vijeta khare ||| rajat subhra chakraborty ||| 
2019 ||| dual band quadrature vco using switched-transformer coupling for wireless robot applications. ||| wen-cheng lai ||| sheng-lyang jang ||| jyun-jhih wang ||| 
2020 ||| instrument recognition in transformer substation base on image recognition algorithm. ||| yunhai song ||| zhenzhen zhou ||| pengfei xiang ||| su fang ||| 
2017 ||| improving rnn with attention and embedding for adverse drug reactions. ||| chandra pandey ||| zina m. ibrahim ||| honghan wu ||| ehtesham iqbal ||| richard j. b. dobson ||| 
2021 ||| self-attention for audio super-resolution. ||| nathana ||| l carraz rakotonirina ||| 
2021 ||| grad-cam guided channel-spatial attention module for fine-grained visual classification. ||| shuai xu ||| dongliang chang ||| jiyang xie ||| zhanyu ma ||| 
2019 ||| interpretable online banking fraud detection based on hierarchical attention mechanism. ||| idan achituve ||| sarit kraus ||| jacob goldberger ||| 
2018 ||| supportive attention in end-to-end memory networks. ||| jen-tzung chien ||| ting-an lin ||| 
2020 ||| cognitive-driven convolutional beamforming using eeg-based auditory attention decoding. ||| ali aroudi ||| marc delcroix ||| tomohiro nakatani ||| keisuke kinoshita ||| shoko araki ||| simon doclo ||| 
2021 ||| small moving target mot tracking with gm-phd filter and attention-based cnn. ||| camilo aguilar ||| mathias ortner ||| josiane zerubia ||| 
2020 ||| deep spatio-temporal attention model for grain storage temperature forecasting. ||| shanshan duan ||| weidong yang ||| xuyu wang ||| shiwen mao ||| yuan zhang ||| 
2020 ||| s-gat: accelerating graph attention networks inference on fpga platform with shift operation. ||| weian yan ||| weiqin tong ||| xiaoli zhi ||| 
2022 ||| system of predicting dementia using transformer based ensemble learning. ||| kazu nishikawa ||| rin hirakawa ||| hideaki kawano ||| yoshihisa nakatoh ||| 
2020 ||| study on mistype correction support using attention in japanese input. ||| ryuki komatsu ||| rin hirakawa ||| hideaki kawano ||| kenichi nakashi ||| yoshihisa nakatoh ||| 
2022 ||| multi-exposure image fusion using cross-attention mechanism. ||| byungnam kim ||| hyungjoo jung ||| kwanghoon sohn ||| 
2022 ||| trafficformer: a transformer-based traffic predictor. ||| junseo ko ||| jeewoo yoon ||| daejin choi ||| eunil park ||| sangheon pack ||| jinyoung han ||| 
2021 ||| logonet: layer-aggregated attention centernet for logo detection. ||| rahul kumar jain ||| taro watasue ||| tomohiro nakagawa ||| takahiro sato ||| yutaro iwamoto ||| xiang ruan ||| yen-wei chen ||| 
2021 ||| differences in visual attention for hdr and sdr content. ||| jie xiang ||| mahsa t. pourazad ||| panos nasiopoulos ||| stelios e. ploumis ||| 
2021 ||| robust malware detection using residual attention network. ||| shamika ganesan ||| vinayakumar ravi ||| moez krichen ||| v. sowmya ||| roobaea alroobaea ||| soman k. p. ||| 
2022 ||| highly reliable vehicle detection through cnn with attention mechanism. ||| li-wen wang ||| wan-chi siu ||| xue-fei yang ||| zhi-song liu ||| daniel pak-kong lun ||| 
2018 ||| transformer-darwin: a miniature humanoid for potential robot companion. ||| jean chagas vaz ||| paul y. oh ||| 
2022 ||| frame attention recurrent back-projection network for accurate video super-resolution. ||| so sasatani ||| yutaro iwamoto ||| yen-wei chen ||| 
2022 ||| fault detection of electric motor coil by yolov3 with spatial attention. ||| mizuki kato ||| yutaro iwamoto ||| yen-wei chen ||| toru aiba ||| toshitaka sugimoto ||| 
2021 ||| automatic detection and segmentation of liver tumors in multi- phase ct images by phase attention mask r-cnn. ||| ryo hasegawa ||| yutaro iwamoto ||| xianhua han ||| lanfen lin ||| hongjie hu ||| xiujun cai ||| yen-wei chen ||| 
2020 ||| study on automatic defect report classification system with self attention visualization. ||| rin hirakawa ||| keitaro tominaga ||| yoshihisa nakatoh ||| 
2019 ||| multi-headed self-attention-based hierarchical model for extractive summarization. ||| rayees ahmad dar ||| aroor dinesh dileep ||| 
2020 ||| a co-attention model with sequential behaviors and side information for session- based recommendation. ||| lin li ||| yuliang shi ||| kun zhang ||| yongjian ren ||| 
2021 ||| sieve: attention-based sampling of end-to-end trace data in distributed microservice systems. ||| zicheng huang ||| pengfei chen ||| guangba yu ||| hongyang chen ||| zibin zheng ||| 
2019 ||| outcome-oriented predictive process monitoring with attention-based bidirectional lstm neural networks. ||| jiaojiao wang ||| dongjin yu ||| chengfei liu ||| xiaoxiao sun ||| 
2020 ||| nafm: neural and attentional factorization machine for web api recommendation. ||| guosheng kang ||| jianxun liu ||| buqing cao ||| manliang cao ||| 
2021 ||| combining label-wise attention and adversarial training for tag prediction of web services. ||| qunbo wang ||| wenjun wu ||| yongchi zhao ||| yuzhang zhuang ||| yanni wang ||| 
2021 ||| mattrip: multi-functional attention-based neural network for semantic travel route recommendation. ||| chenxiao yang ||| jiale zhang ||| xiaofeng gao ||| guihai chen ||| 
2021 ||| service recommendation for composition creation based on collaborative attention convolutional network. ||| ruyu yan ||| yushun fan ||| jia zhang ||| junqi zhang ||| haozhe lin ||| 
2021 ||| sequence and distance aware transformer for recommendation systems. ||| runqiang zang ||| meiyun zuo ||| jilei zhou ||| yining xue ||| keman huang ||| 
2020 ||| an attention-based neural model for popularity prediction in social service. ||| chao wang ||| weizhi gong ||| xiaofeng gao ||| guihai chen ||| 
2021 ||| heterogeneous graph attention network-enhanced web service classification. ||| mi peng ||| buqing cao ||| junjie chen ||| guosheng kang ||| jianxun liu ||| yiping wen ||| 
2020 ||| adaptation of multilingual transformer encoder for robust enhanced universal dependency parsing. ||| han he ||| jinho d. choi ||| 
2018 ||| director's cut: a combined dataset for visual attention analysis in cinematic vr content. ||| sebastian knorr ||| cagri ozcinar ||| colm o. fearghail ||| aljosa smolic ||| 
2017 ||| work-in-progress: analysis of meditation and attention level of human brain. ||| prajna p. nanda ||| ashima rout ||| ramesh kumar sahoo ||| srinivas sethi ||| 
2020 ||| instant and accurate instance segmentation equipped with path aggregation and attention gate. ||| seung il lee ||| hyun kim ||| 
2021 ||| a 2-ghz reconfigurable transmitter using a class-d pa and a multi-tapped transformer. ||| reza e. rad ||| soon ho choi ||| sungjin kim ||| behnam samadpoor rikan ||| kang-yoon lee ||| 
2018 ||| human visual attention analysis-based image segmentation using color histogram. ||| ho sub lee ||| young hwan kim ||| 
2021 ||| layer-wise pruning of transformer attention heads for efficient language modeling. ||| kyuhong shim ||| iksoo choi ||| wonyong sung ||| jungwook choi ||| 
2021 ||| graph attention network with dependency parsing for aspect-level sentiment classification. ||| ling gan ||| qiao tang ||| 
2021 ||| chinese text similarity calculation model based on multi-attention siamese bi-lstm. ||| zhongguo wang ||| bao zhang ||| 
2018 ||| digitalization of regulation unit selection process in power transformer design. ||| mislay gazdovic ||| tatjana simovic ||| 
2020 ||| semi-analytical estimation of on-chip intertwined rectangular transformer parameters in 180 nm cmos technology. ||| ivan brezovec ||| josip mikulic ||| gregor schatzberger ||| adrijan baric ||| 
2017 ||| fem analysis and design of a voltage instrument transformer for digital sampling wattmeter. ||| martin dadic ||| karlo petrovic ||| roman malaric ||| 
2017 ||| a multi-task framework for monitoring health conditions via attention-based recurrent neural networks. ||| qiuling suo ||| fenglong ma ||| giovanni canino ||| jing gao ||| aidong zhang ||| pierangelo veltri ||| agostino gnasso ||| 
2018 ||| hierarchical attention-based prediction model for discovering the persistence of chronic opioid therapy from a large clinical dataset. ||| ram ||| n maldonado ||| mark sullivan ||| meliha yetisgen ||| sanda m. harabagiu ||| 
2020 ||| patient cohort retrieval using transformer language models. ||| sarvesh soni ||| kirk roberts ||| 
2020 ||| interpretable automatic adverse event detection with hierarchical attention networks. ||| zilong zhang ||| cathy eastwood ||| natalie wiebe ||| adam g. d'souza ||| yuan xu ||| hude quan ||| quan long ||| 
2020 ||| learning hierarchical transformer-based representations of clinical notes. ||| xin su ||| timothy a. miller ||| majid afshar ||| dmitriy dligach ||| 
2020 ||| extracting angina symptoms from clinical notes using pre-trained transformer architectures. ||| aaron s. eisman ||| nishant r. shah ||| carsten eickhoff ||| george zerveas ||| elizabeth s. chen ||| wen-chih wu ||| indra neil sarkar ||| 
2020 ||| automated diagnosis coding from clinical notes using attention-augmented recurrent convolutional neural networks. ||| edidiong okon ||| lingyun shi ||| rich tsui ||| 
2020 ||| face completion with pyramid semantic attention and latent codes. ||| shilei cao ||| kouichi sakurai ||| 
2021 ||| anticipative video transformer. ||| rohit girdhar ||| kristen grauman ||| 
2021 ||| generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. ||| hila chefer ||| shir gur ||| lior wolf ||| 
2021 ||| a latent transformer for disentangled face editing in images and videos. ||| xu yao ||| alasdair newson ||| yann gousseau ||| pierre hellier ||| 
2021 |||  image visual attention modelling with unsupervised learning. ||| yasser abdelaziz dahou djilali ||| tarun krishna ||| kevin mcguinness ||| noel e. o'connor ||| 
2019 ||| segeqa: video segmentation based visual attention for embodied question answering. ||| haonan luo ||| guosheng lin ||| zichuan liu ||| fayao liu ||| zhenmin tang ||| yazhou yao ||| 
2021 ||| voxel transformer for 3d object detection. ||| jiageng mao ||| yujing xue ||| minzhe niu ||| haoyue bai ||| jiashi feng ||| xiaodan liang ||| hang xu ||| chunjing xu ||| 
2021 ||| high-resolution optical flow from 1d attention and correlation. ||| haofei xu ||| jiaolong yang ||| jianfei cai ||| juyong zhang ||| xin tong ||| 
2021 ||| stvgbert: a visual-linguistic transformer based framework for spatio-temporal video grounding. ||| rui su ||| qian yu ||| dong xu ||| 
2021 ||| learning spatio-temporal transformer for visual tracking. ||| bin yan ||| houwen peng ||| jianlong fu ||| dong wang ||| huchuan lu ||| 
2021 ||| transreid: transformer-based object re-identification. ||| shuting he ||| hao luo ||| pichao wang ||| fan wang ||| hao li ||| wei jiang ||| 
2019 ||| second-order non-local attention networks for person re-identification. ||| bryan bryan ||| yuan gong ||| yizhe zhang ||| christian poellabauer ||| 
2021 ||| localtrans: a multiscale local transformer network for cross-resolution homography estimation. ||| ruizhi shao ||| gaochang wu ||| yuemei zhou ||| ying fu ||| lu fang ||| yebin liu ||| 
2019 ||| group-wise deep object co-segmentation with co-attention recurrent neural network. ||| bo li ||| zhengxing sun ||| qian li ||| yunjie wu ||| anqi hu ||| 
2021 ||| tokens-to-token vit: training vision transformers from scratch on imagenet. ||| li yuan ||| yunpeng chen ||| tao wang ||| weihao yu ||| yujun shi ||| zihang jiang ||| francis e. h. tay ||| jiashi feng ||| shuicheng yan ||| 
2021 ||| dynamic high-pass filtering and multi-spectral attention for image super-resolution. ||| salma abdel magid ||| yulun zhang ||| donglai wei ||| won-dong jang ||| zudi lin ||| yun fu ||| hanspeter pfister ||| 
2019 ||| deep floor plan recognition using a multi-task network with room-boundary-guided attention. ||| zhiliang zeng ||| xianzhi li ||| ying kin yu ||| chi-wing fu ||| 
2021 ||| an empirical study of training self-supervised vision transformers. ||| xinlei chen ||| saining xie ||| kaiming he ||| 
2019 ||| sharpen focus: learning with attention separability and consistency. ||| lezi wang ||| ziyan wu ||| srikrishna karanam ||| kuan-chuan peng ||| rajat vikram singh ||| bo liu ||| dimitris n. metaxas ||| 
2017 ||| video fill in the blank using lr/rl lstms with spatial-temporal attentions. ||| amir mazaheri ||| dong zhang ||| mubarak shah ||| 
2017 ||| multi-modal factorized bilinear pooling with co-attention learning for visual question answering. ||| zhou yu ||| jun yu ||| jianping fan ||| dacheng tao ||| 
2021 ||| paint transformer: feed forward neural painting with stroke prediction. ||| songhua liu ||| tianwei lin ||| dongliang he ||| fu li ||| ruifeng deng ||| xin li ||| errui ding ||| hao wang ||| 
2021 ||| the benefit of distraction: denoising camera-based physiological measurements using inverse attention. ||| ewa magdalena nowara ||| daniel mcduff ||| ashok veeraraghavan ||| 
2021 ||| swin transformer: hierarchical vision transformer using shifted windows. ||| ze liu ||| yutong lin ||| yue cao ||| han hu ||| yixuan wei ||| zheng zhang ||| stephen lin ||| baining guo ||| 
2021 ||| rethinking and improving relative position encoding for vision transformer. ||| kan wu ||| houwen peng ||| minghao chen ||| jianlong fu ||| hongyang chao ||| 
2021 ||| bossnas: exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. ||| changlin li ||| tao tang ||| guangrun wang ||| jiefeng peng ||| bing wang ||| xiaodan liang ||| xiaojun chang ||| 
2019 ||| griddehazenet: attention-based multi-scale network for image dehazing. ||| xiaohong liu ||| yongrui ma ||| zhihao shi ||| jun chen ||| 
2021 ||| topic scene graph generation by attention distillation from caption. ||| wenbin wang ||| ruiping wang ||| xilin chen ||| 
2021 ||| thundr: transformer-based 3d human reconstruction with markers. ||| mihai zanfir ||| andrei zanfir ||| eduard gabriel bazavan ||| william t. freeman ||| rahul sukthankar ||| cristian sminchisescu ||| 
2021 ||| attentional pyramid pooling of salient visual residuals for place recognition. ||| guohao peng ||| jun zhang ||| heshan li ||| danwei wang ||| 
2019 ||| dynamic graph attention for referring expression comprehension. ||| sibei yang ||| guanbin li ||| yizhou yu ||| 
2021 ||| geometry-free view synthesis: transformers and no 3d priors. ||| robin rombach ||| patrick esser ||| bj ||| rn ommer ||| 
2021 ||| learning deep local features with multiple dynamic attentions for large-scale image retrieval. ||| hui wu ||| min wang ||| wengang zhou ||| houqiang li ||| 
2019 ||| contextual attention for hand detection in the wild. ||| supreeth narasimhaswamy ||| zhengwei wei ||| yang wang ||| justin zhang ||| minh hoai nguyen ||| 
2021 ||| co-scale conv-attentional image transformers. ||| weijian xu ||| yifan xu ||| tyler a. chang ||| zhuowen tu ||| 
2021 ||| fashionmirror: co-attention feature-remapping virtual try-on with sequential template poses. ||| chieh-yun chen ||| ling lo ||| pin-jui huang ||| hong-han shuai ||| wen-huang cheng ||| 
2021 ||| pyramid point cloud transformer for large-scale place recognition. ||| le hui ||| hang yang ||| mingmei cheng ||| jin xie ||| jian yang ||| 
2019 ||| pyramid graph networks with connection attentions for region-based one-shot semantic segmentation. ||| chi zhang ||| guosheng lin ||| fayao liu ||| jiushuang guo ||| qingyao wu ||| rui yao ||| 
2019 ||| attention-aware polarity sensitive embedding for affective image retrieval. ||| xingxu yao ||| dongyu she ||| sicheng zhao ||| jie liang ||| yu-kun lai ||| jufeng yang ||| 
2017 ||| structured attentions for visual question answering. ||| chen zhu ||| yanpeng zhao ||| shuaiyi huang ||| kewei tu ||| yi ma ||| 
2021 ||| fuseformer: fusing fine-grained information in transformers for video inpainting. ||| rui liu ||| hanming deng ||| yangyi huang ||| xiaoyu shi ||| lewei lu ||| wenxiu sun ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2017 ||| segmentation-aware convolutional networks using local attention masks. ||| adam w. harley ||| konstantinos g. derpanis ||| iasonas kokkinos ||| 
2017 ||| paying attention to descriptions generated by image captioning models. ||| hamed r. tavakoli ||| rakshith shetty ||| ali borji ||| jorma laaksonen ||| 
2021 ||| uncertainty-guided transformer reasoning for camouflaged object detection. ||| fan yang ||| qiang zhai ||| xin li ||| rui huang ||| ao luo ||| hong cheng ||| deng-ping fan ||| 
2021 ||| residual attention: a simple but effective method for multi-label recognition. ||| ke zhu ||| jianxin wu ||| 
2019 ||| sid4vam: a benchmark dataset with synthetic images for visual attention modeling. ||| david berga ||| xos |||  ram ||| n fern ||| ndez-vidal ||| xavier otazu ||| xos |||  m. pardo ||| 
2017 ||| learning visual attention to identify people with autism spectrum disorder. ||| ming jiang ||| qi zhao ||| 
2021 ||| scouter: slot attention-based classifier for explainable image recognition. ||| liangzhi li ||| bowen wang ||| manisha verma ||| yuta nakashima ||| ryo kawasaki ||| hajime nagahara ||| 
2021 ||| unit: multimodal multitask learning with a unified transformer. ||| ronghang hu ||| amanpreet singh ||| 
2021 ||| group-free 3d object detection via transformers. ||| ze liu ||| zheng zhang ||| yue cao ||| han hu ||| xin tong ||| 
2021 ||| vision transformers for dense prediction. ||| ren |||  ranftl ||| alexey bochkovskiy ||| vladlen koltun ||| 
2021 ||| levit: a vision transformer in convnet's clothing for faster inference. ||| benjamin graham ||| alaaeldin el-nouby ||| hugo touvron ||| pierre stock ||| armand joulin ||| herv |||  j ||| gou ||| matthijs douze ||| 
2021 ||| oadtr: online action detection with transformers. ||| xiang wang ||| shiwei zhang ||| zhiwu qing ||| yuanjie shao ||| zhengrong zuo ||| changxin gao ||| nong sang ||| 
2019 ||| integral object mining via online attention accumulation. ||| peng-tao jiang ||| qibin hou ||| yang cao ||| ming-ming cheng ||| yunchao wei ||| hongkai xiong ||| 
2021 ||| adaattn: revisit attention mechanism in arbitrary neural style transfer. ||| songhua liu ||| tianwei lin ||| dongliang he ||| fu li ||| meiling wang ||| xin li ||| zhengxing sun ||| qian li ||| errui ding ||| 
2019 ||| co-segmentation inspired attention networks for video-based person re-identification. ||| arulkumar subramaniam ||| athira m. nambiar ||| anurag mittal ||| 
2021 ||| attention-based multi-reference learning for image super-resolution. ||| marco pesavento ||| marco volino ||| adrian hilton ||| 
2017 ||| recursive spatial transformer (rest) for alignment-free face recognition. ||| wanglong wu ||| meina kan ||| xin liu ||| yi yang ||| shiguang shan ||| xilin chen ||| 
2019 ||| self-critical attention learning for person re-identification. ||| guangyi chen ||| chunze lin ||| liangliang ren ||| jiwen lu ||| jie zhou ||| 
2021 ||| pcam: product of cross-attention matrices for rigid registration of point clouds. ||| anh-quan cao ||| gilles puy ||| alexandre boulch ||| renaud marlet ||| 
2019 ||| attentional neural fields for crowd counting. ||| anran zhang ||| lei yue ||| jiayi shen ||| fan zhu ||| xiantong zhen ||| xianbin cao ||| ling shao ||| 
2021 ||| transforensics: image forgery localization with dense self-attention. ||| jing hao ||| zhixin zhang ||| shicai yang ||| di xie ||| shiliang pu ||| 
2021 ||| groupformer: group activity recognition with clustered spatial-temporal transformer. ||| shuaicheng li ||| qianggang cao ||| lingbo liu ||| kunlin yang ||| shinan liu ||| jun hou ||| shuai yi ||| 
2021 ||| layouttransformer: layout generation and completion with self-attention. ||| kamal gupta ||| justin lazarow ||| alessandro achille ||| larry davis ||| vijay mahadevan ||| abhinav shrivastava ||| 
2021 ||| cloud transformers: a universal approach to point cloud processing tasks. ||| kirill mazur ||| victor lempitsky ||| 
2021 ||| multi-scale vision longformer: a new vision transformer for high-resolution image encoding. ||| pengchuan zhang ||| xiyang dai ||| jianwei yang ||| bin xiao ||| lu yuan ||| lei zhang ||| jianfeng gao ||| 
2021 ||| dynamic detr: end-to-end object detection with dynamic attention. ||| xiyang dai ||| yinpeng chen ||| jianwei yang ||| pengchuan zhang ||| lu yuan ||| lei zhang ||| 
2021 ||| stochastic transformer networks with linear competing units: application to end-to-end sl translation. ||| andreas voskou ||| konstantinos p. panousis ||| dimitrios kosmopoulos ||| dimitris n. metaxas ||| sotirios chatzis ||| 
2021 ||| sotr: segmenting objects with transformers. ||| ruohao guo ||| dantong niu ||| liao qu ||| zhenbo li ||| 
2021 ||| handwriting transformers. ||| ankan kumar bhunia ||| salman h. khan ||| hisham cholakkal ||| rao muhammad anwer ||| fahad shahbaz khan ||| mubarak shah ||| 
2019 ||| entangled transformer for image captioning. ||| guang li ||| linchao zhu ||| ping liu ||| yi yang ||| 
2021 ||| deep symmetric network for underexposed image enhancement with recurrent attentional learning. ||| lin zhao ||| shao-ping lu ||| tao chen ||| zhenglu yang ||| ariel shamir ||| 
2021 ||| unified questioner transformer for descriptive question generation in goal-oriented visual dialogue. ||| shoya matsumori ||| kosuke shingyouchi ||| yuki abe ||| yosuke fukuchi ||| komei sugiura ||| michita imai ||| 
2021 ||| revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. ||| zhaoshuo li ||| xingtong liu ||| nathan drenkow ||| andy s. ding ||| francis x. creighton ||| russell h. taylor ||| mathias unberath ||| 
2019 ||| controllable attention for structured layered video decomposition. ||| jean-baptiste alayrac ||| jo ||| o carreira ||| relja arandjelovic ||| andrew zisserman ||| 
2021 ||| crackformer: transformer network for fine-grained crack detection. ||| huajun liu ||| xiangyu miao ||| christoph mertz ||| chengzhong xu ||| hui kong ||| 
2021 ||| instance-level image retrieval using reranking transformers. ||| fuwen tan ||| jiangbo yuan ||| vicente ordonez ||| 
2021 ||| pyramid vision transformer: a versatile backbone for dense prediction without convolutions. ||| wenhai wang ||| enze xie ||| xiang li ||| deng-ping fan ||| kaitao song ||| ding liang ||| tong lu ||| ping luo ||| ling shao ||| 
2019 ||| ranet: ranking attention network for fast video object segmentation. ||| ziqin wang ||| jun xu ||| li liu ||| fan zhu ||| ling shao ||| 
2021 ||| variational attention: propagating domain-specific knowledge for multi-domain learning in crowd counting. ||| binghui chen ||| zhaoyi yan ||| ke li ||| pengyu li ||| biao wang ||| wangmeng zuo ||| lei zhang ||| 
2021 ||| t-automl: automated machine learning for lesion segmentation using transformers in 3d medical imaging. ||| dong yang ||| andriy myronenko ||| xiaosong wang ||| ziyue xu ||| holger r. roth ||| daguang xu ||| 
2021 ||| multiscale vision transformers. ||| haoqi fan ||| bo xiong ||| karttikeya mangalam ||| yanghao li ||| zhicheng yan ||| jitendra malik ||| christoph feichtenhofer ||| 
2019 ||| 3d scene reconstruction with multi-layer depth and epipolar transformers. ||| daeyun shin ||| zhile ren ||| erik b. sudderth ||| charless c. fowlkes ||| 
2021 ||| learning motion-appearance co-attention for zero-shot video object segmentation. ||| shu yang ||| lu zhang ||| jinqing qi ||| huchuan lu ||| shuo wang ||| xiaoxing zhang ||| 
2017 ||| interpretable learning for self-driving cars by visualizing causal attention. ||| jinkyu kim ||| john f. canny ||| 
2019 ||| relation-aware graph attention network for visual question answering. ||| linjie li ||| zhe gan ||| yu cheng ||| jingjing liu ||| 
2021 ||| diagonal attention and style-based gan for content-style disentanglement in image generation and translation. ||| gihyun kwon ||| jong chul ye ||| 
2021 ||| star: a structure-aware lightweight transformer for real-time image enhancement. ||| zhaoyang zhang ||| yitong jiang ||| jun jiang ||| xiaogang wang ||| ping luo ||| jinwei gu ||| 
2021 ||| context reasoning attention network for image super-resolution. ||| yulun zhang ||| donglai wei ||| can qin ||| huan wang ||| hanspeter pfister ||| yun fu ||| 
2021 ||| an end-to-end transformer model for 3d object detection. ||| ishan misra ||| rohit girdhar ||| armand joulin ||| 
2021 ||| scalable vision transformers with hierarchical pooling. ||| zizheng pan ||| bohan zhuang ||| jing liu ||| haoyu he ||| jianfei cai ||| 
2019 ||| attpool: towards hierarchical feature representation in graph convolutional networks via attention mechanism. ||| jingjia huang ||| zhangheng li ||| nannan li ||| shan liu ||| ge li ||| 
2019 ||| depth-induced multi-scale recurrent attention network for saliency detection. ||| yongri piao ||| wei ji ||| jingjing li ||| miao zhang ||| huchuan lu ||| 
2021 ||| visual relationship detection using part-and-sum transformers with composite queries. ||| qi dong ||| zhuowen tu ||| haofu liao ||| yuting zhang ||| vijay mahadevan ||| stefano soatto ||| 
2021 ||| high-performance discriminative tracking with transformers. ||| bin yu ||| ming tang ||| linyu zheng ||| guibo zhu ||| jinqiao wang ||| hao feng ||| xuetao feng ||| hanqing lu ||| 
2021 ||| planetr: structure-guided transformers for 3d plane recovery. ||| bin tan ||| nan xue ||| song bai ||| tianfu wu ||| gui-song xia ||| 
2021 ||| rethinking spatial dimensions of vision transformers. ||| byeongho heo ||| sangdoo yun ||| dongyoon han ||| sanghyuk chun ||| junsuk choe ||| seong joon oh ||| 
2021 ||| episodic transformer for vision-and-language navigation. ||| alexander pashevich ||| cordelia schmid ||| chen sun ||| 
2021 ||| transpose: keypoint localization via transformer. ||| sen yang ||| zhibin quan ||| mu nie ||| wankou yang ||| 
2021 ||| action-conditioned 3d human motion synthesis with transformer vae. ||| mathis petrovich ||| michael j. black ||| g ||| l varol ||| 
2021 ||| parts: unsupervised segmentation with slots, attention and independence maximization. ||| daniel zoran ||| rishabh kabra ||| alexander lerchner ||| danilo j. rezende ||| 
2021 ||| medirl: predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning. ||| sonia baee ||| erfan pakdamanian ||| inki kim ||| lu feng ||| vicente ordonez ||| laura e. barnes ||| 
2019 ||| motion guided attention for video salient object detection. ||| haofeng li ||| guanqi chen ||| guanbin li ||| yizhou yu ||| 
2021 ||| pointr: diverse point cloud completion with geometry-aware transformers. ||| xumin yu ||| yongming rao ||| ziyi wang ||| zuyan liu ||| jiwen lu ||| jie zhou ||| 
2021 ||| going deeper with image transformers. ||| hugo touvron ||| matthieu cord ||| alexandre sablayrolles ||| gabriel synnaeve ||| herv |||  j ||| gou ||| 
2021 ||| pare: part attention regressor for 3d human body estimation. ||| muhammed kocabas ||| chun-hao p. huang ||| otmar hilliges ||| michael j. black ||| 
2021 ||| neat: neural attention fields for end-to-end autonomous driving. ||| kashyap chitta ||| aditya prakash ||| andreas geiger ||| 
2019 ||| bilinear attention networks for person retrieval. ||| pengfei fang ||| jieming zhou ||| soumava kumar roy ||| lars petersson ||| mehrtash harandi ||| 
2021 ||| the right to talk: an audio-visual transformer approach. ||| thanh-dat truong ||| chi nhan duong ||| the de vu ||| hoang anh pham ||| bhiksha raj ||| ngan le ||| khoa luu ||| 
2019 ||| align, attend and locate: chest x-ray diagnosis via contrast induced attention network with limited supervision. ||| jingyu liu ||| gangming zhao ||| yu fei ||| ming zhang ||| yizhou wang ||| yizhou yu ||| 
2021 ||| hit: hierarchical transformer with momentum contrast for video-text retrieval. ||| song liu ||| haoqi fan ||| shengsheng qian ||| yiru chen ||| wenkui ding ||| zhongyuan wang ||| 
2017 ||| rpan: an end-to-end recurrent pose-attention network for action recognition in videos. ||| wenbin du ||| yali wang ||| yu qiao ||| 
2019 ||| fingerspelling recognition in the wild with iterative visual attention. ||| bowen shi ||| aurora martinez del rio ||| jonathan keane ||| diane brentari ||| greg shakhnarovich ||| karen livescu ||| 
2021 ||| vision transformer with progressive sampling. ||| xiaoyu yue ||| shuyang sun ||| zhanghui kuang ||| meng wei ||| philip h. s. torr ||| wayne zhang ||| dahua lin ||| 
2021 ||| autoformer: searching transformers for visual recognition. ||| minghao chen ||| houwen peng ||| jianlong fu ||| haibin ling ||| 
2017 ||| learning multi-attention convolutional neural network for fine-grained image recognition. ||| heliang zheng ||| jianlong fu ||| tao mei ||| jiebo luo ||| 
2021 ||| on the robustness of vision transformers to adversarial examples. ||| kaleel mahmood ||| rigel mahmood ||| marten van dijk ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| peng gao ||| minghang zheng ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2021 ||| segmenter: transformer for semantic segmentation. ||| robin strudel ||| ricardo garcia pinel ||| ivan laptev ||| cordelia schmid ||| 
2019 ||| attentional feature-pair relation networks for accurate face recognition. ||| bong-nam kang ||| yonghyun kim ||| bongjin jun ||| daijin kim ||| 
2021 ||| glimpse-attend-and-explore: self-attention for active visual exploration. ||| soroush seifi ||| abhishek jha ||| tinne tuytelaars ||| 
2021 ||| counterfactual attention learning for fine-grained visual categorization and re-identification. ||| yongming rao ||| guangyi chen ||| jiwen lu ||| jie zhou ||| 
2021 ||| multi-view 3d reconstruction with transformers. ||| dan wang ||| xinrui cui ||| xun chen ||| zhengxia zou ||| tianyang shi ||| septimiu salcudean ||| z. jane wang ||| rabab ward ||| 
2017 ||| vqs: linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. ||| chuang gan ||| yandong li ||| haoxiang li ||| chen sun ||| boqing gong ||| 
2021 ||| simpler is better: few-shot semantic segmentation with classifier weight transformer. ||| zhihe lu ||| sen he ||| xiatian zhu ||| li zhang ||| yi-zhe song ||| tao xiang ||| 
2019 ||| attention on attention for image captioning. ||| lun huang ||| wenmin wang ||| jie chen ||| xiaoyong wei ||| 
2019 ||| agss-vos: attention guided single-shot video object segmentation. ||| huaijia lin ||| xiaojuan qi ||| jiaya jia ||| 
2021 ||| event-based video reconstruction using transformer. ||| wenming weng ||| yueyi zhang ||| zhiwei xiong ||| 
2019 ||| a dual-path model with adaptive attention for vehicle re-identification. ||| pirazh khorramshahi ||| amit kumar ||| neehar peri ||| sai saketh rambhatla ||| jun-cheng chen ||| rama chellappa ||| 
2021 ||| relaxed transformer decoders for direct action proposal generation. ||| jing tan ||| jiaqi tang ||| limin wang ||| gangshan wu ||| 
2019 ||| ccnet: criss-cross attention for semantic segmentation. ||| zilong huang ||| xinggang wang ||| lichao huang ||| chang huang ||| yunchao wei ||| wenyu liu ||| 
2019 ||| what would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention. ||| antonino furnari ||| giovanni maria farinella ||| 
2021 ||| 3d human pose estimation with spatial and temporal transformers. ||| ce zheng ||| sijie zhu ||| mat ||| as mendieta ||| taojiannan yang ||| chen chen ||| zhengming ding ||| 
2019 ||| attention bridging network for knowledge transfer. ||| kunpeng li ||| yulun zhang ||| kai li ||| yuanyuan li ||| yun fu ||| 
2019 ||| attention-based autism spectrum disorder screening with privileged modality. ||| shi chen ||| qi zhao ||| 
2021 ||| neural image compression via attentional multi-scale back projection and frequency decomposition. ||| ge gao ||| pei you ||| rong pan ||| shunyuan han ||| yuanyuan zhang ||| yuchao dai ||| hojae lee ||| 
2019 ||| hierarchical self-attention network for action localization in videos. ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2019 ||| accelerate learning of deep hashing with gradient attention. ||| long-kai huang ||| jianda chen ||| sinno jialin pan ||| 
2019 ||| progressive sparse local attention for video object detection. ||| chaoxu guo ||| bin fan ||| jie gu ||| qian zhang ||| shiming xiang ||| v ||| ronique prinet ||| chunhong pan ||| 
2021 ||| fcanet: frequency channel attention networks. ||| zequn qin ||| pengyi zhang ||| fei wu ||| xi li ||| 
2021 ||| learning multi-scene absolute pose regression with transformers. ||| yoli shavit ||| ron ferens ||| yosi keller ||| 
2021 ||| emerging properties in self-supervised vision transformers. ||| mathilde caron ||| hugo touvron ||| ishan misra ||| herv |||  j ||| gou ||| julien mairal ||| piotr bojanowski ||| armand joulin ||| 
2021 ||| vision-language transformer and query generation for referring segmentation. ||| henghui ding ||| chang liu ||| suchen wang ||| xudong jiang ||| 
2021 ||| rain: reinforced hybrid attention inference network for motion forecasting. ||| jiachen li ||| fan yang ||| hengbo ma ||| srikanth malla ||| masayoshi tomizuka ||| chiho choi ||| 
2017 ||| deep spatial-semantic attention for fine-grained sketch-based image retrieval. ||| jifei song ||| qian yu ||| yi-zhe song ||| tao xiang ||| timothy m. hospedales ||| 
2019 ||| saliency-guided attention network for image-sentence matching. ||| zhong ji ||| haoran wang ||| jungong han ||| yanwei pang ||| 
2019 ||| dual attention matching for audio-visual event localization. ||| yu wu ||| linchao zhu ||| yan yan ||| yi yang ||| 
2021 ||| dual-camera super-resolution with aligned attention modules. ||| tengfei wang ||| jiaxin xie ||| wenxiu sun ||| qiong yan ||| qifeng chen ||| 
2021 ||| oscar-net: object-centric scene graph attention for image attribution. ||| eric nguyen ||| tu bui ||| viswanathan (vishy) swaminathan ||| john p. collomosse ||| 
2019 ||| learning lightweight lane detection cnns by self attention distillation. ||| yuenan hou ||| zheng ma ||| chunxiao liu ||| chen change loy ||| 
2021 ||| vivit: a video vision transformer. ||| anurag arnab ||| mostafa dehghani ||| georg heigold ||| chen sun ||| mario lucic ||| cordelia schmid ||| 
2021 ||| agentformer: agent-aware transformers for socio-temporal multi-agent forecasting. ||| ye yuan ||| xinshuo weng ||| yanglan ou ||| kris kitani ||| 
2017 ||| attention-aware deep reinforcement learning for video face recognition. ||| yongming rao ||| jiwen lu ||| jie zhou ||| 
2019 ||| mixture-kernel graph attention network for situation recognition. ||| mohammed suhail ||| leonid sigal ||| 
2017 ||| online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. ||| qi chu ||| wanli ouyang ||| hongsheng li ||| xiaogang wang ||| bin liu ||| nenghai yu ||| 
2019 ||| asymmetric cross-guided attention network for actor and action video segmentation from natural language query. ||| hao wang ||| cheng deng ||| junchi yan ||| dacheng tao ||| 
2019 ||| mask-guided attention network for occluded pedestrian detection. ||| yanwei pang ||| jin xie ||| muhammad haris khan ||| rao muhammad anwer ||| fahad shahbaz khan ||| ling shao ||| 
2021 ||| panoptic segmentation of satellite image time series with convolutional temporal attention networks. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| 
2017 ||| multi-label image recognition by recurrently discovering attentional regions. ||| zhouxia wang ||| tianshui chen ||| guanbin li ||| ruijia xu ||| liang lin ||| 
2021 ||| musiq: multi-scale image quality transformer. ||| junjie ke ||| qifei wang ||| yilin wang ||| peyman milanfar ||| feng yang ||| 
2021 ||| transfer: learning relation-aware facial expression representations with transformers. ||| fanglei xue ||| qiangchang wang ||| guodong guo ||| 
2021 ||| describing and localizing multiple changes with transformers. ||| yue qiu ||| shintaro yamamoto ||| kodai nakashima ||| ryota suzuki ||| kenji iwata ||| hirokatsu kataoka ||| yutaka satoh ||| 
2021 ||| 3d human texture estimation from a single image with transformers. ||| xiangyu xu ||| chen change loy ||| 
2017 ||| identity-aware textual-visual matching with latent co-attention. ||| shuang li ||| tong xiao ||| hongsheng li ||| wei yang ||| xiaogang wang ||| 
2021 ||| point transformer. ||| hengshuang zhao ||| li jiang ||| jiaya jia ||| philip h. s. torr ||| vladlen koltun ||| 
2021 ||| motion guided attention fusion to recognize interactions from videos. ||| tae soo kim ||| jonathan d. jones ||| gregory d. hager ||| 
2017 ||| egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules. ||| congqi cao ||| yifan zhang ||| yi wu ||| hanqing lu ||| jian cheng ||| 
2021 ||| unsupervised point cloud object co-segmentation by co-contrastive learning and mutual attention sampling. ||| cheng-kun yang ||| yung-yu chuang ||| yen-yu lin ||| 
2021 ||| attention is not enough: mitigating the distribution discrepancy in asynchronous multimodal sequence fusion. ||| tao liang ||| guosheng lin ||| lei feng ||| yan zhang ||| fengmao lv ||| 
2017 ||| areas of attention for image captioning. ||| marco pedersoli ||| thomas lucas ||| cordelia schmid ||| jakob verbeek ||| 
2019 ||| discriminative feature learning with consistent attention regularization for person re-identification. ||| sanping zhou ||| fei wang ||| zeyi huang ||| jinjun wang ||| 
2019 ||| attribute attention for semantic disambiguation in zero-shot learning. ||| yang liu ||| jishun guo ||| deng cai ||| xiaofei he ||| 
2021 ||| occlude them all: occlusion-aware attention network for occluded person re-id. ||| peixian chen ||| wenfeng liu ||| pingyang dai ||| jianzhuang liu ||| qixiang ye ||| mingliang xu ||| qi'an chen ||| rongrong ji ||| 
2019 ||| reasoning about human-object interactions through dual attention networks. ||| tete xiao ||| quanfu fan ||| danny gutfreund ||| mathew monfort ||| aude oliva ||| bolei zhou ||| 
2021 ||| saccadecam: adaptive visual attention for monocular depth sensing. ||| brevin tilmon ||| sanjeev j. koppal ||| 
2021 ||| visio-temporal attention for multi-camera multi-target association. ||| yu-jhe li ||| xinshuo weng ||| yan xu ||| kris kitani ||| 
2021 ||| transformer-based dual relation graph for multi-label image recognition. ||| jiawei zhao ||| ke yan ||| yifan zhao ||| xiaowei guo ||| feiyue huang ||| jia li ||| 
2021 ||| glit: neural architecture search for global and local image transformer. ||| boyu chen ||| peixia li ||| chuming li ||| baopu li ||| lei bai ||| chen lin ||| ming sun ||| junjie yan ||| wanli ouyang ||| 
2021 ||| dance with self-attention: a new look of conditional random fields on anomaly detection in videos. ||| didik purwanto ||| yie-tarng chen ||| wen-hsien fang ||| 
2021 ||| self-supervised geometric features discovery via interpretable attention for vehicle re-identification and beyond. ||| ming li ||| xinming huang ||| ziming zhang ||| 
2021 ||| ts-cam: token semantic coupled attention map for weakly supervised object localization. ||| wei gao ||| fang wan ||| xingjia pan ||| zhiliang peng ||| qi tian ||| zhenjun han ||| bolei zhou ||| qixiang ye ||| 
2019 ||| mixed high-order attention network for person re-identification. ||| binghui chen ||| weihong deng ||| jiani hu ||| 
2021 ||| salient object ranking with position-preserved attention. ||| hao fang ||| daoxin zhang ||| yi zhang ||| minghao chen ||| jiawei li ||| yao hu ||| deng cai ||| xiaofei he ||| 
2021 ||| spatial-temporal transformer for dynamic scene graph generation. ||| yuren cong ||| wentong liao ||| hanno ackermann ||| bodo rosenhahn ||| michael ying yang ||| 
2021 ||| visual saliency transformer. ||| nian liu ||| ni zhang ||| kaiyuan wan ||| ling shao ||| junwei han ||| 
2021 ||| transvg: end-to-end visual grounding with transformers. ||| jiajun deng ||| zhengyuan yang ||| tianlang chen ||| wengang zhou ||| houqiang li ||| 
2021 ||| vidtr: video transformer without convolutions. ||| yanyi zhang ||| xinyu li ||| chunhui liu ||| bing shuai ||| yi zhu ||| biagio brattoli ||| hao chen ||| ivan marsic ||| joseph tighe ||| 
2019 ||| relational attention network for crowd counting. ||| anran zhang ||| jiayi shen ||| zehao xiao ||| fan zhu ||| xiantong zhen ||| xianbin cao ||| ling shao ||| 
2021 ||| cvt: introducing convolutions to vision transformers. ||| haiping wu ||| bin xiao ||| noel codella ||| mengchen liu ||| xiyang dai ||| lu yuan ||| lei zhang ||| 
2021 ||| multimodal co-attention transformer for survival prediction in gigapixel whole slide images. ||| richard j. chen ||| ming y. lu ||| wei-hung weng ||| tiffany y. chen ||| drew f. k. williamson ||| trevor manz ||| maha shady ||| faisal mahmood ||| 
2021 ||| ensemble attention distillation for privacy-preserving federated learning. ||| xuan gong ||| abhishek sharma ||| srikrishna karanam ||| ziyan wu ||| terrence chen ||| david s. doermann ||| arun innanje ||| 
2017 ||| deep cropping via attention box prediction and aesthetics assessment. ||| wenguan wang ||| jianbing shen ||| 
2017 ||| single shot text detector with regional attention. ||| pan he ||| weilin huang ||| tong he ||| qile zhu ||| yu qiao ||| xiaolin li ||| 
2021 ||| encoder-decoder with multi-level attention for 3d human shape and pose estimation. ||| ziniu wan ||| zhengjia li ||| maoqing tian ||| jianbo liu ||| shuai yi ||| hongsheng li ||| 
2021 ||| dual contrastive loss and attention for gans. ||| ning yu ||| guilin liu ||| aysegul dundar ||| andrew tao ||| bryan catanzaro ||| larry davis ||| mario fritz ||| 
2017 ||| faster than real-time facial alignment: a 3d spatial transformer network approach in unconstrained poses. ||| chandrasekhar bhagavatula ||| chenchen zhu ||| khoa luu ||| marios savvides ||| 
2021 ||| pnp-detr: towards efficient visual analysis with transformers. ||| tao wang ||| li yuan ||| yunpeng chen ||| jiashi feng ||| shuicheng yan ||| 
2019 ||| attention augmented convolutional networks. ||| irwan bello ||| barret zoph ||| quoc le ||| ashish vaswani ||| jonathon shlens ||| 
2021 ||| ctrl-c: camera calibration transformer with line-classification. ||| jinwoo lee ||| hyunsung go ||| hyunjoon lee ||| sunghyun cho ||| min-hyuk sung ||| junho kim ||| 
2019 ||| real image denoising with feature attention. ||| saeed anwar ||| nick barnes ||| 
2021 ||| visformer: the vision-friendly transformer. ||| zhengsu chen ||| lingxi xie ||| jianwei niu ||| xuefeng liu ||| longhui wei ||| qi tian ||| 
2021 ||| causal attention for unbiased visual recognition. ||| tan wang ||| chang zhou ||| qianru sun ||| hanwang zhang ||| 
2021 ||| improving 3d object detection with channel-wise transformer. ||| hualian sheng ||| sijia cai ||| yuan liu ||| bing deng ||| jianqiang huang ||| xian-sheng hua ||| min-jian zhao ||| 
2021 ||| high-fidelity pluralistic image completion with transformers. ||| ziyu wan ||| jingbo zhang ||| dongdong chen ||| jing liao ||| 
2021 ||| incorporating convolution designs into visual transformers. ||| kun yuan ||| shaopeng guo ||| ziwei liu ||| aojun zhou ||| fengwei yu ||| wei wu ||| 
2021 ||| trar: routing the attention spans in transformer for visual question answering. ||| yiyi zhou ||| tianhe ren ||| chaoyang zhu ||| xiaoshuai sun ||| jianzhuang liu ||| xinghao ding ||| mingliang xu ||| rongrong ji ||| 
2019 ||| attentionrnn: a structured spatial attention mechanism. ||| siddhesh khandelwal ||| leonid sigal ||| 
2017 ||| attention-based multimodal fusion for video description. ||| chiori hori ||| takaaki hori ||| teng-yok lee ||| ziming zhang ||| bret harsham ||| john r. hershey ||| tim k. marks ||| kazuhiro sumi ||| 
2019 ||| expectation-maximization attention networks for semantic segmentation. ||| xia li ||| zhisheng zhong ||| jianlong wu ||| yibo yang ||| zhouchen lin ||| hong liu ||| 
2021 ||| crossvit: cross-attention multi-scale vision transformer for image classification. ||| chun-fu (richard) chen ||| quanfu fan ||| rameswar panda ||| 
2021 ||| transformer-based attention networks for continuous pixel-wise prediction. ||| guanglei yang ||| hao tang ||| mingli ding ||| nicu sebe ||| elisa ricci ||| 
2021 ||| frequency-aware spatiotemporal transformers for video inpainting detection. ||| bingyao yu ||| wanhua li ||| xiu li ||| jiwen lu ||| jie zhou ||| 
2019 ||| image inpainting with learnable bidirectional attention maps. ||| chaohao xie ||| shaohui liu ||| chao li ||| ming-ming cheng ||| wangmeng zuo ||| xiao liu ||| shilei wen ||| errui ding ||| 
2019 ||| acfnet: attentional class feature network for semantic segmentation. ||| fan zhang ||| yanqin chen ||| zhihang li ||| zhibin hong ||| jingtuo liu ||| feifei ma ||| junyu han ||| errui ding ||| 
2021 ||| temporal-wise attention spiking neural networks for event streams classification. ||| man yao ||| huanhuan gao ||| guangshe zhao ||| dingheng wang ||| yihan lin ||| zhao-xu yang ||| guoqi li ||| 
2019 ||| deep contextual attention for human-object interaction detection. ||| tiancai wang ||| rao muhammad anwer ||| muhammad haris khan ||| fahad shahbaz khan ||| yanwei pang ||| ling shao ||| jorma laaksonen ||| 
2021 ||| wb-detr: transformer-based detector without backbone. ||| fanfan liu ||| haoran wei ||| wenzhe zhao ||| guozhen li ||| jingquan peng ||| zihao li ||| 
2021 ||| cotr: correspondence transformer for matching across images. ||| wei jiang ||| eduard trulls ||| jan hosang ||| andrea tagliasacchi ||| kwang moo yi ||| 
2021 ||| snowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer. ||| peng xiang ||| xin wen ||| yu-shen liu ||| yan-pei cao ||| pengfei wan ||| wen zheng ||| zhizhong han ||| 
2021 ||| 3dvg-transformer: relation modeling for visual grounding on point clouds. ||| lichen zhao ||| daigang cai ||| lu sheng ||| dong xu ||| 
2021 ||| understanding robustness of transformers for image classification. ||| srinadh bhojanapalli ||| ayan chakrabarti ||| daniel glasner ||| daliang li ||| thomas unterthiner ||| andreas veit ||| 
2021 ||| the center of attention: center-keypoint grouping via attention for multi-person pose estimation. ||| guillem bras ||| nikita kister ||| laura leal-taix ||| 
2021 ||| context-aware scene graph generation with seq2seq transformers. ||| yichao lu ||| himanshu rai ||| jason chang ||| boris knyazev ||| guang wei yu ||| shashank shekhar ||| graham w. taylor ||| maksims volkovs ||| 
2021 ||| class semantics-based attention for action detection. ||| deepak sridhar ||| niamul quader ||| srikanth muralidharan ||| yaoxin li ||| peng dai ||| juwei lu ||| 
2021 ||| visual transformers: where do transformers really belong in vision models? ||| bichen wu ||| chenfeng xu ||| xiaoliang dai ||| alvin wan ||| peizhao zhang ||| zhicheng yan ||| masayoshi tomizuka ||| joseph gonzalez ||| kurt keutzer ||| peter vajda ||| 
2021 ||| the animation transformer: visual correspondence via segment matching. ||| evan casey ||| v ||| ctor p ||| rez ||| zhuoru li ||| 
2019 ||| human attention in image captioning: dataset and analysis. ||| sen he ||| hamed rezazadegan tavakoli ||| ali borji ||| nicolas pugeault ||| 
2021 ||| hift: hierarchical feature transformer for aerial tracking. ||| ziang cao ||| changhong fu ||| junjie ye ||| bowen li ||| yiming li ||| 
2017 ||| focusing attention: towards accurate text recognition in natural images. ||| zhanzhan cheng ||| fan bai ||| yunlu xu ||| gang zheng ||| shiliang pu ||| shuigeng zhou ||| 
2021 ||| agkd-bml: defense against adversarial attack by attention guided knowledge distillation and bi-directional metric learning. ||| hong wang ||| yuefan deng ||| shinjae yoo ||| haibin ling ||| yuewei lin ||| 
2019 ||| coherent semantic attention for image inpainting. ||| hongyu liu ||| bin jiang ||| yi xiao ||| chao yang ||| 
2021 ||| rethinking transformer-based set prediction for object detection. ||| zhiqing sun ||| shengcao cao ||| yiming yang ||| kris kitani ||| 
2021 ||| image harmonization with transformer. ||| zonghui guo ||| dongsheng guo ||| haiyong zheng ||| zhaorui gu ||| bing zheng ||| junyu dong ||| 
2021 ||| hierarchical graph attention network for few-shot visual-semantic learning. ||| chengxiang yin ||| kun wu ||| zhengping che ||| bo jiang ||| zhiyuan xu ||| jian tang ||| 
2019 ||| patchwork: a patch-wise attention network for efficient object detection and segmentation in video streams. ||| yuning chai ||| 
2021 ||| contrastive attention maps for self-supervised co-localization. ||| minsong ki ||| youngjung uh ||| junsuk choe ||| hyeran byun ||| 
2019 ||| an empirical study of spatial attention mechanisms in deep networks. ||| xizhou zhu ||| dazhi cheng ||| zheng zhang ||| stephen lin ||| jifeng dai ||| 
2021 ||| docformer: end-to-end transformer for document understanding. ||| srikar appalaraju ||| bhavan jasani ||| bhargava urala kota ||| yusheng xie ||| r. manmatha ||| 
2020 ||| geometric attention for prediction of differential properties in 3d point clouds. ||| albert matveev ||| alexey artemov ||| denis zorin ||| evgeny burnaev ||| 
2017 ||| analysis of human attentions for face recognition on natural videos and comparison with cv algorithm on performance. ||| mona ragab sayed ||| rosary yuting lim ||| bappaditya mandal ||| liyuan li ||| joo-hwee lim ||| terence sim ||| 
2017 ||| a deep neural model for emotion-driven multimodal attention. ||| german ignacio parisi ||| pablo v. a. barros ||| haiyan wu ||| guochun yang ||| zhenghan li ||| xun liu ||| stefan wermter ||| 
2020 ||| using pre-trained transformer deep learning models to identify named entities and syntactic relations for clinical protocol analysis. ||| miao chen ||| fang du ||| ganhui lan ||| victor s. lobanov ||| 
2021 ||| self-adaptive physics-informed neural networks using a soft attention mechanism. ||| levi d. mcclenny ||| ulisses m. braga-neto ||| 
2019 ||| artificial agency requires attention: the case of intentional action. ||| paul bello ||| kevin o'neill ||| will bridewell ||| 
2018 ||| neural networks with attention for word sense induction. ||| oleg struyanskiy ||| nikolay arefyev ||| 
2021 ||| multimodal fusion with bert and attention mechanism for fake news detection. ||| nguyen manh duc tuan ||| pham quang nhat minh ||| 
2021 ||| an improved deep neural network based on a novel visual attention mechanism for text recognition. ||| nguyen trong thai ||| nguyen hoang thuan ||| dinh viet sang ||| 
2018 ||| design and modeling of new configuration of three phase transformer for high voltage operation using in microwave industrial. ||| hamid outzguinrimt ||| ali bouzit ||| mohammed chrayagne ||| mouhcine lahame ||| rajaa oumghar ||| mohamed ferfra ||| 
2021 ||| contextual bi-directional attention flow with embeddings from language models: a generative approach to emotion detection. ||| prashant kumar nag ||| vishnu priya r ||| 
2017 ||| power transformer condition forecast with time-series extrapolation. ||| andrey a. radionov ||| alexander s. karandaev ||| igor m. yachikov ||| olga i. karandaeva ||| vadim r. gasiyarov ||| 
2021 ||| towards efficient cross-modal visual textual retrieval using transformer-encoder deep features. ||| nicola messina ||| giuseppe amato ||| fabrizio falchi ||| claudio gennaro ||| st ||| phane marchand-maillet ||| 
2017 ||| connoisseur: classification of styles of mexican architectural heritage with deep learning and visual attention prediction. ||| abraham montoya obeso ||| mireya s. garc ||| a-v ||| zquez ||| alejandro alvaro ram ||| rez-acosta ||| jenny benois-pineau ||| 
2019 ||| dropping activations in convolutional neural networks with visual attention maps. ||| abraham montoya obeso ||| jenny benois-pineau ||| mireya sara |||  garc ||| a-v ||| zquez ||| alejandro alvaro ram ||| rez-acosta ||| 
2021 ||| operation-wise attention network for tampering localization fusion. ||| polychronis charitidis ||| giorgos kordopatis-zilos ||| symeon papadopoulos ||| ioannis kompatsiaris ||| 
2021 ||| a gru neural network with attention mechanism for detection of risk situations on multimodal lifelog data. ||| rupayan mallick ||| thinhinane yebda ||| jenny benois-pineau ||| akka zemmari ||| marion pech ||| h ||| l ||| ne amieva ||| 
2021 ||| bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification. ||| am ||| lie gruel ||| jean martinet ||| 
2021 ||| elsa: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. ||| tae jun ham ||| yejin lee ||| seong hoon seo ||| soosung kim ||| hyunji choi ||| sung jun jung ||| jae w. lee ||| 
2019 ||| an attention-based cnn for ecg classification. ||| alexander kuvaev ||| roman khudorozhkov ||| 
2022 ||| exploring transformers for intruder detection in complex maritime environment. ||| mrunalini nalamati ||| muhammad saqib ||| nabin sharma ||| michael blumenstein ||| 
2022 ||| predicting outcomes for cancer patients with transformer-based multi-task learning. ||| leah gerrard ||| xueping peng ||| allison clarke ||| clement schlegel ||| jing jiang ||| 
2021 ||| lcl: light contactless low-delay load monitoring via compressive attentional multi-label learning. ||| xiaoyu wang ||| hao zhou ||| nikolaos m. freris ||| wangqiu zhou ||| xing guo ||| zhi liu ||| yusheng ji ||| xiang-yang li ||| 
2021 ||| byte-label joint attention learning for packet-grained network traffic classification. ||| kelong mao ||| xi xiao ||| guangwu hu ||| xiapu luo ||| bin zhang ||| shutao xia ||| 
2020 ||| back-guard: wireless backscattering based user activity recognition and identification with parallel attention model. ||| manjiang yin ||| xiang-yang li ||| yanyong zhang ||| panlong yang ||| chengchen wan ||| 
2021 ||| a-ddpg: attention mechanism-based deep reinforcement learning for nfv. ||| nan he ||| song yang ||| fan li ||| stojan trajanovski ||| fernando a. kuipers ||| xiaoming fu ||| 
2021 ||| ikan: interactive knowledge-aware attention network for recommendation. ||| shihu wu ||| bo yang ||| yihu zhang ||| yaohai zeng ||| 
2021 ||| emotion cause extraction by combining intra-clause sentiment-enhanced attention and inter-clause consistency interaction. ||| wenhui yu ||| chongyang shi ||| 
2021 ||| research on crowd counting based on attention mechanism and dilation convolution. ||| pingping li ||| hongmin zhang ||| xiaobing fang ||| shunyuan li ||| hao zhou ||| xu zhuang ||| 
2020 ||| feature retrieving for human action recognition by mixed scale deep feature combined with attention model. ||| xiaolei zhao ||| yang yi ||| zemin qiu ||| qingqing zeng ||| 
2021 ||| temperature prediction based on integrated deep learning and attention mechanism. ||| xuan zhao ||| lvwen huang ||| yanming nie ||| 
2021 ||| a multi-grained attention network for multi-labeled distant supervision relation extraction. ||| mingjie tang ||| bo yang ||| hao xu ||| 
2021 ||| a new siamese co-attention network for unsupervised video object segmentation. ||| zhenghao zhang ||| liguo sun ||| lingyu si ||| changwen zheng ||| 
2021 ||| a new method of wave parameter retrieve based on transformer. ||| shuaiyu chen ||| changwen zheng ||| liguo sun ||| 
2021 ||| attention-based efficient lightweight model for accurate real-time face verification on embedded device. ||| dongmei wei ||| xingjun wu ||| guoqiang bai ||| linlin su ||| sufen xu ||| 
2019 ||| implementation of marketing communication strategy in attention, interest, search, action, and share (aisas) model through vlog. ||| yunus natanael pelawi ||| irwansyah ||| monika pretty aprilia ||| 
2020 ||| text steganalysis with attentional l stm-cnn. ||| yongjian bao ||| hao yang ||| zhongliang yang ||| sheng liu ||| yongfeng huang ||| 
2020 ||| self-attention for cyberbullying detection. ||| ankit pradhan ||| venu madhav yatam ||| padmalochan bera ||| 
2017 ||| efficient speaker naming via deep audio-face fusion and end-to-end attention model. ||| xin liu ||| jiajia geng ||| haibin ling ||| 
2019 ||| early diagnosis of alzheimer's disease based on selective kernel network with spatial attention. ||| huanhuan ji ||| zhenbing liu ||| wei qi yan ||| reinhard klette ||| 
2019 ||| attention guided unsupervised image-to-image translation with progressively growing strategy. ||| yuchen wu ||| runtong zhang ||| keiji yanai ||| 
2019 ||| spatial-temporal graph attention network for video-based gait recognition. ||| xinhui wu ||| weizhi an ||| shiqi yu ||| weiyu guo ||| edel b. garc ||| a reyes ||| 
2019 ||| dual-attention graph convolutional network. ||| xueya zhang ||| tong zhang ||| wenting zhao ||| zhen cui ||| jian yang ||| 
2019 ||| attention recurrent neural networks for image-based sequence text recognition. ||| guoqiang zhong ||| guohua yue ||| 
2019 ||| residual attention encoding neural network for terrain texture classification. ||| xulin song ||| jingyu yang ||| zhong jin ||| 
2017 ||| attention-set based metric learning for video face recognition. ||| yibo hu ||| xiang wu ||| ran he ||| 
2019 ||| aggregating motion and attention for video object detection. ||| ruyi zhang ||| zhenjiang miao ||| cong ma ||| shanshan hao ||| 
2019 ||| action recognition in untrimmed videos with composite self-attention two-stream framework. ||| dong cao ||| lisha xu ||| haibo chen ||| 
2019 ||| ssa-gan: end-to-end time-lapse video generation with spatial self-attention. ||| daichi horita ||| keiji yanai ||| 
2020 ||| attention-aware linear depthwise convolution for single image super-resolution. ||| seong-min hwang ||| juhwan lee ||| cheolkon jung ||| jinyoung kim ||| 
2020 ||| multimodal fusion with attention mechanism for trustworthiness prediction in car advertisements. ||| van thong huynh ||| hyung-jeong yang ||| guee-sang lee ||| jung hee kim ||| soo-hyung kim ||| 
2020 ||| comparison of attention module for acoustic scene classification. ||| nisan aryal ||| sang-woong lee ||| 
2021 ||| msa-cnn: face morphing detection via a multiple scales attention convolutional neural network. ||| le-bing zhang ||| juan cai ||| fei peng ||| min long ||| 
2021 ||| double-stream segmentation network with temporal self-attention for deepfake video detection. ||| beibei liu ||| yifei gao ||| yongjian hu ||| jingjing guo ||| yufei wang ||| 
2019 ||| rca-net: image recovery network with channel attention group for image dehazing. ||| juan du ||| jiajia zhang ||| zhe zhang ||| wei tan ||| shangzhen song ||| huixin zhou ||| 
2019 ||| improving the attention span of elementary school children in mexico through a s4 technology platform. ||| edgar omar l ||| pez-caudana ||| pedro ponce ||| nancy mazon ||| luis marquez ||| ivan mejia ||| german baltazar reyes ||| 
2018 ||| detecting attention in pivotal response treatment video probes. ||| corey d. c. heath ||| hemanth venkateswara ||| troy mcdaniel ||| sethuraman panchanathan ||| 
2021 ||| transformer-based language models for semantic search and mobile applications retrieval. ||| jo ||| o coelho ||| ant ||| nio neto ||| miguel tavares ||| carlos coutinho ||| jo ||| o oliveira ||| ricardo ribeiro ||| fernando batista ||| 
2020 ||| multi-label classification for clinical text with feature-level attention. ||| disheng pan ||| xizi zheng ||| weijie liu ||| mengya li ||| meng ma ||| ying zhou ||| li yang ||| ping wang ||| 
2019 ||| design of air-cooled control system for intelligent transformer. ||| dantian zhong ||| qiang gao ||| jiayu pan ||| zhannan guo ||| maojun wang ||| 
2019 ||| using eye movement to assess auditory attention. ||| alaa bakry ||| radwa al-khatib ||| randa negm ||| eslam sabra ||| mohamed maher ||| zainab mohamed ||| doaa shawky ||| ashraf h. badawi ||| 
2021 ||| fault recognition of analog circuits based on ultra-lightweight subspace attention module. ||| aihua zhang ||| xinglong yu ||| yang zhang ||| 
2021 ||| a deep attention-driven model to forecast solar irradiance. ||| abdelkader dairi ||| fouzi harrou ||| ying sun ||| 
2019 ||| better predictability of arbitrage strategies trained among less attentionstocks. ||| jie fang ||| jianwu lin ||| 
2019 ||| neurofeedback and ai for analyzing child temperament and attention levels. ||| maria r. lee ||| anna yu-ju yen ||| lun chang ||| 
2022 ||| meta-learning fine-tuned feature extractor for few-shot image classification: a case study on fine-tuning cnn backbone with transformer for few-shot learning. ||| yulin shen ||| xian shuai ||| 
2020 ||| guiding the illumination estimation using the attention mechanism. ||| karlo koscevic ||| marko subasic ||| sven loncaric ||| 
2022 ||| transformer-based neural texture synthesis and style transfer. ||| jiahao lu ||| 
2018 ||| time-variant visual attention in 360-degree video playback. ||| huiwen huang ||| jinling chen ||| hong xue ||| yaping huang ||| tiesong zhao ||| 
2021 ||| evolving transformer architecture for neural machine translation. ||| ben feng ||| dayiheng liu ||| yanan sun ||| 
2021 ||| a tag-based transformer community question answering learning-to-rank model in the home improvement domain. ||| macedo maia ||| siegfried handschuh ||| markus endres ||| 
2021 ||| gace: graph-attention-network-based cardinality estimator. ||| daobing zhu ||| dongsheng he ||| shu huan fan ||| jianming liao ||| mengshu hou ||| 
2021 ||| log-based anomaly detection with multi-head scaled dot-product attention mechanism. ||| qingfeng du ||| liang zhao ||| jincheng xu ||| yongqi han ||| shuangli zhang ||| 
2018 ||| a transformerless high gain switched-inductor switched-capacitor cuk converter in step-up mode. ||| yasser almalaq ||| ayoob alateeq ||| mohammad matin ||| 
2018 ||| internal fault analysis in disk-type transformer winding using network reduction. ||| yazid alkraimeen ||| pablo gomez ||| 
2017 ||| high efficiency three level transformerless inverter based on sic mosfets for pv applications. ||| fahad m. almasoudi ||| khaled s. alatawi ||| mohammad matin ||| 
2021 ||| material texture recognition using ultrasonic images with transformer neural networks. ||| xin zhang ||| jafar saniie ||| 
2019 ||| friend recommendation model based on multi-dimensional academic feature and attention mechanism. ||| yi he ||| liu wang ||| chengjie mao ||| ying li ||| saimei sun ||| yixiang cai ||| 
2019 ||| chinese-vietnamese news documents summarization based on feature-related attention mechanism. ||| jinjuan wu ||| zhengtao yu ||| shengxiang gao ||| junjun guo ||| ran song ||| 
2019 ||| a graph representation learning algorithm based on attention mechanism and node similarity. ||| kun guo ||| deqin wang ||| jiangsheng huang ||| yuzhong chen ||| zhihao zhu ||| jianning zheng ||| 
2019 ||| charge prediction for multi-defendant cases with multi-scale attention. ||| sicheng pan ||| tun lu ||| ning gu ||| huajuan zhang ||| chunlin xu ||| 
2019 ||| a mobile application classification method with enhanced topic attention mechanism. ||| junjie chen ||| buqing cao ||| yingcheng cao ||| jianxun liu ||| rong hu ||| yiping wen ||| 
2019 ||| measuring node similarity for the collective attention flow network. ||| manfu ma ||| zhangyun gong ||| yong li ||| huifang li ||| qiang zhang ||| xiaokang zhang ||| changqing wang ||| 
2019 ||| a multi-domain named entity recognition method based on part-of-speech attention mechanism. ||| shun zhang ||| ying sheng ||| jiangfan gao ||| jianhui chen ||| jiajin huang ||| shaofu lin ||| 
2020 ||| modeling limited attention in opinion dynamics by topological interactions. ||| francesca ceragioli ||| paolo frasca ||| wilbert samuel rossi ||| 
2017 ||| deep learning based automatic diagnoses of attention deficit hyperactive disorder. ||| liang zou ||| jiannan zheng ||| martin j. mckeown ||| 
2019 ||| a divide-and-conquer framework for attention-based combination of multiple investment strategies. ||| xiao yang ||| weiqing liu ||| lewen wang ||| cheng qu ||| jiang bian ||| 
2019 ||| an attention based deep neural network for automatic lexical stress detection. ||| tian xia ||| xianfeng rui ||| chien-lin huang ||| iek heng chu ||| shaojun wang ||| mei han ||| 
2019 ||| an: a temporal-frequency fusion attention network for spectrum energy level prediction. ||| kehan li ||| zebo liu ||| shibo he ||| jiming chen ||| 
2019 ||| fovr: attention-based vr streaming through bandwidth-limited wireless networks. ||| songzhou yang ||| yuan he ||| xiaolong zheng ||| 
2020 ||| extended abstract - transformers: intrusion detection data in disguise. ||| james boorman ||| benjamin green ||| daniel prince ||| 
2020 ||| evaluating pretrained transformer models for citation recommendation. ||| rodrigo nogueira ||| zhiying jiang ||| kyunghyun cho ||| jimmy lin ||| 
2017 ||| distributing attention between environment and navigation system to increase spatial knowledge acquisition during assisted wayfinding. ||| annina br ||| gger ||| kai-florian richter ||| sara irina fabrikant ||| 
2020 ||| sentiment prediction using attention on user-specific rating distribution. ||| ting lin ||| aixin sun ||| 
2018 ||| product question intent detection using indicative clause attention and adversarial learning. ||| qian yu ||| wai lam ||| 
2019 ||| sadhan: hierarchical attention networks to learn latent aspect embeddings for fake news detection. ||| rahul mishra ||| vinay setty ||| 
2017 ||| estimation of students' attention in the classroom from kinect features. ||| janez zaletelj ||| 
2021 ||| pneumoxttention: a cnn compensating for human fallibility when detecting pneumonia through cxr images with attention. ||| sanskriti singh ||| 
2021 ||| illuminant estimation error detection for outdoor scenes using transformers. ||| donik vrsnak ||| ilija domislovic ||| marko subasic ||| sven loncaric ||| 
2019 ||| attention-based convolutional neural network for computer vision color constancy. ||| karlo koscevic ||| marko subasic ||| sven loncaric ||| 
2021 ||| effective attention sheds light on interpretability. ||| kaiser sun ||| ana marasovic ||| 
2021 ||| transforming term extraction: transformer-based approaches to multilingual term extraction across domains. ||| christian lang ||| lennart wachowiak ||| barbara heinisch ||| dagmar gromann ||| 
2018 ||| efficient large-scale neural domain classification with personalized attention. ||| young-bum kim ||| dongchan kim ||| anjishnu kumar ||| ruhi sarikaya ||| 
2020 ||| sentibert: a transferable transformer-based architecture for compositional sentiment semantics. ||| da yin ||| tao meng ||| kai-wei chang ||| 
2021 ||| personalized transformer for explainable recommendation. ||| lei li ||| yongfeng zhang ||| li chen ||| 
2019 ||| assessing the ability of self-attention networks to learn word order. ||| baosong yang ||| longyue wang ||| derek f. wong ||| lidia s. chao ||| zhaopeng tu ||| 
2021 ||| attention calibration for transformer in neural machine translation. ||| yu lu ||| jiali zeng ||| jiajun zhang ||| shuangzhi wu ||| mu li ||| 
2020 ||| regularized context gates on transformer for machine translation. ||| xintong li ||| lemao liu ||| rui wang ||| guoping huang ||| max meng ||| 
2021 ||| error detection in large-scale natural language understanding systems using transformer models. ||| rakesh chada ||| pradeep natarajan ||| darshan fofadiya ||| prathap ramachandra ||| 
2019 ||| learning attention-based embeddings for relation prediction in knowledge graphs. ||| deepak nathani ||| jatin chauhan ||| charu sharma ||| manohar kaul ||| 
2021 ||| length-adaptive transformer: train once with length drop, use anytime with search. ||| gyuwan kim ||| kyunghyun cho ||| 
2021 ||| multimodal graph-based transformer framework for biomedical relation extraction. ||| sriram pingali ||| shweta yadav ||| pratik dutta ||| sriparna saha ||| 
2020 ||| learning to deceive with attention-based explanations. ||| danish pruthi ||| mansi gupta ||| bhuwan dhingra ||| graham neubig ||| zachary c. lipton ||| 
2020 ||| entity-aware dependency-based deep graph attention network for comparative preference classification. ||| nianzu ma ||| sahisnu maz ||| umder ||| hao wang ||| bing liu ||| 
2019 ||| incremental transformer with deliberation decoder for document grounded conversations. ||| zekang li ||| cheng niu ||| fandong meng ||| yang feng ||| qian li ||| jie zhou ||| 
2017 ||| learning attention for historical text normalization by learning to pronounce. ||| marcel bollmann ||| joachim bingel ||| anders s ||| gaard ||| 
2020 ||| inset: sentence infilling with inter-sentential transformer. ||| yichen huang ||| yizhe zhang ||| oussama elachqar ||| yu cheng ||| 
2019 ||| aspect sentiment classification towards question-answering with reinforced bidirectional attention network. ||| jingjing wang ||| changlong sun ||| shoushan li ||| xiaozhong liu ||| luo si ||| min zhang ||| guodong zhou ||| 
2020 ||| enhancing machine translation with dependency-aware self-attention. ||| emanuele bugliarello ||| naoaki okazaki ||| 
2019 ||| extracting multiple-relations in one-pass with pre-trained transformers. ||| haoyu wang ||| ming tan ||| mo yu ||| shiyu chang ||| dakuo wang ||| kun xu ||| xiaoxiao guo ||| saloni potdar ||| 
2020 ||| relational graph attention network for aspect-based sentiment analysis. ||| kai wang ||| weizhou shen ||| yunyi yang ||| xiaojun quan ||| rui wang ||| 
2021 ||| enhancing transformers with gradient boosted decision trees for nli fine-tuning. ||| benjamin minixhofer ||| milan gritta ||| ignacio iacobacci ||| 
2021 ||| can transformer models measure coherence in text: re-thinking the shuffle test. ||| philippe laban ||| luke dai ||| lucas bandarkar ||| marti a. hearst ||| 
2019 ||| lattice-based transformer encoder for neural machine translation. ||| fengshun xiao ||| jiangtong li ||| hai zhao ||| rui wang ||| kehai chen ||| 
2020 ||| hiring now: a skill-aware multi-attention model for job posting generation. ||| liting liu ||| jie liu ||| wenzheng zhang ||| ziming chi ||| wenxuan shi ||| yalou huang ||| 
2020 ||| heterogeneous graph transformer for graph-to-sequence learning. ||| shaowei yao ||| tianming wang ||| xiaojun wan ||| 
2020 ||| specter: document-level representation learning using citation-informed transformers. ||| arman cohan ||| sergey feldman ||| iz beltagy ||| doug downey ||| daniel s. weld ||| 
2017 ||| exploiting argument information to improve event detection via supervised attention mechanisms. ||| shulin liu ||| yubo chen ||| kang liu ||| jun zhao ||| 
2020 ||| understanding attention for text classification. ||| xiaobing sun ||| wei lu ||| 
2019 ||| improving textual network embedding with global attention via optimal transport. ||| liqun chen ||| guoyin wang ||| chenyang tao ||| dinghan shen ||| pengyu cheng ||| xinyuan zhang ||| wenlin wang ||| yizhe zhang ||| lawrence carin ||| 
2021 ||| cascaded head-colliding attention. ||| lin zheng ||| zhiyong wu ||| lingpeng kong ||| 
2019 ||| self-attention architectures for answer-agnostic neural question generation. ||| thomas scialom ||| benjamin piwowarski ||| jacopo staiano ||| 
2019 ||| multilingual constituency parsing with self-attention and pre-training. ||| nikita kitaev ||| steven cao ||| dan klein ||| 
2020 ||| line graph enhanced amr-to-text generation with mix-order graph attention networks. ||| yanbin zhao ||| lu chen ||| zhi chen ||| ruisheng cao ||| su zhu ||| kai yu ||| 
2021 |||  marbert: deep bidirectional transformers for arabic. ||| muhammad abdul-mageed ||| abdelrahim a. elmadany ||| el moatez billah nagoudi ||| 
2017 ||| domain attention with an ensemble of experts. ||| young-bum kim ||| karl stratos ||| dongchan kim ||| 
2018 ||| cross-target stance classification with self-attention networks. ||| chang xu ||| c ||| cile paris ||| surya nepal ||| ross sparks ||| 
2020 ||| hat: hardware-aware transformers for efficient natural language processing. ||| hanrui wang ||| zhanghao wu ||| zhijian liu ||| han cai ||| ligeng zhu ||| chuang gan ||| song han ||| 
2020 ||| lexically constrained neural machine translation with levenshtein transformer. ||| raymond hendy susanto ||| shamil chollampatt ||| liling tan ||| 
2021 ||| memory-efficient differentiable transformer architecture search. ||| yuekai zhao ||| li dong ||| yelong shen ||| zhihua zhang ||| furu wei ||| weizhu chen ||| 
2018 ||| a multi-sentiment-resource enhanced attention network for sentiment classification. ||| zeyang lei ||| yujiu yang ||| min yang ||| yi liu ||| 
2021 ||| diagnosing transformers in task-oriented semantic parsing. ||| shrey desai ||| ahmed aly ||| 
2018 ||| accelerating neural transformer via an average attention network. ||| biao zhang ||| deyi xiong ||| jinsong su ||| 
2017 ||| a nested attention neural hybrid model for grammatical error correction. ||| jianshu ji ||| qinlong wang ||| kristina toutanova ||| yongen gong ||| steven truong ||| jianfeng gao ||| 
2020 ||| automated topical component extraction using neural network attention scores from source-based essay scoring. ||| haoran zhang ||| diane j. litman ||| 
2020 ||| quantifying attention flow in transformers. ||| samira abnar ||| willem h. zuidema ||| 
2019 ||| using human attention to extract keyphrase from microblog post. ||| yingyi zhang ||| chengzhi zhang ||| 
2017 ||| an unsupervised neural attention model for aspect extraction. ||| ruidan he ||| wee sun lee ||| hwee tou ng ||| daniel dahlmeier ||| 
2020 ||| joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge. ||| yuanhe tian ||| yan song ||| xiang ao ||| fei xia ||| xiaojun quan ||| tong zhang ||| yonggang wang ||| 
2021 ||| the case for translation-invariant self-attention in transformer-based language models. ||| ulme wennberg ||| gustav eje henter ||| 
2021 ||| what context features can transformer language models use? ||| joe o'connor ||| jacob andreas ||| 
2021 ||| a bidirectional transformer based alignment model for unsupervised word alignment. ||| jingyi zhang ||| josef van genabith ||| 
2021 ||| multimodal fusion with co-attention networks for fake news detection. ||| yang wu ||| pengwei zhan ||| yunjian zhang ||| liming wang ||| zhen xu ||| 
2019 ||| multi-grained attention with object-level grounding for visual question answering. ||| pingping huang ||| jianhui huang ||| yuqing guo ||| min qiao ||| yong zhu ||| 
2021 ||| a span-based dynamic local attention model for sequential sentence classification. ||| xichen shang ||| qianli ma ||| zhenxi lin ||| jiangyue yan ||| zipeng chen ||| 
2019 ||| you only need attention to traverse trees. ||| mahtab ahmed ||| muhammad rifayat samee ||| robert e. mercer ||| 
2018 ||| document embedding enhanced event detection with hierarchical and supervised attention. ||| yue zhao ||| xiaolong jin ||| yuanzhuo wang ||| xueqi cheng ||| 
2021 ||| transformer-exclusive cross-modal representation for vision and language. ||| andrew shin ||| takuya narihira ||| 
2017 ||| neural relation extraction with multi-lingual attention. ||| yankai lin ||| zhiyuan liu ||| maosong sun ||| 
2021 ||| tan-ntm: topic attention networks for neural topic modeling. ||| madhur panwar ||| shashank shailabh ||| milan aggarwal ||| balaji krishnamurthy ||| 
2021 ||| select, extract and generate: neural keyphrase generation with layer-wise coverage attention. ||| wasi uddin ahmad ||| xiao bai ||| soomin lee ||| kai-wei chang ||| 
2020 ||| highway transformer: self-gating enhanced self-attentive networks. ||| yekun chai ||| jin shuo ||| xinwen hou ||| 
2017 ||| gated-attention readers for text comprehension. ||| bhuwan dhingra ||| hanxiao liu ||| zhilin yang ||| william w. cohen ||| ruslan salakhutdinov ||| 
2018 ||| neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering. ||| rui zhang ||| c ||| cero nogueira dos santos ||| michihiro yasunaga ||| bing xiang ||| dragomir r. radev ||| 
2019 ||| attention and lexicon regularized lstm for aspect-based sentiment analysis. ||| lingxian bao ||| patrik lambert ||| toni badia ||| 
2021 ||| mention flags (mf): constraining transformer-based text generators. ||| yufei wang ||| ian d. wood ||| stephen wan ||| mark dras ||| mark johnson ||| 
2020 ||| lipschitz constrained parameter initialization for deep transformers. ||| hongfei xu ||| qiuhui liu ||| josef van genabith ||| deyi xiong ||| jingyi zhang ||| 
2020 ||| exbert: a visual analysis tool to explore learned representations in transformer models. ||| benjamin hoover ||| hendrik strobelt ||| sebastian gehrmann ||| 
2020 ||| dtca: decision tree-based co-attention networks for explainable claim verification. ||| lianwei wu ||| yuan rao ||| yongqiang zhao ||| hao liang ||| ambreen nazir ||| 
2018 ||| improving slot filling in spoken language understanding with joint pointer and attention. ||| lin zhao ||| zhe feng ||| 
2019 ||| style transformer: unpaired text style transfer without disentangled latent representation. ||| ning dai ||| jianze liang ||| xipeng qiu ||| xuanjing huang ||| 
2021 ||| on the interplay between fine-tuning and composition in transformers. ||| lang yu ||| allyson ettinger ||| 
2019 ||| is attention interpretable? ||| sofia serrano ||| noah a. smith ||| 
2019 ||| modeling intra-relation in math word problems with different functional multi-head attentions. ||| jierui li ||| lei wang ||| jipeng zhang ||| yan wang ||| bing tian dai ||| dongxiang zhang ||| 
2021 ||| unsupervised out-of-domain detection via pre-trained transformers. ||| keyang xu ||| tongzheng ren ||| shikun zhang ||| yihao feng ||| caiming xiong ||| 
2019 ||| leveraging local and global patterns for self-attention networks. ||| mingzhou xu ||| derek f. wong ||| baosong yang ||| yue zhang ||| lidia s. chao ||| 
2021 ||| are pretrained convolutions better than pretrained transformers? ||| yi tay ||| mostafa dehghani ||| jai prakash gupta ||| vamsi aribandi ||| dara bahri ||| zhen qin ||| donald metzler ||| 
2021 ||| irene: interpretable energy prediction for transformers. ||| qingqing cao ||| yash kumar lal ||| harsh trivedi ||| aruna balasubramanian ||| niranjan balasubramanian ||| 
2019 ||| multimodal transformer networks for end-to-end video-grounded dialogue systems. ||| hung le ||| doyen sahoo ||| nancy f. chen ||| steven c. h. hoi ||| 
2019 ||| learning deep transformer models for machine translation. ||| qiang wang ||| bei li ||| tong xiao ||| jingbo zhu ||| changliang li ||| derek f. wong ||| lidia s. chao ||| 
2020 ||| how does selective mechanism improve self-attention networks? ||| xinwei geng ||| longyue wang ||| xing wang ||| bing qin ||| ting liu ||| zhaopeng tu ||| 
2021 ||| ma-bert: learning representation by incorporating multi-attribute knowledge in transformers. ||| you zhang ||| jin wang ||| liang-chih yu ||| xuejie zhang ||| 
2021 ||| a multi-level attention model for evidence-based fact checking. ||| canasai kruengkrai ||| junichi yamagishi ||| xin wang ||| 
2021 ||| readonce transformers: reusable representations of text for transformers. ||| shih-ting lin ||| ashish sabharwal ||| tushar khot ||| 
2020 ||| fine-grained fact verification with kernel graph attention network. ||| zhenghao liu ||| chenyan xiong ||| maosong sun ||| zhiyuan liu ||| 
2019 ||| syntactically supervised transformers for faster neural machine translation. ||| nader akoury ||| kalpesh krishna ||| mohit iyyer ||| 
2020 ||| enhancing pre-trained chinese character representation with word-aligned attention. ||| yanzeng li ||| bowen yu ||| mengge xue ||| tingwen liu ||| 
2017 ||| diversity driven attention model for query-based abstractive summarization. ||| preksha nema ||| mitesh m. khapra ||| anirban laha ||| balaraman ravindran ||| 
2019 ||| monotonic infinite lookback attention for simultaneous machine translation. ||| naveen arivazhagan ||| colin cherry ||| wolfgang macherey ||| chung-cheng chiu ||| semih yavuz ||| ruoming pang ||| wei li ||| colin raffel ||| 
2020 ||| location attention for extrapolation to longer sequences. ||| yann dubois ||| gautier dagan ||| dieuwke hupkes ||| elia bruni ||| 
2021 ||| xeroalign: zero-shot cross-lingual transformer alignment. ||| milan gritta ||| ignacio iacobacci ||| 
2021 ||| attending self-attention: a case study of visually grounded supervision in vision-and-language transformers. ||| jules samaran ||| noa garcia ||| mayu otani ||| chenhui chu ||| yuta nakashima ||| 
2020 ||| improving transformer models by reordering their sublayers. ||| ofir press ||| noah a. smith ||| omer levy ||| 
2021 ||| on-the-fly attention modulation for neural generation. ||| yue dong ||| chandra bhagavatula ||| ximing lu ||| jena d. hwang ||| antoine bosselut ||| jackie chi kit cheung ||| yejin choi ||| 
2020 ||| how does bert's attention change when you fine-tune? an analysis methodology and a case study in negation scope. ||| yiyun zhao ||| steven bethard ||| 
2019 ||| token-level dynamic self-attention network for multi-passage reading comprehension. ||| yimeng zhuang ||| huadong wang ||| 
2021 ||| on orthogonality constraints for transformers. ||| aston zhang ||| alvin chan ||| yi tay ||| jie fu ||| shuohang wang ||| shuai zhang ||| huajie shao ||| shuochao yao ||| roy ka-wei lee ||| 
2019 ||| scheduled sampling for transformers. ||| tsvetomila mihaylova ||| andr |||  f. t. martins ||| 
2019 ||| rumor detection by exploiting user credibility information, attention and multi-task learning. ||| quanzhi li ||| qiong zhang ||| luo si ||| 
2020 ||| the cascade transformer: an application for efficient answer sentence selection. ||| luca soldaini ||| alessandro moschitti ||| 
2021 ||| parameter selection: why we should pay more attention to it. ||| jie-jyun liu ||| tsung-han yang ||| si-an chen ||| chih-jen lin ||| 
2021 ||| bertac: enhancing transformer-based language models with adversarially pretrained convolutional neural networks. ||| jong-hoon oh ||| ryu iida ||| julien kloetzer ||| kentaro torisawa ||| 
2021 ||| argument pair extraction via attention-guided multi-layer multi-cross encoding. ||| liying cheng ||| tianyu wu ||| lidong bing ||| luo si ||| 
2021 ||| parallel attention network with sequence matching for video grounding. ||| hao zhang ||| aixin sun ||| wei jing ||| liangli zhen ||| joey tianyi zhou ||| rick siow mong goh ||| 
2021 ||| global attention decoder for chinese spelling error correction. ||| zhao guo ||| yuan ni ||| keqiang wang ||| wei zhu ||| guotong xie ||| 
2017 ||| segmentation guided attention networks for visual question answering. ||| vasu sharma ||| ankita bishnu ||| labhesh patel ||| 
2018 ||| visual attention model for name tagging in multimodal social media. ||| di lu ||| leonardo neves ||| vitor carvalho ||| ning zhang ||| heng ji ||| 
2018 ||| document modeling with external attention for sentence extraction. ||| shashi narayan ||| ronald cardenas ||| nikos papasarantopoulos ||| shay b. cohen ||| mirella lapata ||| jiangsheng yu ||| yi chang ||| 
2020 ||| adaptive transformers for learning multimodal representations. ||| prajjwal bhargava ||| 
2021 ||| ernie-doc: a retrospective long-document modeling transformer. ||| siyu ding ||| junyuan shang ||| shuohuan wang ||| yu sun ||| hao tian ||| hua wu ||| haifeng wang ||| 
2019 ||| pay attention when you pay the bills. a multilingual corpus with dependency-based and semantic annotation of collocations. ||| marcos garc ||| a ||| marcos garc ||| a-salido ||| susana sotelo ||| estela mosqueira su ||| rez ||| margarita alonso ramos ||| 
2020 ||| dynamically adjusting transformer batch size by monitoring gradient direction change. ||| hongfei xu ||| josef van genabith ||| deyi xiong ||| qiuhui liu ||| 
2020 ||| hard-coded gaussian attention for neural machine translation. ||| weiqiu you ||| simeng sun ||| mohit iyyer ||| 
2021 ||| glancing transformer for non-autoregressive neural machine translation. ||| lihua qian ||| hao zhou ||| yu bao ||| mingxuan wang ||| lin qiu ||| weinan zhang ||| yong yu ||| lei li ||| 
2021 ||| it's all in the heads: using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. ||| alexey tikhonov ||| max ryabinin ||| 
2021 ||| contributions of transformer attention heads in multi- and cross-lingual tasks. ||| weicheng ma ||| kai zhang ||| renze lou ||| lili wang ||| soroush vosoughi ||| 
2019 ||| arnor: attention regularization based noise reduction for distant supervision relation classification. ||| wei jia ||| dai dai ||| xinyan xiao ||| hua wu ||| 
2017 ||| morphological inflection generation with hard monotonic attention. ||| roee aharoni ||| yoav goldberg ||| 
2017 ||| incorporating word reordering knowledge into attention-based neural machine translation. ||| jinchao zhang ||| mingxuan wang ||| qun liu ||| jie zhou ||| 
2019 ||| adversarial attention modeling for multi-dimensional emotion regression. ||| suyang zhu ||| shoushan li ||| guodong zhou ||| 
2020 ||| a transformer-based approach for source code summarization. ||| wasi uddin ahmad ||| saikat chakraborty ||| baishakhi ray ||| kai-wei chang ||| 
2019 ||| a multiscale visualization of attention in the transformer model. ||| jesse vig ||| 
2019 ||| fine-tuning pre-trained transformer language models to distantly supervised relation extraction. ||| christoph alt ||| marc h ||| bner ||| leonhard hennig ||| 
2020 ||| successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. ||| christopher brix ||| parnia bahar ||| hermann ney ||| 
2021 ||| bert busters: outlier dimensions that disrupt transformers. ||| olga kovaleva ||| saurabh kulshreshtha ||| anna rogers ||| anna rumshisky ||| 
2020 ||| pretrained transformers improve out-of-distribution robustness. ||| dan hendrycks ||| xiaoyuan liu ||| eric wallace ||| adam dziedzic ||| rishabh krishnan ||| dawn song ||| 
2020 ||| combining subword representations into word-level representations in the transformer architecture. ||| noe casas ||| marta r. costa-juss ||| jos |||  a. r. fonollosa ||| 
2019 ||| zero-shot cross-lingual abstractive sentence summarization through teaching generation and attention. ||| xiangyu duan ||| mingming yin ||| min zhang ||| boxing chen ||| weihua luo ||| 
2019 ||| multi-step reasoning via recurrent dual attention for visual dialog. ||| zhe gan ||| yu cheng ||| ahmed el kholy ||| linjie li ||| jingjing liu ||| jianfeng gao ||| 
2021 ||| incorporating global information in local attention for knowledge representation learning. ||| yu zhao ||| han zhou ||| ruobing xie ||| fuzhen zhuang ||| qing li ||| ji liu ||| 
2019 ||| hibert: document level pre-training of hierarchical bidirectional transformers for document summarization. ||| xingxing zhang ||| furu wei ||| ming zhou ||| 
2021 ||| video-guided machine translation with spatial hierarchical attention network. ||| weiqi gu ||| haiyue song ||| chenhui chu ||| sadao kurohashi ||| 
2019 ||| exact hard monotonic attention for character-level transduction. ||| shijie wu ||| ryan cotterell ||| 
2020 ||| differentiable window for dynamic local attention. ||| thanh-tung nguyen ||| xuan-phi nguyen ||| shafiq r. joty ||| xiaoli li ||| 
2020 ||| transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. ||| changmao li ||| jinho d. choi ||| 
2021 ||| focus attention: promoting faithfulness and diversity in summarization. ||| rahul aralikatte ||| shashi narayan ||| joshua maynez ||| sascha rothe ||| ryan t. mcdonald ||| 
2019 ||| look harder: a neural machine translation model with hard attention. ||| sathish reddy indurthi ||| insoo chung ||| sangha kim ||| 
2021 ||| minilmv2: multi-head self-attention relation distillation for compressing pretrained transformers. ||| wenhui wang ||| hangbo bao ||| shaohan huang ||| li dong ||| furu wei ||| 
2018 ||| multi-turn response selection for chatbots with deep attention matching network. ||| xiangyang zhou ||| lu li ||| daxiang dong ||| yi liu ||| ying chen ||| wayne xin zhao ||| dianhai yu ||| hua wu ||| 
2020 ||| gated convolutional bidirectional attention-based model for off-topic spoken response detection. ||| yefei zha ||| ruobing li ||| hui lin ||| 
2021 ||| do multilingual neural machine translation models contain language pair specific attention heads? ||| zae myung kim ||| laurent besacier ||| vassilina nikoulina ||| didier schwab ||| 
2020 ||| towards transparent and explainable attention models. ||| akash kumar mohankumar ||| preksha nema ||| sharan narasimhan ||| mitesh m. khapra ||| balaji vasan srinivasan ||| balaraman ravindran ||| 
2019 ||| self-attentional models for lattice inputs. ||| matthias sperber ||| graham neubig ||| ngoc-quan pham ||| alex waibel ||| 
2021 ||| assessing the syntactic capabilities of transformer-based multilingual language models. ||| laura p ||| rez-mayos ||| alba t ||| boas garc ||| a ||| simon mille ||| leo wanner ||| 
2020 ||| human attention maps for text classification: do humans and neural networks focus on the same words? ||| cansu sen ||| thomas hartvigsen ||| biao yin ||| xiangnan kong ||| elke a. rundensteiner ||| 
2020 ||| a contextual hierarchical attention network with adaptive objective for dialogue state tracking. ||| yong shan ||| zekang li ||| jinchao zhang ||| fandong meng ||| yang feng ||| cheng niu ||| jie zhou ||| 
2021 ||| best of both worlds: making high accuracy non-incremental transformer-based disfluency detection incremental. ||| morteza rohanian ||| julian hough ||| 
2021 ||| multi-scale progressive attention network for video question answering. ||| zhicheng guo ||| jiaxuan zhao ||| licheng jiao ||| xu liu ||| lingling li ||| 
2019 ||| comet: commonsense transformers for automatic knowledge graph construction. ||| antoine bosselut ||| hannah rashkin ||| maarten sap ||| chaitanya malaviya ||| asli celikyilmaz ||| yejin choi ||| 
2018 ||| sparse and constrained attention for neural machine translation. ||| chaitanya malaviya ||| pedro ferreira ||| andr |||  f. t. martins ||| 
2017 ||| a recurrent neural model with attention for the recognition of chinese implicit discourse relations. ||| samuel r ||| nnqvist ||| niko schenk ||| christian chiarcos ||| 
2019 ||| reading turn by turn: hierarchical attention architecture for spoken dialogue comprehension. ||| zhengyuan liu ||| nancy f. chen ||| 
2021 ||| do context-aware translation models pay the right attention? ||| kayo yin ||| patrick fernandes ||| danish pruthi ||| aditi chaudhary ||| andr |||  f. t. martins ||| graham neubig ||| 
2017 ||| joint ctc/attention decoding for end-to-end speech recognition. ||| takaaki hori ||| shinji watanabe ||| john r. hershey ||| 
2021 ||| structural guidance for transformer language models. ||| peng qian ||| tahira naseem ||| roger levy ||| ram ||| n fernandez astudillo ||| 
2021 ||| realformer: transformer likes residual attention. ||| ruining he ||| anirudh ravula ||| bhargav kanagal ||| joshua ainslie ||| 
2020 ||| self-attention guided copy mechanism for abstractive summarization. ||| song xu ||| haoran li ||| peng yuan ||| youzheng wu ||| xiaodong he ||| bowen zhou ||| 
2018 ||| attention focusing for neural machine translation by bridging source and target embeddings. ||| shaohui kuang ||| junhui li ||| ant ||| nio branco ||| weihua luo ||| deyi xiong ||| 
2021 ||| recursive tree-structured self-attention for answer sentence selection. ||| khalil mrini ||| emilia farcas ||| ndapa nakashole ||| 
2017 ||| a generative attentional neural network model for dialogue act classification. ||| quan hung tran ||| gholamreza haffari ||| ingrid zukerman ||| 
2019 ||| attention is (not) all you need for commonsense reasoning. ||| tassilo klein ||| moin nabi ||| 
2021 ||| cluster-former: clustering-based sparse transformer for question answering. ||| shuohang wang ||| luowei zhou ||| zhe gan ||| yen-chun chen ||| yuwei fang ||| siqi sun ||| yu cheng ||| jingjing liu ||| 
2019 ||| hierarchical transformers for multi-document summarization. ||| yang liu ||| mirella lapata ||| 
2021 ||| space efficient context encoding for non-task-oriented dialogue generation with graph attention transformer. ||| fabian galetzka ||| jewgeni rose ||| david schlangen ||| jens lehmann ||| 
2021 ||| attention flows are shapley value explanations. ||| kawin ethayarajh ||| dan jurafsky ||| 
2021 ||| is sparse attention more interpretable? ||| clara meister ||| stefan lazov ||| isabelle augenstein ||| ryan cotterell ||| 
2021 ||| enhancing content preservation in text style transfer using reverse attention and conditional layer normalization. ||| dongkyu lee ||| zhiliang tian ||| lanqing xue ||| nevin l. zhang ||| 
2017 ||| attention strategies for multi-source sequence-to-sequence learning. ||| jindrich libovick ||| jindrich helcl ||| 
2020 ||| character-level translation with self-attention. ||| yingqiang gao ||| nikola i. nikolov ||| yuhuang hu ||| richard h. r. hahnloser ||| 
2019 ||| multimodal transformer for unaligned multimodal language sequences. ||| yao-hung hubert tsai ||| shaojie bai ||| paul pu liang ||| j. zico kolter ||| louis-philippe morency ||| ruslan salakhutdinov ||| 
2021 ||| how knowledge graph and attention help? a qualitative analysis into bag-level relation extraction. ||| zikun hu ||| yixin cao ||| lifu huang ||| tat-seng chua ||| 
2021 ||| g-transformer for document-level machine translation. ||| guangsheng bao ||| yue zhang ||| zhiyang teng ||| boxing chen ||| weihua luo ||| 
2021 ||| mect: multi-metadata embedding based cross-transformer for chinese named entity recognition. ||| shuang wu ||| xiaoning song ||| zhen-hua feng ||| 
2021 ||| generalized supervised attention for text generation. ||| yixian liu ||| liwen zhang ||| xinyu zhang ||| yong jiang ||| yue zhang ||| kewei tu ||| 
2021 ||| improving bert with syntax-aware local attention. ||| zhongli li ||| qingyu zhou ||| chao li ||| ke xu ||| yunbo cao ||| 
2019 ||| opendialkg: explainable conversational reasoning with attention-based walks over knowledge graphs. ||| seungwhan moon ||| pararth shah ||| anuj kumar ||| rajen subba ||| 
2021 ||| graph relational topic model with higher-order graph attention auto-encoders. ||| qianqian xie ||| jimin huang ||| pan du ||| min peng ||| 
2018 ||| multimodal affective analysis using hierarchical attention strategy with word-level alignment. ||| yue gu ||| kangning yang ||| shiyu fu ||| shuhong chen ||| xinyu li ||| ivan marsic ||| 
2021 ||| tilgan: transformer-based implicit latent gan for diverse and coherent text generation. ||| shizhe diao ||| xinwei shen ||| kashun shum ||| yan song ||| tong zhang ||| 
2021 ||| realtrans: end-to-end simultaneous speech translation with convolutional weighted-shrinking transformer. ||| xingshan zeng ||| liangyou li ||| qun liu ||| 
2020 ||| integrating multimodal information in large pretrained transformers. ||| wasifur rahman ||| md. kamrul hasan ||| sangwu lee ||| amirali bagher zadeh ||| chengfeng mao ||| louis-philippe morency ||| mohammed e. hoque ||| 
2017 ||| attention-over-attention neural networks for reading comprehension. ||| yiming cui ||| zhipeng chen ||| si wei ||| shijin wang ||| ting liu ||| guoping hu ||| 
2019 ||| sentence-level evidence embedding for claim verification with hierarchical attention networks. ||| jing ma ||| wei gao ||| shafiq r. joty ||| kam-fai wong ||| 
2021 ||| self-attention networks can process bounded hierarchical languages. ||| shunyu yao ||| binghui peng ||| christos h. papadimitriou ||| karthik narasimhan ||| 
2019 ||| progressive self-supervised attention learning for aspect-level sentiment analysis. ||| jialong tang ||| ziyao lu ||| jinsong su ||| yubin ge ||| linfeng song ||| le sun ||| jiebo luo ||| 
2021 ||| more identifiable yet equally performant transformers for text classification. ||| rishabh bhardwaj ||| navonil majumder ||| soujanya poria ||| eduard h. hovy ||| 
2021 ||| topic-driven and knowledge-aware transformer for dialogue emotion detection. ||| lixing zhu ||| gabriele pergola ||| lin gui ||| deyu zhou ||| yulan he ||| 
2020 ||| sas: dialogue state tracking via slot attention and slot information sharing. ||| jiaying hu ||| yan yang ||| chencai chen ||| liang he ||| zhou yu ||| 
2018 ||| multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. ||| wei wang ||| chen wu ||| ming yan ||| 
2021 ||| parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. ||| rabeeh karimi mahabadi ||| sebastian ruder ||| mostafa dehghani ||| james henderson ||| 
2021 ||| multimodal incremental transformer with visual grounding for visual dialogue generation. ||| feilong chen ||| fandong meng ||| xiuyi chen ||| peng li ||| jie zhou ||| 
2020 ||| deformer: decomposing pre-trained transformers for faster question answering. ||| qingqing cao ||| harsh trivedi ||| aruna balasubramanian ||| niranjan balasubramanian ||| 
2021 ||| optimizing deeper transformers on small datasets. ||| peng xu ||| dhruv kumar ||| wei yang ||| wenjie zi ||| keyi tang ||| chenyang huang ||| jackie chi kit cheung ||| simon j. d. prince ||| yanshuai cao ||| 
2021 ||| a semantics-aware transformer model of relation linking for knowledge base question answering. ||| tahira naseem ||| srinivas ravishankar ||| nandana mihindukulasooriya ||| ibrahim abdelaziz ||| young-suk lee ||| pavan kapanipathi ||| salim roukos ||| alfio gliozzo ||| alexander g. gray ||| 
2021 ||| learning slice-aware representations with mixture of attentions. ||| cheng wang ||| sungjin lee ||| sunghyun park ||| han li ||| young-bum kim ||| ruhi sarikaya ||| 
2021 ||| on the distribution, sparsity, and inference-time quantization of attention values in transformers. ||| tianchu ji ||| shraddhan jain ||| michael ferdman ||| peter a. milder ||| h. andrew schwartz ||| niranjan balasubramanian ||| 
2017 ||| end-to-end non-factoid question answering with an interactive visualization of neural attention weights. ||| andreas r ||| ckl ||| iryna gurevych ||| 
2020 ||| document modeling with graph attention networks for multi-grained machine reading comprehension. ||| bo zheng ||| haoyang wen ||| yaobo liang ||| nan duan ||| wanxiang che ||| daxin jiang ||| ming zhou ||| ting liu ||| 
2020 ||| do transformers need deep long-range memory? ||| jack w. rae ||| ali razavi ||| 
2020 ||| gcan: graph-aware co-attention networks for explainable fake news detection on social media. ||| yi-ju lu ||| cheng-te li ||| 
2018 ||| cold-start aware user and product attention for sentiment classification. ||| reinald kim amplayo ||| jihyeok kim ||| sua sung ||| seung-won hwang ||| 
2021 ||| probing image-language transformers for verb understanding. ||| lisa anne hendricks ||| aida nematzadeh ||| 
2018 ||| ruminating reader: reasoning with gated multi-hop attention. ||| yichen gong ||| samuel r. bowman ||| 
2020 ||| multimodal and multiresolution speech recognition with transformers. ||| georgios paraskevopoulos ||| srinivas parthasarathy ||| aparna khare ||| shiva sundaram ||| 
2020 ||| multimodal transformer for multimodal machine translation. ||| shaowei yao ||| xiaojun wan ||| 
2020 ||| dependency graph enhanced dual-transformer structure for aspect-based sentiment classification. ||| hao tang ||| donghong ji ||| chenliang li ||| qiji zhou ||| 
2019 ||| attention-based conditioning methods for external knowledge integration. ||| katerina margatina ||| christos baziotis ||| alexandros potamianos ||| 
2021 ||| h-transformer-1d: fast one-dimensional hierarchical attention for sequences. ||| zhenhai zhu ||| radu soricut ||| 
2021 ||| transformer-based direct hidden markov model for machine translation. ||| weiyue wang ||| zijian yang ||| yingbo gao ||| hermann ney ||| 
2021 ||| improving the faithfulness of attention-based explanations with task-specific information for text classification. ||| george chrysostomou ||| nikolaos aletras ||| 
2019 ||| recosa: detecting the relevant contexts with self-attention for multi-turn dialogue generation. ||| hainan zhang ||| yanyan lan ||| liang pang ||| jiafeng guo ||| xueqi cheng ||| 
2021 ||| code summarization with structure-induced transformer. ||| hongqiu wu ||| hai zhao ||| min zhang ||| 
2019 ||| attention over heads: a multi-hop attention for neural machine translation. ||| shohei iida ||| ryuichiro kimura ||| hongyi cui ||| po-hsuan hung ||| takehito utsuro ||| masaaki nagata ||| 
2021 ||| convolutions and self-attention: re-interpreting relative positions in pre-trained language models. ||| tyler a. chang ||| yifan xu ||| weijian xu ||| zhuowen tu ||| 
2021 ||| contrastive attention for automatic chest x-ray report generation. ||| fenglin liu ||| changchang yin ||| xian wu ||| shen ge ||| ping zhang ||| xu sun ||| 
2018 ||| multi-input attention for unsupervised ocr correction. ||| rui dong ||| david smith ||| 
2021 ||| hi-transformer: hierarchical interactive transformer for efficient and effective long document modeling. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| yongfeng huang ||| 
2021 ||| hierarchical task learning from language instructions with unified transformers and self-monitoring. ||| yichi zhang ||| joyce chai ||| 
2017 ||| abstractive document summarization with a graph-based attentional neural model. ||| jiwei tan ||| xiaojun wan ||| jianguo xiao ||| 
2021 ||| how does attention affect the model? ||| cheng zhang ||| qiuchi li ||| lingyu hua ||| dawei song ||| 
2021 ||| how many layers and why? an analysis of the model depth in transformers. ||| antoine simoulin ||| beno ||| t crabb ||| 
2021 ||| r2d2: recursive transformer based on differentiable tree for interpretable hierarchical language modeling. ||| xiang hu ||| haitao mi ||| zujie wen ||| yafang wang ||| yi su ||| jing zheng ||| gerard de melo ||| 
2021 ||| n-best asr transformer: enhancing slu performance using multiple asr hypotheses. ||| karthik ganesan ||| pakhi bamdev ||| jaivarsan b ||| amresh venugopal ||| abhinav tushar ||| 
2019 ||| transformer-xl: attentive language models beyond a fixed-length context. ||| zihang dai ||| zhilin yang ||| yiming yang ||| jaime g. carbonell ||| quoc viet le ||| ruslan salakhutdinov ||| 
2021 ||| synchronous syntactic attention for transformer neural machine translation. ||| hiroyuki deguchi ||| akihiro tamura ||| takashi ninomiya ||| 
2021 ||| neural retrieval for question answering with cross-attention supervised data augmentation. ||| yinfei yang ||| ning jin ||| kuo lin ||| mandy guo ||| daniel cer ||| 
2017 ||| pay attention to the ending: strong neural baselines for the roc story cloze task. ||| zheng cai ||| lifu tu ||| kevin gimpel ||| 
2020 ||| self-attention with cross-lingual position representation. ||| liang ding ||| longyue wang ||| dacheng tao ||| 
2021 ||| long-span summarization via local attention and content selection. ||| potsawee manakul ||| mark j. f. gales ||| 
2019 ||| adaptive attention span in transformers. ||| sainbayar sukhbaatar ||| edouard grave ||| piotr bojanowski ||| armand joulin ||| 
2019 ||| semantically conditioned dialog response generation via hierarchical disentangled self-attention. ||| wenhu chen ||| jianshu chen ||| pengda qin ||| xifeng yan ||| william yang wang ||| 
2019 ||| lattice transformer for speech translation. ||| pei zhang ||| niyu ge ||| boxing chen ||| kai fan ||| 
2020 ||| mart: memory-augmented recurrent transformer for coherent video paragraph captioning. ||| jie lei ||| liwei wang ||| yelong shen ||| dong yu ||| tamara l. berg ||| mohit bansal ||| 
2017 ||| an end-to-end model for question answering over knowledge base with cross-attention combining global knowledge. ||| yanchao hao ||| yuanzhe zhang ||| kang liu ||| shizhu he ||| zhanyi liu ||| hua wu ||| jun zhao ||| 
2020 ||| flat: chinese ner using flat-lattice transformer. ||| xiaonan li ||| hang yan ||| xipeng qiu ||| xuanjing huang ||| 
2021 ||| can the transformer learn nested recursion with symbol masking? ||| jean-philippe bernardy ||| adam ek ||| vladislav maraev ||| 
2019 ||| attention guided graph convolutional networks for relation extraction. ||| zhijiang guo ||| yan zhang ||| wei lu ||| 
2018 ||| how much attention do you need? a granular analysis of neural machine translation architectures. ||| tobias domhan ||| 
2021 ||| reservoir transformers. ||| sheng shen ||| alexei baevski ||| ari s. morcos ||| kurt keutzer ||| michael auli ||| douwe kiela ||| 
2020 ||| improving multimodal named entity recognition via entity span detection with unified multimodal transformer. ||| jianfei yu ||| jing jiang ||| li yang ||| rui xia ||| 
2021 ||| dot: an efficient double transformer for nlp tasks with tables. ||| syrine krichene ||| thomas m ||| ller ||| julian eisenschlos ||| 
2021 ||| hit - a hierarchically fused deep attention network for robust code-mixed language representation. ||| ayan sengupta ||| sourabh kumar bhattacharjee ||| tanmoy chakraborty ||| md. shad akhtar ||| 
2020 ||| hierarchical modeling for user personality prediction: the role of message-level attention. ||| veronica e. lynn ||| niranjan balasubramanian ||| h. andrew schwartz ||| 
2021 ||| attention-based contextual language model adaptation for speech recognition. ||| richard diehl martinez ||| scott novotney ||| ivan bulyko ||| ariya rastrow ||| andreas stolcke ||| ankur gandhe ||| 
2019 ||| analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be pruned. ||| elena voita ||| david talbot ||| fedor moiseev ||| rico sennrich ||| ivan titov ||| 
2021 ||| an exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers. ||| tharindu ranasinghe ||| constantin orasan ||| ruslan mitkov ||| 
2020 ||| roles and utilization of attention heads in transformer-based neural language models. ||| jae-young jo ||| sung-hyon myaeng ||| 
2021 ||| highlight-transformer: leveraging key phrase aware attention to improve abstractive multi-document summarization. ||| shuaiqi liu ||| jiannong cao ||| ruosong yang ||| zhiyuan wen ||| 
2019 ||| topic sensitive attention on generic corpora corrects sense bias in pretrained embeddings. ||| vihari piratla ||| sunita sarawagi ||| soumen chakrabarti ||| 
2019 ||| the convex hull of finitely generable subsets and its predicate transformer. ||| mohammad-javad davari ||| abbas edalat ||| andr |||  lieutier ||| 
2020 ||| channel-wise attention and channel combination for knowledge distillation. ||| chan sik han ||| keon myung lee ||| 
2018 ||| abstractive summarization by neural attention model with document content memory. ||| yunseok choi ||| dahae kim ||| jee-hyong lee ||| 
2020 ||| semantic classification of emf-related literature using deep learning models with attention mechanism. ||| kwanghee won ||| youjeong jang ||| hyung-do choi ||| sung shin ||| 
2020 ||| spiking neural network transformer for deploying into a deep learning framework. ||| chan sik han ||| keon myung lee ||| 
2021 ||| ast-transformer: encoding abstract syntax trees efficiently for code summarization. ||| ze tang ||| chuanyi li ||| jidong ge ||| xiaoyu shen ||| zheling zhu ||| bin luo ||| 
2017 ||| fib: squeezing loop invariants by interpolation between forward/backward predicate transformers. ||| shang-wei lin ||| jun sun ||| hao xiao ||| yang liu ||| david san ||| n ||| henri hansen ||| 
2021 ||| thinking like a developer? comparing the attention of humans with neural models of code. ||| matteo paltenghi ||| michael pradel ||| 
2021 ||| javabert: training a transformer-based model for the java programming language. ||| nelson tavares de sousa ||| wilhelm hasselbring ||| 
2020 ||| detecting and explaining self-admitted technical debts with attention-based neural networks. ||| xin wang ||| jin liu ||| li li ||| xiao chen ||| xiao liu ||| hao wu ||| 
2019 ||| multi-modal attention network learning for semantic source code retrieval. ||| yao wan ||| jingdong shu ||| yulei sui ||| guandong xu ||| zhou zhao ||| jian wu ||| philip s. yu ||| 
2019 ||| autofocus: interpreting attention-based neural networks by code perturbation. ||| nghi d. q. bui ||| yijun yu ||| lingxiao jiang ||| 
2021 ||| to pay or not to pay attention: classifying and interpreting visual selective attention frequency features. ||| lora fanda ||| yashin dicente cid ||| pawel j. matusz ||| davide calvaresi ||| 
2021 ||| attention actor-critic algorithm for multi-agent constrained co-operative reinforcement learning. ||| p. parnika ||| raghuram bharadwaj diddigi ||| sai koti reddy danda ||| shalabh bhatnagar ||| 
2020 ||| an interpretable multimodal visual question answering system using attention-based weighted contextual features. ||| yu wang ||| yilin shen ||| hongxia jin ||| 
2020 ||| anchor attention for hybrid crowd forecasts aggregation. ||| yuzhong huang ||| andr ||| s abeliuk ||| fred morstatter ||| pavel atanasov ||| aram galstyan ||| 
2019 ||| active attention-modified policy shaping: socially interactive agents track. ||| taylor kessler faulkner ||| reymundo a. gutierrez ||| elaine schaertl short ||| guy hoffman ||| andrea lockerd thomaz ||| 
2019 ||| modelling the dynamic joint policy of teammates with attention multi-agent ddpg. ||| hangyu mao ||| zhengchao zhang ||| zhen xiao ||| zhibo gong ||| 
2021 ||| self-attention meta-learner for continual learning. ||| ghada sokar ||| decebal constantin mocanu ||| mykola pechenizkiy ||| 
2019 ||| attention-based deep reinforcement learning for multi-view environments. ||| elaheh barati ||| xuewen chen ||| zichun zhong ||| 
2021 ||| multi-agent graph-attention communication and teaming. ||| yaru niu ||| rohan r. paleja ||| matthew c. gombolay ||| 
2018 ||| attention-aware generative adversarial networks (ata-gans). ||| dimitris kastaniotis ||| ioanna ntinou ||| dimitrios tsourounis ||| george economou ||| spiros fotopoulos ||| 
2021 ||| retrieval-augmented transformer-xl for close-domain dialog generation. ||| giovanni bonetta ||| rossella cancelliere ||| ding liu ||| paul vozila ||| 
2020 ||| emptransfo: a multi-head transformer architecture for creating empathetic dialog systems. ||| rohola zandie ||| mohammad h. mahoor ||| 
2021 ||| modeling age of acquisition norms using transformer networks. ||| antonio laverghetta jr. ||| john licato ||| 
2019 ||| visual attention model for cross-sectional stock return prediction and end-to-end multimodal market representation learning. ||| ran zhao ||| yuntian deng ||| mark dredze ||| arun verma ||| david s. rosenberg ||| amanda stent ||| 
2018 ||| self-attention for synopsis-based multi-label movie genre classification. ||| jonatas wehrmann ||| mauricio a. lopes ||| rodrigo c. barros ||| 
2019 ||| different flavors of attention networks for argument mining. ||| johanna frau ||| milagro teruel ||| laura alonso alemany ||| serena villata ||| 
2020 ||| attend to the beginning: a study on bidirectional attention for extractive summarization. ||| ahmed magooda ||| cezary marcjan ||| 
2019 ||| opinion spam detection with attention-based neural networks. ||| zeinab sedighi ||| hossein ebrahimpour-komleh ||| ayoub bagheri ||| leila kosseim ||| 
2017 ||| toward extractive summarization of online forum discussions via hierarchical attention networks. ||| sansiri tarnpradab ||| fei liu ||| kien a. hua ||| 
2021 ||| towards improving open student answer assessment using pretrained transformers. ||| nisrine ait khayi ||| vasile rus ||| lasang jimba tamang ||| 
2020 ||| attention based transformer for student answers assessment. ||| nisrine ait khayi ||| vasile rus ||| 
2021 ||| show me what you're looking for visualizing abstracted transformer attention for enhancing their local interpretability on time series data. ||| leonid schwenke ||| martin atzmueller ||| 
2021 ||| uhf rfid chip impedance and sensitivity measurement using a transmission line transformer. ||| florian muralter ||| michael hani ||| hugo landaluce ||| asier perallos ||| erwin m. biebl ||| 
2021 ||| dualnet: locate then detect effective payload with deep attention network. ||| shiyi yang ||| peilun wu ||| hui guo ||| 
2020 ||| 3d visualization of temporal data: exploring visual attention and machine learning. ||| leonardo souza silva ||| renan vinicius aranha ||| matheus alberto de oliveira ribeiro ||| luiz ricardo nakamura ||| f ||| tima l. s. nunes ||| 
2020 ||| escape room virtual reality: a tool for diagnosis and treatment of attention deficit disorder. ||| andr ||| s felipe bejarano ||| juan diego correa ||| pablo a. figueroa ||| 
2017 ||| cognitive investigation on pilot attention during take-offs and landings using flight simulator. ||| zbigniew gomolka ||| boguslaw twarog ||| ewa zeslawska ||| 
2020 ||| pre-training polish transformer-based language models at scale. ||| slawomir dadas ||| michal perelkiewicz ||| rafal poswiata ||| 
2019 ||| text language identification using attention-based recurrent neural networks. ||| michal perelkiewicz ||| rafal poswiata ||| 
2021 ||| cagu-net: category attention guidance u-net for retinal blood vessel segmentation. ||| kexin sun ||| yuelan xin ||| yunliang qi ||| meng lou ||| kai ye ||| yinru ye ||| 
2021 ||| transfer learning and attention mechanism for breast cancer classification. ||| chenyang wang ||| feng xiao ||| wenjuan zhang ||| shujuan huang ||| wanyu zhang ||| pinrong zou ||| 
2020 ||| domain-specific chinese transformer-xl language model with part-of-speech information. ||| huaichang qu ||| haifeng zhao ||| xin wang ||| 
2019 ||| a self-attention-based approach for named entity recognition in cybersecurity. ||| tao li ||| yuanbo guo ||| ankang ju ||| 
2019 ||| relation extraction via attention-based cnns using token-level representations. ||| yan wang ||| xin xin ||| ping guo ||| 
2018 ||| siamese networks with discriminant correlation filters and channel attention. ||| si chen ||| dehui qiu ||| qirun huo ||| 
2019 ||| chinese ner by span-level self-attention. ||| xiaoyu dong ||| xin xin ||| ping guo ||| 
2021 ||| a sample-based training method for distantly supervised relation extraction with pre-trained transformers. ||| mehrdad nasser ||| mohamad bagher sajadi ||| behrouz minaei-bidgoli ||| 
2021 ||| bloomnet: a robust transformer based model for bloom's learning outcome classification. ||| abdul waheed ||| muskan goyal ||| nimisha mittal ||| deepak gupta ||| ashish khanna ||| moolchand sharma ||| 
2021 ||| arabic named entity recognition using transformer-based-crf model. ||| muhammad saleh al-qurishi ||| riad souissi ||| 
2020 ||| the driver's attention level. ||| ionut-adrian tarba ||| mihail gaianu ||| sebastian-aurelian stefaniga ||| 
2020 ||| automatic polyp segmentation using channel-spatial attention with deep supervision. ||| sahadev poudel ||| sang-woong lee ||| 
2020 ||| emotion and themes recognition in music with convolutional and recurrent attention-blocks. ||| maurice gerczuk ||| shahin amiriparian ||| sandra ottl ||| srividya tirunellai rajamani ||| bj ||| rn w. schuller ||| 
2019 ||| predicting media memorability using deep features with attention and recurrent network. ||| le-vu tran ||| vinh-loc huynh ||| minh-triet tran ||| 
2020 ||| transfer of knowledge: fine-tuning for polyp segmentation with attention. ||| rabindra khadka ||| 
2020 ||| efficient supervision net: polyp segmentation using efficientnet and attention unit. ||| sabari nathan ||| suganya ramamoorthy ||| 
2019 ||| music theme recognition using cnn and self-attention. ||| manoj sukhavasi ||| sainath adapa ||| 
2020 ||| a temporal-spatial attention model for medical image detection. ||| maxwell hwang ||| cai wu ||| kao-shing hwang ||| yong si xu ||| chien-hsing wu ||| 
2020 ||| predicting media memorability from a multimodal late fusion of self-attention and lstm models. ||| ricardo kleinlein ||| cristina luna jim ||| nez ||| zoraida callejas ||| fernando fern ||| ndez mart ||| nez ||| 
2020 ||| emotion and theme recognition in music using attention-based methods. ||| srividya tirunellai rajamani ||| kumar t. rajamani ||| bj ||| rn w. schuller ||| 
2020 ||| recognizing music mood and theme using convolutional neural networks and attention. ||| alish dipani ||| gaurav iyer ||| veeky baths ||| 
2020 ||| automatic polyp segmentation via parallel reverse attention network. ||| ge-peng ji ||| deng-ping fan ||| tao zhou ||| geng chen ||| huazhu fu ||| ling shao ||| 
2018 ||| double attention mechanism for sentence embedding. ||| miguel kakanakou ||| hongwei xie ||| yan qiang ||| 
2021 ||| named entity recognition of bert-bilstm-crf combined with self-attention. ||| lei xu ||| shuang li ||| yuchen wang ||| lizhen xu ||| 
2021 ||| graph-encoder and multi-decoders solution framework with multi-attention. ||| hui cai ||| tiancheng zhang ||| xianghui sun ||| minghe yu ||| ge yu ||| 
2020 ||| collaborative filtering: graph neural network with attention. ||| yanli guo ||| zhongmin yan ||| 
2021 ||| entity alignment of knowledge graph by joint graph attention and translation representation. ||| shixian jiang ||| tiezheng nie ||| derong shen ||| yue kou ||| ge yu ||| 
2017 ||| the influence of the attention decay in an information spreading model. ||| zili xiong ||| zaobin gan ||| haifeng xiang ||| hongwei lu ||| 
2019 ||| a sequence-to-sequence text summarization model with topic based attention mechanism. ||| heng-xi pan ||| hai liu ||| yong tang ||| 
2020 ||| cross-language generative automatic summarization based on attention mechanism. ||| feiyang yang ||| rongyi cui ||| zhiwei yi ||| yahui zhao ||| 
2020 ||| explainable enterprise rating using attention based convolutional neural network. ||| weiyu guo ||| bin cao ||| zhenxing li ||| 
2018 ||| modularized and attention-based recurrent convolutional neural network for automatic academic paper aspect scoring. ||| feng qiao ||| lizhen xu ||| xiaowei han ||| 
2021 ||| the code generation method based on gated attention and interaction-lstm. ||| yuxuan wang ||| junhua wu ||| 
2021 ||| robust graph collaborative filtering algorithm based on hierarchical attention. ||| ping feng ||| yang qian ||| xiaohan liu ||| guoliang li ||| jian zhao ||| 
2021 ||| egat: edge-featured graph attention network. ||| ziming wang ||| jun chen ||| haopeng chen ||| 
2019 ||| targeted sentiment classification with attentional encoder network. ||| youwei song ||| jiahai wang ||| tao jiang ||| zhiyue liu ||| yanghui rao ||| 
2018 ||| a multi-level attention model for text matching. ||| qiang sun ||| yue wu ||| 
2019 ||| attention and edge memory convolution for bioactivity prediction. ||| michael withnall ||| edvard lindel ||| f ||| ola engkvist ||| hongming chen ||| 
2020 ||| attention based mechanism for load time series forecasting: an-lstm. ||| jatin bedi ||| 
2021 ||| interpretable visual understanding with cognitive attention network. ||| xuejiao tang ||| wenbin zhang ||| yi yu ||| kea turner ||| tyler derr ||| mengyu wang ||| eirini ntoutsi ||| 
2019 ||| a recurrent attention network for judgment prediction. ||| ze yang ||| pengfei wang ||| lei zhang ||| linjun shou ||| wenwen xu ||| 
2019 ||| signed graph attention networks. ||| junjie huang ||| huawei shen ||| liang hou ||| xueqi cheng ||| 
2019 ||| a transformer model for retrosynthesis. ||| pavel karpov ||| guillaume godin ||| igor v. tetko ||| 
2019 ||| spatial attention network for few-shot learning. ||| xianhao he ||| peng qiao ||| yong dou ||| xin niu ||| 
2021 ||| gattanet: global attention agreement for convolutional neural networks. ||| rufin vanrullen ||| andrea alamia ||| 
2021 ||| attention-based 3d neural architectures for predicting cracks in designs. ||| naresh s. iyer ||| sathyanarayanan raghavan ||| yiming zhang ||| yang jiao ||| dean robinson ||| 
2021 ||| daema: denoising autoencoder with mask attention. ||| simon tihon ||| muhammad usama javaid ||| damien fourure ||| nicolas posocco ||| thomas peel ||| 
2021 ||| tstnet: a sequence to sequence transformer network for spatial-temporal traffic prediction. ||| xiaozhuang song ||| ying wu ||| chenhan zhang ||| 
2019 ||| hierarchical attentional hybrid neural networks for document classification. ||| jader abreu ||| luis fred ||| david mac ||| do ||| cleber zanchettin ||| 
2020 ||| cabin: a novel cooperative attention based location prediction network using internal-external trajectory dependencies. ||| tangwen qian ||| fei wang ||| yongjun xu ||| yu jiang ||| tao sun ||| yong yu ||| 
2017 ||| instance-adaptive attention mechanism for relation classification. ||| yao lu ||| chunyun zhang ||| weiran xu ||| 
2021 ||| attention-based bi-lstm for anomaly detection on time-series data. ||| sanket mishra ||| varad kshirsagar ||| rohit dwivedula ||| chittaranjan hota ||| 
2021 ||| stgatp: a spatio-temporal graph attention network for long-term traffic prediction. ||| mengting zhu ||| xianqiang zhu ||| cheng zhu ||| 
2019 ||| an attention-based id-cnns-crf model for named entity recognition on clinical electronic medical records. ||| ming gao ||| qifeng xiao ||| shaochun wu ||| kun deng ||| 
2019 ||| surrounding-based attention networks for aspect-level sentiment classification. ||| yuheng sun ||| xianchen wang ||| hongtao liu ||| wenjun wang ||| pengfei jiao ||| 
2021 ||| tinet: multi-dimensional traffic data imputation via transformer network. ||| xiaozhuang song ||| yongchao ye ||| james j. q. yu ||| 
2019 ||| lstm with uniqueness attention for human activity recognition. ||| zengwei zheng ||| lifei shi ||| chi wang ||| lin sun ||| gang pan ||| 
2019 ||| improving deep image clustering with spatial transformer layers. ||| thiago v. m. souza ||| cleber zanchettin ||| 
2021 ||| spatial-temporal traffic data imputation via graph attention convolutional network. ||| yongchao ye ||| shiyao zhang ||| james j. q. yu ||| 
2020 ||| more attentional local descriptors for few-shot learning. ||| hui li ||| liu yang ||| fei gao ||| 
2021 ||| entity-aware biaffine attention for constituent parsing. ||| xinyi bai ||| nan yin ||| xiang zhang ||| xin wang ||| zhigang luo ||| 
2019 ||| referring expression comprehension via co-attention and visual context. ||| youming gao ||| yi ji ||| ting xu ||| yunlong xu ||| chunping liu ||| 
2021 ||| knowledge graph enhanced transformer for generative question answering tasks. ||| chaojie liang ||| jingying yang ||| xianghua fu ||| 
2018 ||| hierarchical attention networks for user profile inference in social media systems. ||| zhezhou kang ||| xiaoxue li ||| yanan cao ||| yanmin shang ||| yanbing liu ||| li guo ||| 
2019 ||| a reinforcement learning approach for sequential spatial transformer networks. ||| fatemeh azimi ||| federico raue ||| j ||| rn hees ||| andreas dengel ||| 
2021 ||| attention-based multi-view feature fusion for cross-domain recommendation. ||| feifei dai ||| xiaoyan gu ||| zhuo wang ||| bo li ||| mingda qian ||| weiping wang ||| 
2019 ||| towards attention based vulnerability discovery using source code representation. ||| junae kim ||| david hubczenko ||| paul montague ||| 
2019 ||| revising attention with position for aspect-level sentiment classification. ||| dong wang ||| tingwen liu ||| bin wang ||| 
2020 ||| multi-scale cross-modal spatial attention fusion for multi-label image recognition. ||| junbing li ||| changqing zhang ||| xueman wang ||| ling du ||| 
2017 ||| attention focused spatial pyramid pooling for boxless action recognition in still images. ||| weijiang feng ||| xiang zhang ||| xuhui huang ||| zhigang luo ||| 
2017 ||| attention aware semi-supervised framework for sentiment analysis. ||| jingshuang liu ||| wenge rong ||| chuan tian ||| min gao ||| zhang xiong ||| 
2019 ||| a label-specific attention-based network with regularized loss for multi-label classification. ||| xiangyang luo ||| xiangying ran ||| wei sun ||| yunlai xu ||| chongjun wang ||| 
2019 ||| heterogeneous information network embedding with meta-path based graph attention networks. ||| meng cao ||| xiying ma ||| ming xu ||| chongjun wang ||| 
2019 ||| the same size dilated attention network for keypoint detection. ||| yuan chang ||| zixuan huang ||| qiwei shen ||| 
2019 ||| scaffolding haptic attention with controller gating. ||| alexandra moringen ||| sascha fleer ||| helge j. ritter ||| 
2019 ||| attention-based improved blstm-cnn for relation classification. ||| qifeng xiao ||| ming gao ||| shaochun wu ||| xiaoqi sun ||| 
2021 ||| an attention module for convolutional neural networks. ||| baozhou zhu ||| h. peter hofstee ||| jinho lee ||| zaid al-ars ||| 
2019 ||| attentional residual dense factorized network for real-time semantic segmentation. ||| lulu yang ||| long lan ||| xiang zhang ||| xuhui huang ||| zhigang luo ||| 
2020 ||| adversarial defense via attention-based randomized smoothing. ||| xiao xu ||| shiyu feng ||| zheng wang ||| lizhe xie ||| yining hu ||| 
2018 ||| attention-based rnn model for joint extraction of intent and word slot based on a tagging strategy. ||| dongjie zhang ||| zheng fang ||| yanan cao ||| yanbing liu ||| xiaojun chen ||| jianlong tan ||| 
2020 ||| face anti-spoofing with a noise-attention network using color-channel difference images. ||| yuanyuan ren ||| yongjian hu ||| beibei liu ||| yixiang xie ||| yufei wang ||| 
2020 ||| bilinear fusion of commonsense knowledge with attention-based nli models. ||| amit gajbhiye ||| thomas winterbottom ||| noura al moubayed ||| steven bradley ||| 
2018 ||| attention enhanced chinese word embeddings. ||| xingzhang ren ||| leilei zhang ||| wei ye ||| hang hua ||| shikun zhang ||| 
2021 ||| covit-gan: vision transformer forcovid-19 detection in ct scan imageswith self-attention gan fordataaugmentation. ||| ara abigail e. ambita ||| eujene nikka v. boquio ||| prospero c. naval ||| 
2019 ||| capsule networks for attention under occlusion. ||| antonio jose rodr ||| guez-s ||| nchez ||| tobias dick ||| 
2019 ||| capturing user and product information for sentiment classification via hierarchical separated attention and neural collaborative filtering. ||| minghui yan ||| changjian wang ||| ying sha ||| 
2018 ||| ynamic multi-agent cooperation via attentional communication model. ||| xue han ||| hongping yan ||| junge zhang ||| lingfeng wang ||| 
2019 ||| hybrid attention driven text-to-image synthesis via generative adversarial networks. ||| qingrong cheng ||| xiaodong gu ||| 
2021 ||| dvamn: dual visual attention matching network for zero-shot action recognition. ||| cheng qi ||| zhiyong feng ||| meng xing ||| yong su ||| 
2020 ||| cross-domain sentiment classification using topic attention and dual-task adversarial training. ||| kwun ping lai ||| jackie chun-sing ho ||| wai lam ||| 
2019 ||| self-attention stargan for multi-domain image-to-image translation. ||| ziliang he ||| zhenguo yang ||| xudong mao ||| jianming lv ||| qing li ||| wenyin liu ||| 
2019 ||| collaborative attention network with word and n-gram sequences modeling for sentiment classification. ||| junwei bao ||| liang zhang ||| bo han ||| 
2018 ||| design of 65-nm cmos transformer-based impedance matching for lte power amplifier applications. ||| giap luong ||| eric kerherve ||| jean-marie pham ||| pierre medrel ||| 
2018 ||| compact transformerless k-band pa with more than 33% pae and 14.8 dbm output power in 65 nm bulk cmos. ||| soenke vehring ||| georg b ||| ck ||| 
2018 ||| on 60 ghz solid-state transformers designed in 65 nm cmos technology. ||| igor m. filanovsky ||| 
2020 ||| divide-by-2 injection-locked frequency divider using 3-path transformer-coupled resonator. ||| wen-cheng lai ||| sheng-lyang jang ||| yu-wen huang ||| miin-horng juang ||| 
2019 ||| multi-level stereo attention model for center channel extraction. ||| wootaek lim ||| seungkwon beack ||| taejin lee ||| 
2019 ||| hierarchically channel-wise attention model for clean and polluted water images classification. ||| yirui wu ||| yao xiao ||| jun feng ||| 
2019 ||| coattention-based recurrent neural networks for sentiment analysis of chinese texts. ||| lifeng liu ||| lizong zhang ||| fengming zhang ||| jianguo qian ||| xinxin zhang ||| peidong chen ||| bo li ||| 
2021 ||| chinese fine-grained sentiment classification based on pre-trained language model and attention mechanism. ||| faguo zhou ||| jing zhang ||| yanan song ||| 
2017 ||| attention-aware path-based relation extraction for medical knowledge graph. ||| desi wen ||| yong liu ||| kaiqi yuan ||| shangchun si ||| ying shen ||| 
2021 ||| real-time prediction of ocean observation data based on transformer model. ||| wenqing chang ||| xiang li ||| huomin dong ||| chunxiao wang ||| zhigang zhao ||| yinglong wang ||| 
2021 ||| intelligent detection of marine organisms with deep learning based on attention mechanism. ||| chuantao li ||| jidong huo ||| zhigang zhao ||| lu wu ||| 
2021 ||| d-rex: static detection of relevant runtime exceptions with location aware transformer. ||| farima farmahinifarahani ||| yadong lu ||| vaibhav saini ||| pierre baldi ||| cristina v. lopes ||| 
2019 ||| power system upgrade planning with on-load tap-changing transformers, switchable topology and operating policies. ||| sandro merkli ||| roy s. smith ||| 
2021 ||| indonesia's fake news detection using transformer network. ||| jibran fawaid ||| aisyah awalina ||| rifky yunus krisnabayu ||| novanto yudistira ||| 
2021 ||| a study of english-indonesian neural machine translation with attention (seq2seq, convseq2seq, rnn, and mha): a comparative study of nmt on english-indonesian. ||| diyah puspitaningrum ||| 
2021 ||| research on copper foil winding of transformer surface defect detection based on machine vision. ||| guihua yang ||| zhicheng dai ||| 
2019 ||| design of electronic current transformer power supply system. ||| wenwen xiao ||| xuan sun ||| henghui xu ||| 
2021 ||| research on transformer fault diagnosis based on sparrow algorithm optimization probabilistic neural network. ||| mingxia chen ||| hanyu shi ||| junjie wu ||| 
2021 ||| abstracting local transformer attention for enhancing interpretability on time series data. ||| leonid schwenke ||| martin atzmueller ||| 
2017 ||| auto-transformer-based power amplifier with totem-pole driver. ||| elena sobotta ||| robert wolf ||| frank ellinger ||| 
2021 ||| robustness of smart transformer based-on sliding mode controller under grid perturbations. ||| hiba helali ||| adel khedher ||| 
2018 ||| optimum design approach of high frequency transformer: including the effects of eddy currents. ||| sobhi barqi ||| 
2021 ||| review on solid state transformer based on microgrids architectures. ||| marwa chalbi ||| ghada boukettaya ||| 
2021 ||| backstepping controller design for the medium and low voltage stages of smart transformer. ||| hiba helali ||| adel khedher ||| 
2021 ||| study of spectral response of transformer oil under low electrical discharge and thermal stress. ||| s. boudraa ||| l. mokhnache ||| issouf fofana ||| f. benabed ||| 
2018 ||| dc transformer for optimal power flow with interior point algorithm. ||| karim sebaa ||| omar kahouli ||| 
2018 ||| improved performance of piezoelectric transformer mode thickness by nonlinear methods. ||| aida cherif ||| djamila zehar ||| khalissa behih ||| mohamed rguiti ||| 
2020 ||| validation of emotions as a measure of selective attention in children with autism spectrum disorder. ||| bilikis banire ||| dena al-thani ||| marwa k. qaraqe ||| 
2020 ||| critical thinking development in collaborative learning: case study of transformer topic through lesson study at junior high school. ||| ratnasari ||| asep k. supriatna ||| sumar hendayana ||| 
2020 ||| faster r-cnn with attention feature map for robust object detection. ||| youlkyeong lee ||| kang-hyun jo ||| 
2021 ||| saliency prediction with relation-aware global attention module. ||| ge cao ||| kang-hyun jo ||| 
2021 ||| efficient spatial-attention module for human pose estimation. ||| tien-dat tran ||| xuan-thuy vo ||| duy-linh nguyen ||| kang-hyun jo ||| 
2019 ||| analysing coreference in transformer outputs. ||| ekaterina lapshinova-koltunski ||| cristina espa ||| a-bonet ||| josef van genabith ||| 
2021 ||| attention edgeconv for 3d point cloud classification. ||| yen-po lin ||| yang-ming yeh ||| yu-chen chou ||| yi-chang lu ||| 
2021 ||| semi-supervised sound event detection using self-attention and multiple techniques of consistency training. ||| yih-wen wang ||| chia-ping chen ||| chung-li lu ||| bo-cheng chan ||| 
2019 ||| phonetic-attention scoring for deep speaker features in speaker verification. ||| lantian li ||| zhiyuan tang ||| ying shi ||| dong wang ||| 
2017 ||| music thumbnailing via neural attention modeling of music emotion. ||| yu-siang huang ||| szu-yu chou ||| yi-hsuan yang ||| 
2021 ||| dual-path transformer for machine condition monitoring. ||| jisheng bai ||| mou wang ||| jianfeng chen ||| 
2020 ||| speaker verification system based on deformable cnn and time-frequency attention. ||| yiming zhang ||| hong yu ||| zhanyu ma ||| 
2021 ||| attention-based multi-channel speaker verification with ad-hoc microphone arrays. ||| chengdong liang ||| junqi chen ||| shanzheng guan ||| xiao-lei zhang ||| 
2021 ||| cyclegan-based non-parallel speech enhancement with an adaptive attention-in-attention mechanism. ||| guochen yu ||| yutian wang ||| chengshi zheng ||| hui wang ||| qin zhang ||| 
2021 ||| ensemble of one model: creating model variations for transformer with layer permutation. ||| andrew liaw ||| jia-hao hsu ||| chung-hsien wu ||| 
2020 ||| module comparison of transformer-tts for speaker adaptation based on fine-tuning. ||| katsuki inoue ||| sunao hara ||| masanobu abe ||| 
2020 ||| class attention network for semantic segmentation of remote sensing images. ||| zhibo rao ||| mingyi he ||| yuchao dai ||| 
2019 ||| bidirectional temporal convolution with self-attention network for ctc-based acoustic modeling. ||| jian sun ||| wu guo ||| bin gu ||| yao liu ||| 
2017 ||| visual attention guided eye movements for 360 degree images. ||| feng li ||| huihui bai ||| yao zhao ||| 
2019 ||| classification of causes of speech recognition errors using attention-based bidirectional long short-term memory and modulation spectrum. ||| jennifer santoso ||| takeshi yamada ||| shoji makino ||| 
2021 ||| non-parallel voice conversion with generative attentional networks. ||| tse wei chiu ||| you sheng guo ||| pao-chi chang ||| 
2019 ||| disfluency detection based on speech-aware token-by-token sequence labeling with blstm-crfs and attention mechanisms. ||| tomohiro tanaka ||| ryo masumura ||| takafumi moriya ||| takanobu oba ||| yushi aono ||| 
2021 ||| effect of visual attention and driving experiences on the event-related potential p300 in the perception of traffic scenes. ||| kota yamamoto ||| sou nobukawa ||| nobuhiko wagatsuma ||| keiichiro inagaki ||| 
2019 ||| multi-lingual transformer training for khmer automatic speech recognition. ||| soky kak ||| sheng li ||| tatsuya kawahara ||| sopheap seng ||| 
2019 ||| towards generation of visual attention map for source code. ||| takeshi d. itoh ||| takatomi kubo ||| kiyoka ikeda ||| yuki maruno ||| yoshiharu ikutani ||| hideaki hata ||| kenichi matsumoto ||| kazushi ikeda ||| 
2020 ||| context-adaptive gaussian attention for text-independent speaker verification. ||| junyi peng ||| rongzhi gu ||| haoran zhang ||| yuexian zou ||| 
2021 ||| improvement of spatial ambiguity in multi-channel speech separation using channel attention. ||| qian-bei hong ||| chung-hsien wu ||| thanh binh nguyen ||| hsin-min wang ||| 
2018 ||| online speaker adaptation for lvcsr based on attention mechanism. ||| jia pan ||| diyuan liu ||| genshun wan ||| jun du ||| qingfeng liu ||| zhongfu ye ||| 
2021 ||| positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition. ||| jiyao liu ||| yanxi zhao ||| hao wu ||| dongmei jiang ||| 
2020 ||| adversarial training using inter/intra-attention architecture for speech enhancement network. ||| yosuke sugiura ||| tetsuya shimamura ||| 
2018 ||| user's intention understanding in question-answering system using attention-based lstm. ||| yuki matsuyoshi ||| tetsuya takiguchi ||| yasuo ariki ||| 
2019 ||| image compression with deeper learned transformer. ||| licheng xiao ||| hairong wang ||| nam ling ||| 
2021 ||| an attention based expert inspection system for smart scalp. ||| sin-ye jhong ||| po-yen yang ||| chih-hsien hsia ||| 
2020 ||| speaker-invariant psychological stress detection using attention-based network. ||| hyeon-kyeong shin ||| hyewon han ||| kyungguen byun ||| hong-goo kang ||| 
2019 ||| learning contextual representation with convolution bank and multi-head self-attention for speech emphasis detection. ||| liangqi liu ||| zhiyong wu ||| runnan li ||| jia jia ||| helen meng ||| 
2020 ||| attentive fusion enhanced audio-visual encoding for transformer based robust speech recognition. ||| liangfa wei ||| jie zhang ||| junfeng hou ||| lirong dai ||| 
2021 ||| comparison of low complexity self-attention mechanisms for acoustic event detection. ||| tatsuya komatsu ||| robin scheibler ||| 
2021 ||| end-to-end speaker age and height estimation using attention mechanism and triplet loss. ||| manav kaushik ||| van tung pham ||| tran the anh ||| eng siong chng ||| 
2019 ||| dynamic attention loss for small-sample image classification. ||| jie cao ||| yinping qiu ||| dongliang chang ||| xiaoxu li ||| zhanyu ma ||| 
2020 ||| rate-distortion optimization for 360-degree image considering visual attention. ||| cheng-yu yang ||| jui-chiu chiang ||| wen-nung lie ||| 
2021 ||| image captioning based on an improved transformer with iou position encoding. ||| yazhou li ||| yihui shi ||| yun liu ||| ruifan li ||| zhanyu ma ||| 
2021 ||| ca-vc: a novel zero-shot voice conversion method with channel attention. ||| ruitong xiao ||| xiaofen xing ||| jichen yang ||| xiangmin xu ||| 
2021 ||| efficient conformer-based speech recognition with linear attention. ||| shengqiang li ||| menglong xu ||| xiao-lei zhang ||| 
2019 ||| end-to-end tibetan ando dialect speech recognition based on hybrid ctc/attention architecture. ||| jingwen sun ||| gang zhou ||| hongwu yang ||| man wang ||| 
2018 ||| attention based fully convolutional network for speech emotion recognition. ||| yuanyuan zhang ||| jun du ||| zi-rui wang ||| jianshu zhang ||| yanhui tu ||| 
2021 ||| speech enhancement network with unsupervised attention using invariant information clustering. ||| yosuke sugiura ||| shunta nagamori ||| tetsuya shimamura ||| 
2019 ||| voice conversion by dual-domain bidirectional long short-term memory networks with temporal attention. ||| xiaokong miao ||| meng sun ||| xiongwei zhang ||| 
2021 ||| an empirical study on transformer-based end-to-end speech recognition with novel decoder masking. ||| shi-yan weng ||| hsuan-sheng chiu ||| berlin chen ||| 
2019 ||| dynamic-attention based encoder-decoder model for speaker extraction with anchor speech. ||| hao li ||| xueliang zhang ||| guanglai gao ||| 
2019 ||| convolutional attention model for retinal edema segmentation. ||| phuong le thi ||| tuan pham ||| jia-ching wang ||| 
2021 ||| a study on virtual reality sickness and visual attention. ||| jeonghaeng lee ||| woojae kim ||| jinwoo kim ||| sanghoon lee ||| 
2020 ||| temporal attention feature encoding for video captioning. ||| na-young kim ||| seong jong ha ||| je-won kang ||| 
2020 ||| supportive and self attentions for image caption. ||| jen-tzung chien ||| ting-an lin ||| 
2021 ||| a self-attention-based ensemble convolution neural network approach for sleep stage classification with merged spectrogram. ||| chih-en kuo ||| po-yu liao ||| yu-syuan lin ||| 
2019 ||| mixed attention mechanism for small-sample fine-grained image classification. ||| xiaoxu li ||| jijie wu ||| dongliang chang ||| weifeng huang ||| zhanyu ma ||| jie cao ||| 
2020 ||| self-attention for multi-channel speech separation in noisy and reverberant environments. ||| conggui liu ||| yoshinao sato ||| 
2021 ||| real-time edge attention-based learning for low-light one-stage object detection. ||| yen-yu pu ||| ching-te chiu ||| shu-yun wu ||| 
2021 ||| rethinking singing voice separation with spectral- temporal transformer. ||| shuai yu ||| chenxing li ||| feng deng ||| xiaorui wang ||| 
2020 ||| an improved method for instantaneous frequency estimation using a finite order hilbert transformer. ||| keisuke takao ||| takahiro natori ||| toma miyata ||| naoyuki aikawa ||| 
2019 ||| prosodic structure prediction using deep self-attention neural network. ||| yao du ||| zhiyong wu ||| shiyin kang ||| dan su ||| dong yu ||| helen meng ||| 
2021 ||| an investigation of enhancing ctc model for triggered attention-based streaming asr. ||| huaibo zhao ||| yosuke higuchi ||| tetsuji ogawa ||| tetsunori kobayashi ||| 
2020 ||| efficient diverse response generation in attention-based neural conversational model with maximum mutual information. ||| yuki kisida ||| tsuneo kato ||| yanan wang ||| jianming wu ||| gen hattori ||| 
2021 ||| self-supervised visual transformers for breast cancer diagnosis. ||| nurbek saidnassim ||| beibit abdikenov ||| rauan kelesbekov ||| muhammad tahir akhtar ||| prashant kumar jamwal ||| 
2018 ||| mmann: multimodal multilevel attention neural network for horror clip detection. ||| xingliang cheng ||| xiaotong zhang ||| mingxing xu ||| thomas fang zheng ||| 
2021 ||| separable temporal convolution plus temporally pooled attention for lightweight high-performance keyword spotting. ||| shenghua hu ||| jing wang ||| yujun wang ||| wenjing yang ||| 
2018 ||| hierarchical attention networks for different types of documents with smaller size of datasets. ||| hon-sang cheong ||| wun-she yap ||| yee-kai tee ||| wai-kong lee ||| 
2019 ||| attention neural networks for pan-tilt-zoom control with active hand-off. ||| tyler highlander ||| john gallagher ||| 
2017 ||| expanding the scope of learning analytics data: preliminary findings on attention and self-regulation using wearable technology. ||| catherine a. spann ||| james schaeffer ||| george siemens ||| 
2021 ||| are you with me? measurement of learners' video-watching attention with eye tracking. ||| namrata srivastava ||| sadia nawaz ||| joshua newn ||| jason m. lodge ||| eduardo velloso ||| sarah m. erfani ||| dragan gasevic ||| james bailey ||| 
2020 ||| visualization of focal cues for visuomotor coordination by gradient-based methods: a recurrent neural network shifts the attention depending on task requirements. ||| hiroshi ito ||| kenjiro yamamoto ||| hiroki mori ||| shuki goto ||| tetsuya ogata ||| 
2018 ||| more attention and less repetitive and stereotyped behaviors using a robot with children with autism. ||| andreia p. costa ||| louise charpiot ||| francisco j. rodr ||| guez lera ||| pouyan ziafati ||| aida nazarikhorram ||| leendert w. n. van der torre ||| georges steffgen ||| 
2019 ||| surprise! predicting infant visual attention in a socially assistive robot contingent learning paradigm. ||| lauren klein ||| laurent itti ||| beth a. smith ||| marcelo r. rosales ||| stefanos nikolaidis ||| maja j. mataric ||| 
2021 ||| designing interface aids to assist collaborative robot operators in attention management. ||| curt henrichs ||| fangyun zhao ||| bilge mutlu ||| 
2017 ||| sociable driving agents to maintain driver's attention in autonomous driving. ||| nihan karatas ||| soshi yoshikawa ||| shintaro tamura ||| sho otaki ||| ryuji funayama ||| michio okada ||| 
2019 ||| verbal explanations for deep reinforcement learning neural networks with attention on extracted features. ||| xinzhi wang ||| shengcheng yuan ||| hui zhang ||| michael lewis ||| katia p. sycara ||| 
2021 ||| attention deep learning based model for predicting the 3d human body pose using the robot human handover phases. ||| javier laplaza ||| albert pumarola ||| francesc moreno-noguer ||| alberto sanfeliu ||| 
2019 ||| smak-net: self-supervised multi-level spatial attention network for knowledge representation towards imitation learning. ||| kartik ramachandruni ||| madhu babu vankadari ||| anima majumder ||| samrat dutta ||| swagat kumar ||| 
2018 ||| predicting response to joint attention performance in human-human interaction based on human-robot interaction for young children with autism spectrum disorder. ||| guangtao nie ||| zhi zheng ||| jazette johnson ||| amy r. swanson ||| amy s. weitlauf ||| zachary e. warren ||| nilanjan sarkar ||| 
2021 ||| attention distribution graph: visualizing student's attention transition in error-finding tasks. ||| ying zhou ||| wei liu ||| huitong liu ||| jing xu ||| wenqing cheng ||| 
2021 ||| medical assistant diagnosis method based on graph neural network and attention mechanism. ||| wanchun yang ||| shurui zhang ||| bozheng zhang ||| 
2017 ||| eeg analysis of brain activity in attention deficit hyperactivity disorder during an attention task. ||| allwin alex ||| stefania coelli ||| anna maria bianchi ||| l. ponzini ||| e. buzzi ||| maria paola canevini ||| 
2021 ||| semantic-guided high-order region attention embedding for zero-shot learning. ||| rui zhang ||| xiangyu xu ||| qi zhu ||| 
2017 ||| fast recognition of human climbing fences in transformer substations. ||| tianzheng wang ||| kangning wang ||| jie li ||| hua yu ||| wang shuai ||| jiang bian ||| xiaoguang zhao ||| 
2018 ||| chinese metaphor sentiment analysis based on attention-based lstm. ||| ying peng ||| chang su ||| yijiang chen ||| 
2020 ||| robust finger vein recognition based on deep cnn with spatial attention and bias field correction. ||| zhe huang ||| chengan guo ||| 
2018 ||| sentence representation and classification using attention and additional language information. ||| hongli deng ||| lei zhang ||| lituan wang ||| 
2019 ||| transformer fault on-line diagnosis system. ||| lin yan ||| wei wang ||| ying zhang ||| 
2018 ||| history attention for source-target alignment in neural machine translation. ||| yuanyuan yu ||| yan huang ||| wenhan chao ||| peidong zhang ||| 
2021 ||| automatic fetus head segmentation in ultrasound images by attention based encoder decoder network. ||| prerna bhalla ||| ramesh kumar sunkaria ||| aman kamboj ||| anterpreet kaur bedi ||| 
2020 ||| an approach for bengali automatic question answering system using attention mechanism. ||| md. rafiuzzaman bhuiyan ||| abu kaisar mohammad masum ||| md. abdullahil-oaphy ||| syed akhter hossain ||| sheikh abujar ||| 
2020 ||| an attention based approach for sentiment analysis of food review dataset. ||| md. rafiuzzaman bhuiyan ||| mahmudul hasan mahedi ||| naimul hossain ||| zerin nasrin tumpa ||| syed akhter hossain ||| 
2021 ||| image caption generator using attention mechanism. ||| vaishnavi agrawal ||| shariva dhekane ||| neha tuniya ||| vibha vyas ||| 
2021 ||| understanding and evaluating commonsense reasoning in transformer-based architectures. ||| manav gakhar ||| nidhi chahal ||| apeksha aggarwal ||| 
2021 ||| clustering text using attention. ||| lovedeep singh ||| 
2021 ||| an attention on sentiment analysis of child abusive public comments towards bangla text and ml. ||| effat ara easmin lucky ||| md mahadi hasan sany ||| mumenunnessa keya ||| sharun akter khushbu ||| sheak rashed haider noori ||| 
2018 ||| bandwidth enhancement of dual band impedance transformer with transmission zero. ||| kochar inderkumar ||| pallav rathore ||| kaustubh prabhu ||| 
2019 ||| virtual reality based avatar-mediated joint attention task for children with autism: implication on performance and physiology. ||| vishav jyoti ||| sanika gupta ||| uttama lahiri ||| 
2020 ||| driver inattention monitoring system based on the orientation of the face using convolutional neural network. ||| manjula p. m ||| s. adarsh ||| k. i. ramachandran ||| 
2021 ||| loss optimised video captioning using deep-lstm, attention mechanism and weighted loss metrices. ||| naveen yadav ||| dinesh naik ||| 
2020 ||| csta-2p1d unet: consecutive spatio-temporal attention for multi-scale 3d pancreas segmentation. ||| bharat giddwani ||| shruti pandey ||| hitesh tekchandani ||| shrish verma ||| 
2021 ||| attention based image captioning using depth-wise separable convolution. ||| vikash raja mallick ||| dinesh naik ||| 
2017 ||| demo: radio rate transformer: a portable vhf/uhf and gigahertz radio combo providing broadband ad-hoc networking services to mobile vehicles. ||| jiejun james kong ||| 
2019 ||| worldly eyes on video: learnt vs. reactive deployment of attention to dynamic stimuli. ||| vittorio cuculo ||| alessandro d'amelio ||| giuliano grossi ||| raffaella lanzarotti ||| 
2019 ||| video-based convolutional attention for person re-identification. ||| marco zamprogno ||| marco passon ||| niki martinel ||| giuseppe serra ||| giuseppe lancioni ||| christian micheloni ||| carlo tasso ||| gian luca foresti ||| 
2019 ||| image memorability using diverse visual features and soft attention. ||| marco leonardi ||| luigi celona ||| paolo napoletano ||| simone bianco ||| raimondo schettini ||| franco manessi ||| alessandro rozza ||| 
2017 ||| exploiting visual saliency algorithms for object-based attention: a new color and scale-based approach. ||| edoardo ardizzone ||| alessandro bruno ||| francesco gugliuzza ||| 
2019 ||| a sequence-to-sequence transformer premised temporal convolutional network for chinese word segmentation. ||| wei jiang ||| yuan wang ||| yan tang ||| 
2020 ||| cross-database micro expression recognition based on apex frame optical flow and multi-head self-attention. ||| jiebin wen ||| wenzhong yang ||| liejun wang ||| wenyu wei ||| sixiang tan ||| yongzhi wu ||| 
2020 ||| a novel attention model of deep learning in image classification. ||| qiang hua ||| liyou chen ||| pan li ||| shipeng zhao ||| yan li ||| 
2021 ||| integrated training for sequence-to-sequence models using non-autoregressive transformer. ||| evgeniia tokarchuk ||| jan rosendahl ||| weiyue wang ||| pavel petrushkov ||| tomer lancewicki ||| shahram khadivi ||| hermann ney ||| 
2019 ||| how transformer revitalizes character-based neural machine translation: an investigation on japanese-vietnamese translation systems. ||| thi-vinh ngo ||| thanh-le ha ||| phuong-thai nguyen ||| le-minh nguyen ||| 
2019 ||| controlling utterance length in nmt-based word segmentation with attention. ||| pierre godard ||| laurent besacier ||| fran ||| ois yvon ||| 
2021 ||| multilingual speech translation with unified transformer: huawei noah's ark lab at iwslt 2021. ||| xingshan zeng ||| liangyou li ||| qun liu ||| 
2019 ||| transformers without tears: improving the normalization of self-attention. ||| toan q. nguyen ||| julian salazar ||| 
2020 ||| efficient automatic punctuation restoration using bidirectional transformers with robust inference. ||| maury courtland ||| adam faulkner ||| gayle mcelvain ||| 
2019 ||| transformer-based cascaded multimodal speech translation. ||| zixiu wu ||| ozan caglayan ||| julia ive ||| josiah wang ||| lucia specia ||| 
2018 ||| new immersive media to broaden attention and awareness. ||| tony langford ||| alistair burleigh ||| nicole ruta ||| robert pepperell ||| 
2021 ||| efficient attentions for long document summarization. ||| luyang huang ||| shuyang cao ||| nikolaus nova parulian ||| heng ji ||| lu wang ||| 
2019 ||| convolutional self-attention networks. ||| baosong yang ||| longyue wang ||| derek f. wong ||| lidia s. chao ||| zhaopeng tu ||| 
2021 ||| enriching transformers with structured tensor-product representations for abstractive summarization. ||| yichen jiang ||| asli celikyilmaz ||| paul smolensky ||| paul soulos ||| sudha rao ||| hamid palangi ||| roland fernandez ||| caitlin smith ||| mohit bansal ||| jianfeng gao ||| 
2019 ||| text generation from knowledge graphs with graph transformers. ||| rik koncel-kedziorski ||| dhanush bekal ||| yi luan ||| mirella lapata ||| hannaneh hajishirzi ||| 
2021 ||| capturing row and column semantics in transformer based question answering over tables. ||| michael r. glass ||| mustafa canim ||| alfio gliozzo ||| saneem a. chemmengath ||| vishwajeet kumar ||| rishav chakravarti ||| avi sil ||| feifei pan ||| samarth bharadwaj ||| nicolas rodolfo fauceglia ||| 
2019 ||| joint multi-label attention networks for social text annotation. ||| hang dong ||| wei wang ||| kaizhu huang ||| frans coenen ||| 
2019 ||| bert: pre-training of deep bidirectional transformers for language understanding. ||| jacob devlin ||| ming-wei chang ||| kenton lee ||| kristina toutanova ||| 
2021 ||| lightseq: a high performance inference library for transformers. ||| xiaohui wang ||| ying xiong ||| yang wei ||| mingxuan wang ||| lei li ||| 
2021 ||| unidrop: a simple yet effective technique to improve transformer without extra cost. ||| zhen wu ||| lijun wu ||| qi meng ||| yingce xia ||| shufang xie ||| tao qin ||| xinyu dai ||| tie-yan liu ||| 
2021 ||| attention head masking for inference time content selection in abstractive summarization. ||| shuyang cao ||| lu wang ||| 
2019 ||| hierarchical user and item representation with three-tier attention for recommendation. ||| chuhan wu ||| fangzhao wu ||| junxin liu ||| yongfeng huang ||| 
2021 ||| predicting discourse trees from transformer-based neural summarizers. ||| wen xiao ||| patrick huber ||| giuseppe carenini ||| 
2019 ||| modeling recurrence for transformer. ||| jie hao ||| xing wang ||| baosong yang ||| longyue wang ||| jinfeng zhang ||| zhaopeng tu ||| 
2021 ||| towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers. ||| andrew silva ||| pradyumna tambwekar ||| matthew c. gombolay ||| 
2019 ||| relation classification using segment-level attention-based cnn and dependency-based rnn. ||| van-hien tran ||| van-thuy phi ||| hiroyuki shindo ||| yuji matsumoto ||| 
2021 ||| on the transformer growth for progressive bert training. ||| xiaotao gu ||| liyuan liu ||| hongkun yu ||| jing li ||| chen chen ||| jiawei han ||| 
2021 ||| an architecture for accelerated large-scale inference of transformer-based language models. ||| amir ganiev ||| colton chapin ||| anderson de andrade ||| chen liu ||| 
2021 ||| mask attention networks: rethinking and strengthen transformer. ||| zhihao fan ||| yeyun gong ||| dayiheng liu ||| zhongyu wei ||| siyuan wang ||| jian jiao ||| nan duan ||| ruofei zhang ||| xuanjing huang ||| 
2021 ||| amr parsing with action-pointer transformer. ||| jiawei zhou ||| tahira naseem ||| ram ||| n fernandez astudillo ||| radu florian ||| 
2021 ||| mt5: a massively multilingual pre-trained text-to-text transformer. ||| linting xue ||| noah constant ||| adam roberts ||| mihir kale ||| rami al-rfou ||| aditya siddhant ||| aditya barua ||| colin raffel ||| 
2021 ||| da-transformer: distance-aware transformer. ||| chuhan wu ||| fangzhao wu ||| yongfeng huang ||| 
2021 ||| event time extraction and propagation via graph attention networks. ||| haoyang wen ||| yanru qu ||| heng ji ||| qiang ning ||| jiawei han ||| avi sil ||| hanghang tong ||| dan roth ||| 
2021 ||| probing for bridging inference in transformer language models. ||| onkar pandit ||| yufang hou ||| 
2019 ||| giving attention to the unexpected: using prosody innovations in disfluency detection. ||| vicky zayats ||| mari ostendorf ||| 
2021 ||| target-specified sequence labeling with multi-head self-attention for target-oriented opinion words extraction. ||| yuhao feng ||| yanghui rao ||| yuyao tang ||| ninghua wang ||| he liu ||| 
2019 ||| simple attention-based representation learning for ranking short social media posts. ||| peng shi ||| jinfeng rao ||| jimmy lin ||| 
2018 ||| how time matters: learning time-decay attention for contextual spoken language understanding in dialogues. ||| shang-yu su ||| pei-chieh yuan ||| yun-nung chen ||| 
2018 ||| combining character and word information in neural machine translation using a multi-level attention. ||| huadong chen ||| shujian huang ||| david chiang ||| xinyu dai ||| jiajun chen ||| 
2021 ||| sparta: efficient open-domain question answering via sparse transformer matching retrieval. ||| tiancheng zhao ||| xiaopeng lu ||| kyusong lee ||| 
2021 ||| on attention redundancy: a comprehensive study. ||| yuchen bian ||| jiaji huang ||| xingyu cai ||| jiahong yuan ||| kenneth church ||| 
2018 ||| generating topic-oriented summaries using neural attention. ||| kundan krishna ||| balaji vasan srinivasan ||| 
2021 ||| spanpredict: extraction of predictive document spans with neural attention. ||| vivek subramanian ||| matthew engelhard ||| samuel berchuck ||| liqun chen ||| ricardo henao ||| lawrence carin ||| 
2021 ||| on biasing transformer attention towards monotonicity. ||| annette rios ||| chantal amrhein ||| no ||| mi aepli ||| rico sennrich ||| 
2021 ||| multi-hop transformer for document-level machine translation. ||| long zhang ||| tong zhang ||| haibo zhang ||| baosong yang ||| wei ye ||| shikun zhang ||| 
2021 ||| too much in common: shifting of embeddings in transformer language models and its implications. ||| daniel bis ||| maksim podkorytov ||| xiuwen liu ||| 
2019 ||| dialogue act classification with context-aware self-attention. ||| vipul raheja ||| joel r. tetreault ||| 
2019 ||| tensorized self-attention: efficiently modeling pairwise and global dependencies together. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2019 ||| attentivechecker: a bi-directional attention flow mechanism for fact verification. ||| santosh tokala ||| vishal g ||| avirup saha ||| niloy ganguly ||| 
2021 ||| incorporating syntax and semantics in coreference resolution with heterogeneous graph attention network. ||| fan jiang ||| trevor cohn ||| 
2019 ||| can-ner: convolutional attention network for chinese named entity recognition. ||| yuying zhu ||| guoxin wang ||| 
2018 ||| target foresight based attention for neural machine translation. ||| xintong li ||| lemao liu ||| zhaopeng tu ||| shuming shi ||| max meng ||| 
2021 ||| emotion classification in a resource constrained language using transformer-based approach. ||| avishek das ||| omar sharif ||| mohammed moshiul hoque ||| iqbal h. sarker ||| 
2021 ||| focused attention improves document-grounded generation. ||| shrimai prabhumoye ||| kazuma hashimoto ||| yingbo zhou ||| alan w. black ||| ruslan salakhutdinov ||| 
2019 ||| sequential attention with keyword mask model for community-based question answering. ||| jianxin yang ||| wenge rong ||| libin shi ||| zhang xiong ||| 
2019 ||| distant supervision relation extraction with intra-bag and inter-bag attentions. ||| zhi-xiu ye ||| zhen-hua ling ||| 
2021 ||| reconsider: improved re-ranking using span-focused cross-attention for open domain question answering. ||| srinivasan iyer ||| sewon min ||| yashar mehdad ||| wen-tau yih ||| 
2021 ||| script: self-critic pretraining of transformers. ||| erik nijkamp ||| bo pang ||| ying nian wu ||| caiming xiong ||| 
2021 ||| empirical evaluation of pre-trained transformers for human-level nlp: the role of sample size and dimensionality. ||| adithya v. ganesan ||| matthew matero ||| aravind reddy ravula ||| huy vu ||| h. andrew schwartz ||| 
2019 ||| incorporating word attention into character-based word segmentation. ||| shohei higashiyama ||| masao utiyama ||| eiichiro sumita ||| masao ideuchi ||| yoshiaki oida ||| yohei sakamoto ||| isaac okada ||| 
2019 ||| saliency learning: teaching the model where to pay attention. ||| reza ghaeini ||| xiaoli z. fern ||| hamed shahbazi ||| prasad tadepalli ||| 
2018 ||| generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization. ||| preksha nema ||| shreyas shetty ||| parag jain ||| anirban laha ||| karthik sankaranarayanan ||| mitesh m. khapra ||| 
2018 ||| read and comprehend by gated-attention reader with more belief. ||| haohui deng ||| yik-cheung tam ||| 
2021 ||| compositional generalization for neural semantic parsing via span-level supervised attention. ||| pengcheng yin ||| hao fang ||| graham neubig ||| adam pauls ||| emmanouil antonios platanios ||| yu su ||| sam thomson ||| jacob andreas ||| 
2021 ||| cort: complementary rankings from transformers. ||| marco wrzalik ||| dirk krechel ||| 
2019 ||| topic spotting using hierarchical networks with self attention. ||| pooja chitkara ||| ashutosh modi ||| pravalika avvaru ||| sepehr janghorbani ||| mubbasir kapadia ||| 
2019 ||| selective attention for context-aware neural machine translation. ||| sameen maruf ||| andr |||  f. t. martins ||| gholamreza haffari ||| 
2021 ||| a million tweets are worth a few points: tuning transformers for customer service tasks. ||| amir hadifar ||| sofie labat ||| v ||| ronique hoste ||| chris develder ||| thomas demeester ||| 
2019 ||| decay-function-free time-aware attention to context and speaker indicator for spoken language understanding. ||| jonggu kim ||| jong-hyeok lee ||| 
2021 ||| practical transformer-based multilingual text classification. ||| cindy wang ||| michele banko ||| 
2021 ||| mtag: modal-temporal attention graph for unaligned human multimodal language sequences. ||| jianing yang ||| yongxin wang ||| ruitao yi ||| yuying zhu ||| azaan rehman ||| amir zadeh ||| soujanya poria ||| louis-philippe morency ||| 
2019 ||| attention is not explanation. ||| sarthak jain ||| byron c. wallace ||| 
2021 ||| syntax-based attention masking for neural machine translation. ||| colin mcdonald ||| david chiang ||| 
2018 ||| a discourse-aware attention model for abstractive summarization of long documents. ||| arman cohan ||| franck dernoncourt ||| doo soon kim ||| trung bui ||| seokhwan kim ||| walter chang ||| nazli goharian ||| 
2021 ||| probing word translations in the transformer and trading decoder for encoder layers. ||| hongfei xu ||| josef van genabith ||| qiuhui liu ||| deyi xiong ||| 
2018 ||| watch, listen, and describe: globally and locally aligned cross-modal attentions for video captioning. ||| xin wang ||| yuan-fang wang ||| william yang wang ||| 
2018 ||| self-attention with relative position representations. ||| peter shaw ||| jakob uszkoreit ||| ashish vaswani ||| 
2018 ||| a mixed hierarchical attention based encoder-decoder approach for standard table summarization. ||| parag jain ||| anirban laha ||| karthik sankaranarayanan ||| preksha nema ||| mitesh m. khapra ||| shreyas shetty ||| 
2021 ||| template filling with generative transformers. ||| xinya du ||| alexander m. rush ||| claire cardie ||| 
2019 ||| tweet stance detection using an attention based neural ensemble model. ||| umme aymun siddiqua ||| abu nowshed chy ||| masaki aono ||| 
2019 ||| star-transformer. ||| qipeng guo ||| xipeng qiu ||| pengfei liu ||| yunfan shao ||| xiangyang xue ||| zheng zhang ||| 
2021 ||| hierarchical transformer for task oriented dialog systems. ||| bishal santra ||| potnuru anusha ||| pawan goyal ||| 
2021 ||| date: detecting anomalies in text via self-supervision of transformers. ||| andrei manolache ||| florin brad ||| elena burceanu ||| 
2018 ||| knowledge-enriched two-layered attention network for sentiment analysis. ||| abhishek kumar ||| daisuke kawahara ||| sadao kurohashi ||| 
2019 ||| bag: bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. ||| yu cao ||| meng fang ||| dacheng tao ||| 
2019 ||| information aggregation for multi-head attention with routing-by-agreement. ||| jian li ||| baosong yang ||| zi-yi dou ||| xing wang ||| michael r. lyu ||| zhaopeng tu ||| 
2018 ||| higher-order syntactic attention network for longer sentence compression. ||| hidetaka kamigaito ||| katsuhiko hayashi ||| tsutomu hirao ||| masaaki nagata ||| 
2021 ||| distantly supervised transformers for e-commerce product qa. ||| happy mittal ||| aniket chakrabarti ||| belhassen bayar ||| animesh anant sharma ||| nikhil rasiwasia ||| 
2021 ||| an effective intrusion detection model for class-imbalanced learning based on smote and attention mechanism. ||| xubin jiao ||| jinguo li ||| 
2021 ||| user identification in online social networks using graph transformer networks. ||| k. n. pavan kumar ||| marina l. gavrilova ||| 
2018 ||| differences in working-memory capacity modulate top-down control of social attention. ||| ali momen ||| eva wiese ||| 
2021 ||| multimodal attention creates the visual input for infant word learning. ||| sara e. schroer ||| chen yu ||| 
2021 ||| unsupervised learning of shape-invariant lie group transformer by embedding ordinary differential equation. ||| takumi takada ||| yoshiyuki ohmura ||| yasuo kuniyoshi ||| 
2020 ||| a visually explainable learning system for skin lesion detection using multiscale input with attention u-net. ||| duy minh ho nguyen ||| abraham obinwanne ezema ||| fabrizio nunnari ||| daniel sonntag ||| 
2021 ||| combining transformer generators with convolutional discriminators. ||| ricard durall ||| stanislav frolov ||| j ||| rn hees ||| federico raue ||| franz-josef pfreundt ||| andreas dengel ||| janis keuper ||| 
2021 ||| an attention method to introduce prior knowledge in dialogue state tracking. ||| zhonghao chen ||| cong liu ||| 
2020 ||| proposing two different feature extraction methods from multi-fractal detrended fluctuation analysis of electroencephalography signals: a case study on attention-deficit hyperactivity disorder. ||| zahra roozbehi ||| mahsa mohaghegh ||| hossein lanjanian ||| peyman hassani abharian ||| 
2020 ||| correlation-aware next basket recommendation using graph attention networks. ||| yuanzhe zhang ||| ling luo ||| jianjia zhang ||| qiang lu ||| yang wang ||| zhiyong wang ||| 
2021 ||| saliency detection framework based on deep enhanced attention network. ||| xing sheng ||| zhuoran zheng ||| qiong wu ||| chunmeng kang ||| yunliang zhuang ||| lei lyu ||| chen lyu ||| 
2021 ||| classmates enhanced diversity-self-attention network for dropout prediction in moocs. ||| dongen wu ||| pengyi hao ||| yuxiang zheng ||| tianxing han ||| cong bai ||| 
2021 ||| spatio-temporal dynamic multi-graph attention network for ride-hailing demand prediction. ||| ya chen ||| wanrong jiang ||| hao fu ||| guiquan liu ||| 
2021 ||| speaker verification with disentangled self-attention. ||| junjie guo ||| zhiyuan ma ||| haodong zhao ||| gongshen liu ||| xiaoyong li ||| 
2021 ||| rethinking the effectiveness of selective attention in neural networks. ||| yulong wang ||| xiaolu zhang ||| jun zhou ||| hang su ||| 
2019 ||| weakly supervised fine-grained visual recognition via adversarial complementary attentions and hierarchical bilinear pooling. ||| xiaofei li ||| jianming liu ||| mingwen wang ||| 
2019 ||| exploration of different attention mechanisms on medical image segmentation. ||| jie tian ||| kaijie wu ||| kai ma ||| hao cheng ||| chaocheng gu ||| 
2019 ||| attention network for product characteristics prediction based on reviews. ||| wan li ||| lei zhang ||| si chen ||| 
2021 ||| transformer with prior language knowledge for image captioning. ||| daisong yan ||| wenxin yu ||| zhiqiang zhang ||| jun gong ||| 
2020 ||| a hybrid self-attention model for pedestrians detection. ||| yuan wang ||| chao zhu ||| xu-cheng yin ||| 
2017 ||| temporal attention neural network for video understanding. ||| jegyung son ||| gil-jin jang ||| minho lee ||| 
2020 ||| dpast-rnn: a dual-phase attention-based recurrent neural network using spatiotemporal lstms for time series prediction. ||| shajia shan ||| ziyu shen ||| bin xia ||| zheng liu ||| yun li ||| 
2021 ||| self-attention long-term dependency modelling in electroencephalography sleep stage prediction. ||| georg brandmayr ||| manfred martin hartmann ||| franz f ||| rbass ||| georg dorffner ||| 
2019 ||| discriminant feature learning with self-attention for person re-identification. ||| yang li ||| xiaoyan jiang ||| jenq-neng hwang ||| 
2017 ||| a multi-attention-based bidirectional long short-term memory network for relation extraction. ||| lingfeng li ||| yuanping nie ||| weihong han ||| jiuming huang ||| 
2021 ||| gru with level-aware attention for rumor early detection in social networks. ||| yu wang ||| wei zhou ||| junhao wen ||| jun zeng ||| haoran he ||| lin liu ||| 
2019 ||| attention-based deep q-network in complex systems. ||| kun ni ||| danning yu ||| yunlong liu ||| 
2019 ||| link prediction with attention-based semantic influence of multiple neighbors. ||| meixian song ||| bo wang ||| xindian ma ||| qinghua hu ||| xin wang ||| yuexian hou ||| dawei song ||| 
2020 ||| prediction of taxi demand based on cnn-bilstm-attention neural network. ||| xudong guo ||| 
2018 ||| memory-based model with multiple attentions for multi-turn response selection. ||| xingwu lu ||| man lan ||| yuanbin wu ||| 
2018 ||| mulattenrec: a multi-level attention-based model for recommendation. ||| zhipeng lin ||| wenjing yang ||| yongjun zhang ||| haotian wang ||| yuhua tang ||| 
2018 ||| attention-based network for cross-view gait recognition. ||| yuanyuan huang ||| jianfu zhang ||| haohua zhao ||| liqing zhang ||| 
2020 ||| multitask learning based on constrained hierarchical attention network for multi-aspect sentiment classification. ||| yang gao ||| jianxun liu ||| pei li ||| dong zhou ||| peng yuan ||| 
2020 ||| adversarial shared-private attention network for joint slot filling and intent detection. ||| mengfei wu ||| longbiao wang ||| yuke si ||| jianwu dang ||| 
2021 ||| a multi-channel graph attention network for chinese ner. ||| yichun zhao ||| kui meng ||| gongshen liu ||| 
2018 ||| hashtag recommendation with attention-based neural image hashtagging network. ||| gaosheng wu ||| yuhua li ||| wenjin yan ||| ruixuan li ||| xiwu gu ||| qi yang ||| 
2020 ||| springnet: transformer and spring dtw for time series forecasting. ||| yang lin ||| irena koprinska ||| mashud rana ||| 
2021 ||| coordinate attention residual deformable u-net for vessel segmentation. ||| cong wu ||| xiao liu ||| shijun li ||| cheng long ||| 
2021 ||| pathsage: spatial graph attention neural networks with random path sampling. ||| junhua ma ||| jiajun li ||| xueming li ||| xu li ||| 
2020 ||| stga-lstm: a spatial-temporal graph attentional lstm scheme for multi-agent cooperation. ||| huimu wang ||| zhen liu ||| zhiqiang pu ||| jianqiang yi ||| 
2020 ||| coarse-to-fine attention network via opinion approximate representation for aspect-level sentiment classification. ||| wei chen ||| wenxin yu ||| gang he ||| ning jiang ||| gang he ||| 
2019 ||| hie-transformer: a hierarchical hybrid transformer for abstractive article summarization. ||| xuewen zhang ||| kui meng ||| gongshen liu ||| 
2021 ||| multi-task perceptual occlusion face detection with semantic attention network. ||| lian shen ||| jia-xiang lin ||| chang-ying wang ||| 
2018 ||| text simplification with self-attention-based pointer-generator networks. ||| tianyu li ||| yun li ||| jipeng qiang ||| yunhao yuan ||| 
2019 ||| high-performance light field reconstruction with channel-wise and sai-wise attention. ||| zexi hu ||| yuk ying chung ||| seid miad zandavi ||| wanli ouyang ||| xiangjian he ||| yuefang gao ||| 
2021 ||| global fusion capsule network with pairwise-relation attention graph routing. ||| xinyi li ||| song wu ||| guoqiang xiao ||| 
2017 ||| position-based content attention for time series forecasting with sequence-to-sequence rnns. ||| yagmur gizem cinar ||| hamid mirisaee ||| parantapa goswami ||| ric gaussier ||| ali a ||| t-bachir ||| vadim v. strijov ||| 
2019 ||| attention-based audio-visual fusion for video summarization. ||| yinghong fang ||| junpeng zhang ||| cewu lu ||| 
2017 ||| aggregating class interactions for hierarchical attention relation extraction. ||| kaiyu huang ||| si li ||| guang chen ||| 
2019 ||| attention based shared representation for multi-task stance detection and sentiment analysis. ||| dushyant singh chauhan ||| rohan kumar ||| asif ekbal ||| 
2021 ||| cpsam: channel and position squeeze attention module. ||| yuchen gong ||| zhihao gu ||| zhenghao zhang ||| lizhuang ma ||| 
2019 ||| delving into precise attention in image captioning. ||| shaohan hu ||| shenglei huang ||| guolong wang ||| zhipeng li ||| zheng qin ||| 
2020 ||| sparse hierarchical modeling of deep contextual attention for document-level neural machine translation. ||| jianshen zhang ||| yong liao ||| yongan li ||| gongshen liu ||| 
2019 ||| gcnda: graph convolutional networks with dual attention mechanisms for aspect based sentiment analysis. ||| junjie chen ||| hongxu hou ||| jing gao ||| yatu ji ||| tiangang bai ||| yi jing ||| 
2018 ||| attention-based combination of cnn and rnn for relation classification. ||| xiaoyu guo ||| hui zhang ||| rui liu ||| xin ding ||| runqi tian ||| bencheng wang ||| 
2020 ||| an attention-based interaction-aware spatio-temporal graph neural network for trajectory prediction. ||| hao zhou ||| dongchun ren ||| huaxia xia ||| mingyu fan ||| xu yang ||| hai huang ||| 
2017 ||| a width-variable window attention model for environmental sensors. ||| cuiqin hou ||| yingju xia ||| jun sun ||| jing shang ||| ryozo takasu ||| masao kondo ||| 
2019 ||| sparse graphic attention lstm for eeg emotion recognition. ||| suyuan liu ||| wenming zheng ||| tengfei song ||| yuan zong ||| 
2021 ||| edge guided attention based densely connected network for single image super-resolution. ||| zijian wang ||| yao lu ||| qingxuan shi ||| 
2021 ||| spatial-temporal attention network with multi-similarity loss for fine-grained skeleton-based action recognition. ||| xiang li ||| shenglan liu ||| yunheng li ||| hao liu ||| jinjing zhao ||| lin feng ||| guihong lao ||| guangzhe li ||| 
2019 ||| reinforcement learning with attention that works: a self-supervised approach. ||| anthony manchin ||| ehsan abbasnejad ||| anton van den hengel ||| 
2017 ||| bi-directional lstm with quantum attention mechanism for sentence modeling. ||| xiaolei niu ||| yuexian hou ||| panpan wang ||| 
2018 ||| aspect-level sentiment classification with conv-attention mechanism. ||| qian yi ||| jie liu ||| guixuan zhang ||| shuwu zhang ||| 
2019 ||| raunet: residual attention u-net for semantic segmentation of cataract surgical instruments. ||| zhen-liang ni ||| gui-bin bian ||| xiao-hu zhou ||| zeng-guang hou ||| xiao-liang xie ||| chen wang ||| yan-jie zhou ||| rui-qi li ||| zhen li ||| 
2021 ||| attention-based 3d resnet for detection of alzheimer's disease process. ||| mingjin liu ||| jialiang tang ||| wenxin yu ||| ning jiang ||| 
2019 ||| a fast convolutional self-attention based speech dereverberation method for robust speech recognition. ||| nan li ||| meng ge ||| longbiao wang ||| jianwu dang ||| 
2021 ||| end-to-end edge detection via improved transformer model. ||| yi gao ||| chenwei tang ||| jiulin lang ||| jiancheng lv ||| 
2021 ||| srgat: social relational graph attention network for human trajectory prediction. ||| yusheng peng ||| gaofeng zhang ||| xiangyu li ||| liping zheng ||| 
2018 ||| attention based dialogue context selection model. ||| weidi xu ||| yong ren ||| ying tan ||| 
2021 ||| hierarchical features integration and attention iteration network for juvenile refractive power prediction. ||| yang zhang ||| risa higashita ||| guodong long ||| rong li ||| daisuke santo ||| jiang liu ||| 
2021 ||| a transformer-based model for low-resource event detection. ||| yanxia qin ||| jingjing ding ||| yiping sun ||| xiangwu ding ||| 
2021 ||| learning discriminative representation with attention and diversity for large-scale face recognition. ||| zhong zheng ||| manli zhang ||| guixia kang ||| 
2019 ||| target-based attention model for aspect-level sentiment analysis. ||| wei chen ||| wenxin yu ||| zhiqiang zhang ||| yunye zhang ||| kepeng xu ||| fengwei zhang ||| yibo fan ||| gang he ||| zhuo yang ||| 
2020 ||| multi-scale attention consistency for multi-label image classification. ||| haotian xu ||| xiaobo jin ||| qiufeng wang ||| kaizhu huang ||| 
2021 ||| a lightweight multidimensional self-attention network for fine-grained action recognition. ||| hao liu ||| shenglan liu ||| lin feng ||| lianyu hu ||| xiang li ||| heyu fu ||| 
2019 ||| sacic: a semantics-aware convolutional image captioner using multi-level pervasive attention. ||| sandeep narayan parameswaran ||| sukhendu das ||| 
2021 ||| sta3dcnn: spatial-temporal attention 3d convolutional neural network for citywide crowd flow prediction. ||| gaozhong tang ||| zhiheng zhou ||| bo li ||| 
2019 ||| pay attention to deep feature fusion in crowd density estimation. ||| huimin guo ||| fujin he ||| xin cheng ||| xinghao ding ||| yue huang ||| 
2017 ||| hierarchical attention blstm for modeling sentences and documents. ||| xiaolei niu ||| yuexian hou ||| 
2021 ||| multimodal named entity recognition via co-attention-based method with dynamic visual concept expansion. ||| xiaoyu zhao ||| buzhou tang ||| 
2019 ||| intra-modality feature interaction using self-attention for visual question answering. ||| huan shao ||| yunlong xu ||| yi ji ||| jianyu yang ||| chunping liu ||| 
2017 ||| boxless action recognition in still images via recurrent visual attention. ||| weijiang feng ||| xiang zhang ||| xuhui huang ||| zhigang luo ||| 
2019 ||| hierarchical attention cnn and entity-aware for relation extraction. ||| xinyu zhu ||| gongshen liu ||| bo su ||| 
2020 ||| spotfast networks with memory augmented lateral transformers for lipreading. ||| peratham wiriyathammabhum ||| 
2020 ||| diabetic retinopathy detection using multi-layer neural networks and split attention with focal loss. ||| usman naseem ||| matloob khushi ||| shah khalid khan ||| nazar waheed ||| adnan mir ||| atika qazi ||| bandar alshammari ||| simon k. poon ||| 
2019 ||| aspect-level sentiment classification with dependency rules and dual attention. ||| yunkai yang ||| tieyun qian ||| zhuang chen ||| 
2019 ||| time-frequency deep representation learning for speech emotion recognition integrating self-attention. ||| jiaxing liu ||| zhilei liu ||| longbiao wang ||| lili guo ||| jianwu dang ||| 
2018 ||| a refined spatial transformer network. ||| chang shu ||| xi chen ||| chong yu ||| hua han ||| 
2020 ||| res2u-net: image inpainting via multi-scale backbone and channel attention. ||| hao yang ||| ying yu ||| 
2020 ||| predicting information diffusion cascades using graph attention networks. ||| meng wang ||| kan li ||| 
2020 ||| multi-modal feature attention for cervical lymph node segmentation in ultrasound and doppler images. ||| xiangling fu ||| tong gao ||| yuan liu ||| mengke zhang ||| chenyi guo ||| ji wu ||| zhili wang ||| 
2021 ||| trufm: a transformer-guided framework for fine-grained urban flow inference. ||| xinchi zhou ||| dongzhan zhou ||| lingbo liu ||| 
2021 ||| bertdan: question-answer dual attention fusion networks with pre-trained models for answer selection. ||| haitian yang ||| chonghui zheng ||| xuan zhao ||| yan wang ||| zheng yang ||| chao ma ||| qi zhang ||| weiqing huang ||| 
2017 ||| relation classification via target-concentrated attention cnns. ||| jizhao zhu ||| jianzhong qiao ||| xinxiao dai ||| xueqi cheng ||| 
2018 ||| a comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese. ||| shiyu zhou ||| linhao dong ||| shuang xu ||| bo xu ||| 
2020 ||| routing attention shift network for image classification and segmentation. ||| yuwei yang ||| yi sun ||| guiping su ||| shiwei ye ||| 
2019 ||| improve image captioning by self-attention. ||| zhenru li ||| yaoyi li ||| hongtao lu ||| 
2021 ||| metric learning based vision transformer for product matching. ||| lei huang ||| wei shao ||| fuzhou wang ||| weidun xie ||| ka-chun wong ||| 
2018 ||| attentional payload anomaly detector for web applications. ||| zhi-quan qin ||| xing-kong ma ||| yong-jun wang ||| 
2020 ||| triple attention network for clothing parsing. ||| ruhan he ||| ming cheng ||| mingfu xiong ||| xiao qin ||| junping liu ||| xinrong hu ||| 
2019 ||| fusion convolutional attention network for opinion spam detection. ||| jiacheng li ||| qianwen ma ||| chunyuan yuan ||| wei zhou ||| jizhong han ||| songlin hu ||| 
2021 ||| generative adversarial domain generalization via cross-task feature attention learning for prostate segmentation. ||| yifang xu ||| dan yu ||| ye luo ||| enbei zhu ||| jianwei lu ||| 
2020 ||| deep cardiovascular disease prediction with risk factors powered bi-attention. ||| yanlong qiu ||| zhichang zhang ||| xiaohui qin ||| shengxin tao ||| 
2019 ||| keyphrase generation with word attention. ||| hai huang ||| tianshuo huang ||| longxuan ma ||| lei zhang ||| 
2017 ||| end-to-end chinese image text recognition with attention model. ||| fenfen sheng ||| chuanlei zhai ||| zhineng chen ||| bo xu ||| 
2020 ||| multi-agent cooperation and competition with two-level attention network. ||| shiguang wu ||| zhiqiang pu ||| jianqiang yi ||| huimu wang ||| 
2020 ||| multi-view subspace adaptive learning via autoencoder and attention. ||| jianwei liu ||| hao-jie xie ||| runkun lu ||| xiong-lin luo ||| 
2018 ||| estimation of student classroom attention using a novel measure of head motion coherence. ||| naoyuki sato ||| atsuko tominaga ||| 
2020 ||| multiple sclerosis lesion filling using a non-lesion attention based convolutional network. ||| hao xiong ||| chaoyue wang ||| michael barnett ||| chenyu wang ||| 
2018 ||| two-stage attention network for aspect-level sentiment classification. ||| kai gao ||| hua xu ||| chengliang gao ||| xiaomin sun ||| junhui deng ||| xiaoming zhang ||| 
2019 ||| transformer-dw: a transformer network with dynamic and weighted head. ||| ruxin tan ||| jiahui sun ||| bo su ||| gongshen liu ||| 
2019 ||| deep residual-dense attention network for image super-resolution. ||| ding qin ||| xiaodong gu ||| 
2019 ||| attention-based image captioning using densenet features. ||| md. zakir hossain ||| ferdous sohel ||| mohd fairuz shiratuddin ||| hamid laga ||| mohammed bennamoun ||| 
2021 ||| gated channel attention network for cataract classification on as-oct image. ||| zunjie xiao ||| xiaoqing zhang ||| risa higashita ||| yan hu ||| jin yuan ||| wan chen ||| jiang liu ||| 
2020 ||| residual spatial attention network for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yugen yi ||| wei zhou ||| haodong bian ||| 
2021 ||| raidu-net: image inpainting via residual attention fusion and gated information distillation. ||| penghao he ||| ying yu ||| chaoyue xu ||| hao yang ||| 
2021 ||| an lstm-based plagiarism detection via attention mechanism and a population-based approach for pre-training parameters with imbalanced classes. ||| seyed vahid moravvej ||| seyed jalaleddin mousavirad ||| mahshid helali moghadam ||| mehrdad saadatmand ||| 
2021 ||| tri-transformer hawkes process: three heads are better than one. ||| zhi-yan song ||| jianwei liu ||| lu-ning zhang ||| ya-nan han ||| 
2020 ||| attention-based multi-component lstm for internet traffic prediction. ||| qian xu ||| zhenjie yao ||| yanhui tu ||| yixin chen ||| 
2017 ||| hierarchical hybrid attention networks for chinese conversation topic classification. ||| yujun zhou ||| changliang li ||| bo xu ||| jiaming xu ||| jie cao ||| bo xu ||| 
2021 ||| multi-attention network for arbitrary style transfer. ||| sihui hua ||| dongdong zhang ||| 
2020 ||| facial expression recognition with an attention network using a single depth image. ||| jianmin cai ||| hongliang xie ||| jianfeng li ||| shigang li ||| 
2021 ||| jstrack: enriching malicious javascript detection based on ast graph analysis and attention mechanism. ||| muhammad fakhrur rozi ||| tao ban ||| seiichi ozawa ||| sangwook kim ||| takeshi takahashi ||| daisuke inoue ||| 
2021 ||| a novel multi-scale key-point detector using residual dense block and coordinate attention. ||| li-dan kuang ||| jiajun tao ||| jianming zhang ||| feng li ||| xi chen ||| 
2020 ||| joint optic disc and optic cup segmentation based on new skip-link attention guidance network and polar transformation. ||| yun jiang ||| jing gao ||| falin wang ||| 
2019 ||| multi-task gated contextual cross-modal attention framework for sentiment and emotion analysis. ||| suyash sangwan ||| dushyant singh chauhan ||| md. shad akhtar ||| asif ekbal ||| pushpak bhattacharyya ||| 
2019 ||| fraud detection with multi-modal attention and correspondence learning. ||| jongchan park ||| min-hyun kim ||| seibum choi ||| in so kweon ||| dong-geol choi ||| 
2021 ||| quantitative analysis and simple monitoring for partial discharge from transformer. ||| seung soo kwak ||| yong sin kim ||| 
2019 ||| improvement of residual attention network for image classification. ||| lu liang ||| jiangdong cao ||| xiaoyan li ||| jane you ||| 
2019 ||| slicenet: mask guided efficient feature augmentation for attention-aware person re-identification. ||| zhipu liu ||| lei zhang ||| 
2019 ||| proposal-aware visual saliency detection with semantic attention. ||| lu wang ||| tian song ||| takafumi katayama ||| takashi shimamoto ||| 
2019 ||| improved ctc-attention based end-to-end speech recognition on air traffic control. ||| kai zhou ||| qun yang ||| xiusong sun ||| shaohan liu ||| jinjun lu ||| 
2019 ||| an attention bi-box regression network for traffic light detection. ||| juncai ma ||| yao zhao ||| ming luo ||| xiang jiang ||| ting liu ||| shikui wei ||| 
2019 ||| attention relational network for few-shot learning. ||| jia shuai ||| jiaming chen ||| meng yang ||| 
2020 ||| an attention-enhanced edge-cloud collaborative framework for multi-task application. ||| zhipeng zhang ||| wenting ma ||| feng li ||| renjie tang ||| jinlang wang ||| wai chen ||| 
2021 ||| ecg-based heart arrhythmia diagnosis through attentional convolutional neural networks. ||| ziyu liu ||| xiang zhang ||| 
2021 ||| behavioral disorder test to identify attention-deficit / hyperactivity disorder (adhd) in children using fuzzy algorithm. ||| nola ristiyanti ||| burhanuddin dirgantoro ||| casi setianingsih ||| 
2017 ||| autonomous, self-calibrating binocular vision based on learned attention and active efficient coding. ||| qingpeng zhu ||| jochen triesch ||| bertram e. shi ||| 
2018 ||| object detection and localization with artificial foveal visual attention. ||| cristina mel ||| cio ||| rui figueiredo ||| ana filipa almeida ||| alexandre bernardino ||| jos |||  santos-victor ||| 
2020 ||| learning over the attentional space with mobile robots. ||| let ||| cia m. berto ||| leonardo de l. rossi ||| eric rohmer ||| paula d. p. costa ||| alexandre da silva sim ||| es ||| ricardo r. gudwin ||| esther luna colombini ||| 
2017 ||| imitation learning and attentional supervision of dual-arm structured tasks. ||| riccardo caccavale ||| matteo saveriano ||| giuseppe andrea fontanelli ||| fanny ficuciello ||| dongheui lee ||| alberto finzi ||| 
2019 ||| toddlers' hands organize parent-toddler attention across different social contexts. ||| steven l. elmlinger ||| sumarga h. suanda ||| linda b. smith ||| chen yu ||| 
2017 ||| shape-based attention for identification and localization of cylindrical objects. ||| rui figueiredo ||| atabak dehban ||| alexandre bernardino ||| jos |||  santos-victor ||| helder ara ||| jo ||| 
2021 ||| hybrid chinese grammar error checking model based on transformer. ||| nawei zhong ||| xiaoge li ||| long qin ||| 
2019 ||| dense attentional network for pancreas segmentation in abdominal ct scans. ||| weihao yu ||| huai chen ||| lisheng wang ||| 
2021 ||| shoeprint image retrieval based on dual attention light hash network. ||| daxiang li ||| yang li ||| ying liu ||| 
2021 ||| text classification method based on bigru-attention and cnn hybrid model. ||| jinbao teng ||| weiwei kong ||| yidan chang ||| qiaoxin tian ||| chenyuan shi ||| long li ||| 
2021 ||| fcos small target detection algorithm combined with multi-layer hybrid attention mechanism. ||| ying liu ||| luyao geng ||| hao yu ||| zhijie xu ||| 
2020 ||| a network combining local features and attention mechanisms for vehicle re-identification. ||| linghui li ||| xiaohui zhang ||| yan xu ||| 
2021 ||| multi-head attention with disagreement regularization for multimodal sentiment analysis. ||| hao ai ||| ying liu ||| jie fang ||| 
2021 ||| floor plan semantic segmentation using deep learning with boundary attention aggregated mechanism. ||| zhongguo xu ||| cheng yang ||| salah alheejawi ||| naresh jha ||| syed mehadi ||| mrinal mandal ||| 
2021 ||| text matching model that fuse position-encoding with multiple attentional mechanisms. ||| xiaowei wang ||| jungang han ||| xiaoying pan ||| 
2021 ||| a transformer district line loss anomaly discrimination model incorporating cross-attention and deep learning algorithm. ||| songhui zhang ||| xinguang xu ||| yu xing ||| tao liu ||| zijian chen ||| jian yang ||| yuqi wang ||| xianguang dong ||| 
2020 ||| an lstm-based traffic prediction algorithm with attention mechanism for satellite network. ||| feiyue zhu ||| lixiang liu ||| teng lin ||| 
2021 ||| deep facial expression recognition algorithm combining channel attention. ||| peixiang zhang ||| ying liu ||| yu hao ||| jiming liu ||| 
2021 ||| sentiment analysis model based on multi-head attention in multimodality. ||| shuyan wang ||| wei wei ||| meng zhang ||| 
2021 ||| graph neural network based on geometric and appearance attention for 6d pose estimation. ||| tianfu wang ||| hongguang wang ||| 
2021 ||| single infrared image super-resolution with lightweight self-corrected attention network. ||| fei mo ||| lianglun cheng ||| heng wu ||| yidan chang ||| 
2021 ||| vehicle re-identification based on multi-view and convolutional block attention. ||| hao zhang ||| tongwei lu ||| shihai jia ||| 
2021 ||| an unmanned aerial vehicle video object tracking algorithm based on siamese attention network. ||| yuhuan zheng ||| dianwei wang ||| pengfei han ||| xincheng ren ||| zhijie xu ||| 
2021 ||| oriented target detection algorithm based on transformer. ||| zhizhong xi ||| jingen wang ||| yanqing kang ||| 
2020 ||| a spatial attention-enhanced multi-timescale graph convolutional network for skeleton-based action recognition. ||| shuqiong zhu ||| xiaolu ding ||| kai yang ||| wai chen ||| 
2021 ||| image caption based on bigru and attention hybrid model. ||| qiao yucong ||| ma li ||| 
2019 ||| attention-based bidirectional gated recurrent unit neural networks for sentiment analysis. ||| qing yu ||| hui zhao ||| zuohua wang ||| 
2020 ||| retinal blood vessel segmentation via attention gate network. ||| kaiqi li ||| zeyi yao ||| yiwen luo ||| xingqun qi ||| pengkun liu ||| zijian wang ||| 
2021 ||| automatic diagnosis of multiple lesions in fundus images based on dual attention mechanism. ||| jiamin gong ||| liufei guo ||| jiewei jiang ||| chengchao wu ||| mengjie pei ||| wei liu ||| 
2021 ||| what should we pay attention to when classifying violent videos? ||| marcos vin ||| cius ad ||| o teixeira ||| sandra avila ||| 
2018 ||| let's talk about refugees: network effects drive contributor attention to wikipedia articles about migration-related topics. ||| j ||| rgen lerner ||| alessandro lomi ||| 
2021 ||| an improved yolov3 algorithm combined with attention mechanism for flame and smoke detection. ||| hao zhang ||| zhiqiang wang ||| man chen ||| yumin peng ||| yanming gao ||| junhuang zhou ||| 
2021 ||| subspace classification of attention deficit hyperactivity disorder with laplacian regularization. ||| yuan wang ||| yuan gao ||| junping jiang ||| min lin ||| yibin tang ||| 
2018 ||| attention-based bidirectional recurrent neural networks for description generation of videos. ||| xiaotong du ||| jiabin yuan ||| hu liu ||| 
2020 ||| joint extraction of entity and semantic relation using encoder - decoder model based on attention mechanism. ||| yubo mai ||| yatian shen ||| guilin qi ||| xiajiong shen ||| 
2020 ||| research on user preference film recommendation based on attention mechanism. ||| lei zhu ||| yufeng liu ||| wei zhang ||| kehua yang ||| 
2021 ||| graph attention network for word embeddings. ||| yunfei long ||| huosheng xu ||| pengyuan qi ||| liguo zhang ||| jun li ||| 
2019 ||| cbam-gan: generative adversarial networks based on convolutional block attention module. ||| bing ma ||| xiaoru wang ||| heng zhang ||| fu li ||| jiawang dan ||| 
2018 ||| attention-based chinese word embedding. ||| yiyuan liang ||| wei zhang ||| kehua yang ||| 
2021 ||| corrosion detection in transformers based on hierarchical annotation. ||| yong cao ||| yinian zhou ||| zhao zhang ||| wenjun wu ||| xihai chen ||| sha yang ||| baili zhang ||| 
2020 ||| ms-sae: a general model of sentiment analysis based on multimode semantic extraction and sentiment attention enhancement mechanism. ||| kai yang ||| zhaowei qu ||| xiaoru wang ||| fu li ||| yueli li ||| dongbai jia ||| 
2020 ||| method of multi-feature fusion based on attention mechanism in malicious software detection. ||| yabo wang ||| shuning xu ||| 
2020 ||| dual residual global context attention network for super-resolution. ||| jingjun zhou ||| jingbing li ||| hui li ||| jing liu ||| qianning dai ||| saqib ali nawaz ||| jian shen ||| 
2017 ||| mindfulness based stress reduction improves tactile selective attention bci accuracy. ||| mei lin chen ||| lin yao ||| ning jiang ||| 
2017 ||| detection of attention alteration of bci users based on eeg analysis. ||| susan aliakbaryhosseinabadi ||| ernest nlandu kamavuako ||| ning jiang ||| dario farina ||| natalie mrachacz-kersting ||| 
2017 ||| electroencephalography (eeg)-derived markers to measure components of attention processing. ||| alessandra anzolin ||| laura astolfi ||| jlenia toppi ||| angela riccio ||| floriana pichiorri ||| febo cincotti ||| donatella mattia ||| 
2021 ||| development of long-term prediction algorithm based on component states using bilstm and attention mechanism. ||| hyojin kim ||| seungho jo ||| jaehyun kim ||| gayoung park ||| jonghyun kim ||| 
2021 ||| machine learning based anomaly detection of log files using ensemble learning and self-attention. ||| markus f ||| lt ||| stefan forsstr ||| m ||| tingting zhang ||| 
2018 ||| intelligent fault diagnosis for power transformer based on dga data using support vector machine (svm). ||| arian dhini ||| isti surjandari ||| akhmad faqih ||| benyamin kusumoputro ||| 
2017 ||| a novel nanocrystalline-based current transformer working on saturated region. ||| cleonilson prot ||| sio de souza ||| jailton ferreira moreira ||| yuri percy molina rodriguez ||| 
2021 ||| miniaturized magnetic energy harvester: lightweight and safe transformer design. ||| gabriel gruber ||| markus neumayer ||| thomas bretterklieber ||| alexander siegl ||| richard felsberger ||| 
2020 ||| analysis of ratio and phase errors over time for low power voltage transformers. ||| alessandro mingotti ||| lorenzo peretto ||| andrea nalli ||| marco pau ||| ferdinanda ponci ||| 
2018 ||| home automation system using brain computer interface paradigm based on auditory selection attention. ||| vinay kumar karigar shivappa ||| brian luu ||| marco solis ||| kiran george ||| 
2020 ||| measurement of dynamic voltage variation effect on instrument transformers for power grid applications. ||| gabriella crotti ||| domenico giordano ||| giovanni d'avanzo ||| antonio delle femine ||| daniele gallo ||| carmine landi ||| mario luiso ||| palma sara letizia ||| luca barbieri ||| paolo mazza ||| daniele palladini ||| 
2017 ||| calibration of mv voltage instrument transformer in a wide frequency range. ||| gabriella crotti ||| daniele gallo ||| domenico giordano ||| carmine landi ||| mario luiso ||| mohammad modarres ||| 
2021 ||| measurement of very fast transient overvoltages in current transformers at open air hv substations. ||| daniel slomovitz ||| marcelo brehm ||| rogelio sandler ||| carlos faverio ||| leonardo trigo ||| alejandro santos ||| gonzalo aristoy ||| 
2020 ||| calibration of burdens for instrument transformers. ||| karel draxler ||| michal ulvr ||| renata styblikova ||| jan hlavacek ||| 
2018 ||| study on zno-based gas sensor for detection of acetylene dissolved in transformer oil. ||| gongwei xiao ||| weigen chen ||| chutian yu ||| lingfeng jin ||| 
2020 ||| procedure for ratio error and phase displacement prediction of inductive current transformers at different operating conditions. ||| abbas ghaderi ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2021 ||| analysis and visualization of time-varying harmonics in transformer inrush currents. ||| julio barros ||| matilde de apr ||| iz ||| ram ||| n i. diego ||| 
2021 ||| improving harmonic measurements with instrument transformers: a comparison among two techniques. ||| giovanni d'avanzo ||| marco faifer ||| carmine landi ||| christian laurano ||| palma sara letizia ||| mario luiso ||| roberto ottoboni ||| sergio toscani ||| 
2019 ||| improving the accuracy of current transformers through harmonic distortion compensation. ||| christian laurano ||| sergio toscani ||| michele zanoni ||| 
2018 ||| fault tracing method for high voltage electronic current transformer during its performance test based on the fmea. ||| wang peng ||| jinsong liu ||| hailong bao ||| 
2018 ||| effect of temperature on the accuracy of inductive current transformers. ||| alessandro mingotti ||| gaetano pasini ||| lorenzo peretto ||| roberto tinarelli ||| 
2020 ||| magnetic energy harvesting on overhead high voltage lines: weight optimized transformer design for high power output. ||| markus neumayer ||| thomas bretterklieber ||| gabriel gruber ||| georg brasseur ||| 
2021 ||| esa-net: a network with efficient spatial attention for smoky vehicle detection. ||| jianan zhou ||| shaowei qian ||| zhongzong yan ||| jingbo zhao ||| he wen ||| 
2020 ||| combined impact of voltage transformer and estimation algorithm on harmonic synchrophasors measurements. ||| christian laurano ||| sergio toscani ||| michele zanoni ||| paolo castello ||| carlo muscas ||| paolo attilio pegoraro ||| 
2019 ||| behavioral modeling of an inductive voltage transformer: comparison between x-parameters and simplified volterra approaches. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2018 ||| low power voltage transformer accuracy class effects on the residual voltage measurement. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2019 ||| low cost procedure for frequency characterization of voltage instrument transformers. ||| palma sara letizia ||| gabriella crotti ||| domenico giordano ||| antonio delle femine ||| daniele gallo ||| carmine landi ||| mario luiso ||| 
2021 ||| attention based inception model for robust eeg motor imagery classification. ||| syed umar amin ||| hamdi altaheri ||| ghulam muhammad ||| mansour alsulaiman ||| wadood abdul ||| 
2018 ||| metrological performances of voltage and current instrument transformers in harmonics measurements. ||| antonio cataliotti ||| valentina cosentino ||| gabriella crotti ||| domenico giordano ||| mohammad modarres ||| dario di cara ||| giovanni tin ||| daniele gallo ||| carmine landi ||| mario luiso ||| 
2018 ||| context-aware end-to-end relation extracting from clinical texts with attention-based bi-tree-gru. ||| dehua chen ||| yunying wu ||| jiajin le ||| qiao pan ||| 
2018 ||| attention based residual network for micro-gesture recognition. ||| min peng ||| chongyang wang ||| tong chen ||| 
2017 ||| improving children's gaze prediction via separate facial areas and attention shift cue. ||| songjiang li ||| wen cui ||| jinshi cui ||| li wang ||| ming li ||| hongbin zha ||| 
2020 ||| dual-attention gan for large-pose face frontalization. ||| yu yin ||| songyao jiang ||| joseph p. robinson ||| yun fu ||| 
2020 ||| end-to-end spatial attention network with feature mimicking for head detection. ||| junjie zhang ||| yuntao liu ||| rongchun li ||| yong dou ||| 
2020 ||| understanding consumer attention on mobile devices. ||| natalia efremova ||| navid hajimirza ||| david bassett ||| felipe thomaz ||| 
2021 ||| demystifying attention mechanisms for deepfake detection. ||| abhijit das ||| srijan das ||| antitza dantcheva ||| 
2021 ||| efficient human pose estimation by maximizing fusion and high-level spatial attention. ||| zhiyuan ren ||| yaohai zhou ||| yizhe chen ||| ruisong zhou ||| yayu gao ||| 
2021 ||| high-accuracy rgb-d face recognition via segmentation-aware face depth estimation and mask-guided attention network. ||| meng-tzu chiu ||| hsun-ying cheng ||| chien-yi wang ||| shang-hong lai ||| 
2021 ||| a coarse-to-fine dual attention network for blind face completion. ||| stefan h ||| rmann ||| zhibing xia ||| martin knoche ||| gerhard rigoll ||| 
2020 ||| multimodality pain and related behaviors recognition based on attention learning. ||| van thong huynh ||| hyung-jeong yang ||| gueesang lee ||| soo-hyung kim ||| 
2021 ||| trouspi-net: spatio-temporal attention on parallel atrous convolutions and u-grus for skeletal pedestrian crossing prediction. ||| joseph gesnouin ||| steve pechberti ||| bogdan stanciulcscu ||| fabien moutarde ||| 
2019 ||| discriminative attention-based convolutional neural network for 3d facial expression recognition. ||| kangkang zhu ||| zhengyin du ||| weixin li ||| di huang ||| yunhong wang ||| liming chen ||| 
2017 ||| attention-based template adaptation for face verification. ||| bin dong ||| zhanfu an ||| jian lin ||| weihong deng ||| 
2021 ||| your "attention" deserves attention: a self-diversified multi-channel attention for facial action analysis. ||| xiaotian li ||| zhihua li ||| huiyuan yang ||| geran zhao ||| lijun yin ||| 
2021 ||| headposr: end-to-end trainable head pose estimation using transformer encoders. ||| naina dhingra ||| 
2018 ||| online attention for interpretable conflict estimation in political debates. ||| ruben vereecken ||| stavros petridis ||| yiannis panagakis ||| maja pantic ||| 
2020 ||| learning guided attention masks for facial action unit recognition. ||| nagashri n. lakshminarayana ||| srirangaraj setlur ||| venu govindaraju ||| 
2018 ||| lcanet: end-to-end lipreading with cascaded attention-ctc. ||| kai xu ||| dawei li ||| nick cassimatis ||| xiaolong wang ||| 
2021 ||| replay attention and data augmentation network for 3d dense alignment and face reconstruction. ||| zhiyuan zhou ||| lei li ||| suping wu ||| 
2021 ||| multi-modal learning for au detection based on multi-head fused transformers. ||| xiang zhang ||| lijun yin ||| 
2020 ||| spatio-temporal attention and magnification for classification of parkinson's disease from videos collected via the internet. ||| mohammad rafayet ali ||| javier hernandez ||| e. ray dorsey ||| ehsan hoque ||| daniel j. mcduff ||| 
2020 ||| deep entwined learning head pose and face alignment inside an attentional cascade with doubly-conditional fusion. ||| arnaud dapogny ||| kevin bailly ||| matthieu cord ||| 
2021 ||| sign, attend and tell: spatial attention for sign language recognition. ||| noha a. sarhan ||| simone frintrop ||| 
2021 ||| adversarial attacks on kinship verification using transformer. ||| jiaxuan zhu ||| ming shao ||| chao xia ||| hong pan ||| siyu xia ||| 
2021 ||| two-stream global-guided attention network for facial expression recognition. ||| yaoli wen ||| xiangmin xu ||| fang liu ||| xiaofen xing ||| lin wang ||| 
2019 ||| stacked hourglass network joint with salient region attention refinement for face alignment. ||| junfeng zhang ||| haifeng hu ||| 
2021 ||| cross attentional audio-visual fusion for dimensional emotion recognition. ||| r. gnana praveen ||| eric granger ||| patrick cardinal ||| 
2019 ||| robust remote heart rate estimation from face utilizing spatial-temporal attention. ||| xuesong niu ||| xingyuan zhao ||| hu han ||| abhijit das ||| antitza dantcheva ||| shiguang shan ||| xilin chen ||| 
2021 ||| toward personalized emotion recognition: a face recognition based attention method for facial emotion recognition. ||| mostafa shahabinejad ||| yang wang ||| yuanhao yu ||| jin tang ||| jiani li ||| 
2020 ||| attention fusion for audio-visual person verification using multi-scale features. ||| stefan h ||| rmann ||| abdul moiz ||| martin knoche ||| gerhard rigoll ||| 
2021 ||| skeleton-based action recognition for human-robot interaction using self-attention mechanism. ||| chaitanya bandi ||| ulrike thomas ||| 
2017 ||| a 350uw 2ghz fbar transformer coupled colpitts oscillator with close-in phase noise reduction. ||| jabeom koo ||| keping wang ||| richard c. ruby ||| brian p. otis ||| 
2017 ||| an isolated dc-dc converter with fully integrated magnetic core transformer. ||| tianting zhao ||| yue zhuo ||| baoxing chen ||| 
2020 ||| keyram: a 0.34 uj/decision 18 k decisions/s recurrent attention in-memory processor for keyword spotting. ||| hassan dbouk ||| sujan k. gonugondla ||| charbel sakr ||| naresh r. shanbhag ||| 
2021 ||| an ultra-compact 16-to-45 ghz power amplifier within a single inductor footprint using folded transformer technique. ||| pingda guan ||| haikun jia ||| wei deng ||| zhihua wang ||| baoyong chi ||| 
2020 ||| mixer-first extremely wideband 43-97 ghz rx frontend with broadband quadrature input matching and current mode transformer-based image rejection for massive mimo applications. ||| amr ahmed ||| min-yu huang ||| hua wang ||| 
2017 ||| on-chip transformer design and application to rf and mm-wave front-ends. ||| john r. long ||| 
2020 ||| channel-wise spatial attention with spatiotemporal heterogeneous framework for action recognition. ||| yiying li ||| yulin li ||| yanfei gu ||| 
2019 ||| face presentation attack detection based on exclusivity regularized attention maps. ||| yong wu ||| qijun zhao ||| 
2021 ||| a facial expression recognition system for smart learning based on yolo and vision transformer. ||| xufeng ling ||| jingxin liang ||| dong wang ||| jie yang ||| 
2020 ||| attention-based graph convolution collaborative filtering. ||| xiao han ||| xiaobin xu ||| 
2021 ||| a self-attention based method for facial expression recognition. ||| xufeng ling ||| jingxin liang ||| jie yang ||| 
2021 ||| a new head pose estimation method using vision transformer model. ||| xufeng ling ||| dong wang ||| jie yang ||| 
2020 ||| attention-based joint representation learning network for short text classification. ||| xinyue liu ||| yexuan tang ||| 
2019 ||| an attention-based sequence learning model for scene text recognition with text correction. ||| fang chen ||| guoqiang xiao ||| conghui chen ||| 
2019 ||| e-dam: encoder-decoder with attention mechanism for city-scale taxi trajectory prediction. ||| zhe chen ||| huan liu ||| yuehan wang ||| zongtao duan ||| chuan chen ||| 
2018 ||| eeg-based attention feedback to improve focus in e-learning. ||| chaitanya sethi ||| harsh dabas ||| chirag dua ||| mohit dalawat ||| divyashikha sethia ||| 
2020 ||| multi-attention mechanism for chinese description of videos. ||| hu liu ||| junxiu wu ||| jiabin yuan ||| 
2020 ||| multi-scale attention net for retina blood vessel segmentation. ||| yulin wu ||| anqi liu ||| lei chen ||| dong zhao ||| hongchao zhou ||| qinghe zheng ||| 
2019 ||| an improved model of multi-attention lstm for multimodal sentiment analysis. ||| anna wang ||| baoshan sun ||| rize jin ||| 
2020 ||| translating natural language instructions for behavioral robot indoor navigation with attention-history based attention. ||| pengpeng zhou ||| hao he ||| 
2018 ||| combining gated recurrent unit and attention pooling for sentimental classification. ||| mingbo hong ||| mantao wang ||| lixin luo ||| xuefeng tan ||| dejun zhang ||| yike lao ||| 
2018 ||| an unmanned aerial vehicle detection algorithm based on semantic segmentation and visual attention mechanism. ||| jiaohao zhang ||| qiang zhang ||| chunlei shi ||| 
2020 ||| region-attentioned network with location scoring dynamic-threshold nms for object detection in remote sensing images. ||| wei guo ||| weihong li ||| weiguo gong ||| chaoyue chen ||| 
2018 ||| improvement of embedding channel-wise activation in soft-attention neural image captioning. ||| yanke li ||| 
2020 ||| generative adversarial and self-attention based fine-grained cross-media retrieval. ||| jin hong ||| haonan luo ||| yazhou yao ||| zhenmin tang ||| 
2021 ||| multi-scale spatial-temporal transformer for 3d human pose estimation. ||| yongpeng wu ||| junna gao ||| 
2017 ||| power line communication through distribution transformers in smart grid. ||| okan ozgonenel ||| david w. p. thomas ||| seda ustun ercan ||| 
2017 ||| tree-lstm guided attention pooling of dcnn for semantic sentence modeling. ||| liu chen ||| guangping zeng ||| qingchuan zhang ||| xingyu chen ||| 
2021 ||| anomalous state detection of power transformer based on k-means clustering algorithm. ||| hairong luo ||| bo gao ||| qingping zhang ||| hongwei han ||| junyu guo ||| xuefeng li ||| 
2021 ||| perfecting short-term stock predictions with multi-attention networks in noise-free settings. ||| kwabena sarpong ||| bei hui ||| xue zhou ||| rutherford agbeshi patamia ||| edwin kwadwo tenagyei ||| 
2021 ||| attention transition prediction based on multi-scale spatiotemporal features. ||| wuhua chi ||| peng wang ||| duo chen ||| xinzhu sang ||| kuiru wang ||| binbin yan ||| 
2021 ||| weakly supervised mitosis detection using ellipse label on attention mask r-cnn. ||| xiaoxue liu ||| xinwei li ||| wei zhang ||| peng ran ||| bingyu zhang ||| zhangyong li ||| 
2019 ||| joint visual-textual sentiment analysis based on cross-modality attention mechanism. ||| xuelin zhu ||| biwei cao ||| shuai xu ||| bo liu ||| jiuxin cao ||| 
2020 ||| classroom attention analysis based on multiple euler angles constraint and head pose estimation. ||| xin xu ||| xin teng ||| 
2021 ||| multi-branch and multi-scale attention learning for fine-grained visual categorization. ||| fan zhang ||| meng li ||| guisheng zhai ||| yizhao liu ||| 
2020 ||| a novel attention enhanced dense network for image super-resolution. ||| zhonghan niu ||| yang-hao zhou ||| yu-bin yang ||| jiancong fan ||| 
2020 ||| unsupervised video summarization via attention-driven adversarial learning. ||| evlampios e. apostolidis ||| eleni adamantidou ||| alexandros i. metsai ||| vasileios mezaris ||| ioannis patras ||| 
2022 ||| personalized fashion recommendation using pairwise attention. ||| donnaphat trakulwaranont ||| marc a. kastner ||| shin'ichi satoh ||| 
2021 ||| multi-granularity recurrent attention graph neural network for few-shot learning. ||| xu zhang ||| youjia zhang ||| zuyu zhang ||| 
2022 ||| learning image representation via attribute-aware attention networks for fashion classification. ||| yongquan wan ||| cairong yan ||| bofeng zhang ||| guobing zou ||| 
2019 ||| spatio-temporal attention model based on multi-view for social relation understanding. ||| jinna lv ||| bin wu ||| 
2022 ||| classroom attention estimation method based on mining facial landmarks of students. ||| liyan chen ||| haoran yang ||| kunhong liu ||| 
2019 ||| two-level attention with multi-task learning for facial emotion estimation. ||| xiaohua wang ||| muzi peng ||| lijuan pan ||| min hu ||| chunhua jin ||| fuji ren ||| 
2021 ||| eeg emotion recognition based on channel attention for e-healthcare applications. ||| xu zhang ||| tianzhi du ||| zuyu zhang ||| 
2019 ||| enhancing scene text detection via fused semantic segmentation network with attention. ||| chao liu ||| yuexian zou ||| dongming yang ||| 
2020 ||| attennet: deep attention based retinal disease classification in oct images. ||| jun wu ||| yao zhang ||| jie wang ||| jianchun zhao ||| dayong ding ||| ningjiang chen ||| lingling wang ||| xuan chen ||| chunhui jiang ||| xuan zou ||| xing liu ||| hui xiao ||| yuan tian ||| zongjiang shang ||| kaiwei wang ||| xirong li ||| gang yang ||| jianping fan ||| 
2019 ||| single-stage detector with semantic attention for occluded pedestrian detection. ||| fang wen ||| zehang lin ||| zhenguo yang ||| wenyin liu ||| 
2022 ||| non-uniform attention network for multi-modal sentiment analysis. ||| binqiang wang ||| gang dong ||| yaqian zhao ||| rengang li ||| qichun cao ||| yinyin chao ||| 
2020 ||| an attention based speaker-independent audio-visual deep learning model for speech enhancement. ||| zhongbo sun ||| yannan wang ||| li cao ||| 
2020 ||| image captioning based on visual and semantic attention. ||| haiyang wei ||| zhixin li ||| canlong zhang ||| 
2022 ||| mevit: motion enhanced video transformer for video classification. ||| li li ||| liansheng zhuang ||| 
2019 ||| video summarization with lstm and deep attention models. ||| luis lebron casas ||| eugenia koblents ||| 
2022 ||| multi-scale cross-modal transformer network for rgb-d object detection. ||| zhibin xiao ||| pengwei xie ||| guijin wang ||| 
2021 ||| generative image inpainting by hybrid contextual attention network. ||| zhijiao xiao ||| donglun li ||| 
2021 ||| dense attention-guided network for boundary-aware salient object detection. ||| zhe zhang ||| junhui ma ||| panpan xu ||| wencheng wang ||| 
2021 ||| musicoder: a universal music-acoustic encoder based on transformer. ||| yilun zhao ||| jia guo ||| 
2021 ||| a sentiment similarity-oriented attention model with multi-task learning for text-based emotion recognition. ||| yahui fu ||| lili guo ||| longbiao wang ||| zhilei liu ||| jiaxing liu ||| jianwu dang ||| 
2021 ||| a hybrid music recommendation algorithm based on attention mechanism. ||| weite feng ||| tong li ||| haiyang yu ||| zhen yang ||| 
2021 ||| unsupervised temporal attention summarization model for user created videos. ||| min hu ||| ruimin hu ||| xiaocheng wang ||| rui sheng ||| 
2022 ||| bi-attention modal separation network for multimodal video fusion. ||| pengfei du ||| yali gao ||| xiaoyong li ||| 
2022 ||| time-frequency attention for speech emotion recognition with squeeze-and-excitation blocks. ||| ke liu ||| chen wang ||| jiayue chen ||| jun feng ||| 
2022 ||| one-stage image inpainting with hybrid attention. ||| lulu zhao ||| ling shen ||| richang hong ||| 
2020 ||| adversarial query-by-image video retrieval based on attention mechanism. ||| ruicong xu ||| li niu ||| liqing zhang ||| 
2019 ||| action recognition using visual attention with reinforcement learning. ||| hongyang li ||| jun chen ||| ruimin hu ||| mei yu ||| huafeng chen ||| zengmin xu ||| 
2018 ||| recursive pyramid network with joint attention for cross-media retrieval. ||| yuxin yuan ||| yuxin peng ||| 
2022 ||| conditional context-aware feature alignment for domain adaptive detection transformer. ||| siyuan chen ||| 
2022 ||| sam: self attention mechanism for scene text recognition based on swin transformer. ||| xiang shuai ||| xiao wang ||| wei wang ||| xin yuan ||| xin xu ||| 
2021 ||| confidence-based global attention guided network for image inpainting. ||| zhilin huang ||| chujun qin ||| lei li ||| ruixin liu ||| yuesheng zhu ||| 
2020 ||| compact position-aware attention network for image semantic segmentation. ||| yajun xu ||| zhendong mao ||| peng zhang ||| bin wang ||| 
2022 ||| adaptive speech intelligibility enhancement for far-and-near-end noise environments based on self-attention stargan. ||| dengshi li ||| lanxin zhao ||| jing xiao ||| jiaqi liu ||| duanzheng guan ||| qianrui wang ||| 
2017 ||| large-scale product classification via spatial attention based cnn learning and multi-class regression. ||| shanshan ai ||| caiyan jia ||| zhineng chen ||| 
2021 ||| a multi-modal transformer-based code summarization approach for smart contracts. ||| zhen yang ||| jacky keung ||| xiao yu ||| xiaodong gu ||| zhengyuan wei ||| xiaoxue ma ||| miao zhang ||| 
2021 ||| locating faulty methods with a mixed rnn and attention model. ||| shouliang yang ||| junming cao ||| hushuang zeng ||| beijun shen ||| hao zhong ||| 
2020 ||| exploiting code knowledge graph for bug localization via bi-directional attention. ||| jinglei zhang ||| rui xie ||| wei ye ||| yuhan zhang ||| shikun zhang ||| 
2020 ||| a self-attentional neural architecture for code completion with multi-task learning. ||| fang liu ||| ge li ||| bolin wei ||| xin xia ||| zhiyi fu ||| zhi jin ||| 
2020 ||| attention mechanism in predictive business process monitoring. ||| abdulrahman jalayer ||| mohsen kahani ||| amin beheshti ||| asef pourmasoumi ||| hamid reza motahari-nezhad ||| 
2019 ||| attention bilinear pooling for fine-grained facial expression recognition. ||| liyuan liu ||| lifeng zhang ||| shixiang jia ||| 
2020 ||| robust gan based on attention mechanism. ||| qian wu ||| chunjie cao ||| jianbin mai ||| fangjian tao ||| 
2019 ||| dense inception attention neural network for in-loop filter. ||| xiaoyu xu ||| jian qian ||| li yu ||| hongkui wang ||| xing zeng ||| zhengang li ||| ning wang ||| 
2018 ||| automatic generation of pseudocode with attention seq2seq model. ||| shaofeng xu ||| yun xiong ||| 
2021 ||| fine-grained pseudo-code generation method via code feature extraction and transformer. ||| guang yang ||| yanlin zhou ||| xiang chen ||| chi yu ||| 
2020 ||| software defect prediction and localization with attention-based models and ensemble learning. ||| tianhang zhang ||| qingfeng du ||| jincheng xu ||| jiechu li ||| xiaojun li ||| 
2019 ||| visual attention system based on fuzzy classifier to define priority of traffic signs for intelligent robotic vehicle navigation purposes. ||| diego renan bruno ||| fernando santos os ||| rio ||| 
2018 ||| attention-based view of online information dissemination. ||| ruochen liao ||| 
2021 ||| urltran: improving phishing url detection using transformers. ||| pranav maneriker ||| jack w. stokes ||| edir garcia lazo ||| diana carutasu ||| farid tajaddodianfar ||| arun gururajan ||| 
2021 ||| towards transformer-based real-time object detection at the edge: a benchmarking study. ||| colin samplawski ||| benjamin m. marlin ||| 
2019 ||| automatic fault detection in a cascaded transformer multilevel inverter using pattern recognition techniques. ||| diego f. salazar-d'antonio ||| nohora meneses-casas ||| manuel guillermo forero ||| oswaldo l ||| pez-santos ||| 
2019 ||| frame by frame pain estimation using locally spatial attention learning. ||| jun yu ||| toru kurihara ||| shu zhan ||| 
2021 ||| trace: early detection of chronic kidney disease onset with transformer-enhanced feature embedding. ||| yu wang ||| ziqiao guan ||| wei hou ||| fusheng wang ||| 
2020 ||| the transformers for polystores - the next frontier for polystore research. ||| edmon begoli ||| sudarshan srinivasan ||| maria mahbub ||| 
2021 ||| madc: multi-scale attention-based deep clustering for workload prediction. ||| jiaming huang ||| chuming xiao ||| weigang wu ||| ye yin ||| hongli chang ||| 
2020 ||| feature envy detection based on bi-lstm with self-attention mechanism. ||| hongze wang ||| jing liu ||| jiexiang kang ||| wei yin ||| haiying sun ||| hui wang ||| 
2020 ||| an ego network embedding model via neighbors sampling and self-attention mechanism. ||| ziyu guo ||| shijun liu ||| li pan ||| qiang he ||| 
2018 ||| hierarchical attention based recurrent neural network framework for mobile moba game recommender systems. ||| qiongjie yao ||| xiaofei liao ||| hai jin ||| 
2019 ||| automatic text summarization based on transformer and switchable normalization. ||| tao luo ||| kun guo ||| hong guo ||| 
2019 ||| an efficient machine reading comprehension method based on attention mechanism. ||| wenzhen jin ||| guocai yang ||| hong zhu ||| 
2021 ||| adaptive attention encoder for attribute graph embedding. ||| ziqiang weng ||| weiyu zhang ||| zhongxiu xia ||| 
2019 ||| counting attention based on classification confidence for visual question answering. ||| mingqin chen ||| yilei wang ||| shan chen ||| yingjie wu ||| 
2021 ||| self-attention based automated vulnerability detection with effective data representation. ||| tongshuai wu ||| liwei chen ||| gewangzi du ||| chenguang zhu ||| gang shi ||| 
2020 ||| avdhram: automated vulnerability detection based on hierarchical representation and attention mechanism. ||| wenyan an ||| liwei chen ||| jinxin wang ||| gewangzi du ||| gang shi ||| dan meng ||| 
2019 ||| an attention-based recommendation algorithm. ||| yan chu ||| shuhao qi ||| yue yang ||| chenqi shan ||| lina wang ||| zhengkui wang ||| 
2020 ||| answer graph-based interactive attention network for question answering over knowledge base. ||| lu ma ||| peng zhang ||| dan luo ||| meilin zhou ||| qi liang ||| bin wang ||| 
2021 ||| attention-based encoder-decoder recurrent neural networks for http payload anomaly detection. ||| shang wu ||| yijie wang ||| 
2019 ||| attention alignment by linear space projection for video features extraction. ||| shenqiang yuan ||| mei xue ||| he yi ||| zhang jin ||| 
2019 ||| resfpa-gan: text-to-image synthesis with generative adversarial network based on residual block feature pyramid attention. ||| jingcong sun ||| yimin zhou ||| bin zhang ||| 
2019 ||| recognising human-object interactions using attention-based lstms. ||| muna almushyti ||| frederick w. b. li ||| 
2019 ||| 5g-transformer service orchestrator: design, implementation, and evaluation. ||| josep mangues-bafalluy ||| jorge baranda ||| i ||| aki pascual ||| ricardo mart ||| nez ||| luca vettori ||| giada landi ||| arturo zurita ||| david salama ||| kiril antevski ||| jorge mart ||| n-p ||| rez ||| dmitriy andrushko ||| konstantin tomakh ||| barbara martini ||| xi li ||| josep x. salvat ||| 
2021 ||| interpreting deep learning based cerebral palsy prediction with channel attention. ||| manli zhu ||| qianhui men ||| edmond s. l. ho ||| howard leung ||| hubert p. h. shum ||| 
2021 ||| multimodal breast lesion classification using cross-attention deep networks. ||| hung q. vo ||| pengyu yuan ||| tiancheng he ||| stephen t. c. wong ||| hien van nguyen ||| 
2018 ||| deep learning from electronic medical records using attention-based cross-modal convolutional neural networks. ||| bing leung patrick cheung ||| deborah dahl ||| 
2021 ||| afa-rn: an abnormal feature attention relation network for multi-class disease classification in gastrointestinal endoscopic images. ||| qian zhao ||| wenming yang ||| qingmin liao ||| 
2018 ||| a hierarchical lstm model with attention for modeling eeg non-stationarity for human decision prediction. ||| md musaddaqul hasib ||| tapsya nayak ||| yufei huang ||| 
2018 ||| a novel channel-aware attention framework for multi-channel eeg seizure detection via multi-view deep learning. ||| ye yuan ||| guangxu xun ||| fenglong ma ||| qiuling suo ||| hongfei xue ||| kebin jia ||| aidong zhang ||| 
2019 ||| ecgnet: learning where to attend for detection of atrial fibrillation with deep visual attention. ||| sajad mousavi ||| fatemeh afghah ||| abolfazl razi ||| u. rajendra acharya ||| 
2021 ||| multi-module recurrent convolutional neural network with transformer encoder for ecg arrhythmia classification. ||| minh duc le ||| vidhiwar singh rathour ||| quang sang truong ||| quan mai ||| patel brijesh ||| ngan le ||| 
2020 ||| spontaneous expression recognition based on visual attention mechanism and co-salient features. ||| ling zhang ||| qiumin ji ||| wenchao jiang ||| dongjun ning ||| 
2018 ||| privacy-preserving network bmi decoding of covert spatial attention. ||| takayuki nakachi ||| hiroyuki ishihara ||| hitoshi kiya ||| 
2021 ||| speech emotion recognition using xgboost and cnn blstm with attention. ||| jingru he ||| liyong ren ||| 
2018 ||| generating expert's review from the crowds': integrating a multi-attention mechanism with encoder-decoder framework. ||| xiaofei ding ||| wenjun jiang ||| jiawei he ||| 
2019 ||| sentiment analysis based on attention mechanisms and bi-directional lstm fusion model. ||| mei wang ||| yangyang zhu ||| shulin liu ||| chunfeng song ||| zheng wang ||| pai wang ||| xuebin qin ||| 
2019 ||| attention-based adaptive sampling for continuous emg data streams. ||| giovanni schiboni ||| juan carlos suarez ||| rui zhang ||| oliver amft ||| 
2018 ||| embedding-level attention and multi-scale convolutional neural networks for behaviour modelling. ||| aitor almeida ||| gorka azkune ||| aritz bilbao ||| 
2019 ||| visual attention-based object detection in cluttered environments. ||| eduardo manuel silva machado ||| ivan carrillo ||| miguel collado ||| liming chen ||| 
2019 ||| a driving attention detection method based on head pose. ||| ya li ||| jiying li ||| xinlong jiang ||| chenlong gao ||| teng zhang ||| 
2019 ||| msahta: mixed spatial attention and hierarchical temporal aggregation for action recognition. ||| jinyuan feng ||| dan yang ||| yongxin ge ||| xiaolei qin ||| yida chen ||| yuangan wang ||| 
2019 ||| a novel relationship extraction scheme based on negative feedback attention. ||| weidong li ||| jing liu ||| jun tie ||| zimao li ||| jun qin ||| lu liu ||| 
2017 ||| dnn-based approach for identification of the level of attention of the tv-viewers using iot network. ||| hamdi amroun ||| m'hamed hamy temkit ||| mehdi ammi ||| 
2019 ||| lstm based semi-supervised attention framework for sentiment analysis. ||| hanxue ji ||| wenge rong ||| jingshuang liu ||| yuanxin ouyang ||| zhang xiong ||| 
2017 ||| a votable concept mapping approach to promoting students' attentional behavior: based on the evidence of mining sequential behavioral patterns. ||| jerry chih-yuan sun ||| gwo-jen hwang ||| yu-yan lin ||| shih-jou yu ||| liu-cheng pan ||| ariel yu-zhen chen ||| 
2020 ||| deephop on edge: hop-by-hop routing bydistributed learning with semantic attention. ||| bo he ||| jingyu wang ||| qi qi ||| haifeng sun ||| zirui zhuang ||| cong liu ||| jianxin liao ||| 
2020 ||| deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence. ||| ashesh chattopadhyay ||| mustafa mustafa ||| pedram hassanzadeh ||| karthik kashinath ||| 
2022 ||| early rumor detection based on data augmentation and pre-training transformer. ||| yanjun hu ||| xinyi ju ||| zhousheng ye ||| sulaiman khan ||| chengwu yuan ||| qiran lai ||| junqiang liu ||| 
2021 ||| video anomaly detection method based on future frame prediction and attention mechanism. ||| chenxu wang ||| yanxin yao ||| han yao ||| 
2020 ||| improve accuracy of speech emotion recognition with attention head fusion. ||| mingke xu ||| fan zhang ||| samee u. khan ||| 
2020 ||| evaluating human-machine translation with attention mechanisms for industry 4.0 environment sql-based systems. ||| silvan ferreira ||| gustavo bezerra paz leitao ||| ivanovitch silva ||| allan de m. martins ||| paolo ferrari ||| 
2019 ||| use of comtrade fault current data to test inductive current transformers. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| junhao zhang ||| 
2019 ||| graph-based attention networks for aspect level sentiment analysis. ||| junjie chen ||| hongxu hou ||| yatu ji ||| jing gao ||| tiangang bai ||| 
2019 ||| sentiment-aware short text classification based on convolutional neural network and attention. ||| zeyu chen ||| yan tang ||| zuowei zhang ||| chengyang zhang ||| luwei wang ||| 
2021 ||| transformer based multi-output regression learning for wastewater treatment. ||| zi-lan huang ||| luo-yi zhang ||| yi zhang ||| shu-wei qian ||| chong-jun wang ||| 
2020 ||| cfgat: a coarse-to-fine graph attention network for semi-supervised node classification. ||| dongmei cui ||| fusheng jin ||| rong-hua li ||| guoren wang ||| 
2021 ||| fashion landmark detection via deep residual spatial attention network. ||| rui wang ||| jun feng ||| qirong bu ||| 
2020 ||| enhanced soft attention mechanism with an inception-like module for image captioning. ||| zheng lian ||| haichang li ||| rui wang ||| xiaohui hu ||| 
2017 ||| an attention mechanism for neural answer selection using a combined global and local view. ||| yoram bachrach ||| andrej zukov gregoric ||| sam coope ||| ed tovell ||| bogdan maksak ||| jos |||  rodr ||| guez ||| conan mcmurtie ||| mahyar bordbar ||| 
2020 ||| sentiment-aware transformer using joint training. ||| hui huang ||| yueyuan jin ||| ruonan rao ||| 
2021 ||| a supervisory mask attentional network for person re-identification in uniform dress scenes. ||| bo li ||| ling bai ||| yang wang ||| zhe wu ||| tong lin ||| 
2021 ||| query-based summarization using reinforcement learning and transformer model. ||| yllias chali ||| asif mahmud ||| 
2020 ||| solving open shop scheduling problem via graph attention neural network. ||| jing li ||| xingye dong ||| kai zhang ||| sheng han ||| 
2019 ||| graph attention networks for neural social recommendation. ||| nan mu ||| daren zha ||| yuanye he ||| zhihao tang ||| 
2019 ||| sparse high-level attention networks for person re-identification. ||| sheng xie ||| canlong zhang ||| zhixin li ||| zhi-wen wang ||| 
2021 ||| kgat-sr: knowledge-enhanced graph attention network for session-based recommendation. ||| qianqian zhang ||| zhuoming xu ||| hanlin liu ||| yan tang ||| 
2021 ||| cvt-assd: convolutional vision-transformer based attentive single shot multibox detector. ||| weiqiang jin ||| hang yu ||| xiangfeng luo ||| 
2021 ||| expert-guided policy optimization by latent space planning with attention. ||| shiqing gao ||| fufei yao ||| yaoru sun ||| haibo shi ||| 
2021 ||| enhanced-memory transformer for coherent paragraph video captioning. ||| leonardo vilela cardoso ||| silvio jamil f. guimar ||| es ||| zenilton k. g. do patroc ||| nio ||| 
2019 ||| math expression image retrieval via attention-based framework. ||| caili wu ||| zhao zhou ||| hao ye ||| jing yang ||| liang he ||| 
2019 ||| aerns: attention-based entity region networks for multi-grained named entity recognition. ||| jianghai dai ||| chong feng ||| xuefeng bai ||| jinming dai ||| huanhuan zhang ||| 
2021 ||| global attention augmentation ghost module: more features from lightweight global attention extraction. ||| hongru liu ||| zhezhou yu ||| xiaowei xu ||| yuanyuan guan ||| boxiang zhang ||| wenhui li ||| 
2020 ||| st-mgat: spatial-temporal multi-head graph attention networks for traffic forecasting. ||| kelang tian ||| jingjie guo ||| kejiang ye ||| cheng-zhong xu ||| 
2021 ||| quantum-inspired hierarchical attention mechanism for question answering. ||| peng guo ||| 
2021 ||| hierarchical triplet attention pooling for graph classification. ||| liande bi ||| xin sun ||| fei zhou ||| junyu dong ||| 
2019 ||| targeted sentiment classification with knowledge powered attention network. ||| ximo bian ||| chong feng ||| arshad ahmad ||| jinming dai ||| guifen zhao ||| 
2021 ||| multi-task learning with attention : constructing auxiliary tasks for learning to learn. ||| benying li ||| aimei dong ||| 
2021 ||| user-guided image inpatinting with transformer. ||| jingjun qiu ||| yan gao ||| 
2020 ||| graph attention auto-encoders. ||| amin salehi ||| hasan davulcu ||| 
2019 ||| distant-supervised relation extraction with hierarchical attention based on knowledge graph. ||| hong yao ||| lijun dong ||| shiqi zhen ||| xiaojun kang ||| xinchuan li ||| qingzhong liang ||| 
2021 ||| zero or few shot knowledge graph completions by text enhancement with multi-grained attention. ||| zhijuan du ||| 
2019 ||| integrating an attention mechanism and deep neural network for detection of dga domain names. ||| fangli ren ||| zhengwei jiang ||| jian liu ||| 
2020 ||| position and channel attention for image inpainting by semantic structure. ||| jingjun qiu ||| yan gao ||| 
2017 ||| neural named entity recognition using a self-attention mechanism. ||| andrej zukov gregoric ||| yoram bachrach ||| pasha minkovsky ||| sam coope ||| bogdan maksak ||| 
2021 ||| utilizing external knowledge with multi-granularity attention for review reading comprehension. ||| zhenda hu ||| yinglin wang ||| 
2020 ||| bi-directional self-attention with relative positional encoding for video summarization. ||| jingxu lin ||| sheng-hua zhong ||| 
2021 ||| fragan-vsr: frame-recurrent attention generative adversarial network for video super-resolution. ||| yuzhen zhang ||| guanqun liu ||| yanyan zhao ||| daren zha ||| xin wang ||| lin zhao ||| lei wang ||| 
2019 ||| ecpnet: an efficient attention-based convolution network with pseudo-3d block for human action recognition. ||| xiuping bao ||| jiabin yuan ||| bei chen ||| 
2018 ||| multivariate attention network for image captioning. ||| weixuan wang ||| zhihong chen ||| haifeng hu ||| 
2020 ||| sequential view synthesis with transformer. ||| phong nguyen-ha ||| lam huynh ||| esa rahtu ||| janne heikkil ||| 
2018 ||| skeleton transformer networks: 3d human pose and skinned mesh from single rgb image. ||| yusuke yoshiyasu ||| ryusuke sagawa ||| ko ayusawa ||| akihiko murai ||| 
2020 ||| transforming multi-concept attention into video summarization. ||| yen-ting liu ||| yu-jhe li ||| yu-chiang frank wang ||| 
2020 ||| attention-aware feature aggregation for real-time stereo matching on edge devices. ||| jia-ren chang ||| pei-chun chang ||| yong-sheng chen ||| 
2018 ||| coarse-to-fine: a rnn-based hierarchical attention model for vehicle re-identification. ||| xiu-shen wei ||| chen-lin zhang ||| lingqiao liu ||| chunhua shen ||| jianxin wu ||| 
2020 ||| channel recurrent attention networks for video pedestrian retrieval. ||| pengfei fang ||| pan ji ||| jieming zhou ||| lars petersson ||| mehrtash harandi ||| 
2018 ||| large scale scene text verification with guided attention. ||| dafang he ||| yeqing li ||| alexander n. gorban ||| derrall heath ||| julian ibarz ||| qian yu ||| daniel kifer ||| c. lee giles ||| 
2020 ||| rgb-d co-attention network for semantic segmentation. ||| hao zhou ||| lu qi ||| zhaoliang wan ||| hai huang ||| xu yang ||| 
2018 ||| paying attention to style: recognizing photo styles with convolutional attentional units. ||| john see ||| lai-kuan wong ||| magzhan kairanbay ||| 
2020 ||| spatial temporal attention graph convolutional networks with mechanics-stream for skeleton-based action recognition. ||| katsutoshi shiraki ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2018 ||| automatic graphics program generation using attention-based hierarchical decoder. ||| zhihao zhu ||| zhan xue ||| zejian yuan ||| 
2018 ||| deep attention-based classification network for robust depth prediction. ||| ruibo li ||| ke xian ||| chunhua shen ||| zhiguo cao ||| hao lu ||| lingxiao hang ||| 
2020 ||| bi-directional attention for joint instance and semantic segmentation in point clouds. ||| guangnan wu ||| zhiyi pan ||| peng jiang ||| changhe tu ||| 
2018 ||| a2a: attention to attention reasoning for movie question answering. ||| chao-ning liu ||| ding-jie chen ||| hwann-tzong chen ||| tyng-luh liu ||| 
2020 ||| second order enhanced multi-glimpse attention in visual question answering. ||| qiang sun ||| binghui xie ||| yanwei fu ||| 
2020 ||| hierarchical x-ray report generation via pathology tags and multi head attention. ||| preethi srinivasan ||| daksh thapar ||| arnav bhavsar ||| aditya nigam ||| 
2018 ||| gated hierarchical attention for image captioning. ||| qingzhong wang ||| antoni b. chan ||| 
2020 ||| epsnet: efficient panoptic segmentation network with cross-layer attention fusion. ||| chia-yuan chang ||| shuo-en chang ||| pei-yung hsiao ||| li-chen fu ||| 
2020 ||| audiovisual transformer with instance attention for audio-visual event localization. ||| yan-bo lin ||| yu-chiang frank wang ||| 
2020 ||| frequency attention network: blind noise removal for real images. ||| hongcheng mo ||| jianfei jiang ||| qin wang ||| dong yin ||| pengyu dong ||| jingjun tian ||| 
2020 ||| rotation axis focused attention network (rafa-net) for estimating head pose. ||| ardhendu behera ||| zachary wharton ||| pradeep hewage ||| swagat kumar ||| 
2018 ||| video-based person re-identification via 3d convolutional networks and non-local attention. ||| xingyu liao ||| lingxiao he ||| zhouwang yang ||| chi zhang ||| 
2020 ||| decoupled spatial-temporal attention network for skeleton-based action-gesture recognition. ||| lei shi ||| yifan zhang ||| jian cheng ||| hanqing lu ||| 
2020 ||| attention-based fine-grained classification of bone marrow cells. ||| weining wang ||| peirong guo ||| lemin li ||| yan tan ||| hongxia shi ||| yan wei ||| xiangmin xu ||| 
2020 ||| in-sample contrastive learning and consistent attention for weakly supervised object localization. ||| minsong ki ||| youngjung uh ||| wonyoung lee ||| hyeran byun ||| 
2020 ||| local context attention for salient object segmentation. ||| jing tan ||| pengfei xiong ||| zhengyi lv ||| kuntao xiao ||| yuwen he ||| 
2018 ||| summarizing videos with attention. ||| jiri fajtl ||| hajar sadeghi sokeh ||| vasileios argyriou ||| dorothy monekosso ||| paolo remagnino ||| 
2020 ||| part-aware attention network for person re-identification. ||| wangmeng xiang ||| jianqiang huang ||| xian-sheng hua ||| lei zhang ||| 
2020 ||| afn: attentional feedback network based 3d terrain super-resolution. ||| ashish kubade ||| diptiben patel ||| avinash sharma ||| k. s. rajan ||| 
2020 ||| modular graph attention network for complex visual relational reasoning. ||| yihan zheng ||| zhiquan wen ||| mingkui tan ||| runhao zeng ||| qi chen ||| yaowei wang ||| qi wu ||| 
2018 ||| semantic aware attention based deep object co-segmentation. ||| hong chen ||| yifei huang ||| hideki nakayama ||| 
2020 ||| image captioning through image transformer. ||| sen he ||| wentong liao ||| hamed r. tavakoli ||| michael ying yang ||| bodo rosenhahn ||| nicolas pugeault ||| 
2020 ||| spatial and channel attention modulated network for medical image segmentation. ||| wenhao fang ||| xian-hua han ||| 
2018 ||| predicting driver attention in critical situations. ||| ye xia ||| danqing zhang ||| jinkyu kim ||| ken nakayama ||| karl zipser ||| david whitney ||| 
2020 ||| parallel-connected residual channel attention network for remote sensing image super-resolution. ||| yinhao li ||| yutaro iwamoto ||| lanfen lin ||| yen-wei chen ||| 
2018 ||| multilevel collaborative attention network for person search. ||| wenbo li ||| ze chen ||| zhenyong fu ||| hongtao lu ||| 
2020 ||| accurate and efficient single image super-resolution with matrix channel attention network. ||| hailong ma ||| xiangxiang chu ||| bo zhang ||| 
2020 ||| class: cross-level attention and supervision for salient objects detection. ||| lv tang ||| bo li ||| 
2020 ||| multi-label x-ray imagery classification via bottom-up attention and meta fusion. ||| benyi hu ||| chi zhang ||| le wang ||| qilin zhang ||| yuehu liu ||| 
2018 ||| attentionmask: attentive, efficient object proposal generation focusing on small objects. ||| christian wilms ||| simone frintrop ||| 
2018 ||| transformer design for 77-ghz down-converter in 28-nm fd-soi cmos technology. ||| andrea cavarra ||| claudio nocera ||| giuseppe papotto ||| egidio ragonese ||| giuseppe palmisano ||| 
2018 ||| an attention-based recurrent neural networks framework for health data analysis. ||| qiuling suo ||| fenglong ma ||| giovanni canino ||| jing gao ||| aidong zhang ||| agostino gnasso ||| giuseppe tradigo ||| pierangelo veltri ||| 
2021 ||| assessing internal and external attention in ar using brain computer interfaces: a pilot study. ||| nataliya kosmyna ||| qiuxuan wu ||| chi-yun hu ||| yujie wang ||| cassandra scheirer ||| pattie maes ||| 
2017 ||| modeling and detecting student attention and interest level using wearable computers. ||| ziwei zhu ||| sebastian ober ||| roozbeh jafari ||| 
2021 ||| dual attention based network with hierarchical convlstm for video object segmentation. ||| zongji zhao ||| sanyuan zhao ||| 
2021 ||| lf-magnet: learning mutual attention guidance of sub-aperture images for light field image super-resolution. ||| zijian wang ||| yao lu ||| yani zhang ||| haowei lu ||| shunzhou wang ||| binglu wang ||| 
2021 ||| attention-based node-edge graph convolutional networks for identification of autism spectrum disorder using multi-modal mri data. ||| yuzhong chen ||| jiadong yan ||| mingxin jiang ||| zhongbo zhao ||| weihua zhao ||| rong zhang ||| keith m. kendrick ||| xi jiang ||| 
2019 ||| spatial-temporal fusion network with residual learning and attention mechanism: a benchmark for video-based group re-id. ||| qiling xu ||| hua yang ||| lin chen ||| 
2018 ||| attention forest for semantic segmentation. ||| jingbo wang ||| yajie xing ||| gang zeng ||| 
2021 ||| saliencybert: recurrent attention network for target-oriented multimodal sentiment classification. ||| jiawei wang ||| zhe liu ||| victor s. sheng ||| yuqing song ||| chenjian qiu ||| 
2019 ||| an automated method with attention network for cervical cancer scanning. ||| lijuan duan ||| fan xu ||| yuanhua qiao ||| di zhao ||| tongtong xu ||| chunli wu ||| 
2021 ||| attention template update model for siamese tracker. ||| fengshou jia ||| zhao tang ||| yun gao ||| 
2020 ||| h-at: hybrid attention transfer for knowledge distillation. ||| yan qu ||| weihong deng ||| jiani hu ||| 
2019 ||| learning attention regularization correlation filter for visual tracking. ||| zhuling qiu ||| yufei zha ||| peng zhu ||| fei zhang ||| 
2021 ||| ipe transformer for depth completion with input-aware positional embeddings. ||| bocen li ||| guozhen li ||| haiting wang ||| lijun wang ||| zhenfei gong ||| xiaohua zhang ||| huchuan lu ||| 
2021 ||| joint attention mechanism for unsupervised video object segmentation. ||| rui yao ||| xin xu ||| yong zhou ||| jiaqi zhao ||| liang fang ||| 
2021 ||| image tampering localization using unified two-stream features enhanced with channel and spatial attention. ||| haodong li ||| xiaoming chen ||| peiyu zhuang ||| bin li ||| 
2021 ||| cetransformer: casual effect estimation via transformer based representation learning. ||| zhenyu guo ||| shuai zheng ||| zhizhe liu ||| kun yan ||| zhenfeng zhu ||| 
2021 ||| idanet: iterative d-linknets with attention for road extraction from high-resolution satellite imagery. ||| benzhu xu ||| shengshuai bao ||| liping zheng ||| gaofeng zhang ||| wenming wu ||| 
2020 ||| pavement crack detection using attention u-net with multiple sources. ||| junfeng wang ||| fan liu ||| wenjie yang ||| guoyan xu ||| tao zhang ||| 
2020 ||| weakly supervised pedestrian attribute recognition with attention in latent space. ||| mingjun sun ||| hua yang ||| guangtao zhai ||| 
2021 ||| early diagnosis of alzheimer's disease using 3d residual attention network based on hippocampal multi-indices feature fusion. ||| yiyu zhang ||| qiang zheng ||| kun zhao ||| honglun li ||| chaoqing ma ||| shuanhu wu ||| xiangrong tong ||| 
2021 ||| insight on attention modules for skeleton-based action recognition. ||| quanyan jiang ||| xiaojun wu ||| josef kittler ||| 
2021 ||| group re-identification based on single feature attention learning network (sfaln). ||| liu xuehai ||| lisha yu ||| jianhuang lai ||| 
2020 ||| attention-based network for semantic image segmentation via adversarial learning. ||| xinnan ding ||| ying tian ||| chenhui wang ||| yilong li ||| haodong yang ||| kejun wang ||| 
2019 ||| shellfish detection based on fusion attention mechanism in end-to-end network. ||| guangyao li ||| zhenbo li ||| chuyue zhang ||| yaodong li ||| jun yue ||| 
2021 ||| feature enhancement and multi-scale cross-modal attention for rgb-d salient object detection. ||| xin wan ||| gang yang ||| boyi zhou ||| chang liu ||| hangxu wang ||| yutao wang ||| 
2021 ||| magan: multi-attention generative adversarial networks for text-to-image generation. ||| xibin jia ||| qing mi ||| qi dai ||| 
2020 ||| mhasiam: mixed high-order attention siamese network for real-time visual tracking. ||| lei pu ||| xinxi feng ||| zhiqiang hou ||| wangsheng yu ||| yufei zha ||| zhiqiang jiao ||| 
2021 ||| interactive attention sampling network for clinical skin disease image classification. ||| xulin chen ||| dong li ||| yun zhang ||| muwei jian ||| 
2018 ||| deep word association: a flexible chinese word association method with iterative attention mechanism. ||| yaoxiong huang ||| zecheng xie ||| manfei liu ||| shuaitao zhang ||| lianwen jin ||| 
2020 ||| residual attention siameserpn for visual tracking. ||| xu cheng ||| enlu li ||| zhangjie fu ||| 
2019 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| zhichao zhang ||| shugong xu ||| tianhao qiao ||| shunqing zhang ||| shan cao ||| 
2020 ||| lsam: local spatial attention module. ||| miao-miao lv ||| si-bao chen ||| bin luo ||| 
2019 ||| a simple and robust attentional encoder-decoder model for license plate recognition. ||| linjiang zhang ||| peng wang ||| fan dang ||| shaojie zhang ||| 
2021 ||| multi-directional attention network for segmentation of pediatric echocardiographic. ||| zhuo xiang ||| cheng zhao ||| libao guo ||| yali qiu ||| yun zhu ||| peng yang ||| wei xiong ||| mingzhu li ||| minsi chen ||| tianfu wang ||| baiying lei ||| 
2021 ||| relational attention with textual enhanced transformer for image captioning. ||| lifei song ||| yiwen shi ||| xinyu xiao ||| chunxia zhang ||| shiming xiang ||| 
2021 ||| hpcreg-net: unsupervised u-net integrating dilated convolution and residual attention for hippocampus registration. ||| hu yu ||| qiang zheng ||| kun zhao ||| honglun li ||| chaoqing ma ||| shuanhu wu ||| xiangrong tong ||| 
2021 ||| a guided attention 4d convolutional neural network for modeling spatio-temporal patterns of functional brain networks. ||| jiadong yan ||| yu zhao ||| mingxin jiang ||| shu zhang ||| tuo zhang ||| shimin yang ||| yuzhong chen ||| zhongbo zhao ||| zhibin he ||| benjamin becker ||| tianming liu ||| keith m. kendrick ||| xi jiang ||| 
2018 ||| attention enhanced convnet-rnn for chinese vehicle license plate recognition. ||| shiming duan ||| wei hu ||| ruirui li ||| wei li ||| shihao sun ||| 
2020 ||| an attention enhanced graph convolutional network for semantic segmentation. ||| ao chen ||| yue zhou ||| 
2018 ||| facial expression recognition based on region-wise attention and geometry difference. ||| heran du ||| huicheng zheng ||| mingjing yu ||| 
2020 ||| inception parallel attention network for small object detection in remote sensing images. ||| shuojin yang ||| liang tian ||| bingyin zhou ||| dong chen ||| dan zhang ||| zhuangnan xu ||| wei guo ||| jing liu ||| 
2021 ||| non-significant information enhancement based attention network for face anti-spoofing. ||| yangwei dong ||| jianjun qian ||| jian yang ||| 
2021 ||| facial expression recognition based on multi-scale feature fusion convolutional neural network and attention mechanism. ||| yana wu ||| kebin jia ||| zhonghua sun ||| 
2019 ||| adsrnet: attention-based densely connected network for image super-resolution. ||| weiqi li ||| yao lu ||| xuebo wang ||| xiaozhen chen ||| zijian wang ||| 
2020 ||| multi-cue and temporal attention for person recognition in videos. ||| wenzhe wang ||| bin wu ||| fangtao li ||| zihe liu ||| 
2021 ||| pyramid self-attention for semantic segmentation. ||| jiyang qi ||| xinggang wang ||| yao hu ||| xu tang ||| wenyu liu ||| 
2018 ||| multi-attention guided activation propagation in cnns. ||| xiangteng he ||| yuxin peng ||| 
2020 ||| lightweight image super-resolution with local attention enhancement. ||| yunchu yang ||| xiumei wang ||| xinbo gao ||| zheng hui ||| 
2019 ||| pointnet-based channel attention vlad network. ||| rongrong fan ||| hui shuai ||| qingshan liu ||| 
2021 ||| relation-guided actor attention for group activity recognition. ||| lifang wu ||| qi wang ||| zeyu li ||| ye xiang ||| xianglong lang ||| 
2020 ||| cross-view image synthesis with deformable convolution and attention mechanism. ||| hao ding ||| songsong wu ||| hao tang ||| fei wu ||| guangwei gao ||| xiao-yuan jing ||| 
2020 ||| principal semantic feature analysis with covariance attention. ||| yuliang chen ||| yazhou liu ||| pongsak lasang ||| quansen sun ||| 
2021 ||| tmd-fs: improving few-shot object detection with transformer multi-modal directing. ||| ying yuan ||| lijuan duan ||| wenjian wang ||| qing en ||| 
2018 ||| domain attention model for domain generalization in object detection. ||| wei-xiong he ||| huicheng zheng ||| jianhuang lai ||| 
2021 ||| multi-level residual attention network for speckle suppression. ||| yu lei ||| shuaiqi liu ||| luyao zhang ||| ling zhao ||| jie zhao ||| 
2019 ||| multi-scale spatial-temporal attention for action recognition. ||| qing zhang ||| hongping yan ||| lingfeng wang ||| 
2021 ||| cross-modality attention method for medical image enhancement. ||| zebin hu ||| hao liu ||| zhendong li ||| zekuan yu ||| 
2019 ||| attention-based label consistency for semi-supervised deep learning. ||| jiaming chen ||| meng yang ||| 
2019 ||| local and global feature learning for subtle facial expression recognition from attention perspective. ||| shaocong wang ||| yuan yuan ||| yachuang feng ||| 
2018 ||| attention-based convolutional networks for ship detection in high-resolution remote sensing images. ||| xiaofeng ma ||| wenyuan li ||| zhenwei shi ||| 
2021 ||| attention guided spatio-temporal artifacts extraction for deepfake detection. ||| zhibing wang ||| xin li ||| rongrong ni ||| yao zhao ||| 
2019 ||| facial expression recognition: disentangling expression based on self-attention conditional generative adversarial nets. ||| haohao li ||| qiong liu ||| xiaoming wei ||| zhenhua chai ||| wenbai chen ||| 
2018 ||| prohibited item detection in airport x-ray security images via attention mechanism based cnn. ||| maoshu xu ||| haigang zhang ||| jinfeng yang ||| 
2021 |||  13.3 db gain sub-6 ghz to 28 ghz transformer-coupled low-voltage upconversion mixer for 5g applications. ||| oner hanay ||| renato negra ||| 
2018 ||| a 15mw -105dbm image-sparse-sliding-if receiver with transformer-based on-chip q-enhanced rf matching network for a 113db-link-budget ble 5.0 trx. ||| tuan thanh ta ||| yosuke ogasawara ||| tong wang ||| masayoshi oshiro ||| naotaka koide ||| akihide sai ||| takashi tokairin ||| 
2019 ||| a 78 fs rms jitter injection-locked clock multiplier using transformer-based ultra-low-power vco. ||| zheng sun ||| hanli liu ||| dingxin xu ||| hongye huang ||| bangan liu ||| zheng li ||| jian pang ||| teruki someya ||| atsushi shirane ||| 
2017 ||| gender, age, colour, position and stress: how they influence attention at workplace? ||| vidas raudonis ||| rytis maskeliunas ||| karolis stankevicius ||| robertas damasevicius ||| 
2020 ||| detection of obstacle features using neural networks with attention in the task of autonomous navigation of mobile robots. ||| kirill sviatov ||| alexander miheev ||| sergey v. sukhov ||| yuriy lapshov ||| stefan rapp ||| 
2018 ||| mental war: an attention-based single/multiplayer brain-computer interface game. ||| gabriel alves mendes vasiljevic ||| leonardo cunha de miranda ||| bruna camila de menezes ||| 
2019 ||| scenes segmentation in self-driving car navigation system using neural network models with attention. ||| kirill sviatov ||| alexander mikheev ||| daniil kanin ||| sergey v. sukhov ||| vadim tronin ||| 
2020 ||| spectral super-resolution using hybrid 2d-3d structure tensor attention networks with camera spectral sensitivity prior. ||| chaoxiong wu ||| jiaojiao li ||| rui song ||| yunsong li ||| 
2019 ||| attention based residual network for high-resolution remote sensing imagery scene classification. ||| runyu fan ||| lizhe wang ||| ruyi feng ||| yingqian zhu ||| 
2018 ||| salient object detection via double sparse representations under visual attention guidance. ||| xiang wang ||| yongjun zhang ||| xunwei xie ||| yansheng li ||| 
2018 ||| ai-net: attention inception neural networks for hyperspectral image classification. ||| zhitong xiong ||| yuan yuan ||| qi wang ||| 
2021 ||| remote sensing image change detection based on fully convolutional network with pyramid attention. ||| shujun li ||| lianzhi huo ||| 
2020 ||| dense docked ship detection via spatial group-wise enhance attention in sar images. ||| xiaoya wang ||| zongyong cui ||| zongjie cao ||| sihang dang ||| 
2021 ||| self-attention fusion module for single remote sensing image super-resolution. ||| han mei ||| haopeng zhang ||| zhiguo jiang ||| 
2021 ||| siammraan: siamese multi-level residual attention adaptive network for hyperspectral videos tracking. ||| ye wang ||| shaohui mei ||| shun zhang ||| qian du ||| 
2019 ||| multiscale ship detection based on dense attention pyramid network in sar images. ||| qi li ||| rui min ||| zongyong cui ||| yiming pi ||| zhengwu xu ||| 
2021 ||| deep vision transformers for remote sensing scene classification. ||| laila bashmal ||| yakoub bazi ||| mohamad mahmoud al rahhal ||| 
2021 ||| hyperspectral imagery super-resolution based on self-calibrated attention residual network. ||| baorui wang ||| shaohui mei ||| yan feng ||| qian du ||| 
2021 ||| small vessel detection based on adaptive dual-polarimetric sar feature fusion and attention-enhanced feature pyramid network. ||| feixiang zhang ||| yongsheng zhou ||| fan zhang ||| qiang yin ||| fei ma ||| 
2017 ||| sar image change detection method based on visual attention. ||| yan zhang ||| chao wang ||| shigang wang ||| hong zhang ||| meng liu ||| 
2019 ||| attention networks for band weighting and selection in hyperspectral remote sensing image classification. ||| jing wang ||| jun zhou ||| weiqing huang ||| jackie fang chen ||| 
2020 ||| hyperspectral image classification based on semi-supervised dual-branch convolutional autoencoder with self-attention. ||| jie feng ||| zhanwei ye ||| di li ||| yuping liang ||| xu tang ||| xiangrong zhang ||| 
2019 ||| attention-based domain adaptation for hyperspectral image classification. ||| robiul hossain md. rafi ||| bo tang ||| qian du ||| nicolas h. younan ||| 
2020 ||| spatial attention network for road extraction. ||| ruonan chen ||| yuan hu ||| tong wu ||| ling peng ||| 
2019 ||| high-order self-attention network for remote sensing scene classification. ||| nanjun he ||| leyuan fang ||| yi li ||| antonio plaza ||| 
2021 ||| attention neural network semblance velocity auto picking with reference velocity curve data augmentation. ||| chenyu qiu ||| bangyu wu ||| delin meng ||| xu zhu ||| meng li ||| nan qin ||| 
2021 ||| triplet attention feature fusion network for sar and optical image land cover classification. ||| zhe xu ||| jinbiao zhu ||| jie geng ||| xinyang deng ||| wen jiang ||| 
2019 ||| a global point-sift attention network for 3d point cloud semantic segmentation. ||| meixia jia ||| aijin li ||| zhaoyang wu ||| 
2019 ||| aircraft detection from remote sensing image based on a weakly supervised attention model. ||| jinsheng ji ||| tao zhang ||| zhen yang ||| linfeng jiang ||| weilin zhong ||| huilin xiong ||| 
2020 ||| instance-aware remote sensing image captioning with cross-hierarchy attention. ||| chengze wang ||| zhiyu jiang ||| yuan yuan ||| 
2021 ||| attention based semantic segmentation on uav dataset for natural disaster damage assessment. ||| tashnim chowdhury ||| maryam rahnemoonfar ||| 
2021 ||| a multi-branch network based on weight sharing and attention mechanism for hyperspectral image classification. ||| zhen guo ||| caihong mu ||| yi liu ||| 
2021 ||| attention mechanism for land cover mapping with image-level labels. ||| teerasit kasetkasem ||| suesarn wilainuch ||| yanatorn chadavadh ||| kulladech pitakpornkasem ||| teera phatrapornnant ||| sanparith marukatat ||| 
2019 ||| super-resolution of sentinel-2 images based on deep channel-attention residual network. ||| xi zhu ||| yang xu ||| zhihui wei ||| 
2021 ||| channel-based attention for land cover classification using sentinel-2 time series. ||| hermann courteille ||| alexandre beno ||| t ||| nicolas m ||| ger ||| abdourrahmane m. atto ||| dino ienco ||| 
2021 ||| self-attention generative adversarial networks for times series vhr multispectral image generation. ||| ferdaous chaabane ||| safa r ||| jichi ||| florence tupin ||| 
2020 ||| dilated residual network based on dual expectation maximization attention for semantic segmentation of remote sensing images. ||| jiachao liu ||| xinyue xiong ||| jiaojiao li ||| chaoxiong wu ||| rui song ||| 
2019 ||| multiscale ship detection based on dense attention pyramid network in sar images. ||| qi li ||| rui min ||| zongyong cui ||| yiming pi ||| zhengwu xu ||| 
2021 ||| daff-net: dual attention feature fusion network for aircraft detection in remote sensing images. ||| min liu ||| qian hu ||| cong wang ||| tian tian ||| weitao chen ||| 
2020 ||| se-hrnet: a deep high-resolution network with attention for remote sensing scene classification. ||| lingling li ||| tian tian ||| hang li ||| lizhe wang ||| 
2021 ||| hyperspectral classification based on spectral indices learned through soft attention units. ||| romain thoreau ||| v ||| ronique achard ||| xavier briottet ||| 
2021 ||| a spatial-temporal-channel attention unet++ for high resolution remote sensing image change detection. ||| mingliang liu ||| jinjie huang ||| lei ma ||| ling wan ||| jialong guo ||| dongpan yao ||| 
2021 ||| hyperspectral and lidar data classification based on linear self-attention. ||| min feng ||| feng gao ||| jian fang ||| junyu dong ||| 
2021 ||| spectral and spatial residual attention network for joint hyperspectral and lidar data classification. ||| jing wang ||| jun zhou ||| xinwen liu ||| farah jahan ||| 
2021 ||| glacier calving front segmentation using attention u-net. ||| michael holzmann ||| amirabbas davari ||| thorsten seehaus ||| matthias h. braun ||| andreas k. maier ||| vincent christlein ||| 
2021 ||| a multi-scale feature aggregation network based on channel-spatial attention for remote sensing scene classification. ||| ming li ||| lin lei ||| xiao li ||| yuli sun ||| 
2021 ||| multi-view attention network for remote sensing image captioning. ||| yun meng ||| yu gu ||| xiutiao ye ||| jingxian tian ||| shuang wang ||| he zhang ||| biao hou ||| licheng jiao ||| 
2019 ||| semantic segmentation of high resolution remote sensing image based on batch-attention mechanism. ||| yanzhou su ||| yongjian wu ||| min wang ||| feng wang ||| jian cheng ||| 
2020 ||| remote sensing scene classification using spatial transformer fusion network. ||| shun tong ||| kunlun qi ||| qingfeng guan ||| qiqi zhu ||| chao yang ||| jie zheng ||| 
2021 ||| dual lightweight network with attention and feature fusion for semantic segmentation of high-resolution remote sensing images. ||| yijie zhang ||| yulan chen ||| qijun ma ||| changtao he ||| jian cheng ||| 
2021 ||| ptgan: a proposal-weighted two-stage gan with attention for hyperspectral target detection. ||| haonan qin ||| weiying xie ||| yunsong li ||| kai jiang ||| jie lei ||| qian du ||| 
2021 ||| research on fracture recognition in well logging images: adversarial learning with attention. ||| wei zhang ||| tong wu ||| zhipeng li ||| yanjun li ||| yibing shi ||| 
2021 ||| modified deep transformers for gnss time series prediction. ||| mostafa kiani shahvandi ||| benedikt soja ||| 
2018 ||| attention-based convolutional neural network for the detection of built-up areas in high-resolution sar images. ||| yunfei wu ||| rong zhang ||| yibing zhan ||| 
2021 ||| self-attention and mutual-attention for few-shot hyperspectral image classification. ||| kai huang ||| xinyang deng ||| jie geng ||| wen jiang ||| 
2020 ||| cloud detection using gabor filters and attention-based convolutional neural network for remote sensing images. ||| jing zhang ||| qin zhou ||| hui wang ||| yuchen wang ||| yunsong li ||| 
2021 ||| sar image object detection based on improved cross-entropy loss function with the attention of hard samples. ||| yangyang li ||| wenxi shi ||| guangyuan liu ||| licheng jiao ||| zhong ma ||| lu wei ||| 
2019 ||| spectral-spatial classification of hyperspectral image based on a joint attention network. ||| erting pan ||| yong ma ||| xiaoguang mei ||| xiaobing dai ||| fan fan ||| xin tian ||| jiayi ma ||| 
2021 ||| attention-driven cross-modal remote sensing image retrieval. ||| ushasi chaudhuri ||| biplab banerjee ||| avik bhattacharya ||| mihai datcu ||| 
2020 ||| deep residual spatial attention network for hyperspectral pansharpening. ||| yuxuan zheng ||| jiaojiao li ||| yunsong li ||| yanzi shi ||| jiahui qu ||| 
2020 ||| towards natural language question answering over earth observation linked data using attention-based neural machine translation. ||| abhishek v. potnis ||| rajat c. shinde ||| surya s. durbha ||| 
2021 ||| triple attention network for multi-class semantic segmentation in aerial images. ||| yu si ||| yuxia li ||| huanping wu ||| lang yuan ||| yuzhen li ||| lei he ||| 
2021 ||| an attention-based system for damage assessment using satellite imagery. ||| hanxiang hao ||| sriram baireddy ||| emily r. bartusiak ||| latisha konz ||| kevin j. latourette ||| michael gribbons ||| moses w. chan ||| edward j. delp ||| mary l. comer ||| 
2020 ||| complex-valued spatial-scattering separated attention network for polsar image classification. ||| zhaohao fan ||| zexuan ji ||| peng fu ||| tao wang ||| xiaobo shen ||| quansen sun ||| 
2020 ||| light-weight attention semantic segmentation network for high-resolution remote sensing images. ||| siyu liu ||| changtao he ||| haiwei bai ||| yijie zhang ||| jian cheng ||| 
2021 ||| attention residual u-net for building segmentation in aerial images. ||| chaohui li ||| yingjian liu ||| haoyu yin ||| yue li ||| qingxiang guo ||| limin zhang ||| pengting du ||| 
2019 ||| a novel multi-attention driven system for multi-label remote sensing image classification. ||| gencer sumbul ||| beg ||| m demir ||| 
2021 ||| adaptive channel attention and feature super-resolution for remote sensing images spatiotemporal fusion. ||| shuai fang ||| siyuan meng ||| yang cao ||| jing zhang ||| weikai shi ||| 
2020 ||| hierarchical attention for ship detection in sar images. ||| chunbo zhu ||| danpei zhao ||| ziming liu ||| yinan mao ||| 
2019 ||| hyperspectral target detection via deep multiple instance self-attention neural network. ||| xiuxiu wang ||| xiaoying chen ||| shuiping gou ||| chao chen ||| yuanbo chen ||| xu tang ||| changzhe jiao ||| 
2021 ||| dsamnet: a deeply supervised attention metric based network for change detection of high-resolution images. ||| mengxi liu ||| qian shi ||| 
2021 ||| attention based convolution autoencoder for dimensionality reduction in hyperspectral images. ||| shivam pande ||| biplab banerjee ||| 
2018 ||| attention based network for remote sensing scene classification. ||| shaoteng liu ||| qi wang ||| xuelong li ||| 
2021 ||| polsar image classification with complex-valued residual attention enhanced u-net. ||| shijie ren ||| feng zhou ||| 
2020 ||| semantic segmentation of urban buildings from vhr remotely sensed imagery using attention-based cnn. ||| zhijie zhang ||| chuanrong zhang ||| weidong li ||| 
2021 ||| residual attention mechanism for construction disturbance detection from satellite image. ||| ning lv ||| hao yuan ||| chen chen ||| jiaxuan deng ||| tao su ||| yang zhou ||| hua yang ||| 
2020 ||| acmu-nets: attention cascading modular u-nets incorporating squeeze and excitation blocks. ||| seokjun kang ||| brian kenji iwana ||| seiichi uchida ||| 
2020 ||| an improved convolutional block attention module for chinese character recognition. ||| kai zhou ||| yongsheng zhou ||| rui zhang ||| xiaolin wei ||| 
2020 ||| multi-tensor fusion network with hybrid attention for multimodal sentiment analysis. ||| haiwei xue ||| xueming yan ||| shengyi jiang ||| helang lai ||| 
2020 ||| improving self-attention based news recommendation with document classification. ||| haowen ke ||| 
2018 ||| identification of attracting attention in thai handicraft products using self-organizing map. ||| natinai jinsakul ||| cheng-fa tsai ||| 
2020 ||| a novel chinese reading comprehension model based on attention mechanism and convolutional neural networks. ||| chin-shyurng fahn ||| yi-lun wang ||| chu-ping lee ||| 
2019 ||| short-text question classification based on dependency parsing and attention mechanism. ||| an fang ||| 
2019 ||| atvr: an attention training system using multitasking and neurofeedback on virtual reality platform. ||| menghe zhang ||| junsong zhang ||| dong zhang ||| 
2020 ||| a qoe and visual attention evaluation on the influence of spatial audio in 360 videos. ||| amit hirway ||| yuansong qiao ||| niall murray ||| 
2021 ||| a technical report for visual attention estimation in hmd challenge. ||| chun tsao ||| po-chyi su ||| 
2019 ||| deep learning on vr-induced attention. ||| gang li ||| muhammad adeel khan ||| 
2020 ||| attention estimation in virtual reality with eeg based image regression. ||| victor delvigne ||| hazem wannous ||| jean-philippe vandeborre ||| laurence ris ||| thierry dutoit ||| 
2019 ||| development of internet of things platform and its application in remote monitoring and control of transformer operation. ||| r. venkataswamy ||| k. uma rao ||| p. meena ||| 
2019 ||| attention model for massive mimo csi compression feedback and recovery. ||| qiuyu cai ||| chao dong ||| kai niu ||| 
2021 ||| graph attention network-based drl for network slicing management in dense cellular networks. ||| yan shao ||| rongpeng li ||| zhifeng zhao ||| honggang zhang ||| 
2021 ||| qaan: question answering attention network for community question classification. ||| yuntao wang ||| weiqing huang ||| 
2021 ||| ble beacon with user traffic awareness using deep correlation and attention network. ||| kang eun jeon ||| james she ||| 
2021 ||| can: complementary attention network for aspect level sentiment classification in social e-commerce. ||| yali luo ||| zhengwei jiang ||| jun jiang ||| peian yang ||| xuren wang ||| kai zhang ||| 
2019 ||| resilient combination of complementary cnn and rnn features for text classification through attention and ensembling. ||| athanasios giannakopoulos ||| maxime coriou ||| andreea hossmann ||| michael baeriswyl ||| claudiu musat ||| 
2020 ||| text sentiment analysis based on parallel tcn model and attention model. ||| dong cao ||| yujie huang ||| yunbin fu ||| 
2020 ||| color recognition of vehicle based on low light enhancement and pixel-wise contextual attention. ||| pengkang zeng ||| jintao zhu ||| guoheng huang ||| lianglun cheng ||| 
2020 ||| investigation on transformer oil parameters using support vector machine. ||| birender singh ||| a. harshith kumar ||| c. c. reddy ||| 
2021 ||| melanoma skin cancer detection using efficientnet and channel attention module. ||| s. papiththira ||| t. kokul ||| 
2017 ||| human attention estimation by a domestic service robot using upper body skeletal information. ||| h. p. chapa sirithunge ||| a. g. buddhika p. jayasekara ||| d. p. chandima ||| 
2017 ||| possibility of blending sesame oil with field aged mineral oil for transformer applications. ||| d. u. bandara ||| j. r. s. s. kumara ||| m. a. r. m. fernando ||| c. s. kalpage ||| 
2017 ||| application of sfra techniques to discriminate short circuit faults of transformer winding. ||| g. a. t. n. aravinda ||| kapila bandara ||| g. a. jayantha ||| j. r. s. s. kumara ||| m. a. r. m. fernando ||| 
2017 ||| comparison of coconut/sesame/castor oils and their blends for transformer insulation. ||| j. r. s. s. kumara ||| m. a. r. m. fernando ||| c. s. kalpage ||| 
2020 ||| single image super-resolution using residual channel attention network. ||| hritam basak ||| rohit kundu ||| anish agarwal ||| shreya giri ||| 
2021 ||| mining medication-effect relations from twitter data using pre-trained transformer language model. ||| keyuan jiang ||| dingkai zhang ||| gordon r. bernard ||| 
2020 ||| modeling dynamic heterogeneous network for link prediction using hierarchical attention with temporal rnn. ||| hansheng xue ||| luwei yang ||| wen jiang ||| yi wei ||| yi hu ||| yu lin ||| 
2020 ||| amqan: adaptive multi-attention question-answer networks for answer selection. ||| haitian yang ||| weiqing huang ||| xuan zhao ||| yan wang ||| yuyan chen ||| bin lv ||| rui mao ||| ning li ||| 
2018 ||| intelligent monitoring of transformer insulation using convolutional neural networks. ||| wei lee woon ||| zeyar aung ||| ayman h. el-hag ||| 
2019 ||| semantically corroborating neural attention for biomedical question answering. ||| marilena oita ||| k. vani ||| fatma oezdemir-zaech ||| 
2019 ||| transformer models for question answering at bioasq 2019. ||| michele resta ||| daniele arioli ||| alessandro fagnani ||| giuseppe attardi ||| 
2018 ||| multimodal tweet sentiment classification algorithm based on attention mechanism. ||| peiyu zou ||| shuangtao yang ||| 
2021 ||| concad: contrastive learning-based cross attention for sleep apnea detection. ||| guanjie huang ||| fenglong ma ||| 
2018 ||| polar: attention-based cnn for one-shot personalized article recommendation. ||| zhengxiao du ||| jie tang ||| yuhui ding ||| 
2019 ||| learning to detect online harassment on twitter with the transformer. ||| margarita constanza bugue ||| o ||| marcelo mendoza ||| 
2020 ||| a self-attention network based node embedding model. ||| dai quoc nguyen ||| tu dinh nguyen ||| dinh phung ||| 
2019 ||| attention-based deep tropical cyclone rapid intensification prediction. ||| ching-yuan bai ||| buo-fu chen ||| hsuan-tien lin ||| 
2020 ||| gram-smot: top-n personalized bundle recommendation via graph attention mechanism and submodular optimization. ||| m. vijaikumar ||| shirish k. shevade ||| m. narasimha murty ||| 
2021 ||| the joy of dressing is an art: outfit generation using self-attention bi-lstm. ||| manchit madan ||| ankur chouragade ||| sreekanth vempati ||| 
2020 ||| self-attention enhanced patient journey understanding in healthcare system. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| 
2021 ||| transformers: "the end of history" for natural language processing? ||| anton chernyavskiy ||| dmitry ilvovsky ||| preslav nakov ||| 
2019 ||| sorecgat: leveraging graph attention mechanism for top-n social recommendation. ||| vijaikumar m ||| shirish k. shevade ||| m. narasimha murty ||| 
2017 ||| sequence generation with target attention. ||| yingce xia ||| fei tian ||| tao qin ||| nenghai yu ||| tie-yan liu ||| 
2020 ||| benchmarking tropical cyclone rapid intensification with satellite images and attention-based deep models. ||| ching-yuan bai ||| buo-fu chen ||| hsuan-tien lin ||| 
2019 ||| attention-based method for categorizing different types of online harassment language. ||| christos karatsalos ||| yannis panagiotakis ||| 
2018 ||| discovering spatio-temporal latent influence in geographical attention dynamics. ||| minoru higuchi ||| kanji matsutani ||| masahito kumano ||| masahiro kimura ||| 
2020 ||| lightweight temporal self-attention for classifying satellite images time series. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| 
2017 ||| sentiment analysis model based on structure attention mechanism. ||| kai lin ||| dazhen lin ||| donglin cao ||| 
2020 ||| visualizing transformers for nlp: a brief survey. ||| adrian m. p. brasoveanu ||| razvan andonie ||| 
2018 ||| extending attention span for children adhd using an attentive visual interface. ||| othman asiry ||| haifeng shen ||| theodor wyeld ||| soher balkhy ||| 
2020 ||| attention support with soft visual cues in control room environments. ||| magnus nylin ||| jonas lundberg ||| jimmy johansson ||| 
2019 ||| condition monitoring of interconnecting transformer through ann approach. ||| monika patel ||| arun pachori ||| 
2021 ||| multimodal emotion recognition using a modified dense co-attention symmetric network. ||| zhi-wei zhao ||| wei liu ||| bao-liang lu ||| 
2019 ||| attentional correlates in somatosensory potentials evoked by ultrasound induced virtual objects in mid-air. ||| caroline lehser ||| daniel j. strauss ||| 
2019 ||| deep neural network with attention mechanism for classification of motor imagery eeg. ||| yen-cheng huang ||| jia-ren chang ||| li-fen chen ||| yong-sheng chen ||| 
2017 ||| circular organization of the instantaneous phase in erps and the ongoing eeg due to selective attention. ||| farah i. corona-strauss ||| daniel j. strauss ||| 
2017 ||| effect of attention division on movement detection and execution in dual-task conditions. ||| susan aliakbaryhosseinabadi ||| ernest nlandu kamavuako ||| dario farina ||| natalie mrachacz-kersting ||| 
2019 ||| phase transfer entropy between frontal and posterior regions during visual spatial attention. ||| jianan wang ||| jiaqi wang ||| junfeng sun ||| shanbao tong ||| xiangfei hong ||| 
2019 ||| cortical tracking of vocoded speech streams with a competing speaker based on attentional selection. ||| lei wang ||| ed x. wu ||| fei chen ||| 
2017 ||| preliminary study of neurocognitive differences in attention and fluency in schizophrenia using fnirs. ||| adrian curtin ||| hasan ayaz ||| junfeng sun ||| lin cheng ||| jijun wang ||| banu onaral ||| shanbao tong ||| 
2017 ||| attention evaluation with eye tracking glasses for eeg-based emotion recognition. ||| zhen-feng shi ||| chang zhou ||| wei-long zheng ||| bao-liang lu ||| 
2019 ||| stock volatility prediction based on self-attention networks with social information. ||| jie zheng ||| andi xia ||| lin shao ||| tao wan ||| zengchang qin ||| 
2019 ||| attention-based lstm for insider threat detection. ||| fangfang yuan ||| yanmin shang ||| yanbing liu ||| yanan cao ||| jianlong tan ||| 
2018 ||| visual attention and memory augmented activity recognition and behavioral prediction. ||| nidhinandana salian ||| 
2019 ||| automated deployment and scaling of automotive safety services in 5g-transformer. ||| jorge baranda ||| marco zanzola ||| giuseppe avino ||| josep mangues-bafalluy ||| luca vettori ||| ricardo mart ||| nez ||| carla-fabiana chiasserini ||| claudio casetti ||| paolo bande ||| marina giordanino ||| 
2018 ||| deploying a containerized ns-3/lena-based lte mobile network service through the 5g-transformer platform. ||| jorge baranda ||| i ||| aki pascual ||| manuel requena-esteso ||| josep mangues-bafalluy ||| 
2017 ||| top-down deep appearance attention for action recognition. ||| rao muhammad anwer ||| fahad shahbaz khan ||| joost van de weijer ||| jorma laaksonen ||| 
2017 ||| wearable gaze trackers: mapping visual attention in 3d. ||| rasmus r. jensen ||| jonathan d. stets ||| seidi suurmets ||| jesper clement ||| henrik aan ||| s ||| 
2020 ||| knowledge based transformer model for information retrieval. ||| jibril frej ||| didier schwab ||| jean-pierre chevallet ||| 
2019 ||| power transformer fault diagnosis based on improved bat algorithms to optimize rnn. ||| chun yan ||| meixuan li ||| wei liu ||| 
2017 ||| the optimal maintenance strategy of power transformers based on the life cycle cost. ||| jian-peng bian ||| su yang ||| xiaoyun sun ||| 
2017 ||| the optimal switching strategy of transformers based on the cost. ||| su yang ||| yanjun feng ||| qinghong wang ||| jianpeng bian ||| 
2018 ||| particle filtering based visual attention model for moving target detection. ||| long liu ||| danyang jing ||| xiaojun chang ||| 
2019 ||| elimination of transformer inrush current by three-phase linkage circuit breakers. ||| chunfang zhao ||| yundong song ||| dewei kong ||| yue yang ||| dan luo ||| yaling jin ||| rui guo ||| 
2017 ||| moving object attention selection using optical flow and pulse coupled neural network. ||| jiancheng wang ||| xiaodong gu ||| 
2021 ||| audio-visual event localization via recursive fusion by joint co-attention. ||| bin duan ||| hao tang ||| wei wang ||| ziliang zong ||| guowei yang ||| yan yan ||| 
2021 ||| spatial context-aware self-attention model for multi-organ segmentation. ||| hao tang ||| xingwei liu ||| kun han ||| xiaohui xie ||| xuming chen ||| qian huang ||| yong liu ||| shanlin sun ||| narisu bai ||| 
2022 ||| video salient object detection via contrastive features and attention modules. ||| yi-wen chen ||| xiaojie jin ||| xiaohui shen ||| ming-hsuan yang ||| 
2021 ||| attentional feature fusion. ||| yimian dai ||| fabian gieseke ||| stefan oehmcke ||| yiquan wu ||| kobus barnard ||| 
2021 ||| self supervision for attention networks. ||| badri n. patro ||| g. s. kasturi ||| ansh jain ||| vinay p. namboodiri ||| 
2021 ||| an explainable attention-guided iris presentation attack detector. ||| cunjian chen ||| arun ross ||| 
2022 ||| siamese transformer pyramid networks for real-time uav tracking. ||| daitao xing ||| nikolaos evangeliou ||| athanasios tsoukalas ||| anthony tzes ||| 
2021 ||| attention-based spatial guidance for image-to-image translation. ||| yu lin ||| yigong wang ||| yifan li ||| yang gao ||| zhuoyi wang ||| latifur khan ||| 
2022 ||| mm-vit: multi-modal video transformer for compressed video action recognition. ||| jiawei chen ||| chiu man ho ||| 
2022 ||| leaky gated cross-attention for weakly supervised multi-modal temporal action localization. ||| jun-tae lee ||| sungrack yun ||| mihir jain ||| 
2018 ||| fine-grained and semantic-guided visual attention for image captioning. ||| zongjian zhang ||| qiang wu ||| yang wang ||| fang chen ||| 
2019 ||| crowd counting using scale-aware attention networks. ||| mohammad asiful hossain ||| mehrdad hosseinzadeh ||| omit chanda ||| yang wang ||| 
2022 ||| learnable multi-level frequency decomposition and hierarchical attention mechanism for generalized face presentation attack detection. ||| meiling fang ||| naser damer ||| florian kirchbuchner ||| arjan kuijper ||| 
2020 ||| iterative and adaptive sampling with spatial attention for black-box model explanations. ||| bhavan vasu ||| chengjiang long ||| 
2021 ||| rotate to attend: convolutional triplet attention module. ||| diganta misra ||| trikay nalamada ||| ajay uppili arasanipalai ||| qibin hou ||| 
2021 ||| atm: attentional text matting. ||| peng kang ||| jianping zhang ||| chen ma ||| guiling sun ||| 
2022 ||| billion-scale pretraining with vision transformers for multi-task visual representations. ||| josh beal ||| hao-yu wu ||| dong huk park ||| andrew zhai ||| dmitry kislyuk ||| 
2019 ||| mask r-cnn with pyramid attention network for scene text detection. ||| zhida huang ||| zhuoyao zhong ||| lei sun ||| qiang huo ||| 
2020 ||| david: dual-attentional video deblurring. ||| junru wu ||| xiang yu ||| ding liu ||| manmohan chandraker ||| zhangyang wang ||| 
2022 ||| unsupervised sounding object localization with bottom-up and top-down attention. ||| jiayin shi ||| chao ma ||| 
2020 ||| actor conditioned attention maps for video action detection. ||| oytun ulutan ||| swati rallapalli ||| mudhakar srivatsa ||| carlos torres ||| b. s. manjunath ||| 
2019 ||| no-reference image quality assessment: an attention driven approach. ||| diqi chen ||| yizhou wang ||| hongyu ren ||| wen gao ||| 
2022 ||| unetr: transformers for 3d medical image segmentation. ||| ali hatamizadeh ||| yucheng tang ||| vishwesh nath ||| dong yang ||| andriy myronenko ||| bennett a. landman ||| holger r. roth ||| daguang xu ||| 
2020 ||| adversarial discriminative attention for robust anomaly detection. ||| daiki kimura ||| subhajit chaudhury ||| minori narita ||| asim munawar ||| ryuki tachibana ||| 
2020 ||| attention flow: end-to-end joint attention estimation. ||| mer s ||| mer ||| peter gerjets ||| ulrich trautwein ||| enkelejda kasneci ||| 
2021 ||| unsupervised attention based instance discriminative learning for person re-identification. ||| kshitij nikhal ||| benjamin s. riggan ||| 
2021 ||| logan: latent graph co-attention network for weakly-supervised video moment retrieval. ||| reuben tan ||| huijuan xu ||| kate saenko ||| bryan a. plummer ||| 
2022 ||| edgeconv with attention module for monocular depth estimation. ||| minhyeok lee ||| sangwon hwang ||| chaewon park ||| sangyoun lee ||| 
2022 ||| sega: semantic guided attention on visual prototype for few-shot learning. ||| fengyuan yang ||| ruiping wang ||| xilin chen ||| 
2022 ||| visualizing paired image similarity in transformer networks. ||| samuel black ||| abby stylianou ||| robert pless ||| richard souvenir ||| 
2020 ||| temporal aggregation with clip-level attention for video-based person re-identification. ||| mengliu li ||| han xu ||| jinjun wang ||| wenpeng li ||| yongli sun ||| 
2021 ||| long-range attention network for multi-view stereo. ||| xudong zhang ||| yutao hu ||| haochen wang ||| xianbin cao ||| baochang zhang ||| 
2022 ||| maps: multimodal attention for product similarity. ||| nilotpal das ||| aniket joshi ||| promod yenigalla ||| gourav agrwal ||| 
2019 ||| scene parsing via dense recurrent neural networks with attentional selection. ||| heng fan ||| peng chu ||| longin jan latecki ||| haibin ling ||| 
2018 ||| predicting facial attributes in video using temporal coherence and motion-attention. ||| emily m. hand ||| carlos domingo castillo ||| rama chellappa ||| 
2022 ||| sac: semantic attention composition for text-conditioned image retrieval. ||| surgan jandial ||| pinkesh badjatiya ||| pranit chawla ||| ayush chopra ||| mausoom sarkar ||| balaji krishnamurthy ||| 
2022 ||| detecting arbitrary intermediate keypoints for human pose estimation with vision transformers. ||| katja ludwig ||| philipp harzig ||| rainer lienhart ||| 
2022 ||| idea-net: adaptive dual self-attention network for single image denoising. ||| zheming zuo ||| xinyu chen ||| han xu ||| jie li ||| wenjuan liao ||| zhi-xin yang ||| shizheng wang ||| 
2021 ||| coarse temporal attention network (cta-net) for driver's activity recognition. ||| zachary wharton ||| ardhendu behera ||| yonghuai liu ||| nik bessis ||| 
2021 ||| efficient attention: attention with linear complexities. ||| zhuoran shen ||| mingyuan zhang ||| haiyu zhao ||| shuai yi ||| hongsheng li ||| 
2022 ||| spatial-temporal transformer for 3d point cloud sequences. ||| yimin wei ||| hao liu ||| tingting xie ||| qiuhong ke ||| yulan guo ||| 
2018 ||| "seeing is believing": pedestrian trajectory forecasting using visual frustum of attention. ||| irtiza hasan ||| francesco setti ||| theodore tsesmelis ||| alessio del bue ||| marco cristani ||| fabio galasso ||| 
2021 ||| dualsanet: dual spatial attention network for iris recognition. ||| kai yang ||| zihao xu ||| jingjing fei ||| 
2022 ||| robust lane detection via expanded self attention. ||| minhyeok lee ||| junhyeop lee ||| dogyoon lee ||| woo jin kim ||| sangwon hwang ||| sangyoun lee ||| 
2020 ||| spatio-temporal ranked-attention networks for video captioning. ||| anoop cherian ||| jue wang ||| chiori hori ||| tim k. marks ||| 
2018 ||| multichannel attention network for analyzing visual behavior in public speaking. ||| rahul sharma ||| tanaya guha ||| gaurav sharma ||| 
2019 ||| ventral-dorsal neural networks: object detection via selective attention. ||| mohammad k. ebrahimpour ||| jiayun li ||| yen-yun yu ||| jackson reesee ||| azadeh moghtaderi ||| ming-hsuan yang ||| david c. noelle ||| 
2022 ||| image-adaptive hint generation via vision transformer for outpainting. ||| daehyeon kong ||| kyeongbo kong ||| kyunghun kim ||| sung-jun min ||| suk-ju kang ||| 
2020 ||| multi-label visual feature learning with attentional aggregation. ||| ziqiao guan ||| kevin g. yager ||| dantong yu ||| hong qin ||| 
2022 ||| beyond mono to binaural: generating binaural audio from mono audio with depth and cross modal attention. ||| kranti kumar parida ||| siddharth srivastava ||| gaurav sharma ||| 
2020 ||| periphery-fovea multi-resolution driving model guided by human attention. ||| ye xia ||| jinkyu kim ||| john f. canny ||| karl zipser ||| teresa canas-bajo ||| david whitney ||| 
2020 ||| self-attention network for skeleton-based human action recognition. ||| sangwoo cho ||| muhammad hasan maqbool ||| fei liu ||| hassan foroosh ||| 
2019 ||| attention mechanisms for object recognition with event-based cameras. ||| marco cannici ||| marco ciccone ||| andrea romanoni ||| matteo matteucci ||| 
2022 ||| m3detr: multi-representation, multi-scale, mutual-relation 3d object detection with transformers. ||| tianrui guan ||| jun wang ||| shiyi lan ||| rohan chandra ||| zuxuan wu ||| larry davis ||| dinesh manocha ||| 
2021 ||| pdan: pyramid dilated attention network for action detection. ||| rui dai ||| srijan das ||| luca minciullo ||| lorenzo garattoni ||| gianpiero francesca ||| fran ||| ois br ||| mond ||| 
2019 ||| location-velocity attention for pedestrian trajectory prediction. ||| hao xue ||| du huynh ||| mark reynolds ||| 
2019 ||| pixel-wise attentional gating for scene parsing. ||| shu kong ||| charless c. fowlkes ||| 
2021 ||| coarse- and fine-grained attention network with background-aware loss for crowd density map estimation. ||| liangzi rong ||| chunping li ||| 
2021 ||| spike-thrift: towards energy-efficient deep spiking neural networks by limiting spiking activity via attention-guided compression. ||| souvik kundu ||| gourav datta ||| massoud pedram ||| peter a. beerel ||| 
2021 ||| regional attention networks with context-aware fusion for group emotion recognition. ||| ahmed-shehab khan ||| zhiyuan li ||| jie cai ||| yan tong ||| 
2022 ||| aa3dnet: attention augmented real time 3d object detection. ||| abhinav sagar ||| 
2022 ||| densely-packed object detection via hard negative-aware anchor attention. ||| sungmin cho ||| jinwook paeng ||| junseok kwon ||| 
2019 ||| fashion attributes-to-image synthesis using attention-based generative adversarial network. ||| hanbit lee ||| sang-goo lee ||| 
2021 ||| integrating human gaze into attention for egocentric activity recognition. ||| kyle min ||| jason j. corso ||| 
2021 ||| end-to-end lane shape prediction with transformers. ||| ruijin liu ||| zejian yuan ||| tie liu ||| zhiliang xiong ||| 
2020 ||| defraudnet: end2end fingerprint spoof detection using patch level attention. ||| b. v. s. anusha ||| sayan banerjee ||| subhasis chaudhuri ||| 
2020 ||| attention-based fusion for multi-source human image generation. ||| st ||| phane lathuili ||| re ||| enver sangineto ||| aliaksandr siarohin ||| nicu sebe ||| 
2021 ||| super - sam: using the supervision signal from a pose estimator to train a spatial attention module for personal protective equipment recognition. ||| adrian sandru ||| georgian-emilian duta ||| mariana-iuliana georgescu ||| radu tudor ionescu ||| 
2022 ||| 3dreftransformer: fine-grained object identification in real-world scenes using natural language. ||| ahmed abdelreheem ||| ujjwal upadhyay ||| ivan skorokhodov ||| rawan al yahya ||| jun chen ||| mohamed elhoseiny ||| 
2021 ||| supervoxel attention graphs for long-range video modeling. ||| yang wang ||| gedas bertasius ||| tae-hyun oh ||| abhinav gupta ||| minh hoai ||| lorenzo torresani ||| 
2021 ||| kernel self-attention for weakly-supervised image classification using deep multiple instance learning. ||| dawid rymarczyk ||| adriana borowa ||| jacek tabor ||| bartosz zielinski ||| 
2022 ||| cloth-changing person re-identification with self-attention. ||| vaibhav bansal ||| gian luca foresti ||| niki martinel ||| 
2022 ||| variational stacked local attention networks for diverse video captioning. ||| tonmoay deb ||| akib sadmanee ||| kishor kumar bhaumik ||| amin ahsan ali ||| m. ashraful amin ||| a. k. m. mahbubur rahman ||| 
2019 ||| cascade attention machine for occluded landmark detection in 2d x-ray angiography. ||| liheng zhang ||| vivek singh ||| guo-jun qi ||| terrence chen ||| 
2020 ||| mhsan: multi-head self-attention network for visual semantic embedding. ||| geondo park ||| chihye han ||| daeshik kim ||| wonjun yoon ||| 
2022 ||| fassst: fast attention based single-stage segmentation net for real-time instance segmentation. ||| yuan cheng ||| rui lin ||| peining zhen ||| tianshu hou ||| chiu wa ng ||| hai-bao chen ||| hao yu ||| ngai wong ||| 
2019 ||| attention based natural language grounding by navigating virtual environment. ||| abhishek sinha ||| akilesh b ||| mausoom sarkar ||| balaji krishnamurthy ||| 
2020 ||| improving object detection with inverted attention. ||| zeyi huang ||| wei ke ||| dong huang ||| 
2020 ||| ulsam: ultra-lightweight subspace attention module for compact convolutional neural networks. ||| rajat saini ||| nandan kumar jha ||| bedanta das ||| sparsh mittal ||| c. krishna mohan ||| 
2020 ||| pointgrow: autoregressively learned point cloud generation with self-attention. ||| yongbin sun ||| yue wang ||| ziwei liu ||| joshua e. siegel ||| sanjay e. sarma ||| 
2022 ||| time-space transformers for video panoptic segmentation. ||| andra petrovai ||| sergiu nedevschi ||| 
2017 ||| enriched deep recurrent visual attention model for multiple object recognition. ||| artsiom ablavatski ||| shijian lu ||| jianfei cai ||| 
2020 ||| component attention guided face super-resolution network: cagface. ||| ratheesh kalarot ||| tao li ||| fatih porikli ||| 
2022 ||| after-unet: axial fusion transformer unet for medical image segmentation. ||| xiangyi yan ||| hao tang ||| shanlin sun ||| haoyu ma ||| deying kong ||| xiaohui xie ||| 
2022 ||| megan: memory enhanced graph attention network for space-time video super-resolution. ||| chenyu you ||| lianyi han ||| aosong feng ||| ruihan zhao ||| hui tang ||| wei fan ||| 
2022 ||| monocular depth estimation with adaptive geometric attention. ||| taher naderi ||| amir sadovnik ||| jason p. hayward ||| hairong qi ||| 
2022 ||| facial attribute transformers for precise and robust makeup transfer. ||| zhaoyi wan ||| haoran chen ||| jie an ||| wentao jiang ||| cong yao ||| jiebo luo ||| 
2021 ||| text-to-image generation grounded by fine-grained user attention. ||| jing yu koh ||| jason baldridge ||| honglak lee ||| yinfei yang ||| 
2019 ||| interpretable visual question answering by visual grounding from attention supervision mining. ||| yundong zhang ||| juan carlos niebles ||| alvaro soto ||| 
2022 ||| all the attention you need: global-local, spatial-channel attention for image retrieval. ||| chull hwan song ||| hye joo han ||| yannis avrithis ||| 
2018 ||| structured triplet learning with pos-tag guided attention for visual question answering. ||| zhe wang ||| xiaoyi liu ||| limin wang ||| yu qiao ||| xiaohui xie ||| charless c. fowlkes ||| 
2022 ||| sign pose-based transformer for word-level sign language recognition. ||| maty ||| s boh ||| cek ||| marek hr ||| z ||| 
2022 ||| no-reference image quality assessment via transformers, relative ranking, and self-consistency. ||| s. alireza golestaneh ||| saba dadsetan ||| kris m. kitani ||| 
2020 ||| proposal-free temporal moment localization of a natural-language query in video using guided attention. ||| cristian rodriguez opazo ||| edison marrese-taylor ||| fatemeh sadat saleh ||| hongdong li ||| stephen gould ||| 
2022 ||| attention guided cosine margin to overcome class-imbalance in few-shot road object detection. ||| ashutosh agarwal ||| anay majee ||| anbumani subramanian ||| chetan arora ||| 
2022 ||| non-local attention improves description generation for retinal images. ||| jia-hong huang ||| ting-wei wu ||| chao-han huck yang ||| zenglin shi ||| i-hung lin ||| jesper tegn ||| r ||| marcel worring ||| 
2018 ||| hierarchical attention-based anomaly detection model for embedded operating systems. ||| mellitus o. ezeme ||| qusay h. mahmoud ||| akramul azim ||| 
2019 ||| a multivariate time series classification method based on self-attention. ||| huiwei lin ||| yunming ye ||| ka-cheong leung ||| bowen zhang ||| 
2019 ||| insulation faults diagnosis of power transformer by decision tree with fuzzy logic. ||| cheng-kuo chang ||| jie shan ||| kuo-chi chang ||| jeng-shyang pan ||| 
2017 ||| development of audio and visual attention assessment system in combination with brain wave instrument: apply to children with attention deficit hyperactivity disorder. ||| chin-ling chen ||| yung-wen tang ||| yong-feng zhou ||| yue-xun chen ||| 
2019 ||| improvement of chromatographic peaks qualitative analysis for power transformer base on decision tree. ||| jie shan ||| cheng-kuo chang ||| hao-min chen ||| jeng-shyang pan ||| 
2017 ||| research on temperature rising prediction of distribution transformer by artificial neural networks. ||| wenxin zhang ||| jeng-shyang pan ||| yen-ming tseng ||| 
2022 ||| pre-routing path delay estimation based on transformer and residual framework. ||| tai yang ||| guoqing he ||| peng cao ||| 
2021 ||| net2: a graph attention network method customized for pre-placement net length estimation. ||| zhiyao xie ||| rongjian liang ||| xiaoqing xu ||| jiang hu ||| yixiao duan ||| yiran chen ||| 
2022 ||| tenet: temporal cnn with attention for anomaly detection in automotive cyber-physical systems. ||| sooryaa vignesh thiruloga ||| vipin kumar kukkala ||| sudeep pasricha ||| 
2021 ||| attention-in-memory for few-shot learning with configurable ferroelectric fet arrays. ||| dayane reis ||| ann franchesca laguna ||| michael t. niemier ||| xiaobo sharon hu ||| 
2019 ||| implementing neural machine translation with bi-directional gru and attention mechanism on fpgas using hls. ||| qin li ||| xiaofan zhang ||| jinjun xiong ||| wen-mei hwu ||| deming chen ||| 
2021 ||| improving graph convolutional networks with transformer layer in social-based items recommendation. ||| thi linh hoang ||| tuan dung pham ||| viet cuong ta ||| 
2019 ||| attention-based multi-input deep learning architecture for biological activity prediction: an application in egfr inhibitors. ||| huy ngoc pham ||| trung hoang le ||| 
2021 ||| attention-based deep learning model for aspect classification on vietnamese e-commerce data. ||| ngoc-tu nguyen ||| trong-dat nguyen ||| duy-cat can ||| mai-vu tran ||| ha luu manh ||| hoang-quynh le ||| 
2020 ||| integrating transformer into global and residual image feature extractor in visual question answering for blind people. ||| tung le ||| huy tien nguyen ||| le minh nguyen ||| 
2021 ||| influence prediction on social media network through contents and interaction behaviors using attention-based knowledge graph. ||| quan m. tran ||| hien d. nguyen ||| binh t. nguyen ||| vuong t. pham ||| trong t. le ||| 
2021 ||| improving the readability of unformatted text using multitask attention networks. ||| viet-anh phan ||| minh-tien nguyen ||| lam thu bui ||| phong dao ngoc ||| 
2019 ||| recovering capitalization for automatic speech recognition of vietnamese using transformer and chunk merging. ||| hien nguyen thi thu ||| binh nguyen thai ||| vu bao hung nguyen ||| quoc truong do ||| luong chi mai ||| huyen nguyen thi minh ||| 
2021 ||| aspect-based sentiment analysis using mini-window locating attention for vietnamese e-commerce reviews. ||| binh le-minh ||| thi-phuong le ||| khanh-hung tran ||| khanh-huyen bui ||| hoang-quynh le ||| duy-cat can ||| hung nguyen chung thanh ||| mai-vu tran ||| 
2020 ||| transformer-based summarization by exploiting social information. ||| minh-tien nguyen ||| van-chien nguyen ||| huy-the vu ||| van-hau nguyen ||| 
2018 ||| effective attention networks for aspect-level sentiment classification. ||| huy-thanh nguyen ||| minh-le nguyen ||| 
2020 |||  movie cuts in users' attention. ||| carlos mara ||| es ||| diego gutierrez ||| ana serrano ||| 
2020 ||| joint attention for automated video editing. ||| hui-yin wu ||| trevor santarra ||| michael leece ||| rolando vargas ||| arnav jhala ||| 
2019 ||| jass: joint attention strategies for paraphrase generation. ||| isaac k. e. ampomah ||| sally i. mcclean ||| zhiwei lin ||| glenn i. hawe ||| 
2019 ||| a study on self-attention mechanism for amr-to-text generation. ||| vu trong sinh ||| le minh nguyen ||| 
2020 ||| natural language generation using transformer network in an open-domain setting. ||| deeksha varshney ||| asif ekbal ||| ganesh prasad nagaraja ||| mrigank tiwari ||| abhijith athreya mysore gopinath ||| pushpak bhattacharyya ||| 
2020 ||| improving the community question retrieval performance using attention-based siamese lstm. ||| nouha othman ||| rim faiz ||| kamel sma ||| li ||| 
2017 ||| a hierarchical iterative attention model for machine comprehension. ||| zhuang liu ||| degen huang ||| yunxia zhang ||| chuan zhang ||| 
2019 ||| bidirectional transformer based multi-task learning for natural language understanding. ||| suraj tripathi ||| chirag singh ||| abhay kumar ||| chandan kumar pandey ||| nishant jain ||| 
2020 ||| studying attention models in sentiment attitude extraction task. ||| nicolay rusnachenko ||| natalia v. loukachevitch ||| 
2019 ||| exploring the attention mechanism in deep models: a case study on sentiment analysis. ||| martina toshevska ||| slobodan kalajdziski ||| 
2017 ||| perceptions of the effect of fragmented attention on mobile web search tasks. ||| morgan harvey ||| matthew pointon ||| 
2021 ||| classifying speech acts using multi-channel deep attention network for task-oriented conversational search agents. ||| souvick ghosh ||| satanu ghosh ||| 
2021 ||| a user study on user attention for an interactive content-based image search system. ||| mahmoud artemi ||| haiming liu ||| 
2020 ||| studying how health literacy influences attention during online information seeking. ||| carla teixeira lopes ||| edgar ramos ||| 
2020 ||| position-aware communication via self-attention for multi-agent reinforcement learning. ||| tsan-hua shih ||| hsien-i lin ||| 
2021 ||| evaluation of attention mechanisms on text to speech. ||| yi-xing lin ||| kai-wen liang ||| bing-jhih huang ||| jia-ching wang ||| 
2021 ||| deep residual and deep dense attentions in english chinese translation. ||| yi-xing lin ||| kai-wen liang ||| chih-hsuan yang ||| jia-ching wang ||| 
2021 ||| modified attention spatial convolution model for skin lesion segmentation. ||| thi-phuong le ||| yi-chiung hsu ||| jia-ching wang ||| 
2019 ||| a methodology for intuitive and low-attention tv remote control on smart phones. ||| guilherme guimar ||| es ||| jose fernandes ||| andr |||  costa ||| r ||| mulo fabr ||| cio ||| manoel j ||| nior ||| eddie batista de lima filho ||| 
2021 ||| poi recommendation model based on attention-based gated recurrent unit network. ||| jinxi wu ||| yi yao ||| ruiming fan ||| dan tao ||| 
2021 ||| green coffee beans classification using attention-based features and knowledge transfer. ||| po-yen yang ||| sin-ye jhong ||| chih-hsien hsia ||| 
2020 ||| prediction of time series data based on transformer with soft dynamic time wrapping. ||| kuo-hao ho ||| pei-shu huang ||| i-chen wu ||| feng-jian wang ||| 
2019 ||| inferring student's attention in a machine learning approach: a feasibility study. ||| jiachun li ||| ruiqi li ||| yuting zhou ||| junlin xian ||| xiong zhang ||| xiaojun hei ||| 
2020 ||| estimation of person-specific visual attention via selection of similar persons. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| degradation level estimation of road structures via attention branch network with text data. ||| naoki ogawa ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2019 ||| user-specific visual attention estimation based on visual similarity and spatial information in images. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2019 ||| field-of-view prediction in 360-degree videos with attention-based neural encoder-decoder networks. ||| jiang yu ||| yong liu ||| 
2020 ||| attention-based learning for missing data imputation in holoclean. ||| richard wu ||| aoqian zhang ||| ihab f. ilyas ||| theodoros rekatsinas ||| 
2020 ||| optimus: optimized matrix multiplication structure for transformer neural network accelerator. ||| junki park ||| hyunsung yoon ||| daehyun ahn ||| jungwook choi ||| jae-joon kim ||| 
2017 ||| reachability analysis of transformer-isolated dc-dc converters. ||| omar beg ||| ali davoudi ||| taylor t. johnson ||| 
2018 ||| power transformer anomaly detection based on adaptive kernel fuzzy c-means clustering and kernel principal component analysis. ||| kan tang ||| tingzhang liu ||| xiaoye xi ||| yue lin ||| jianfei zhao ||| 
2020 ||| neural topic model with attention for supervised learning. ||| xinyi wang ||| yi yang ||| 
2017 ||| information transfer within human robot teams: multimodal attention management in human-robot interaction. ||| bruce j. p. mortimer ||| linda r. elliott ||| 
2017 ||| rightward attentional bias in windshield displays: implication towards external human machine interfaces for self-driving cars. ||| qiang liu ||| birte emmermann ||| oscar suen ||| bryan grant ||| jake hercules ||| erik glaser ||| brian lathrop ||| 
2020 ||| on-line monitoring method of power transformer insulation fault based on bayesian network. ||| yehui chen ||| linglong tan ||| ying-hua liu ||| 
2020 ||| design of sealing transformer for vacuum packaging machine based on single chip microcomputer control. ||| ming-fei qu ||| xin zhang ||| 
2019 ||| analysis and visualization of attention area of tweets during disasters. ||| sanetoshi yamada ||| keisuke utsu ||| kohei cho ||| osamu uchida ||| 
2017 ||| initialized frame attention networks for video question answering. ||| kun gao ||| xianglei zhu ||| yahong han ||| 
2018 ||| video question answering via multi-granularity temporal attention network learning. ||| shaoning xiao ||| yimeng li ||| yunan ye ||| zhou zhao ||| jun xiao ||| fei wu ||| jiang zhu ||| yueting zhuang ||| 
2018 ||| object-oriented video prediction with pixel-level attention. ||| siyuan wu ||| hanli wang ||| qinyu li ||| 
2021 ||| multi scale attention network for crowd counting. ||| xiangpeng yang ||| xiaobo lu ||| 
2021 ||| object tracking algorithm based on channel-interconnection-spatial attention mechanism and siamese region proposal network. ||| junchang zhang ||| siqi lei ||| 
2020 ||| text sentiment classification based on lstm-tcn hybrid model and attention mechanism. ||| dong cao ||| yujie huang ||| hui li ||| xu zhao ||| qunhe zhao ||| yunbin fu ||| 
2021 ||| smoky vehicle detection based on improved vision transformer. ||| li yuan ||| shuzhen tong ||| xiaobo lu ||| 
2021 ||| implicit discourse relation classification based on semantic graph attention networks. ||| yuhao ma ||| yu yan ||| jie liu ||| 
2021 ||| ssd target detection algorithm based on multi-scale fusion and attention. ||| chengyang jin ||| lei li ||| mengting li ||| yijian pei ||| 
2021 ||| attention mechanism driven yolov3 on fpga acceleration for efficient vision based defect inspection. ||| longzhen yu ||| qian zhao ||| zhixian wang ||| 
2020 ||| attention v-net: a residual u-net with attention gate block for lung organs at risk segmentation. ||| zesen cheng ||| lijuan lai ||| tianyu zeng ||| sijuan huang ||| xin yang ||| 
2021 ||| spatial-temporal multi-head attention networks for traffic flow forecasting. ||| zhao zhang ||| ming liu ||| wenquan xu ||| 
2019 ||| knowledge graph completion via complete attention between knowledge graph and entity descriptions. ||| minjun zhao ||| yawei zhao ||| bing xu ||| 
2020 ||| self-attention and dynamic convolution hybrid model for neural machine translation. ||| zhebin zhang ||| sai wu ||| gang chen ||| dawei jiang ||| 
2021 ||| a character-word graph attention networks for chinese text classification. ||| shigang yang ||| yongguo liu ||| 
2018 ||| short-attention mechanism for generative dialogue system. ||| pengda si ||| yujiu yang ||| yi liu ||| 
2018 ||| stance detection with target and target towards attention. ||| wenqiang gao ||| yujiu yang ||| yi liu ||| 
2020 ||| prediction of financial big data stock trends based on attention mechanism. ||| jiannan chen ||| junping du ||| zhe xue ||| feifei kou ||| 
2018 ||| sentiment and semantic deep hierarchical attention neural network for fine grained news classification. ||| sri teja allaparthi ||| ganesh yaparla ||| vikram pudi ||| 
2020 ||| heterogeneous dynamic graph attention network. ||| qiuyan li ||| yanlei shang ||| xiuquan qiao ||| wei dai ||| 
2019 ||| adversarial graph attention network for multi-modal cross-modal retrieval. ||| hongchang wu ||| ziyu guan ||| tao zhi ||| wei zhao ||| cai xu ||| hong han ||| yaming yang ||| 
2019 ||| open-domain document-based automatic qa models based on cnn and attention mechanism. ||| guangjie zhang ||| xumin fan ||| canghong jin ||| minghui wu ||| 
2019 ||| large-scale video-based person re-identification via non-local attention and feature erasing. ||| zhao yang ||| zhigang chang ||| shibao zheng ||| 
2019 ||| convolutional-block-attention dual path networks for slide transition detection in lecture videos. ||| minhuang guan ||| kai li ||| ran ma ||| ping an ||| 
2019 ||| attention-based top-down single-task action recognition in still images. ||| jinhai yang ||| xiao zhou ||| hua yang ||| 
2019 ||| preliminary study on visual attention maps of experts and nonexperts when examining pathological microscopic images. ||| wangyang yu ||| menghan hu ||| shuning xu ||| qingli li ||| 
2017 ||| fault classification in transformer using low frequency component. ||| chaiyan jettanasen ||| atthapol ngaopitakkul ||| dimas anton asfani ||| i made yulistya negara ||| 
2021 ||| a 6.54-to-26.03 tops/w computing-in-memory rnn processor using input similarity optimization and attention-based context-breaking with output speculation. ||| ruiqi guo ||| hao li ||| ruhui liu ||| zhixiao zhang ||| limei tang ||| hao sun ||| leibo liu ||| meng-fan chang ||| shaojun wei ||| shouyi yin ||| 
2017 ||| attracting versus sustaining attention in the information economy. ||| yimiao zhang ||| kim huat goh ||| 
2020 ||| homicidal event forecasting and interpretable analysis using hierarchical attention model. ||| angeela acharya ||| jitin krishnan ||| desmond arias ||| huzefa rangwala ||| 
2020 ||| optimizing attention-aware opinion seeding strategies. ||| charles e. martin ||| dana warmsley ||| samuel d. johnson ||| 
2018 ||| aspect level sentiment classification with attention-over-attention neural networks. ||| binxuan huang ||| yanglan ou ||| kathleen m. carley ||| 
2021 ||| identifying shifts in collective attention to topics on social media. ||| yuzi he ||| ashwin rao ||| keith burghardt ||| kristina lerman ||| 
2021 ||| quasi character-level transformers to improve neural machine translation on small datasets. ||| salvador carri ||| n ||| francisco casacuberta ||| 
2021 ||| predicting human behavior with transformer considering the mutual relationship between categories and regions. ||| ryo osawa ||| keiichi suekane ||| ryoko nakamura ||| aozora inagaki ||| tomohiro takagi ||| isshu munemasa ||| 
2020 ||| attention-based lstm for automatic evaluation of press conferences. ||| shengzhou yi ||| koshiro mochitomi ||| isao suzuki ||| xueting wang ||| toshihiko yamasaki ||| 
2020 ||| aesthetics-assisted multi-task learning with attention for image memorability prediction. ||| tong zhu ||| feng zhu ||| hancheng zhu ||| leida li ||| 
2020 ||| cross-domain visual attention model adaption with one-shot gan. ||| daowei li ||| kui fu ||| yifan zhao ||| long xu ||| jia li ||| 
2021 ||| xm2a: multi-scale multi-head attention with cross-talk for multi-variate time series analysis. ||| yash garg ||| k. sel ||| uk candan ||| 
2019 ||| automatic generation of chinese couplets with attention based encoder-decoder model. ||| shengqiong yuan ||| luo zhong ||| lin li ||| rui zhang ||| 
2020 ||| video review analysis via transformer-based sentiment change detection. ||| zilong wu ||| siyuan huang ||| rui zhang ||| lin li ||| 
2021 ||| multimodal machine translation enhancement by fusing multimodal-attention and fine-grained image features. ||| lin li ||| turghun tayir ||| 
2021 ||| transformer based neural network for fine-grained classification of vehicle color. ||| yingjin wang ||| chuanming wang ||| yuchao zheng ||| huiyuan fu ||| huadong ma ||| 
2020 ||| learn to pay attention via switchable attention for image recognition. ||| qishang cheng ||| hongliang li ||| qingbo wu ||| fanman meng ||| linfeng xu ||| king ngi ngan ||| 
2021 ||| scarf: a semantic constrained attention refinement network for semantic segmentation. ||| xiaofeng ding ||| chaomin shen ||| zhengping che ||| tieyong zeng ||| yaxin peng ||| 
2021 ||| scat: stride consistency with auto-regressive regressor and transformer for hand pose estimation. ||| daiheng gao ||| bang zhang ||| qi wang ||| xindi zhang ||| pan pan ||| yinghui xu ||| 
2019 ||| weakly-supervised completion moment detection using temporal attention. ||| farnoosh heidarivincheh ||| majid mirmehdi ||| dima damen ||| 
2021 ||| where did i see it? object instance re-identification with attention. ||| vaibhav bansal ||| gian luca foresti ||| niki martinel ||| 
2017 ||| spatial attention improves object localization: a biologically plausible neuro-computational model for use in virtual reality. ||| amirhossein jamalian ||| julia bergelt ||| helge ulo dinkelbach ||| 
2021 ||| vtgan: semi-supervised retinal image synthesis and disease prediction using vision transformers. ||| sharif amit kamran ||| khondker fariha hossain ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| salah a. baker ||| 
2017 ||| dynamic computational time for visual attention. ||| zhichao li ||| yi yang ||| xiao liu ||| feng zhou ||| shilei wen ||| wei xu ||| 
2019 ||| spatial attention for multi-scale feature refinement for object detection. ||| haoran wang ||| zexin wang ||| meixia jia ||| aijin li ||| tuo feng ||| wenhua zhang ||| licheng jiao ||| 
2019 ||| indoor depth completion with boundary consistency and self-attention. ||| yu-kai huang ||| tsung-han wu ||| yueh-cheng liu ||| winston h. hsu ||| 
2021 ||| swinir: image restoration using swin transformer. ||| jingyun liang ||| jiezhang cao ||| guolei sun ||| kai zhang ||| luc van gool ||| radu timofte ||| 
2019 ||| learning spatiotemporal attention for egocentric action recognition. ||| minlong lu ||| danping liao ||| ze-nian li ||| 
2021 ||| semantic segmentation with multi scale spatial attention for self driving cars. ||| abhinav sagar ||| rajkumar soundrapandiyan ||| 
2019 ||| part matching with multi-level attention for person re-identification. ||| jiaze wang ||| 
2019 ||| a hvs-inspired attention to improve loss metrics for cnn-based perception-oriented super-resolution. ||| taimoor tariq ||| juan luis gonzalez bello ||| munchurl kim ||| 
2021 ||| self-attention agreement among capsules. ||| rita pucci ||| christian micheloni ||| niki martinel ||| 
2021 ||| semi-autoregressive transformer for image captioning. ||| yuanen zhou ||| yong zhang ||| zhenzhen hu ||| meng wang ||| 
2021 ||| siamsta: spatio-temporal attention based siamese tracker for tracking uavs. ||| bo huang ||| junjie chen ||| tingfa xu ||| ying wang ||| shenwang jiang ||| yuncheng wang ||| lei wang ||| jianan li ||| 
2019 ||| image super-resolution via attention based back projection networks. ||| zhi-song liu ||| li-wen wang ||| chu-tak li ||| wan-chi siu ||| yui-lam chan ||| 
2019 ||| retinal image classification via vasculature-guided sequential attention. ||| mengliu zhao ||| ghassan hamarneh ||| 
2021 ||| a transformer-based framework for automatic covid19 diagnosis in chest cts. ||| lei zhang ||| yan wen ||| 
2019 ||| eyenet: attention based convolutional encoder-decoder network for eye region segmentation. ||| priya kansal ||| sabarinathan devanathan ||| 
2021 ||| video transformer network. ||| daniel neimark ||| omri bar ||| maya zohar ||| dotan asselmann ||| 
2021 ||| transblast: self-supervised learning using augmented subspace with transformer for background/foreground separation. ||| islam i. osman ||| mohamed h. abdelpakey ||| mohamed s. shehata ||| 
2017 ||| understanding scenery quality: a visual attention measure and its computational model. ||| yuen peng loh ||| song tong ||| xuefeng liang ||| takatsune kumada ||| chee seng chan ||| 
2019 ||| dual attention mobdensenet(damdnet) for robust 3d face alignment. ||| lei jiang ||| xiao-jun wu ||| josef kittler ||| 
2019 ||| great ape detection in challenging jungle camera trap footage via attention-based spatial and temporal feature blending. ||| xinyu yang ||| majid mirmehdi ||| tilo burghardt ||| 
2021 ||| trans4trans: efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world. ||| jiaming zhang ||| kailun yang ||| angela constantinescu ||| kunyu peng ||| karin m ||| ller ||| rainer stiefelhagen ||| 
2021 ||| manet: a motion-driven attention network for detecting the pulse from a facial video with drastic motions. ||| xuenan liu ||| xuezhi yang ||| ziyan meng ||| ye wang ||| jie zhang ||| alexander wong ||| 
2021 ||| a computer vision-based attention generator using dqn. ||| jordan b. chipka ||| shuqing zeng ||| thanura r. elvitigala ||| priyantha mudalige ||| 
2017 ||| attending to distinctive moments: weakly-supervised attention models for action localization in video. ||| lei chen ||| mengyao zhai ||| greg mori ||| 
2019 ||| interpretable spatio-temporal attention for video action recognition. ||| lili meng ||| bo zhao ||| bo chang ||| gao huang ||| wei sun ||| frederick tung ||| leonid sigal ||| 
2019 ||| free-lunch saliency via attention in atari agents. ||| dmitry nikulin ||| anastasia ianina ||| vladimir aliev ||| sergey i. nikolenko ||| 
2021 ||| tiled squeeze-and-excite: channel attention with local spatial context. ||| niv vosco ||| alon shenkler ||| mark grobman ||| 
2021 ||| ressanet: a hybrid backbone of residual block and self-attention module for masked face recognition. ||| wei-yi chang ||| ming-ying tsai ||| shih-chieh lo ||| 
2019 ||| forced spatial attention for driver foot activity classification. ||| akshay rangesh ||| mohan m. trivedi ||| 
2021 ||| sa-det3d: self-attention based context-aware 3d object detection. ||| prarthana bhattacharyya ||| chengjie huang ||| krzysztof czarnecki ||| 
2019 ||| manipulation-skill assessment from videos with spatial attention network. ||| zhenqiang li ||| yifei huang ||| minjie cai ||| yoichi sato ||| 
2019 ||| image super-resolution via residual block attention networks. ||| tao dai ||| hua zha ||| yong jiang ||| shu-tao xia ||| 
2021 ||| efficientarl: improving skin cancer diagnoses by combining lightweight attention on efficientnet. ||| miguel nehmad alche ||| daniel acevedo ||| marta mejail ||| 
2021 ||| multimodal continuous visual attention mechanisms. ||| ant ||| nio farinhas ||| andr |||  f. t. martins ||| pedro m. q. aguiar ||| 
2019 ||| semantically consistent hierarchical text to fashion image synthesis with an enhanced-attentional generative adversarial network. ||| kenan emir ak ||| joo hwee lim ||| jo yew tham ||| ashraf a. kassim ||| 
2019 ||| residual attention graph convolutional network for geometric 3d scene classification. ||| albert mosella-montoro ||| javier ruiz hidalgo ||| 
2021 ||| learning tracking representations via dual-branch fully transformer networks. ||| fei xie ||| chunyu wang ||| guangting wang ||| wankou yang ||| wenjun zeng ||| 
2021 ||| convnets vs. transformers: whose visual representations are more transferable? ||| hong-yu zhou ||| chixiang lu ||| sibei yang ||| yizhou yu ||| 
2021 ||| studying the effects of self-attention for medical image analysis. ||| adrit rao ||| jongchan park ||| sanghyun woo ||| joon-young lee ||| oliver aalami ||| 
2019 ||| pose guided attention for multi-label fashion image classification. ||| beatriz quintino ferreira ||| jo ||| o paulo costeira ||| ricardo gamelas sousa ||| liang-yan gui ||| jo ||| o pedro gomes ||| 
2017 ||| temporal localization and spatial segmentation of joint attention in multiple first-person videos. ||| yifei huang ||| minjie cai ||| hiroshi kera ||| ryo yonetani ||| keita higuchi ||| yoichi sato ||| 
2019 ||| attention-translation-relation network for scalable scene graph generation. ||| nikolaos gkanatsios ||| vassilis pitsikalis ||| petros koutras ||| petros maragos ||| 
2021 ||| attention aware debiasing for unbiased model prediction. ||| puspita majumdar ||| richa singh ||| mayank vatsa ||| 
2019 ||| extreme low resolution action recognition with spatial-temporal multi-head self-attention and knowledge distillation. ||| didik purwanto ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2019 ||| reverse and boundary attention network for road segmentation. ||| jee-young sun ||| seung-wook kim ||| sang-won lee ||| ye-won kim ||| sung-jea ko ||| 
2019 ||| photometric transformer networks and label adjustment for breast density prediction. ||| jaehwan lee ||| donggeun yoo ||| hyo-eun kim ||| 
2021 ||| a unified efficient pyramid transformer for semantic segmentation. ||| fangrui zhu ||| yi zhu ||| li zhang ||| chongruo wu ||| yanwei fu ||| mu li ||| 
2019 ||| adherent raindrop removal with self-supervised attention maps and spatio-temporal generative adversarial networks. ||| stefano alletto ||| casey carlin ||| luca rigazio ||| yasunori ishii ||| sotaro tsukizawa ||| 
2019 ||| video multitask transformer network. ||| hongje seong ||| junhyuk hyun ||| euntai kim ||| 
2021 ||| audio-visual transformer based crowd counting. ||| usman sajid ||| xiangyu chen ||| hasan sajid ||| taejoon kim ||| guanghui wang ||| 
2021 ||| mila: multi-task learning from videos via efficient inter-frame attention. ||| donghyun kim ||| tian lan ||| chuhang zou ||| ning xu ||| bryan a. plummer ||| stan sclaroff ||| jayan eledath ||| g ||| rard g. medioni ||| 
2019 ||| attention-aware age-agnostic visual place recognition. ||| ziqi wang ||| jiahui li ||| seyran khademi ||| jan van gemert ||| 
2019 ||| motion-guided spatial time attention for video object segmentation. ||| qiang zhou ||| zilong huang ||| lichao huang ||| yongchao gong ||| han shen ||| wenyu liu ||| xinggang wang ||| 
2017 ||| human action recognition: pose-based attention draws focus to hands. ||| fabien baradel ||| christian wolf ||| julien mille ||| 
2019 ||| spatio-temporal attention network for video instance segmentation. ||| xiaoyu liu ||| haibing ren ||| tingmeng ye ||| 
2021 ||| medskip: medical report generation using skip connections and integrated attention. ||| esha pahwa ||| dwij mehta ||| sanjeet kapadia ||| devansh jain ||| achleshwar luthra ||| 
2019 ||| detecting visual relationships using box attention. ||| alexander kolesnikov ||| alina kuznetsova ||| christoph lampert ||| vittorio ferrari ||| 
2021 ||| the value of visual attention for covid-19 classification in ct scans. ||| adrit rao ||| jongchan park ||| oliver aalami ||| 
2021 ||| investigating transformers in the decomposition of polygonal shapes as point collections. ||| andrea alfieri ||| yancong lin ||| jan c. van gemert ||| 
2021 ||| background/foreground separation: guided attention based adversarial modeling (gaam) versus robust subspace learning methods. ||| maryam sultana ||| arif mahmood ||| thierry bouwmans ||| muhammad haris khan ||| soon ki jung ||| 
2017 ||| two-stream flow-guided convolutional attention networks for action recognition. ||| an tran ||| loong-fah cheong ||| 
2021 ||| saliency-guided transformer network combined with local embedding for no-reference image quality assessment. ||| mengmeng zhu ||| guanqun hou ||| xinjia chen ||| jiaxing xie ||| haixian lu ||| jun che ||| 
2019 ||| an indoor crowd detection network framework based on feature aggregation module and hybrid attention selection module. ||| wenxiang shen ||| pinle qin ||| jianchao zeng ||| 
2021 ||| vit-yolo: transformer-based yolo for object detection. ||| zixiao zhang ||| xiaoqiang lu ||| guojin cao ||| yuting yang ||| licheng jiao ||| fang liu ||| 
2017 ||| 3d morphable models as spatial transformer networks. ||| anil bas ||| patrik huber ||| william a. p. smith ||| muhammad awais ||| josef kittler ||| 
2019 ||| improving fashion landmark detection by dual attention feature enhancement. ||| ming chen ||| yingjie qin ||| lizhe qi ||| yunquan sun ||| 
2021 ||| tph-yolov5: improved yolov5 based on transformer prediction head for object detection on drone-captured scenarios. ||| xingkui zhu ||| shuchang lyu ||| xu wang ||| qi zhao ||| 
2021 ||| transformer meets part model: adaptive part division for person re-identification. ||| shenqi lai ||| zhenhua chai ||| xiaolin wei ||| 
2019 ||| two-stream video classification with cross-modality attention. ||| lu chi ||| guiyu tian ||| yadong mu ||| qi tian ||| 
2021 ||| leveraging batch normalization for vision transformers. ||| zhuliang yao ||| yue cao ||| yutong lin ||| ze liu ||| zheng zhang ||| han hu ||| 
2021 ||| an investigation of attention mechanisms in histopathology whole-slide-image analysis for regression objectives. ||| philippe weitz ||| yinxi wang ||| johan hartman ||| mattias rantalainen ||| 
2019 ||| cross-granularity attention network for semantic segmentation. ||| lingyu zhu ||| tinghuai wang ||| emre aksu ||| joni-kristian kamarainen ||| 
2021 ||| dyadformer: a multi-modal transformer for long-range modeling of dyadic interactions. ||| david curto ||| albert clap ||| s ||| javier selva ||| sorina smeureanu ||| j ||| lio c. s. jacques j ||| nior ||| david gallardo-pujol ||| georgina guilera ||| david leiva ||| thomas b. moeslund ||| sergio escalera ||| cristina palmero ||| 
2021 ||| abd-net: attention based decomposition network for 3d point cloud decomposition. ||| siddharth katageri ||| shashidhar veerappa kudari ||| akshaykumar gunari ||| ramesh ashok tabib ||| uma mudenagudi ||| 
2019 ||| attention routing between capsules. ||| jaewoong choi ||| hyun seo ||| suii im ||| myungjoo kang ||| 
2021 ||| skeletonnetv2: a dense channel attention blocks for skeleton extraction. ||| sabari nathan ||| priya kansal ||| 
2021 ||| pose transformers (potr): human motion prediction with non-autoregressive transformers. ||| ngel mart ||| nez-gonz ||| lez ||| michael villamizar ||| jean-marc odobez ||| 
2020 ||| coordinated movement for prosthesis reference trajectory generation: temporal factors and attention. ||| vijeth rai ||| abhishek sharma ||| pornthep preechayasomboon ||| eric rombokas ||| 
2018 ||| preliminary testing of a telerobotic haptic system and analysis of visual attention during a playful activity. ||| javier l. castellanos-cruz ||| maria f. gomez-medina ||| mahdi tavakoli ||| patrick m. pilarski ||| kim d. adams ||| 
2019 ||| patenttransformer-1.5: measuring patent claim generation by span relevancy. ||| jieh-sheng lee ||| jieh hsiang ||| 
2019 ||| eeg-based decoding of auditory attention to a target instrument in polyphonic music. ||| giorgia cantisani ||| slim essid ||| ga ||| l richard ||| 
2021 ||| df-conformer: integrated architecture of conv-tasnet and conformer using linear complexity self-attention for speech enhancement. ||| yuma koizumi ||| shigeki karita ||| scott wisdom ||| hakan erdogan ||| john r. hershey ||| llion jones ||| michiel bacchiani ||| 
2019 ||| end-to-end melody note transcription based on a beat-synchronous attention mechanism. ||| ryo nishikimi ||| eita nakamura ||| masataka goto ||| kazuyoshi yoshii ||| 
2019 ||| attention wave-u-net for speech enhancement. ||| ritwik giri ||| umut isik ||| arvindh krishnaswamy ||| 
2021 ||| detecting android malware based on dynamic feature sequence and attention mechanism. ||| hanlin long ||| zhicheng tian ||| yang liu ||| 
2019 ||| a biologically-inspired attentional approach for face recognition. ||| souad khellat-kihel ||| massimo tistarelli ||| 
2019 ||| automatic short answer grading via multiway attention networks. ||| tianqiao liu ||| wenbiao ding ||| zhiwei wang ||| jiliang tang ||| gale yan huang ||| zitao liu ||| 
2019 ||| improving short answer grading using transformer-based pre-training. ||| chul sung ||| tejas indulal dhamecha ||| nirmal mukhi ||| 
2021 ||| paraphrasing academic text: a study of back-translating anatomy and physiology with transformers. ||| andrew m. olney ||| 
2020 ||| learning from interpretable analysis: attention-based knowledge tracing. ||| jia zhu ||| weihao yu ||| zetao zheng ||| changqin huang ||| yong tang ||| gabriel pui cheong fung ||| 
2020 ||| the sound of inattention: predicting mind wandering with automatically derived features of instructor speech. ||| ian gliser ||| caitlin mills ||| nigel bosch ||| shelby smith ||| daniel smilek ||| jeffrey d. wammes ||| 
2020 ||| deep knowledge tracing with transformers. ||| shi pu ||| michael yudelson ||| lu ou ||| yuchi huang ||| 
2021 ||| dyadic joint visual attention interaction in face-to-face collaborative problem-solving at k-12 maths education: a multimodal approach. ||| chiao-wei yang ||| mutlu cukurova ||| kaska porayska-pomsta ||| 
2020 ||| deep-cross-attention recommendation model for knowledge sharing micro learning service. ||| jiayin lin ||| geng sun ||| jun shen ||| david pritchard ||| tingru cui ||| dongming xu ||| li li ||| ghassan beydoun ||| shiping chen ||| 
2019 ||| automatic construction of a phonics curriculum for reading education using the transformer neural network. ||| cassandra potier watkins ||| olivier dehaene ||| stanislas dehaene ||| 
2020 ||| investigating transformers for automatic short answer grading. ||| leon camus ||| anna filighera ||| 
2020 ||| scanpath analysis of student attention during problem solving with worked examples. ||| samantha stranc ||| kasia muldner ||| 
2017 ||| the impact of student individual differences and visual attention to pedagogical agents during learning with metatutor. ||| s ||| bastien lall ||| michelle taub ||| nicholas v. mudrick ||| cristina conati ||| roger azevedo ||| 
2019 ||| development of serious games for neurorehabilitation of children with attention-deficit/hyperactivity disorder through neurofeedback. ||| fabiana s. v. machado ||| wagner d. casagrande ||| anselmo frizera ||| flavia e. m. da rocha ||| 
2019 ||| trust-aware group recommendation with attention mechanism in social network. ||| jinghua zhu ||| zhichao li ||| chenbo yue ||| yong liu ||| 
2020 ||| call attention to stances: detect rumor with a stance attention network. ||| lingyu zeng ||| bin wu ||| bai wang ||| 
2018 ||| computer vision and internet of things: attention system in educational context. ||| teodor savov ||| valentina terzieva ||| katia todorova ||| 
2021 ||| dyngraphtrans: dynamic graph embedding via modified universal transformer networks for financial transaction data. ||| shilei zhang ||| toyotaro suzumura ||| li zhang ||| 
2020 ||| m2nn: rare event inference through multi-variate multi-scale attention. ||| manjusha ravindranath ||| k. sel ||| uk candan ||| maria luisa sapino ||| 
2021 ||| an efficient link prediction model in dynamic heterogeneous information networks based on multiple self-attention. ||| beibei ruan ||| cui zhu ||| 
2017 ||| an effective gated and attention-based neural network model for fine-grained financial target-dependent sentiment analysis. ||| mengxiao jiang ||| jianxiang wang ||| man lan ||| yuanbin wu ||| 
2019 ||| uafa: unsupervised attribute-friendship attention framework for user representation. ||| yuchen zhou ||| yanmin shang ||| yaman cao ||| yanbing liu ||| jianlong tan ||| 
2021 ||| landscape-enhanced graph attention network for rumor detection. ||| jianguo jiang ||| qiang liu ||| min yu ||| gang li ||| mingqi liu ||| chao liu ||| weiqing huang ||| 
2021 ||| medication combination prediction via attention neural networks with prior medical knowledge. ||| haiqiang wang ||| xuyuan dong ||| zheng luo ||| junyou zhu ||| peican zhu ||| chao gao ||| 
2020 ||| edge features enhanced graph attention network for relation extraction. ||| xuefeng bai ||| chong feng ||| huanhuan zhang ||| xiaomei wang ||| 
2021 ||| attentional neural factorization machines for knowledge tracing. ||| xiaowu zhang ||| li li ||| 
2018 ||| attention aware bidirectional gated recurrent unit based framework for sentiment analysis. ||| zhengxi tian ||| wenge rong ||| libin shi ||| jingshuang liu ||| zhang xiong ||| 
2021 ||| combining knowledge with attention neural networks for short text classification. ||| wei li ||| li li ||| 
2020 ||| document-improved hierarchical modular attention for event detection. ||| yiwei ni ||| qingfeng du ||| jincheng xu ||| 
2018 ||| fine-grained correlation learning with stacked co-attention networks for cross-modal information retrieval. ||| yuhang lu ||| jing yu ||| yanbing liu ||| jianlong tan ||| li guo ||| weifeng zhang ||| 
2021 ||| graph attention mechanism with cardinality preservation for knowledge graph completion. ||| cong ding ||| xiao wei ||| yongqi chen ||| rui zhao ||| 
2021 ||| text-aware recommendation model based on multi-attention neural networks. ||| gang qiu ||| xiaoli yu ||| liping jiang ||| baoying ma ||| 
2020 ||| robotic pushing and grasping knowledge learning via attention deep q-learning network. ||| zipeng yang ||| huiliang shang ||| 
2020 ||| moocrec: an attention meta-path based model for top-k recommendation in mooc. ||| deming sheng ||| jingling yuan ||| qing xie ||| pei luo ||| 
2021 ||| integrating task information into few-shot classifier by channel attention. ||| zhaochen li ||| kedian mu ||| 
2019 ||| multi-attention item recommendation model based on social relations. ||| yuan li ||| kedian mu ||| 
2019 ||| knowledge-aware self-attention networks for document grounded dialogue generation. ||| xiangru tang ||| po hu ||| 
2021 ||| logattn: unsupervised log anomaly detection with an autoencoder based attention mechanism. ||| linming zhang ||| wenzhong li ||| zhijie zhang ||| qingning lu ||| ce hou ||| peng hu ||| tong gui ||| sanglu lu ||| 
2021 ||| the novel efficient transformer for nlp. ||| benjamin mensa-bonsu ||| tao cai ||| tresor y. koffi ||| dejiao niu ||| 
2021 ||| attention based short-term metro passenger flow prediction. ||| ang gao ||| linjiang zheng ||| zixu wang ||| xuanxuan luo ||| congjun xie ||| yuankai luo ||| 
2019 ||| nrsa: neural recommendation with summary-aware attention. ||| qiyao peng ||| peiyi wang ||| wenjun wang ||| hongtao liu ||| yueheng sun ||| pengfei jiao ||| 
2020 ||| fine-tuned transformer model for sentiment analysis. ||| sishun liu ||| pengju shuai ||| xiaowu zhang ||| shuang chen ||| li li ||| ming liu ||| 
2019 ||| tagdeeprec: tag recommendation for software information sites using attention-based bi-lstm. ||| can li ||| ling xu ||| meng yan ||| jianjun he ||| zuli zhang ||| 
2021 ||| readmission prediction with knowledge graph attention and rnn-based ordinary differential equations. ||| su pei ||| ke niu ||| xueping peng ||| jingni zeng ||| 
2021 ||| similarity-based heterogeneous graph attention network for knowledge-enhanced recommendation. ||| fan zhang ||| rui li ||| ke xu ||| hongguang xu ||| 
2021 ||| a deep learning model based on neural bag-of-words attention for sentiment analysis. ||| jing liao ||| zhixiang yi ||| 
2020 ||| a hybrid model with pre-trained entity-aware transformer for relation extraction. ||| jinxin yao ||| min zhang ||| biyang wang ||| xianda xu ||| 
2020 ||| attention-based knowledge tracing with heterogeneous information network embedding. ||| nan zhang ||| ye du ||| ke deng ||| li li ||| jun shen ||| geng sun ||| 
2021 ||| aspect and opinion terms co-extraction using position-aware attention and auxiliary labels. ||| chao liu ||| xintong wei ||| min yu ||| gang li ||| xiangmei ma ||| jianguo jiang ||| weiqing huang ||| 
2019 ||| sequential recommendation based on long-term and short-term user behavior with self-attention. ||| xing wei ||| xianglin zuo ||| bo yang ||| 
2021 ||| sentence matching with deep self-attention and co-attention features. ||| zhipeng wang ||| danfeng yan ||| 
2019 ||| self-supervised attention model for weakly labeled audio event classification. ||| bongjun kim ||| shabnam ghaffarzadegan ||| 
2021 ||| spontaneous speech summarization: transformers all the way through. ||| tomoki hayashi ||| takenori yoshimura ||| masaya inuzuka ||| ibuki kuroyanagi ||| osamu segawa ||| 
2021 ||| attention-based distributed speech enhancement for unconstrained microphone arrays with varying number of nodes. ||| nicolas furnon ||| romain serizel ||| slim essid ||| irina illina ||| 
2020 ||| selective adaptation of end-to-end speech recognition using hybrid ctc/attention architecture for noise robustness. ||| cong-thanh do ||| shucong zhang ||| thomas hain ||| 
2020 ||| noise-robust attention learning for end-to-end speech recognition. ||| yosuke higuchi ||| naohiro tawara ||| atsunori ogawa ||| tomoharu iwata ||| tetsunori kobayashi ||| tetsuji ogawa ||| 
2021 ||| anomalous sound detection based on attention mechanism. ||| hayato mori ||| satoshi tamura ||| satoru hayamizu ||| 
2021 ||| wavetransformer: an architecture for audio captioning based on learning temporal and time-frequency information. ||| an tran ||| konstantinos drossos ||| tuomas virtanen ||| 
2020 ||| diagnosis of attention deficit and hyperactivity disorder (adhd) using hidden markov models. ||| maria camila maya-piedrahita ||| david c ||| rdenas-pe ||| a ||| lvaro- ||| ngel orozco-gutierrez ||| 
2021 ||| attention augmented cnns for musical instrument identification. ||| andrew wise ||| anthony s. maida ||| ashok kumar ||| 
2020 ||| exploiting attention-based sequence-to-sequence architectures for sound event localization. ||| christopher schymura ||| tsubasa ochiai ||| marc delcroix ||| keisuke kinoshita ||| tomohiro nakatani ||| shoko araki ||| dorothea kolossa ||| 
2019 ||| a new metric to evaluate auditory attention detection performance based on a markov chain. ||| simon geirnaert ||| tom francart ||| alexander bertrand ||| 
2020 ||| few-shot learning of signal modulation recognition based on attention relation network. ||| zilin zhang ||| yan li ||| meiguo gao ||| 
2021 ||| auditory attention decoding from eeg using convolutional recurrent neural network. ||| zhen fu ||| bo wang ||| xihong wu ||| jing chen ||| 
2017 ||| eeg-based attention-driven speech enhancement for noisy speech mixtures using n-fold multi-channel wiener filters. ||| neetha das ||| simon van eyndhoven ||| tom francart ||| alexander bertrand ||| 
2021 ||| speaker-aware speech enhancement with self-attention. ||| ju lin ||| adriaan j. van wijngaarden ||| melissa c. smith ||| kuang-ching wang ||| 
2021 ||| waveglove: transformer-based hand gesture recognition using multiple inertial sensors. ||| matej kr ||| lik ||| marek suppa ||| 
2021 ||| attention vs non-attention for a shapley-based explanation method. ||| tom kersten ||| hugh mee wong ||| jaap jumelet ||| dieuwke hupkes ||| 
2021 ||| kw-attn: knowledge infused attention for accurate and interpretable text classification. ||| hyeju jang ||| seojin bang ||| wen xiao ||| giuseppe carenini ||| raymond t. ng ||| young ji lee ||| 
2021 ||| transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. ||| zeyu yun ||| yubei chen ||| bruno a. olshausen ||| yann lecun ||| 
2019 ||| amcnet: attention-based multiscale convolutional network for dcm mri segmentation. ||| chao luo ||| canghong shi ||| xian zhang ||| jing peng ||| xiaojie li ||| yucheng chen ||| 
2020 ||| modeling an ar serious game to increase attention of adhd patients. ||| saad alqithami ||| 
2021 ||| learning to match workers and tasks via a multi-view graph attention network. ||| nan cui ||| chunqi chen ||| beijun shen ||| yuting chen ||| 
2021 ||| casr: a collaborative attention model for session-based recommendation. ||| peiyao han ||| nan wang ||| kun li ||| xiaokun li ||| 
2021 ||| using cognitive interest graph and knowledge-activated attention for learning resource recommendation. ||| zeyu he ||| jianzong kuang ||| wang li ||| yonghong yan ||| 
2021 ||| predicting entity relations across different security databases by using graph attention network. ||| liu yuan ||| yude bai ||| zhenchang xing ||| sen chen ||| xiaohong li ||| zhidong deng ||| 
2020 ||| accuracy improvement for neural program synthesis via attention mechanism and program slicing. ||| yating zhang ||| wei dong ||| daiyan wang ||| binbin liu ||| jiaxin liu ||| 
2021 ||| p4 transformer: towards unified programming for the data plane of software defined network. ||| zijun hang ||| yongjie wang ||| shuguang huang ||| 
2020 ||| few-shot ontology alignment model with attribute attentions. ||| jingyu sun ||| susumu takeuchi ||| ikuo yamasaki ||| 
2021 ||| attention guidance agents with eye-tracking - a use-case based on the matbii cockpit task. ||| szonya durant ||| benedict wilkins ||| callum woods ||| emanuele uliana ||| kostas stathis ||| 
2020 ||| : accelerating attention mechanisms in neural networks with approximation. ||| tae jun ham ||| sungjun jung ||| seonghak kim ||| young h. oh ||| yeonhong park ||| yoonho song ||| jung-hun park ||| sanghee lee ||| kyoung park ||| jae w. lee ||| deog-kyoon jeong ||| 
2021 ||| spatten: efficient sparse attention architecture with cascade token and head pruning. ||| hanrui wang ||| zhekai zhang ||| song han ||| 
2021 ||| loglab: attention-based labeling of log data anomalies via weak supervision. ||| thorsten wittkopp ||| philipp wiesner ||| dominik scheinert ||| alexander acker ||| 
2021 ||| an attention-based forecasting network for intelligent services in manufacturing. ||| xinyi zhou ||| xiaofeng gao ||| 
2021 ||| mma-net: a multimodal-attention-based deep neural network for web services classification. ||| jing zhang ||| changran lei ||| yilong yang ||| borui wang ||| yang chen ||| 
2021 ||| video abnormal event detection and location based on spatial attention. ||| zhenzhou guo ||| guangli wu ||| leiting li ||| 
2021 ||| $\mathcal{laja}{-}$ label attention transformer architectures for icd-10 coding of unstructured clinical notes. ||| veena mayya ||| s. sowmya kamath ||| vijayan sugumaran ||| 
2017 ||| attention estimation system via smart glasses. ||| oscal t.-c. chen ||| pin-chih chen ||| yi-ting tsai ||| 
2018 ||| forecasting user attention during everyday mobile interactions using device-integrated and wearable sensors. ||| julian steil ||| philipp m ||| ller ||| yusuke sugano ||| andreas bulling ||| 
2018 ||| image to latex with densenet encoder and joint attention. ||| jian wang ||| yunchuan sun ||| shenling wang ||| 
2019 ||| using automated state space planning for effective management of visual information and learner's attention in virtual reality. ||| opeoluwa ladeinde ||| mohammad abdur razzaque ||| the anh han ||| 
2019 ||| an introductory survey on attention mechanisms in nlp problems. ||| dichao hu ||| 
2018 ||| effective strategies for combining attention mechanism with lstm for aspect-level sentiment classification. ||| kai shuang ||| xintao ren ||| hao guo ||| jonathan loo ||| peng xu ||| 
2021 ||| enhancing lstm models with self-attention and stateful training. ||| alexander katrompas ||| vangelis metsis ||| 
2021 ||| sbilsan: stacked bidirectional self-attention lstm network for anomaly detection and diagnosis from system logs. ||| chenyu you ||| qiwen wang ||| chao sun ||| 
2021 ||| an attention-based deep learning model with interpretable patch-weight sharing for diagnosing cervical dysplasia. ||| jinyeong chae ||| ying zhang ||| roger zimmermann ||| dongho kim ||| jihie kim ||| 
2021 ||| one-class self-attention model for anomaly detection in manufacturing lines. ||| linh le ||| srivatsa mallapragada ||| shashank hebbar ||| david a. guerra-zubiaga ||| 
2020 ||| adaptive attention mechanism based semantic compositional network for video captioning. ||| zhaoyu dong ||| xian zhong ||| shuqin chen ||| wenxuan liu ||| qi cui ||| luo zhong ||| 
2021 ||| reputation analysis based on weakly-supervised bi-lstm-attention network. ||| kun xiang ||| akihiro fujii ||| 
2021 ||| attention-enabled object detection to improve one-stage tracker. ||| neelu madan ||| kamal nasrollahi ||| thomas b. moeslund ||| 
2020 ||| image denoising using attention-residual convolutional neural networks. ||| rafael goncalves pires ||| daniel f. s. santos ||| cl ||| udio f. g. santos ||| marcos c. s. santana ||| jo ||| o p. papa ||| 
2020 ||| a lightweight 2d pose machine with attention enhancement. ||| luiz jos |||  schirmer silva ||| djalma l ||| cio ||| alberto raposo ||| luiz velho ||| h ||| lio lopes ||| 
2020 ||| superpixel image classification with graph attention networks. ||| pedro h. c. avelar ||| anderson r. tavares ||| thiago l. t. da silveira ||| cl ||| udio r. jung ||| lu ||| s c. lamb ||| 
2021 ||| tvanet: a spatial and feature-based attention model for self-driving car. ||| victor flores-benites ||| carlos mugruza-vassallo ||| rensso mora colque ||| 
2021 ||| fast spatial-temporal transformer network. ||| rafael molossi escher ||| rodrigo andrade de bem ||| paulo lilles jorge drews junior ||| 
2021 ||| gaze estimation via self-attention augmented convolutions. ||| gabriel lefundes vieira ||| luciano oliveira ||| 
2021 ||| sgat: semantic graph attention for 3d human pose estimation. ||| luiz schirmer ||| djalma lucio ||| leandro cruz ||| alberto barbosa raposo ||| luiz velho ||| h ||| lio lopes ||| 
2020 ||| multilingual joint fine-tuning of transformer models for identifying trolling, aggression and cyberbullying at trac 2020. ||| sudhanshu mishra ||| shivangi prasad ||| shubhanshu mishra ||| 
2020 ||| the go transformer: natural language modeling for game play. ||| matthew ciolino ||| josh kalin ||| david noever ||| 
2021 ||| towards vulnerability types classification using pure self-attention: a common weakness enumeration based approach. ||| tianyi wang ||| shengzhi qin ||| kam-pui chow ||| 
2021 ||| extracting discriminative features for cross-view gait recognition based on the attention mechanism. ||| ruicheng sun ||| shuo han ||| weihang peng ||| hanxiang zhuang ||| xin zeng ||| xingang liu ||| 
2021 ||| multimodal aesthetic analysis assisted by styles through a multimodal co-transformer model. ||| haotian miao ||| yifei zhang ||| daling wang ||| shi feng ||| 
2021 ||| a novel sentiment classification based on "word-phrase" attention mechanism. ||| guangyao pang ||| guobei peng ||| zizhen peng ||| jie he ||| yan yang ||| zhiyi mo ||| 
2017 ||| modelling an intelligent interaction system for increasing the level of attention. ||| dalila dur ||| es ||| david castro ||| javier bajo ||| paulo novais ||| 
2018 ||| supervising attention in an e-learning system. ||| dalila dur ||| es ||| javier bajo ||| paulo novais ||| 
2017 ||| an algorithm for simulating human selective attention. ||| giovanna broccia ||| paolo milazzo ||| peter csaba  ||| lveczky ||| 
2021 ||| detectornet: transformer-enhanced spatial temporal graph neural network for traffic prediction. ||| he li ||| shiyu zhang ||| xuejiao li ||| liangcai su ||| hongjie huang ||| duo jin ||| linghao chen ||| jianbin huang ||| jaesoo yoo ||| 
2021 ||| attention-based spatial interpolation for house price prediction. ||| darniton viana ||| luciano barbosa ||| 
2021 ||| geo-attention network for traffic condition prediction and travel time estimation. ||| jiezhang li ||| wanyi zhou ||| zebin chen ||| yue-jiao gong ||| 
2021 ||| dual-attention multi-scale graph convolutional networks for highway accident delay time prediction. ||| i-ying wu ||| fandel lin ||| hsun-ping hsieh ||| 
2019 ||| bike-share demand prediction using attention based sequence to sequence and conditional variational autoencoder. ||| tomohiro mimura ||| shin ishiguro ||| satoshi kawasaki ||| yusuke fukazawa ||| 
2020 ||| multi-scale feature fusion uav image object detection method based on dilated convolution and attention mechanism. ||| yuanzhu liu ||| zhiming ding ||| yang cao ||| mengmeng chang ||| 
2019 ||| session-based recommendation with context-aware attention network. ||| jinsheng wu ||| zhonghong ou ||| meina song ||| 
2019 ||| an efficient non-local attention network for video-based person re-identification. ||| zhen wang ||| shixian luo ||| he sun ||| huadong pan ||| jun yin ||| 
2019 ||| traffic flow prediction based on self-attention mechanism and deep packet residual network. ||| xuebin jia ||| tong li ||| rui zhu ||| zhan wang ||| zehui zhang ||| jiawei wang ||| 
2020 ||| a hierarchical attention-based neural network model for socialbot detection in osn. ||| mohd fazil ||| amit kumar sah ||| muhammad abulaish ||| 
2018 ||| fine-grained deep knowledge-aware network for news recommendation with self-attention. ||| jie gao ||| xin xin ||| junshuai liu ||| rui wang ||| jing lu ||| biao li ||| xin fan ||| ping guo ||| 
2017 ||| emotion and attention: predicting electrodermal activity through video visual descriptors. ||| alejandro hern ||| ndez-garc ||| a ||| fernando fern ||| ndez mart ||| nez ||| fernando d ||| az-de-mar ||| a ||| 
2020 ||| ggtan: graph gated talking-heads attention networks for traveling salesman problem. ||| shichao guo ||| yang xiao ||| lingfeng niu ||| 
2019 ||| an efficient co-attention neural network for social recommendation. ||| munan li ||| kenji tei ||| yoshiaki fukazawa ||| 
2021 ||| visual attention analysis and user guidance in cinematic vr film. ||| haoshuo wang ||| colm o. fearghail ||| emin zerman ||| karsten braungart ||| aljosa smolic ||| sebastian knorr ||| 
2017 ||| ethical issues of a smart system to enhance students' attention. ||| andreia art ||| fice ||| jo ||| o sarraipa ||| ricardo jardim-gon ||| alves ||| juan carlos guevara ||| manuella kadar ||| 
2020 ||| history repeats itself: human motion prediction via motion attention. ||| wei mao ||| miaomiao liu ||| mathieu salzmann ||| 
2018 ||| knowing when to look for what and where: evaluating generation of spatial descriptions with adaptive attention. ||| mehdi ghanimifard ||| simon dobnik ||| 
2020 ||| character region attention for text spotting. ||| youngmin baek ||| seung shin ||| jeonghun baek ||| sungrae park ||| junyeop lee ||| daehyun nam ||| hwalsuk lee ||| 
2020 ||| a dual residual network with channel attention for image restoration. ||| shichao nie ||| chengconghui ma ||| dafan chen ||| shuting yin ||| haoran wang ||| licheng jiao ||| fang liu ||| 
2020 ||| example-guided image synthesis using masked spatial-channel attention and self-supervision. ||| haitian zheng ||| haofu liao ||| lele chen ||| wei xiong ||| tianlang chen ||| jiebo luo ||| 
2020 ||| a recurrent transformer network for novel view action synthesis. ||| kara marie schatz ||| erik quintanilla ||| shruti vyas ||| yogesh singh rawat ||| 
2018 ||| dependency-aware attention control for unconstrained face recognition with image sets. ||| xiaofeng liu ||| b. v. k. vijaya kumar ||| chao yang ||| qingming tang ||| jane you ||| 
2020 ||| empowering relational network by self-attention augmented conditional random fields for group activity recognition. ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2020 ||| progressive transformers for end-to-end sign language production. ||| ben saunders ||| necati cihan camg ||| z ||| richard bowden ||| 
2018 ||| attention-gan for object transfiguration in wild images. ||| xinyuan chen ||| chang xu ||| xiaokang yang ||| dacheng tao ||| 
2020 ||| gatcluster: self-supervised gaussian-attention network for image clustering. ||| chuang niu ||| jun zhang ||| ge wang ||| jimin liang ||| 
2020 ||| feature pyramid transformer. ||| dong zhang ||| hanwang zhang ||| jinhui tang ||| meng wang ||| xiansheng hua ||| qianru sun ||| 
2018 ||| look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss. ||| jianbo jiao ||| ying cao ||| yibing song ||| rynson w. h. lau ||| 
2020 ||| multi-modal transformer for video retrieval. ||| valentin gabeur ||| chen sun ||| karteek alahari ||| cordelia schmid ||| 
2020 ||| hand-transformer: non-autoregressive structured modeling for 3d hand pose estimation. ||| lin huang ||| jianchao tan ||| ji liu ||| junsong yuan ||| 
2018 ||| pairwise body-part attention for recognizing human-object interactions. ||| haoshu fang ||| jinkun cao ||| yu-wing tai ||| cewu lu ||| 
2020 ||| efficient image super-resolution using pixel attention. ||| hengyuan zhao ||| xiangtao kong ||| jingwen he ||| yu qiao ||| chao dong ||| 
2020 ||| an attention-driven two-stage clustering method for unsupervised person re-identification. ||| zilong ji ||| xiaolong zou ||| xiaohan lin ||| xiao liu ||| tiejun huang ||| si wu ||| 
2020 ||| cafe-gan: arbitrary face attribute editing with complementary attention feature. ||| jeong-gi kwak ||| david k. han ||| hanseok ko ||| 
2018 ||| image super-resolution using very deep residual channel attention networks. ||| yulun zhang ||| kunpeng li ||| kai li ||| lichen wang ||| bineng zhong ||| yun fu ||| 
2020 ||| spatially aware multimodal transformers for textvqa. ||| yash kant ||| dhruv batra ||| peter anderson ||| alexander g. schwing ||| devi parikh ||| jiasen lu ||| harsh agrawal ||| 
2018 ||| deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization. ||| guoliang kang ||| liang zheng ||| yan yan ||| yi yang ||| 
2018 ||| reinforced temporal attention and split-rate transfer for depth-based person re-identification. ||| nikolaos karianakis ||| zicheng liu ||| yinpeng chen ||| stefano soatto ||| 
2018 ||| temporal attention mechanism with conditional inference for large-scale multi-label video classification. ||| eun-sol kim ||| kyoung-woon on ||| jongseok kim ||| yu-jung heo ||| seong-ho choi ||| hyun-dong lee ||| byoung-tak zhang ||| 
2020 ||| c4av: learning cross-modal representations from transformers. ||| shujie luo ||| hang dai ||| ling shao ||| yong ding ||| 
2020 ||| solar: second-order loss and attention for image retrieval. ||| tony ng ||| vassileios balntas ||| yurun tian ||| krystian mikolajczyk ||| 
2020 ||| few-shot semantic segmentation with democratic attention networks. ||| haochen wang ||| xudong zhang ||| yutao hu ||| yandan yang ||| xianbin cao ||| xiantong zhen ||| 
2018 ||| attention-aware deep adversarial hashing for cross-modal retrieval. ||| xi zhang ||| hanjiang lai ||| jiashi feng ||| 
2020 ||| attentionnas: spatiotemporal attention cell search for video classification. ||| xiaofang wang ||| xuehan xiong ||| maxim neumann ||| a. j. piergiovanni ||| michael s. ryoo ||| anelia angelova ||| kris m. kitani ||| wei hua ||| 
2020 ||| monocular expressive body regression through body-driven attention. ||| vasileios choutas ||| georgios pavlakos ||| timo bolkart ||| dimitrios tzionas ||| michael j. black ||| 
2018 ||| reverse attention for salient object detection. ||| shuhan chen ||| xiuli tan ||| ben wang ||| xuelong hu ||| 
2018 ||| cbam: convolutional block attention module. ||| sanghyun woo ||| jongchan park ||| joon-young lee ||| in so kweon ||| 
2020 ||| da4ad: end-to-end deep attention-based visual localization for autonomous driving. ||| yao zhou ||| guowei wan ||| shenhua hou ||| li yu ||| gang wang ||| xiaofei rui ||| shiyu song ||| 
2018 ||| question type guided attention in visual question answering. ||| yang shi ||| tommaso furlanello ||| sheng zha ||| animashree anandkumar ||| 
2020 ||| box2seg: attention weighted loss and discriminative feature learning for weakly supervised segmentation. ||| viveka kulharia ||| siddhartha chandra ||| amit agrawal ||| philip h. s. torr ||| ambrish tyagi ||| 
2020 ||| single image super-resolution via a holistic attention network. ||| ben niu ||| weilei wen ||| wenqi ren ||| xiangde zhang ||| lianping yang ||| shuzhen wang ||| kaihao zhang ||| xiaochun cao ||| haifeng shen ||| 
2020 ||| attention-based query expansion learning. ||| albert gordo ||| filip radenovic ||| tamara berg ||| 
2020 ||| semantic line detection using mirror attention and comparative ranking and matching. ||| dongkwon jin ||| jun-tae lee ||| chang-su kim ||| 
2020 ||| weight excitation: built-in attention mechanisms in convolutional neural networks. ||| niamul quader ||| md mafijul islam bhuiyan ||| juwei lu ||| peng dai ||| wei li ||| 
2018 ||| bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. ||| lei zhu ||| zijun deng ||| xiaowei hu ||| chi-wing fu ||| xuemiao xu ||| jing qin ||| pheng-ann heng ||| 
2020 ||| assemblenet++: assembling modality representations via attention connections. ||| michael s. ryoo ||| a. j. piergiovanni ||| juhana kangaspunta ||| anelia angelova ||| 
2020 ||| read: reciprocal attention discriminator for image-to-video re-identification. ||| minho shim ||| hsuan-i ho ||| jinhyung kim ||| dongyoon wee ||| 
2018 ||| multimodal dual attention memory for video story question answering. ||| kyung-min kim ||| seong-ho choi ||| jin-hwa kim ||| byoung-tak zhang ||| 
2020 ||| dmd: a large-scale multi-modal driver monitoring dataset for attention and alertness analysis. ||| juan diego ortega ||| neslihan kose ||| paola ca ||| as ||| min-an chao ||| alexander unnervik ||| marcos nieto ||| oihana otaegui ||| luis salgado ||| 
2018 ||| psanet: point-wise spatial attention network for scene parsing. ||| hengshuang zhao ||| yi zhang ||| shu liu ||| jianping shi ||| chen change loy ||| dahua lin ||| jiaya jia ||| 
2020 ||| axial-deeplab: stand-alone axial-attention for panoptic segmentation. ||| huiyu wang ||| yukun zhu ||| bradley green ||| hartwig adam ||| alan l. yuille ||| liang-chieh chen ||| 
2020 ||| span: spatial pyramid attention network for image manipulation localization. ||| xuefeng hu ||| zhihan zhang ||| zhenye jiang ||| syomantak chaudhuri ||| zhenheng yang ||| ram nevatia ||| 
2018 ||| attend and rectify: a gated attention mechanism for fine-grained recovery. ||| pau rodr ||| guez ||| josep m. gonfaus ||| guillem cucurull ||| f. xavier roca ||| jordi gonz ||| lez ||| 
2018 ||| mancs: a multi-task attentional network with curriculum sampling for person re-identification. ||| cheng wang ||| qian zhang ||| chang huang ||| wenyu liu ||| xinggang wang ||| 
2018 ||| stacked cross attention for image-text matching. ||| kuang-huei lee ||| xi chen ||| gang hua ||| houdong hu ||| xiaodong he ||| 
2018 ||| single image water hazard detection using fcn with reflection attention units. ||| xiaofeng han ||| chuong v. nguyen ||| shaodi you ||| jianfeng lu ||| 
2018 ||| boosted attention: leveraging human attention for image captioning. ||| shi chen ||| qi zhao ||| 
2020 ||| end-to-end low cost compressive spectral imaging with spatial-spectral self-attention. ||| ziyi meng ||| jiawei ma ||| xin yuan ||| 
2020 ||| end-to-end object detection with transformers. ||| nicolas carion ||| francisco massa ||| gabriel synnaeve ||| nicolas usunier ||| alexander kirillov ||| sergey zagoruyko ||| 
2018 ||| spatial-temporal attention res-tcn for skeleton-based dynamic hand gesture recognition. ||| jingxuan hou ||| guijin wang ||| xinghao chen ||| jing-hao xue ||| rui zhu ||| huazhong yang ||| 
2018 ||| connecting gaze, scene, and attention: generalized attention estimation via joint modeling of gaze and scene saliency. ||| eunji chong ||| nataniel ruiz ||| yongxin wang ||| yun zhang ||| agata rozga ||| james m. rehg ||| 
2020 ||| few-shot action recognition with permutation-invariant attention. ||| hongguang zhang ||| li zhang ||| xiaojuan qi ||| hongdong li ||| philip h. s. torr ||| piotr koniusz ||| 
2020 ||| air: attention with reasoning capability. ||| shi chen ||| ming jiang ||| jinhui yang ||| qi zhao ||| 
2020 ||| spatio-temporal graph transformer networks for pedestrian trajectory prediction. ||| cunjun yu ||| xiao ma ||| jiawei ren ||| haiyu zhao ||| shuai yi ||| 
2020 ||| password-conditioned anonymization and deanonymization with face identity transformers. ||| xiuye gu ||| weixin luo ||| michael s. ryoo ||| yong jae lee ||| 
2018 ||| generative adversarial network with spatial attention for face attribute editing. ||| gang zhang ||| meina kan ||| shiguang shan ||| xilin chen ||| 
2018 ||| deep imbalanced attribute classification using visual attention aggregation. ||| nikolaos sarafianos ||| xiang xu ||| ioannis a. kakadiaris ||| 
2020 ||| deep reinforced attention learning for quality-aware visual recognition. ||| duo li ||| qifeng chen ||| 
2018 ||| deep residual attention network for spectral image super-resolution. ||| zhan shi ||| chang chen ||| zhiwei xiong ||| dong liu ||| zheng-jun zha ||| feng wu ||| 
2020 ||| pynet-ca: enhanced pynet with channel attention for end-to-end mobile image signal processing. ||| byung-hoon kim ||| joonyoung song ||| jong chul ye ||| jaehyun baek ||| 
2020 ||| pyramidal edge-maps and attention based guided thermal super-resolution. ||| honey gupta ||| kaushik mitra ||| 
2020 ||| deep surface normal estimation on the 2-sphere with confidence guided semantic attention. ||| quewei li ||| jie guo ||| yang fei ||| qinyu tang ||| wenxiu sun ||| jin zeng ||| yanwen guo ||| 
2018 ||| fine-grained video categorization with redundancy reduction attention. ||| chen zhu ||| xiao tan ||| feng zhou ||| xiao liu ||| kaiyu yue ||| errui ding ||| yi ma ||| 
2018 ||| knowing where to look? analysis on attention of visual question answering system. ||| wei li ||| zehuan yuan ||| xiangzhong fang ||| changhu wang ||| 
2020 ||| learning trailer moments in full-length movies with co-contrastive attention. ||| lezi wang ||| dong liu ||| rohit puri ||| dimitris n. metaxas ||| 
2020 ||| attend and segment: attention guided active semantic segmentation. ||| soroush seifi ||| tinne tuytelaars ||| 
2020 ||| feedback attention for cell image segmentation. ||| hiroki tsuda ||| eisuke shibuya ||| kazuhiro hotta ||| 
2020 ||| attention guided anomaly localization in images. ||| shashanka venkataramanan ||| kuan-chuan peng ||| rajat vikram singh ||| abhijit mahalanobis ||| 
2020 ||| the devil is in the details: self-supervised attention for vehicle re-identification. ||| pirazh khorramshahi ||| neehar peri ||| jun-cheng chen ||| rama chellappa ||| 
2018 ||| attention-based ensemble for deep metric learning. ||| wonsik kim ||| bhavya goyal ||| kunal chawla ||| jungmin lee ||| keunjoo kwon ||| 
2020 ||| efficient attention mechanism for visual dialog that can handle all the interactions between multiple inputs. ||| van-quang nguyen ||| masanori suganuma ||| takayuki okatani ||| 
2020 ||| multi-attention based ultra lightweight image super-resolution. ||| abdul muqeet ||| jiwon hwang ||| subin yang ||| jung heum kang ||| yongwoo kim ||| sung-ho bae ||| 
2020 ||| spatial attention pyramid network for unsupervised domain adaptation. ||| congcong li ||| dawei du ||| libo zhang ||| longyin wen ||| tiejian luo ||| yanjun wu ||| pengfei zhu ||| 
2018 ||| spatio-temporal transformer network for video restoration. ||| tae hyun kim ||| mehdi s. m. sajjadi ||| michael hirsch ||| bernhard sch ||| lkopf ||| 
2018 ||| "factual" or "emotional": stylized image captioning with adaptive learning and attention. ||| tianlang chen ||| zhongping zhang ||| quanzeng you ||| chen fang ||| zhaowen wang ||| hailin jin ||| jiebo luo ||| 
2020 ||| forecasting human-object interaction: joint prediction of motor attention and actions in first person video. ||| miao liu ||| siyu tang ||| yin li ||| james m. rehg ||| 
2020 ||| multi-channel transformers for multi-articulatory sign language translation. ||| necati cihan camg ||| z ||| oscar koller ||| simon hadfield ||| richard bowden ||| 
2020 ||| suppressing mislabeled data via grouping and self-attention. ||| xiaojiang peng ||| kai wang ||| zhaoyang zeng ||| qing li ||| jianfei yang ||| yu qiao ||| 
2020 ||| unsupervised domain attention adaptation network for caricature attribute recognition. ||| wen ji ||| kelei he ||| jing huo ||| zheng gu ||| yang gao ||| 
2018 ||| video object segmentation with joint re-identification and attention-aware mask propagation. ||| xiaoxiao li ||| chen change loy ||| 
2020 ||| look here! a parametric learning based approach to redirect visual attention. ||| youssef a. mejjati ||| celso f. gomez ||| kwang in kim ||| eli shechtman ||| zoya bylinskii ||| 
2020 ||| attention-driven dynamic graph convolutional network for multi-label image recognition. ||| jin ye ||| junjun he ||| xiaojiang peng ||| wenhao wu ||| yu qiao ||| 
2018 ||| learning visual question answering by bootstrapping hard attention. ||| mateusz malinowski ||| carl doersch ||| adam santoro ||| peter w. battaglia ||| 
2018 ||| predicting gaze in egocentric video by learning task-dependent attention transition. ||| yifei huang ||| minjie cai ||| zhenqiang li ||| yoichi sato ||| 
2020 ||| supervised edge attention network for accurate image instance segmentation. ||| xier chen ||| yanchao lian ||| licheng jiao ||| haoran wang ||| yanjie gao ||| lingling shi ||| 
2020 ||| attention enhanced single stage multimodal reasoner. ||| jie ou ||| xinying zhang ||| 
2018 ||| deep adaptive attention for joint facial action unit detection and face alignment. ||| zhiwen shao ||| zhilei liu ||| jianfei cai ||| lizhuang ma ||| 
2018 ||| give ear to my face: modelling multimodal attention to social interactions. ||| giuseppe boccignone ||| vittorio cuculo ||| alessandro d'amelio ||| giuliano grossi ||| raffaella lanzarotti ||| 
2020 ||| volumetric transformer networks. ||| seungryong kim ||| sabine s ||| sstrunk ||| mathieu salzmann ||| 
2018 ||| online multi-object tracking with dual matching attention networks. ||| ji zhu ||| hua yang ||| nian liu ||| minyoung kim ||| wenjun zhang ||| ming-hsuan yang ||| 
2018 ||| deepphys: video-based physiological measurement using convolutional attention networks. ||| weixuan chen ||| daniel j. mcduff ||| 
2020 ||| attention deeplabv3+: multi-level context attention mechanism for skin lesion segmentation. ||| reza azad ||| maryam asadi-aghbolaghi ||| mahmood fathy ||| sergio escalera ||| 
2020 ||| how to track your dragon: a multi-attentional framework for real-time rgb-d 6-dof object pose tracking. ||| isidoros marougkas ||| petros koutras ||| nikolaos kardaris ||| georgios retsinas ||| georgia chalvatzaki ||| petros maragos ||| 
2020 ||| attngrounder: talking to cars with attention. ||| vivek mittal ||| 
2020 ||| self-calibrated attention neural network for real-world super resolution. ||| kaihua cheng ||| chenhuan wu ||| 
2020 ||| orientation-aware vehicle re-identification with semantics-guided part attention network. ||| tsai-shien chen ||| chih-ting liu ||| chih-wei wu ||| shao-yi chien ||| 
2020 ||| take an emotion walk: perceiving emotions from gaits using hierarchical attention pooling and affective mapping. ||| uttaran bhattacharya ||| christian roncal ||| trisha mittal ||| rohan chandra ||| kyra kapsaskis ||| kurt gray ||| aniket bera ||| dinesh manocha ||| 
2018 ||| agil: learning attention from human for visuomotor tasks. ||| ruohan zhang ||| zhuode liu ||| luxin zhang ||| jake a. whritner ||| karl s. muller ||| mary m. hayhoe ||| dana h. ballard ||| 
2018 ||| egocentric activity prediction via event modulated attention. ||| yang shen ||| bingbing ni ||| zefan li ||| ning zhuang ||| 
2018 ||| multi-attention multi-class constraint for fine-grained image recognition. ||| ming sun ||| yuchen yuan ||| feng zhou ||| errui ding ||| 
2018 ||| interaction-aware spatio-temporal pyramid attention networks for action classification. ||| yang du ||| chunfeng yuan ||| bing li ||| lili zhao ||| yangxi li ||| weiming hu ||| 
2018 ||| deep fashion analysis with feature map upsampling and landmark-driven attention. ||| jingyuan liu ||| hong lu ||| 
2020 ||| we learn better road pothole detection: from attention aggregation to adversarial domain adaptation. ||| rui fan ||| hengli wang ||| mohammud junaid bocus ||| ming liu ||| 
2020 ||| cross-attention in coupled unmixing nets for unsupervised hyperspectral super-resolution. ||| jing yao ||| danfeng hong ||| jocelyn chanussot ||| deyu meng ||| xiaoxiang zhu ||| zongben xu ||| 
2020 ||| unsupervised deep metric learning with transformed attention consistency and contrastive clustering loss. ||| yang li ||| shichao kan ||| zhihai he ||| 
2020 ||| guiding monocular depth estimation using depth-attention volume. ||| lam huynh ||| phong nguyen-ha ||| jiri matas ||| esa rahtu ||| janne heikkil ||| 
2018 ||| deep attention neural tensor network for visual question answering. ||| yalong bai ||| jianlong fu ||| tiejun zhao ||| tao mei ||| 
2020 ||| visbert: hidden-state visualizations for transformers. ||| betty van aken ||| benjamin winter ||| alexander l ||| ser ||| felix a. gers ||| 
2019 ||| dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. ||| qitian wu ||| hengrui zhang ||| xiaofeng gao ||| peng he ||| paul weng ||| han gao ||| guihai chen ||| 
2018 ||| deepmove: predicting human mobility with attentional recurrent networks. ||| jie feng ||| yong li ||| chao zhang ||| funing sun ||| fanchao meng ||| ang guo ||| depeng jin ||| 
2021 ||| galaxc: graph neural networks with labelwise attention for extreme classification. ||| deepak saini ||| arnav kumar jain ||| kushal dave ||| jian jiao ||| amit singh ||| ruofei zhang ||| manik varma ||| 
2020 ||| html: hierarchical transformer-based multi-task learning for volatility prediction. ||| linyi yang ||| tin lok james ng ||| barry smyth ||| ruihai dong ||| 
2020 ||| domain adaptation with category attention network for deep sentiment analysis. ||| dongbo xi ||| fuzhen zhuang ||| ganbin zhou ||| xiaohu cheng ||| fen lin ||| qing he ||| 
2020 |||  vu: a contextualized temporal attention mechanism for sequential recommendation. ||| jibang wu ||| renqin cai ||| hongning wang ||| 
2021 ||| ntam: neighborhood-temporal attention model for disk failure prediction in cloud platforms. ||| chuan luo ||| pu zhao ||| bo qiao ||| youjiang wu ||| hongyu zhang ||| wei wu ||| weihai lu ||| yingnong dang ||| saravanakumar rajmohan ||| qingwei lin ||| dongmei zhang ||| 
2018 ||| : attention based autoencoder for rap lyrics representation learning. ||| hongru liang ||| qian li ||| haozheng wang ||| hang li ||| jin-mao wei ||| zhenglu yang ||| 
2021 ||| l3i_lbpam at the finsim-2 task: learning financial semantic similarities with siamese transformers. ||| nhu khoa nguyen ||| emanuela boros ||| ga ||| l lejeune ||| antoine doucet ||| thierry delahaut ||| 
2020 ||| earn more social attention: user popularity based tag recommendation system. ||| xueting wang ||| yiwei zhang ||| toshihiko yamasaki ||| 
2020 ||| an end-to-end topic-enhanced self-attention network for social emotion classification. ||| chang wang ||| bang wang ||| 
2018 ||| an attention factor graph model for tweet entity linking. ||| chenwei ran ||| wei shen ||| jianyong wang ||| 
2019 ||| inferring search queries from web documents via a graph-augmented sequence to attention network. ||| fred x. han ||| di niu ||| kunfeng lai ||| weidong guo ||| yancheng he ||| yu xu ||| 
2021 ||| linear-time self attention with codeword histogram for efficient recommendation. ||| yongji wu ||| defu lian ||| neil zhenqiang gong ||| lu yin ||| mingyang yin ||| jingren zhou ||| hongxia yang ||| 
2020 ||| hierarchically structured transformer networks for fine-grained spatial event forecasting. ||| xian wu ||| chao huang ||| chuxu zhang ||| nitesh v. chawla ||| 
2020 ||| multi-context attention for entity matching. ||| dongxiang zhang ||| yuyang nie ||| sai wu ||| yanyan shen ||| kian-lee tan ||| 
2018 ||| content attention model for aspect based sentiment analysis. ||| qiao liu ||| haibin zhang ||| yifu zeng ||| ziqi huang ||| zufeng wu ||| 
2020 ||| attention please: your attention check questions in survey studies can be automatically answered. ||| weiping pei ||| arthur mayer ||| kaylynn tu ||| chuan yue ||| 
2017 ||| attention to science. ||| kiran garimella ||| han xiao ||| 
2018 ||| attention network for information diffusion prediction. ||| zhitao wang ||| chengyao chen ||| wenjie li ||| 
2020 ||| domain adaptive multi-modality neural attention network for financial forecasting. ||| dawei zhou ||| lecheng zheng ||| yada zhu ||| jianbo li ||| jingrui he ||| 
2017 ||| the effect of aging on visual attention shifting in collaborative document editing. ||| patricia duangcham ||| vajirasak vanijja ||| sachi mizobuchi ||| 
2019 ||| link prediction with mutual attention for text-attributed networks. ||| robin brochier ||| adrien guille ||| julien velcin ||| 
2021 ||| tcs_witm_2021 @finsim-2: transformer based models for automatic classification of financial terms. ||| tushar goel ||| vipul chauhan ||| ishan verma ||| tirthankar dasgupta ||| lipika dey ||| 
2018 ||| when e-commerce meets social media: identifying business on wechat moment using bilateral-attention lstm. ||| tianlang chen ||| yuxiao chen ||| han guo ||| jiebo luo ||| 
2021 ||| deep co-attention network for multi-view subspace learning. ||| lecheng zheng ||| yu cheng ||| hongxia yang ||| nan cao ||| jingrui he ||| 
2018 ||| user-guided hierarchical attention network for multi-modal social image popularity prediction. ||| wei zhang ||| wen wang ||| jun wang ||| hongyuan zha ||| 
2020 ||| transmodality: an end2end fusion method with transformer for multimodal sentiment analysis. ||| zilong wang ||| zhaohong wan ||| xiaojun wan ||| 
2019 ||| a hierarchical attention retrieval model for healthcare question answering. ||| ming zhu ||| aman ahuja ||| wei wei ||| chandan k. reddy ||| 
2021 ||| polyu-cbs at the finsim-2 task: combining distributional, string-based and transformers-based features for hypernymy detection in the financial domain. ||| emmanuele chersoni ||| chu-ren huang ||| 
2018 ||| neural attentional rating regression with review-level explanations. ||| chong chen ||| min zhang ||| yiqun liu ||| shaoping ma ||| 
2020 ||| heterogeneous graph transformer. ||| ziniu hu ||| yuxiao dong ||| kuansan wang ||| yizhou sun ||| 
2021 ||| cross-positional attention for debiasing clicks. ||| honglei zhuang ||| zhen qin ||| xuanhui wang ||| michael bendersky ||| xinyu qian ||| po hu ||| dan chary chen ||| 
2021 ||| stan: spatio-temporal attention network for next location recommendation. ||| yingtao luo ||| qiang liu ||| zhaocheng liu ||| 
2020 ||| graph attention topic modeling network. ||| liang yang ||| fan wu ||| junhua gu ||| chuan wang ||| xiaochun cao ||| di jin ||| yuanfang guo ||| 
2019 ||| heterographic pun recognition via pronunciation and spelling understanding gated attention network. ||| yufeng diao ||| hongfei lin ||| liang yang ||| xiaochao fan ||| di wu ||| dongyu zhang ||| kan xu ||| 
2020 ||| learning bi-directional social influence in information cascades using graph sequence attention networks. ||| zhenhua huang ||| zhenyu wang ||| rui zhang ||| yangyang zhao ||| fadong zheng ||| 
2019 ||| event detection using hierarchical multi-aspect attention. ||| sneha mehta ||| mohammad raihanul islam ||| huzefa rangwala ||| naren ramakrishnan ||| 
2020 ||| iart: intent-aware response ranking with transformers in information-seeking conversation systems. ||| liu yang ||| minghui qiu ||| chen qu ||| cen chen ||| jiafeng guo ||| yongfeng zhang ||| w. bruce croft ||| haiqing chen ||| 
2021 ||| using prior knowledge to guide bert's attention in semantic textual matching tasks. ||| tingyu xia ||| yue wang ||| yuan tian ||| yi chang ||| 
2019 ||| black hat trolling, white hat trolling, and hacking the attention landscape. ||| jeanna n. matthews ||| matt goerzen ||| 
2020 ||| probabilistic logic graph attention networks for reasoning. ||| l. vivek harsha vardhan ||| guo jia ||| stanley kok ||| 
2020 ||| condition aware and revise transformer for question answering. ||| xinyan zhao ||| feng xiao ||| haoming zhong ||| jun yao ||| huanhuan chen ||| 
2020 ||| herding a deluge of good samaritans: how github projects respond to increased attention. ||| danaja maldeniya ||| ceren budak ||| lionel p. robert jr. ||| daniel m. romero ||| 
2019 ||| tissa: a time slice self-attention approach for modeling sequential user behaviors. ||| chenyi lei ||| shouling ji ||| zhao li ||| 
2018 ||| attention convolutional neural network for advertiser-level click-through rate forecasting. ||| hongchang gao ||| deguang kong ||| miao lu ||| xiao bai ||| jian yang ||| 
2019 ||| neural multimodal belief tracker with adaptive attention for dialogue systems. ||| zheng zhang ||| lizi liao ||| minlie huang ||| xiaoyan zhu ||| tat-seng chua ||| 
2019 ||| attention - from neuroscience to the web and wellbeing. ||| 
2019 ||| predicting human mobility via variational attention. ||| qiang gao ||| fan zhou ||| goce trajcevski ||| kunpeng zhang ||| ting zhong ||| fengli zhang ||| 
2018 ||| latent relational metric learning via memory-based attention for collaborative ranking. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2018 ||| may i have your attention, please: - building a dystopian attention economy. ||| sven helmer ||| 
2019 ||| an attention-based model for joint extraction of entities and relations with implicit entity features. ||| yan zhou ||| longtao huang ||| tao guo ||| songlin hu ||| jizhong han ||| 
2019 ||| heterogeneous graph attention network. ||| xiao wang ||| houye ji ||| chuan shi ||| bai wang ||| yanfang ye ||| peng cui ||| philip s. yu ||| 
2020 ||| high quality candidate generation and sequential graph attention network for entity linking. ||| zheng fang ||| yanan cao ||| ren li ||| zhenyu zhang ||| yanbing liu ||| shi wang ||| 
2021 ||| : dual attention matching network with normalized hard sample mining. ||| xin mao ||| wenting wang ||| yuanbin wu ||| man lan ||| 
2021 ||| beyond outlier detection: outlier interpretation by attention-guided triplet deviation network. ||| hongzuo xu ||| yijie wang ||| songlei jian ||| zhenyu huang ||| yongjun wang ||| ning liu ||| fei li ||| 
2020 ||| weakly supervised attention for hashtag recommendation using graph data. ||| amin javari ||| zhankui he ||| zijie huang ||| jeetu raj ||| kevin chen-chuan chang ||| 
2019 ||| quantifying the impact of user attentionon fair group representation in ranked lists. ||| piotr sapiezynski ||| wesley zeng ||| ronald e. robertson ||| alan mislove ||| christo wilson ||| 
2019 ||| focusing attention network for answer ranking. ||| yufei xie ||| shuchun liu ||| tangren yao ||| yao peng ||| zhao lu ||| 
2018 ||| what we read, what we search: media attention and public attention among 193 countries. ||| haewoon kwak ||| jisun an ||| joni salminen ||| soon-gyo jung ||| bernard j. jansen ||| 
2020 ||| outfitnet: fashion outfit recommendation with attention-based multiple instance learning. ||| yusan lin ||| maryam moosaei ||| hao yang ||| 
2021 ||| tweet-aware news summarization with dual-attention mechanism. ||| xin zheng ||| aixin sun ||| karthik muthuswamy ||| 
2020 ||| towards fine-grained flow forecasting: a graph attention approach for bike sharing systems. ||| suining he ||| kang g. shin ||| 
2018 ||| laan: a linguistic-aware attention network for sentiment analysis. ||| zeyang lei ||| yujiu yang ||| yi liu ||| 
2020 ||| multiple knowledge syncretic transformer for natural dialogue generation. ||| xiangyu zhao ||| longbiao wang ||| ruifang he ||| ting yang ||| jinxin chang ||| ruifang wang ||| 
2019 ||| user-video co-attention network for personalized micro-video recommendation. ||| shang liu ||| zhenzhong chen ||| hongyi liu ||| xinghai hu ||| 
2020 ||| dual-attentional factorization-machines based neural network for user response prediction. ||| feng liu ||| wei guo ||| huifeng guo ||| ruiming tang ||| yunming ye ||| xiuqiang he ||| 
2021 ||| predicting customer value with social relationships via motif-based graph attention networks. ||| jinghua piao ||| guozhen zhang ||| fengli xu ||| zhilong chen ||| yong li ||| 
2018 ||| monitoring students' attention in a classroom through computer vision. ||| daniel canedo ||| alina trifan ||| ant ||| nio j. r. neves ||| 
2021 ||| an attentional model for earthquake prediction using seismic data. ||| alana de santana correia ||| iury cleveston ||| viviane bonadia dos santos ||| sandra avila ||| esther luna colombini ||| 
2020 ||| attention in recurrent neural networks for energy disaggregation. ||| nikolaos virtsionis gkalinikis ||| christoforos nalmpantis ||| dimitris vrakas ||| 
2018 ||| using stop-motion video as visual indicator to strength children with asd's attention focus on specific nonverbal social cues to enhance perception judgments and situational awareness. ||| i-jui lee ||| 
2019 ||| mixed attention-aware network for person re-identification. ||| wenchen sun ||| fang'ai liu ||| weizhi xu ||| 
2017 ||| power transformer fault diagnosis using support vector machine and particle swarm optimization. ||| wenxiong mo ||| tusongjiang kari ||| hongbing wang ||| le luan ||| wensheng gao ||| 
2021 ||| attention-based joint feature extraction model for static music emotion classification. ||| meixian zhang ||| yonghua zhu ||| ning ge ||| yunwen zhu ||| tianyu feng ||| wenjun zhang ||| 
2019 ||| a densely connected transformer for machine translation. ||| zhikui zhu ||| jun ruan ||| kehao wang ||| jingfan zhou ||| guanglu ye ||| chenchen wu ||| 
2020 ||| short text classification model based on multi-attention. ||| yunxiang liu ||| qi xu ||| 
2019 ||| attention and multi-layer fusion for real-time semantic segmentation. ||| qinghe cheng ||| canlong zhang ||| zhixin li ||| zhiwen wang ||| 
2019 ||| generating topical and emotional responses using topic attention. ||| zhanzhao zhou ||| maofu liu ||| zhenlian zhang ||| yang fu ||| junyi xiang ||| 
2021 ||| malaria parasite detection using residual attention u-net. ||| chiang kang tan ||| chuan meng goh ||| sayed a. zikri bin sayed aluwee ||| siak wang khor ||| chai meei tyng ||| 
2020 ||| the application of transformer model architecture for the dependency parsing task. ||| artem chernyshov ||| valentin klimov ||| anita balandina ||| boris a. shchukin ||| 
2019 ||| a bioinspired model of decision making considering spatial attention for goal-driven behaviour. ||| raymundo ramirez-pedraza ||| natividad vargas ||| carlos johnnatan sandoval ||| juan luis del valle-padilla ||| f ||| lix ramos ||| 
2018 ||| modeling spatial auditory attention in act-r: a constraint-based approach. ||| jaelle scheuerman ||| kristen brent venable ||| maxwell t. anderson ||| edward j. golob ||| 
2019 ||| input-cell attention reduces vanishing saliency of recurrent neural networks. ||| aya abdelsalam ismail ||| mohamed k. gunady ||| luiz pessoa ||| h ||| ctor corrada bravo ||| soheil feizi ||| 
2017 ||| attention is all you need. ||| ashish vaswani ||| noam shazeer ||| niki parmar ||| jakob uszkoreit ||| llion jones ||| aidan n. gomez ||| lukasz kaiser ||| illia polosukhin ||| 
2017 ||| attend and predict: understanding gene regulation by selective attention on chromatin. ||| ritambhara singh ||| jack lanchantin ||| arshdeep sekhon ||| yanjun qi ||| 
2019 ||| ouroboros: on accelerating training of transformer-based language models. ||| qian yang ||| zhouyuan huo ||| wenlin wang ||| heng huang ||| lawrence carin ||| 
2020 ||| attention-gated brain propagation: how the brain can implement reward-based error backpropagation. ||| isabella pozzi ||| sander m. boht ||| pieter r. roelfsema ||| 
2020 ||| ranet: region attention network for semantic segmentation. ||| dingguo shen ||| yuanfeng ji ||| ping li ||| yi wang ||| di lin ||| 
2018 ||| unsupervised attention-guided image-to-image translation. ||| youssef alami mejjati ||| christian richardt ||| james tompkin ||| darren cosker ||| kwang in kim ||| 
2020 ||| o(n) connections are expressive enough: universal approximability of sparse transformers. ||| chulhee yun ||| yin-wen chang ||| srinadh bhojanapalli ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2020 ||| coot: cooperative hierarchical transformer for video-text representation learning. ||| simon ging ||| mohammadreza zolfaghari ||| hamed pirsiavash ||| thomas brox ||| 
2020 ||| modern hopfield networks and attention for immune repertoire classification. ||| michael widrich ||| bernhard sch ||| fl ||| milena pavlovic ||| hubert ramsauer ||| lukas gruber ||| markus holzleitner ||| johannes brandstetter ||| geir kjetil sandve ||| victor greiff ||| sepp hochreiter ||| g ||| nter klambauer ||| 
2019 ||| attentionxml: label tree-based attention-aware deep model for high-performance extreme multi-label text classification. ||| ronghui you ||| zihan zhang ||| ziye wang ||| suyang dai ||| hiroshi mamitsuka ||| shanfeng zhu ||| 
2020 ||| smyrf - efficient attention using asymmetric clustering. ||| giannis daras ||| nikita kitaev ||| augustus odena ||| alexandros g. dimakis ||| 
2017 ||| vain: attentional multi-agent predictive modeling. ||| yedid hoshen ||| 
2019 ||| semantic-guided multi-attention localization for zero-shot learning. ||| yizhe zhu ||| jianwen xie ||| zhiqiang tang ||| xi peng ||| ahmed elgammal ||| 
2020 ||| self-supervised graph transformer on large-scale molecular data. ||| yu rong ||| yatao bian ||| tingyang xu ||| weiyang xie ||| ying wei ||| wenbing huang ||| junzhou huang ||| 
2020 ||| untangling tradeoffs between recurrence and self-attention in artificial neural networks. ||| giancarlo kerg ||| bhargav kanuparthi ||| anirudh goyal ||| kyle goyette ||| yoshua bengio ||| guillaume lajoie ||| 
2020 ||| auto learning attention. ||| benteng ma ||| jing zhang ||| yong xia ||| dacheng tao ||| 
2020 ||| sac: accelerating and structuring self-attention via sparse adaptive connection. ||| xiaoya li ||| yuxian meng ||| mingxin zhou ||| qinghong han ||| fei wu ||| jiwei li ||| 
2018 ||| learning attentional communication for multi-agent cooperation. ||| jiechuan jiang ||| zongqing lu ||| 
2018 ||| watch your step: learning node embeddings via graph attention. ||| sami abu-el-haija ||| bryan perozzi ||| rami al-rfou ||| alexander a. alemi ||| 
2017 ||| a regularized framework for sparse and structured neural attention. ||| vlad niculae ||| mathieu blondel ||| 
2019 ||| social-bigat: multimodal trajectory forecasting using bicycle-gan and graph attention networks. ||| vineet kosaraju ||| amir sadeghian ||| roberto mart ||| n-mart ||| n ||| ian d. reid ||| hamid rezatofighi ||| silvio savarese ||| 
2019 ||| induced attention invariance: defending vqa models against adversarial attacks. ||| vasu sharma ||| ankita kalra ||| louis-philippe morency ||| 
2019 ||| incremental few-shot learning with attention attractor networks. ||| mengye ren ||| renjie liao ||| ethan fetaya ||| richard s. zemel ||| 
2020 ||| accelerating training of transformer-based language models with progressive layer dropping. ||| minjia zhang ||| yuxiong he ||| 
2020 ||| deep reinforcement learning with stacked hierarchical attention for text-based games. ||| yunqiu xu ||| meng fang ||| ling chen ||| yali du ||| joey tianyi zhou ||| chengqi zhang ||| 
2020 ||| big bird: transformers for longer sequences. ||| manzil zaheer ||| guru guruganesh ||| kumar avinava dubey ||| joshua ainslie ||| chris alberti ||| santiago onta ||| n ||| philip pham ||| anirudh ravula ||| qifan wang ||| li yang ||| amr ahmed ||| 
2019 ||| efficient graph generation with graph recurrent attention networks. ||| renjie liao ||| yujia li ||| yang song ||| shenlong wang ||| william l. hamilton ||| david duvenaud ||| raquel urtasun ||| richard s. zemel ||| 
2019 ||| supervised multimodal bitransformers for classifying images and text. ||| douwe kiela ||| suvrat bhooshan ||| hamed firooz ||| davide testuggine ||| 
2020 ||| comprehensive attention self-distillation for weakly-supervised object detection. ||| zeyi huang ||| yang zou ||| b. v. k. vijaya kumar ||| dong huang ||| 
2019 ||| novel positional encodings to enable tree-based transformers. ||| vighnesh leonardo shiv ||| chris quirk ||| 
2019 ||| adaptively aligned image captioning via adaptive attention time. ||| lun huang ||| wenmin wang ||| yaxian xia ||| jie chen ||| 
2019 ||| graph transformer networks. ||| seongjun yun ||| minbyul jeong ||| raehyun kim ||| jaewoo kang ||| hyunwoo j. kim ||| 
2017 ||| variational laws of visual attention for dynamic scenes. ||| dario zanca ||| marco gori ||| 
2020 ||| se(3)-transformers: 3d roto-translation equivariant attention networks. ||| fabian fuchs ||| daniel e. worrall ||| volker fischer ||| max welling ||| 
2020 ||| why are adaptive methods good for attention models? ||| jingzhao zhang ||| sai praneeth karimireddy ||| andreas veit ||| seungyeon kim ||| sashank j. reddi ||| sanjiv kumar ||| suvrit sra ||| 
2018 ||| bilinear attention networks. ||| jin-hwa kim ||| jaehyun jun ||| byoung-tak zhang ||| 
2018 ||| densely connected attention propagation for reading comprehension. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| jian su ||| 
2017 ||| attentional pooling for action recognition. ||| rohit girdhar ||| deva ramanan ||| 
2019 ||| enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. ||| shiyang li ||| xiaoyong jin ||| yao xuan ||| xiyou zhou ||| wenhu chen ||| yu-xiang wang ||| xifeng yan ||| 
2020 ||| improving natural language processing tasks with human gaze-guided neural attention. ||| ekta sood ||| simon tannert ||| philipp m ||| ller ||| andreas bulling ||| 
2019 ||| a tensorized transformer for language modeling. ||| xindian ma ||| peng zhang ||| shuai zhang ||| nan duan ||| yuexian hou ||| ming zhou ||| dawei song ||| 
2019 ||| towards interpretable reinforcement learning using attention augmented agents. ||| alexander mott ||| daniel zoran ||| mike chrzanowski ||| daan wierstra ||| danilo jimenez rezende ||| 
2017 ||| visual reference resolution using attention memory for visual dialog. ||| paul hongsuck seo ||| andreas m. lehrmann ||| bohyung han ||| leonid sigal ||| 
2020 ||| relationnet++: bridging visual representations for object detection via transformer decoder. ||| cheng chi ||| fangyun wei ||| han hu ||| 
2019 ||| modulated self-attention convolutional network for vqa. ||| jean-benoit delbrouck ||| 
2020 ||| bayesian attention modules. ||| xinjie fan ||| shujian zhang ||| bo chen ||| mingyuan zhou ||| 
2017 ||| learning deep structured multi-scale features using attention-gated crfs for contour prediction. ||| dan xu ||| wanli ouyang ||| xavier alameda-pineda ||| elisa ricci ||| xiaogang wang ||| nicu sebe ||| 
2018 ||| recurrent transformer networks for semantic correspondence. ||| seungryong kim ||| stephen lin ||| sangryul jeon ||| dongbo min ||| kwanghoon sohn ||| 
2020 ||| object-centric learning with slot attention. ||| francesco locatello ||| dirk weissenborn ||| thomas unterthiner ||| aravindh mahendran ||| georg heigold ||| jakob uszkoreit ||| alexey dosovitskiy ||| thomas kipf ||| 
2019 ||| leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog. ||| shachi h. kumar ||| eda okur ||| saurav sahay ||| jonathan huang ||| lama nachman ||| 
2020 ||| cascaded text generation with markov transformers. ||| yuntian deng ||| alexander m. rush ||| 
2019 ||| visually grounded video reasoning in selective attention memory. ||| t. s. jayram ||| vincent albouy ||| tomasz kornuta ||| emre sevgen ||| ahmet s. ozcan ||| 
2019 ||| compositional de-attention networks. ||| yi tay ||| anh tuan luu ||| aston zhang ||| shuohang wang ||| siu cheung hui ||| 
2018 ||| a^2-nets: double attention networks. ||| yunpeng chen ||| yannis kalantidis ||| jianshu li ||| shuicheng yan ||| jiashi feng ||| 
2017 ||| high-order attention models for visual question answering. ||| idan schwartz ||| alexander g. schwing ||| tamir hazan ||| 
2019 ||| one-shot object detection with co-attention and co-excitation. ||| ting-i hsieh ||| yi-chen lo ||| hwann-tzong chen ||| tyng-luh liu ||| 
2020 ||| multi-task temporal shift attention networks for on-device contactless vitals measurement. ||| xin liu ||| josh fromm ||| shwetak n. patel ||| daniel j. mcduff ||| 
2018 ||| uncertainty-aware attention for reliable interpretation and prediction. ||| jay heo ||| haebeom lee ||| saehoon kim ||| juho lee ||| kwang joon kim ||| eunho yang ||| sung ju hwang ||| 
2019 ||| levenshtein transformer. ||| jiatao gu ||| changhan wang ||| junbo zhao ||| 
2020 ||| limits to depth efficiencies of self-attention. ||| yoav levine ||| noam wies ||| or sharir ||| hofit bata ||| amnon shashua ||| 
2020 ||| attend and decode: 4d fmri task state decoding using attention models. ||| sam nguyen ||| brenda ng ||| alan david kaplan ||| priyadip ray ||| 
2020 ||| ratt: recurrent attention to transient tasks for continual image captioning. ||| riccardo del chiaro ||| bartlomiej twardowski ||| andrew d. bagdanov ||| joost van de weijer ||| 
2019 ||| understanding attention and generalization in graph neural networks. ||| boris knyazev ||| graham w. taylor ||| mohamed r. amer ||| 
2020 ||| multi-agent trajectory prediction with fuzzy query attention. ||| nitin kamra ||| hao zhu ||| dweep trivedi ||| ming zhang ||| yan liu ||| 
2020 ||| semg gesture recognition with a simple model of attention. ||| david josephs ||| carson drake ||| andy heroy ||| john santerre ||| 
2017 ||| saliency-based sequential image attention with multiset prediction. ||| sean welleck ||| jialin mao ||| kyunghyun cho ||| zheng zhang ||| 
2020 ||| adversarial sparse transformer for time series forecasting. ||| sifan wu ||| xi xiao ||| qianggang ding ||| peilin zhao ||| ying wei ||| junzhou huang ||| 
2020 ||| fast transformers with clustered attention. ||| apoorv vyas ||| angelos katharopoulos ||| fran ||| ois fleuret ||| 
2020 ||| prophet attention: predicting attention with future attention. ||| fenglin liu ||| xuancheng ren ||| xian wu ||| shen ge ||| wei fan ||| yuexian zou ||| xu sun ||| 
2018 ||| attention in convolutional lstm for gesture recognition. ||| liang zhang ||| guangming zhu ||| lin mei ||| peiyi shen ||| syed afaq ali shah ||| mohammed bennamoun ||| 
2019 ||| learning dynamics of attention: human prior for interpretable machine reasoning. ||| wonjae kim ||| yoonho lee ||| 
2020 ||| learning to execute programs with instruction pointer attention graph neural networks. ||| david bieber ||| charles sutton ||| hugo larochelle ||| daniel tarlow ||| 
2019 ||| predicting utilization of healthcare services from individual disease trajectories using rnns with multi-headed attention. ||| yogesh kumar ||| henri salo ||| tuomo nieminen ||| kristian vepsalainen ||| sangita kulathinal ||| pekka marttinen ||| 
2020 ||| sparse and continuous attention mechanisms. ||| andr |||  f. t. martins ||| ant ||| nio farinhas ||| marcos v. treviso ||| vlad niculae ||| pedro m. q. aguiar ||| m ||| rio a. t. figueiredo ||| 
2020 ||| funnel-transformer: filtering out sequential redundancy for efficient language processing. ||| zihang dai ||| guokun lai ||| yiming yang ||| quoc le ||| 
2019 ||| saccader: improving accuracy of hard attention models for vision. ||| gamaleldin f. elsayed ||| simon kornblith ||| quoc v. le ||| 
2019 ||| nat: neural architecture transformer for accurate and compact architectures. ||| yong guo ||| yin zheng ||| mingkui tan ||| qi chen ||| jian chen ||| peilin zhao ||| junzhou huang ||| 
2019 ||| cross attention network for few-shot classification. ||| ruibing hou ||| hong chang ||| bingpeng ma ||| shiguang shan ||| xilin chen ||| 
2020 ||| attendlight: universal attention-based reinforcement learning model for traffic signal control. ||| afshin oroojlooy ||| mohammadreza nazari ||| davood hajinezhad ||| jorge silva ||| 
2019 ||| a self validation network for object-level human attention estimation. ||| zehua zhang ||| chen yu ||| david j. crandall ||| 
2020 ||| neural encoding with visual attention. ||| meenakshi khosla ||| gia h. ngo ||| keith jamison ||| amy kuceyeski ||| mert r. sabuncu ||| 
2020 ||| deep transformers with latent depth. ||| xian li ||| asa cooper stickland ||| yuqing tang ||| xiang kong ||| 
2018 ||| stacked semantics-guided attention model for fine-grained zero-shot learning. ||| yunlong yu ||| zhong ji ||| yanwei fu ||| jichang guo ||| yanwei pang ||| zhongfei (mark) zhang ||| 
2020 ||| focus of attention improves information transfer in visual features. ||| matteo tiezzi ||| stefano melacci ||| alessandro betti ||| marco maggini ||| marco gori ||| 
2020 ||| musical speech: a transformer-based composition tool. ||| jason d'eon ||| sri harsha dumpala ||| chandramouli shama sastry ||| daniel oore ||| sageev oore ||| 
2020 ||| kalman filtering attention for user behavior modeling in ctr prediction. ||| hu liu ||| jing lu ||| xiwei zhao ||| sulong xu ||| hao peng ||| yutong liu ||| zehua zhang ||| jian li ||| junsheng jin ||| yongjun bao ||| weipeng yan ||| 
2018 ||| latent alignment and variational attention. ||| yuntian deng ||| yoon kim ||| justin t. chiu ||| demi guo ||| alexander m. rush ||| 
2020 ||| minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. ||| wenhui wang ||| furu wei ||| li dong ||| hangbo bao ||| nan yang ||| ming zhou ||| 
2020 ||| crosstransformers: spatially-aware few-shot transfer. ||| carl doersch ||| ankush gupta ||| andrew zisserman ||| 
2020 ||| measuring systematic generalization in neural proof generation with transformers. ||| nicolas gontier ||| koustuv sinha ||| siva reddy ||| christopher pal ||| 
2019 ||| self-attention with functional time representation learning. ||| da xu ||| chuanwei ruan ||| evren k ||| rpeoglu ||| sushant kumar ||| kannan achan ||| 
2018 ||| self-erasing network for integral object attention. ||| qibin hou ||| peng-tao jiang ||| yunchao wei ||| ming-ming cheng ||| 
2020 ||| neurosymbolic transformers for multi-agent communication. ||| jeevana priya inala ||| yichen yang ||| james paulos ||| yewen pu ||| osbert bastani ||| vijay kumar ||| martin rinard ||| armando solar-lezama ||| 
2019 ||| stand-alone self-attention in vision models. ||| niki parmar ||| prajit ramachandran ||| ashish vaswani ||| irwan bello ||| anselm levskaya ||| jon shlens ||| 
2018 ||| attention-based semantic priming for slot-filling. ||| jiewen wu ||| rafael e. banchs ||| luis fernando d'haro ||| pavitra krishnaswamy ||| nancy f. chen ||| 
2019 ||| residual attention regression for 3d hand pose estimation. ||| jing li ||| long zhang ||| zhaojie ju ||| 
2021 ||| gaze based implicit intention inference with historical information of visual attention for human-robot interaction. ||| yujie nie ||| xin ma ||| 
2019 ||| the effectiveness of eeg-feedback on attention in 3d virtual environment. ||| yue wang ||| xiaotong shen ||| haowen liu ||| tiantong zhou ||| sari merilampi ||| ling zou ||| 
2020 ||| progressive attentional learning for underwater image super-resolution. ||| xuelei chen ||| shiqing wei ||| chao yi ||| lingwei quan ||| cunyue lu ||| 
2019 ||| select and focus: action recognition with spatial-temporal attention. ||| wensong chan ||| zhiqiang tian ||| shuai liu ||| jing ren ||| xuguang lan ||| 
2021 ||| bearing fault diagnosis based on attentional multi-scale cnn. ||| shuai yang ||| yan liu ||| xincheng tian ||| lixin ma ||| 
2021 ||| research on chinese text summarization based on core word attention mechanism. ||| wenxiang xu ||| caiquan xiong ||| huasong cheng ||| 
2021 ||| improved am-lstm for power transformer error forecasting model. ||| xiong gu ||| teng yao ||| xue wang ||| huan wang ||| chunyang jiang ||| qiong xiang ||| lingyu yan ||| 
2020 ||| attention-based state decoupler of vibration transmission in multi-working conditions of rolling bearings. ||| yang liao ||| jianzhong hu ||| 
2021 ||| self-attention based multitasking sequential recom mendation. ||| guangjie liu ||| xin ma ||| jinlong zhu ||| yu zhang ||| danyang yang ||| 
2021 ||| facial expression recognition based on hybrid attention mechanism. ||| lingyu yan ||| menghan sheng ||| chunzhi wang ||| ming wei ||| xianjing zhou ||| 
2020 ||| dynamic facial expression recognition model based on bilstm-attention. ||| lingyu chen ||| yong ouyang ||| yawen zeng ||| yuanhang li ||| 
2021 ||| calculation of error of multi-winding voltage transformer under arbitrary secondary load by determinant method. ||| teng yao ||| xiong gu ||| xue wang ||| chunyang jiang ||| lihua zhou ||| hong yang ||| 
2017 ||| an attention based model for off-topic spontaneous spoken response detection: an initial study. ||| andrey malinin ||| kate knill ||| anton ragni ||| yu wang ||| mark j. f. gales ||| 
2019 ||| gaming the attention with a ssvep-based brain-computer interface. ||| m. a. lopez-gordo ||| eduardo perez ||| jesus minguillon ||| 
2017 ||| robust joint visual attention for hri using a laser pointer for perspective alignment and deictic referring. ||| dar ||| o maravall ||| javier de lope ||| juan pablo fuentes brea ||| 
2017 ||| a neurologically inspired network model for graziano's attention schema theory for consciousness. ||| erik van den boogaard ||| jan treur ||| maxim turpijn ||| 
2020 ||| lvbert: transformer-based model for latvian language understanding. ||| arturs znotins ||| guntis barzdins ||| 
2021 ||| progressive guidance categorization using transformer-based deep neural network architecture. ||| tanjim taharat aurpa ||| md. shoaib ahmed ||| rifat sadik ||| sabbir anwar ||| md abdul mazid adnan ||| md musfique anwar ||| 
2021 ||| wifimod: transformer-based indoor human mobility modeling using passive sensing. ||| amee trivedi ||| kate silverstein ||| emma strubell ||| prashant j. shenoy ||| mohit iyyer ||| 
2019 ||| nuclei detection using residual attention feature pyramid networks. ||| panagiotis dimitrakopoulos ||| giorgos sfikas ||| christophoros nikou ||| 
2020 ||| evaluation of hyperbolic attention in histopathology images. ||| renyu zhang ||| aly a. khan ||| robert l. grossman ||| 
2021 ||| detecting attention in hilbert-transformed eeg brain signals from simple-reaction and choice-reaction cognitive tasks. ||| patrycja dzianok ||| marcin kolodziej ||| ewa kublik ||| 
2018 ||| interpretable prediction of vascular diseases from electronic health records via deep attention networks. ||| seunghyun park ||| you jin kim ||| jeong-whun kim ||| jin joo park ||| borim ryu ||| jung-woo ha ||| 
2018 ||| attention spanned: comprehensive vulnerability analysis of at commands within the android ecosystem. ||| dave (jing) tian ||| grant hernandez ||| joseph i. choi ||| vanessa frost ||| christie ruales ||| patrick traynor ||| hayawardh vijayakumar ||| lee harrison ||| amir rahmati ||| michael grace ||| kevin r. b. butler ||| 
2021 ||| reducing test cases with attention mechanism of neural networks. ||| xing zhang ||| jiongyi chen ||| chao feng ||| ruilin li ||| yunfei su ||| bin zhang ||| jing lei ||| chaojing tang ||| 
2021 ||| siamhan: ipv6 address correlation attacks on tls encrypted traffic via siamese heterogeneous graph attention network. ||| tianyu cui ||| gaopeng gou ||| gang xiong ||| zhen li ||| mingxin cui ||| chang liu ||| 
2017 ||| leveraging contextual sentence relations for extractive summarization using a neural attention model. ||| pengjie ren ||| zhumin chen ||| zhaochun ren ||| furu wei ||| jun ma ||| maarten de rijke ||| 
2021 ||| hierarchical multi-modal contextual attention network for fake news detection. ||| shengsheng qian ||| jinguang wang ||| jun hu ||| quan fang ||| changsheng xu ||| 
2018 ||| equity of attention: amortizing individual fairness in rankings. ||| asia j. biega ||| krishna p. gummadi ||| gerhard weikum ||| 
2021 ||| cross-graph attention enhanced multi-modal correlation learning for fine-grained image-text retrieval. ||| yi he ||| xin liu ||| yiu-ming cheung ||| shu-juan peng ||| jinhan yi ||| wentao fan ||| 
2018 ||| ca-lstm: search task identification with context attention based lstm. ||| cong du ||| peng shu ||| yong li ||| 
2018 ||| modeling dynamic pairwise attention for crime classification over legal articles. ||| pengfei wang ||| ze yang ||| shuzi niu ||| yongfeng zhang ||| lei zhang ||| shaozhang niu ||| 
2021 ||| presize: predicting size in e-commerce using transformers. ||| yotam eshel ||| or levi ||| haggai roitman ||| alexander nus ||| 
2018 ||| multihop attention networks for question answer matching. ||| nam khanh tran ||| claudia nieder ||| e ||| 
2021 ||| augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. ||| zhiwei liu ||| ziwei fan ||| yu wang ||| philip s. yu ||| 
2017 ||| searching on the go: the effects of fragmented attention on mobile web search tasks. ||| morgan harvey ||| matthew pointon ||| 
2021 ||| package recommendation with intra- and inter-package attention networks. ||| chen li ||| yuanfu lu ||| wei wang ||| chuan shi ||| ruobing xie ||| haili yang ||| cheng yang ||| xu zhang ||| leyu lin ||| 
2017 ||| video question answering via attribute-augmented attention network learning. ||| yunan ye ||| zhou zhao ||| yimeng li ||| long chen ||| jun xiao ||| yueting zhuang ||| 
2020 ||| a transformer-based embedding model for personalized product search. ||| keping bi ||| qingyao ai ||| w. bruce croft ||| 
2018 ||| attention-driven factor model for explainable personalized recommendation. ||| jingwu chen ||| fuzhen zhuang ||| xin hong ||| xiang ao ||| xing xie ||| qing he ||| 
2020 ||| multiplex behavioral relation learning for recommendation via memory augmented transformer network. ||| lianghao xia ||| chao huang ||| yong xu ||| peng dai ||| bo zhang ||| liefeng bo ||| 
2020 ||| 3d self-attention for unsupervised video quantization. ||| jingkuan song ||| ruimin lang ||| xiaosu zhu ||| xing xu ||| lianli gao ||| heng tao shen ||| 
2020 ||| choppy: cut transformer for ranked list truncation. ||| dara bahri ||| yi tay ||| che zheng ||| donald metzler ||| andrew tomkins ||| 
2021 ||| improving transformer-kernel ranking model using conformer and query term independence. ||| bhaskar mitra ||| sebastian hofst ||| tter ||| hamed zamani ||| nick craswell ||| 
2020 ||| spatio-temporal dual graph attention network for query-poi matching. ||| zixuan yuan ||| hao liu ||| yanchi liu ||| denghui zhang ||| fei yi ||| nengjun zhu ||| hui xiong ||| 
2020 ||| multi-level multimodal transformer network for multimodal recipe comprehension. ||| ao liu ||| shuai yuan ||| chenbin zhang ||| congjian luo ||| yaqing liao ||| kun bai ||| zenglin xu ||| 
2020 ||| neural unified review recommendation with cross attention. ||| hongtao liu ||| wenjun wang ||| hongyan xu ||| qiyao peng ||| pengfei jiao ||| 
2021 ||| hybrid fusion with intra- and cross-modality attention for image-recipe retrieval. ||| jiao li ||| xing xu ||| wei yu ||| fumin shen ||| zuo cao ||| kai zuo ||| heng tao shen ||| 
2017 ||| enhancing recurrent neural networks with positional attention for question answering. ||| qin chen ||| qinmin hu ||| jimmy xiangji huang ||| liang he ||| weijie an ||| 
2021 ||| transformer-based banking products recommender system. ||| davide liu ||| george philippe farajalla ||| alexandre boulenger ||| 
2020 ||| creating a children-friendly reading environment via joint learning of content and human attention. ||| guoxiu he ||| yangyang kang ||| zhuoren jiang ||| jiawei liu ||| changlong sun ||| xiaozhong liu ||| wei lu ||| 
2020 ||| local self-attention over long text for efficient document retrieval. ||| sebastian hofst ||| tter ||| hamed zamani ||| bhaskar mitra ||| nick craswell ||| allan hanbury ||| 
2020 ||| deep interest with hierarchical attention network for click-through rate prediction. ||| weinan xu ||| hengxu he ||| minshi tan ||| yunming li ||| jun lang ||| dongbai guo ||| 
2019 ||| adaptive multi-attention network incorporating answer information for duplicate question detection. ||| di liang ||| fubao zhang ||| weidong zhang ||| qi zhang ||| jinlan fu ||| minlong peng ||| tao gui ||| xuanjing huang ||| 
2018 ||| large scale taxonomy classification using bilstm with self-attention. ||| hang gao ||| tim oates ||| 
2017 ||| attentive collaborative filtering: multimedia recommendation with item- and component-level attention. ||| jingyuan chen ||| hanwang zhang ||| xiangnan he ||| liqiang nie ||| wei liu ||| tat-seng chua ||| 
2020 ||| a knowledge-enhanced recommendation model with attribute-level co-attention. ||| deqing yang ||| zengchun song ||| lvxin xue ||| yanghua xiao ||| 
2018 ||| saan: a sentiment-aware attention network for sentiment analysis. ||| zeyang lei ||| yujiu yang ||| min yang ||| 
2020 ||| efficient document re-ranking for transformers by precomputing term representations. ||| sean macavaney ||| franco maria nardini ||| raffaele perego ||| nicola tonellotto ||| nazli goharian ||| ophir frieder ||| 
2020 ||| attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view. ||| jibing gong ||| shen wang ||| jinlong wang ||| wenzheng feng ||| hao peng ||| jie tang ||| philip s. yu ||| 
2021 ||| transformer reasoning network for personalized review summarization. ||| hongyan xu ||| hongtao liu ||| pengfei jiao ||| wenjun wang ||| 
2019 ||| personalized fashion recommendation with visual explanations based on multimodal attention network: towards visually explainable recommendation. ||| xu chen ||| hanxiong chen ||| hongteng xu ||| yongfeng zhang ||| yixin cao ||| zheng qin ||| hongyuan zha ||| 
2020 ||| guided transformer: leveraging multiple external sources for representation learning in conversational search. ||| helia hashemi ||| hamed zamani ||| w. bruce croft ||| 
2020 ||| relevance transformer: generating concise code snippets with relevance feedback. ||| carlos gemmell ||| federico rossetto ||| jeffrey dalton ||| 
2018 ||| update delivery mechanisms for prospective information needs: an analysis of attention in mobile users. ||| jimmy lin ||| salman mohammed ||| royal sequiera ||| luchen tan ||| 
2019 ||| nrpa: neural recommendation with personalized attention. ||| hongtao liu ||| fangzhao wu ||| wenjun wang ||| xianchen wang ||| pengfei jiao ||| chuhan wu ||| xing xie ||| 
2021 ||| fast attention-based learning-to-rank model for structured map search. ||| chiqun zhang ||| michael r. evans ||| max lepikhin ||| dragomir yankov ||| 
2020 ||| learning efficient representations of mouse movements to predict user attention. ||| ioannis arapakis ||| luis a. leiva ||| 
2021 ||| looking at ctr prediction again: is attention all you need? ||| yuan cheng ||| yanbo xue ||| 
2021 ||| heterogeneous attention network for effective and efficient cross-modal retrieval. ||| tan yu ||| yi yang ||| yi li ||| lin liu ||| hongliang fei ||| ping li ||| 
2021 ||| position enhanced mention graph attention network for dialogue relation extraction. ||| xinwei long ||| shuzi niu ||| yucheng li ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond. ||| andrew yates ||| rodrigo nogueira ||| jimmy lin ||| 
2021 ||| learning a fine-grained review-based transformer model for personalized product search. ||| keping bi ||| qingyao ai ||| w. bruce croft ||| 
2021 ||| dsgpt: domain-specific generative pre-training of transformers for text generation in e-commerce title and review summarization. ||| xueying zhang ||| yunjiang jiang ||| yue shang ||| zhaomeng cheng ||| chi zhang ||| xiaochuan fan ||| yun xiao ||| bo long ||| 
2019 ||| user attention-guided multimodal dialog systems. ||| chen cui ||| wenjie wang ||| xuemeng song ||| minlie huang ||| xin-shun xu ||| liqiang nie ||| 
2020 ||| improving neural chinese word segmentation with lexicon-enhanced adaptive attention. ||| xiaoyan zhao ||| min yang ||| qiang qu ||| yang sun ||| 
2020 ||| reranking for efficient transformer-based answer selection. ||| yoshitomo matsubara ||| thuy vu ||| alessandro moschitti ||| 
2020 ||| social media user geolocation via hybrid attention. ||| cheng zheng ||| jyun-yu jiang ||| yichao zhou ||| sean d. young ||| wei wang ||| 
2017 ||| a hierarchical multimodal attention-based neural network for image captioning. ||| yong cheng ||| fei huang ||| lian zhou ||| cheng jin ||| yuejie zhang ||| tao zhang ||| 
2020 ||| contextual re-ranking with behavior aware transformers. ||| chen qu ||| chenyan xiong ||| yizhe zhang ||| corby rosset ||| w. bruce croft ||| paul bennett ||| 
2018 ||| attention-based hierarchical neural query suggestion. ||| wanyu chen ||| fei cai ||| honghui chen ||| maarten de rijke ||| 
2017 ||| learning to diversify search results via subtopic attention. ||| zhengbao jiang ||| ji-rong wen ||| zhicheng dou ||| wayne xin zhao ||| jian-yun nie ||| ming yue ||| 
2018 ||| mention recommendation for multimodal microblog with cross-attention memory network. ||| renfeng ma ||| qi zhang ||| jiawen wang ||| lizhen cui ||| xuanjing huang ||| 
2018 ||| a contextual attention recurrent architecture for context-aware venue recommendation. ||| jarana manotumruksa ||| craig macdonald ||| iadh ounis ||| 
2021 ||| does bert pay attention to cyberbullying? ||| fatma elsafoury ||| stamos katsigiannis ||| steven r. wilson ||| naeem ramzan ||| 
2019 ||| video dialog via multi-grained convolutional self-attention context networks. ||| weike jin ||| zhou zhao ||| mao gu ||| jun yu ||| jun xiao ||| yueting zhuang ||| 
2021 ||| dual attention transfer in session-based recommendation with multi-dimensional integration. ||| chen chen ||| jie guo ||| bin song ||| 
2021 ||| lighter and better: low-rank decomposed self-attention networks for next-item recommendation. ||| xinyan fan ||| zheng liu ||| jianxun lian ||| wayne xin zhao ||| xing xie ||| ji-rong wen ||| 
2019 ||| interact and decide: medley of sub-attention networks for effective group recommendation. ||| lucas vinh tran ||| tuan-anh nguyen pham ||| yi tay ||| yiding liu ||| gao cong ||| xiaoli li ||| 
2017 ||| toroidal vector-potential transformer. ||| masahiro daibo ||| 
2020 ||| process outcome prediction: cnn vs. lstm (with attention). ||| hans weytjens ||| jochen de weerdt ||| 
2021 ||| combating informational denial-of-service (idos) attacks: modeling and mitigation of attentional human vulnerability. ||| linan huang ||| quanyan zhu ||| 
2020 ||| moving target defense for robust monitoring of electric grid transformers in adversarial environments. ||| sailik sengupta ||| kaustav basu ||| arunabha sen ||| subbarao kambhampati ||| 
2019 ||| amas: attention model for attributed sequence classification. ||| zhongfang zhuang ||| xiangnan kong ||| elke a. rundensteiner ||| 
2018 ||| deep attention model for triage of emergency department patients. ||| djordje gligorijevic ||| jelena stojanovic ||| wayne satz ||| ivan stojkovic ||| kraftin schreyer ||| daniel del portal ||| zoran obradovic ||| 
2019 ||| hierarchical attention networks for cyberbullying detection on the instagram social network. ||| lu cheng ||| ruocheng guo ||| yasin n. silva ||| deborah l. hall ||| huan liu ||| 
2021 ||| attention-based autoregression for accurate and efficient multivariate time series forecasting. ||| jaemin yoo ||| u kang ||| 
2021 ||| deep multi-instance contrastive learning with dual attention for anomaly precursor detection. ||| dongkuan xu ||| wei cheng ||| jingchao ni ||| dongsheng luo ||| masanao natsumeda ||| dongjin song ||| bo zong ||| haifeng chen ||| xiang zhang ||| 
2019 ||| attentional heterogeneous graph neural network: application to program reidentification. ||| shen wang ||| zhengzhang chen ||| ding li ||| zhichun li ||| lu-an tang ||| jingchao ni ||| junghwan rhee ||| haifeng chen ||| philip s. yu ||| 
2019 ||| geoattn: localization of social media messages via attentional memory network. ||| sha li ||| chao zhang ||| dongming lei ||| ji li ||| jiawei han ||| 
2020 ||| attention-aware answers of the crowd. ||| jingzheng tu ||| guoxian yu ||| jun wang ||| carlotta domeniconi ||| xiangliang zhang ||| 
2021 ||| inter-series attention model for covid-19 forecasting. ||| xiaoyong jin ||| yu-xiang wang ||| xifeng yan ||| 
2019 ||| predicting multiple demographic attributes with task specific embedding transformation and attention network. ||| raehyun kim ||| hyunjae kim ||| janghyuk lee ||| jaewoo kang ||| 
2021 ||| session-based recommendation with hypergraph attention networks. ||| jianling wang ||| kaize ding ||| ziwei zhu ||| james caverlee ||| 
2019 ||| bus travel speed prediction using attention network of heterogeneous correlation features. ||| yidan sun ||| guiyuan jiang ||| siew-kei lam ||| shicheng chen ||| peilan he ||| 
2020 ||| semi-supervised classification using attention-based regularization on coarse-resolution data. ||| guruprasad nayak ||| rahul ghosh ||| xiaowei jia ||| varun mithal ||| vipin kumar ||| 
2020 ||| dual-attention recurrent networks for affine registration of neuroimaging data. ||| xin dai ||| xiangnan kong ||| xinyue liu ||| john boaz lee ||| constance m. moore ||| 
2017 ||| improving distantly supervised relation extraction using word and entity based attention. ||| sharmistha jat ||| siddhesh khandelwal ||| partha p. talukdar ||| 
2021 ||| reasoning with transformer-based models: deep learning, but shallow reasoning. ||| chadi helwe ||| chlo |||  clavel ||| fabian m. suchanek ||| 
2021 ||| multi-branch recurrent attention convolutional neural network with evidence theory for fine-grained image classification. ||| zhikang xu ||| bofeng zhang ||| haijie fu ||| xiaodong yue ||| ying lv ||| 
2020 ||| improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. ||| zhe li ||| lianwen jin ||| songxuan lai ||| yecheng zhu ||| 
2020 ||| attention combination of sequence models for handwritten chinese text recognition. ||| zheng-yu zhu ||| fei yin ||| da-han wang ||| 
2020 ||| handwritten historical music recognition by sequence-to-sequence with attention mechanism. ||| arnau bar ||| carles badal ||| alicia forn ||| s ||| 
2018 ||| memory-augmented attention model for scene text recognition. ||| cong wang ||| fei yin ||| cheng-lin liu ||| 
2020 ||| attention based writer independent verification. ||| mohammad abuzar shaikh ||| tiehang duan ||| mihir chauhan ||| sargur n. srihari ||| 
2020 ||| attention augmented convolutional recurrent network for handwritten japanese text recognition. ||| nam tuan ly ||| cuong tuan nguyen ||| masaki nakagawa ||| 
2020 ||| an attention based method for offline handwritten urdu text recognition. ||| tayaba anjum ||| nazar khan ||| 
2021 ||| exploring the digital identity divide: a call for attention to computing identity at hbcus. ||| takeria blunt ||| tamara pearson ||| 
2022 ||| communicating alternative grading schemes: how to shift students' attention to their learning from grades. ||| sarah brown ||| victoria ch ||| vez ||| 
2020 ||| look at me and grab this! materiality and the practices around negotiation of social attention with children on the autistic spectrum. ||| justyna wierbilowicz ||| alessandro cappelletti ||| davide giovanelli ||| angela pasqualotto ||| arianna bentenuto ||| elisabetta farella ||| massimo zancanaro ||| 
2020 ||| using mouse movement heatmaps to visualize user attention to words. ||| ilan kirsh ||| 
2020 ||| grounding dialogue history: strengths and weaknesses of pre-trained transformers. ||| claudio greco ||| alberto testoni ||| raffaella bernardi ||| 
2018 ||| the cowriter robot: improving attention in a learning-by-teaching setup. ||| pierre le denmat ||| thomas gargot ||| mohamed chetouani ||| dominique archambault ||| david cohen ||| salvatore maria anzalone ||| 
2017 ||| detecting attention breakdowns in robotic neurofeedback systems. ||| parisa nahaltahmasebi ||| mohamed chetouani ||| david cohen ||| salvatore maria anzalone ||| 
2019 ||| applying self-interaction attention for extracting drug-drug interactions. ||| luca putelli ||| alfonso gerevini ||| alberto lavelli ||| ivan serina ||| 
2020 ||| explainable attentional neural recommendations for personalized social learning. ||| luca marconi ||| ricardo anibal matamoros aragon ||| italo zoppis ||| sara manzoni ||| giancarlo mauri ||| francesco epifania ||| 
2018 ||| cross attention for selection-based question answering. ||| alessio gravina ||| federico rossetto ||| silvia severini ||| giuseppe attardi ||| 
2020 ||| which turn do neural models exploit the most to solve guesswhat? diving into the dialogue history encoding in transformers and lstms. ||| claudio greco ||| alberto testoni ||| raffaella bernardi ||| 
2018 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2021 ||| evaluating transformer models for punctuation restoration in italian. ||| alessio miaschi ||| andrea amelio ravelli ||| felice dell'orletta ||| 
2019 ||| inspecting unification of encoding and matching with transformer: a case study of machine reading comprehension. ||| hangbo bao ||| li dong ||| furu wei ||| wenhui wang ||| nan yang ||| lei cui ||| songhao piao ||| ming zhou ||| 
2019 ||| question answering using hierarchical attention on top of bert features. ||| reham a. osama ||| nagwa m. el-makky ||| marwan torki ||| 
2020 ||| paying attention to rhythm in hci: some thoughts on methods. ||| brigid mary costello ||| 
2020 ||| jointly learning to align and transcribe using attention-based alignment and uncertainty-to-weigh losses. ||| shreekantha nadig ||| sumit chakraborty ||| anuj shah ||| chaitanay sharma ||| v. ramasubramanian ||| sachit rao ||| 
2020 ||| dendrogram based clustering and separation of individual and simultaneously active incipient discharges in transformer insulation. ||| niyas k. haneefa ||| b. m. ashwin desai ||| ramanujam sarathi ||| manivasakan rathinam ||| 
2020 ||| multi-target hybrid ctc-attentional decoder for joint phoneme-grapheme recognition. ||| shreekantha nadig ||| v. ramasubramanian ||| sachit rao ||| 
2020 ||| joint language identification of code-switching speech using attention-based e2e network. ||| ganji sreeram ||| kunal dhawan ||| kumar priyadarshi ||| rohit sinha ||| 
2020 ||| improved feed forward attention mechanism in bidirectional recurrent neural networks for robust sequence classification. ||| sai bharath chandra gutha ||| m. ali basha shaik ||| tejas udayakumar ||| ajit ashok saunshikhar ||| 
2021 ||| hotspot detection via multi-task learning and transformer encoder. ||| binwu zhu ||| ran chen ||| xinyun zhang ||| fan yang ||| xuan zeng ||| bei yu ||| martin d. f. wong ||| 
2021 ||| accelerating framework of transformer by hardware design and model compression co-optimization. ||| panjie qi ||| edwin hsing-mean sha ||| qingfeng zhuge ||| hongwu peng ||| shaoyi huang ||| zhenglun kong ||| yuhong song ||| bingbing li ||| 
2020 ||| retransformer: reram-based processing-in-memory architecture for transformer acceleration. ||| xiaoxuan yang ||| bonan yan ||| hai li ||| yiran chen ||| 
2020 ||| hotspot detection via attention-based deep layout metric learning. ||| hao geng ||| haoyu yang ||| lu zhang ||| jin miao ||| fan yang ||| xuan zeng ||| bei yu ||| 
2021 ||| bit-transformer: transforming bit-level sparsity into higher preformance in reram-based accelerator. ||| fangxin liu ||| wenbo zhao ||| zhezhi he ||| zongwu wang ||| yilong zhao ||| yongbiao chen ||| li jiang ||| 
2021 ||| autogtco: graph and tensor co-optimize for image recognition with transformers on gpu. ||| yang bai ||| xufeng yao ||| qi sun ||| bei yu ||| 
2019 ||| attention neural network for user behavior modeling. ||| kang yang ||| jinghua zhu ||| 
2019 ||| relation extraction based on dual attention mechanism. ||| xue li ||| yuan rao ||| long sun ||| yi lu ||| 
2021 ||| attention residual convolution neural network based on u-net for covid-19 lung infection segmentation. ||| qiang zuo ||| songyu chen ||| zhifang wang ||| 
2020 ||| feature extraction by using attention mechanism in text classification. ||| yaling wang ||| yue wang ||| 
2021 ||| ecg-based arrhythmia detection using attention-based convolutional neural network. ||| renxing zhao ||| runnan he ||| 
2021 ||| a transformer model-based approach to bearing fault diagnosis. ||| zhenshan bao ||| jialei du ||| wenbo zhang ||| jiajing wang ||| tao qiu ||| yan cao ||| 
2020 ||| cp-net: channel attention and pixel attention network for single image dehazing. ||| shunan gao ||| jinghua zhu ||| yan yang ||| 
2021 ||| channel context and dual-domain attention based u-net for skin lesion attributes segmentation. ||| xuelian mu ||| haiwei pan ||| kejia zhang ||| teng teng ||| xiaofei bian ||| chunling chen ||| 
2020 ||| deeper attention-based network for structured data. ||| xiaohua wu ||| youping fan ||| wanwan peng ||| hong pang ||| yu luo ||| 
2019 ||| superimposed attention mechanism-based cnn network for reading comprehension and question answering. ||| mingqi li ||| xuefei hou ||| jiaoe li ||| kai gao ||| 
2020 ||| poi recommendations using self-attention based on side information. ||| chenbo yue ||| jinghua zhu ||| shuo zhang ||| xinxing ma ||| 
2019 ||| visual sentiment analysis with local object regions attention. ||| guoyong cai ||| xinhao he ||| jiao pan ||| 
2021 ||| combining global and local attention with positional encoding for video summarization. ||| evlampios e. apostolidis ||| georgios balaouras ||| vasileios mezaris ||| ioannis patras ||| 
2021 ||| adaptation and attention for neural video coding. ||| nannan zou ||| honglei zhang ||| francesco cricri ||| ramin ghaznavi youvalari ||| hamed r. tavakoli ||| jani lainema ||| emre aksu ||| miska m. hannuksela ||| esa rahtu ||| 
2019 ||| versatile video coding of 360-degree video using frame-based fov and visual attention. ||| jo ||| o carreira ||| s ||| rgio m. m. de faria ||| luis m. n. tavora ||| antonio navarro ||| pedro a. amado assun ||| o ||| 
2017 ||| sustained attention function evaluation during cooking based on egocentric vision. ||| sho ooi ||| mutsuo sano ||| hajime tabuchi ||| fumie saito ||| satoshi umeda ||| 
2021 ||| sailboat detection based on automated search attention mechanism and deep learning models. ||| ziyuan luo ||| minh nguyen ||| wei qi yan ||| 
2020 ||| salient motion features for visual attention models. ||| aisha ajmal ||| harith al-sahaf ||| christopher hollitt ||| 
2017 ||| active shift attention based object tracking system. ||| aisha ajmal ||| christopher hollitt ||| marcus frean ||| 
2018 ||| a comparison of rgb and hsv colour spaces for visual attention models. ||| aisha ajmal ||| christopher hollitt ||| marcus frean ||| harith al-sahaf ||| 
2018 ||| universal model for millimeter-wave integrated transformers. ||| johannes herrmann ||| david bierbuesse ||| renato negra ||| 
2018 ||| geometric form factors-based power transformers design. ||| giulia di capua ||| nicola femia ||| 
2018 ||| visual attention toward socially rich context information for autism spectrum disorder (asd) and normal developing children: an eye tracking study. ||| emad bataineh ||| mohamed basel almourad ||| farhi marir ||| joana stocker ||| 
2017 ||| prioritizing attention in analytic monitoring. ||| peter bailis ||| edward gan ||| kexin rong ||| sahaana suri ||| 
2018 ||| proposal of robot-interaction based intervention for joint-attention development. ||| itsaso perez ||| itziar rekalde ||| leire ozaeta ||| manuel gra ||| a ||| 
2018 ||| captioning with language-based attention. ||| anshu rajendra ||| ritwik rajendra ||| ole j. mengshoel ||| ming zeng ||| momina haider ||| 
2021 ||| interpretable prediction of diabetes from tabular health screening records using an attentional neural network. ||| yuki oba ||| taro tezuka ||| masaru sanuki ||| yukiko wagatsuma ||| 
2020 ||| hanke: hierarchical attention networks for knowledge extraction in political science domain. ||| erick skorupa parolin ||| latifur khan ||| javier osorio ||| vito d'orazio ||| patrick t. brandt ||| jennifer s. holmes ||| 
2021 ||| improving portuguese semantic role labeling with transformers and transfer learning. ||| sofia oliveira ||| daniel loureiro ||| al ||| pio jorge ||| 
2021 ||| 3m-transformers for event coding on organized crime domain. ||| erick skorupa parolin ||| latifur khan ||| javier osorio ||| patrick t. brandt ||| vito d'orazio ||| jennifer s. holmes ||| 
2020 ||| sesamebert: attention for anywhere. ||| ta-chun su ||| hsiang-chih cheng ||| 
2021 ||| a neural network architecture with an attention-based layer for spatial prediction of fine particulate matter. ||| luis e. colchado ||| edwin villanueva ||| jos |||  ochoa luna ||| 
2021 ||| resgcn: attention-based deep residual modeling for anomaly detection on attributed networks. ||| yulong pei ||| tianjin huang ||| werner van ipenburg ||| mykola pechenizkiy ||| 
2019 ||| mars: memory attention-aware recommender system. ||| lei zheng ||| chun-ta lu ||| lifang he ||| sihong xie ||| he huang ||| chaozhuo li ||| vahid noroozi ||| bowen dong ||| philip s. yu ||| 
2021 ||| constructing global coherence representations: identifying interpretability and coherences of transformer attention in time series data. ||| leonid schwenke ||| martin atzmueller ||| 
2019 ||| transformer fault diagnosis model based on iterative nearest neighbor interpolation and ensemble learning. ||| yunfei liu ||| jing li ||| zhao li ||| lin qiao ||| shuo chen ||| xuming lv ||| 
2021 ||| human fall detection algorithm based on mixed attention mechanism. ||| wei ge ||| xin luo ||| ran tao ||| youqun shi ||| 
2021 ||| a study on the cognitive efficiency of visual attention in pop-up ads based on eye-movement experiments. ||| yanhui chen ||| 
2020 ||| power load forecasting based on vmd and attention-lstm. ||| han-chieh chao ||| fu lin ||| jeng-shyang pan ||| wei-che chien ||| chin-feng lai ||| 
2021 ||| sa-hardnest: a self-attention network for polyp segmentation. ||| yaoduo zhang ||| 
2021 ||| multi-head self-attention transformer for dogecoin price prediction. ||| sashank sridhar ||| sowmya sanagavarapu ||| 
2020 ||| a convolutional neural network with word-level attention for text classification. ||| baihan kang ||| 
2021 ||| a multi-scale spatial and temporal attention network on dynamic connectivity to localize the eloquent cortex in brain tumor patients. ||| naresh nandakumar ||| komal manzoor ||| shruti agarwal ||| jay j. pillai ||| sachin k. gujar ||| haris i. sair ||| archana venkataraman ||| 
2019 ||| ultrasound image representation learning by modeling sonographer visual attention. ||| richard droste ||| yifan cai ||| harshita sharma ||| pierre chatelain ||| lior drukker ||| aris t. papageorghiou ||| j. alison noble ||| 
2019 ||| melanoma recognition via visual attention. ||| yiqi yan ||| jeremy kawahara ||| ghassan hamarneh ||| 
2020 ||| bidirectional transformer language models for smart autocompletion of source code. ||| felix binder ||| johannes villmow ||| adrian ulges ||| 
2020 ||| an entity relation extraction algorithm based on bert(wwm-ext)-bigru-attention. ||| ruihui hou ||| hanhao li ||| honghai feng ||| yunpeng li ||| jun li ||| yatian shen ||| 
2019 ||| student attention evaluation system using machine learning for decision making. ||| dalila dur ||| es ||| 
2021 ||| neural text categorization with transformers for learning portuguese as a second language. ||| rodrigo santos ||| jo ||| o rodrigues ||| ant ||| nio branco ||| rui vaz ||| 
2019 ||| hyper-parameter optimization of multi-attention recurrent neural network for battery state-of-charge forecasting. ||| aleksei mashlakov ||| ville tikka ||| lasse lensu ||| aleksei romanenko ||| samuli honkapuro ||| 
2021 ||| answering fill-in-the-blank questions in portuguese with transformer language models. ||| hugo gon ||| alo oliveira ||| 
2019 ||| spatio-temporal attention deep recurrent q-network for pomdps. ||| mariano etchart ||| pawel ladosz ||| david mulvaney ||| 
2018 ||| wireless linear variable differential transformer design and structural performance analysis. ||| shaun veilleux ||| ali abedi ||| chuck wilkerson ||| delisa wilkerson ||| darren boyd ||| 
2019 ||| a study of primary school pupils' motivation, emotional intelligence and attentional control ability. ||| ruonan hu ||| junjie j. shang ||| qi xia ||| 
2017 ||| saliency attention and sift keypoints combination for automatic target recognition on mstar dataset. ||| ayoub karine ||| abdelmalek toumi ||| ali khenchaf ||| mohammed el hassouni ||| 
2018 ||| a visual attention model based on human visual cognition. ||| na li ||| xinbo zhao ||| baoyuan ma ||| xiaochun zou ||| 
2019 ||| long short-term attention. ||| guoqiang zhong ||| xin lin ||| kang chen ||| qingyang li ||| kaizhu huang ||| 
2018 ||| attend to knowledge: memory-enhanced attention network for image captioning. ||| hui chen ||| guiguang ding ||| zijia lin ||| yuchen guo ||| jungong han ||| 
2018 ||| dau-gan: unsupervised object transfiguration via deep attention unit. ||| zihan ye ||| fan lyu ||| jinchang ren ||| yu sun ||| qiming fu ||| fuyuan hu ||| 
2019 ||| msa-net: multiscale spatial attention network for the classification of breast histology images. ||| zhanbo yang ||| lingyan ran ||| yong xia ||| yanning zhang ||| 
2018 ||| a study of the role of attention in classifying covert and overt motor activities. ||| banghua yang ||| jinlong wang ||| cuntai guan ||| chenxiao hu ||| jianguo wang ||| 
2021 ||| optimizing person re-identification using generated attention masks. ||| leonardo capozzi ||| jo ||| o ribeiro pinto ||| jaime s. cardoso ||| ana rebelo ||| 
2019 ||| applying self-attention for stance classification. ||| margarita constanza bugue ||| o ||| marcelo mendoza ||| 
2020 ||| creating dialogue between a tutee agent and a tutor in a lecture video improves students' attention. ||| ari nugraha ||| izhar almizan wahono ||| jianpeng zhanghe ||| tomoyuki harada ||| tomoo inoue ||| 
2020 ||| multi-stage attention convolutional neural networks for hevc in-loop filtering. ||| peng-ren lai ||| jia-shung wang ||| 
2019 ||| sparse autoencoder with attention mechanism for speech emotion recognition. ||| ting-wei sun ||| an-yeu andy wu ||| 
2020 ||| event-based attention and tracking on neuromorphic hardware. ||| alpha renner ||| matthew evanusa ||| garrick orchard ||| yulia sandamirskaya ||| 
2017 ||| miniattention: attention management in minimal invasive surgery using an iot synchronization approach. ||| bernhard anzengruber ||| nina hochedlinger ||| michael matscheko ||| alois ferscha ||| andreas shamiyeh ||| bettina klugsberger ||| gamze demireli ||| 
2020 ||| malware family classification using lstm with attention. ||| qi xie ||| yongjun wang ||| zhiquan qin ||| 
2019 ||| on-line detection method of transformer measurement error based on bp neural network. ||| helong li ||| jia liu ||| xiaolei yuan ||| xiaojian zhao ||| jinquan zhao ||| 
2019 ||| a bi-sru neural network based on soft attention for hrrp target recognition. ||| xin li ||| zunhua guo ||| 
2019 ||| translate and summarize complaints of patient to electronic health record by bilstm-cnn attention model. ||| haowei song ||| gangmin li ||| size hou ||| yuanying qu ||| hai-ning liang ||| xuming bai ||| 
2019 ||| sentiment analysis of web text based on deep learning with a attention mechanism. ||| qing yu ||| hui zhao ||| kaiwen jiang ||| 
2019 ||| experimental analysis on auditory attention saliency calculation models. ||| xinyu bai ||| zhijun zhao ||| lingyun xie ||| 
2019 ||| summarizing articles into sentences by hierarchical attention model and rnn language model. ||| takashi kuremoto ||| takuji tsuruda ||| shingo mabu ||| 
2020 ||| single-image super-resolution based on a self-attention deep neural network. ||| linfu jiang ||| minzhi zhong ||| fangchi qiu ||| 
2020 ||| synchrony detection of epileptic eeg signals based on attention and pearson's correlation coefficient. ||| tenghui zhou ||| zhen mei ||| xiumei zhu ||| zhihua huang ||| 
2020 ||| spatial-temporal graph attention model on traffic forecasting. ||| xinlan zhang ||| zhenguo zhang ||| xiaofeng jin ||| 
2019 ||| ocdad: an overlapping community detecting algorithm using attention degree in directed ex-egonet. ||| furong chang ||| bofeng zhang ||| songxian wu ||| yue zhao ||| bingchun li ||| jiarila maimaitiriyimu ||| 
2020 ||| attention-based bidirectional long short-term memory networks for relation classification using knowledge distillation from bert. ||| zihan wang ||| bo yang ||| 
2021 ||| self-attention based text matching model with generative pre-training. ||| xiaolin zhang ||| fengpei lei ||| shengji yu ||| 
2021 ||| st-tap: a traffic accident prediction framework based on spatio-temporal transformer. ||| weitao liu ||| xuanyi liu ||| hui feng ||| yiran wang ||| lintao guan ||| weifeng xu ||| guojiang shen ||| zhi liu ||| xiangjie kong ||| 
2019 ||| multi-factor based stock price prediction using hybrid neural networks with attention mechanism. ||| chen li ||| xu zhang ||| mahboob qaosar ||| saleh ahmed ||| kazi md. rokibul alam ||| yasuhiko morimoto ||| 
2021 ||| sfnet: stage spatial attention and feature waterfall fusion for mathematical document text line detection. ||| chang liu ||| jiaxin liu ||| yong zhang ||| lei huang ||| 
2021 ||| a deep learning algorithm for groundwater level prediction based on spatial-temporal attention mechanism. ||| chong chen ||| xiaoyu zhu ||| xiaobin kang ||| han zhou ||| 
2021 ||| improving domestic nilm using an attention-enabled seq2point learning approach. ||| jing zhang ||| jiawei sun ||| jixiang gan ||| qi liu ||| xioadong liu ||| 
2021 ||| design and implementation of novel power electronic transformer for smart grid. ||| zixin wang ||| 
2019 ||| academic performance estimation with attention-based graph convolutional networks. ||| qian hu ||| huzefa rangwala ||| 
2021 ||| math question solving and mcq distractor generation with attentional gru networks. ||| neisarg dave ||| riley owen bakes ||| bart pursel ||| c. lee giles ||| 
2019 ||| a meta-learning augmented bidirectional transformer model for automatic short answer grading. ||| zichao wang ||| andrew s. lan ||| andrew e. waters ||| phillip grimaldi ||| richard g. baraniuk ||| 
2020 ||| legal language modeling with transformers. ||| lazar peric ||| stefan mijic ||| dominik stammbach ||| elliott ash ||| 
2019 ||| patenttransformer: a framework for personalized patent claim generation. ||| jieh-sheng lee ||| 
2020 ||| cross-domain generalization and knowledge transfer in transformers trained on legal data. ||| jarom ||| r savelka ||| hannes westermann ||| karim benyekhlef ||| 
2019 ||| weakly supervised one-shot classification using recurrent neural networks with attention: application to claim acceptance detection. ||| charles condevaux ||| s ||| bastien harispe ||| st ||| phane mussard ||| guillaume zambrano ||| 
2020 ||| transformers for classifying fourth amendment elements and factors tests. ||| evan gretok ||| david langerman ||| wesley m. oliver ||| 
2021 ||| named entity recognition of wa cultural information resources based on attention mechanism. ||| xiangxu deng ||| shu zhang ||| jun wang ||| ken chen ||| 
2018 ||| a transfer learning based hierarchical attention neural network for sentiment classification. ||| zhaowei qu ||| yuan wang ||| xiaoru wang ||| shuqiang zheng ||| 
2020 ||| adaptive and dynamic knowledge transfer in multi-task learning with attention networks. ||| tao ma ||| ying tan ||| 
2021 ||| lightweight object tracking algorithm based on siamese network with efficient attention. ||| hanhua yu ||| qingling liu ||| 
2021 ||| prediction of oil temperature for transformers using gated recurrent unit. ||| yuwen liu ||| yihong yang ||| yuqing wang ||| 
2020 ||| attention u-net based adversarial architectures for chest x-ray lung segmentation information. ||| guszt ||| v ga ||| l ||| bal ||| zs maga ||| andr ||| s luk ||| cs ||| 
2020 ||| multimodal matching transformer for live commenting. ||| chaoqun duan ||| lei cui ||| shuming ma ||| furu wei ||| conghui zhu ||| tiejun zhao ||| 
2020 ||| human activity recognition from wearable sensor data using self-attention. ||| saif mahmud ||| m. tanjid hasan tonmoy ||| kishor kumar bhaumik ||| a k m mahbubur rahman ||| m. ashraful amin ||| mohammad shoyaib ||| muhammad asif hossain khan ||| amin ahsan ali ||| 
2020 ||| saliency detection with deformable convolution and feature attention. ||| zhe zhang ||| junhui ma ||| panpan xu ||| wencheng wang ||| 
2020 ||| transsketchnet: attention-based sketch recognition using transformers. ||| gaurav jain ||| shivang chopra ||| suransh chopra ||| anil singh parihar ||| 
2020 ||| mid-weight image super-resolution with bypass connection attention network. ||| hao shen ||| zhong-qiu zhao ||| 
2020 ||| dual attention-based adversarial autoencoder for attributed network embedding. ||| ming liu ||| jianxin liao ||| jingyu wang ||| qi qi ||| haifeng sun ||| 
2020 ||| feature importance estimation with self-attention networks. ||| blaz skrlj ||| saso dzeroski ||| nada lavrac ||| matej petkovic ||| 
2020 ||| forecaster: a graph transformer for forecasting spatial and time-dependent data. ||| yang li ||| jos |||  m. f. moura ||| 
2020 ||| self-attention-based fully-inception networks for continuous sign language recognition. ||| mingjie zhou ||| michael ng ||| zixin cai ||| ka chun cheung ||| 
2020 ||| transformer-based argument mining for healthcare applications. ||| tobias mayer ||| elena cabrio ||| serena villata ||| 
2020 ||| si-agan: spatial interpolation with attentional generative adversarial networks for environment monitoring. ||| yujia gao ||| liang liu ||| chi zhang ||| xiao wang ||| huadong ma ||| 
2020 ||| a lightweight recurrent attention network for real-time guidewire segmentation and tracking in interventional x-ray fluoroscopy. ||| yan-jie zhou ||| xiao-liang xie ||| gui-bin bian ||| zeng-guang hou ||| 
2020 ||| group behavior recognition using attention- and graph-based neural networks. ||| fangkai yang ||| wenjie yin ||| tetsunari inamura ||| m ||| rten bj ||| rkman ||| christopher e. peters ||| 
2020 ||| simplifying graph attention networks with source-target separation. ||| hantao guo ||| rui yan ||| yansong feng ||| xuesong gao ||| zhanxing zhu ||| 
2020 ||| span-based joint entity and relation extraction with transformer pre-training. ||| markus eberts ||| adrian ulges ||| 
2018 ||| towards crossmodal learning for smooth multimodal attention orientation. ||| frederik haarslev ||| david docherty ||| stefan-daniel suvei ||| william kristian juel ||| leon bodenhagen ||| danish shaikh ||| norbert kr ||| ger ||| poramate manoonpong ||| 
2018 ||| autonomous assistance control based on inattention of the driver when driving a truck tract. ||| elvis bunces ||| v ||| ctor danilo zambrano ||| 
2020 ||| towards the design of a robot for supporting children's attention during long distance learning. ||| dante arroyo ||| yijie guo ||| mingyue yu ||| mohammad shidujaman ||| rodrigo fernandes ||| 
2018 ||| an attention-aware model for human action recognition on tree-based skeleton sequences. ||| runwei ding ||| chang liu ||| hong liu ||| 
2018 ||| training autistic children on joint attention skills with a robot. ||| kelsey carlson ||| alvin hong yee wong ||| tran anh dung ||| chern yuen anthony wong ||| yeow kee tan ||| agnieszka wykowska ||| 
2021 ||| developing a robot's empathetic reactive response inspired by a bottom-up attention model. ||| randy gomez ||| yu fang ||| serge thill ||| ricardo ragel ||| heike brock ||| keisuke nakamura ||| yurii vasylkiv ||| eric nichols ||| luis merino ||| 
2020 ||| a fault ride through strategy of multi-ports dc transformer. ||| hao wu ||| 
2020 ||| a method of prediction for transformer malfunction based on oil chromatography. ||| hao wu ||| yang zhou ||| chuanqi yang ||| hongmei zhu ||| dongxin hao ||| shuangzan ren ||| 
2020 ||| a decoupling control method for multi-ports dc transformer. ||| hao wu ||| 
2020 ||| stable style transformer: delete and generate approach with encoder-decoder for text style transfer. ||| joosung lee ||| 
2020 ||| memory attentive fusion: external language model integration for transformer-based sequence-to-sequence model. ||| mana ihori ||| ryo masumura ||| naoki makishima ||| tomohiro tanaka ||| akihiko takashima ||| shota orihashi ||| 
2021 ||| biomedical data-to-text generation via fine-tuning transformers. ||| ruslan yermakov ||| nicholas drago ||| angelo ziletti ||| 
2021 ||| controllable sentence simplification with a unified text-to-text transfer transformer. ||| kim cheng sheang ||| horacio saggion ||| 
2019 ||| can neural image captioning be controlled via forced attention? ||| philipp sadler ||| tatjana scheffler ||| david schlangen ||| 
2021 ||| attention is indeed all you need: semantically attention-guided decoding for data-to-text nlg. ||| juraj juraska ||| marilyn a. walker ||| 
2020 ||| generating quantified referring expressions through attention-driven incremental perception. ||| gordon briggs ||| 
2020 ||| chart-to-text: generating natural language descriptions for charts by adapting the transformer model. ||| jason obeid ||| enamul hoque ||| 
2020 ||| overview of the transformer-based models for nlp tasks. ||| anthony gillioz ||| jacky casas ||| elena mugellini ||| omar abou khaled ||| 
2021 ||| evaluation of neural network transformer models for named-entity recognition on low-resourced languages. ||| ridewaan hanslo ||| 
2017 ||| refocusing attention on unobserved attributes to reach consensus in decision making problems involving a heterogeneous group of experts. ||| marcelo loor ||| ana tapia-rosero ||| guy de tr ||| 
2021 ||| towards a question answering assistant for software development using a transformer-based language model. ||| liliane do nascimento vale ||| marcelo de almeida maia ||| 
2021 ||| exploring use of transformer based models on incident reports in aviation. ||| samuel kierszbaum ||| laurent lapasset ||| thierry klein ||| 
2021 ||| multilingual machine translation systems at wat 2021: one-to-many and many-to-one transformer based nmt. ||| shivam mhaskar ||| aditya jain ||| aakash banerjee ||| pushpak bhattacharyya ||| 
2019 ||| nlprl at wat2019: transformer-based tamil - english indic task neural machine translation system. ||| amit kumar ||| anil kumar singh ||| 
2020 ||| transformer-based double-token bidirectional autoregressive decoding in neural machine translation. ||| kenji imamura ||| eiichiro sumita ||| 
2018 ||| iitp-mt at wat2018: transformer-based multilingual indic-english neural machine translation system. ||| sukanta sen ||| kamal kumar gupta ||| asif ekbal ||| pushpak bhattacharyya ||| 
2021 ||| a fuzzy logic proposal for diagnosis multiple incipient faults in a power transformer. ||| juan carlos fern ||| ndez blanco ||| luis benigno corrales ||| f ||| lix herminio hern ||| ndez ||| israel francisco ben ||| tez ||| jos |||  ricardo n ||| ez ||| 
2021 ||| multi-level graph attention network based unsupervised network alignment. ||| yilin xiao ||| ruimin hu ||| dengshi li ||| junhang wu ||| yu zhen ||| lingfei ren ||| 
2019 ||| a neural attention model for real-time network intrusion detection. ||| mengxuan tan ||| alfonso iacovazzi ||| ngai-man cheung ||| yuval elovici ||| 
2020 ||| a novel data-to-text generation model with transformer planning and a wasserstein auto-encoder. ||| xiaohong xu ||| ting he ||| huazhen wang ||| 
2020 ||| personality traits prediction based on users' digital footprints in social networks via attention rnn. ||| shipeng wang ||| lizhen cui ||| lei liu ||| xudong lu ||| qingzhong li ||| 
2019 ||| service recommendation based on attentional factorization machine. ||| yingcheng cao ||| jianxun liu ||| min shi ||| buqing cao ||| ting chen ||| yiping wen ||| 
2019 ||| tuning multilingual transformers for language-specific named entity recognition. ||| mikhail y. arkhipov ||| maria trofimova ||| yuri kuratov ||| alexey sorokin ||| 
2019 ||| multilingual named entity recognition using pretrained embeddings, attention mechanism and ncrf. ||| anton a. emelyanov ||| ekaterina artemova ||| 
2021 ||| empirical based approaches to evaluating the residual life for oil-immersed transformers - a case study. ||| b. a. thango ||| aloys oriedi akumu ||| l. s. sikhosana ||| agha francis nnachi ||| jaco a. jordaan ||| 
2017 ||| transformer condition assessment for maintenance ranking: a comparison of three standards and different weighting techniques. ||| g. kimani irungu ||| aloys oriedi akumu ||| j. lange munda ||| 
2019 ||| improving the real power capacity of a furnace transformer through capacitance injection. ||| nonhlanhla mahlangu ||| agha francis nnachi ||| aloys oriedi akumu ||| wonderful mubatanhema ||| 
2017 ||| off-load tap-change transformer impact under high distributed pv generation. ||| zama goqo ||| sanjeeth sewchurran ||| david george dorrell ||| 
2019 ||| a review of detection and mitigation of negative dissipation factor in high voltage power transformers. ||| permit magabane ||| pitshou n. bokoro ||| johan jordaan ||| 
2021 ||| on the impact of grid harmonics in transformers: a case study. ||| b. a. thango ||| aloys oriedi akumu ||| l. s. sikhosana ||| agha francis nnachi ||| jaco a. jordaan ||| 
2021 ||| residual life estimation of power transformer based on karl fischer and adaptive neuro-fuzzy interference system. ||| permit mathuhu sekatane ||| thomas otieno olwal ||| 
2019 ||| simulation studies on the grounding of clustered single phase transformers in rural electrification. ||| erwin normanyo ||| philip blewushie ||| kwame mawutor addo ||| 
2017 ||| the design of an integrated diplexer-power divider based on dual-band impedance transformers and a five-port power-divider. ||| ayman s. al-zayed ||| mohammed a. kourah ||| 
2021 ||| method for increasing the accuracy of tracking the center of attention of the gaze. ||| boronenko m. p. ||| isaeva o. l. ||| zelensky v. i. ||| 
2021 ||| power grid stability prediction model based on bilstm with attention. ||| yan zhang ||| hongmei zhang ||| ji zhang ||| liangyu li ||| ziyao zheng ||| 
2021 ||| batae-gru: attention-based aspect sentiment analysis model. ||| yuan wang ||| qian wang ||| 
2019 ||| dividi2: reinforcing divided attention in children with ad/hd through a mobile application. ||| ivett daniela j ||| come v. ||| juan sebasti ||| n p ||| ez o. ||| c ||| sar alberto collazos ord ||| ez ||| habib m. fardoun ||| 
2019 ||| development of a system for the identification of adhd in children: attention monitor. ||| alfredo garcia ||| juan manuel gonz ||| lez ||| josefina guerrero garc ||| a ||| amparo dora palomino ||| 
2017 ||| feasibility of detecting adhd patients' attention levels by classifying their eeg signals. ||| alaa eddin alchalabi ||| mohamed elsharnouby ||| shervin shirmohammadi ||| amer nour eddin ||| 
2017 ||| optoelectronic method for determining the aluminium involved in symptoms of attention deficit hyperactivity disorder children. ||| elena truta ||| ana maria davitoiu ||| ana mihaela mitu ||| alexandra andrada bojescu ||| paul schiopu ||| marian vladescu ||| genica caragea ||| luminita horhota ||| maria gabriela neicu ||| mihai ionica ||| 
2017 ||| diagnosis of attention deficit hyperactivity disorder using deep belief network based on greedy approach. ||| saeed farzi ||| sahar kianian ||| ilnaz rastkhadive ||| 
2021 ||| fast haptic terrain classification for legged robots using transformer. ||| michal bednarek ||| mikolaj lysakowski ||| jakub bednarek ||| michal r. nowicki ||| krzysztof walas ||| 
2019 ||| forward-backward visual saliency propagation in deep nns vs internal attentional mechanisms. ||| abraham montoya obeso ||| jenny benois-pineau ||| mireya sara |||  garc ||| a-v ||| zquez ||| alejandro alvaro ram ||| rez-acosta ||| 
2019 ||| attention-guided deep convolutional neural networks for skin cancer classification. ||| arshiya aggarwal ||| nisheet das ||| indu sreedevi ||| 
2019 ||| an optimized modeling method for transformer design. ||| yingying liang ||| xiaoming liu ||| jing jin ||| 
2019 ||| a 60 ghz single-to-differential lna using slow-wave cpw and transformer coupling in 28 nm cmos. ||| benqing guo ||| haifeng liu ||| yao wang ||| jun chen ||| 
2019 ||| a35.2 dbm cmos rf power amplifier using an 8-way current-voltage combining transformer with harmonic control. ||| hejia cai ||| yan hu ||| zhiliang hong ||| 
2021 ||| facial expressions and body postures emotion recognition based on convolutional attention network. ||| tiehua zhou ||| shiru gao ||| yuanhao mei ||| ling wang ||| 
2020 ||| effects of different representation styles on user attention in mooc. ||| xinyong zhang ||| 
2019 ||| method name suggestion with hierarchical attention networks. ||| sihan xu ||| sen zhang ||| weijing wang ||| xinya cao ||| chenkai guo ||| jing xu ||| 
2020 ||| iitk at semeval-2020 task 10: transformers for emphasis selection. ||| vipul singhal ||| sahil dhull ||| rishabh agarwal ||| ashutosh modi ||| 
2020 ||| iscas at semeval-2020 task 5: pre-trained transformers for counterfactual statement modeling. ||| yaojie lu ||| annan li ||| hongyu lin ||| xianpei han ||| le sun ||| 
2018 ||| yuanfudao at semeval-2018 task 11: three-way attention and relational knowledge for commonsense machine comprehension. ||| liang wang ||| meng sun ||| wei zhao ||| kewei shen ||| jingming liu ||| 
2020 ||| kafk at semeval-2020 task 12: checkpoint ensemble of transformers for hate speech classification. ||| kaushik amar das ||| arup baruah ||| ferdous ahmed barbhuiya ||| kuntal dey ||| 
2021 ||| nlp-iis@ut at semeval-2021 task 4: machine reading comprehension using the long document transformer. ||| hossein basafa ||| sajad movahedi ||| ali ebrahimi ||| azadeh shakery ||| heshaam faili ||| 
2019 ||| ubc-nlp at semeval-2019 task 4: hyperpartisan news detection with attention-based bi-lstms. ||| chiyu zhang ||| arun rajendran ||| muhammad abdul-mageed ||| 
2020 ||| ujnlp at semeval-2020 task 12: detecting offensive language using bidirectional transformers. ||| yinnan yao ||| nan su ||| kun ma ||| 
2020 ||| uaics at semeval-2020 task 4: using a bidirectional transformer for task a. ||| ciprian-gabriel cusmuliuc ||| lucia georgiana coca ||| adrian iftene ||| 
2020 ||| gorynych transformer at semeval-2020 task 6: multi-task learning for definition extraction. ||| adis davletov ||| nikolay arefyev ||| alexander shatilov ||| denis gordeev ||| alexey rey ||| 
2020 ||| csecu_kde_ma at semeval-2020 task 8: a neural attention model for memotion analysis. ||| abu nowshed chy ||| umme aymun siddiqua ||| masaki aono ||| 
2020 ||| xd at semeval-2020 task 12: ensemble approach to offensive language identification in social media using transformer encoders. ||| xiangjue dong ||| jinho d. choi ||| 
2017 ||| datastories at semeval-2017 task 6: siamese lstm with attention for humorous text comparison. ||| christos baziotis ||| nikos pelekis ||| christos doulkeridis ||| 
2021 ||| rg pa at semeval-2021 task 1: a contextual attention-based model with roberta for lexical complexity prediction. ||| gang rao ||| maochang li ||| xiaolong hou ||| lian-xin jiang ||| yang mo ||| jianping shen ||| 
2018 ||| tcs research at semeval-2018 task 1: learning robust representations using multi-attention architecture. ||| hardik meisheri ||| lipika dey ||| 
2018 ||| amobee at semeval-2018 task 1: gru neural network with a cnn attention mechanism for sentiment classification. ||| alon rozental ||| daniel fleischer ||| 
2017 ||| datastories at semeval-2017 task 4: deep lstm with attention for message-level and topic-based sentiment analysis. ||| christos baziotis ||| nikos pelekis ||| christos doulkeridis ||| 
2018 ||| ynu deep at semeval-2018 task 12: a bilstm model with neural attention for argument reasoning comprehension. ||| peng ding ||| xiaobing zhou ||| 
2020 ||| hitachi at semeval-2020 task 3: exploring the representation spaces of transformers for human sense word similarity. ||| terufumi morishita ||| gaku morio ||| hiroaki ozaki ||| toshinori miyoshi ||| 
2018 ||| thu_ngn at semeval-2018 task 2: residual cnn-lstm network with attention for english emoji prediction. ||| chuhan wu ||| fangzhao wu ||| sixing wu ||| zhigang yuan ||| junxin liu ||| yongfeng huang ||| 
2019 ||| nuli at semeval-2019 task 6: transfer learning for offensive language detection using bidirectional transformers. ||| ping liu ||| wen li ||| liang zou ||| 
2019 ||| uc davis at semeval-2019 task 1: dag semantic parsing with attention-based decoder. ||| dian yu ||| kenji sagae ||| 
2019 ||| cn-hit-mi.t at semeval-2019 task 6: offensive language identification based on bilstm with double attention. ||| yaojie zhang ||| bing xu ||| tiejun zhao ||| 
2021 ||| ta-mamc at semeval-2021 task 4: task-adaptive pretraining and multi-head attention for abstract meaning reading comprehension. ||| jing zhang ||| yimeng zhuang ||| yinpei su ||| 
2018 ||| ynu-hpcc at semeval-2018 task 1: bilstm with attention based sentiment analysis for affect in tweets. ||| you zhang ||| jin wang ||| xuejie zhang ||| 
2018 ||| emojiit at semeval-2018 task 2: an effective attention-based recurrent neural network model for emoji prediction with characters gated words. ||| shiyun chen ||| maoquan wang ||| liang he ||| 
2020 ||| hr@just team at semeval-2020 task 4: the impact of roberta transformer for evaluation common sense understanding. ||| heba al-jarrah ||| rahaf al-hamouri ||| mohammad al-smadi ||| 
2020 ||| lt3 at semeval-2020 task 7: comparing feature-based and transformer-based approaches to detect funny headlines. ||| bram vanroy ||| sofie labat ||| olha kaminska ||| els lefever ||| v ||| ronique hoste ||| 
2020 ||| dothemath at semeval-2020 task 12 : deep neural networks with self attention for arabic offensive language detection. ||| zoher orabe ||| bushr haddad ||| nada ghneim ||| anas al-abood ||| 
2020 ||| upb at semeval-2020 task 9: identifying sentiment in code-mixed social media texts using transformers and multi-task learning. ||| george-eduard zaharia ||| george-alexandru vlad ||| dumitru-clementin cercel ||| traian rebedea ||| costin-gabriel chiru ||| 
2017 ||| neobility at semeval-2017 task 1: an attention-based sentence similarity model. ||| wenli zhuang ||| ernie chang ||| 
2017 ||| adullam at semeval-2017 task 4: sentiment analyzer using lexicon integrated convolutional neural networks with attention. ||| joosung yoon ||| kigon lyu ||| hyeoncheol kim ||| 
2019 ||| but-fit at semeval-2019 task 7: determining the rumour stance with pre-trained deep bidirectional transformers. ||| martin fajcik ||| pavel smrz ||| luk ||| s burget ||| 
2021 ||| humorhunter at semeval-2021 task 7: humor and offense recognition with disentangled attention. ||| yubo xie ||| junze li ||| pearl pu ||| 
2017 ||| lipn-iimas at semeval-2017 task 1: subword embeddings, attention recurrent neural networks and cross word alignment for semantic textual similarity. ||| ignacio arroyo-fern ||| ndez ||| iv ||| n vladimir meza ru ||| z ||| 
2021 ||| iiith at semeval-2021 task 7: leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining. ||| tathagata raha ||| ishan sanjeev upadhyay ||| radhika mamidi ||| vasudeva varma ||| 
2020 ||| uor at semeval-2020 task 4: pre-trained sentence transformer models for commonsense validation and explanation. ||| thanet markchom ||| bhuvana dhruva ||| chandresh pravin ||| huizhi liang ||| 
2018 ||| ynu-hpcc at semeval-2018 task 2: multi-ensemble bi-gru model with attention mechanism for multilingual emoji prediction. ||| nan wang ||| jin wang ||| xuejie zhang ||| 
2021 ||| csecu-dsg at semeval-2021 task 1: fusion of transformer models for lexical complexity prediction. ||| abdul aziz ||| md. akram hossain ||| abu nowshed chy ||| 
2017 ||| oxford at semeval-2017 task 9: neural amr parsing with pointer-augmented attention. ||| jan buys ||| phil blunsom ||| 
2021 ||| roma at semeval-2021 task 7: a transformer-based approach for detecting and rating humor and offense. ||| roberto labadie ||| mariano jason rodriguez cisnero ||| reynier ortega bueno ||| paolo rosso ||| 
2018 ||| ynu-hpcc at semeval-2018 task 11: using an attention-based cnn-lstm for machine comprehension using commonsense knowledge. ||| hang yuan ||| jin wang ||| xuejie zhang ||| 
2021 ||| 1213li at semeval-2021 task 6: detection of propaganda with multi-modal attention and pre-trained models. ||| peiguang li ||| xuan li ||| xian sun ||| 
2020 ||| uhh-lt at semeval-2020 task 12: fine-tuning of pre-trained transformer networks for offensive language detection. ||| gregor wiedemann ||| seid muhie yimam ||| chris biemann ||| 
2019 ||| clp at semeval-2019 task 3: multi-encoder in hierarchical attention networks for contextual emotion detection. ||| changjie li ||| yun xing ||| 
2018 ||| ntua-slp at semeval-2018 task 2: predicting emojis using rnns with context-aware attention. ||| christos baziotis ||| athanasiou nikolaos ||| athanasia kolovou ||| georgios paraskevopoulos ||| nikolaos ellinas ||| alexandros potamianos ||| 
2020 ||| palomino-ochoa at semeval-2020 task 9: robust system based on transformer for code-mixed sentiment classification. ||| daniel palomino ||| jos |||  ochoa luna ||| 
2018 ||| ynu_deep at semeval-2018 task 11: an ensemble of attention-based bilstm models for machine comprehension. ||| peng ding ||| xiaobing zhou ||| 
2021 ||| ecnu_ica_1 semeval-2021 task 4: leveraging knowledge-enhanced graph attention networks for reading comprehension of abstract meaning. ||| pingsheng liu ||| linlin wang ||| qian zhao ||| hao chen ||| yuxi feng ||| xin lin ||| liang he ||| 
2019 ||| ynu nlp at semeval-2019 task 5: attention and capsule ensemble for identifying hate speech. ||| bin wang ||| haiyan ding ||| 
2020 ||| wessa at semeval-2020 task 9: code-mixed sentiment analysis using transformers. ||| ahmed sultan ||| mahmoud salim ||| amina gaber ||| islam el hosary ||| 
2018 ||| itnlp-arc at semeval-2018 task 12: argument reasoning comprehension with attention. ||| wenjie liu ||| chengjie sun ||| lei lin ||| bingquan liu ||| 
2021 ||| utnlp at semeval-2021 task 5: a comparative analysis of toxic span detection using attention-based, named entity recognition, and ensemble models. ||| alireza salemi ||| nazanin sabri ||| emad kebriaei ||| behnam bahrak ||| azadeh shakery ||| 
2021 ||| cs-um6p at semeval-2021 task 1: a deep learning model-based pre-trained transformer encoder for lexical complexity. ||| nabil el mamoun ||| abdelkader el mahdaouy ||| abdellah el mekki ||| kabil essefar ||| ismail berrada ||| 
2021 ||| cs60075_team2 at semeval-2021 task 1 : lexical complexity prediction using transformer-based language models pre-trained on various text corpora. ||| abhilash nandy ||| sayantan adak ||| tanurima halder ||| sai mahesh pokala ||| 
2019 ||| pkuse at semeval-2019 task 3: emotion detection with emotion-oriented neural attention network. ||| luyao ma ||| long zhang ||| wei ye ||| wenhui hu ||| 
2020 ||| lmml at semeval-2020 task 7: siamese transformers for rating humor in edited news headlines. ||| pramodith ballapuram ||| 
2020 ||| hitachi at semeval-2020 task 11: an empirical study of pre-trained transformer family for propaganda detection. ||| gaku morio ||| terufumi morishita ||| hiroaki ozaki ||| toshinori miyoshi ||| 
2018 ||| attnconvnet at semeval-2018 task 1: attention-based convolutional neural networks for multi-label emotion classification. ||| yanghoon kim ||| hwanhee lee ||| kyomin jung ||| 
2020 ||| transformers at semeval-2020 task 11: propaganda fragment detection using diversified bert architectures based ensemble learning. ||| ekansh verma ||| vinodh motupalli ||| souradip chakraborty ||| 
2021 ||| ynu-hpcc at semeval-2021 task 10: using a transformer-based source-free domain adaptation model for semantic processing. ||| zhewen yu ||| jin wang ||| xuejie zhang ||| 
2021 ||| t at semeval-2021 task 5: integrating transformer and crf for toxic spans detection. ||| chenyi wang ||| tianshu liu ||| tiejun zhao ||| 
2019 ||| ynu-hpcc at semeval-2019 task 8: using a lstm-attention model for fact-checking in community forums. ||| peng liu ||| jin wang ||| xuejie zhang ||| 
2019 ||| amrita school of engineering - cse at semeval-2019 task 6: manipulating attention with temporal convolutional neural network for offense identification and classification. ||| murali sridharan ||| swapna t. r. ||| 
2020 ||| baksa at semeval-2020 task 9: bolstering cnn with self-attention for sentiment analysis of code mixed text. ||| ayush kumar ||| harsh agarwal ||| keshav bansal ||| ashutosh modi ||| 
2020 ||| ynu-oxz at semeval-2020 task 5: detecting counterfactuals based on ordered neurons lstm and hierarchical attention network. ||| xiaozhi ou ||| shengyan liu ||| hongling li ||| 
2021 ||| alpha at semeval-2021 task 6: transformer based propaganda classification. ||| zhida feng ||| jiji tang ||| jiaxiang liu ||| weichong yin ||| shikun feng ||| yu sun ||| li chen ||| 
2020 ||| brums at semeval-2020 task 12: transformer based multilingual offensive language identification in social media. ||| tharindu ranasinghe ||| hansi hettiarachchi ||| 
2020 ||| ks@lth at semeval-2020 task 12: fine-tuning multi- and monolingual transformer models for offensive language detection. ||| kasper socha ||| 
2021 ||| liori at semeval-2021 task 8: ask transformer for measurements. ||| adis davletov ||| denis gordeev ||| nikolay arefyev ||| emil t. davletov ||| 
2018 ||| jiangnan at semeval-2018 task 11: deep neural network with attention method for machine comprehension task. ||| jiangnan xia ||| 
2018 ||| ecnu at semeval-2018 task 12: an end-to-end attention-based neural network for the argument reasoning comprehension task. ||| junfeng tian ||| man lan ||| yuanbin wu ||| 
2020 ||| cnrl at semeval-2020 task 5: modelling causal reasoning in language with multi-head self-attention weights based counterfactual detection. ||| rajaswa patil ||| veeky baths ||| 
2021 ||| lecun at semeval-2021 task 6: detecting persuasion techniques in text using ensembled pretrained transformers and data augmentation. ||| dia abujaber ||| ahmed qarqaz ||| malak abdullah ||| 
2018 ||| thu_ngn at semeval-2018 task 1: fine-grained tweet sentiment intensity analysis with attention cnn-lstm. ||| chuhan wu ||| fangzhao wu ||| junxin liu ||| zhigang yuan ||| sixing wu ||| yongfeng huang ||| 
2018 ||| ynu-hpcc at semeval-2018 task 12: the argument reasoning comprehension task using a bi-directional lstm with attention model. ||| quanlei liao ||| xutao yang ||| jin wang ||| xuejie zhang ||| 
2017 ||| swissalps at semeval-2017 task 3: attention-based convolutional neural network for community question answering. ||| jan deriu ||| mark cieliebak ||| 
2019 ||| lijunyi at semeval-2019 task 9: an attention-based lstm and ensemble of different models for suggestion mining from online reviews and forums. ||| junyi li ||| 
2018 ||| tweety at semeval-2018 task 2: predicting emojis using hierarchical attention neural networks and support vector machine. ||| daniel kopev ||| atanas atanasov ||| dimitrina zlatkova ||| momchil hardalov ||| ivan koychev ||| ivelina nikolova ||| galia angelova ||| 
2021 ||| mcl@iitk at semeval-2021 task 2: multilingual and cross-lingual word-in-context disambiguation using augmented data, signals, and transformers. ||| rohan gupta ||| jay mundra ||| deepak mahajan ||| ashutosh modi ||| 
2018 ||| joker at semeval-2018 task 12: the argument reasoning comprehension with neural attention. ||| guobin sui ||| wen-han chao ||| zhunchen luo ||| 
2020 ||| justers at semeval-2020 task 4: evaluating transformer models against commonsense validation and explanation. ||| ali fadel ||| mahmoud al-ayyoub ||| erik cambria ||| 
2021 ||| iapucp at semeval-2021 task 1: stacking fine-tuned transformers is almost all you need for lexical complexity prediction. ||| kervy rivas rojas ||| fernando alva-manchego ||| 
2019 ||| lastus/taln at semeval-2019 task 6: identification and categorization of offensive language in social media with attention-based bi-lstm model. ||| lutfiye seda mut altin ||| lex bravo serrano ||| horacio saggion ||| 
2020 ||| ecnu at semeval-2020 task 7: assessing humor in edited news headlines using bilstm with attention. ||| tiantian zhang ||| zhixuan chen ||| man lan ||| 
2019 ||| thu_ngn at semeval-2019 task 3: dialog emotion classification using attentional lstm-cnn. ||| suyu ge ||| tao qi ||| chuhan wu ||| yongfeng huang ||| 
2021 ||| wlv-rit at semeval-2021 task 5: a neural transformer framework for detecting toxic spans. ||| tharindu ranasinghe ||| diptanu sarkar ||| marcos zampieri ||| alexander g. ororbia ||| 
2021 ||| xrjl-hkust at semeval-2021 task 4: wordnet-enhanced dual multi-head co-attention for reading comprehension of abstract meaning. ||| yuxin jiang ||| ziyi shou ||| qijun wang ||| hao wu ||| fangzhen lin ||| 
2021 ||| ynu-hpcc at semeval-2021 task 5: using a transformer-based model with auxiliary information for toxic span detection. ||| ruijun chen ||| jin wang ||| xuejie zhang ||| 
2021 ||| csecu-dsg at semeval-2021 task 7: detecting and rating humor and offense employing transformers. ||| afrin sultana ||| nabila ayman ||| abu nowshed chy ||| 
2020 ||| uld@nuig at semeval-2020 task 9: generative morphemes with an attention model for sentiment analysis in code-mixed text. ||| koustava goswami ||| priya rani ||| bharathi raja chakravarthi ||| theodorus fransen ||| john p. mccrae ||| 
2021 ||| uot-uwf-partai at semeval-2021 task 5: self attention based bi-gru with multi-embedding representation for toxicity highlighter. ||| hamed babaei giglou ||| taher rahgooy ||| mostafa rahgouy ||| jafar razmara ||| 
2021 ||| aimh at semeval-2021 task 6: multimodal classification using an ensemble of transformer models. ||| nicola messina ||| fabrizio falchi ||| claudio gennaro ||| giuseppe amato ||| 
2020 ||| problemconquero at semeval-2020 task 12: transformer and soft label-based approaches. ||| karishma laud ||| jagriti singh ||| randeep kumar sahu ||| ashutosh modi ||| 
2021 ||| transwic at semeval-2021 task 2: transformer-based multilingual and cross-lingual word-in-context disambiguation. ||| hansi hettiarachchi ||| tharindu ranasinghe ||| 
2018 ||| blcu_nlp at semeval-2018 task 12: an ensemble model for argument reasoning based on hierarchical attention. ||| meiqian zhao ||| chunhua liu ||| lu liu ||| yan zhao ||| dong yu ||| 
2020 ||| pum at semeval-2020 task 12: aggregation of transformer-based models' features for offensive language recognition. ||| piotr janiszewski ||| mateusz skiba ||| urszula walinska ||| 
2019 ||| caire_hkust at semeval-2019 task 3: hierarchical attention for dialogue emotion classification. ||| genta indra winata ||| andrea madotto ||| zhaojiang lin ||| jamin shin ||| yan xu ||| peng xu ||| pascale fung ||| 
2020 ||| boun-rex at clef-2020 chemu task 2: evaluating pretrained transformers for event extraction. ||| hilal d ||| nmez ||| abdullatif k ||| ksal ||| elif  ||| zkirimli ||| arzucan  ||| zg ||| r ||| 
2019 ||| recurrent attention networks for medical concept prediction. ||| sam maksoud ||| arnold wiliem ||| brian c. lovell ||| 
2019 ||| a hierarchical attention network for bots and gender profiling. ||| cristian onose ||| claudiu-marcel nedelcu ||| dumitru-clementin cercel ||| stefan trausan-matu ||| 
2020 ||| 2020: if you say so: post-hoc fact-checking of claims using transformer-based models. ||| evan m. williams ||| paul rodrigues ||| valerie novak ||| 
2020 ||| transformers in semantic indexing of clinical codes. ||| rishivardhan k ||| kayalvizhi s ||| d. thenmozhi ||| sachin krishan ||| chandrabose aravindan ||| 
2020 ||| exploring argument retrieval with transformers. ||| christopher akiki ||| martin potthast ||| 
2021 ||| attention-based cnn-gru model for automatic medical images captioning: imageclef 2021. ||| djamila romaissa beddiar ||| mourad oussalah ||| tapio sepp ||| nen ||| 
2021 ||| efficientnets and vision transformers for snake species identification using image and location information. ||| louise bloch ||| christoph m. friedrich ||| 
2021 ||| comparing transformer-based ner approaches for analysing textual medical diagnoses. ||| marco polignano ||| marco de gemmis ||| giovanni semeraro ||| 
2021 ||| lijie at imageclefmed vqa-med 2021: attention model-based efficient interaction between multimodality. ||| jie li ||| shengyan liu ||| 
2020 ||| transformer-based open domain biomedical question answering at bioasq8 challenge. ||| ashot kazaryan ||| uladzislau sazanovich ||| vladislav belyaev ||| 
2020 ||| check_square at checkthat! 2020 claim detection in social media via fusion of transformer and syntactic features. ||| gullal s. cheema ||| sherzod hakimov ||| ralph ewerth ||| 
2020 ||| early risk detection of self-harm and depression severity using bert-based transformers. ||| rodrigo mart ||| nez-casta ||| o ||| amal htait ||| leif azzopardi ||| yashar moshfeghi ||| 
2021 ||| 2021: ensemble transformer model for fake news classification. ||| hariharan ramakrishnaiyer lekshmiammal ||| anand kumar madasamy ||| 
2020 ||| 2020: approaching fact checking from a sentence similarity perspective through the lens of transformers. ||| lucia c. passaro ||| alessandro bondielli ||| alessandro lenci ||| francesco marcelloni ||| 
2020 ||| convolutional attention models with post-processing heuristics at clef ehealth 2020. ||| elias moons ||| marie-francine moens ||| 
2021 ||| ranked list fusion and re-ranking with pre-trained transformers for arqmath lab. ||| shaurya rohatgi ||| jian wu ||| c. lee giles ||| 
2017 ||| author profiling with word+character neural attention network. ||| yasuhide miura ||| tomoki taniguchi ||| motoki taniguchi ||| tomoko ohkuma ||| 
2020 ||| harendrakv at vqa-med 2020: sequential vqa with attention for medical visual question answering. ||| harendra verma ||| sindhu ramachandran ||| 
2021 ||| identify hate speech spreaders on twitter using transformer embeddings features and automl classifiers. ||| talha anwar ||| 
2020 ||| multilingual icd-10 code assignment with transformer architectures using mimic-iii discharge summaries. ||| henning sch ||| fer ||| christoph m. friedrich ||| 
2017 ||| audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms. ||| antoine sevilla ||| herv |||  glotin ||| 
2021 ||| bert-based transformers for early detection of mental health illnesses. ||| rodrigo mart ||| nez-casta ||| o ||| amal htait ||| leif azzopardi ||| yashar moshfeghi ||| 
2021 ||| 2021: check-worthiness estimation as a regression problem on transformers. ||| albert pritzkau ||| 
2021 ||| transformer-based language models for factoid question answering at bioasq9b. ||| urvashi khanna ||| diego moll ||| 
2019 ||| event sentence detection task using attention model. ||| ali safaya ||| 
2019 ||| extracting protests from news using lstm models with different attention mechanisms. ||| d. thenmozhi ||| chandrabose aravindan ||| abishek shyamsunder ||| adithya viswanathan ||| akash kumar pujari ||| 
2021 ||| 2021: check-worthiness estimation and fake news detection using transformer models. ||| juan r. martinez-rico ||| juan mart ||| nez-romo ||| lourdes araujo ||| 
2021 ||| 2021: fado-fake news detection and domain identification using transformers ensembling. ||| fazlourrahman balouchzahi ||| hosahalli lakshmaiah shashirekha ||| grigori sidorov ||| 
2020 ||| 2020: identifying check-worthy tweets with transformer models. ||| alex nikolov ||| giovanni da san martino ||| ivan koychev ||| preslav nakov ||| 
2017 ||| attention-based medical caption generation with image modality classification and clinical concept mapping. ||| sadid a. hasan ||| yuan ling ||| joey liu ||| rithesh sreenivasan ||| shreya anand ||| tilak raj arora ||| vivek v. datla ||| kathy lee ||| ashequl qadir ||| christine leon swisher ||| oladimeji farri ||| 
2021 ||| profiling hate speech spreaders on twitter: transformers and mixed pooling. ||| lvaro huertas-garc ||| a ||| javier huertas-tato ||| alejandro mart ||| n ||| david camacho ||| 
2017 ||| author profiling with bidirectional rnns using attention with grus. ||| don kodiyan ||| florin hardegger ||| stephan neuhaus ||| mark cieliebak ||| 
2021 ||| stft transformers for bird song recognition. ||| jean-francois puget ||| 
2020 ||| 2020: tweet check worthiness using transformers, convolutional neural networks and support vector machines. ||| sachin krishan thyaharajan ||| kayalvizhi sampath ||| thenmozhi durairaj ||| rishivardhan krishnamoorthy ||| 
2021 ||| 2021: integration of transformers in misinformation detection and topic classification. ||| lvaro huertas-garc ||| a ||| javier huertas-tato ||| alejandro mart ||| n ||| david camacho ||| 
2018 ||| a parallel hierarchical attention network for style change detection: notebook for pan at clef 2018. ||| marjan hosseinia ||| arjun mukherjee ||| 
2020 ||| lifelog moment retrieval with self-attention based joint embedding model. ||| hoang-phuc trang-trung ||| hoang-anh le ||| minh-triet tran ||| 
2017 ||| deep networks for human visual attention: a hybrid model using foveal vision. ||| ana filipa almeida ||| rui figueiredo ||| alexandre bernardino ||| jos |||  santos-victor ||| 
2018 ||| integrating global attention for pairwise text comparison. ||| jie mei ||| xiang jiang ||| aminul islam ||| abidalrahman moh'd ||| evangelos e. milios ||| 
2021 ||| ordering sentences and paragraphs with pre-trained encoder-decoder transformers and pointer ensembles. ||| r ||| mi calizzano ||| malte ostendorff ||| georg rehm ||| 
2019 ||| quantifying the effects of temperature and noise on attention-level using eda and eeg sensors. ||| zhengrui xue ||| luning yang ||| prapa rattadilok ||| shanshan li ||| longyue gao ||| 
2021 ||| a wideband cmos power amplifier with integrated digital linearizer and tunable transformer. ||| selvakumar mariappan ||| jagadheswaran rajendran ||| narendra kumar ||| andrei grebennikov ||| arokia nathan ||| binboga siddik yarman ||| 
2021 ||| a broadband tri-coil based transformer design for mm-wave cascode amplifiers. ||| haoyang jia ||| guangyin feng ||| yanjie wang ||| 
2021 ||| a differential rf front-end cmos transformer matching for ambient rf energy harvesting systems. ||| wen xun lian ||| harikrishnan ramiah ||| gabriel chong ||| p. c. kishore kumar ||| 
2019 ||| convolutional neural network and attention mechanism for bone age prediction. ||| yanisa mahayossanunt ||| titichaya thannamitsomboon ||| chadaporn keatmanee ||| 
2022 ||| ami: attention based adaptative feedback with augmented reality to improve takeover performances in highly automated vehicles. ||| baptiste wojtkowski ||| indira thouvenin ||| veronica teichrieb ||| 
2019 ||| action units: directing user attention in 360-degree video based vr. ||| lingwei tong ||| sungchul jung ||| robert w. lindeman ||| 
2020 ||| using multiple perspective projections to guide visual attention in glyph-based data visualisations in vr. ||| robert richter ||| tobias g ||| nther ||| rainer groh ||| 
2019 ||| impact of gamified interaction with virtual nature on sustained attention and self-reported restoration - a pilot study. ||| oankar patil ||| heng yao ||| benjamin lok ||| 
2019 ||| esn-ner: entity storage network using attention mechanism for chinese ner. ||| wenkai wang ||| liang chang ||| chenzhong bin ||| wen xuan ||| wei chen ||| long li ||| 
2020 ||| extending equational monadic reasoning with monad transformers. ||| reynald affeldt ||| david nowak ||| 
2018 ||| web service discovery based on information gain theory and bilstm with attention mechanism. ||| xiangping zhang ||| jianxun liu ||| buqing cao ||| qiaoxiang xiao ||| yiping wen ||| 
2019 ||| relation extraction toward patent domain based on keyword strategy and attention+bilstm model (short paper). ||| xueqiang lv ||| xiangru lv ||| xindong you ||| zhian dong ||| junmei han ||| 
2019 ||| a next location predicting approach based on a recurrent neural network and self-attention. ||| jun zeng ||| xin he ||| haoran tang ||| junhao wen ||| 
2019 ||| web services classification with topical attention based bi-lstm. ||| yingcheng cao ||| jianxun liu ||| buqing cao ||| min shi ||| yiping wen ||| zhenlian peng ||| 
2019 ||| attention-based bilinear joint learning framework for entity linking. ||| min cao ||| penglong wang ||| honghao gao ||| jiangang shi ||| yuan tao ||| weilin zhang ||| 
2021 ||| question classification using universal sentence encoder and deep contextualized transformer. ||| najam arif ||| seemab latif ||| rabia latif ||| 
2021 ||| operationalizing a national digital library: the case for a norwegian transformer model. ||| per egil kummervold ||| javier de la rosa ||| freddy wetjen ||| svein arne brygfjeld ||| 
2022 ||| a predicate transformer for choreographies - computing preconditions in choreographic programming. ||| sung-shik jongmans ||| petra van den bos ||| 
2020 ||| an answer sorting method combining multiple neural networks and attentional mechanisms. ||| liguo duan ||| jin zhang ||| long wang ||| jianying gao ||| aiping li ||| 
2020 ||| link prediction of attention flow network based on maximum entropy model. ||| yong li ||| jingpeng wu ||| zhangyun gong ||| qiang zhang ||| xiaokang zhang ||| fangqi cheng ||| fang wang ||| changqing wang ||| 
2020 ||| graph representation learning using attention network. ||| bijay gaudel ||| donghai guan ||| weiwei yuan ||| deepanjal shrestha ||| bing chen ||| yaofeng tu ||| 
2019 ||| attentional transformer networks for target-oriented sentiment classification. ||| jianing tong ||| wei chen ||| zhihua wei ||| 
2021 ||| driver attention assistance by pedestrian/cyclist distance estimation from a single rgb image: a cnn-based semantic segmentation approach. ||| angelo genovese ||| vincenzo piuri ||| francesco rundo ||| fabio scotti ||| concetto spampinato ||| 
2019 ||| optimal design and experimental validation of a novel line-frequency zig-zag transformer employed in a unified ac-dc system. ||| annoy kumar das ||| akshatha shetty ||| baylon g. fernandes ||| 
2020 ||| averaged models of a six-phase, dual-interleaved dc-dc buck-boost converter with interphase transformers. ||| enrique vel ||| zquez-elizondo ||| ilse cervantes ||| ismael araujo-vargas ||| kevin cano-pulido ||| 
2018 ||| analysis of 132kv/33kv 15mva power transformer dissolved gas using transport-x kelman kit through duval's triangle and roger's ratio prediction. ||| nitin zope ||| syed imran ali ||| sanjeevikumar padmanaban ||| mahajan sagar bhaskar ||| lucian mihet-popa ||| 
2021 ||| flexible control structure of a smart transformer for universal operation. ||| francisco huerta ||| daniel santamargarita ||| emilio jos |||  bueno ||| rongwu zhu ||| marco liserre ||| 
2017 ||| comparative assessment of three-phase transformerless grid-connected solar inverters. ||| deepak ronanki ||| phuoc huynh sang ||| vijay sood ||| sheldon s. williamson ||| 
2018 ||| research on a novel hybrid transformer for smart distribution network. ||| jun liu ||| huarong zeng ||| peilong chen ||| bin yang ||| jianhua wang ||| zhendong ji ||| jingyu song ||| 
2018 ||| development of a partial discharge testing system for potential transformers. ||| t. prombud ||| p. kitcharoen ||| p. yutthagowith ||| 
2019 ||| solid state transformer with integrated input stage. ||| nimrod v ||| zquez ||| marco liserre ||| 
2018 ||| design of a 4-phase intercell transformer converter for a space charge measuring system. ||| thierry martir ||| jean-charles laurentie ||| mourad jebli ||| ludovic boyer ||| mickael petit ||| 
2018 ||| fault diagnosis of transformer based on kpca and elman neural network. ||| jun lin ||| gehao sheng ||| yuhao gao ||| yingjie yan ||| xiuchen jiang ||| 
2017 ||| power control of a multi-cell solid-state transformer with extended functions. ||| jorge almaguer ||| victor m. c ||| rdenas ||| alejandro aganza torres ||| homero miranda ||| janeth alcal ||| fortino mendoza-mondrag ||| n ||| 
2021 ||| modulation strategy and control of modular cascade h-bridge converters as input-side of a multi-port smart transformer. ||| selene s ||| nchez-cruz ||| enrique romero-cadaval ||| bego ||| a montes cabrera ||| eva gonz ||| lez romera ||| mar ||| a isabel milan ||| s-montero ||| ferm ||| n barrero-gonz ||| lez ||| 
2018 ||| high-temperature coplanar transformer. ||| maxime semard ||| christian martin ||| cyril buttay ||| charles joubert ||| 
2017 ||| computation of rectifier transformers employed in railway networks. ||| gholamhossein shirkoohi ||| alex jenkins ||| 
2019 ||| a doubly-grounded transformer-less single-phase pv inverter with boost capability. ||| zhong wang ||| zhilei yao ||| qin wang ||| lan xiao ||| qunfang wu ||| tao liu ||| chen zhang ||| jiasheng xu ||| 
2018 ||| testing of non-toroidal shape primary pass-through current transformer for electrical machine monitoring and protection. ||| carlos a. platero ||| ricardo granizo ||| francisco bl ||| zquez ||| e. marchesi ||| 
2019 ||| impacts of linear controllers for power interfaces in ideal transformer model based power hardware-in-the-loop. ||| nathan d. marks ||| wang y. kong ||| daniel s. birt ||| 
2018 ||| model predictive control of packed u cells based transformerless single-phase dynamic voltage restorer. ||| mohamed trabelsi ||| hasan komurcugil ||| shady s. refaat ||| haitham abu-rub ||| 
2021 ||| vocabulary-constrained question generation with rare word masking and dual attention. ||| emil biju ||| 
2018 ||| a neural attention based approach for clickstream mining. ||| chandramohan t. n ||| balaraman ravindran ||| 
2020 ||| unitor @ dankmeme: combining convolutional models and transformer-based architectures for accurate meme management. ||| claudia breazzano ||| edoardo rubino ||| danilo croce ||| roberto basili ||| 
2020 ||| unitor @ sardistance2020: combining transformer-based architectures and transfer learning for robust stance detection. ||| simone giorgioni ||| marcello politi ||| samir salman ||| roberto basili ||| danilo croce ||| 
2020 ||| ssn nlp @ sardistance : stance detection from italian tweets using rnn and transformers (short paper). ||| kayalvizhi s ||| thenmozhi d ||| aravindan chandrabose ||| 
2018 ||| hate speech detection using attention-based lstm. ||| gretel liz de la pe ||| a sarrac ||| n ||| reynaldo gil pons ||| carlos enrique mu ||| iz-cuza ||| paolo rosso ||| 
2020 ||| fontana-unipi @ haspeede2: ensemble of transformers for the hate speech task at evalita (short paper). ||| michele fontana ||| giuseppe attardi ||| 
2018 ||| bidirectional attentional lstm for aspect based sentiment analysis on italian. ||| giancarlo nicola ||| 
2021 ||| anomaly detection in unstructured logs using attention-based bi-lstm network. ||| dongqing yu ||| xiaowei hou ||| ce li ||| qiujian lv ||| yan wang ||| ning li ||| 
2021 ||| a novel fabric defect detection network based on attention mechanism and multi-task fusion. ||| zhengrui peng ||| xinyi gong ||| zhenfeng lu ||| xiangyi xu ||| bengang wei ||| mukesh prasad ||| 
2021 ||| speech separation based on dptnet with sparse attention. ||| beom jun woo ||| hyung yong kim ||| jeunghun kim ||| nam soo kim ||| 
2021 ||| a safety-helmet detection algorithm based on attention mechanism. ||| haotian sun ||| ping gong ||| 
2021 ||| dsamt: dual-source aligned multimodal transformers for textcaps. ||| chenyang liao ||| ruifang liu ||| sheng gao ||| 
2021 ||| ancient chinese recognition method based on attention mechanism. ||| lingjing wu ||| chuang zhang ||| mengqiu xu ||| ming wu ||| 
2021 ||| combined coverage, attention and pointer networks for improving slot filling in spoken language understanding. ||| yaping wang ||| huiqin shao ||| zhen li ||| yan zhu ||| zhenyu liu ||| 
2021 ||| multi-scene safety helmet detection with multi-scale spatial attention feature. ||| xinbo ai ||| cheng chen ||| yingjian wang ||| yanjun guo ||| 
2021 ||| zero-shot voice cloning using variational embedding with attention mechanism. ||| jaeuk lee ||| jiye kim ||| joon-hyuk chang ||| 
2021 ||| attention-guided soft ranking loss for resource-constrained head pose estimation. ||| wenqi xu ||| tangzheng lian ||| wei liu ||| kaili zhao ||| 
2020 ||| current political news translation model based on attention mechanism. ||| xixi luo ||| jiaqi yan ||| xinyu chen ||| yingjiang wu ||| ke wu ||| meili lu ||| liang cai ||| 
2018 ||| visual attention mechanisms revisited. ||| cristina mendoza ||| pilar bachiller ||| antonio bandera ||| pablo bustos ||| 
2018 ||| attentional mechanism based on a microphone array for embedded devices and a single camera. ||| antonio martinez-colon ||| jos |||  manuel p ||| rez-lorenzo ||| fernando rivas ||| raquel viciana-abad ||| pedro reche l ||| pez ||| 
2020 ||| integrating openface 2.0 toolkit for driver attention estimation in challenging accidental scenarios. ||| javier araluce ||| luis miguel bergasa ||| carlos g ||| mez hu ||| lamo ||| rafael barea ||| elena l ||| pez guill ||| n ||| juan felipe arango ||| scar p ||| rez-gil ||| 
2020 ||| ftrans: energy-efficient acceleration of transformers using fpga. ||| bingbing li ||| santosh pandey ||| haowen fang ||| yanjun lyv ||| ji li ||| jieyang chen ||| mimi xie ||| lipeng wan ||| hang liu ||| caiwen ding ||| 
2019 ||| cnn-based camera-less user attention detection for smartphone power management. ||| daniele jahier pagliari ||| matteo ansaldi ||| enrico macii ||| massimo poncino ||| 
2017 ||| why the mapping process in ontology integration deserves attention. ||| samira babalou ||| alsayed algergawy ||| birger lantow ||| birgitta k ||| nig-ries ||| 
2017 ||| exploring alternative security warning dialog for attracting user attention: evaluation of "kawaii" effect and its additional stimulus combination. ||| ryo minakawa ||| tetsuji takada ||| 
2021 ||| analysis of graphsum's attention weights to improve the explainability of multi-document summarization. ||| m. lautaro hickmann ||| fabian wurzberger ||| megi hoxhalli ||| arne lochner ||| jessica t ||| llich ||| ansgar scherp ||| 
2019 ||| low supply voltage control oscillator with transformer feedback for photoplethysmography. ||| wen-cheng lai ||| sheng-lyang jang ||| yan-cu lin ||| 
2021 ||| transformer empowered csi feedback for massive mimo systems. ||| yang xu ||| mingqi yuan ||| man-on pun ||| 
2021 ||| activity recognition based on fr-cnn and attention-based lstm network. ||| tan-hsu tan ||| ching-jung huang ||| munkhjargal gochoo ||| yung-fu chen ||| 
2020 ||| requirements for monitoring inattention of the responsible human in an autonomous vehicle: the recall and precision tradeoff. ||| johnathan dimatteo ||| daniel m. berry ||| krzysztof czarnecki ||| 
2018 ||| inattention-management middleware for human-in-the-loop multi-display applications. ||| max nicosia ||| per ola kristensson ||| 
2017 ||| attention and engagement-awareness in the wild: a large-scale study with adaptive notifications. ||| tadashi okoshi ||| kota tsubouchi ||| masaya taji ||| takanori ichikawa ||| hideyuki tokuda ||| 
2021 ||| deep triplet networks with attention for sensor-based human activity recognition. ||| bulat khaertdinov ||| esam ghaleb ||| stylianos asteriadis ||| 
2021 ||| tap: a transformer based activity prediction exploiting temporal relations in collaborative tasks. ||| hyunju kim ||| dongman lee ||| 
2019 ||| outcome-driven clustering of acute coronary syndrome patients using multi-task neural network with attention. ||| eryu xia ||| xin du ||| jing mei ||| wen sun ||| suijun tong ||| zhiqing kang ||| jian sheng ||| jian li ||| changsheng ma ||| jianzeng dong ||| shaochun li ||| 
2021 ||| a joint self-attention model for aspect category detection in e-commerce reviews. ||| siyu wang ||| jiangtao qiu ||| chuanyang hong ||| 
2021 ||| catching audiences' attention through narrative sensory cues on digital distribution platforms. ||| mengyao fu ||| bingqing xiong ||| eric t. k. lim ||| chee-wee tan ||| weiquan wang ||| 
2018 ||| using virtual reality for museum exhibitions: the effects of attention and engagement for national palace museum. ||| hsin-lu chang ||| yung-chi shih ||| kai wang ||| rua-huan tsaih ||| zhiyan lin ||| 
2019 ||| artificial intelligence-powered cognitive training applications for children with attention deficit hyperactivity disorder: a brief review. ||| federica somma ||| angelo rega ||| onofrio gigliotta ||| 
2020 ||| relation aware attention model for uncertainty detection in text. ||| manjira sinha ||| nilesh agarwal ||| tirthankar dasgupta ||| 
2019 ||| episodic memory network with self-attention for emotion detection. ||| jiangping huang ||| zhong lin ||| xin liu ||| 
2020 ||| code2text: dual attention syntax annotation networks for structure-aware code translation. ||| yun xiong ||| shaofeng xu ||| keyao rong ||| xinyue liu ||| xiangnan kong ||| shanshan li ||| philip s. yu ||| yangyong zhu ||| 
2019 ||| dmmam: deep multi-source multi-task attention model for intensive care unit diagnosis. ||| zhenkun shi ||| wanli zuo ||| weitong chen ||| lin yue ||| yuwei hao ||| shining liang ||| 
2021 ||| dcan: deep co-attention network by modeling user preference and news lifecycle for news recommendation. ||| lingkang meng ||| chongyang shi ||| shufeng hao ||| xiangrui su ||| 
2020 ||| aspect category sentiment analysis with self-attention fusion networks. ||| zelin huang ||| hui zhao ||| feng peng ||| qinhui chen ||| gang zhao ||| 
2021 ||| neural adversarial review summarization with hierarchical personalized attention. ||| hongyan xu ||| hongtao liu ||| wenjun wang ||| pengfei jiao ||| 
2021 ||| sans: setwise attentional neural similarity method for few-shot recommendation. ||| zhenghao zhang ||| tun lu ||| dongsheng li ||| peng zhang ||| hansu gu ||| ning gu ||| 
2019 ||| agree: attention-based tour group recommendation with multi-modal data. ||| fang hu ||| xiuqi huang ||| xiaofeng gao ||| guihai chen ||| 
2019 ||| sparsemaac: sparse attention for multi-agent reinforcement learning. ||| wenhao li ||| bo jin ||| xiangfeng wang ||| 
2021 ||| an attention-based approach to rule learning in large knowledge graphs. ||| minghui li ||| kewen wang ||| zhe wang ||| hong wu ||| zhiyong feng ||| 
2019 ||| adaptive attention-aware gated recurrent unit for sequential recommendation. ||| anjing luo ||| pengpeng zhao ||| yanchi liu ||| jiajie xu ||| zhixu li ||| lei zhao ||| victor s. sheng ||| zhiming cui ||| 
2019 ||| neural review rating prediction with hierarchical attentions and latent factors. ||| xianchen wang ||| hongtao liu ||| peiyi wang ||| fangzhao wu ||| hongyan xu ||| wenjun wang ||| xing xie ||| 
2019 ||| combining meta-graph and attention for recommendation over heterogenous information network. ||| chenfei zhao ||| hengliang wang ||| yuan li ||| kedian mu ||| 
2018 ||| exploiting context graph attention for poi recommendation in location-based social networks. ||| siyuan zhang ||| hong cheng ||| 
2021 ||| relation-aware alignment attention network for multi-view multi-label learning. ||| yi zhang ||| jundong shen ||| cheng yu ||| chongjun wang ||| 
2019 ||| attention and convolution enhanced memory network for sequential recommendation. ||| jian liu ||| pengpeng zhao ||| yanchi liu ||| jiajie xu ||| junhua fang ||| lei zhao ||| victor s. sheng ||| zhiming cui ||| 
2021 ||| multi-scale gated inpainting network with patch-wise spacial attention. ||| xinrong hu ||| junjie jin ||| mingfu xiong ||| junping liu ||| tao peng ||| zili zhang ||| jia chen ||| ruhan he ||| xiao qin ||| 
2021 ||| dfilan: domain-based feature interactions learning via attention networks for ctr prediction. ||| yongliang han ||| yingyuan xiao ||| hongya wang ||| wenguang zheng ||| ke zhu ||| 
2019 ||| attention-based abnormal-aware fusion network for radiology report generation. ||| xiancheng xie ||| yun xiong ||| philip s. yu ||| kangan li ||| suhua zhang ||| yangyong zhu ||| 
2019 ||| attention-based neural tag recommendation. ||| jiahao yuan ||| yuanyuan jin ||| wenyan liu ||| xiaoling wang ||| 
2021 ||| graph attention networks for new product sales forecasting in e-commerce. ||| chuanyu xu ||| xiuchong wang ||| binbin hu ||| da zhou ||| yu dong ||| chengfu huo ||| weijun ren ||| 
2020 ||| detection of wrong disease information using knowledge-based embedding and attention. ||| wei ge ||| wei guo ||| lizhen cui ||| hui li ||| lijin liu ||| 
2021 ||| attention-based multimodal entity linking with high-quality images. ||| li zhang ||| zhixu li ||| qiang yang ||| 
2020 ||| modeling periodic pattern with self-attention network for sequential recommendation. ||| jun ma ||| pengpeng zhao ||| yanchi liu ||| victor s. sheng ||| jiajie xu ||| lei zhao ||| 
2020 ||| attention with long-term interval-based gated recurrent units for modeling sequential user behaviors. ||| zhao li ||| chenyi lei ||| pengcheng zou ||| donghui ding ||| shichang hu ||| zehong hu ||| shouling ji ||| jianliang gao ||| 
2021 ||| zh-ner: chinese named entity recognition with adversarial multi-task learning and self-attentions. ||| peng zhu ||| dawei cheng ||| fangzhou yang ||| yifeng luo ||| weining qian ||| aoying zhou ||| 
2018 ||| improving short text modeling by two-level attention networks for sentiment classification. ||| yulong li ||| yi cai ||| ho-fung leung ||| qing li ||| 
2021 ||| scsg attention: a self-centered star graph with attention for pedestrian trajectory prediction. ||| xu chen ||| shuncheng liu ||| zhi xu ||| yupeng diao ||| shaozhi wu ||| kai zheng ||| han su ||| 
2021 ||| graph attention collaborative similarity embedding for recommender system. ||| jinbo song ||| chao chang ||| fei sun ||| zhenyang chen ||| guoyong hu ||| peng jiang ||| 
2021 ||| entity resolution with hybrid attention-based networks. ||| chenchen sun ||| derong shen ||| 
2020 ||| hybrid attention based neural architecture for text semantics similarity measurement. ||| kaixin liu ||| yong zhang ||| chunxiao xing ||| 
2020 ||| hierarchical variational attention for sequential recommendation. ||| jing zhao ||| pengpeng zhao ||| yanchi liu ||| victor s. sheng ||| zhixu li ||| lei zhao ||| 
2020 ||| mutual self attention recommendation with gated fusion between ratings and reviews. ||| qiyao peng ||| hongtao liu ||| yang yu ||| hongyan xu ||| weidi dai ||| pengfei jiao ||| 
2019 ||| mdal: multi-task dual attention lstm model for semi-supervised network embedding. ||| longcan wu ||| daling wang ||| shi feng ||| yifei zhang ||| ge yu ||| 
2020 ||| sast-gnn: a self-attention based spatio-temporal graph neural network for traffic prediction. ||| yi xie ||| yun xiong ||| yangyong zhu ||| 
2021 ||| an attention-based bi-gru for route planning and order dispatch of bus-booking platform. ||| yucen gao ||| yuanning gao ||| yuhao li ||| xiaofeng gao ||| xiang li ||| guihai chen ||| 
2019 ||| hospitalization behavior prediction based on attention and time adjustment factors in bidirectional lstm. ||| lin cheng ||| yongjian ren ||| kun zhang ||| li pan ||| yuliang shi ||| 
2019 ||| modeling more globally: a hierarchical attention network via multi-task learning for aspect-based sentiment analysis. ||| xiangying ran ||| yuanyuan pan ||| wei sun ||| chongjun wang ||| 
2021 ||| multi-head attention with hint mechanisms for joint extraction of entity and relation. ||| chih-hsien fang ||| yi-ling chen ||| mi-yen yeh ||| yan-shuo lin ||| 
2021 ||| spatial-temporal attention network for temporal knowledge graph completion. ||| jiasheng zhang ||| shuang liang ||| zhiyi deng ||| jie shao ||| 
2020 ||| question answering over knowledge base with symmetric complementary attention. ||| yingjiao wu ||| xiaofeng he ||| 
2019 ||| align reviews with topics in attention network for rating prediction. ||| yile liang ||| tieyun qian ||| huilin yu ||| 
2020 ||| modeling long-term and short-term interests with parallel attentions for session-based recommendation. ||| jing zhu ||| yanan xu ||| yanmin zhu ||| 
2018 ||| core loss evaluation of high-frequency transformers in high-power dc-dc converters. ||| m. amin bahmani ||| 
2018 ||| transformerless dc/dc converter based on the autotransformer concept for the interconnection of hvdc grids. ||| joan sau-bassols ||| eduardo prieto-araujo ||| oriol gomis-bellmunt ||| roberto alves baraciarte ||| alireza nami ||| 
2020 ||| a medium frequency transformer design tool with methodologies adapted to various structures. ||| alexis fouineau ||| martin guillet ||| bruno lefebvre ||| marie-ange raulet ||| fabien sixdenier ||| 
2021 ||| medium-frequency transformers for fast rise time pwm voltages: modelling and design considerations. ||| andrea cremasco ||| mitrofan curti ||| siamak pourkeivannour ||| elena a. lomonova ||| 
2017 ||| individual trait oriented scanpath prediction for visual attention analysis. ||| aoqi li ||| zhenzhong chen ||| 
2021 ||| attention toward neighbors: a context aware framework for high resolution image segmentation. ||| fahim faisal niloy ||| m. ashraful amin ||| amin ahsan ali ||| akm mahbubur rahman ||| 
2019 ||| temporal regularized spatial attention for video-based person re-identification. ||| xueying wang ||| xu zhao ||| 
2021 ||| two-stream hybrid attention network for multimodal classification. ||| qipin chen ||| zhenyu shi ||| zhen zuo ||| jinmiao fu ||| yi sun ||| 
2020 ||| dcm: a dense-attention context module for semantic segmentation. ||| shenghua li ||| quan zhou ||| jia liu ||| jie wang ||| yawen fan ||| xiaofu wu ||| longin jan latecki ||| 
2018 ||| learning semantics-guided visual attention for few-shot image classification. ||| wen-hsuan chu ||| yu-chiang frank wang ||| 
2019 ||| cascade attention: multiple feature based learning for image captioning. ||| jiahe shi ||| yali li ||| shengjin wang ||| 
2018 ||| infrared and visible image registration using transformer adversarial network. ||| lan wang ||| chenqiang gao ||| yue zhao ||| tiecheng song ||| qi feng ||| 
2017 ||| cham: action recognition using convolutional hierarchical attention model. ||| shiyang yan ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2018 ||| action recognition: first-and second-order 3d feature in bi-directional attention network. ||| oh chul kwon ||| junyeong kim ||| chang d. yoo ||| 
2019 ||| cross attention network for semantic segmentation. ||| mengyu liu ||| hujun yin ||| 
2021 ||| single image super-resolution via global-context attention networks. ||| pengcheng bian ||| zhonglong zheng ||| dawei zhang ||| liyuan chen ||| minglu li ||| 
2020 ||| graph pattern loss based diversified attention network for cross-modal retrieval. ||| xueying chen ||| rong zhang ||| yibing zhan ||| 
2021 ||| spectral-spatial fused attention network for hyperspectral image classification. ||| ningyang li ||| zhaohui wang ||| 
2021 ||| sparse and structured visual attention. ||| pedro henrique martins ||| vlad niculae ||| zita marinho ||| andr |||  f. t. martins ||| 
2019 ||| frame attention networks for facial expression recognition in videos. ||| debin meng ||| xiaojiang peng ||| kai wang ||| yu qiao ||| 
2019 ||| feature-attentioned object detection in remote sensing imagery. ||| chengzheng li ||| chunyan xu ||| zhen cui ||| dan wang ||| tong zhang ||| jian yang ||| 
2021 ||| transformer for image quality assessment. ||| junyong you ||| jari korhonen ||| 
2021 ||| attention-based multi-task learning for fine-grained image classification. ||| dichao liu ||| yu wang ||| kenji mase ||| jien kato ||| 
2020 ||| automatic measurement of fetal cavum septum pellucidum from ultrasound images using deep attention network. ||| yuzhou wu ||| kuifang shen ||| zhigang chen ||| jia wu ||| 
2019 ||| a ranking based attention approach for visual tracking. ||| shenhui peng ||| sei-ichiro kamata ||| toby p. breckon ||| 
2021 ||| analysis of the novel transformer module combination for scene text recognition. ||| yeon-gyu kim ||| hyunsu kim ||| minseok kang ||| hyug jae lee ||| rokkyu lee ||| gunhan park ||| 
2021 ||| improving the robustness of convolutional neural networks via sketch attention. ||| tianshu chu ||| zuopeng yang ||| jie yang ||| xiaolin huang ||| 
2019 ||| spatial temporal attentional glimpse for human activity classification in video. ||| jiangtao kong ||| rongchao xu ||| junliang xing ||| kai li ||| wei ma ||| 
2020 ||| convolutional attention model for restaurant recommendation with multi-view visual features. ||| haihua luo ||| xiaoyan zhang ||| guibing guo ||| 
2019 ||| dual reverse attention networks for person re-identification. ||| shuangwei liu ||| lin qi ||| yunzhou zhang ||| weidong shi ||| 
2017 ||| multi-layer linear model for top-down modulation of visual attention in natural egocentric vision. ||| keng teck ma ||| liyuan li ||| peilun dai ||| joo-hwee lim ||| chengyao shen ||| qi zhao ||| 
2020 ||| attention boosted deep networks for video classification. ||| junyong you ||| jari korhonen ||| 
2019 ||| bira-net: bilinear attention net for diabetic retinopathy grading. ||| ziyuan zhao ||| kerui zhang ||| xuejie hao ||| jing tian ||| matthew chin heng chua ||| li chen ||| xin xu ||| 
2017 ||| an integrated approach to visual attention modelling using spatial-temporal saliency and objectness. ||| jean-baptiste weibel ||| hui li tan ||| shijian lu ||| 
2019 ||| cascade attention network for person re-identification. ||| haiyun guo ||| huiyao wu ||| chaoyang zhao ||| huichen zhang ||| jinqiao wang ||| hanqing lu ||| 
2020 ||| wavelet channel attention module with a fusion network for single image deraining. ||| hao-hsiang yang ||| chao-han huck yang ||| yu-chiang frank wang ||| 
2020 ||| attention-enhanced and more balanced r-cnn for object detection. ||| ruohong mei ||| haiying wang ||| aidong men ||| 
2019 ||| segmenting hepatic lesions using residual attention u-net with an adaptive weighted dice loss. ||| yu-cheng liu ||| daniel stanley tan ||| jyh-cheng chen ||| wen-huang cheng ||| kai-lung hua ||| 
2021 ||| light-field view synthesis using a convolutional block attention module. ||| muhammad shahzeb khan gul ||| m. umair mukati ||| michel b ||| tz ||| s ||| ren forchhammer ||| joachim keinert ||| 
2017 ||| person re-identification using visual attention. ||| alireza rahimpour ||| liu liu ||| ali taalimi ||| yang song ||| hairong qi ||| 
2021 ||| a heterogeneous face recognition via part adaptive and relation attention module. ||| rushuang xu ||| myeongah cho ||| sangyoun lee ||| 
2021 ||| mask guided attention for fine-grained patchy image classification. ||| jun wang ||| xiaohan yu ||| yongsheng gao ||| 
2021 ||| explore connection pattern and attention mechanism for lightweightimage super-resolution. ||| zhu qin ||| taiping zhang ||| 
2020 ||| gsanet: semantic segmentation with global and selective attention. ||| qingfeng liu ||| mostafa el-khamy ||| dongwoon bai ||| jungwon lee ||| 
2018 ||| image captioning with word level attention. ||| fang fang ||| hanli wang ||| pengjie tang ||| 
2021 ||| semantic role aware correlation transformer for text to video retrieval. ||| burak satar ||| hongyuan zhu ||| xavier bresson ||| joo hwee lim ||| 
2019 ||| learning hierarchical self-attention for video summarization. ||| yen-ting liu ||| yu-jhe li ||| fu-en yang ||| shang-fu chen ||| yu-chiang frank wang ||| 
2021 ||| semantic-compensated and attention-guided network for scene text detection. ||| yizhan zhao ||| sumei li ||| yueyang li ||| 
2018 ||| hierarchical relational attention for video question answering. ||| muhammad iqbal hasan chowdhury ||| kien nguyen ||| sridha sridharan ||| clinton fookes ||| 
2020 ||| infrared target detection using intensity saliency and self-attention. ||| ruiheng zhang ||| min xu ||| yaxin shi ||| jian fan ||| chengpo mu ||| lixin xu ||| 
2017 ||| visual comfort assessment of stereoscopic images using deep visual and disparity features based on human attention. ||| hyunwook jeong ||| hak gu kim ||| yong man ro ||| 
2021 ||| self attention based semantic segmentation on a natural disaster dataset. ||| tashnim chowdhury ||| maryam rahnemoonfar ||| 
2021 ||| attention based network for no-reference ugc video quality assessment. ||| fuwang yi ||| mianyi chen ||| wei sun ||| xiongkuo min ||| yuan tian ||| guangtao zhai ||| 
2019 ||| dressing for attention: outfit based fashion popularity prediction. ||| ling lo ||| chia-lin liu ||| rong-an lin ||| bo wu ||| hong-han shuai ||| wen-huang cheng ||| 
2021 ||| co-saliency detection via unified hierarchical graph neural network with geometric attention. ||| jiaqing qiao ||| shaowei sun ||| mingzhu xu ||| yongqiang li ||| bing liu ||| 
2018 ||| topic-guided attention for image captioning. ||| zhihao zhu ||| zhan xue ||| zejian yuan ||| 
2020 ||| feature aggregation attention network for single image dehazing. ||| lan yan ||| wenbo zheng ||| chao gou ||| fei-yue wang ||| 
2021 ||| rar-u-net: a residual encoder to attention decoder by residual connections framework for spine segmentation under noisy labels. ||| ziyang wang ||| zhengdong zhang ||| irina voiculescu ||| 
2021 ||| deep features fusion with mutual attention transformer for skin lesion diagnosis. ||| li zhou ||| yan luo ||| 
2020 ||| knowledge-guided and hyper-attention aware joint network for benign-malignant lung nodule classification. ||| weixin xu ||| kun wang ||| jingkai lin ||| yuting lu ||| sheng huang ||| xiaohong zhang ||| 
2021 ||| pan: personalized attention network for outfit recommendation. ||| huijing zhan ||| jie lin ||| 
2021 ||| inter-modality fusion based attention for zero-shot cross-modal retrieval. ||| bela chakraborty ||| peng wang ||| lei wang ||| 
2020 ||| deep regression forest with soft-attention for head pose estimation. ||| xiangtian ma ||| nan sang ||| xupeng  ||| wang ||| shihua xiao ||| 
2017 ||| deep partial person re-identification via attention model. ||| junyeong kim ||| chang d. yoo ||| 
2021 ||| learning regional attention over multi-resolution deep convolutional features for trademark retrieval. ||| osman tursun ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2021 ||| resolution-invariant person reid based on feature transformation and self-weighted attention. ||| ziyue zhang ||| shuai jiang ||| congzhentao huang ||| richard yi da xu ||| 
2019 ||| a convolutional neural network for pavement surface crack segmentation using residual connections and attention gating. ||| jacob k ||| nig ||| mark david jenkins ||| peter barrie ||| mike mannion ||| gordon morison ||| 
2020 ||| triple attention for robust video crowd counting. ||| qiyao wu ||| chongyang zhang ||| xiyu kong ||| muming zhao ||| yanjun chen ||| 
2020 ||| point set attention network for semantic segmentation. ||| jie jiang ||| jing liu ||| jun fu ||| xinxin zhu ||| hanqing lu ||| 
2017 ||| convolutional feature pyramid fusion via attention network. ||| sangryul jeon ||| seungryong kim ||| kwanghoon sohn ||| 
2018 ||| bottom-up attention guidance for recurrent image recognition. ||| hamed r. tavakoli ||| ali borji ||| rao muhammad anwer ||| esa rahtu ||| juho kannala ||| 
2020 ||| semi-supervised multi-spectral land cover classification with multi-attention and adaptive kernel. ||| kexin zhang ||| hua yang ||| 
2020 ||| attention unet++: a nested attention-aware u-net for liver ct image segmentation. ||| chen li ||| yusong tan ||| wei chen ||| xin luo ||| yuanming gao ||| xiaogang jia ||| zhiying wang ||| 
2021 ||| blind image deblurring based on dual attention network and 2d blur kernel estimation. ||| senmao tian ||| shunli zhang ||| beibei lin ||| 
2020 ||| fusion target attention mask generation network for video segmentation. ||| yunyi li ||| fangping chen ||| fan yang ||| yuan li ||| huizhu jia ||| xiaodong xie ||| 
2021 ||| robust monocular 3d lane detection with dual attention. ||| yujie jin ||| xiangxuan ren ||| fengxiang chen ||| weidong zhang ||| 
2020 ||| gapnet: generic-attribute-pose network for fine-grained visual categorization using multi-attribute attention module. ||| minjeong ju ||| hobin ryu ||| sangkeun moon ||| chang dong yoo ||| 
2020 ||| detection features as attention (defat): a keypoint-free approach to amur tiger re-identification. ||| xinhua cheng ||| jianing zhu ||| nan zhang ||| qian wang ||| qijun zhao ||| 
2021 ||| sagan: skip-attention gan for anomaly detection. ||| guoliang liu ||| shiyong lan ||| ting zhang ||| weikang huang ||| wenwu wang ||| 
2020 ||| context-aware hierarchical feature attention network for multi-scale object detection. ||| xuelong xu ||| xiangfeng luo ||| liyan ma ||| 
2021 ||| attention-based local region aggregation network for hierarchical point cloud learning. ||| gaojie chen ||| ran sun ||| jie ma ||| bingli wu ||| 
2021 ||| filter pruning via softmax attention. ||| sungmin cho ||| hyeseong kim ||| junseok kwon ||| 
2017 ||| person re-identification with coarse-to-fine visual attention. ||| zijie zhuang ||| haizhou ai ||| chong shang ||| lihu xiao ||| 
2021 ||| attention-driven tile splitting method for improved efficiency of omnidirectional versatile video coding. ||| j. carreira ||| s ||| rgio m. m. de faria ||| luis m. n. tavora ||| antonio navarro ||| pedro a. amado assun ||| o ||| 
2021 ||| joint co-attention and co-reconstruction representation learning for one-shot object detection. ||| jinghui chu ||| jiawei feng ||| peiguang jing ||| wei lu ||| 
2020 ||| self-attention dense depth estimation network for unrectified video sequences. ||| alwyn mathew ||| aditya prakash patra ||| jimson mathew ||| 
2021 ||| transformer and node-compressed dnn based dual-path system for manipulated face detection. ||| zhengbo luo ||| sei-ichiro kamata ||| zitang sun ||| 
2019 ||| local to global with multi-scale attention network for person re-identification. ||| lingchuan sun ||| jianlei liu ||| yingxin zhu ||| zhuqing jiang ||| 
2018 ||| temporal attention network for action proposal. ||| chenyang liu ||| xiangyu xu ||| yujin zhang ||| 
2019 ||| text recognition in images based on transformer with hierarchical attention. ||| yiwei zhu ||| shilin wang ||| zheng huang ||| kai chen ||| 
2020 ||| feature comparison based channel attention for fine-grained visual classification. ||| shukun jia ||| yan bai ||| jing zhang ||| 
2019 ||| weakly-supervised learning for attention-guided skull fracture classification in computed tomography imaging. ||| cheng-yen yang ||| chi-hsin lo ||| huan-chih wang ||| jen-hai chou ||| yu-chiang frank wang ||| 
2019 ||| learning target-oriented dual attention for robust rgb-t tracking. ||| rui yang ||| yabin zhu ||| xiao wang ||| chenglong li ||| jin tang ||| 
2021 ||| cascade attention blend residual network for single image super-resolution. ||| tianyu chen ||| guoqiang xiao ||| xiaoqin tang ||| xianfeng han ||| wenzhuo ma ||| xinye gou ||| 
2021 ||| attention-based partial face recognition. ||| stefan h ||| rmann ||| zeyuan zhang ||| martin knoche ||| torben teepe ||| gerhard rigoll ||| 
2021 ||| captioning transformer with scene graph guiding. ||| haishun chen ||| ying wang ||| xin yang ||| jie li ||| 
2020 ||| spatio-temporal slowfast self-attention network for action recognition. ||| myeongjun kim ||| taehun kim ||| daijin kim ||| 
2021 ||| two-pathway transformer network for video action recognition. ||| bo jiang ||| jiahong yu ||| lei zhou ||| kailin wu ||| yang yang ||| 
2019 ||| a dual-attention dilated residual network for liver lesion classification and localization on ct images. ||| xiao chen ||| jian wu ||| lanfen lin ||| dong liang ||| hongjie hu ||| qiaowei zhang ||| yutaro iwamoto ||| xian-hua han ||| yen-wei chen ||| ruofeng tong ||| 
2021 ||| correlation-aware attention branch network using multi-modal data for deterioration level estimation of infrastructures. ||| naoki ogawa ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2020 ||| multimodal attention-mechanism for temporal emotion recognition. ||| esam ghaleb ||| jan niehues ||| stylianos asteriadis ||| 
2021 ||| localization uncertainty-based attention for object detection. ||| sanghun park ||| kunhee kim ||| eunseop lee ||| daijin kim ||| 
2019 ||| spatial constraint multiple granularity attention network for clothesretrieval. ||| zhonghua luo ||| jiahui yuan ||| jie yang ||| wei wen ||| 
2017 ||| improving human action recognitionby temporal attention. ||| zhikang liu ||| ye tian ||| zilei wang ||| 
2020 ||| chroma intra prediction with attention-based cnn architectures. ||| marc g ||| rriz blanch ||| saverio g. blasi ||| alan f. smeaton ||| noel e. o'connor ||| marta mrak ||| 
2019 ||| efficient motion deblurring with feature transformation and spatial attention. ||| kuldeep purohit ||| a. n. rajagopalan ||| 
2018 ||| attention-enhanced sensorimotor object recognition. ||| spyridon thermos ||| georgios th. papadopoulos ||| petros daras ||| gerasimos potamianos ||| 
2021 ||| linked attention-based dynamic graph convolution module for point cloud classification. ||| xiaolong lu ||| baodi liu ||| weifeng liu ||| kai zhang ||| ye li ||| xiaoping lu ||| 
2020 ||| single image super-resolution via residual neuron attention networks. ||| wenjie ai ||| xiaoguang tu ||| shilei cheng ||| mei xie ||| 
2021 ||| vision and text transformer for predicting answerability on visual question answering. ||| tung le ||| huy tien nguyen ||| minh le nguyen ||| 
2021 ||| afdn: attention-based feedback dehazing network for uav remote sensing image haze removal. ||| shan wang ||| hanlin wu ||| libao zhang ||| 
2021 ||| hyperspectral classification using cooperative spatial-spectral attention network with tensor low-rank reconstruction. ||| sen li ||| xiaoyan luo ||| qixiong wang ||| lei li ||| weifa shen ||| jihao yin ||| 
2019 ||| compression artifact removal with stacked multi-context channel-wise attention network. ||| binglin li ||| jie liang ||| yang wang ||| 
2021 ||| deep gaussian denoiser epistemic uncertainty and decoupled dual-attention fusion. ||| xiaoqi ma ||| xiaoyu lin ||| majed el helou ||| sabine s ||| sstrunk ||| 
2020 ||| sea-net: squeeze-and-excitation attention net for diabetic retinopathy grading. ||| ziyuan zhao ||| kartik chopra ||| zeng zeng ||| xiaoli li ||| 
2021 ||| advanced deep network with attention and genetic-driven reinforcement learning layer for an efficient cancer treatment outcome prediction. ||| francesco rundo ||| giuseppe luigi banna ||| francesca trenta ||| sebastiano battiato ||| 
2019 ||| estimation of emotion labels via tensor-based spatiotemporal visual attention analysis. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| pocformer: a lightweight transformer architecture for detection of covid-19 using point of care ultrasound. ||| shehan perera ||| srikar adhikari ||| alper yilmaz ||| 
2020 ||| bae-net: a band attention aware ensemble network for hyperspectral object tracking. ||| zhuanfeng li ||| fengchao xiong ||| jun zhou ||| jing wang ||| jianfeng lu ||| yuntao qian ||| 
2019 ||| acnet: attention based network to exploit complementary features for rgbd semantic segmentation. ||| xinxin hu ||| kailun yang ||| lei fei ||| kaiwei wang ||| 
2021 ||| an attention fusion network for event-based vehicle object detection. ||| mengyun liu ||| na qi ||| yunhui shi ||| baocai yin ||| 
2021 ||| cyclic diffeomorphic transformer nets for contour alignment. ||| ilya kaufman ||| ron shapira weber ||| oren freifeld ||| 
2021 ||| sparse spatial attention network for semantic segmentation. ||| mengyu liu ||| hujun yin ||| 
2020 ||| jitter-robust video retargeting with kalman filter and attention saliency fusion network. ||| hyunwoo nam ||| dubok park ||| kangwon jeon ||| 
2021 ||| action segmentation on representations of skeleton sequences using transformer networks. ||| simon h ||| ring ||| raphael memmesheimer ||| dietrich paulus ||| 
2021 ||| attention-based self-supervised learning monocular depth estimation with edge refinement. ||| chenweinan jiang ||| haichun liu ||| lanzhen li ||| changchun pan ||| 
2021 ||| get to the point: content classification of animated graphics interchange formats with key-frame attention. ||| yongjuan ma ||| yu wang ||| pengfei zhu ||| junwen pan ||| hong shi ||| 
2021 ||| enhanced back projection network based stereo image super-resolution considering parallax attention. ||| li ma ||| sumei li ||| 
2020 ||| complex spatial-temporal attention aggregation for video person re-identification. ||| wenjie ding ||| xing wei ||| xiaopeng hong ||| yihong gong ||| 
2021 ||| self-supervised bodymap-to-appearance co-attention for partial person re-identification. ||| ci-siang lin ||| yu-chiang frank wang ||| 
2020 ||| fake video detection with certainty-based attention network. ||| dae hwi choi ||| hong joo lee ||| sangmin lee ||| jung uk kim ||| yong man ro ||| 
2021 ||| deepfake video detection using 3d-attentional inception convolutional neural network. ||| changlei lu ||| bin liu ||| wenbo zhou ||| qi chu ||| nenghai yu ||| 
2021 ||| attend, correct and focus: a bidirectional correct attention network for image-text matching. ||| yang liu ||| huaqiu wang ||| fanyang meng ||| mengyuan liu ||| hong liu ||| 
2017 ||| audio-visual attention: eye-tracking dataset and analysis toolbox. ||| pierre marighetto ||| antoine coutrot ||| nicolas riche ||| nathalie guyader ||| matei mancas ||| bernard gosselin ||| robert lagani ||| re ||| 
2020 ||| a convlstm-combined hierarchical attention network for saliency detection. ||| lei wang ||| liping shen ||| 
2019 ||| 3d deep attention network for survival prediction from magnetic resonance images in glioblastoma. ||| zijia liu ||| qiuchang sun ||| hongmin bai ||| chaofeng liang ||| yinsheng chen ||| zhi-cheng li ||| 
2021 ||| salypath: a deep-based architecture for visual attention prediction. ||| mohamed amine kerkouri ||| marouane tliba ||| aladine chetouani ||| rachid harba ||| 
2018 ||| dense chained attention network for scene text recognition. ||| yunze gao ||| yingying chen ||| jinqiao wang ||| ming tang ||| hanqing lu ||| 
2021 ||| plnl-3dssd: part-aware 3d single stage detector using local and non-local attention. ||| haizhuang liu ||| huimin ma ||| yanxian chen ||| xi li ||| tianyu hu ||| 
2019 ||| a coarse-to-fine framework for learned color enhancement with non-local attention. ||| chaowei shan ||| zhizheng zhang ||| zhibo chen ||| 
2019 ||| insect classification using squeeze-and-excitation and attention modules - a benchmark study. ||| yoon jin park ||| gervase tuxworth ||| jun zhou ||| 
2019 ||| optical flow estimation using spatial-channel combinational attention-based pyramid networks. ||| xuezhi xiang ||| mingliang zhai ||| rongfang zhang ||| ning lv ||| abdulmotaleb el-saddik ||| 
2019 ||| group re-identification with hybrid attention model and residual distance. ||| qiling xu ||| hua yang ||| lin chen ||| guangtao zhai ||| 
2018 ||| general recurrent attention model for jointly multiple object recognition and weakly supervised localization. ||| zijian zhao ||| xingming wu ||| peter c. y. chen ||| weihai chen ||| 
2020 ||| improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images. ||| sharif amit kamran ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| 
2020 ||| learning discriminative part features through attentions for effective and scalable person search. ||| jicheol park ||| boseung jeong ||| jongju shin ||| juyoung lee ||| suha kwak ||| 
2019 ||| attentional road safety networks. ||| sonu gupta ||| deepak srivatsav ||| a. venkata subramanyam ||| ponnurangam kumaraguru ||| 
2021 ||| zero-shot object detection with transformers. ||| ye zheng ||| li cui ||| 
2021 ||| cmdm-vac: improving a perceptual quality metric for 3d graphics by integrating a visual attention complexity measure. ||| yana nehm ||| mona abid ||| guillaume lavou ||| matthieu perreira da silva ||| patrick le callet ||| 
2019 ||| deeply supervised multimodal attentional translation embeddings for visual relationship detection. ||| nikolaos gkanatsios ||| vassilis pitsikalis ||| petros koutras ||| athanasia zlatintsi ||| petros maragos ||| 
2020 ||| video summarization with anchors and multi-head attention. ||| yi-lin sung ||| cheng-yao hong ||| yen-chi hsu ||| tyng-luh liu ||| 
2021 ||| temporal memory attention for video semantic segmentation. ||| hao wang ||| weining wang ||| jing liu ||| 
2019 ||| showcasing deeply supervised multimodal attentional translation embeddings: a demo for visual relationship detection. ||| nikolaos gkanatsios ||| vassilis pitsikalis ||| petros koutras ||| athanasia zlatintsi ||| petros maragos ||| 
2020 ||| few-shot learning with attention-weighted graph convolutional networks for hyperspectral image classification. ||| xinyi tong ||| jihao yin ||| bingnan han ||| hui qv ||| 
2020 ||| attention selective network for face synthesis and pose-invariant face recognition. ||| jiashu liao ||| alex c. kot ||| tanaya guha ||| victor sanchez ||| 
2019 ||| monad transformers and modular algebraic effects: what binds them together. ||| tom schrijvers ||| maciej pir ||| g ||| nicolas wu ||| mauro jaskelioff ||| 
2018 ||| effective attention modeling for aspect-level sentiment classification. ||| ruidan he ||| wee sun lee ||| hwee tou ng ||| daniel dahlmeier ||| 
2020 ||| porous lattice transformer encoder for chinese ner. ||| mengge xue ||| bowen yu ||| tingwen liu ||| yue zhang ||| erli meng ||| bin wang ||| 
2020 ||| interactively-propagative attention learning for implicit discourse relation recognition. ||| huibin ruan ||| yu hong ||| yang xu ||| zhen huang ||| guodong zhou ||| min zhang ||| 
2020 ||| mixup-transformer: dynamic data augmentation for nlp tasks. ||| lichao sun ||| congying xia ||| wenpeng yin ||| tingting liang ||| philip s. yu ||| lifang he ||| 
2020 ||| attention transfer network for aspect-level sentiment classification. ||| fei zhao ||| zhen wu ||| xinyu dai ||| 
2020 ||| bayes-enhanced lifelong attention networks for sentiment classification. ||| hao wang ||| shuai wang ||| sahisnu mazumder ||| bing liu ||| yan yang ||| tianrui li ||| 
2020 ||| dual-decoder transformer for joint automatic speech recognition and multilingual speech translation. ||| hang le ||| juan miguel pino ||| changhan wang ||| jiatao gu ||| didier schwab ||| laurent besacier ||| 
2020 ||| seeing both the forest and the trees: multi-head attention for joint classification on different compositional levels. ||| miruna pislar ||| marek rei ||| 
2020 ||| interactive key-value memory-augmented attention for image paragraph captioning. ||| chunpu xu ||| yu li ||| chengming li ||| xiang ao ||| min yang ||| jinwen tian ||| 
2020 ||| hierarchical bi-directional self-attention networks for paper review rating recommendation. ||| zhongfen deng ||| hao peng ||| congying xia ||| jianxin li ||| lifang he ||| philip s. yu ||| 
2020 ||| flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization. ||| travis r. goodwin ||| max e. savery ||| dina demner-fushman ||| 
2020 ||| graph enhanced dual attention network for document-level relation extraction. ||| bo li ||| wei ye ||| zhonghao sheng ||| rui xie ||| xiangyu xi ||| shikun zhang ||| 
2018 ||| dynamic feature selection with attention in incremental parsing. ||| ryosuke kohita ||| hiroshi noji ||| yuji matsumoto ||| 
2020 ||| attention word embedding. ||| shashank sonkar ||| andrew e. waters ||| richard g. baraniuk ||| 
2020 ||| language model transformers as evaluators for open-domain dialogues. ||| rostislav nedelchev ||| jens lehmann ||| ricardo usbeck ||| 
2020 ||| complaint identification in social media with transformer networks. ||| mali jin ||| nikolaos aletras ||| 
2020 ||| rethinking the value of transformer components. ||| wenxuan wang ||| zhaopeng tu ||| 
2020 ||| catching attention with automatic pull quote selection. ||| tanner a. bohn ||| charles x. ling ||| 
2020 ||| syntax-aware graph attention network for aspect-level sentiment classification. ||| lianzhe huang ||| xin sun ||| sujian li ||| linhao zhang ||| houfeng wang ||| 
2018 ||| lstms with attention for aggression detection. ||| nishant nikhil ||| ramit pahwa ||| mehul kumar nirala ||| rohan khilnani ||| 
2020 ||| how relevant are selectional preferences for transformer-based language models? ||| eleni metheniti ||| tim van de cruys ||| nabil hathout ||| 
2020 ||| forcereader: a bert-based interactive machine reading comprehension model with attention separation. ||| zheng chen ||| kangjian wu ||| 
2018 ||| one vs. many qa matching with both word-level and sentence-level attention network. ||| lu wang ||| shoushan li ||| changlong sun ||| luo si ||| xiaozhong liu ||| min zhang ||| guodong zhou ||| 
2020 ||| generating plausible counterfactual explanations for deep transformers in financial text classification. ||| linyi yang ||| eoin m. kenny ||| tin lok james ng ||| yi yang ||| barry smyth ||| ruihai dong ||| 
2020 ||| supervised visual attention for multimodal neural machine translation. ||| tetsuro nishihara ||| akihiro tamura ||| takashi ninomiya ||| yutaro omote ||| hideki nakayama ||| 
2020 ||| span-based joint entity and relation extraction with attention-based span-specific and contextual semantic representations. ||| bin ji ||| jie yu ||| shasha li ||| jun ma ||| qingbo wu ||| yusong tan ||| huijun liu ||| 
2018 ||| variational attention for sequence-to-sequence models. ||| hareesh bahuleyan ||| lili mou ||| olga vechtomova ||| pascal poupart ||| 
2018 ||| zero pronoun resolution with attention-based neural network. ||| qingyu yin ||| yu zhang ||| weinan zhang ||| ting liu ||| william yang wang ||| 
2020 ||| how far does bert look at: distance-based clustering and analysis of bert's attention. ||| yue guan ||| jingwen leng ||| chao li ||| quan chen ||| minyi guo ||| 
2018 ||| implicit discourse relation recognition using neural tensor network with interactive attention and sparse learning. ||| fengyu guo ||| ruifang he ||| di jin ||| jianwu dang ||| longbiao wang ||| xiangang li ||| 
2020 ||| dual attention network for cross-lingual entity alignment. ||| jian sun ||| yu zhou ||| chengqing zong ||| 
2020 ||| dual attention model for citation recommendation. ||| yang zhang ||| qiang ma ||| 
2020 ||| improving long-tail relation extraction with collaborating relation-augmented attention. ||| yang li ||| tao shen ||| guodong long ||| jing jiang ||| tianyi zhou ||| chengqi zhang ||| 
2020 ||| optimizing transformer for low-resource neural machine translation. ||| ali araabi ||| christof monz ||| 
2018 ||| a position-aware bidirectional attention network for aspect-level sentiment analysis. ||| shuqin gu ||| lipeng zhang ||| yuexian hou ||| yin song ||| 
2020 ||| joint chinese word segmentation and part-of-speech tagging via multi-channel attention of character n-grams. ||| yuanhe tian ||| yan song ||| fei xia ||| 
2020 ||| multi-task learning of spoken language understanding by integrating n-best hypotheses with hierarchical attention. ||| mingda li ||| xinyue liu ||| weitong ruan ||| luca soldaini ||| wael hamza ||| chengwei su ||| 
2020 ||| debunking rumors on twitter with tree transformer. ||| jing ma ||| wei gao ||| 
2020 ||| hierarchical chinese legal event extraction via pedal attention mechanism. ||| shirong shen ||| guilin qi ||| zhen li ||| sheng bi ||| lusheng wang ||| 
2020 ||| learning to decouple relations: few-shot relation classification with entity-guided attention and confusion-aware training. ||| yingyao wang ||| junwei bao ||| guangyi liu ||| youzheng wu ||| xiaodong he ||| bowen zhou ||| tiejun zhao ||| 
2018 ||| interaction-aware topic model for microblog conversations through network embedding and user attention. ||| ruifang he ||| xuefei zhang ||| di jin ||| longbiao wang ||| jianwu dang ||| xiangang li ||| 
2020 ||| a contextual alignment enhanced cross graph attention network for cross-lingual entity alignment. ||| zhiwen xie ||| runjie zhu ||| kunsong zhao ||| jin liu ||| guangyou zhou ||| jimmy xiangji huang ||| 
2018 ||| multilingual neural machine translation with task-specific attention. ||| graeme w. blackwood ||| miguel ballesteros ||| todd ward ||| 
2020 ||| increasing learning efficiency of self-attention networks through direct position interactions, learnable temperature, and convoluted attention. ||| philipp dufter ||| martin schmitt ||| hinrich sch ||| tze ||| 
2018 ||| a comparison of transformer and recurrent neural networks on multilingual neural machine translation. ||| surafel melaku lakew ||| mauro cettolo ||| marcello federico ||| 
2018 ||| a lexicon-based supervised attention model for neural sentiment analysis. ||| yicheng zou ||| tao gui ||| qi zhang ||| xuanjing huang ||| 
2018 ||| incorporating argument-level interactions for persuasion comments evaluation using co-attention model. ||| lu ji ||| zhongyu wei ||| xiangkun hu ||| yang liu ||| qi zhang ||| xuanjing huang ||| 
2020 ||| hitrans: a transformer-based context- and speaker-sensitive model for emotion detection in conversations. ||| jingye li ||| donghong ji ||| fei li ||| meishan zhang ||| yijiang liu ||| 
2018 ||| hybrid attention based multimodal network for spoken language classification. ||| yue gu ||| kangning yang ||| shiyu fu ||| shuhong chen ||| xinyu li ||| ivan marsic ||| 
2020 ||| autoregressive reasoning over chains of facts with transformers. ||| ruben cartuyvels ||| graham spinks ||| marie-francine moens ||| 
2020 ||| better sign language translation with stmc-transformer. ||| kayo yin ||| jesse read ||| 
2020 ||| improving sentiment analysis over non-english tweets using multilingual transformers and automatic translation for data-augmentation. ||| valentin barri ||| re ||| alexandra balahur ||| 
2020 ||| interpretable multi-headed attention for abstractive summarization at controllable lengths. ||| ritesh sarkhel ||| moniba keymanesh ||| arnab nandi ||| srinivasan parthasarathy ||| 
2020 ||| aprile: attention with pseudo residual connection for knowledge graph embedding. ||| yuzhang liu ||| peng wang ||| yingtai li ||| yizhan shao ||| zhongkai xu ||| 
2020 ||| knowledge aware emotion recognition in textual conversations via multi-task incremental transformer. ||| duzhen zhang ||| xiuyi chen ||| shuang xu ||| bo xu ||| 
2018 ||| visual question answering dataset for bilingual image understanding: a study of cross-lingual transfer using attention maps. ||| nobuyuki shimizu ||| na rong ||| takashi miyazaki ||| 
2018 ||| who is killed by police: introducing supervised attention for hierarchical lstms. ||| minh nguyen ||| thien huu nguyen ||| 
2018 ||| neural machine translation with decoding history enhanced attention. ||| mingxuan wang ||| jun xie ||| zhixing tan ||| jinsong su ||| deyi xiong ||| chao bian ||| 
2020 ||| evaluating pretrained transformer-based models on the task of fine-grained named entity recognition. ||| cedric lothritz ||| kevin allix ||| lisa veiber ||| tegawend |||  f. bissyand ||| jacques klein ||| 
2020 ||| incorporating noisy length constraints into transformer with length-aware positional encodings. ||| yui oka ||| katsuki chousa ||| katsuhito sudoh ||| satoshi nakamura ||| 
2020 ||| scale down transformer by grouping features for a lightweight character-level language model. ||| sungrae park ||| geewook kim ||| junyeop lee ||| junbum cha ||| ji-hoon kim ||| hwalsuk lee ||| 
2018 ||| a multi-attention based neural network with external knowledge for story ending predicting task. ||| qian li ||| ziwei li ||| jin-mao wei ||| yanhui gu ||| adam jatowt ||| zhenglu yang ||| 
2020 ||| the devil is in the details: evaluating limitations of transformer-based methods for granular tasks. ||| brihi joshi ||| neil shah ||| francesco barbieri ||| leonardo neves ||| 
2020 ||| joint transformer/rnn architecture for gesture typing in indic languages. ||| emil biju ||| anirudh sriram ||| mitesh m. khapra ||| pratyush kumar ||| 
2020 ||| transquest: translation quality estimation with cross-lingual transformers. ||| tharindu ranasinghe ||| constantin orasan ||| ruslan mitkov ||| 
2020 ||| conan: a complementary neighboring-based attention network for referring expression generation. ||| jungjun kim ||| hanbin ko ||| jialin wu ||| 
2020 ||| context-aware cross-attention for non-autoregressive translation. ||| liang ding ||| longyue wang ||| di wu ||| dacheng tao ||| zhaopeng tu ||| 
2018 ||| stance detection with hierarchical attention network. ||| qingying sun ||| zhongqing wang ||| qiaoming zhu ||| guodong zhou ||| 
2018 ||| identification of internal faults in indirect symmetrical phase shift transformers using ensemble learning. ||| pallav kumar bera ||| rajesh kumar ||| can isik ||| 
2018 ||| sam-gcnn: a gated convolutional neural network with segment-level attention mechanism for home activity monitoring. ||| yu-han shen ||| ke-xin he ||| wei-qiang zhang ||| 
2018 ||| using attention to process rf for cognitive radio. ||| james graham ||| ashwin fisher ||| 
2020 ||| a joint detection-classification model for weakly supervised sound event detection using multi-scale attention method. ||| yaoguang wang ||| liang he ||| 
2021 ||| improving stateful premise selection with transformers. ||| krsto prorokovic ||| michael wand ||| j ||| rgen schmidhuber ||| 
2021 |||  attention: a self-attention based neural network for remaining useful lifetime predictions. ||| yuanjun liu ||| xingang wang ||| 
2018 ||| a reading comprehension style question answering model based on attention mechanism. ||| linlong xiao ||| nanzhi wang ||| guocai yang ||| 
2019 ||| reconstructing attention with dynamic regularization. ||| teng jiang ||| chengjun zhang ||| yupu yang ||| 
2021 ||| face shows your intention: visual search based on full-face gaze estimation with channel-spatial attention. ||| song liu ||| xiang-dong zhou ||| xingyu jiang ||| haiyng wu ||| yu shi ||| 
2021 ||| divided caption model with global attention. ||| yamin cheng ||| hancong duan ||| zitian zhao ||| zhi wang ||| 
2021 ||| optical flow estimation with foreground attention guided network. ||| dongdong hou ||| gan sun ||| 
2021 ||| soft-gated self-supervision network for action reasoning: soft-gated self-supervision network with attention mechanism and joint multi-task training strategy for action reasoning. ||| shengli wang ||| lifang wang ||| liang gu ||| shichang he ||| huijuan hao ||| meiling yao ||| 
2021 ||| deep multi-scale recursive residual attention network for spectral super resolution. ||| shaolei zhang ||| guangyuan fu ||| hongqiao wang ||| yuqing zhao ||| 
2020 ||| an object detection algorithm based on attention mechanism and lightweight network (amln). ||| xuemei yuan ||| hanming huang ||| zhengfeng jiang ||| simin xue ||| 
2021 ||| parallel attention with weighted efficient network for video-based person re-identification. ||| junting yang ||| zuliu yang ||| jing zhou ||| yong zhao ||| qifei dai ||| fuchi li ||| 
2021 ||| chinese description of videos incorporating multimodal features and attention mechanism. ||| hu liu ||| junxiu wu ||| jiabin yuan ||| 
2021 ||| deep recommendation model based on local attention and gru. ||| jinghua zhu ||| huafeng hou ||| heran xi ||| 
2020 ||| attention guided multi-scale regression for scene text detection. ||| zhiwei zheng ||| 
2017 ||| evaluating bad and good eeg segments based on extracted features: towards an automated understanding of infant behavior and attention. ||| mhd saeed sharif ||| mohammed hayyan alsibai ||| elena kushnerenko ||| 
2017 ||| a systematic review: attention assessment of virtual reality based intervention for learning in children with autism spectrum disorder. ||| bilikis banire ||| dena al-thani ||| marwa k. qaraqe ||| bilal mansoor ||| 
2018 ||| multisensory virtual game with use of the device leap motion to improve the lack of attention in children of 7-12 years with adhd. ||| david chilca ||| n capelo ||| milton escobar s ||| nchez ||| jhonatan salazar hurtado ||| daniela benalc ||| zar chicaiza ||| 
2021 ||| games, attention and brain signals. ||| diana janeth lancheros cuesta ||| jordy jacob puentes beltr ||| n ||| andr ||| s felipe bol ||| var naranjo ||| peter marin ||| 
2021 ||| exploring transformer-based language recognition using phonotactic information. ||| david romero ||| luis fernando d'haro ||| christian salamea ||| 
2018 ||| end-to-end speech translation with the transformer. ||| laura cross vila ||| carlos escolano ||| jos |||  a. r. fonollosa ||| marta r. costa-juss ||| 
2018 ||| self-attention linguistic-acoustic decoder. ||| santiago pascual ||| antonio bonafonte ||| joan serr ||| 
2017 ||| recognizing entailments in legal texts using sentence encoding-based and decomposable attention models. ||| nguyen truong son ||| viet-anh phan ||| le minh nguyen ||| 
2019 ||| combining similarity and transformer methods for case law entailment. ||| juliano rabelo ||| mi-young kim ||| randy goebel ||| 
2021 ||| using transformers to improve answer retrieval for legal questions. ||| andrew vold ||| jack g. conrad ||| 
2019 ||| neural attention learning for legal query reformulation. ||| arunprasath shankar ||| venkata nagaraju buddarapu ||| 
2017 ||| legal question answering system using neural attention. ||| ayaka morimoto ||| daiki kubo ||| motoki sato ||| hiroyuki shindo ||| yuji matsumoto ||| 
2020 ||| alphanet: an attention guided deep network for automatic image matting. ||| rishab sharma ||| rahul deora ||| anirudha vishvakarma ||| 
2021 ||| a microcontroller is all you need: enabling transformer execution on low-power iot endnodes. ||| alessio burrello ||| moritz scherer ||| marcello zanghieri ||| francesco conti ||| luca benini ||| 
2021 ||| attention-based reinforcement learning for real-time uav semantic communication. ||| won joon yun ||| byungju lim ||| soyi jung ||| young-chai ko ||| jihong park ||| joongheon kim ||| mehdi bennis ||| 
2019 ||| customized hidden layered ann based pattern recognition technique for differential protection of power transformer. ||| harish balaga ||| deepthi marrapu ||| 
2020 ||| guiding symbolic natural language grammar induction via transformer-based sequence probabilities. ||| ben goertzel ||| andr ||| s su ||| rez-madrigal ||| gino yu ||| 
2020 ||| an attentional control mechanism for reasoning and learning. ||| peter isaev ||| patrick hammer ||| 
2017 ||| development of microtransformers using mcm and electronic packaging technologies. ||| m. m. rocha ||| antonio c. c. telles ||| ricardo cotrin teixeira ||| 
2017 ||| measuring readiness-to-hand through differences in attention to the task vs. attention to the tool. ||| ayman alzayat ||| mark hancock ||| miguel a. nacenta ||| 
2020 ||| towards computational identification of visual attention on interactive tabletops. ||| alberta ansah ||| caitlin mills ||| orit shaer ||| andrew l. kun ||| 
2017 ||| lyrics-based music genre classification using a hierarchical attention network. ||| alexandros tsaptsinos ||| 
2017 ||| automatic drum transcription for polyphonic recordings using soft attention mechanisms and convolutional neural networks. ||| carl southall ||| ryan stables ||| jason hockman ||| 
2019 ||| learning soft-attention models for tempo-invariant audio-sheet music retrieval. ||| stefan balke ||| matthias dorfer ||| luis carvalho ||| andreas arzt ||| gerhard widmer ||| 
2019 ||| harmony transformer: incorporating chord segmentation into harmony recognition. ||| tsung-ping chen ||| li su ||| 
2020 ||| the jazz transformer on the front line: exploring the shortcomings of ai-composed music through quantitative measures. ||| shih-lun wu ||| yi-hsuan yang ||| 
2019 ||| a bi-directional transformer for musical chord recognition. ||| jonggwon park ||| kyoyun choi ||| sungwook jeon ||| dokyun kim ||| jonghun park ||| 
2019 ||| an attention mechanism for musical instrument recognition. ||| siddharth gururani ||| mohit sharma ||| alexander lerch ||| 
2021 ||| sequence-to-sequence piano transcription with transformers. ||| curtis hawthorne ||| ian simon ||| rigel swavely ||| ethan manilow ||| jesse h. engel ||| 
2021 ||| semi-supervised music tagging transformer. ||| minz won ||| keunwoo choi ||| xavier serra ||| 
2020 ||| automatic composition of guitar tabs by transformers and groove modeling. ||| yu-hua chen ||| yu-siang huang ||| wen-yi hsiao ||| yi-hsuan yang ||| 
2021 ||| spectnt: a time-frequency transformer for music audio. ||| wei tsung lu ||| ju-chiang wang ||| minz won ||| keunwoo choi ||| xuchen song ||| 
2020 ||| sentiment analysis with contextual embeddings and self-attention. ||| katarzyna biesialska ||| magdalena biesialska ||| henryk rybinski ||| 
2020 ||| attention to emotions: detecting mental disorders in social media. ||| mario ezra arag ||| n ||| adri ||| n pastor l ||| pez-monroy ||| luis carlos gonz ||| lez-gurrola ||| manuel montes-y-g ||| mez ||| 
2021 ||| transformer-based automatic punctuation prediction and word casing reconstruction of the asr output. ||| jan svec ||| jan lehecka ||| lubos sm ||| dl ||| pavel ircing ||| 
2021 ||| attention-based end-to-end named entity recognition from speech. ||| dejan porjazovski ||| juho leinonen ||| mikko kurimo ||| 
2021 ||| lstm-xl: attention enhanced long-term memory for lstm cells. ||| tam ||| s gr ||| sz ||| mikko kurimo ||| 
2018 ||| application of amplified reality to the cognitive effect of children with attention deficit hyperactivity disorder(adhd) - an example of italian chicco-app interactive building blocks. ||| pei-hua wang ||| taoi hsu ||| 
2019 ||| adjustment of the power factor and the impedance by the phase of the flux in a transformer circuit. ||| aquila h. lee ||| hijung chai ||| won don lee ||| 
2021 ||| ask2transformers: zero-shot domain labelling with pretrained language models. ||| oscar sainz ||| german rigau ||| 
2021 ||| a review of methods to detect divided attention impairments in alzheimer's disease. ||| c. d. angekumbura ||| t. h. t. dilshani ||| k. t. d. perera ||| s. n. jayarathna ||| k. a. d. c. p. kahandawarachchi ||| s. w. i. udara ||| 
2020 ||| toward semantic iot load inference attention management for facilitating healthcare and public health collaboration: a survey. ||| sachiko lim ||| rahim rahmani ||| 
2018 ||| 3d vae-attention network: a parallel system for single-view 3d reconstruction. ||| fei hu ||| xinyan yang ||| wei zhong ||| long ye ||| qin zhang ||| 
2019 ||| gaze attention and flow visualization using the smudge effect. ||| sangbong yoo ||| seongmin jeong ||| seokyeon kim ||| yun jang ||| 
2020 ||| book rating model based on self-attention and lstm. ||| xiaotong zhao ||| 
2020 ||| where attention goes, energy flows: enhancing individual sustainability in software engineering. ||| birgit penzenstadler ||| 
2019 ||| sequence-level knowledge distillation for model compression of attention-based sequence-to-sequence speech recognition. ||| raden mu'az mun'im ||| nakamasa inoue ||| koichi shinoda ||| 
2021 ||| transformer in action: a comparative study of transformer-based acoustic models for large scale speech recognition applications. ||| yongqiang wang ||| yangyang shi ||| frank zhang ||| chunyang wu ||| julian chan ||| ching-feng yeh ||| alex xiao ||| 
2019 ||| triggered attention for end-to-end speech recognition. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2021 ||| frequency-temporal attention network for singing melody extraction. ||| shuai yu ||| xiaoheng sun ||| yi yu ||| wei li ||| 
2019 ||| attention-augmented end-to-end multi-task learning for emotion prediction from speech. ||| zixing zhang ||| bingwen wu ||| bj ||| rn w. schuller ||| 
2021 ||| attention-guided second-order pooling convolutional networks. ||| shannan chen ||| qiule sun ||| cunhua li ||| jianxin zhang ||| qiang zhang ||| 
2021 ||| topic-aware dialogue generation with two-hop based graph attention. ||| shijie zhou ||| wenge rong ||| jianfei zhang ||| yanmeng wang ||| libin shi ||| zhang xiong ||| 
2020 ||| deep monocular video depth estimation using temporal attention. ||| haoyu ren ||| mostafa el-khamy ||| jungwon lee ||| 
2020 ||| lightweight and efficient end-to-end speech recognition using low-rank transformer. ||| genta indra winata ||| samuel cahyawijaya ||| zhaojiang lin ||| zihan liu ||| pascale fung ||| 
2019 ||| windowed attention mechanisms for speech recognition. ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2021 ||| tstnn: two-stage transformer based neural network for speech enhancement in the time domain. ||| kai wang ||| bengbeng he ||| wei-ping zhu ||| 
2020 ||| improving end-to-end speech synthesis with local recurrent neural network enhanced transformer. ||| yibin zheng ||| xinhui li ||| fenglong xie ||| li lu ||| 
2020 ||| a regularized attention mechanism for graph attention networks. ||| uday shankar shanthamallu ||| jayaraman j. thiagarajan ||| andreas spanias ||| 
2021 ||| domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition. ||| yuan gao ||| jiaxing liu ||| longbiao wang ||| jianwu dang ||| 
2020 ||| multi-resolution multi-head attention in deep speaker embedding. ||| zhiming wang ||| kaisheng yao ||| xiaolong li ||| shuo fang ||| 
2021 ||| transformer language models with lstm-based cross-utterance information representation. ||| guangzhi sun ||| chao zhang ||| philip c. woodland ||| 
2021 ||| top-down attention in end-to-end spoken language understanding. ||| yixin chen ||| weiyi lu ||| alejandro mottini ||| li erran li ||| jasha droppo ||| zheng du ||| belinda zeng ||| 
2021 ||| riemannian geometry-based decoding of the directional focus of auditory attention using eeg. ||| simon geirnaert ||| tom francart ||| alexander bertrand ||| 
2021 ||| hierarchical refined attention for scene text recognition. ||| min zhang ||| meng ma ||| ping wang ||| 
2020 ||| self-attention and retrieval enhanced neural networks for essay generation. ||| wei wang ||| hai-tao zheng ||| zibo lin ||| 
2019 ||| end-to-end audio visual scene-aware dialog using multimodal attention-based video features. ||| chiori hori ||| huda alamri ||| jue wang ||| gordon wichern ||| takaaki hori ||| anoop cherian ||| tim k. marks ||| vincent cartillier ||| raphael gontijo lopes ||| abhishek das ||| irfan essa ||| dhruv batra ||| devi parikh ||| 
2020 ||| deblurring and super-resolution using deep gated fusion attention networks for face images. ||| chao-hsun yang ||| long-wen chang ||| 
2019 ||| single-channel speech extraction using speaker inventory and attention network. ||| xiong xiao ||| zhuo chen ||| takuya yoshioka ||| hakan erdogan ||| changliang liu ||| dimitrios dimitriadis ||| jasha droppo ||| yifan gong ||| 
2021 ||| attention-embedded decomposed network with unpaired ct images prior for metal artifact reduction. ||| binyu zhao ||| qianqian ren ||| jinbao li ||| yafeng zhao ||| 
2020 ||| cross-view attention network for breast cancer screening from multi-view mammograms. ||| xuran zhao ||| luyang yu ||| xun wang ||| 
2019 ||| discriminative saliency-pose-attention covariance for action recognition. ||| jianhai zhang ||| zhiyong feng ||| yong su ||| meng xing ||| 
2021 ||| multimodal cross- and self-attention network for speech emotion recognition. ||| licai sun ||| bin liu ||| jianhua tao ||| zheng lian ||| 
2021 ||| adaptable multi-domain language model for transformer asr. ||| taewoo lee ||| min-joong lee ||| tae gyoon kang ||| seokyeoung jung ||| minseok kwon ||| yeona hong ||| jungin lee ||| kyoung-gu woo ||| ho-gyeong kim ||| jiseung jeong ||| jihyun lee ||| hosik lee ||| young sang choi ||| 
2018 ||| speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. ||| linhao dong ||| shuang xu ||| bo xu ||| 
2020 ||| parsing map guided multi-scale attention network for face hallucination. ||| chenyang wang ||| zhiwei zhong ||| junjun jiang ||| deming zhai ||| xianming liu ||| 
2021 ||| cascade attention fusion for fine-grained image captioning based on multi-layer lstm. ||| shuang wang ||| yun meng ||| yu gu ||| lei zhang ||| xiutiao ye ||| jingxian tian ||| licheng jiao ||| 
2020 ||| multi-head attention for speech emotion recognition with auxiliary learning of gender recognition. ||| anish nediyanchath ||| periyasamy paramasivam ||| promod yenigalla ||| 
2020 ||| attention-guided deraining network via stage-wise learning. ||| kui jiang ||| zhongyuan wang ||| peng yi ||| chen chen ||| yuhong yang ||| xin tian ||| junjun jiang ||| 
2020 ||| deep exposure fusion with deghosting via homography estimation and attention learning. ||| sheng-yeh chen ||| yung-yu chuang ||| 
2018 ||| minimum word error rate training for attention-based sequence-to-sequence models. ||| rohit prabhavalkar ||| tara n. sainath ||| yonghui wu ||| patrick nguyen ||| zhifeng chen ||| chung-cheng chiu ||| anjuli kannan ||| 
2021 ||| joint alignment learning-attention based model for grapheme-to-phoneme conversion. ||| yonghe wang ||| feilong bao ||| hui zhang ||| guanglai gao ||| 
2020 ||| improving the performance of transformer based low resource speech recognition for indian languages. ||| vishwas m. shetty ||| metilda sagaya mary n. j ||| srinivasan umesh ||| 
2021 ||| attention-based multi-encoder automatic pronunciation assessment. ||| binghuai lin ||| liyuan wang ||| 
2020 ||| key action and joint ctc-attention based sign language recognition. ||| haibo li ||| liqing gao ||| ruize han ||| liang wan ||| wei feng ||| 
2020 ||| improving auditory attention decoding performance of linear and non-linear methods using state-space model. ||| ali aroudi ||| tobias de taillez ||| simon doclo ||| 
2020 ||| speech enhancement using self-adaptation and multi-head self-attention. ||| yuma koizumi ||| kohei yatabe ||| marc delcroix ||| yoshiki masuyama ||| daiki takeuchi ||| 
2020 ||| fcem: a novel fast correlation extract model for real time steganalysis of voip stream via multi-head attention. ||| hao yang ||| zhongliang yang ||| yongjian bao ||| sheng liu ||| yongfeng huang ||| 
2020 ||| h-vectors: utterance-level speaker embedding using a hierarchical attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2021 ||| streaming simultaneous speech translation with augmented memory transformer. ||| xutai ma ||| yongqiang wang ||| mohammad javad dousti ||| philipp koehn ||| juan miguel pino ||| 
2020 ||| facial emotion recognition using light field images with deep attention-based bidirectional lstm. ||| alireza sepas-moghaddam ||| s. ali etemad ||| fernando pereira ||| paulo lobato correia ||| 
2020 ||| attention-based curiosity-driven exploration in deep reinforcement learning. ||| patrik reizinger ||| m ||| rton szemenyei ||| 
2020 ||| t-gsa: transformer with gaussian-weighted self-attention for speech enhancement. ||| jaeyoung kim ||| mostafa el-khamy ||| jungwon lee ||| 
2020 ||| an attention-based joint acoustic and text on-device end-to-end model. ||| tara n. sainath ||| ruoming pang ||| ron j. weiss ||| yanzhang he ||| chung-cheng chiu ||| trevor strohman ||| 
2021 ||| pre-training transformer decoder for end-to-end asr model with unpaired text data. ||| changfeng gao ||| gaofeng cheng ||| runyan yang ||| han zhu ||| pengyuan zhang ||| yonghong yan ||| 
2020 ||| structured sparse attention for end-to-end automatic speech recognition. ||| jiabin xue ||| tieran zheng ||| jiqing han ||| 
2019 ||| models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. ||| william n. havard ||| jean-pierre chevrot ||| laurent besacier ||| 
2018 ||| efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. ||| hideyuki tachibana ||| katsuya uenoyama ||| shunsuke aihara ||| 
2020 ||| improved end-to-end spoken utterance classification with a self-attention acoustic classifier. ||| ryan price ||| mahnoosh mehrabani ||| srinivas bangalore ||| 
2021 ||| hybrid model for network anomaly detection with gradient boosting decision trees and tabtransformer. ||| xinyue xu ||| xiaolu zheng ||| 
2020 ||| bio-mimetic attentional feedback in music source separation. ||| ashwin bellur ||| mounya elhilali ||| 
2020 ||| residual attention network for wavelet domain super-resolution. ||| jing liu ||| yuan xie ||| haichuan song ||| wang yuan ||| lizhuang ma ||| 
2020 ||| interpretable self-attention temporal reasoning for driving behavior understanding. ||| yi-chieh liu ||| yung-an hsieh ||| min-hung chen ||| chao-han huck yang ||| jesper tegn ||| r ||| yi-chang james tsai ||| 
2020 ||| channel-attention dense u-net for multichannel speech enhancement. ||| bahareh tolooshams ||| ritwik giri ||| andrew h. song ||| umut isik ||| arvindh krishnaswamy ||| 
2020 ||| weakly-supervised sound event detection with self-attention. ||| koichi miyazaki ||| tatsuya komatsu ||| tomoki hayashi ||| shinji watanabe ||| tomoki toda ||| kazuya takeda ||| 
2019 ||| cognitive-driven binaural lcmv beamformer using eeg-based auditory attention decoding. ||| ali aroudi ||| simon doclo ||| 
2021 ||| vset: a multimodal transformer for visual speech enhancement. ||| karthik ramesh ||| chao xing ||| wupeng wang ||| dong wang ||| xiao chen ||| 
2020 ||| frequency and temporal convolutional attention for text-independent speaker recognition. ||| sarthak yadav ||| atul rai ||| 
2019 ||| dilated residual network with multi-head self-attention for speech emotion recognition. ||| runnan li ||| zhiyong wu ||| jia jia ||| sheng zhao ||| helen meng ||| 
2019 ||| non-local self-attention structure for function approximation in deep reinforcement learning. ||| zhixiang wang ||| xi xiao ||| guangwu hu ||| yao yao ||| dianyan zhang ||| zhendong peng ||| qing li ||| shutao xia ||| 
2021 ||| sub-band grouping spectral feature-attention block for hyperspectral image classification. ||| weilian zhou ||| sei-ichiro kamata ||| zhengbo luo ||| 
2019 ||| replay attack detection using magnitude and phase information with attention-based adaptive filters. ||| meng liu ||| longbiao wang ||| jianwu dang ||| seiichi nakagawa ||| haotian guan ||| xiangang li ||| 
2021 ||| mutually-constrained monotonic multihead attention for online asr. ||| jaeyun song ||| hajin shim ||| eunho yang ||| 
2019 ||| context modelling using hierarchical attention networks for sentiment and self-assessed emotion detection in spoken narratives. ||| lukas stappen ||| nicholas cummins ||| eva-maria me ||| ner ||| harald baumeister ||| judith dineley ||| bj ||| rn w. schuller ||| 
2021 ||| meta-learning with attention for improved few-shot learning. ||| zejiang hou ||| anwar walid ||| sun-yuan kung ||| 
2020 ||| voice conversion with transformer network. ||| ruolan liu ||| xiao chen ||| xue wen ||| 
2019 ||| automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism. ||| ryo nishikimi ||| eita nakamura ||| satoru fukayama ||| masataka goto ||| kazuyoshi yoshii ||| 
2019 ||| attention-based graph convolutional network for recommendation system. ||| chenyuan feng ||| zuozhu liu ||| shaowei lin ||| tony q. s. quek ||| 
2020 ||| deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition. ||| swapnil bhosale ||| rupayan chakraborty ||| sunil kumar kopparapu ||| 
2020 ||| arnet: attention-based refinement network for few-shot semantic segmentation. ||| rusheng li ||| hanhui liu ||| yuesheng zhu ||| zhiqiang bai ||| 
2021 ||| attentionlite: towards efficient self-attention models for vision. ||| souvik kundu ||| sairam sundaresan ||| 
2020 ||| efficient scene text detection with textual attention tower. ||| liang zhang ||| yufei liu ||| hang xiao ||| lu yang ||| guangming zhu ||| syed afaq ali shah ||| mohammed bennamoun ||| peiyi shen ||| 
2018 ||| eeg-based auditory attention decoding using steerable binaural superdirective beamformer. ||| ali aroudi ||| daniel marquardt ||| simon doclo ||| 
2021 ||| speech emotion recognition with multiscale area attention and data augmentation. ||| mingke xu ||| fan zhang ||| xiaodong cui ||| wei zhang ||| 
2021 ||| catiloc: camera image transformer for indoor localization. ||| ali ghofrani ||| rahil mahdian toroghi ||| seyed mojtaba tabatabaie ||| 
2021 ||| attention on attention sparse dense convolutional network for financial signal processing. ||| tianlei zhu ||| jiawei li ||| xinji liu ||| yong jiang ||| shu-tao xia ||| 
2017 ||| joint ctc-attention based end-to-end speech recognition using multi-task learning. ||| suyoun kim ||| takaaki hori ||| shinji watanabe ||| 
2018 ||| advancing connectionist temporal classification with attention modeling. ||| amit das ||| jinyu li ||| rui zhao ||| yifan gong ||| 
2019 ||| video quality assessment for encrypted http adaptive streaming: attention-based hybrid rnn-hmm model. ||| shuang tang ||| xiaowei qin ||| xiaohui chen ||| guo wei ||| 
2021 ||| improving audio anomalies recognition using temporal convolutional attention networks. ||| qiang huang ||| thomas hain ||| 
2020 ||| attention mechanism enhanced kernel prediction networks for denoising of burst images. ||| bin zhang ||| shenyao jin ||| yili xia ||| yongming huang ||| zixiang xiong ||| 
2019 ||| seq2seq attentional siamese neural networks for text-dependent speaker verification. ||| yichi zhang ||| meng yu ||| na li ||| chengzhu yu ||| jia cui ||| dong yu ||| 
2018 ||| a time-restricted self-attention layer for asr. ||| daniel povey ||| hossein hadian ||| pegah ghahremani ||| ke li ||| sanjeev khudanpur ||| 
2021 ||| synergic feature attention for image restoration. ||| chong mou ||| jian zhang ||| 
2020 ||| full-reference speech quality estimation with attentional siamese neural networks. ||| gabriel mittag ||| sebastian m ||| ller ||| 
2019 ||| visual relationship recognition via language and position guided attention. ||| hao zhou ||| chuanping hu ||| chongyang zhang ||| shengyang shen ||| 
2021 ||| monaural speech enhancement with complex convolutional block attention module and joint time frequency losses. ||| shengkui zhao ||| trung hieu nguyen ||| bin ma ||| 
2020 ||| emet: embeddings from multilingual-encoder transformer for fake news detection. ||| stephane schwarz ||| ant ||| nio the ||| philo ||| anderson rocha ||| 
2021 ||| towards immediate backchannel generation using attention-based early prediction model. ||| amalia istiqlali adiba ||| takeshi homma ||| toshinori miyoshi ||| 
2019 ||| a sequential guiding network with attention for image captioning. ||| daouda sow ||| zengchang qin ||| mouhamed niasse ||| tao wan ||| 
2020 ||| bba-net: a bi-branch attention network for crowd counting. ||| yi hou ||| chengyang li ||| fan yang ||| cong ma ||| liping zhu ||| yuan li ||| huizhu jia ||| xiaodong xie ||| 
2020 ||| stock movement prediction that integrates heterogeneous data sources using dilated causal convolution networks with attention. ||| divyanshu daiya ||| min-sheng wu ||| che lin ||| 
2021 ||| atvio: attention guided visual-inertial odometry. ||| li liu ||| ge li ||| thomas h. li ||| 
2020 ||| distilling attention weights for ctc-based asr systems. ||| takafumi moriya ||| hiroshi sato ||| tomohiro tanaka ||| takanori ashihara ||| ryo masumura ||| yusuke shinohara ||| 
2021 ||| jointly trained transformers models for spoken language translation. ||| hari krishna vydana ||| martin karafi ||| t ||| katerina zmol ||| kov ||| luk ||| s burget ||| honza cernock ||| 
2021 ||| blind deinterleaving of signals in time series with self-attention based soft min-cost flow learning. ||| ogul can ||| yeti ziya g ||| rb ||| z ||| berkin yildirim ||| a. aydin alatan ||| 
2021 ||| image super-resolution using multi-resolution attention network. ||| anqi liu ||| sumei li ||| yongli chang ||| 
2021 ||| co-attentional transformers for story-based video understanding. ||| bj ||| rn bebensee ||| byoung-tak zhang ||| 
2021 ||| multi-task transformer with input feature reconstruction for dysarthric speech recognition. ||| chaoyue ding ||| shiliang sun ||| jing zhao ||| 
2018 ||| attention-based lstm for psychological stress detection from spoken language using distant supervision. ||| genta indra winata ||| onno pepijn kampman ||| pascale fung ||| 
2018 ||| attention-based dialog state tracking for conversational interview coaching. ||| ming-hsiang su ||| chung-hsien wu ||| kun-yi huang ||| chu-kwang chen ||| 
2021 ||| real image super-resolution using token based contextual attention. ||| zhihong pan ||| baopu li ||| 
2020 ||| synchronous transformers for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| ye bai ||| jianhua tao ||| shuai zhang ||| zhengqi wen ||| 
2020 ||| end-to-end multi-speaker speech recognition with transformer. ||| xuankai chang ||| wangyou zhang ||| yanmin qian ||| jonathan le roux ||| shinji watanabe ||| 
2021 ||| transformer-based end-to-end speech recognition with local dense synthesizer attention. ||| menglong xu ||| shengqiang li ||| xiao-lei zhang ||| 
2021 ||| an attention model for hypernasality prediction in children with cleft palate. ||| vikram c. mathad ||| nancy scherer ||| kathy chapman ||| julie liss ||| visar berisha ||| 
2019 ||| noise-tolerant audio-visual online person verification using an attention-based neural network fusion. ||| suwon shon ||| tae-hyun oh ||| james r. glass ||| 
2019 ||| knowledge distillation using output errors for self-attention end-to-end models. ||| ho-gyeong kim ||| hwidong na ||| hoshik lee ||| jihyun lee ||| tae gyoon kang ||| min-joong lee ||| young sang choi ||| 
2021 ||| a further study of unsupervised pretraining for transformer based speech recognition. ||| dongwei jiang ||| wubo li ||| ruixiong zhang ||| miao cao ||| ne luo ||| yang han ||| wei zou ||| kun han ||| xiangang li ||| 
2019 ||| attention in recurrent neural networks for ransomware detection. ||| rakshit agrawal ||| jack w. stokes ||| karthik selvaraj ||| mady marinescu ||| 
2020 ||| weakly labelled audio tagging via convolutional networks with spatial and channel-wise attention. ||| sixin hong ||| yuexian zou ||| wenwu wang ||| meng cao ||| 
2020 ||| trilingual semantic embeddings of visually grounded speech with self-attention mechanisms. ||| yasunori ohishi ||| akisato kimura ||| takahito kawanishi ||| kunio kashino ||| david harwath ||| james r. glass ||| 
2019 ||| an attention-aware bidirectional multi-residual recurrent neural network (abmrnn): a study about better short-term text classification. ||| ye wang ||| han wang ||| xinxiang zhang ||| theodora chaspari ||| yoonsuck choe ||| mi lu ||| 
2020 ||| adrn: attention-based deep residual network for hyperspectral image denoising. ||| yongsen zhao ||| deming zhai ||| junjun jiang ||| xianming liu ||| 
2020 ||| spatial attentional bilinear 3d convolutional network for video-based autism spectrum disorder detection. ||| kangbo sun ||| lin li ||| lianqiang li ||| ningyu he ||| jie zhu ||| 
2020 ||| audio-attention discriminative language model for asr rescoring. ||| ankur gandhe ||| ariya rastrow ||| 
2021 ||| multi-dialect speech recognition in english using attention on ensemble of experts. ||| amit das ||| kshitiz kumar ||| jian wu ||| 
2021 ||| attention is all you need in speech separation. ||| cem subakan ||| mirco ravanelli ||| samuele cornell ||| mirko bronzi ||| jianyuan zhong ||| 
2017 ||| dynamic tracking attention model for action recognition. ||| chien-yao wang ||| chin-chin chiang ||| jian-jiun ding ||| jia-ching wang ||| 
2021 ||| bayesian transformer language models for speech recognition. ||| boyang xue ||| jianwei yu ||| junhao xu ||| shansong liu ||| shoukang hu ||| zi ye ||| mengzhe geng ||| xunying liu ||| helen meng ||| 
2019 ||| modality attention for end-to-end audio-visual speech recognition. ||| pan zhou ||| wenwen yang ||| wei chen ||| yanfeng wang ||| jia jia ||| 
2020 ||| deja-vu: double feature presentation and iterated loss in deep transformer networks. ||| andros tjandra ||| chunxi liu ||| frank zhang ||| xiaohui zhang ||| yongqiang wang ||| gabriel synnaeve ||| satoshi nakamura ||| geoffrey zweig ||| 
2020 ||| transformer-based text-to-speech with weighted forced attention. ||| takuma okamoto ||| tomoki toda ||| yoshinori shiga ||| hisashi kawai ||| 
2020 ||| hierarchical attention transfer networks for depression assessment from speech. ||| ziping zhao ||| zhongtian bao ||| zixing zhang ||| nicholas cummins ||| haishuai wang ||| bj ||| rn w. schuller ||| 
2020 ||| speaker-aware training of attention-based end-to-end speech recognition using neural speaker embeddings. ||| aku rouhe ||| tuomas kaseva ||| mikko kurimo ||| 
2019 ||| improving facial attractiveness prediction via co-attention learning. ||| shengjie shi ||| fei gao ||| xuantong meng ||| xingxin xu ||| jingjie zhu ||| 
2021 ||| low-dimensional denoising embedding transformer for ecg classification. ||| jian guan ||| wenbo wang ||| pengming feng ||| xinxin wang ||| wenwu wang ||| 
2020 ||| deep audio-visual speech separation with attention mechanism. ||| chenda li ||| yanmin qian ||| 
2021 ||| head-synchronous decoding for transformer-based streaming asr. ||| mohan li ||| catalin zorila ||| rama doddipatla ||| 
2020 ||| fixed-point optimization of transformer neural network. ||| yoonho boo ||| wonyong sung ||| 
2020 ||| compare learning: bi-attention network for few-shot learning. ||| li ke ||| meng pan ||| weigao wen ||| dong li ||| 
2020 ||| preference-aware mask for session-based recommendation with bidirectional transformer. ||| yuanxing zhang ||| pengyu zhao ||| yushuo guan ||| lin chen ||| kaigui bian ||| lingyang song ||| bin cui ||| xiaoming li ||| 
2021 ||| hierarchical transformer-based large-context end-to-end asr with large-context knowledge distillation. ||| ryo masumura ||| naoki makishima ||| mana ihori ||| akihiko takashima ||| tomohiro tanaka ||| shota orihashi ||| 
2019 ||| ad-net: attention guided network for optical flow estimation using dilated convolution. ||| mingliang zhai ||| xuezhi xiang ||| rongfang zhang ||| ning lv ||| abdulmotaleb el-saddik ||| 
2021 ||| image-assisted transformer in zero-resource multi-modal translation. ||| ping huang ||| shiliang sun ||| hao yang ||| 
2020 ||| video question generation via semantic rich cross-modal self-attention networks learning. ||| yu-siang wang ||| hung-ting su ||| chen-hsi chang ||| zhe yu liu ||| winston h. hsu ||| 
2020 ||| weakly supervised crowd-wise attention for robust crowd counting. ||| xiyu kong ||| muming zhao ||| hao zhou ||| chongyang zhang ||| 
2019 ||| detecting attention shift from neural response based on beat-frequency-modulated musical excerpts. ||| takashi g. sato ||| yoshifumi shiraki ||| takehiro moriya ||| 
2020 ||| iq-stan: image quality guided spatio-temporal attention network for license plate recognition. ||| cong zhang ||| qi wang ||| xuelong li ||| 
2021 ||| channel attention residual u-net for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yangtao hu ||| wenle wang ||| wei zhou ||| yugen yi ||| 
2019 ||| an interaction-aware attention network for speech emotion recognition in spoken dialogs. ||| sung-lin yeh ||| yun-shao lin ||| chi-chun lee ||| 
2021 ||| a co-interactive transformer for joint slot filling and intent detection. ||| libo qin ||| tailu liu ||| wanxiang che ||| bingbing kang ||| sendong zhao ||| ting liu ||| 
2021 ||| self-attention generative adversarial network for speech enhancement. ||| huy phan ||| huy le nguyen ||| oliver y. ch ||| n ||| philipp koch ||| ngoc q. k. duong ||| ian mcloughlin ||| alfred mertins ||| 
2019 ||| self-attention based model for punctuation prediction using word and speech embeddings. ||| jiangyan yi ||| jianhua tao ||| 
2019 ||| deep recurrent neural networks with layer-wise multi-head attentions for punctuation restoration. ||| seokhwan kim ||| 
2019 ||| self-attention networks for connectionist temporal classification in speech recognition. ||| julian salazar ||| katrin kirchhoff ||| zhiheng huang ||| 
2020 ||| hka: a hierarchical knowledge attention mechanism for multi-turn dialogue system. ||| jian song ||| kailai zhang ||| xuesi zhou ||| ji wu ||| 
2021 ||| gaussian kernelized self-attention for long sequence data and its application to ctc-based speech recognition. ||| yosuke kashiwagi ||| emiru tsunoo ||| shinji watanabe ||| 
2019 ||| utterance-level end-to-end language identification using attention-based cnn-blstm. ||| weicheng cai ||| danwei cai ||| shen huang ||| ming li ||| 
2020 ||| attention-mask dense merger (attendense) deep hdr for ghost removal. ||| kareem metwaly ||| vishal monga ||| 
2019 ||| a region based attention method for weakly supervised sound event detection and classification. ||| jie yan ||| yan song ||| wu guo ||| li-rong dai ||| ian mcloughlin ||| liang chen ||| 
2019 ||| multi-step self-attention network for cross-modal retrieval based on a limited text space. ||| zheng yu ||| wenmin wang ||| ge li ||| 
2020 ||| design-gan: cross-category fashion translation driven by landmark attention. ||| yining lang ||| yuan he ||| jianfeng dong ||| fan yang ||| hui xue ||| 
2021 ||| memory layers with multi-head attention mechanisms for text-dependent speaker verification. ||| victoria mingote ||| antonio miguel ||| alfonso ortega gim ||| nez ||| eduardo lleida ||| 
2021 ||| wake word detection with streaming transformers. ||| yiming wang ||| hang lv ||| daniel povey ||| lei xie ||| sanjeev khudanpur ||| 
2020 ||| channel attention based generative network for robust visual tracking. ||| ying hu ||| hanyu xuan ||| jian yang ||| yan yan ||| 
2021 ||| developing real-time streaming transformer transducer for speech recognition on large-scale dataset. ||| xie chen ||| yu wu ||| zhenghao wang ||| shujie liu ||| jinyu li ||| 
2021 ||| hcag: a hierarchical context-aware graph attention model for depression detection. ||| meng niu ||| kai chen ||| qingcai chen ||| lufeng yang ||| 
2019 ||| an attention-based neural network approach for single channel speech enhancement. ||| xiang hao ||| changhao shan ||| yong xu ||| sining sun ||| lei xie ||| 
2020 ||| unsupervised speaker adaptation using attention-based speaker memory for end-to-end asr. ||| leda sari ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2019 ||| end-to-end language recognition using attention based hierarchical gated recurrent unit models. ||| bharat padi ||| anand mohan ||| sriram ganapathy ||| 
2021 ||| history utterance embedding transformer lm for speech recognition. ||| keqi deng ||| gaofeng cheng ||| haoran miao ||| pengyuan zhang ||| yonghong yan ||| 
2020 ||| high-resolution attention network with acoustic segment model for acoustic scene classification. ||| xue bai ||| jun du ||| jia pan ||| hengshun zhou ||| yanhui tu ||| chin-hui lee ||| 
2021 ||| transformer-transducers for code-switched speech recognition. ||| siddharth dalmia ||| yuzong liu ||| srikanth ronanki ||| katrin kirchhoff ||| 
2019 ||| spatial and channel attention based convolutional neural networks for modeling noisy speech. ||| sirui xu ||| eric fosler-lussier ||| 
2021 ||| end-to-end spoken language understanding using transformer networks and self-supervised pre-trained features. ||| edmilson da silva morais ||| hong-kwang jeff kuo ||| samuel thomas ||| zolt ||| n t ||| ske ||| brian kingsbury ||| 
2019 ||| the speechtransformer for large-scale mandarin chinese speech recognition. ||| yuanyuan zhao ||| jie li ||| xiaorui wang ||| yan li ||| 
2018 ||| incorporating asr errors with attention-based, jointly trained rnn for intent detection and slot filling. ||| raphael schumann ||| pongtep angkititrakul ||| 
2020 ||| transformer-based online ctc/attention end-to-end speech recognition architecture. ||| haoran miao ||| gaofeng cheng ||| changfeng gao ||| pengyuan zhang ||| yonghong yan ||| 
2019 ||| exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed. ||| atta norouzian ||| bogdan mazoure ||| dermot connolly ||| daniel willett ||| 
2019 ||| self-attention aligner: a latency-control end-to-end model for asr using self-attention network and chunk-hopping. ||| linhao dong ||| feng wang ||| bo xu ||| 
2021 ||| dense attention module for accurate pulmonary nodule detection. ||| jiannan liu ||| jie li ||| fanyong xue ||| chentao wu ||| 
2021 ||| transformer based unsupervised pre-training for acoustic representation learning. ||| ruixiong zhang ||| haiwei wu ||| wubo li ||| dongwei jiang ||| wei zou ||| xiangang li ||| 
2019 ||| decoupling category-wise independence and relevance with self-attention for multi-label image classification. ||| luchen liu ||| sheng guo ||| weilin huang ||| matthew r. scott ||| 
2021 ||| graph attention and interaction network with multi-task learning for fact verification. ||| rui yang ||| runze wang ||| zhen-hua ling ||| 
2019 ||| attention-based transfer learning for brain-computer interface. ||| chuanqi tan ||| fuchun sun ||| tao kong ||| bin fang ||| wenchang zhang ||| 
2021 ||| efficient speech emotion recognition using multi-scale cnn and attention. ||| zixuan peng ||| yu lu ||| shengfeng pan ||| yunfeng liu ||| 
2021 ||| hierarchical attention fusion for geo-localization. ||| liqi yan ||| yiming cui ||| yingjie victor chen ||| dongfang liu ||| 
2021 ||| attention enhanced spatial temporal neural network for hrrp recognition. ||| yuchen chu ||| zunhua guo ||| 
2021 ||| multitask learning and joint optimization for transformer-rnn-transducer speech recognition. ||| jae-jin jeon ||| eesung kim ||| 
2020 ||| spatial attention for far-field speech recognition with deep beamforming neural networks. ||| weipeng he ||| lu lu ||| biqiao zhang ||| jay mahadeokar ||| kaustubh kalgaonkar ||| christian fuegen ||| 
2021 ||| tabular transformers for modeling multivariate time series. ||| inkit padhi ||| yair schiff ||| igor melnyk ||| mattia rigotti ||| youssef mroueh ||| pierre l. dognin ||| jerret ross ||| ravi nair ||| erik altman ||| 
2021 ||| a multi-channel temporal attention convolutional neural network model for environmental sound classification. ||| you wang ||| chuyao feng ||| david v. anderson ||| 
2019 ||| speech emotion recognition using multi-hop attention mechanism. ||| seunghyun yoon ||| seokhyun byun ||| subhadeep dey ||| kyomin jung ||| 
2019 ||| learning to match transient sound events using attentional similarity for few-shot sound recognition. ||| szu-yu chou ||| kai-hsiang cheng ||| jyh-shing roger jang ||| yi-hsuan yang ||| 
2019 ||| adversarial examples for improving end-to-end attention-based small-footprint keyword spotting. ||| xiong wang ||| sining sun ||| changhao shan ||| jingyong hou ||| lei xie ||| shen li ||| xin lei ||| 
2020 ||| strategic attention learning for modality translation. ||| jonathan martinez ||| ali akbari ||| kaan sel ||| roozbeh jafari ||| 
2021 ||| double multi-head attention for speaker verification. ||| miquel india ||| pooyan safari ||| javier hernando ||| 
2020 ||| non-local nested residual attention network for stereo image super-resolution. ||| wangduo xie ||| jian zhang ||| zhisheng lu ||| meng cao ||| yong zhao ||| 
2019 ||| phonemic-level duration control using attention alignment for natural speech synthesis. ||| jungbae park ||| kijong han ||| yuneui jeong ||| sang wan lee ||| 
2021 ||| end-to-end multi-channel transformer for speech recognition. ||| feng-ju chang ||| martin radfar ||| athanasios mouchtaris ||| brian king ||| siegfried kunzmann ||| 
2020 ||| transformer-based acoustic modeling for hybrid speech recognition. ||| yongqiang wang ||| abdelrahman mohamed ||| duc le ||| chunxi liu ||| alex xiao ||| jay mahadeokar ||| hongzhao huang ||| andros tjandra ||| xiaohui zhang ||| frank zhang ||| christian fuegen ||| geoffrey zweig ||| michael l. seltzer ||| 
2021 ||| neuro-steered music source separation with eeg-based auditory attention decoding and contrastive-nmf. ||| giorgia cantisani ||| slim essid ||| ga ||| l richard ||| 
2020 ||| focusing on attention: prosody transfer and adaptative optimization strategy for multi-speaker end-to-end speech synthesis. ||| ruibo fu ||| jianhua tao ||| zhengqi wen ||| jiangyan yi ||| tao wang ||| 
2021 ||| graphspeech: syntax-aware graph attention network for neural speech synthesis. ||| rui liu ||| berrak sisman ||| haizhou li ||| 
2019 ||| investigation of enhanced tacotron text-to-speech synthesis systems with self-attention for pitch accent language. ||| yusuke yasuda ||| xin wang ||| shinji takaki ||| junichi yamagishi ||| 
2021 ||| an end-to-end speech accent recognition method based on hybrid ctc/attention transformer asr. ||| qiang gao ||| haiwei wu ||| yanqing sun ||| yitao duan ||| 
2020 ||| attention-based gated scaling adaptive acoustic model for ctc-based speech recognition. ||| fenglin ding ||| wu guo ||| lirong dai ||| jun du ||| 
2020 ||| all in one network for driver attention monitoring. ||| dawei yang ||| xinlei li ||| xiaotian dai ||| rui zhang ||| lizhe qi ||| wenqiang zhang ||| zhe jiang ||| 
2020 ||| attention-based asr with lightweight and dynamic convolutions. ||| yuya fujita ||| aswin shanmugam subramanian ||| motoi omachi ||| shinji watanabe ||| 
2021 ||| sa-net: shuffle attention for deep convolutional neural networks. ||| qing-long zhang ||| yu-bin yang ||| 
2021 ||| unidirectional memory-self-attention transducer for online speech recognition. ||| jian luo ||| jianzong wang ||| ning cheng ||| jing xiao ||| 
2020 ||| selective attention encoders by syntactic graph convolutional networks for document summarization. ||| haiyang xu ||| yun wang ||| kun han ||| baochang ma ||| junwen chen ||| xiangang li ||| 
2020 ||| redundant convolutional network with attention mechanism for monaural speech enhancement. ||| tian lan ||| yilan lyu ||| guoqiang hui ||| refuoe mokhosi ||| sen li ||| qiao liu ||| 
2020 ||| automotive radar signal interference mitigation using rnn with self attention. ||| jiwoo mun ||| seok hyeon ha ||| jungwoo lee ||| 
2021 ||| patnet : a phoneme-level autoregressive transformer network for speech synthesis. ||| shiming wang ||| zhenhua ling ||| ruibo fu ||| jiangyan yi ||| jianhua tao ||| 
2020 ||| a hybrid text normalization system using multi-head self-attention for mandarin. ||| junhui zhang ||| junjie pan ||| xiang yin ||| chen li ||| shichao liu ||| yang zhang ||| yuxuan wang ||| zejun ma ||| 
2021 ||| reinforcement stacked learning with semantic-associated attention for visual question answering. ||| xinyu xiao ||| chunxia zhang ||| shiming xiang ||| chunhong pan ||| 
2020 ||| streaming automatic speech recognition with the transformer model. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2021 ||| multi-rate attention architecture for fast streamable text-to-speech spectrum modeling. ||| qing he ||| zhiping xiu ||| thilo k ||| hler ||| jilong wu ||| 
2020 ||| generating synthetic audio data for attention-based speech recognition systems. ||| nick rossenbach ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2019 ||| a bayesian attention neural network layer for speaker recognition. ||| weizhong zhu ||| jason w. pelecanos ||| 
2021 ||| mixed precision quantization of transformer language models for speech recognition. ||| junhao xu ||| shoukang hu ||| jianwei yu ||| xunying liu ||| helen meng ||| 
2019 ||| atts2s-vc: sequence-to-sequence voice conversion with attention and context preservation mechanisms. ||| kou tanaka ||| hirokazu kameoka ||| takuhiro kaneko ||| nobukatsu hojo ||| 
2019 ||| attention-based wavenet autoencoder for universal voice conversion. ||| adam polyak ||| lior wolf ||| 
2020 ||| attentional fused temporal transformation network for video action recognition. ||| ke yang ||| zhiyuan wang ||| huadong dai ||| tianlong shen ||| peng qiao ||| xin niu ||| jie jiang ||| dongsheng li ||| yong dou ||| 
2020 ||| mixup multi-attention multi-tasking model for early-stage leukemia identification. ||| puneet mathur ||| mehak piplani ||| ramit sawhney ||| amit jindal ||| rajiv ratn shah ||| 
2021 ||| capturing multi-resolution context by dilated self-attention. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| sequence-to-sequence singing synthesis using the feed-forward transformer. ||| merlijn blaauw ||| jordi bonada ||| 
2020 ||| signal-aware broadband doa estimation using attention mechanisms. ||| wolfgang mack ||| ullas bharadwaj ||| soumitro chakrabarty ||| emanu ||| l anco peter habets ||| 
2020 ||| complex transformer: a framework for modeling complex-valued sequence. ||| muqiao yang ||| martin q. ma ||| dongyu li ||| yao-hung hubert tsai ||| ruslan salakhutdinov ||| 
2021 ||| don't shoot butterfly with rifles: multi-channel continuous speech separation with early exit transformer. ||| sanyuan chen ||| yu wu ||| zhuo chen ||| takuya yoshioka ||| shujie liu ||| jin-yu li ||| xiangzhan yu ||| 
2020 ||| gated mechanism for attention based multi modal sentiment analysis. ||| ayush kumar ||| jithendra vepa ||| 
2021 ||| representation learning with spectro-temporal-channel attention for speech emotion recognition. ||| lili guo ||| longbiao wang ||| chenglin xu ||| jianwu dang ||| eng siong chng ||| haizhou li ||| 
2020 ||| investigation of methods to improve the recognition performance of tamil-english code-switched data in transformer framework. ||| metilda sagaya mary n. j ||| vishwas m. shetty ||| srinivasan umesh ||| 
2020 ||| attention driven fusion for multi-modal emotion recognition. ||| darshana priyasad ||| tharindu fernando ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2021 ||| arrhythmia classification with heartbeat-aware transformer. ||| bin wang ||| chang liu ||| chuanyan hu ||| xudong liu ||| jun cao ||| 
2019 ||| enhancing hybrid self-attention structure with relative-position-aware bias for speech synthesis. ||| shan yang ||| heng lu ||| shiying kang ||| lei xie ||| dong yu ||| 
2020 ||| spidernet: attention network for one-shot anomaly detection in sounds. ||| yuma koizumi ||| masahiro yasuda ||| shin murata ||| shoichiro saito ||| hisashi uematsu ||| noboru harada ||| 
2021 ||| focus on the present: a regularization method for the asr source-target attention layer. ||| nanxin chen ||| piotr zelasko ||| jes ||| s villalba ||| najim dehak ||| 
2021 ||| non-autoregressive transformer asr with ctc-enhanced decoder input. ||| xingchen song ||| zhiyong wu ||| yiheng huang ||| chao weng ||| dan su ||| helen m. meng ||| 
2021 ||| stock movement prediction and portfolio management via multimodal learning with transformer. ||| divyanshu daiya ||| che lin ||| 
2019 ||| co-attention network and low-rank bilinear pooling for aspect based sentiment analysis. ||| peiran zhang ||| hongbo zhu ||| tao xiong ||| yihui yang ||| 
2019 ||| self-attention based prosodic boundary prediction for chinese speech synthesis. ||| chunhui lu ||| pengyuan zhang ||| yonghong yan ||| 
2020 ||| fast domain adaptation for goal-oriented dialogue using a hybrid generative-retrieval transformer. ||| igor shalyminov ||| alessandro sordoni ||| adam atkinson ||| hannes schulz ||| 
2017 ||| automatic speech emotion recognition using recurrent neural networks with local attention. ||| seyedmahdad mirsamadi ||| emad barsoum ||| cha zhang ||| 
2021 ||| skip attention gan for remote sensing image synthesis. ||| kai deng ||| kun zhang ||| ping yao ||| siyuan cheng ||| peng he ||| 
2019 ||| dynamically context-sensitive time-decay attention for dialogue modeling. ||| shang-yu su ||| pei-chieh yuan ||| yun-nung chen ||| 
2018 ||| fault detection using attention models based on visual saliency. ||| muhammad amir shafiq ||| zhiling long ||| haibin di ||| ghassan al-regib ||| mohamed a. deriche ||| 
2019 ||| stream attention-based multi-array end-to-end speech recognition. ||| xiaofei wang ||| ruizhi li ||| sri harish mallidi ||| takaaki hori ||| shinji watanabe ||| hynek hermansky ||| 
2020 ||| transformer transducer: a streamable speech recognition model with transformer encoders and rnn-t loss. ||| qian zhang ||| han lu ||| hasim sak ||| anshuman tripathi ||| erik mcdermott ||| stephen koo ||| shankar kumar ||| 
2018 ||| audio set classification with attention model: a probabilistic perspective. ||| qiuqiang kong ||| yong xu ||| wenwu wang ||| mark d. plumbley ||| 
2021 ||| an attention based wavelet convolutional model for visual saliency detection. ||| reshmi s. bhooshan ||| suresh k ||| 
2021 ||| emformer: efficient memory transformer based acoustic model for low latency streaming speech recognition. ||| yangyang shi ||| yongqiang wang ||| chunyang wu ||| ching-feng yeh ||| julian chan ||| frank zhang ||| duc le ||| mike seltzer ||| 
2021 ||| adaptive bi-directional attention: exploring multi-granularity representations for machine reading comprehension. ||| nuo chen ||| fenglin liu ||| chenyu you ||| peilin zhou ||| yuexian zou ||| 
2018 ||| effective attention mechanism in dynamic models for speech emotion recognition. ||| po-wei hsiao ||| chia-ping chen ||| 
2020 ||| attention guided region division for crowd counting. ||| xiaoqi pan ||| hong mo ||| zhong zhou ||| wei wu ||| 
2018 ||| spatiotemporal attention based deep neural networks for emotion recognition. ||| jiyoung lee ||| sunok kim ||| seungryong kim ||| kwanghoon sohn ||| 
2021 ||| spatiotemporal attention for multivariate time series prediction and interpretation. ||| tryambak gangopadhyay ||| sin yong tan ||| zhanhong jiang ||| rui meng ||| soumik sarkar ||| 
2021 ||| a global-local attention framework for weakly labelled audio tagging. ||| helin wang ||| yuexian zou ||| wenwu wang ||| 
2021 ||| bidirectional focused semantic alignment attention network for cross-modal retrieval. ||| shuli cheng ||| liejun wang ||| anyu du ||| yongming li ||| 
2020 ||| exploring a zero-order direct hmm based on latent attention for automatic speech recognition. ||| parnia bahar ||| nikita makarov ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| fragmentvc: any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention. ||| yist y. lin ||| chung-ming chien ||| jheng-hao lin ||| hung-yi lee ||| lin-shan lee ||| 
2021 ||| an attention-seq2seq model based on crnn encoding for automatic labanotation generation from motion capture data. ||| min li ||| zhenjiang miao ||| xiao-ping zhang ||| wanru xu ||| 
2018 ||| hierarchical attention and context modeling for group activity recognition. ||| longteng kong ||| jie qin ||| di huang ||| yunhong wang ||| luc van gool ||| 
2018 ||| attention-based models for text-dependent speaker verification. ||| f. a. rezaur rahman chowdhury ||| quan wang ||| ignacio lopez-moreno ||| li wan ||| 
2019 ||| token-wise training for attention based end-to-end speech recognition. ||| peidong wang ||| jia cui ||| chao weng ||| dong yu ||| 
2021 ||| geometric scattering attention networks. ||| yimeng min ||| frederik wenkel ||| guy wolf ||| 
2020 |||  trading attention for feed-forward layers. ||| kazuki irie ||| alexander gerstenberger ||| ralf schl ||| ter ||| hermann ney ||| 
2018 ||| forward attention in sequence- to-sequence acoustic modeling for speech synthesis. ||| jing-xuan zhang ||| zhen-hua ling ||| li-rong dai ||| 
2021 ||| continuous-time self-attention in neural differential equation. ||| jen-tzung chien ||| yi-hsiang chen ||| 
2020 ||| look globally, age locally: face aging with an attention mechanism. ||| haiping zhu ||| zhizhong huang ||| hongming shan ||| junping zhang ||| 
2021 ||| dnanet: dense nested attention network for single image dehazing. ||| dongdong ren ||| jinbao li ||| meng han ||| minglei shu ||| 
2020 ||| characterizing speech adversarial examples using self-attention u-net enhancement. ||| chao-han huck yang ||| jun qi ||| pin-yu chen ||| xiaoli ma ||| chin-hui lee ||| 
2020 ||| audio sound determination using feature space attention based convolution recurrent neural network. ||| xianjun xia ||| jingjing pan ||| yannan wang ||| 
2019 ||| scanet: spatial-channel attention network for 3d object detection. ||| haihua lu ||| xuesong chen ||| guiying zhang ||| qiuhao zhou ||| yanbo ma ||| yong zhao ||| 
2021 ||| a novel attention-based gated recurrent unit and its efficacy in speech emotion recognition. ||| srividya tirunellai rajamani ||| kumar t. rajamani ||| adria mallol-ragolta ||| shuo liu ||| bj ||| rn w. schuller ||| 
2018 ||| acoustic-to-word attention-based model complemented with character-level ctc-based model. ||| sei ueno ||| hirofumi inaguma ||| masato mimura ||| tatsuya kawahara ||| 
2021 ||| transmask: a compact and fast speech separation model based on transformer. ||| zining zhang ||| bingsheng he ||| zhenjie zhang ||| 
2020 ||| a time-frequency network with channel attention and non-local modules for artificial bandwidth extension. ||| yuanjie dong ||| yaxing li ||| xiaoqi li ||| shan xu ||| dan wang ||| zhihui zhang ||| shengwu xiong ||| 
2021 ||| hsan: a hierarchical self-attention network for multi-turn dialogue generation. ||| yawei kong ||| lu zhang ||| can ma ||| cong cao ||| 
2020 ||| correction of automatic speech recognition with transformer sequence-to-sequence model. ||| oleksii hrinchuk ||| mariya popova ||| boris ginsburg ||| 
2021 ||| a novel end-to-end speech emotion recognition network with stacked transformer layers. ||| xianfeng wang ||| min wang ||| wenbo qi ||| wanqi su ||| xiangqian wang ||| huan zhou ||| 
2021 ||| query-by-example keyword spotting system using multi-head attention and soft-triple loss. ||| jinmiao huang ||| waseem gharbieh ||| han suk shim ||| eugene kim ||| 
2019 ||| a joint auditory attention decoding and adaptive binaural beamforming algorithm for hearing devices. ||| wenqiang pu ||| jinjun xiao ||| tao zhang ||| zhi-quan luo ||| 
2020 ||| an attention enhanced multi-task model for objective speech assessment in real-world environments. ||| xuan dong ||| donald s. williamson ||| 
2020 ||| enhanced non-local cascading network with attention mechanism for hyperspectral image denoising. ||| hanwen ma ||| ganchao liu ||| yuan yuan ||| 
2021 ||| confidence estimation for attention-based sequence-to-sequence models for speech recognition. ||| qiujia li ||| david qiu ||| yu zhang ||| bo li ||| yanzhang he ||| philip c. woodland ||| liangliang cao ||| trevor strohman ||| 
2021 ||| hoca: higher-order channel attention for single image super-resolution. ||| yalei lv ||| tao dai ||| bin chen ||| jian lu ||| shu-tao xia ||| jingchao cao ||| 
2020 ||| mockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders. ||| andy t. liu ||| shu-wen yang ||| po-han chi ||| po-chun hsu ||| hung-yi lee ||| 
2020 ||| an empirical study of transformer-based neural language model adaptation. ||| ke li ||| zhe liu ||| tianxing he ||| hongzhao huang ||| fuchun peng ||| daniel povey ||| sanjeev khudanpur ||| 
2021 ||| graph attention networks for speaker verification. ||| jee-weon jung ||| hee-soo heo ||| ha-jin yu ||| joon son chung ||| 
2020 ||| towards decoding selective attention from single-trial eeg data in cochlear implant users based on deep neural networks. ||| waldo nogueira ||| hanna dolhopiatenko ||| 
2021 ||| hierarchical attention-based temporal convolutional networks for eeg-based emotion recognition. ||| chao li ||| boyang chen ||| ziping zhao ||| nicholas cummins ||| bj ||| rn w. schuller ||| 
2020 ||| high-accuracy classification of attention deficit hyperactivity disorder with l2, 1-norm linear discriminant analysis. ||| yibin tang ||| xufei li ||| ying chen ||| yuan zhong ||| aimin jiang ||| xiaofeng liu ||| 
2019 ||| perceptual quality preserving image super-resolution via channel attention. ||| wei-yu lee ||| po-yu chuang ||| yu-chiang frank wang ||| 
2018 ||| query-by-example spoken term detection using attention-based multi-hop networks. ||| chia-wei ao ||| hung-yi lee ||| 
2020 ||| evaluation of joint auditory attention decoding and adaptive binaural beamforming approach for hearing devices with attention switching. ||| wenqiang pu ||| peng zan ||| jinjun xiao ||| tao zhang ||| zhi-quan luo ||| 
2020 ||| reversal no longer matters: attention-based arrhythmia detection with lead-reversal ecg data. ||| zheng cao ||| jialin shi ||| ji wu ||| 
2021 ||| fma-eta: estimating travel time entirely based on ffn with attention. ||| yiwen sun ||| yulu wang ||| kun fu ||| zheng wang ||| ziang yan ||| changshui zhang ||| jieping ye ||| 
2020 ||| transformer vae: a hierarchical model for structure-aware and interpretable music representation learning. ||| junyan jiang ||| gus xia ||| dave b. carlton ||| chris n. anderson ||| ryan h. miyakawa ||| 
2019 ||| multi-attention network for thoracic disease classification and localization. ||| yanbo ma ||| qiuhao zhou ||| xuesong chen ||| haihua lu ||| yong zhao ||| 
2020 ||| addressing the polysemy problem in language modeling with attentional multi-sense embeddings. ||| rao ma ||| lesheng jin ||| qi liu ||| lu chen ||| kai yu ||| 
2018 ||| attention-based end-to-end speech recognition on voice search. ||| changhao shan ||| junbo zhang ||| yujun wang ||| lei xie ||| 
2020 ||| multimodal transformer fusion for continuous emotion recognition. ||| jian huang ||| jianhua tao ||| bin liu ||| zheng lian ||| mingyue niu ||| 
2020 ||| controllable time-delay transformer for real-time punctuation prediction and disfluency detection. ||| qian chen ||| mengzhe chen ||| bo li ||| wen wang ||| 
2019 ||| attention-based atrous convolutional neural networks: visualisation and understanding perspectives of acoustic scenes. ||| zhao ren ||| qiuqiang kong ||| jing han ||| mark d. plumbley ||| bj ||| rn w. schuller ||| 
2020 ||| location-relative attention mechanisms for robust long-form speech synthesis. ||| eric battenberg ||| r. j. skerry-ryan ||| soroosh mariooryad ||| daisy stanton ||| david kao ||| matt shannon ||| tom bagby ||| 
2021 ||| cass-nat: ctc alignment-based single step non-autoregressive transformer for speech recognition. ||| ruchao fan ||| wei chu ||| peng chang ||| jing xiao ||| 
2021 ||| decoding music attention from "eeg headphones": a user-friendly auditory brain-computer interface. ||| winko w. an ||| barbara g. shinn-cunningham ||| hannes gamper ||| dimitra emmanouilidou ||| david johnston ||| mihai jalobeanu ||| edward cutrell ||| andrew d. wilson ||| kuan-jung chiang ||| ivan tashev ||| 
2020 ||| spectrogram analysis via self-attention for realizing cross-model visual-audio generation. ||| huadong tan ||| guang wu ||| pengcheng zhao ||| yanxiang chen ||| 
2017 ||| a multiple bandwidth objective speech intelligibility estimator based on articulation index band correlations and attention. ||| stephen d. voran ||| 
2019 ||| multi-scale attention aided multi-resolution network for human pose estimation. ||| srinika selvam ||| deepak mishra ||| 
2019 ||| gradually growing residual and self-attention based dense deep back projection network for large scale super-resolution of image. ||| manoj sharma ||| avinash upadhyay ||| ajay pratap singh ||| megh makwana ||| swati bhugra ||| brejesh lall ||| santanu chaudhury ||| deepak mishra ||| anil k. saini ||| 
2018 ||| research on investment decision model of distribution grid project based on transformer district. ||| luhua zhang ||| yuejin zhang ||| jun wang ||| xiao wang ||| haifeng li ||| runtong cheng ||| 
2019 ||| find the overwhelming transformers in power grid with an optimized clustering method. ||| haifeng li ||| yuejin zhang ||| mo hai ||| 
2021 ||| incorporating attention mechanism in enhancing classification of alzheimer's disease. ||| nur amirah abd hamid ||| mohd ibrahim shapiai ||| uzma batool ||| ranjit singh sarban singh ||| muhamad kamal mohammed amin ||| khairil ashraf elias ||| 
2021 ||| what attracts people's attention in banner advertisements? a study on banner advertisements using a human attention model. ||| kendrick mikhael f. pua ||| josh darren w. ang ngo ching ||| gian brennan b. betonio ||| macario o. cordel ||| 
2021 ||| a one-stage temporal detector with attentional lstm for video object detection. ||| jiahui yu ||| zhaojie ju ||| hongwei gao ||| dalin zhou ||| 
2018 ||| detection for joint attention based on a multi-sensor visual system. ||| wanqi zhang ||| zhiyong wang ||| haibin cai ||| honghai liu ||| 
2021 ||| mag-net: multi-task attention guided network for brain tumor segmentation and classification. ||| sachin gupta ||| narinder singh punn ||| sanjay kumar sonbhadra ||| sonali agarwal ||| 
2022 ||| towards efficient vision transformer inference: a first study of transformers on mobile devices. ||| xudong wang ||| li lyna zhang ||| yang wang ||| mao yang ||| 
2020 ||| pair-wise convolution network with transformers for sequential recommendation. ||| jiangpeng shi ||| xiaochun cheng ||| jianfeng wang ||| 
2019 ||| change detection in synthetic aperture radar images based on convolutional block attention module. ||| dong wang ||| feng gao ||| junyu dong ||| shengke wang ||| 
2022 |||  sur transformers. ||| mohamed louay rabah ||| nedra mellouli ||| imed riadh farah ||| 
2020 ||| can i get your (robot) attention? human sensitivity to subtle hints of human-likeness in a humanoid robot's behavior. ||| davide ghiglino ||| davide de tommaso ||| cesco willemse ||| serena marchesi ||| agnieszka wykowska ||| 
2017 ||| goal-directed deployment of attention in a computational model: a study in multiple-object tracking. ||| andrew m. lovett ||| will bridewell ||| paul bello ||| 
2017 ||| children's eeg indices of directed attention during somatosensory anticipation: relations with executive function. ||| staci meredith weiss ||| peter j. marshall ||| 
2020 ||| may i have your attention? testing a subjective attention scale. ||| matthew welsh ||| 
2018 ||| how do pragmatic and object cues affect monolingual and bilingual toddlers' visual attention during word learning? ||| christina schonberg ||| catherine m. sandhofer ||| scott p. johnson ||| 
2018 ||| effects of illustration details on attention and comprehension in beginning readers. ||| cassondra m. eng ||| karrie e. godwin ||| kristen boyle ||| anna v. fisher ||| 
2018 ||| dimension-based attention in learning and understanding spoken language. ||| frederic dick ||| lori l. holt ||| howard nusbaum ||| neeraj sharma ||| barbara g. shinn-cunningham ||| 
2017 ||| evidence for overt visual attention to hand gestures as a function of redundancy and speech disfluency. ||| amelia yeo ||| martha w. alibali ||| 
2017 ||| exploitative and exploratory attention in a four-armed bandit task. ||| adrian walker ||| mike le pelley ||| tom beesley ||| 
2017 ||| cognitive and attentional process in insight problem solving of the puzzle game "tangram". ||| yoshiki nakano ||| 
2018 ||| the impact of gesture and prior knowledge on visual attention during math instruction. ||| katharine f. guarino ||| elizabeth wakefield ||| miriam a. novack ||| eliza congdon ||| steven franconeri ||| susan goldin-meadow ||| 
2017 ||| a computational model of the role of attention in subitizing and enumeration. ||| gordon briggs ||| will bridewell ||| paul bello ||| 
2019 ||| movements and visuospatial working memory: examining the role of movement and attention to movement. ||| divya bhatia ||| pietro spataro ||| clelia rossi-arnaud ||| 
2018 ||| computational model of spatial auditory attention in act-r. ||| jaelle scheuerman ||| maxwell t. anderson ||| kristen brent venable ||| edward j. golob ||| 
2018 ||| how to open the "window of attention" in serial verb constructions. ||| yu deng ||| 
2019 ||| book design, attention, and reading performance: current practices and opportunities for optimization. ||| karrie e. godwin ||| cassondra m. eng ||| grace murray ||| anna v. fisher ||| 
2020 ||| investigation of attentional decay: implications for instruction. ||| karrie e. godwin ||| freya kaur ||| 
2018 |||  personalized attention tasks for children with developmental disorders. ||| amarnath dasaka ||| bapiraju surampudi ||| 
2017 ||| computational modeling of auditory spatial attention. ||| edward j. golob ||| kristen brent venable ||| jaelle scheuerman ||| maxwell t. anderson ||| 
2019 ||| look out, it's going to fall!: does physical instability capture attention and lead to distraction? ||| marta kryven ||| sholei croom ||| brian j. scholl ||| josh tenenbaum ||| 
2017 ||| executive function and attention predict low-income preschoolers' active category learning. ||| katherine a. adams ||| george kachergis ||| 
2018 ||| object-based attention in multiple frames of reference. ||| weizhi nan ||| lizhu yan ||| jiamin huang ||| ya fan ||| hong lu ||| shimin fu ||| 
2020 ||| does looking time predict choice in domestic dogs? examining visual attention in man's best friend. ||| liyuzhi dong ||| julia espinosa ||| daphna buchsbaum ||| 
2020 ||| covert attention shift by sequence-space synesthesia (sss): a cognitive grammar approach. ||| mohsen dolatabadi ||| mehrdad dowlatabadi ||| 
2017 ||| effects of attention to emergent phenomena on rule discovery. ||| hitoshi terai ||| kazuhisa miwa ||| sho yokoyama ||| souta fujimura ||| gotaro nakayama ||| 
2018 ||| an attention-driven computational model of human causal reasoning. ||| paul bello ||| andrew m. lovett ||| gordon briggs ||| kevin o'neill ||| 
2017 ||| a model-based approach for assessing attentional biases in people with depressive symptoms. ||| isa rutten ||| wouter voorspoels ||| ernst h. w. koster ||| wolf vanpaemel ||| 
2020 ||| better together: exploration prior to instruction facilitates rule-learning and modifies attention to demonstration. ||| mia radovanovic ||| natalie brezack ||| laura shneidman ||| amanda woodward ||| 
2020 ||| do environmental resource distributions affect attentional styles? ||| gunes sonmez ||| calvin isch ||| 
2018 ||| rapid learning in early attentional processing: bayesian estimation of trial-by-trial updating. ||| aaron cochrane ||| joseph l. austerweil ||| vanessa r. simmering ||| c. shawn green ||| 
2020 ||| striatal and cortical components of inattentional responses: an experimental and computational study of thewisconsin card sorting test in adults with adhd traits. ||| andrea caso ||| richard p. cooper ||| 
2020 ||| a large-scale analysis of attentional deployment across one hundred sessions of adaptive multitask training. ||| omar claflin ||| 
2019 ||| does children's shape knowledge contribute to age-related improvements in selective sustained attention measured in a trackit task? ||| emily keebler ||| jaeah kim ||| erik d. thiessen ||| anna v. fisher ||| 
2020 ||| using the trackit task to measure the development of selective sustained attention in children ages 2-7. ||| emily keebler ||| jaeah kim ||| oceann stanley ||| erik d. thiessen ||| anna v. fisher ||| 
2017 ||| connecting stimulus-driven attention to the properties of infant-directed speech - is exaggerated intonation also more surprising? ||| okko r ||| s ||| nen ||| sofoklis kakouros ||| melanie soderstrom ||| 
2020 ||| dynamics of spatio-temporal scope of attention: temporal correlations in reaction time data. ||| devpriya kumar ||| akanksha malik ||| 
2018 ||| hand-eye coordination and visual attention in infancy. ||| drew h. abney ||| hadar karmazyn ||| linda b. smith ||| chen yu ||| 
2020 ||| does the effect of labels on sustained attention depend on target familiarity? ||| emily keebler ||| catarina vales ||| jaeah kim ||| tishya girdhar ||| anna v. fisher ||| 
2017 ||| attention modulation effects on visual feature-selectivity of neurons in brain-inspired categorization models. ||| saeed masoudnia ||| abdolhossein vahabie ||| majid nili ahmadabadi ||| babak nadjar araabi ||| 
2018 ||| contingent responsiveness in digital storybooks: effects on children's comprehension and the role of individual differences in attention. ||| cassondra m. eng ||| anthony tomasic ||| erik d. thiessen ||| 
2020 ||| attentional allocation as optimal compression in visual search. ||| christopher bates ||| robert jacobs ||| 
2018 ||| contextual separation shifts attentional biases. ||| michelle luna ||| catherine m. sandhofer ||| 
2017 ||| variables involved in selective sustained attention development: advances in measurement. ||| jaeah kim ||| anna vande velde ||| erik d. thiessen ||| anna v. fisher ||| 
2018 ||| attention selectively boosts learning of statistical structure. ||| tess allegra forest ||| amy sue finn ||| 
2018 ||| a dynamic neural field model of memory, attention and cross-situational word learning. ||| ajaz bhat ||| john p. spencer ||| larissa k. samuelson ||| 
2018 ||| enumeration by pattern recognition requires attention: evidence against immediate holistic processing of canonical patterns. ||| gordon briggs ||| christina wasylyshyn ||| paul bello ||| 
2018 ||| the 'goldilocks effect' in preschoolers' attention to spoken language. ||| ruthe foushee ||| fei xu ||| mahesh srinivasan ||| 
2017 ||| numbers uniquely bias spatial attention: a novel paradigm for understanding spatial-numerical associations. ||| lauren aulet ||| sami yousif ||| stella f. lourenco ||| 
2018 ||| measuring attention control abilities with a gaze following antisaccade paradigm. ||| jade yonehiro ||| nicholas d. duran ||| 
2019 ||| "give me a break": can brief bouts of physical activity reduce elementary children's attentional failures and improve learning? ||| grace murray ||| karrie e. godwin ||| 
2020 ||| how to navigate everyday distractions: leveraging optimal feedback to train attention control. ||| maria wirzberger ||| anastasia lado ||| lisa eckerstorfer ||| ivan oreshnikov ||| jean-claude passy ||| adrian stock ||| amitai shenhav ||| falk lieder ||| 
2020 ||| attentional competition in genuine classrooms: analysis of the classroom visual environment. ||| karrie e. godwin ||| howard j. seltman ||| peter scupelli ||| anna v. fisher ||| 
2020 ||| nonlinear probability weighting can reflect attentional biases in sequential sampling. ||| veronika zilker ||| thorsten pachur ||| 
2017 ||| it's time: quantifying the relevant timescales for joint attention. ||| drew h. abney ||| linda b. smith ||| chen yu ||| 
2020 ||| visual attention during e-learning: eye-tracking shows that making salient areas more prominent helps learning in online tutors. ||| farnaz tehranchi ||| frank e. ritter ||| chungil chae ||| 
2019 ||| measuring selective sustained attention in children with trackit and eyetracking. ||| jaeah kim ||| shashank singh ||| emily keebler ||| erik d. thiessen ||| anna v. fisher ||| 
2018 ||| inferring attention through cursor trajectories. ||| kiran kumar ||| samuel harding ||| richard m. shiffrin ||| 
2017 ||| children's attention to semantic content versus emotional tone: differences between two cultural groups. ||| yang yang ||| li wang ||| qi wang ||| 
2018 ||| beyond principles and outcomes: children determine fairness based on attention and exactness. ||| madison flowers ||| rosie aboody ||| julian jara-ettinger ||| 
2019 ||| a computational model of feature formation, event prediction, and attention switching. ||| eman awad ||| fintan j. costello ||| 
2020 ||| staying and returning dynamics of sustained attention in young children. ||| jaeah kim ||| shashank singh ||| erik d. thiessen ||| anna v. fisher ||| 
2017 ||| seeing is not enough for sustained visual attention. ||| lei yuan ||| tian (linger) xu ||| chen yu ||| linda b. smith ||| 
2020 ||| an evidence accumulation model of motivational and developmental influences over sustained attention. ||| harrison ritz ||| joe degutis ||| michael joshua frank ||| michael esterman ||| amitai shenhav ||| 
2019 ||| attentional capture: modeling automatic mechanisms and top-down control. ||| andrew m. lovett ||| will bridewell ||| paul bello ||| 
2020 ||| the impact of semantic versus perceptual attention on memory representation. ||| sagana vijayarajah ||| eilidh mcalister ||| margaret l. schlichting ||| 
2018 ||| spatial language and visual attention: a new approach to test linguistic relativity. ||| florian goller ||| soonja choi ||| ulrich ansorge ||| 
2019 ||| predicting learned inattention from attentional selectivity and optimization. ||| nathaniel blanco ||| vladimir m. sloutsky ||| 
2018 ||| tuning to the task at hand: processing goals shape adults' attention to unfolding activity. ||| jessica e. kosie ||| dare a. baldwin ||| 
2020 ||| visual attention and real-world decision making: sharing photos on social media. ||| shawn fagan ||| lauren wade ||| kurt hugenberg ||| apu kapadia ||| bennett i. bertenthal ||| 
2019 ||| individual differences in bodily attention: variability in anticipatory mu rhythm power is associated with executive function abilities and processing speed. ||| staci meredith weiss ||| rebecca laconi ||| peter j. marshall ||| 
2018 ||| sign language experience affects comprehension and attention to gesture. ||| jenny lu ||| nicole burke ||| susan goldin-meadow ||| amanda woodward ||| 
2018 ||| exploration and attention in young children. ||| nathaniel blanco ||| vladimir m. sloutsky ||| 
2017 ||| bottom-up attentional cueing in category learning in children. ||| nathaniel blanco ||| vladimir m. sloutsky ||| 
2018 ||| sequences of discrete attentional shifts emerge from a neural dynamic architecture for conjunctive visual search that operates in continuous time. ||| raul grieben ||| jan tek ||| lve ||| stephan k. u. zibner ||| sebastian schneegans ||| gregor sch ||| ner ||| 
2020 ||| online article comprehension in monolingual spanish-speaking preschoolers with specific language impairment: a language-mediated visual attention study. ||| andrea helo ||| carmen julia coloma ||| zulema de barbieri ||| ernesto guerra ||| 
2017 ||| perspective-taking in referential communication: does stimulated attention to addressee's perspective influence speakers' reference production? ||| debby damen ||| per van der wijst ||| marije van amelsvoort ||| emiel krahmer ||| 
2019 ||| visual spatial attention skills and holistic processing in high school students with and without dyslexia. ||| ronald chan ||| chin-wai kwok ||| duo liu ||| ricky van-yip tso ||| 
2017 ||| predicting preschool-aged children's behavior regulation from attention tasks in the lab. ||| chelsea j. andrews ||| emily coates ||| kristine kovack-lesh ||| vanessa r. simmering ||| 
2019 ||| effects of instructor presence in video lectures: rapport, attention, and learning. ||| andrew t. stull ||| logan fiorella ||| rebecca similuk ||| stevi ibonie ||| richard e. mayer ||| 
2018 ||| modeling morphological affixation with interpretable recurrent networks: sequential rebinding controlled by hierarchical attention. ||| colin wilson ||| 
2019 ||| novel labels modify visual attention in 2-year-old children. ||| alexander latourrette ||| miriam a. novack ||| sandra waxman ||| 
2020 ||| does children's visual attention to objects influence their verb learning? ||| jane childers ||| bibiana cutilletta ||| katherine capps ||| sneh lalani ||| priscilla tovar-perez ||| 
2020 ||| examining sustained attention in child-parent interaction: a comparative study of typically developing children and children with autism spectrum disorder. ||| julia yurkovic ||| grace lisandrelli ||| rebecca c. schaffer ||| kelli dominick ||| ernest v. pedapati ||| craig a. erickson ||| daniel p. kennedy ||| chen yu ||| 
2018 ||| changing minds: the effect of stimulated attention to another's different point of view on visual perspective-taking. ||| debby damen ||| marije van amelsvoort ||| per van der wijst ||| emiel krahmer ||| 
2019 ||| a re-examination of the interrelationships between attention, eye behavior, and creative thought. ||| shadab tabatabaeian ||| colin holbrook ||| carolyn jennings ||| 
2018 ||| evidence that the attention blink reflects categorical perceptual dynamics. ||| lucas huszar ||| david huber ||| 
2018 ||| wiggleometer: measuring selective sustained attention in children. ||| karrie e. godwin ||| anna v. fisher ||| 
2019 ||| exploring the role of social priming in alcohol attentional bias. ||| stephen cantarutti ||| emmanuel m. pothos ||| 
2019 ||| parametric control of distractor-oriented attention. ||| harrison ritz ||| amitai shenhav ||| 
2019 ||| the impact of speech complexity on preschooler attention, speaker preference, and learning. ||| ruthe foushee ||| mahesh srinivasan ||| fei xu ||| 
2019 ||| controlling attention in a memory-augmented neural network to solve working memory tasks. ||| t. s. jayram ||| younes bouhadjar ||| tomasz kornuta ||| ryan l. mcavoy ||| alexis asseman ||| ahmet s. ozcan ||| 
2019 ||| hands in mind: learning to write with both hands improves inhibitory control, but not attention. ||| mukesh makwana ||| biswajit boity ||| prasanth chandran ||| amogh sirnoorkar ||| sanjay chandrasekharan ||| 
2017 ||| brief mindfulness meditation improves attention in novices. ||| catherine norris ||| daniel creem ||| 
2020 ||| examining a developmental pathway of early word learning: from qualitative characteristics of parent speech, to sustained attention, to vocabulary size. ||| ryan peters ||| chen yu ||| 
2020 ||| the attentional demands of learning by doing: a developmental study. ||| karrie e. godwin ||| paulo carvalho ||| 
2018 ||| understanding attentional selectivity, flexibility, and stability: a dynamic neural field model predicts behavior in 3- and 4-year-olds. ||| anastasia kerr-german ||| kara lowery ||| aaron t. buss ||| 
2019 ||| numerosity capture of attention. ||| santiago alonso-diaz ||| jessica f. cantlon ||| 
2019 ||| distinguishing learned categorical perception from selective attention to a dimension: preliminary evidence from a new method. ||| janet k. andrews ||| josh de leeuw ||| rebecca andrews ||| cole landolt ||| chrissy griesmer ||| 
2020 ||| do language effects on attention persist in complex task contexts? ||| jessica j. joseph ||| barbara c. malt ||| 
2020 ||| self-reference effect for faces is mediated by attention. ||| aditi jublie ||| devpriya kumar ||| 
2019 ||| inattentional blindness in visual search. ||| matt rounds ||| chris lucas ||| frank keller ||| 
2018 ||| coupling dynamical and connectionist models: representation of spatial attention via learned deictic gestures in human-robot interaction. ||| baris serhan ||| john p. spencer ||| angelo cangelosi ||| 
2021 ||| novel design and simulation of heric transformerless pv inverter in matlab/simulink. ||| shuaibu musa adam ||| vladimir hahanov ||| svetlana chumachenko ||| eugenia litvinova ||| ka lok man ||| 
2018 ||| automatic text summarization using customizable fuzzy features and attention on the context and vocabulary. ||| ramin sahba ||| nima ebadi ||| mo m. jamshidi ||| paul rad ||| 
2019 ||| composing services in 5g-transformer. ||| jorge baranda ||| josep mangues-bafalluy ||| luca vettori ||| ricardo mart ||| nez ||| giada landi ||| kiril antevski ||| 
2018 ||| attention-based neural network for joint diarization and speaker extraction. ||| shlomo e. chazan ||| sharon gannot ||| jacob goldberger ||| 
2018 ||| internet of tangibles: exploring the interaction-attention continuum. ||| leonardo angelini ||| elena mugellini ||| omar abou khaled ||| nadine couture ||| elise van den hoven ||| saskia bakker ||| 
2021 ||| assessing the effectiveness of multilingual transformer-based text embeddings for named entity recognition in portuguese. ||| diego bernardes de lima santos ||| frederico giffoni de carvalho dutra ||| fernando silva parreiras ||| wladmir cardoso brand ||| o ||| 
2018 ||| research on human-robot interaction security strategy of movement authorization for service robot based on people's attention monitoring. ||| jiale gong ||| hong wang ||| zhiguo lu ||| naishi feng ||| fo hu ||| 
2019 ||| statcom evaluation in electrified railway using v/v and scott power transformers. ||| luis a. m. barros ||| mohamed tanta ||| ant ||| nio p. martins ||| jo ||| o luiz afonso ||| j. g. pinto ||| 
2021 ||| acgvd: vulnerability detection based on comprehensive graph via graph neural network with attention. ||| min li ||| chunfang li ||| shuailou li ||| yanna wu ||| boyang zhang ||| yu wen ||| 
2019 ||| a character-level bigru-attention for phishing classification. ||| lijuan yuan ||| zhiyong zeng ||| yikang lu ||| xiaofeng ou ||| tao feng ||| 
2021 ||| convolutional recurrent neural network with attention gates for real-time single-channel speech enhancement. ||| wen-yu wu ||| pin-hsuan li ||| kai-wen liang ||| pao-chi chang ||| 
2017 ||| study on suppressing harmonic flux density of converter transformer core under the dc bias. ||| hanlong hong ||| pengfei shao ||| xifeng xu ||| hao chen ||| 
2021 ||| diabetic retinopathy detection using cnn, transformer and mlp based architectures. ||| nikhil sathya kumar ||| b. ramaswamy karthikeyan ||| 
2021 ||| towards an attention-based accurate intrusion detection approach. ||| arunavo dey ||| md. shohrab hossain ||| md. nazmul hoq ||| suryadipta majumdar ||| 
2021 ||| h-bert: enhancing chinese pretrained models with attention to hownet. ||| wei zhu ||| 
2021 ||| matching with transformers in melt. ||| sven hertling ||| jan portisch ||| heiko paulheim ||| 
2019 ||| easy web api development with sparql transformer. ||| pasquale lisena ||| albert mero ||| o-pe ||| uela ||| tobias kuhn ||| rapha ||| l troncy ||| 
2019 ||| pretrained transformers for simple question answering over knowledge graphs. ||| denis lukovnikov ||| asja fischer ||| jens lehmann ||| 
2019 ||| explaining customer activation with deep attention models. ||| koen weterings ||| stefano bromuri ||| marko c. j. d. van eekelen ||| 
2019 ||| designing user-adaptive information dashboards: considering limited attention and working memory. ||| peyman toreini ||| moritz langner ||| 
2019 ||| the impact of using it artefacts on organizational attention: the case of a city hall. ||| isabel ramos ||| victor barros ||| 
2019 ||| unfolding the clickbait: a siren's call in the attention economy. ||| wenping zhang ||| qiqi jiang ||| chih-hung peng ||| 
2018 ||| designing attention-aware business intelligence and analytics dashboards to support task resumption. ||| peyman toreini ||| moritz langner ||| alexander maedche ||| 
2020 ||| do transformers dream of inference, or can pretrained generative models learn implicit inferential rules? ||| zhengzhong liang ||| mihai surdeanu ||| 
2020 ||| on task-level dialogue composition of generative transformer model. ||| prasanna parthasarathi ||| sharan narang ||| arvind neelakantan ||| 
2020 ||| sentiment analysis for software engineering: how far can pre-trained transformer models go? ||| ting zhang ||| bowen xu ||| ferdian thung ||| stefanus agus haryono ||| david lo ||| lingxiao jiang ||| 
2018 ||| recurrent attention for deep neural object detection. ||| georgios symeonidis ||| anastasios tefas ||| 
2020 ||| drug-drug interaction classification using attention based neural networks. ||| dimitrios zaikis ||| ioannis p. vlahavas ||| 
2018 ||| dialog state tracking for unseen values using an extended attention mechanism. ||| takami yoshida ||| kenji iwata ||| hiroshi fujimura ||| masami akamine ||| 
2018 ||| attention based joint model with negative sampling for new slot values recognition. ||| mulan hou ||| xiaojie wang ||| caixia yuan ||| guohua yang ||| shuo hu ||| yuanyuan shi ||| 
2020 ||| dialog state tracking with incorporation of target values in attention models. ||| takami yoshida ||| kenji iwata ||| yuka kobayashi ||| hiroshi fujimura ||| 
2018 ||| hierarchical feature fusion with text attention for multi-scale text detection. ||| chao liu ||| yuexian zou ||| wenjie guan ||| 
2018 ||| information distance based self-attention-bgru layer for end-to-end speech recognition. ||| yunhao yan ||| qinmengying yan ||| guang hua ||| haijian zhang ||| 
2020 ||| mlab-bilstm: online web attack detection via attention-based deep neural networks. ||| jun yang ||| mengyu zhou ||| baojiang cui ||| 
2019 ||| verifying asynchronous event-driven programs using partial abstract transformers. ||| peizun liu ||| thomas wahl ||| akash lal ||| 
2017 ||| drug-drug interaction extraction via recurrent neural network with multiple attention layers. ||| zibo yi ||| shasha li ||| jie yu ||| yusong tan ||| qingbo wu ||| hong yuan ||| ting wang ||| 
2020 ||| cross product and attention based deep neural collaborative filtering. ||| zhigao zhang ||| jing qin ||| feng li ||| bin wang ||| 
2019 ||| damtrnn: a delta attention-based multi-task rnn for intention recognition. ||| weitong chen ||| lin yue ||| bohan li ||| can wang ||| quan z. sheng ||| 
2021 ||| stct: spatial-temporal conv-transformer network for cardiac arrhythmias recognition. ||| yixuan qiu ||| weitong chen ||| lin yue ||| miao xu ||| baofeng zhu ||| 
2018 ||| event extraction with deep contextualized word representation and multi-attention layer. ||| ruixue ding ||| zhoujun li ||| 
2020 ||| interprocedural shape analysis using separation logic-based transformer summaries. ||| hugo illous ||| matthieu lemerre ||| xavier rival ||| 
2017 ||| a new abstraction framework for affine transformers. ||| tushar sharma ||| thomas w. reps ||| 
2020 ||| improving auto-encoder novelty detection using channel attention and entropy minimization. ||| miao tian ||| dongyan guo ||| ying cui ||| xiang pan ||| shengyong chen ||| 
2021 ||| attention-based dual-branches localization network for weakly supervised object localization. ||| wenjun hui ||| chuangchuang tan ||| guanghua gu ||| 
2019 ||| dense attention network for facial expression recognition in the wild. ||| cong wang ||| ke lu ||| jian xue ||| yanfu yan ||| 
2021 ||| deep adaptive attention triple hashing. ||| yang shi ||| xiushan nie ||| quan zhou ||| li zou ||| yilong yin ||| 
2020 ||| integrating aspect-aware interactive attention and emotional position-aware for multi-aspect sentiment analysis. ||| xiaoye wang ||| xiaowen zhou ||| zan gao ||| peng yang ||| xianbin wen ||| hongyun ning ||| 
2020 ||| attention feature matching for weakly-supervised video relocalization. ||| haoyu tang ||| jihua zhu ||| zan gao ||| tao zhuo ||| zhiyong cheng ||| 
2021 ||| score transformer: generating musical score from note-level representation. ||| masahiro suzuki ||| 
2020 ||| scene graph generation via multi-relation classification and cross-modal attention coordinator. ||| xiaoyi zhang ||| zheng wang ||| xing xu ||| jiwei wei ||| yang yang ||| 
2021 ||| local self-attention on fine-grained cross-media retrieval. ||| chen wang ||| yazhou yao ||| qiong wang ||| zhenmin tang ||| 
2021 ||| hard-boundary attention network for nuclei instance segmentation. ||| yalu cheng ||| pengchong qiao ||| hongliang he ||| guoli song ||| jie chen ||| 
2019 ||| multi-label image classification with attention mechanism and graph convolutional networks. ||| quanling meng ||| weigang zhang ||| 
2021 ||| focusing attention across multiple images for multimodal event detection. ||| yangyang li ||| jun li ||| hao jin ||| liang peng ||| 
2019 ||| attention-aware feature pyramid ordinal hashing for image retrieval. ||| xie sun ||| lu jin ||| zechao li ||| 
2020 ||| graph convolution network with node feature optimization using cross attention for few-shot learning. ||| ying liu ||| yanbo lei ||| sheikh faisal rashid ||| 
2019 ||| selective attention network for image dehazing and deraining. ||| xiao liang ||| runde li ||| jinhui tang ||| 
2020 ||| attention-constraint facial expression recognition. ||| qisheng jiang ||| 
2020 ||| table detection and cell segmentation in online handwritten documents with graph attention networks. ||| ying liu ||| heng zhang ||| xiao-long yun ||| jun-yu ye ||| cheng-lin liu ||| 
2020 ||| motion-transformer: self-supervised pre-training for skeleton-based action recognition. ||| yi-bin cheng ||| xipeng chen ||| dongyu zhang ||| liang lin ||| 
2020 ||| multi-level expression guided attention network for referring expression comprehension. ||| liang peng ||| yang yang ||| xing xu ||| jingjing li ||| xiaofeng zhu ||| 
2018 ||| size-invariant attention accuracy metric for image captioning with high-resolution residual attention. ||| zongjian zhang ||| qiang wu ||| yang wang ||| fang chen ||| 
2021 ||| attention-based long-term modeling for deep visual odometry. ||| sangni xu ||| hao xiong ||| qiuxia wu ||| zhiyong wang ||| 
2017 ||| recurrent highway networks with attention mechanism for scene text recognition. ||| haodong yang ||| shuohao li ||| xiaoqing yin ||| anqi han ||| jun zhang ||| 
2017 ||| attention to the scale: deep multi-scale salient object detection. ||| jing zhang ||| yuchao dai ||| bo li ||| mingyi he ||| 
2019 ||| ogaze: gaze prediction in egocentric videos for attentional object selection. ||| mohammad al-naser ||| shoaib ahmed siddiqui ||| hiroki ohashi ||| sheraz ahmed ||| nakamura katsuyki ||| takuto sato ||| andreas dengel ||| 
2021 ||| ear-net: error attention refining network for retinal vessel segmentation. ||| jun wang ||| yang zhao ||| linglong qian ||| xiaohan yu ||| yongsheng gao ||| 
2018 ||| convolutional 3d attention network for video based freezing of gait recognition. ||| renfei sun ||| zhiyong wang ||| kaylena ehgoetz martens ||| simon j. g. lewis ||| 
2019 ||| multi-pooling attention learning for melanoma recognition. ||| ruolin liang ||| qiuxia wu ||| xiaowei yang ||| 
2020 ||| a-deeppixbis: attentional angular margin for face anti-spoofing. ||| md. sourave hossain ||| labiba rupty ||| koushik roy ||| md. hasan ||| shirshajit sengupta ||| nabeel mohammed ||| 
2020 ||| evolutionary attention network for medical image segmentation. ||| tahereh hassanzadeh ||| daryl essam ||| ruhul a. sarker ||| 
2019 ||| bi-san-cap: bi-directional self-attention for image captioning. ||| md. zakir hossain ||| ferdous sohel ||| mohd fairuz shiratuddin ||| hamid laga ||| mohammed bennamoun ||| 
2020 ||| data augmentation for transformer-based g2p. ||| zach ryan ||| mans hulden ||| 
2020 ||| grapheme-to-phoneme conversion with a multilingual transformer model. ||| omnia s. elsaadany ||| benjamin suter ||| 
2020 ||| one model to pronounce them all: multilingual grapheme-to-phoneme conversion with a transformer ensemble. ||| kaili vesik ||| muhammad abdul-mageed ||| miikka silfverberg ||| 
2021 ||| m3d-cam - a pytorch library to generate 3d attention maps for medical deep learning. ||| karol gotkowski ||| camila gonz ||| lez ||| andreas bucher ||| anirban mukhopadhyay ||| 
2021 ||| ultrasound breast lesion detection using extracted attention maps from a weakly supervised convolutional neural network. ||| dalia rodr ||| guez-salas ||| mathias seuret ||| sulaiman vesal ||| andreas maier ||| 
2020 ||| sos syphilis: smartphone application for the mapping of syphilis attention networks. ||| gustavo kleber ||| danilo alves ||| jailton de paiva ||| diego di ||| genes ||| ricardo alexsandro de medeiros valentim ||| aryel medeiros ||| 
2020 ||| two computational models for analyzing political attention in social media. ||| libby hemphill ||| angela m. sch ||| pke-gonzalez ||| 
2021 ||| ceam: the effectiveness of cyclic and ephemeral attention models of user behavior on social platforms. ||| farhan asif chowdhury ||| yozen liu ||| koustuv saha ||| nicholas vincent ||| leonardo neves ||| neil shah ||| maarten w. bos ||| 
2021 ||| sudden attention shifts on wikipedia during the covid-19 crisis. ||| manoel horta ribeiro ||| kristina gligoric ||| maxime peyrard ||| florian lemmerich ||| markus strohmaier ||| robert west ||| 
2020 ||| the effects of an informational intervention on attention to anti-vaccination content on youtube. ||| sangyeon kim ||| omer f. yalcin ||| samuel e. bestvater ||| kevin munger ||| burt l. monroe ||| bruce a. desmarais ||| 
2017 ||| what gets media attention and how media attention evolves over time: large-scale empirical evidence from 196 countries. ||| jisun an ||| haewoon kwak ||| 
2017 ||| why do men get more attention? exploring factors behind success in an online design community. ||| johannes wachs ||| aniko hannak ||| andr ||| s v ||| r ||| s ||| b ||| lint dar ||| czy ||| 
2021 ||| exercise? i thought you said 'extra fries': leveraging sentence demarcations and multi-hop attention for meme affect analysis. ||| shraman pramanick ||| md. shad akhtar ||| tanmoy chakraborty ||| 
2018 ||| couplenet: paying attention to couples with coupled attention for relationship recommendation. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2020 ||| characterizing collective attention via descriptor context: a case study of public discussions of crisis events. ||| ian stewart ||| diyi yang ||| jacob eisenstein ||| 
2018 ||| sustained attention driving task analysis based on recurrent residual neural network using eeg data. ||| yurui ming ||| yu-kai wang ||| mukesh prasad ||| dongrui wu ||| chin-teng lin ||| 
2021 ||| hierarchical fuzzy graph attention network for group recommendation. ||| ruxia liang ||| qian zhang ||| jianqiang wang ||| 
2021 ||| fuzzy explainable attention-based deep active learning on mental-health data. ||| usman ahmed ||| jerry chun-wei lin ||| gautam srivastava ||| 
2019 ||| sepsis prediction: an attention-based interpretable approach. ||| kourosh teimouri baghaei ||| shahram rahimi ||| 
2019 ||| hierarchical attention-based fuzzy neural network for subject classification of power customer service work orders. ||| gangjie zhou ||| lijun lv ||| xinlei qiao ||| lijun jin ||| 
2019 ||| fuzzattention on session-based recommender system. ||| chi-shiang wang ||| jung-hsien chiang ||| 
2019 ||| aleap: attention-based lstm with event embedding for attack projection. ||| shuhan fan ||| songyun wu ||| zhiliang wang ||| zimu li ||| jiahai yang ||| heng liu ||| xinran liu ||| 
2020 ||| att: a fault-tolerant reram accelerator for attention-based neural networks. ||| haoqiang guo ||| lu peng ||| jian zhang ||| qing chen ||| travis lecompte ||| 
2021 ||| aidetectorx: a vulnerability detector based on tcn and self-attention mechanism. ||| jin-fu chen ||| bo liu ||| saihua cai ||| weijia wang ||| shengran wang ||| 
2018 ||| analyzing distribution transformers at city scale and the impact of evs and storage. ||| john wamburu ||| stephen lee ||| prashant j. shenoy ||| david e. irwin ||| 
2020 ||| convolutional network with densely backward attention for facial expression recognition. ||| cam-hao hua ||| thien huynh-the ||| hyunseok seo ||| sungyoung lee ||| 
2019 ||| dilated lstm with attention for classification of suicide notes. ||| annika marie schoene ||| george lacey ||| alexander p. turner ||| nina dethlefs ||| 
2018 ||| patient risk assessment and warning symptom detection using deep attention-based neural networks. ||| ivan girardi ||| pengfei ji ||| an-phi nguyen ||| nora hollenstein ||| adam ivankay ||| lorenz kuhn ||| chiara marchiori ||| ce zhang ||| 
2019 ||| ontological attention ensembles for capturing semantic concepts in icd code prediction from clinical text. ||| mat ||| s falis ||| maciej pajak ||| aneta lisowska ||| patrick schrempf ||| lucas deckers ||| shadia mikhael ||| sotirios a. tsaftaris ||| alison o'neil ||| 
2020 ||| multitask learning of negation and speculation using transformers. ||| aditya khandelwal ||| benita kathleen britto ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| yana agafonova ||| alexey tikhonov ||| ivan p. yamshchikov ||| 
2020 ||| from genome to phenome: predicting multiple cancer phenotypes based on somatic genomicalterations via the genomic impact transformer. ||| yifeng tao ||| chunhui cai ||| william w. cohen ||| xinghua lu ||| 
2020 ||| multilevel self-attention model and its use on medical risk prediction. ||| xianlong zeng ||| yunyi feng ||| soheil moosavinasab ||| deborah lin ||| simon m. lin ||| chang liu ||| 
2020 ||| compressed-transformer: distilling knowledge from transformer for neural machine translation. ||| chen yuan ||| rong pan ||| 
2018 ||| multi-attention network for sentiment analysis. ||| tingting du ||| yunyin huang ||| xian wu ||| huiyou chang ||| 
2020 ||| character-level transformer-based neural machine translation. ||| nikolay banar ||| walter daelemans ||| mike kestemont ||| 
2019 ||| effect of attention adaptive personal audio deliverable system on digital signage. ||| noko kuratomo ||| kazuki yamada ||| soh masuko ||| toshimasa yamanaka ||| keiichi zempo ||| 
2019 ||| human motion denoising using attention-based bidirectional recurrent neural network. ||| seong uk kim ||| hanyoung jang ||| jongmin kim ||| 
2019 ||| structure-aware image expansion with global attention. ||| dewen guo ||| jie feng ||| bingfeng zhou ||| 
2020 ||| covr: co-located virtual reality experience sharing for facilitating joint attention via projected view of hmd users. ||| ikuo kamei ||| changyo han ||| takefumi hiraki ||| shogo fukushima ||| takeshi naemura ||| 
2021 ||| occlusion robust part-aware object classification through part attention and redundant features suppression. ||| sohee kim ||| seungkyu lee ||| 
2021 ||| learning english to chinese character: calligraphic art production based on transformer. ||| yifan jin ||| yi zhang ||| xi yang ||| 
2020 ||| dmcr-gan: adversarial denoising for monte carlo renderings with residual attention networks and hierarchical features modulation of auxiliary buffers. ||| yifan lu ||| ning xie ||| heng tao shen ||| 
2018 ||| single shot attention-based face detector. ||| chubin zhuang ||| shifeng zhang ||| xiangyu zhu ||| zhen lei ||| stan z. li ||| 
2021 ||| wavelet-based face inpainting with channel relation attention. ||| huiwen shao ||| yunlian sun ||| 
2018 ||| attention detection by learning hierarchy feature fusion on eye movement. ||| bing liu ||| peilin jiang ||| fei wang ||| xuetao zhang ||| haifan hao ||| shanglin bai ||| 
2021 ||| multi-lingual hybrid handwritten signature recognition based on deep residual attention network. ||| wanying li ||| mahpirat ||| wenxiong kang ||| alimjan aysa ||| kurban ubul ||| 
2021 ||| a deep attention transformer network for pain estimation with facial expression video. ||| haochen xu ||| manhua liu ||| 
2021 ||| one-class face anti-spoofing based on attention auto-encoder. ||| xiaobin huang ||| jingtian xia ||| linlin shen ||| 
2021 ||| an improved finger vein recognition model with a residual attention mechanism. ||| weiye liu ||| huimin lu ||| yupeng li ||| yifan wang ||| yuanyuan dang ||| 
2021 ||| attention network with gmm based feature for asv spoofing detection. ||| zhenchun lei ||| hui yu ||| yingen yang ||| minglei ma ||| 
2019 ||| global and local spatial-attention network for isolated gesture recognition. ||| qi yuan ||| jun wan ||| chi lin ||| yunan li ||| qiguang miao ||| stan z. li ||| lihua wang ||| yunxiang lu ||| 
2021 ||| rect: a recursive transformer architecture for generalizable mathematical reasoning. ||| rohan deshpande ||| jerry chen ||| isabelle lee ||| 
2020 ||| dacnn: dynamic weighted attention with multi-channel convolutional neural network for emotion recognition. ||| cheng-ta yang ||| yi-ling chen ||| 
2019 ||| attention based stack resnet for citywide traffic accident prediction. ||| zhengyang zhou ||| 
2019 ||| clustering noisy trajectories via robust deep attention auto-encoders. ||| rui zhang ||| peng xie ||| hongbo jiang ||| zhu xiao ||| chen wang ||| ling liu ||| 
2021 ||| dual sequence transformer for query-based interactive recommendation. ||| guohao cai ||| xiaoguang li ||| quanyu dai ||| gang wang ||| zhenhua dong ||| chaoliang zhang ||| xiuqiang he ||| lifeng shang ||| 
2019 ||| sequence-aware recommendation with long-term and short-term attention memory networks. ||| daochang chen ||| rui zhang ||| bo yuan ||| 
2020 ||| attention based caption augmented w2vv++ adhoc video search (avs) trecvid task. ||| rahul sharma ||| deepak mishra ||| haresh s. bhatt ||| 
2020 ||| a multimodal fusion model based on hybrid attention mechanism for gesture recognition. ||| yajie li ||| yiqiang chen ||| yang gu ||| jianquan ouyang ||| 
2020 ||| graph transformer: learning better representations for graph neural networks. ||| boyuan wang ||| lixin cui ||| lu bai ||| edwin r. hancock ||| 
2020 ||| selecting features from time series using attention-based recurrent neural networks. ||| michal myller ||| michal kawulok ||| jakub nalepa ||| 
2020 ||| predicting polypharmacy side effects through a relation-wise graph attention network. ||| vincenzo carletti ||| pasquale foggia ||| antonio greco ||| antonio roberto ||| mario vento ||| 
2021 ||| channel estimation for full-duplex ris-assisted haps backhauling with graph attention networks. ||| k ||| rsat tekbiyik ||| g ||| nes karabulut-kurt ||| chongwen huang ||| ali riza ekti ||| halim yanikomeroglu ||| 
2021 ||| neighboring-aware caching in heterogeneous edge networks by actor-attention-critic learning. ||| yiwei zhao ||| ruibin li ||| chenyang wang ||| xiaofei wang ||| victor c. m. leung ||| 
2021 ||| joint localization and radio map generation using transformer networks with limited rss samples. ||| ankur pandey ||| ryan sequeira ||| sudhir kumar ||| 
2019 ||| agrm: attention-based graph representation model for telecom fraud detection. ||| ming liu ||| jianxin liao ||| jingyu wang ||| qi qi ||| 
2019 ||| a novel attention mechanism considering decoder input for abstractive text summarization. ||| jianwei niu ||| mingsheng sun ||| joel j. p. c. rodrigues ||| xuefeng liu ||| 
2021 ||| enhancing transformer with horizontal and vertical guiding mechanisms for neural language modeling. ||| anlin qu ||| jianwei niu ||| shasha mo ||| 
2021 ||| assessment of self-attention on learned features for sound event localization and detection. ||| parthasaarathy sudarsanam ||| archontis politis ||| konstantinos drossos ||| 
2021 ||| semi-supervised sound event detection using multiscale channel attention and multiple consistency training. ||| yih-wen wang ||| chia-ping chen ||| chung-li lu ||| bo-cheng chan ||| 
2020 ||| audio tag representation guided dual attention network for acoustic scene classification. ||| ju-ho kim ||| jee-weon jung ||| hye-jin shim ||| ha-jin yu ||| 
2021 ||| many-to-many audio spectrogram tansformer: transformer for sound event localization and detection. ||| sooyoung park ||| youngho jeong ||| taejin lee ||| 
2021 ||| toward interpretable polyphonic sound event detection with attention maps based on local prototypes. ||| pablo zinemanas ||| mart ||| n rocamora ||| eduardo fonseca ||| frederic font ||| xavier serra ||| 
2021 ||| multi-scale network based on split attention for semi-supervised sound event detection. ||| xiujuan zhu ||| sun xinghao ||| 
2021 ||| transfer learning followed by transformer for automated audio captioning. ||| baekseung kim ||| hyejin won ||| il-youp kwak ||| changwon lim ||| 
2020 ||| audio captioning based on transformer and pre-trained cnn. ||| kun chen ||| yusong wu ||| ziyue wang ||| xuan zhang ||| fudong nian ||| shengchen li ||| xi shao ||| 
2018 ||| multi-level attention model for weakly supervised audio classification. ||| changsong yu ||| karim said barsim ||| qiuqiang kong ||| bin yang ||| 
2018 ||| attention-based convolutional neural networks for acoustic scene classification. ||| zhao ren ||| qiuqiang kong ||| kun qian ||| mark d. plumbley ||| bj ||| rn w. schuller ||| 
2021 ||| audio captioning transformer. ||| xinhao mei ||| xubo liu ||| qiushi huang ||| mark d. plumbley ||| wenwu wang ||| 
2019 ||| registration and analysis of a pilot's attention using a mobile eyetracking system. ||| zbigniew gomolka ||| boguslaw twarog ||| ewa zeslawska ||| damian kordos ||| 
2019 ||| hierarchical text-label integrated attention network for document classification. ||| changjin gong ||| kaize shi ||| zhendong niu ||| 
2019 ||| amnet: convolutional neural network embeded with attention mechanism for semantic segmentation. ||| baiyi shu ||| jiong mu ||| yu zhu ||| 
2019 ||| text sentiment classification based on layered attention network. ||| jinhao wu ||| kai zheng ||| jun sun ||| 
2019 ||| rational, emotional, and attentional choice models for recommender systems. ||| ameed almomani ||| cristina monreal ||| jorge sieira ||| juan gra ||| a ||| eduardo s ||| nchez ||| 
2017 ||| quantifying the effects of learning styles on attention. ||| dalila dur ||| es ||| cesar analide ||| javier bajo ||| paulo novais ||| 
2021 ||| improved multi-scale fusion of attention network for hyperspectral image classification. ||| fengqi zhang ||| lina yang ||| hailong su ||| patrick shen-pei wang ||| 
2021 ||| w-core transformer model for chinese word segmentation. ||| hai lin ||| lina yang ||| patrick shen-pei wang ||| 
2021 ||| a deep learning approach based on feature reconstruction and multi-dimensional attention mechanism for drug-drug interaction prediction. ||| jiang xie ||| jiaming ouyang ||| chang zhao ||| hongjian he ||| xin dong ||| 
2021 ||| bindtransnet: a transferable transformer-based architecture for cross-cell type dna-protein binding sites prediction. ||| zixuan wang ||| xiaoyao tan ||| beichen li ||| yuhang liu ||| qi shao ||| zijing li ||| yihan yang ||| yongqing zhang ||| 
2021 ||| litetrans: reconstruct transformer with convolution for medical image segmentation. ||| shuying xu ||| hongyan quan ||| 
2021 ||| ecg arrhythmia detection based on hidden attention residual neural network. ||| yuxia guan ||| jinrui xu ||| ning liu ||| jianxin wang ||| ying an ||| 
2021 ||| improved depression recognition using attention and multitask learning of gender recognition. ||| yang liu ||| xiaoyong lu ||| daimin shi ||| jingyi yuan ||| tao pan ||| haizhen an ||| 
2019 ||| duplicate question detection based on neural networks and multi-head attention. ||| heng zhang ||| liangyu chen ||| 
2019 ||| employing gated attention and multi-similarities to resolve document-level chinese event coreference. ||| haoyi cheng ||| peifeng li ||| qiaoming zhu ||| 
2019 ||| using mention segmentation to improve event detection with multi-head attention. ||| jiali chen ||| yu hong ||| jingli zhang ||| jianmin yao ||| 
2018 ||| a hybrid algorithm for text classification based on cnn-blstm with attention. ||| lei fu ||| zhaoxia yin ||| xin wang ||| yi liu ||| 
2021 ||| aspect-based sentiment classification with dependency relation and structured attention. ||| yuxiang jia ||| yadong wang ||| hongying zan ||| qi xie ||| yingjie yan ||| yalei liu ||| 
2017 ||| recursive annotations for attention-based neural machine translation. ||| shaolin ye ||| wu guo ||| 
2018 ||| shared representation learning with self-attention for cross-domain chinese hedge cue recognition. ||| huiwei zhou ||| shixian ning ||| zhe liu ||| zhuang liu ||| chengkun lang ||| 
2020 ||| transformer-based arabic dialect identification. ||| wanqiu lin ||| maulik c. madhavi ||| rohan kumar das ||| haizhou li ||| 
2020 ||| sentiment classification with syntactic relationship and attention for teaching evaluation texts. ||| lingling mu ||| yadi li ||| hongying zan ||| 
2018 ||| topical-relevance detection using attention-based neural network. ||| xia li ||| zhanyuan yang ||| minping chen ||| wenhe feng ||| 
2019 ||| fusion of image-text attention for transformer-based multimodal machine translation. ||| junteng ma ||| shihao qin ||| lan su ||| xia li ||| lixian xiao ||| 
2019 ||| extremely low resource text simplification with pre-trained transformer language model. ||| takumi maruyama ||| kazuhide yamamoto ||| 
2019 ||| syntax-aware transformer encoder for neural machine translation. ||| sufeng duan ||| hai zhao ||| junru zhou ||| rui wang ||| 
2020 ||| detect turn-takings in subtitle streams with semantic recall transformer encoder. ||| yuhai liang ||| qiang zhou ||| 
2021 ||| resa: relation enhanced self-attention for low-resource neural machine translation. ||| xing wu ||| shumin shi ||| heyan huang ||| 
2020 ||| enhancing attention models via multi-head collaboration. ||| huadong wang ||| mei tu ||| 
2020 ||| structurally enhanced interactive attention network for aspect-level sentiment classification. ||| chunfeng liang ||| yumeng fu ||| chengguo lv ||| 
2021 ||| vortx: volumetric 3d reconstruction with transformers for voxelwise view selection and fusion. ||| noah stier ||| alexander rich ||| pradeep sen ||| tobias h ||| llerer ||| 
2020 ||| spatial attention improves iterative 6d object pose estimation. ||| stefan stevsic ||| otmar hilliges ||| 
2020 ||| a transformer-based network for dynamic hand gesture recognition. ||| andrea d'eusanio ||| alessandro simoni ||| stefano pini ||| guido borghi ||| roberto vezzani ||| rita cucchiara ||| 
2019 ||| pairwise attention encoding for point cloud feature learning. ||| yunxiao shi ||| haoyu fang ||| jing zhu ||| yi fang ||| 
2021 ||| sceneformer: indoor scene generation with transformers. ||| xinpeng wang ||| chandan yeshwanth ||| matthias nie ||| ner ||| 
2019 ||| res3atn - deep 3d residual attention network for hand gesture recognition in videos. ||| naina dhingra ||| andreas m. kunz ||| 
2021 ||| investigating attention mechanism in 3d point cloud object detection. ||| shi qiu ||| yunfan wu ||| saeed anwar ||| chongyi li ||| 
2021 ||| air-nets: an attention-based framework for locally conditioned implicit representations. ||| simon giebenhain ||| bastian goldl ||| cke ||| 
2021 ||| attention meets geometry: geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation. ||| patrick ruhkamp ||| daoyi gao ||| hanzhi chen ||| nassir navab ||| beniamin busam ||| 
2021 ||| channel-wise attention-based network for self-supervised monocular depth estimation. ||| jiaxing yan ||| hong zhao ||| penghui bu ||| yusheng jin ||| 
2021 ||| gascn: graph attention shape completion network. ||| haojie huang ||| ziyi yang ||| robert platt ||| 
2021 ||| a spatio-temporal transformer for 3d human motion prediction. ||| emre aksan ||| manuel kaufmann ||| peng cao ||| otmar hilliges ||| 
2018 ||| detail preserving depth estimation from a single image using attention guided networks. ||| zhixiang hao ||| yu li ||| shaodi you ||| feng lu ||| 
2021 ||| named entity recognition and relation extraction for covid-19: explainable active learning with word2vec embeddings and transformer-based bert models. ||| mercedes arg ||| ello casteleiro ||| nava maroto ||| chris wroe ||| c. sevillano torrado ||| c. henson ||| jose julio des diz ||| maria jesus fernandez prieto ||| thomas furmston ||| diego maseda-fernandez ||| m. kulshrestha ||| robert stevens ||| john a. keane ||| simon peters ||| 
2020 ||| forecasting corporate financial time series using multi-phase attention recurrent neural networks. ||| shuhei yoshimi ||| koji eguchi ||| 
2020 ||| entity matching with transformer architectures - a step forward in data integration. ||| ursin brunner ||| kurt stockinger ||| 
2021 ||| accelerating transformer-based deep learning models on fpgas using column balanced block pruning. ||| hongwu peng ||| shaoyi huang ||| tong geng ||| ang li ||| weiwen jiang ||| hang liu ||| shusen wang ||| caiwen ding ||| 
2021 ||| three-dimensional memristive deep neural network with programmable attention mechanism. ||| hongyu an ||| kangjun bai ||| yang yi ||| 
2021 ||| attention analysis in flipped classroom using 1d multi-point local ternary patterns. ||| rabi shaw ||| chinmay mohanty ||| animesh pradhan ||| bidyut kr. patra ||| 
2018 ||| code-switched named entity recognition with embedding attention. ||| changhan wang ||| kyunghyun cho ||| douwe kiela ||| 
2019 ||| inriafbk drawing attention to offensive language at germeval2019. ||| michele corazza ||| stefano menini ||| elena cabrio ||| sara tonelli ||| serena villata ||| 
2019 ||| neural classification with attention assessment of the implicit-association test omt and prediction of subsequent academic success. ||| dirk johann ||| en ||| chris biemann ||| 
2021 ||| incorporating distinct translation system outputs into statistical and transformer model. ||| mani bansal ||| d. k. lobiyal ||| 
2019 ||| transmembrane topology identification by fusing evolutionary and co-evolutionary information with cascaded bidirectional transformers. ||| zhen li ||| chongming ni ||| jinbo xu ||| xin gao ||| shuguang cui ||| sheng wang ||| 
2020 ||| cross-global attention graph kernel network prediction of drug prescription. ||| hao-ren yao ||| der-chen chang ||| ophir frieder ||| wendy huang ||| i-chia liang ||| chi-feng hung ||| 
2020 ||| exam: an explainable attention-based model for covid-19 automatic diagnosis. ||| wenqi shi ||| li tong ||| yuchen zhuang ||| yuanda zhu ||| may d. wang ||| 
2017 ||| protein-protein interaction extraction using attention-based convolution neural networks. ||| hao zhang ||| mary qu yang ||| xiaoyue feng ||| william yang ||| weida tong ||| renchu guan ||| 
2020 ||| transforming the language of life: transformer neural networks for protein prediction tasks. ||| ananthan nambiar ||| maeve heflin ||| simon liu ||| sergei maslov ||| mark hopkins ||| anna m. ritz ||| 
2021 ||| transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. ||| xianlong zeng ||| simon m. lin ||| chang liu ||| 
2017 ||| interpretable predictions of clinical outcomes with an attention-based recurrent neural network. ||| ying sha ||| may d. wang ||| 
2021 ||| transformer-based named entity recognition for parsing clinical trial eligibility criteria. ||| shubo tian ||| arslan erdengasileng ||| xi yang ||| yi guo ||| yonghui wu ||| jinfeng zhang ||| jiang bian ||| zhe he ||| 
2021 ||| kgdal: knowledge graph guided double attention lstm for rolling mortality prediction for aki-d patients. ||| lucas jing liu ||| victor ortiz-soriano ||| javier a. neyra ||| jin chen ||| 
2019 ||| transmembrane topology identification by fusing evolutionary and co-evolutionary information with cascaded bidirectional transformers. ||| zhen li ||| chongming ni ||| jinbo xu ||| xin gao ||| shuguang cui ||| sheng wang ||| 
2020 ||| a deep learning framework based on spatio-temporal attention mechanism for traffic prediction. ||| jun hu ||| bo li ||| 
2019 ||| word image representation based on sequence to sequence model with attention mechanism for out-of-vocabulary keyword spotting. ||| hongxi wei ||| yanke kang ||| hui zhang ||| 
2018 ||| aspect level sentiment classification with memory network using word sentiment vectors and a new attention mechanism am-pposc. ||| xingfu wang ||| lei wang ||| ammar hawbani ||| fuyou miao ||| 
2019 ||| extract, attend, predict: aspect-based sentiment analysis with deep self-attention network. ||| yiwei lv ||| minghao hu ||| chao yang ||| yuanyan tang ||| hongjun wang ||| 
2019 ||| attention-based neural network: a novel approach for predicting the popularity of online content. ||| minh-tri nguyen ||| duong h. le ||| takuma nakajima ||| masato yoshimi ||| nam thoai ||| 
2020 ||| attention-guided multi-view stereo network for depth estimation. ||| penghui sun ||| suping wu ||| kui lin ||| 
2018 ||| head movements are correlated with other measures of visual attention at smaller spatial scales. ||| brian hu ||| ishmael johnson-bey ||| mansi sharma ||| ernst niebur ||| 
2021 ||| predicting acute kidney injury via interpretable ensemble learning and attention weighted convoutional-recurrent neural networks. ||| yu-chung peng ||| niharika shimona d'souza ||| brian bush ||| charles brown ||| archana venkataraman ||| 
2018 ||| recognizing unconstrained vietnamese handwriting by attention based encoder decoder model. ||| anh duc le ||| hung tuan nguyen ||| masaki nakagawa ||| 
2019 ||| microblog sentiment classification method based on dual attention mechanism and bidirectional lstm. ||| wenjie wei ||| yangsen zhang ||| ruixue duan ||| wen zhang ||| 
2019 ||| linguistic knowledge based on attention neural network for targeted sentiment classification. ||| chengyu du ||| pengyuan liu ||| 
2018 ||| the attention to safety issues from mainland china and taiwan. ||| shan wang ||| xinyan wang ||| 
2018 ||| attention-based bi-lstm for chinese named entity recognition. ||| kai zhang ||| weiping ren ||| yangsen zhang ||| 
2021 ||| the next 700 program transformers. ||| geoff w. hamilton ||| 
2019 ||| deep residual network with self attention improves person re-identification accuracy. ||| jean-paul ainam ||| ke qin ||| guisong liu ||| guangchun luo ||| 
2019 ||| attention based echo state network: a novel approach for fault prognosis. ||| chongdang liu ||| rong yao ||| linxuan zhang ||| yuan liao ||| 
2020 ||| weakly supervised fine-grained recognition in a segmentation-attention network. ||| nannan yu ||| wenfeng zhang ||| huanhuan cai ||| 
2021 ||| leveraging cnn and bi-lstm in indonesian g2p using transformer. ||| alief aditya rachman ||| suyanto suyanto ||| ema rachmawati ||| 
2019 ||| feature fusion attention visual question answering. ||| chunlin wang ||| jianyong sun ||| xiaolin chen ||| 
2020 ||| an attention-based deep network for ctr prediction. ||| hailong zhang ||| jinyao yan ||| yuan zhang ||| 
2019 ||| a deep attention network for chinese word segment. ||| lanxin li ||| ping gong ||| likun ji ||| 
2020 ||| the study of pso-svm and pso-grnn algorithm used in the fault pattern classification of transformer. ||| shiling zhang ||| 
2020 ||| dual pyramid attention network for high-resolution remotely sensed image change detection. ||| tengfei bao ||| chenqin fu ||| salayidin sirajidin ||| tao fang ||| hong huo ||| 
2019 ||| bidirectional-gru based on attention mechanism for aspect-level sentiment analysis. ||| penghua zhai ||| dingyi zhang ||| 
2017 ||| attention-based experience replay in deep q-learning. ||| mirza ramicic ||| andrea bonarini ||| 
2021 ||| gcn2-naa: two-stage graph convolutional networks with node-aware attention for joint entity and relation extraction. ||| weicai niu ||| quan chen ||| weiwen zhang ||| jianwen ma ||| zhongqiang hu ||| 
2021 ||| low light image enhancement in usv imaging system via u-net and attention mechanism. ||| sheng zhang ||| tianxiao cai ||| yihang chen ||| 
2021 ||| spectral-wise attention-based residual network for hyperspectral image classification. ||| yaxin chen ||| zhiqiang guo ||| jie yang ||| 
2021 ||| accelerating transformer for neural machine translation. ||| li huang ||| wenyu chen ||| hong qu ||| 
2021 ||| multi-perspective reasoning transformers. ||| dagmawi alemu moges ||| rubungo andre niyongabo ||| hong qu ||| 
2021 ||| intelligent gastric histopathology image classification using hierarchical conditional random field based attention mechanism. ||| yixin li ||| xinran wu ||| chen li ||| changhao sun ||| xiaoyan li ||| md mamunur rahaman ||| yong zhang ||| 
2017 ||| towards enhanced hierarchical attention networks in icd-9 tagging of clinical notes. ||| mary jane c. samonte ||| bobby d. gerardo ||| ruji p. medina ||| 
2021 ||| large power transformer overload detection using sound analysis. ||| nguyen cong phuong ||| 
2019 ||| multi-scale deep convolutional nets with attention model and conditional random fields for semantic image segmentation. ||| ming liu ||| caiming zhang ||| zhao zhang ||| 
2019 ||| an attention-enhanced recurrent graph convolutional network for skeleton-based action recognition. ||| xiaolu ding ||| kai yang ||| wai chen ||| 
2017 ||| global-local feature attention network with reranking strategy for image caption generation. ||| jie wu ||| siya xie ||| xinbao shi ||| yaowen chen ||| 
2021 ||| how software architects focus their attention. ||| eoin woods ||| rabih bashroush ||| 
2019 ||| modeling of transformer-rectifier sets for the energization of electrostatic precipitators using modelica. ||| mads nannestad ||| benoit bidoggia ||| zhe zhang ||| tiberiu-gabriel zsurzsan ||| kasper skriver ||| 
2018 ||| reference based on adaptive attention mechanism for image captioning. ||| shuang liu ||| liang bai ||| yanming guo ||| haoran wang ||| 
2019 ||| visual attention and haptic control: a cross-study. ||| hong xue ||| tiesong zhao ||| weiling chen ||| qian liu ||| shaohua zheng ||| chang wen chen ||| 
2019 ||| are you paying attention? detecting distracted driving in real-time. ||| maitree leekha ||| mononito goswami ||| rajiv ratn shah ||| yifang yin ||| roger zimmermann ||| 
2019 ||| related attention network for person re-identification. ||| jiali liang ||| dan zeng ||| shuaijun chen ||| qi tian ||| 
2019 ||| pedestrian detection based on spatial attention module for outdoor video surveillance. ||| xiaoyan wang ||| hai-miao hu ||| yugui zhang ||| 
2018 ||| spatial- temporal attention for image captioning. ||| junwei zhou ||| xi wang ||| jizhong han ||| songlin hu ||| hongchao gao ||| 
2019 ||| impression prediction of oral presentation using lstm and dot-product attention mechanism. ||| shengzhou yi ||| xueting wang ||| toshihiko yamasaki ||| 
2018 ||| saliency-based spatiotemporal attention for video captioning. ||| yangyu chen ||| weigang zhang ||| shuhui wang ||| liang li ||| qingming huang ||| 
2020 ||| classification of propagation path and tweets for rumor detection using graphical convolutional networks and transformer based encodings. ||| bhaye malhotra ||| dinesh kumar vishwakarma ||| 
2019 ||| video summarization using global attention with memory network and lstm. ||| dhruva sahrawat ||| mohit agarwal ||| sanchit sinha ||| aditya adhikary ||| mansi agarwal ||| rajiv ratn shah ||| roger zimmermann ||| 
2018 ||| enhanced text-guided attention model for image captioning. ||| yuanen zhou ||| zhenzhen hu ||| ye zhao ||| xueliang liu ||| richang hong ||| 
2021 ||| a-a kd: attention and activation knowledge distillation. ||| aorui gou ||| chao liu ||| heming sun ||| xiaoyang zeng ||| yibo fan ||| 
2021 ||| missformer: (in-)attention-based handling of missing observations for trajectory filtering and prediction. ||| stefan becker ||| ronny hug ||| wolfgang h ||| bner ||| michael arens ||| brendan tran morris ||| 
2020 ||| domain adaptive transfer learning on visual attention aware data augmentation for fine-grained visual categorization. ||| ashiq imran ||| vassilis athitsos ||| 
2019 ||| improving visual reasoning with attention alignment. ||| komal sharan ||| ashwinkumar ganesan ||| tim oates ||| 
2018 ||| denssiam: end-to-end densely-siamese network with self-attention model for object tracking. ||| mohamed h. abdelpakey ||| mohamed s. shehata ||| mostafa m. mohamed ||| 
2019 ||| adaptive attention model for lidar instance segmentation. ||| peixi xiong ||| xuetao hao ||| yunming shao ||| jerry yu ||| 
2021 ||| non-homogeneous haze removal through a multiple attention module architecture. ||| patricia l. su ||| rez ||| dario carpio ||| angel domingo sappa ||| 
2019 ||| cnns and transfer learning for lecture venue occupancy and student attention monitoring. ||| antonie j. smith ||| barend j. van wyk ||| shengzhi du ||| 
2021 ||| mobile application behavior recognition based on dual-domain attention and meta-learning. ||| wenjun zhang ||| 
2021 ||| analysis of network public opinion based on bilstm and self-attention fusion mechanism. ||| jianming sun ||| yibo sun ||| 
2022 ||| differentiating endogenous and exogenous attention shifts based on fixation-related potentials. ||| lisa-marie vortmann ||| moritz schult ||| felix putze ||| 
2021 ||| ruite: refining ui layout aesthetics using transformer encoder. ||| soliha rahman ||| vinoth pandian sermuga pandian ||| matthias jarke ||| 
2017 ||| evaluation of attention guiding techniques for augmented reality-based assistance in picking and assembly tasks. ||| patrick renner ||| thies pfeiffer ||| 
2018 ||| providing adaptive and personalized visual support based on behavioral tracking of children with autism for assessing reciprocity and coordination skills in a joint attention training application. ||| tiffany ya tang ||| pinata winoto ||| 
2019 ||| transformer: a database-driven approach to generating forms for constrained interaction. ||| protiva rahman ||| arnab nandi ||| 
2020 ||| multi-attention deep recurrent neural network for nursing action evaluation using wearable sensor. ||| zhihang zhong ||| chingszu lin ||| taiki ogata ||| jun ota ||| 
2018 ||| visualizing gaze direction to support video coding of social attention for children with autism spectrum disorder. ||| keita higuchi ||| soichiro matsuda ||| rie kamikubo ||| takuya enomoto ||| yusuke sugano ||| junichi yamamoto ||| yoichi sato ||| 
2020 ||| deep attention-based model for helpfulness prediction of healthcare online reviews. ||| sergio consoli ||| danilo dess ||| gianni fenu ||| mirko marras ||| 
2018 ||| webcam-based attention tracking in online learning: a feasibility study. ||| tarmo robal ||| yue zhao ||| christoph lofi ||| claudia hauff ||| 
2021 ||| multi-modal multi-scale attention guidance in cyber-physical environments. ||| guillermo reyes ||| alexandra alles ||| 
2019 ||| streaming end-to-end speech recognition with joint ctc-attention based models. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2021 ||| relaxed attention: a simple method to boost performance of end-to-end automatic speech recognition. ||| timo lohrenz ||| patrick schwarz ||| zhengyang li ||| tim fingscheidt ||| 
2019 ||| a comparison of transformer and lstm encoder decoder models for asr. ||| albert zeyer ||| parnia bahar ||| kazuki irie ||| ralf schl ||| ter ||| hermann ney ||| 
2019 ||| character-aware attention-based end-to-end speech recognition. ||| zhong meng ||| yashesh gaur ||| jinyu li ||| yifan gong ||| 
2021 ||| cross-attention conformer for context modeling in speech enhancement for asr. ||| arun narayanan ||| chung-cheng chiu ||| tom o'malley ||| quan wang ||| yanzhang he ||| 
2019 ||| end-to-end neural speaker diarization with self-attention. ||| yusuke fujita ||| naoyuki kanda ||| shota horiguchi ||| yawen xue ||| kenji nagamatsu ||| shinji watanabe ||| 
2021 ||| learning how long to wait: adaptively-constrained monotonic multihead attention for streaming asr. ||| jaeyun song ||| hajin shim ||| eunho yang ||| 
2019 ||| improved multi-stage training of online attention-based encoder-decoder models. ||| abhinav garg ||| dhananjaya gowda ||| ankur kumar ||| kwangyoun kim ||| mehul kumar ||| chanwoo kim ||| 
2017 ||| attention-based wav2text with feature transfer learning. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2019 ||| explicit alignment of text and speech encodings for attention-based end-to-end speech recognition. ||| jennifer drexler ||| james r. glass ||| 
2019 ||| cnn with phonetic attention for text-independent speaker verification. ||| tianyan zhou ||| yong zhao ||| jinyu li ||| yifan gong ||| jian wu ||| 
2021 ||| action item detection in meetings using pretrained transformers. ||| kishan sachdeva ||| joshua maynez ||| olivier siohan ||| 
2017 ||| dynamic time-aware attention to speaker roles and contexts for spoken language understanding. ||| po-chun chen ||| ta-chung chi ||| shang-yu su ||| yun-nung chen ||| 
2019 ||| speaker-aware speech-transformer. ||| zhiyun fan ||| jie li ||| shiyu zhou ||| bo xu ||| 
2021 ||| improving hybrid ctc/attention end-to-end speech recognition with pretrained acoustic and language models. ||| keqi deng ||| songjun cao ||| yike zhang ||| long ma ||| 
2019 ||| orthogonality constrained multi-head attention for keyword spotting. ||| mingu lee ||| jinkyu lee ||| hye jin jang ||| byeonggeun kim ||| wonil chang ||| kyuwoong hwang ||| 
2017 ||| a hierarchical attention based model for off-topic spontaneous spoken response detection. ||| andrey malinin ||| kate knill ||| mark j. f. gales ||| 
2019 ||| improving mandarin end-to-end speech synthesis by self-attention and learnable gaussian bias. ||| fengyu yang ||| shan yang ||| pengcheng zhu ||| pengju yan ||| lei xie ||| 
2021 ||| attention based model for segmental pronunciation error detection. ||| jose antonio lopez saenz ||| md asif jalal ||| rosanna milner ||| thomas hain ||| 
2021 ||| duality temporal-channel-frequency attention enhanced speaker representation learning. ||| li zhang ||| qing wang ||| lei xie ||| 
2021 ||| efficient conformer: progressive downsampling and grouped attention for automatic speech recognition. ||| maxime burchi ||| valentin vielzeuf ||| 
2019 ||| attention based on-device streaming speech recognition with large speech corpus. ||| kwangyoun kim ||| seokyeong jung ||| jungin lee ||| myoungji han ||| chanwoo kim ||| kyungmin lee ||| dhananjaya gowda ||| junmo park ||| sungsoo kim ||| sichen jin ||| young-yoon lee ||| jinsu yeo ||| daehyun kim ||| 
2017 ||| unwritten languages demand attention too! word discovery with encoder-decoder models. ||| marcely zanon boito ||| alexandre berard ||| aline villavicencio ||| laurent besacier ||| 
2021 ||| improving hs-dacs based streaming transformer asr with deep reinforcement learning. ||| mohan li ||| rama doddipatla ||| 
2021 ||| attention-based multi-hypothesis fusion for speech summarization. ||| takatomo kano ||| atsunori ogawa ||| marc delcroix ||| shinji watanabe ||| 
2021 ||| attention-based scaling adaptation for target speech extraction. ||| jiangyu han ||| wei rao ||| yanhua long ||| jiaen liang ||| 
2021 ||| context-aware transformer transducer for speech recognition. ||| feng-ju chang ||| jing liu ||| martin radfar ||| athanasios mouchtaris ||| maurizio omologo ||| ariya rastrow ||| siegfried kunzmann ||| 
2019 ||| hierarchical transformers for long document classification. ||| raghavendra pappagari ||| piotr zelasko ||| jes ||| s villalba ||| yishay carmiel ||| najim dehak ||| 
2019 ||| adapting pretrained transformer to lattices for spoken language understanding. ||| chao-wei huang ||| yun-nung chen ||| 
2019 ||| integrating source-channel and attention-based sequence-to-sequence models for speech recognition. ||| qiujia li ||| chao zhang ||| philip c. woodland ||| 
2019 ||| transformer asr with contextual block processing. ||| emiru tsunoo ||| yosuke kashiwagi ||| toshiyuki kumakura ||| shinji watanabe ||| 
2021 ||| distilling knowledge from ensembles of acoustic models for joint ctc-attention end-to-end speech recognition. ||| yan gao ||| titouan parcollet ||| nicholas d. lane ||| 
2019 ||| a comparative study on transformer vs rnn in speech applications. ||| shigeki karita ||| xiaofei wang ||| shinji watanabe ||| takenori yoshimura ||| wangyou zhang ||| nanxin chen ||| tomoki hayashi ||| takaaki hori ||| hirofumi inaguma ||| ziyan jiang ||| masao someki ||| nelson enrique yalta soplin ||| ryuichi yamamoto ||| 
2019 ||| attention-based speech recognition using gaze information. ||| osamu segawa ||| tomoki hayashi ||| kazuya takeda ||| 
2021 ||| multi-task learning with cross attention for keyword spotting. ||| takuya higuchi ||| anmol gupta ||| chandra dhir ||| 
2019 ||| state-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions. ||| kyu j. han ||| ramon prieto ||| tao ma ||| 
2021 ||| using self attention dnns to discover phonemic features for audio deep fake detection. ||| hira dhamyal ||| ayesha ali ||| ihsan ayyub qazi ||| agha ali raza ||| 
2021 ||| fully integrated transformer less floating gate driver for 3d power supply on chip. ||| yuske ogushi ||| satoshi matsumoto ||| 
2019 ||| transformer-less floating gate driver for 3d power soc. ||| minami nakayama ||| seiya abe ||| satoshi matsumoto ||| 
2019 ||| dependency-based self-attention for transformer nmt. ||| hiroyuki deguchi ||| akihiro tamura ||| takashi ninomiya ||| 
2021 ||| towards the application of calibrated transformers to the unsupervised estimation of question difficulty from text. ||| ekaterina loginova ||| luca benedetto ||| dries benoit ||| paolo cremonesi ||| 
2021 ||| are the multilingual models better? improving czech sentiment with transformers. ||| pavel prib ||| n ||| josef steinberger ||| 
2021 ||| on the contribution of per-icd attention mechanisms to classify health records in languages with fewer resources than english. ||| alberto blanco ||| sonja remmer ||| alicia p ||| rez ||| hercules dalianis ||| arantza casillas ||| 
2021 ||| predicting the factuality of reporting of news media using observations about user attention in their youtube channels. ||| krasimira bozhanova ||| yoan dinkov ||| ivan koychev ||| maria castaldo ||| tommaso venturini ||| preslav nakov ||| 
2019 ||| self-attention networks for intent detection. ||| sevinj yolchuyeva ||| g ||| za n ||| meth ||| b ||| lint gyires-t ||| th ||| 
2021 ||| enriching the transformer with linguistic factors for low-resource machine translation. ||| jordi armengol-estap ||| marta r. costa-juss ||| carlos escolano ||| 
2019 ||| persistence pays off: paying attention to what the lstm gating mechanism persists. ||| giancarlo d. salton ||| john d. kelleher ||| 
2019 ||| turkish tweet classification with transformer encoder. ||| atif emre y ||| ksel ||| yacsar alim t ||| rkmen ||| arzucan  ||| zg ||| r ||| berna altinel ||| 
2021 ||| can the transformer be used as a drop-in replacement for rnns in text-generating gans? ||| kevin blin ||| andrei kucharavy ||| 
2021 ||| masking and transformer-based models for hyperpartisanship detection in news. ||| javier s ||| nchez-junquera ||| paolo rosso ||| manuel montes-y-g ||| mez ||| simone paolo ponzetto ||| 
2019 ||| dependency-based relative positional encoding for transformer nmt. ||| yutaro omote ||| akihiro tamura ||| takashi ninomiya ||| 
2021 ||| transformer with syntactic position encoding for machine translation. ||| yikuan xie ||| wenyong wang ||| mingqian du ||| qing he ||| 
2019 ||| discourse-aware hierarchical attention network for extractive single-document summarization. ||| tatsuya ishigaki ||| hidetaka kamigaito ||| hiroya takamura ||| manabu okumura ||| 
2021 ||| can multilingual transformers fight the covid-19 infodemic? ||| lasitha uyangodage ||| tharindu ranasinghe ||| hansi hettiarachchi ||| 
2021 ||| on the usability of transformers-based models for a french question-answering task. ||| oralie cattan ||| christophe servan ||| sophie rosset ||| 
2021 ||| a pre-trained transformer and cnn model with joint language id and part-of-speech tagging for code-mixed social-media text. ||| suman dowlagar ||| radhika mamidi ||| 
2017 ||| real-time news summarization with adaptation to media attention. ||| andreas r ||| ckl ||| iryna gurevych ||| 
2019 ||| quasi bidirectional encoder representations from transformers for word sense disambiguation. ||| michele bevilacqua ||| roberto navigli ||| 
2021 ||| decoupled transformer for scalable inference in open-domain question answering. ||| haytham elfdaeel ||| stanislav peshterliev ||| 
2019 ||| self-attentional models application in task-oriented dialogue generation systems. ||| mansour saffar mehrjardi ||| amine trabelsi ||| osmar r. za ||| ane ||| 
2021 ||| character-based thai word segmentation with multiple attentions. ||| thodsaporn chay-intr ||| hidetaka kamigaito ||| manabu okumura ||| 
2020 ||| garnet: graph attention residual networks based on adversarial learning for 3d human pose estimation. ||| zhihua chen ||| xiaoli liu ||| bing sheng ||| ping li ||| 
2021 ||| add-net: attention u-net with dilated skip connection and dense connected decoder for retinal vessel segmentation. ||| dongjin huang ||| hao guo ||| yue zhang ||| 
2019 ||| realistic pedestrian simulation based on the environmental attention model. ||| di jiao ||| tianyu huang ||| gangyi ding ||| yiran du ||| 
2021 ||| a classification network for ocular diseases based on structure feature and visual attention. ||| yang wen ||| yupeng xu ||| kun liu ||| bin sheng ||| lei bi ||| jinman kim ||| xiangui he ||| xun xu ||| 
2021 ||| compact double attention module embedded cnn for palmprint recognition. ||| yongmin zheng ||| lunke fei ||| wei jia ||| jie wen ||| shaohua teng ||| imad rida ||| 
2020 ||| broad-classifier for remote sensing scene classification with spatial and channel-wise attention. ||| zhihua chen ||| yunna liu ||| han zhang ||| bin sheng ||| ping li ||| guangtao xue ||| 
2018 ||| open-domain question answering using feature encoded dynamic coattention networks. ||| sumedh kale ||| aniket kulkarni ||| rohan patil ||| yashodhara haribhakta ||| krishnanjan bhattacharjee ||| swati mehta ||| swathi mithran ||| ajai kumar ||| 
2021 ||| an attention-based cnn-lstm model with limb synergy for joint angles prediction. ||| chang zhu ||| quan liu ||| wei meng ||| qingsong ai ||| sheng quan xie ||| 
2021 ||| accommodating transformer onto fpga: coupling the balanced model compression and fpga-implementation optimization. ||| panjie qi ||| yuhong song ||| hongwu peng ||| shaoyi huang ||| qingfeng zhuge ||| edwin hsing-mean sha ||| 
2021 ||| hmc-tran: a tensor-core inspired hierarchical model compression for transformer-based dnns on gpu. ||| shaoyi huang ||| shiyang chen ||| hongwu peng ||| daniel manu ||| zhenglun kong ||| geng yuan ||| lei yang ||| shusen wang ||| hang liu ||| caiwen ding ||| 
2020 ||| effective piecewise cnn with attention mechanism for distant supervision on relation extraction task. ||| yuming li ||| pin ni ||| gangmin li ||| victor chang ||| 
2020 ||| can a transformer assist in scientific writing? generating semantic web paper snippets with gpt-2. ||| albert mero ||| o-pe ||| uela ||| dayana spagnuelo ||| 
2019 ||| a hybrid approach for aspect-based sentiment analysis using a lexicalized domain ontology and attentional neural models. ||| olaf wallaart ||| flavius frasincar ||| 
2021 ||| grounding dialogue systems via knowledge graph aware decoding with pre-trained transformers. ||| debanjan chaudhuri ||| md. rashad al hasan rony ||| jens lehmann ||| 
2018 ||| knowledge guided attention and inference for describing images containing unseen objects. ||| aditya mogadala ||| umanga bista ||| lexing xie ||| achim rettinger ||| 
2021 ||| retra: recurrent transformers for learning temporally contextualized knowledge graph embeddings. ||| simon werner ||| achim rettinger ||| lavdim halilaj ||| j ||| rgen l ||| ttin ||| 
2021 ||| context transformer with stacked pointer networks for conversational question answering over knowledge graphs. ||| joan plepi ||| endri kacupaj ||| kuldeep singh ||| harsh thakkar ||| jens lehmann ||| 
2021 ||| extractive summarization for explainable sentiment analysis using transformers. ||| luca bacco ||| andrea cimino ||| felice dell'orletta ||| mario merone ||| 
2021 ||| structurally guided channel attention networks: sgca-net. ||| veysi yildiz ||| jennifer g. dy ||| j. peter campbell ||| susan ostmo ||| michael f. chiang ||| stratis ioannidis ||| deniz erdogmus ||| 
2020 ||| ar-glasses-based attention guiding for complex environments: requirements, classification and evaluation. ||| patrick renner ||| thies pfeiffer ||| 
2020 ||| a three-module proposed solution to improve cognitive and social skills of students with attention deficit disorder (add) and high functioning autism (hfa): innovative technological advancements for students with neurodevelopmental disorders. ||| ourania manta ||| thelma androutsou ||| athanasios anastasiou ||| yiannis koumpouros ||| george k. matsopoulos ||| dimitrios d. koutsouris ||| 
2020 ||| chinese zero pronoun resolution based on biaffine attention mechanism. ||| likai wang ||| weiguang qu ||| tinxin wei ||| junsheng zhou ||| yanhui gu ||| bin li ||| 
2018 ||| collective attention in wechat public platform. ||| xiling xiong ||| xiaofeng li ||| xinrui wang ||| haiting wu ||| lingnan he ||| 
2020 ||| an efficient intrusion detection model combined bidirectional gated recurrent units with attention mechanism. ||| jingyi wang ||| naiyue chen ||| jinhui yu ||| yi jin ||| yidong li ||| 
2018 ||| uncovering determinants of discontinuous intention of attention to mobile line-p messages. ||| hsin-yi chiu ||| jia-sing lin ||| chien-hsing wu ||| 
2019 ||| design of efficient distribution transformer: a deep learning approach. ||| junhyun park ||| keun-ho park ||| hak-ju lee ||| bum-in shin ||| tae-ho kim ||| kyung ho park ||| ayoung jang ||| sung-chin hahn ||| jangwu jo ||| 
2019 ||| transformer design software with parallel processing. ||| junhyun park ||| keun-ho park ||| hak-ju lee ||| bum-in shin ||| tae-ho kim ||| kyung ho park ||| ayoung jang ||| sung-chin hahn ||| jangwu jo ||| 
2017 ||| video attention using multiple temporal scales coding reconstruction. ||| yu lin ||| yuanlong yu ||| 
2019 ||| a method for human parsing based on deep learning and attention mechanism. ||| rui yang ||| chaobing huang ||| 
2021 ||| efficient channel attention u-net for mesh crack detection. ||| die huang ||| jianxi yang ||| hao li ||| shixin jiang ||| 
2017 ||| an anti-power theft method for secondary circuit of energy meter current transformer. ||| sitao li ||| haibo yu ||| helong li ||| jinquan zhao ||| jianzhi liu ||| zhibin zheng ||| jing zhang ||| lixuan jia ||| 
2019 ||| driver action recognition based on attention mechanism. ||| wenhao wang ||| xiaobo lu ||| pengguo zhang ||| huibin xie ||| wenbin zeng ||| 
2021 ||| vehicle trajectory prediction based on attention mechanism and gan. ||| yi wang ||| wangqiao chen ||| chao wang ||| shuang wang ||| 
2021 ||| uav-based cross-view geo-localization fusion spatial attention mechanism and netvlad. ||| zongbao liang ||| xing liu ||| bo chen ||| yunfei yuan ||| yang song ||| haifei jiang ||| 
2021 ||| a multiscale attention mechanism based vehicle re-identification. ||| lifang du ||| chaoshun yu ||| cong shuai ||| xinlong liu ||| jianxi yang ||| yufan zhang ||| 
2021 ||| spiking transformer networks: a rate coded approach for processing sequential data. ||| etienne mueller ||| viktor studenyak ||| daniel auge ||| alois c. knoll ||| 
2018 ||| the location of partial discharge source in the power transformer with uhf sensors. ||| naifan xue ||| junjie yang ||| 
2019 ||| transformer image recognition system based on deep learning. ||| yu xu ||| tao jin ||| yunjiong xu ||| xiaofeng shi ||| shengke chen ||| wei sun ||| yang xue ||| haidong wu ||| 
2021 ||| an automatic data acquisition device for transformer oscillating switching impulse voltage tests. ||| meichun huang ||| zhaohui li ||| weiming ouyang ||| chunlei li ||| lisheng pang ||| 
2021 ||| comment text grading for chinese graduate academic dissertation using attention convolutional neural networks. ||| yupei zhang ||| yaya zhou ||| min xiao ||| xuequn shang ||| 
2018 ||| a short text semantic classification method for power grid service based on attention_gated recurrent unit (at_gru) neural network. ||| yukun cao ||| junyi chao ||| 
2019 ||| abstract model based on location attention and competition mechanism. ||| peng jiang ||| yuelin chen ||| fei qin ||| xiaodong cai ||| 
2021 ||| supervised classification of plant image based on attention mechanism. ||| jie li ||| jie yang ||| 
2021 ||| deep homography estimation based on attention mechanism. ||| shuang wang ||| feiyun yuan ||| bo chen ||| haifei jiang ||| wangqiao chen ||| yi wang ||| 
2017 ||| a hybrid method for current transformer saturation detection and compensation in smart grid. ||| guoying lin ||| qiang song ||| dingqu zhang ||| feng pan ||| lingyun wang ||| 
2019 ||| an attention convolutional neural network for forest fire smoke recognition. ||| dexiong zhang ||| yichao cao ||| guangming zhang ||| xiaobo lu ||| 
2019 ||| outlier detection for transformer's oil chromatographic data based on metric learning and the weighted local outlier factor. ||| jiafeng qin ||| yi yang ||| chao gu ||| zijing hong ||| hongyi du ||| 
2019 ||| a residual network of water scene recognition based on optimized inception module and convolutional block attention module. ||| lianming xie ||| chaobing huang ||| 
2018 ||| hierarchical gated convolutional networks with multi-head attention for text classification. ||| haizhou du ||| jingu qian ||| 
2019 ||| attention please: consider mockito when evaluating newly proposed automated program repair techniques. ||| shangwen wang ||| ming wen ||| xiaoguang mao ||| deheng yang ||| 
2019 ||| experiences of studying attention through eeg in the context of review tasks. ||| jefferson seide moll ||| ri ||| indira nurdiani ||| farnaz fotrousi ||| kai petersen ||| 
2018 ||| pixel-parallel architecture for neuromorphic smart image sensor with visual attention. ||| md jubaer hossain pantho ||| pankaj bhowmik ||| christophe bobda ||| 
2021 ||| re-transformer: a self-attention based model for machine translation. ||| huey-ing liu ||| wei-lin chen ||| 
2021 ||| attention based abstractive summarization of malayalam document. ||| sindhya k. nambiar ||| david peter s ||| sumam mary idicula ||| 
2017 ||| arabic machine transliteration using an attention-based encoder-decoder model. ||| mohamed seghir hadj ameur ||| farid meziane ||| ahmed guessoum ||| 
2020 ||| double attention-based multimodal neural machine translation with semantic image regions. ||| yuting zhao ||| mamoru komachi ||| tomoyuki kajiwara ||| chenhui chu ||| 
2020 ||| fine-grained human evaluation of transformer and recurrent approaches to neural machine translation for english-to-chinese. ||| yuying ye ||| antonio toral ||| 
2021 ||| a co-attention method based on generative adversarial networks for multi-view images. ||| qi-xian huang ||| shu-pei shi ||| guo-shiang lin ||| day-fann shen ||| hung-min sun ||| 
2019 ||| share price trend prediction using attention with lstm structure. ||| wun-syun jhang ||| shao-en gao ||| chuin-mu wang ||| ming-chu hsieh ||| 
2017 ||| classifying semantic clause types: modeling context and genre characteristics with recurrent neural networks and attention. ||| maria becker ||| michael staniek ||| vivi nastase ||| alexis palmer ||| anette frank ||| 
2021 ||| can transformer language models predict psychometric properties? ||| antonio laverghetta jr. ||| animesh nighojkar ||| jamshidbek mirzakhalov ||| john licato ||| 
2021 ||| did the cat drink the coffee? challenging transformers with generalized event knowledge. ||| paolo pedinotti ||| giulia rambelli ||| emmanuele chersoni ||| enrico santus ||| alessandro lenci ||| philippe blache ||| 
2020 ||| generalized admittance matrix model of transformers. ||| carlos s ||| nchez-l ||| pez ||| luis abraham s ||| nchez-gaspariano ||| 
2017 ||| neurofeedback training system with audiovisual stimuli for the attention state induction during cognitive processes. ||| bruno balbuena-vera ||| diego f. reyes-ramirez ||| blanca tovar-corona ||| lvaro anzueto-r ||| os ||| jos |||  gonzalo sol ||| s-villela ||| 
2019 ||| a new time-frequency attention mechanism for tdnn and cnn-lstm-tdnn, with application to language identification. ||| xiaoxiao miao ||| ian mcloughlin ||| yonghong yan ||| 
2020 ||| multi-modal attention for speech emotion recognition. ||| zexu pan ||| zhaojie luo ||| jichen yang ||| haizhou li ||| 
2021 ||| keyword transformer: a self-attention model for keyword spotting. ||| axel berg ||| mark o'connor ||| miguel tairum cruz ||| 
2020 ||| a cross-channel attention-based wave-u-net for multi-channel speech enhancement. ||| minh tri ho ||| jinyoung lee ||| bong-ki lee ||| dong hoon yi ||| hong-goo kang ||| 
2020 ||| environment sound classification using multiple feature channels and attention based deep convolutional neural network. ||| jivitesh sharma ||| ole-christoffer granmo ||| morten goodwin ||| 
2021 ||| event specific attention for polyphonic sound event detection. ||| harshavardhan sundar ||| ming sun ||| chao wang ||| 
2019 ||| improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration. ||| shigeki karita ||| nelson enrique yalta soplin ||| shinji watanabe ||| marc delcroix ||| atsunori ogawa ||| tomohiro nakatani ||| 
2018 ||| temporal transformer networks for acoustic scene classification. ||| teng zhang ||| kailai zhang ||| ji wu ||| 
2020 ||| attention to indexical information improves voice recall. ||| grant l. mcguire ||| molly babel ||| 
2018 ||| improving attention based sequence-to-sequence models for end-to-end english conversational speech recognition. ||| chao weng ||| jia cui ||| guangsen wang ||| jun wang ||| chengzhu yu ||| dan su ||| dong yu ||| 
2020 ||| memory controlled sequential self attention for sound recognition. ||| arjun pankajakshan ||| helen l. bear ||| vinod subramanian ||| emmanouil benetos ||| 
2020 ||| attention-based speaker embeddings for one-shot voice conversion. ||| tatsuma ishihara ||| daisuke saito ||| 
2018 ||| exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition. ||| ziping zhao ||| yu zheng ||| zixing zhang ||| haishuai wang ||| yiqin zhao ||| chao li ||| 
2021 ||| tvqvc: transformer based vector quantized variational autoencoder with ctc loss for voice conversion. ||| ziyi chen ||| pengyuan zhang ||| 
2020 ||| durian-sc: duration informed attention network based singing voice conversion system. ||| liqiang zhang ||| chengzhu yu ||| heng lu ||| chao weng ||| chunlei zhang ||| yusong wu ||| xiang xie ||| zijin li ||| dong yu ||| 
2021 ||| weakly-supervised speech-to-text mapping with visually connected non-parallel speech-text data using cyclic partially-aligned transformer. ||| johanes effendi ||| sakriani sakti ||| satoshi nakamura ||| 
2017 ||| advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm. ||| takaaki hori ||| shinji watanabe ||| yu zhang ||| william chan ||| 
2019 ||| speaker adaptation for attention-based end-to-end speech recognition. ||| zhong meng ||| yashesh gaur ||| jinyu li ||| yifan gong ||| 
2020 ||| attention wave-u-net for acoustic echo cancellation. ||| jung-hee kim ||| joon-hyuk chang ||| 
2020 ||| enhancing monotonic multihead attention for streaming asr. ||| hirofumi inaguma ||| masato mimura ||| tatsuya kawahara ||| 
2020 ||| speech transformer with speaker aware persistent memory. ||| yingzhu zhao ||| chongjia ni ||| cheung-chi leung ||| shafiq r. joty ||| eng siong chng ||| bin ma ||| 
2019 ||| attentive to individual: a multimodal emotion recognition network with personalized attention profile. ||| jeng-lin li ||| chi-chun lee ||| 
2021 ||| knowledge distillation for streaming transformer-transducer. ||| atsushi kojima ||| 
2020 ||| robust beam search for encoder-decoder attention based speech recognition without length bias. ||| wei zhou ||| ralf schl ||| ter ||| hermann ney ||| 
2018 ||| sequence-to-sequence neural network model with 2d attention for learning japanese pitch accents. ||| antoine bruguier ||| heiga zen ||| arkady arkhangorodsky ||| 
2021 ||| end-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention. ||| bo wei ||| meirong yang ||| tao zhang ||| xiao tang ||| xing huang ||| kyuhong kim ||| jaeyun lee ||| kiho cho ||| sung-un park ||| 
2021 ||| attention-based keyword localisation in speech using visual grounding. ||| kayode olaleye ||| herman kamper ||| 
2020 ||| a noise-aware memory-attention network architecture for regression-based speech enhancement. ||| yu-xuan wang ||| jun du ||| li chai ||| chin-hui lee ||| jia pan ||| 
2021 ||| efficient conformer with prob-sparse attention mechanism for end-to-end speech recognition. ||| xiong wang ||| sining sun ||| lei xie ||| long ma ||| 
2021 ||| sequence-to-sequence learning for deep gaussian process based speech synthesis using self-attention gp layer. ||| taiki nakamura ||| tomoki koriyama ||| hiroshi saruwatari ||| 
2019 ||| automatic hierarchical attention neural network for detecting ad. ||| yilin pan ||| bahman mirheidari ||| markus reuber ||| annalena venneri ||| daniel blackburn ||| heidi christensen ||| 
2021 ||| improving polyphone disambiguation for mandarin chinese by combining mix-pooling strategy and window-based attention. ||| junjie li ||| zhiyu zhang ||| minchuan chen ||| jun ma ||| shaojun wang ||| jing xiao ||| 
2020 ||| exploring transformers for large-scale speech recognition. ||| liang lu ||| changliang liu ||| jinyu li ||| yifan gong ||| 
2020 ||| understanding self-attention of self-supervised audio transformers. ||| shu-wen yang ||| andy t. liu ||| hung-yi lee ||| 
2021 ||| pilot: introducing transformers for probabilistic sound event localization. ||| christopher schymura ||| benedikt t. b ||| nninghoff ||| tsubasa ochiai ||| marc delcroix ||| keisuke kinoshita ||| tomohiro nakatani ||| shoko araki ||| dorothea kolossa ||| 
2020 ||| ecapa-tdnn: emphasized channel attention, propagation and aggregation in tdnn based speaker verification. ||| brecht desplanques ||| jenthe thienpondt ||| kris demuynck ||| 
2019 ||| multi-stride self-attention for speech recognition. ||| kyu j. han ||| jing huang ||| yun tang ||| xiaodong he ||| bowen zhou ||| 
2020 ||| transformer vq-vae for unsupervised unit discovery and speech synthesis: zerospeech 2020 challenge. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| investigating methods to improve language model integration for attention-based encoder-decoder asr models. ||| mohammad zeineldeen ||| aleksandr glushko ||| wilfried michel ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2020 ||| tmt: a transformer-based modal translator for improving multimodal sequence representations in audio visual scene-aware dialog. ||| wubo li ||| dongwei jiang ||| wei zou ||| xiangang li ||| 
2019 ||| online hybrid ctc/attention architecture for end-to-end speech recognition. ||| haoran miao ||| gaofeng cheng ||| pengyuan zhang ||| ta li ||| yonghong yan ||| 
2021 ||| advanced long-context end-to-end speech recognition using context-expanded transformers. ||| takaaki hori ||| niko moritz ||| chiori hori ||| jonathan le roux ||| 
2020 ||| enhancing monotonicity for robust autoregressive transformer tts. ||| xiangyu liang ||| zhiyong wu ||| runnan li ||| yanqing liu ||| sheng zhao ||| helen meng ||| 
2021 ||| end-to-end speaker-attributed asr with transformer. ||| naoyuki kanda ||| guoli ye ||| yashesh gaur ||| xiaofei wang ||| zhong meng ||| zhuo chen ||| takuya yoshioka ||| 
2018 ||| multi-channel attention for end-to-end speech recognition. ||| stefan braun ||| daniel neil ||| jithendar anumula ||| enea ceolini ||| shih-chii liu ||| 
2021 ||| transcribing paralinguistic acoustic cues to target language text in transformer-based speech-to-text translation. ||| hirotaka tokuyama ||| sakriani sakti ||| katsuhito sudoh ||| satoshi nakamura ||| 
2019 ||| attention-based word vector prediction with lstms and its application to the oov problem in asr. ||| alejandro coucheiro-limeres ||| fernando fern ||| ndez mart ||| nez ||| rub ||| n san segundo ||| javier ferreiros l ||| pez ||| 
2020 ||| environmental sound classification with parallel temporal-spectral attention. ||| helin wang ||| yuexian zou ||| dading chong ||| wenwu wang ||| 
2021 ||| modular multi-modal attention network for alzheimer's disease detection using patient audio and language data. ||| ning wang ||| yupeng cao ||| shuai hao ||| zongru shao ||| k. p. subbalakshmi ||| 
2018 ||| multi-task learning with augmentation strategy for acoustic-to-word attention-based encoder-decoder speech recognition. ||| takafumi moriya ||| sei ueno ||| yusuke shinohara ||| marc delcroix ||| yoshikazu yamaguchi ||| yushi aono ||| 
2020 ||| end-to-end text-to-speech synthesis with unaligned multiple language units based on attention. ||| masashi aso ||| shinnosuke takamichi ||| hiroshi saruwatari ||| 
2020 ||| attention and encoder-decoder based models for transforming articulatory movements at different speaking rates. ||| abhayjeet singh ||| aravind illa ||| prasanta kumar ghosh ||| 
2018 ||| an investigation of convolution attention based models for multilingual speech synthesis of indian languages. ||| pallavi baljekar ||| sai krishna rallabandi ||| alan w. black ||| 
2021 ||| lightweight causal transformer with local self-attention for real-time speech enhancement. ||| koen oostermeijer ||| qing wang ||| jun du ||| 
2021 ||| hierarchical context-aware transformers for non-autoregressive text to speech. ||| jae-sung bae ||| taejun bak ||| young-sun joo ||| hoon-young cho ||| 
2021 ||| federated learning with dynamic transformer for text to speech. ||| zhenhou hong ||| jianzong wang ||| xiaoyang qu ||| jie liu ||| chendong zhao ||| jing xiao ||| 
2019 ||| using attention networks and adversarial augmentation for styrian dialect continuous sleepiness and baby sound recognition. ||| sung-lin yeh ||| gao-yi chao ||| bo-hao su ||| yu-lin huang ||| meng-han lin ||| yin-chun tsai ||| yu-wen tai ||| zheng-chi lu ||| chieh-yu chen ||| tsung-ming tai ||| chiu-wang tseng ||| cheng-kuang lee ||| chi-chun lee ||| 
2020 ||| multi-encoder-decoder transformer for code-switching speech recognition. ||| xinyuan zhou ||| emre yilmaz ||| yanhua long ||| yijie li ||| haizhou li ||| 
2021 ||| multi-encoder learning and stream fusion for transformer-based end-to-end automatic speech recognition. ||| timo lohrenz ||| zhengyang li ||| tim fingscheidt ||| 
2021 ||| detection of lexical stress errors in non-native (l2) english with data augmentation and attention. ||| daniel korzekwa ||| roberto barra-chicote ||| szymon zaporowski ||| grzegorz beringer ||| jaime lorenzo-trueba ||| alicja serafinowicz ||| jasha droppo ||| thomas drugman ||| bozena kostek ||| 
2021 ||| avatr: one-shot speaker extraction with transformers. ||| shell xu hu ||| md rifat arefin ||| viet-nhat nguyen ||| alish dipani ||| xaq pitkow ||| andreas savas tolias ||| 
2021 ||| knowledge distillation from bert transformer to speech transformer for intent classification. ||| yidi jiang ||| bidisha sharma ||| maulik c. madhavi ||| haizhou li ||| 
2021 ||| stochastic attention head removal: a simple and effective method for improving transformer based asr models. ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2021 ||| stableemit: selection probability discount for reducing emission latency of streaming monotonic attention asr. ||| hirofumi inaguma ||| tatsuya kawahara ||| 
2020 ||| gated multi-head attention pooling for weakly labelled audio tagging. ||| sixin hong ||| yuexian zou ||| wenwu wang ||| 
2019 ||| detecting mismatch between speech and transcription using cross-modal attention. ||| qiang huang ||| thomas hain ||| 
2019 ||| conversational emotion analysis via attention mechanisms. ||| zheng lian ||| jianhua tao ||| bin liu ||| jian huang ||| 
2021 ||| tdca-net: time-domain channel attention network for depression detection. ||| cong cai ||| mingyue niu ||| bin liu ||| jianhua tao ||| xuefei liu ||| 
2017 ||| directing attention during perceptual training: a preliminary study of phonetic learning in southern min by mandarin speakers. ||| ying chen ||| eric pederson ||| 
2019 ||| large margin training for attention based end-to-end speech recognition. ||| peidong wang ||| jia cui ||| chao weng ||| dong yu ||| 
2018 ||| encoder transfer for attention-based acoustic-to-word speech recognition. ||| sei ueno ||| takafumi moriya ||| masato mimura ||| shinsuke sakai ||| yusuke shinohara ||| yoshikazu yamaguchi ||| yushi aono ||| tatsuya kawahara ||| 
2021 ||| automatic lip-reading with hierarchical pyramidal convolution and self-attention for image sequences with no word boundaries. ||| hang chen ||| jun du ||| yu hu ||| li-rong dai ||| bao-cai yin ||| chin-hui lee ||| 
2020 ||| the implication of sound level on spatial selective auditory attention for cochlear implant users: behavioral and electrophysiological measurement. ||| sara akbarzadeh ||| sungmin lee ||| chin-tuan tan ||| 
2019 ||| a saliency-based attention lstm model for cognitive load classification from speech. ||| ascensi ||| n gallardo-antol ||| n ||| juan manuel montero ||| 
2019 ||| multi-scale time-frequency attention for acoustic event detection. ||| jingyang zhang ||| wenhao ding ||| jintao kang ||| liang he ||| 
2020 ||| dual attention in time and frequency domain for voice activity detection. ||| joohyung lee ||| youngmoon jung ||| hoirin kim ||| 
2019 ||| self attention in variational sequential learning for summarization. ||| jen-tzung chien ||| chun-wei wang ||| 
2019 ||| multi-stream network with temporal attention for environmental sound classification. ||| xinyu li ||| venkata chebiyyam ||| katrin kirchhoff ||| 
2019 ||| multi-task multi-resolution char-to-bpe cross-attention decoder for end-to-end speech recognition. ||| dhananjaya gowda ||| abhinav garg ||| kwangyoun kim ||| mehul kumar ||| chanwoo kim ||| 
2021 ||| contextualized attention-based knowledge transfer for spoken conversational question answering. ||| chenyu you ||| nuo chen ||| yuexian zou ||| 
2018 ||| an interlocutor-modulated attentional lstm for differentiating between subgroups of autism spectrum disorder. ||| yun-shao lin ||| susan shur-fen gau ||| chi-chun lee ||| 
2019 ||| the influence of distraction on speech processing: how selective is selective attention? ||| sandra i. parhammer ||| miriam ebersberg ||| jenny tippmann ||| katja st ||| rk ||| andreas opitz ||| barbara hinger ||| sonja rossi ||| 
2019 ||| attention model for articulatory features detection. ||| ievgen karaulov ||| dmytro tkanov ||| 
2021 ||| online compressive transformer for end-to-end speech recognition. ||| chi-hang leong ||| yu-han huang ||| jen-tzung chien ||| 
2020 ||| end-to-end neural transformer based spoken language understanding. ||| martin radfar ||| athanasios mouchtaris ||| siegfried kunzmann ||| 
2021 ||| optimizing latency for online video captioning using audio-visual transformers. ||| chiori hori ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| removing bias with residual mixture of multi-view attention for speech emotion recognition. ||| md asif jalal ||| rosanna milner ||| thomas hain ||| roger k. moore ||| 
2018 ||| conversational analysis using utterance-level attention-based bidirectional recurrent neural networks. ||| chandrakant bothe ||| sven magg ||| cornelius weber ||| stefan wermter ||| 
2019 ||| transformer based grapheme-to-phoneme conversion. ||| sevinj yolchuyeva ||| g ||| za n ||| meth ||| b ||| lint gyires-t ||| th ||| 
2020 ||| weakly supervised training of hierarchical attention networks for speaker identification. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2020 ||| the effect of language dominance on the selective attention of segments and tones in urdu-cantonese speakers. ||| yi liu ||| jinghong ning ||| 
2017 ||| jointly trained sequential labeling and classification by sparse attention neural networks. ||| mingbo ma ||| kai zhao ||| liang huang ||| bing xiang ||| bowen zhou ||| 
2018 ||| triplet network with attention for speaker diarization. ||| huan song ||| megan m. willi ||| jayaraman j. thiagarajan ||| visar berisha ||| andreas spanias ||| 
2021 ||| self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition. ||| rong gong ||| carl quillen ||| dushyant sharma ||| andrew goderre ||| jos |||  la ||| nez ||| ljubomir milanovic ||| 
2020 ||| empirical interpretation of speech emotion perception with attention based model for speech emotion recognition. ||| md. asif jalal ||| rosanna milner ||| thomas hain ||| 
2020 ||| streaming transformer-based acoustic models using self-attention with augmented memory. ||| chunyang wu ||| yongqiang wang ||| yangyang shi ||| ching-feng yeh ||| frank zhang ||| 
2021 ||| ast: audio spectrogram transformer. ||| yuan gong ||| yu-an chung ||| james r. glass ||| 
2019 ||| language modeling with deep transformers. ||| kazuki irie ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| real-time multi-channel speech enhancement based on neural network masking with attention model. ||| cheng xue ||| weilong huang ||| weiguang chen ||| jinwei feng ||| 
2017 ||| online adaptation of an attention-based neural network for natural language generation. ||| matthieu riou ||| bassam jabaian ||| st ||| phane huet ||| fabrice lef ||| vre ||| 
2020 ||| weak-attention suppression for transformer based speech recognition. ||| yangyang shi ||| yongqiang wang ||| chunyang wu ||| christian fuegen ||| frank zhang ||| duc le ||| ching-feng yeh ||| michael l. seltzer ||| 
2020 ||| congruent audiovisual speech enhances cortical envelope tracking during auditory selective attention. ||| zhen fu ||| jing chen ||| 
2017 ||| attention based cldnns for short-duration acoustic scene classification. ||| jinxi guo ||| ning xu ||| li-jia li ||| abeer alwan ||| 
2019 ||| latent topic attention for domain classification. ||| peisong huang ||| peijie huang ||| wencheng ai ||| jiande ding ||| jinchuan zhang ||| 
2020 ||| conversational emotion recognition using self-attention mechanisms and graph neural networks. ||| zheng lian ||| jianhua tao ||| bin liu ||| jian huang ||| zhanlei yang ||| rongjun li ||| 
2020 ||| self-attention encoding and pooling for speaker recognition. ||| pooyan safari ||| miquel india ||| javier hernando ||| 
2020 ||| all-in-one transformer: unifying speech recognition, audio tagging, and event detection. ||| niko moritz ||| gordon wichern ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| voice transformer network: sequence-to-sequence voice conversion using transformer with text-to-speech pretraining. ||| wen-chin huang ||| tomoki hayashi ||| yi-chiao wu ||| hirokazu kameoka ||| tomoki toda ||| 
2021 ||| speech emotion recognition based on attention weight correction using word-level confidence measure. ||| jennifer santoso ||| takeshi yamada ||| shoji makino ||| kenkichi ishizuka ||| takekatsu hiramura ||| 
2020 ||| attention-driven projections for soundscape classification. ||| dhanunjaya varma devalraju ||| h. muralikrishna ||| padmanabhan rajan ||| dileep aroor dinesh ||| 
2021 ||| rapid speaker adaptation for conformer transducer: attention and bias are all you need. ||| yan huang ||| guoli ye ||| jinyu li ||| yifan gong ||| 
2017 ||| attention networks for modeling behaviors in addiction counseling. ||| james gibson ||| dogan can ||| panayiotis g. georgiou ||| david c. atkins ||| shrikanth s. narayanan ||| 
2020 ||| mlnet: an adaptive multiple receptive-field attention neural network for voice activity detection. ||| zhenpeng zheng ||| jianzong wang ||| ning cheng ||| jian luo ||| jing xiao ||| 
2020 ||| speaker-utterance dual attention for speaker and utterance verification. ||| tianchi liu ||| rohan kumar das ||| maulik c. madhavi ||| shengmei shen ||| haizhou li ||| 
2017 ||| attentional factors in listeners' uptake of gesture cues during speech processing. ||| raheleh saryazdi ||| craig g. chambers ||| 
2020 ||| affective conditioning on hierarchical attention networks applied to depression detection from transcribed clinical interviews. ||| danai xezonaki ||| georgios paraskevopoulos ||| alexandros potamianos ||| shrikanth narayanan ||| 
2021 ||| streaming end-to-end speech recognition for hybrid rnn-t/attention architecture. ||| takafumi moriya ||| tomohiro tanaka ||| takanori ashihara ||| tsubasa ochiai ||| hiroshi sato ||| atsushi ando ||| ryo masumura ||| marc delcroix ||| taichi asami ||| 
2020 ||| parallel rescoring with transformer for streaming on-device speech recognition. ||| wei li ||| james qin ||| chung-cheng chiu ||| ruoming pang ||| yanzhang he ||| 
2021 ||| multi-channel transformer transducer for speech recognition. ||| feng-ju chang ||| martin radfar ||| athanasios mouchtaris ||| maurizio omologo ||| 
2021 ||| noise robust acoustic modeling for single-channel speech recognition based on a stream-wise transformer architecture. ||| masakiyo fujimoto ||| hisashi kawai ||| 
2019 ||| spatio-temporal attention pooling for audio scene classification. ||| huy phan ||| oliver y. ch ||| n ||| lam dang pham ||| philipp koch ||| maarten de vos ||| ian mcloughlin ||| alfred mertins ||| 
2020 ||| multi-stream attention-based blstm with feature segmentation for speech emotion recognition. ||| yuya chiba ||| takashi nose ||| akinori ito ||| 
2020 ||| improved hybrid streaming asr with transformer language models. ||| pau baquero-arnal ||| javier jorge ||| adri |||  gim ||| nez ||| joan albert silvestre-cerd ||| javier iranzo-s ||| nchez ||| albert sanch ||| s ||| jorge civera ||| alfons juan ||| 
2019 ||| deep attention gated dilated temporal convolutional networks with intra-parallel convolutional modules for end-to-end monaural speech separation. ||| ziqiang shi ||| huibin lin ||| liu liu ||| rujie liu ||| jiqing han ||| anyan shi ||| 
2019 ||| few-shot audio classification with attentional graph neural networks. ||| shilei zhang ||| yong qin ||| kewei sun ||| yonghua lin ||| 
2020 ||| transformer-based long-context end-to-end speech recognition. ||| takaaki hori ||| niko moritz ||| chiori hori ||| jonathan le roux ||| 
2017 ||| gaussian prediction based attention for online end-to-end speech recognition. ||| junfeng hou ||| shiliang zhang ||| li-rong dai ||| 
2018 ||| analysing the focus of a hierarchical attention network: the importance of enjambments when classifying post-modern poetry. ||| timo baumann ||| hussein hussein ||| burkhard meyer-sickendiek ||| 
2020 ||| improving transformer-based speech recognition with unsupervised pre-training and multi-task semantic knowledge learning. ||| song li ||| lin li ||| qingyang hong ||| lingling liu ||| 
2020 ||| speech-xlnet: unsupervised acoustic model pretraining for self-attention networks. ||| xingchen song ||| guangsen wang ||| yiheng huang ||| zhiyong wu ||| dan su ||| helen meng ||| 
2020 ||| exploration of audio quality assessment and anomaly localisation using attention models. ||| qiang huang ||| thomas hain ||| 
2021 ||| residual echo and noise cancellation with feature attention module and multi-domain loss function. ||| jianjun gu ||| longbiao cheng ||| xingwei sun ||| junfeng li ||| yonghong yan ||| 
2020 ||| speaker identification for household scenarios with self-attention and adversarial training. ||| ruirui li ||| jyun-yu jiang ||| xian wu ||| chu-cheng hsieh ||| andreas stolcke ||| 
2020 ||| multispeech: multi-speaker text to speech with transformer. ||| mingjian chen ||| xu tan ||| yi ren ||| jin xu ||| hao sun ||| sheng zhao ||| tao qin ||| 
2020 ||| eeg-based short-time auditory attention detection using multi-task deep learning. ||| zhuo zhang ||| gaoyan zhang ||| jianwu dang ||| shuang wu ||| di zhou ||| longbiao wang ||| 
2020 ||| transformer with bidirectional decoder for speech recognition. ||| xi chen ||| songyang zhang ||| dandan song ||| peng ouyang ||| shouyi yin ||| 
2020 ||| durian: duration informed attention network for speech synthesis. ||| chengzhu yu ||| heng lu ||| na hu ||| meng yu ||| chao weng ||| kun xu ||| peng liu ||| deyi tuo ||| shiyin kang ||| guangzhi lei ||| dan su ||| dong yu ||| 
2020 ||| advancing multiple instance learning with attention modeling for categorical speech emotion recognition. ||| shuiyang mao ||| p. c. ching ||| c.-c. jay kuo ||| tan lee ||| 
2021 ||| cross-modal transformer-based neural correction models for automatic speech recognition. ||| tomohiro tanaka ||| ryo masumura ||| mana ihori ||| akihiko takashima ||| takafumi moriya ||| takanori ashihara ||| shota orihashi ||| naoki makishima ||| 
2021 ||| visual transformers for primates classification and covid detection. ||| steffen illium ||| robert m ||| ller ||| andreas sedlmeier ||| claudia linnhoff-popien ||| 
2020 ||| self-and-mixed attention decoder with deep acoustic structure for transformer-based lvcsr. ||| xinyuan zhou ||| grandee lee ||| emre yilmaz ||| yanhua long ||| jiaen liang ||| haizhou li ||| 
2020 ||| cross attention with monotonic alignment for speech transformer. ||| yingzhu zhao ||| chongjia ni ||| cheung-chi leung ||| shafiq r. joty ||| eng siong chng ||| bin ma ||| 
2020 ||| end-to-end asr with adaptive span self-attention. ||| xuankai chang ||| aswin shanmugam subramanian ||| pengcheng guo ||| shinji watanabe ||| yuya fujita ||| motoi omachi ||| 
2020 ||| ctc-synchronous training for monotonic attention model. ||| hirofumi inaguma ||| masato mimura ||| tatsuya kawahara ||| 
2019 ||| phonetically-aware embeddings, wide residual networks with time-delay neural networks and self attention models for the 2018 nist speaker recognition evaluation. ||| ignacio vi ||| als ||| dayana ribas ||| victoria mingote ||| jorge llombart ||| pablo gimeno ||| antonio miguel ||| alfonso ortega gim ||| nez ||| eduardo lleida ||| 
2021 ||| cough-based covid-19 detection with contextual attention convolutional neural networks and gender information. ||| adria mallol-ragolta ||| helena cuesta ||| emilia g ||| mez ||| bj ||| rn w. schuller ||| 
2020 ||| self-distillation for improving ctc-transformer-based asr systems. ||| takafumi moriya ||| tsubasa ochiai ||| shigeki karita ||| hiroshi sato ||| tomohiro tanaka ||| takanori ashihara ||| ryo masumura ||| yusuke shinohara ||| marc delcroix ||| 
2018 ||| stream attention for distributed multi-microphone speech recognition. ||| xiaofei wang ||| ruizhi li ||| hynek hermansky ||| 
2020 ||| a transformer-based audio captioning model with keyword estimation. ||| yuma koizumi ||| ryo masumura ||| kyosuke nishida ||| masahiro yasuda ||| shoichiro saito ||| 
2020 ||| temporal attention convolutional network for speech emotion recognition with latent representation. ||| jiaxing liu ||| zhilei liu ||| longbiao wang ||| yuan gao ||| lili guo ||| jianwu dang ||| 
2021 ||| estimating articulatory movements in speech production with transformer networks. ||| sathvik udupa ||| anwesha roy ||| abhayjeet singh ||| aravind illa ||| prasanta kumar ghosh ||| 
2021 ||| nisqa: a deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. ||| gabriel mittag ||| babak naderi ||| assmaa chehadi ||| sebastian m ||| ller ||| 
2019 ||| robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts. ||| mutian he ||| yan deng ||| lei he ||| 
2020 ||| conformer: convolution-augmented transformer for speech recognition. ||| anmol gulati ||| james qin ||| chung-cheng chiu ||| niki parmar ||| yu zhang ||| jiahui yu ||| wei han ||| shibo wang ||| zhengdong zhang ||| yonghui wu ||| ruoming pang ||| 
2020 ||| identify speakers in cocktail parties with end-to-end attention. ||| junzhe zhu ||| mark hasegawa-johnson ||| leda sari ||| 
2019 ||| bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. ||| guan-lin chao ||| ian r. lane ||| 
2019 ||| attention based hybrid i-vector blstm model for language recognition. ||| bharat padi ||| anand mohan ||| sriram ganapathy ||| 
2018 ||| monaural multi-talker speech recognition with attention mechanism and gated convolutional networks. ||| xuankai chang ||| yanmin qian ||| dong yu ||| 
2019 ||| very deep self-attention networks for end-to-end speech recognition. ||| ngoc-quan pham ||| thai-son nguyen ||| jan niehues ||| markus m ||| ller ||| alex waibel ||| 
2021 ||| temporal convolutional network with frequency dimension adaptive attention for speech enhancement. ||| qiquan zhang ||| qi song ||| aaron nicolson ||| tian lan ||| haizhou li ||| 
2018 ||| forward-backward attention decoder. ||| masato mimura ||| shinsuke sakai ||| tatsuya kawahara ||| 
2020 ||| hybrid transformer/ctc networks for hardware efficient voice triggering. ||| saurabh adya ||| vineet garg ||| siddharth sigtia ||| pramod simha ||| chandra dhir ||| 
2018 ||| self-assessed affect recognition using fusion of attentional blstm and static acoustic features. ||| bo-hao su ||| sung-lin yeh ||| ming-ya ko ||| huan-yu chen ||| shun-chang zhong ||| jeng-lin li ||| chi-chun lee ||| 
2021 ||| serialized multi-layer multi-head attention for neural speaker embedding. ||| hongning zhu ||| kong aik lee ||| haizhou li ||| 
2018 ||| improved training of end-to-end attention models for speech recognition. ||| albert zeyer ||| kazuki irie ||| ralf schl ||| ter ||| hermann ney ||| 
2018 ||| attention-based sequence classification for affect detection. ||| cristina gorrostieta ||| richard brutti ||| kye taylor ||| avi shapiro ||| joseph moran ||| ali azarbayejani ||| john kane ||| 
2019 ||| an online attention-based model for speech recognition. ||| ruchao fan ||| pan zhou ||| wei chen ||| jia jia ||| gang liu ||| 
2019 ||| a hierarchical attention network-based approach for depression detection from transcribed clinical interviews. ||| adria mallol-ragolta ||| ziping zhao ||| lukas stappen ||| nicholas cummins ||| bj ||| rn w. schuller ||| 
2021 ||| graph attention networks for anti-spoofing. ||| hemlata tak ||| jee-weon jung ||| jose patino ||| massimiliano todisco ||| nicholas w. d. evans ||| 
2020 ||| low latency auditory attention detection with common spatial pattern analysis of eeg signals. ||| siqi cai ||| enze su ||| yonghao song ||| longhan xie ||| haizhou li ||| 
2020 ||| attention forcing for speech synthesis. ||| qingyun dou ||| joshua efiong ||| mark j. f. gales ||| 
2019 ||| learning how to listen: a temporal-frequential attention model for sound event detection. ||| yu-han shen ||| ke-xin he ||| wei-qiang zhang ||| 
2021 ||| out-of-vocabulary words detection with attention and ctc alignments in an end-to-end asr system. ||| ekaterina egorova ||| hari krishna vydana ||| luk ||| s burget ||| jan cernock ||| 
2019 ||| attention-enhanced connectionist temporal classification for discrete speech emotion recognition. ||| ziping zhao ||| zhongtian bao ||| zixing zhang ||| nicholas cummins ||| haishuai wang ||| bj ||| rn w. schuller ||| 
2021 ||| streaming transformer for hardware efficient voice trigger detection and false trigger mitigation. ||| vineet garg ||| wonil chang ||| siddharth sigtia ||| saurabh adya ||| pramod simha ||| pranay dighe ||| chandra dhir ||| 
2020 ||| multi-task network for noise-robust keyword spotting and speaker verification using ctc-based soft vad and global query attention. ||| myunghun jung ||| youngmoon jung ||| jahyun goo ||| hoirin kim ||| 
2019 ||| predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models. ||| ricardo kleinlein ||| cristina luna jim ||| nez ||| juan manuel montero ||| zoraida callejas ||| fernando fern ||| ndez mart ||| nez ||| 
2020 ||| attentron: few-shot text-to-speech utilizing attention-based variable-length embedding. ||| seungwoo choi ||| seungju han ||| dongyoung kim ||| sungjoo ha ||| 
2017 ||| prosodic analysis of attention-drawing speech. ||| carlos toshinori ishi ||| jun arai ||| norihiro hagita ||| 
2021 ||| t5g2p: using text-to-text transfer transformer for grapheme-to-phoneme conversion. ||| mark ||| ta rez ||| ckov ||| jan svec ||| daniel tihelka ||| 
2019 ||| improved end-to-end speech emotion recognition using self attention mechanism and multitask learning. ||| yuanchao li ||| tianyu zhao ||| tatsuya kawahara ||| 
2019 ||| investigation of transformer based spelling correction model for ctc-based end-to-end mandarin speech recognition. ||| shiliang zhang ||| ming lei ||| zhijie yan ||| 
2018 ||| unsupervised word segmentation from speech with attention. ||| pierre godard ||| marcely zanon boito ||| lucas ondel ||| alexandre berard ||| fran ||| ois yvon ||| aline villavicencio ||| laurent besacier ||| 
2019 ||| end-to-end multi-channel speech enhancement using inter-channel time-restricted attention on raw waveform. ||| hyeon seung lee ||| hyung yong kim ||| woo hyun kang ||| jeunghun kim ||| nam soo kim ||| 
2020 ||| end-to-end keyword search based on attention and energy scorer for low resource languages. ||| zeyu zhao ||| wei-qiang zhang ||| 
2021 ||| end-to-end neural diarization: from transformer to conformer. ||| yi-chieh liu ||| eunjung han ||| chul lee ||| andreas stolcke ||| 
2020 ||| peking opera synthesis via duration informed attention network. ||| yusong wu ||| shengchen li ||| chengzhu yu ||| heng lu ||| chao weng ||| liqiang zhang ||| dong yu ||| 
2019 ||| variational attention using articulatory priors for generating code mixed speech using monolingual corpora. ||| sai krishna rallabandi ||| alan w. black ||| 
2018 ||| syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese. ||| shiyu zhou ||| linhao dong ||| shuang xu ||| bo xu ||| 
2018 ||| self-attentional acoustic models. ||| matthias sperber ||| jan niehues ||| graham neubig ||| sebastian st ||| ker ||| alex waibel ||| 
2021 ||| detection and analysis of attention errors in sequence-to-sequence text-to-speech. ||| cassia valentini-botinhao ||| simon king ||| 
2020 ||| speaker adaptive training for speech recognition based on attention-over-attention mechanism. ||| genshun wan ||| jia pan ||| qingran wang ||| jianqing gao ||| zhongfu ye ||| 
2021 ||| dropout regularization for self-supervised learning of transformer encoder speech representation. ||| jian luo ||| jianzong wang ||| ning cheng ||| jing xiao ||| 
2019 ||| self-attention for speech emotion recognition. ||| lorenzo tarantino ||| philip n. garner ||| alexandros lazaridis ||| 
2021 ||| simulating reading mistakes for child speech transformer-based phone recognition. ||| lucile gelin ||| thomas pellegrini ||| julien pinquier ||| morgane daniel ||| 
2019 ||| individual differences in implicit attention to phonetic detail in speech perception. ||| natalie lewandowski ||| daniel duran ||| 
2020 ||| atss-net: target speaker separation via attention-based neural network. ||| tingle li ||| qingjian lin ||| yuanyuan bao ||| ming li ||| 
2021 ||| transformer based end-to-end mispronunciation detection and diagnosis. ||| minglin wu ||| kun li ||| wai-kim leung ||| helen meng ||| 
2019 ||| adapting transformer to end-to-end spoken language translation. ||| mattia antonino di gangi ||| matteo negri ||| marco turchi ||| 
2017 ||| attention-based lstm with multi-task learning for distant speech recognition. ||| yu zhang ||| pengyuan zhang ||| yonghong yan ||| 
2021 ||| transformer-based asr incorporating time-reduction layer and fine-tuning with self-knowledge distillation. ||| md. akmal haidar ||| chao xing ||| mehdi rezagholizadeh ||| 
2019 ||| an analysis of local monotonic attention variants. ||| andr |||  merboldt ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| optimally encoding inductive biases into the transformer improves end-to-end speech translation. ||| piyush vyas ||| anastasia kuznetsova ||| donald s. williamson ||| 
2020 ||| group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition. ||| pengfei liu ||| kun li ||| helen meng ||| 
2021 ||| neural speaker extraction with speaker-speech cross-attention network. ||| wupeng wang ||| chenglin xu ||| meng ge ||| haizhou li ||| 
2021 ||| tecanet: temporal-contextual attention network for environment-aware speech dereverberation. ||| helin wang ||| bo wu ||| lianwu chen ||| meng yu ||| jianwei yu ||| yong xu ||| shi-xiong zhang ||| chao weng ||| dan su ||| dong yu ||| 
2021 ||| an improved single step non-autoregressive transformer for automatic speech recognition. ||| ruchao fan ||| wei chu ||| peng chang ||| jing xiao ||| abeer alwan ||| 
2019 ||| lattice generation in attention-based speech recognition models. ||| michal zapotoczny ||| piotr pietrzak ||| adrian lancucki ||| jan chorowski ||| 
2020 ||| should we hard-code the recurrence concept or learn it instead ? exploring the transformer architecture for audio-visual speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2021 ||| factorization-aware training of transformers for natural language understanding on the edge. ||| hamidreza saghir ||| samridhi choudhary ||| sepehr eghbali ||| clement chung ||| 
2021 ||| dual causal/non-causal self-attention for streaming end-to-end speech recognition. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2018 ||| an attention pooling based representation learning method for speech emotion recognition. ||| pengcheng li ||| yan song ||| ian mcloughlin ||| wu guo ||| lirong dai ||| 
2020 ||| universal speech transformer. ||| yingzhu zhao ||| chongjia ni ||| cheung-chi leung ||| shafiq r. joty ||| eng siong chng ||| bin ma ||| 
2019 ||| self multi-head attention for speaker recognition. ||| miquel india ||| pooyan safari ||| javier hernando ||| 
2020 ||| dual-path transformer network: direct context-aware modeling for end-to-end monaural speech separation. ||| jingjing chen ||| qirong mao ||| dong liu ||| 
2019 ||| environment-dependent attention-driven recurrent convolutional neural network for robust speech enhancement. ||| meng ge ||| longbiao wang ||| nan li ||| hao shi ||| jianwu dang ||| xiangang li ||| 
2021 ||| multimodal sentiment analysis with temporal modality attention. ||| fan qian ||| jiqing han ||| 
2018 ||| multi-modal attention mechanisms in lstm and its application to acoustic scene classification. ||| teng zhang ||| kailai zhang ||| ji wu ||| 
2019 ||| sequence-to-sequence learning via attention transfer for incremental speech recognition. ||| sashi novitasari ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2020 ||| text-independent speaker verification with dual attention network. ||| jingyu li ||| tan lee ||| 
2021 ||| improving multilingual transformer transducer models by reducing language confusions. ||| eric sun ||| jinyu li ||| zhong meng ||| yu wu ||| jian xue ||| shujie liu ||| yifan gong ||| 
2017 ||| attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging. ||| yong xu ||| qiuqiang kong ||| qiang huang ||| wenwu wang ||| mark d. plumbley ||| 
2020 ||| whisper activity detection using cnn-lstm based attention pooling network trained for a speaker identification task. ||| abinay reddy naini ||| malla satyapriya ||| prasanta kumar ghosh ||| 
2020 ||| evolved speech-transformer: applying neural architecture search to end-to-end automatic speech recognition. ||| jihwan kim ||| jisung wang ||| sangki kim ||| yeha lee ||| 
2021 ||| transformer-based acoustic modeling for streaming speech synthesis. ||| chunyang wu ||| zhiping xiu ||| yangyang shi ||| ozlem kalinli ||| christian fuegen ||| thilo k ||| hler ||| qing he ||| 
2020 ||| spike-triggered non-autoregressive transformer for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| ye bai ||| shuai zhang ||| zhengqi wen ||| 
2019 ||| an attention-based hybrid network for automatic detection of alzheimer's disease from narrative speech. ||| jun chen ||| ji zhu ||| jieping ye ||| 
2021 ||| mixture model attention: flexible streaming and non-streaming automatic speech recognition. ||| kartik audhkhasi ||| tongzhou chen ||| bhuvana ramabhadran ||| pedro j. moreno ||| 
2021 ||| feature fusion by attention networks for robust doa estimation. ||| rongliang liu ||| nengheng zheng ||| xi chen ||| 
2020 ||| streaming chunk-aware multihead attention for online end-to-end speech recognition. ||| shiliang zhang ||| zhifu gao ||| haoneng luo ||| ming lei ||| jie gao ||| zhijie yan ||| lei xie ||| 
2019 ||| self-attention transducers for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| ye bai ||| zhengqi wen ||| 
2020 ||| lvcsr with transformer language models. ||| eugen beck ||| ralf schl ||| ter ||| hermann ney ||| 
2020 ||| multimodal speech emotion recognition using cross attention with aligned audio and text. ||| yoonhyung lee ||| seunghyun yoon ||| kyomin jung ||| 
2020 ||| singing voice extraction with attention-based spectrograms fusion. ||| hao shi ||| longbiao wang ||| sheng li ||| chenchen ding ||| meng ge ||| nan li ||| jianwu dang ||| hiroshi seki ||| 
2020 ||| a recursive network with dynamic attention for monaural speech enhancement. ||| andong li ||| chengshi zheng ||| cunhang fan ||| renhua peng ||| xiaodong li ||| 
2021 ||| domain-aware self-attention for multi-domain neural machine translation. ||| shiqi zhang ||| yan liu ||| deyi xiong ||| pei zhang ||| boxing chen ||| 
2021 ||| on-device streaming transformer-based end-to-end speech recognition. ||| yoo rhee oh ||| kiyoung park ||| 
2020 ||| jdi-t: jointly trained duration informed transformer for text-to-speech without explicit alignment. ||| dan lim ||| won jang ||| gyeonghwan o ||| heayoung park ||| bongwan kim ||| jaesam yoon ||| 
2020 ||| asr error correction with augmented transformer for entity retrieval. ||| haoyu wang ||| shuyan dong ||| yue liu ||| james logan ||| ashish kumar agrawal ||| yang liu ||| 
2019 ||| vectorized beam search for ctc-attention-based speech recognition. ||| hiroshi seki ||| takaaki hori ||| shinji watanabe ||| niko moritz ||| jonathan le roux ||| 
2018 ||| improving mandarin tone recognition using convolutional bidirectional long short-term memory with attention. ||| longfei yang ||| yanlu xie ||| jinsong zhang ||| 
2020 ||| semantic mask for transformer based end-to-end speech recognition. ||| chengyi wang ||| yu wu ||| yujiao du ||| jinyu li ||| shujie liu ||| liang lu ||| shuo ren ||| guoli ye ||| sheng zhao ||| ming zhou ||| 
2021 ||| end to end transformer-based contextual speech recognition based on pointer network. ||| binghuai lin ||| liyuan wang ||| 
2019 ||| rwth asr systems for librispeech: hybrid vs attention. ||| christoph l ||| scher ||| eugen beck ||| kazuki irie ||| markus kitza ||| wilfried michel ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2020 ||| singing synthesis: with a little help from my attention. ||| orazio angelini ||| alexis moinet ||| kayoko yanagisawa ||| thomas drugman ||| 
2021 ||| multi-mode transformer transducer with stochastic future context. ||| kwangyoun kim ||| felix wu ||| prashant sridhar ||| kyu j. han ||| shinji watanabe ||| 
2019 ||| improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation. ||| sheng li ||| raj dabre ||| xugang lu ||| peng shen ||| tatsuya kawahara ||| hisashi kawai ||| 
2020 ||| abstractive spoken document summarization using hierarchical model with multi-stage attention diversity optimization. ||| potsawee manakul ||| mark j. f. gales ||| linlin wang ||| 
2020 ||| speech driven talking head generation via attentional landmarks based representation. ||| wentao wang ||| yan wang ||| jianqing sun ||| qingsong liu ||| jiaen liang ||| teng li ||| 
2021 ||| real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network. ||| midia yousefi ||| john h. l. hansen ||| 
2020 ||| acoustic scene analysis with multi-head attention networks. ||| weimin wang ||| weiran wang ||| ming sun ||| chao wang ||| 
2018 ||| who are you listening to? towards a dynamic measure of auditory attention to speech-on-speech. ||| mo ||| ra-phoeb |||  huet ||| christophe micheyl ||| etienne gaudrain ||| etienne parizet ||| 
2019 ||| cross-attention end-to-end asr for two-party conversations. ||| suyoun kim ||| siddharth dalmia ||| florian metze ||| 
2020 ||| naagn: noise-aware attention-gated network for speech enhancement. ||| feng deng ||| tao jiang ||| xiaorui wang ||| chen zhang ||| yan li ||| 
2020 ||| finnish asr with deep transformer models. ||| abhilash jain ||| aku rouhe ||| stig-arne gr ||| nroos ||| mikko kurimo ||| 
2020 ||| noisy-reverberant speech enhancement using denseunet with time-frequency attention. ||| yan zhao ||| deliang wang ||| 
2020 ||| bi-encoder transformer network for mandarin-english code-switching speech recognition using mixture of experts. ||| yizhou lu ||| mingkun huang ||| hao li ||| jiaqi guo ||| yanmin qian ||| 
2021 ||| attention-based convolutional neural network for asv spoofing detection. ||| hefei ling ||| leichao huang ||| junrui huang ||| baiyan zhang ||| ping li ||| 
2020 ||| multilingual speech recognition with self-attention structured parameterization. ||| yun zhu ||| parisa haghani ||| anshuman tripathi ||| bhuvana ramabhadran ||| brian farris ||| hainan xu ||| han lu ||| hasim sak ||| isabel leal ||| neeraj gaur ||| pedro j. moreno ||| qian zhang ||| 
2021 ||| attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams. ||| yuanbo hou ||| zhesong yu ||| xia liang ||| xingjian du ||| bilei zhu ||| zejun ma ||| dick botteldooren ||| 
2021 ||| vad-free streaming hybrid ctc/attention asr for unsegmented recording. ||| hirofumi inaguma ||| tatsuya kawahara ||| 
2019 ||| a time delay neural network with shared weight self-attention for small-footprint keyword spotting. ||| ye bai ||| jiangyan yi ||| jianhua tao ||| zhengqi wen ||| zhengkun tian ||| chenghao zhao ||| cunhang fan ||| 
2021 ||| shallow convolution-augmented transformer with differentiable neural computer for low-complexity classification of variable-length acoustic scene. ||| soonshin seo ||| donghyun lee ||| ji-hwan kim ||| 
2019 ||| pyramid memory block and timestep attention for speech emotion recognition. ||| miao cao ||| chun yang ||| fang zhou ||| xu-cheng yin ||| 
2018 ||| end-to-end audio replay attack detection using deep convolutional networks with attention. ||| francis tom ||| mohit jain ||| prasenjit dey ||| 
2021 ||| triple m: a practical text-to-speech synthesis system with multi-guidance attention and multi-band multi-time lpcnet. ||| shilun lin ||| fenglong xie ||| li meng ||| xinhui li ||| li lu ||| 
2020 ||| multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks. ||| krishna d. n ||| ankita patil ||| 
2020 ||| conv-transformer transducer: low latency, low frame rate, streamable end-to-end speech recognition. ||| wenyong huang ||| wenchao hu ||| yu ting yeung ||| xiao chen ||| 
2021 ||| an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech. ||| jianrong wang ||| nan gu ||| mei yu ||| xuewei li ||| qiang fang ||| li liu ||| 
2019 ||| neural text clustering with document-level attention based on dynamic soft labels. ||| zhi chen ||| wu guo ||| li-rong dai ||| zhen-hua ling ||| jun du ||| 
2020 ||| san-m: memory equipped self-attention for end-to-end speech recognition. ||| zhifu gao ||| shiliang zhang ||| ming lei ||| ian mcloughlin ||| 
2018 ||| attention-based end-to-end models for small-footprint keyword spotting. ||| changhao shan ||| junbo zhang ||| yujun wang ||| lei xie ||| 
2017 ||| parallel hierarchical attention networks with shared memory reader for multi-stream conversational document classification. ||| naoki sawada ||| ryo masumura ||| hiromitsu nishizaki ||| 
2020 ||| single headed attention based sequence-to-sequence model for state-of-the-art results on switchboard. ||| zolt ||| n t ||| ske ||| george saon ||| kartik audhkhasi ||| brian kingsbury ||| 
2017 ||| an analysis of "attention" in sequence-to-sequence models. ||| rohit prabhavalkar ||| tara n. sainath ||| bo li ||| kanishka rao ||| navdeep jaitly ||| 
2021 ||| improving streaming transformer based asr under a framework of self-supervised learning. ||| songjun cao ||| yueteng kang ||| yanzhe fu ||| xiaoshuo xu ||| sining sun ||| yike zhang ||| long ma ||| 
2021 ||| learning mutual correlation in multimodal transformer for speech emotion recognition. ||| yuhua wang ||| guang shen ||| yuezhu xu ||| jiahang li ||| zhengdao zhao ||| 
2017 ||| end-to-end speech recognition with auditory attention for multi-microphone distance speech recognition. ||| suyoun kim ||| ian r. lane ||| 
2018 ||| image semantic description based on deep learning with multi-attention mechanisms. ||| jian yang ||| zuqiang meng ||| 
2018 ||| attention-based temporal weighted convolutional neural network for action recognition. ||| jinliang zang ||| le wang ||| zi-yi liu ||| qilin zhang ||| gang hua ||| nanning zheng ||| 
2018 ||| content-aware attention network for action recognition. ||| zi-yi liu ||| le wang ||| nanning zheng ||| 
2018 ||| the cognitive philosophical problems in visual attention and its influence on artificial intelligence modeling. ||| jing-jing zhao ||| 
2019 ||| an eye-tracking dataset for visual attention modelling in a virtual museum context. ||| yunzhan zhou ||| tian feng ||| shihui shuai ||| xiangdong li ||| lingyun sun ||| henry b. l. duh ||| 
2021 ||| dg-trans: automatic code summarization via dynamic graph attention-based transformer. ||| jianwei zeng ||| tao zhang ||| zhou xu ||| 
2021 ||| aclm: software aging prediction of virtual machine monitor based on attention mechanism of cnn-lstm model. ||| xueyong tan ||| jing liu ||| 
2021 ||| a novel api recommendation approach by using graph attention network. ||| zijie chen ||| tao zhang ||| xiao peng ||| 
2020 ||| a sentiment classification model based on bi-directional lstm with positional attention for fresh food consumer reviews. ||| tong-qiang jiang ||| xue-mei xu ||| qing-chuan zhang ||| zheng wang ||| 
2021 ||| a transformer based sales prediction of smart container in new retail era. ||| ying jin ||| ming gao ||| jixiang yu ||| 
2019 ||| practice in caption generation with keras: the design and evaluation for attention models. ||| rong wang ||| toru wakahara ||| 
2018 ||| attention-based neural network for short-text question answering. ||| yongxing peng ||| bo liu ||| 
2020 ||| now, over here: leveraging extended attentional capabilities in human-robot interaction. ||| xiang zhi tan ||| sean andrist ||| dan bohus ||| eric horvitz ||| 
2020 ||| modeling the interplay of trust and attention in hri: an autonomous vehicle study. ||| indu p. bodala ||| bing cai kok ||| weicong sng ||| harold soh ||| 
2020 ||| attention-based multimodal fusion for estimating human emotion in real-world hri. ||| yuanchao li ||| tianyu zhao ||| xun shen ||| 
2019 ||| welcoming robot behaviors for drawing attention. ||| elie saad ||| mark a. neerincx ||| koen v. hindriks ||| 
2019 ||| welcoming robot behaviors for drawing attention. ||| elie saad ||| mark a. neerincx ||| koen v. hindriks ||| 
2017 ||| movers, shakers, and those who stand still: visual attention-grabbing techniques in robot teleoperation. ||| daniel j. rea ||| stela hanbyeol seo ||| neil d. b. bruce ||| james e. young ||| 
2020 ||| addressing attention difficulties in autistic children using multimodal cues from a humanoid robot. ||| jamy li ||| suncica petrovic ||| daniel p. davison ||| snezana babovic dimitrijevic ||| pauline chevalier ||| vanessa evers ||| 
2021 ||| effects of gaze and arm motion kinesics on a humanoid's perceived confidence, eagerness to learn, and attention to the task in a teaching scenario. ||| pourya aliasghari ||| moojan ghafurian ||| chrystopher l. nehaniv ||| kerstin dautenhahn ||| 
2020 ||| joint attention estimator. ||| wallace lawson ||| anthony m. harrison ||| eric s. vorm ||| j. gregory trafton ||| 
2021 ||| design and cost analysis of two different types of 400 kva distribution transformers. ||| moein attar ||| suleyman sungur tezcan ||| 
2017 ||| evaluation of orientation performance of attention patterns for blind person. ||| shoichiro fujisawa ||| tatsuki ishibashi ||| katsuya sato ||| sin-ichi ito ||| osamu sueda ||| 
2019 ||| attention-guided model for robust face detection system. ||| laksono kurnianggoro ||| kang-hyun jo ||| 
2021 ||| the influence of media attention and equity incentives on corporate tax avoidance in the information age. ||| jinmei tian ||| 
2018 ||| does the research question structure impact the attention model? user study experiment. ||| malwina dzisko ||| anna lewandowska ||| anna samborska-owczarek ||| 
2020 ||| locating cephalometric x-ray landmarks with foveated pyramid attention. ||| logan gilmour ||| nilanjan ray ||| 
2021 ||| feedback graph attention convolutional network for mr images enhancement by exploring self-similarity features. ||| xiaobin hu ||| yanyang yan ||| wenqi ren ||| hongwei li ||| amirhossein bayat ||| yu zhao ||| bjoern h. menze ||| 
2021 ||| image sequence generation and analysis via gru and attention for trachomatous trichiasis classification. ||| juan-carlos prieto ||| hina shah ||| kasey jones ||| robert f. chew ||| hashiya m. kana ||| jerusha weaver ||| rebecca m. flueckiger ||| scott mcpherson ||| emily w. gower ||| 
2020 ||| prostate cancer semantic segmentation by gleason score group in bi-parametric mri with self attention model on the peripheral zone. ||| audrey duran ||| pierre-marc jodoin ||| carole lartizien ||| 
2020 ||| automated labelling using an attention model for radiology reports of mri scans (alarm). ||| david a. wood ||| jeremy lynch ||| sina kafiabadi ||| emily guilhem ||| aisha al busaidi ||| antanas montvila ||| thomas varsavsky ||| juveria siddiqui ||| naveen gadapa ||| matthew townend ||| martin kiik ||| keena patel ||| gareth j. barker ||| s ||| bastien ourselin ||| james h. cole ||| thomas c. booth ||| 
2020 ||| automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study. ||| luyao shi ||| deepta rajan ||| shafiq abedin ||| manikanta srikar yellapragada ||| david beymer ||| ehsan dehghan ||| 
2021 ||| unsupervised brain anomaly detection and segmentation with transformers. ||| walter hugo lopez pinaya ||| petru-daniel tudosiu ||| robert gray ||| geraint rees ||| parashkev nachev ||| s ||| bastien ourselin ||| m. jorge cardoso ||| 
2019 ||| group-attention single-shot detector (ga-ssd): finding pulmonary nodules in large-scale ct images. ||| jiechao ma ||| xiang li ||| hongwei li ||| bjoern h. menze ||| sen liang ||| rongguo zhang ||| wei-shi zheng ||| 
2019 ||| care: class attention to regions of lesion for classification on imbalanced data. ||| jiaxin zhuang ||| jiabin cai ||| ruixuan wang ||| jianguo zhang ||| weishi zheng ||| 
2021 ||| attention via scattering transforms for segmentation of small intravascular ultrasound data sets. ||| lennart bargsten ||| katharina a. riedl ||| tobias wissel ||| fabian j. brunner ||| klaus schaefers ||| michael grass ||| stefan blankenberg ||| moritz seiffert ||| alexander schlaefer ||| 
2020 ||| sau-net: efficient 3d spine mri segmentation using inter-slice attention. ||| yichi zhang ||| lin yuan ||| yujia wang ||| jicong zhang ||| 
2019 ||| xlsor: a robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. ||| youbao tang ||| yuxing tang ||| jing xiao ||| ronald m. summers ||| 
2019 ||| assessing knee oa severity with cnn attention-based end-to-end architectures. ||| marc g ||| rriz ||| joseph antony ||| kevin mcguinness ||| xavier gir ||| -i-nieto ||| noel e. o'connor ||| 
2021 ||| security requirements classification into groups using nlp transformers. ||| vasily varenov ||| aydar gabdrahmanov ||| 
2021 ||| power grid cascading failure prediction based on transformer. ||| tianxin zhou ||| xiang li ||| haibing lu ||| 
2021 ||| incorporating transformer models for sentiment analysis and news classification in khmer. ||| md. rifatul islam rifat ||| abdullah al imran ||| 
2021 ||| deep bangla authorship attribution using transformer models. ||| abdullah al imran ||| md nur amin ||| 
2018 ||| enable an innovative prolonged exposure therapy of attention deficits on autism spectrum through adaptive virtual environments. ||| chao mei ||| rongkai guo ||| 
2019 ||| improving visual attention guiding by differentiation between fine and coarse navigation. ||| philipp hein ||| max bernhagen ||| angelika c. bullinger ||| 
2017 ||| ynu-hpcc at ijcnlp-2017 task 5: multi-choice question answering in exams using an attention-based lstm model. ||| hang yuan ||| you zhang ||| jin wang ||| xuejie zhang ||| 
2017 ||| ynu-hpcc at ijcnlp-2017 task 4: attention-based bi-directional gru model for customer feedback analysis task of english. ||| nan wang ||| jin wang ||| xuejie zhang ||| 
2020 ||| reconstructing event regions for event extraction via graph attention networks. ||| pei chen ||| hang yang ||| kang liu ||| ruihong huang ||| yubo chen ||| taifeng wang ||| jun zhao ||| 
2017 ||| sentence modeling with deep neural architecture using lexicon and character attention mechanism for sentiment classification. ||| huy-thanh nguyen ||| minh-le nguyen ||| 
2020 ||| comparing probabilistic, distributional and transformer-based models on logical metonymy interpretation. ||| giulia rambelli ||| emmanuele chersoni ||| alessandro lenci ||| philippe blache ||| chu-ren huang ||| 
2017 ||| key-value attention mechanism for neural machine translation. ||| hideya mino ||| masao utiyama ||| eiichiro sumita ||| takenobu tokunaga ||| 
2020 ||| graph attention network with memory fusion for aspect-level sentiment analysis. ||| li yuan ||| jin wang ||| liang-chih yu ||| xuejie zhang ||| 
2017 ||| cascading multiway attentions for document-level sentiment classification. ||| dehong ma ||| sujian li ||| xiaodong zhang ||| houfeng wang ||| xu sun ||| 
2017 ||| what does attention in neural machine translation pay attention to? ||| hamidreza ghader ||| christof monz ||| 
2020 ||| two-headed monster and crossed co-attention networks. ||| yaoyiran li ||| jing jiang ||| 
2017 ||| ynudlg at ijcnlp-2017 task 5: a cnn-lstm model with attention for multi-choice question answering in examinations. ||| min wang ||| qingxun liu ||| peng ding ||| yongbin li ||| xiaobing zhou ||| 
2017 ||| supervised attention for sequence-to-sequence constituency parsing. ||| hidetaka kamigaito ||| katsuhiko hayashi ||| tsutomu hirao ||| hiroya takamura ||| manabu okumura ||| masaaki nagata ||| 
2017 ||| multilingual hierarchical attention networks for document classification. ||| nikolaos pappas ||| andrei popescu-belis ||| 
2020 ||| training with adversaries to improve faithfulness of attention in neural machine translation. ||| pooya moradi ||| nishant kambhatla ||| anoop sarkar ||| 
2020 ||| making a point: pointer-generator transformers for disjoint vocabularies. ||| nikhil prabhu ||| katharina kann ||| 
2017 ||| local monotonic attention mechanism for end-to-end speech and language processing. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2020 ||| heads-up! unsupervised constituency parsing via self-attention heads. ||| bowen li ||| taeuk kim ||| reinald kim amplayo ||| frank keller ||| 
2017 ||| cky-based convolutional attention for neural machine translation. ||| taiki watanabe ||| akihiro tamura ||| takashi ninomiya ||| 
2020 ||| transformer-based approach for predicting chemical compound structures. ||| yutaro omote ||| kyoumoto matsushita ||| tomoya iwakura ||| akihiro tamura ||| takashi ninomiya ||| 
2020 ||| multi-source attention for unsupervised domain adaptation. ||| xia cui ||| danushka bollegala ||| 
2021 ||| attention convolutional u-net for automatic liver tumor segmentation. ||| asima bibi ||| muhammad salman khan ||| 
2018 ||| generating abstractive summaries using sequence to sequence attention model. ||| tooba siddiqui ||| jawwad ahmed shamsi ||| 
2021 ||| a vision transformer with improved leff and vision combinative self-attention mechanism for waste image classification. ||| yuxiang guo ||| di cao ||| wuchao li ||| shang hu ||| jiabin gao ||| lixin huang ||| zengrong ye ||| 
2021 ||| influence and simulation of multibarrier isolation facilities on noise attenuation distribution of transformer. ||| zhenhuan liu ||| yulong chen ||| hao wan ||| 
2021 ||| it is time to laugh: discovering specific contexts for laughter with attention mechanism. ||| kaibin xu ||| junpei zhong ||| kristiina jokinen ||| 
2021 ||| influence and simulation of transformer firewall device on audible noise propagation characteristics. ||| zhenhuan liu ||| yulong chen ||| rui huang ||| 
2020 ||| evaluating the effect of user-given guiding attention on the learning process. ||| richard nordsieck ||| michael heider ||| andreas angerer ||| j ||| rg h ||| hner ||| 
2017 ||| convergence of media attention across 129 countries. ||| jisun an ||| hassan aldarbesti ||| haewoon kwak ||| 
2018 ||| quantifying media influence and partisan attention on twitter during the uk eu referendum. ||| genevieve gorrell ||| ian roberts ||| mark a. greenwood ||| mehmet e. bakir ||| benedetta iavarone ||| kalina bontcheva ||| 
2018 ||| assessing competition for social media attention among non-profits. ||| rosta farzan ||| claudia l ||| pez ||| 
2017 ||| attention please! - exploring attention management on wikipedia in the context of the ukrainian crisis. ||| jon roozenbeek ||| mariia terentieva ||| 
2021 ||| multi-head fusion attention for transformer-based end-to-end automatic speech recognition. ||| timo lohrenz ||| patrick schwarz ||| zhengyang li ||| tim fingscheidt ||| 
2018 ||| a gaze-based attention model for spatially-aware hearing aids. ||| giso grimm ||| hendrik kayser ||| maartje m. e. hendrikse ||| volker hohmann ||| 
2020 ||| blac: a named entity recognition model incorporating part-of-speech attention in irregular short text. ||| ming zhu ||| huakang li ||| xiaoyu sun ||| zhuo yang ||| 
2021 ||| attention residual network with 3d convolutional neural network for 3d human pose estimation. ||| jianyu yan ||| kuizhi mei ||| 
2021 ||| towards autonomous driving decision by combining self-attention and deep reinforcement learning. ||| meiling chen ||| yanjie li ||| qi liu ||| shaohua lv ||| yunhong xu ||| yuecheng liu ||| 
2021 ||| attention mechanism-based monocular depth estimation and visual odometry. ||| qieshi zhang ||| dian lin ||| ziliang ren ||| yuhang kang ||| fuxiang wu ||| jun cheng ||| 
2017 ||| attention guiding techniques using peripheral vision and eye tracking for feedback in augmented-reality-based assistance systems. ||| patrick renner ||| thies pfeiffer ||| 
2020 ||| attention-based neural networks for sentiment attitude extraction using distant supervision. ||| nicolay rusnachenko ||| natalia v. loukachevitch ||| 
2020 ||| attention-based text recognition in the wild. ||| zhi-chen yan ||| stephanie a. yu ||| 
2021 ||| selective and divided attention for vibrotactile stimuli on both arms. ||| gina m. clepper ||| juan sebasti ||| n mart ||| nez ||| hong z. tan ||| 
2017 ||| can tactile suppression be explained by attentional capture? ||| georgiana juravle ||| charles spence ||| 
2019 ||| exogenous cueing of visual attention using small, directional, tactile cues applied to the fingertip. ||| john de grosbois ||| massimiliano di luca ||| raymond j. king ||| cesare parise ||| mounia ziat ||| 
2019 ||| electromagnetic force of power transformer with different short circuit current based on fem. ||| yan wu ||| lingyun gu ||| xue zhang ||| jinyu wang ||| 
2019 ||| operator functional state: measure it with attention intensity and selectivity, explain it with cognitive control. ||| alexandre kostenko ||| philippe rauffet ||| sorin moga ||| gilles coppin ||| 
2019 ||| enhancing transformer for end-to-end speech-to-text translation. ||| mattia antonino di gangi ||| matteo negri ||| roldano cattoni ||| roberto dess ||| marco turchi ||| 
2021 ||| product review translation using phrase replacement and attention guided noise augmentation. ||| kamal kumar gupta ||| soumya chennabasavaraj ||| nikesh garera ||| asif ekbal ||| 
2019 ||| a multi-hop attention for rnn based neural machine translation. ||| shohei iida ||| ryuichiro kimura ||| hongyi cui ||| po-hsuan hung ||| takehito utsuro ||| masaaki nagata ||| 
2021 ||| frozen pretrained transformers for neural sign language translation. ||| mathieu de coster ||| karel d'oosterlinck ||| marija pizurica ||| paloma rabaey ||| severine verlinden ||| mieke van herreweghe ||| joni dambre ||| 
2020 ||| leveraging multilingual transformers for hate speech detection. ||| sayar ghosh roy ||| ujwal narayan ||| tathagata raha ||| zubair abid ||| vasudeva varma ||| 
2020 ||| cfilt iit bombay@hasoc-dravidian-codemix fire 2020: assisting ensemble of transformers with random transliteration. ||| pankaj singh ||| pushpak bhattacharyya ||| 
2019 ||| ynu_wb at hasoc 2019: ordered neurons lstm with attention for identifying hate speech and offensive language. ||| bin wang ||| yunxia ding ||| shengyan liu ||| xiaobing zhou ||| 
2020 ||| huiping shi@hasoc 2020: multi-top k self-attention with k-max pooling for discrimination between hate profane and offensive posts. ||| huiping shi ||| xiaobing zhou ||| 
2020 ||| astralis @ hasoc 2020: analysis on identification of hate speech in indo-european languages with fine-tuned transformers. ||| hiren madhu ||| shrey satapara ||| harsh rathod ||| 
2019 ||| 3idiots at hasoc 2019: fine-tuning transformer neural networks for hate speech identification in indo-european languages. ||| shubhanshu mishra ||| sudhanshu mishra ||| 
2020 ||| bi-directional encoder representation of transformer model for sequential music recommender system. ||| naina yadav ||| anil kumar singh ||| 
2021 ||| attention based end to end speech recognition for voice search in hindi and english. ||| raviraj joshi ||| venkateshan kannan ||| 
2020 ||| cmsaone@dravidian-codemix-fire2020: a meta embedding and transformer model for code-mixed sentiment analysis on social media text. ||| suman dowlagar ||| radhika mamidi ||| 
2019 ||| multi-task bidirectional transformer representations for irony detection. ||| chiyu zhang ||| muhammad abdul-mageed ||| 
2020 ||| hub@hasoc 2020: fine-tuning pre-trained transformer language models for hate speech and offensive content identification in indo-european languages. ||| bo huang ||| yang bai ||| 
2020 ||| spectre@aila-fire2020: supervised rhetorical role labeling for legal judgments using transformers. ||| racchit jain ||| abhishek agarwal ||| yashvardhan sharma ||| 
2020 ||| attention based anaphora resolution for code-mixed social media text for hindi language. ||| sandhya singh ||| kevin patel ||| pushpak bhattacharyya ||| 
2021 ||| e.t.: re-thinking self-attention for transformer models on gpus. ||| shiyang chen ||| shaoyi huang ||| santosh pandey ||| bingbing li ||| guang r. gao ||| long zheng ||| caiwen ding ||| hang liu ||| 
2020 ||| calling attention to passages for biomedical question answering. ||| tiago almeida ||| s ||| rgio matos ||| 
2018 ||| attention-based neural text segmentation. ||| pinkesh badjatiya ||| litton j. kurisinkel ||| manish gupta ||| vasudeva varma ||| 
2021 ||| multi-head self-attention with role-guided masks. ||| dongsheng wang ||| casper hansen ||| lucas chaves lima ||| christian hansen ||| maria maistro ||| jakob grue simonsen ||| christina lioma ||| 
2019 ||| end-to-end neural relation extraction using deep biaffine attention. ||| dat quoc nguyen ||| karin verspoor ||| 
2020 ||| dake: document-level attention for keyphrase extraction. ||| tokala yaswanth sri sai santosh ||| debarshi kumar sanyal ||| plaban kumar bhowmick ||| partha pratim das ||| 
2021 ||| answer sentence selection using local and global context in transformer models. ||| ivano lauriola ||| alessandro moschitti ||| 
2020 ||| an attention model of customer expectation to improve review helpfulness prediction. ||| xianshan qu ||| xiaopeng li ||| csilla farkas ||| john r. rose ||| 
2021 ||| drug and disease interpretation learning with biomedical entity representation transformer. ||| zulfat miftahutdinov ||| artur kadurin ||| roman kudrin ||| elena tutubalina ||| 
2020 ||| recognizing semantic relations: attention-based transformers vs. recurrent models. ||| dmitri roussinov ||| serge sharoff ||| nadezhda puchnina ||| 
2021 ||| a multi-task approach to neural multi-label hierarchical patent classification using transformers. ||| subhash |||  chandra pujari ||| annemarie friedrich ||| jannik str ||| tgen ||| 
2020 ||| temporal embeddings and transformer models for narrative text understanding. ||| vani kanjirangat ||| simone mellace ||| alessandro antonucci ||| 
2017 ||| a neural attention model for categorizing patient safety events. ||| arman cohan ||| allan fong ||| nazli goharian ||| raj m. ratwani ||| 
2020 ||| dynamic heterogeneous graph embedding using hierarchical attentions. ||| luwei yang ||| zhibo xiao ||| wen jiang ||| yi wei ||| yi hu ||| hao wang ||| 
2021 ||| classifying scientific publications with bert - is self-attention a feature selection method? ||| andr ||| s garc ||| a-silva ||| jos |||  manu ||| l g ||| mez-p ||| rez ||| 
2020 ||| readnet: a hierarchical transformer framework for web article readability analysis. ||| changping meng ||| muhao chen ||| jie mao ||| jennifer neville ||| 
2021 ||| mitigating the position bias of transformer models in passage re-ranking. ||| sebastian hofst ||| tter ||| aldo lipani ||| sophia althammer ||| markus zlabinger ||| allan hanbury ||| 
2019 ||| zero-shot language transfer for cross-lingual sentence retrieval using bidirectional attention model. ||| goran glavas ||| ivan vulic ||| 
2021 ||| comparing score aggregation approaches for document retrieval with pretrained transformers. ||| xinyu zhang ||| andrew yates ||| jimmy lin ||| 
2021 ||| transformer-based approach towards music emotion recognition from lyrics. ||| yudhik agrawal ||| ramaguru guru ravi shanker ||| vinoo alluri ||| 
2020 ||| inductive document network embedding with topic-word attention. ||| robin brochier ||| adrien guille ||| julien velcin ||| 
2018 ||| topical stance detection for twitter: a two-phase lstm model using attention. ||| kuntal dey ||| ritvik shrivastava ||| saroj kaushik ||| 
2021 ||| pgt: pseudo relevance feedback using a graph-based transformer. ||| hongchien yu ||| zhuyun dai ||| jamie callan ||| 
2021 ||| open-domain conversational search assistant with transformers. ||| rafael ferreira ||| mariana leite ||| david semedo ||| jo ||| o magalh ||| es ||| 
2018 ||| malware analysis of imaged binary samples by convolutional neural network with attention mechanism. ||| hiromu yakura ||| shinnosuke shinozaki ||| reon nishimura ||| yoshihiro oyama ||| jun sakuma ||| 
2019 ||| attention-based recurrent neural network for urban vehicle trajectory prediction. ||| seongjin choi ||| jiwon kim ||| hwasoo yeo ||| 
2019 ||| attention-based autoencoder topic model for short texts. ||| tian tian ||| zheng felix fang ||| 
2021 ||| smart contracts implementation based on bidirectional encoder representations from transformers. ||| bajeela aejas ||| abdelaziz bouras ||| abdelhak belhi ||| houssem gasmi ||| 
2021 ||| a fast detection method for polynomial fitting lane with self-attention module added. ||| xi li ||| zhen huang ||| xiongfeng sun ||| tianliang liu ||| 
2021 ||| speech enhancement based on attention mechanism and pg-lstm neural network. ||| yuxi qin ||| youming wang ||| 
2021 ||| an improved speech recognition system based on transformer language model. ||| qi yue ||| weiliang shi ||| yi he ||| jing chu ||| zhan han ||| xiaokai han ||| 
2021 ||| a novel view image generation network based on attention mechanism refining features. ||| mengni yi ||| bingwei hui ||| weidong hu ||| min he ||| 
2020 ||| boosting toponym interlinking by paying attention to both machine and deep learning. ||| konstantinos alexis ||| vassilis kaffes ||| giorgos giannopoulos ||| 
2017 ||| macrobase: prioritizing attention in fast data. ||| peter bailis ||| edward gan ||| samuel madden ||| deepak narayanan ||| kexin rong ||| sahaana suri ||| 
2021 ||| apan: asynchronous propagation attention network for real-time temporal graph embedding. ||| xuhong wang ||| ding lyu ||| mengjian li ||| yang xia ||| qi yang ||| xinwen wang ||| xinguang wang ||| ping cui ||| yupu yang ||| bowen sun ||| zhenyu guo ||| 
2021 ||| combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of covid-19 fake tweets. ||| rachit bansal ||| william scott paka ||| nidhi ||| shubhashis sengupta ||| tanmoy chakraborty ||| 
2021 ||| glad-paw: graph-based log anomaly detection by position aware weighted graph attention network. ||| yi wan ||| yilin liu ||| dong wang ||| yujin wen ||| 
2019 ||| aaane: attention-based adversarial autoencoder for multi-scale network embedding. ||| lei sang ||| min xu ||| shengsheng qian ||| xindong wu ||| 
2020 ||| attention-based graph evolution. ||| shuangfei fan ||| bert huang ||| 
2021 ||| iacn: influence-aware and attention-based co-evolutionary network for recommendation. ||| shalini pandey ||| george karypis ||| jaideep srivastava ||| 
2019 ||| text feature extraction and selection based on attention mechanism. ||| longxuan ma ||| lei zhang ||| 
2018 ||| call attention to rumors: deep attention based recurrent neural networks for early rumor detection. ||| tong chen ||| xue li ||| hongzhi yin ||| jun zhang ||| 
2019 ||| early churn user classification in social networking service using attention-based long short-term memory. ||| koya sato ||| mizuki oka ||| kazuhiko kato ||| 
2021 ||| graph attention networks with positional embeddings. ||| liheng ma ||| reihaneh rabbany ||| adriana romero-soriano ||| 
2021 ||| hierarchical self attention based autoencoder for open-set human activity recognition. ||| m. tanjid hasan tonmoy ||| saif mahmud ||| a. k. m. mahbubur rahman ||| m. ashraful amin ||| amin ahsan ali ||| 
2021 ||| adaptive graph co-attention networks for traffic forecasting. ||| boyu li ||| ting guo ||| yang wang ||| amir h. gandomi ||| fang chen ||| 
2021 ||| a deep hybrid pooling architecture for graph classification with hierarchical attention. ||| sambaran bandyopadhyay ||| manasvi aggarwal ||| m. narasimha murty ||| 
2020 ||| slgat: soft labels guided graph attention networks. ||| yubin wang ||| zhenyu zhang ||| tingwen liu ||| li guo ||| 
2019 ||| aspect level sentiment analysis with aspect attention. ||| changliang li ||| hailiang wang ||| saike he ||| 
2021 ||| tantp: conversational emotion recognition using tree-based attention networks with transformer pre-training. ||| haozhe liu ||| hongzhan lin ||| guang chen ||| 
2021 ||| using transformer based ensemble learning to classify scientific articles. ||| sohom ghosh ||| ankush chopra ||| 
2018 ||| research and application of mapping relationship based on learning attention mechanism. ||| wanwan jiang ||| lingyu xu ||| jie yu ||| gaowei zhang ||| 
2020 ||| attention-based aggregation graph networks for knowledge graph information transfer. ||| ming zhao ||| weijia jia ||| yusheng huang ||| 
2020 ||| gamma: a graph and multi-view memory attention mechanism for top-n heterogeneous recommendation. ||| vijaikumar m ||| shirish k. shevade ||| m. narasimha murty ||| 
2020 ||| optimized transformer models for faq answering. ||| sonam damani ||| kedhar nath narahari ||| ankush chatterjee ||| manish gupta ||| puneet agrawal ||| 
2020 ||| cacrnn: a context-aware attention-based convolutional recurrent neural network for fine-grained taxi demand prediction. ||| wenbin wu ||| tong liu ||| jiahao yang ||| 
2020 ||| role equivalence attention for label propagation in graph neural networks. ||| hogun park ||| jennifer neville ||| 
2021 ||| upgraded attention-based local feature learning block for speech emotion recognition. ||| huan zhao ||| yingxue gao ||| yufeng xiao ||| 
2019 ||| context-aware dual-attention network for natural language inference. ||| kun zhang ||| guangyi lv ||| enhong chen ||| le wu ||| qi liu ||| c. l. philip chen ||| 
2021 ||| transformer-based multi-task learning for queuing time aware next poi recommendation. ||| sajal halder ||| kwan hui lim ||| jeffrey chan ||| xiuzhen zhang ||| 
2021 ||| heterogeneous graph attention network for small and medium-sized enterprises bankruptcy prediction. ||| yizhen zheng ||| vincent c. s. lee ||| zonghan wu ||| shirui pan ||| 
2020 ||| msfcnet: multi-scale feature-crossing attention network for multi-field sparse data. ||| zhifeng xie ||| wenling zhang ||| huiming ding ||| lizhuang ma ||| 
2019 ||| complaint classification using hybrid-attention gru neural network. ||| shuyang wang ||| bin wu ||| bai wang ||| xuesong tong ||| 
2019 ||| topic attentional neural network for abstractive document summarization. ||| hao liu ||| hai-tao zheng ||| wei wang ||| 
2018 ||| adaptive attention network for review sentiment classification. ||| chuantao zong ||| wenfeng feng ||| vincent w. zheng ||| hankz hankui zhuo ||| 
2021 ||| learning attention-based translational knowledge graph embedding via nonlinear dynamic mapping. ||| zhihao wang ||| honggang xu ||| xin li ||| yuxin deng ||| 
2019 ||| multivariate time series early classification with interpretability using deep learning and attention mechanism. ||| en-yu hsu ||| chien-liang liu ||| vincent s. tseng ||| 
2018 ||| a deep neural spoiler detection model using a genre-aware attention mechanism. ||| buru chang ||| hyunjae kim ||| raehyun kim ||| deahan kim ||| jaewoo kang ||| 
2019 ||| sentiment analysis based on lstm architecture with emoticon attention. ||| changliang li ||| changsong li ||| pengyuan liu ||| 
2021 ||| raga: relation-aware graph attention networks for global entity alignment. ||| renbo zhu ||| meng ma ||| ping wang ||| 
2021 ||| meta-context transformers for domain-specific response generation. ||| debanjana kar ||| suranjana samanta ||| amar prakash azad ||| 
2019 ||| atnet: answering cloze-style questions via intra-attention and inter-attention. ||| chengzhen fu ||| yuntao li ||| yan zhang ||| 
2019 ||| attention-based hierarchical recurrent neural network for phenotype classification. ||| nan xu ||| yanyan shen ||| yanmin zhu ||| 
2020 ||| mask-guided region attention network for person re-identification. ||| cong zhou ||| han yu ||| 
2020 ||| relational metric learning with dual graph attention networks for social recommendation. ||| xiaodong wang ||| zhen liu ||| nana wang ||| wentao fan ||| 
2019 ||| dependency-aware attention model for emotion analysis for online news. ||| xue zhao ||| ying zhang ||| xiaojie yuan ||| 
2021 ||| scarlet: explainable attention based graph neural network for fake news spreader prediction. ||| bhavtosh rath ||| xavier morales ||| jaideep srivastava ||| 
2019 ||| semi-interactive attention network for answer understanding in reverse-qa. ||| qing yin ||| guan luo ||| xiaodong zhu ||| qinghua hu ||| ou wu ||| 
2021 ||| densely connected graph attention network based on iterative path reasoning for document-level relation extraction. ||| hongya zhang ||| zhen huang ||| zhenzhen li ||| dongsheng li ||| feng liu ||| 
2020 ||| temporalgat: attention-based dynamic graph representation learning. ||| ahmed fathy ||| kan li ||| 
2019 ||| consistency checking of attention aware systems. ||| yensen lim ||| n ||| everardo b ||| rcenas ||| edgard ben ||| tez-guerrero ||| javier gomez ||| 
2021 ||| fast and precise certification of transformers. ||| gregory bonaert ||| dimitar i. dimitrov ||| maximilian baader ||| martin t. vechev ||| 
2021 ||| generating bug-fixes using pretrained transformers. ||| dawn drain ||| chen wu ||| alexey svyatkovskiy ||| neel sundaresan ||| 
2020 ||| prediction of protein tertiary structure using pre-trained self-supervised learning based on transformer. ||| alif kurniawan ||| wisnu jatmiko ||| rukman hertadi ||| novian habibie ||| 
2021 ||| canonical segmentation using affix characters as a unit on transformer for javanese language. ||| sri hartati wijono ||| machmud roby alhamidi ||| muhammad hafizhuddin hilman ||| wisnu jatmiko ||| 
2021 ||| clamp: cross-level attention for multi-party conversational emotion recognition. ||| fernando h. calderon alvarado ||| mau-yun ma ||| yen-hao huang ||| yi-shin chen ||| 
2020 ||| attention-guided generative adversarial network to address atypical anatomy in synthetic ct generation. ||| hajar emami ||| ming dong ||| carri k. glide-hurst ||| 
2021 ||| learning dynamic connectivity with residual-attention network for autism classification in 4d fmri brain images. ||| kyoung-won park ||| seok-jun bu ||| sung-bae cho ||| 
2019 ||| orchids classification using spatial transformer network with adaptive scaling. ||| watcharin sarachai ||| jakramate bootkrajang ||| jeerayut chaijaruwanich ||| samerkae somhom ||| 
2021 ||| directional graph transformer-based control flow embedding for malware classification. ||| hyung-jun moon ||| seok-jun bu ||| sung-bae cho ||| 
2020 ||| driver monitoring system based on cnn models: an approach for attention level detection. ||| myriam elizabeth vaca recalde ||| joshu |||  p ||| rez ||| javier echanobe ||| 
2020 ||| an automatic glioma segmentation system based on a separable attention u-net (saunet). ||| zhenyu zhang ||| shouwei gao ||| zheng huang ||| 
2020 ||| boundary-attention loss function in neural network for pathological lymph nodes segmentation based on pet/ct images. ||| guoping xu ||| hanqiang cao ||| guoxing jiang ||| 
2021 ||| a multi-scale self-attention network for diabetic retinopathy retrieval. ||| ming zeng ||| jiansheng fang ||| hanpei miao ||| tianyang zhang ||| jiang liu ||| 
2017 ||| attention-based recurrent neural network for location recommendation. ||| bin xia ||| yun li ||| qianmu li ||| tao li ||| 
2019 ||| the attention based blstm model integrating sentence embeddings for biomedical event trigger identification. ||| yuxuan wang ||| bo yu ||| hui shi ||| xinyu he ||| yonggong ren ||| 
2019 ||| image caption model based on multi-head attention and encoder-decoder framework. ||| jianwei luo ||| li ma ||| 
2019 ||| inter-person relation classification via attentionbased bidirectional gated recurrent unit. ||| dandan zhao ||| degen huang ||| jiana meng ||| jing zhang ||| shichang sun ||| yuhai yu ||| 
2019 ||| msanet: a multi-scale attention module. ||| yucheng huang ||| wei liu ||| chao li ||| yongsheng liang ||| huo-xiang yang ||| fanyang meng ||| 
2019 ||| english drug name entity recognition method based on attention mechanism bilstm-crf. ||| yu zhang ||| li guo ||| degen huang ||| kaiyu huang ||| jiuyi li ||| zhang pan ||| 
2019 ||| pay attention, please: formal language improves attention in volunteer and paid online experiments. ||| tal august ||| katharina reinecke ||| 
2017 ||| attention allocation aid for visual search. ||| arturo deza ||| jeffrey r. peters ||| grant s. taylor ||| amit surana ||| miguel p. eckstein ||| 
2018 ||| intelligent interruptions for ivr: investigating the interplay between presence, workload and attention. ||| ceenu george ||| manuel demmler ||| heinrich hussmann ||| 
2020 ||| red alert: a cognitive countermeasure to mitigate attentional tunneling. ||| julie saint-lot ||| jean-paul imbert ||| fr ||| d ||| ric dehais ||| 
2018 ||| increasing user attention with a comic-based policy. ||| madiha tabassum ||| abdulmajeed alqhatani ||| marran aldossari ||| heather richter lipford ||| 
2017 ||| attention, comprehension, execution: effects of different designs of biofeedback display. ||| zhida sun ||| nan cao ||| xiaojuan ma ||| 
2020 ||| turkeyes: a web-based toolbox for crowdsourcing attention data. ||| anelise newman ||| barry a. mcnamara ||| camilo fosco ||| yun bin zhang ||| pat sukhum ||| matthew tancik ||| nam wook kim ||| zoya bylinskii ||| 
2020 ||| enhancing social attention using eye-movement modeling and simulated dyadic social interactions. ||| catherine a. bacos ||| 
2019 ||| dynamics of visual attention in multiparty collaborative problem solving using multidimensional recurrence quantification analysis. ||| hana vrzakova ||| mary jean amon ||| angela e. b. stewart ||| sidney k. d'mello ||| 
2021 ||| do cross-cultural differences in visual attention patterns affect search efficiency on websites? ||| amanda baughan ||| nigini oliveira ||| tal august ||| naomi yamashita ||| katharina reinecke ||| 
2020 ||| quantification of users' visual attention during everyday mobile device interactions. ||| mihai b ||| ce ||| sander staal ||| andreas bulling ||| 
2020 ||| attention-aware brain computer interface to avoid distractions in augmented reality. ||| lisa-marie vortmann ||| felix putze ||| 
2017 ||| modeling sub-document attention using viewport time. ||| max grusky ||| jeiran jahani ||| josh schwartz ||| dan valente ||| yoav artzi ||| mor naaman ||| 
2017 ||| facial thermography for attention tracking on smart eyewear: an initial study. ||| benjamin tag ||| ryan mannschreck ||| kazunori sugiura ||| george chernyshov ||| naohisa ohta ||| kai kunze ||| 
2017 ||| a framework for interactive mindfulness meditation using attention-regulation process. ||| kavous salehzadeh niksirat ||| chaklam silpasuwanchai ||| mahmoud mohamed hussien ahmed ||| peng cheng ||| xiangshi ren ||| 
2020 ||| hivefive: immersion preserving attention guidance in virtual reality. ||| daniel lange ||| tim claudius stratmann ||| uwe gruenefeld ||| susanne boll ||| 
2021 ||| breaking out of the lab: mitigating mind wandering with gaze-based attention-aware technology in classrooms. ||| stephen hutt ||| kristina krasich ||| james r. brockmole ||| sidney k. d'mello ||| 
2019 ||| aila: attentive interactive labeling assistant for document classification through attention-based deep neural networks. ||| minsuk choi ||| cheonbok park ||| soyoung yang ||| yonggyu kim ||| jaegul choo ||| sungsoo ray hong ||| 
2017 ||| undertanding and detecting divided attention in mobile mooc learning. ||| xiang xiao ||| jingtao wang ||| 
2020 ||| faces of focus: a study on the facial cues of attentional states. ||| ebrahim babaei ||| namrata srivastava ||| joshua newn ||| qiushi zhou ||| tilman dingler ||| eduardo velloso ||| 
2021 ||| human-ai interactive and continuous sensemaking: a case study of image classification using scribble attention maps. ||| haifeng shen ||| kewen liao ||| zhibin liao ||| job n. doornberg ||| maoying qiao ||| anton van den hengel ||| johan w. verjans ||| 
2019 ||| search as news curator: the role of google in shaping attention to news information. ||| daniel trielli ||| nicholas diakopoulos ||| 
2021 ||| mindless attractor: a false-positive resistant intervention for drawing attention using auditory perturbation. ||| riku arakawa ||| hiromu yakura ||| 
2020 ||| using mobile augmented reality to improve attention in adults with autism spectrum disorder. ||| katherine wang ||| bingqing zhang ||| youngjun cho ||| 
2020 ||| classification of functional attention in video meetings. ||| anastasia kuzminykh ||| sean rintel ||| 
2020 ||| the interaction attention continuum: an education case study. ||| misha croes ||| michel van dartel ||| 
2021 ||| impact of task on attentional tunneling in handheld augmented reality. ||| brandon victor syiem ||| ryan m. kelly ||| jorge gon ||| alves ||| eduardo velloso ||| tilman dingler ||| 
2019 ||| "watch out!": semi-autonomous vehicles using assertive voices to grab distracted drivers' attention. ||| priscilla n. y. wong ||| duncan p. brumby ||| harsha vardhan ramesh babu ||| kota kobayashi ||| 
2019 ||| towards novel urban planning methods - using eye-tracking systems to understand human attention in urban environments. ||| teija vainio ||| ilari karppi ||| ari jokinen ||| helena leino ||| 
2019 ||| feature extraction and classification of odor using attention based neural network. ||| kohei fukuyama ||| kenji matsui ||| sigeru omatu ||| alberto rivas ||| juan manuel corchado ||| 
2020 ||| dynamic multi-level attention models for dialogue response generation. ||| yanmeng wang ||| wenge rong ||| shijie zhou ||| yuanxin ouyang ||| zhang xiong ||| 
2020 ||| s-cogit: a natural language processing tool for linguistic analysis of the social interaction between individuals with attention-deficit disorder. ||| jairo i. v ||| lez ||| luis fernando castillo ||| manuel gonz ||| lez bedia ||| 
2021 ||| malware analysis with artificial intelligence and a particular attention on results interpretability. ||| benjamin marais ||| tony quertier ||| christophe chesneau ||| 
2019 ||| ld-parser: leaf detection based dependency parsing using bilstm and attention mechanism. ||| weidong wen ||| zhonglu wang ||| jianbo liu ||| chen chen ||| ni li ||| 
2021 ||| social media named entity recognition based on graph attention network. ||| wei zhang ||| jianying luo ||| kehua yang ||| 
2021 ||| transformer based refinement network for accurate crack detection. ||| jing-ming guo ||| herleeyandi markoni ||| 
2021 ||| impact of virtual reality head mounted display on the attentional visual field. ||| vasilii marshev ||| jean-louis de bougrenet de la tocnaye ||| b ||| atrice cochener ||| vincent nourrit ||| 
2017 ||| tilers, tilemakers, transformers! ||| stefan langerman ||| 
2019 ||| an attention-based recurrent convolutional network for vehicle taillight recognition. ||| kuan-hui lee ||| takaaki tagawa ||| jia-en m. pan ||| adrien gaidon ||| bertrand douillard ||| 
2020 ||| deep learning with attention mechanism for predicting driver intention at intersection. ||| abenezer girma ||| seifemichael b. amsalu ||| abrham workineh ||| mubbashar altaf khan ||| abdollah homaifar ||| 
2021 ||| stgt: forecasting pedestrian motion using spatio-temporal graph transformer. ||| arsal syed ||| brendan morris ||| 
2021 ||| bi-directional attention feature enhancement for video instance segmentation. ||| tianyun fu ||| jianming hu ||| 
2021 ||| predicting vehicles trajectories in urban scenarios with transformer networks and augmented information. ||| lvaro quintanar ||| david fern ||| ndez llorca ||| ignacio parra ||| rub ||| n izquierdo ||| miguel  ||| ngel sotelo ||| 
2020 ||| multi-head attention based probabilistic vehicle trajectory prediction. ||| hayoung kim ||| dongchan kim ||| gihoon kim ||| jeongmin cho ||| kunsoo huh ||| 
2020 ||| attention r-cnn for accident detection. ||| trung-nghia le ||| shintaro ono ||| akihiro sugimoto ||| hiroshi kawasaki ||| 
2021 ||| trajectory prediction for autonomous driving based on multi-head attention with joint agent-map representation. ||| kaouther messaoud ||| nachiket deo ||| mohan m. trivedi ||| fawzi nashashibi ||| 
2017 ||| a computational framework for driver's visual attention using a fully convolutional architecture. ||| ashish tawari ||| byeongkeun kang ||| 
2021 ||| scout: socially-consistent and understandable graph attention network for trajectory prediction of vehicles and vrus. ||| sandra carrasco ||| david fern ||| ndez llorca ||| miguel  ||| ngel sotelo ||| 
2019 ||| attention monitoring and hazard assessment with bio-sensing and vision: empirical analysis utilizing cnns on the kitti dataset. ||| siddharth ||| mohan m. trivedi ||| 
2021 ||| novelty detection and analysis of traffic scenario infrastructures in the latent space of a vision transformer-based triplet autoencoder. ||| jonas wurst ||| lakshman balasubramanian ||| michael botsch ||| wolfgang utschick ||| 
2019 ||| visual explanation by attention branch network for end-to-end learning-based self-driving. ||| keisuke mori ||| hiroshi fukui ||| takuya murase ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2017 ||| detection and recognition of traffic signs inside the attentional visual field of drivers. ||| s. j. zabihi ||| s. m. zabihi ||| steven s. beauchemin ||| michael a. bauer ||| 
2021 ||| pedestrian trajectory prediction via spatial interaction transformer network. ||| tong su ||| yu meng ||| yan xu ||| 
2021 ||| eeg-based system using deep learning and attention mechanism for driver drowsiness detection. ||| miankuan zhu ||| haobo li ||| jiangfan chen ||| mitsuhiro kamezaki ||| zutao zhang ||| zexi hua ||| shigeki sugano ||| 
2021 ||| dr-tanet: dynamic receptive temporal attention network for street scene change detection. ||| shuo chen ||| kailun yang ||| rainer stiefelhagen ||| 
2020 ||| traffic agent trajectory prediction using social convolution and attention mechanism. ||| tao yang ||| zhixiong nan ||| he zhang ||| shitao chen ||| nanning zheng ||| 
2018 ||| on the novel approach to parallel coupled-line bandpass filters that have diverse wavelenght impedance scaling i/o transformers. ||| marek bogdan zaradny ||| 
2017 ||| compact thermal model of planar transformers. ||| krzysztof g ||| recki ||| krzysztof gorski ||| 
2020 ||| histopathologic cancer detection by dense-attention network with incorporation of prior knowledge. ||| mingyuan liu ||| yang yu ||| qingcheng liao ||| jicong zhang ||| 
2021 ||| two-stream attention spatio-temporal network for classification of echocardiography videos. ||| zishun feng ||| joseph a. sivak ||| ashok k. krishnamurthy ||| 
2020 ||| synaptic partner assignment using attentional voxel association networks. ||| nicholas l. turner ||| kisuk lee ||| ran lu ||| jingpeng wu ||| dodam ih ||| h. sebastian seung ||| 
2021 ||| deep transformers for fast small intestine grounding in capsule endoscope video. ||| xinkai zhao ||| chaowei fang ||| feng gao ||| de-jun fan ||| xutao lin ||| guanbin li ||| 
2020 ||| spectral graph transformer networks for brain surface parcellation. ||| ran he ||| karthik gopinath ||| christian desrosiers ||| herve lombaert ||| 
2019 ||| mri reconstruction via cascaded channel-wise attention network. ||| qiaoying huang ||| dong yang ||| pengxiang wu ||| hui qu ||| jingru yi ||| dimitris n. metaxas ||| 
2021 ||| smocam: smooth conditional attention mask for 3d-regression models. ||| salamata konate ||| l ||| o lebrat ||| rodrigo santa cruz ||| pierrick bourgeat ||| vincent dor ||| jurgen fripp ||| andrew p. bradley ||| clinton fookes ||| olivier salvado ||| 
2021 ||| adasan: adaptive cosine similarity self-attention network for gastrointestinal endoscopy image classification. ||| qian zhao ||| wenming yang ||| qingmin liao ||| 
2021 ||| parallel res2net-based network with reverseattention for polyp segmentation. ||| chenghui yu ||| jiangpeng yana ||| xiu li ||| 
2019 ||| epithelial segmentation from in situ hybridisation histological samples using a deep central attention learning approach. ||| tzu-hsi song ||| gabriel landini ||| shereen fouad ||| hisham mehanna ||| 
2019 ||| attentionnet: learning where to focus via attention mechanism for anatomical segmentation of whole breast ultrasound images. ||| hang li ||| jie-zhi cheng ||| yi-hong chou ||| jing qin ||| shan huang ||| baiying lei ||| 
2020 ||| relational learning between multiple pulmonary nodules via deep set attention transformers. ||| jiancheng yang ||| haoran deng ||| xiaoyang huang ||| bingbing ni ||| yi xu ||| 
2017 ||| decoding dynamic auditory attention during naturalistic experience. ||| liting wang ||| xintao hu ||| meng wang ||| jinglei lv ||| junwei han ||| shijie zhao ||| qinglin dong ||| lei guo ||| tianming liu ||| 
2020 ||| fine-grained multi-instance classification in microscopy through deep attention. ||| mengran fan ||| tapabrata chakraborti ||| eric i-chao chang ||| yan xu ||| jens rittscher ||| 
2020 ||| efficient aortic valve multilabel segmentation using a spatial transformer network. ||| daniel h. pak ||| andr ||| s caballero ||| wei sun ||| james s. duncan ||| 
2019 ||| residual attention based network for hand bone age assessment. ||| eric wu ||| bin kong ||| xin wang ||| junjie bai ||| yi lu ||| feng gao ||| shaoting zhang ||| kunlin cao ||| qi song ||| siwei lyu ||| youbing yin ||| 
2020 ||| longitudinal analysis of mild cognitive impairment via sparse smooth network and attention-based stacked bi-directional long-short term memory. ||| dongdong liu ||| yanwu xu ||| ahmed elazab ||| peng yang ||| wei wang ||| tianfu wang ||| baiying lei ||| 
2020 ||| weakly-supervised balanced attention network for gastric pathology image localization and classification. ||| zhonghang zhu ||| xin ding ||| defu zhang ||| liansheng wang ||| 
2019 ||| recurrent attention mechanism networks for enhanced classification of biomedical images. ||| mazhar shaikh ||| varghese alex kollerathu ||| ganapathy krishnamurthi ||| 
2021 ||| mga-net: multi-scale guided attention models for an automated diagnosis of idiopathic pulmonary fibrosis (ipf). ||| wenxi yu ||| hua zhou ||| youngwon choi ||| jonathan g. goldin ||| hyun j. grace kim ||| 
2020 ||| super-resolution and self-attention with generative adversarial network for improving malignancy characterization of hepatocellular carcinoma. ||| yunling li ||| hui huang ||| lijuan zhang ||| guangyi wang ||| honglai zhang ||| wu zhou ||| 
2021 ||| multi-channel sparse graph transformer network for early alzheimer's disease identification. ||| yali qiu ||| shuangzhi yu ||| yanhong zhou ||| dongdong liu ||| xuegang song ||| tianfu wang ||| baiying lei ||| 
2021 ||| automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. ||| giammarco la barbera ||| pietro gori ||| haithem boussaid ||| bruno belucci ||| alessandro delmonte ||| jeanne goulin ||| sabine sarnacki ||| laurence rouet ||| isabelle bloch ||| 
2019 ||| a novel focal tversky loss function with improved attention u-net for lesion segmentation. ||| nabila abraham ||| naimul mefraz khan ||| 
2021 ||| retinal vessel segmentation via context guide attention net with joint hard sample mining strategy. ||| changwei wang ||| rongtao xu ||| yuyang zhang ||| shibiao xu ||| xiaopeng zhang ||| 
2020 ||| jointly analyzing alzheimer's disease related structure-function using deep cross-model attention network. ||| lu zhang ||| li wang ||| dajiang zhu ||| 
2020 ||| classification of ocular diseases employing attention-based unilateral and bilateral feature weighting and fusion. ||| junjun he ||| cheng li ||| jin ye ||| shanshan wang ||| yu qiao ||| lixu gu ||| 
2019 ||| look, investigate, and classify: a deep hybrid attention method for breast cancer classification. ||| bolei xu ||| jingxin liu ||| xianxu hou ||| bozhi liu ||| jon garibaldi ||| ian o. ellis ||| andy green ||| linlin shen ||| guoping qiu ||| 
2020 ||| a multi-modality fusion network based on attention mechanism for brain tumor segmentation. ||| tongxue zhou ||| su ruan ||| yu guo ||| st ||| phane canu ||| 
2021 ||| mssa-net: multi-scale self-attention network for breast ultrasound image segmentation. ||| meng xu ||| kuan huang ||| qiuxiao chen ||| xiaojun qi ||| 
2020 ||| ceus-net: lesion segmentation in dynamic contrast-enhanced ultrasound with feature-reweighted attention mechanism. ||| peng wan ||| fang chen ||| xiaowei zhu ||| chunrui liu ||| yidan zhang ||| wentao kong ||| daoqiang zhang ||| 
2020 ||| csaf-cnn: cross-layer spatial attention map fusion network for organ-at-risk segmentation in head and neck ct images. ||| zuhao liu ||| huan wang ||| wenhui lei ||| guotai wang ||| 
2021 ||| structural visual guidance attention networks in retinopathy of prematurity. ||| veysi yildiz ||| stratis ioannidis ||| ilkay yildiz ||| peng tian ||| john peter campbell ||| susan ostmo ||| jayashree kalpathy-cramer ||| michael f. chiang ||| deniz erdogmus ||| jennifer g. dy ||| 
2021 ||| medical image enhancement for lesion detection based on class-aware attention and deep colorization. ||| jiachang guo ||| jun chen ||| chao lu ||| haifeng huang ||| 
2020 ||| attentionanatomy: a unified framework for whole-body organs at risk segmentation using multiple partially annotated datasets. ||| shanlin sun ||| yang liu ||| narisu bai ||| hao tang ||| xuming chen ||| qian huang ||| yong liu ||| xiaohui xie ||| 
2020 ||| deep learning fast mri using channel attention in magnitude domain. ||| joonhyung lee ||| hyunjong kim ||| hyungjin chung ||| jong chul ye ||| 
2021 ||| asymmetric attention upsampling: rethinking upsampling for biological image segmentation. ||| chunyu dong ||| qunfei zhao ||| kun chen ||| xiaolin huang ||| 
2019 ||| focusnet: an attention-based fully convolutional network for medical image segmentation. ||| chaitanya kaul ||| suresh manandhar ||| nick e. pears ||| 
2021 ||| focal-balanced attention u-net with dynamic thresholding by spatial regression for segmentation of aortic dissection in ct imagery. ||| tsung-han lee ||| li-ting huang ||| paul kuo ||| chien-kuo wang ||| jiun-in guo ||| 
2020 ||| robust brain magnetic resonance image segmentation for hydrocephalus patients: hard and soft attention. ||| xuhua ren ||| jiayu huo ||| kai xuan ||| dongming wei ||| lichi zhang ||| qian wang ||| 
2021 ||| an attention-based hybrid deep learning framework integrating temporal coherence and dynamics for discriminating schizophrenia. ||| min zhao ||| weizheng yan ||| rongtao xu ||| dongmei zhi ||| rongtao jiang ||| tianzi jiang ||| vince d. calhoun ||| jing sui ||| 
2021 ||| global multi-level attention network for the segmentation of clinical target volume in the planning ct for cervical cancer. ||| huite yi ||| jun shi ||| bing yan ||| xudong xue ||| hong an ||| hongyan zhang ||| 
2020 ||| attention-based cnn for kl grade classification: data from the osteoarthritis initiative. ||| bofei zhang ||| jimin tan ||| kyunghyun cho ||| gregory chang ||| cem m. deniz ||| 
2021 ||| attention-guided deep multi-instance learning for staging retinopathy of prematurity. ||| shaobin chen ||| rugang zhang ||| guozhen chen ||| jinfeng zhao ||| tianfu wang ||| guoming zhang ||| baiying lei ||| 
2021 ||| mda-net: multi-dimensional attention-based neural network for 3d image segmentation. ||| rutu gandhi ||| yi hong ||| 
2019 ||| region proposal networks with contextual selective attention for real-time organ detection. ||| awais mansoor ||| antonio r. porras ||| marius george linguraru ||| 
2019 ||| self-attention equipped graph convolutions for disease prediction. ||| anees kazi ||| s. arvind krishna ||| shayan shekarforoush ||| karsten kortuem ||| shadi albarqouni ||| nassir navab ||| 
2020 ||| surround sound spreads visual attention and increases cognitive effort in immersive media reproductions. ||| catarina mendon ||| a ||| victoria korshunova ||| 
2018 ||| a prototype mixer to improve cross-modal attention during audio mixing. ||| joshua mycroft ||| tony stockman ||| j. d. reiss ||| 
2018 ||| my sound space: an attentional shield for immersive redirection. ||| martin ljungdahl eriksson ||| lena pareto ||| ricardo atienza ||| kjetil falkenberg hansen ||| 
2021 ||| audio-visual interactive art: investigating the effect of gaze-controlled audio on visual attention and short term memory. ||| josefine h ||| lling ||| maria svahn ||| sandra pauletto ||| 
2020 ||| bi-lattice lstm model with self-attention for chinese ner. ||| meng yuan ||| yubai li ||| 
2021 ||| short-term power load probability density forecasting based on a double-layer lstm-attention quantile regression. ||| xiaofeng tao ||| yang lu ||| xueliang yang ||| 
2020 ||| modeling and analysis of three-phase distribution transformer connections by phase-coordinates based on matrix operation method. ||| zhigang zhang ||| mingrui mo ||| caizhu wu ||| 
2019 ||| multi-layer attention mechanism based speech separation model. ||| meng li ||| tian lan ||| chuan peng ||| yuxin qian ||| qiao liu ||| 
2020 ||| pos scaling attention model for joint slot filling and intent classification. ||| chao wei ||| ke yu ||| xiaofei wu ||| 
2020 ||| ampa-net: optimization-inspired attention neural network for deep compressed sensing. ||| nanyu li ||| charles c. zhou ||| 
2021 ||| tpe-mha: a malicious traffic detection model based on time position encoding and multi-head attention. ||| yi zhai ||| bin lu ||| xiaowei li ||| 
2020 ||| a comparative research on the influence of commercial complex waterscape atrium on human emotion based on computer visual attention and eeg data. ||| yunrui sun ||| suliu chen ||| 
2020 ||| semantic segmentation of high resolution remote sensing images with extra context attention mechanism. ||| weifu fu ||| qing peng ||| yanxiang gong ||| mei xie ||| shicheng wang ||| feng li ||| 
2020 ||| electromagnetic parameters optimization design of industrial dc transformer based on improved genetic algorithm. ||| li hai ||| jinfeng xiao ||| 
2021 ||| cross-channel fusion image dehazing network with feature attention. ||| yong liu ||| xiaorong hou ||| 
2021 ||| attention mechanism-driven potential fault cause identification in optical networks. ||| chunyu zhang ||| danshi wang ||| jinwei jia ||| lingling wang ||| songlin liu ||| luyao guan ||| min zhang ||| 
2020 ||| business model canvas should pay more attention to the software startup team. ||| kai-kristian kemell ||| atte elonen ||| mari suoranta ||| anh nguyen-duc ||| juan garbajosa ||| rafael chanin ||| jorge melegati ||| usman rafiq ||| abdullah aldaeej ||| nana assyne ||| afonso sales ||| sami hyrynsalmi ||| juhani risku ||| henry edison ||| pekka abrahamsson ||| 
2021 ||| classification of autism spectrum disorder severity using eye tracking data based on visual attention model. ||| mirian c. revers ||| jessica s. oliveira ||| felipe o. franco ||| joana portolese ||| thiago v. cardoso ||| andr ||| ia f. silva ||| ariane machado-lima ||| f ||| tima l. s. nunes ||| helena brentani ||| 
2021 ||| assessing the clinical validity of attention-based and shap temporal explanations for adverse drug event predictions. ||| jonathan rebane ||| isak samsten ||| panteleimon pantelidis ||| panagiotis papapetrou ||| 
2021 ||| personalised short-term glucose prediction via recurrent self-attention network. ||| ran cui ||| chirath hettiarachchi ||| christopher j. nolan ||| elena daskalaki ||| hanna suominen ||| 
2017 ||| an eye tracker based computer system to support oculomotor and attention deficit investigations. ||| daniela giordano ||| carmelo pino ||| isaak kavasidis ||| concetto spampinato ||| massimo di pietro ||| renata rizzo ||| anna scuderi ||| rita barone ||| 
2020 ||| exploring visual attention and machine learning in 3d visualization of medical temporal data. ||| leonardo souza silva ||| renan vinicius aranha ||| matheus alberto de oliveira ribeiro ||| luiz ricardo nakamura ||| f ||| tima l. s. nunes marques ||| 
2021 ||| apehr: automated prognosis in electronic health records using multi-head self-attention. ||| alexander ylnner choquenaira florez ||| lucas c. scabora ||| danilo medeiros eler ||| jos |||  f. rodrigues jr. ||| 
2021 ||| trident: change point detection for multivariate time series via dual-level attention learning. ||| ziyi duan ||| haizhou du ||| yang zheng ||| 
2020 ||| the impact of constant field of attention on properties of contextual neural networks. ||| erik dawid burnell ||| krzysztof wolk ||| krzysztof waliczek ||| rafal kern ||| 
2021 ||| residual attention network vs real attention on aesthetic assessment. ||| ranju mandal ||| susanne becken ||| rod m. connolly ||| bela stantic ||| 
2020 ||| dynamic prototype selection by fusing attention mechanism for few-shot relation classification. ||| linfang wu ||| hua-ping zhang ||| yaofei yang ||| xin liu ||| kai gao ||| 
2021 ||| the impact of aggregation window width on properties of contextual neural networks with constant field of attention. ||| miroslava mikusov ||| antonin fuchs ||| marcin jodlowiec ||| erik dawid burnell ||| krzysztof wolk ||| 
2020 ||| stock return prediction using dual-stage attention model with stock relation inference. ||| tanawat chiewhawan ||| peerapon vateekul ||| 
2021 ||| empirical study of tweets topic classification using transformer-based language models. ||| ranju mandal ||| jinyan chen ||| susanne becken ||| bela stantic ||| 
2020 ||| antidote: attention-based dynamic optimization for neural network runtime efficiency. ||| fuxun yu ||| chenchen liu ||| di wang ||| yanzhi wang ||| xiang chen ||| 
2021 ||| in-memory computing based accelerator for transformer networks for long sequences. ||| ann franchesca laguna ||| arman kazemi ||| michael t. niemier ||| x. sharon hu ||| 
2019 ||| ean: event attention network for stock price trend prediction based on sentimental embedding. ||| yaowei wang ||| qing li ||| zhexue huang ||| mark junjie li ||| 
2018 ||| collective attention towards scientists and research topics. ||| claudia wagner ||| olga zagovora ||| tatiana sennikova ||| fariba karimi ||| 
2021 ||| efficient detection of multilingual hate speech by using interactive attention network with minimal human feedback. ||| fedor vitiugin ||| yasas senarath ||| hemant purohit ||| 
2019 ||| characterizing attention cascades in whatsapp groups. ||| josemar alves caetano ||| gabriel magno ||| marcos andr |||  gon ||| alves ||| jussara m. almeida ||| humberto torres marques-neto ||| virg ||| lio a. f. almeida ||| 
2020 ||| act : automatic fake news classification through self-attention. ||| nujud aloshban ||| 
2017 ||| the effect of collective attention on controversial debates on social media. ||| kiran garimella ||| gianmarco de francisci morales ||| aristides gionis ||| michael mathioudakis ||| 
2019 ||| frequency domain transformer networks for video prediction. ||| hafez farazi ||| sven behnke ||| 
2018 ||| image-to-text transduction with spatial self-attention. ||| sebastian springenberg ||| egor lakomkin ||| cornelius weber ||| stefan wermter ||| 
2020 ||| motion segmentation using frequency domain transformer networks. ||| hafez farazi ||| sven behnke ||| 
2018 ||| regularize and explicit collaborative filtering with textual attention. ||| charles-emmanuel dias ||| vincent guigue ||| patrick gallinari ||| 
2017 ||| attention-based information fusion using multi-encoder-decoder recurrent neural networks. ||| stephan baier ||| sigurd spieckermann ||| volker tresp ||| 
2021 ||| attention-based hybrid precoding for mmwave mimo systems. ||| hao jiang ||| yu lu ||| xueru li ||| bichai wang ||| yongxing zhou ||| linglong dai ||| 
2021 ||| adaptive parking slot occupancy detection using vision transformer and llie. ||| karthick pannerselvam ||| 
2020 ||| attention-enabled network-level traffic speed prediction. ||| shuyi yin ||| jiahui wang ||| zhiyong cui ||| yinhai wang ||| 
2018 ||| coarse to fine: multi-label image classification with global/local attention. ||| fan lyu ||| fuyuan hu ||| victor s. sheng ||| zhengtian wu ||| qiming fu ||| baochuan fu ||| 
2021 ||| towards a real-time system based on regression model to evaluate driver's attention. ||| thiago k. lago ||| ernesto rodr ||| guez gonz ||| lez ||| miguel elias m. campista ||| 
2018 ||| eye-tracking for user attention evaluation in adaptive serious games. ||| alexander streicher ||| sebastian leidig ||| wolfgang roller ||| 
2020 ||| a novel collaborative filtering framework based on variational self-attention gan. ||| weifeng sun ||| shumiao yu ||| jin yang ||| boxiang dong ||| 
2021 ||| augmented convolutional neural networks with transformer for wireless interference identification. ||| pengyu wang ||| yufan cheng ||| binhong dong ||| 
2021 ||| attention-aware multi-encoder for session-based recommendation. ||| jin wei ||| linjie zhang ||| xiaoyan zhu ||| jianfeng ma ||| 
2021 ||| mrainf: multilayer relation attention based social influence prediction net with local stimulation. ||| zhenhua tan ||| fan li ||| danke wu ||| 
2021 ||| ppdtsa: privacy-preserving deep transformation self-attention framework for object detection. ||| bo ma ||| jinsong wu ||| edmund lai ||| shuolin hu ||| 
2020 ||| a video popularity prediction scheme with attention-based lstm and feature embedding. ||| longwei yang ||| xin guo ||| haiming wang ||| wei chen ||| 
2021 ||| distributed signal strength prediction using satellite map empowered by deep vision transformer. ||| haiyao yu ||| zhanwei hou ||| yifan gu ||| peng cheng ||| wanli ouyang ||| yonghui li ||| branka vucetic ||| 
2021 ||| mcformer: a transformer based deep neural network for automatic modulation classification. ||| shahab hamidi-rad ||| swayambhoo jain ||| 
2021 ||| an attention-aided deep neural network design for channel estimation in massive mimo systems. ||| jiabao gao ||| mu hu ||| caijun zhong ||| zhaoyang zhang ||| geoffrey ye li ||| 
2021 ||| performance optimization for semantic communications: an attention-based learning approach. ||| yining wang ||| mingzhe chen ||| walid saad ||| tao luo ||| shuguang cui ||| h. vincent poor ||| 
2021 ||| deep learning based ofdm channel estimation using frequency-time division and attention mechanism. ||| ang yang ||| peng sun ||| tamrakar rakesh ||| bule sun ||| fei qin ||| 
2019 ||| adaptive multi-attention convolutional neural network for fine-grained image recognition. ||| ang li ||| jianxin chen ||| bin kang ||| wenqin zhuang ||| xuguang zhang ||| 
2018 ||| decoding behavioral accuracy in an attention task using brain fmri data. ||| zhe wang ||| yu zheng ||| michael jigo ||| taosheng liu ||| jian ren ||| zhi tian ||| tongtong li ||| 
2021 ||| packet routing with graph attention multi-agent reinforcement learning. ||| xuan mai ||| quanzhi fu ||| yi chen ||| 
2019 ||| graph attention spatial-temporal network for deep learning based mobile traffic prediction. ||| kaiwen he ||| yufen huang ||| xu chen ||| zhi zhou ||| shuai yu ||| 
2019 ||| a deep learning framework with spatial-temporal attention mechanism for cellular traffic prediction. ||| yun gao ||| xin wei ||| liang zhou ||| haibing lv ||| 
2021 ||| attention mechanism based resnext network for automatic modulation classification. ||| zhi liang ||| ling wang ||| mingliang tao ||| jian xie ||| xin yang ||| 
2021 ||| explainable health state prediction for social iots through multi-channel attention. ||| yu-li chan ||| hong-han shuai ||| 
2020 ||| machine learning-based regression and classification models for oil assessment of power transformers. ||| neha kamalraj bhatia ||| ayman h. el-hag ||| khaled bashir shaban ||| 
2021 ||| siamese attention and point adaptive network for visual tracking. ||| thang hoang dinh ||| long tran quoc ||| kien thai trung ||| 
2021 ||| quantum attention based language model for answer selection. ||| qin zhao ||| chenguang hou ||| ruifeng xu ||| 
2018 ||| sentiment analysis based on hybrid bi-attention mechanism in mobile application. ||| pengcheng zhu ||| yujiu yang ||| yi liu ||| 
2021 ||| multimodal social media sentiment analysis based on cross-modal hierarchical attention fusion. ||| kezhong wang ||| ting jin ||| 
2020 ||| attention-based asymmetric fusion network for saliency prediction in 3d images. ||| xinyue zhang ||| ting jin ||| 
2020 ||| attention-based interaction trajectory prediction. ||| zhe liu ||| lizong zhang ||| zhihong rao ||| guisong liu ||| 
2020 ||| deep reinforcement learning with transformers for text adventure games. ||| yunqiu xu ||| ling chen ||| meng fang ||| yang wang ||| chengqi zhang ||| 
2020 ||| influencing the affective state and attention restoration in vr-supported psychotherapy. ||| dietmar pisalski ||| mario hierhager ||| christoph stein ||| michael zaudig ||| christoph bichlmeier ||| 
2021 ||| designing vr games with gaze control for directing attention of children with adhd. ||| linda graf ||| leslie scholemann ||| maic masuch ||| 
2021 ||| inventory management with attention-based meta actions. ||| keisuke izumiya ||| edgar simo-serra ||| 
2021 ||| towards federated learning with attention transfer to mitigate system and data heterogeneity of clients. ||| hongrui shi ||| valentin radu ||| 
2022 ||| real-time style transfer with efficient vision transformers. ||| hadjer benmeziane ||| hamza ouarnoughi ||| kaoutar el maghraoui ||| sma ||| l niar ||| 
2019 ||| gesture class prediction by recurrent neural network and attention mechanism. ||| fajrian yunus ||| chlo |||  clavel ||| catherine pelachaud ||| 
2017 ||| you can leave your head on - attention management and turn-taking in multi-party interaction with a virtual human/robot duo. ||| jeroen linssen ||| meike berkhoff ||| max bode ||| eduard rens ||| mari ||| t theune ||| daan wiltenburg ||| 
2021 ||| attention-guidance method based on conforming behavior of multiple virtual agents for pedestrians. ||| naoto yoshida ||| tomoko yonezawa ||| 
2019 ||| effects of a virtual human appearance fidelity continuum on visual attention in virtual reality. ||| matias volonte ||| andrew t. duchowski ||| sabarish v. babu ||| 
2019 ||| can a signing virtual human engage a baby's attention? ||| setareh nasihati gilani ||| david r. traum ||| rachel sortino ||| grady gallagher ||| kailyn aaron-lozano ||| cryss padilla ||| ari shapiro ||| jason lamberton ||| laura-ann petitto ||| 
2017 ||| integration of multi-modal cues in synthetic attention processes to drive virtual agent behavior. ||| sven seele ||| tobias haubrich ||| tim metzler ||| jonas schild ||| rainer herpers ||| marcin grzegorzek ||| 
2021 ||| power transformer design resorting to metaheuristics techniques. ||| pedro alves ||| p. m. fonte ||| r. pereira ||| 
2021 ||| design of an attention tool using hci and work-related variables. ||| patricia gamboa ||| cl ||| udia quaresma ||| rui varandas ||| helena canh ||| o ||| rute dinis de sousa ||| ana m. rodrigues ||| sofia jacinto ||| jo ||| o rodrigues ||| c ||| tia cepeda ||| hugo gamboa ||| 
2017 ||| student's attention improvement supported by physiological measurements analysis. ||| andreia art ||| fice ||| fernando ferreira ||| elsa marcelino-jesus ||| jo ||| o sarraipa ||| ricardo jardim-gon ||| alves ||| 
2018 ||| high-frequency transformer isolated ac-dc converter for resilient low voltage dc residential grids. ||| nelson santos ||| j. fernando a. da silva ||| vasco soares ||| 
2017 ||| a generalized geometric programming sub-problem of transformer design optimization. ||| tam ||| s orosz ||| tam ||| s nagy ||| zolt ||| n  ||| d ||| m tamus ||| 
2020 ||| cascaded solid state transformer structure to power fast ev charging stations from medium voltage transmission lines. ||| syed rahman ||| ahmed imteaj ||| irfan khan ||| m. hadi amini ||| 
2021 ||| rational inattention in choice overload: clustering for discrete choices. ||| pankaj sharma ||| lav r. varshney ||| 
2021 ||| attention-based deep feature learning network for scene classification of hyperspectral images. ||| kejie xu ||| hong huang ||| peifang deng ||| 
2017 ||| modulation classification using convolutional neural networks and spatial transformer networks. ||| moein mirmohammadsadeghi ||| samer s. hanna ||| danijela cabric ||| 
2019 ||| meda: multi-output encoder-decoder for spatial attention in convolutional neural networks. ||| huayu li ||| abolfazl razi ||| 
2021 ||| two-exposure image fusion based on cross attention fusion. ||| sha-wo huang ||| yan-tsung peng ||| tzu-hsien chen ||| yung-ching yang ||| 
2021 ||| synthesized speech detection using convolutional transformer-based spectrogram analysis. ||| emily r. bartusiak ||| edward j. delp ||| 
2020 ||| pay attention to categories: syntax-based sentence modeling with metadata projection matrix. ||| won-ik cho ||| nam soo kim ||| 
2020 ||| attention-based domain adaption using transfer learning for part-of-speech tagging: an experiment on the hindi language. ||| rajesh kumar mundotiya ||| vikrant kumar ||| arpit mehta ||| anil kumar singh ||| 
2018 ||| customized attention mechanism for relation classification. ||| shirong shen ||| yang wen ||| lijuan zhou ||| hongying zan ||| 
2020 ||| understanding transformers for information extraction with limited data. ||| minh-tien nguyen ||| dung tien le ||| nguyen hong son ||| bui cong minh ||| do hoang thai duong ||| le thai linh ||| 
2018 ||| attention-based blstm-crf architecture for mongolian named entity recognition. ||| yuzhu xiong ||| minghua nuo ||| 
2020 ||| tdp - a hybrid diacritic restoration with transformer decoder. ||| dang trung anh ||| nguyen thi thu trang ||| 
2018 ||| feature attention network: interpretable depression detection from social media. ||| hoyun song ||| jinseon you ||| jin-woo chung ||| jong c. park ||| 
2017 ||| extracting important tweets for news writers using recurrent neural network with attention mechanism and multi-task learning. ||| taro miyazaki ||| shin toriumi ||| yuka takei ||| ichiro yamada ||| jun goto ||| 
2020 ||| improving sequence tagging for vietnamese text using transformer-based neural models. ||| the viet bui ||| thi oanh tran ||| phuong le-hong ||| 
2018 ||| detecting free translation in parallel corpora from attention scores. ||| qi chen ||| oi yee kwong ||| jingbo zhu ||| 
2018 ||| metaphor identification with paragraph and word vectorization: an attention-based neural approach. ||| timour igamberdiev ||| hyopil shin ||| 
2018 ||| japanese sentiment classification using a tree-structured long short-term memory with attention. ||| ryosuke miyazaki ||| mamoru komachi ||| 
2018 ||| semantic role labeling in conversational chat using deep bi-directional long short-term memory networks with attention mechanism. ||| valdi rachman ||| rahmad mahendra ||| alfan farizki wicaksono ||| ahmad rizqi meydiarso ||| fariz ikhwantri ||| 
2020 ||| imbalanced chinese multi-label text classification based on alternating attention. ||| hongliang bi ||| han hu ||| pengyuan liu ||| 
2020 ||| attention-based bidirectional long short-term memory neural network for short answer scoring. ||| linzhong xia ||| mingxiang guan ||| jun liu ||| xuemei cao ||| dean luo ||| 
2019 ||| cyberbullying detection with birnn and attention mechanism. ||| anman zhang ||| bohan li ||| shuo wan ||| kai wang ||| 
2020 ||| guiding the operator's attention among a plurality of operator workstation screens. ||| veronika domova ||| 
2020 ||| grid structure attention for natural language interface to bash commands. ||| jia-wei kan ||| wei-chin chien ||| sheng-de wang ||| 
2020 ||| evaluation of fatigue and attention levels in multi-target scenario using cnn. ||| d. sandeep vara sankar ||| li-wei ko ||| 
2020 ||| android malware detection system integrating block feature extraction and multi-head attention mechanism. ||| yi-ming chen ||| an-chi he ||| guo-chung chen ||| yu-chi liu ||| 
2018 ||| multimodal attention agents in visual conversation. ||| lorena kodra ||| elinda kajo me ||| e ||| 
2020 ||| attentional neural mechanisms for social recommendations in educational platforms. ||| italo zoppis ||| sara manzoni ||| giancarlo mauri ||| ricardo anibal matamoros aragon ||| luca marconi ||| francesco epifania ||| 
2020 ||| classification of students' conceptual understanding in stem education using their visual attention distributions: a comparison of three machine-learning approaches. ||| stefan k ||| chemann ||| pascal klein ||| sebastian becker ||| niharika kumari ||| jochen kuhn ||| 
2018 ||| reducing the split-attention effect in assembly based instruction by merging physical parts with holograms in mixed reality. ||| david dixon ||| uwe terton ||| ruth greenaway ||| 
2020 ||| constraining the transformer nmt model with heuristic grid beam search. ||| guodong xie ||| andy way ||| 
2020 ||| investigation of transformer-based latent attention models for neural machine translation. ||| parnia bahar ||| nikita makarov ||| hermann ney ||| 
2021 |||  (transformer-based argument mining for healthcare applications). ||| tobias mayer ||| elena cabrio ||| serena villata ||| 
2020 |||  base de transformers (transformer based approach for answer generation). ||| imen akermi ||| johannes heinecke ||| fr ||| d ||| ric herledan ||| 
2018 ||| lective pour classification de microblogs (deft 2018 : selective attention for microblogging classification ). ||| charles-emmanuel dias ||| clara gainon de forsan de gabriac ||| patrick gallinari ||| vincent guigue ||| 
2021 ||| ais (generative pre-trained transformer in______ (french) we introduce a french adaptation from the well-known gpt model). ||| antoine simoulin ||| beno ||| t crabb ||| 
2021 ||| dire l'aspect linguistique en anglais au moyen de transformers (classifying linguistic aspect in english with transformers ). ||| eleni metheniti ||| tim van de cruys ||| nabil hathout ||| 
2018 ||| canisme d'attention (customer satisfaction prediction with attention-based rnns from a chat contact center corpus). ||| j ||| r ||| my auguste ||| delphine charlet ||| g ||| raldine damnati ||| beno ||| t favre ||| fr ||| d ||| ric b ||| chet ||| 
2020 ||| decoding auditory and tactile attention for use in an eeg-based brain-computer interface. ||| winko w. an ||| hakim si-mohammed ||| nicholas huang ||| hannes gamper ||| adrian k. c. lee ||| christian holz ||| david johnston ||| mihai jalobeanu ||| dimitra emmanouilidou ||| edward cutrell ||| andrew d. wilson ||| ivan tashev ||| 
2020 ||| importance of reliable eeg data in motor imagery classification: attention level-based approach. ||| seho lee ||| young-tak kim ||| seung-ouk hwang ||| hakseung kim ||| dong-joo kim ||| 
2021 ||| fine-grained temporal attention network for eeg-based seizure detection. ||| seungwoo jeong ||| eunjin jeon ||| wonjun ko ||| heung-il suk ||| 
2021 ||| attention-based spatio-temporal-spectral feature learning for subject-specific eeg classification. ||| dong-hee ko ||| dong-hee shin ||| tae-eui kam ||| 
2017 ||| the effect of selective attention on multiple assrs for future bci application. ||| netiwit kaongoen ||| sungho jo ||| 
2020 ||| classification of selective attention based on steady-state somatosensory evoked potentials using high-frequency vibration stimuli. ||| keun-tae kim ||| jaehyung lee ||| hyungmin kim ||| song joo lee ||| 
2022 ||| decoding 3d representation of visual imagery eeg using attention-based dual-stream convolutional neural network. ||| hyung-ju ahn ||| dae-hyeok lee ||| 
2017 ||| identification of attention state for menu-selection using in-ear eeg recording. ||| donghwa jeong ||| jaeseung jeong ||| yongwook chae ||| hyeonyoung choi ||| 
2021 ||| classification of tactile perception and attention on natural textures from eeg signals. ||| myoung-ki kim ||| jeong-hyun cho ||| ji-hoon jeong ||| 
2022 ||| eeg-transformer: self-attention from transformer architecture for decoding eeg of imagined speech. ||| young eun lee ||| seo-hyun lee ||| 
2022 ||| decoding high-level imagined speech using attention-based deep neural networks. ||| dae-hyeok lee ||| sung-jin kim ||| keon-woo lee ||| 
2017 ||| multimodal integration, attention and sensory augmentation? ||| basil wahn ||| peter k ||| nig ||| 
2020 ||| domain adaptation of transformers for english word segmentation. ||| ruan chaves rodrigues ||| acquila santos rocha ||| marcelo akira inuzuka ||| hugo alexandre dantas do nascimento ||| 
2021 ||| mrat-sql+gap: a portuguese text-to-sql transformer. ||| marcelo archanjo jos ||| f ||| bio gagliardi cozman ||| 
2021 ||| code autocomplete using transformers. ||| gabriel t. meyrer ||| denis a. de araujo ||| sandro jos |||  rigo ||| 
2020 ||| bidirectional transformer based on online text-based information to implement convolutional neural network model for secure business investment. ||| maryam heidari ||| setareh rafatirad ||| 
2020 ||| the effect of spatial reference on visual attention and workload during viewpoint guidance in augmented reality. ||| daniela markov-vetter ||| martin luboschik ||| a. b. m. tariqul islam ||| peter gauger ||| oliver g. staadt ||| 
2019 ||| visual cues to restore student attention based on eye gaze drift, and application to an offshore training system. ||| andrew yoshimura ||| adil khokhar ||| christoph w. borst ||| 
2021 ||| altering non-verbal cues to implicitly direct attention in social vr. ||| radiah rivu ||| ken pfeuffer ||| philipp m ||| ller ||| yomna abdelrahman ||| andreas bulling ||| florian alt ||| 
2019 ||| multi-label aerial image classification using a bidirectional class-wise attention network. ||| yuansheng hua ||| lichao mou ||| xiao xiang zhu ||| 
2019 ||| mapping human settlements with multi-seasonal sentinel-2 imagery and attention-based resnext. ||| chunping qiu ||| michael schmitt ||| hannes taubenb ||| ck ||| xiao xiang zhu ||| 
2020 ||| attention-based secure feature extraction in near sensor processing: work-in-progress. ||| pankaj bhowmik ||| md jubaer hossain pantho ||| sujan kumar saha ||| christophe bobda ||| 
2021 ||| graph attention network based object detection and classification in crowded scenario. ||| guangyuan xu ||| shaungxi huang ||| 
2021 ||| hierarchical transformer encoders for vietnamese spelling correction. ||| hieu tran ||| cuong v. dinh ||| long phan ||| son truong nguyen ||| 
2021 ||| an efficient transformer-based model for vietnamese punctuation prediction. ||| hieu tran ||| cuong v. dinh ||| quang pham ||| binh t. nguyen ||| 
2021 ||| key point matching with transformers. ||| emanuele cosenza ||| 
2019 ||| is it worth the attention? a comparative evaluation of attention layers for argument unit segmentation. ||| maximilian splieth ||| ver ||| jonas klaff ||| hendrik heuer ||| 
2017 ||| technology demo of using real-time biofeedback of heart rate variability measures to track and help improve levels of attention and relaxation. ||| gareth loudon ||| dimitrios zampelis ||| 
2017 ||| using real-time biofeedback of heart rate variability measures to track and help improve levels of attention and relaxation. ||| gareth loudon ||| dimitrios zampelis ||| gina deininger ||| 
2021 ||| what you see is what you get? - relating eye-tracking metrics to students' attention to game elements. ||| amirbahador shojaee ||| hyeon woo kim ||| kimberly cook-chennault ||| idalis villanueva alarc ||| n ||| 
2021 ||| non-intrusive classroom attention tracking system (nicats). ||| andrew sanders ||| bradley boswell ||| gursimran singh walia ||| andrew a. allen ||| 
2018 ||| improving the teaching of vector group of three-phase transformer by integrating software and hardware tools into classroom. ||| prechanon kumkratug ||| 
2020 ||| data mining approach for determining student attention pattern. ||| sujan poudyal ||| mahnas jean mohammadi-aragh ||| john e. ball ||| 
2020 ||| teaching computational thinking to a student with attention deficit through programming. ||| felippe fernandes da silva ||| linnyer beatrys ruiz aylon ||| daniela eloise fl ||| r ||| 
2017 ||| visual attention based evaluation for multiple-choice tests in e-learning applications. ||| wei liu ||| mengling yu ||| zijian fan ||| jing xu ||| yuan tian ||| 
2020 ||| analysis of balance controllers for cascaded modular solid-state transformer during steady-state, transient and fault conditions. ||| naga brahmendra yadav gorla ||| jaydeep saha ||| rohit chandra ||| sanjib kumar panda ||| 
2020 ||| study of the impact of processes in electric power systems with res on the operation of numerical differential transformer protection. ||| mikhail v. andreev ||| aleksey a. suvorov ||| nikolay yu. ruban ||| ruslan a. ufa ||| alexander s. gusev ||| igor razzhivin ||| yuly bay ||| anton kievets ||| alisher askarov ||| vladimir rudnik ||| 
2019 ||| the opportunities for efficiency increase of phase-shifting transformers in power transmission operational modes. ||| l. p. kalinin ||| d. a. zaitsev ||| m. s. tirsu ||| i. v. golub ||| 
2021 ||| wide voltage-regulation range tap-changing transformer model for power system studies. ||| jos |||  m. cano ||| md rejwanur r. mojumdar ||| gonzalo a. orcajo ||| 
2019 ||| a new transformerless configuration for grid-connected photovoltaic inverters. ||| babak rooholahi ||| esmaeil zangeneh bighash ||| 
2020 ||| application of demand response and smart battery electric vehicles charging for capacity utilization of the distribution transformer. ||| saifal talpur ||| tek tjing lie ||| ramon zamora ||| 
2020 ||| comparative analysis of transformer-energizing and fault-caused voltage dips on the dynamic behavior of dfig-based wind turbines. ||| roger alves de oliveira ||| cheng chen ||| math h. j. bollen ||| roberto chouhy leborgne ||| 
2021 ||| a novel approach for incipient fault diagnosis in power transformers by artificial neural networks. ||| sofia moreira de andrade lopes ||| rog ||| rio andrade flauzino ||| 
2018 ||| smart transformer based loop power controller in radial power distribution grid. ||| chandan kumar ||| xiang gao ||| marco liserre ||| 
2017 ||| an autonomous voltage control for distribution power system using pole-transformer. ||| takeshi nagata ||| shinya kuris ||| hikaru kamigaichi ||| 
2017 ||| impacts of tap stagger on currents of power transformers. ||| dongmiao wang ||| linwei chen ||| haiyu li ||| zhongdong wang ||| victoria turnham ||| 
2019 ||| transformer loss of life mitigation in the presence of energy storage and pv generation. ||| milad soleimani ||| carolina m. affonso ||| mladen kezunovic ||| 
2019 ||| planning of oltc transformers in lv systems under conservation voltage reduction strategy. ||| alireza nouri ||| andrew keane ||| 
2021 ||| assessing the impact of high penetration pv on the power transformer loss of life on a distribution system. ||| xiaochu wang ||| keith d'souza ||| wenyuan tang ||| mesut a. baran ||| 
2020 ||| instantaneous flicker control strategy with oltc-fitted distribution transformers in lv networks. ||| ammar arshad ||| matti lehtonen ||| 
2021 ||| influence of the power factor on the vibration behavior of transformers for primary and secondary distribution. ||| andre w ||| rde ||| jannis nikolas kahlen ||| nils langenberg ||| albert moser ||| 
2019 ||| novel method for numerical transformer differential protection setting up using its detailed mathematical model. ||| mikhail v. andreev ||| vladimir rudnik ||| aleksey a. suvorov ||| nikolay yu. ruban ||| ruslan a. ufa ||| alexander s. gusev ||| igor razzhivin ||| yuly bay ||| anton kievets ||| alisher askarov ||| 
2019 ||| modeling students' attention in the classroom using eyetrackers. ||| narayanan veliyath ||| pradipta de ||| andrew a. allen ||| charles b. hodges ||| aniruddha mitra ||| 
2021 ||| benefits of combining dimensional attention and working memory for partially observable reinforcement learning problems. ||| ngozi omatu ||| joshua l. phillips ||| 
2021 ||| fast streaming translation using machine learning with transformer. ||| jiabao qiu ||| melody moh ||| teng-sheng moh ||| 
2020 ||| attention patterns detection using brain computer interfaces. ||| felix g. hamza-lup ||| aditya suri ||| ionut emil iacob ||| ioana r. goldbach ||| lateef rasheed ||| paul nicolae borza ||| 
2021 ||| emotion detection on greek social media using bidirectional encoder representations from transformers. ||| georgios alexandridis ||| konstantinos korovesis ||| iraklis varlamis ||| panagiotis tsantilas ||| george caridakis ||| 
2020 ||| banner advertisement effectiveness using big-5 personality traits, advertisement recall, and visual attention. ||| semira maria evangelou ||| michalis xenos ||| 
2018 ||| a study of micro-augmentations: personality, gender, emotions and effects on attention and brain waves. ||| konstantinos alachouzakis ||| nikolaos-dimitrios veneris ||| spyridon kavvadias ||| angeliki antoniou ||| george lepouras ||| 
2020 ||| frequency-based multi task learning with attention mechanism for fault detection in power systems. ||| peyman tehrani ||| marco levorato ||| 
2021 ||| defense against power system time delay attacks via attention-based multivariate deep learning. ||| shahram ghahremani ||| rajvir sidhu ||| david k. y. yau ||| ngai-man cheung ||| justin albrethsen ||| 
2019 ||| a methodology for detecting stealthy transformer tap command injection attacks in smart grids. ||| shantanu chakrabarty ||| biplab sikdar ||| 
2020 ||| dynamic state estimation based monitoring of high frequency transformer. ||| boqi xie ||| dongbo zhao ||| tianqi hong ||| alex q. huang ||| zhicheng guo ||| yuzhang lin ||| 
2021 ||| generative adversarial networks based on mixed-attentions for citation intent classification in scientific publications. ||| yuh-shyang wang ||| chao-yi chen ||| lung-hao lee ||| 
2021 ||| incorporating domain knowledge into language transformers for multi-label classification of chinese medical questions. ||| po-han chen ||| yu-xiang zeng ||| lung-hao lee ||| 
2021 ||| multi-label classification of chinese humor texts using hypergraph attention networks. ||| hao-chuan kao ||| man-chen hung ||| lung-hao lee ||| yuen-hsien tseng ||| 
2017 |||  (two-stage attentional auditory model inspired neural network and its application to speaker identification) [in chinese]. ||| yu-wen lo ||| yuan-fu liao ||| tai-shih chi ||| 
2021 ||| ncu-nlp at rocling-2021 shared task: using macbert transformers for dimensional sentiment analysis. ||| man-chen hung ||| chao-yi chen ||| pin-jung chen ||| lung-hao lee ||| 
2019 ||| spatial attention lesion detection on automated breast ultrasound. ||| feiqian wang ||| xiaotong liu ||| buyue qian ||| litao ruan ||| rongjian zhao ||| changchang yin ||| na yuan ||| rong wei ||| xin ma ||| jishang wei ||| 
2019 ||| investigation on the dependencies between hrv, physical training, and focus of attention in virtual environment. ||| edgaras sciglinskas ||| aurimas maciukas ||| ausra vidugiriene ||| tomas krilavicius ||| 
2021 ||| bslkt: a bagging model with self-attention and lightgbm for knowledge tracing. ||| zhuoxu zhang ||| haoyun li ||| 
2021 ||| ship detection in large-scale sar images based on dense spatial attention and multi-level feature fusion. ||| limin zhang ||| yingjian liu ||| qingxiang guo ||| haoyu yin ||| yue li ||| pengting du ||| 
2019 ||| graph attention propagation for few-shot learning. ||| xiaolu hui ||| riquan chen ||| tianshui chen ||| 
2020 ||| fast and precise energy consumption prediction based on fully convolutional attention res2net. ||| chao yang ||| zhongwen guo ||| yuan liu ||| 
2020 ||| kt-xl: a knowledge tracing model for predicting learning performance based on transformer-xl. ||| yu he ||| xinying hu ||| zhongtian xu ||| guangzhong sun ||| 
2019 ||| an attention-based ambient network with 3d convolutional network for incomplete traffic flow prediction. ||| feng lin ||| haifeng zheng ||| xinxin feng ||| 
2020 ||| feeling scarcity: augmenting human feelings through physicalizations of energy consumption, attention depletion and animal murder. ||| fabian hemmert ||| gina lohkamp ||| g ||| rkan orak ||| alexander salice ||| 
2019 ||| attention guidance in second screen applications. ||| valentin lohm ||| ller ||| philip eiermann ||| peter zeitlh ||| fler ||| christian wolff ||| 
2021 ||| seneca: an attention support tool for context-related content learning. ||| alessia auriemma citarella ||| luigi di biasi ||| stefano piotto ||| michele risi ||| genoveffa tortora ||| 
2021 ||| ut-atd: universal transformer for anomalous trajectory detection by embedding trajectory information. ||| yun zhang ||| nianwen ning ||| pengpeng zhou ||| bin wu ||| 
2017 ||| 3d memristor-based adjustable deep recurrent neural network with programmable attention mechanism. ||| hongyu an ||| zhen zhou ||| yang yi ||| 
2021 ||| marl: multimodal attentional representation learning for disease prediction. ||| ali hamdi ||| amr aboeleneen ||| khaled b. shaban ||| 
2017 ||| selection and execution of simple actions via visual attention and direct parameter specification. ||| jan t ||| nnermann ||| steffen gr ||| ne ||| b ||| rbel mertsching ||| 
2021 ||| thermal image super-resolution using second-order channel attention with varying receptive fields. ||| nolan b. gutierrez ||| william j. beksi ||| 
2021 ||| object localization with attribute preference based on top-down attention. ||| soubarna banik ||| mikko lauri ||| alois c. knoll ||| simone frintrop ||| 
2020 ||| language-oriented sentiment analysis based on the grammar structure and improved self-attention network. ||| hien d. nguyen ||| tai huynh ||| suong n. hoang ||| vuong t. pham ||| ivan zelinka ||| 
2019 ||| specialized visual sensor coupled to a dynamic neural field for embedded attentional process. ||| marino rasamuel ||| lyes khacef ||| laurent rodriguez ||| beno ||| t miramond ||| 
2021 ||| occtransformers: learning occupancy using attention. ||| bogdan maxim ||| sergiu nedevschi ||| 
2019 ||| investigation of automatic video summarization using viewer's physiological, facial and attentional features. ||| s ||| rgio cavalcanti de paiva ||| herman martins gomes ||| 
2019 ||| accuracy improvement of fashion style estimation with attention control of a classifier. ||| risako aoki ||| takeshi nakajima ||| takuro oki ||| ryusuke miyamoto ||| 
2020 ||| fake news detection on fake.br using hierarchical attention networks. ||| emerson yoshiaki okano ||| zebin liu ||| donghong ji ||| evandro eduardo seron ruiz ||| 
2022 ||| a targeted assessment of the syntactic abilities of transformer models for galician-portuguese. ||| marcos garc ||| a ||| alfredo crespo-otero ||| 
2020 ||| hardware accelerator for multi-head attention and position-wise feed-forward in the transformer. ||| siyuan lu ||| meiqi wang ||| shuang liang ||| jun lin ||| zhongfeng wang ||| 
2019 ||| abstractive text summarization using enhanced attention model. ||| rajendra kumar roul ||| pratik madhav joshi ||| jajati keshari sahoo ||| 
2020 ||| grabbing pedestrian attention with interactive signboard for street advertising. ||| heeyoon jeong ||| gerard kim ||| 
2017 ||| the design of a virtual reality game for stroke-induced attention deficits. ||| hanne huygelier ||| c ||| line r. gillebert ||| raymond van ee ||| vero vanden abeele ||| 
2018 ||| the transmutation of perception: research of attention and visual guidance in virtual reality context. ||| yulin tian ||| 
2019 ||| getting the player's attention: comparing the effectiveness of common notification types in task management games. ||| wen bo yu ||| daniel maccormick ||| loutfouz zaman ||| pejman mirza-babaei ||| 
2019 ||| bi-directional attention flow for video alignment. ||| reham abobeah ||| marwan torki ||| amin a. shoukry ||| jiro katto ||| 
2021 ||| multi-task architecture with attention for imaging atmospheric cherenkov telescope data analysis. ||| mika ||| l jacquemont ||| thomas vuillaume ||| alexandre beno ||| t ||| gilles maurin ||| patrick lambert ||| 
2022 ||| attention-based gender recognition on masked faces. ||| vincenzo carletti ||| antonio greco ||| alessia saggese ||| mario vento ||| 
2022 ||| study of lidar segmentation and model's uncertainty using transformer for different pre-trainings. ||| mohammed hassoubah ||| ibrahim sobh ||| mohamed elhelw ||| 
2019 ||| supervised spatial transformer networks for attention learning in fine-grained action recognition. ||| dichao liu ||| yu wang ||| jien kato ||| 
2021 ||| embedding human knowledge into deep neural network via attention map. ||| masahiro mitsuhara ||| hiroshi fukui ||| yusuke sakashita ||| takanori ogata ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2019 ||| real time eye gaze tracking system using cnn-based facial features for human attention measurement. ||| oliver lorenz ||| ulrike thomas ||| 
2020 ||| localizing visitors in natural sites exploiting modality attention on egocentric images and gps data. ||| giovanni pasqualino ||| stefano scafiti ||| antonino furnari ||| giovanni maria farinella ||| 
2021 ||| latent video transformer. ||| ruslan rakhimov ||| denis volkhonskiy ||| alexey artemov ||| denis zorin ||| evgeny burnaev ||| 
2022 ||| multimodal personality recognition using cross-attention transformer and behaviour encoding. ||| tanay agrawal ||| dhruv agarwal ||| michal balazia ||| neelabh sinha ||| fran ||| ois br ||| mond ||| 
2022 ||| bispectral pedestrian detection augmented with saliency maps using transformer. ||| mohamed amine marnissi ||| ikram hattab ||| hajer fradi ||| anis sahbani ||| najoua essoukri ben amara ||| 
2022 ||| transformers in self-supervised monocular depth estimation with unknown camera intrinsics. ||| arnav varma ||| hemang chawla ||| bahram zonooz ||| elahe arani ||| 
2022 ||| skeleton-based online sign language recognition using monotonic attention. ||| natsuki takayama ||| gibran benitez-garcia ||| hiroki takahashi ||| 
2021 ||| upsampling attention network for single image super-resolution. ||| zhijie zheng ||| yuhang jiao ||| guangyou fang ||| 
2020 ||| semantic segmentation using light attention mechanism. ||| yuki hiramatsu ||| kazuhiro hotta ||| 
2021 ||| interpretation of human behavior from multi-modal brain mri images based on graph deep neural networks and attention mechanism. ||| refka hanachi ||| akrem sellami ||| imed riadh farah ||| 
2022 ||| structurenet: deep context attention learning for structural component recognition. ||| akash kaothalkar ||| bappaditya mandal ||| niladri b. puhan ||| 
2022 ||| a comprehensive study of vision transformers on dense prediction tasks. ||| kishaan jeeveswaran ||| senthilkumar kathiresan ||| arnav varma ||| omar magdy ||| bahram zonooz ||| elahe arani ||| 
2019 ||| subjective annotations for vision-based attention level estimation. ||| andrea coifman ||| p ||| ter rohoska ||| miklas s. kristoffersen ||| sven ewan shepstone ||| zheng-hua tan ||| 
2021 ||| long-term behaviour recognition in videos with actor-focused region attention. ||| luca ballan ||| ombretta strafforello ||| klamer schutte ||| 
2020 ||| learn more from context: joint modeling of local and global attention for aspect sentiment classification. ||| siyuan wang ||| peng liu ||| jinqiao shi ||| xuebin wang ||| can zhao ||| zelin yin ||| 
2019 ||| effective self attention modeling for aspect based sentiment analysis. ||| ningning cai ||| can ma ||| weiping wang ||| dan meng ||| 
2017 ||| investigation of the visual attention role in clinical bioethics decision-making using machine learning algorithms. ||| daniel louzada fernandes ||| rodrigo siqueira-batista ||| andreia patricia gomes ||| camila r. souza ||| israel t. da costa ||| felippe da s. l. cardoso ||| jo ||| o v. de assis ||| gustavo h. l. caetano ||| fabio ribeiro cerqueira ||| 
2019 ||| meta-graph based attention-aware recommendation over heterogeneous information networks. ||| feifei dai ||| xiaoyan gu ||| bo li ||| jinchao zhang ||| mingda qian ||| weiping wang ||| 
2020 ||| an empirical evaluation of attention and pointer networks for paraphrase generation. ||| varun gupta ||| adam krzyzak ||| 
2020 ||| detecting the most insightful parts of documents using a regularized attention-based model. ||| kourosh modarresi ||| 
2021 ||| transformer based models in fake news detection. ||| sebastian kula ||| rafal kozik ||| michal choras ||| michal wozniak ||| 
2019 ||| rumor detection on social media: a multi-view model using self-attention mechanism. ||| yue geng ||| zheng lin ||| peng fu ||| weiping wang ||| 
2021 ||| exploiting extensive external information for event detection through semantic networks word representation and attention map. ||| zechen wang ||| shupeng wang ||| lei zhang ||| yong wang ||| 
2021 ||| automated method for evaluating neural network's attention focus. ||| tomasz szandala ||| henryk maciejewski ||| 
2019 ||| short-term traffic congestion forecasting using attention-based long short-term memory recurrent neural network. ||| tianlin zhang ||| ying liu ||| zhenyu cui ||| jiaxu leng ||| weihong xie ||| liang zhang ||| 
2021 ||| combining transformer-based models with traditional machine learning approaches for sexism identification in social networks at exist 2021. ||| ezequiel lopez-lopez ||| jorge carrillo-de-albornoz ||| laura plaza ||| 
2018 ||| attention mechanism for aggressive detection. ||| carlos enrique mu ||| iz-cuza ||| gretel liz de la pe ||| a sarrac ||| n ||| paolo rosso ||| 
2019 ||| from recurrency to attention in opinion analysis: comparing rnn vs transformer models. ||| rosa mar ||| a monta ||| es-salas ||| rafael del-hoyo-alonso ||| roc ||| o aznar-gimeno ||| 
2021 ||| umuteam at meoffendes 2021: ensemble learning for offensive language identification using linguistic features, fine-grained negation, and transformers. ||| jos |||  antonio garc ||| a-d ||| az ||| salud mar ||| a jim ||| nez zafra ||| rafael valencia-garc ||| a ||| 
2021 ||| transformer based offensive language identification in spanish. ||| sreelakshmi k ||| premjith b ||| k. p. soman ||| 
2019 ||| elirf-upv at irosva: transformer encoders for spanish irony detection. ||| jos ||| - ||| ngel gonz ||| lez ||| llu ||| s-felip hurtado ||| ferran pla ||| 
2020 ||| a parallel-attention model for tumor named entity recognition in spanish. ||| tong wang ||| yuanyu zhang ||| yongbin li ||| 
2021 ||| umuteam at emoevales 2021: emotion analysis for spanish based on explainable linguistic features and transformers. ||| jos |||  antonio garc ||| a-d ||| az ||| ricardo colomo palacios ||| rafael valencia-garc ||| a ||| 
2020 ||| identification of cancer entities in clinical text combining transformers with dictionary features. ||| john d. osborne ||| tobias o'leary ||| james del monte ||| kuleen sasse ||| 
2021 ||| ai-upv at iberlef-2021 detoxis task: toxicity detection in immigration-related web news comments using transformers and statistical models. ||| angel felipe magnoss ||| o de paula ||| ipek baris schlicht ||| 
2020 ||| transformers and data augmentation for aggressiveness detection in mexican spanish. ||| mario guzman-silverio ||| ngel balderas-paredes ||| adri ||| n pastor l ||| pez-monroy ||| 
2021 ||| boosting transformers for job expression extraction and classification in a low-resource setting. ||| lukas lange ||| heike adel ||| jannik str ||| tgen ||| 
2019 ||| elirf-upv at tass 2019: transformer encoders for twitter sentiment analysis in spanish. ||| jos ||| - ||| ngel gonz ||| lez ||| llu ||| s-felip hurtado ||| ferran pla ||| 
2021 ||| haha@iberlef2021: humor analysis using ensembles of simple transformers. ||| karish grover ||| tanishq goel ||| 
2021 ||| exist2021: detecting sexism with transformers and translation-augmented data. ||| guillem garc ||| a subies ||| 
2020 ||| detecting aggressiveness in mexican spanish social media content by fine-tuning transformer-based models. ||| mircea-adrian tanase ||| george-eduard zaharia ||| dumitru-clementin cercel ||| mihai dascalu ||| 
2021 ||| everything transformers: recognition, classification and normalisation of professions and family relations. ||| salvador medina herrera ||| jordi turmo borras ||| 
2021 ||| transformers pipeline for offensiveness detection in mexican spanish social media. ||| victor g ||| mez-espinosa ||| victor mu ||| iz-sanchez ||| adri ||| n pastor l ||| pez-monroy ||| 
2021 ||| emotion detection for spanish with data augmentation and transformer-based models. ||| hongxin luo ||| 
2020 ||| a tumor named entity recognition model based on pre-trained language model and attention mechanism. ||| xin taou ||| renyuan liu ||| xiaobing zhou ||| 
2020 ||| palomino-ochoa at tass 2020: transformer-based data augmentation for overcoming few-shot learning. ||| daniel palomino ||| jos |||  ochoa luna ||| 
2021 ||| automatic sexism detection with multilingual transformer models ait fhstp@exist2021. ||| mina sch ||| tz ||| jaqueline boeck ||| daria liakhovets ||| djordje slijepcevic ||| armin kirchknopf ||| manuel hecht ||| johannes bogensperger ||| sven schlarb ||| alexander schindler ||| matthias zeppelzauer ||| 
2021 ||| transformer ensembles for sexism detection. ||| lily davies ||| marta baldracchi ||| carlo alessandro borella ||| konstantinos perifanos ||| 
2021 ||| umuteam at exist 2021: sexist language identification based on linguistic features and transformers in spanish and english. ||| jos |||  antonio garc ||| a-d ||| az ||| ricardo colomo palacios ||| rafael valencia-garc ||| a ||| 
2021 ||| system description for exist shared task at iberlef 2021: automatic misogyny identification using pretrained transformers. ||| ignacio talavera ||| david carreto fidalgo ||| daniel vila-suero ||| 
2020 ||| automatic icd code classification with label description attention mechanism. ||| kathryn annette chapman ||| g ||| nter neumann ||| 
2021 ||| umuteam at haha 2021: linguistic features and transformers for analysing spanish humor. the what, the how, and to whom. ||| jos |||  antonio garc ||| a-d ||| az ||| rafael valencia-garc ||| a ||| 
2019 ||| effectiveness of facial animated avatar and voice transformer in elearning programming course. ||| rex hsieh ||| akihiko shirai ||| hisashi sato ||| 
2018 ||| the virtual schoolyard: attention training in virtual reality for children with attentional disorders. ||| katharina kr ||| sl ||| anna felnhofer ||| johanna xenia kafka ||| laura schuster ||| alexandra rinnerthaler ||| michael wimmer ||| oswald d. kothgassner ||| 
2020 ||| video captioning using attention based visual fusion with bi-temporal context and bi-modal semantic feature learning. ||| noorhan k. fawzy ||| mohammed a. marey ||| mostafa m. aref ||| 
2019 ||| winding deformation detection of transformer based on sweep frequency impedance. ||| hui zhang ||| tao wang ||| yunshan zhang ||| yujie pei ||| zhongbin bai ||| ling guan ||| yaoding gu ||| jianguo xu ||| 
2019 ||| analysis winding deformation of power transformer detection using sweep frequency impedance technology. ||| tao wang ||| yaqing hu ||| xianfeng li ||| hua zhang ||| zhenwei e ||| lei zhang ||| zhongbin bai ||| chunmei guan ||| 
2021 ||| a pre-ln transformer network model with lexical features for fine-grained sentiment classification. ||| kaixin wang ||| xiujuan xu ||| yu liu ||| zhehuan zhao ||| 
2018 ||| prior knowledge integrated with self-attention for event detection. ||| yan li ||| chenliang li ||| weiran xu ||| junliang li ||| 
2020 ||| position-aware hybrid attention network for aspect-level sentiment analysis. ||| yongqiang zheng ||| xia li ||| guixin su ||| junteng ma ||| chaolin ning ||| 
2018 ||| question-answering aspect classification with multi-attention representation. ||| hanqian wu ||| mumu liu ||| jingjing wang ||| jue xie ||| shoushan li ||| 
2020 ||| hierarchical attention network in stock prediction. ||| liming huang ||| hongfei yan ||| siping ying ||| yansong li ||| rui miao ||| chong chen ||| qi su ||| 
2021 ||| lda-transformer model in chinese poetry authorship attribution. ||| ai zhou ||| yijia zhang ||| hao wei ||| mingyu lu ||| 
2017 ||| combine non-text features with deep learning structures based on attention-lstm for answer selection. ||| chang'e jia ||| chengjie sun ||| bingquan liu ||| lei lin ||| 
2018 ||| joint attention lstm network for aspect-level sentiment analysis. ||| guoyong cai ||| hongyu li ||| 
2017 ||| more attention, less deficit: wearable eeg-based serious game for focus improvement. ||| alaa eddin alchalabi ||| amer nour eddin ||| shervin shirmohammadi ||| 
2020 ||| vera: virtual environments recording attention. ||| victor delvigne ||| laurence ris ||| thierry dutoit ||| hazem wannous ||| jean-philippe vandeborre ||| 
2018 ||| assessing attention in visual and textual programming using neuroeducation approaches. ||| spyridon doukakis ||| mary-angela papalaskari ||| panayiotis m. vlamos ||| antonia plerou ||| panagiota giannopoulou ||| 
2020 ||| notional machines in computing education: the education of attention. ||| sally fincher ||| johan jeuring ||| craig s. miller ||| peter donaldson ||| benedict du boulay ||| matthias hauswirth ||| arto hellas ||| felienne hermans ||| colleen m. lewis ||| andreas m ||| hling ||| janice l. pearce ||| andrew petersen ||| 
2020 ||| extracting biomedical relations via a multi-head attention based graph convolutional network. ||| erniu wang ||| fan wang ||| zhihao yang ||| lei wang ||| yin zhang ||| hongfei lin ||| jian wang ||| 
2020 ||| an end-to-end oxford nanopore basecaller using convolution-augmented transformer. ||| xuan lv ||| zhiguang chen ||| yutong lu ||| yuedong yang ||| 
2020 ||| predicting drugs for covid-19/sars-cov-2 via heterogeneous graph attention networks. ||| yahui long ||| yu zhang ||| min wu ||| shaoliang peng ||| chee keong kwoh ||| jiawei luo ||| xiaoli li ||| 
2020 ||| multi-view multi-label learning with dual-attention networks for stroke screen. ||| jundong shen ||| yi zhang ||| cheng yu ||| chongjun wang ||| 
2020 ||| deeparc: an attention-based hybrid model for predicting transcription factor binding sites from positional embedded dna sequence. ||| jialong chen ||| lei deng ||| 
2020 ||| structured information extraction of pathology reports with attention-based graph convolutional network. ||| jialun wu ||| kaiwen tang ||| haichuan zhang ||| chunbao wang ||| chen li ||| 
2021 ||| sgat: a self-supervised graph attention network for biomedical relation extraction. ||| qiming liu ||| zhihao yang ||| lei wang ||| yin zhang ||| hongfei lin ||| jinzhong ning ||| 
2019 ||| disease prediction model based on bilstm and attention mechanism. ||| yang yang ||| xiangwei zheng ||| cun ji ||| 
2021 ||| low-dimensional depth local dual-view features embedded transformer for electrocardiogram signal quality assessment. ||| shuaiying yuan ||| ziyang he ||| jianhui zhao ||| zhiyong yuan ||| 
2019 ||| attentiondta: prediction of drug-target binding affinity using attention model. ||| qichang zhao ||| fen xiao ||| mengyun yang ||| yaohang li ||| jianxin wang ||| 
2021 ||| deeppppred: deep ensemble learning with transformers, recurrent and convolutional neural networks for human protein-phenotype co-mention classification. ||| morteza pourreza shahri ||| katrina lyon ||| julia schearer ||| indika kahanda ||| 
2021 ||| hydrogen bonds meet self-attention: all you need for protein structure embedding. ||| cheng chen ||| yuguo zha ||| daming zhu ||| kang ning ||| xuefeng cui ||| 
2020 ||| attention based detection for central serious chorioretinopathy in fundus image. ||| chuan zhou ||| tian zhang ||| leiting chen ||| yang wen ||| ting lei ||| junjing chen ||| 
2021 ||| document-level biomedical relation extraction with generative adversarial network and dual-attention multi-instance learning. ||| lishuang li ||| ruiyuan lian ||| hongbin lu ||| 
2019 ||| an attention-based neural network basecaller for oxford nanopore sequencing data. ||| neng huang ||| fan nie ||| peng ni ||| feng luo ||| jianxin wang ||| 
2020 ||| brain functional connectivity pattern recognition for attention-deficit/hyperactivity disorder diagnosis. ||| harun pirimy ||| miaolin fan ||| haifeng wang ||| 
2021 ||| attent: domain-adaptive medical image segmentation via attention-aware translation and adversarial entropy minimization. ||| chen li ||| xin luo ||| wei chen ||| yulin he ||| mingfei wu ||| yusong tan ||| 
2020 ||| attention-based saliency hashing for ophthalmic image retrieval. ||| jiansheng fang ||| yanwu xu ||| xiaoqing zhang ||| yan hu ||| jiang liu ||| 
2018 ||| protein-protein interaction article classification: a knowledge-enriched self-attention convolutional neural network approach. ||| ling luo ||| zhihao yang ||| lei wang ||| yin zhang ||| hongfei lin ||| jian wang ||| liang yang ||| kan xu ||| yijia zhang ||| 
2021 ||| multi-scale hierarchical transformer structure for 3d medical image segmentation. ||| luyao wang ||| xiaoyan wang ||| bangze zhang ||| xiaojie huang ||| cong bai ||| ming xia ||| peiliang sun ||| 
2020 ||| deep multi-instance learning with induced self-attention for medical image classification. ||| zhenliang li ||| liming yuan ||| haixia xu ||| rui cheng ||| xianbin wen ||| 
2021 ||| pg-tfnet: transformer-based fusion network integrating pathological images and genomic data for cancer survival analysis. ||| zhilong lv ||| yuexiao lin ||| rui yan ||| zhenghe yang ||| ying wang ||| fa zhang ||| 
2020 ||| constructing a relevance-oriented dataset for training transformer rankers for medical search. ||| zhi zheng ||| ben he ||| 
2021 ||| haunet-3d: a novel hierarchical attention 3d unet for lung nodule segmentation. ||| fu zhou ||| fei luo ||| kafui efio-akolly ||| ronald bbosa ||| wen cai huang ||| jia ni zou ||| yi-ping phoebe chen ||| feng liu ||| 
2021 ||| transmixnet: an attention based double-branch model for white blood cell classification and its training with the fuzzified training data. ||| hua chen ||| juan liu ||| chunbing hua ||| zhiqun zuo ||| jing feng ||| baochuan pang ||| di xiao ||| 
2021 ||| paenet: a progressive attention-enhanced network for 3d to 2d retinal vessel segmentation. ||| zhuojie wu ||| zijian wang ||| wenxuan zou ||| fan ji ||| hao dang ||| wanting zhou ||| muyi sun ||| 
2021 ||| attention-based convolutional neural networks for protein-protein interaction site prediction. ||| shuai lu ||| yuguang li ||| xiaofei nan ||| shoutao zhang ||| 
2018 ||| breast cancer classification with electronic medical records using hierarchical attention bidirectional networks. ||| dehua chen ||| guangjun qian ||| qiao pan ||| 
2021 ||| hgna-hti: heterogeneous graph neural network with attention mechanism for prediction of herb-target interactions. ||| wenhui zhao ||| hao wu ||| jieyue he ||| 
2021 ||| transformer-based multi-target regression on electronic health records for primordial prevention of cardiovascular disease. ||| raphael poulain ||| mehak gupta ||| randi e. foraker ||| rahmatollah beheshti ||| 
2021 ||| a graph attention neural network for diagnosing asd with fmri data. ||| wutao yin ||| longhai li ||| fang-xiang wu ||| 
2021 ||| ammasurv: asymmetrical multi-modal attention for accurate survival analysis with whole slide images and gene expression data. ||| ruoqi wang ||| ziwang huang ||| haitao wang ||| hejun wu ||| 
2021 ||| personalized clinical pathway recommendation via attention based pre-training. ||| xijie lin ||| yuan li ||| yonghui xu ||| wei guo ||| wei he ||| honglu zhang ||| lizhen cui ||| chunyan miao ||| 
2020 ||| attention-based transformers for instance segmentation of cells in microstructures. ||| tim prangemeier ||| christoph reich ||| heinz koeppl ||| 
2021 ||| sasa-net: a spatial-aware self-attention mechanism for building protein 3d structure directly from inter-residue distances. ||| tiansu gong ||| fusong ju ||| shiwei sun ||| dongbo bu ||| 
2021 ||| darnet: dual-attention residual network for automatic diagnosis of covid-19 via ct images. ||| jun shi ||| huite yi ||| shulan ruan ||| zhaohui wang ||| xiaoyu hao ||| hong an ||| wei wei ||| 
2020 ||| multi-class metabolic pathway prediction by graph attention-based deep learning method. ||| zhihui yang ||| juan liu ||| zeyu wang ||| yufan wang ||| jing feng ||| 
2019 ||| predicting disease-related rna associations based on graph convolutional attention network. ||| jinli zhang ||| xiaohua hu ||| zongli jiang ||| bo song ||| wei quan ||| zheng chen ||| 
2021 ||| a transformer-based network for pathology image classification. ||| meidan ding ||| aiping qu ||| haiqin zhong ||| hao liang ||| 
2021 ||| emotion transformer fusion: complementary representation properties of eeg and eye movements on recognizing anger and surprise. ||| yiting wang ||| wei-bang jiang ||| rui li ||| bao-liang lu ||| 
2019 ||| deeptriager: a neural attention model for emergency triage with electronic health records. ||| guangyu wang ||| xiaohong liu ||| ken xie ||| ning chen ||| ting chen ||| 
2019 ||| dense encoder-decoder network based on two-level context enhanced residual attention mechanism for segmentation of breast tumors in magnetic resonance imaging. ||| ying gao ||| yin zhao ||| xiongwen luo ||| xiping hu ||| changhong liang ||| 
2019 ||| semi-supervised attention-guided cyclegan for data augmentation on medical images. ||| zhenghua xu ||| chang qi ||| guizhi xu ||| 
2019 ||| fusing transformer model with temporal features for ecg heartbeat classification. ||| genshen yan ||| shen liang ||| yanchun zhang ||| fan liu ||| 
2021 ||| accurate brain age prediction model for healthy children and adolescents using 3d-cnn and dimensional attention. ||| guozhen hu ||| qinjian zhang ||| zhi yang ||| baobin li ||| 
2021 ||| deepanis: predicting antibody paratope from concatenated cdr sequences by integrating bidirectional long-short-term memory and transformer neural networks. ||| pan zhang ||| shuangjia zheng ||| jianwen chen ||| yaoqi zhou ||| yuedong yang ||| 
2021 ||| cac-emvt: efficient coronary artery calcium segmentation with multi-scale vision transformers. ||| yang ning ||| shouyi zhang ||| xiaoming xi ||| jie guo ||| peide liu ||| caiming zhang ||| 
2017 ||| chemical-induced disease extraction via convolutional neural networks with attention. ||| haodi li ||| qingcai chen ||| buzhou tang ||| xiaolong wang ||| 
2021 ||| an interpretable multi-level enhanced graph attention network for disease diagnosis with gene expression data. ||| xiaohan xing ||| fan yang ||| hang li ||| jun zhang ||| yu zhao ||| mingxuan gao ||| junzhou huang ||| jianhua yao ||| 
2019 ||| emotion recognition from children speech signals using attention based time series deep learning. ||| guitao cao ||| yunming tang ||| jiyu sheng ||| wenming cao ||| 
2021 ||| radiology report generation for rare diseases via few-shot transformer. ||| xing jia ||| yun xiong ||| jiawei zhang ||| yao zhang ||| suzanne v. blackley ||| yangyong zhu ||| chunlei tang ||| 
2021 ||| bioie: biomedical information extraction with multi-head attention enhanced graph convolutional network. ||| jialun wu ||| ruonan zhang ||| tieliang gong ||| yang liu ||| chunbao wang ||| chen li ||| 
2020 ||| multi-scale strategy based 3d dual-encoder brain tumor segmentation network with attention mechanism. ||| yazhou zhu ||| xiang pan ||| jing zhu ||| lihua li ||| 
2021 ||| emotion recognition from multi-channel eeg data through a dual-pipeline graph attention network. ||| xiang li ||| jing li ||| yazhou zhang ||| prayag tiwari ||| 
2021 ||| wearable sensor gait analysis of fall detection using attention network. ||| haben yhdego ||| jiang li ||| christopher paolini ||| michel a. audette ||| 
2019 ||| fine-grained thyroid nodule classification via multi-semantic attention network. ||| shuai li ||| yuting guo ||| wenfeng song ||| zhennan pang ||| aimin hao ||| bo zhang ||| hong qin ||| 
2020 ||| msdan: multi-scale self-attention unsupervised domain adaptation network for thyroid ultrasound images. ||| xiang ying ||| yulin zhang ||| xi wei ||| mei yu ||| jialin zhu ||| jie gao ||| zhiqiang liu ||| xuewei li ||| ruiguo yu ||| 
2021 ||| transpicker: a transformer-based framework for particle picking in cryoem micrographs. ||| chi zhang ||| hongjia li ||| xiaohua wan ||| xuemei chen ||| zhenghe yang ||| jieqing feng ||| fa zhang ||| 
2021 ||| agmi: attention-guided multi-omics integration for drug response prediction with graph neural networks. ||| ruiwei feng ||| yufeng xie ||| minshan lai ||| danny z. chen ||| ji cao ||| jian wu ||| 
2020 ||| extraction and classification of tcm medical records based on bert and bi-lstm with attention mechanism. ||| ye hui ||| lin du ||| shuyuan lin ||| yiqian qu ||| dong cao ||| 
2019 ||| automatic epileptic seizure detection via attention-based cnn-birnn. ||| chengbin huang ||| weiting chen ||| guitao cao ||| 
2021 ||| main: multimodal attention-based fusion networks for diagnosis prediction. ||| ying an ||| haojia zhang ||| yu sheng ||| jianxin wang ||| xianlai chen ||| 
2021 ||| structure-based protein-drug affinity prediction with spatial attention mechanisms. ||| yuxiao wang ||| zongzhao qiu ||| qihong jiao ||| cheng chen ||| zhaoxu meng ||| xuefeng cui ||| 
2018 ||| attention-based recurrent multi-channel neural network for influenza epidemic prediction. ||| bofeng fu ||| yaodong yang ||| yu ma ||| jianye hao ||| siqi chen ||| shuang liu ||| tiegang li ||| zhenyu liao ||| xianglei zhu ||| 
2020 ||| modeling multivariate time series via prototype learning: a multi-level attention-based perspective. ||| dengjuan ma ||| zhu wang ||| jia xie ||| zhiwen yu ||| bin guo ||| xingshe zhou ||| 
2019 ||| drug target interaction prediction using multi-task learning and co-attention. ||| yuyou weng ||| chen lin ||| xiangxiang zeng ||| yun liang ||| 
2020 ||| a diversified supervised based u-shape colorectal lesion segmentor with meaningful feature supplement and multi-level residual attention mechanism. ||| jinjie wang ||| xiongwen luo ||| linsen xie ||| ying gao ||| 
2020 ||| structure enhanced protein-drug interaction prediction using transformer and graph embedding. ||| fan hu ||| yishen hu ||| jianye zhang ||| dongqi wang ||| peng yin ||| 
2021 ||| detecting chronic vascular damage with attention-guided neural system. ||| muhammad zubair khan ||| yugyung lee ||| arslan munir ||| muazzam ali khan ||| 
2020 ||| cross-modal self-attention distillation for prostate cancer segmentation. ||| guokai zhang ||| xiaoang shen ||| ye luo ||| jihao luo ||| zeju wang ||| weigang wang ||| binghui zhao ||| jianwei lu ||| 
2018 ||| full-attention based drug drug interaction extraction exploiting user-generated content. ||| bo xu ||| xiufeng shi ||| zhehuan zhao ||| wei zheng ||| hongfei lin ||| zhihao yang ||| jian wang ||| feng xia ||| 
2020 ||| attention u-net for interpretable classification on chest x-ray image. ||| xuan zhang ||| ting chen ||| 
2019 ||| multi-stage attention-unet for wireless capsule endoscopy image bleeding area segmentation. ||| sizhe li ||| jiawei zhang ||| chunyang ruan ||| yanchun zhang ||| 
2021 ||| rcga-net: an improved multi-hybrid attention mechanism network in biomedical image segmentation. ||| feng xiao ||| cong shen ||| yu chen ||| tian yang ||| shengyong chen ||| zhijun liao ||| jijun tang ||| 
2019 ||| an attention-based semi-supervised neural network for thyroid nodules segmentation. ||| jianrong wang ||| ruixuan zhang ||| xi wei ||| xuewei li ||| mei yu ||| jialin zhu ||| jie gao ||| zhiqiang liu ||| ruiguo yu ||| 
2021 ||| arsc-net: adventitious respiratory sound classification network using parallel paths with channel-spatial attention. ||| lei xu ||| jianhong cheng ||| jin liu ||| hulin kuang ||| fan wu ||| jianxin wang ||| 
2021 ||| attention-enhanced graph cross-convolution for protein-ligand binding affinity prediction. ||| xianbing feng ||| jingwei qu ||| tianle wang ||| bei wang ||| xiaoqing lyu ||| zhi tang ||| 
2017 ||| exploiting argument information to improve biomedical event trigger identification via recurrent neural networks and supervised attention mechanisms. ||| lishuang li ||| yang liu ||| 
2021 ||| exploring feasibility of truth-involved automatic sleep staging combined with transformer. ||| ziwei yang ||| dong wang ||| zheng chen ||| ming huang ||| naoaki ono ||| md. altaf-ul-amin ||| shigehiko kanaya ||| 
2018 ||| correlated attention networks for multimodal emotion recognition. ||| jie-lin qiu ||| xiao-yu li ||| kai hu ||| 
2019 ||| multi-level glioma segmentation using 3d u-net combined attention mechanism with atrous convolution. ||| jianhong cheng ||| jin liu ||| liangliang liu ||| yi pan ||| jianxin wang ||| 
2020 ||| dce-mri based breast intratumor heterogeneity analysis via dual attention deep clustering network and its application in molecular typing. ||| tianxu lv ||| xiang pan ||| lihua li ||| 
2020 ||| predicting prescriptions via dsca-dual sequences with cross attention network. ||| wu lee ||| yuliang shi ||| lin cheng ||| yongqing zheng ||| zhongmin yan ||| 
2020 ||| chemical-protein interaction extraction via chemicalbert and attention guided graph convolutional networks in parallel. ||| lei qin ||| gaocai dong ||| jing peng ||| 
2021 ||| jointly learning to align and aggregate with cross attention pooling for peptide-mhc class i binding prediction. ||| cheng chen ||| zongzhao qiu ||| zhenghe yang ||| bin yu ||| xuefeng cui ||| 
2019 ||| cascaded convolutional neural network with attention mechanism for mobile eeg-based driver drowsiness detection system. ||| sirui ding ||| zhiyong yuan ||| panfeng an ||| guotong xue ||| wenxiang sun ||| jianhui zhao ||| 
2021 ||| fam: fully attention module for medical image segmentation. ||| guoping xu ||| xinglong wu ||| 
2020 ||| respiratory sound classification based on bigru-attention network with xgboost. ||| xuesong zhao ||| yanbo shao ||| juanyun mai ||| airu yin ||| sihan xu ||| 
2018 ||| biomedical event trigger detection based on bilstm integrating attention mechanism and sentence vector. ||| xinyu he ||| lishuang li ||| jia wan ||| dingxin song ||| jun meng ||| zhanjie wang ||| 
2019 ||| combined self-attention mechanism for biomedical event trigger identification. ||| zhichang zhang ||| ruifang zhang ||| 
2021 ||| a meta-path based drug-target prediction model with collaborative attention mechanisms. ||| bing hu ||| feng xia ||| ruolan chen ||| shuting jin ||| xiangrong liu ||| 
2021 ||| an attention based deep learning model for direct estimation of pharmacokinetic maps from dce-mri images. ||| qingyuan zeng ||| wu zhou ||| 
2020 ||| a two-level attention-based sequence-to-sequence model for accurate inter-patient arrhythmia detection. ||| kun jiang ||| shen liang ||| lingxiao meng ||| yanchun zhang ||| peng wang ||| wei wang ||| 
2018 ||| an attention-based bi-gru-capsnet model for hypernymy detection between compound entities. ||| qi wang ||| chenming xu ||| yangming zhou ||| tong ruan ||| daqi gao ||| ping he ||| 
2019 ||| an improved biomedical event trigger identification framework via modeling document with hierarchical attention. ||| jinyong zhang ||| dandan fang ||| weizhong zhao ||| jincai yang ||| wen zou ||| xingpeng jiang ||| tingting he ||| 
2021 ||| disease correlation enhanced attention network for icd coding. ||| ping gu ||| song yang ||| qiang li ||| jiangxing wang ||| 
2020 ||| hierarchical attention-based multiple instance learning network for patient-level lung cancer diagnosis. ||| qingfeng wang ||| ying zhou ||| jun huang ||| zhiqin liu ||| ling li ||| weiyun xu ||| jie-zhi cheng ||| 
2021 ||| predicting drug-mirna resistance with layer attention graph convolution network and multi channel feature extraction. ||| haorui wang ||| shahanavaj khan ||| shichao liu ||| fang zheng ||| wen zhang ||| 
2021 ||| cc-denseunet: densely connected u-net with criss-cross attention for liver and tumor segmentation in ct volumes. ||| qiang li ||| hong song ||| weiwei zhang ||| jingfan fan ||| danni ai ||| yucong lin ||| jian yang ||| 
2021 ||| automatic icd-10 coding based on multi-head attention mechanism and gated residual network. ||| xiaowei wang ||| jungang han ||| ben li ||| xiaoying pan ||| hui xu ||| 
2021 ||| dcet-net: dual-stream convolution expanded transformer for breast cancer histopathological image classification. ||| ying zou ||| shannan chen ||| qiule sun ||| bin liu ||| jianxin zhang ||| 
2021 ||| ct-cad: context-aware transformers for end-to-end chest abnormality detection on x-rays. ||| qiran kong ||| yirui wu ||| chi yuan ||| yongli wang ||| 
2021 ||| automated grading of knee osteoarthritis x-ray images based on attention mechanism. ||| yibo feng ||| jing liu ||| huan zhang ||| dawei qiu ||| 
2021 ||| ect-nas: searching efficient cnn-transformers architecture for medical image segmentation. ||| shuying xu ||| hongyan quan ||| 
2019 ||| cross attention densely connected networks for multiple sclerosis lesion segmentation. ||| beibei hou ||| guixia kang ||| xin xu ||| chuan hu ||| 
2018 ||| attention-based multi-task learning in pharmacovigilance. ||| shinan zhang ||| shantanu dev ||| joseph voyles ||| anand s. rao ||| 
2020 ||| brain mr image super-resolution using 3d feature attention network. ||| lulu wang ||| jinglong du ||| huazheng zhu ||| zhongshi he ||| yuanyuan jia ||| 
2021 ||| graph attention mechanism-based deep tensor factorization for predicting disease-associated mirna-mirna pairs. ||| jiawei luo ||| zihan lai ||| cong shen ||| pei liu ||| heyuan shi ||| 
2018 ||| attention and concentration in normal and deaf gamers. ||| ana rita teixeira ||| ana maria tom ||| lu ||| s m. roseiro ||| anabela gomes ||| 
2021 ||| an acne grading framework on face images via skin attention and sfnet. ||| yi lin ||| yi guan ||| zhaoyang ma ||| haiyan you ||| xue cheng ||| jingchi jiang ||| 
2021 ||| dual attention feature fusion network for monocular depth estimation. ||| yifang xu ||| ming li ||| chenglei peng ||| yang li ||| sidan du ||| 
2021 ||| attention scale-aware deformable network for inshore ship detection in surveillance videos. ||| di liu ||| yan zhang ||| yan zhao ||| yu zhang ||| 
2021 ||| a hierarchical multi-label classification algorithm for scientific papers based on graph attention networks. ||| changwei zheng ||| zhe xue ||| junping du ||| feifei kou ||| meiyu liang ||| mingying xu ||| 
2021 ||| syllable level speech emotion recognition based on formant attention. ||| abdul rehman ||| zhen-tao liu ||| jin-meng xu ||| 
2021 ||| dga-net: dynamic gaussian attention network for sentence semantic matching. ||| kun zhang ||| guangyi lv ||| meng wang ||| enhong chen ||| 
2021 ||| unsupervised domain adaptation via attention augmented mutual networks for person re-identification. ||| hui tian ||| junlin hu ||| 
2021 ||| enhanced attribute alignment based on semantic co-attention for text-based person search. ||| hao wang ||| zhenzhen hu ||| 
2021 ||| attention guided retinex architecture search for robust low-light image enhancement. ||| xiaoke shang ||| jingjie shang ||| long ma ||| shaomin zhang ||| nai ding ||| 
2021 ||| multi-view relevance matching model of scientific papers based on graph convolutional network and attention mechanism. ||| jie song ||| zhe xue ||| junping du ||| feifei kou ||| meiyu liang ||| mingying xu ||| 
2019 ||| collaborative attention network for natural language inference. ||| shiyi zhang ||| yinghua ma ||| shenghong li ||| weikai sun ||| 
2017 ||| chinese dialects identification using attention-based deep neural networks. ||| yuanhang qiu ||| yong ma ||| yun jin ||| shidang li ||| mingliang gu ||| 
2019 ||| a study of visual attention elements with experiment analysis based on composition. ||| wei jiang ||| yanan su ||| shuang wang ||| 
2018 ||| a multi-label image classification algorithm based on attention model. ||| yugang li ||| yongbin wang ||| 
2021 ||| transformer-ic: the solution to information loss. ||| zhigang song ||| jiazhao chai ||| wenqian shang ||| guo yuning ||| 
2019 ||| design of intelligent artificial agents: its application in joint attention task for children with autism. ||| vishav jyoti ||| uttama lahiri ||| 
2018 ||| image-based attention level estimation of interaction scene by head pose and gaze information. ||| rinko komiya ||| takeshi saitoh ||| kazutaka shimada ||| 
2020 ||| interpretable deep attention model for multivariate time series prediction in building energy systems. ||| tryambak gangopadhyay ||| sin yong tan ||| zhanhong jiang ||| soumik sarkar ||| 
2017 ||| a guided spatial transformer network for histology cell differentiation. ||| marc aubreville ||| maximilian krappmann ||| christof bertram ||| robert klopfleisch ||| andreas k. maier ||| 
2019 ||| inter and intra document attention for depression risk assessment. ||| diego maupom ||| marc queudot ||| marie-jean meurs ||| 
2019 ||| efficient transformer-based sentence encoding for sentence pair modelling. ||| mahtab ahmed ||| robert e. mercer ||| 
2020 ||| query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models. ||| md. tahmid rahman laskar ||| enamul hoque ||| jimmy huang ||| 
2019 ||| the unreasonable effectiveness of transformer language models in grammatical error correction. ||| dimitris alikaniotis ||| vipul raheja ||| joel r. tetreault ||| 
2018 ||| co-attention based neural network for source-dependent essay scoring. ||| haoran zhang ||| diane j. litman ||| 
2021 ||| on the application of transformers for estimating the difficulty of multiple-choice questions from text. ||| luca benedetto ||| giovanni aradelli ||| paolo cremonesi ||| andrea cappelli ||| andrea giussani ||| roberto turrin ||| 
2020 ||| an exploratory study of argumentative writing by young students: a transformer-based approach. ||| debanjan ghosh ||| beata beigman klebanov ||| yi song ||| 
2019 ||| tmu transformer system using bert for re-ranking at bea 2019 grammatical error correction on restricted track. ||| masahiro kaneko ||| kengo hotate ||| satoru katsumata ||| mamoru komachi ||| 
2021 ||| unsupervised methods for the study of transformer embeddings. ||| mira ait saada ||| fran ||| ois role ||| mohamed nadif ||| 
2018 ||| analysis of resting state eeg signals of adults with attention-deficit hyperactivity disorder. ||| simranjit kaur ||| sukhwinder singh ||| priti arun ||| damanjeet kaur ||| 
2020 ||| assessing differences on eye fixations by attention levels in an assembly environment. ||| thomas shotton ||| jung hyup kim ||| 
2020 ||| detection of subject attention in an active environment through facial expressions using deep learning techniques and computer vision. ||| naqash gerard ||| talha yousuf ||| ahmed husnain johar ||| umer asgher ||| muhammad imran malik ||| adnan ul-hasan ||| faisal shafait ||| 
2021 ||| attentional and emotional engagement of sustainability in tourism marketing: electroencephalographic (eeg) and peripheral neuroscientific approach. ||| michela balconi ||| federico cassioli ||| giulia fronda ||| 
2020 ||| technological innovation to assess cognitive functions in attention deficit hyperactivity disorder. ||| carlos ramos-galarza ||| m ||| nica acosta-rodas ||| patricia acosta-vargas ||| luis salvador-ullauri ||| 
2021 ||| modeling the dynamic visual attention resource allocation in cockpit with discrete event simulation. ||| chaoran liang ||| shuang liu ||| xiaoru wanyan ||| hao chen ||| yuchen min ||| 
2020 ||| evaluation of the attention effect of the fraser-wilcox illusion in a visual discrimination task. ||| kota tokonabe ||| keiichi watanuki ||| kazunori kaede ||| keiichi muramatsu ||| 
2020 ||| the impact of a biological driver state monitoring system on visual attention during partially automated driving. ||| alice stephenson ||| iveta eimontaite ||| praminda caleb-solly ||| chris alford ||| 
2021 ||| measure of the attentional bias in children using eye tracking during a psychological test. ||| andrea argudo-v ||| sconez ||| omar alvarado-cando ||| cristian caldero ||| franklin buele ||| patricia ortega-chasi ||| martha cobos-cali ||| 
2020 ||| driver's visual attention analysis in smart car with fhud. ||| yanjun zhang ||| tian yang ||| xia zhang ||| yongjin zhang ||| youchao sun ||| 
2020 ||| collaborative cognitive training game to enhance selective sustained attention in preschoolers. ||| carlos a. arce-lopera ||| mateo torres ||| steven vacilescu ||| 
2020 ||| characterizing driver workload and attention in a simulated automated vehicle. ||| brittany e. holthausen ||| bruce n. walker ||| 
2017 ||| eeg-engagement index and auditory alarm misperception: an inattentional deafness study in actual flight condition. ||| fr ||| d ||| ric dehais ||| rapha ||| lle n. roy ||| gautier durantin ||| thibault gateau ||| daniel e. callan ||| 
2019 ||| effect of different visual stimuli on joint attention of asd children using nao robot. ||| sara ali ||| faisal mehmood ||| yasar ayaz ||| umer asgher ||| muhammad jawad khan ||| 
2020 ||| exploring attention in vr: effects of visual and auditory modalities. ||| alexandra voinescu ||| liviu-andrei fodor ||| dana |||  stanton fraser ||| daniel o. david ||| 
2019 ||| gear shifter design - lack of dedicated positions and the contribution to cognitive load and inattention. ||| sanna lohilahti bladf ||| lt ||| camilla grane ||| peter bengtsson ||| 
2021 ||| visual attention of pedestrians in traffic scenes: a crowdsourcing experiment. ||| pavlo bazilinskyy ||| dimitra dodou ||| joost c. f. de winter ||| 
2021 ||| the influence of e-commerce web page format on information area under attention mechanism. ||| fan zhang ||| yi su ||| jie liu ||| nan zhang ||| feng gao ||| 
2020 ||| engagement in the cinematography of videogames: proposal of an algorithm for colors and foci of attention analysis. ||| jesus gomezromero-borquez ||| carolina del-valle-soto ||| 
2019 ||| emotion measurement from attention analysis on imagery in virtual reality. ||| lucas paletta ||| amir dini ||| martin pszeida ||| 
2020 ||| the role of attentional networks in secondary task engagement in the context of partially automated driving. ||| rui lin ||| yuchen xu ||| wei zhang ||| 
2021 ||| an interactive guide based on learning objects to train teachers on the detection and support of children with attention deficit hyperactivity disorder. ||| karina parra-luzuriaga ||| yaroslava robles-bykbaev ||| vladimir robles-bykbaev ||| pa ||| l le ||| n-gom ||| z ||| 
2019 ||| happiness on instagram - content analysis and engagement based on attention theory. ||| qiuwen li ||| young ae kim ||| 
2021 ||| the hierarchy in the temporary interaction micro-processes that precede the breaking points of focal attention in an object of the new media. ||| lorena olmos pineda ||| jorge gil tejeda ||| 
2020 ||| augmented reality assisted sensory integration therapy for improving attention of children with autism. ||| xin he ||| xin song ||| 
2021 ||| sharing photos on social media: visual attention affects real-world decision making. ||| shawn e. fagan ||| lauren wade ||| kurt hugenberg ||| apu kapadia ||| bennett i. bertenthal ||| 
2020 ||| effect of paired stimuli on joint attention of children with asd. ||| sara ali ||| faisal mehmood ||| yasar ayaz ||| muhammad jawad khan ||| umer asgher ||| 
2017 ||| visual attention and recall in website advertisements: an eye tracking study. ||| hanne s ||| rum ||| 
2019 ||| towards the development of a universal testing environment for attention guiding techniques. ||| max bernhagen ||| philipp hein ||| andr |||  dettmann ||| angelika c. bullinger ||| 
2017 ||| predicting stimulus-driven attentional selection within mobile interfaces. ||| jeremiah d. still ||| john m. hicks ||| ashley a. cain ||| dorrit billman ||| 
2020 ||| comparing effect of active vs. passive robotic interaction on joint attention of children with asd. ||| faisal mehmood ||| sara ali ||| yasar ayaz ||| muhammad jawad khan ||| umer asgher ||| 
2021 ||| spatio-temporal 3d action recognition with hierarchical self-attention mechanism. ||| soheil araei ||| ali nadian ghomsheh ||| 
2021 ||| attribute-image similarity measure for multimodal attention mechanism. ||| ali salehi najafabadi ||| ali nadian ghomsheh ||| 
2019 ||| patient-level classification on clinical note sequences guided by attributed hierarchical attention. ||| cansu sen ||| thomas hartvigsen ||| xiangnan kong ||| elke a. rundensteiner ||| 
2020 ||| t-egat: a temporal edge enhanced graph attention network for tax evasion detection. ||| yiyang wang ||| qinghua zheng ||| jianfei ruan ||| yuda gao ||| yan chen ||| xuanya li ||| bo dong ||| 
2018 ||| applied attention-based lstm neural networks in stock prediction. ||| li-chen cheng ||| yu-hsiang huang ||| mu-en wu ||| 
2020 ||| hyper-parameter optimization with reinforce and masked attention auto-regressive density estimators. ||| chepuri shri krishna ||| ashish gupta ||| swarnim narayan ||| himanshu rai ||| diksha manchanda ||| 
2021 ||| come-ke: a new transformers based approach for knowledge extraction in conflict and mediation domain. ||| erick skorupa parolin ||| yibo hu ||| latifur khan ||| javier osorio ||| patrick t. brandt ||| vito d'orazio ||| 
2019 ||| non-local attention learning on large heterogeneous information networks. ||| yuxin xiao ||| zecheng zhang ||| carl yang ||| chengxiang zhai ||| 
2021 ||| two-stage image dehazing with depth information and cross-scale non-local attention. ||| lu cheng ||| li zhao ||| 
2020 ||| egad: evolving graph representation learning with self-attention and knowledge distillation for live video streaming events. ||| stefanos antaris ||| dimitrios rafailidis ||| sarunas girdzijauskas ||| 
2021 ||| session-aware item-combination recommendation with transformer network. ||| tzu-heng lin ||| chen gao ||| 
2018 ||| comparative study of cnn and lstm based attention neural networks for aspect-level opinion mining. ||| wei quan ||| zheng chen ||| jianliang gao ||| xiaohua tony hu ||| 
2021 ||| impact of attention on adversarial robustness of image classification models. ||| prachi agrawal ||| narinder singh punn ||| sanjay kumar sonbhadra ||| sonali agarwal ||| 
2020 ||| hypergraph attention isomorphism network by learning line graph expansion. ||| sambaran bandyopadhyay ||| kishalay das ||| m. narasimha murty ||| 
2021 ||| human-like explanation for text classification with limited attention supervision. ||| dongyu zhang ||| cansu sen ||| jidapa thadajarassiri ||| thomas hartvigsen ||| xiangnan kong ||| elke a. rundensteiner ||| 
2020 ||| evanet: an extreme value attention network for long-term air quality prediction. ||| zechuan chen ||| haomin yu ||| yangli-ao geng ||| qingyong li ||| yingjun zhang ||| 
2018 ||| spatio-temporal attention based recurrent neural network for next location prediction. ||| basmah altaf ||| lu yu ||| xiangliang zhang ||| 
2021 ||| dra u-net: an attention based u-net framework for 2d medical image segmentation. ||| xian zhang ||| ziyuan feng ||| tianchi zhong ||| sicheng shen ||| ruolin zhang ||| lijie zhou ||| bo zhang ||| wendong wang ||| 
2019 ||| hierarchical-document-structure-aware attention with adaptive cost sensitive learning for biomedical document classification. ||| dandan fang ||| jinyong zhang ||| weizhong zhao ||| xiaowei xu ||| xingpeng jiang ||| xiaohua hu ||| tingting he ||| 
2021 ||| hamlet: hierarchical attention-based model with multi-task self-training for user profiling. ||| fuxin ren ||| zhongbao zhang ||| yang yan ||| zhi wang ||| sen su ||| philip s. yu ||| 
2019 ||| attention-based multi-task learning for sensor analytics. ||| yujing chen ||| huzefa rangwala ||| 
2017 ||| product function need recognition via semi-supervised attention network. ||| hu xu ||| sihong xie ||| lei shu ||| philip s. yu ||| 
2021 ||| an ensemble of transformer and lstm approach for multivariate time series data classification. ||| aryan narayan ||| bodhi satwa mishra ||| p. g. sunitha hiremath ||| neha tarannum pendari ||| shankar gangisetty ||| 
2019 ||| modeling human attention by learning from large amount of emotional images. ||| macario o. cordel ii ||| 
2020 ||| explainable software vulnerability detection based on attention-based bidirectional recurrent neural networks. ||| yi mao ||| yun li ||| jiatai sun ||| yixin chen ||| 
2019 ||| costock: a deepfm model for stock market prediction with attentional embeddings. ||| jieyun huang ||| xi zhang ||| binxing fang ||| 
2021 ||| performance profile of transformer fine-tuning in multi-gpu cloud environments. ||| edmon begoli ||| seung-hwan lim ||| sudarshan srinivasan ||| 
2021 ||| glow : global weighted self-attention network for web search. ||| xuan shan ||| chuanjie liu ||| yiqian xia ||| qi chen ||| yusi zhang ||| kaize ding ||| yaobo liang ||| angen luo ||| yuxiang luo ||| 
2021 ||| transformer oil temperature prediction based on long and short-term memory networks. ||| jianxin sui ||| xiao ling ||| xing xiang ||| genwei zhang ||| xiangchi zhang ||| 
2020 ||| graphsanet: a graph neural network and self attention based approach for spatial temporal prediction in sensor network. ||| he li ||| shiyu zhang ||| liangcai su ||| hongjie huang ||| duo jin ||| xuejiao li ||| 
2019 ||| explainable authorship verification in social media via attention-based similarity learning. ||| benedikt t. boenninghoff ||| steffen hessler ||| dorothea kolossa ||| robert m. nickel ||| 
2021 ||| attention-augmented spatio-temporal segmentation for land cover mapping. ||| rahul ghosh ||| praveen ravirathinam ||| xiaowei jia ||| chenxi lin ||| zhenong jin ||| vipin kumar ||| 
2019 ||| abr-hic: attention based bidirectional rnn for hierarchical industry classification. ||| rongzhe wei ||| qinghua zheng ||| bo dong ||| kuanzheng yang ||| huan he ||| jianfei ruan ||| 
2019 ||| metapath enhanced graph attention encoder for hins representation learning. ||| yuwei fu ||| yun xiong ||| philip s. yu ||| tianyi tao ||| yangyong zhu ||| 
2021 ||| multi-input-output fusion attention module for deblurring networks. ||| yiqing fan ||| chaoqun hong ||| xiaodong wang ||| zhiqiang zeng ||| zetian guo ||| 
2021 ||| on exploring attention-based explanation for transformer models in text classification. ||| shengzhong liu ||| franck le ||| supriyo chakraborty ||| tarek f. abdelzaher ||| 
2020 ||| 2d-att: causal inference for mobile game organic installs with 2-dimensional attentional neural network. ||| boxiang dong ||| hui bill li ||| yang ryan wang ||| rami safadi ||| 
2020 ||| attention-based lstm network for covid-19 clinical trial parsing. ||| xiong liu ||| luca a. finelli ||| greg l. hersch ||| iya khalil ||| 
2018 ||| how to become instagram famous: post popularity prediction with dual-attention. ||| zhongping zhang ||| tianlang chen ||| zheng zhou ||| jiaxin li ||| jiebo luo ||| 
2021 ||| transforming fake news: robust generalisable news classification using transformers. ||| ciara blackledge ||| amir atapour-abarghouei ||| 
2018 ||| market abnormality period detection via co-movement attention model. ||| yue wang ||| chenwei zhang ||| shen wang ||| philip s. yu ||| lu bai ||| lixin cui ||| 
2021 ||| non-parallel text style transfer using self-attentional discriminator as supervisor. ||| kuan feng ||| yanmin zhu ||| jiadi yu ||| 
2018 ||| cam: a combined attention model for natural language inference. ||| amit gajbhiye ||| sardar f. jaf ||| noura al moubayed ||| steven bradley ||| a. stephen mcgough ||| 
2021 ||| a hierarchical attention graph convolutional network for traffic incident impact forecasting. ||| kaiqun fu ||| taoran ji ||| nathan self ||| zhiqian chen ||| chang-tien lu ||| 
2020 ||| cosine similarity distance pruning algorithm based on graph attention mechanism. ||| huaxiong yao ||| yang huang ||| jiabei hu ||| wenqi xie ||| 
2020 ||| link prediction based on heuristics and graph attention. ||| innocent boakye ababio ||| jianxia chen ||| yu chen ||| liang xiao ||| 
2021 ||| a re-thinking asr modeling framework using attention mechanisms. ||| chih-ying yang ||| kuan-yu chen ||| 
2019 ||| attention-based multi-layer chinese word embedding. ||| bing ma ||| haifeng sun ||| jingyu wang ||| qi qi ||| 
2018 ||| efficient super resolution for large-scale images using attentional gan. ||| harsh nilesh pathak ||| xinxin li ||| shervin minaee ||| brooke cowan ||| 
2020 ||| mastgn: multi-attention spatio-temporal graph networks for air pollution prediction. ||| peijiang zhao ||| koji zettsu ||| 
2021 ||| soft sensing transformer: hundreds of sensors are worth a single word. ||| chao zhang ||| jaswanth yella ||| yu huang ||| xiaoye qian ||| sergei petrov ||| andrey rzhetsky ||| sthitie bom ||| 
2020 ||| hardening soft information: a transformer-based approach to forecasting stock return volatility. ||| matthew caron ||| oliver m ||| ller ||| 
2020 ||| self-calibrated attention residual network for image super-resolution. ||| anqi rong ||| li zhao ||| pengcheng huang ||| jiawei xu ||| 
2021 ||| multi-feature urban traffic prediction based on unconstrained graph attention network. ||| hangtao he ||| kejiang ye ||| cheng-zhong xu ||| 
2019 ||| study of the effects of visual complexity and consumer experience on visual attention and purchase behavior through the use of eye tracking. ||| ken ishibashi ||| chen xiao ||| katsutoshi yada ||| 
2021 ||| soft-sensing conformer: a curriculum learning-based convolutional transformer. ||| jaswanth yella ||| chao zhang ||| sergei petrov ||| yu huang ||| xiaoye qian ||| ali a. minai ||| sthitie bom ||| 
2021 ||| object interaction recommendation with multi-modal attention-based hierarchical graph neural network. ||| huijuan zhang ||| lipeng liang ||| dongqing wang ||| 
2020 ||| urban crowdsensing using social media: an empirical study on transformer and recurrent neural networks. ||| jerome heng ||| junhua liu ||| kwan hui lim ||| 
2020 ||| glima: global and local time series imputation with multi-directional attention learning. ||| qiuling suo ||| weida zhong ||| guangxu xun ||| jianhui sun ||| changyou chen ||| aidong zhang ||| 
2019 ||| learning to generate diverse and authentic reviews via an encoder-decoder model with transformer and gru. ||| kaifu jin ||| xi zhang ||| jiayuan zhang ||| 
2021 ||| spatiotemporal vision transformer for short time weather forecasting. ||| alabi bojesomo ||| hasan al-marzouqi ||| panos liatsis ||| 
2019 ||| ctc-attention based non-parametric inference modeling for clinical state progression. ||| riazat ryan ||| handong zhao ||| ming shao ||| 
2021 ||| temporal and spatial attention network model based evolution model for bulk commodity price fluctuation risk. ||| zhang wenjing ||| zhao gang ||| 
2021 ||| ise-yolo: improved squeeze-and-excitation attention module based yolo for blood cells detection. ||| cong liu ||| dengwang li ||| pu huang ||| 
2018 ||| per: a probabilistic attentional model for personalized text recommendations. ||| lei zheng ||| yixue wang ||| lifang he ||| sihong xie ||| fengjiao wang ||| philip s. yu ||| 
2019 ||| deep multi-head attention network for aspect-based sentiment analysis. ||| danfeng yan ||| jiyuan chen ||| jianfei cui ||| ao shan ||| wenting shi ||| 
2019 ||| weighted focus-attention deep network for fine-grained image classification. ||| cong zou ||| rui wang ||| xiaochun cao ||| feixiao lv ||| 
2019 ||| human-object contour for action recognition with attentional multi-modal fusion network. ||| miao yu ||| weizhe zhang ||| qingxiang zeng ||| chao wang ||| jie li ||| 
2022 ||| ciafill: lightweight and fast image inpainting with channel independent attention. ||| chung-il kim ||| saim shin ||| han-mu park ||| 
2020 ||| predictive analysis of business processes using neural networks with attention mechanism. ||| patrick philipp ||| ruben jacob ||| sebastian robert ||| j ||| rgen beyerer ||| 
2022 ||| multiview attention for 3d object detection in lidar point cloud. ||| kevin tirta wijaya ||| dong-hee paek ||| seung-hyun kong ||| 
2019 ||| chinese story generation with fasttext transformer network. ||| jhe-wei lin ||| yu-che gao ||| rong-guey chang ||| 
2022 ||| multi-head cnn and lstm with attention for user status estimation from biometric information. ||| hyunseo park ||| nakyoung kim ||| gyeong ho lee ||| jaeseob han ||| hyeontaek oh ||| jun kyun choi ||| 
2020 ||| effective-target representation via lstm with attention for aspect-level sentiment analysis. ||| quan liu ||| hiroaki mukaidani ||| 
2022 ||| whole slide image analysis and detection of prostate cancer using vision transformers. ||| kobiljon ikromjanov ||| subrata bhattacharjee ||| yeong-byn hwang ||| rashadul islam sumon ||| hee-cheol kim ||| heung-kook choi ||| 
2021 ||| small object detection using context and attention. ||| jeong-seon lim ||| marcella astrid ||| hyun-jin yoon ||| seung-ik lee ||| 
2020 ||| multi person pose estimation with attention. ||| jaekyu sim ||| kwanghyun park ||| 
2018 ||| document level polarity classification with attention gated recurrent unit. ||| hoon-keng poon ||| wun-she yap ||| yee-kai tee ||| bok-min goi ||| wai-kong lee ||| 
2022 ||| economic denial of sustainability (edos) attack detection by attention on flow-based in software defined network (sdn). ||| quoc vinh ta ||| minho park ||| 
2021 ||| non-local self-attention mechanism for real-time context embedding deep shadow removal network. ||| dohyun kim ||| joongheon kim ||| 
2018 ||| classification of human attention to multimedia lecture. ||| heejun lee ||| youngjoo kim ||| cheolsoo park ||| 
2019 ||| application of granger causality in decoding covert selective attention with human eeg. ||| weikun niu ||| yuying jiang ||| yujin zhang ||| xin zhang ||| shan yu ||| 
2021 ||| trans-attention multiple instance learning for cancer tissue classification in digital histopathology images. ||| afaf alharbi ||| yaqi wang ||| qianni zhang ||| 
2021 ||| crossmodal matching transformer based x-ray and ct image registration for tevar. ||| meng li ||| changyan lin ||| lixia shu ||| xin pu ||| yu chen ||| heng wu ||| jiasong li ||| hongshuai cao ||| 
2021 ||| fa-net: attention-based fusion network for malware https traffic classification. ||| siqi liu ||| yanni han ||| yanjie hu ||| qian tan ||| 
2020 ||| an ai-based visual attention model for vehicle make and model recognition. ||| xiren ma ||| azzedine boukerche ||| 
2021 ||| fktan: fusion keystroke time-textual attention networks for continuous authentication. ||| haitian yang ||| degang sun ||| yan wang ||| he zhu ||| ning li ||| weiqing huang ||| 
2021 ||| improved face detector on fisheye images via spherical-domain attention. ||| jingbo miao ||| yanwei liu ||| jinxia liu ||| antonios argyriou ||| zhen xu ||| yanni han ||| 
2021 ||| multi-modal fake news detection on social media with dual attention fusion networks. ||| haitian yang ||| xuan zhao ||| degang sun ||| yan wang ||| he zhu ||| chao ma ||| weiqing huang ||| 
2021 ||| interpretable deep learning method for attack detection based on spatial domain attention. ||| hongyu liu ||| bo lang ||| shaojie chen ||| mengyang yuan ||| 
2020 ||| a multivariate time series prediction schema based on multi-attention in recurrent neural network. ||| xiang yin ||| yanni han ||| hongyu sun ||| zhen xu ||| haibo yu ||| xiaoyu duan ||| 
2018 ||| multi-agent communication with attentional and recurrent message integration. ||| zhaoqing peng ||| libo zhang ||| tiejian luo ||| 
2021 ||| improved cnn-based magnetic indoor positioning system using attention mechanism. ||| mahdi abid ||| paul compagnon ||| gr ||| goire lefebvre ||| 
2021 ||| drug-drug interaction extraction from biomedical texts based on multi-attention mechanism. ||| chengkun wu ||| wei wang ||| xi yang ||| canqun yang ||| 
2018 ||| exploring the correlation between attention and cognitive load of students when attending different classes. ||| shu-chen cheng ||| yu-ping cheng ||| chien-hao huang ||| yueh-min huang ||| 
2019 ||| visual attention analysis during program debugging using virtual reality eye tracker. ||| chun-chia wang ||| jason c. hung ||| shih-cheng wang ||| yueh-min huang ||| 
2019 ||| a comparative study of attention-based encoder-decoder approaches to natural scene text recognition. ||| fu'ze cong ||| wenping hu ||| qiang huo ||| li guo ||| 
2021 ||| going full-tilt boogie on document understanding with text-image-layout transformer. ||| rafal powalski ||| lukasz borchmann ||| dawid jurkiewicz ||| tomasz dwojak ||| michal pietruszka ||| gabriela palka ||| 
2019 ||| reelfa: a scene text recognizer with encoded location and focused attention. ||| qingqing wang ||| wenjing jia ||| xiangjian he ||| yue lu ||| michael blumenstein ||| ye huang ||| shujing lyu ||| 
2021 ||| recognizing handwritten chinese texts with insertion and swapping using a structural attention network. ||| shi yan ||| jin-wen wu ||| fei yin ||| cheng-lin liu ||| 
2019 ||| recurrent comparator with attention models to detect counterfeit documents. ||| albert berenguel centeno ||| oriol ramos terrades ||| josep llad ||| s canet ||| cristina ca ||| ero morales ||| 
2017 ||| attention-based extraction of structured information from street view imagery. ||| zbigniew wojna ||| alexander n. gorban ||| dar-shyang lee ||| kevin murphy ||| qian yu ||| yeqing li ||| julian ibarz ||| 
2017 ||| visual attention models for scene text recognition. ||| suman k. ghosh ||| ernest valveny ||| andrew d. bagdanov ||| 
2021 ||| on the use of attention in deep learning based denoising method for ancient cham inscription images. ||| tien-nam nguyen ||| jean-christophe burie ||| thi-lan le ||| anne-val ||| rie schweyer ||| 
2021 ||| multi-task learning for newspaper image segmentation and baseline detection using attention-based u-net architecture. ||| anukriti bansal ||| prerana mukherjee ||| divyansh joshi ||| devashish tripathi ||| arun pratap singh ||| 
2021 ||| multimodal attention-based learning for imbalanced corporate documents classification. ||| ibrahim souleiman mahamoud ||| joris voerman ||| micka ||| l coustaty ||| aur ||| lie joseph ||| vincent poulain d'andecy ||| jean-marc ogier ||| 
2021 ||| a transformer-based math language model for handwritten math expression recognition. ||| quang huy ung ||| cuong tuan nguyen ||| hung tuan nguyen ||| thanh-nghia truong ||| masaki nakagawa ||| 
2019 ||| woodblock-printing mongolian words recognition by bi-lstm with attention mechanism. ||| yanke kang ||| hongxi wei ||| hui zhang ||| guanglai gao ||| 
2019 ||| document binarization via multi-resolutional attention model with drd loss. ||| xujun peng ||| chao wang ||| huaigu cao ||| 
2019 ||| lpga: line-of-sight parsing with graph-based attention for math formula recognition. ||| mahshad mahdavi ||| michael condon ||| kenny davila ||| richard zanibbi ||| 
2021 ||| a-vlad: an end-to-end attention-based neural network for writer identification in historical documents. ||| trung tan ngo ||| hung tuan nguyen ||| masaki nakagawa ||| 
2019 ||| an attention-based end-to-end model for multiple text lines recognition in japanese historical documents. ||| nam tuan ly ||| cuong tuan nguyen ||| masaki nakagawa ||| 
2021 ||| labeling document images for e-commence products with tree-based segment re-organizing and hierarchical transformer. ||| peng li ||| pingguang yuan ||| yong li ||| yongjun bao ||| weipeng yan ||| 
2021 ||| 2d self-attention convolutional recurrent network for offline handwritten text recognition. ||| nam tuan ly ||| hung tuan nguyen ||| masaki nakagawa ||| 
2017 ||| attention based rnn model for document image quality assessment. ||| pengchao li ||| liangrui peng ||| junyang cai ||| xiaoqing ding ||| shuangkui ge ||| 
2021 ||| attention to warp: deep metric learning for multivariate time series. ||| shinnosuke matsuo ||| xiaomeng wu ||| gantugs atarsaikhan ||| akisato kimura ||| kunio kashino ||| brian kenji iwana ||| seiichi uchida ||| 
2021 ||| fast recognition for multidirectional and multi-type license plates with 2d spatial attention. ||| qi liu ||| song-lu chen ||| zhen-jia li ||| chun yang ||| feng chen ||| xu-cheng yin ||| 
2019 ||| on the use of attention mechanism in a seq2seq based approach for off-line handwritten digit string recognition. ||| thibault lupinski ||| abdel bela ||| d ||| afef kacem echi ||| 
2021 ||| a transcription is all you need: learning to align through attention. ||| pau torras ||| mohamed ali souibgui ||| jialuo chen ||| alicia forn ||| s ||| 
2021 ||| key-guided identity document classification method by graph attention network. ||| xiaojie xia ||| wei liu ||| ying zhang ||| liuan wang ||| jun sun ||| 
2021 ||| vision transformer for fast and efficient scene text recognition. ||| rowel atienza ||| 
2017 ||| scan, attend and read: end-to-end handwritten paragraph recognition with mdlstm attention. ||| th ||| odore bluche ||| j ||| r ||| me louradour ||| ronaldo o. messina ||| 
2019 ||| recognition of japanese historical text lines by an attention-based encoder-decoder and text line generation. ||| anh duc le ||| daichi mochihashi ||| katsuya masuda ||| hideki mima ||| nam tuan ly ||| 
2019 ||| eaten: entity-aware attention for single shot visual text extraction. ||| he guo ||| xiameng qin ||| jiaming liu ||| junyu han ||| jingtuo liu ||| errui ding ||| 
2021 ||| dynamic receptive field adaptation for attention-based text recognition. ||| haibo qin ||| chun yang ||| xiaobin zhu ||| xu-cheng yin ||| 
2019 ||| attention after attention: reading text in the wild with cross attention. ||| yunlong huang ||| canjie luo ||| lianwen jin ||| qingxiang lin ||| weiying zhou ||| 
2019 ||| a character attention generative adversarial network for degraded historical document restoration. ||| kha cong nguyen ||| cuong tuan nguyen ||| seiji hotta ||| masaki nakagawa ||| 
2021 ||| transformer for handwritten text recognition using bidirectional post-decoding. ||| christoph wick ||| jochen z ||| llner ||| tobias gr ||| ning ||| 
2019 ||| multiple comparative attention network for offline handwritten chinese character recognition. ||| qingquan xu ||| xiang bai ||| wenyu liu ||| 
2017 ||| a gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. ||| jianshu zhang ||| jun du ||| lirong dai ||| 
2021 ||| attention based multiple siamese network for offline signature verification. ||| yu-jie xiong ||| song-yang cheng ||| 
2017 ||| weakly supervised text attention network for generating text proposals in scene images. ||| rong li ||| mengyi en ||| jianqiang li ||| haibin zhang ||| 
2019 ||| multi-modal attention network for handwritten mathematical expression recognition. ||| jia-ming wang ||| jun du ||| jianshu zhang ||| zi-rui wang ||| 
2021 ||| handwritten mathematical expression recognition with bidirectionally trained transformer. ||| wenqi zhao ||| liangcai gao ||| zuoyu yan ||| shuai peng ||| lin du ||| ziyin zhang ||| 
2017 ||| segmentation-free printed traditional mongolian ocr using sequence to sequence with attention model. ||| hui zhang ||| hongxi wei ||| feilong bao ||| guanglai gao ||| 
2019 ||| contextual stroke classification in online handwritten documents with graph attention networks. ||| jun-yu ye ||| yan-ming zhang ||| qing yang ||| cheng-lin liu ||| 
2017 ||| convolutional neural network with attention mechanism for historical chinese character recognition. ||| haoyu qin ||| liangrui peng ||| 
2021 ||| hcadecoder: a hybrid ctc-attention decoder for chinese text recognition. ||| siqi cai ||| wenyuan xue ||| qingyong li ||| peng zhao ||| 
2021 ||| an encoder-decoder approach to handwritten mathematical expression recognition with multi-head attention and stacked decoder. ||| haisong ding ||| kai chen ||| qiang huo ||| 
2019 ||| a deep learning method for automatic visual attention detection in older drivers. ||| belkacem chikhaoui ||| perrine ruer ||| evelyne f. valli ||| res ||| 
2019 ||| poster: attention-based spatio-temporal model for har using multivariate time series. ||| rui xi ||| ming li ||| daibo liu ||| mengshu hou ||| 
2019 ||| poster: watchyouwatch a web-cam based natural customer attention tracking shelf. ||| mengxin cao ||| zhiyuan liu ||| haotian long ||| guang li ||| 
2020 ||| short-term load forecasting based on cnn-bilstm with bayesian optimization and attention mechanism. ||| kai miao ||| qiang hua ||| huifeng shi ||| 
2019 ||| attention-based supply-demand prediction for autonomous vehicles. ||| zikai zhang ||| hairong dong ||| yidong li ||| yizhe you ||| fengping zhao ||| 
2019 ||| an affect computing based attention estimation. ||| amey desai ||| samarth jain ||| lohit marodi ||| sreejith v ||| 
2021 ||| structure optimization method of power transformer based on intelligent algorithm. ||| yuan tian ||| peng yang ||| huan li ||| zhiyong chen ||| 
2021 ||| short text generation based on adversarial graph attention networks. ||| meng chen ||| 
2021 ||| interactive attention graph convolution networks for aspect-level sentiment classification. ||| hu han ||| xiaoya qin ||| qitao zhao ||| 
2021 ||| partial discharge detection of transformer winding. ||| jiahao zou ||| 
2021 ||| intelligent detection system of transformer winding temperature based on distributed optical fiber. ||| jiahao zou ||| 
2021 ||| research on cloud-edge collaborative processing method of distribution internet of things based on attention-lstm. ||| xiaoming huang ||| pan zhang ||| rongqiang feng ||| chenxi huang ||| kun zhang ||| kai jin ||| 
2021 ||| attention-based feature fusion network for fake reviews detection. ||| yizhang xu ||| qi li ||| 
2021 ||| the cause of transformer zero sequence overcurrent protection act. ||| jing hua ||| wei xiong ||| 
2019 ||| a hierarchy transformer network for extractive summaries. ||| yuanbo gao ||| qiang zhang ||| 
2021 ||| using graph attention network to predicte urban traffic flow. ||| gaohao zhou ||| changyuan wang ||| qiang mei ||| 
2018 ||| an attention-based long-short-term-memory model for paraphrase generation. ||| khuong nguyen-ngoc ||| anh-cuong le ||| viet-ha nguyen ||| 
2020 ||| track-assignment detailed routing using attention-based policy model with supervision. ||| haiguang liao ||| qingyi dong ||| weiyi qi ||| elias fallon ||| levent burak kara ||| 
2021 ||| a circuit attention network-based actor-critic learning approach to robust analog transistor sizing. ||| yaguang li ||| yishuang lin ||| meghna madhusudan ||| arvind k. sharma ||| sachin s. sapatnekar ||| ramesh harjani ||| jiang hu ||| 
2021 ||| dynamic transformer for efficient machine translation on embedded devices. ||| hishan parry ||| lei xun ||| amin sabet ||| jia bi ||| jonathon hare ||| geoff v. merrett ||| 
2020 ||| 5g-transformer meets network service federation: design, implementation and evaluation. ||| jorge baranda ||| josep mangues-bafalluy ||| ricardo mart ||| nez ||| luca vettori ||| kiril antevski ||| carlos j. bernardos ||| xi li ||| 
2022 ||| remot: a hardware-software architecture for attention-guided multi-object tracking with dynamic vision sensors on fpgas. ||| yizhao gao ||| song wang ||| hayden kwok-hay so ||| 
2018 ||| scaling notifications beyond alerts: from subtly drawing attention up to forcing the user to take action. ||| denys j. c. matthies ||| laura milena daza parra ||| bodo urban ||| 
2020 ||| flashattention: data-centric interaction for data transformation using programming-by-example. ||| minori narita ||| nolwenn maudet ||| yi lu ||| takeo igarashi ||| 
2018 ||| augmented collaboration in shared space design with shared attention and manipulation. ||| yoonjeong cha ||| sungu nam ||| mun yong yi ||| jaeseung jeong ||| woontack woo ||| 
2021 ||| motion improvisation: 3d human motion synthesis with a transformer. ||| yimeng liu ||| misha sra ||| 
2019 ||| attention-based recurrent neural networks (rnns) for short text classification: an application in public health monitoring. ||| oduwa edo-osagie ||| iain lake ||| obaghe edeghere ||| beatriz de la iglesia ||| 
2021 ||| detection of tumor morphology mentions in clinical reports in spanish using transformers. ||| guillermo l ||| pez-garc ||| a ||| jos |||  m. jerez ||| nuria ribelles ||| emilio alba ||| francisco j. veredas ||| 
2019 ||| document model with attention bidirectional recurrent network for gender identification. ||| bassem bsir ||| mounir zrigui ||| 
2017 ||| enjoyment, immersion, and attentional focus in a virtual reality exergame with differing visual environments. ||| michael abernathy ||| lindsay alexander shaw ||| christof lutteroth ||| jude buckley ||| paul m. corballis ||| burkhard c. w ||| nsche ||| 
2020 ||| attention, awareness, and analysis: video clubs as meaningful venues for teacher noticing and culturally-sustaining pedagogy. ||| melissa j. luna ||| malayna bernstein ||| janet walkoe ||| 
2020 ||| designing for joint attention and co-presence across parallel realities. ||| rolf steier ||| 
2020 ||| simultaneous paraphrasing and translation by fine-tuning transformer models. ||| rakesh chada ||| 
2017 ||| an empirical study of adequate vision span for attention-based neural machine translation. ||| raphael shu ||| hideki nakayama ||| 
2018 ||| a shared attention mechanism for interpretation of neural automatic post-editing systems. ||| inigo jauregi unanue ||| ehsan zare borzeshi ||| massimo piccardi ||| 
2018 ||| enhancement of encoder and attention using target monolingual corpora in neural machine translation. ||| kenji imamura ||| atsushi fujita ||| eiichiro sumita ||| 
2020 ||| balancing cost and benefit with tied-multi transformers. ||| raj dabre ||| raphael rubino ||| atsushi fujita ||| 
2019 ||| a weakly supervised text detection based on attention mechanism. ||| lanfang dong ||| diancheng zhou ||| hanchao liu ||| 
2017 ||| combining object-based attention and attributes for image captioning. ||| cong li ||| jiansheng chen ||| weitao wan ||| tianpeng li ||| 
2021 ||| towards boosting channel attention for real image denoising: sub-band pyramid attention. ||| huayu li ||| haiyu wu ||| xiwen chen ||| hao wang ||| abolfazl razi ||| 
2021 ||| sa-gnn: stereo attention and graph neural network for stereo image super-resolution. ||| huiling li ||| qiong liu ||| you yang ||| 
2021 ||| dual attention guided r2 u-net architecture for right ventricle segmentation in mri images. ||| lei jiang ||| hengfei cui ||| chang yuwen ||| yanning zhang ||| 
2021 ||| 6d object pose estimation with mutual attention fusion. ||| lu zou ||| zhangjin huang ||| naijie gu ||| 
2017 ||| neural image caption generation with global feature based attention scheme. ||| yongzhuang wang ||| hongkai xiong ||| 
2019 ||| residual joint attention network with graph structure inference for object detection. ||| chuansheng xu ||| gaoyun an ||| qiuqi ruan ||| 
2021 ||| multi-scale attention-based feature pyramid networks for object detection. ||| xiaodong zhao ||| junliang chen ||| minmin liu ||| kai ye ||| linlin shen ||| 
2019 ||| attention to head locations for crowd counting. ||| youmei zhang ||| chunluan zhou ||| faliang chang ||| alex c. kot ||| wei zhang ||| 
2021 ||| facial action unit detection based on transformer and attention mechanism. ||| wenyu song ||| shuze shi ||| gaoyun an ||| 
2021 ||| pst-net: point cloud sampling via point-based transformer. ||| xu wang ||| yi jin ||| yigang cen ||| congyan lang ||| yidong li ||| 
2021 ||| aroa: attention refinement one-stage anchor-free detector for objects in remote sensing imagery. ||| xu he ||| shiping ma ||| linyuan he ||| fei zhang ||| xulun liu ||| le ru ||| 
2017 ||| attention-sharing correlation learning for cross-media retrieval. ||| xin huang ||| zhaoda ye ||| yuxin peng ||| 
2017 ||| an unsupervised change detection approach for remote sensing image using visual attention mechanism. ||| lin wu ||| guanghua feng ||| jiangtao long ||| 
2019 ||| attention-aware invertible hashing network. ||| shanshan li ||| qiang cai ||| zhuangzi li ||| haisheng li ||| naiguang zhang ||| jian cao ||| 
2021 ||| multi-level features selection network based on multi-attention for salient object detection. ||| jianyi ren ||| zheng wang ||| meijun sun ||| 
2021 ||| human-object interaction detection based on multi-scale attention fusion. ||| qianling wu ||| yongzhao zhan ||| 
2021 ||| attention-guided siamese network for clothes-changing person re-identification. ||| zhan-xiang feng ||| sien huang ||| jianhuang lai ||| 
2019 ||| online handwritten diagram recognition with graph attention networks. ||| xiao-long yun ||| yan-ming zhang ||| jun-yu ye ||| cheng-lin liu ||| 
2021 ||| learning cross-domain descriptors for 2d-3d matching with hard triplet loss and spatial transformer network. ||| baiqi lai ||| weiquan liu ||| cheng wang ||| xuesheng bian ||| yanfei su ||| xiuhong lin ||| zhimin yuan ||| siqi shen ||| ming cheng ||| 
2021 ||| cab-net: channel attention block network for pathological image cell nucleus segmentation. ||| meixuan li ||| huijie fan ||| dawei yang ||| 
2019 ||| visual tracking with attentional convolutional siamese networks. ||| ke tan ||| zhenzhong wei ||| 
2021 ||| hetero-stan: crowd flow prediction by heterogeneous spatio-temporal attention network. ||| kai fang ||| enze yang ||| yuxin liu ||| shuoyan liu ||| 
2019 ||| u-net with attention mechanism for retinal vessel segmentation. ||| ze si ||| dongmei fu ||| jiahao li ||| 
2019 ||| a stackable attention-guided multi-scale cnn for number plate detection. ||| yixuan wang ||| shangdong zheng ||| wei xu ||| yang xu ||| tianming zhan ||| peng zheng ||| zhihui wei ||| zebin wu ||| 
2021 ||| gscam: global spatial coordinate attention module for fine-grained image recognition. ||| haojie guo ||| zhe guo ||| zhaojun pan ||| 
2021 ||| 3d-resnet fused attention for autism spectrum disorder classification. ||| xiangjun chen ||| zhaohui wang ||| faouzi alaya cheikh ||| mohib ullah ||| 
2019 ||| mma: motion memory attention network for video object detection. ||| huai hu ||| wenzhong wang ||| aihua zheng ||| bin luo ||| 
2019 ||| spatial-temporal bottom-up top-down attention model for action recognition. ||| jinpeng wang ||| andy j. ma ||| 
2021 ||| hpcseg-net: hippocampus segmentation network integrating autofocus attention mechanism and feature recombination and recalibration module. ||| bin liu ||| qiang zheng ||| kun zhao ||| honglun li ||| chaoqing ma ||| shuanhu wu ||| xiangrong tong ||| 
2021 ||| uav track planning algorithm based on graph attention network and deep q network. ||| xinyu hu ||| jingpeng gao ||| zhiye jiang ||| 
2021 ||| semi-supervised attention-guided vnet for breast cancer detection via multi-task learning. ||| yiyao liu ||| yi yang ||| wei jiang ||| tianfu wang ||| baiying lei ||| 
2020 ||| gate-fusion transformer for multimodal sentiment analysis. ||| long-fei xie ||| xu-yao zhang ||| 
2019 ||| transferable attention for domain adaptation. ||| ximei wang ||| liang li ||| weirui ye ||| mingsheng long ||| jianmin wang ||| 
2017 ||| let your photos talk: generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. ||| yu liu ||| jianlong fu ||| tao mei ||| chang wen chen ||| 
2019 ||| context-aware self-attention networks. ||| baosong yang ||| jian li ||| derek f. wong ||| lidia s. chao ||| xing wang ||| zhaopeng tu ||| 
2018 ||| path-based attention neural model for fine-grained entity typing. ||| denghui zhang ||| manling li ||| pengshan cai ||| yantao jia ||| yuanzhuo wang ||| 
2020 ||| partial correlation-based attention for multivariate time series forecasting. ||| won kyung lee ||| 
2021 ||| an attention based multi-view model for sarcasm cause detection (student abstract). ||| hejing liu ||| qiudan li ||| zaichuan tang ||| jie bai ||| 
2019 ||| recurrent attention model for pedestrian attribute recognition. ||| xin zhao ||| liufang sang ||| guiguang ding ||| jungong han ||| na di ||| chenggang yan ||| 
2019 ||| structured two-stream attention network for video question answering. ||| lianli gao ||| pengpeng zeng ||| jingkuan song ||| yuan-fang li ||| wu liu ||| tao mei ||| heng tao shen ||| 
2019 ||| sta: spatial-temporal attention for large-scale video-based person re-identification. ||| yang fu ||| xiaoyang wang ||| yunchao wei ||| thomas s. huang ||| 
2021 ||| attention beam: an image captioning approach (student abstract). ||| anubhav shrimal ||| tanmoy chakraborty ||| 
2018 ||| hierarchical attention flow for multiple-choice reading comprehension. ||| haichao zhu ||| furu wei ||| bing qin ||| ting liu ||| 
2019 ||| connecting language to images: a progressive attention-guided network for simultaneous image captioning and language grounding. ||| lingyun song ||| jun liu ||| buyue qian ||| yihe chen ||| 
2021 ||| dual-level collaborative transformer for image captioning. ||| yunpeng luo ||| jiayi ji ||| xiaoshuai sun ||| liujuan cao ||| yongjian wu ||| feiyue huang ||| chia-wen lin ||| rongrong ji ||| 
2020 ||| a knowledge-aware attentional reasoning network for recommendation. ||| qiannan zhu ||| xiaofei zhou ||| jia wu ||| jianlong tan ||| li guo ||| 
2017 ||| battrae: bidimensional attention-based recursive autoencoders for learning bilingual phrase embeddings. ||| biao zhang ||| deyi xiong ||| jinsong su ||| 
2021 ||| act: an attentive convolutional transformer for efficient text classification. ||| pengfei li ||| peixiang zhong ||| kezhi mao ||| dongzhe wang ||| xuefeng yang ||| yunfeng liu ||| jianxiong yin ||| simon see ||| 
2021 ||| cascade network with guided loss and hybrid attention for finding good correspondences. ||| zhi chen ||| fan yang ||| wenbing tao ||| 
2020 ||| sneq: semi-supervised attributed network embedding with attention-based quantisation. ||| tao he ||| lianli gao ||| jingkuan song ||| xin wang ||| kejie huang ||| yuanfang li ||| 
2020 ||| context-transformer: tackling object confusion for few-shot detection. ||| ze yang ||| yali wang ||| xianyu chen ||| jianzhuang liu ||| yu qiao ||| 
2021 ||| improving image captioning by leveraging intra- and inter-layer global representation in transformer network. ||| jiayi ji ||| yunpeng luo ||| xiaoshuai sun ||| fuhai chen ||| gen luo ||| yongjian wu ||| yue gao ||| rongrong ji ||| 
2021 ||| transformer-style relational reasoning with dynamic memory updating for temporal network modeling. ||| dongkuan xu ||| junjie liang ||| wei cheng ||| hua wei ||| haifeng chen ||| xiang zhang ||| 
2021 ||| learning light-weight translation models from deep transformer. ||| bei li ||| ziyang wang ||| hui liu ||| quan du ||| tong xiao ||| chunliang zhang ||| jingbo zhu ||| 
2021 ||| modeling the momentum spillover effect for stock prediction via attribute-driven graph attention networks. ||| rui cheng ||| qing li ||| 
2020 ||| decoupled attention network for text recognition. ||| tianwei wang ||| yuanzhi zhu ||| lianwen jin ||| canjie luo ||| xiaoxue chen ||| yaqiang wu ||| qianying wang ||| mingxiang cai ||| 
2018 ||| adaptive co-attention network for named entity recognition in tweets. ||| qi zhang ||| jinlan fu ||| xiaoyu liu ||| xuanjing huang ||| 
2020 ||| ultrafast video attention prediction with coupled knowledge distillation. ||| kui fu ||| peipei shi ||| yafei song ||| shiming ge ||| xiangju lu ||| jia li ||| 
2020 ||| graph-based transformer with cross-candidate verification for semantic parsing. ||| bo shao ||| yeyun gong ||| weizhen qi ||| guihong cao ||| jianshu ji ||| xiaola lin ||| 
2020 ||| graph attention based proposal 3d convnets for action detection. ||| jin li ||| xianglong liu ||| zhuofan zong ||| wanru zhao ||| mingyuan zhang ||| jingkuan song ||| 
2019 ||| logic attention based neighborhood aggregation for inductive knowledge graph embedding. ||| peifeng wang ||| jialong han ||| chenliang li ||| rong pan ||| 
2020 ||| trimodal attention module for multimodal sentiment analysis (student abstract). ||| anirudh bindiganavale harish ||| fatiha sadat ||| 
2019 ||| a radical-aware attention-based model for chinese text classification. ||| hanqing tao ||| shiwei tong ||| hongke zhao ||| tong xu ||| binbin jin ||| qi liu ||| 
2019 ||| residual attribute attention network for face image super-resolution. ||| jingwei xin ||| nannan wang ||| xinbo gao ||| jie li ||| 
2020 ||| weakly-supervised video re-localization with multiscale attention model. ||| yung-han huang ||| kuang-jui hsu ||| shyh-kang jeng ||| yen-yu lin ||| 
2020 ||| learning long- and short-term user literal-preference with multimodal hierarchical transformer network for personalized image caption. ||| wei zhang ||| yue ying ||| pan lu ||| hongyuan zha ||| 
2019 ||| a dual attention network with semantic embedding for few-shot learning. ||| shipeng yan ||| songyang zhang ||| xuming he ||| 
2020 ||| multi-view deep attention network for reinforcement learning (student abstract). ||| yueyue hu ||| shiliang sun ||| xin xu ||| jing zhao ||| 
2021 ||| an efficient transformer decoder with compressed sub-layers. ||| yanyang li ||| ye lin ||| tong xiao ||| jingbo zhu ||| 
2020 ||| salsac: a video saliency prediction model with shuffled attentions and correlation-based convlstm. ||| xinyi wu ||| zhenyao wu ||| jinglin zhang ||| lili ju ||| song wang ||| 
2019 ||| cross-relation cross-bag attention for distantly-supervised relation extraction. ||| yujin yuan ||| liyuan liu ||| siliang tang ||| zhongfei zhang ||| yueting zhuang ||| shiliang pu ||| fei wu ||| xiang ren ||| 
2017 ||| coupled multi-layer attentions for co-extraction of aspect and opinion terms. ||| wenya wang ||| sinno jialin pan ||| daniel dahlmeier ||| xiaokui xiao ||| 
2018 ||| modeling attention and memory for auditory selection in a cocktail party environment. ||| jiaming xu ||| jing shi ||| guangcan liu ||| xiuyi chen ||| bo xu ||| 
2018 ||| facial landmarks detection by self-iterative regression based landmarks-attention network. ||| tao hu ||| honggang qi ||| jizheng xu ||| qingming huang ||| 
2017 ||| deterministic attention for sequence-to-sequence constituent parsing. ||| chunpeng ma ||| lemao liu ||| akihiro tamura ||| tiejun zhao ||| eiichiro sumita ||| 
2017 ||| attention correctness in neural image captioning. ||| chenxi liu ||| junhua mao ||| fei sha ||| alan l. yuille ||| 
2017 ||| text-guided attention model for image captioning. ||| jonghwan mun ||| minsu cho ||| bohyung han ||| 
2019 ||| cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. ||| binbin hu ||| zhiqiang zhang ||| chuan shi ||| jun zhou ||| xiaolong li ||| yuan qi ||| 
2018 ||| a question-focused multi-factor attention network for question answering. ||| souvik kundu ||| hwee tou ng ||| 
2019 ||| beyond rnns: positional self-attention with co-attention for video question answering. ||| xiangpeng li ||| jingkuan song ||| lianli gao ||| xianglong liu ||| wenbing huang ||| xiangnan he ||| chuang gan ||| 
2021 ||| tackling the infodemic: analysis using transformer based models. ||| anand zutshi ||| aman raj ||| 
2018 ||| improving neural fine-grained entity typing with knowledge attention. ||| ji xin ||| yankai lin ||| zhiyuan liu ||| maosong sun ||| 
2018 ||| "attention" for detecting unreliable news in the information age. ||| venkatesh duppada ||| 
2020 ||| crowd-assisted disaster scene assessment with human-ai interactive attention. ||| daniel yue zhang ||| yifeng huang ||| yang zhang ||| dong wang ||| 
2021 ||| dast: unsupervised domain adaptation in semantic segmentation based on discriminator attention and self-training. ||| fei yu ||| mo zhang ||| hexin dong ||| sheng hu ||| bin dong ||| li zhang ||| 
2019 ||| wais: word attention for joint intent detection and slot filling. ||| sixuan chen ||| shuai yu ||| 
2021 ||| efficient folded attention for medical image reconstruction and segmentation. ||| hang zhang ||| jinwei zhang ||| rongguang wang ||| qihao zhang ||| pascal spincemaille ||| thanh d. nguyen ||| yi wang ||| 
2019 ||| self-ensembling attention networks: addressing domain shift for semantic segmentation. ||| yonghao xu ||| bo du ||| lefei zhang ||| qian zhang ||| guoli wang ||| liangpei zhang ||| 
2021 ||| explicitly modeled attention maps for image classification. ||| andong tan ||| duc tam nguyen ||| maximilian dax ||| matthias nie ||| ner ||| thomas brox ||| 
2021 ||| self-supervised attention-aware reinforcement learning. ||| haiping wu ||| khimya khetarpal ||| doina precup ||| 
2017 ||| attention based lstm for target dependent sentiment classification. ||| min yang ||| wenting tu ||| jingxuan wang ||| fei xu ||| xiaojun chen ||| 
2018 ||| gated-attention architectures for task-oriented language grounding. ||| devendra singh chaplot ||| kanthashree mysore sathyendra ||| rama kumar pasumarthi ||| dheeraj rajagopal ||| ruslan salakhutdinov ||| 
2019 ||| skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module. ||| chenyang li ||| xin zhang ||| lufan liao ||| lianwen jin ||| weixin yang ||| 
2020 ||| attention-guide walk model in heterogeneous information network for multi-style recommendation explanation. ||| xin wang ||| ying wang ||| yunzhi ling ||| 
2018 ||| a neural attention model for urban air quality inference: learning the weights of monitoring stations. ||| weiyu cheng ||| yanyan shen ||| yanmin zhu ||| linpeng huang ||| 
2021 ||| a hybrid attention mechanism for weakly-supervised temporal action localization. ||| ashraful islam ||| chengjiang long ||| richard j. radke ||| 
2018 ||| atrank: an attention-based user behavior modeling framework for recommendation. ||| chang zhou ||| jinze bai ||| junshuai song ||| xiaofei liu ||| zhengchao zhao ||| xiusi chen ||| jun gao ||| 
2020 ||| sentence generation for entity description with content-plan attention. ||| bayu distiawan trisedya ||| jianzhong qi ||| rui zhang ||| 
2019 ||| semantic adversarial network with multi-scale pyramid attention for video classification. ||| de xie ||| cheng deng ||| hao wang ||| chao li ||| dapeng tao ||| 
2021 ||| show, attend and distill: knowledge distillation via attention-based feature matching. ||| mingi ji ||| byeongho heo ||| sungrae park ||| 
2020 ||| generative attention networks for multi-agent behavioral modeling. ||| max guangyu li ||| bo jiang ||| hao zhu ||| zhengping che ||| yan liu ||| 
2020 ||| gret: global representation enhanced transformer. ||| rongxiang weng ||| hao-ran wei ||| shujian huang ||| heng yu ||| lidong bing ||| weihua luo ||| jiajun chen ||| 
2019 ||| afs: an attention-based mechanism for supervised feature selection. ||| ning gui ||| danni ge ||| ziyin hu ||| 
2019 ||| hybrid attention-based prototypical networks for noisy few-shot relation classification. ||| tianyu gao ||| xu han ||| zhiyuan liu ||| maosong sun ||| 
2019 ||| neural speech synthesis with transformer network. ||| naihan li ||| shujie liu ||| yanqing liu ||| sheng zhao ||| ming liu ||| 
2018 ||| lateral inhibition-inspired convolutional neural network for visual attention and saliency detection. ||| chunshui cao ||| yongzhen huang ||| zilei wang ||| liang wang ||| ninglong xu ||| tieniu tan ||| 
2018 ||| mention and entity description co-attention for entity disambiguation. ||| feng nie ||| yunbo cao ||| jinpeng wang ||| chin-yew lin ||| rong pan ||| 
2021 ||| mango: a mask attention guided one-stage scene text spotter. ||| liang qiao ||| ying chen ||| zhanzhan cheng ||| yunlu xu ||| yi niu ||| shiliang pu ||| fei wu ||| 
2019 ||| motiontransformer: transferring neural inertial tracking between domains. ||| changhao chen ||| yishu miao ||| chris xiaoxuan lu ||| linhai xie ||| phil blunsom ||| andrew markham ||| niki trigoni ||| 
2020 ||| fine-grained machine teaching with attention modeling. ||| jiacheng liu ||| xiaofeng hou ||| feilong tang ||| 
2018 ||| disan: directional self-attention network for rnn/cnn-free language understanding. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| shirui pan ||| chengqi zhang ||| 
2021 ||| alp-kd: attention-based layer projection for knowledge distillation. ||| peyman passban ||| yimeng wu ||| mehdi rezagholizadeh ||| qun liu ||| 
2021 ||| modular graph transformer networks for multi-label image classification. ||| hoang d. nguyen ||| xuan-son vu ||| duc-trong le ||| 
2018 ||| rnn-based sequence-preserved attention for dependency parsing. ||| yi zhou ||| junying zhou ||| lu liu ||| jiangtao feng ||| haoyuan peng ||| xiaoqing zheng ||| 
2020 ||| neural simile recognition with cyclic multitask learning and local attention. ||| jiali zeng ||| linfeng song ||| jinsong su ||| jun xie ||| wei song ||| jiebo luo ||| 
2020 ||| attention-over-attention field-aware factorization machine. ||| zhibo wang ||| jinxin ma ||| yongquan zhang ||| qian wang ||| ju ren ||| peng sun ||| 
2017 ||| distant supervision for relation extraction with sentence-level attention and entity descriptions. ||| guoliang ji ||| kang liu ||| shizhu he ||| jun zhao ||| 
2020 ||| compressed self-attention for deep metric learning. ||| ziye chen ||| mingming gong ||| yanwu xu ||| chaohui wang ||| kun zhang ||| bo du ||| 
2017 ||| coherent dialogue with attention-based language models. ||| hongyuan mei ||| mohit bansal ||| matthew r. walter ||| 
2021 ||| continuous self-attention models with neural ode networks. ||| jing zhang ||| peng zhang ||| baiwen kong ||| junqiu wei ||| xin jiang ||| 
2019 ||| motion guided spatial attention for video captioning. ||| shaoxiang chen ||| yu-gang jiang ||| 
2019 ||| fully convolutional video captioning with coarse-to-fine and inherited attention. ||| kuncheng fang ||| lian zhou ||| cheng jin ||| yuejie zhang ||| kangnian weng ||| tao zhang ||| weiguo fan ||| 
2020 ||| multi-scale self-attention for text classification. ||| qipeng guo ||| xipeng qiu ||| pengfei liu ||| xiangyang xue ||| zheng zhang ||| 
2020 ||| tapnet: multivariate time series classification with attentional prototypical network. ||| xuchao zhang ||| yifeng gao ||| jessica lin ||| chang-tien lu ||| 
2018 ||| hierarchical recurrent attention network for response generation. ||| chen xing ||| yu wu ||| wei wu ||| yalou huang ||| ming zhou ||| 
2021 ||| knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. ||| lianghao xia ||| chao huang ||| yong xu ||| peng dai ||| xiyue zhang ||| hongsheng yang ||| jian pei ||| liefeng bo ||| 
2021 ||| confidence-aware non-repetitive multimodal transformers for textcaps. ||| zhaokai wang ||| renda bao ||| qi wu ||| si liu ||| 
2021 ||| graph-based tri-attention network for answer ranking in cqa. ||| wei zhang ||| zeyuan chen ||| chao dong ||| wen wang ||| hongyuan zha ||| jianyong wang ||| 
2021 ||| split then refine: stacked attention-guided resunets for blind single image visible watermark removal. ||| xiaodong cun ||| chi-man pun ||| 
2019 ||| hierarchical attention network for image captioning. ||| weixuan wang ||| zhihong chen ||| haifeng hu ||| 
2019 ||| a two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels. ||| shaobo min ||| xuejin chen ||| zheng-jun zha ||| feng wu ||| yongdong zhang ||| 
2019 ||| deep short text classification with knowledge powered attention. ||| jindong chen ||| yizhou hu ||| jingping liu ||| yanghua xiao ||| haiyun jiang ||| 
2020 ||| a new dataset and boundary-attention semantic segmentation for face parsing. ||| yinglu liu ||| hailin shi ||| hao shen ||| yue si ||| xiaobo wang ||| tao mei ||| 
2018 ||| event detection via gated multilingual attention mechanism. ||| jian liu ||| yubo chen ||| kang liu ||| jun zhao ||| 
2020 ||| self-attention convlstm for spatiotemporal prediction. ||| zhihui lin ||| maomao li ||| zhuobin zheng ||| yangyang cheng ||| chun yuan ||| 
2021 ||| gradient-based localization and spatial attention for confidence measure in fine-grained recognition using deep neural networks. ||| charles a. kantor ||| l ||| onard boussioux ||| brice rauby ||| hugues talbot ||| 
2019 ||| convolutional spatial attention model for reading comprehension with multiple-choice questions. ||| zhipeng chen ||| yiming cui ||| wentao ma ||| shijin wang ||| guoping hu ||| 
2019 ||| character-level language modeling with deeper self-attention. ||| rami al-rfou ||| dokook choe ||| noah constant ||| mandy guo ||| llion jones ||| 
2021 ||| dynamic multi-context attention networks for citation forecasting of scientific publications. ||| taoran ji ||| nathan self ||| kaiqun fu ||| zhiqian chen ||| naren ramakrishnan ||| chang-tien lu ||| 
2020 ||| an attention-based graph neural network for heterogeneous structural learning. ||| huiting hong ||| hantao guo ||| yucheng lin ||| xiaoqing yang ||| zang li ||| jieping ye ||| 
2021 ||| stock selection via spatiotemporal hypergraph attention network: a learning to rank approach. ||| ramit sawhney ||| shivam agarwal ||| arnav wadhwa ||| tyler derr ||| rajiv ratn shah ||| 
2021 ||| patch-wise attention network for monocular depth estimation. ||| sihaeng lee ||| janghyeon lee ||| byungju kim ||| eojindl yi ||| junmo kim ||| 
2021 ||| rarebert: transformer architecture for rare disease patient identification using administrative claims. ||| p. k. s. prakash ||| srinivas chilukuri ||| nikhil ranade ||| shankar viswanathan ||| 
2018 ||| sentiment lexicon enhanced attention-based lstm for sentiment classification. ||| zeyang lei ||| yujiu yang ||| min yang ||| 
2019 ||| to find where you talk: temporal sentence localization in video with attention based location regression. ||| yitian yuan ||| tao mei ||| wenwu zhu ||| 
2020 ||| robutrans: a robust transformer-based text-to-speech model. ||| naihan li ||| yanqing liu ||| yu wu ||| shujie liu ||| sheng zhao ||| ming liu ||| 
2020 ||| schema-guided multi-domain dialogue state tracking with graph attention neural networks. ||| lu chen ||| boer lv ||| chi wang ||| su zhu ||| bowen tan ||| kai yu ||| 
2020 ||| relation-guided spatial attention and temporal refinement for video-based person re-identification. ||| xingze li ||| wengang zhou ||| yun zhou ||| houqiang li ||| 
2020 ||| syntactically look-ahead attention network for sentence compression. ||| hidetaka kamigaito ||| manabu okumura ||| 
2020 ||| multi-label patent categorization with non-local attention-based graph convolutional network. ||| pingjie tang ||| meng jiang ||| bryan (ning) xia ||| jed w. pitera ||| jeffrey welser ||| nitesh v. chawla ||| 
2017 ||| title learning latent subevents in activity videos using temporal attention filters. ||| a. j. piergiovanni ||| chenyou fan ||| michael s. ryoo ||| 
2020 ||| joint entity and relation extraction with a hybrid transformer and reinforcement learning based model. ||| ya xiao ||| chengxiang tan ||| zhijie fan ||| qian xu ||| wenye zhu ||| 
2021 ||| dynamic graph representation learning for video dialog via multi-modal shuffled transformers. ||| shijie geng ||| peng gao ||| moitreya chatterjee ||| chiori hori ||| jonathan le roux ||| yongfeng zhang ||| hongsheng li ||| anoop cherian ||| 
2021 ||| abusive language detection in heterogeneous contexts: dataset collection and the role of supervised attention. ||| hongyu gong ||| alberto valido ||| katherine m. ingram ||| giulia fanti ||| suma bhat ||| dorothy l. espelage ||| 
2019 ||| sam-net: integrating event-level and chain-level attentions to predict what happens next. ||| shangwen lv ||| wanhui qian ||| longtao huang ||| jizhong han ||| songlin hu ||| 
2020 ||| cawa: an attention-network for credit attribution. ||| saurav manchanda ||| george karypis ||| 
2019 ||| deliberate attention networks for image captioning. ||| lianli gao ||| kaixuan fan ||| jingkuan song ||| xianglong liu ||| xing xu ||| heng tao shen ||| 
2018 ||| syntax-directed attention for neural machine translation. ||| kehai chen ||| rui wang ||| masao utiyama ||| eiichiro sumita ||| tiejun zhao ||| 
2020 ||| dianet: dense-and-implicit attention network. ||| zhongzhan huang ||| senwei liang ||| mingfu liang ||| haizhao yang ||| 
2020 ||| data-gru: dual-attention time-aware gated recurrent unit for irregular multivariate time series. ||| qingxiong tan ||| mang ye ||| baoyao yang ||| siqi liu ||| andy jinhua ma ||| terry cheuk-fung yip ||| grace lai-hung wong ||| pong chi yuen ||| 
2021 ||| encoding syntactic knowledge in transformer encoder for intent detection and slot filling. ||| jixuan wang ||| kai wei ||| martin radfar ||| weiwei zhang ||| clement chung ||| 
2021 ||| attnmove: history enhanced trajectory recovery via attentional network. ||| tong xia ||| yunhan qi ||| jie feng ||| fengli xu ||| funing sun ||| diansheng guo ||| yong li ||| 
2021 ||| melodic phrase attention network for symbolic data-based music genre classification (student abstract). ||| li li ||| rui zhang ||| zhenyu wang ||| 
2019 ||| dan: deep attention neural network for news recommendation. ||| qiannan zhu ||| xiaofei zhou ||| zeliang song ||| jianlong tan ||| li guo ||| 
2020 ||| ma-dst: multi-attention-based scalable dialog state tracking. ||| adarsh kumar ||| peter ku ||| anuj kumar goyal ||| angeliki metallinou ||| dilek hakkani-t ||| r ||| 
2020 ||| bidirectional dilated lstm with attention for fine-grained emotion classification in tweets. ||| annika marie schoene ||| alexander p. turner ||| nina dethlefs ||| 
2020 ||| graph transformer for graph-to-sequence learning. ||| deng cai ||| wai lam ||| 
2018 ||| word attention for sequence to sequence text understanding. ||| lijun wu ||| fei tian ||| li zhao ||| jianhuang lai ||| tie-yan liu ||| 
2021 ||| attanet: attention-augmented network for fast and accurate scene parsing. ||| qi song ||| kangfu mei ||| rui huang ||| 
2020 ||| age progression and regression with spatial attention modules. ||| qi li ||| yunfan liu ||| zhenan sun ||| 
2020 ||| multiple positional self-attention network for text classification. ||| biyun dai ||| jinlong li ||| ruoyi xu ||| 
2021 ||| global fusion attention for vision and language understanding (student abstract). ||| zixin guo ||| chen liang ||| ziyu wan ||| yang bai ||| 
2019 ||| exploring answer stance detection with recurrent conditional attention. ||| jianhua yuan ||| yanyan zhao ||| jingfang xu ||| bing qin ||| 
2020 ||| cross-modal attention network for temporal inconsistent audio-visual event localization. ||| hanyu xuan ||| zhenyu zhang ||| shuo chen ||| jian yang ||| yan yan ||| 
2021 ||| understood in translation: transformers for domain understanding. ||| dimitrios christofidellis ||| matteo manica ||| leonidas georgopoulos ||| hans vandierendonck ||| 
2021 ||| tdaf: top-down attention framework for vision tasks. ||| bo pang ||| yizhuo li ||| jiefeng li ||| muchen li ||| hanwen cao ||| cewu lu ||| 
2020 ||| loss-based attention for deep multiple instance learning. ||| xiaoshuang shi ||| fuyong xing ||| yuanpu xie ||| zizhao zhang ||| lei cui ||| lin yang ||| 
2020 ||| sequential recommendation with relation-aware kernelized self-attention. ||| mingi ji ||| weonyoung joo ||| kyungwoo song ||| yoon-yeong kim ||| il-chul moon ||| 
2018 ||| multi-attention recurrent network for human communication comprehension. ||| amir zadeh ||| paul pu liang ||| soujanya poria ||| prateek vij ||| erik cambria ||| louis-philippe morency ||| 
2020 ||| tanet: robust 3d object detection from point clouds with triple attention. ||| zhe liu ||| xin zhao ||| tengteng huang ||| ruolan hu ||| yu zhou ||| xiang bai ||| 
2020 ||| generating diverse translation by manipulating multi-head attention. ||| zewei sun ||| shujian huang ||| hao-ran wei ||| xinyu dai ||| jiajun chen ||| 
2020 ||| multi-type self-attention guided degraded saliency detection. ||| ziqi zhou ||| zheng wang ||| huchuan lu ||| song wang ||| meijun sun ||| 
2020 ||| pyramid constrained self-attention network for fast video salient object detection. ||| yuchao gu ||| lijuan wang ||| ziqin wang ||| yun liu ||| ming-ming cheng ||| shao-ping lu ||| 
2021 ||| knowledge-aware dialogue generation with hybrid attention (student abstract). ||| yaru zhao ||| bo cheng ||| yingying zhang ||| 
2021 ||| future-guided incremental transformer for simultaneous translation. ||| shaolei zhang ||| yang feng ||| liangyou li ||| 
2020 ||| multi-agent actor-critic with hierarchical graph attention network. ||| heechang ryu ||| hayong shin ||| jinkyoo park ||| 
2021 ||| named entity recognition from synthesis procedural text in materials science domain with attention-based approach. ||| huichen yang ||| william h. hsu ||| 
2020 ||| atloc: attention guided camera localization. ||| bing wang ||| changhao chen ||| chris xiaoxuan lu ||| peijun zhao ||| niki trigoni ||| andrew markham ||| 
2021 ||| contrastive triple extraction with generative transformer. ||| hongbin ye ||| ningyu zhang ||| shumin deng ||| mosha chen ||| chuanqi tan ||| fei huang ||| huajun chen ||| 
2021 ||| two-stream convolution augmented transformer for human activity recognition. ||| bing li ||| wei cui ||| wei wang ||| le zhang ||| zhenghua chen ||| min wu ||| 
2021 ||| object relation attention for image paragraph captioning. ||| li-chuan yang ||| chih-yuan yang ||| jane yung-jen hsu ||| 
2019 ||| attention based spatial-temporal graph convolutional networks for traffic flow forecasting. ||| shengnan guo ||| youfang lin ||| ning feng ||| chao song ||| huaiyu wan ||| 
2018 ||| recurrent attentional reinforcement learning for multi-label image recognition. ||| tianshui chen ||| zhouxia wang ||| guanbin li ||| liang lin ||| 
2021 ||| attributes-guided and pure-visual attention alignment for few-shot recognition. ||| siteng huang ||| min zhang ||| yachen kang ||| donglin wang ||| 
2020 ||| domain adaptive attention learning for unsupervised person re-identification. ||| yangru huang ||| peixi peng ||| yi jin ||| yidong li ||| junliang xing ||| 
2019 ||| attention guided imitation learning and reinforcement learning. ||| ruohan zhang ||| 
2021 ||| informer: beyond efficient transformer for long sequence time-series forecasting. ||| haoyi zhou ||| shanghang zhang ||| jieqi peng ||| shuai zhang ||| jianxin li ||| hui xiong ||| wancai zhang ||| 
2021 ||| relation-aware graph attention model with adaptive self-adversarial training. ||| xiao qin ||| nasrullah sheikh ||| berthold reinwald ||| lingfei wu ||| 
2020 ||| not all attention is needed: gated attention network for sequence data. ||| lanqing xue ||| xiaopeng li ||| nevin l. zhang ||| 
2019 ||| tied transformers: neural machine translation with shared encoder and decoder. ||| yingce xia ||| tianyu he ||| xu tan ||| fei tian ||| di he ||| tao qin ||| 
2020 ||| re-attention for visual question answering. ||| wenya guo ||| ying zhang ||| xiaoping wu ||| jufeng yang ||| xiangrui cai ||| xiaojie yuan ||| 
2018 ||| multimodal keyless attention fusion for video classification. ||| xiang long ||| chuang gan ||| gerard de melo ||| xiao liu ||| yandong li ||| fu li ||| shilei wen ||| 
2021 ||| a supervised multi-head self-attention network for nested named entity recognition. ||| yongxiu xu ||| heyan huang ||| chong feng ||| yue hu ||| 
2020 ||| tanda: transfer and adapt pre-trained transformer models for answer sentence selection. ||| siddhant garg ||| thuy vu ||| alessandro moschitti ||| 
2021 ||| audio-oriented multimodal machine comprehension via dynamic inter- and intra-modality attention. ||| zhiqi huang ||| fenglin liu ||| xian wu ||| shen ge ||| helin wang ||| wei fan ||| yuexian zou ||| 
2021 ||| exploring text-transformers in aaai 2021 shared task: covid-19 fake news detection in english. ||| xiangyang li ||| yu xia ||| xiang long ||| zheng li ||| sujian li ||| 
2021 ||| gate: graph attention transformer encoder for cross-lingual relation and event extraction. ||| wasi uddin ahmad ||| nanyun peng ||| kai-wei chang ||| 
2021 ||| uag: uncertainty-aware attention graph neural network for defending adversarial attacks. ||| boyuan feng ||| yuke wang ||| yufei ding ||| 
2017 ||| recurrent attentional topic model. ||| shuangyin li ||| yu zhang ||| rong pan ||| mingzhi mao ||| yang yang ||| 
2020 ||| transformer-capsule model for intent detection (student abstract). ||| aleksander obuchowski ||| michal lew ||| 
2021 ||| m-based algorithm for approximating self-attention. ||| yunyang xiong ||| zhanpeng zeng ||| rudrasis chakraborty ||| mingxing tan ||| glenn fung ||| yin li ||| vikas singh ||| 
2021 ||| hargan: heterogeneous argument attention network for persuasiveness prediction. ||| kuo yu huang ||| hen-hsen huang ||| hsin-hsi chen ||| 
2021 ||| let: linguistic knowledge enhanced graph transformer for chinese short text matching. ||| boer lyu ||| lu chen ||| su zhu ||| kai yu ||| 
2021 ||| implicit kernel attention. ||| kyungwoo song ||| yohan jung ||| dongjun kim ||| il-chul moon ||| 
2020 ||| attention-based multi-modal fusion network for semantic scene completion. ||| siqi li ||| changqing zou ||| yipeng li ||| xibin zhao ||| yue gao ||| 
2021 ||| segatron: segment-aware transformer for language modeling and understanding. ||| he bai ||| peng shi ||| jimmy lin ||| yuqing xie ||| luchen tan ||| kun xiong ||| wen gao ||| ming li ||| 
2019 ||| difficulty-aware attention network with confidence learning for medical image segmentation. ||| dong nie ||| li wang ||| lei xiang ||| sihang zhou ||| ehsan adeli ||| dinggang shen ||| 
2021 ||| salnet: semi-supervised few-shot text classification with attention-based lexicon construction. ||| ju hyoung lee ||| sang-ki ko ||| yo-sub han ||| 
2021 ||| co-gat: a co-interactive graph attention network for joint dialog act recognition and sentiment classification. ||| libo qin ||| zhouyang li ||| wanxiang che ||| minheng ni ||| ting liu ||| 
2021 ||| systems at sdu-2021 task 1: transformers for sentence level sequence label. ||| feng li ||| zhensheng mai ||| wuhe zou ||| wenjie ou ||| xiaolei qin ||| yue lin ||| weidong zhang ||| 
2019 ||| deep metric learning by online soft mining and class-aware attention. ||| xinshao wang ||| yang hua ||| elyor kodirov ||| guosheng hu ||| neil martin robertson ||| 
2021 ||| regional attention with architecture-rebuilt 3d network for rgb-d gesture recognition. ||| benjia zhou ||| yunan li ||| jun wan ||| 
2021 ||| the heads hypothesis: a unifying statistical approach towards understanding multi-headed attention in bert. ||| madhura pande ||| aakriti budhraja ||| preksha nema ||| pratyush kumar ||| mitesh m. khapra ||| 
2020 ||| high tissue contrast mri synthesis using multi-stage attention-gan for segmentation. ||| mohammad hamghalam ||| baiying lei ||| tianfu wang ||| 
2021 ||| attention-based multi-level fusion network for light field depth estimation. ||| jiaxin chen ||| shuo zhang ||| youfang lin ||| 
2020 ||| filtration and distillation: enhancing region attention for fine-grained visual categorization. ||| chuanbin liu ||| hongtao xie ||| zheng-jun zha ||| lingfeng ma ||| lingyun yu ||| yongdong zhang ||| 
2021 ||| context-aware attentional pooling (cap) for fine-grained visual classification. ||| ardhendu behera ||| zachary wharton ||| pradeep r. p. g. hewage ||| asish bera ||| 
2018 ||| dual attention network for product compatibility and function satisfiability analysis. ||| hu xu ||| sihong xie ||| lei shu ||| philip s. yu ||| 
2019 ||| vistanet: visual aspect attention network for multimodal sentiment analysis. ||| quoc-tuan truong ||| hady w. lauw ||| 
2021 ||| tune-in: training under negative environments with interference for attention networks simulating cocktail party effect. ||| jun wang ||| max w. y. lam ||| dan su ||| dong yu ||| 
2020 ||| can eruptions be predicted? short-term prediction of volcanic eruptions via attention-based long short-term memory. ||| hiep v. le ||| tsuyoshi murata ||| masato iguchi ||| 
2021 ||| dynamic memory based attention network for sequential recommendation. ||| qiaoyu tan ||| jianwei zhang ||| ninghao liu ||| xiao huang ||| hongxia yang ||| jingren zhou ||| xia hu ||| 
2021 ||| multi-document transformer for personality detection. ||| feifan yang ||| xiaojun quan ||| yunyi yang ||| jianxing yu ||| 
2017 ||| image caption with global-local attention. ||| linghui li ||| sheng tang ||| lixi deng ||| yongdong zhang ||| qi tian ||| 
2020 ||| path ranking with attention to type hierarchies. ||| weiyu liu ||| angel andres daruna ||| zsolt kira ||| sonia chernova ||| 
2021 ||| revisiting mahalanobis distance for transformer-based out-of-domain detection. ||| alexander podolskiy ||| dmitry lipin ||| andrey bout ||| ekaterina artemova ||| irina piontkovskaya ||| 
2021 ||| over-map: structural attention mechanism and automated semantic segmentation ensembled for uncertainty prediction. ||| charles a. kantor ||| l ||| onard boussioux ||| brice rauby ||| hugues talbot ||| 
2020 ||| pose-guided multi-granularity attention network for text-based person search. ||| ya jing ||| chenyang si ||| junbo wang ||| wei wang ||| liang wang ||| tieniu tan ||| 
2018 ||| hierarchical attention transfer network for cross-domain sentiment classification. ||| zheng li ||| ying wei ||| yu zhang ||| qiang yang ||| 
2020 ||| learning the graphical structure of electronic health records with graph convolutional transformer. ||| edward choi ||| zhen xu ||| yujia li ||| michael dusenberry ||| gerardo flores ||| emily xue ||| andrew m. dai ||| 
2020 ||| adversarial cross-domain action recognition with co-attention. ||| boxiao pan ||| zhangjie cao ||| ehsan adeli ||| juan carlos niebles ||| 
2021 ||| classification by attention: scene graph classification with prior knowledge. ||| sahand sharifzadeh ||| sina moayed baharlou ||| volker tresp ||| 
2017 ||| localizing by describing: attribute-guided attention localization for fine-grained recognition. ||| xiao liu ||| jiang wang ||| shilei wen ||| errui ding ||| yuanqing lin ||| 
2021 ||| on scalar embedding of relative positions in attention models. ||| junshuang wu ||| richong zhang ||| yongyi mao ||| junfan chen ||| 
2021 ||| regularizing attention networks for anomaly detection in visual question answering. ||| doyup lee ||| yeongjae cheon ||| wook-shin han ||| 
2020 ||| who did they respond to? conversation structure modeling using masked hierarchical transformer. ||| henghui zhu ||| feng nan ||| zhiguo wang ||| ramesh nallapati ||| bing xiang ||| 
2020 ||| hierarchical attention network with pairwise loss for chinese zero pronoun resolution. ||| peiqin lin ||| meng yang ||| 
2019 ||| dynamic capsule attention for visual question answering. ||| yiyi zhou ||| rongrong ji ||| jinsong su ||| xiaoshuai sun ||| weiqiu chen ||| 
2020 ||| real-time emotion recognition via attention gated hierarchical memory network. ||| wenxiang jiao ||| michael r. lyu ||| irwin king ||| 
2020 ||| guiding attention in sequence-to-sequence models for dialogue act prediction. ||| pierre colombo ||| emile chapuis ||| matteo manica ||| emmanuel vignon ||| giovanna varni ||| chlo |||  clavel ||| 
2019 ||| interactive attention transfer network for cross-domain sentiment classification. ||| kai zhang ||| hefu zhang ||| qi liu ||| hongke zhao ||| hengshu zhu ||| enhong chen ||| 
2021 ||| dynamic modeling cross- and self-lattice attention network for chinese ner. ||| shan zhao ||| minghao hu ||| zhiping cai ||| haiwen chen ||| fang liu ||| 
2019 ||| hirenet: a hierarchical attention model for the automatic analysis of asynchronous video job interviews. ||| l ||| o hemamou ||| ghazi felhi ||| vincent vandenbussche ||| jean-claude martin ||| chlo |||  clavel ||| 
2021 ||| humor knowledge enriched transformer for understanding multimodal humor. ||| md. kamrul hasan ||| sangwu lee ||| wasifur rahman ||| amir zadeh ||| rada mihalcea ||| louis-philippe morency ||| ehsan hoque ||| 
2021 ||| lightxml: transformer with dynamic negative sampling for high-performance extreme multi-label text classification. ||| ting jiang ||| deqing wang ||| leilei sun ||| huayi yang ||| zhengyang zhao ||| fuzhen zhuang ||| 
2020 ||| shallow feature based dense attention network for crowd counting. ||| yunqi miao ||| zijia lin ||| guiguang ding ||| jungong han ||| 
2020 ||| understanding medical conversations with scattered keyword attention and weak supervision from responses. ||| xiaoming shi ||| haifeng hu ||| w ||| anxiang che ||| zhongqian sun ||| ting liu ||| junzhou huang ||| 
2020 ||| gman: a graph multi-attention network for traffic prediction. ||| chuanpan zheng ||| xiaoliang fan ||| cheng wang ||| jianzhong qi ||| 
2021 ||| dual sparse attention network for session-based recommendation. ||| jiahao yuan ||| zihan song ||| mingyou sun ||| xiaoling wang ||| wayne xin zhao ||| 
2018 ||| deep semantic role labeling with self-attention. ||| zhixing tan ||| mingxuan wang ||| jun xie ||| yidong chen ||| xiaodong shi ||| 
2020 ||| symbiotic attention with privileged information for egocentric action recognition. ||| xiaohan wang ||| yu wu ||| linchao zhu ||| yi yang ||| 
2021 ||| faster depth-adaptive transformers. ||| yijin liu ||| fandong meng ||| jie zhou ||| yufeng chen ||| jinan xu ||| 
2020 ||| natural image matting via guided contextual attention. ||| yaoyi li ||| hongtao lu ||| 
2021 ||| hot-vae: learning high-order label correlation for multi-label classification via attention-based variational autoencoders. ||| wenting zhao ||| shufeng kong ||| junwen bai ||| daniel fink ||| carla p. gomes ||| 
2021 ||| paragraph-level commonsense transformers with recurrent memory. ||| saadia gabriel ||| chandra bhagavatula ||| vered shwartz ||| ronan le bras ||| maxwell forbes ||| yejin choi ||| 
2018 ||| learning attention model from human for visuomotor tasks. ||| luxin zhang ||| ruohan zhang ||| zhuode liu ||| mary m. hayhoe ||| dana h. ballard ||| 
2021 ||| gta: graph truncated attention for retrosynthesis. ||| seung-woo seo ||| you young song ||| june yong yang ||| seohui bae ||| hankook lee ||| jinwoo shin ||| sung ju hwang ||| eunho yang ||| 
2020 ||| self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction. ||| yang li ||| guodong long ||| tao shen ||| tianyi zhou ||| lina yao ||| huan huo ||| jing jiang ||| 
2020 ||| spatio-temporal attention-based neural network for credit card fraud detection. ||| dawei cheng ||| sheng xiang ||| chencheng shang ||| yiyi zhang ||| fangzhou yang ||| liqing zhang ||| 
2019 ||| multi-task learning with multi-view attention for answer selection and knowledge base question answering. ||| yang deng ||| yuexiang xie ||| yaliang li ||| min yang ||| nan du ||| wei fan ||| kai lei ||| ying shen ||| 
2020 ||| fact: fused attention for clothing transfer with generative adversarial networks. ||| yicheng zhang ||| lei li ||| li song ||| rong xie ||| wenjun zhang ||| 
2021 ||| structured co-reference graph attention for video-grounded dialogue. ||| junyeong kim ||| sunjae yoon ||| dahyun kim ||| chang d. yoo ||| 
2018 ||| order-free rnn with visual attention for multi-label classification. ||| shang-fu chen ||| yi-chen chen ||| chih-kuan yeh ||| yu-chiang frank wang ||| 
2021 ||| efficient license plate recognition via holistic position attention. ||| yesheng zhang ||| zilei wang ||| jiafan zhuang ||| 
2019 ||| learning a key-value memory co-attention matching network for person re-identification. ||| yaqing zhang ||| xi li ||| zhongfei zhang ||| 
2020 ||| cross-modality attention with semantic graph embedding for multi-label classification. ||| renchun you ||| zhiyao guo ||| lei cui ||| xiang long ||| yingze bao ||| shilei wen ||| 
2020 ||| an end-to-end visual-audio attention network for emotion recognition in user-generated videos. ||| sicheng zhao ||| yunsheng ma ||| yang gu ||| jufeng yang ||| tengfei xing ||| pengfei xu ||| runbo hu ||| hua chai ||| kurt keutzer ||| 
2020 ||| motif-matching based subgraph-level attentional convolutional network for graph classification. ||| hao peng ||| jianxin li ||| qiran gong ||| yuanxing ning ||| senzhang wang ||| lifang he ||| 
2021 ||| effective ensembling of transformer based language models for acronyms identification. ||| divesh r. kubal ||| apurva nagvenkar ||| 
2020 ||| learning signed network embedding via graph attention. ||| yu li ||| yuan tian ||| jiawei zhang ||| yi chang ||| 
2020 ||| multi-agent game abstraction via graph attention neural network. ||| yong liu ||| weixun wang ||| yujing hu ||| jianye hao ||| xingguo chen ||| yang gao ||| 
2021 ||| task adaptive pretraining of transformers for hostility detection. ||| tathagata raha ||| sayar ghosh roy ||| ujwal narayan ||| zubair abid ||| vasudeva varma ||| 
2020 ||| treegen: a tree-based transformer architecture for code generation. ||| zeyu sun ||| qihao zhu ||| yingfei xiong ||| yican sun ||| lili mou ||| lu zhang ||| 
2020 ||| multi-level head-wise match and aggregation in transformer for textual sequence matching. ||| shuohang wang ||| yunshi lan ||| yi tay ||| jing jiang ||| jingjing liu ||| 
2018 ||| attention-based transactional context embedding for next-item recommendation. ||| shoujin wang ||| liang hu ||| longbing cao ||| xiaoshui huang ||| defu lian ||| wei liu ||| 
2021 ||| noninvasive self-attention for side information fusion in sequential recommendation. ||| chang liu ||| xiaoguang li ||| guohao cai ||| zhenhua dong ||| hong zhu ||| lifeng shang ||| 
2020 ||| two-level transformer and auxiliary coherence modeling for improved text segmentation. ||| goran glavas ||| swapna somasundaran ||| 
2020 ||| channel attention is all you need for video frame interpolation. ||| myungsub choi ||| heewon kim ||| bohyung han ||| ning xu ||| kyoung mu lee ||| 
2019 ||| hierarchical attention networks for sentence ordering. ||| tianming wang ||| xiaojun wan ||| 
2020 ||| type-aware anchor link prediction across heterogeneous networks based on graph attention network. ||| xiaoxue li ||| yanmin shang ||| yanan cao ||| yangxi li ||| jianlong tan ||| yanbing liu ||| 
2021 ||| continuous-time attention for sequential learning. ||| jen-tzung chien ||| yi-hsiang chen ||| 
2019 ||| attention-based multi-context guiding for few-shot semantic segmentation. ||| tao hu ||| pengwan yang ||| chiliang zhang ||| gang yu ||| yadong mu ||| cees g. m. snoek ||| 
2020 ||| distraction-aware feature learning for human attribute recognition via coarse-to-fine attention mechanism. ||| mingda wu ||| di huang ||| yuanfang guo ||| yunhong wang ||| 
2018 ||| a cascaded inception of inception network with attention modulated feature fusion for human pose estimation. ||| wentao liu ||| jie chen ||| cheng li ||| chen qian ||| xiao chu ||| xiaolin hu ||| 
2019 ||| point2sequence: learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. ||| xinhai liu ||| zhizhong han ||| yu-shen liu ||| matthias zwicker ||| 
2020 ||| co-attention hierarchical network: generating coherent long distractors for reading comprehension. ||| xiaorui zhou ||| senlin luo ||| yunfang wu ||| 
2019 ||| an affect-rich neural conversational model with biased attention and weighted cross-entropy loss. ||| peixiang zhong ||| di wang ||| chunyan miao ||| 
2021 ||| self-attention attribution: interpreting information interactions inside transformer. ||| yaru hao ||| li dong ||| furu wei ||| ke xu ||| 
2021 ||| kan: knowledge-aware attention network for fake news detection. ||| yaqian dun ||| kefei tu ||| chen chen ||| chunyan hou ||| xiaojie yuan ||| 
2020 ||| attention-based view selection networks for light-field disparity estimation. ||| yu-ju tsai ||| yu-lun liu ||| ming ouhyoung ||| yung-yu chuang ||| 
2019 ||| gaussian transformer: a lightweight approach for natural language inference. ||| maosheng guo ||| yu zhang ||| ting liu ||| 
2020 ||| attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems. ||| zihan liu ||| genta indra winata ||| zhaojiang lin ||| peng xu ||| pascale fung ||| 
2017 ||| multi-focus attention network for efficient deep reinforcement learning. ||| jinyoung choi ||| beom-jin lee ||| byoung-tak zhang ||| 
2020 ||| aateam: achieving the ad hoc teamwork by employing the attention mechanism. ||| shuo chen ||| ewa andrejczuk ||| zhiguang cao ||| jie zhang ||| 
2022 ||| comparing vision transformers and convolutional nets for safety critical systems. ||| michal filipiuk ||| vasu singh ||| 
2020 ||| relational graph neural network with hierarchical attention for knowledge graph completion. ||| zhao zhang ||| fuzhen zhuang ||| hengshu zhu ||| zhi-ping shi ||| hui xiong ||| qing he ||| 
2020 ||| ffa-net: feature fusion attention network for single image dehazing. ||| xu qin ||| zhilin wang ||| yuanchao bai ||| xiaodong xie ||| huizhu jia ||| 
2020 ||| attention based data hiding with generative adversarial networks. ||| chong yu ||| 
2018 ||| chinese liwc lexicon expansion via hierarchical classification of word embeddings with sememe attention. ||| xiangkai zeng ||| cheng yang ||| cunchao tu ||| zhiyuan liu ||| maosong sun ||| 
2018 ||| attention-via-attention neural machine translation. ||| shenjian zhao ||| zhihua zhang ||| 
2018 ||| attend and diagnose: clinical time series analysis using attention models. ||| huan song ||| deepta rajan ||| jayaraman j. thiagarajan ||| andreas spanias ||| 
2020 ||| predicting students' attention level with interpretable facial and head dynamic features in an online tutoring system (student abstract). ||| shimeng peng ||| lujie chen ||| chufan gao ||| richard jiarui tong ||| 
2019 ||| attention-aware sampling via deep reinforcement learning for action recognition. ||| wenkai dong ||| zhaoxiang zhang ||| tieniu tan ||| 
2020 ||| divide and conquer: question-guided spatio-temporal contextual attention for video question answering. ||| jianwen jiang ||| ziqiang chen ||| haojie lin ||| xibin zhao ||| yue gao ||| 
2018 ||| neural knowledge acquisition via mutual attention between knowledge graph and text. ||| xu han ||| zhiyuan liu ||| maosong sun ||| 
2018 ||| an unsupervised model with attention autoencoders for question retrieval. ||| minghua zhang ||| yunfang wu ||| 
2021 ||| multi-decoder attention model with embedding glimpse for solving vehicle routing problems. ||| liang xin ||| wen song ||| zhiguang cao ||| jie zhang ||| 
2020 ||| explanation vs attention: a two-player game to obtain attention for vqa. ||| badri n. patro ||| anupriy ||| vinay namboodiri ||| 
2021 ||| compound word transformer: learning to compose full-song music over dynamic directed hypergraphs. ||| wen-yi hsiao ||| jen-yu liu ||| yin-cheng yeh ||| yi-hsuan yang ||| 
2020 ||| why attention? analyze bilstm deficiency and its remedies in the case of ner. ||| peng-hsuan li ||| tsu-jui fu ||| wei-yun ma ||| 
2020 ||| convolutional hierarchical attention network for query-focused video summarization. ||| shuwen xiao ||| zhou zhao ||| zijian zhang ||| xiaohui yan ||| min yang ||| 
2017 ||| an end-to-end spatio-temporal attention model for human action recognition from skeleton data. ||| sijie song ||| cuiling lan ||| junliang xing ||| wenjun zeng ||| jiaying liu ||| 
2020 ||| an attentional recurrent neural network for personalized next location recommendation. ||| qing guo ||| zhu sun ||| jie zhang ||| yin-leng theng ||| 
2018 ||| exploring human-like attention supervision in visual question answering. ||| tingting qiao ||| jianfeng dong ||| duanqing xu ||| 
2018 ||| improving review representations with user attention and product attention for sentiment classification. ||| zhen wu ||| xin-yu dai ||| cunyan yin ||| shujian huang ||| jiajun chen ||| 
2020 ||| alignment-enhanced transformer for constraining nmt with pre-specified translations. ||| kai song ||| kun wang ||| heng yu ||| yue zhang ||| zhongqiang huang ||| weihua luo ||| xiangyu duan ||| min zhang ||| 
2020 ||| message passing attention networks for document understanding. ||| giannis nikolentzos ||| antoine j.-p. tixier ||| michalis vazirgiannis ||| 
2020 ||| pyramid attention aggregation network for semantic segmentation of surgical instruments. ||| zhen-liang ni ||| gui-bin bian ||| guan'an wang ||| xiao-hu zhou ||| zeng-guang hou ||| hua-bin chen ||| xiao-liang xie ||| 
2021 ||| symbolic music generation with transformer-gans. ||| aashiq muhamed ||| liang li ||| xingjian shi ||| suri yaddanapudi ||| wayne chi ||| dylan jackson ||| rahul suresh ||| zachary c. lipton ||| alexander j. smola ||| 
2021 ||| transformer-based language model fine-tuning methods for covid-19 fake news detection. ||| ben chen ||| bin chen ||| dehong gao ||| qijin chen ||| chengfu huo ||| xiaonan meng ||| weijun ren ||| yang zhou ||| 
2021 ||| mau-net: multiple attention 3d u-net for lung cancer segmentation on ct images. ||| wei chen ||| fengchang yang ||| xianru zhang ||| xin xu ||| xu qiao ||| 
2021 ||| improving user attention to chatbots through a controlled intensity of changes within the interface. ||| kacper fornalczyk ||| kamil bortko ||| jaroslaw jankowski ||| 
2020 ||| towards expert gaze modeling and recognition of a user's attention in realtime. ||| nora castner ||| lea ge ||| ler ||| david geisler ||| fabian h ||| ttig ||| enkelejda kasneci ||| 
2021 ||| a single-run recognition of nested named entities with transformers. ||| michal marcinczuk ||| jarema radom ||| 
2020 ||| attracting user attention to visual elements within website with the use of fitts's law and flickering effect. ||| kamil bortko ||| jaroslaw jankowski ||| piotr bartk ||| w ||| patryk pazura ||| bozena smialkowska ||| 
2021 ||| attention-based network for effective action recognition from multi-view video. ||| hoang-thuyen nguyen ||| thi-oanh nguyen ||| 
2021 ||| pseudo-labeling with transformers for improving question answering systems. ||| karolina kuligowska ||| bartlomiej kowalczuk ||| 
2021 ||| semi-supervised anomaly detection in business process event data using self-attention based classification. ||| philippe krajsic ||| bogdan franczyk ||| 
2017 ||| mitigation of residual flux for high-temperature superconductor (hts) transformer by controlled switching of hts breaker arc model. ||| aasim ullah ||| tek tjing lie ||| kosala gunawardane ||| nirmal-kumar c. nair ||| 
2021 ||| modular high-frequency high-power transformers for offshore wind turbines. ||| weichong yao ||| tania parveen ||| junwei lu ||| andrew seagar ||| 
2021 ||| sizing transformer considering transformer thermal limits and wind farm wake effect. ||| zhongtian li ||| kateryna morozovska ||| patrik hilber ||| tor laneryd ||| stefan ivanell ||| 
2021 ||| assessment of effect of winding geometry on thermal performance of retrofilled transformers. ||| anupam dixit ||| muhammad daghrah ||| 
2021 ||| integration of solid-state transformer of off-shore wind turbine systems. ||| tania parveen ||| weichong yao ||| junwei lu ||| 
2021 ||| vibration profile comparison of grid connected and battery-grid connected transformers. ||| jakob pallot ||| vincent le ||| lakshitha naranpanawe ||| 
2021 ||| sfra based deterioration index for transformer condition monitoring. ||| sreeram v ||| rajkumar m ||| s. sudhakara reddy ||| t. gurudev ||| maroti m ||| 
2021 ||| study on down-sizing inverter transformers in solar farms. ||| xin zhong ||| 
2017 ||| an investigation into improving the measurement of the water content of transformer electrical insulation. ||| d. martin ||| t. saha ||| j. hockey ||| g. caldwell ||| g. buckley ||| s. chinnarajan ||| 
2017 ||| solid state transformer control aspects for various smart grid scenarios. ||| naga brahmendra yadav gorla ||| sandeep kolluri ||| sanjib kumar panda ||| 
2021 ||| transformer through fault protection - challenges and improvements in asset monitoring for precise predictive maintenance. ||| venkatesh rokkam ||| chakravarthy m ||| emmoji vundekari ||| 
2021 ||| study of unbalance reduction in 25kv ac traction system by different transformer configurations. ||| varsha singh ||| wataru ohnishi ||| takafumi koseki ||| 
2017 ||| solid state transformer parallel operation with a tap changing line frequency transformer. ||| nuwantha fernando ||| lasantha meegahapola ||| chathura thilakarathne ||| 
2021 ||| impact of battery energy storage system fed super grid transformer on distance protection. ||| eko prasetyo ||| peter crossley ||| 
2018 ||| attention patterns for code animations: using eye trackers to evaluate dynamic code presentation techniques. ||| louis spinelli ||| maulishree pandey ||| steve oney ||| 
2020 ||| aspect level sentiment classification with unbiased attention and target enhanced representations. ||| peng liu ||| tingwen liu ||| jinqiao shi ||| xuebin wang ||| zelin yin ||| can zhao ||| 
2021 ||| explaining a neural attention model for aspect-based sentiment classification using diagnostic classification. ||| lisa meijer ||| flavius frasincar ||| maria mihaela trusca ||| 
2020 ||| attention history-based attention for abstractive text summarization. ||| hyunsoo lee ||| yunseok choi ||| jee-hyong lee ||| 
2019 ||| aldona: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalised domain ontology and a neural attention model. ||| donatas meskele ||| flavius frasincar ||| 
2021 ||| pay attention to the cough: early diagnosis of covid-19 using interpretable symptoms embeddings with cough sound signal processing. ||| ankit pal ||| malaikannan sankarasubbu ||| 
2020 ||| de novo drug design using self attention mechanism. ||| vedang mandhana ||| rutuja taware ||| 
2019 ||| amv-lstm: an attention-based model with multiple positional text matching. ||| thiziri belkacem ||| taoufiq dkaki ||| jose g. moreno ||| mohand boughanem ||| 
2018 ||| affectiveroad system and database to assess driver's attention. ||| neska el haouij ||| jean-michel poggi ||| sylvie sevestre-ghalila ||| raja ghozi ||| m ||| riem ja ||| dane ||| 
2021 ||| adela: attention based deep ensemble learning for activity recognition in smart collaborative environments. ||| hyunju kim ||| dongman lee ||| 
2021 ||| attention-based stress detection exploiting non-contact monitoring of movement patterns with ir-uwb radar. ||| jonghoon shin ||| junhyung moon ||| beomsik kim ||| jihwan eom ||| noseong park ||| kyoungwoo lee ||| 
2017 ||| cogvis: attention-driven cognitive architecture for visual change detection. ||| shailesh deshpande ||| arcot sowmya ||| piyush yadav ||| shamsuddin ladha ||| priyanka verma ||| karthikeyan vaiapury ||| jay gubbi ||| p. balamuralidhar ||| 
2019 ||| library and information science papers discussed on twitter: a new network-based approach for measuring public attention. ||| robin haunschild ||| loet leydesdorff ||| lutz bornmann ||| 
2019 ||| social media attention of the esi highly cited papers: an altmetrics-based overview. ||| jos |||  antonio moral-mu ||| oz ||| alejandro salazar ||| david lucena-anton ||| pablo garc ||| a-s ||| nchez ||| manuel j. cobo ||| 
2019 ||| online attention of scholarly papers on psychosocial hazards - job stress, bullying and burnout. ||| witold sygocki ||| malgorzata rychlik ||| 
2019 ||| the impact of preprints in library and information science: citations, usage, and social attention. ||| zhiqi wang ||| wolfgang gl ||| nzel ||| yue chen ||| 
2017 ||| why do some research articles receive more online attention? reasons for online success as measured with altmetrics. ||| kim holmberg ||| julia vainio ||| 
2019 ||| altmetrics - on the way to the "economy of attention"? feasibility study altmetrics for the german ministry of science and research (bmbf). ||| dirk tunger ||| 
2019 ||| specialized user attention on twitter: identifying scientific fields of interest among social users of science. ||| jonathan dudek ||| rodrigo costas ||| 
2019 ||| unsupervised keyphrase extraction in academic publications using human attention. ||| yingyi zhang ||| chengzhi zhang ||| 
2021 ||| event attention network for stock trend prediction. ||| hongyu jiang ||| chunyang ye ||| shanyan lai ||| hui zhou ||| 
2021 ||| deepqsc: a gnn and attention mechanism-based framework for qos-aware service composition. ||| xiao ren ||| wenjun zhang ||| liang bao ||| jinqiu song ||| shuai wang ||| rong cao ||| xinlei wang ||| 
2021 ||| pyramid dilated attention network for action segmentation. ||| zexing du ||| feng mei ||| xiaohan lai ||| qing wang ||| 
2021 ||| a cybertwin-driven task offloading scheme based on deep reinforcement learning and graph attention networks. ||| xiaoxu zhong ||| yejun he ||| 
2021 ||| dual attention fusion network for single image dehazing. ||| hong zhu ||| dengyin zhang ||| yingjie kou ||| 
2021 ||| an attention-based bidirectional gated recurrent unit network for location prediction. ||| yu cao ||| ang li ||| jinglei lou ||| mingkai chen ||| xuguang zhang ||| bin kang ||| 
2021 ||| expression recognition based on attention mechanism and length feature of facial landmark. ||| qian yang ||| feng liu ||| zhenglai zhao ||| 
2020 ||| channel estimation method based on transformer in high dynamic environment. ||| zhuolin chen ||| fanglin gu ||| rui jiang ||| 
2021 ||| capsule network based on self-attention mechanism. ||| yunhao shang ||| ning xu ||| zhenzhou jin ||| xiao yao ||| 
2021 ||| cran: an hybrid cnn-rnn attention-based model for arabic machine translation. ||| nouhaila bensalah ||| habib ayad ||| abdellah adib ||| abdelhamid ibn el farouk ||| 
2020 ||| simulation design of power electronic transformer with dual-pwm. ||| jiu-yang mu ||| en fang ||| guan-bao zhang ||| song-hai zhou ||| 
2019 ||| attention-based hybrid model for automatic short answer scoring. ||| hui qi ||| yue wang ||| jinyu dai ||| jinqing li ||| xiaoqiang di ||| 
2020 ||| attention aware deep learning object detection and simulation. ||| jiping xiong ||| lingyun zhu ||| lingfeng ye ||| jinhong li ||| 
2020 ||| algorithm for double-layer structure multi-label classification with optimal sequence based on attention mechanism. ||| geqiao liu ||| mingjie tan ||| 
2021 ||| semi-supervised graph attention networks for event representation learning. ||| jo ||| o pedro rodrigues mattos ||| ricardo m. marcacini ||| 
2017 ||| dataset construction via attention for aspect term extraction with distant supervision. ||| athanasios giannakopoulos ||| diego antognini ||| claudiu musat ||| andreea hossmann ||| michael baeriswyl ||| 
2019 ||| spatiotemporal attention networks for wind power forecasting. ||| xingbo fu ||| feng gao ||| jiang wu ||| xinyu wei ||| fangwei duan ||| 
2019 ||| temporal self-attention network for medical concept embedding. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| michael blumenstein ||| 
2019 ||| an augmented transformer architecture for natural language generation tasks. ||| hailiang li ||| adele y. c. wang ||| yang liu ||| du tang ||| zhibin lei ||| wenye li ||| 
2021 ||| sting: self-attention based time-series imputation networks using gan. ||| eunkyu oh ||| taehun kim ||| yunhu ji ||| sushil khyalia ||| 
2018 ||| next point-of-interest recommendation with temporal and multi-level context attention. ||| ranzhen li ||| yanyan shen ||| yanmin zhu ||| 
2017 ||| multi-level multiple attentions for contextual multimodal sentiment analysis. ||| soujanya poria ||| erik cambria ||| devamanyu hazarika ||| navonil majumder ||| amir zadeh ||| louis-philippe morency ||| 
2019 ||| dynamic news recommendation with hierarchical attention network. ||| hui zhang ||| xu chen ||| shuai ma ||| 
2020 ||| multi-attention 3d residual neural network for origin-destination crowd flow prediction. ||| jiaman ma ||| jeffrey chan ||| sutharshan rajasegarar ||| goce ristanoski ||| christopher leckie ||| 
2018 ||| astm: an attentional segmentation based topic model for short texts. ||| jiamiao wang ||| ling chen ||| lu qin ||| xindong wu ||| 
2021 ||| deep reinforced attention regression for partial sketch based image retrieval. ||| dingrong wang ||| hitesh sapkota ||| xumin liu ||| qi yu ||| 
2021 ||| attention-based feature interaction for efficient online knowledge distillation. ||| tongtong su ||| qiyu liang ||| jinsong zhang ||| zhaoyang yu ||| gang wang ||| xiaoguang liu ||| 
2020 ||| attentionfm: incorporating attention mechanism and factorization machine for credit scoring. ||| ying liu ||| wei wang ||| tianlin zhang ||| zhenyu cui ||| 
2020 ||| multivariate time-series anomaly detection via graph attention network. ||| hang zhao ||| yujing wang ||| juanyong duan ||| congrui huang ||| defu cao ||| yunhai tong ||| bixiong xu ||| jing bai ||| jie tong ||| qi zhang ||| 
2020 ||| learning latent correlation of heterogeneous sensors using attention based temporal convolutional network. ||| xin wang ||| yunji liang ||| zhiwen yu ||| bin guo ||| 
2020 ||| multi-task time series forecasting with shared attention. ||| zekai chen ||| jiaze e ||| xiao zhang ||| hao sheng ||| xiuzheng cheng ||| 
2020 ||| community attention network for semi-supervised node classification. ||| zhongjing yu ||| han wang ||| yang liu ||| christian b ||| hm ||| junming shao ||| 
2017 ||| an cnn-lstm attention approach to understanding user query intent from online health communities. ||| ruichu cai ||| binjun zhu ||| lei ji ||| tianyong hao ||| jun yan ||| wenyin liu ||| 
2021 ||| gcn-se: attention as explainability for node classification in dynamic graphs. ||| yucai fan ||| yuhang yao ||| carlee joe-wong ||| 
2021 ||| psanet - subspace attention for personalized compatibility. ||| meet taraviya ||| anurag beniwal ||| yen-liang lin ||| larry davis ||| 
2018 ||| selective graph attention networks for account takeover detection. ||| jialing tao ||| hui wang ||| tao xiong ||| 
2018 ||| tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction. ||| tong chen ||| hongzhi yin ||| hongxu chen ||| lin wu ||| hao wang ||| xiaofang zhou ||| xue li ||| 
2020 ||| interactive knowledge graph attention network for recommender systems. ||| li yang ||| e. shijia ||| shiyao xu ||| yang xiang ||| 
2021 ||| joint scence network and attention-guided for image captioning. ||| dongming zhou ||| jing yang ||| canlong zhang ||| yanping tang ||| 
2019 ||| an attention ensemble based approach for multilabel profanity detection. ||| pratik ratadiya ||| deepak mishra ||| 
2019 ||| an integrated multimodal attention-based approach for bank stress test prediction. ||| farid razzak ||| fei yi ||| yang yang ||| hui xiong ||| 
2021 ||| hypertenet: hypergraph and transformer-based neural network for personalized list continuation. ||| m. vijaikumar ||| deepesh v. hada ||| shirish k. shevade ||| 
2018 ||| attend2trend: attention model for real-time detecting and forecasting of trending topics. ||| ahmed saleh ||| ansgar scherp ||| 
2017 ||| discovering cooperative structure among online items for attention dynamics. ||| kanji matsutani ||| masahito kumano ||| masahiro kimura ||| kazumi saito ||| kouzou ohara ||| hiroshi motoda ||| 
2019 ||| learning attentional temporal cues of brainwaves with spatial embedding for motion intent detection. ||| dalin zhang ||| kaixuan chen ||| debao jian ||| lina yao ||| sen wang ||| po li ||| 
2019 ||| counterfactual attention supervision. ||| seungtaek choi ||| haeju park ||| seung-won hwang ||| 
2021 ||| bi-level attention graph neural networks. ||| roshni g. iyer ||| wei wang ||| yizhou sun ||| 
2019 ||| few-shot learning based on attention relation compare network. ||| xian-qin ma ||| chongchong yu ||| xin yang ||| xiuxin chen ||| 
2021 ||| label dependent attention model for disease risk prediction using multimodal electronic health records. ||| shuai niu ||| qing yin ||| yunya song ||| yike guo ||| xian yang ||| 
2019 ||| st-attn: spatial-temporal attention mechanism for multi-step citywide crowd flow prediction. ||| yirong zhou ||| hao chen ||| jun li ||| ye wu ||| jiangjiang wu ||| luo chen ||| 
2020 ||| agstn: learning attention-adjusted graph spatio-temporal networks for short-term urban sensor value forecasting. ||| yi-ju lu ||| cheng-te li ||| 
2021 ||| stargat: star-shaped hierarchical graph attentional network for heterogeneous network representation learning. ||| wen-zhi li ||| ling huang ||| chang-dong wang ||| yu-xin ye ||| 
2020 ||| tado: time-varying attention with dual-optimizer model. ||| yuexin wu ||| tianyu gao ||| sihao wang ||| zhongmin xiong ||| 
2021 ||| sequential diagnosis prediction with transformer and ontological representation. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| 
2018 ||| muvan: a multi-view attention network for multivariate temporal data. ||| ye yuan ||| guangxu xun ||| fenglong ma ||| yaqing wang ||| nan du ||| kebin jia ||| lu su ||| aidong zhang ||| 
2019 ||| camp: co-attention memory networks for diagnosis prediction in healthcare. ||| jingyue gao ||| xiting wang ||| yasha wang ||| zhao yang ||| junyi gao ||| jiangtao wang ||| wen tang ||| xing xie ||| 
2017 ||| live on tv, alive on twitter: quantifying continuous partial attention of viewers during live television telecasts. ||| rohit saxena ||| savita bhat ||| niranjan pedanekar ||| 
2020 ||| learning space-time-frequency representation with two-stream attention based 3d network for motor imagery classification. ||| zhenqi li ||| jing wang ||| ziyu jia ||| youfang lin ||| 
2020 ||| interactive attention networks for semantic text matching. ||| sendong zhao ||| yong huang ||| chang su ||| yuantong li ||| fei wang ||| 
2018 ||| diagnosis prediction via medical context attention networks using deep generative modeling. ||| wonsung lee ||| sungrae park ||| weonyoung joo ||| il-chul moon ||| 
2021 ||| bat: beat-aligned transformer for electrocardiogram classification. ||| xiaoyu li ||| chen li ||| yuhua wei ||| yuyao sun ||| jishang wei ||| xiang li ||| buyue qian ||| 
2020 ||| camta: causal attention model for multi-touch attribution. ||| sachin kumar ||| garima gupta ||| ranjitha prasad ||| arnab chatterjee ||| lovekesh vig ||| gautam shroff ||| 
2020 ||| predict the next attack location via an attention-based fused-spatialtemporal lstm. ||| zhuang liu ||| juhua pu ||| nana zhan ||| xingwu liu ||| 
2021 ||| child face age progression and regression using self-attention multi-scale patch gan. ||| praveen kumar chandaliya ||| neeta nain ||| 
2019 ||| sclerasegnet: an improved u-net model with attention for accurate sclera segmentation. ||| caiyong wang ||| yong he ||| yunfan liu ||| zhaofeng he ||| ran he ||| zhenan sun ||| 
2021 ||| visual-semantic transformer for face forgery detection. ||| yuting xu ||| gengyun jia ||| huaibo huang ||| junxian duan ||| ran he ||| 
2021 ||| iris presentation attack detection by attention-based and deep pixel-wise binary supervision network. ||| meiling fang ||| naser damer ||| fadi boutros ||| florian kirchbuchner ||| arjan kuijper ||| 
2021 ||| bita-net: bi-temporal attention network for facial video forgery detection. ||| yiwei ru ||| wanting zhou ||| yunfan liu ||| jianxin sun ||| qi li ||| 
2021 ||| attention aware wavelet-based detection of morphed face images. ||| poorya aghdaie ||| baaria chaudhary ||| sobhan soleymani ||| jeremy m. dawson ||| nasser m. nasrabadi ||| 
2021 ||| video-based physiological measurement using 3d central difference convolution attention network. ||| yu zhao ||| bochao zou ||| fan yang ||| lin lu ||| abdelkader nasreddine belkacem ||| chao chen ||| 
2019 ||| polarimetric thermal to visible face verification via self-attention guided synthesis. ||| xing di ||| benjamin s. riggan ||| shuowen hu ||| nathaniel j. short ||| vishal m. patel ||| 
2019 ||| sanet: smoothed attention network for single stage face detector. ||| lei shi ||| xiang xu ||| ioannis a. kakadiaris ||| 
2021 ||| attention-guided progressive mapping for profile face recognition. ||| junyang huang ||| changxing ding ||| 
2020 ||| partial fingerprint verification via spatial transformer networks. ||| zhiyuan he ||| eryun liu ||| zhiyu xiang ||| 
2018 ||| improving face recognition by exploring local features with visual attention. ||| yichun shi ||| anil k. jain ||| 
2021 ||| on the effectiveness of vision transformers for zero-shot face anti-spoofing. ||| anjith george ||| s ||| bastien marcel ||| 
2021 ||| latent space transformers for generalizing deep networks. ||| hamed farkhari ||| joseanne viana ||| nidhi ||| lu ||| s miguel campos ||| pedro sebasti ||| o ||| albena mihovska ||| purnima lala mehta ||| lu ||| s bernardo ||| 
2019 ||| a non-invasive tool for attention-deficit disorder analysis based on gaze tracks. ||| dario cazzato ||| silvia m. castro ||| osvaldo agamennoni ||| gerardo fern ||| ndez ||| holger voos ||| 
2019 ||| named entity recognition in chinese electronic medical record using attention mechanism. ||| menglong li ||| yu zhang ||| mengxing huang ||| jing chen ||| wenlong feng ||| 
2018 ||| social image captioning with tags-based attention model. ||| yiwei wei ||| chunlei wu ||| xiaoliang chu ||| weishan zhang ||| leiquan wang ||| 
2020 ||| gaussian image denoiser based on deep convolutional sparse coding with attention mechanism. ||| yu shi ||| yingying hua ||| yige xu ||| haoran gao ||| zhenya wang ||| benchang zheng ||| 
2020 ||| energy-efficient inference service of transformer-based deep learning models on gpus. ||| yuxin wang ||| qiang wang ||| xiaowen chu ||| 
2018 ||| the distribution of transient magnetic field and eddy current losses of three-phase five-legged transformer under dc bias. ||| dong xia ||| 
2020 ||| attention-based hierarchical convolution neural network for fine-grained crop image classification. ||| jiannan yang ||| fan zhang ||| tiantian qian ||| 
2021 ||| safsn: a self-attention based neural network for encrypted mobile traffic classification. ||| chengyuan zhang ||| changqing an ||| jessie hui wang ||| ziyi zhao ||| tao yu ||| jilong wang ||| 
2019 ||| spatio-temporal attention lstm model for flood forecasting. ||| yukai ding ||| yuelong zhu ||| yirui wu ||| feng jun ||| zirun cheng ||| 
2021 ||| a mm-wave gm-assisted transformer-based matching network 2x2 phased-array receiver for 5g communication and radar systems. ||| kun-da chu ||| jacques christophe rudell ||| 
2019 ||| transformer-based 24 ghz power amplifier in 65nm cmos technology for fmcw applications. ||| nedim muharemovic ||| andreas bauch ||| amelie hagelauer ||| robert weigel ||| 
2021 ||| transformer interpretability beyond attention visualization. ||| hila chefer ||| shir gur ||| lior wolf ||| 
2020 ||| boosting the transferability of adversarial samples via attention. ||| weibin wu ||| yuxin su ||| xixian chen ||| shenglin zhao ||| irwin king ||| michael r. lyu ||| yu-wing tai ||| 
2020 ||| correlation-guided attention for corner detection based visual tracking. ||| fei du ||| peng liu ||| wei zhao ||| xianglong tang ||| 
2019 ||| temporal transformer networks: joint learning of invariant and discriminative time warping. ||| suhas lohit ||| qiao wang ||| pavan k. turaga ||| 
2020 ||| modality shifting attention network for multi-modal video question answering. ||| junyeong kim ||| minuk ma ||| trung x. pham ||| kyungsu kim ||| chang d. yoo ||| 
2019 ||| towards universal object detection by domain attention. ||| xudong wang ||| zhaowei cai ||| dashan gao ||| nuno vasconcelos ||| 
2017 ||| attentional correlation filter network for adaptive visual tracking. ||| jongwon choi ||| hyung jin chang ||| sangdoo yun ||| tobias fischer ||| yiannis demiris ||| jin young choi ||| 
2020 ||| recognizing handwritten mathematical expressions via paired dual loss attention network and printed mathematical expressions. ||| anh duc le ||| 
2021 ||| dynamic head: unifying object detection heads with attentions. ||| xiyang dai ||| yinpeng chen ||| bin xiao ||| dongdong chen ||| mengchen liu ||| lu yuan ||| lei zhang ||| 
2021 ||| general multi-label image classification with transformers. ||| jack lanchantin ||| tianlu wang ||| vicente ordonez ||| yanjun qi ||| 
2021 ||| self-supervised video hashing via bidirectional transformers. ||| shuyan li ||| xiu li ||| jiwen lu ||| jie zhou ||| 
2019 ||| attention-guided unified network for panoptic segmentation. ||| yanwei li ||| xinze chen ||| zheng zhu ||| lingxi xie ||| guan huang ||| dalong du ||| xingang wang ||| 
2019 ||| multi-modal face presentation attack detection via spatial and channel attentions. ||| guoqing wang ||| chuanxin lan ||| hu han ||| shiguang shan ||| xilin chen ||| 
2019 ||| masked graph attention network for person re-identification. ||| liqiang bao ||| bingpeng ma ||| hong chang ||| xilin chen ||| 
2019 ||| attention-based dropout layer for weakly supervised object localization. ||| junsuk choe ||| hyunjung shim ||| 
2019 ||| feratt: facial expression recognition with attention net. ||| pedro d. marrero-fern ||| ndez ||| fidel a. guerrero-pe ||| a ||| tsang ing ren ||| alexandre cunha ||| 
2021 ||| wide receptive field and channel attention network for jpeg compressed image deblurring. ||| donghyeon lee ||| chulhee lee ||| taesung kim ||| 
2018 ||| focal visual-text attention for visual question answering. ||| junwei liang ||| lu jiang ||| liangliang cao ||| li-jia li ||| alexander g. hauptmann ||| 
2018 ||| picanet: learning pixel-wise contextual attention for saliency detection. ||| nian liu ||| junwei han ||| ming-hsuan yang ||| 
2020 ||| lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. ||| junbo yin ||| jianbing shen ||| chenye guan ||| dingfu zhou ||| ruigang yang ||| 
2018 ||| deep-bcn: deep networks meet biased competition to create a brain-inspired model of attention control. ||| hossein adeli ||| gregory j. zelinsky ||| 
2019 ||| adcrowdnet: an attention-injective deformable convolutional network for crowd understanding. ||| ning liu ||| yongchao long ||| changqing zou ||| qun niu ||| li pan ||| hefeng wu ||| 
2017 ||| temporally steered gaussian attention for video understanding. ||| shagan sah ||| thang nguyen ||| miguel dom ||| nguez ||| felipe petroski such ||| raymond w. ptucha ||| 
2018 ||| st-gan: spatial transformer generative adversarial networks for image compositing. ||| chen-hsuan lin ||| ersin yumer ||| oliver wang ||| eli shechtman ||| simon lucey ||| 
2020 ||| learning oracle attention for high-fidelity face completion. ||| tong zhou ||| changxing ding ||| shaowen lin ||| xinchao wang ||| dacheng tao ||| 
2018 ||| hierarchical recurrent attention networks for structured online maps. ||| namdar homayounfar ||| wei-chiu ma ||| shrinidhi kowshika lakshmikanth ||| raquel urtasun ||| 
2020 ||| attention convolutional binary neural tree for fine-grained visual categorization. ||| ruyi ji ||| longyin wen ||| libo zhang ||| dawei du ||| yanjun wu ||| chen zhao ||| xianglong liu ||| feiyue huang ||| 
2020 ||| attention scaling for crowd counting. ||| xiaoheng jiang ||| li zhang ||| mingliang xu ||| tianzhu zhang ||| pei lv ||| bing zhou ||| xin yang ||| yanwei pang ||| 
2019 ||| attentional pointnet for 3d-object detection in point clouds. ||| anshul paigwar ||| zg ||| r erkent ||| christian wolf ||| christian laugier ||| 
2020 ||| learning selective self-mutual attention for rgb-d saliency detection. ||| nian liu ||| ni zhang ||| junwei han ||| 
2017 ||| learning dynamic gmm for attention distribution on single-face videos. ||| yun ren ||| zulin wang ||| mai xu ||| haoyu dong ||| shengxi li ||| 
2018 ||| global pose estimation with an attention-based recurrent network. ||| emilio parisotto ||| devendra singh chaplot ||| jian zhang ||| ruslan salakhutdinov ||| 
2020 ||| fusatnet: dual attention based spectrospatial multimodal fusion network for hyperspectral and lidar classification. ||| satyam mohla ||| shivam pande ||| biplab banerjee ||| subhasis chaudhuri ||| 
2021 ||| end-to-end human pose and mesh reconstruction with transformers. ||| kevin lin ||| lijuan wang ||| zicheng liu ||| 
2021 ||| an improved attention for visual question answering. ||| tanzila rahman ||| shih-han chou ||| leonid sigal ||| giuseppe carenini ||| 
2018 ||| attentional shapecontextnet for point cloud recognition. ||| saining xie ||| sainan liu ||| zeyu chen ||| zhuowen tu ||| 
2021 ||| soe-net: a self-attention and orientation encoding network for point cloud based place recognition. ||| yan xia ||| yusheng xu ||| shuang li ||| rui wang ||| juan du ||| daniel cremers ||| uwe stilla ||| 
2018 ||| an end-to-end textspotter with explicit alignment and attention. ||| tong he ||| zhi tian ||| weilin huang ||| chunhua shen ||| yu qiao ||| changming sun ||| 
2021 ||| line segment detection using transformers without edges. ||| yifan xu ||| weijian xu ||| david cheung ||| zhuowen tu ||| 
2020 ||| learning temporal co-attention models for unsupervised video action localization. ||| guoqiang gong ||| xinghan wang ||| yadong mu ||| qi tian ||| 
2021 ||| isolated sign recognition from rgb video using pose flow and self-attention. ||| mathieu de coster ||| mieke van herreweghe ||| joni dambre ||| 
2020 ||| imram: iterative matching with recurrent attention memory for cross-modal image-text retrieval. ||| hui chen ||| guiguang ding ||| xudong liu ||| zijia lin ||| ji liu ||| jungong han ||| 
2017 ||| episodic camn: contextual attention-based memory networks with iterative feedback for scene labeling. ||| abrar h. abdulnabi ||| bing shuai ||| stefan winkler ||| gang wang ||| 
2020 ||| end-to-end learning for video frame compression with self-attention. ||| nannan zou ||| honglei zhang ||| francesco cricri ||| hamed r. tavakoli ||| jani lainema ||| emre aksu ||| miska m. hannuksela ||| esa rahtu ||| 
2018 ||| learning attentions: residual attentional siamese network for high performance online visual tracking. ||| qiang wang ||| zhu teng ||| junliang xing ||| jin gao ||| weiming hu ||| stephen j. maybank ||| 
2019 ||| a dual attention network with semantic embedding for few-shot learning. ||| shipeng yan ||| songyang zhang ||| xuming he ||| 
2019 ||| graph attention convolution for point cloud semantic segmentation. ||| lei wang ||| yuchun huang ||| yaolin hou ||| shenman zhang ||| jie shan ||| 
2018 ||| mattnet: modular attention network for referring expression comprehension. ||| licheng yu ||| zhe lin ||| xiaohui shen ||| jimei yang ||| xin lu ||| mohit bansal ||| tamara l. berg ||| 
2017 ||| end-to-end instance segmentation with recurrent attention. ||| mengye ren ||| richard s. zemel ||| 
2020 ||| attention-driven cropping for very high resolution facial landmark detection. ||| prashanth chandran ||| derek bradley ||| markus h. gross ||| thabo beeler ||| 
2021 ||| (asna) an attention-based siamese-difference neural network with surrogate ranking loss function for perceptual image quality assessment. ||| seyed mehdi ayyoubzadeh ||| ali royat ||| 
2021 ||| thinking fast and slow: efficient text-to-visual retrieval with transformers. ||| antoine miech ||| jean-baptiste alayrac ||| ivan laptev ||| josef sivic ||| andrew zisserman ||| 
2019 ||| scan: spatial color attention networks for real single image super-resolution. ||| xuan xu ||| xin li ||| 
2018 ||| amnet: memorability estimation with attention. ||| jiri fajtl ||| vasileios argyriou ||| dorothy monekosso ||| paolo remagnino ||| 
2021 ||| ednet: efficient disparity estimation with cost volume combination and attention-based spatial residual. ||| songyan zhang ||| zhicheng wang ||| qiang wang ||| jinshuo zhang ||| gang wei ||| xiaowen chu ||| 
2019 ||| looking for the devil in the details: learning trilinear attention sampling network for fine-grained image recognition. ||| heliang zheng ||| jianlong fu ||| zheng-jun zha ||| jiebo luo ||| 
2020 ||| dynamic attention-based visual odometry. ||| xin-yu kuo ||| chien liu ||| kai-chen lin ||| chun-yi lee ||| 
2019 ||| residual attention-based fusion for video classification. ||| samira pouyanfar ||| tianyi wang ||| shu-ching chen ||| 
2021 ||| adaptive image transformer for one-shot object detection. ||| ding-jie chen ||| he-yen hsieh ||| tyng-luh liu ||| 
2019 ||| visual attention in multi-label image classification. ||| yan luo ||| ming jiang ||| qi zhao ||| 
2017 ||| residual attention network for image classification. ||| fei wang ||| mengqing jiang ||| chen qian ||| shuo yang ||| cheng li ||| honggang zhang ||| xiaogang wang ||| xiaoou tang ||| 
2017 ||| look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition. ||| jianlong fu ||| heliang zheng ||| tao mei ||| 
2020 ||| weakly-supervised action localization by generative attention modeling. ||| baifeng shi ||| qi dai ||| yadong mu ||| jingdong wang ||| 
2019 ||| multi-view vehicle re-identification using temporal attention model and metadata re-ranking. ||| tsung-wei huang ||| jiarui cai ||| hao yang ||| hung-min hsu ||| jenq-neng hwang ||| 
2018 ||| end-to-end flow correlation tracking with spatial-temporal attention. ||| zheng zhu ||| wei wu ||| wei zou ||| junjie yan ||| 
2021 ||| visual navigation with spatial attention. ||| bar mayo ||| tamir hazan ||| ayellet tal ||| 
2019 ||| end-to-end multi-task learning with attention. ||| shikun liu ||| edward johns ||| andrew j. davison ||| 
2020 ||| relation-aware global attention for person re-identification. ||| zhizheng zhang ||| cuiling lan ||| wenjun zeng ||| xin jin ||| zhibo chen ||| 
2019 ||| improving referring expression grounding with cross-modal attention-guided erasing. ||| xihui liu ||| zihao wang ||| jing shao ||| xiaogang wang ||| hongsheng li ||| 
2021 ||| pixel-guided dual-branch attention network for joint image deblurring and super-resolution. ||| si xi ||| jia wei ||| weidong zhang ||| 
2017 ||| amc: attention guided multi-modal correlation learning for image search. ||| kan chen ||| trung bui ||| chen fang ||| zhaowen wang ||| ram nevatia ||| 
2019 ||| attention driven vehicle re-identification and unsupervised anomaly detection for traffic understanding. ||| pirazh khorramshahi ||| neehar peri ||| amit kumar ||| anshul shah ||| rama chellappa ||| 
2020 ||| dynamic convolution: attention over convolution kernels. ||| yinpeng chen ||| xiyang dai ||| mengchen liu ||| dongdong chen ||| lu yuan ||| zicheng liu ||| 
2019 ||| classification of computer generated and natural images based on efficient deep convolutional recurrent attention model. ||| diangarti bhalang tariang ||| prithviraj senguptab ||| aniket roy ||| rajat subhra chakraborty ||| ruchira naskar ||| 
2021 ||| multi-task learning with attention for end-to-end autonomous driving. ||| keishi ishihara ||| anssi kanervisto ||| jun miura ||| ville hautam ||| ki ||| 
2019 ||| depth-attentional features for single-image rain removal. ||| xiaowei hu ||| chi-wing fu ||| lei zhu ||| pheng-ann heng ||| 
2021 ||| toward accurate and realistic outfits visualization with attention to details. ||| kedan li ||| min jin chong ||| jeffrey zhang ||| jingen liu ||| 
2019 ||| recursive visual attention in visual dialog. ||| yulei niu ||| hanwang zhang ||| manli zhang ||| jianhong zhang ||| zhiwu lu ||| ji-rong wen ||| 
2021 ||| 3d-man: 3d multi-frame attention network for object detection. ||| zetong yang ||| yin zhou ||| zhifeng chen ||| jiquan ngiam ||| 
2018 ||| deep diffeomorphic transformer networks. ||| nicki skafte detlefsen ||| oren freifeld ||| s ||| ren hauberg ||| 
2018 ||| weakly supervised phrase localization with multi-scale anchored transformer network. ||| fang zhao ||| jianshu li ||| jian zhao ||| jiashi feng ||| 
2019 ||| cross-modal self-attention network for referring image segmentation. ||| linwei ye ||| mrigank rochan ||| zhi liu ||| yang wang ||| 
2018 ||| image caption generation with hierarchical contextual visual spatial attention. ||| mahmoud khademi ||| oliver schulte ||| 
2021 ||| perceptual image quality assessment with transformers. ||| manri cheon ||| sung-jun yoon ||| byungyeon kang ||| junwoo lee ||| 
2018 ||| decidenet: counting varying density crowds through attention guided detection and density estimation. ||| jiang liu ||| chenqiang gao ||| deyu meng ||| alexander g. hauptmann ||| 
2018 ||| attention in multimodal neural networks for person re-identification. ||| aske r. lejb ||| lle ||| benjamin krogh ||| kamal nasrollahi ||| thomas b. moeslund ||| 
2021 ||| up-detr: unsupervised pre-training for object detection with transformers. ||| zhigang dai ||| bolun cai ||| yugeng lin ||| junying chen ||| 
2020 ||| polytransform: deep polygon transformer for instance segmentation. ||| justin liang ||| namdar homayounfar ||| wei-chiu ma ||| yuwen xiong ||| rui hu ||| raquel urtasun ||| 
2021 ||| bottleneck transformers for visual recognition. ||| aravind srinivas ||| tsung-yi lin ||| niki parmar ||| jonathon shlens ||| pieter abbeel ||| ashish vaswani ||| 
2020 ||| cars can't fly up in the sky: improving urban-scene segmentation via height-driven attention networks. ||| sungha choi ||| joanne taery kim ||| jaegul choo ||| 
2021 ||| self-attention based text knowledge mining for text detection. ||| qi wan ||| haoqin ji ||| linlin shen ||| 
2017 ||| attentional push: a deep convolutional network for augmenting image salience with shared attention modeling in social scenes. ||| siavash gorji ||| james j. clark ||| 
2021 ||| dual attention suppression attack: generate adversarial camouflage in physical world. ||| jiakai wang ||| aishan liu ||| zixin yin ||| shunchang liu ||| shiyu tang ||| xianglong liu ||| 
2020 ||| towards robust image classification using sequential attention models. ||| daniel zoran ||| mike chrzanowski ||| po-sen huang ||| sven gowal ||| alex mott ||| pushmeet kohli ||| 
2020 ||| s2a: wasserstein gan with spatio-spectral laplacian attention for multi-spectral band synthesis. ||| litu rout ||| indranil misra ||| s. manthira moorthi ||| debajyoti dhar ||| 
2020 ||| prime sample attention in object detection. ||| yuhang cao ||| kai chen ||| chen change loy ||| dahua lin ||| 
2020 ||| interactive image segmentation with first click attention. ||| zheng lin ||| zhao zhang ||| lin-zhuo chen ||| ming-ming cheng ||| shao-ping lu ||| 
2020 ||| tesa: tensor element self-attention via matricization. ||| francesca babiloni ||| ioannis marras ||| gregory g. slabaugh ||| stefanos zafeiriou ||| 
2020 ||| self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. ||| adrian johnston ||| gustavo carneiro ||| 
2021 ||| a joint spatial and magnification based attention framework for large scale histopathology classification. ||| jingwei zhang ||| ke ma ||| john s. van arnam ||| rajarsi gupta ||| joel h. saltz ||| maria vakalopoulou ||| dimitris samaras ||| 
2019 ||| pyramid feature attention network for saliency detection. ||| ting zhao ||| xiangqian wu ||| 
2020 ||| your local gan: designing two dimensional local attention mechanisms for generative models. ||| giannis daras ||| augustus odena ||| han zhang ||| alexandros g. dimakis ||| 
2020 ||| epipolar transformers. ||| yihui he ||| rui yan ||| katerina fragkiadaki ||| shoou-i yu ||| 
2021 ||| multimodal motion prediction with stacked transformers. ||| yicheng liu ||| jinghuai zhang ||| liangji fang ||| qinhong jiang ||| bolei zhou ||| 
2017 ||| temporal attention-gated model for robust sequence classification. ||| wenjie pei ||| tadas baltrusaitis ||| david m. j. tax ||| louis-philippe morency ||| 
2021 ||| layouttransformer: scene layout generation with conceptual and spatial diversity. ||| cheng-fu yang ||| wan-cyuan fan ||| fu-en yang ||| yu-chiang frank wang ||| 
2019 ||| progressive pose attention transfer for person image generation. ||| zhen zhu ||| tengteng huang ||| baoguang shi ||| miao yu ||| bofei wang ||| xiang bai ||| 
2017 ||| inverse compositional spatial transformer networks. ||| chen-hsuan lin ||| simon lucey ||| 
2019 ||| factor graph attention. ||| idan schwartz ||| seunghak yu ||| tamir hazan ||| alexander g. schwing ||| 
2021 ||| graph attention tracking. ||| dongyan guo ||| yanyan shao ||| ying cui ||| zhenhua wang ||| liyan zhang ||| chunhua shen ||| 
2021 ||| mr image super-resolution with squeeze and excitation reasoning attention network. ||| yulun zhang ||| kai li ||| kunpeng li ||| yun fu ||| 
2020 ||| deformable siamese attention networks for visual object tracking. ||| yuechen yu ||| yilei xiong ||| weilin huang ||| matthew r. scott ||| 
2019 ||| learning roi transformer for oriented object detection in aerial images. ||| jian ding ||| nan xue ||| yang long ||| gui-song xia ||| qikai lu ||| 
2019 ||| attention-aware multi-stroke style transfer. ||| yuan yao ||| jianqiang ren ||| xuansong xie ||| weidong liu ||| yong-jin liu ||| jun wang ||| 
2021 ||| all you can embed: natural language based vehicle retrieval with spatio-temporal transformers. ||| carmelo scribano ||| davide sapienza ||| giorgia franchini ||| micaela verucchi ||| marko bertogna ||| 
2021 ||| rstnet: captioning with adaptive attention on visual and non-visual words. ||| xuying zhang ||| xiaoshuai sun ||| yunpeng luo ||| jiayi ji ||| yiyi zhou ||| yongjian wu ||| feiyue huang ||| rongrong ji ||| 
2019 ||| the pros and cons: rank-aware temporal attention for skill determination in long videos. ||| hazel doughty ||| walterio w. mayol-cuevas ||| dima damen ||| 
2019 ||| learning unsupervised video object segmentation through visual attention. ||| wenguan wang ||| hongmei song ||| shuyang zhao ||| jianbing shen ||| sanyuan zhao ||| steven c. h. hoi ||| haibin ling ||| 
2021 ||| co-grounding networks with semantic attention for referring expression comprehension in videos. ||| sijie song ||| xudong lin ||| jiaying liu ||| zongming guo ||| shih-fu chang ||| 
2019 ||| multi-layer depth and epipolar feature transformers for 3d scene reconstruction. ||| daeyun shin ||| zhile ren ||| erik b. sudderth ||| charless c. fowlkes ||| 
2019 ||| kernel transformer networks for compact spherical convolution. ||| yu-chuan su ||| kristen grauman ||| 
2021 ||| revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning. ||| amaia salvador ||| erhan gundogdu ||| loris bazzani ||| michael donoser ||| 
2021 ||| reinforced attention for few-shot learning and beyond. ||| jie hong ||| pengfei fang ||| weihao li ||| tong zhang ||| christian simon ||| mehrtash harandi ||| lars petersson ||| 
2018 ||| diversity regularized spatiotemporal attention for video-based person re-identification. ||| shuang li ||| slawomir bak ||| peter carr ||| xiaogang wang ||| 
2017 ||| global context-aware attention lstm networks for 3d action recognition. ||| jun liu ||| gang wang ||| ping hu ||| ling-yu duan ||| alex c. kot ||| 
2020 ||| context-aware group captioning via self-attention and contrastive features. ||| zhuowan li ||| quan tran ||| long mai ||| zhe lin ||| alan l. yuille ||| 
2017 ||| dual attention networks for multimodal reasoning and matching. ||| hyeonseob nam ||| jung-woo ha ||| jeonghee kim ||| 
2021 ||| end-to-end video instance segmentation with transformers. ||| yuqing wang ||| zhaoliang xu ||| xinlong wang ||| chunhua shen ||| baoshan cheng ||| hao shen ||| huaxia xia ||| 
2018 ||| da-gan: instance-level image translation by deep attention generative adversarial networks. ||| shuang ma ||| jianlong fu ||| chang wen chen ||| tao mei ||| 
2017 ||| sca-cnn: spatial and channel-wise attention in convolutional networks for image captioning. ||| long chen ||| hanwang zhang ||| jun xiao ||| liqiang nie ||| jian shao ||| wei liu ||| tat-seng chua ||| 
2020 ||| learning texture transformer network for image super-resolution. ||| fuzhi yang ||| huan yang ||| jianlong fu ||| hongtao lu ||| baining guo ||| 
2020 ||| multi-modality cross attention network for image and sentence matching. ||| xi wei ||| tianzhu zhang ||| yan li ||| yongdong zhang ||| feng wu ||| 
2018 ||| stacked latent attention for multimodal reasoning. ||| haoqi fan ||| jiatong zhou ||| 
2018 ||| multimodal attention for fusion of audio and spatiotemporal features for video description. ||| chiori hori ||| takaaki hori ||| gordon wichern ||| jue wang ||| teng-yok lee ||| anoop cherian ||| tim k. marks ||| 
2021 ||| festa: flow estimation via spatial-temporal attention for scene point clouds. ||| haiyan wang ||| jiahao pang ||| muhammad a. lodhi ||| yingli tian ||| dong tian ||| 
2018 ||| attention clusters: purely attention based local feature integration for video classification. ||| xiang long ||| chuang gan ||| gerard de melo ||| jiajun wu ||| xiao liu ||| shilei wen ||| 
2019 ||| multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation. ||| hao tang ||| dan xu ||| nicu sebe ||| yanzhi wang ||| jason j. corso ||| yan yan ||| 
2021 ||| rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. ||| sixiao zheng ||| jiachen lu ||| hengshuang zhao ||| xiatian zhu ||| zekun luo ||| yabiao wang ||| yanwei fu ||| jianfeng feng ||| tao xiang ||| philip h. s. torr ||| li zhang ||| 
2021 ||| temporal-relational crosstransformers for few-shot action recognition. ||| toby perrett ||| alessandro masullo ||| tilo burghardt ||| majid mirmehdi ||| dima damen ||| 
2020 ||| learned image compression with discretized gaussian mixture likelihoods and attention modules. ||| zhengxue cheng ||| heming sun ||| masaru takeuchi ||| jiro katto ||| 
2020 ||| robust superpixel-guided attentional adversarial attack. ||| xiaoyi dong ||| jiangfan han ||| dongdong chen ||| jiayang liu ||| huanyu bian ||| zehua ma ||| hongsheng li ||| xiaogang wang ||| weiming zhang ||| nenghai yu ||| 
2019 ||| are you paying attention? classifying attention in pivotal response treatment videos. ||| corey d. c. heath ||| hemanth venkateswara ||| sethuraman panchanathan ||| 
2018 ||| bottom-up and top-down attention for image captioning and visual question answering. ||| peter anderson ||| xiaodong he ||| chris buehler ||| damien teney ||| mark johnson ||| stephen gould ||| lei zhang ||| 
2020 ||| attention-aware multi-view stereo. ||| keyang luo ||| tao guan ||| lili ju ||| yuesong wang ||| zhuo chen ||| yawei luo ||| 
2020 ||| channel attention based iterative residual learning for depth map super-resolution. ||| xibin song ||| yuchao dai ||| dingfu zhou ||| liu liu ||| wei li ||| hongdong li ||| ruigang yang ||| 
2018 ||| where and why are they looking? jointly inferring human attention and intentions in complex tasks. ||| ping wei ||| yang liu ||| tianmin shu ||| nanning zheng ||| song-chun zhu ||| 
2020 ||| hypergraph attention networks for multimodal learning. ||| eun-sol kim ||| woo-young kang ||| kyoung-woon on ||| yu-jung heo ||| byoung-tak zhang ||| 
2018 ||| differential attention for visual question answering. ||| badri n. patro ||| vinay p. namboodiri ||| 
2017 ||| supervising neural attention models for video captioning by human gaze data. ||| youngjae yu ||| jongwook choi ||| yeonhwa kim ||| kyung yoo ||| sang-hun lee ||| gunhee kim ||| 
2021 ||| multi-stage aggregated transformer network for temporal language localization in videos. ||| mingxing zhang ||| yang yang ||| xinghan chen ||| yanli ji ||| xing xu ||| jingjing li ||| heng tao shen ||| 
2019 ||| second-order attention network for single image super-resolution. ||| tao dai ||| jianrui cai ||| yongbing zhang ||| shu-tao xia ||| lei zhang ||| 
2021 ||| transformer-based text detection in the wild. ||| zobeir raisi ||| mohamed a. naiel ||| georges younes ||| steven wardell ||| john s. zelek ||| 
2020 ||| hierarchical graph attention network for visual relationship detection. ||| li mi ||| zhenzhong chen ||| 
2020 ||| inflated episodic memory with region self-attention for long-tailed visual recognition. ||| linchao zhu ||| yi yang ||| 
2020 ||| fantastic answers and where to find them: immersive question-directed visual attention. ||| ming jiang ||| shi chen ||| jinhui yang ||| qi zhao ||| 
2020 ||| meshed-memory transformer for image captioning. ||| marcella cornia ||| matteo stefanini ||| lorenzo baraldi ||| rita cucchiara ||| 
2019 ||| mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks. ||| junjie zhang ||| qi wu ||| jian zhang ||| chunhua shen ||| jianfeng lu ||| 
2021 ||| gated spatio-temporal attention-guided video deblurring. ||| maitreya suin ||| a. n. rajagopalan ||| 
2020 ||| normalized and geometry-aware self-attention network for image captioning. ||| longteng guo ||| jing liu ||| xinxin zhu ||| peng yao ||| shichen lu ||| hanqing lu ||| 
2020 ||| nonlocal channel attention for nonhomogeneous image dehazing. ||| kareem metwaly ||| xuelu li ||| tiantong guo ||| vishal monga ||| 
2019 ||| kernel transformer networks for compact spherical convolution. ||| yu-chuan su ||| kristen grauman ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| yilun chen ||| chiyu dong ||| praveen palanisamy ||| priyantha mudalige ||| katharina muelling ||| john m. dolan ||| 
2019 ||| attention-guided network for ghost-free high dynamic range imaging. ||| qingsen yan ||| dong gong ||| qinfeng shi ||| anton van den hengel ||| chunhua shen ||| ian d. reid ||| yanning zhang ||| 
2021 ||| point 4d transformer networks for spatio-temporal modeling in point cloud videos. ||| hehe fan ||| yi yang ||| mohan s. kankanhalli ||| 
2021 ||| unsupervised visual attention and invariance for reinforcement learning. ||| xudong wang ||| long lian ||| stella x. yu ||| 
2019 ||| pay attention! - robustifying a deep visuomotor policy through task-focused visual attention. ||| pooya abolghasemi ||| amir mazaheri ||| mubarak shah ||| ladislau b ||| l ||| ni ||| 
2019 ||| salient object detection with pyramid attention and salient edges. ||| wenguan wang ||| shuyang zhao ||| jianbing shen ||| steven c. h. hoi ||| ali borji ||| 
2018 ||| attngan: fine-grained text to image generation with attentional generative adversarial networks. ||| tao xu ||| pengchuan zhang ||| qiuyuan huang ||| han zhang ||| zhe gan ||| xiaolei huang ||| xiaodong he ||| 
2020 ||| attention-based context aware reasoning for situation recognition. ||| thilini cooray ||| ngai-man cheung ||| wei lu ||| 
2020 ||| video super-resolution with temporal group attention. ||| takashi isobe ||| songjiang li ||| xu jia ||| shanxin yuan ||| gregory g. slabaugh ||| chunjing xu ||| ya-li li ||| shengjin wang ||| qi tian ||| 
2020 ||| large scale vehicle re-identification by knowledge transfer from simulated data and temporal attention. ||| viktor eckstein ||| arne schumann ||| andreas specker ||| 
2020 ||| explaining autonomous driving by learning end-to-end visual attention. ||| luca cultrera ||| lorenzo seidenari ||| federico becattini ||| pietro pala ||| alberto del bimbo ||| 
2019 ||| multi-scale body-part mask guided attention for person re-identification. ||| honglong cai ||| zhiguan wang ||| jinxing cheng ||| 
2017 ||| multi-attention network for one shot learning. ||| peng wang ||| lingqiao liu ||| chunhua shen ||| zi huang ||| anton van den hengel ||| heng tao shen ||| 
2020 ||| non-local neural networks with grouped bilinear attentional transforms. ||| lu chi ||| zehuan yuan ||| yadong mu ||| changhu wang ||| 
2020 ||| adaptive weighted attention network with camera spectral sensitivity prior for spectral reconstruction from rgb images. ||| jiaojiao li ||| chaoxiong wu ||| rui song ||| yunsong li ||| fei liu ||| 
2019 ||| lsta: long short-term attention for egocentric action recognition. ||| swathikiran sudhakaran ||| sergio escalera ||| oswald lanz ||| 
2018 ||| fooling vision and language models despite localization and attention mechanism. ||| xiaojun xu ||| xinyun chen ||| chang liu ||| anna rohrbach ||| trevor darrell ||| dawn song ||| 
2018 ||| inferring shared attention in social scene videos. ||| lifeng fan ||| yixin chen ||| ping wei ||| wenguan wang ||| song-chun zhu ||| 
2021 ||| attention! stay focus! ||| tu vo ||| 
2020 ||| point cloud completion by skip-attention network with hierarchical folding. ||| xin wen ||| tianyang li ||| zhizhong han ||| yu-shen liu ||| 
2021 ||| person re-identification using heterogeneous local graph attention networks. ||| zhong zhang ||| haijia zhang ||| shuang liu ||| 
2019 ||| object detection with location-aware deformable convolution and backward attention filtering. ||| chen zhang ||| joohee kim ||| 
2019 ||| practical stacked non-local attention modules for image compression. ||| haojie liu ||| tong chen ||| qiu shen ||| zhan ma ||| 
2019 ||| deep attention model for the hierarchical diagnosis of skin lesions. ||| catarina barata ||| jorge s. marques ||| m. emre celebi ||| 
2021 ||| ssan: separable self-attention network for video representation learning. ||| xudong guo ||| xun guo ||| yan lu ||| 
2021 ||| sstvos: sparse spatiotemporal transformers for video object segmentation. ||| brendan duke ||| abdalla ahmed ||| christian wolf ||| parham aarabi ||| graham w. taylor ||| 
2021 ||| transformer tracking. ||| xin chen ||| bin yan ||| jiawen zhu ||| dong wang ||| xiaoyun yang ||| huchuan lu ||| 
2021 ||| expectation-maximization attention cross residual network for single image super-resolution. ||| xiaobiao du ||| jie niu ||| chongjin liu ||| 
2019 ||| visual attention consistency under image transforms for multi-label image classification. ||| hao guo ||| kang zheng ||| xiaochuan fan ||| hongkai yu ||| song wang ||| 
2021 ||| manipulation detection in satellite images using vision transformer. ||| j ||| nos horv ||| th ||| sriram baireddy ||| hanxiang hao ||| daniel mas montserrat ||| edward j. delp ||| 
2021 ||| transformer meets tracker: exploiting temporal context for robust visual tracking. ||| ning wang ||| wengang zhou ||| jie wang ||| houqiang li ||| 
2020 ||| color-wise attention network for low-light image enhancement. ||| yousef atoum ||| mao ye ||| liu ren ||| ying tai ||| xiaoming liu ||| 
2017 ||| attention-based natural language person retrieval. ||| tao zhou ||| muhao chen ||| jie yu ||| demetri terzopoulos ||| 
2021 ||| loftr: detector-free local feature matching with transformers. ||| jiaming sun ||| zehong shen ||| yuang wang ||| hujun bao ||| xiaowei zhou ||| 
2021 ||| dual attention guided gaze target detection in the wild. ||| yi fang ||| jiapeng tang ||| wang shen ||| wei shen ||| xiao gu ||| li song ||| guangtao zhai ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| yilun chen ||| chiyu dong ||| praveen palanisamy ||| priyantha mudalige ||| katharina muelling ||| john m. dolan ||| 
2019 ||| dual attention network for scene segmentation. ||| jun fu ||| jing liu ||| haijie tian ||| yong li ||| yongjun bao ||| zhiwei fang ||| hanqing lu ||| 
2018 ||| generative image inpainting with contextual attention. ||| jiahui yu ||| zhe lin ||| jimei yang ||| xiaohui shen ||| xin lu ||| thomas s. huang ||| 
2020 ||| hierarchical pyramid diverse attention networks for face recognition. ||| qiangchang wang ||| tianyi wu ||| he zheng ||| guodong guo ||| 
2021 ||| coordinate attention for efficient mobile network design. ||| qibin hou ||| daquan zhou ||| jiashi feng ||| 
2021 ||| introvert: human trajectory prediction via conditional 3d attention. ||| nasim shafiee ||| taskin padir ||| ehsan elhamifar ||| 
2021 ||| hotr: end-to-end human-object interaction detection with transformers. ||| bumsoo kim ||| junhyun lee ||| jaewoo kang ||| eun-sol kim ||| hyunwoo j. kim ||| 
2021 ||| facial action unit detection with transformers. ||| geethu miriam jacob ||| bj ||| rn stenger ||| 
2019 ||| emotion-aware human attention prediction. ||| macario o. cordel ii ||| shaojing fan ||| zhiqi shen ||| mohan s. kankanhalli ||| 
2020 ||| image search with text feedback by visiolinguistic attention learning. ||| yanbei chen ||| shaogang gong ||| loris bazzani ||| 
2020 ||| residual pixel attention network for spectral reconstruction from rgb images. ||| hao peng ||| xiaomei chen ||| jie zhao ||| 
2021 ||| discrete-continuous action space policy gradient-based attention for image-text matching. ||| shiyang yan ||| li yu ||| yuan xie ||| 
2020 ||| context-aware attention network for image-text retrieval. ||| qi zhang ||| zhen lei ||| zhaoxiang zhang ||| stan z. li ||| 
2018 ||| attention-aware compositional network for person re-identification. ||| jing xu ||| rui zhao ||| feng zhu ||| huaming wang ||| wanli ouyang ||| 
2021 ||| topological planning with transformers for vision-and-language navigation. ||| kevin chen ||| junshen k. chen ||| jo chuang ||| marynel v ||| zquez ||| silvio savarese ||| 
2021 ||| a2-fpn: attention aggregation based feature pyramid network for instance segmentation. ||| miao hu ||| yali li ||| lu fang ||| shengjin wang ||| 
2020 ||| fake news detection using higher-order user to user mutual-attention progression in propagation paths. ||| rahul mishra ||| 
2020 ||| further non-local and channel attention networks for vehicle re-identification. ||| kai liu ||| zheng xu ||| zhaohui hou ||| zhicheng zhao ||| fei su ||| 
2019 ||| directing dnns attention for facial attribution classification using gradient-weighted class activation mapping. ||| xi yang ||| bojian wu ||| issei sato ||| takeo igarashi ||| 
2020 ||| x-linear attention networks for image captioning. ||| yingwei pan ||| ting yao ||| yehao li ||| tao mei ||| 
2020 ||| leaf spot attention network for apple leaf disease identification. ||| hee-jin yu ||| chang-hwan son ||| 
2020 ||| end-to-end adversarial-attention network for multi-modal clustering. ||| runwu zhou ||| yi-dong shen ||| 
2021 ||| keep your eyes on the lane: real-time attention-guided lane detection. ||| lucas tabelini torres ||| rodrigo ferreira berriel ||| thiago m. paix ||| o ||| claudine badue ||| alberto f. de souza ||| thiago oliveira-santos ||| 
2021 ||| multi-modal fusion transformer for end-to-end autonomous driving. ||| aditya prakash ||| kashyap chitta ||| andreas geiger ||| 
2020 ||| sign language transformers: joint end-to-end sign language recognition and translation. ||| necati cihan camg ||| z ||| oscar koller ||| simon hadfield ||| richard bowden ||| 
2019 ||| attention based image compression post-processing convlutional neural network. ||| yuyang xue ||| 
2019 ||| an attention enhanced graph convolutional lstm network for skeleton-based action recognition. ||| chenyang si ||| wentao chen ||| wei wang ||| liang wang ||| tieniu tan ||| 
2020 ||| iterative answer prediction with pointer-augmented multimodal transformers for textvqa. ||| ronghang hu ||| amanpreet singh ||| trevor darrell ||| marcus rohrbach ||| 
2021 ||| mega-cda: memory guided attention for category-aware unsupervised domain adaptive object detection. ||| vibashan vs ||| vikram gupta ||| poojan oza ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2019 ||| interpretation of feature space using multi-channel attentional sub-networks. ||| masanari kimura ||| masayuki tanaka ||| 
2019 ||| learning parallax attention for stereo image super-resolution. ||| longguang wang ||| yingqian wang ||| zhengfa liang ||| zaiping lin ||| jun-gang yang ||| wei an ||| yulan guo ||| 
2018 ||| occluded pedestrian detection through guided attention in cnns. ||| shanshan zhang ||| jian yang ||| bernt schiele ||| 
2019 ||| densenet with deep residual channel-attention blocks for single image super resolution. ||| dong-won jang ||| rae-hong park ||| 
2021 ||| delving deep into many-to-many attention for few-shot video object segmentation. ||| haoxin chen ||| hanjie wu ||| nanxuan zhao ||| sucheng ren ||| shengfeng he ||| 
2020 ||| satellite image time series classification with pixel-set encoders and temporal self-attention. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| s ||| bastien giordano ||| nesrine chehata ||| 
2018 ||| progressive attention guided recurrent network for salient object detection. ||| xiaoning zhang ||| tiantian wang ||| jinqing qi ||| huchuan lu ||| gang wang ||| 
2021 ||| clusformer: a transformer based clustering approach to unsupervised large-scale face and visual landmark recognition. ||| xuan-bac nguyen ||| duc toan bui ||| chi nhan duong ||| tien d. bui ||| khoa luu ||| 
2020 ||| scatter: selective context attentional scene text recognizer. ||| ron litman ||| oron anschel ||| shahar tsiper ||| roee litman ||| shai mazor ||| r. manmatha ||| 
2021 ||| attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton. ||| xi zhang ||| xiaolin wu ||| 
2020 ||| one-shot adversarial attacks on visual tracking with dual attention. ||| xuesong chen ||| xiyu yan ||| feng zheng ||| yong jiang ||| shu-tao xia ||| yong zhao ||| rongrong ji ||| 
2019 ||| enhancing salient object segmentation through attention. ||| anuj pahuja ||| avishek majumder ||| anirban chakraborty ||| r. venkatesh babu ||| 
2019 ||| medical time series classification with hierarchical attention-based temporal convolutional networks: a case study of myotonic dystrophy diagnosis. ||| lei lin ||| beilei xu ||| wencheng wu ||| trevor w. richardson ||| edgar a. bernal ||| 
2020 ||| residual channel attention generative adversarial network for image super-resolution and noise reduction. ||| jie cai ||| zibo meng ||| chiu man ho ||| 
2019 ||| scene memory transformer for embodied agents in long-horizon tasks. ||| kuan fang ||| alexander toshev ||| li fei-fei ||| silvio savarese ||| 
2020 ||| focus longer to see better: recursively refined attention for fine-grained image classification. ||| prateek shroff ||| tianlong chen ||| yunchao wei ||| zhangyang wang ||| 
2021 ||| deep rgb-d saliency detection with depth-sensitive attention and automatic multi-modal fusion. ||| peng sun ||| wenhu zhang ||| huanyu wang ||| songyuan li ||| xi li ||| 
2020 ||| vsgnet: spatial attention network for detecting human object interactions using graph convolutions. ||| oytun ulutan ||| a. s. m. iftekhar ||| b. s. manjunath ||| 
2021 ||| micro-expression classification based on landmark relations with graph attention convolutional network. ||| ankith jain rakesh kumar ||| bir bhanu ||| 
2019 ||| end-to-end optimized image compression with attention mechanism. ||| lei zhou ||| zhenhong sun ||| xiangji wu ||| junmin wu ||| 
2020 ||| squeeze-and-attention networks for semantic segmentation. ||| zilong zhong ||| zhong qiu lin ||| rene bidart ||| xiaodan hu ||| ibrahim ben daya ||| zhifeng li ||| wei-shi zheng ||| jonathan li ||| alexander wong ||| 
2021 ||| csanet: high speed channel spatial attention network for mobile isp. ||| ming-chun hsyu ||| chih-wei liu ||| chao-hung chen ||| chao-wei chen ||| wen-chia tsai ||| 
2020 ||| post-processing network based on dense inception attention for video compression. ||| hao tao ||| jian qian ||| li yu ||| hongkui wang ||| wenhao zhang ||| zhengang li ||| ning wang ||| xing zeng ||| 
2018 ||| dual attention matching network for context-aware feature sequence based person re-identification. ||| jianlou si ||| honggang zhang ||| chun-guang li ||| jason kuen ||| xiangfei kong ||| alex c. kot ||| gang wang ||| 
2020 ||| an accurate segmentation-based scene text detector with context attention and repulsive text border. ||| xi liu ||| gaojing zhou ||| rui zhang ||| xiaolin wei ||| 
2021 ||| bgt-net: bidirectional gru transformer network for scene graph generation. ||| naina dhingra ||| florian ritter ||| andreas m. kunz ||| 
2021 ||| skeletor: skeletal transformers for robust body-pose estimation. ||| tao jiang ||| necati cihan camg ||| z ||| richard bowden ||| 
2020 ||| on recognizing texts of arbitrary shapes with 2d self-attention. ||| junyeop lee ||| sungrae park ||| jeonghun baek ||| seong joon oh ||| seonghyeon kim ||| hwalsuk lee ||| 
2018 ||| estimating attention of faces due to its growing level of emotions. ||| ravi kant kumar ||| jogendra garain ||| dakshina ranjan kisku ||| goutam sanyal ||| 
2018 ||| parallel attention: a unified framework for visual object discovery through dialogs and queries. ||| bohan zhuang ||| qi wu ||| chunhua shen ||| ian d. reid ||| anton van den hengel ||| 
2018 ||| going from image to video saliency: augmenting image salience with dynamic attentional push. ||| siavash gorji ||| james j. clark ||| 
2021 ||| end-to-end human object interaction detection with hoi transformer. ||| cheng zou ||| bohan wang ||| yue hu ||| junqi liu ||| qian wu ||| yu zhao ||| boxun li ||| chenguang zhang ||| chi zhang ||| yichen wei ||| jian sun ||| 
2017 ||| attention-aware face hallucination via deep reinforcement learning. ||| qingxing cao ||| liang lin ||| yukai shi ||| xiaodan liang ||| guanbin li ||| 
2020 ||| self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. ||| yude wang ||| jie zhang ||| meina kan ||| shiguang shan ||| xilin chen ||| 
2019 ||| progressive attention memory network for movie story question answering. ||| junyeong kim ||| minuk ma ||| kyungsu kim ||| sungjin kim ||| chang d. yoo ||| 
2019 ||| pcan: 3d attention map learning using contextual information for point cloud based retrieval. ||| wenxiao zhang ||| chunxia xiao ||| 
2020 ||| eca-net: efficient channel attention for deep convolutional neural networks. ||| qilong wang ||| banggu wu ||| pengfei zhu ||| peihua li ||| wangmeng zuo ||| qinghua hu ||| 
2021 ||| adnet: attention-guided deformable convolutional network for high dynamic range imaging. ||| zhen liu ||| wenjie lin ||| xinpeng li ||| qing rao ||| ting jiang ||| mingyan han ||| haoqiang fan ||| jian sun ||| shuaicheng liu ||| 
2020 ||| adaptive graph convolutional network with attention graph clustering for co-saliency detection. ||| kaihua zhang ||| tengpeng li ||| shiwen shen ||| bo liu ||| jin chen ||| qingshan liu ||| 
2017 ||| multi-context attention for human pose estimation. ||| xiao chu ||| wei yang ||| wanli ouyang ||| cheng ma ||| alan l. yuille ||| xiaogang wang ||| 
2021 ||| encoder fusion network with co-attention embedding for referring image segmentation. ||| guang feng ||| zhiwei hu ||| lihe zhang ||| huchuan lu ||| 
2020 ||| few-shot object detection with attention-rpn and multi-relation detector. ||| qi fan ||| wei zhuo ||| chi-keung tang ||| yu-wing tai ||| 
2020 ||| inferring attention shift ranks of objects for image saliency. ||| avishek siris ||| jianbo jiao ||| gary k. l. tam ||| xianghua xie ||| rynson w. h. lau ||| 
2021 ||| lesion-aware transformers for diabetic retinopathy grading. ||| rui sun ||| yihao li ||| tianzhu zhang ||| zhendong mao ||| feng wu ||| yongdong zhang ||| 
2021 ||| gaussian context transformer. ||| dongsheng ruan ||| daiyin wang ||| yuan zheng ||| nenggan zheng ||| min zheng ||| 
2021 ||| variational transformer networks for layout generation. ||| diego mart ||| n arroyo ||| janis postels ||| federico tombari ||| 
2021 ||| connecting what to say with where to look by modeling human attention traces. ||| zihang meng ||| licheng yu ||| ning zhang ||| tamara l. berg ||| babak damavandi ||| vikas singh ||| amy bearman ||| 
2021 ||| line art colorization with concatenated spatial attention. ||| mingcheng yuan ||| edgar simo-serra ||| 
2021 ||| appearance-based gaze estimation using attention and difference mechanism. ||| l. r. d. murthy ||| pradipta biswas ||| 
2017 ||| knowing when to look: adaptive attention via a visual sentinel for image captioning. ||| jiasen lu ||| caiming xiong ||| devi parikh ||| richard socher ||| 
2021 ||| guided interactive video object segmentation using reliability-based attention maps. ||| yuk heo ||| yeong jun koh ||| chang-su kim ||| 
2021 ||| image super-resolution with non-local sparse attention. ||| yiqun mei ||| yuchen fan ||| yuqian zhou ||| 
2020 ||| a shared multi-attention framework for multi-label zero-shot learning. ||| dat huynh ||| ehsan elhamifar ||| 
2021 ||| co-attention for conditioned image matching. ||| olivia wiles ||| s ||| bastien ehrhardt ||| andrew zisserman ||| 
2020 ||| visual parsing with query-driven global graph attention (qd-gga): preliminary results for handwritten math formula recognition. ||| mahshad mahdavi ||| leilei sun ||| richard zanibbi ||| 
2019 ||| attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions. ||| masanori suganuma ||| xing liu ||| takayuki okatani ||| 
2019 ||| hybrid-attention based decoupled metric learning for zero-shot image retrieval. ||| binghui chen ||| weihong deng ||| 
2021 ||| mdmmt: multidomain multimodal transformer for video retrieval. ||| maksim dzabraev ||| maksim kalashnikov ||| stepan komkov ||| aleksandr petiushko ||| 
2021 ||| visual focus of attention estimation in 3d scene with an arbitrary number of targets. ||| r ||| my siegfried ||| jean-marc odobez ||| 
2018 ||| visual grounding via accumulated attention. ||| chaorui deng ||| qi wu ||| qingyao wu ||| fuyuan hu ||| fan lyu ||| mingkui tan ||| 
2019 ||| attentional pointnet for 3d-object detection in point clouds. ||| anshul paigwar ||| zg ||| r erkent ||| christian wolf ||| christian laugier ||| 
2021 ||| causal attention for vision-language tasks. ||| xu yang ||| hanwang zhang ||| guojun qi ||| jianfei cai ||| 
2020 ||| attention mechanism exploits temporal contexts: real-time 3d human pose reconstruction. ||| ruixu liu ||| ju shen ||| he wang ||| chen chen ||| sen-ching s. cheung ||| vijayan k. asari ||| 
2020 ||| attention-guided hierarchical structure aggregation for image matting. ||| yu qiao ||| yuhao liu ||| xin yang ||| dongsheng zhou ||| mingliang xu ||| qiang zhang ||| xiaopeng wei ||| 
2018 ||| end-to-end dense video captioning with masked transformer. ||| luowei zhou ||| yingbo zhou ||| jason j. corso ||| richard socher ||| caiming xiong ||| 
2018 ||| emotional attention: a study of image sentiment and visual attention. ||| shaojing fan ||| zhiqi shen ||| ming jiang ||| bryan l. koenig ||| juan xu ||| mohan s. kankanhalli ||| qi zhao ||| 
2018 ||| improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. ||| duy-kien nguyen ||| takayuki okatani ||| 
2019 ||| aanet: attribute attention network for person re-identifications. ||| chiat-pin tay ||| sharmili roy ||| kim-hui yap ||| 
2021 ||| max-deeplab: end-to-end panoptic segmentation with mask transformers. ||| huiyu wang ||| yukun zhu ||| hartwig adam ||| alan l. yuille ||| liang-chieh chen ||| 
2019 ||| dynamic fusion with intra- and inter-modality attention flow for visual question answering. ||| peng gao ||| zhengkai jiang ||| haoxuan you ||| pan lu ||| steven c. h. hoi ||| xiaogang wang ||| hongsheng li ||| 
2020 ||| attentional bottleneck: towards an interpretable deep driving network. ||| jinkyu kim ||| mayank bansal ||| 
2019 ||| heterogeneous memory enhanced multimodal attention model for video question answering. ||| chenyou fan ||| xiaofan zhang ||| shu zhang ||| wensheng wang ||| chi zhang ||| heng huang ||| 
2020 ||| sketchformer: transformer-based representation for sketched structure. ||| leo sampaio ferraz ribeiro ||| tu bui ||| john p. collomosse ||| moacir ponti ||| 
2021 ||| taming transformers for high-resolution image synthesis. ||| patrick esser ||| robin rombach ||| bj ||| rn ommer ||| 
2020 ||| actor-transformers for group activity recognition. ||| kirill gavrilyuk ||| ryan sanford ||| mehrsan javan ||| cees g. m. snoek ||| 
2021 ||| rethinking the self-attention in vision transformers. ||| kyungmin kim ||| bichen wu ||| xiaoliang dai ||| peizhao zhang ||| zhicheng yan ||| peter vajda ||| seon joo kim ||| 
2021 ||| symmetric parallax attention for stereo image super-resolution. ||| yingqian wang ||| xinyi ying ||| longguang wang ||| jungang yang ||| wei an ||| yulan guo ||| 
2021 ||| improving accuracy of respiratory rate estimation by restoring high resolution features with transformers and recursive convolutional models. ||| alicja kwasniewska ||| maciej szankin ||| jacek ruminski ||| anthony sarah ||| david gamba ||| 
2020 ||| sct: set constrained temporal transformer for set supervised action segmentation. ||| mohsen fayyaz ||| j ||| rgen gall ||| 
2019 ||| see more, know more: unsupervised video object segmentation with co-attention siamese networks. ||| xiankai lu ||| wenguan wang ||| chao ma ||| jianbing shen ||| ling shao ||| fatih porikli ||| 
2019 ||| channel attention networks. ||| alexei bastidas ||| hanlin tang ||| 
2019 ||| on attention modules for audio-visual synchronization. ||| naji khosravan ||| shervin ardeshir ||| rohit puri ||| 
2020 ||| exploring self-attention for image recognition. ||| hengshuang zhao ||| jiaya jia ||| vladlen koltun ||| 
2020 ||| predicting goal-directed human attention using inverse reinforcement learning. ||| zhibo yang ||| lihan huang ||| yupei chen ||| zijun wei ||| seoyoung ahn ||| gregory j. zelinsky ||| dimitris samaras ||| minh hoai ||| 
2020 ||| fine-grained generalized zero-shot learning via dense attribute-based attention. ||| dat huynh ||| ehsan elhamifar ||| 
2019 ||| improved automating seismic facies analysis using deep dilated attention autoencoders. ||| zengyan wang ||| fangyu li ||| thiab r. taha ||| hamid r. arabnia ||| 
2018 ||| mask-guided contrastive attention model for person re-identification. ||| chunfeng song ||| yan huang ||| wanli ouyang ||| liang wang ||| 
2018 ||| tell me where to look: guided attention inference network. ||| kunpeng li ||| ziyan wu ||| kuan-chuan peng ||| jan ernst ||| yun fu ||| 
2019 ||| video action transformer network. ||| rohit girdhar ||| jo ||| o carreira ||| carl doersch ||| andrew zisserman ||| 
2021 ||| diverse part discovery: occluded person re-identification with part-aware transformer. ||| yulin li ||| jianfeng he ||| tianzhu zhang ||| xiang liu ||| yongdong zhang ||| feng wu ||| 
2019 ||| neighbourhood watch: referring expression comprehension via language-guided graph attention networks. ||| peng wang ||| qi wu ||| jiewei cao ||| chunhua shen ||| lianli gao ||| anton van den hengel ||| 
2018 ||| structured attention guided convolutional neural fields for monocular depth estimation. ||| dan xu ||| wei wang ||| hao tang ||| hong liu ||| nicu sebe ||| elisa ricci ||| 
2018 ||| pay attention to virality: understanding popularity of social media videos with the attention mechanism. ||| adam bielski ||| tomasz trzcinski ||| 
2019 ||| deep modular co-attention networks for visual question answering. ||| zhou yu ||| jun yu ||| yuhao cui ||| dacheng tao ||| qi tian ||| 
2020 ||| visual-semantic matching by exploring high-order attention and distraction. ||| yongzhi li ||| duo zhang ||| yadong mu ||| 
2019 ||| attention branch network: learning of attention mechanism for visual explanation. ||| hiroshi fukui ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2018 ||| jersey number recognition with semi-supervised spatial transformer network. ||| gen li ||| shikun xu ||| xiang liu ||| lei li ||| changhu wang ||| 
2019 ||| modeling point clouds with self-attention and gumbel subset sampling. ||| jiancheng yang ||| qiang zhang ||| bingbing ni ||| linguo li ||| jinxian liu ||| mengdie zhou ||| qi tian ||| 
2017 ||| multi-level attention networks for visual question answering. ||| dongfei yu ||| jianlong fu ||| tao mei ||| yong rui ||| 
2021 ||| embedded discriminative attention mechanism for weakly supervised semantic segmentation. ||| tong wu ||| junshi huang ||| guangyu gao ||| xiaoming wei ||| xiaolin wei ||| xuan luo ||| chi harold liu ||| 
2017 ||| dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting. ||| zhen-hua feng ||| josef kittler ||| william j. christmas ||| patrik huber ||| xiaojun wu ||| 
2017 ||| generating the future with adversarial transformers. ||| carl vondrick ||| antonio torralba ||| 
2020 ||| image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. ||| yiqun mei ||| yuchen fan ||| yuqian zhou ||| lichao huang ||| thomas s. huang ||| honghui shi ||| 
2021 ||| pose recognition with cascade transformers. ||| ke li ||| shijie wang ||| xiang zhang ||| yifan xu ||| weijian xu ||| zhuowen tu ||| 
2019 ||| shifting more attention to video salient object detection. ||| deng-ping fan ||| wenguan wang ||| ming-ming cheng ||| jianbing shen ||| 
2021 ||| scaling local self-attention for parameter efficient visual backbones. ||| ashish vaswani ||| prajit ramachandran ||| aravind srinivas ||| niki parmar ||| blake a. hechtman ||| jonathon shlens ||| 
2021 ||| hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. ||| mingyu ding ||| xiaochen lian ||| linjie yang ||| peng wang ||| xiaojie jin ||| zhiwu lu ||| ping luo ||| 
2018 ||| harmonious attention network for person re-identification. ||| wei li ||| xiatian zhu ||| shaogang gong ||| 
2021 ||| mrscatt: a spatio-channel attention-guided network for mars rover image classification. ||| anirudh srinivasan chakravarthy ||| roshan roy ||| praveen ravirathinam ||| 
2021 ||| pre-trained image processing transformer. ||| hanting chen ||| yunhe wang ||| tianyu guo ||| chang xu ||| yiping deng ||| zhenhua liu ||| siwei ma ||| chunjing xu ||| chao xu ||| wen gao ||| 
2020 ||| epipolar transformer for multi-view human pose estimation. ||| yihui he ||| rui yan ||| katerina fragkiadaki ||| shoou-i yu ||| 
2021 ||| multi-attentional deepfake detection. ||| hanqing zhao ||| wenbo zhou ||| dongdong chen ||| tianyi wei ||| weiming zhang ||| nenghai yu ||| 
2020 ||| sketch-bert: learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. ||| hangyu lin ||| yanwei fu ||| xiangyang xue ||| yu-gang jiang ||| 
2021 ||| mist: multiple instance spatial transformer. ||| baptiste angles ||| yuhe jin ||| simon kornblith ||| andrea tagliasacchi ||| kwang moo yi ||| 
2019 ||| attention based glaucoma detection: a large-scale database and cnn model. ||| liu li ||| mai xu ||| xiaofei wang ||| lai jiang ||| hanruo liu ||| 
2019 ||| event-based attention and tracking on neuromorphic hardware. ||| alpha renner ||| matthew evanusa ||| yulia sandamirskaya ||| 
2019 ||| arbitrary style transfer with style-attentional networks. ||| dae young park ||| kwang hee lee ||| 
2019 ||| predicting methylation from sequence and gene expression using deep learning with attention. ||| alona levy-jurgenson ||| xavier tekpli ||| vessela n. kristensen ||| zohar yakhini ||| 
2021 ||| diverse image inpainting with bidirectional and autoregressive transformers. ||| yingchen yu ||| fangneng zhan ||| rongliang wu ||| jianxiong pan ||| kaiwen cui ||| shijian lu ||| feiying ma ||| xuansong xie ||| chunyan miao ||| 
2021 ||| multimodal sentiment analysis based on recurrent neural network and multimodal attention. ||| cong cai ||| yu he ||| licai sun ||| zheng lian ||| bin liu ||| jianhua tao ||| mingyu xu ||| kexin wang ||| 
2018 ||| multi-scale context attention network for image retrieval. ||| yihang lou ||| yan bai ||| shiqi wang ||| ling-yu duan ||| 
2018 ||| csan: contextual self-attention network for user sequential recommendation. ||| xiaowen huang ||| shengsheng qian ||| quan fang ||| jitao sang ||| changsheng xu ||| 
2021 ||| fingerspelling recognition in the wild with fixed-query based visual attention. ||| srinivas kruthiventi s. s ||| george jose ||| nitya tandon ||| rajesh roshan biswal ||| aashish kumar ||| 
2020 ||| fine-grained iterative attention network for temporal language localization in videos. ||| xiaoye qu ||| pengwei tang ||| zhikang zou ||| yu cheng ||| jianfeng dong ||| pan zhou ||| zichuan xu ||| 
2021 ||| video background music generation with controllable music transformer. ||| shangzhe di ||| zeren jiang ||| si liu ||| zhaokai wang ||| leyan zhu ||| zexin he ||| hongming liu ||| shuicheng yan ||| 
2021 ||| gccn: geometric constraint co-attention network for 6d object pose estimation. ||| yongming wen ||| yiquan fang ||| junhao cai ||| kimwa tung ||| hui cheng ||| 
2019 ||| cra-net: composed relation attention network for visual question answering. ||| liang peng ||| yang yang ||| zheng wang ||| xiao wu ||| zi huang ||| 
2018 ||| attention-based multi-patch aggregation for image aesthetic assessment. ||| kekai sheng ||| weiming dong ||| chongyang ma ||| xing mei ||| feiyue huang ||| bao-gang hu ||| 
2020 ||| multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism. ||| licai sun ||| zheng lian ||| jianhua tao ||| bin liu ||| mingyue niu ||| 
2021 ||| tsa-net: tube self-attention network for action quality assessment. ||| shunli wang ||| dingkang yang ||| peng zhai ||| chixiao chen ||| lihua zhang ||| 
2021 ||| attention-driven graph clustering network. ||| zhihao peng ||| hui liu ||| yuheng jia ||| junhui hou ||| 
2017 ||| video description with spatial-temporal attention. ||| yunbin tu ||| xishan zhang ||| bingtao liu ||| chenggang yan ||| 
2019 ||| small and dense commodity object detection with multi-scale receptive field attention. ||| zhong ji ||| qiankun kong ||| haoran wang ||| yanwei pang ||| 
2021 ||| underwater species detection using channel sharpening attention. ||| lihao jiang ||| yi wang ||| qi jia ||| shengwei xu ||| yu liu ||| xin fan ||| haojie li ||| risheng liu ||| xinwei xue ||| ruili wang ||| 
2021 ||| vehicle counting network with attention-based mask refinement and spatial-awareness block loss. ||| ji zhang ||| jian-jun qiao ||| xiao wu ||| wei li ||| 
2021 ||| video relation detection via tracklet based visual transformer. ||| kaifeng gao ||| long chen ||| yifeng huang ||| jun xiao ||| 
2021 ||| simullr: simultaneous lip reading transducer with attention-guided adaptive memory. ||| zhijie lin ||| zhou zhao ||| haoyuan li ||| jinglin liu ||| meng zhang ||| xingshan zeng ||| xiaofei he ||| 
2018 ||| your attention is unique: detecting 360-degree video saliency in head-mounted display for head movement prediction. ||| anh nguyen ||| zhisheng yan ||| klara nahrstedt ||| 
2019 ||| multi-level attention network using text, audio and video for depression prediction. ||| anupama ray ||| siddharth kumar ||| rutvik reddy ||| prerana mukherjee ||| ritu garg ||| 
2019 ||| pdanet: polarity-consistent deep attention network for fine-grained visual emotion regression. ||| sicheng zhao ||| zizhou jia ||| hui chen ||| leida li ||| guiguang ding ||| kurt keutzer ||| 
2021 ||| long short-term convolutional transformer for no-reference video quality assessment. ||| junyong you ||| 
2018 ||| twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm. ||| yuxiao chen ||| jianbo yuan ||| quanzeng you ||| jiebo luo ||| 
2020 ||| object-level attention for aesthetic rating distribution prediction. ||| jingwen hou ||| sheng yang ||| weisi lin ||| 
2018 ||| facial expression recognition in the wild: a cycle-consistent adversarial attention transfer approach. ||| feifei zhang ||| tianzhu zhang ||| qirong mao ||| lingyu duan ||| changsheng xu ||| 
2021 ||| distributed attention for grounded image captioning. ||| nenglun chen ||| xingjia pan ||| runnan chen ||| lei yang ||| zhiwen lin ||| yuqiang ren ||| haolei yuan ||| xiaowei guo ||| feiyue huang ||| wenping wang ||| 
2020 ||| one-shot text field labeling using attention and belief propagation for structure information extraction. ||| mengli cheng ||| minghui qiu ||| xing shi ||| jun huang ||| wei lin ||| 
2020 ||| attention based dual branches fingertip detection network and virtual key system. ||| chong mou ||| xin zhang ||| 
2017 ||| learning deep contextual attention network for narrative photo stream captioning. ||| hanqi wang ||| siliang tang ||| yin zhang ||| tao mei ||| yueting zhuang ||| fei wu ||| 
2019 ||| fine-grained cross-media representation learning with deep quantization attention network. ||| meiyu liang ||| junping du ||| wu liu ||| zhe xue ||| yue geng ||| cong-xian yang ||| 
2021 ||| m3tr: multi-modal multi-label recognition with transformer. ||| jiawei zhao ||| yifan zhao ||| jia li ||| 
2020 ||| privacy-preserving visual content tagging using graph transformer networks. ||| xuan-son vu ||| duc-trong le ||| christoffer edlund ||| lili jiang ||| hoang d. nguyen ||| 
2020 ||| cross-modal non-linear guided attention and temporal coherence in multi-modal deep video models. ||| saurabh sahu ||| palash goyal ||| shalini ghosh ||| chul lee ||| 
2019 ||| critic-based attention network for event-based video captioning. ||| elaheh barati ||| xuewen chen ||| 
2017 ||| attention transfer from web images for video recognition. ||| junnan li ||| yongkang wong ||| qi zhao ||| mohan s. kankanhalli ||| 
2018 ||| content-based video relevance prediction with second-order relevance and attention modeling. ||| xusong chen ||| rui zhao ||| shengjie ma ||| dong liu ||| zheng-jun zha ||| 
2017 ||| watch what you just said: image captioning with text-conditional attention. ||| luowei zhou ||| chenliang xu ||| parker a. koch ||| jason j. corso ||| 
2021 ||| group-based distinctive image captioning with memory attention. ||| jiuniu wang ||| wenjia xu ||| qingzhong wang ||| antoni b. chan ||| 
2021 ||| dpt: deformable patch-based transformer for visual recognition. ||| zhiyang chen ||| yousong zhu ||| chaoyang zhao ||| guosheng hu ||| wei zeng ||| jinqiao wang ||| ming tang ||| 
2021 ||| recursive fusion and deformable spatiotemporal attention for video compression artifact reduction. ||| minyi zhao ||| yi xu ||| shuigeng zhou ||| 
2021 ||| transformer-based feature reconstruction network for robust multimodal sentiment analysis. ||| ziqi yuan ||| wei li ||| hua xu ||| wenmeng yu ||| 
2019 ||| l2g auto-encoder: understanding point clouds by local-to-global reconstruction with hierarchical self-attention. ||| xinhai liu ||| zhizhong han ||| xin wen ||| yu-shen liu ||| matthias zwicker ||| 
2017 ||| cross-domain image retrieval with attention modeling. ||| xin ji ||| wei wang ||| meihui zhang ||| yang yang ||| 
2020 ||| cluster attention contrast for video anomaly detection. ||| ziming wang ||| yuexian zou ||| zeming zhang ||| 
2019 ||| aberrance-aware gradient-sensitive attentions for scene recognition with rgb-d videos. ||| xinhang song ||| sixian zhang ||| yuyun hua ||| shuqiang jiang ||| 
2020 ||| hierarchical gumbel attention network for text-based person search. ||| kecheng zheng ||| wu liu ||| jiawei liu ||| zheng-jun zha ||| tao mei ||| 
2020 ||| beyond the attention: distinguish the discriminative and confusable features for fine-grained image classification. ||| xiruo shi ||| liutong xu ||| pengfei wang ||| yuanyuan gao ||| haifang jian ||| wu liu ||| 
2020 ||| compact bilinear augmented query structured attention for sport highlights classification. ||| yanbin hao ||| hao zhang ||| chong-wah ngo ||| qiang liu ||| xiaojun hu ||| 
2020 ||| hot-net: non-autoregressive transformer for 3d hand-object pose estimation. ||| lin huang ||| jianchao tan ||| jingjing meng ||| ji liu ||| junsong yuan ||| 
2019 ||| attention transfer (ant) network for view-invariant action recognition. ||| yanli ji ||| feixiang xu ||| yang yang ||| ning xie ||| heng tao shen ||| tatsuya harada ||| 
2019 ||| fine-grained fitting experience prediction: a 3d-slicing attention approach. ||| shan huang ||| zhi wang ||| laizhong cui ||| yong jiang ||| rui gao ||| 
2020 ||| guided attention network for object detection and counting on drones. ||| yuanqiang cai ||| dawei du ||| libo zhang ||| longyin wen ||| weiqiang wang ||| yanjun wu ||| siwei lyu ||| 
2021 ||| yes, "attention is all you need", for exemplar based colorization. ||| wang yin ||| peng lu ||| zhaoran zhao ||| xujun peng ||| 
2017 ||| image caption with synchronous cross-attention. ||| yue wang ||| jinlai liu ||| xiaojie wang ||| 
2020 ||| multimodal attention with image text spatial relationship for ocr-based image captioning. ||| jing wang ||| jinhui tang ||| jiebo luo ||| 
2020 ||| attention cube network for image restoration. ||| yucheng hang ||| qingmin liao ||| wenming yang ||| yupeng chen ||| jie zhou ||| 
2021 ||| multiview detection with shadow transformer (and view-coherent data augmentation). ||| yunzhong hou ||| liang zheng ||| 
2019 ||| self-attention and ingredient-attention based model for recipe retrieval from image queries. ||| matthias fontanellaz ||| stergios christodoulidis ||| stavroula g. mougiakakou ||| 
2021 ||| attention-guided temporally coherent video object matting. ||| yunke zhang ||| chi wang ||| miaomiao cui ||| peiran ren ||| xuansong xie ||| xian-sheng hua ||| hujun bao ||| qixing huang ||| weiwei xu ||| 
2020 ||| deeprhythm: exposing deepfakes with attentional visual heartbeat rhythms. ||| hua qi ||| qing guo ||| felix juefei-xu ||| xiaofei xie ||| lei ma ||| wei feng ||| yang liu ||| jianjun zhao ||| 
2021 ||| multi-label pattern image retrieval via attention mechanism driven graph convolutional network. ||| ying li ||| hongwei zhou ||| yeyu yin ||| jiaquan gao ||| 
2021 ||| combining attention with flow for person image synthesis. ||| yurui ren ||| yubo wu ||| thomas h. li ||| shan liu ||| ge li ||| 
2019 ||| gastrointestinal tract diseases detection with deep attention neural network. ||| yuan chang ||| zixuan huang ||| weizhao chen ||| qiwei shen ||| 
2020 ||| curriculum learning for wide multimedia-based transformer with graph target detection. ||| weilong chen ||| feng hong ||| chenghao huang ||| shaoliang zhang ||| rui wang ||| ruobing xie ||| feng xia ||| leyu lin ||| yanru zhang ||| yan wang ||| 
2019 ||| action recognition with bootstrapping based long-range temporal context attention. ||| ziming liu ||| guangyu gao ||| a. kai qin ||| tong wu ||| chi harold liu ||| 
2021 ||| cascade cross-modal attention network for video actor and action segmentation from a sentence. ||| weidong chen ||| guorong li ||| xinfeng zhang ||| hongyang yu ||| shuhui wang ||| qingming huang ||| 
2017 ||| manet: a modal attention network for describing videos. ||| sang phan ||| yusuke miyao ||| shin'ichi satoh ||| 
2017 ||| learning multimodal attention lstm networks for video captioning. ||| jun xu ||| ting yao ||| yongdong zhang ||| tao mei ||| 
2020 ||| multimodal deep learning for social media popularity prediction with attention mechanism. ||| kele xu ||| zhimin lin ||| jianqiao zhao ||| peichang shi ||| wei deng ||| huaimin wang ||| 
2021 ||| pixel-wise graph attention networks for person re-identification. ||| wenyu zhang ||| qing ding ||| jian hu ||| yi ma ||| mingzhe lu ||| 
2020 ||| transformer-based label set generation for multi-modal multi-label emotion detection. ||| xincheng ju ||| dong zhang ||| junhui li ||| guodong zhou ||| 
2021 ||| few-shot fine-grained action recognition via bidirectional attention and contrastive meta-learning. ||| jiahao wang ||| yunhong wang ||| sheng liu ||| annan li ||| 
2021 ||| former-dfer: dynamic facial expression recognition transformer. ||| zengqun zhao ||| qingshan liu ||| 
2020 ||| pay attention selectively and comprehensively: pyramid gating network for human pose estimation without pre-training. ||| chenru jiang ||| kaizhu huang ||| shufei zhang ||| xinheng wang ||| jimin xiao ||| 
2020 ||| multi-group multi-attention: towards discriminative spatiotemporal representation. ||| zhensheng shi ||| liangjie cao ||| cheng guan ||| ju liang ||| qianqian li ||| zhaorui gu ||| haiyong zheng ||| bing zheng ||| 
2020 ||| unsupervised representation learning with attention and sequence to sequence autoencoders to predict sleepiness from speech. ||| shahin amiriparian ||| pawel winokurow ||| vincent karas ||| sandra ottl ||| maurice gerczuk ||| bj ||| rn w. schuller ||| 
2018 ||| examine before you answer: multi-task learning with adaptive-attentions for multiple-choice vqa. ||| lianli gao ||| pengpeng zeng ||| jingkuan song ||| xianglong liu ||| heng tao shen ||| 
2020 ||| au-assisted graph attention convolutional network for micro-expression recognition. ||| hong-xia xie ||| ling lo ||| hong-han shuai ||| wen-huang cheng ||| 
2019 ||| audiovisual transformer architectures for large-scale classification and synchronization of weakly labeled audio events. ||| wim boes ||| hugo van hamme ||| 
2021 ||| stst: spatial-temporal specialized transformer for skeleton-based action recognition. ||| yuhan zhang ||| bo wu ||| wen li ||| lixin duan ||| chuang gan ||| 
2020 ||| sequential attention gan for interactive image editing. ||| yu cheng ||| zhe gan ||| yitong li ||| jingjing liu ||| jianfeng gao ||| 
2020 ||| hybrid dynamic-static context-aware attention network for action assessment in long videos. ||| ling-an zeng ||| fa-ting hong ||| wei-shi zheng ||| qi-zhi yu ||| wei zeng ||| yao-wei wang ||| jian-huang lai ||| 
2017 ||| multi-scale context based attention for dynamic music emotion prediction. ||| ye ma ||| xinxing li ||| mingxing xu ||| jia jia ||| lianhong cai ||| 
2021 ||| convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. ||| xinyang feng ||| dongjin song ||| yuncong chen ||| zhengzhang chen ||| jingchao ni ||| haifeng chen ||| 
2018 ||| new feature-level video classification via temporal attention model. ||| hongje seong ||| junhyuk hyun ||| suhyeon lee ||| suhan woo ||| hyunbae chang ||| euntai kim ||| 
2019 ||| multi-modal knowledge-aware hierarchical attention network for explainable medical question answering. ||| yingying zhang ||| shengsheng qian ||| quan fang ||| changsheng xu ||| 
2020 ||| codan: counting-driven attention network for vehicle detection in congested scenes. ||| wei li ||| zhenting wang ||| xiao wu ||| ji zhang ||| qiang peng ||| hongliang li ||| 
2020 ||| perceptual characterization of 3d graphical contents based on attention complexity measures. ||| mona abid ||| matthieu perreira da silva ||| patrick le callet ||| 
2021 ||| direction relation transformer for image captioning. ||| zeliang song ||| xiaofei zhou ||| linhua dong ||| jianlong tan ||| li guo ||| 
2020 ||| jointly cross- and self-modal graph attention network for query-based moment localization. ||| daizong liu ||| xiaoye qu ||| xiao-yang liu ||| jianfeng dong ||| pan zhou ||| zichuan xu ||| 
2021 ||| progressive graph attention network for video question answering. ||| liang peng ||| shuangji yang ||| yi bin ||| guoqing wang ||| 
2020 ||| query twice: dual mixture attention meta learning for video summarization. ||| junyan wang ||| yang bai ||| yang long ||| bingzhang hu ||| zhenhua chai ||| yu guan ||| xiaolin wei ||| 
2021 ||| dual graph convolutional networks with transformer and curriculum learning for image captioning. ||| xinzhi dong ||| chengjiang long ||| wenju xu ||| chunxia xiao ||| 
2021 ||| multimodal video summarization via time-aware transformers. ||| xindi shang ||| zehuan yuan ||| anran wang ||| changhu wang ||| 
2017 ||| modeling image virality with pairwise spatial transformer networks. ||| abhimanyu dubey ||| sumeet agarwal ||| 
2018 ||| object-difference attention: a simple relational attention for visual question answering. ||| chenfei wu ||| jinlai liu ||| xiaojie wang ||| xuan dong ||| 
2019 ||| ingredient-guided cascaded multi-attention network for food recognition. ||| weiqing min ||| linhu liu ||| zhengdong luo ||| shuqiang jiang ||| 
2017 ||| the role of visual attention in sentiment prediction. ||| shaojing fan ||| ming jiang ||| zhiqi shen ||| bryan l. koenig ||| mohan s. kankanhalli ||| qi zhao ||| 
2021 ||| end-to-end video object detection with spatial-temporal transformers. ||| lu he ||| qianyu zhou ||| xiangtai li ||| li niu ||| guangliang cheng ||| xiao li ||| wenxuan liu ||| yunhai tong ||| lizhuang ma ||| liqing zhang ||| 
2019 ||| understanding the teaching styles by an attention based multi-task cross-media dimensional modeling. ||| suping zhou ||| jia jia ||| yufeng yin ||| xiang li ||| yang yao ||| ying zhang ||| zeyang ye ||| kehua lei ||| yan huang ||| jialie shen ||| 
2020 ||| alanet: adaptive latent attention network for joint video deblurring and interpolation. ||| akash gupta ||| abhishek aich ||| amit k. roy-chowdhury ||| 
2021 ||| ftaface: context-enhanced face detector with fine-grained task attention. ||| deyu wang ||| dongchao wen ||| wei tao ||| lingxiao yin ||| tse-wei chen ||| tadayuki ito ||| kinya osa ||| masami kato ||| 
2021 ||| sensor-augmented egocentric-video captioning with dynamic modal attention. ||| katsuyuki nakamura ||| hiroki ohashi ||| mitsuhiro okada ||| 
2019 ||| deep adversarial graph attention convolution network for text-based person search. ||| jiawei liu ||| zheng-jun zha ||| richang hong ||| meng wang ||| yongdong zhang ||| 
2017 ||| video question answering via hierarchical dual-level attention network learning. ||| zhou zhao ||| jinghao lin ||| xinghua jiang ||| deng cai ||| xiaofei he ||| yueting zhuang ||| 
2021 ||| multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model. ||| licai sun ||| mingyu xu ||| zheng lian ||| bin liu ||| jianhua tao ||| meng wang ||| yuan cheng ||| 
2019 ||| erasing-based attention learning for visual question answering. ||| fei liu ||| jing liu ||| richang hong ||| hanqing lu ||| 
2019 ||| joint-attention discriminator for accurate super-resolution via adversarial training. ||| rong chen ||| yuan xie ||| xiaotong luo ||| yanyun qu ||| cuihua li ||| 
2021 ||| latent memory-augmented graph transformer for visual storytelling. ||| mengshi qi ||| jie qin ||| di huang ||| zhiqiang shen ||| yi yang ||| jiebo luo ||| 
2021 ||| exploring sequence feature alignment for domain adaptive detection transformers. ||| wen wang ||| yang cao ||| jing zhang ||| fengxiang he ||| zheng-jun zha ||| yonggang wen ||| dacheng tao ||| 
2020 ||| asta-net: adaptive spatio-temporal attention network for person re-identification in videos. ||| xierong zhu ||| jiawei liu ||| haoze wu ||| meng wang ||| zheng-jun zha ||| 
2018 ||| regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval. ||| zehang lin ||| zhenguo yang ||| feitao huang ||| junhong chen ||| 
2019 ||| visual relation detection with multi-level attention. ||| sipeng zheng ||| shizhe chen ||| qin jin ||| 
2021 ||| anchor-free 3d single stage detector with mask-guided attention for point cloud. ||| jiale li ||| hang dai ||| ling shao ||| yong ding ||| 
2019 ||| dadnet: dilated-attention-deformable convnet for crowd counting. ||| dan guo ||| kun li ||| zheng-jun zha ||| meng wang ||| 
2020 ||| joint self-attention and scale-aggregation for self-calibrated deraining network. ||| cong wang ||| yutong wu ||| zhixun su ||| junyang chen ||| 
2017 ||| temporally selective attention model for social and affective state recognition in multimedia content. ||| hongliang yu ||| liangke gui ||| michael a. madaio ||| amy ogan ||| justine cassell ||| louis-philippe morency ||| 
2021 ||| spatio-temporal convolutional attention network for spotting macro- and micro-expression intervals. ||| hang pan ||| lun xie ||| zhiliang wang ||| 
2018 ||| visual spatial attention network for relationship detection. ||| chaojun han ||| fumin shen ||| li liu ||| yang yang ||| heng tao shen ||| 
2019 ||| pedestrian attribute recognition via hierarchical multi-task learning and relationship attention. ||| lian gao ||| di huang ||| yuanfang guo ||| yunhong wang ||| 
2021 ||| multifocal attention-based cross-scale network for image de-raining. ||| zheyu zhang ||| yurui zhu ||| xueyang fu ||| zhiwei xiong ||| zheng-jun zha ||| feng wu ||| 
2021 ||| heterogeneous face recognition with attention-guided feature disentangling. ||| shanmin yang ||| xiao yang ||| yi lin ||| peng cheng ||| yi zhang ||| jianwei zhang ||| 
2021 ||| tritransnet: rgb-d salient object detection with a triplet transformer embedding network. ||| zhengyi liu ||| yuan wang ||| zhengzheng tu ||| yun xiao ||| bin tang ||| 
2020 ||| multi-scale generalized attention-based regional maximum activation of convolutions for beauty product retrieval. ||| kele xu ||| yuzhong liu ||| ming feng ||| jianqiao zhao ||| huaimin wang ||| hengxing cai ||| 
2019 ||| multi-level fusion based class-aware attention model for weakly labeled audio tagging. ||| yifang yin ||| meng-jiun chiou ||| zhenguang liu ||| harsh shrivastava ||| rajiv ratn shah ||| roger zimmermann ||| 
2021 ||| information-growth attention network for image super-resolution. ||| zhuangzi li ||| ge li ||| thomas h. li ||| shan liu ||| wei gao ||| 
2021 ||| pre-training graph transformer with multimodal side information for recommendation. ||| yong liu ||| susen yang ||| chenyi lei ||| guoxin wang ||| haihong tang ||| juyong zhang ||| aixin sun ||| chunyan miao ||| 
2021 ||| token shift transformer for video classification. ||| hao zhang ||| yanbin hao ||| chong-wah ngo ||| 
2021 ||| rams-trans: recurrent attention multi-scale transformer for fine-grained image recognition. ||| yunqing hu ||| xuan jin ||| yin zhang ||| haiwen hong ||| jingfeng zhang ||| yuan he ||| hui xue ||| 
2020 ||| pop music transformer: beat-based modeling and generation of expressive pop piano compositions. ||| yu-siang huang ||| yi-hsuan yang ||| 
2020 ||| exploring language prior for mode-sensitive visual attention modeling. ||| xiaoshuai sun ||| xuying zhang ||| liujuan cao ||| yongjian wu ||| feiyue huang ||| rongrong ji ||| 
2021 ||| position-augmented transformers with entity-aligned mesh for textvqa. ||| xuanyu zhang ||| qing yang ||| 
2021 ||| unifying multimodal transformer for bi-directional image and text generation. ||| yupan huang ||| hongwei xue ||| bei liu ||| yutong lu ||| 
2020 ||| dual-view attention networks for single image super-resolution. ||| jingcai guo ||| shiheng ma ||| jie zhang ||| qihua zhou ||| song guo ||| 
2020 ||| sst-emotionnet: spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition. ||| ziyu jia ||| youfang lin ||| xiyang cai ||| haobin chen ||| haijun gou ||| jing wang ||| 
2021 ||| group-level focus of visual attention for improved next speaker prediction. ||| chris birmingham ||| kalin stefanov ||| maja j. mataric ||| 
2021 ||| hat: hierarchical aggregation transformers for person re-identification. ||| guowen zhang ||| pingping zhang ||| jinqing qi ||| huchuan lu ||| 
2019 ||| what i see is what you see: joint attention learning for first and third person video co-analysis. ||| huangyue yu ||| minjie cai ||| yunfei liu ||| feng lu ||| 
2021 ||| a transformer based approach for image manipulation chain detection. ||| jiaxiang you ||| yuanman li ||| jiantao zhou ||| zhongyun hua ||| weiwei sun ||| xia li ||| 
2021 ||| video semantic segmentation via sparse temporal transformer. ||| jiangtong li ||| wentao wang ||| junjie chen ||| li niu ||| jianlou si ||| chen qian ||| liqing zhang ||| 
2021 ||| knowing when to quit: selective cascaded regression with patch attention for real-time face alignment. ||| gil shapira ||| noga levy ||| ishay goldin ||| roy josef jevnisek ||| 
2018 ||| attention and language ensemble for scene text recognition with convolutional sequence modeling. ||| shancheng fang ||| hongtao xie ||| zheng-jun zha ||| nannan sun ||| jianlong tan ||| yongdong zhang ||| 
2020 ||| look, listen, and attend: co-attention network for self-supervised audio-visual representation learning. ||| ying cheng ||| ruize wang ||| zhihao pan ||| rui feng ||| yuejie zhang ||| 
2020 ||| cascade grouped attention network for referring expression segmentation. ||| gen luo ||| yiyi zhou ||| rongrong ji ||| xiaoshuai sun ||| jinsong su ||| chia-wen lin ||| qi tian ||| 
2020 ||| attention-driven unsupervised image retrieval for beauty products with visual and textual clues. ||| jingwen hou ||| sijie ji ||| annan wang ||| 
2021 ||| hda-net: horizontal deformable attention network for stereo matching. ||| qi zhang ||| xuesong zhang ||| baoping li ||| yuzhong chen ||| anlong ming ||| 
2021 ||| video transformer for deepfake detection with incremental learning. ||| sohail ahmed khan ||| hang dai ||| 
2020 ||| single image deraining via scale-space invariant attention neural network. ||| bo pang ||| deming zhai ||| junjun jiang ||| xianming liu ||| 
2019 ||| focus your attention: a bidirectional focal attention network for image-text matching. ||| chunxiao liu ||| zhendong mao ||| an-an liu ||| tianzhu zhang ||| bin wang ||| yongdong zhang ||| 
2021 ||| learning hierarchal channel attention for fine-grained visual classification. ||| xiang guan ||| guoqing wang ||| xing xu ||| yi bin ||| 
2020 ||| cf-sis: semantic-instance segmentation of 3d point clouds by context fusion with self-attention. ||| xin wen ||| zhizhong han ||| geunhyuk youk ||| yu-shen liu ||| 
2021 ||| mix-order attention networks for image restoration. ||| tao dai ||| yalei lv ||| bin chen ||| zhi wang ||| zexuan zhu ||| shu-tao xia ||| 
2021 ||| structext: structured text understanding with multi-modal transformers. ||| yulin li ||| yuxi qian ||| yuechen yu ||| xiameng qin ||| chengquan zhang ||| yan liu ||| kun yao ||| junyu han ||| jingtuo liu ||| errui ding ||| 
2021 ||| zero-shot video emotion recognition via multimodal protagonist-aware transformer network. ||| fan qi ||| xiaoshan yang ||| changsheng xu ||| 
2017 ||| learning social image embedding with deep multimodal attention networks. ||| feiran huang ||| xiaoming zhang ||| zhoujun li ||| tao mei ||| yueying he ||| zhonghua zhao ||| 
2020 ||| unpaired image enhancement with quality-attention generative adversarial network. ||| zhangkai ni ||| wenhan yang ||| shiqi wang ||| lin ma ||| sam kwong ||| 
2020 ||| isia food-500: a dataset for large-scale food recognition via stacked global-local attention network. ||| weiqing min ||| linhu liu ||| zhiling wang ||| zhengdong luo ||| xiaoming wei ||| xiaolin wei ||| shuqiang jiang ||| 
2018 ||| net: contextual-attentional attribute-appearance network for person re-identification. ||| jiawei liu ||| zheng-jun zha ||| hongtao xie ||| zhiwei xiong ||| yongdong zhang ||| 
2018 ||| audio-visual attention networks for emotion recognition. ||| jiyoung lee ||| sunok kim ||| seungryong kim ||| kwanghoon sohn ||| 
2019 ||| linestofacephoto: face photo generation from lines with conditional self-attention generative adversarial networks. ||| yuhang li ||| xuejin chen ||| feng wu ||| zheng-jun zha ||| 
2019 ||| beauty product retrieval based on regional maximum activation of convolutions with generalized attention. ||| jun yu ||| guochen xie ||| mengyan li ||| haonian xie ||| lingyun yu ||| 
2021 ||| transrefer3d: entity-and-relation aware transformer for fine-grained 3d visual grounding. ||| dailan he ||| yusheng zhao ||| junyu luo ||| tianrui hui ||| shaofei huang ||| aixi zhang ||| si liu ||| 
2017 ||| video question answering via gradually refined attention over appearance and motion. ||| dejing xu ||| zhou zhao ||| jun xiao ||| fei wu ||| hanwang zhang ||| xiangnan he ||| yueting zhuang ||| 
2017 ||| unconstrained fashion landmark detection via hierarchical recurrent transformer networks. ||| sijie yan ||| ziwei liu ||| ping luo ||| shi qiu ||| xiaogang wang ||| xiaoou tang ||| 
2020 ||| light field super-resolution via attention-guided fusion of hybrid lenses. ||| jing jin ||| junhui hou ||| jie chen ||| sam kwong ||| jingyi yu ||| 
2021 ||| svhan: sequential view based hierarchical attention network for 3d shape recognition. ||| yue zhao ||| weizhi nie ||| an-an liu ||| zan gao ||| yuting su ||| 
2018 ||| attribute-aware attention model for fine-grained representation learning. ||| kai han ||| jianyuan guo ||| chao zhang ||| mingjian zhu ||| 
2021 ||| image search with text feedback by deep hierarchical attention mutual information maximization. ||| chunbin gu ||| jiajun bu ||| zhen zhang ||| zhi yu ||| dongfang ma ||| wei wang ||| 
2021 ||| uacanet: uncertainty augmented context attention for polyp segmentation. ||| taehun kim ||| hyemin lee ||| daijin kim ||| 
2021 ||| pose-guided inter- and intra-part relational transformer for occluded person re-identification. ||| zhongxing ma ||| yifan zhao ||| jia li ||| 
2018 ||| temporal hierarchical attention at category- and item-level for micro-video click-through prediction. ||| xusong chen ||| dong liu ||| zheng-jun zha ||| wengang zhou ||| zhiwei xiong ||| yan li ||| 
2019 ||| an attentional-lstm for improved classification of brain activities evoked by images. ||| sheng-hua zhong ||| ahmed fares ||| jianmin jiang ||| 
2020 ||| dual attention gans for semantic image synthesis. ||| hao tang ||| song bai ||| nicu sebe ||| 
2021 ||| face hallucination via split-attention in split-attention network. ||| tao lu ||| yuanzhi wang ||| yanduo zhang ||| yu wang ||| wei liu ||| zhongyuan wang ||| junjun jiang ||| 
2020 ||| context-aware attention network for predicting image aesthetic subjectivity. ||| munan xu ||| jia-xing zhong ||| yurui ren ||| shan liu ||| ge li ||| 
2019 ||| attention-based densely connected lstm for video captioning. ||| yongqing zhu ||| shuqiang jiang ||| 
2018 ||| learning joint multimodal representation with adversarial attention networks. ||| feiran huang ||| xiaoming zhang ||| zhoujun li ||| 
2021 ||| scene text image super-resolution via parallelly contextual attention network. ||| cairong zhao ||| shuyang feng ||| brian nlong zhao ||| zhijun ding ||| jun wu ||| fumin shen ||| heng tao shen ||| 
2021 ||| learning contextual transformer network for image inpainting. ||| ye deng ||| siqi hui ||| sanping zhou ||| deyu meng ||| jinjun wang ||| 
2018 ||| attention-based pyramid aggregation network for visual place recognition. ||| yingying zhu ||| jiong wang ||| lingxi xie ||| liang zheng ||| 
2017 ||| generative attention model with adversarial self-learning for visual question answering. ||| ilija ilievski ||| jiashi feng ||| 
2018 ||| mining semantics-preserving attention for group activity recognition. ||| yansong tang ||| zian wang ||| peiyang li ||| jiwen lu ||| ming yang ||| jie zhou ||| 
2020 ||| attention based beauty product retrieval using global and local descriptors. ||| jun yu ||| guochen xie ||| mengyan li ||| haonian xie ||| xinlong hao ||| fang gao ||| feng shuang ||| 
2021 ||| doctr: document image transformer for geometric unwarping and illumination correction. ||| hao feng ||| yuechen wang ||| wengang zhou ||| jiajun deng ||| houqiang li ||| 
2019 ||| bert4sessrec: content-based video relevance prediction with bidirectional encoder representations from transformer. ||| xusong chen ||| dong liu ||| chenyi lei ||| rui li ||| zheng-jun zha ||| zhiwei xiong ||| 
2020 ||| desmoothgan: recovering details of smoothed images via spatial feature-wise transformation and full attention. ||| yifei huang ||| chenhui li ||| xiaohu guo ||| jing liao ||| chenxu zhang ||| changbo wang ||| 
2020 ||| a structured graph attention network for vehicle re-identification. ||| yangchun zhu ||| zheng-jun zha ||| tianzhu zhang ||| jiawei liu ||| jiebo luo ||| 
2019 ||| learning fragment self-attention embeddings for image-text matching. ||| yiling wu ||| shuhui wang ||| guoli song ||| qingming huang ||| 
2021 ||| hierarchical multi-task learning for diagram question answering with multi-modal transformer. ||| zhaoquan yuan ||| xiao peng ||| xiao wu ||| changsheng xu ||| 
2020 ||| occluded prohibited items detection: an x-ray security inspection benchmark and de-occlusion attention module. ||| yanlu wei ||| renshuai tao ||| zhangjie wu ||| yuqing ma ||| libo zhang ||| xianglong liu ||| 
2020 ||| deep learning-based person search with visual attention embedding. ||| liviu-daniel stefan ||| seila abdulamit ||| mihai dogariu ||| mihai gabriel constantin ||| bogdan ionescu ||| 
2020 ||| fused recurrent network via channel attention for remote sensing satellite image super-resolution. ||| xinyao li ||| dongyang zhang ||| zhenwen liang ||| deqiang ouyang ||| jie shao ||| 
2021 ||| hierarchical transformer: unsupervised representation learning for skeleton-based human action recognition. ||| yi-bin cheng ||| xipeng chen ||| junhong chen ||| pengxu wei ||| dongyu zhang ||| liang lin ||| 
2021 ||| non-local attention learning for medical image classification. ||| yang wen ||| leiting chen ||| haisheng chen ||| ximan tang ||| yu deng ||| yongbiao chen ||| chuan zhou ||| 
2021 ||| multimodal transformer networks with latent interaction for audio-visual event localization. ||| yixuan he ||| xing xu ||| xin liu ||| weihua ou ||| huimin lu ||| 
2021 ||| pyramid orthogonal attention network based on dual self-similarity for accurate mr image super-resolution. ||| xiaowan hu ||| haoqian wang ||| yuanhao cai ||| xiaole zhao ||| yulun zhang ||| 
2020 ||| spanet: spatial pyramid attention network for enhanced image recognition. ||| jingda guo ||| xu ma ||| andrew sansom ||| mara mcguire ||| andrew kalaani ||| qi chen ||| sihai tang ||| qing yang ||| song fu ||| 
2021 ||| astm: an attention based spatiotemporal model for video prediction using 3d convolutional neural networks. ||| zheng chang ||| xinfeng zhang ||| shanshe wang ||| siwei ma ||| yan ye ||| wen gao ||| 
2017 ||| learning attentional recurrent neural network for visual tracking. ||| qiurui wang ||| chun yuan ||| zhihui lin ||| 
2021 ||| depth-guided adain and shift attention network for vision-and-language navigation. ||| qiang sun ||| yifeng zhuang ||| zhengqing chen ||| yanwei fu ||| xiangyang xue ||| 
2019 ||| multimodal semantic attention network for video captioning. ||| liang sun ||| bing li ||| chunfeng yuan ||| zhengjun zha ||| weiming hu ||| 
2021 ||| driving video fixation prediction model via spatio-temporal networks and attention gates. ||| tao deng ||| fei yan ||| hongmei yan ||| 
2019 ||| locality-constrained spatial transformer network for video crowd counting. ||| yanyan fang ||| biyun zhan ||| wandi cai ||| shenghua gao ||| bo hu ||| 
2019 ||| personalized image recommendation with photo importance and user-item interactive attention. ||| wan zhang ||| zepeng wang ||| tao chen ||| 
2019 ||| video prediction with temporal-spatial attention mechanism and deep perceptual similarity branch. ||| qian wu ||| wenmin wang ||| xiongtao chen ||| weimian li ||| 
2019 ||| attention based semi-supervised dictionary learning for diagnosis of autism spectrum disorders. ||| meng yang ||| qin zhong ||| lin chen ||| fanglin huang ||| baiying lei ||| 
2021 ||| truth inference with bipartite attention graph neural network from a comprehensive view. ||| jiacheng liu ||| feilong tang ||| jielong huang ||| 
2021 ||| graph attention-based deep neural network for 3d point cloud processing. ||| xun li ||| feng xue ||| chao chen ||| xiaohui yuan ||| qiang lu ||| 
2019 ||| deep color image demosaicking with feature pyramid channel attention. ||| qi kang ||| ying fu ||| hua huang ||| 
2021 ||| pyramid feature attention network for monocular depth prediction. ||| yifang xu ||| chenglei peng ||| ming li ||| yang li ||| sidan du ||| 
2021 ||| stereo superpixel segmentation via dual-attention fusion networks. ||| ruiqi wu ||| yajuan du ||| hua li ||| yucong dai ||| 
2019 ||| audio2face: generating speech/face animation from single audio with attention-based bidirectional lstm networks. ||| guanzhong tian ||| yi yuan ||| yong liu ||| 
2021 ||| star-net: spatial-temporal attention residual network for video deraining. ||| wei zhong ||| xuefeng zhang ||| long ma ||| risheng liu ||| xin fan ||| zhongxuan luo ||| 
2019 ||| multi-modal language analysis with hierarchical interaction-level and selection-level attentions. ||| dong zhang ||| liangqing wu ||| shoushan li ||| qiaoming zhu ||| guodong zhou ||| 
2021 ||| multi-scale attention constraint network for fine-grained visual classification. ||| yaqing hou ||| wenkai zhang ||| dongsheng zhou ||| hongwei ge ||| qiang zhang ||| xiaopeng wei ||| 
2021 ||| improving convolutional networks with boosting attention convolutions. ||| chao li ||| yongsheng liang ||| huo-xiang yang ||| fanyang meng ||| wei liu ||| handong wang ||| 
2019 ||| channel-wise temporal attention network for video action recognition. ||| jianjun lei ||| yalong jia ||| bo peng ||| qingming huang ||| 
2021 ||| relationship-aware primal-dual graph attention network for scene graph generation. ||| hao zhou ||| tingjin luo ||| jun zhang ||| jun lei ||| shuohao li ||| 
2019 ||| learning recurrent structure-guided attention network for multi-person pose estimation. ||| zhongwei qiu ||| kai qiu ||| jianlong fu ||| dongmei fu ||| 
2021 ||| attention-guided knowledge distillation for efficient single-stage detector. ||| tong wang ||| yousong zhu ||| chaoyang zhao ||| xu zhao ||| jinqiao wang ||| ming tang ||| 
2021 ||| depth super-resolution by texture-depth transformer. ||| chao yao ||| shuaiyong zhang ||| mengyao yang ||| meiqin liu ||| junpeng qi ||| 
2021 ||| learning outfit compatibility with graph attention network and visual-semantic embedding. ||| jianfeng wang ||| xiaochun cheng ||| ruomei wang ||| shaohui liu ||| 
2020 ||| attention-based network for low-light image enhancement. ||| cheng zhang ||| qingsen yan ||| yu zhu ||| xianjun li ||| jinqiu sun ||| yanning zhang ||| 
2021 ||| qau-net: quartet attention u-net for liver and liver-tumor segmentation. ||| luminzi hong ||| risheng wang ||| tao lei ||| xiaogang du ||| yong wan ||| 
2021 ||| graph attention neural network for image restoration. ||| chong mou ||| jian zhang ||| 
2021 ||| supervised video summarization via multiple feature sets with parallel attention. ||| junaid ahmed ghauri ||| sherzod hakimov ||| ralph ewerth ||| 
2019 ||| herding effect based attention for personalized time-sync video recommendation. ||| wenmian yang ||| wenyuan gao ||| xiaojie zhou ||| weijia jia ||| shaohua zhang ||| yutao luo ||| 
2019 ||| convolutional temporal attention model for video-based person re-identification. ||| tanzila rahman ||| mrigank rochan ||| yang wang ||| 
2019 ||| residual dilated network with attention for image blind denoising. ||| guanqun hou ||| yujiu yang ||| jing-hao xue ||| 
2021 ||| vanet: a view attention guided network for 3d reconstruction from single and multi-view images. ||| yi yuan ||| jilin tang ||| zhengxia zou ||| 
2020 ||| self-adaptive embedding for few-shot classification by hierarchical attention. ||| xueliang wang ||| feng wu ||| jie wang ||| 
2021 ||| a novel attention enhanced residual-in-residual dense network for text image super-resolution. ||| minglong xue ||| zhiheng huang ||| ruo-ze liu ||| tong lu ||| 
2019 ||| graph attention neural networks for point cloud recognition. ||| zongmin li ||| jun zhang ||| guanlin li ||| yujie liu ||| siyuan li ||| 
2021 ||| salient object detection via attention-aware cascaded bottom-up feature aggregation. ||| fengming sun ||| lufei huang ||| xia yuan ||| chunxia zhao ||| 
2019 ||| spatial-aware non-local attention for fashion landmark detection. ||| yixin li ||| shengqin tang ||| yun ye ||| jinwen ma ||| 
2021 ||| saliency-guided complementary attention for improved few-shot learning. ||| linglan zhao ||| ge liu ||| dashan guo ||| wei li ||| xiangzhong fang ||| 
2019 ||| multi-level attention model with deep scattering spectrum for acoustic scene classification. ||| zhitong li ||| yuanbo hou ||| xiang xie ||| shengchen li ||| liqiang zhang ||| shixuan du ||| wei liu ||| 
2019 ||| context attention module for human hand detection. ||| zhihuai xie ||| shaojie wang ||| wentian zhao ||| zhenhua guo ||| 
2020 ||| graph attention model embedded with multi-modal knowledge for depression detection. ||| wenbo zheng ||| lan yan ||| chao gou ||| fei-yue wang ||| 
2019 ||| self-attention relation network for few-shot learning. ||| binyuan hui ||| pengfei zhu ||| qinghua hu ||| qilong wang ||| 
2021 ||| multiple hub-driven attention graph network for scene graph generation. ||| yang yao ||| bo gu ||| 
2019 ||| attentiondrop for convolutional neural networks. ||| zhihao ouyang ||| yan feng ||| zihao he ||| tianbo hao ||| tao dai ||| shu-tao xia ||| 
2020 ||| ransp: ranking attention network for saliency prediction on omnidirectional images. ||| dandan zhu ||| yongqing chen ||| tian han ||| defang zhao ||| yucheng zhu ||| qiangqiang zhou ||| guangtao zhai ||| xiaokang yang ||| 
2021 ||| cranet: cascade residual attention network for crowd counting. ||| zhongyuan wu ||| jun sang ||| ying shi ||| qi liu ||| nong sang ||| xinyue liu ||| 
2021 ||| a keypoint transformer to discover spine structure for cobb angle estimation. ||| yue guo ||| yanmei li ||| xiaowei zhou ||| wenhao he ||| 
2021 ||| multi-pretext attention network for few-shot learning with self-supervision. ||| hainan li ||| renshuai tao ||| jun li ||| haotong qin ||| yifu ding ||| shuo wang ||| xianglong liu ||| 
2021 ||| learning connected attentions for convolutional neural networks. ||| xu ma ||| jingda guo ||| sihai tang ||| zhinan qiao ||| qi chen ||| qing yang ||| song fu ||| paparao palacharla ||| nannan wang ||| xi wang ||| 
2020 ||| double shot: preserve and erase based class attention networks for weakly supervised localization (peca-net). ||| lishu luo ||| chun yuan ||| ke zhang ||| yong jiang ||| yuwei zhang ||| honglei zhang ||| 
2018 ||| refining attention: a sequential attention model for image captioning. ||| fang fang ||| qinyu li ||| hanli wang ||| pengjie tang ||| 
2021 ||| distance restricted transformer encoder for multi-label classification. ||| xiaomei wang ||| yaqian li ||| tong luo ||| yandong guo ||| yanwei fu ||| xiangyang xue ||| 
2017 ||| top attention in line with time: a light-weight strategy. ||| youjiang xu ||| shichao zhao ||| yahong han ||| qinghua hu ||| fei wu ||| 
2021 ||| multi-view face recognition using deep attention-based face frontalization. ||| xiaohu shao ||| junliang xing ||| ruihan pan ||| zhenghao li ||| xiangdong zhou ||| yu shi ||| 
2021 ||| person retrieval with conv-transformer. ||| shengsen wu ||| yan bai ||| ce wang ||| lingyu duan ||| 
2019 ||| text-attentional conditional generative adversarial network for super-resolution of text images. ||| yuyang wang ||| feng su ||| ye qian ||| 
2019 ||| unsupervised monocular depth estimation based on dual attention mechanism and depth-aware loss. ||| xinchen ye ||| mingliang zhang ||| rui xu ||| wei zhong ||| xin fan ||| zhu liu ||| jiaao zhang ||| 
2021 ||| multi-scale gated attention for weakly labelled sound event detection. ||| zhenwei hou ||| liping yang ||| 
2019 ||| visual attention modeling for autism spectrum disorder by semantic features. ||| yuming fang ||| hanqin huang ||| boyang wan ||| yi-fan zuo ||| 
2019 ||| sequential behavior modeling for next micro-video recommendation with collaborative transformer. ||| shang liu ||| zhenzhong chen ||| 
2020 ||| saan: semantic attention adaptation network for face super-resolution. ||| tianyu zhao ||| changqing zhang ||| 
2019 ||| improving human pose estimation with self-attention generative adversarial networks. ||| zhongzheng cao ||| rui wang ||| xiangyang wang ||| zhi liu ||| xiaoqiang zhu ||| 
2020 ||| two-way feature-aligned and attention-rectified adversarial training. ||| haitao zhang ||| fan jia ||| quanxin zhang ||| yahong han ||| xiaohui kuang ||| yu-an tan ||| 
2017 ||| a simple method to obtain visual attention data in head mounted virtual reality. ||| evgeniy upenik ||| touradj ebrahimi ||| 
2019 ||| knowledge distillation with category-aware attention and discriminant logit losses. ||| lei jiang ||| wengang zhou ||| houqiang li ||| 
2019 ||| learning to segment unseen category objects using gradient gaussian attention. ||| pengbo zhang ||| zhihui wang ||| xinzhu ma ||| haojie li ||| jianjun li ||| 
2020 ||| lightweight single image super-resolution through efficient second-order attention spindle network. ||| yiyun chen ||| yihong chen ||| jing-hao xue ||| wenming yang ||| qingmin liao ||| 
2021 ||| residual attention block search for lightweight image super-resolution. ||| wenrui liao ||| zhong-qiu zhao ||| hao shen ||| weidong tian ||| 
2019 ||| low-light image enhancement with attention and multi-level feature fusion. ||| lei wang ||| guangtao fu ||| zhuqing jiang ||| guodong ju ||| aidong men ||| 
2021 ||| self-attention recurrent summarization network with reinforcement learning for video summarization task. ||| aniwat phaphuangwittayakul ||| yi guo ||| fangli ying ||| wentian xu ||| zheng zheng ||| 
2021 ||| pmae: pseudo multi-label attention ensemble. ||| xueman wang ||| ling du ||| junbing li ||| 
2021 ||| attention-guided semantic hashing for unsupervised cross-modal retrieval. ||| xiao shen ||| haofeng zhang ||| lunbo li ||| li liu ||| 
2020 ||| pa-ggan: session-based recommendation with position-aware gated graph attention network. ||| jinshan wang ||| qianfang xu ||| jiahuan lei ||| chaoqun lin ||| bo xiao ||| 
2021 ||| cognition-driven real-time personality detection via language-guided contrastive visual attention. ||| xiaoya gao ||| jingjing wang ||| shoushan li ||| guodong zhou ||| 
2021 ||| spatial attention-based non-reference perceptual quality prediction network for omnidirectional images. ||| li yang ||| mai xu ||| xin deng ||| bo feng ||| 
2017 ||| a joint model for action localization and classification in untrimmed video with visual attention. ||| weimian li ||| wenmin wang ||| xiongtao chen ||| jinzhuo wang ||| ge li ||| 
2017 ||| deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition. ||| che-wei huang ||| shrikanth s. narayanan ||| 
2021 ||| spatial reasoning and context-aware attention network for skeleton-based action recognition. ||| dianlong you ||| ling wang ||| da han ||| shunpan liang ||| hongyang liu ||| fuyong yuan ||| 
2021 ||| attention-based relation reasoning network for video-text retrieval. ||| ni wang ||| zheng wang ||| xing xu ||| fumin shen ||| yang yang ||| heng tao shen ||| 
2017 ||| scanpath mining of eye movement trajectories for visual attention analysis. ||| aoqi li ||| yingxue zhang ||| zhenzhong chen ||| 
2019 ||| towards accurate instance-level text spotting with guided attention. ||| haiyan wang ||| xuejian rong ||| yingli tian ||| 
2019 ||| salient object detection via recurrently aggregating spatial attention weighted cross-level deep features. ||| chang tang ||| xinzhong zhu ||| xinwang liu ||| pichao wang ||| 
2019 ||| multi-scale capsule attention-based salient object detection with multi-crossed layer connections. ||| qi qi ||| sanyuan zhao ||| jianbing shen ||| kin-man lam ||| 
2021 ||| underexposed image enhancement via unsupervised feature attention network. ||| fengji ma ||| haitao li ||| 
2020 ||| video anomaly detection via predictive autoencoder with gradient-based attention. ||| yuandu lai ||| rui liu ||| yahong han ||| 
2020 ||| local-variance-based attention for visual tracking. ||| changlun guo ||| xianbin wen ||| liming yuan ||| haixia xu ||| 
2019 ||| cross-database micro-expression recognition: a style aggregated and attention transfer approach. ||| ling zhou ||| qirong mao ||| luoyang xue ||| 
2019 ||| an attention residual neural network with recurrent greedy approach as loop filter for inter frames. ||| jiabao yao ||| li wang ||| fangdong chen ||| chaoyi lin ||| shiliang pu ||| 
2019 ||| self-attention guided deep features for action recognition. ||| renyi xiao ||| yonghong hou ||| zihui guo ||| chuankun li ||| pichao wang ||| wanqing li ||| 
2019 ||| scene text recognition via gated cascade attention. ||| siwei wang ||| yongtao wang ||| xiaoran qin ||| qijie zhao ||| zhi tang ||| 
2021 ||| halder: hierarchical attention-guided learning with detail-refinement for multi-exposure image fusion. ||| jinyuan liu ||| jingjie shang ||| risheng liu ||| xin fan ||| 
2021 ||| let's find fluorescein: cross-modal dual attention learning for fluorescein leakage segmentation in fundus fluorescein angiography. ||| yang wen ||| leiting chen ||| lifeng qiao ||| yu deng ||| haisheng chen ||| tian zhang ||| chuan zhou ||| 
2021 ||| improved lightcnn with attention modules for asv spoofing detection. ||| xinyue ma ||| tianyu liang ||| shanshan zhang ||| shen huang ||| liang he ||| 
2021 ||| attention driven self-similarity capture for motion deblurring. ||| jie zhang ||| chuanfa zhang ||| jiangzhou wang ||| qingyue xiong ||| yingtao zhang ||| wenqiang zhang ||| 
2020 ||| dual focus attention network for video emotion recognition. ||| haonan qiu ||| liang he ||| feng wang ||| 
2020 ||| residual attention based network for automatic classification of phonation modes. ||| xiaoheng sun ||| yiliang jiang ||| wei li ||| 
2019 ||| skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention. ||| guyue hu ||| bo cui ||| shan yu ||| 
2019 ||| pasiam: predicting attention inspired siamese network, for space-borne satellite video tracking. ||| jia shao ||| bo du ||| chen wu ||| pingkun yan ||| 
2017 ||| adaptive attention fusion network for visual question answering. ||| geonmo gu ||| seong tae kim ||| yong man ro ||| 
2020 ||| attention meets normalization and beyond. ||| xu ma ||| jingda guo ||| qi chen ||| sihai tang ||| qing yang ||| song fu ||| 
2020 ||| a spatial attention based convolutional neural network for gesture recognition with hd-semg signals. ||| sirong hao ||| ruxin wang ||| yishan wang ||| ye li ||| 
2020 ||| attention bias in emotional conflict in major depression disorder: an eye tracking study. ||| jing zhu ||| tao gong ||| zihan wang ||| chen xia ||| zhijie ding ||| xiaowei li ||| 
2020 ||| an adaptive attention regulation method based on biocybernetic loop. ||| yi zhang ||| han xiao ||| jian zhang ||| hanshu cai ||| 
2020 ||| bgsga: combining bi-gru and syntactic graph attention for improving distant supervision relation extraction. ||| chengcheng peng ||| bin wu ||| zekun li ||| 
2021 ||| fu covid-19 ai agent built on attention algorithm using a combination of transformer, albert model, and rasa framework. ||| tran quy ban ||| thai van nguyen ||| thang duc phung ||| viet tan nguyen ||| dat duy tran ||| ngo tung son ||| 
2019 ||| multi-attention network for aspect sentiment analysis. ||| huiyu han ||| xiaoge li ||| shuting zhi ||| haoyue wang ||| 
2019 ||| spectral normalization and relativistic adversarial training for conditional pose generation with self-attention. ||| yusuke horiuchi ||| satoshi iizuka ||| edgar simo-serra ||| hiroshi ishikawa ||| 
2021 ||| attention mining branch for optimizing attention map. ||| takaaki iwayoshi ||| masahiro mitsuhara ||| masayuki takada ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2019 ||| automatic measurement of visual attention to video content using deep learning. ||| attila schulc ||| jeffrey f. cohn ||| jie shen ||| maja pantic ||| 
2019 ||| welding joints inspection via residual attention network. ||| jinguo zhu ||| zejian yuan ||| tie liu ||| 
2021 ||| action spotting and temporal attention analysis in soccer videos. ||| hiroaki minoura ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| mitsuru nakazawa ||| yeongnam chae ||| bj ||| rn stenger ||| 
2021 ||| video summarization with frame index vision transformer. ||| tzu-chun hsu ||| yi-sheng liao ||| chun-rong huang ||| 
2021 ||| hma-depth: a new monocular depth estimation model using hierarchical multi-scale attention. ||| zhaofeng niu ||| yuichiro fujimoto ||| masayuki kanbara ||| hirokazu kato ||| 
2017 ||| attention to describe products with attributes. ||| shuohao li ||| kota yamaguchi ||| 
2021 ||| recurrent rlcn-guided attention network for single image deraining. ||| yizhou li ||| yusuke monno ||| masatoshi okutomi ||| 
2017 ||| transformer monitoring using kalman filtering. ||| subramanian v. shastri ||| emma m. stewart ||| ciaran m. roberts ||| 
2017 ||| infusing autonomy in power distribution networks using smart transformers. ||| aranya chakrabortty ||| 
2020 ||| coordinated charging of large electric vehicle fleet in a charging station with limited transformer power. ||| yassir dahmane ||| rapha ||| l chenouard ||| malek ghanes ||| mario alvarado-ruiz ||| 
2020 ||| makeup style transfer on low-quality images with weighted multi-scale attention. ||| daniel organisciak ||| edmond s. l. ho ||| hubert p. h. shum ||| 
2020 ||| manet: multimodal attention network based point-view fusion for 3d shape recognition. ||| yaxin zhao ||| jichao jiao ||| ning li ||| zhongliang deng ||| 
2020 ||| attention stereo matching network. ||| doudou zhang ||| jing cai ||| yanbing xue ||| zan gao ||| hua zhang ||| 
2020 ||| taan: task-aware attention network for few-shot classification. ||| zhe wang ||| li liu ||| fanzhang li ||| 
2020 ||| self and channel attention network for person re-identification. ||| asad munir ||| niki martinel ||| christian micheloni ||| 
2020 ||| selective kernel and motion-emphasized loss based attention-guided network for hdr imaging of dynamic scenes. ||| yipeng deng ||| qin liu ||| takeshi ikenaga ||| 
2020 ||| two-level attention-based fusion learning for rgb-d face recognition. ||| hardik uppal ||| alireza sepas-moghaddam ||| michael a. greenspan ||| ali etemad ||| 
2018 ||| focusing on what is relevant: time-series learning and understanding using attention. ||| phongtharin vinayavekhin ||| subhajit chaudhury ||| asim munawar ||| don joven agravante ||| giovanni de magistris ||| daiki kimura ||| ryuki tachibana ||| 
2020 ||| equation attention relationship network (earn) : a geometric deep metric framework for learning similar math expression embedding. ||| saleem ahmed ||| kenny davila ||| srirangaraj setlur ||| venu govindaraju ||| 
2018 ||| action recognition with visual attention on skeleton images. ||| zhengyuan yang ||| yuncheng li ||| jianchao yang ||| jiebo luo ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| chiara plizzari ||| marco cannici ||| matteo matteucci ||| 
2020 ||| sca net: sparse channel attention module for action recognition. ||| hang song ||| yonghong song ||| yuanlin zhang ||| 
2020 ||| motion attention deep transfer network for cross-database micro-expression recognition. ||| wanchuang xia ||| wenming zheng ||| yuan zong ||| xingxun jiang ||| 
2020 ||| automatic fake news detection with pre-trained transformer models. ||| mina sch ||| tz ||| alexander schindler ||| melanie siegel ||| kawa nazemi ||| 
2020 ||| answer-checking in context: a multi-modal fully attention network for visual question answering. ||| hantao huang ||| tao han ||| wei han ||| deep yap ||| cheng-ming chiang ||| 
2020 ||| acrm: attention cascade r-cnn with mix-nms for metallic surface defect detection. ||| junting fang ||| xiaoyang tan ||| yuhui wang ||| 
2020 ||| transformer networks for trajectory forecasting. ||| francesco giuliari ||| irtiza hasan ||| marco cristani ||| fabio galasso ||| 
2020 ||| a novel disaster image data-set and characteristics analysis using attention model. ||| fahim faisal niloy ||| arif ||| abu bakar siddik nayem ||| anis sarker ||| ovi paul ||| m. ashraful amin ||| amin ahsan ali ||| moinul islam zaber ||| a. k. m. mahbubur rahman ||| 
2020 ||| wavelet attention embedding networks for video super-resolution. ||| young ju choi ||| young-woon lee ||| byung-gyu kim ||| 
2018 ||| conditional transfer with dense residual attention: synthesizing traffic signs from street-view imagery. ||| ries uittenbogaard ||| clint sebastian ||| julien a. vijverberg ||| bas boom ||| peter h. n. de with ||| 
2020 ||| dual-attention guided dropblock module for weakly supervised object localization. ||| junhui yin ||| siqing zhang ||| dongliang chang ||| zhanyu ma ||| jun guo ||| 
2020 ||| bcau-net: a novel architecture with binary channel attention module for mri brain segmentation. ||| yongpei zhu ||| zicong zhou ||| guojun liao ||| kehong yuan ||| 
2020 ||| multi-scale residual pyramid attention network for monocular depth estimation. ||| jing liu ||| xiaona zhang ||| zhaoxin li ||| tianlu mao ||| 
2020 ||| deep learning for astrophysics, understanding the impact of attention on variability induced by parameter initialization. ||| mika ||| l jacquemont ||| thomas vuillaume ||| alexandre beno ||| t ||| gilles maurin ||| patrick lambert ||| 
2020 ||| a transformer-based radical analysis network for chinese character recognition. ||| chen yang ||| qing wang ||| jun du ||| jianshu zhang ||| changjie wu ||| jiaming wang ||| 
2020 ||| weakly supervised attention rectification for scene text recognition. ||| chengyu gu ||| shilin wang ||| yiwei zhu ||| zheng huang ||| kai chen ||| 
2020 ||| free-form image inpainting via contrastive attention network. ||| xin ma ||| xiaoqiang zhou ||| huaibo huang ||| zhenhua chai ||| xiaolin wei ||| ran he ||| 
2020 ||| progressive scene segmentation based on self-attention mechanism. ||| yunyi pan ||| yuan gan ||| kun liu ||| yan zhang ||| 
2020 ||| cascade saliency attention network for object detection in remote sensing images. ||| dayang yu ||| rong zhang ||| shan qin ||| 
2020 ||| arbitrary style transfer with parallel self-attention. ||| tiange zhang ||| ying gao ||| feng gao ||| lin qi ||| junyu dong ||| 
2020 ||| transformer reasoning network for image- text matching and retrieval. ||| nicola messina ||| fabrizio falchi ||| andrea esuli ||| giuseppe amato ||| 
2020 ||| magnet: multi-region attention-assisted grounding of natural language queries at phrase level. ||| amar shrestha ||| krittaphat pugdeethosapol ||| haowen fang ||| qinru qiu ||| 
2020 ||| region and relations based multi attention network for graph classification. ||| manasvi aggarwal ||| m. n. murty ||| 
2020 ||| a lightweight spatial attention module with adaptive receptive fields in 3d convolutional neural network for alzheimer's disease classification. ||| fei yu ||| baoqi zhao ||| qingqing ge ||| zhijie zhang ||| junmei sun ||| xiumei li ||| 
2020 ||| nested attention u-net: a splicing detection method for satellite images. ||| j ||| nos horv ||| th ||| daniel mas montserrat ||| edward j. delp ||| 
2020 ||| second-order attention guided convolutional activations for visual recognition. ||| shannan chen ||| qian wang ||| qiule sun ||| bin liu ||| jianxin zhang ||| qiang zhang ||| 
2020 ||| single image deblurring using bi-attention network. ||| yaowei li ||| ye luo ||| jianwei lu ||| 
2020 ||| open set domain recognition via attention-based gcn and semantic matching optimization. ||| xinxing he ||| yuan yuan ||| zhiyu jiang ||| 
2020 ||| foanet: a focus of attention network with application to myocardium segmentation. ||| zhou zhao ||| lodie puybareau ||| nicolas boutry ||| thierry g ||| raud ||| 
2020 ||| vtt: long-term visual tracking with transformers. ||| tianling bian ||| yang hua ||| tao song ||| zhengui xue ||| ruhui ma ||| neil robertson ||| haibing guan ||| 
2020 ||| object detection model based on scene-level region proposal self-attention. ||| yu quan ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2020 ||| ddanet: dual decoder attention network for automatic polyp segmentation. ||| nikhil kumar tomar ||| debesh jha ||| sharib ali ||| h ||| vard d. johansen ||| dag johansen ||| michael a. riegler ||| p ||| l halvorsen ||| 
2020 ||| stroke based posterior attention for online handwritten mathematical expression recognition. ||| changjie wu ||| qing wang ||| jianshu zhang ||| jun du ||| jia-ming wang ||| jiajia wu ||| jin-shui hu ||| 
2020 ||| exploiting saliency in attention based convolutional neural network for classification of vertical root fractures. ||| zhenxing xu ||| peng wan ||| gulibire aihemaiti ||| daoqiang zhang ||| 
2020 ||| reads: a rectified attentional double supervised network for scene text recognition. ||| qi song ||| qianyi jiang ||| nan li ||| rui zhang ||| xiaolin wei ||| 
2020 ||| local attention and global representation collaborating for fine-grained classification. ||| he zhang ||| yunming bai ||| hui zhang ||| jing liu ||| xingguang li ||| zhaofeng he ||| 
2020 ||| attention based multi-instance thyroid cytopathological diagnosis with multi-scale feature fusion. ||| shuhao qiu ||| yao guo ||| chuang zhu ||| wenli zhou ||| huang chen ||| 
2020 ||| cspa-dn: channel and spatial attention dense network for fusing pet and mri images. ||| bicao li ||| zhoufeng liu ||| shan gao ||| jenq-neng hwang ||| jun sun ||| zongmin wang ||| 
2020 ||| few-shot few-shot learning and the role of spatial attention. ||| yann lifchitz ||| yannis avrithis ||| sylvaine picard ||| 
2020 ||| decoupled self-attention module for person re-identification. ||| chao zhao ||| zhenyu zhang ||| jian yan ||| yan yan ||| 
2020 ||| multi-branch attention networks for classifying galaxy clusters. ||| yu zhang ||| gongbo liang ||| yuanyuan su ||| nathan jacobs ||| 
2020 ||| exploring spatial-temporal representations for fnirs-based intimacy detection via an attention-enhanced cascade convolutional recurrent neural network. ||| chao li ||| qian zhang ||| ziping zhao ||| li gu ||| bj ||| rn w. schuller ||| 
2018 ||| revised spatial transformer network towards improved image super-resolutions. ||| hossam m. kasem ||| kwok-wai hung ||| jianmin jiang ||| 
2020 ||| deep multiple instance learning with spatial attention for rop case classification, instance selection and abnormality localization. ||| xirong li ||| wencui wan ||| yang zhou ||| jianchun zhao ||| qijie wei ||| junbo rong ||| pengyi zhou ||| limin xu ||| lijuan lang ||| yuying liu ||| chengzhi niu ||| dayong ding ||| xuemin jin ||| 
2020 ||| question-agnostic attention for visual question answering. ||| moshiur r. farazi ||| salman h. khan ||| nick barnes ||| 
2020 ||| ordinal depth classification using region-based self-attention. ||| minh hieu phan ||| son lam phung ||| abdesselam bouzerdoum ||| 
2020 ||| attention-oriented action recognition for real- time human-robot interaction. ||| ziyang song ||| ziyi yin ||| zejian yuan ||| chong zhang ||| wanchao chi ||| yonggen ling ||| shenghao zhang ||| 
2020 ||| accurate cell segmentation in digital pathology images via attention enforced networks. ||| zeyi yao ||| kaiqi li ||| yiwen luo ||| xiaoguang zhou ||| muyi sun ||| guanhong zhang ||| 
2018 ||| attention-based neural network for traffic sign detection. ||| jing zhang ||| le hui ||| jianfeng lu ||| yuhua zhu ||| 
2020 ||| nighttime pedestrian detection based on feature attention and transformation. ||| gang li ||| shanshan zhang ||| jian yang ||| 
2020 ||| attention-based multi-modal emotion recognition from art. ||| tsegaye misikir tashu ||| tom ||| s horv ||| th ||| 
2020 ||| attention as activation. ||| yimian dai ||| stefan oehmcke ||| fabian gieseke ||| yiquan wu ||| kobus barnard ||| 
2020 ||| attention based pruning for shift networks. ||| ghouthi boukli hacene ||| carlos lassance ||| vincent gripon ||| matthieu courbariaux ||| yoshua bengio ||| 
2020 ||| sat-net: self-attention and temporal fusion for facial action unit detection. ||| zhihua li ||| zheng zhang ||| lijun yin ||| 
2020 ||| cross-regional attention network for point cloud completion. ||| hang wu ||| yubin miao ||| 
2020 ||| cascade attention guided residue learning gan for cross-modal translation. ||| bin duan ||| wei wang ||| hao tang ||| hugo latapie ||| yan yan ||| 
2020 ||| sa-unet: spatial attention u-net for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yugen yi ||| wenle wang ||| buer chen ||| changqi fan ||| 
2020 ||| coarse-to-fine foreground segmentation based on co-occurrence pixel-block and spatio-temporal attention model. ||| dong liang ||| xinyu liu ||| 
2020 ||| a novel attention-based aggregation function to combine vision and language. ||| matteo stefanini ||| marcella cornia ||| lorenzo baraldi ||| rita cucchiara ||| 
2020 ||| edge-aware graph attention network for ratio of edge-user estimation in mobile networks. ||| jiehui deng ||| sheng wan ||| xiang wang ||| enmei tu ||| xiaolin huang ||| jie yang ||| chen gong ||| 
2020 ||| collaborative human machine attention module for character recognition. ||| chetan ralekar ||| tapan kumar gandhi ||| santanu chaudhury ||| 
2020 ||| flow-guided spatial attention tracking for egocentric activity recognition. ||| tianshan liu ||| kin-man lam ||| 
2020 ||| attention based coupled framework for road and pothole segmentation. ||| shaik masihullah ||| ritu garg ||| prerana mukherjee ||| anupama ray ||| 
2020 ||| mean: multi - element attention network for scene text recognition. ||| ruijie yan ||| liangrui peng ||| shanyu xiao ||| gang yao ||| jaesik min ||| 
2020 ||| recurrent deep attention network for person re-identification. ||| changhao wang ||| jun zhou ||| xianfei duan ||| guanwen zhang ||| wei zhou ||| 
2020 ||| hierarchical multimodal attention for deep video summarization. ||| melissa sanabria ||| fr ||| d ||| ric precioso ||| thomas menguy ||| 
2018 ||| scene text detection via deep semantic feature fusion and attention-based refinement. ||| yu song ||| yuanshun cui ||| hu han ||| shiguang shan ||| xilin chen ||| 
2020 ||| aggregating object features based on attention weights for fine-grained image retrieval. ||| hongli lin ||| yongqi song ||| zixuan zeng ||| weisheng wang ||| jiayi wang ||| 
2020 ||| attention-driven body pose encoding for human activity recognition. ||| bappaditya debnath ||| mary o'brien ||| swagat kumar ||| ardhendu behera ||| 
2020 ||| cardiogan: an attention-based generative adversarial network for generation of electrocardiograms. ||| subhrajyoti dasgupta ||| sudip das ||| ujjwal bhattacharya ||| 
2020 ||| temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition. ||| negar heidari ||| alexandros iosifidis ||| 
2020 ||| segmentation of intracranial aneurysm remnant in mra using dual-attention atrous net. ||| subhashis banerjee ||| ashis kumar dhara ||| johan wikstr ||| m ||| robin strand ||| 
2020 ||| integrating historical states and co-attention mechanism for visual dialog. ||| tianling jiang ||| yi ji ||| chunping liu ||| 
2020 ||| interpretable attention guided network for fine-grained visual classification. ||| zhenhuan huang ||| xiaoyue duan ||| bo zhao ||| jinhu l ||| baochang zhang ||| 
2020 ||| efficient-receptive field block with group spatial attention mechanism for object detection. ||| jiacheng zhang ||| zhicheng zhao ||| fei su ||| 
2020 ||| hanet: hybrid attention-aware network for crowd counting. ||| xinxing su ||| yuchen yuan ||| xiangbo su ||| zhikang zou ||| shilei wen ||| pan zhou ||| 
2020 ||| generalized local attention pooling for deep metric learning. ||| carlos roig ||| david varas ||| issey masuda ||| juan carlos riveiro ||| elisenda bou-balust ||| 
2020 ||| attentional wavelet network for traditional chinese painting transfer. ||| rui wang ||| huaibo huang ||| aihua zheng ||| ran he ||| 
2020 ||| deep residual attention network for hyperspectral image reconstruction. ||| yorimoto kohei ||| xian-hua han ||| 
2020 ||| 3d attention mechanism for fine-grained classification of table tennis strokes using a twin spatio-temporal convolutional neural networks. ||| pierre-etienne martin ||| jenny benois-pineau ||| renaud p ||| teri ||| julien morlier ||| 
2020 ||| reinforcement learning with dual attention guided graph convolution for relation extraction. ||| zhixin li ||| yaru sun ||| suqin tang ||| canlong zhang ||| huifang ma ||| 
2020 ||| multi-scale relational reasoning with regional attention for visual question answering. ||| yun-tao ma ||| tong lu ||| yirui wu ||| 
2020 ||| ma-lstm: a multi-attention based lstm for complex pattern extraction. ||| jingjie guo ||| kelang tian ||| kejiang ye ||| cheng-zhong xu ||| 
2020 ||| transformer-encoder detector module: using context to improve robustness to adversarial attacks on object detection. ||| faisal alamri ||| sinan kalkan ||| nicolas pugeault ||| 
2020 ||| trajectory-user link with attention recurrent networks. ||| tao sun ||| yongjun xu ||| fei wang ||| lin wu ||| tangwen qian ||| zezhi shao ||| 
2020 ||| attention-based model with attribute classification for cross-domain person re-identification. ||| simin xu ||| lingkun luo ||| shiqiang hu ||| 
2020 ||| privattnet: predicting privacy risks in images using visual attention. ||| zhang chen ||| thivya kandappu ||| vigneshwaran subbaraju ||| 
2020 ||| deep attention based semi-supervised 2d-pose estimation for surgical instruments. ||| mert kayhan ||| okan k ||| p ||| kl ||| mhd hasan sarhan ||| mehmet yigitsoy ||| abouzar eslami ||| gerhard rigoll ||| 
2020 ||| range-doppler hand gesture recognition using deep residual-3dcnn with transformer network. ||| gaurav jaswal ||| seshan srirangarajan ||| sumantra dutta roy ||| 
2020 ||| anchors vs attention: comparing xai on a real-life use case. ||| ga ||| lle jouis ||| harold mouch ||| re ||| fabien picarougne ||| alexandre hardouin ||| 
2018 ||| weakly supervised domain-specific color naming based on attention. ||| lu yu ||| yongmei cheng ||| joost van de weijer ||| 
2020 ||| attention2angiogan: synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks. ||| sharif amit kamran ||| khondker fariha hossain ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| 
2020 ||| tcatd: text contour attention for scene text detection. ||| ziling hu ||| xingjiao wu ||| jing yang ||| 
2020 ||| sdma: saliency-driven mutual cross attention for multi-variate time series. ||| yash garg ||| k. sel ||| uk candan ||| 
2020 ||| context matters: self-attention for sign language recognition. ||| fares ben slimane ||| mohamed bouguessa ||| 
2018 ||| multi-scale attention with dense encoder for handwritten mathematical expression recognition. ||| jianshu zhang ||| jun du ||| lirong dai ||| 
2020 ||| attention pyramid module for scene recognition. ||| zhinan qiao ||| xiaohui yuan ||| chengyuan zhuang ||| abolfazl meyarian ||| 
2018 ||| an attention-based approach for single image super resolution. ||| yuan liu ||| yuancheng wang ||| nan li ||| xu cheng ||| yifeng zhang ||| yongming huang ||| guojun lu ||| 
2020 ||| global context-based network with transformer for image2latex. ||| nuo pang ||| chun yang ||| xiaobin zhu ||| jixuan li ||| xu-cheng yin ||| 
2020 ||| predicting chemical properties using self-attention multi-task learning based on smiles representation. ||| sangrak lim ||| yong oh lee ||| 
2020 ||| improving reliability of attention branch network by introducing uncertainty. ||| takuya tsukahara ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2020 ||| multi-stage attention based visual question answering. ||| aakansha mishra ||| ashish anand ||| prithwijit guha ||| 
2018 ||| a multi-part convolutional attention network for fine-grained image recognition. ||| weilin zhong ||| linfeng jiang ||| tao zhang ||| jinsheng ji ||| huilin xiong ||| 
2020 ||| avd-net: attention value decomposition network for deep multi-agent reinforcement learning. ||| yuanxin zhang ||| huimin ma ||| yu wang ||| 
2020 ||| attention-based selection strategy for weakly supervised object localization. ||| zhenfei zhang ||| tien d. bui ||| 
2020 ||| global-local attention network for semantic segmentation in aerial images. ||| minglong li ||| lianlei shan ||| xiaobin li ||| yang bai ||| dengji zhou ||| weiqiang wang ||| ke lv ||| bin luo ||| si-bao chen ||| 
2020 ||| a transformer-based network for anisotropic 3d medical image segmentation. ||| danfeng guo ||| demetri terzopoulos ||| 
2020 ||| utilising visual attention cues for vehicle detection and tracking. ||| feiyan hu ||| g. m. venkatesh ||| noel e. o'connor ||| alan f. smeaton ||| suzanne little ||| 
2020 ||| 3d point cloud registration based on cascaded mutual information attention network. ||| xiang pan ||| xiaoyi ji ||| sisi cheng ||| 
2018 ||| context-aware attention lstm network for flood prediction. ||| yirui wu ||| zhaoyang liu ||| weigang xu ||| jun feng ||| shivakumara palaiahnakote ||| tong lu ||| 
2020 ||| gaussian constrained attention network for scene text recognition. ||| zhi qiao ||| xugong qin ||| yu zhou ||| fei yang ||| weiping wang ||| 
2018 ||| aggregated sparse attention for steering angle prediction. ||| sen he ||| dmitry kangin ||| yang mi ||| nicolas pugeault ||| 
2020 ||| atsal: an attention based architecture for saliency prediction in 360$^\circ $ videos. ||| yasser dahou ||| marouane tliba ||| kevin mcguinness ||| noel e. o'connor ||| 
2020 ||| on the global self-attention mechanism for graph convolutional networks. ||| chen wang ||| chengyuan deng ||| 
2020 ||| rsan: residual subtraction and attention network for single image super-resolution. ||| shuo wei ||| xin sun ||| haoran zhao ||| junyu dong ||| 
2020 ||| attention-based deep metric learning for near-duplicate video retrieval. ||| kuan-hsun wang ||| chia-chun cheng ||| yi-ling chen ||| yale song ||| shang-hong lai ||| 
2020 ||| fuzzy-based pseudo segmentation approach for handwritten word recognition using a sequence to sequence model with attention. ||| rajdeep bhattacharya ||| samir malakar ||| friedhelm schwenker ||| ram sarkar ||| 
2020 ||| $p^{2}$ net: augmented parallel-pyramid net for attention guided pose estimation. ||| luanxuan hou ||| jie cao ||| yuan zhao ||| haifeng shen ||| jian tang ||| ran he ||| 
2020 ||| cross-media hash retrieval using multi-head attention network. ||| zhixin li ||| feng ling ||| chuansheng xu ||| canlong zhang ||| huifang ma ||| 
2020 ||| da-refinenet: dual-inputs attention refinenet for whole slide image segmentation. ||| ziqiang li ||| rentuo tao ||| qianrun wu ||| bin li ||| 
2020 ||| multiscale attention-based prototypical network for few-shot semantic segmentation. ||| yifei zhang ||| d ||| sir |||  sidib ||| olivier morel ||| fabrice m ||| riaudeau ||| 
2020 ||| multi-scale and attention based resnet for heartbeat classification. ||| haojie zhang ||| gongping yang ||| yuwen huang ||| feng yuan ||| yilong yin ||| 
2020 ||| feature-aware unsupervised learning with joint variational attention and automatic clustering. ||| ru wang ||| lin li ||| peipei wang ||| xiaohui tao ||| peiyu liu ||| 
2020 ||| attendaffectnet: self-attention based networks for predicting affective responses from movies. ||| ha thi phuong thao ||| balamurali b. t. ||| dorien herremans ||| gemma roig ||| 
2020 ||| understanding when spatial transformer networks do not support invariance, and what to do about it. ||| lukas finnveden ||| ylva jansson ||| tony lindeberg ||| 
2018 ||| robust attentional pooling via feature selection. ||| jianbo zheng ||| teng-yok lee ||| chen feng ||| xiaohua li ||| ziming zhang ||| 
2020 ||| video summarization with a dual attention capsule network. ||| hao fu ||| hongxing wang ||| jianyu yang ||| 
2018 ||| time-dependent pre-attention model for image captioning. ||| fuwei wang ||| xiaolong gong ||| linpeng huang ||| 
2018 ||| self-attention based network for punctuation restoration. ||| feng wang ||| wei chen ||| zhen yang ||| bo xu ||| 
2020 ||| translating adult's focus of attention to elderly's. ||| onkar krishna ||| go irie ||| takahito kawanishi ||| kunio kashino ||| kiyoharu aizawa ||| 
2021 ||| root cause analysis in the industrial domain using knowledge graphs: a case study on power transformers. ||| jorge mart ||| nez gil ||| georg buchgeher ||| david gabauer ||| bernhard freudenthaler ||| dominik filipiak ||| anna fensel ||| 
2020 ||| utilization of residual cnn-gru with attention mechanism for classification of 12-lead ecg. ||| petr nejedly ||| adam ivora ||| ivo viscor ||| josef hal ||| mek ||| pavel jur ||| k ||| filip plesinger ||| 
2021 ||| generative pre-trained transformer for cardiac abnormality detection. ||| pierre louis gaudilliere ||| halla sigurthorsdottir ||| cl ||| mentine aguet ||| j ||| r ||| me van zaen ||| mathieu lemay ||| ricard delgado-gonzalo ||| 
2021 ||| classification of ecg using ensemble of residual cnns with attention mechanism. ||| petr nejedly ||| adam ivora ||| radovan sm ||| sek ||| ivo viscor ||| zuzana koscova ||| pavel jur ||| k ||| filip plesinger ||| 
2020 ||| classification of 12-lead ecgs using intra-heartbeat discrete-time fourier transform and inter-heartbeat attention. ||| ibrahim hammoud ||| i. v. ramakrishnan ||| petar m. djuric ||| 
2020 ||| a wide and deep transformer neural network for 12-lead ecg classification. ||| annamalai natarajan ||| yale chang ||| sara mariani ||| asif rahman ||| gregory boverman ||| shruti vij ||| jonathan rubin ||| 
2021 ||| a mixed-domain self-attention network for multilabel cardiac irregularity classification using reduced-lead electrocardiogram. ||| hao-chun yang ||| wan-ting hsieh ||| trista pei-chun chen ||| 
2020 ||| multi-label classification of 12-lead ecgs by using residual cnn and class-wise attention. ||| yang liu ||| kuanquan wang ||| yongfeng yuan ||| qince li ||| yacong li ||| yongpeng xu ||| henggui zhang ||| 
2020 ||| automatic classification of arrhythmias by residual network and bigru with attention mechanism. ||| runnan he ||| kuanquan wang ||| na zhao ||| qiang sun ||| yacong li ||| qince li ||| henggui zhang ||| 
2018 ||| pay more attention with fewer parameters: a novel 1-d convolutional neural network for heart sounds classification. ||| yunqiu xu ||| bin xiao ||| xiuli bi ||| weisheng li ||| junhui zhang ||| xu ma ||| 
2020 ||| madnn: a multi-scale attention deep neural network for arrhythmia classification. ||| ran duan ||| xiaodong he ||| zhuoran ouyang ||| 
2021 ||| convolution-free waveform transformers for multi-lead ecg classification. ||| annamalai natarajan ||| gregory boverman ||| yale chang ||| corneliu antonescu ||| jonathan rubin ||| 
2021 ||| channel self-attention deep learning framework for multi-cardiac abnormality diagnosis from varied-lead ecg signals. ||| apoorva srivastava ||| ajith hari ||| sawon pratiher ||| sazedul alam ||| nirmalya ghosh ||| nilanjan banerjee ||| amit patra ||| 
2021 ||| cardiac abnormalities recognition in ecg using a convolutional network with attention and input with an adaptable number of leads. ||| tom ||| s vicar ||| petra novotna ||| jakub hejc ||| oto janousek ||| marina ronzhina ||| 
2019 ||| pay attention and watch temporal correlation: a novel 1-d convolutional neural network for ecg record classification. ||| yongchao wang ||| bin xiao ||| xiuli bi ||| weisheng li ||| junhui zhang ||| xu ma ||| 
2021 ||| multi-label classification of multi-lead ecg based on deep 1d convolutional neural networks with residual and attention mechanism. ||| yamin liu ||| hanshuang xie ||| qineng cao ||| jiayi yan ||| fan wu ||| huaiyu zhu ||| yun pan ||| 
2019 ||| identifying protein-protein interaction using tree lstm and structured attention. ||| mahtab ahmed ||| jumayel islam ||| muhammad rifayat samee ||| robert e. mercer ||| 
2019 ||| improving tree-lstm with tree attention. ||| mahtab ahmed ||| muhammad rifayat samee ||| robert e. mercer ||| 
2020 ||| ican: introspective convolutional attention network for semantic text classification. ||| sounak mondal ||| suraj modi ||| sakshi garg ||| dhruva das ||| siddhartha mukherjee ||| 
2021 ||| automatic title generation for text with pre-trained transformer language model. ||| prakhar mishra ||| chaitali diwan ||| srinath srinivasa ||| g. srinivasaraghavan ||| 
2020 ||| deepcontext: parameterized compatibility-based attention cnn for human context recognition. ||| abdulaziz alajaji ||| walter gerych ||| kavin chandrasekaran ||| luke buquicchio ||| emmanuel agu ||| elke a. rundensteiner ||| 
2020 ||| similarity of speech emotion in different languages revealed by a neural network with attention. ||| changzeng fu ||| thilina dissanayake ||| kazufumi hosoda ||| takuya maekawa ||| hiroshi ishiguro ||| 
2022 ||| modeling review helpfulness with augmented transformer neural networks. ||| yunkai xiao ||| tianle wang ||| xinhua sun ||| yicong li ||| yang song ||| jialin cui ||| qinjin jia ||| chengyuan liu ||| edward f. gehringer ||| 
2019 ||| arabic poem generation with hierarchical recurrent attentional network. ||| sameerah talafha ||| banafsheh rekabdar ||| 
2020 ||| attentional adversarial variational video generation via decomposing motion and content. ||| sameerah talafha ||| banafsheh rekabdar ||| chinwe ekenna ||| christos mousas ||| 
2020 ||| deep neural attention-based model for the evaluation of italian sentences complexity. ||| daniele schicchi ||| giovanni pilato ||| giosu |||  lo bosco ||| 
2019 ||| sadeepsense: self-attention deep learning framework for heterogeneous on-device sensors in internet of things applications. ||| shuochao yao ||| yiran zhao ||| huajie shao ||| dongxin liu ||| shengzhong liu ||| yifan hao ||| ailing piao ||| shaohan hu ||| su lu ||| tarek f. abdelzaher ||| 
2020 ||| image translation with attention mechanism based on generative adversarial networks. ||| yu lu ||| ju liu ||| xueyin zhao ||| xiaoxi liu ||| weiqiang chen ||| xuesong gao ||| 
2021 ||| dual attention-based federated learning for wireless traffic prediction. ||| chuanting zhang ||| shuping dang ||| basem shihada ||| mohamed-slim alouini ||| 
2017 ||| computing continuous-time markov chains as transformers of unbounded observables. ||| vincent danos ||| tobias heindel ||| ilias garnier ||| jakob grue simonsen ||| 
2021 ||| epistemic planning with attention as a bounded resource. ||| gaia belardinelli ||| rasmus k. rendsvig ||| 
2020 ||| improving passage re-ranking with word n-gram aware coattention encoder. ||| chaitanya sai alaparthi ||| manish shrivastava ||| 
2020 ||| native-language identification with attention. ||| stian steinbakken ||| bj ||| rn gamb ||| ck ||| 
2020 ||| solving arithmetic word problems using transformer and pre-processing of problem texts. ||| kaden griffith ||| jugal kalita ||| 
2021 ||| softermax: hardware/software co-design of an efficient softmax for transformers. ||| jacob r. stevens ||| rangharajan venkatesan ||| steve dai ||| brucek khailany ||| anand raghunathan ||| 
2021 ||| dancing along battery: enabling transformer with run-time reconfigurability on mobile devices. ||| yuhong song ||| weiwen jiang ||| bingbing li ||| panjie qi ||| qingfeng zhuge ||| edwin hsing-mean sha ||| sakyasingha dasgupta ||| yiyu shi ||| caiwen ding ||| 
2021 ||| mat: processing in-memory acceleration for long-sequence attention. ||| minxuan zhou ||| yunhui guo ||| weihong xu ||| bin li ||| kevin w. eliceiri ||| tajana rosing ||| 
2021 ||| late breaking results: attention in graph2seq neural networks towards push-button analog ic placement. ||| ant ||| nio gusm ||| o ||| nuno horta ||| nuno louren ||| o ||| ricardo martins ||| 
2021 ||| attentional transfer is all you need: technology-aware layout pattern generation. ||| xiaopeng zhang ||| haoyu yang ||| evangeline f. y. young ||| 
2019 ||| improved densenet with convolutional attention module for brain tumor segmentation. ||| bin chen ||| jiajun wang ||| zheru chi ||| 
2019 ||| cardiac motion scoring based on cnn with attention mechanism. ||| zejian chen ||| wufeng xue ||| tianfu wang ||| dong ni ||| 
2018 ||| the effect of spatial consistence on multisensory integration in a divided attention task. ||| jingjing yang ||| jinlong chu ||| xiujun li ||| dan tong ||| 
2020 ||| amil: an attentional multi-instance learning for computer-aided diagnosis of skin diagnosis. ||| mengqun jin ||| delong zhang ||| peng cao ||| 
2019 ||| left ventricle segmentation and quantification with attention-enhanced segmentation and shape correction. ||| hongrong wei ||| wufeng xue ||| dong ni ||| 
2017 ||| experimental design and collection of brain and respiratory data for detection of driver's attention. ||| roman moucek ||| luk ||| s hnojsk ||| luk ||| s vareka ||| tom ||| s prokop ||| petr bruha ||| 
2022 ||| selection of representative instances using ant colony: a case study in a database of children and adolescents with attention-deficit/hyperactivity disorder. ||| henrique r. hott ||| caroline jandre ||| pedro h. s. xavier ||| amal miloud-aouidate ||| d ||| bora m. miranda ||| mark a. j. song ||| luis enrique z ||| rate ||| cristiane neri nobre ||| 
2020 ||| an attention-based architecture for eeg classification. ||| italo zoppis ||| alessio zanga ||| sara manzoni ||| giulia cisotto ||| angela morreale ||| fabio stella ||| giancarlo mauri ||| 
2022 ||| vision transformers for brain tumor classification. ||| eliott simon ||| alexia briassouli ||| 
2022 ||| improved mri-based pseudo-ct synthesis via segmentation guided attention networks. ||| gurbandurdy dovletov ||| duc duy pham ||| josef pauli ||| marcel gratz ||| harald quick ||| 
2022 ||| learning embeddings from free-text triage notes using pretrained transformer models. ||| milien arnaud ||| mahmoud elbattah ||| maxime gignon ||| gilles dequen ||| 
2020 ||| data mining in clinical trial text: transformers for classification and question answering tasks. ||| lena schmidt ||| julie weeds ||| julian p. t. higgins ||| 
2021 ||| the application of multichannel neuro-electrostimulation for working memory and attention improvement of young subjects. ||| anna petrenko ||| vladimir s. kublanov ||| 
2021 ||| analysis of school performance of children and adolescents with attention-deficit/hyperactivity disorder: a dimensionality reduction approach. ||| caroline jandre ||| bruno c. santos ||| marcelo balbino ||| d ||| bora de miranda ||| luis enrique z ||| rate ||| cristiane nobre ||| 
2017 ||| correction of attention in a learning ability task with using non-invasive neurostimulation of peripheral nervous system. ||| vladimir s. kublanov ||| anna petrenko ||| aleksandra nabiullina ||| 
2020 ||| attention-based sequential generative conversational agent. ||| nripesh kumar ||| g. srinath ||| r. abhishek prataap ||| s. jaya nirmala ||| 
2020 ||| machine learning based transformer health monitoring using iot edge computing. ||| imtiyaz ahmad ||| yaduvir singh ||| jameel ahamad ||| 
2021 ||| eeg classification with transformer-based models. ||| jiayao sun ||| jin xie ||| huihui zhou ||| 
2021 ||| improvement of visual attention by dotted background noise. ||| motoharu takao ||| yasuhiro uehara ||| akihiko izawa ||| 
2019 ||| estimation of visual attention via canonical correlation between visual and gaze-based features. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2020 ||| distress level classification of road infrastructures via cnn generating attention map. ||| naoki ogawa ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| attention-based model for predicting question relatedness on stack overflow. ||| jiayan pei ||| yimin wu ||| zishan qin ||| yao cong ||| jingtao guan ||| 
2020 ||| improved automatic summarization of subroutines via attention to file context. ||| sakib haque ||| alexander leclair ||| lingfei wu ||| collin mcmillan ||| 
2019 ||| the transformer substation scenario modeling and visual perception visualization system. ||| guangceng he ||| yushan dong ||| 
2021 ||| comparative analysis of high- and low-performing factory workers with attention-based neural networks. ||| qingxin xia ||| atsushi wada ||| takanori yoshii ||| yasuo namioka ||| takuya maekawa ||| 
2019 ||| a deep spatio-temporal attention-based neural network for passenger flow prediction. ||| yanling cui ||| beihong jin ||| fusang zhang ||| xingwu sun ||| 
2021 ||| ar-t: temporal relation embedded transformer for the real world activity recognition. ||| hyunju kim ||| dongman lee ||| 
2021 ||| use of a computational tool for the assessment of attention of medical residents after a day on duty. ||| argelia p ||| rez-pacheco ||| jos |||  a. garc ||| a-garc ||| a ||| j. eduardo lugo ||| jocelyn faubert ||| 
2021 ||| dual-pathway attention based supervised adversarial hashing for cross-modal retrieval. ||| xiaoxiao wang ||| meiyu liang ||| xiaowen cao ||| junping du ||| 
2021 ||| air quality prediction with 1-dimensional convolution and attention on multi-modal features. ||| junyoung choi ||| joonyoung kim ||| kyomin jung ||| 
2020 ||| source code summarization using attention-based keyword memory networks. ||| yunseok choi ||| suah kim ||| jee-hyong lee ||| 
2019 ||| ceam: a novel approach using cycle embeddings with attention mechanism for stock price prediction. ||| mu-hui yu ||| jheng-long wu ||| 
2022 ||| cross-attention model for multi-modal bio-signal processing. ||| son heesoo ||| lee sangseok ||| lee sael ||| 
2021 ||| attention on personalized clinical decision support system: federated learning approach. ||| chu myaet thwal ||| kyi thar ||| ye lin tun ||| choong seon hong ||| 
2019 ||| query-focused summarization enhanced with sentence attention mechanism. ||| tasuku kimura ||| ryo tagami ||| hisashi miyamori ||| 
2020 ||| paraphrase bidirectional transformer with multi-task learning. ||| bowon ko ||| ho-jin choi ||| 
2022 ||| aptamer- protein interaction prediction using transformer. ||| incheol shin ||| giltae song ||| 
2018 ||| convolutional neural network with sdp-based attention for relation classification. ||| ning li ||| hui zhang ||| yong chen ||| 
2022 ||| transformer-based embedding applied to classify bacterial species using sequencing reads. ||| ho-jin gwak ||| mina rho ||| 
2020 ||| multi-label patent classification using attention-aware deep learning model. ||| arousha haghighian roudsari ||| jafar afshar ||| charles cheolgi lee ||| wookey lee ||| 
2020 ||| unsupervised image-to-image translation with self-attention networks. ||| taewon kang ||| kwang hee lee ||| 
2022 ||| transformer networks for trajectory classification. ||| keywoong bae ||| suan lee ||| wookey lee ||| 
2017 ||| head pose-free eye gaze prediction for driver attention study. ||| yafei wang ||| tongtong zhao ||| xueyan ding ||| jiming bian ||| xianping fu ||| 
2017 ||| an attention-based neural popularity prediction model for social media events. ||| guandan chen ||| qingchao kong ||| wenji mao ||| 
2019 ||| privacy protection in transformer-based neural network. ||| jiaqi lang ||| linjing li ||| weiyun chen ||| daniel zeng ||| 
2017 ||| analyzing multimodal public sentiment based on hierarchical semantic attentional network. ||| nan xu ||| 
2019 ||| a prior knowledge based neural attention model for opioid topic identification. ||| riheng yao ||| qiudan li ||| wei-hsuan lo-ciganic ||| daniel dajun zeng ||| 
2019 ||| attention allocation of twitter users in geopolitics. ||| saike he ||| changliang li ||| hailiang wang ||| xiaolong zheng ||| zhu zhang ||| jiaojiao wang ||| daniel zeng ||| 
2019 ||| sentiment analysis based on background knowledge attention. ||| changliang li ||| yujun zhou ||| saike he ||| hailiang wang ||| 
2018 ||| attention-based multi-hop reasoning for knowledge graph. ||| zikang wang ||| linjing li ||| daniel dajun zeng ||| yue chen ||| 
2019 ||| context-aware multi-view attention networks for emotion cause extraction. ||| xinglin xiao ||| penghui wei ||| wenji mao ||| lei wang ||| 
2021 ||| gatps: an attention-based graph neural network for predicting sdc-causing instructions. ||| junchi ma ||| zongtao duan ||| lei tang ||| 
2019 ||| lame: light-controlled attention guidance for multi-monitor environments. ||| tim claudius stratmann ||| felix kempa ||| susanne boll ||| 
2018 ||| exploring vibrotactile and peripheral cues for spatial attention guidance. ||| tim claudius stratmann ||| andreas l ||| cken ||| uwe gruenefeld ||| wilko heuten ||| susanne boll ||| 
2020 ||| time-aware transformer-based network for clinical notes series prediction. ||| dongyu zhang ||| jidapa thadajarassiri ||| cansu sen ||| elke a. rundensteiner ||| 
2019 ||| self-attention based molecule representation for predicting drug-target interaction. ||| bonggun shin ||| sungsoo park ||| keunsoo kang ||| joyce c. ho ||| 
2018 ||| learning to exploit invariances in clinical time-series data using sequence transformer networks. ||| jeeheh oh ||| jiaxuan wang ||| jenna wiens ||| 
2020 ||| dynamically extracting outcome-specific problem lists from clinical notes with guided multi-headed attention. ||| justin r. lovelace ||| nathan c. hurley ||| adrian d. haimovich ||| bobak j. mortazavi ||| 
2020 ||| attention-based network for weak labels in neonatal seizure detection. ||| dmitry yu. isaev ||| dmitry tchapyjnikov ||| c. michael cotten ||| david tanaka ||| natalia mart ||| nez ||| mart ||| n bertr ||| n ||| guillermo sapiro ||| david carlson ||| 
2020 ||| predicting drug sensitivity of cancer cell lines via collaborative filtering with contextual attention. ||| yifeng tao ||| shuangxia ren ||| michael q. ding ||| russell schwartz ||| xinghua lu ||| 
2020 ||| students need more attention: bert-based attention model for small data with application to automatic patient message triage. ||| shijing si ||| rui wang ||| jedrek wosik ||| hao zhang ||| david dov ||| guoyin wang ||| lawrence carin ||| 
2018 ||| towards understanding ecg rhythm classification using convolutional neural networks and attention mappings. ||| sebastian d. goodfellow ||| andrew j. goodwin ||| robert greer ||| peter c. laussen ||| mjaye mazwi ||| danny eytan ||| 
2021 ||| an end-to-end attention transfer network for cross-domain service recommendation. ||| ruyu yan ||| yushun fan ||| 
2020 ||| multi-indicators prediction in microservice using granger causality test and attention lstm. ||| suozhao ji ||| wenjun wu ||| yanjun pu ||| 
2021 ||| natural language interface for covid-19 amharic database using lstm encoder decoder architecture with attention. ||| ephrem tadesse degu ||| rosa tsegaye aga ||| 
2020 ||| magnet: multi-label text classification using attention-based graph neural network. ||| ankit pal ||| muru selvakumar ||| malaikannan sankarasubbu ||| 
2022 ||| subtst: a combination of sub-word latent topics and sentence transformer for semantic similarity detection. ||| binh dang ||| tran-thai dang ||| le-minh nguyen ||| 
2018 ||| personalized sentiment analysis and a framework with attention-based hawkes process model. ||| siwen guo ||| sviatlana h ||| hn ||| feiyu xu ||| christoph schommer ||| 
2017 ||| which saliency detection method is the best to estimate the human attention for adjective noun concepts?. ||| marco stricker ||| syed saqib bukhari ||| mohammad al-naser ||| seyyed saleh mozafari chanijani ||| damian borth ||| andreas dengel ||| 
2020 ||| vmrfanet: view-specific multi-receptive field attention network for person re-identification. ||| honglong cai ||| yuedong fang ||| zhiguan wang ||| tingchun yeh ||| jinxing cheng ||| 
2021 ||| informer, an information organization transformer architecture. ||| cristian david estupi ||| n ojeda ||| cayetano guerra-artal ||| mario hern ||| ndez-tejera ||| 
2022 ||| bilinear multi-head attention graph neural network for traffic prediction. ||| haibing hu ||| kai han ||| zhizhuo yin ||| 
2022 ||| transformers for low-resource neural machine translation. ||| andargachew mekonnen gezmu ||| andreas n ||| rnberger ||| 
2022 ||| quantifying student attention using convolutional neural networks. ||| andreea coaja ||| catalin v. rusu ||| 
2022 ||| comparing rnn and transformer context representations in the czech answer selection task. ||| marek medved ||| radoslav sabol ||| ales hor ||| k ||| 
2021 ||| human sentence processing: recurrence or attention? ||| danny merkx ||| stefan l. frank ||| 
2021 ||| accounting for agreement phenomena in sentence comprehension with transformer language models: effects of similarity-based interference on surprisal and attention. ||| soo-hyun ryu ||| richard l. lewis ||| 
2021 ||| relation classification with cognitive attention supervision. ||| erik mcguire ||| noriko tomuro ||| 
2021 ||| cognlp-sheffield at cmcl 2021 shared task: blending cognitively inspired features with transformer-based language models for predicting eye tracking patterns. ||| peter vickers ||| rosa wainwright ||| harish tayyar madabushi ||| aline villavicencio ||| 
2021 ||| tempera: spatial transformer feature pyramid network for cardiac mri segmentation. ||| christoforos galazis ||| huiyi wu ||| zhuoyu li ||| camille petri ||| anil a. bharath ||| marta varela ||| 
2021 ||| a multi-view crossover attention u-net cascade with fourier domain adaptation for multi-domain cardiac mri segmentation. ||| marcel beetz ||| jorge corral acero ||| vicente grau ||| 
2021 ||| consistency based co-segmentation for multi-view cardiac mri using vision transformer. ||| zheyao gao ||| xiahai zhuang ||| 
2019 ||| towards predicting attention and workload during math problem solving. ||| ange tato ||| roger nkambou ||| ramla ghali ||| 
2021 ||| emotional robbert and insensitive bertje: combining transformers and affect lexica for dutch emotion detection. ||| luna de bruyne ||| orph ||| e de clercq ||| v ||| ronique hoste ||| 
2021 ||| wassa@iitk at wassa 2021: multi-task learning and transformer finetuning for emotion classification and empathy prediction. ||| jay mundra ||| rohan gupta ||| sagnik mukherjee ||| 
2017 ||| lexicon integrated cnn models with attention for sentiment analysis. ||| bonggun shin ||| timothy lee ||| jinho d. choi ||| 
2017 ||| mining fine-grained opinions on closed captions of youtube videos with an attention-rnn. ||| edison marrese-taylor ||| jorge a. balazs ||| yutaka matsuo ||| 
2021 ||| towards emotion recognition in hindi-english code-mixed data: a transformer based approach. ||| anshul wadhawan ||| akshita aggarwal ||| 
2019 ||| online abuse detection: the value of preprocessing and neural attention models. ||| dhruv kumar ||| robin cohen ||| lukasz golab ||| 
2021 ||| pvg at wassa 2021: a multi-input, multi-task, transformer-based architecture for empathy and distress prediction. ||| atharva kulkarni ||| sunanda somwase ||| shivam rajput ||| manisha marathe ||| 
2018 ||| self-attention: a better building block for sentiment analysis neural network classifiers. ||| artaches ambartsoumian ||| fred popowich ||| 
2017 ||| emoatt at emoint-2017: inner attention sentence embedding for emotion intensity. ||| edison marrese-taylor ||| yutaka matsuo ||| 
2018 ||| nlp at iest 2018: bilstm-attention and lstm-attention via soft voting in emotion classification. ||| qimin zhou ||| hao wu ||| 
2017 ||| improving attention to security in software design with analytics and cognitive techniques. ||| jim whitmore ||| william tobin ||| 
2019 ||| attention-based gated convolutional neural networks for distant supervised relation extraction. ||| xingya li ||| yufeng chen ||| jinan xu ||| yujie zhang ||| 
2018 ||| coherence-based automated essay scoring using self-attention. ||| xia li ||| minping chen ||| jianyun nie ||| zhenxing liu ||| ziheng feng ||| yingdan cai ||| 
2018 ||| tsabcnn: two-stage attention-based convolutional neural network for frame identification. ||| hongyan zhao ||| ru li ||| fei duan ||| zepeng wu ||| shaoru guo ||| 
2019 ||| contextualized word representations with effective attention for aspect-based sentiment analysis. ||| zixuan cao ||| yongmei zhou ||| aimin yang ||| jiahui fu ||| 
2018 ||| attention-based convolutional neural networks for chinese relation extraction. ||| wenya wu ||| yufeng chen ||| jinan xu ||| yujie zhang ||| 
2018 ||| trigger words detection by integrating attention mechanism into bi-lstm neural network - a case study in pubmed-wide trigger words detection for pancreatic cancer. ||| kaiyin zhou ||| xinzhi yao ||| shuguang wang ||| jin-dong kim ||| kevin bretonnel cohen ||| ruiying chen ||| yuxing wang ||| jingbo xia ||| 
2019 ||| a comprehensive verification of transformer in text classification. ||| xiuyuan yang ||| liang yang ||| ran bi ||| hongfei lin ||| 
2020 ||| clickbait detection with style-aware title modeling and co-attention. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| yongfeng huang ||| 
2019 ||| an attention-based approach for mongolian news named entity recognition. ||| mingyan tan ||| feilong bao ||| guanglai gao ||| weihua wang ||| 
2018 ||| medical knowledge attention enhanced neural model for named entity recognition in chinese emr. ||| zhichang zhang ||| yu zhang ||| tong zhou ||| 
2019 ||| multi-label aspect classification on question-answering text with contextualized attention-based neural network. ||| hanqian wu ||| shangbin zhang ||| jingjing wang ||| mumu liu ||| shoushan li ||| 
2018 ||| an end-to-end entity and relation extraction network with multi-head attention. ||| lishuang li ||| yuankai guo ||| shuang qian ||| anqiao zhou ||| 
2017 ||| memory augmented attention model for chinese implicit discourse relation recognition. ||| yang liu ||| jiajun zhang ||| chengqing zong ||| 
2019 ||| syntax-aware attention for natural language inference with phrase-level matching. ||| mingtong liu ||| yasong wang ||| yujie zhang ||| jinan xu ||| yufeng chen ||| 
2019 ||| leveraging multi-head attention mechanism to improve event detection. ||| meihan tong ||| bin xu ||| lei hou ||| juanzi li ||| shuai wang ||| 
2020 ||| mongolian questions classification based on multi-head attention. ||| guangyi wang ||| feilong bao ||| weihua wang ||| 
2020 ||| attention-based graph neural network with global context awareness for document understanding. ||| yuan hua ||| zheng huang ||| jie guo ||| weidong qiu ||| 
2018 ||| question-answering aspect classification with hierarchical attention network. ||| hanqian wu ||| mumu liu ||| jingjing wang ||| jue xie ||| chenlin shen ||| 
2018 ||| attention-based cnn-blstm networks for joint intent detection and slot filling. ||| yufan wang ||| li tang ||| tingting he ||| 
2021 ||| multi-level emotion cause analysis by multi-head attention based multi-task learning. ||| xiangju li ||| shi feng ||| yifei zhang ||| daling wang ||| 
2020 ||| message structure aided attentional convolution network for rf device fingerprinting. ||| lintianran weng ||| jianhua peng ||| jinsong li ||| yuhang zhu ||| 
2020 ||| double attention-based deformable convolutional network for recommendation. ||| zhe li ||| honglong chen ||| kai lin ||| vladimir v. shakhov ||| leyi shi ||| 
2020 ||| gatae: graph attention-based anomaly detection on attributed networks. ||| ziquan you ||| xiaoying gan ||| luoyi fu ||| zhen wang ||| 
2019 ||| learning spatial-channel attention for visual tracking. ||| yingsen zeng ||| haiying wang ||| ting lu ||| 
2021 ||| meta-neighbor aggregated graph attention network for heterogeneous graph representation. ||| jincheng zhang ||| xiaoying gan ||| 
2020 ||| interactive attention encoder network with local context features for aspect-level sentiment analysis. ||| ruyan wang ||| zhongyuan tao ||| 
2019 ||| eye tracking as a method of neuromarketing for attention research - an empirical analysis using the online appointment booking platform from mercedes-benz. ||| veit etzold ||| anika braun ||| tabea wanner ||| 
2020 ||| multimodal fusion with co-attention mechanism. ||| pei li ||| xinde li ||| 
2021 ||| improving the performance of transformer context encoders for ner. ||| avi chawla ||| nidhi mulay ||| vikas bishnoi ||| gaurav dhama ||| 
2020 ||| a method for dissolved gas forecasting in power transformers using ls-svm. ||| j. atherfold ||| terence l. van zyl ||| 
2021 ||| next generation multitarget trackers: random finite set methods vs transformer-based deep learning. ||| juliano pinto ||| georg hess ||| william ljungbergh ||| yuxuan xia ||| lennart svensson ||| henk wymeersch ||| 
2019 ||| dual stream spatio-temporal motion fusion with self-attention for action recognition. ||| md asif jalal ||| waqas aftab ||| roger k. moore ||| lyudmila mihaylova ||| 
2021 ||| emotiv insight with convolutional neural network: visual attention test classification. ||| chean khim toa ||| kok-swee sim ||| shing chiang tan ||| 
2020 ||| aspect level sentiment analysis using bi-directional lstm encoder with the attention mechanism. ||| win lei kay khine ||| nyein thwet thwet aung ||| 
2021 ||| scale input adapted attention for image denoising using a densely connected u-net: sade-net. ||| vedat acar ||| ender mete eksioglu ||| 
2020 ||| simple fine-tuning attention modules for human pose estimation. ||| tien-dat tran ||| xuan-thuy vo ||| moahamammad-ashraf russo ||| kang-hyun jo ||| 
2021 ||| hybrid vision transformer for domain adaptable person re-identification. ||| muhammad danish waseem ||| muhammad atif tahir ||| muhammad nouman durrani ||| 
2019 ||| an hybrid attention-based system for the prediction of facial attributes. ||| souad khellat-kihel ||| zhenan sun ||| massimo tistarelli ||| 
2019 ||| emotion recognition with spatial attention and temporal softmax pooling. ||| masih aminbeidokhti ||| marco pedersoli ||| patrick cardinal ||| eric granger ||| 
2018 ||| a computational model of multi-scale spatiotemporal attention in video data. ||| roman m. palenichka ||| rafael falcon ||| rami s. abielmona ||| emil m. petriu ||| 
2021 ||| a super-resolution method of remote sensing image using transformers. ||| chongjun ye ||| lingyu yan ||| yucheng zhang ||| jun zhan ||| jie yang ||| junfang wang ||| 
2021 ||| a static and dynamic co-attention network for social recommendation. ||| fan bian ||| lingyu yan ||| rong gao ||| kunpeng zheng ||| yucheng zhang ||| jie yang ||| 
2017 ||| comparison of direct and transformer drive high voltage ultrasonic pulser topologies. ||| andrius chaziachmetovas ||| tomas g ||| mez  ||| lvarez-arenas ||| linas svilainis ||| 
2019 ||| a low-cost autonomous attention assessment system for robot intervention with autistic children. ||| fady s. alnajjar ||| abdulrahman majed renawi ||| massimiliano lorenzo cappuccio ||| omar mubin ||| 
2021 ||| a 3d rhythm-based serious game for collaboration improvement of children with attention deficit hyperactivity disorder (adhd). ||| marina giannaraki ||| nektarios moumoutzis ||| yiannis papatzanis ||| elias kourkoutas ||| katerina mania ||| 
2021 ||| metrics based on attention metadata for learning resource and assessment repository. ||| francesco maiorana ||| andrew paul csizmadia ||| gretchen m. richards ||| 
2017 ||| how to throw chocolate at students: a survey of extrinsic means for increased audience attention. ||| mark cieliebak ||| amani magid ||| beatrice pradarelli ||| 
2020 ||| urse forum with self-attention. ||| ming zhang ||| shixing liu ||| yifan wang ||| 
2019 ||| domain specific nmt based on knowledge graph embedding and attention. ||| hao yang ||| gengui xie ||| ying qin ||| song peng ||| 
2022 ||| sea surface temperature prediction approach based on 3d cnn and lstm with attention mechanism. ||| baiyou qiao ||| zhongqiang wu ||| zhong tang ||| gang wu ||| 
2021 ||| sea surface temperature prediction approach based on 3d cnn and lstm with attention mechanism. ||| baiyou qiao ||| zhongqiang wu ||| zhong tang ||| gang wu ||| 
2022 ||| recognition of transformer high frequency partial discharge based on time domain feature. ||| meng liu ||| ying lin ||| qingdong zhu ||| junxin wang ||| yi yang ||| chao gu ||| wenbing zhu ||| wenjie zheng ||| demeng bai ||| jiafeng qin ||| jian wang ||| fengda zhang ||| zhuangzhuang li ||| 
2018 ||| attention based neural architecture for rumor detection with author context awareness. ||| sansiri tarnpradab ||| kien a. hua ||| 
2018 ||| lead-lag relationship between investor sentiment in social media9 investor attention in google, and stock return. ||| aldila rizkiana ||| sari hasrini ||| pameodji hardjomidjojo ||| budhi prihartono ||| indryati sunaryo ||| ilham reza prasetyo ||| 
2018 ||| text classification based on lstm and attention. ||| xuemei bai ||| 
2021 ||| bronchial light microscopy image segmentation based on boundary attention. ||| kunchen li ||| zhexin li ||| yicheng liu ||| qinzhi fang ||| bangwangke tang ||| liping huang ||| xinyu xiong ||| 
2020 ||| study of transformer fault diagnosis based on sparrow optimization algorithm. ||| huang man li ||| yong zhang ||| 
2021 ||| cross-modal retrieval based on big transfer and regional maximum activation of convolutions with generalized attention. ||| wenwen yang ||| yan hua ||| 
2020 ||| survey on automatic text summarization and transformer models applicability. ||| guan wang ||| ivan smetannikov ||| tianxing man ||| 
2017 ||| base on transmission line model to investigate the power margins of main transformers. ||| yen-ming tseng ||| rong-ching wu ||| jeng-shyang pan ||| en-chih chang ||| peijiang li ||| 
2017 ||| capacity reduction of distribution transformer by harmonic effect. ||| yen-ming tseng ||| li-shan chen ||| jeng-shyang pan ||| hsi-shan huang ||| lee ku ||| 
2017 ||| facial emotion recognition skills and measures in children and adolescents with attention deficit hyperactivity disorder (adhd). ||| aliki economides ||| yiannis laouris ||| massimiliano conson ||| anna esposito ||| 
2021 ||| a comparison of evolutionary and neural attention modeling relative to adversarial learning. ||| charlie t. veal ||| marshall lindsay ||| scott d. kovaleski ||| derek t. anderson ||| stanton r. price ||| 
2021 ||| attention-oriented brain storm optimization for multimodal optimization problems. ||| jian yang ||| yuhui shi ||| 
2020 ||| safety isolating transformer design using hyde-df algorithm. ||| jo ||| o p. soares ||| fernando lezama ||| zita a. vale ||| stephane brisset ||| bruno fran ||| ois ||| 
2020 ||| enhanced interactive estimation of distribution algorithms with attention mechanism and restricted boltzmann machine. ||| lin bao ||| xiaoyan sun ||| dunwei gong ||| yong zhang ||| biao xu ||| 
2020 ||| the elephant in the interpretability room: why use attention as explanation when we have saliency methods? ||| jasmijn bastings ||| katja filippova ||| 
2019 ||| transcoding compositionally: using attention to find more generalizable solutions. ||| kris korrel ||| dieuwke hupkes ||| verna dankers ||| elia bruni ||| 
2020 ||| unsupervised evaluation for question answering with transformers. ||| lukas muttenthaler ||| isabelle augenstein ||| johannes bjerva ||| 
2019 ||| from balustrades to pierre vinken: looking for syntax in transformer self-attentions. ||| david marecek ||| rudolf rosa ||| 
2019 ||| analyzing the structure of attention in a transformer language model. ||| jesse vig ||| yonatan belinkov ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| marius mosbach ||| anna khokhlova ||| michael a. hedderich ||| dietrich klakow ||| 
2019 ||| what does bert look at? an analysis of bert's attention. ||| kevin clark ||| urvashi khandelwal ||| omer levy ||| christopher d. manning ||| 
2020 ||| probing for multilingual numerical understanding in transformer-based language models. ||| devin johnson ||| denise mak ||| andrew barker ||| lexi loessberg-zahl ||| 
2020 ||| dissecting lottery ticket transformers: structural and behavioral study of sparse neural machine translation. ||| rajiv movva ||| jason y. zhao ||| 
2020 ||| structured self-attentionweights encode semantics in sentiment analysis. ||| zhengxuan wu ||| thanh-son nguyen ||| desmond c. ong ||| 
2019 ||| detecting political bias in news articles using headline attention. ||| rama rohit reddy gangula ||| suma reddy duggenpudi ||| radhika mamidi ||| 
2019 ||| learning the dyck language with attention-based seq2seq models. ||| xiang yu ||| ngoc thang vu ||| jonas kuhn ||| 
2017 ||| visual attention during simulated autonomous driving in the us and japan. ||| yumiko shinohara ||| rebecca currano ||| wendy ju ||| yukiko nishizaki ||| 
2018 ||| led visualizations for drivers' attention: an exploratory study on experience and associated information contents. ||| sandra tr ||| sterer ||| benedikt streitwieser ||| alexander meschtscherjakov ||| manfred tscheligi ||| 
2018 ||| reducing the attentional demands of in-vehicle touchscreens with stencil overlays. ||| andy cockburn ||| dion woolley ||| kien tran pham thai ||| don clucas ||| simon hoermann ||| carl gutwin ||| 
2018 ||| let me finish before i take over: towards attention aware device integration in highly automated vehicles. ||| philipp wintersberger ||| andreas riener ||| clemens schartm ||| ller ||| anna-katharina frison ||| klemens weigl ||| 
2018 ||| where to look: exploring peripheral cues for shifting attention to spatially distributed out-of-view objects. ||| uwe gruenefeld ||| andreas l ||| cken ||| yvonne br ||| ck ||| susanne boll ||| wilko heuten ||| 
2017 ||| tutorial: how does your hmi design affect the visual attention of the driver. ||| sebastian feuerstack ||| bertram wortelen ||| 
2019 ||| voices in self-driving cars should be assertive to more quickly grab a distracted driver's attention. ||| priscilla n. y. wong ||| duncan p. brumby ||| harsha vardhan ramesh babu ||| kota kobayashi ||| 
2017 ||| guiding driver visual attention with leds. ||| gerald j. schmidt ||| lena rittger ||| 
2020 ||| attention mechanism based adversarial attack against deep reinforcement learning. ||| jinyin chen ||| xueke wang ||| yan zhang ||| haibin zheng ||| shouling ji ||| 
2020 ||| carnet: context attention refine network for semantic segmentation. ||| guanghai wang ||| song wu ||| guoqiang xiao ||| 
2021 ||| a transformer-based chinese non-autoregressive speech synthesis scheme. ||| yueqing cai ||| wenbi rao ||| 
2021 ||| meta-learning based siamese network with channel-wise self-attention for visual tracking. ||| rui wang ||| bin kang ||| wei-ping zhu ||| 
2019 ||| exploring the usability of nesplora aquarium, a virtual reality system for neuropsychological assessment of attention and executive functioning. ||| alexandra voinescu ||| liviu-andrei fodor ||| dana |||  stanton fraser ||| miguel mej ||| as ||| daniel o. david ||| 
2020 ||| modified playback of avatar clip sequences based on student attention in educational vr. ||| adil khokhar ||| andrew yoshimura ||| christoph w. borst ||| 
2020 |||  movie cuts in users' attention. ||| carlos mara ||| es ||| diego gutierrez ||| ana serrano ||| 
2018 ||| a path-based attention guiding technique for assembly environments with target occlusions. ||| patrick renner ||| jonas blattgerste ||| thies pfeiffer ||| 
2018 ||| attention guiding using augmented reality in complex environments. ||| patrick renner ||| thies pfeiffer ||| 
2020 ||| a methodology of eye gazing attention determination for vr training. ||| jingjing zhang ||| meg'n mullikin ||| yi li ||| chao mei ||| 
2019 ||| optimizing visual element placement via visual attention analysis. ||| rawan alghofaili ||| michael s. solah ||| haikun huang ||| yasuhito sawahata ||| marc pomplun ||| lap-fai yu ||| 
2017 ||| coordinating attention and cooperation in multi-user virtual reality narratives. ||| cullen brown ||| ghanshyam bhutra ||| mohamed suhail ||| qinghong xu ||| eric d. ragan ||| 
2021 ||| an interface for enhanced teacher awareness of student actions and attention in a vr classroom. ||| david m. broussard ||| yitoshee rahman ||| arun k. kulshreshth ||| christoph w. borst ||| 
2017 ||| attention guidance for immersive video content in head-mounted displays. ||| fabien danieau ||| antoine guillo ||| renaud dore ||| 
2018 ||| the relationship between visual attention and simulator sickness: a driving simulation study. ||| anne hoesch ||| sandra poeschl ||| florian weidner ||| roberto walter ||| nicola doering ||| 
2021 |||  task attention in cybersickness: a call for a standardized approach to data sharing. ||| stephen b. gilbert ||| angelica jasper ||| nathan c. sepich ||| taylor a. doty ||| jonathan w. kelly ||| michael c. dorneich ||| 
2019 ||| the matter of attention and motivation - understanding unexpected results from auditory localization training using augmented reality. ||| song hui chon ||| sungyoung kim ||| 
2018 ||| empirical evaluation of virtual human conversational and affective animations on visual attention in inter-personal simulations. ||| matias volonte ||| andrew c. robb ||| andrew t. duchowski ||| sabarish v. babu ||| 
2021 ||| text2gestures: a transformer-based network for generating emotive body gestures for virtual agents**this work has been supported in part by aro grants w911nf1910069 and w911nf1910315, and intel. code and additional materials available at: https: //gamma.umd.edu/t2g. ||| uttaran bhattacharya ||| nicholas rewkowski ||| abhishek banerjee ||| pooja guhan ||| aniket bera ||| dinesh manocha ||| 
2019 ||| eye-gaze-triggered visual cues to restore attention in educational vr. ||| andrew yoshimura ||| adil khokhar ||| christoph w. borst ||| 
2021 ||| exploiting object-of-interest information to understand attention in vr classrooms. ||| efe bozkir ||| philipp stark ||| hong gao ||| lisa hasenbein ||| jens-uwe hahn ||| enkelejda kasneci ||| richard g ||| llner ||| 
2018 ||| towards joint attention training for children with asd - a vr game approach and eye gaze exploration. ||| chao mei ||| bushra tasnim zahed ||| lee mason ||| john quarles ||| 
2020 ||| directing versus attracting attention: exploring the effectiveness of central and peripheral cues in panoramic videos. ||| anastasia schmitz ||| andrew macquarrie ||| simon j. julier ||| nicola binetti ||| anthony steed ||| 
2020 ||| infosalgail: visual attention-empowered imitation learning of pedestrian behavior in critical traffic scenarios. ||| igor vozniak ||| matthias klusch ||| andr |||  antakli ||| christian m ||| ller ||| 
2021 ||| collecting data for machine learning on office workers' attention, fatigue, overload, and stress during computer use. ||| rita kovordanyi ||| 
2021 ||| spatio-temporal attention mechanism and knowledge distillation for lip reading. ||| shahd elashmawy ||| marian ramsis ||| hesham m. eraqi ||| farah eldeshnawy ||| hadeel mabrouk ||| omar abugabal ||| nourhan sakr ||| 
2020 ||| combining angiodysplasia classification and segmentation on capsule endoscopy images using attentional albunet. ||| sirichart gobpradit ||| peerapon vateekul ||| 
2019 ||| attention guided relation network for few-shot image classification. ||| imranul ashrafi ||| muntasir mohammad ||| arani shawkat mauree ||| khan mohammad habibullah ||| 
2020 ||| enhance attentional lstm models for power consumption forecasting using asymmetric loss and renewable energy factors. ||| chukwan siridhipakul ||| peerapon vateekul ||| 
2020 ||| machine reading comprehension on multiclass questions using bidirectional attention flow models with contextual embeddings and transfer learning in thai corpus. ||| theerit lapchaicharoenkit ||| peerapon vateekul ||| 
2021 ||| sanger: a co-design framework for enabling sparse attention using reconfigurable architecture. ||| liqiang lu ||| yicheng jin ||| hangrui bi ||| zizhang luo ||| peng li ||| tao wang ||| yun liang ||| 
2020 ||| gobo: quantizing attention-based nlp models for low latency and energy efficient inference. ||| ali hadi zadeh ||| isak edo ||| omar mohamed awad ||| andreas moshovos ||| 
2020 ||| abstract podcast summarization using bart with longformer attention. ||| hannes karlbom ||| ann clifton ||| 
2020 ||| evaluating transformer-kernel models at trec deep learning 2020. ||| sebastian hofst ||| tter ||| allan hanbury ||| 
2020 ||| impact of tokenization, pretraining task, and transformer depth on text ranking. ||| jaap kamps ||| nikolaos kondylidis ||| david rau ||| 
2019 ||| a method to estimate request sentences using lstm with self-attention mechanism. ||| tsubasa ueda ||| makoto okada ||| naoki mori ||| kiyota hashimoto ||| 
2017 ||| effects of design factors of game-based english vocabulary learning app on learning performance, sustained attention, emotional state, and memory retention. ||| chih-fan hsu ||| chih-ming chen ||| ding cao ||| 
2017 ||| an english diagnosis and review system based on brainwave attention recognition technology for the paper-based learning context with digital-pen support. ||| ya-ling huang ||| chih-ming chen ||| mi lin ||| 
2017 ||| internal fault classification algorithm in power transformer based on discrete wavelet transform and fuzzy logic. ||| santipont ananwattanaporn ||| monthon leelajindakrairerk ||| chaiyan jettanasen ||| chaichan pothisarn ||| atthapol ngaopitakkul ||| dimas anton asfani ||| 
2018 ||| advertising visual attention to facebook social network: evidence from eye movements. ||| chun-chia wang ||| jason c. hung ||| chun-hong huang ||| jia-yu chen ||| 
2018 ||| using brainwave characteristics for exploring the effect of integrating graduated-prompting strategy into interactive e-books on students' learning attention. ||| ya-jing yu ||| hsiu-jou chen ||| po-han wu ||| 
2020 ||| text entailment generation with attention-based sequence-to-sequence model. ||| xiaomei zhao ||| hidekazu yanagimoto ||| 
2020 ||| multiple perspective caption generation with attention mechanism. ||| hidekazu yanagimoto ||| maaki shozu ||| 
2019 ||| combining virtual reality advertising and eye tracking to understand visual attention: a pilot study. ||| chun-chia wang ||| shih-cheng wang ||| chiung-pei chu ||| 
2019 ||| a learning environment attracting attention to the nested structure of metacognition of self-regulated learning. ||| kai morita ||| koji tanaka ||| chanakarn kingkaew ||| mitsuru ikeda ||| 
2018 ||| improving effectiveness of learners' review of video lectures by using an attention-based video lecture review mechanism based on brainwave signals. ||| cheng-ho lin ||| chih-ming chen ||| yong-teng lin ||| 
2019 ||| attention analysis in caption generation. ||| maaki shozu ||| hidekazu yanagimoto ||| 
2021 ||| multi-intent attention and top-k network with interactive framework for joint multiple intent detection and slot filling. ||| xu jia ||| jiaxin pan ||| youliang yuan ||| min peng ||| 
2020 ||| multi-layer joint learning of chinese nested named entity recognition based on self-attention mechanism. ||| haoru li ||| haoliang xu ||| longhua qian ||| guodong zhou ||| 
2021 ||| adaptive transformer for multilingual neural machine translation. ||| junpeng liu ||| kaiyu huang ||| jiuyi li ||| huan liu ||| degen huang ||| 
2020 ||| transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis. ||| zhen wu ||| chengcan ying ||| xinyu dai ||| shujian huang ||| jiajun chen ||| 
2020 ||| sentence constituent-aware aspect-category sentiment analysis with graph attention networks. ||| yuncong li ||| cunxiang yin ||| sheng-hua zhong ||| 
2018 ||| a3net: adversarial-and-attention network for machine reading comprehension. ||| jiuniu wang ||| xingyu fu ||| guangluan xu ||| yirong wu ||| ziyan chen ||| yang wei ||| li jin ||| 
2020 ||| memory attention neural network for multi-domain dialogue state tracking. ||| zihan xu ||| zhi chen ||| lu chen ||| su zhu ||| kai yu ||| 
2019 ||| a transformer-based semantic parser for nlpcc-2019 shared task 2. ||| donglai ge ||| junhui li ||| muhua zhu ||| 
2020 ||| encoding sentences with a syntax-aware self-attention neural network for emotion distribution prediction. ||| chang wang ||| bang wang ||| 
2018 ||| cross-lingual emotion classification with auxiliary and attention neural networks. ||| lu zhang ||| liangqing wu ||| shoushan li ||| zhongqing wang ||| guodong zhou ||| 
2021 ||| autonlu: architecture search for sentence and cross-sentence attention modeling with re-designed search space. ||| wei zhu ||| 
2019 ||| attentional neural network for emotion detection in conversations with speaker influence awareness. ||| jia wei ||| shi feng ||| daling wang ||| yifei zhang ||| xiangju li ||| 
2021 ||| attention based reinforcement learning with reward shaping for knowledge graph reasoning. ||| sheng wang ||| xiaoying chen ||| shengwu xiong ||| 
2020 ||| deep hierarchical attention flow for visual commonsense reasoning. ||| yuansheng song ||| ping jian ||| 
2021 ||| autotrans: automating transformer design via reinforced architecture search. ||| wei zhu ||| xiaoling wang ||| yuan ni ||| guotong xie ||| 
2021 ||| searching effective transformer for seq2seq keyphrase generation. ||| yige xu ||| yichao luo ||| yicheng zhou ||| zhengyan li ||| qi zhang ||| xipeng qiu ||| xuanjing huang ||| 
2020 ||| dca: diversified co-attention towards informative live video commenting. ||| zhihan zhang ||| zhiyi yin ||| shuhuai ren ||| xinhang li ||| shicheng li ||| 
2019 ||| interpretable spatial-temporal attention graph convolution network for service part hierarchical demand forecast. ||| wenli ouyang ||| yahong zhang ||| mingda zhu ||| xiuling zhang ||| hongye chen ||| yinghao ren ||| wei fan ||| 
2019 ||| many vs. many query matching with hierarchical bert and transformer. ||| yang xu ||| qiyuan liu ||| dong zhang ||| shoushan li ||| guodong zhou ||| 
2019 ||| charge prediction with legal attention. ||| qiaoben bao ||| hongying zan ||| peiyuan gong ||| junyi chen ||| yanghua xiao ||| 
2019 ||| feature-level attention based sentence encoding for neural relation extraction. ||| longqi dai ||| bo xu ||| hui song ||| 
2021 ||| multi-modal sarcasm detection based on contrastive attention mechanism. ||| xiaoqiang zhang ||| ying chen ||| guangyuan li ||| 
2018 ||| using entity relation to improve event detection via attention mechanism. ||| jingli zhang ||| wenxuan zhou ||| yu hong ||| jianmin yao ||| min zhang ||| 
2018 ||| hierarchical attention based semi-supervised network representation learning. ||| jie liu ||| junyi deng ||| guanghui xu ||| zhicheng he ||| 
2019 ||| using bidirectional transformer-crf for spoken language understanding. ||| linhao zhang ||| houfeng wang ||| 
2020 ||| a submodular optimization-based vae-transformer framework for paraphrase generation. ||| xiaoning fan ||| danyang liu ||| xuejian wang ||| yiding liu ||| gongshen liu ||| bo su ||| 
2017 ||| abstractive document summarization via neural model with joint attention. ||| liwei hou ||| po hu ||| chao bei ||| 
2021 ||| a dual-attention neural network for pun location and using pun-gloss pairs for interpretation. ||| shen liu ||| meirong ma ||| hao yuan ||| jianguo zhu ||| yuanbin wu ||| man lan ||| 
2018 ||| employing multiple decomposable attention networks to resolve event coreference. ||| jie fang ||| peifeng li ||| guodong zhou ||| 
2019 ||| co-attention networks for aspect-level sentiment analysis. ||| haihui li ||| yun xue ||| hongya zhao ||| xiaohui hu ||| sancheng peng ||| 
2017 ||| a convolutional attention model for text classification. ||| jiachen du ||| lin gui ||| ruifeng xu ||| yulan he ||| 
2021 ||| context-aware and data-augmented transformer for interactive argument pair identification. ||| yuanling geng ||| shuqun li ||| fan zhang ||| shaowu zhang ||| liang yang ||| hongfei lin ||| 
2019 ||| neural machine translation with bilingual history involved attention. ||| haiyang xue ||| yang feng ||| di you ||| wen zhang ||| jingyu li ||| 
2019 ||| co-attention and aggregation based chinese recognizing textual entailment model. ||| pengcheng liu ||| lingling mu ||| hongying zan ||| 
2019 ||| variational attention for commonsense knowledge aware conversation generation. ||| guirong bai ||| shizhu he ||| kang liu ||| jun zhao ||| 
2017 ||| look-ahead attention for generation in neural machine translation. ||| long zhou ||| jiajun zhang ||| chengqing zong ||| 
2019 ||| kg-to-text generation with slot-attention and link-attention. ||| yashen wang ||| huanhuan zhang ||| yifeng liu ||| haiyong xie ||| 
2021 ||| knowledge enhanced transformers system for claim stance classification. ||| xiangyang li ||| zheng li ||| sujian li ||| zhimin li ||| shimin yang ||| 
2020 ||| hierarchical multimodal transformer with localness and speaker aware attention for emotion recognition in conversations. ||| xiao jin ||| jianfei yu ||| zixiang ding ||| rui xia ||| xiangsheng zhou ||| yaofeng tu ||| 
2020 ||| hierarchical multi-view attention for neural review-based recommendation. ||| hongtao liu ||| wenjun wang ||| huitong chen ||| wang zhang ||| qiyao peng ||| lin pan ||| pengfei jiao ||| 
2019 ||| using dependency information to enhance attention mechanism for aspect-based sentiment analysis. ||| luwen pu ||| yuexian zou ||| jian zhang ||| shilei huang ||| lin yao ||| 
2019 ||| multi-task multi-head attention memory network for fine-grained sentiment analysis. ||| zehui dai ||| wei dai ||| zhenhua liu ||| fengyun rao ||| huajie chen ||| guangpeng zhang ||| yadong ding ||| jiyang liu ||| 
2020 ||| rumor detection on hierarchical attention network with user and sentiment information. ||| sujun dong ||| zhong qian ||| peifeng li ||| xiaoxu zhu ||| qiaoming zhu ||| 
2019 ||| improving multi-head attention with capsule networks. ||| shuhao gu ||| yang feng ||| 
2018 ||| a novel attention based cnn model for emotion intensity prediction. ||| hongliang xie ||| shi feng ||| daling wang ||| yifei zhang ||| 
2019 ||| cross aggregation of multi-head attention for neural machine translation. ||| juncheng cao ||| hai zhao ||| kai yu ||| 
2018 ||| dependency parsing and attention network for aspect-level sentiment classification. ||| zhifan ouyang ||| jindian su ||| 
2021 ||| skeleton-based sign language recognition with attention-enhanced graph convolutional networks. ||| wuyan liang ||| xiaolong xu ||| 
2019 ||| extending the transformer with context and multi-dimensional mechanism for dialogue response generation. ||| ruxin tan ||| jiahui sun ||| bo su ||| gongshen liu ||| 
2018 ||| summary++: summarizing chinese news articles with attention. ||| juan zhao ||| tong lee chung ||| bin xu ||| minghu jiang ||| 
2017 ||| ahnn: an attention-based hybrid neural network for sentence modeling. ||| xiaomin zhang ||| li huang ||| hong qu ||| 
2021 ||| variational autoencoder with interactive attention for affective text generation. ||| ruijun chen ||| jin wang ||| xuejie zhang ||| 
2018 ||| research on construction method of chinese nt clause based on attention-lstm. ||| teng mao ||| yuyao zhang ||| yuru jiang ||| yangsen zhang ||| 
2019 ||| improving transformer with sequential context representations for abstractive text summarization. ||| tian cai ||| mengjun shen ||| huailiang peng ||| lei jiang ||| qiong dai ||| 
2017 ||| an effective approach for chinese news headline classification based on multi-representation mixed model with attention and ensemble learning. ||| zhonglei lu ||| wenfen liu ||| yanfang zhou ||| xuexian hu ||| binyu wang ||| 
2017 ||| detecting deceptive review spam via attention-based neural networks. ||| xuepeng wang ||| kang liu ||| jun zhao ||| 
2020 ||| non-local second-order attention network for single image super resolution. ||| jiawen lyn ||| sen yan ||| 
2021 ||| transformer-based hierarchical encoder for document classification. ||| harsh sakhrani ||| saloni parekh ||| pratik ratadiya ||| 
2021 ||| attention augmented convolutional transformer for tabular time-series. ||| sharath m. shankaranarayana ||| davor runje ||| 
2021 ||| transformers4rec: bridging the gap between nlp and sequential / session-based recommendation. ||| gabriel de souza pereira moreira ||| sara rabhi ||| jeong min lee ||| ronay ak ||| even oldridge ||| 
2020 ||| tafa: two-headed attention fused autoencoder for context-aware recommendations. ||| jin peng zhou ||| zhaoyue cheng ||| felipe p ||| rez ||| maksims volkovs ||| 
2021 ||| tops, bottoms, and shoes: building capsule wardrobes via cross-attention tensor network. ||| huiyuan chen ||| yusan lin ||| fei wang ||| hao yang ||| 
2018 ||| risk "attention" or "adventure": a qualitative study of novelty and familiarity in music listening. ||| vikas kumar ||| sabirat rubya ||| joseph a. konstan ||| loren terveen ||| 
2020 ||| fissa: fusing item similarity models with self-attention networks for sequential recommendation. ||| jing lin ||| weike pan ||| zhong ming ||| 
2020 ||| meantime: mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. ||| sung min cho ||| eunhyeok park ||| sungjoo yoo ||| 
2017 ||| modeling user session and intent with an attention-based encoder-decoder architecture. ||| pablo loyola ||| chen liu ||| yu hirate ||| 
2020 ||| sse-pt: sequential recommendation via personalized transformer. ||| liwei wu ||| shuqing li ||| cho-jui hsieh ||| james sharpnack ||| 
2017 ||| interpretable convolutional neural networks with dual local and global attention for review rating prediction. ||| sungyong seo ||| jing huang ||| hao yang ||| yan liu ||| 
2018 ||| qualitydeepsense: quality-aware deep learning framework for internet of things applications with sensor-temporal attention. ||| shuochao yao ||| yiran zhao ||| shaohan hu ||| tarek f. abdelzaher ||| 
2020 ||| a novel three-phase transformerless cascaded multilevel inverter topology for grid-connected solar pv applications. ||| phani kumar chamarthi ||| ahmed al-durra ||| saleh a. saleh ||| 
2017 ||| different shapes and dimensions of laminated core on characteristics of a practical single-phase distribution transformer using finite-element analysis. ||| li wang ||| i-ting huang ||| anton v. prokhorov ||| 
2019 ||| a new magnetic linked active neutral point clamp converter for transformer-less direct grid integration of solar photovoltaic systems. ||| a. m. mahfuz-ur-rahman ||| md. rabiul islam ||| kashem m. muttaqi ||| danny sutanto ||| 
2021 ||| a new h7 transformer-less single-phase inverter to improve the performance of grid-connected solar photovoltaic systems. ||| mohua biswas ||| shuvra prokash biswas ||| md. rabiul islam ||| md. ashib rahman ||| kashem m. muttaqi ||| 
2020 ||| portable device for transformer oil inhibitor content analysis using near-infrared spectroscopy wavelength. ||| yang sing leong ||| pin jern ker ||| m. h. hasnul ||| m. a. khamis ||| m. a. hannan ||| md zaini jamaludin ||| looe hui mun ||| 
2019 ||| efficiency characterization and optimal power sharing in a unified ac-dc system employing a line-frequency zig-zag transformer with high winding leakage inductance. ||| annoy kumar das ||| akshatha shetty ||| baylon g. fernandes ||| 
2017 ||| mitigation of electric arc furnace transformer inrush current using soft-starter-based controlled energization. ||| igor a. pires ||| alysson a. p. machado ||| braz de jesus cardoso filho ||| 
2021 ||| a charging strategy for electric vehicle fast charging station to mitigate distribution transformer aging and reduce operation cost. ||| liran zheng ||| zheng an ||| mickael mauger ||| rajendra prasad kandula ||| deepak divan ||| santiago grijalva ||| 
2018 ||| a seamless transition scheme of position sensorless control in industrial permanent magnet motor (pmm) drives with output filter and transformer for electric submersible pumps. ||| jingbo liu ||| thomas a. nondahl ||| jingya dai ||| semyon royak ||| peter b. schmidt ||| 
2018 ||| design and implementation of 10-kv mw-level electronic power transformer (ept). ||| dan wang ||| yun yang ||| jie tian ||| chengxiong mao ||| junfeng zhang ||| 
2019 ||| physics-based design optimization of high frequency transformers for solid state transformer applications. ||| temitayo o. olowu ||| hassan jafari ||| masood moghaddami ||| arif i. sarwat ||| 
2021 ||| a multilevel solid-state transformer-based grid-connected solar photovoltaic systems. ||| amir taghvaie ||| md. enamul haque ||| sajeeb saha ||| md. apel mahmud ||| 
2019 ||| comparative analysis of oil-filled transformer and solid-state transformer for electric arc furnace. ||| o. j. soto-mar ||| n ||| e. a. cano-plata ||| juan carlos balda ||| a. j. ustariz-farf ||| n ||| 
2020 ||| design and implementation of a smart solid-state transformer based power converter for solar pv system. ||| cameron smith ||| md. enamul haque ||| 
2017 ||| transformer management system for energy control of customer demand response and pv systems. ||| t. t. ku ||| chao shun chen ||| c. h. lin ||| cheng ting hsu ||| 
2018 ||| analysis of a shunt phase-shift transformer for multi-generator harmonic elimination. ||| jongwan kim ||| jih-sheng lai ||| 
2020 ||| calculation of model based capacitances of a two-winding high-frequency transformer to predict its natural resonance frequencies. ||| annoy kumar das ||| baylon g. fernandes ||| 
2020 |||  solid-state transformers. ||| saleh a. saleh ||| emre ozkop ||| 
2020 ||| a soft-switching transformer-less step-down converter based on resonant current balance module. ||| yueshi guan ||| hongye cai ||| xiangjun zhang ||| yijie wang ||| wei wang ||| dianguo xu ||| 
2018 ||| condition monitoring techniques of dielectrics in liquid immersed power transformers - a review. ||| saravanan balamurugan ||| rathinam ananthanarayanan ||| 
2021 ||| h9 and h10 transformer-less solar photovoltaic inverters for leakage current suppression and harmonic current reduction. ||| md. faruk kibria ||| sumaya jahan ||| shuvra prokash biswas ||| md. rabiul islam ||| md. ashib rahman ||| kashem m. muttaqi ||| 
2020 ||| soft-switching, self-tuning and optimization technique for solid state transformers based on direct ac-ac matrix converter topology. ||| temitayo o. olowu ||| masood moghaddami ||| arif i. sarwat ||| 
2018 ||| a controlled switching approach to reduction of three-phase transformer inrush currents. ||| joydeep mitra ||| xufeng xu ||| mohammed benidris ||| 
2020 ||| transformerless six-switch (h6)-based single-phase inverter for grid-connected photovoltaic system with reducd leakage current. ||| md. mizanur rahman ||| mohammad shafayet hossain ||| md. shoriful islam talukder ||| m. nasir uddin ||| 
2017 ||| reduction of three-phase voltage unbalance subject to special winding connections of two single-phase distribution transformers of a microgrid system using a designed d-statcom controller. ||| li wang ||| wei-sheng liu ||| chuan-chieh yeh ||| chien-hsiang yu ||| xiu-yu lu ||| bing-lin kuan ||| anton v. prokhorov ||| 
2019 |||  converter transformers. ||| saleh a. saleh ||| xavier f. st-onge ||| c. richard ||| emre ozkop ||| 
2018 ||| enhancement of the dbd power for current-mode converters using the step-up transformer elements. ||| vanesa rueda ||| arnold wiesner ||| rafael d ||| ez ||| hubert piquet ||| 
2017 ||| a cascaded dstatcom integrated with d-y connection distribution transformer for reactive power compensation. ||| yu chen ||| minghao wen ||| xianggen yin ||| ertao lei ||| jinmu lai ||| zhen wang ||| 
2018 ||| exploring a unified attention-based pooling framework for speaker verification. ||| yi liu ||| liang he ||| weiwei liu ||| jia liu ||| 
2021 ||| sams-net: a sliced attention-based neural network for music source separation. ||| tingle li ||| jiawei chen ||| haowen hou ||| ming li ||| 
2021 ||| leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning. ||| zhiping zeng ||| van tung pham ||| haihua xu ||| yerbolat khassanov ||| eng siong chng ||| chongjia ni ||| bin ma ||| 
2021 ||| transformer-based empathetic response generation using dialogue situation and advanced-level definition of empathy. ||| yi-hsuan wang ||| jia-hao hsu ||| chung-hsien wu ||| tsung-hsien yang ||| 
2018 ||| hybrid ctc-attention based end-to-end speech recognition using subword units. ||| zhangyu xiao ||| zhijian ou ||| wei chu ||| hui lin ||| 
2021 ||| context-dependent label smoothing regularization for attention-based end-to-end code-switching speech recognition. ||| zheying huang ||| peng li ||| ji xu ||| pengyuan zhang ||| yonghong yan ||| 
2021 ||| non-autoregressive deliberation-attention based end-to-end asr. ||| changfeng gao ||| gaofeng cheng ||| jun zhou ||| pengyuan zhang ||| yonghong yan ||| 
2021 ||| improving attention-based end-to-end asr by incorporating an n-gram neural network. ||| junyi ao ||| tom ko ||| 
2021 ||| dialogue act recognition using branch architecture with attention mechanism for imbalanced data. ||| mengfei wu ||| longbiao wang ||| yuke si ||| jianwu dang ||| 
2018 ||| multi-head attention for end-to-end neural machine translation. ||| ivan fung ||| brian mak ||| 
2021 ||| an attention-augmented fully convolutional neural network for monaural speech enhancement. ||| zezheng xu ||| ting jiang ||| chao li ||| jiacheng yu ||| 
2018 ||| an analysis of decoding for attention-based end-to-end mandarin speech recognition. ||| dongwei jiang ||| wei zou ||| shuaijiang zhao ||| guilin yang ||| xiangang li ||| 
2021 ||| an investigation of positional encoding in transformer-based end-to-end speech recognition. ||| fengpeng yue ||| tom ko ||| 
2021 ||| attention-guided cutmix data augmentation network for fine-grained bird recognition. ||| wenming guo ||| yifei wang ||| fang han ||| 
2021 ||| chinese entity relation extraction based on multi-level gated recurrent mechanism and self-attention. ||| zicheng zhong ||| 
2021 ||| target-specific sentiment analysis of dual-channel interactive attention network. ||| kai wen ||| xuhong zhang ||| na huo ||| yuhui wan ||| 
2021 ||| application of channel attention for speaker recognition in the wild. ||| zhi chen ||| lei wang ||| 
2021 ||| a transformer network for captcha recognition. ||| yuliang shi ||| xin liu ||| song han ||| yingguang lu ||| xiangdong zhang ||| 
2021 ||| entity relationship extraction based on bi-lstm and attention mechanism. ||| ming wei ||| zhipeng xu ||| jiwei hu ||| 
2021 ||| irony recognition combined with lda and improved one-dimensional intra-attention model. ||| kai wen ||| hanlin wang ||| jiali lu ||| zhenkang zhang ||| 
2021 ||| audio-visual salieny network with audio attention module. ||| shuaiyang cheng ||| xing gao ||| liang song ||| jianbing xiahou ||| 
2020 ||| imshell-dec: pay more attention to external links in powershell. ||| ruidong han ||| chao yang ||| jianfeng ma ||| siqi ma ||| yunbo wang ||| feng li ||| 
2017 ||| on active sharing and responses to joint attention bids by children with autism in a loosely coupled collaborative play environment. ||| tiffany ya tang ||| pinata winoto ||| aonan guan ||| 
2017 ||| a multi-user tabletop application to train children with autism social attention coordination skills without forcing eye-gaze following. ||| pinata winoto ||| tiffany ya tang ||| 
2020 ||| cross-lingual transformers for neural automatic post-editing. ||| dongjun lee ||| 
2018 ||| a transformer-based multi-source automatic post-editing system. ||| santanu pal ||| nico herbig ||| antonio kr ||| ger ||| josef van genabith ||| 
2018 ||| input combination strategies for multi-source transformer decoder. ||| jindrich libovick ||| jindrich helcl ||| david marecek ||| 
2018 ||| parameter sharing methods for multilingual self-attentional translation models. ||| devendra singh sachan ||| graham neubig ||| 
2018 ||| cuni transformer neural mt system for wmt18. ||| martin popel ||| 
2018 ||| an analysis of attention mechanisms: the case of word sense disambiguation in neural machine translation. ||| gongbo tang ||| rico sennrich ||| joakim nivre ||| 
2019 ||| transformer-based automatic post-editing model with joint encoder and multi-source attention of decoder. ||| wonkee lee ||| jaehun shin ||| jong-hyeok lee ||| 
2018 ||| neural machine translation with the transformer and multi-source romance languages for the biomedical wmt 2018 task. ||| brian tubay ||| marta r. costa-juss ||| 
2019 ||| the en-ru two-way integrated machine translation system based on transformer. ||| doron yu ||| 
2020 ||| translating similar languages: role of mutual intelligibility in multilingual transformers. ||| ife adebara ||| el moatez billah nagoudi ||| muhammad abdul-mageed ||| 
2020 ||| attention transformer model for translation of similar languages. ||| farhan dhanani ||| muhammad rafi ||| 
2018 ||| multi-source transformer with combined losses for automatic post editing. ||| amirhossein tebbifakhr ||| ruchit agrawal ||| matteo negri ||| marco turchi ||| 
2018 ||| multi-encoder transformer network for automatic post-editing. ||| jaehun shin ||| jong-hyeok lee ||| 
2020 ||| transformer-based neural machine translation system for hindi - marathi: wmt20 shared task. ||| amit kumar ||| rupjyoti baruah ||| rajesh kumar mundotiya ||| anil kumar singh ||| 
2021 ||| translation transformers rediscover inherent data domains. ||| maksym del ||| elizaveta korotkova ||| mark fishel ||| 
2018 ||| quality estimation with force-decoded attention and cross-lingual embeddings. ||| elizaveta yankovskaya ||| andre t ||| ttar ||| mark fishel ||| 
2020 ||| filtering noisy parallel corpus using transformers with proxy task learning. ||| haluk a ||| ar ||| i ||| ek ||| talha  ||| olakoglu ||| pinar ece aktan hatipoglu ||| chong hsuan huang ||| wei peng ||| 
2019 ||| incorporating source syntax into transformer-based neural machine translation. ||| anna currey ||| kenneth heafield ||| 
2018 ||| ms-uedin submission to the wmt2018 ape shared task: dual-source transformer for automatic post-editing. ||| marcin junczys-dowmunt ||| roman grundkiewicz ||| 
2019 ||| english-czech systems in wmt19: document-level transformer. ||| martin popel ||| dominik mach ||| cek ||| michal auersperger ||| ondrej bojar ||| pavel pecina ||| 
2021 ||| direct exploitation of attention weights for translation quality estimation. ||| lisa yankovskaya ||| mark fishel ||| 
2018 ||| on the alignment problem in multi-head attention-based neural machine translation. ||| tamer alkhouli ||| gabriel bretschner ||| hermann ney ||| 
2019 ||| multi-source transformer for kazakh-russian-english neural machine translation. ||| patrick littell ||| chi-kiu lo ||| samuel larkin ||| darlene a. stewart ||| 
2017 ||| biasing attention-based recurrent neural networks using external alignment information. ||| tamer alkhouli ||| hermann ney ||| 
2019 ||| fast and accurate capitalization and punctuation for automatic speech recognition using transformer and chunk merging. ||| binh nguyen ||| vu bao hung nguyen ||| hien nguyen ||| pham ngoc phuong ||| the-loc nguyen ||| quoc truong do ||| luong chi mai ||| 
2021 ||| simultaneous speech-to-speech translation system with transformer-based incremental asr, mt, and tts. ||| ryo fukuda ||| sashi novitasari ||| yui oka ||| yasumasa kano ||| yuki yano ||| yuka ko ||| hirotaka tokuyama ||| kosuke doi ||| tomoya yanagita ||| sakriani sakti ||| katsuhito sudoh ||| satoshi nakamura ||| 
2021 ||| multi-encoder sequential attention network for context-aware speech recognition in japanese dialog conversation. ||| nobuya tachimori ||| sakriani sakti ||| satoshi nakamura ||| 
2018 ||| from coarse attention to fine-grained gaze: a two-stage 3d fully convolutional network for predicting eye gaze in first person video. ||| zehua zhang ||| david j. crandall ||| chen yu ||| sven bambach ||| 
2019 ||| construct dynamic graphs for hand gesture recognition via spatial-temporal attention. ||| yuxiao chen ||| long zhao ||| xi peng ||| jianbo yuan ||| dimitris n. metaxas ||| 
2020 ||| image harmonization with attention-based deep feature modulation. ||| guoqing hao ||| satoshi iizuka ||| kazuhiro fukui ||| 
2018 ||| stacked dense u-nets with dual transformers for robust face alignment. ||| jia guo ||| jiankang deng ||| niannan xue ||| stefanos zafeiriou ||| 
2018 ||| reciprocal attention fusion for visual question answering. ||| moshiur r. farazi ||| salman h. khan ||| 
2017 ||| deep reinforcement learning attention selection for person re-identification. ||| xu lan ||| hangxiao wang ||| shaogang gong ||| xiatian zhu ||| 
2019 ||| learning target-aware attention for robust tracking with conditional adversarial network. ||| xiao wang ||| rui yang ||| tao sun ||| bin luo ||| 
2019 ||| body part alignment and temporal attention pooling for video-based person re-identification. ||| sai saketh rambhatla ||| michael jones ||| 
2018 ||| ican: instance-centric attention network for human-object interaction detection. ||| chen gao ||| yuliang zou ||| jia-bin huang ||| 
2019 ||| joint spatial and layer attention for convolutional networks. ||| tony joseph ||| konstantinos g. derpanis ||| faisal z. qureshi ||| 
2019 ||| document binarization using recurrent attention generative model. ||| shuchun liu ||| feiyun zhang ||| mingxi chen ||| yufei xie ||| pan he ||| jie shao ||| 
2020 ||| attention distillation for learning video representations. ||| miao liu ||| xin chen ||| yun zhang ||| yin li ||| james m. rehg ||| 
2019 ||| spatial transformer spectral kernels for deformable image registration. ||| ebrahim al safadi ||| xubo song ||| 
2017 ||| autoscaler: scale-attention networks for visual correspondence. ||| shenlong wang ||| linjie luo ||| ning zhang ||| jia li ||| 
2019 ||| relation-aware multiple attention siamese networks for robust visual tracking. ||| fangyi zhang ||| bingpeng ma ||| hong chang ||| shiguang shan ||| xilin chen ||| 
2020 ||| asap-net: attention and structure aware point cloud sequence segmentation. ||| hanwen cao ||| yongyi lu ||| bo pang ||| cewu lu ||| alan l. yuille ||| gongshen liu ||| 
2019 ||| pcas: pruning channels with attention statistics for deep network compression. ||| kohei yamamoto ||| kurato maeno ||| 
2018 ||| regional attention based deep feature for image retrieval. ||| jaeyoon kim ||| sung-eui yoon ||| 
2020 ||| contrastively-reinforced attention convolutional neural network for fine-grained image recognition. ||| dichao liu ||| yu wang ||| jien kato ||| kenji mase ||| 
2018 ||| human activity recognition with pose-driven attention to rgb. ||| fabien baradel ||| christian wolf ||| julien mille ||| 
2018 ||| destnet: densely fused spatial transformer networks. ||| roberto annunziata ||| christos sagonas ||| jacques cal ||| 
2020 ||| paying more attention to snapshots of iterative pruning: improving model compression via ensemble distillation. ||| duong h. le ||| vo trung nhan ||| nam thoai ||| 
2018 ||| point attention network for gesture recognition using point cloud data. ||| cherdsak kingkan ||| joshua owoyemi ||| koichi hashimoto ||| 
2020 ||| two-stream spatiotemporal compositional attention network for videoqa. ||| taiki miyanishi ||| takuya maekawa ||| motoaki kawanabe ||| 
2020 ||| a better use of audio-visual cues: dense video captioning with bi-modal transformer. ||| vladimir iashin ||| esa rahtu ||| 
2020 ||| marginalized graph attention hashing for zero-shot image retrieval. ||| meixue huang ||| dayan wu ||| wanqian zhang ||| zhi xiong ||| bo li ||| weiping wang ||| 
2019 ||| ms-gan: text to image synthesis with attention-modulated generators and similarity-aware discriminators. ||| fengling mao ||| bingpeng ma ||| hong chang ||| shiguang shan ||| xilin chen ||| 
2019 ||| attention-based facial behavior analytics insocial communication. ||| lezi wang ||| chongyang bai ||| maksim bolonkin ||| judee k. burgoon ||| norah e. dunbar ||| v. s. subrahmanian ||| dimitris n. metaxas ||| 
2018 ||| progressive attention networks for visual attribute prediction. ||| paul hongsuck seo ||| zhe lin ||| scott cohen ||| xiaohui shen ||| bohyung han ||| 
2018 ||| attentional alignment networks. ||| lei yue ||| xin miao ||| pengbo wang ||| baochang zhang ||| xiantong zhen ||| xianbin cao ||| 
2019 ||| graph-based knowledge distillation by multi-head attention network. ||| seunghyun lee ||| byung cheol song ||| 
2018 ||| attention is all we need: nailing down object-centric attention for egocentric activity recognition. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2019 ||| spatially and temporally efficient non-local attention network for video-based person re-identification. ||| chih-ting liu ||| chih-wei wu ||| yu-chiang frank wang ||| shao-yi chien ||| 
2018 ||| deep attentional structured representation learning for visual recognition. ||| krishna kanth nakka ||| mathieu salzmann ||| 
2018 ||| bam: bottleneck attention module. ||| jongchan park ||| sanghyun woo ||| joon-young lee ||| in so kweon ||| 
2020 ||| learning to pay attention to mistakes. ||| moucheng xu ||| neil oxtoby ||| daniel c. alexander ||| joseph jacob ||| 
2019 ||| progressive face super-resolution via attention to facial landmark. ||| deokyun kim ||| minseon kim ||| gihyun kwon ||| daeshik kim ||| 
2020 ||| sofa-net: second-order and first-order attention network for crowd counting. ||| haoran duan ||| shidong wang ||| yu guan ||| 
2019 ||| attentional demand estimation with attentive driving models. ||| petar palasek ||| nilli lavie ||| luke palmer ||| 
2019 ||| end-to-end information extraction by character-level embedding and multi-stage attentional u-net. ||| tuan anh nguyen dang ||| dat nguyen thanh ||| 
2018 ||| recurrent transformer network for remote sensing scene categorisation. ||| zan chen ||| shidong wang ||| xingsong hou ||| ling shao ||| 
2019 ||| focused attention for action recognition. ||| vladyslav sydorov ||| karteek alahari ||| cordelia schmid ||| 
2018 ||| self-attention learning for person re-identification. ||| minyue jiang ||| yuan yuan ||| qi wang ||| 
2017 ||| semantic segmentation with reverse attention. ||| qin huang ||| chi-hao wu ||| chunyang xia ||| ye wang ||| c.-c. jay kuo ||| 
2020 ||| conditional attention for content-based image retrieval. ||| zechao hu ||| adrian g. bors ||| 
2018 ||| pyramid attention network for semantic segmentation. ||| hanchao li ||| pengfei xiong ||| jie an ||| lingxue wang ||| 
2020 ||| taxi demand prediction based on lstm with residuals and multi-head attention. ||| chih-jung hsu ||| hung-hsuan chen ||| 
2020 ||| light field image compression using multi-branch spatial transformer networks based view synthesis. ||| jin wang ||| qianwen wang ||| ruiqin xiong ||| qing zhu ||| baocai yin ||| 
2020 ||| wide and deep learning for video summarization via attention mechanism and independently recurrent neural network. ||| juanping zhou ||| lu lu ||| 
2017 ||| analyzing students' attention in class using wearable devices. ||| xin zhang ||| cheng-wei wu ||| philippe fournier-viger ||| lan-da van ||| yu-chee tseng ||| 
2021 ||| sign language translation using multi context transformer. ||| m. badri narayanan ||| k. mahesh bharadwaj ||| g. r. nithin ||| dhiganth rao padamnoor ||| vineeth vijayaraghavan ||| 
2021 ||| nahuatl neural machine translation using attention based architectures: a comparative analysis for rnns and transformers as a mobile application service. ||| sergio khalil bello garc ||| a ||| eduardo s ||| nchez lucero ||| edmundo bonilla huerta ||| jos |||  crisp ||| n hern ||| ndez hern ||| ndez ||| jos |||  federico ram ||| rez-cruz ||| blanca estela pedroza-m ||| ndez ||| 
2020 ||| data augmentation with transformers for text classification. ||| jos |||  medardo tapia-t ||| llez ||| hugo jair escalante ||| 
2021 ||| modelling of high frequency coreless planar transformer with twr hexagonal winding. ||| m. firdaus zainal abidin ||| mohd nadzri mamat ||| mohd fadzil bin ain ||| 
2021 ||| effect of harmonics current on the performance of current transformers. ||| muhammad syarifuddin bin sha'ari ||| noramalina abdullah ||| 
2018 ||| retargeting 4k video for mobile access using visual attention and temporal stabilization. ||| rohit kumar ||| lino ferreira ||| pedro a. amado assun ||| o ||| antonio navarro ||| 
2019 ||| entity synonym discovery via multiple attentions. ||| jiale yu ||| weiming lu ||| wei xu ||| zeyun tang ||| 
2019 ||| attention-based direct interaction model for knowledge graph embedding. ||| bo zhou ||| yubo chen ||| kang liu ||| jun zhao ||| 
2019 ||| dispute generation in law documents via joint context and topic attention. ||| sheng bi ||| xiya cheng ||| jiamin chen ||| guilin qi ||| meng wang ||| youyong zhou ||| lusheng wang ||| 
2021 ||| identification of dietary supplement use from electronic health records using transformer-based language models. ||| sicheng zhou ||| dalton schutte ||| aiwen xing ||| jiyang chen ||| julian wolfson ||| zhe he ||| fang yu ||| rui zhang ||| 
2021 ||| automatic assignment of icd-10 codes to diagnostic texts using transformers based techniques. ||| mihai horia popescu ||| kevin roitero ||| stefano travasci ||| vincenzo della mea ||| 
2021 ||| identify diabetic retinopathy-related clinical concepts using transformer-based natural language processing methods. ||| zehao yu ||| xi yang ||| gianna l. sweeting ||| yinghan ma ||| skylar e. stolte ||| ruogu fang ||| yonghui wu ||| 
2021 ||| catan: chart-aware temporal attention network for adverse outcome prediction. ||| zelalem gero ||| joyce c. ho ||| 
2021 ||| (m)slae-net: multi-scale multi-level attention embedded network for retinal vessel segmentation. ||| shreshth saini ||| geetika agrawal ||| 
2019 ||| multiple mace risk prediction using multi-task recurrent neural network with attention. ||| enliang xu ||| shiwan zhao ||| jing mei ||| eryu xia ||| yiqin yu ||| songfang huang ||| 
2019 ||| combined attention mechanism for named entity recognition in chinese electronic medical records. ||| luqi li ||| li hou ||| 
2019 ||| multimodal attention network for trauma activity recognition from spoken language and environmental sound. ||| yue gu ||| ruiyu zhang ||| xinwei zhao ||| shuhong chen ||| jalal abdulbaqi ||| ivan marsic ||| megan cheng ||| randall s. burd ||| 
2018 ||| chinese clinical entity recognition via attention-based cnn-lstm-crf. ||| zengjian liu ||| xiaolong wang ||| qingcai chen ||| buzhou tang ||| 
2019 ||| a self-attention based deep learning method for lesion attribute detection from ct reports. ||| yifan peng ||| ke yan ||| veit sandfort ||| ronald m. summers ||| zhiyong lu ||| 
2021 ||| dementia detection using transformer-based deep learning and natural language processing models. ||| ploypaphat saltz ||| shih yin lin ||| sunny chieh cheng ||| dong si ||| 
2020 ||| traffic flow forecasting using a spatial-temporal attention graph convolutional network predictor. ||| shan jiang ||| meiling zhu ||| jin li ||| 
2021 ||| st-gwann: a novel spatial-temporal graph wavelet attention neural network for traffic prediction. ||| zunhao liu ||| zhiming ding ||| bowen yang ||| lei yuan ||| lutong li ||| nannan jia ||| 
2020 ||| environment classification for global navigation satellite systems using attention-based recurrent neural networks. ||| haichun liu ||| minmin zhang ||| ling pei ||| wei wang ||| lanzhen li ||| changchun pan ||| zeya li ||| 
2020 ||| attention u-net for road extraction in remote sensing images. ||| minyu tao ||| zhiming ding ||| yang cao ||| 
2021 ||| a dynamic traffic community prediction model based on hierarchical graph attention network. ||| lutong li ||| mengmeng chang ||| zhiming ding ||| zunhao liu ||| nannan jia ||| 
2018 ||| modeling of rosen-type piezoelectric transformer by mean of a polynomial approach. ||| d. j. falimiaramanana ||| f. e. ratolojanahary ||| i. naciri ||| p. m. rabotovao ||| l. elmaimouni ||| jean etienne lefebvre ||| m. rguiti ||| tadeusz gryba ||| 
2021 ||| the effect of outdoor monitor on people's attention. ||| chau tran ||| ahmad bilal aslam ||| muhammad waqas ||| islam tariq ||| 
2019 ||| lean model of services for the improvement in the times of attention of the emergency areas of the health sector. ||| lucero calero ||| aracelli maccasi ||| carlos raymundo ||| 
2019 ||| improvement of attention times and efficiency of container movements in a port terminal using a truck appointment system, lifo management and poka yoke. ||| luis serme ||| o ||| jimmy orellana ||| juan eyzaguirre ||| carlos raymundo ||| 
2021 ||| lost people: how national ai-strategies paying attention to users. ||| pertti saariluoma ||| henrikki salo-p ||| ntinen ||| 
2021 ||| iteration of children with attention deficit disorder, impulsivity and hyperactivity, cognitive behavioral therapy, and artificial intelligence. ||| luis serpa-andrade ||| roberto garc ||| a v ||| lez ||| graciela serpa-andrade ||| 
2019 ||| genesis of attention in the process of interaction weak visual person - work system in a local environment. ||| jorge gil tejeda ||| lorena olmos pineda ||| 
2019 ||| visual attention convergence index for virtual reality experiences. ||| pawel kobylinski ||| grzegorz pochwatko ||| 
2021 ||| the impact of virtual reality, augmented reality, and interactive whiteboards on the attention management in secondary school stem teaching. ||| maria erofeeva ||| nils oliver klowait ||| 
2019 ||| positional self-attention based hierarchical image captioning. ||| qianxia ma ||| jingyan song ||| tao zhang ||| 
2019 ||| an attention module for multi-person pose estimation. ||| daxing chen ||| xinghao song ||| shixi fan ||| hongpeng wang ||| 
2019 ||| attention grasping network: a real-time approach to generating grasp synthesis. ||| qipeng gu ||| jianhua su ||| xueting bi ||| 
2019 ||| how does algorithmic filtering influence attention inequality on social media? ||| kayla guangrui li ||| sunil mithas ||| zhixing zhang ||| kar yan tam ||| 
2020 ||| assessing ad attention through clustering viewport trajectories. ||| lennard schmidt ||| erik maier ||| 
2020 ||| emerging leaders in digital work: toward a theory of attentional leadership. ||| julian prester ||| dubravka cecez-kecmanovic ||| daniel schlagwein ||| michael cahalane ||| 
2017 ||| helping employees to be digital transformers - the olympus.connect case. ||| benjamin mueller ||| uta renken ||| 
2019 ||| detecting senior executives' personalities for predicting corporate behaviors: an attention-based deep learning approach. ||| kai yang ||| raymond y. k. lau ||| 
2019 ||| impact of it use on the collective attentional engagement to innovation: the case of a organization in the cork sector. ||| victor barros ||| isabel ramos ||| 
2019 ||| selling information when attention is limited: an empirical analysis of an online investment advisory platform. ||| ding li ||| nan chen ||| khim yong-goh ||| 
2018 ||| i or you: whom should online reviewers direct their attention to, and when? ||| zhanfei lei ||| dezhi yin ||| han zhang ||| 
2020 ||| exploring transformer text generation for medical dataset augmentation. ||| ali amin-nejad ||| julia ive ||| sumithra velupillai ||| 
2020 ||| sign language recognition with transformer networks. ||| mathieu de coster ||| mieke van herreweghe ||| joni dambre ||| 
2020 ||| recognizing semantic relations by combining transformers and fully connected models. ||| dmitri roussinov ||| serge sharoff ||| nadezhda puchnina ||| 
2020 ||| why attention is not explanation: surgical intervention and causal reasoning about neural models. ||| christopher grimsley ||| elijah mayfield ||| julia r. s. bursten ||| 
2018 ||| incorporating semantic attention in video description generation. ||| natsuda laokulrat ||| naoaki okazaki ||| hideki nakayama ||| 
2020 ||| unior nlp at mwsa task - globalex 2020: siamese lstm with attention for word sense alignment. ||| raffaele manna ||| giulia speranza ||| maria pia di buono ||| johanna monti ||| 
2020 ||| adaptation of deep bidirectional transformers for afrikaans language. ||| sello ralethe ||| 
2020 ||| transfer learning from transformers to fake news challenge stance detection (fnc-1) task. ||| valeriya slovikovskaya ||| giuseppe attardi ||| 
2020 ||| evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering. ||| sarvesh soni ||| kirk roberts ||| 
2018 ||| a multimodal corpus for mutual gaze and joint attention in multiparty situated interaction. ||| dimosthenis kontogiorgos ||| vanya avramova ||| simon alexandersson ||| patrik jonell ||| catharine oertel ||| jonas beskow ||| gabriel skantze ||| joakim gustafson ||| 
2020 ||| sentence level human translation quality estimation with attention-based neural networks. ||| yu yuan ||| serge sharoff ||| 
2018 ||| attention for implicit discourse relation recognition. ||| andre cianflone ||| leila kosseim ||| 
2020 ||| stress test evaluation of transformer-based models in natural language understanding tasks. ||| carlos aspillaga ||| andr ||| s carvallo ||| vladimir araujo ||| 
2020 ||| contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task. ||| md. tahmid rahman laskar ||| jimmy xiangji huang ||| enamul hoque ||| 
2022 ||| dota: detect and omit weak attentions for scalable transformer acceleration. ||| zheng qu ||| liu liu ||| fengbin tu ||| zhaodong chen ||| yufei ding ||| yuan xie ||| 
2020 ||| densegats: a graph-attention-based network for nonlinear character deformation. ||| tianxing li ||| rui shi ||| takashi kanai ||| 
2020 ||| a multi-input cnns with attention for skin lesion classification. ||| jing wu ||| wei hu ||| yonghao wang ||| yuan wen ||| 
2020 ||| dual-level attention based on heterogeneous graph convolution network for aspect-based sentiment classification. ||| peng yuan ||| lei jiang ||| jianxun liu ||| dong zhou ||| pei li ||| yang gao ||| 
2020 ||| a framework of distribution transformer health early warning system based on edc-gru network. ||| wei liu ||| jian su ||| lvying qin ||| yu huang ||| xu huai ||| qi liu ||| limei zhou ||| yuwei shang ||| xueyang liu ||| 
2021 ||| comformer: code comment generation via transformer and fusion method-based hybrid code representation. ||| guang yang ||| xiang chen ||| jinxin cao ||| shuyuan xu ||| zhanqi cui ||| chi yu ||| ke liu ||| 
2021 ||| use of deep learning model with attention mechanism for software fault prediction. ||| ting-yan yu ||| chin-yu huang ||| neil c. fang ||| 
2020 ||| reliable and robust weakly supervised attention networks for surface defect detection. ||| zijian zhang ||| chaozhang lv ||| meijun sun ||| zheng wang ||| 
2020 ||| no-reference stereoscopic image quality assessment based on visual attention mechanism. ||| sumei li ||| ping zhao ||| yongli chang ||| 
2018 ||| channel attention and multi-level features fusion for single image super-resolution. ||| yue lu ||| yun zhou ||| zhuqing jiang ||| xiaoqiang guo ||| zixuan yang ||| 
2021 ||| progressive co-attention network for fine-grained visual classification. ||| tian zhang ||| dongliang chang ||| zhanyu ma ||| jun guo ||| 
2018 ||| convolutional neural networks with generalized attentional pooling for action recognition. ||| yunfeng wang ||| wengang zhou ||| qilin zhang ||| houqiang li ||| 
2019 ||| attentional part-based network for person re-identification. ||| yinsong xu ||| zhuqing jiang ||| aidong men ||| jiangbo pei ||| guodong ju ||| bo yang ||| 
2021 ||| underwater image enhancement with multi-scale residual attention network. ||| yosuke ueki ||| masaaki ikehara ||| 
2018 ||| potential of attention mechanism for classification of optical coherence tomography images. ||| zhihua shang ||| zilong fu ||| chuanbin liu ||| hongtao xie ||| yongdong zhang ||| 
2021 ||| data transformer for anomalous trajectory detection. ||| hsuan-jen psan ||| wen-jiin tsai ||| 
2019 ||| efficient dual attention module for real-time visual tracking. ||| yingsen zeng ||| xiaoqiang guo ||| haiying wang ||| mingjin geng ||| ting lu ||| 
2019 ||| dsan: double supervised network with attention mechanism for scene text recognition. ||| yuting gao ||| zheng huang ||| yuchen dai ||| cheng xu ||| kai chen ||| jie guo ||| 
2019 ||| enhanced semantic features via attention for real-time visual tracking. ||| mingjin geng ||| haiying wang ||| yingsen zeng ||| 
2020 ||| learning graph topology representation with attention networks. ||| yuanyuan qi ||| jiayue zhang ||| weiran xu ||| jun guo ||| honggang zhang ||| 
2020 ||| attention-guided fusion network of point cloud and multiple views for 3d shape recognition. ||| bo peng ||| zengrui yu ||| jianjun lei ||| jiahui song ||| 
2021 ||| attention-guided convolutional neural network for lightweight jpeg compression artifacts removal. ||| gang zhang ||| haoquan wang ||| yedong wang ||| haijie shen ||| 
2021 ||| hcit: deepfake video detection using a hybrid model of cnn features and vision transformer. ||| bachir kaddar ||| sid ahmed fezza ||| wassim hamidouche ||| zahid akhtar ||| abdenour hadid ||| 
2021 ||| maps: joint multimodal attention and pos sequence generation for video captioning. ||| cong zou ||| xuchen wang ||| yaosi hu ||| zhenzhong chen ||| shan liu ||| 
2017 ||| object localization in weakly labeled data using regularized attention networks. ||| eu wern teh ||| zhenyu guo ||| yang wang ||| 
2021 ||| action recognition improved by correlations and attention of subjects and scene. ||| manh-hung ha ||| oscal tzyh-chiang chen ||| 
2018 ||| spatiotemporal attention on sliced parts for video-based person re-identification. ||| xu yang ||| bin zhang ||| yuan dong ||| fengye xiong ||| hongliang bai ||| 
2018 ||| deep network with spatial and channel attention for person re-identification. ||| tiansheng guo ||| dongfei wang ||| zhuqing jiang ||| aidong men ||| yun zhou ||| 
2019 ||| multi-heads attention graph convolutional networks for skeleton-based action recognition. ||| guowei zhang ||| xin zhang ||| 
2018 ||| omnidirectional video streaming using visual attention-driven dynamic tiling for vr. ||| cagri ozcinar ||| juli ||| n cabrera ||| aljosa smolic ||| 
2017 ||| clothing retrieval with visual attention model. ||| zhonghao wang ||| yujun gu ||| ya zhang ||| jun zhou ||| xiao gu ||| 
2020 ||| learning convolution feature aggregation via edge attention convolution network for person re-identification. ||| chaoqun lin ||| ruo-pei guo ||| mingkun li ||| xianbiao qi ||| chun-guang li ||| 
2019 ||| facial attention based convolutional neural network for 2d+3d facial expression recognition. ||| yang jiao ||| yi niu ||| yuting zhang ||| fu li ||| chunbo zou ||| guangming shi ||| 
2020 ||| a hybrid intelligent system for insider threat detection using iterative attention. ||| xueshuang ren ||| liming wang ||| 
2019 ||| a novel attention-based neural network for video scene classification in complex background. ||| yan fu ||| ru xin ||| ou ye ||| 
2018 ||| learning to communicate via supervised attentional message processing. ||| zhaoqing peng ||| libo zhang ||| tiejian luo ||| 
2021 ||| smat: an attention-based deep learning solution to the automation of schema matching. ||| jing zhang ||| bonggun shin ||| jinho d. choi ||| joyce c. ho ||| 
2021 ||| attention-guided memory model for video object segmentation. ||| yunjian lin ||| yihua tan ||| 
2019 ||| self-attention deep saliency network for fabric defect detection. ||| jinjin wang ||| zhoufeng liu ||| chunlei li ||| ruimin yang ||| bicao li ||| 
2017 ||| siamese network with soft attention for semantic text understanding. ||| kolawole john adebayo ||| luigi di caro ||| guido boella ||| 
2021 ||| llod-driven bilingual word embeddings rivaling cross-lingual transformers in quality of life concept detection from french online health communities. ||| katharina allgaier ||| susana ver ||| ssimo ||| sherry tan ||| matthias orlikowski ||| matthias hartung ||| 
2021 ||| attentional learn-able pooling for human activity recognition. ||| bappaditya debnath ||| mary o'brien ||| swagat kumar ||| ardhendu behera ||| 
2021 ||| semantic reinforced attention learning for visual place recognition. ||| guohao peng ||| yufeng yue ||| jun zhang ||| zhenyu wu ||| xiaoyu tang ||| danwei wang ||| 
2020 ||| attention-guided lightweight network for real-time segmentation of robotic surgical instruments. ||| zhen-liang ni ||| gui-bin bian ||| zeng-guang hou ||| xiao-hu zhou ||| xiao-liang xie ||| zhen li ||| 
2021 ||| sct-cnn: a spatio-channel-temporal attention cnn for grasp stability prediction. ||| gang yan ||| alexander schmitz ||| satoshi funabashi ||| sophon somlor ||| tito pradhono tomo ||| shigeki sugano ||| 
2021 ||| perceive, attend, and drive: learning spatial attention for safe self-driving. ||| bob wei ||| mengye ren ||| wenyuan zeng ||| ming liang ||| bin yang ||| raquel urtasun ||| 
2021 ||| attentional-gcnn: adaptive pedestrian trajectory prediction towards generic autonomous vehicle use cases. ||| kunming li ||| stuart eiffert ||| mao shan ||| francisco gomez-donoso ||| stewart worrall ||| eduardo m. nebot ||| 
2021 ||| referring image segmentation via language-driven attention. ||| ding-jie chen ||| he-yen hsieh ||| tyng-luh liu ||| 
2017 ||| show, attend and interact: perceivable human-robot social interaction through neural attention q-network. ||| ahmed hussain qureshi ||| yutaka nakamura ||| yuichiro yoshikawa ||| hiroshi ishiguro ||| 
2021 ||| gcc-phat with speech-oriented attention for robotic sound source localization. ||| jiadong wang ||| xinyuan qian ||| zihan pan ||| malu zhang ||| haizhou li ||| 
2019 ||| learning to write anywhere with spatial transformer image-to-motion encoder-decoder networks. ||| barry ridge ||| rok pahic ||| ales ude ||| jun morimoto ||| 
2019 ||| a cane-based low cost sensor to implement attention mechanisms in telecare robots. ||| joaqu ||| n ballesteros ||| alberto j. tudela ||| juan rafael caro-romero ||| cristina urdiales ||| 
2021 ||| ndt-transformer: large-scale 3d point cloud localisation using the normal distribution transform representation. ||| zhicheng zhou ||| cheng zhao ||| daniel adolfsson ||| songzhi su ||| yang gao ||| tom duckett ||| li sun ||| 
2021 ||| avgcn: trajectory prediction using graph convolutional networks guided by human attention. ||| congcong liu ||| yuying chen ||| ming liu ||| bertram e. shi ||| 
2017 ||| attention and anticipation in fast visual-inertial navigation. ||| luca carlone ||| sertac karaman ||| 
2021 ||| deep3dranker: a novel framework for learning to rank 3d models with self-attention in robotic vision. ||| frank po wen lo ||| yao guo ||| yingnan sun ||| jianing qiu ||| benny lo ||| 
2020 ||| attentive task-net: self supervised task-attention network for imitation learning using video demonstration. ||| kartik ramachandruni ||| madhu babu vankadari ||| anima majumder ||| samrat dutta ||| swagat kumar ||| 
2021 ||| attention-based probabilistic planning with active perception. ||| haoxiang ma ||| jie fu ||| 
2021 ||| bidirectional attention network for monocular depth estimation. ||| shubhra aich ||| jean marie uwabeza vianney ||| md. amirul islam ||| mannat kaur ||| bingbing liu ||| 
2021 ||| covariance self-attention dual path unet for rectal tumor segmentation. ||| haijun gao ||| xiangyin zeng ||| dazhi pan ||| bochuan zheng ||| 
2020 ||| ap-mtl: attention pruned multi-task learning model for real-time instrument detection and segmentation in robot-assisted surgery. ||| mobarakol islam ||| vibashan vs ||| hongliang ren ||| 
2020 ||| ccan: constraint co-attention network for instance grasping. ||| junhao cai ||| xuefeng tao ||| hui cheng ||| zhanpeng zhang ||| 
2021 ||| multibranch learning for angiodysplasia segmentation with attention-guided networks and domain adaptation. ||| xiao jia ||| xiaochun mai ||| xiaohan xing ||| yutian shen ||| jiankun wang ||| max q.-h. meng ||| 
2019 ||| crowd-robot interaction: crowd-aware robot navigation with attention-based deep reinforcement learning. ||| changan chen ||| yuejiang liu ||| sven kreiss ||| alexandre alahi ||| 
2021 ||| verbal focus-of-attention system for learning-from-observation. ||| naoki wake ||| iori yanokura ||| kazuhiro sasabuchi ||| katsushi ikeuchi ||| 
2021 ||| maast: map attention with semantic transformers for efficient visual navigation. ||| zachary seymour ||| kowshik thopalli ||| niluthpol chowdhury mithun ||| han-pang chiu ||| supun samarasekera ||| rakesh kumar ||| 
2021 ||| long-range hand gesture recognition via attention-based ssd network. ||| liguang zhou ||| chenping du ||| zhenglong sun ||| tin lun lam ||| yangsheng xu ||| 
2021 ||| a graph attention spatio-temporal convolutional network for 3d human pose estimation in video. ||| junfa liu ||| juan rojas ||| yihui li ||| zhijun liang ||| yisheng guan ||| ning xi ||| haifei zhu ||| 
2019 ||| fast radar motion estimation with a learnt focus of attention using weak supervision. ||| roberto aldera ||| daniele de martini ||| matthew gadd ||| paul newman ||| 
2020 ||| multi-head attention for multi-modal joint vehicle motion forecasting. ||| jean mercat ||| thomas gilles ||| nicole el zoghby ||| guillaume sandou ||| dominique beauvois ||| guillermo pita gil ||| 
2020 ||| smart: training shallow memory-aware transformers for robotic explainability. ||| marcella cornia ||| lorenzo baraldi ||| rita cucchiara ||| 
2018 ||| social attention: modeling attention in human crowds. ||| anirudh vemula ||| katharina muelling ||| jean oh ||| 
2019 ||| lightweight contrast modeling for attention-aware visual localization. ||| lili huang ||| guanbin li ||| ya li ||| liang lin ||| 
2019 ||| attention-based lane change prediction. ||| oliver scheel ||| naveen shankar nagaraja ||| loren arthur schwarz ||| nassir navab ||| federico tombari ||| 
2017 ||| tweeting mass shootings: the dynamics of issue attention on social media. ||| yini zhang ||| yidong wang ||| jordan m. foley ||| jiyoun suk ||| devin conathan ||| 
2018 ||| #thanksfortheinvite: examining attention to social exclusion signals online. ||| jessica m. covert ||| michael a. stefanone ||| brooke foucault welles ||| zhiying yue ||| zena toh ||| 
2020 ||| salient attention model and classes imbalance remission for video anomaly analysis with weak label. ||| hang zhou ||| huifen xia ||| yongzhao zhan ||| qirong mao ||| 
2020 ||| image fusion method for transformer substation based on nsct and visual saliency. ||| fang zhang ||| xin dong ||| minghui liu ||| chengchang liu ||| 
2020 ||| scale-aware network with attentional selection for human pose estimation. ||| tianqi lv ||| lingrui wu ||| junhua zhou ||| zhonghua liao ||| xiang zhai ||| 
2019 ||| character-level attention convolutional neural networks for short-text classification. ||| feiyang yin ||| zhilin yao ||| jia liu ||| 
2018 ||| wi-fi attention network for indoor fingerprint positioning. ||| ting zhang ||| yi man ||| xiaoshi fang ||| ligang ren ||| yue ma ||| 
2021 ||| a novel wifi gesture recognition method based on cnn-lstm and channel attention. ||| yu gu ||| jiangan li ||| 
2021 ||| combined metapath based attention network for heterogenous networks node classification. ||| kang chen ||| dehong qiu ||| 
2021 ||| an improved gail based on object detection, gru, and attention. ||| qinghe liu ||| yinghong tian ||| 
2021 ||| attention-based sub-word network for multilingual short text classification. ||| yaru sun ||| ying yang ||| yongjian wang ||| 
2021 ||| facial expression recognition based on deep learning and attention mechanism. ||| yexuan ma ||| chaobing huang ||| 
2020 ||| learning target-specific response attention for siamese network based visual tracking. ||| penghui zhao ||| haosheng chen ||| yanjie liang ||| yan yan ||| hanzi wang ||| 
2021 ||| research on 110kv oil impregnated paper capac-itance graded transformer bushings based on the design principle of equal capacitance and steps. ||| junwei diao ||| jianbin ye ||| yu zhang ||| haoen cao ||| 
2021 ||| lightweight url-based phishing detection using natural language processing transformers for mobile devices. ||| katherine haynes ||| hossein shirazi ||| indrakshi ray ||| 
2021 ||| an improved cnn based on attention mechanism with multi-domain feature fusion for bearing fault diagnosis. ||| mingzhu yu ||| heli liu ||| rengen wang ||| xiangwei kong ||| zhiyong hu ||| xueyi li ||| 
2021 ||| control of soft switching solid state transformer based on lyapunov energy function for three-phase ac-ac power conversion. ||| vikram roy chowdhury ||| rajendra prasad kandula ||| deepak divan ||| 
2021 ||| breadth-first search leakage tolerant commutation method for matrix converters in three-phase solid state transformers. ||| pedro b. c. costa ||| s ||| nia ferreira pinto ||| j. fernando silva ||| 
2020 ||| dual transformer based dual active bridge for solid state transformer in distribution system. ||| mohd tariq ||| mohammad tauquir iqbal ||| ali i. maswood ||| md shafquat ullah khan ||| 
2020 ||| a single stage multilevel converter based on transformer multi-tap voltages control fed by low dc voltage source. ||| yaojun chen ||| guanru chen ||| wei gao ||| gang xue ||| cuihua tian ||| baichao chen ||| 
2017 ||| operation and control of smart transformer-based electric vehicles charging system. ||| chandan kumar ||| giampaolo buticchi ||| marco liserre ||| 
2019 ||| a new fault tolerant single phase 5-level inverter topology with capacitor voltage balancing for solid state transformer. ||| sai krishna saketi ||| pradyumn chaturvedi ||| hiralal murlidhar suryawanshi ||| dharmendra yadeo ||| dipesh atkar ||| 
2018 ||| design of nonlinear dry-type transformer for all-electric ship and marine applications. ||| boubacar housseini ||| aime francis okou ||| mohamed tarbouchi ||| derrick bouchard ||| aboelsood zidan ||| 
2018 ||| analysis and simulation of transformer isolated high current 48 v dc power supply with dc-ups capability based on scaldo technique for google's new open rack power architecture. ||| thilanga ariyarathna ||| nihal kularatna ||| d. alistair steyn-ross ||| 
2018 ||| a solid state transformer based fast charging station for all categories of electric vehicles. ||| arun chandrasekharan nair ||| b. g. fernandes ||| 
2019 ||| load management design methods by displacement and affectation in ageing level of distribution transformers. ||| joselyne del rosario ||| carlos brito ||| iv ||| n endara ||| javier urquizo ||| 
2020 ||| bi-directional cllc resonant converter with integrated planar transformer for energy storage systems. ||| abhinav soni ||| ajeet k. dhakar ||| 
2021 ||| series resonant converter with embedded filters for dcx of solid-state transformer. ||| shota okutani ||| pin-yu huang ||| ryo nishiyama ||| yuichi kado ||| 
2019 ||| lanet: a ladder attention network for image semantic segmentation. ||| dongli wang ||| bo wang ||| yan zhou ||| 
2017 ||| simplified rail power conditioner based on a half-bridge indirect ac/dc/ac modular multilevel converter and a v/v power transformer. ||| mohamed tanta ||| vitor monteiro ||| bruno exposto ||| j. g. pinto ||| ant ||| nio p. martins ||| adriano s. carvalho ||| andr ||| s a. nogueiras mel ||| ndez ||| jo ||| o luiz afonso ||| 
2018 ||| optimal sizing of a power electronic traction transformer for railway applications. ||| caroline stackler ||| florent morel ||| philippe ladoux ||| alexis fouineau ||| francois wallart ||| nathan evans ||| 
2017 ||| visual attention distribution map for artificial misdirection. ||| hiroshi igarashi ||| yoshihiro ishihara ||| 
2020 ||| an asymmetrical loosely coupled transformer and constant current wireless charging scheme for warehouse vehicles. ||| xuecheng liu ||| wenjie guan ||| pengcheng yu ||| pengfei pan ||| jing zhou ||| 
2020 ||| analysis, design and modelling of two fully- integrated transformers with segmental magnetic shunt for llc resonant converters. ||| sajad arab ansari ||| jonathan n. davidson ||| martin p. foster ||| 
2017 ||| voltage quality improvement in smart transformer integrated distribution grid. ||| v. m. hrishikesan ||| chandan kumar ||| marco liserre ||| 
2019 ||| smart transformer modelling in optimal power flow analysis. ||| junru chen ||| ran li ||| alireza soroudi ||| andrew keane ||| damian flynn ||| terence o'donnell ||| 
2020 ||| six-switch and seven-switch grid-connected current source inverters for transformerless photovoltaic applications. ||| kawther ezzeddine ||| mahmoud hamouda ||| hadi youssef kanaan ||| kamal al-haddad ||| 
2017 ||| thermal modeling and transient behavior analysis of a medium-frequency high-power transformer. ||| annoy kumar das ||| zhongbao wei ||| sriram vaisambhayana ||| shuyu cao ||| haonan tian ||| anshuman tripathi ||| philip came kjar ||| 
2020 ||| design of a high-voltage-insulation and high-efficiency medium frequency transformer. ||| shilong zhu ||| shuai shao ||| muhammad awais ||| junming zhang ||| 
2017 ||| topology and control of transformerless high voltage grid-connected pv systems with a cascade step-up structure. ||| zilong yang ||| zhe wang ||| ying zhang ||| zhe zhang ||| 
2019 ||| methodology for assessment of the impact of smart transformers on power system reliability. ||| junru chen ||| alireza nouri ||| andrew keane ||| terence o'donnell ||| 
2021 ||| multilevel inverter with a new modulation method applied to solid-state transformer in pv applications. ||| hesamodin abdoli ||| javad shokrollahi moghani ||| sadegh vaez-zadeh ||| amir babaki ||| alireza jafari-natanzi ||| jos |||  rodr ||| guez ||| 
2020 ||| a multiport power electronic transformer with shared medium-frequency transformer. ||| dajun ma ||| wu chen ||| liangcai shu ||| 
2018 ||| smart transformer for the provision of coordinated voltage and frequency support in the grid. ||| junru chen ||| rongwu zhu ||| muyang liu ||| giovanni de carne ||| marco liserre ||| federico milano ||| terence o'donnell ||| 
2021 ||| on-line method for high-sensitivity leakage current measurement of converter-connected transformers in microgrids. ||| geye lu ||| yang wu ||| dayong zheng ||| pinjia zhang ||| 
2019 ||| tab based multiport converter with optimized transformer rms current and improved zvs range for dc microgrid applications. ||| ishita biswas ||| debaprasad kastha ||| prabodh bajpai ||| 
2017 ||| dynamic demand minimization using a smart transformer. ||| junru chen ||| cathal o'loughlin ||| terence o'donnell ||| 
2017 ||| corrosion evaluation of the grounding grid in transformer substation using electrical impedance tomography technology. ||| jia-yuan hu ||| jian-gen hu ||| dao-lin lan ||| ju-lan ming ||| yu-tong zhou ||| yan-wei li ||| 
2020 ||| smart transformer based meshed hybrid microgrid with mvdc interconnection. ||| hrishikesan v. m. ||| chandan kumar ||| 
2017 ||| post-fault operation of hybrid dc-dc converter for solid-state transformer. ||| tomasz gajowik ||| cezary sobol ||| sebastian stynski ||| mariusz malinowski ||| 
2021 ||| a two-phase interleaved high-voltage gain dc-dc converter with coupled inductor and built-in transformer for photovoltaic applications. ||| ramin rahimi ||| saeed habibi ||| mehdi ferdowsi ||| pourya shamsi ||| 
2020 ||| multi-port converter integrating automatic current balancing interleaved pwm converter and dual active bridge converter with improved transformer utilization. ||| motoki sato ||| yoshiya tada ||| masatoshi uno ||| 
2021 ||| control of cross-circulating currents in a mmc with parallel connected arms in solid state transformers. ||| felipe l. ruiz ||| marcelo a. p ||| rez ||| freddy flores-bahamonde ||| mariusz malinowski ||| 
2019 ||| design of an igbt-series-based solid-state circuit breaker for battery energy storage system terminal in solid-state transformer. ||| rui wang ||| biao zhang ||| shanshan zhao ||| lin liang ||| yu chen ||| 
2018 ||| sensorless starting control of permanent magnet synchronous motors with step-up transformer for downhole electric drilling. ||| zhixiong li ||| quanli zhang ||| huaidong luo ||| hongwei wang ||| jin wang ||| fei han ||| aiguo wang ||| xicai liu ||| xiaoming yu ||| libing zhou ||| 
2019 ||| high-efficiency solid state transformer architecture for large-scale pv application. ||| kangan wang ||| rongwu zhu ||| youngjong ko ||| marco liserre ||| 
2018 ||| high power quality voltage control of smart transformer-fed distribution grid. ||| rongwu zhu ||| zhixiang zou ||| marco liserre ||| 
2017 ||| investigation of load compensation features of smart transformer in medium voltage grid. ||| chandan kumar ||| rongwu zhu ||| marco liserre ||| 
2018 ||| laboratory investigations of parallel connected inverters feeding medium voltage transformer. ||| maciej kozak ||| 
2020 ||| a new transformer model with separate common-mode and differential-mode capacitance. ||| christian  ||| stergaard ||| claus kjeldsen ||| morten nymand ||| 
2019 ||| distribution network hybrid transformer for load current and grid voltage compensation. ||| alvaro carreno ||| marcelo a. perez ||| carlos r. baier ||| jos |||  r. espinoza ||| 
2019 ||| hybrid transformers with virtual inertia for future distribution networks. ||| carlos r. baier ||| miguel a. torres ||| marcelo a. p ||| rez ||| roberto c ||| rdenas ||| roberto o. ramirez ||| pedro e. melin ||| 
2019 ||| optimum design of planar transformer for llc resonant converter using metaheuristic method. ||| emad nazerian ||| farzad tahami ||| 
2020 ||| a bidirectional matrix-based ac-dc dual-active bridge for modular solid-state-transformers. ||| jaydeep saha ||| naga brahmendra yadav gorla ||| sanjib kumar panda ||| 
2018 ||| zero-sequence injection technique for capacitor lifetime extension on the low-voltage converter of a smart transformer. ||| rongwu zhu ||| vito giuseppe monopoli ||| marco liserre ||| 
2020 ||| application of gen-3 10 kv sic mosfets in xhv-6 packaging for a mobile utility support equipment based solid state transformer (muse-sst). ||| anup anurag ||| sayan acharya ||| subhashish bhattacharya ||| todd r. weatherford ||| 
2018 ||| modular ev fast charging station architectures based on multiphase-medium-frequency transformer. ||| felix hoffmann ||| luis camur ||| a ||| marco liserre ||| 
2017 ||| two-transformer-series approach in developing a transistor based-ac voltage regulator for consumer-end applications. ||| priyanwada nimesha wijesooriya ||| nihal kularatna ||| jayathu fernando ||| d. alistair steyn-ross ||| 
2020 ||| improving automated visual fault detection by combining a biologically plausible model of visual attention with deep learning. ||| frederik beuth ||| tobias schlosser ||| michael friedrich ||| danny kowerko ||| 
2018 ||| robustness analysis of voltage control strategies of smart transformer. ||| federico cecati ||| markus andresen ||| rongwu zhu ||| zhixiang zou ||| marco liserre ||| 
2017 ||| modeling and analysis on the sensing characteristic of fiber optical current transformer. ||| yansong li ||| bing wang ||| jun liu ||| 
2017 ||| an improved partially interleaved transformer structure for high-voltage high-frequency multiple-output applications. ||| bin zhao ||| ziwei ouyang ||| michael a. e. andersen ||| maeve duffy ||| william gerard hurley ||| 
2019 ||| a frequency multiplying circuit containing a high-frequency output inverter and an impedance matching transformer. ||| koji orikawa ||| satoshi ogasawara ||| masatsugu takemoto ||| jun-ichi itoh ||| 
2021 ||| computational feasibility of multi-objective optimal design techniques for grid-connected multi-cell solid-state-transformers. ||| jaydeep saha ||| naga brahmendra yadav gorla ||| aravinth subramaniam ||| sanjib kumar panda ||| 
2018 ||| energy storage systems to prevent distribution transformers overload with high nzeb penetration. ||| renato verlssimo ||| rui amaral lopes ||| jo ||| o f. martins ||| 
2018 ||| an ac-ac modular multilevel converter-based partially-rated solid-state transformer for power flow control. ||| qichen yang ||| maryam saeedifard ||| 
2018 ||| experimental verification on thermal modeling of medium frequency transformers. ||| haonan tian ||| zhongbao wei ||| madasamy palavesha thevar ||| sriram vaisambhayana ||| anshuman tripathi ||| philip carne kjaer ||| 
2020 ||| protection scheme for a medium voltage mobile utility support equipment based solid state transformer (muse-sst). ||| anup anurag ||| sayan acharya ||| nithin kolli ||| subhashish bhattacharya ||| todd r. weatherford ||| 
2019 ||| unbalanced load compensation for solid-state transformer using smoothing capacitors of cascaded h-bridges as energy buffer. ||| akihiro motoki ||| takanori isobe ||| katsushi terazono ||| hiroshi tadano ||| 
2019 ||| high performances voltage control of bidirectional-asymmetrical dc/dc converter in smart transformer for limited reverse power flow. ||| rongwu zhu ||| marco liserre ||| nimrod v ||| zquez ||| 
2017 ||| voltage control strategies of smart transformer considering dc capacitor lifetime. ||| rongwu zhu ||| holger jedtberg ||| marco liserre ||| 
2021 ||| comparison of different multi-winding transformer models in multi-port ac-coupled converter application. ||| haojun qin ||| huan zhang ||| ming liu ||| chengbin ma ||| 
2017 ||| integration of ripple correlation mppt technique with one-cycle-controlled transformerless inverter for single-phase single-stage photovoltaic systems. ||| venkata r. reddy ||| e. s. sreeraj ||| 
2017 ||| multi-bus flexible interconnection scheme for balancing power transformers in low-voltage distribution systems. ||| jianqiao zhou ||| jianwen zhang ||| xu cai ||| jiacheng wang ||| jiajie zang ||| 
2020 ||| transformerless common ground quasi-z-source three phase inverter for pv applications. ||| kharan shiluveru ||| akash singh ||| rajeev kumar singh ||| 
2019 ||| modeling and design of a 1.2 pf common-mode capacitance transformer for powering mv sic mosfets gate drivers. ||| hongbo zhao ||| dipen narendra dalal ||| xiongfei wang ||| jannick kj ||| r j ||| rgensen ||| christian uhrenfeldt ||| szymon beczkowski ||| stig munk-nielsen ||| 
2021 ||| attention based object classification for drone imagery. ||| jehwan choi ||| kang-hyun jo ||| 
2019 ||| multi-port system for braking energy recovery in diesel-electric locomotives - focus on the multi-interphase transformer design. ||| sergio luis brockveld ||| caio guilherme da silva moraes ||| pedro p. cavilha ||| antonio l. s. pacheco ||| marcelo lobo heldwein ||| gierri waltrich ||| 
2021 ||| new operation opportunities for the solid-state transformer in smart homes: a comprehensive analysis. ||| vitor monteiro ||| jo ||| o a. pe ||| as lopes ||| carlos l. moreira ||| jo ||| o luiz afonso ||| 
2021 ||| potentials and challenges of multiwinding transformer-based dc-dc converters for solid-state transformer. ||| thiago pereira ||| felix hoffmann ||| marco liserre ||| 
2018 ||| design strategy and simulation of medium-frequency transformers for a three-phase dual active bridge. ||| tobias kauder ||| kay hameyer ||| thierry belgrand ||| 
2018 ||| flexible power transfer in smart transformer interconnected microgrids. ||| v. m. hrishikesan ||| chandan kumar ||| marco liserre ||| 
2019 ||| 3-phase medium frequency transformer for a 100kw 1.2kv 20khz dual active bridge converter. ||| piotr dworakowski ||| andrzej wilk ||| michal michna ||| bruno lefebvre ||| thomas lagier ||| 
2018 ||| model predictive control of h5 inverter for transformerless pv systems with maximum power point tracking and leakage current reduction. ||| abdulrahman j. babqi ||| zhehan yi ||| di shi ||| xiaoying zhao ||| 
2021 ||| ff-gat: feature fusion using graph attention networks. ||| ahmed n. ahmed ||| ali anwar ||| siegfried mercelis ||| steven latr ||| peter hellinckx ||| 
2021 ||| a novel current limiting control strategy for three-phase three-wire inverter with transformer. ||| xuecheng liang ||| li peng ||| 
2018 ||| high frequency transformer based improved gamma zsi with lossless snubber. ||| zeeshan aleem ||| simon winberg ||| atif iqbal ||| mohammed a. al-hitmi ||| 
2021 ||| novel topology of modular-matrix-converter-based smart transformer for hybrid microgrid. ||| lijun zhang ||| alexandre bento ||| guilherme marto para ||| so ||| pedro b. c. costa ||| s ||| nia ferreira pinto ||| jos |||  fernando silva ||| 
2017 ||| research on a dual active bridge based power electronics transformer using nanocrystalline and silicon carbide. ||| bin cui ||| xinyang li ||| pengfei yao ||| xiaohua jiang ||| siqi li ||| tao wang ||| le zhang ||| 
2018 ||| sensitivity of leakage inductance for detecting winding movements in transformers. ||| pritam mukhcrjee ||| elango jeyasankar ||| santosh janaki raman ||| sanjib kumar panda ||| 
2020 ||| control and stabilization of grid-connected converters operating as constant power load in a smart transformer grid scenario. ||| chi zhang ||| giampaolo buticchi ||| jiajun yang ||| zhi-xiang zou ||| 
2018 ||| protection design considerations of a 10 kv sic mosfet enabled mobile utilities support equipment based solid state transformer (muse-sst). ||| venkat n. jakka ||| sayan acharya ||| anup anurag ||| yos prabowo ||| ashish kumar ||| sanket parashar ||| subhashish bhattacharya ||| 
2020 ||| startup scheme for the active front end converter in a medium voltage mobile utility support equipment based solid state transformer (muse-sst). ||| anup anurag ||| sayan acharya ||| nithin kolli ||| subhashish bhattacharya ||| todd r. weatherford ||| 
2018 ||| grid fault detection technique of microgrid inverter according to the structure of three phase output transformer. ||| jae uk lim ||| seung woo beak ||| hag-wone kim ||| kwan-yuhl cho ||| jae-ho choi ||| joung-hwan bae ||| 
2018 ||| voltage feedback of an llc resonant converter with a rotary transformer. ||| gabriele rizzoli ||| michele mengoni ||| luca zarri ||| angelo tani ||| 
2021 ||| reduced dc voltage fed grid connected transformer-less shunt compensator with ac-side impedance-source configuration. ||| guddy satpathy ||| burle tulasi rao ||| dipankar de ||| 
2020 ||| a dual attention module for real-time facial expression recognition. ||| muhamad dwisnanto putro ||| duy-linh nguyen ||| kang-hyun jo ||| 
2019 ||| reliability modeling and assessment of dual active bridge based dc transformer for dc power distribution application. ||| yating gou ||| jiachen tian ||| kefan yu ||| cuicui liu ||| fang zhuo ||| feng wang ||| jiarui zhang ||| 
2020 ||| modeling and control of a hybrid transformer based on a cascaded h-bridge multilevel converter. ||| alvaro carreno ||| marcelo a. perez ||| carlos r. baier ||| jos |||  r. espinoza ||| 
2019 ||| a fast simulation model of cascaded h bridge-power electronic transformer. ||| fei xu ||| zixin li ||| fanqiang gao ||| cong zhao ||| ping wang ||| yaohua li ||| 
2020 ||| power loss minimization in smart transformer based meshed hybrid distribution network. ||| chandan kumar ||| rampelli manojkumar ||| sanjib ganguly ||| marco liserre ||| 
2018 ||| modular multilevel converter based topology for electric locomotive with medium frequency step-down transformer. ||| bishwajyoti purkayastha ||| tanmoy bhattacharya ||| 
2018 ||| low cm leakage current and high efficiency h6 inverter with active clamping for transformerless pv system. ||| jianyu hu ||| wenxun xiao ||| bo zhang ||| dong yuan qiu ||| carl ngai man ho ||| 
2020 ||| an iterative algorithm for optimum design of high frequency transformer in sst application. ||| sherin joseph ||| ajay koshy abraham ||| pinkymol harikrishna raj ||| jineeth joseph ||| k. r. m. nair ||| 
2017 ||| a novel multi-port solid state transformer enabled isolated hybrid microgrid architecture. ||| arun chandrasekharan nair ||| b. g. fernandes ||| 
2017 ||| transformerless single-phase upqc for large scale led lighting networks. ||| radwa mohamed abdalaal ||| carl ngai man ho ||| 
2017 ||| optimization of lcl filter with intercell transformer for interleaved voltage source converter. ||| ki-bum park ||| frederick kieferndorf ||| uwe drofenik ||| sami pettersson ||| francisco canales ||| 
2017 ||| single-phase transformer-less buck-boost inverter with zero leakage current for pv systems. ||| ali mostaan ||| ahmed abdelhakim ||| mohsen soltani ||| frede blaabjerg ||| 
2020 ||| virtual transformer operation of solid state transformer (sst). ||| qian tao ||| jianjun ma ||| miao zhu ||| shuli wen ||| qing duan ||| guanglin sha ||| 
2020 ||| a magnetic saturation suppression scheme of the output line-frequency transformer in photovoltaic inverter. ||| song xu ||| daolian chen ||| hanchao zeng ||| 
2017 ||| retiring strategies of transformer lcc with reliability of power transmission system considered. ||| mingxing zhu ||| liangyan jia ||| hao hu ||| jing chen ||| zhongchao wu ||| dabo zhang ||| yigang he ||| haifeng ye ||| 
2020 ||| temporal pattern attention-based sequence to sequence model for multistep individual load forecasting. ||| chongchong xu ||| guo chen ||| xiaojun zhou ||| 
2021 ||| transformer oil-paper insulation aging evaluation system based on different aging characteristics. ||| chunming zhao ||| wenxie xu ||| zhaohong deng ||| 
2020 ||| xss detection technology based on lstm-attention. ||| li lei ||| ming chen ||| chengwan he ||| duojiao li ||| 
2021 ||| a visual target tracking algorithm integrating attention mechanism. ||| xun li ||| heng cui ||| yanduo zhang ||| 
2019 ||| speeding up reinforcement learning by combining attention and agency features. ||| berkay demirel ||| mart |||  s ||| nchez-fibla ||| 
2021 ||| prevent the occurrence of false signals in the power transformer internal protection in substation to improve reliability with differential operational amplifier safety. ||| soni asmaul fuadi ||| ario dwi prabowo ||| 
2019 ||| optical qpsk signal generation based on the circular trajectory of phase-shifted optical vsb modulation using high-pass hilbert transformers. ||| daichi sato ||| kariyawasam indipalage amila sampath ||| katsumi takano ||| 
2017 ||| efficient inverse discrete wavelet transformer. ||| goran savic ||| milan prokin ||| vladimir m. rajovic ||| dragana prokin ||| 
2017 ||| efficient forward discrete wavelet transformer. ||| goran savic ||| milan prokin ||| vladimir m. rajovic ||| dragana prokin ||| 
2020 ||| transformer region proposal for object detection. ||| yiming jiang ||| jinlong chen ||| minghao yang ||| junwei hu ||| 
2018 ||| vision-based joint attention detection for autism spectrum disorders. ||| wanqi zhang ||| zhiyong wang ||| honghai liu ||| 
2018 ||| sar image fast online atr based on visual attention and scale analysis. ||| hongqiao wang ||| yan-ning cai ||| junyi yao ||| shaolei zhang ||| guangyuan fu ||| 
2020 ||| automatic leaf recognition based on attention densenet. ||| huisi wu ||| zhouan shi ||| haiming huang ||| zhenkun wen ||| fuchun sun ||| 
2018 ||| computer-based attention training improves brain cognitive control function: evidences from event-related potentials. ||| lei zheng ||| dong-ni pan ||| yi wang ||| xuebing li ||| 
2020 ||| mwoa auxiliary diagnosis via rsn-based 3d deep multiple instance learning with spatial attention mechanism. ||| xiang li ||| benzheng wei ||| tianyang li ||| na zhang ||| 
2020 ||| a sequence-to-sequence model based on attention mechanism for wave spectrum prediction. ||| xiao zeng ||| lin qi ||| tong yi ||| tong liu ||| 
2020 ||| a lightweight transformer with convolutional attention. ||| kungan zeng ||| incheon paik ||| 
2017 ||| neurofeedback based attention training for children with adhd. ||| chin-ling chen ||| yung-wen tang ||| nian-qiao zhang ||| jungpil shin ||| 
2021 ||| coordinate attention unet. ||| quoc an dang ||| duc dung nguyen ||| 
2018 ||| modified transformerless dual buck inverter with improved lifetime for pv applications. ||| ahmad khan ||| frede blaabjerg ||| 
2019 ||| perceptual attention-based predictive control. ||| keuntaek lee ||| gabriel nakajima an ||| viacheslav zakharov ||| evangelos a. theodorou ||| 
2018 ||| learning 6-dof grasping and pick-place using attention focus. ||| marcus gualtieri ||| robert platt jr. ||| 
2020 ||| attentional separation-and-aggregation network for self-supervised depth-pose learning in dynamic scenes. ||| feng gao ||| jincheng yu ||| hao shen ||| yu wang ||| huazhong yang ||| 
2020 ||| transformers for one-shot visual imitation. ||| sudeep dasari ||| abhinav gupta ||| 
2019 ||| multimodal attention branch network for perspective-free sentence generation. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2020 ||| attention-privileged reinforcement learning. ||| sasha salter ||| dushyant rao ||| markus wulfmeier ||| raia hadsell ||| ingmar posner ||| 
2021 ||| security threat modeling for power transformers in cyber-physical environments. ||| bohyun ahn ||| taesic kim ||| scott c. smith ||| young-woo youn ||| myung-hyo ryu ||| 
2017 ||| voltage control by using capacitor banks and tap changing transformers in a renewable microgrid. ||| pavan penkey ||| husam samkari ||| brian k. johnson ||| herbert l. hess ||| 
2021 ||| on ambient temperature of transformer substations in desert climates. ||| javier hernandez fernandez ||| bo wang ||| ahmed massoud ||| salem talib ||| mazher shhaab ||| afsal mohamed ||| 
2020 ||| distribution transformer health monitoring using smart meter data. ||| kavya ashok ||| dan li ||| deepak divan ||| nagi gebraeel ||| 
2021 ||| radial deformation emplacement in power transformers using long short-term memory networks. ||| arash moradzadeh ||| kazem pourhossein ||| behnam mohammadi-ivatloo ||| tohid khalili ||| ali bidram ||| 
2019 ||| transformer rating due to high penetrations of pv, ev charging, and energy storage. ||| kerry d. mcbee ||| p. rudraraju ||| j. chong ||| 
2020 ||| secondary network parameter estimation for distribution transformers. ||| kavya ashok ||| matthew j. reno ||| deepak divan ||| 
2020 ||| sensing service transformer secondary currents using planar magnetic pick-up coils. ||| shreyas kulkarni ||| deepak divan ||| 
2020 ||| is attention contagious? estimating the spillover effect of investor attention in digital networks. ||| wuyue phoebe shangguan ||| xi chen ||| alvin chung man leung ||| 
2018 ||| eco-feedback interventions: selective attention and actual behavior change. ||| liliane ableitner ||| verena tiefenbeck ||| elgar fleisch ||| thorsten staake ||| 
2018 ||| the role of attention and neutralization in posting malicious comments online. ||| han-min kim ||| gee-woo bock ||| 
2017 ||| paying attention to news briefs about innovative technologies. ||| nancy k. lankton ||| d. harrison mcknight ||| 
2021 ||| vitbis: vision transformer for biomedical image segmentation. ||| abhinav sagar ||| 
2021 ||| attention-guided pancreatic duct segmentation from abdominal ct volumes. ||| chen shen ||| holger r. roth ||| yuichiro hayashi ||| masahiro oda ||| tadaaki miyamoto ||| gen sato ||| kensaku mori ||| 
2019 ||| a comparison of word-embeddings in emotion detection from text using bilstm, cnn and self-attention. ||| marco polignano ||| pierpaolo basile ||| marco de gemmis ||| giovanni semeraro ||| 
2017 ||| "out of the fr-eye-ing pan": towards gaze-based models of attention during learning with technology in the classroom. ||| stephen hutt ||| caitlin mills ||| nigel bosch ||| kristina krasich ||| james r. brockmole ||| sidney k. d'mello ||| 
2019 ||| personalized serious games for self-regulated attention training. ||| nadia hocine ||| 
2018 ||| a joint attention model for automated editing. ||| hui-yin wu ||| arnav jhala ||| 
2020 ||| field simulation method for secondary signal of zero-flux current transformer. ||| jinbo wu ||| quan hong ||| ting zhou ||| li li ||| zhihao liu ||| cheng peng ||| yusheng gong ||| 
2020 ||| stock movement classification from twitter via mogrifier based memory cells with attention mechanism. ||| tian qin ||| 
2019 ||| finite element three-dimensional modeling of cooling system for strong oil circulating air-cooled transformer. ||| zheng kou ||| qi li ||| jianli zhao ||| jiankun zhao ||| jianzhong yang ||| 
2019 ||| calculation and analysis of short circuit performance of transformer based on finite element method. ||| jianli zhao ||| zheng kou ||| linxi huang ||| 
2019 ||| experimental study on insulation characteristics of oil-immersed transformer in alpine environment. ||| jianquan liang ||| hongda zhang ||| chunlai yu ||| peng zhang ||| wei sun ||| lin li ||| 
2019 ||| study on suppressing the voltage fluctuation in testing of transformer sudden short circuit based on grid supply. ||| zhiyao zheng ||| zhi li ||| gang wu ||| langqi zhu ||| biao hu ||| haojie chen ||| 
2019 ||| transient current limit control to transformer short circuit based on harmonics voltage injection method. ||| zhiyao zheng ||| yibo gao ||| shaofeng yu ||| gang wu ||| wenjie xiang ||| yifeng wang ||| 
2017 ||| malware analysis of imaged binary samples by convolutional neural network with attention mechanism. ||| hiromu yakura ||| shinnosuke shinozaki ||| reon nishimura ||| yoshihiro oyama ||| jun sakuma ||| 
2018 ||| gender differences in selective attention and shopping intention in the case of taobao live-show: an eye-tracking study. ||| qing xu ||| mengqi fei ||| huizhong tan ||| 
2018 ||| the invisible gorilla revisited: using eye tracking to investigate inattentional blindness in interface design. ||| helene gelderblom ||| leanne menge ||| 
2018 ||| snap-changes: a dynamic editing strategy for directing viewer's attention in streaming virtual reality videos. ||| lucile sassatelli ||| anne-marie pinna-dery ||| marco winckler ||| savino dambra ||| giuseppe samela ||| romaric pighetti ||| ramon aparicio-pardo ||| 
2021 ||| two-stream graph attention convolutional for video action recognition. ||| deyuan zhang ||| hongwei gao ||| hailong dai ||| xiangbin shi ||| 
2021 ||| dual attention-based interest network for personalized recommendation system. ||| xuan zhou ||| xiaoming wang ||| guangyao pang ||| yaguang lin ||| pengfei wan ||| meiling ge ||| 
2020 ||| real-time social media analytics with deep transformer language models: a big data approach. ||| ahmed ahmet ||| tariq abdullah ||| 
2017 ||| attention-based memory network for sentence-level question answering. ||| pei liu ||| chunhong zhang ||| weiming zhang ||| zhiqiang zhan ||| benhui zhuang ||| 
2017 ||| dependency-attention-based lstm for target-dependent sentiment analysis. ||| xinbo wang ||| guang chen ||| 
2019 ||| risk avoidance through reliable attention management at control room workstations. ||| rico gan ||| auge ||| annette hoppe ||| anna-sophia henke ||| norman re ||| ut ||| 
2020 ||| a 28-37 ghz triple-stage transformer-coupled sige lna with 2.5 db minimum nf for low power wideband phased array receivers. ||| abdulrahman a. alhamed ||| gabriel m. rebeiz ||| 
2019 ||| a compact ka-band transformer-coupled power amplifier for 5g in 0.15um gaas. ||| valdrin qunaj ||| patrick reynaert ||| 
2021 ||| a 268-325 ghz 5.2 dbm psat frequency doubler using transformer-based mode separation in sige bicmos technology. ||| sascha breun ||| albert-marcel schrotz ||| marco dietz ||| vadim issakov ||| robert weigel ||| 
2019 ||| a 6-12 ghz reconfigurable transformer-based outphasing combiner in 250-nm gaas. ||| daniel martin ||| michael roberg ||| zoya popovic ||| taylor barton ||| 
2020 ||| attention vs. precision: latency scheduling for uncertainty resilient control systems. ||| rodrigo aldana-l ||| pez ||| rosario arag ||| s ||| carlos sag ||| s ||| 
2021 ||| dynamic allocation of visual attention for vision-based autonomous navigation under data rate constraints. ||| ali reza pedram ||| riku funada ||| takashi tanaka ||| 
2020 ||| an approach to minimum attention control by sparse derivative. ||| masaaki nagahara ||| dragan nesic ||| 
2020 ||| using reverse interactive audio systems (rias) to direct attention in virtual reality narrative practices: a case study. ||| aletta j. steynberg ||| 
2021 ||| lean-back machina: attention-based skippable segments in interactive cinema. ||| niels erik raurs ||| malte elk ||| r rasmussen ||| mikkel kappel persson ||| tor arnth petersen ||| kristinn bragi gar ||| arsson ||| henrik schoenau-fog ||| 
2018 ||| visual question answering using explicit visual attention. ||| vasileios lioutas ||| nikolaos passalis ||| anastasios tefas ||| 
2020 ||| transformer-combining digital pa with efficiency peaking at 0, -6, and -12 db backoff in 32nm cmos. ||| parmoon seddighrad ||| yorgos palaskas ||| hongtao xu ||| paolo madoglio ||| kailash chandrashekar ||| david j. allstot ||| 
2019 ||| a 28 ghz 8-bit calibration-free lo-path phase shifter using transformer-based vector summing topology in 40 nm cmos. ||| zhengkun shen ||| zexue liu ||| haoyun jiang ||| yi tan ||| heyi li ||| xiucheng hao ||| junhua liu ||| huailin liao ||| 
2021 ||| knowledge distillation based on positive-unlabeled classification and attention mechanism. ||| jialiang tang ||| mingjin liu ||| ning jiang ||| wenxin yu ||| changzheng yang ||| jinjia zhou ||| 
2020 ||| mam: mixed attention module with random disruption augmentation for image classification. ||| weiyu zeng ||| jiuwen cao ||| jianzhong wang ||| xiaoping lai ||| zhiping lin ||| 
2020 ||| investigation of effect of stray capacitances in air-core toroidal transformer at high-frequency oscillation based on internal magnetic flux density. ||| kazuki hashimoto ||| takafumi okuda ||| takashi hikihara ||| 
2020 ||| pedestrian tracking with gated recurrent units and attention mechanisms. ||| mahdi elhousni ||| xinming huang ||| 
2020 ||| unsupervised multiple granularities attention-attribute learning for person re-identification. ||| rui yang ||| song wu ||| guoqiang xiao ||| 
2021 ||| sdan: stacked diverse attention network for video action recognition. ||| xiaoguang zhu ||| siran huang ||| wenjing fan ||| yuhao cheng ||| huaqing shao ||| peilin liu ||| 
2017 ||| better deep visual attention with reinforcement learning in action recognition. ||| gang wang ||| wenmin wang ||| jingzhuo wang ||| yaohua bu ||| 
2021 ||| a novel low-complexity attention-driven composite model for speech enhancement. ||| mojtaba hasannezhad ||| wei-ping zhu ||| beno ||| t champagne ||| 
2020 ||| through-the-barrier communications in isolated class-e converters embedding a low-k transformer. ||| fabio pareschi ||| andrea celentano ||| mauro mangia ||| riccardo rovatti ||| gianluca setti ||| 
2017 ||| a transformer-less duplexer with out-of-band filtering for same-channel full-duplex radios. ||| prateek kumar sharma ||| nagarjuna nallam ||| 
2019 ||| transformer-based ultra-wide band 43 ghz vco in 28 nm cmos for fmcw radar system. ||| zhiyuan chen ||| guopei chen ||| zipeng chen ||| baoyong chi ||| guolin li ||| 
2021 ||| spatial and channel dimensions attention feature transfer for better convolutional neural networks. ||| jialiang tang ||| mingjin liu ||| ning jiang ||| wenxin yu ||| changzheng yang ||| 
2020 ||| generative image inpainting based on wavelet transform attention model. ||| chen wang ||| jin wang ||| qing zhu ||| baocai yin ||| 
2018 ||| cmos rectifier with on-chip transformer-coupled tunable matching network for biomedical implants. ||| ziyu wang ||| shahriar mirabbasi ||| 
2020 ||| dynamic spatial-temporal graph attention graph convolutional network for short-term traffic flow forecasting. ||| cong tang ||| jingru sun ||| yichuang sun ||| 
2021 ||| attention-based bidirectional lstm-cnn model for remaining useful life estimation. ||| jou won song ||| ye in park ||| jong-ju hong ||| seonggyun kim ||| suk-ju kang ||| 
2019 ||| spatial-temporal visual attention model for video quality assessment. ||| wei-juen suen ||| hsin-hua liu ||| soo-chang pei ||| kuan-hsien liu ||| tsung-jung liu ||| 
2021 ||| dual-path deep supervision network with self-attention for visible-infrared person re-identification. ||| yunzhou cheng ||| xinyi li ||| guoqiang xiao ||| wenzhuo ma ||| xinye gou ||| 
2021 ||| attention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario. ||| chiang-jen peng ||| yun-ju chan ||| cheng yu ||| syu-siang wang ||| yu tsao ||| tai-shih chi ||| 
2020 ||| tweet stance detection: a two-stage dc-bilstm model based on semantic attention. ||| yuanyu yang ||| bin wu ||| kai zhao ||| wenying guo ||| 
2019 ||| short-term traffic flow prediction using attention-based long short-term memory network. ||| peng peng ||| dongwei xu ||| he gao ||| qi xuan ||| yi liu ||| haifeng guo ||| defeng he ||| 
2020 ||| word level domain-diversity attention based lstm model for sentiment classification. ||| haoliang zhang ||| hongbo xu ||| jinqiao shi ||| tingwen liu ||| chun liao ||| 
2020 ||| blhnn: a novel charge prediction model based on bi-attention lstm-cnn hybrid neural network. ||| jia guo ||| bin wu ||| pengpeng zhou ||| 
2019 ||| attention-based text recognition in image. ||| yiwei zhu ||| shilin wang ||| zheng huang ||| kai chen ||| xiang lin ||| quanhai zhang ||| 
2019 ||| stns-csg: syntax tree networks with self-attention for complex sql generation. ||| miaomiao hong ||| bin wu ||| bai wang ||| pengpeng zhou ||| 
2020 ||| a transformer-based model for sentence-level chinese mandarin lipreading. ||| shihui ma ||| shilin wang ||| xiang lin ||| 
2019 ||| evaluating performance and accuracy improvements for attention-ocr. ||| adam brzeski ||| kamil grinholc ||| kamil nowodworski ||| adam przybylek ||| 
2021 ||| bapm: block attention profiling model for multi-tab website fingerprinting attacks on tor. ||| zhong guan ||| gang xiong ||| gaopeng gou ||| zhen li ||| mingxin cui ||| chang liu ||| 
2019 ||| attention learning with retrievable acoustic embedding of personality for emotion recognition. ||| jeng-lin li ||| chi-chun lee ||| 
2021 ||| using multimodal transformers in affective computing. ||| juan vazquez-rodriguez ||| 
2021 ||| modality fusion network and personalized attention in momentary stress detection in the wild. ||| han yu ||| thomas vaessen ||| inez myin-germeys ||| akane sano ||| 
2019 ||| slices of attention in asynchronous video job interviews. ||| l ||| o hemamou ||| ghazi felhi ||| jean-claude martin ||| chlo |||  clavel ||| 
2021 ||| using knowledge-embedded attention to augment pre-trained language models for fine-grained emotion recognition. ||| varsha suresh ||| desmond c. ong ||| 
2017 ||| visual attention in schizophrenia: eye contact and gaze aversion during clinical interactions. ||| alexandria katarina vail ||| tadas baltrusaitis ||| luciana pennant ||| elizabeth s. liebson ||| justin t. baker ||| louis-philippe morency ||| 
2017 ||| effect of different music genre: attention vs. meditation. ||| esther ramdinmawii ||| vinay kumar mittal ||| 
2019 ||| attention/distraction estimation for surgeons during laparoscopic cholecystectomies. ||| bernhard anzengruber-tanase ||| alois ferscha ||| martin schobesberger ||| 
2021 ||| emotion-aware transformer encoder for empathetic dialogue generation. ||| raman goel ||| seba susan ||| sachin vashisht ||| armaan dhanda ||| 
2019 ||| learning temporal and bodily attention in protective movement behavior detection. ||| chongyang wang ||| min peng ||| temitayo a. olugbade ||| nicholas d. lane ||| amanda c. de c. williams ||| nadia bianchi-berthouze ||| 
2019 ||| combining gated convolutional networks and self-attention mechanism for speech emotion recognition. ||| chao li ||| jinlong jiao ||| yiqin zhao ||| ziping zhao ||| 
2020 ||| unseen filler generalization in attention-based natural language reasoning models. ||| chin-hui chen ||| yi-fu fu ||| hsiao-hua cheng ||| shou-de lin ||| 
2018 ||| seeing signs of danger: attention-accelerated hazmat label detection. ||| mahmoud a. mohamed ||| jan t ||| nnermann ||| b ||| rbel mertsching ||| 
2018 ||| human spatio-temporal attention modeling using head pose tracking for implicit object of interest discrimination in robot agents. ||| corey johnson ||| lynne e. parker ||| 
2020 ||| eye-tracking study of direction influence of user's attention for intelligence system design. ||| veronika ander ||| petr cihelka ||| jan tyrychtr ||| tom ||| s benda ||| hana vostr |||  vydrov ||| dana klimesov ||| 
2017 ||| intelligent notification and attention management on mobile devices. ||| dominik weber ||| alexandra voit ||| anja exler ||| svenja schr ||| der ||| matthias b ||| hmer ||| tadashi okoshi ||| 
2021 ||| the attention kitchen: comparing modalities for smart home notifications in a cooking scenario. ||| alexandra voit ||| thomas kosch ||| henrike weing ||| rtner ||| pawel w. wozniak ||| 
2019 ||| evaluation of attention inducing effects using ubiquitous humanlike face robots. ||| ryosuke eguchi ||| naoya isoyama ||| tsutomu terada ||| masahiko tsukamoto ||| 
2020 ||| addressing inattentional blindness with smart eyewear and vibrotactile feedback on the finger, wrist, and forearm. ||| adrian aiordachioae ||| david gherasim ||| alexandru-ilie maciuc ||| bogdan-florin gheran ||| radu-daniel vatavu ||| 
2021 ||| simplifying paragraph-level question generation via transformer language models. ||| luis enrico lopez ||| diane kathryn cruz ||| jan christian blaise cruz ||| charibeth cheng ||| 
2021 ||| abae: utilize attention to boost graph auto-encoder. ||| tianyu liu ||| yifan li ||| yujie sun ||| lixin cui ||| lu bai ||| 
2021 ||| online multi-object tracking with pose-guided object location and dual self-attention network. ||| xin zhang ||| shihao wang ||| yuanzhe yang ||| chengxiang chu ||| zhong zhou ||| 
2019 ||| hierarchical convolutional attention networks using joint chinese word embedding for text classification. ||| kaiqiang zhang ||| shupeng wang ||| binbin li ||| feng mei ||| jianyu zhang ||| 
2019 ||| deep learning method with attention for extreme multi-label text classification. ||| si chen ||| liangguo wang ||| wan li ||| kun zhang ||| 
2021 ||| span: subgraph prediction attention network for dynamic graphs. ||| yuan li ||| chuanchang chen ||| yubo tao ||| hai lin ||| 
2019 ||| saf: semantic attention fusion mechanism for pedestrian detection. ||| ruizhe yu ||| shunzhou wang ||| yao lu ||| huijun di ||| lin zhang ||| lihua lu ||| 
2021 ||| adversarial training for image captioning incorporating relation attention. ||| tianyu chen ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2019 ||| cd-abm: curriculum design with attention branch model for person re-identification. ||| junhong chen ||| jiuchao qian ||| xiaoguang zhu ||| fei wen ||| yan hong ||| peilin liu ||| 
2021 ||| vsec: transformer-based model for vietnamese spelling correction. ||| dinh-truong do ||| ha thanh nguyen ||| thang ngoc bui ||| hieu dinh vo ||| 
2019 ||| a hierarchical attention based seq2seq model for chinese lyrics generation. ||| haoshen fan ||| jie wang ||| bojin zhuang ||| shaojun wang ||| jing xiao ||| 
2019 ||| multi-label recognition of paintings with cascaded attention network. ||| yue li ||| tingting wang ||| guangwei huang ||| xiaojun tang ||| 
2019 ||| hint-embedding attention-based lstm for aspect identification sentiment analysis. ||| murtadha h. m. ahmed ||| qun chen ||| yanyan wang ||| zhanhuai li ||| 
2019 ||| discriminative deep attention-aware hashing for face image retrieval. ||| zhi xiong ||| bo li ||| xiaoyan gu ||| wen gu ||| weiping wang ||| 
2018 ||| phonologically aware bilstm model for mongolian phrase break prediction with attention mechanism. ||| rui liu ||| feilong bao ||| guanglai gao ||| hui zhang ||| yonghe wang ||| 
2021 ||| pupilface: a cascaded face detection and location network fusing attention. ||| xiang li ||| jiancheng zou ||| 
2021 ||| van: voting and attention based network for unsupervised medical image registration. ||| zhiang zu ||| guixu zhang ||| yaxin peng ||| zhen ye ||| chaomin shen ||| 
2018 ||| attention-based linguistically constraints network for aspect-level sentiment. ||| jinyu lu ||| yuexian hou ||| 
2018 ||| reading more efficiently: multi-sentence summarization with a dual attention and copy-generator network. ||| xi zhang ||| hua-ping zhang ||| lei zhao ||| 
2018 ||| attention based meta path fusion for heterogeneous information network embedding. ||| houye ji ||| chuan shi ||| bai wang ||| 
2021 ||| graph attention convolutional network with motion tempo enhancement for skeleton-based action recognition. ||| ruwen bai ||| xiang meng ||| bo meng ||| miao jiang ||| junxing ren ||| yang yang ||| min li ||| degang sun ||| 
2019 ||| time-guided high-order attention model of longitudinal heterogeneous healthcare data. ||| yi huang ||| xiaoshan yang ||| changsheng xu ||| 
2021 ||| anf: attention-based noise filtering strategy for unsupervised few-shot classification. ||| guangsen ni ||| hongguang zhang ||| jing zhao ||| liyang xu ||| wenjing yang ||| long lan ||| 
2018 ||| matching attention network for domain adaptation optimized by joint gans and kl-mmd. ||| yuan-zhu gan ||| hai-qing wang ||| lu-fei liu ||| yu-bin yang ||| 
2021 ||| an attention-based approach to accelerating sequence generative adversarial nets. ||| minglei gao ||| sai zhang ||| xiaowang zhang ||| zhiyong feng ||| 
2021 ||| random walk erasing with attention calibration for action recognition. ||| yuze tian ||| xian zhong ||| wenxuan liu ||| xuemei jia ||| shilei zhao ||| mang ye ||| 
2021 ||| off-tanet: a lightweight neural micro-expression recognizer with optical flow features and integrated attention mechanism. ||| jiahao zhang ||| feng liu ||| aimin zhou ||| 
2021 ||| heterogeneous graph attention network for user geolocation. ||| xuan zhang ||| fuqiang lin ||| diwen dong ||| wangqun chen ||| bo liu ||| 
2018 ||| two-step multi-factor attention neural network for answer selection. ||| pengqing zhang ||| yuexian hou ||| zhan su ||| yi su ||| 
2019 ||| a light-weight context-aware self-attention model for skin lesion segmentation. ||| dongliang ma ||| hao wu ||| jun sun ||| chunjing yu ||| li liu ||| 
2019 ||| duo attention with deep learning on tomato yield prediction and factor interpretation. ||| sandya de alwis ||| yishuo zhang ||| myung na ||| gang li ||| 
2021 ||| punctuation prediction in vietnamese asrs using transformer-based models. ||| viet the bui ||| oanh thi tran ||| 
2018 ||| unrest news amount prediction with context-aware attention lstm. ||| xiuling wang ||| hao chen ||| zhoujun li ||| zhonghua zhao ||| 
2019 ||| kb-transformer: incorporating knowledge into end-to-end task-oriented dialog systems. ||| haihong e ||| wenjing zhang ||| meina song ||| 
2019 ||| distribution transformer condition monitoring based on edge intelligence for industrial iot. ||| leny thangiah ||| chandrashekar ramanathan ||| lakshmi sirisha chodisetty ||| 
2021 ||| environmental sound classification with tiny transformers in noisy edge environments. ||| steven wyatt ||| david elliott ||| akshay aravamudan ||| carlos e. otero ||| luis daniel otero ||| georgios c. anagnostopoulos ||| anthony o. smith ||| adrian m. peter ||| wesley jones ||| steven leung ||| eric lam ||| 
2021 ||| malicious login detection using long short-term memory with an attention mechanism. ||| yanna wu ||| fucheng liu ||| yu wen ||| 
2021 ||| pfedatt: attention-based personalized federated learning on heterogeneous clients. ||| zichen ma ||| yu lu ||| wenye li ||| jinfeng yi ||| shuguang cui ||| 
2019 ||| a model of text-enhanced knowledge graph representation learning with collaborative attention. ||| yashen wang ||| huanhuan zhang ||| haiyong xie ||| 
2018 ||| stock price prediction using attention-based multi-input lstm. ||| hao li ||| yanyan shen ||| yanmin zhu ||| 
2019 ||| multi-scale visual semantics aggregation with self-attention for end-to-end image-text matching. ||| zhuobin zheng ||| youcheng ben ||| chun yuan ||| 
2021 ||| relation also need attention: integrating relation information into image captioning. ||| tianyu chen ||| zhixin li ||| tiantao xian ||| canlong zhang ||| huifang ma ||| 
2019 ||| realistic image generation using region-phrase attention. ||| wanming huang ||| richard yi da xu ||| ian oppermann ||| 
2020 ||| inverse visual question answering with multi-level attentions. ||| yaser alwatter ||| yuhong guo ||| 
2019 ||| unpaired data based cross-domain synthesis and segmentation using attention neural network. ||| xiaoming liu ||| xiangkai wei ||| aihui yu ||| zhifang pan ||| 
2020 ||| bidirectional dependency-guided attention for relation extraction. ||| xingchen deng ||| lei zhang ||| yixing fan ||| long bai ||| jiafeng guo ||| pengfei wang ||| 
2021 ||| s2tnet: spatio-temporal transformer networks for trajectory prediction in autonomous driving. ||| weihuang chen ||| fangfang wang ||| hongbin sun ||| 
2020 ||| aarm: action attention recalibration module for action recognition. ||| zhonghong li ||| yang yi ||| ying she ||| jialun song ||| yukun wu ||| 
2020 ||| efficient attention calibration network for real-time semantic segmentation. ||| hengfeng zha ||| rui liu ||| dongsheng zhou ||| xin yang ||| qiang zhang ||| xiaopeng wei ||| 
2018 ||| tvt: two-view transformer network for video captioning. ||| ming chen ||| yingming li ||| zhongfei zhang ||| siyu huang ||| 
2018 ||| adversarial tableqa: attention supervision for question answering on tables. ||| minseok cho ||| reinald kim amplayo ||| seung-won hwang ||| jonghyuck park ||| 
2018 ||| a hierarchical conditional attention-based neural networks for paraphrase generation. ||| khuong nguyen-ngoc ||| anh-cuong le ||| viet-ha nguyen ||| 
2017 ||| hierarchical attention network with xgboost for recognizing insufficiently supported argument. ||| derwin suhartono ||| aryo pradipta gema ||| suhendro winton ||| theodorus david ||| mohamad ivan fanany ||| aniati murni arymurthy ||| 
2020 ||| heterogeneous graph attention networks for scalable multi-robot scheduling with temporospatial constraints. ||| zheyuan wang ||| matthew c. gombolay ||| 
2020 ||| transformer-based context-aware sarcasm detection in conversation threads from social media. ||| xiangjue dong ||| changmao li ||| jinho d. choi ||| 
2020 ||| applying transformers and aspect-based sentiment analysis approaches on sarcasm detection. ||| taha shangipour ataei ||| soroush javdan ||| behrouz minaei-bidgoli ||| 
2020 ||| transformers on sarcasm detection with context. ||| amardeep kumar ||| vivek anand ||| 
2020 ||| a transformer approach to contextual sarcasm detection in twitter. ||| hunter gregory ||| steven li ||| pouya mohammadi ||| natalie tarn ||| rachel lea draelos ||| cynthia rudin ||| 
2020 ||| detecting sarcasm in conversation context using transformer-based models. ||| adithya avvaru ||| sanath vobilisetty ||| radhika mamidi ||| 
2020 ||| go figure! multi-task transformer-based architecture for metaphor detection using idioms: ets team in 2020 metaphor shared task. ||| xianyang chen ||| chee wee leong ||| michael flor ||| beata beigman klebanov ||| 
2020 ||| metaphor detection using contextual word embeddings from transformers. ||| jerry liu ||| nathan o'hara ||| alexander rubin ||| rachel lea draelos ||| cynthia rudin ||| 
2021 ||| contextual-semantic-aware linkable knowledge prediction in stack overflow via self-attention. ||| zhaolin luo ||| ling xu ||| zhou xu ||| meng yan ||| yan lei ||| can li ||| 
2021 ||| exception handling recommendation based on self-attention network. ||| kai lin ||| chuanqi tao ||| zhiqiu huang ||| 
2021 ||| artificial intelligence in the innovation process - do we pay attention to this participant in innovative projects? ||| zornitsa yordanova ||| 
2017 ||| a state-based game attention model for cloud gaming. ||| ebrahim babaei ||| mahmoud reza hashemi ||| shervin shirmohammadi ||| 
2017 ||| towards improving visual attention models using influencing factors in a video gaming context. ||| saman zadtootaghaj ||| steven schmidt ||| hamed ahmadi ||| sebastian m ||| ller ||| 
2021 ||| msa transformer. ||| roshan rao ||| jason liu ||| robert verkuil ||| joshua meier ||| john f. canny ||| pieter abbeel ||| tom sercu ||| alexander rives ||| 
2019 ||| processing megapixel images with deep attention-sampling models. ||| angelos katharopoulos ||| fran ||| ois fleuret ||| 
2021 ||| thinking like transformers. ||| gail weiss ||| yoav goldberg ||| eran yahav ||| 
2021 ||| convit: improving vision transformers with soft convolutional inductive biases. ||| st ||| phane d'ascoli ||| hugo touvron ||| matthew l. leavitt ||| ari s. morcos ||| giulio biroli ||| levent sagun ||| 
2020 ||| learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. ||| sarthak mittal ||| alex lamb ||| anirudh goyal ||| vikram voleti ||| murray shanahan ||| guillaume lajoie ||| michael mozer ||| yoshua bengio ||| 
2018 ||| overcoming catastrophic forgetting with hard attention to the task. ||| joan serr ||| didac suris ||| marius miron ||| alexandros karatzoglou ||| 
2021 ||| evolving attention with residual convolutions. ||| yujing wang ||| yaming yang ||| jiangang bai ||| mingliang zhang ||| jing bai ||| jing yu ||| ce zhang ||| gao huang ||| yunhai tong ||| 
2021 ||| learning self-modulating attention in continuous time space with applications to sequential recommendation. ||| chao chen ||| haoyu geng ||| nianzu yang ||| junchi yan ||| daiyue xue ||| jianping yu ||| xiaokang yang ||| 
2020 ||| improving transformer optimization through better initialization. ||| xiao shi huang ||| felipe p ||| rez ||| jimmy ba ||| maksims volkovs ||| 
2021 ||| trees with attention for set prediction tasks. ||| roy hirsch ||| ran gilad-bachrach ||| 
2020 ||| on layer normalization in the transformer architecture. ||| ruibin xiong ||| yunchang yang ||| di he ||| kai zheng ||| shuxin zheng ||| chen xing ||| huishuai zhang ||| yanyan lan ||| liwei wang ||| tie-yan liu ||| 
2021 ||| sparsebert: rethinking the importance analysis in self-attention. ||| han shi ||| jiahui gao ||| xiaozhe ren ||| hang xu ||| xiaodan liang ||| zhenguo li ||| james tin-yau kwok ||| 
2019 ||| self-attention graph pooling. ||| junhyun lee ||| inyeop lee ||| jaewoo kang ||| 
2021 ||| synthesizer: rethinking self-attention for transformer models. ||| yi tay ||| dara bahri ||| donald metzler ||| da-cheng juan ||| zhe zhao ||| che zheng ||| 
2021 ||| is space-time attention all you need for video understanding? ||| gedas bertasius ||| heng wang ||| lorenzo torresani ||| 
2017 ||| online and linear-time attention by enforcing monotonic alignments. ||| colin raffel ||| minh-thang luong ||| peter j. liu ||| ron j. weiss ||| douglas eck ||| 
2021 ||| linear transformers are secretly fast weight programmers. ||| imanol schlag ||| kazuki irie ||| j ||| rgen schmidhuber ||| 
2019 ||| the evolved transformer. ||| david r. so ||| quoc v. le ||| chen liang ||| 
2021 ||| a unified generative adversarial network training via self-labeling and self-attention. ||| tomoki watanabe ||| paolo favaro ||| 
2021 ||| poolingformer: long document modeling with pooling attention. ||| hang zhang ||| yeyun gong ||| yelong shen ||| weisheng li ||| jiancheng lv ||| nan duan ||| weizhu chen ||| 
2019 ||| insertion transformer: flexible sequence generation via insertion operations. ||| mitchell stern ||| william chan ||| jamie kiros ||| jakob uszkoreit ||| 
2021 ||| perceiver: general perception with iterative attention. ||| andrew jaegle ||| felix gimeno ||| andy brock ||| oriol vinyals ||| andrew zisserman ||| jo ||| o carreira ||| 
2020 ||| infinite attention: nngp and ntk for deep attention networks. ||| jiri hron ||| yasaman bahri ||| jascha sohl-dickstein ||| roman novak ||| 
2021 ||| you only sample (almost) once: linear cost self-attention via bernoulli sampling. ||| zhanpeng zeng ||| yunyang xiong ||| sathya n. ravi ||| shailesh acharya ||| glenn moo fung ||| vikas singh ||| 
2020 ||| stabilizing transformers for reinforcement learning. ||| emilio parisotto ||| h. francis song ||| jack w. rae ||| razvan pascanu ||| aglar g ||| l ||| ehre ||| siddhant m. jayakumar ||| max jaderberg ||| rapha ||| l lopez kaufman ||| aidan clark ||| seb noury ||| matthew botvinick ||| nicolas heess ||| raia hadsell ||| 
2021 ||| el-attention: memory efficient lossless attention for generation. ||| yu yan ||| jiusheng chen ||| weizhen qi ||| nikhil bhendawade ||| yeyun gong ||| nan duan ||| ruofei zhang ||| 
2020 ||| cost-effective interactive attention learning with neural attention processes. ||| jay heo ||| junhyeon park ||| hyewon jeong ||| kwang joon kim ||| juho lee ||| eunho yang ||| sung ju hwang ||| 
2018 ||| image transformer. ||| niki parmar ||| ashish vaswani ||| jakob uszkoreit ||| lukasz kaiser ||| noam shazeer ||| alexander ku ||| dustin tran ||| 
2020 ||| transformers are rnns: fast autoregressive transformers with linear attention. ||| angelos katharopoulos ||| apoorv vyas ||| nikolaos pappas ||| fran ||| ois fleuret ||| 
2018 ||| differentiable dynamic programming for structured prediction and attention. ||| arthur mensch ||| mathieu blondel ||| 
2020 ||| low-rank bottleneck in multi-head attention models. ||| srinadh bhojanapalli ||| chulhee yun ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2021 ||| pixeltransformer: sample conditioned signal generation. ||| shubham tulsiani ||| abhinav gupta ||| 
2021 ||| omninet: omnidirectional representations from transformers. ||| yi tay ||| mostafa dehghani ||| vamsi aribandi ||| jai prakash gupta ||| philip pham ||| zhen qin ||| dara bahri ||| da-cheng juan ||| donald metzler ||| 
2018 ||| attention-based deep multiple instance learning. ||| maximilian ilse ||| jakub m. tomczak ||| max welling ||| 
2021 ||| catformer: designing stable transformers via sensitivity analysis. ||| jared quincy davis ||| albert gu ||| krzysztof choromanski ||| tri dao ||| christopher r ||| chelsea finn ||| percy liang ||| 
2021 ||| relative positional encoding for transformers with linear complexity. ||| antoine liutkus ||| ondrej c ||| fka ||| shih-lun wu ||| umut simsekli ||| yi-hsuan yang ||| ga ||| l richard ||| 
2019 ||| set transformer: a framework for attention-based permutation-invariant neural networks. ||| juho lee ||| yoonho lee ||| jungtaek kim ||| adam r. kosiorek ||| seungjin choi ||| yee whye teh ||| 
2021 ||| lipschitz normalization for self-attention layers with application to graph neural networks. ||| george dasoulas ||| kevin scaman ||| aladin virmaux ||| 
2021 ||| autoattend: automated attention representation search. ||| chaoyu guan ||| xin wang ||| wenwu zhu ||| 
2020 ||| transformer hawkes process. ||| simiao zuo ||| haoming jiang ||| zichong li ||| tuo zhao ||| hongyuan zha ||| 
2020 ||| powernorm: rethinking batch normalization in transformers. ||| sheng shen ||| zhewei yao ||| amir gholami ||| michael w. mahoney ||| kurt keutzer ||| 
2019 ||| bert and pals: projected attention layers for efficient adaptation in multi-task learning. ||| asa cooper stickland ||| iain murray ||| 
2020 ||| sparse sinkhorn attention. ||| yi tay ||| dara bahri ||| liu yang ||| donald metzler ||| da-cheng juan ||| 
2021 ||| which transformer architecture fits my data? a vocabulary bottleneck in self-attention. ||| noam wies ||| yoav levine ||| daniel jannai ||| amnon shashua ||| 
2021 ||| tfix: learning to fix coding errors with a text-to-text transformer. ||| berkay berabi ||| jingxuan he ||| veselin raychev ||| martin t. vechev ||| 
2017 ||| image-to-markup generation with coarse-to-fine attention. ||| yuntian deng ||| anssi kanervisto ||| jeffrey ling ||| alexander m. rush ||| 
2019 ||| actor-attention-critic for multi-agent reinforcement learning. ||| shariq iqbal ||| fei sha ||| 
2021 ||| vilt: vision-and-language transformer without convolution or region supervision. ||| wonjae kim ||| bokyung son ||| ildoo kim ||| 
2021 |||  distillation through attention. ||| hugo touvron ||| matthieu cord ||| matthijs douze ||| francisco massa ||| alexandre sablayrolles ||| herv |||  j ||| gou ||| 
2020 ||| learning to encode position for transformer with continuous dynamical model. ||| xuanqing liu ||| hsiang-fu yu ||| inderjit s. dhillon ||| cho-jui hsieh ||| 
2020 ||| non-autoregressive machine translation with disentangled context transformer. ||| jungo kasai ||| james cross ||| marjan ghazvininejad ||| jiatao gu ||| 
2019 ||| self-attention generative adversarial networks. ||| han zhang ||| ian j. goodfellow ||| dimitris n. metaxas ||| augustus odena ||| 
2021 ||| lietransformer: equivariant self-attention for lie groups. ||| michael hutchinson ||| charline le lan ||| sheheryar zaidi ||| emilien dupont ||| yee whye teh ||| hyunjik kim ||| 
2021 ||| generative adversarial transformers. ||| drew a. hudson ||| larry zitnick ||| 
2020 ||| encoding musical style with transformer autoencoders. ||| kristy choi ||| curtis hawthorne ||| ian simon ||| monica dinculescu ||| jesse h. engel ||| 
2021 ||| cate: computation-aware neural architecture encoding with transformers. ||| shen yan ||| kaiqiang song ||| fei liu ||| mi zhang ||| 
2019 ||| area attention. ||| yang li ||| lukasz kaiser ||| samy bengio ||| si si ||| 
2021 ||| the lipschitz constant of self-attention. ||| hyunjik kim ||| george papamakarios ||| andriy mnih ||| 
2019 ||| equivariant transformer networks. ||| kai sheng tai ||| peter bailis ||| gregory valiant ||| 
2021 ||| attention is not all you need: pure attention loses rank doubly exponentially with depth. ||| yihe dong ||| jean-baptiste cordonnier ||| andreas loukas ||| 
2020 ||| train big, then compress: rethinking model size for efficient training and inference of transformers. ||| zhuohan li ||| eric wallace ||| sheng shen ||| kevin lin ||| kurt keutzer ||| dan klein ||| joey gonzalez ||| 
2021 ||| bayesian attention belief networks. ||| shujian zhang ||| xinjie fan ||| bo chen ||| mingyuan zhou ||| 
2021 ||| simam: a simple, parameter-free attention module for convolutional neural networks. ||| lingxiao yang ||| ru-yuan zhang ||| lida li ||| xiaohua xie ||| 
2021 ||| differentiable spatial planning using transformers. ||| devendra singh chaplot ||| deepak pathak ||| jitendra malik ||| 
2021 ||| pipetransformer: automated elastic pipelining for distributed training of large-scale models. ||| chaoyang he ||| shen li ||| mahdi soltanolkotabi ||| salman avestimehr ||| 
2021 ||| generative video transformer: can objects be the words? ||| yi-fu wu ||| jaesik yoon ||| sungjin ahn ||| 
2017 ||| position shift of phosphene and attention attraction in arbitrary direction with galvanic retina stimulation. ||| daiki higuchi ||| kazuma aoyama ||| masahiro furukawa ||| taro maeda ||| hideyuki ando ||| 
2021 ||| exploiting triangle patterns for heterogeneous graph attention network. ||| eunjeong yi ||| min-soo kim ||| 
2021 ||| visualizing web users' attention to text with selection heatmaps. ||| ilan kirsh ||| 
2020 ||| detecting rumor on microblogging platforms via a hybrid stance attention mechanism. ||| lingyu zeng ||| bin wu ||| bai wang ||| 
2020 ||| a hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention. ||| maria mihaela trusca ||| daan wassenberg ||| flavius frasincar ||| rommert dekker ||| 
2022 ||| multi-resolution attention for personalized item search. ||| furkan kocayusufoglu ||| tao wu ||| anima singh ||| georgios roumpos ||| heng-tze cheng ||| sagar jain ||| ed chi ||| ambuj singh ||| 
2020 ||| interpretable click-through rate prediction through hierarchical attention. ||| zeyu li ||| wei cheng ||| yang chen ||| haifeng chen ||| wei wang ||| 
2019 ||| social attentional memory network: modeling aspect- and friend-level differences in recommendation. ||| chong chen ||| min zhang ||| yiqun liu ||| shaoping ma ||| 
2020 ||| jointly optimized neural coreference resolution with mutual attention. ||| jie ma ||| jun liu ||| yufei li ||| xin hu ||| yudai pan ||| shen sun ||| qika lin ||| 
2021 ||| incorporating wide context information for deep knowledge tracing using attentional bi-interaction. ||| raghava krishnan ||| janmajay singh ||| masahiro sato ||| qian zhang ||| tomoko ohkuma ||| 
2020 ||| dysat: deep neural representation learning on dynamic graphs via self-attention networks. ||| aravind sankar ||| yanhong wu ||| liang gou ||| wei zhang ||| hao yang ||| 
2022 ||| pretraining multi-modal representations for chinese ner task with cross-modality attention. ||| chengcheng mai ||| mengchuan qiu ||| kaiwen luo ||| ziyan peng ||| jian liu ||| chunfeng yuan ||| yihua huang ||| 
2022 ||| a neighborhood-attention fine-grained entity typing for knowledge graph completion. ||| jianhuan zhuo ||| qiannan zhu ||| yinliang yue ||| yuhong zhao ||| weisi han ||| 
2020 ||| recurrent attention walk for semi-supervised classification. ||| uchenna akujuobi ||| qiannan zhang ||| han yufei ||| xiangliang zhang ||| 
2019 ||| msa: jointly detecting drug name and adverse drug reaction mentioning tweets with multi-head self-attention. ||| chuhan wu ||| fangzhao wu ||| zhigang yuan ||| junxin liu ||| yongfeng huang ||| xing xie ||| 
2020 ||| time interval aware self-attention for sequential recommendation. ||| jiacheng li ||| yujie wang ||| julian j. mcauley ||| 
2021 ||| origin-aware next destination recommendation with personalized preference attention. ||| nicholas lim ||| bryan hooi ||| see-kiong ng ||| xueou wang ||| yong liang goh ||| renrong weng ||| rui tan ||| 
2022 ||| scope-aware re-ranking with gated attention in feed. ||| hao qian ||| qintong wu ||| kai zhang ||| zhiqiang zhang ||| lihong gu ||| xiaodong zeng ||| jun zhou ||| jinjie gu ||| 
2019 ||| session-based social recommendation via dynamic graph attention networks. ||| weiping song ||| zhiping xiao ||| yifan wang ||| laurent charlin ||| ming zhang ||| jian tang ||| 
2017 ||| multi-column convolutional neural networks with causality-attention for why-question answering. ||| jong-hoon oh ||| kentaro torisawa ||| canasai kruengkrai ||| ryu iida ||| julien kloetzer ||| 
2021 ||| attentionflow: visualising influence in networks of time series. ||| minjeong shin ||| alasdair tran ||| siqi wu ||| alexander patrick mathews ||| rong wang ||| georgiana lyall ||| lexing xie ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond. ||| andrew yates ||| rodrigo nogueira ||| jimmy lin ||| 
2018 ||| predicting multi-step citywide passenger demands using attention-based neural networks. ||| xian zhou ||| yanyan shen ||| yanmin zhu ||| linpeng huang ||| 
2019 ||| how is attention allocated?: data-driven studies of popularity and engagement in online videos. ||| siqi wu ||| 
2022 ||| a personalized cross-platform post style transfer method based on transformer and bi-attention mechanism. ||| zhuo chen ||| baoxi liu ||| peng zhang ||| tun lu ||| hansu gu ||| ning gu ||| 
2021 ||| combining rnn with transformer for modeling multi-leg trips. ||| yoshihiro sakatani ||| 
2021 ||| modeling across-context attention for long-tail query classification in e-commerce. ||| junhao zhang ||| weidi xu ||| jianhui ji ||| xi chen ||| hongbo deng ||| keping yang ||| 
2022 ||| learning concept prerequisite relations from educational data via multi-head attention variational graph auto-encoders. ||| juntao zhang ||| nanzhou lin ||| xuelong zhang ||| wei song ||| xiandi yang ||| zhiyong peng ||| 
2021 ||| multi-interactive attention network for fine-grained feature learning in ctr prediction. ||| kai zhang ||| hao qian ||| qing cui ||| qi liu ||| longfei li ||| jun zhou ||| jianhui ma ||| enhong chen ||| 
2021 ||| attention-based neural re-ranking approach for next city in trip recommendations. ||| aleksandr petrov ||| yuriy makarov ||| 
2021 ||| test students' attention in class using a mobile application. ||| doru anastasiu popescu ||| cosmin iulian gosoiu ||| ion alexandru popescu ||| 
2018 ||| a survey of technologies utilized in the treatment and diagnosis of attention deficit hyperactivity disorder. ||| lauren e. johnson ||| james m. conrad ||| 
2019 ||| goal-oriented conversational system using transfer learning and attention mechanism. ||| amartya hatua ||| trung t. nguyen ||| andrew h. sung ||| 
2021 ||| interactive attention network for chinese address element recognition. ||| yusheng bi ||| lihua tian ||| chen li ||| 
2018 ||| quantifying the attention potential of pervasive display placements. ||| marcus winter ||| ian brunswick ||| derek williams ||| 
2019 ||| attention-based visual-audio fusion for video caption generation. ||| ningning guo ||| huaping liu ||| linhua jiang ||| 
2019 ||| dog face recognition algorithm based on dc-attention-ssd neural network. ||| peng wang ||| quanbo ge ||| zhenyu lu ||| tianming zhan ||| 
2017 ||| design and application of a compatible clamping fixture for current transformers auto-testing line. ||| gang kong ||| meng zhang ||| baijiang an ||| jian yang ||| 
2019 ||| running state prediction and evaluation of power transformers. ||| lingming kong ||| le luan ||| kai zhou ||| chao chen ||| jinmei chen ||| zhuzhu wang ||| 
2021 ||| learning knowledge graph embeddings by multi-attention mechanism for link prediction. ||| meihong wang ||| han li ||| linling qiu ||| 
2021 ||| temporal convolution network based on attention for intelligent anomaly detection of wind turbine blades. ||| jianwen ding ||| fan lin ||| shengbo lv ||| 
2020 ||| automatic medical image report generation with multi-view and multi-modal attention mechanism. ||| shaokang yang ||| jianwei niu ||| jiyan wu ||| xuefeng liu ||| 
2021 ||| attention-based cross-domain gesture recognition using wifi channel state information. ||| hao hong ||| baoqi huang ||| yu gu ||| bing jia ||| 
2021 ||| transformer-based rating-aware sequential recommendation. ||| yang li ||| qianmu li ||| shunmei meng ||| jun hou ||| 
2020 ||| lexicon-enhanced transformer with pointing for domains specific generative question answering. ||| jingying yang ||| xianghua fu ||| shuxin wang ||| wenhao xie ||| 
2021 ||| spatio-temporal topology routing algorithm for opportunistic network based on self-attention mechanism. ||| xiaorui wu ||| gang xu ||| xinyu hao ||| baoqi huang ||| xiangyu bai ||| 
2021 ||| multi-relational hierarchical attention for top-k recommendation. ||| shiwen yang ||| jinghua zhu ||| heran xi ||| 
2021 ||| an efficient message dissemination scheme for cooperative drivings via multi-agent hierarchical attention reinforcement learning. ||| bingyi liu ||| weizhen han ||| enshu wang ||| xin ma ||| shengwu xiong ||| chunming qiao ||| jianping wang ||| 
2021 ||| optimising knee injury detection with spatial attention and validating localisation ability. ||| niamh belton ||| ivan welaratne ||| adil dahlan ||| ronan t. hearne ||| misgina tsighe hagos ||| aonghus lawlor ||| kathleen m. curran ||| 
2019 ||| convolutional attention on images for locating macular edema. ||| maximilian bryan ||| gerhard heyer ||| nathanael philipp ||| matus rehak ||| peter wiedemann ||| 
2020 |||  ambient notification and attention management. ||| niels van berkel ||| anja exler ||| martin gjoreski ||| tine kolenik ||| tadashi okoshi ||| veljko pejovic ||| aku visuri ||| alexandra voit ||| 
2019 ||| toward digital image processing and eye tracking to promote visual attention for people with autism. ||| viseth sean ||| franceli l. cibrian ||| jazette johnson ||| hollis pass ||| louanne e. boyd ||| 
2017 |||  ambient notification and attention management. ||| tadashi okoshi ||| anja exler ||| alexandra voit ||| dominik weber ||| martin pielot ||| benjamin poppinga ||| niels henze ||| sven gehring ||| matthias b ||| hmer ||| seungjun kim ||| veljko pejovic ||| 
2019 ||| capturing attentional problems with smart eyewear. ||| kati pettersson ||| kiti m ||| ller ||| laura sokka ||| satu pakarinen ||| 
2017 ||| a user-centered approach towards attention visualization for learning activities. ||| marvin andujar ||| juan e. gilbert ||| 
2019 ||| nurse care activity recognition: a gru-based approach with attention mechanism. ||| md nazmul haque ||| mahir mahbub ||| md. hasan tarek ||| lutfun nahar lota ||| amin ahsan ali ||| 
2021 ||| an ensemble of convtransformer networks for the sussex-huawei locomotion-transportation (shl) recognition challenge. ||| aosheng tian ||| ye zhang ||| huiling chen ||| chao ma ||| shilin zhou ||| 
2019 ||| attention computing: overview of mobile sensing applied to measuring attention. ||| aku visuri ||| niels van berkel ||| 
2018 ||| on attention models for human activity recognition. ||| vishvak s. murahari ||| thomas pl ||| tz ||| 
2018 ||| attentivu: evaluating the feasibility of biofeedback glasses to monitor and improve attention. ||| nataliya kosmyna ||| utkarsh sarawgi ||| pattie maes ||| 
2021 ||| dark-channel mixed attention based neural networks for smoke detection in fog environment. ||| le yang ||| xiaoli gong ||| zhengwei wu ||| yizeng han ||| lijun he ||| fan li ||| 
2020 ||| behavification: bypassing human's attentional and cognitive systems for automated behavior change. ||| tadashi okoshi ||| wataru sasaki ||| jin nakazawa ||| 
2018 ||| tweet emoji prediction using hierarchical model with attention. ||| chuhan wu ||| fangzhao wu ||| sixing wu ||| yongfeng huang ||| xing xie ||| 
2018 ||| attention management for improved renewable energy usage at households using iot-enabled ambient displays. ||| niaz chowdhury ||| john moore ||| 
2019 ||| gesture recognition based on convlstm-attention implementation of small data semg signals. ||| xin cao ||| masami iwase ||| jun inoue ||| eisaku maeda ||| 
2017 ||| symbiotic attention management in the context of internet of things. ||| shahram jalaliniya ||| thomas pederson ||| diako mardanbegi ||| 
2017 ||| using corneal imaging for measuring a human's visual attention. ||| christian lander ||| felix kosmalla ||| frederik wiehr ||| sven gehring ||| 
2018 ||| understanding and improving recurrent networks for human activity recognition by continuous attention. ||| ming zeng ||| haoxiang gao ||| tong yu ||| ole j. mengshoel ||| helge langseth ||| ian r. lane ||| xiaobing liu ||| 
2021 ||| tacnet: task-aware electroencephalogram classification for brain-computer interface through a novel temporal attention convolutional network. ||| xiaolin liu ||| qianxin hui ||| susu xu ||| shuai wang ||| rui na ||| ying sun ||| xinlei chen ||| dezhi zheng ||| 
2018 |||  ambient notification and attention management. ||| dominik weber ||| anja exler ||| alexandra voit ||| veljko pejovic ||| niels henze ||| sven gehring ||| tadashi okoshi ||| 
2019 |||  ambient notification and attention management. ||| anja exler ||| alexandra voit ||| dominik weber ||| martin pielot ||| nitesh goyal ||| sven gehring ||| tadashi okoshi ||| veljko pejovic ||| 
2020 ||| blink rate variability: a marker of sustained attention during a visual task. ||| rahul gavas ||| mithun b. sheshachala ||| debatri chatterjee ||| ramesh kumar ramakrishnan ||| venkata subramanian viraraghavan ||| achanna anil kumar ||| m. girish chandra ||| 
2020 ||| transformer fault diagnosis based on bp neural network by improved apriori algorithm. ||| guoxiang chang ||| qiaoli gao ||| xinming gao ||| junting cheng ||| 
2018 ||| multi-modal sequence fusion via recursive attention for emotion recognition. ||| rory beard ||| ritwik das ||| raymond w. m. ng ||| p. g. keerthana gopalakrishnan ||| luka eerens ||| pawel swietojanski ||| ondrej miksik ||| 
2018 ||| attention-free encoder decoder for morphological processing. ||| stefan daniel dumitrescu ||| tiberiu boros ||| 
2018 ||| hierarchical attention based position-aware network for aspect-level sentiment analysis. ||| lishuang li ||| yang liu ||| anqiao zhou ||| 
2019 ||| variational semi-supervised aspect-term sentiment analysis via transformer. ||| xingyi cheng ||| weidi xu ||| taifeng wang ||| wei chu ||| weipeng huang ||| kunlong chen ||| junfeng hu ||| 
2017 ||| character sequence-to-sequence model with global attention for universal morphological reinflection. ||| qile zhu ||| yanjun li ||| xiaolin li ||| 
2017 ||| attention-based recurrent convolutional neural network for automatic essay scoring. ||| fei dong ||| yue zhang ||| jie yang ||| 
2021 ||| do pretrained transformers infer telicity like humans? ||| yiyun zhao ||| jian gang ngui ||| lucy hall hartley ||| steven bethard ||| 
2017 ||| su-rug at the conll-sigmorphon 2017 shared task: morphological inflection with attentional sequence-to-sequence models. ||| robert  ||| stling ||| johannes bjerva ||| 
2018 ||| sequence classification with human attention. ||| maria barrett ||| joachim bingel ||| nora hollenstein ||| marek rei ||| anders s ||| gaard ||| 
2020 ||| on the computational power of transformers and its implications in sequence modeling. ||| satwik bhattamishra ||| arkil patel ||| navin goyal ||| 
2019 ||| a dual-attention hierarchical recurrent neural network for dialogue act classification. ||| ruizhe li ||| chenghua lin ||| matthew collinson ||| xiao li ||| guanyi chen ||| 
2019 ||| effective attention modeling for neural relation extraction. ||| tapas nayak ||| hwee tou ng ||| 
2019 ||| triplenet: triple attention network for multi-turn response selection in retrieval-based chatbots. ||| wentao ma ||| yiming cui ||| nan shao ||| su he ||| weinan zhang ||| ting liu ||| shijin wang ||| guoping hu ||| 
2018 ||| comparing attention-based convolutional and recurrent neural networks: success and limitations in machine reading comprehension. ||| matthias blohm ||| glorianna jagfeld ||| ekta sood ||| xiang yu ||| ngoc thang vu ||| 
2018 ||| pervasive attention: 2d convolutional neural networks for sequence-to-sequence prediction. ||| maha elbayad ||| laurent besacier ||| jakob verbeek ||| 
2020 ||| interpreting attention models with human visual attention in machine reading comprehension. ||| ekta sood ||| simon tannert ||| diego frassinelli ||| andreas bulling ||| ngoc thang vu ||| 
2019 ||| jbnu at mrp 2019: multi-level biaffine attention for semantic dependency parsing. ||| seung-hoon na ||| jinwoon min ||| kwanghyeon park ||| jong-hun shin ||| young-kil kim ||| 
2018 ||| global attention for name tagging. ||| boliang zhang ||| spencer whitehead ||| lifu huang ||| heng ji ||| 
2019 ||| in conclusion not repetition: comprehensive abstractive summarization with diversified attention based on determinantal point processes. ||| lei li ||| wei liu ||| marina litvak ||| natalia vanetik ||| zuying huang ||| 
2019 ||| mrmep: joint extraction of multiple relations and multiple entity pairs based on triplet attention. ||| jiayu chen ||| caixia yuan ||| xiaojie wang ||| ziwei bai ||| 
2021 ||| vqa-mhug: a gaze dataset to study multimodal neural attention in visual question answering. ||| ekta sood ||| fabian k ||| gel ||| florian strohm ||| prajit dhar ||| andreas bulling ||| 
2017 ||| it's not just about attention to details: redefining the talents autistic software developers bring to software development. ||| hala annabi ||| karthika sundaresan ||| annuska zolyomi ||| 
2020 ||| comparison of attention behaviour across user sets through automatic identification of common areas of interest. ||| prithiviraj k. muthumanickam ||| jouni helske ||| aida nordman ||| jimmy johansson ||| matthew cooper ||| 
2021 ||| could you please pay attention?' comparing in-person and mturk responses on a computer code review task. ||| anthony gibson ||| gene m. alarcon ||| michael a. lee ||| izz aldin hamdan ||| 
2020 ||| attention or appreciation? the impact of feedback on online volunteering. ||| jane tan ||| fujie jin ||| alan r. dennis ||| 
2019 ||| exposing attention-decision-learning cycles in engineering project teams through collaborative design experiments. ||| lorena pelegrin ||| bryan r. moser ||| vivek sakhrani ||| 
2017 ||| an exploratory study on consumers' attention towards social media advertising: an electroencephalography approach. ||| hui-chih wang ||| hersen doong ||| 
2021 ||| cagan: text-to-image generation with combined attention generative adversarial networks. ||| henning schulze ||| dogucan yaman ||| alexander waibel ||| 
2021 ||| t6d-direct: transformers for multi-object 6d pose direct regression. ||| arash amini ||| arul selvam periyasamy ||| sven behnke ||| 
2020 ||| pet-guided attention network for segmentation of lung tumors from pet/ct images. ||| varaha karthik pattisapu ||| imant daunhawer ||| thomas j. weikert ||| alexander sauter ||| bram stieltjes ||| julia e. vogt ||| 
2018 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in cnns. ||| timo hackel ||| mikhail usvyatsov ||| silvano galliani ||| jan dirk wegner ||| konrad schindler ||| 
2019 ||| exploiting attention for visual relationship detection. ||| tongxin hu ||| wentong liao ||| michael ying yang ||| bodo rosenhahn ||| 
2021 ||| implicit and explicit attention for zero-shot learning. ||| faisal alamri ||| anjan dutta ||| 
2018 ||| convolve, attend and spell: an attention-based sequence-to-sequence model for handwritten word recognition. ||| lei kang ||| juan ignacio toledo ||| pau riba ||| mauricio villegas ||| alicia forn ||| s ||| mar ||| al rusi ||| ol ||| 
2021 ||| txt: crossmodal end-to-end learning with transformers. ||| jan-martin o. steitz ||| jonas pfeiffer ||| iryna gurevych ||| stefan roth ||| 
2017 ||| reconstructing readerly attention: citational practices and the canon, 1789-2016. ||| mark andrew algee-hewitt ||| david mcclure ||| hannah walser ||| 
2017 ||| the use of the cognitive digital games in school: contributions for the attention. ||| daniela karine ramos ||| bruna anastacio ||| 
2020 ||| not all swear words are used equal: attention over word n-grams for abusive language identification. ||| horacio jes ||| s jarqu ||| n-v ||| squez ||| manuel montes-y-g ||| mez ||| luis villase ||| or pineda ||| 
2020 ||| optimal real-time scheduling of human attention for a human and multi-robot collaboration system. ||| ningshi yao ||| fumin zhang ||| 
2020 ||| improved attention models for memory augmented neural network adaptive controllers. ||| deepan muthirayan ||| scott a. nivison ||| pramod p. khargonekar ||| 
2019 ||| investigating the effect of lexical segmentation in transformer-based models on medical datasets. ||| vincent nguyen ||| sarvnaz karimi ||| zhenchang xing ||| 
2021 ||| study on artificial intelligence approaches for power transformer health index assessment. ||| dhanu rediansyah ||| rahman azis prasojo ||| suwarno ||| 
2018 ||| recurrent attention lstm model for image chinese caption generation. ||| chaoying zhang ||| yaping dai ||| yanyan cheng ||| zhiyang jia ||| kaoru hirota ||| 
2018 ||| rehabilitation support system for attentional deficits based on trail-making test. ||| ryuta tonomura ||| tadamitsu tadamitsu ||| atsushi manji ||| naoyuki kubota ||| takenori obo ||| 
2021 ||| depth inpainting via vision transformer. ||| ilya makarov ||| gleb borisenko ||| 
2020 ||| tga: two-level group attention for assembly state detection. ||| hangfan liu ||| yongzhi su ||| jason r. rambach ||| alain pagani ||| didier stricker ||| 
2018 ||| guiding smombies: augmenting peripheral vision with low-cost glasses to shift the attention of smartphone users. ||| uwe gruenefeld ||| tim claudius stratmann ||| jinki jung ||| hyeopwoo lee ||| jeehye choi ||| abhilasha nanda ||| wilko heuten ||| 
2020 ||| enhancing visitor experience or hindering docent roles: attentional issues in augmented reality supported installations. ||| brandon victor syiem ||| ryan m. kelly ||| eduardo velloso ||| jorge gon ||| alves ||| tilman dingler ||| 
2017 ||| the impact of the frame of reference on attention shifts between augmented reality and real-world environment. ||| andrea schankin ||| daniel reichert ||| matthias berning ||| michael beigl ||| 
2021 ||| two-hand pose estimation from the non-cropped rgb image with self-attention based network. ||| zhoutao sun ||| yong hu ||| xukun shen ||| 
2021 ||| subtle attention guidance for real walking in virtual environments. ||| emanuele nonino ||| joy gisler ||| valentin holzwarth ||| christian hirt ||| andreas m. kunz ||| 
2019 ||| visualization-guided attention direction in dynamic control tasks. ||| jason orlosky ||| chang liu ||| denis kalkofen ||| kiyoshi kiyokawa ||| 
2017 ||| handling instrument transformers and pmu errors for the estimation of line parameters in distribution grids. ||| paolo attilio pegoraro ||| paolo castello ||| carlo muscas ||| kyle brady ||| alexandra von meier ||| 
2018 ||| an efficient digitizer for calibration of instrument transformers. ||| noby george ||| prashanth v. ooka ||| s. gopalakrishna ||| 
2021 ||| instrument transformers for power quality measurements: a review of literature and standards. ||| gabriella crotti ||| giovanni d'avanzo ||| carmine landi ||| palma sara letizia ||| mario luiso ||| fabio mu ||| oz ||| helko e. van den brom ||| 
2018 ||| a simple method for compensating the harmonic distortion introduced by voltage transformers. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2018 ||| impact of capacitor voltage transformers on phasor measurement units dynamic performance. ||| roberto ferrero ||| paolo attilio pegoraro ||| sergio toscani ||| 
2018 ||| calibration of synchronized measurement system: from the instrument transformer to the pmu. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| andrea angioni ||| antonello monti ||| ferdinanda ponci ||| 
2021 ||| compensating the harmonic distortion introduced by instrument transformers: an improved method based on frequency-domain polynomials. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| 
2017 ||| assessment of metrological characteristics of calibration systems for accuracy vs. temperature verification of voltage transformer. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| flavio mauri ||| ivano gentilini ||| 
2018 ||| on the remote calibration of voltage transformers: validation of opportunity. ||| san |||  rens ||| johan rens ||| johann holm ||| gerhard botha ||| jan desmet ||| 
2018 ||| impact of careless current transformer position on current measurement. ||| yukai xiang ||| xu lin ||| xianan chen ||| kun-long chen ||| 
2017 ||| calibration of commercial test sets for non-conventional instrument transformers. ||| enrico mohns ||| alessandro mortara ||| h. cayci ||| ernest houtzager ||| soeren fricke ||| marco agustoni ||| b. ayhan ||| 
2019 ||| expressing uncertainty of voltage transformers: a proposal. ||| marco faifer ||| alessandro ferrero ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2018 ||| compensation of current transformers' non-linearities by means of frequency coupling matrices. ||| adam j. collin ||| antonio delle femine ||| daniele gallo ||| roberto langella ||| mario luiso ||| 
2021 ||| novel calibration systems for the dynamic and steady-state testing of digital instrument transformers. ||| yeying chen ||| gabriella crotti ||| alexander dubowik ||| palma sara letizia ||| enrico mohns ||| mario luiso ||| jorge bruna ||| 
2019 ||| a proposed method for evaluating the frequency response of 22 kv outdoor current transformers for harmonic measurements in renewable energy plant applications. ||| ruan murray ||| jan a. de kock ||| 
2019 ||| setup and characterisation of reference current-to-voltage transformers for wideband current transformers calibration up to 2 ka. ||| yeying chen ||| enrico mohns ||| henrik badura ||| peter r ||| ther ||| mario luiso ||| 
2020 ||| automatic diacritic restoration with transformer model based neural machine translation for east-central european languages. ||| l ||| szl |||  j ||| nos laki ||| zijian gyozo yang ||| 
2022 ||| measuring human auditory attention with eeg. ||| chirag ahuja ||| divyashikha setia ||| 
2021 ||| mflamegaze: mobile-based flame gazing for improving sustained attention. ||| divyashikha sethia ||| anil singh parihar ||| aheli ghosh ||| tanish grover ||| deep diwakar ||| shivam kumar ||| utkarsh tyagi ||| 
2022 ||| short term effect of physical exercise on selective attention using eeg and stroop task. ||| divyashikha sethia ||| taksh kamlesh ||| mrigank singh ||| sonia baloni ray ||| 
2017 ||| an integrated computational framework for attention, reinforcement learning, and working memory. ||| andrea stocco ||| 
2021 ||| end-to-end russian speech recognition models with multi-head attention. ||| irina s. kipyatkova ||| 
2020 ||| audio adversarial examples for robust hybrid ctc/attention speech recognition. ||| ludwig k ||| rzinger ||| edgar ricardo chavez rosas ||| lujun li ||| tobias watzel ||| gerhard rigoll ||| 
2019 ||| exploring hybrid ctc/attention end-to-end speech recognition with gaussian processes. ||| ludwig k ||| rzinger ||| tobias watzel ||| lujun li ||| robert baumgartner ||| gerhard rigoll ||| 
2021 ||| an equal data setting for attention-based encoder-decoder and hmm/dnn models: a case study in finnish asr. ||| aku rouhe ||| astrid van camp ||| mittul singh ||| hugo van hamme ||| mikko kurimo ||| 
2021 ||| regularized forward-backward decoder for attention models. ||| tobias watzel ||| ludwig k ||| rzinger ||| lujun li ||| gerhard rigoll ||| 
2020 ||| hate speech detection using transformer ensembles on the hasoc dataset. ||| pedro alonso ||| rajkumar saini ||| gy ||| rgy kov ||| cs ||| 
2021 ||| induced local attention for transformer models in speech recognition. ||| tobias watzel ||| ludwig k ||| rzinger ||| lujun li ||| gerhard rigoll ||| 
2020 ||| experimenting with attention mechanisms in joint ctc-attention models for russian speech recognition. ||| irina s. kipyatkova ||| nikita markovnikov ||| 
2019 ||| investigating joint ctc-attention models for end-to-end russian speech recognition. ||| nikita markovnikov ||| irina s. kipyatkova ||| 
2020 ||| synchronized forward-backward transformer for end-to-end speech recognition. ||| tobias watzel ||| ludwig k ||| rzinger ||| lujun li ||| gerhard rigoll ||| 
2021 ||| human and transformer-based prosodic phrasing in two speech genres. ||| jan vol ||| n ||| mark ||| ta rez ||| ckov ||| jindrich matousek ||| 
2020 ||| testing pre-trained transformer models for lithuanian news clustering. ||| lukas stankevicius ||| mantas lukosevicius ||| 
2020 ||| pert: payload encoding representation from transformer for encrypted traffic classification. ||| hong ye he ||| zhi guo yang ||| xiang ning chen ||| 
2021 ||| clinically guided trainable soft attention for early detection of oral cancer. ||| roshan alex welikala ||| paolo remagnino ||| jian han lim ||| chee seng chan ||| senthilmani rajendran ||| thomas george kallarakkal ||| rosnah binti zain ||| ruwan duminda jayasinghe ||| jyotsna rimal ||| alexander ross kerr ||| rahmi amtha ||| karthikeya patil ||| wanninayake mudiyanselage tilakaratne ||| sok ching cheong ||| sarah ann barman ||| 
2017 ||| attention-based two-phase model for video action detection. ||| xiongtao chen ||| wenmin wang ||| weimian li ||| jinzhuo wang ||| 
2019 ||| patent citation dynamics modeling via multi-attention recurrent networks. ||| taoran ji ||| zhiqian chen ||| nathan self ||| kaiqun fu ||| chang-tien lu ||| naren ramakrishnan ||| 
2019 ||| dual visual attention network for visual dialog. ||| dan guo ||| hui wang ||| meng wang ||| 
2019 ||| open-ended long-form video question answering via hierarchical convolutional self-attention networks. ||| zhu zhang ||| zhou zhao ||| zhijie lin ||| jingkuan song ||| xiaofei he ||| 
2020 ||| multi-scale group transformer for long sequence modeling in speech separation. ||| yucheng zhao ||| chong luo ||| zheng-jun zha ||| wenjun zeng ||| 
2021 ||| adaptive edge attention for graph matching with outliers. ||| jingwei qu ||| haibin ling ||| chenrui zhang ||| xiaoqing lyu ||| zhi tang ||| 
2020 ||| compressed self-attention for deep metric learning with low-rank approximation. ||| ziye chen ||| mingming gong ||| lingjuan ge ||| bo du ||| 
2018 ||| multiway attention networks for modeling sentence pairs. ||| chuanqi tan ||| furu wei ||| wenhui wang ||| weifeng lv ||| ming zhou ||| 
2021 ||| guided attention network for concept extraction. ||| songtao fang ||| zhenya huang ||| ming he ||| shiwei tong ||| xiaoqing huang ||| ye liu ||| jie huang ||| qi liu ||| 
2017 ||| hashtag recommendation for multimodal microblog using co-attention network. ||| qi zhang ||| jiawen wang ||| haoran huang ||| xuanjing huang ||| yeyun gong ||| 
2020 ||| cross-interaction hierarchical attention networks for urban anomaly prediction. ||| chao huang ||| chuxu zhang ||| peng dai ||| liefeng bo ||| 
2017 ||| learning sentence representation with guidance of human attention. ||| shaonan wang ||| jiajun zhang ||| chengqing zong ||| 
2018 ||| a^3ncf: an adaptive aspect attention model for rating prediction. ||| zhiyong cheng ||| ying ding ||| xiangnan he ||| lei zhu ||| xuemeng song ||| mohan s. kankanhalli ||| 
2019 ||| graph contextualized self-attention network for session-based recommendation. ||| chengfeng xu ||| pengpeng zhao ||| yanchi liu ||| victor s. sheng ||| jiajie xu ||| fuzhen zhuang ||| junhua fang ||| xiaofang zhou ||| 
2018 ||| code completion with neural attention and pointer networks. ||| jian li ||| yue wang ||| michael r. lyu ||| irwin king ||| 
2020 ||| neighbor combinatorial attention for critical structure mining. ||| tanli zuo ||| yukun qiu ||| weishi zheng ||| 
2018 ||| memory attention networks for skeleton-based action recognition. ||| chunyu xie ||| ce li ||| baochang zhang ||| chen chen ||| jungong han ||| jianzhuang liu ||| 
2021 ||| deep neural network loses attention to adversarial images. ||| shashank kotyan ||| danilo vasconcellos vargas ||| 
2019 ||| pre-training of graph augmented transformers for medication recommendation. ||| junyuan shang ||| tengfei ma ||| cao xiao ||| jimeng sun ||| 
2020 ||| evidence-aware hierarchical interactive attention networks for explainable claim verification. ||| lianwei wu ||| yuan rao ||| xiong yang ||| wanzhen wang ||| ambreen nazir ||| 
2020 ||| hierarchical multi-scale gaussian transformer for stock movement prediction. ||| qianggang ding ||| sifan wu ||| hao sun ||| jiadong guo ||| jian guo ||| 
2019 ||| attain: attention-based time-aware lstm networks for disease progression modeling. ||| yuan zhang ||| xi yang ||| julie s. ivy ||| min chi ||| 
2019 ||| hierarchical diffusion attention network. ||| zhitao wang ||| wenjie li ||| 
2020 ||| attention-based multi-level feature fusion for named entity recognition. ||| zhiwei yang ||| hechang chen ||| jiawei zhang ||| jing ma ||| yi chang ||| 
2020 ||| arbitrary talking face generation via attentional audio-visual coherence learning. ||| hao zhu ||| huaibo huang ||| yi li ||| aihua zheng ||| ran he ||| 
2019 ||| an actor-critic-attention mechanism for deep reinforcement learning in multi-view environments. ||| elaheh barati ||| xuewen chen ||| 
2021 ||| information bottleneck approach to spatial attention learning. ||| qiuxia lai ||| yu li ||| ailing zeng ||| minhao liu ||| hanqiu sun ||| qiang xu ||| 
2018 ||| aspect term extraction with history attention and selective transformation. ||| xin li ||| lidong bing ||| piji li ||| wai lam ||| zhimou yang ||| 
2018 ||| multi-turn video question answering via multi-stream hierarchical attention context network. ||| zhou zhao ||| xinghua jiang ||| deng cai ||| jun xiao ||| xiaofei he ||| shiliang pu ||| 
2021 ||| medical image segmentation using squeeze-and-expansion transformers. ||| shaohua li ||| xiuchao sui ||| xiangde luo ||| xinxing xu ||| yong liu ||| rick siow mong goh ||| 
2020 ||| attan: attention adversarial networks for 3d point cloud semantic segmentation. ||| gege zhang ||| qinghua ma ||| licheng jiao ||| fang liu ||| qigong sun ||| 
2019 ||| adaptive joint attention with reinforcement training for convolutional image caption. ||| ruoyu chen ||| zhongnian li ||| daoqiang zhang ||| 
2020 ||| improving attention mechanism in graph neural networks via cardinality preservation. ||| shuo zhang ||| lei xie ||| 
2020 ||| hype-han: hyperbolic hierarchical attention network for semantic embedding. ||| chengkun zhang ||| junbin gao ||| 
2017 ||| an attention-based regression model for grounding textual phrases in images. ||| ko endo ||| masaki aono ||| eric nichols ||| kotaro funakoshi ||| 
2021 ||| fine-grained air quality inference via multi-channel attention model. ||| qilong han ||| dan lu ||| rui chen ||| 
2018 ||| attention-fused deep matching network for natural language inference. ||| chaoqun duan ||| lei cui ||| xinchi chen ||| furu wei ||| conghui zhu ||| tiejun zhao ||| 
2020 ||| a new attention mechanism to classify multivariate time series. ||| yifan hao ||| huiping cao ||| 
2020 ||| attention as relation: learning supervised multi-head self-attention for relation extraction. ||| jie liu ||| shaowei chen ||| bingquan wang ||| jiaxin zhang ||| na li ||| tong xu ||| 
2021 ||| gasp: gated attention for saliency prediction. ||| fares abawi ||| tom weber ||| stefan wermter ||| 
2021 ||| multi-hop attention graph neural networks. ||| guangtao wang ||| rex ying ||| jing huang ||| jure leskovec ||| 
2021 ||| attention-based pyramid dilated lattice network for blind image denoising. ||| mohammad nikzad ||| yongsheng gao ||| jun zhou ||| 
2021 ||| laughing heads: can transformers detect what makes a sentence funny? ||| maxime peyrard ||| beatriz borges ||| kristina gligoric ||| robert west ||| 
2017 ||| video question answering via hierarchical spatio-temporal attention networks. ||| zhou zhao ||| qifan yang ||| deng cai ||| xiaofei he ||| yueting zhuang ||| 
2019 ||| clvsa: a convolutional lstm based variational sequence-to-sequence model with attention for predicting trends of financial markets. ||| jia wang ||| tong sun ||| benyuan liu ||| yu cao ||| hongwei zhu ||| 
2018 ||| neural machine translation with key-value memory-augmented attention. ||| fandong meng ||| zhaopeng tu ||| yong cheng ||| haiyang wu ||| junjie zhai ||| yuekui yang ||| di wang ||| 
2020 ||| self-supervised gait encoding with locality-aware attention for person re-identification. ||| haocong rao ||| siqi wang ||| xiping hu ||| mingkui tan ||| huang da ||| jun cheng ||| bin hu ||| 
2020 ||| a relation-specific attention network for joint entity and relation extraction. ||| yue yuan ||| xiaofei zhou ||| shirui pan ||| qiannan zhu ||| zeliang song ||| li guo ||| 
2019 ||| matching user with item set: collaborative bundle recommendation with deep attention network. ||| liang chen ||| yang liu ||| xiangnan he ||| lianli gao ||| zibin zheng ||| 
2018 ||| sequential recommender system based on hierarchical attention networks. ||| haochao ying ||| fuzhen zhuang ||| fuzheng zhang ||| yanchi liu ||| guandong xu ||| xing xie ||| hui xiong ||| jian wu ||| 
2018 ||| cross-media multi-level alignment with relation attention network. ||| jinwei qi ||| yuxin peng ||| yuxin yuan ||| 
2020 ||| gesturedet: real-time student gesture analysis with multi-dimensional attention-based detector. ||| rui zheng ||| fei jiang ||| ruimin shen ||| 
2019 ||| attnsense: multi-level attention mechanism for multimodal human activity recognition. ||| haojie ma ||| wenzhong li ||| xiao zhang ||| songcheng gao ||| sanglu lu ||| 
2020 ||| transformers as soft reasoners over language. ||| peter clark ||| oyvind tafjord ||| kyle richardson ||| 
2019 ||| t-cvae: transformer-based conditioned variational autoencoder for story completion. ||| tianming wang ||| xiaojun wan ||| 
2019 ||| semi-supervised user profiling with heterogeneous graph attention networks. ||| weijian chen ||| yulong gu ||| zhaochun ren ||| xiangnan he ||| hongtao xie ||| tong guo ||| dawei yin ||| yongdong zhang ||| 
2020 ||| feature augmented memory with global attention network for videoqa. ||| jiayin cai ||| chun yuan ||| cheng shi ||| lei li ||| yangyang cheng ||| ying shan ||| 
2018 ||| beyond polarity: interpretable financial sentiment analysis with hierarchical query-driven attention. ||| ling luo ||| xiang ao ||| feiyang pan ||| jin wang ||| tong zhao ||| ningzi yu ||| qing he ||| 
2017 ||| stance classification with target-specific neural attention. ||| jiachen du ||| ruifeng xu ||| yulan he ||| lin gui ||| 
2019 ||| locate-then-detect: real-time web attack detection via attention-based deep neural networks. ||| tianlong liu ||| yu qi ||| liang shi ||| jianan yan ||| 
2018 ||| scanpath prediction for visual attention using ior-roi lstm. ||| zhenzhong chen ||| wanjie sun ||| 
2019 ||| 3dviewgraph: learning global features for 3d shapes from a graph of unordered views with attention. ||| zhizhong han ||| xiyang wang ||| chi-man vong ||| yu-shen liu ||| matthias zwicker ||| c. l. philip chen ||| 
2019 ||| beyond word attention: using segment attention in neural relation extraction. ||| bowen yu ||| zhenyu zhang ||| tingwen liu ||| bin wang ||| sujian li ||| quangang li ||| 
2018 ||| hermitian co-attention networks for text matching in asymmetrical domains. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| 
2018 ||| learning to recognize transient sound events using attentional supervision. ||| szu-yu chou ||| jyh-shing roger jang ||| yi-hsuan yang ||| 
2019 ||| risk assessment for networked-guarantee loans using high-order graph attention representation. ||| dawei cheng ||| yi tu ||| zhen-wei ma ||| zhibin niu ||| liqing zhang ||| 
2020 ||| infobox-to-text generation with tree-like planning based attention network. ||| yang bai ||| ziran li ||| ning ding ||| ying shen ||| hai-tao zheng ||| 
2019 ||| addgraph: anomaly detection in dynamic graph using attention-based temporal gcn. ||| li zheng ||| zhenpeng li ||| jian li ||| zhao li ||| jun gao ||| 
2021 ||| segmenting transparent objects in the wild with transformer. ||| enze xie ||| wenjia wang ||| wenhai wang ||| peize sun ||| hang xu ||| ding liang ||| ping luo ||| 
2020 ||| pay attention to devils: a photometric stereo network for better details. ||| yakun ju ||| kin-man lam ||| yang chen ||| lin qi ||| junyu dong ||| 
2019 ||| densely connected attention flow for visual question answering. ||| fei liu ||| jing liu ||| zhiwei fang ||| richang hong ||| hanqing lu ||| 
2020 ||| hierarchical attention based spatial-temporal graph-to-sequence learning for grounded video description. ||| kai shen ||| lingfei wu ||| fangli xu ||| siliang tang ||| jun xiao ||| yueting zhuang ||| 
2018 ||| multi-modality sensor data classification with selective attention. ||| xiang zhang ||| lina yao ||| chaoran huang ||| sen wang ||| mingkui tan ||| guodong long ||| can wang ||| 
2021 ||| local representation is not enough: soft point-wise transformer for descriptor and detector of local features. ||| zihao wang ||| xueyi li ||| zhen li ||| 
2018 ||| reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| sen wang ||| chengqi zhang ||| 
2019 ||| a deep bi-directional attention network for human motion recovery. ||| qiongjie cui ||| huaijiang sun ||| yupeng li ||| yue kong ||| 
2017 ||| modeling spatial auditory attention: handling equiprobable attended locations. ||| jaelle scheuerman ||| kristen brent venable ||| maxwell t. anderson ||| edward j. golob ||| 
2021 ||| a sketch-transformer network for face photo-sketch synthesis. ||| mingrui zhu ||| changcheng liang ||| nannan wang ||| xiaoyu wang ||| zhifeng li ||| xinbo gao ||| 
2020 ||| sbat: video captioning with sparse boundary-aware transformer. ||| tao jin ||| siyu huang ||| ming chen ||| yingming li ||| zhongfei zhang ||| 
2020 ||| learning regional attention convolutional neural network for motion intention recognition based on eeg data. ||| zhijie fang ||| weiqun wang ||| shixin ren ||| jiaxing wang ||| weiguo shi ||| xu liang ||| chen-chen fan ||| zeng-guang hou ||| 
2018 ||| commonsense knowledge aware conversation generation with graph attention. ||| hao zhou ||| tom young ||| minlie huang ||| haizhou zhao ||| jingfang xu ||| xiaoyan zhu ||| 
2019 ||| dmran: a hierarchical fine-grained attention-based network for recommendation. ||| huizhao wang ||| guanfeng liu ||| an liu ||| zhixu li ||| kai zheng ||| 
2019 ||| on retrospecting human dynamics with attention. ||| minjing dong ||| chang xu ||| 
2019 ||| detecting robust co-saliency with recurrent co-attention neural network. ||| bo li ||| zhengxing sun ||| lv tang ||| yunhan sun ||| jinlong shi ||| 
2021 ||| kdexplainer: a task-oriented attention model for explaining knowledge distillation. ||| mengqi xue ||| jie song ||| xinchao wang ||| ying chen ||| xingen wang ||| mingli song ||| 
2019 ||| spagan: shortest path graph attention network. ||| yiding yang ||| xinchao wang ||| mingli song ||| junsong yuan ||| dacheng tao ||| 
2019 ||| position focused attention network for image-text matching. ||| yaxiong wang ||| hao yang ||| xueming qian ||| lin ma ||| jing lu ||| biao li ||| xin fan ||| 
2018 ||| listen, think and listen again: capturing top-down auditory attention for speaker-independent speech separation. ||| jing shi ||| jiaming xu ||| guangcan liu ||| bo xu ||| 
2019 ||| earlier attention? aspect-aware lstm for aspect-based sentiment analysis. ||| bowen xing ||| lejian liao ||| dandan song ||| jingang wang ||| fuzheng zhang ||| zhongyuan wang ||| heyan huang ||| 
2018 ||| translating embeddings for knowledge graph completion with relation attention mechanism. ||| wei qian ||| cong fu ||| yu zhu ||| deng cai ||| xiaofei he ||| 
2019 ||| mina: multilevel knowledge-guided attention for modeling electrocardiography signals. ||| shenda hong ||| cao xiao ||| tengfei ma ||| hongyan li ||| jimeng sun ||| 
2020 ||| weakly supervised few-shot object segmentation using co-attention with visual and semantic embeddings. ||| mennatullah siam ||| naren doraiswamy ||| boris n. oreshkin ||| hengshuai yao ||| martin j ||| gersand ||| 
2020 ||| a graphical and attentional framework for dual-target cross-domain recommendation. ||| feng zhu ||| yan wang ||| chaochao chen ||| guanfeng liu ||| xiaolin zheng ||| 
2017 ||| interactive attention networks for aspect-level sentiment classification. ||| dehong ma ||| sujian li ||| xiaodong zhang ||| houfeng wang ||| 
2017 ||| mam-rnn: multi-level attention model based rnn for video captioning. ||| xuelong li ||| bin zhao ||| xiaoqiang lu ||| 
2020 ||| towards fully 8-bit integer inference for the transformer model. ||| ye lin ||| yanyang li ||| tengbo liu ||| tong xiao ||| tongran liu ||| jingbo zhu ||| 
2018 ||| attentional image retweet modeling via multi-faceted ranking network learning. ||| zhou zhao ||| lingtao meng ||| jun xiao ||| min yang ||| fei wu ||| deng cai ||| xiaofei he ||| yueting zhuang ||| 
2018 ||| a brand-level ranking system with the customized attention-gru model. ||| yu zhu ||| junxiong zhu ||| jie hou ||| yongliang li ||| beidou wang ||| ziyu guan ||| deng cai ||| 
2017 ||| hierarchical lstm with adjusted temporal attention for video captioning. ||| jingkuan song ||| lianli gao ||| zhao guo ||| wu liu ||| dongxiang zhang ||| heng tao shen ||| 
2021 ||| state-aware value function approximation with attention mechanism for restless multi-armed bandits. ||| shuang wu ||| jingyu zhao ||| guangjian tian ||| jun wang ||| 
2018 ||| medical concept embedding with time-aware attention. ||| xiangrui cai ||| jinyang gao ||| kee yuan ngiam ||| beng chin ooi ||| ying zhang ||| xiaojie yuan ||| 
2019 ||| multi-domain sentiment classification based on domain-aware embedding and attention. ||| yitao cai ||| xiaojun wan ||| 
2018 ||| show, observe and tell: attribute-driven attention model for image captioning. ||| hui chen ||| guiguang ding ||| zijia lin ||| sicheng zhao ||| jungong han ||| 
2020 ||| self-attentional credit assignment for transfer in reinforcement learning. ||| johan ferret ||| rapha ||| l marinier ||| matthieu geist ||| olivier pietquin ||| 
2019 ||| musical: multi-scale image contextual attention learning for inpainting. ||| ning wang ||| jingyuan li ||| lefei zhang ||| bo du ||| 
2019 ||| dyat nets: dynamic attention networks for state forecasting in cyber-physical systems. ||| nikhil muralidhar ||| sathappan muthiah ||| naren ramakrishnan ||| 
2021 ||| proposal-free one-stage referring expression via grid-word cross-attention. ||| wei suo ||| mengyang sun ||| peng wang ||| qi wu ||| 
2017 ||| predicting human interaction via relative attention model. ||| yichao yan ||| bingbing ni ||| xiaokang yang ||| 
2020 ||| barnet: bilinear attention network with adaptive receptive fields for surgical instrument segmentation. ||| zhen-liang ni ||| gui-bin bian ||| guan'an wang ||| xiao-hu zhou ||| zeng-guang hou ||| xiao-liang xie ||| zhen li ||| yu-han wang ||| 
2019 ||| attributed graph clustering: a deep attentional embedding approach. ||| chun wang ||| shirui pan ||| ruiqi hu ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2021 ||| posegtac: graph transformer encoder-decoder with atrous convolution for 3d human pose estimation. ||| yiran zhu ||| xing xu ||| fumin shen ||| yanli ji ||| lianli gao ||| heng tao shen ||| 
2020 ||| relation-aware transformer for portfolio policy learning. ||| ke xu ||| yifan zhang ||| deheng ye ||| peilin zhao ||| mingkui tan ||| 
2017 ||| cascade dynamics modeling with attention-based recurrent neural network. ||| yongqing wang ||| huawei shen ||| shenghua liu ||| jinhua gao ||| xueqi cheng ||| 
2018 ||| feature enhancement in attention for visual question answering. ||| yuetan lin ||| zhangyang pang ||| donghui wang ||| yueting zhuang ||| 
2019 ||| emoji-aware attention-based bi-directional gru network model for chinese sentiment analysis. ||| da li ||| rafal rzepka ||| michal ptaszynski ||| kenji araki ||| 
2019 ||| dense transformer networks for brain electron microscopy image segmentation. ||| jun li ||| yongjun chen ||| lei cai ||| ian davidson ||| shuiwang ji ||| 
2018 ||| aspect sentiment classification with both word-level and clause-level attention networks. ||| jingjing wang ||| jie li ||| shoushan li ||| yangyang kang ||| min zhang ||| luo si ||| guodong zhou ||| 
2019 ||| hierarchical inter-attention network for document classification with multi-task learning. ||| bing tian ||| yong zhang ||| jin wang ||| chunxiao xing ||| 
2020 ||| neural abstractive summarization with structural attention. ||| tanya chowdhury ||| sachin kumar ||| tanmoy chakraborty ||| 
2019 ||| rthn: a rnn-transformer hierarchical network for emotion cause extraction. ||| rui xia ||| mengran zhang ||| zixiang ding ||| 
2019 ||| multi-agent attentional activity recognition. ||| kaixuan chen ||| lina yao ||| dalin zhang ||| bin guo ||| zhiwen yu ||| 
2020 ||| a label attention model for icd coding from clinical text. ||| thanh vu ||| dat quoc nguyen ||| anthony nguyen ||| 
2018 ||| geoman: multi-level attention networks for geo-sensory time series prediction. ||| yuxuan liang ||| songyu ke ||| junbo zhang ||| xiuwen yi ||| yu zheng ||| 
2017 ||| attentional factorization machines: learning the weight of feature interactions via attention networks. ||| jun xiao ||| hao ye ||| xiangnan he ||| hanwang zhang ||| fei wu ||| tat-seng chua ||| 
2017 ||| inferring human attention by learning latent intentions. ||| ping wei ||| dan xie ||| nanning zheng ||| song-chun zhu ||| 
2020 ||| a attention network. ||| zhen ye ||| yu qin ||| wei xu ||| 
2019 ||| feature-level deeper self-attention network for sequential recommendation. ||| tingting zhang ||| pengpeng zhao ||| yanchi liu ||| victor s. sheng ||| jiajie xu ||| deqing wang ||| guanfeng liu ||| xiaofang zhou ||| 
2021 ||| learning attributed graph representation with communicative message passing transformer. ||| jianwen chen ||| shuangjia zheng ||| ying song ||| jiahua rao ||| yuedong yang ||| 
2018 ||| multi-modal sentence summarization with modality attention and image filtering. ||| haoran li ||| junnan zhu ||| tianshang liu ||| jiajun zhang ||| chengqing zong ||| 
2019 ||| vulsniper: focus your attention to shoot fine-grained vulnerabilities. ||| xu duan ||| jingzheng wu ||| shouling ji ||| zhiqing rui ||| tianyue luo ||| mutian yang ||| yanjun wu ||| 
2020 ||| multi-attention meta learning for few-shot fine-grained image recognition. ||| yaohui zhu ||| chenlong liu ||| shuqiang jiang ||| 
2020 ||| a structured latent variable recurrent network with stochastic attention for generating weibo comments. ||| shijie yang ||| liang li ||| shuhui wang ||| weigang zhang ||| qingming huang ||| qi tian ||| 
2017 ||| link prediction via ranking metric dual-level attention network learning. ||| zhou zhao ||| ben gao ||| vincent w. zheng ||| deng cai ||| xiaofei he ||| yueting zhuang ||| 
2018 ||| from pixels to objects: cubic visual attention for visual question answering. ||| jingkuan song ||| pengpeng zeng ||| lianli gao ||| heng tao shen ||| 
2017 ||| completely heterogeneous transfer learning with attention - what and what not to transfer. ||| seungwhan moon ||| jaime g. carbonell ||| 
2019 ||| knowledge-enhanced hierarchical attention for community question answering with multi-task and adaptive learning. ||| min yang ||| lei chen ||| xiaojun chen ||| qingyao wu ||| wei zhou ||| ying shen ||| 
2019 ||| multi-level visual-semantic alignments with relation-wise dual attention network for image and text matching. ||| zhibin hu ||| yongsheng luo ||| jiong lin ||| yan yan ||| jian chen ||| 
2018 ||| co-attention cnns for unsupervised object co-segmentation. ||| kuang-jui hsu ||| yen-yu lin ||| yung-yu chuang ||| 
2018 ||| same representation, different attentions: shareable sentence representation learning from multiple tasks. ||| renjie zheng ||| junkun chen ||| xipeng qiu ||| 
2021 ||| leveraging human attention in novel object captioning. ||| xianyu chen ||| ming jiang ||| qi zhao ||| 
2019 ||| sharing attention weights for fast transformer. ||| tong xiao ||| yinqiao li ||| jingbo zhu ||| zhengtao yu ||| tongran liu ||| 
2017 ||| a dual-stage attention-based recurrent neural network for time series prediction. ||| yao qin ||| dongjin song ||| haifeng chen ||| wei cheng ||| guofei jiang ||| garrison w. cottrell ||| 
2021 ||| dynamic lane traffic signal control with group attention and multi-timescale reinforcement learning. ||| qize jiang ||| jingze li ||| weiwei sun ||| baihua zheng ||| 
2019 ||| applying attention mechanism and deep neural network for medical object segmentation and classification in x-ray fluoroscopy images. ||| yong zhang ||| jun yan ||| haitao huang ||| christopher yencha ||| 
2020 ||| action-guided attention mining and relation reasoning network for human-object interaction detection. ||| xue lin ||| qi zou ||| xixia xu ||| 
2019 ||| neighborhood-aware attentional representation for multilingual knowledge graphs. ||| qiannan zhu ||| xiaofei zhou ||| jia wu ||| jianlong tan ||| li guo ||| 
2021 ||| bi-isca: bidirectional inter-sentence contextual attention mechanism for detecting sarcasm in user generated noisy short text. ||| prakamya mishra ||| saroj kaushik ||| kuntal dey ||| 
2020 ||| collaborative self-attention network for session-based recommendation. ||| anjing luo ||| pengpeng zhao ||| yanchi liu ||| fuzhen zhuang ||| deqing wang ||| jiajie xu ||| junhua fang ||| victor s. sheng ||| 
2020 ||| an attention-based model for conversion rate prediction with delayed feedback via post-click calibration. ||| yumin su ||| liang zhang ||| quanyu dai ||| bo zhang ||| jinyao yan ||| dan wang ||| yongjun bao ||| sulong xu ||| yang he ||| weipeng yan ||| 
2018 ||| densely connected cnn with multi-scale feature attention for text classification. ||| shiyao wang ||| minlie huang ||| zhidong deng ||| 
2019 ||| mnn: multimodal attentional neural networks for diagnosis prediction. ||| zhi qiao ||| xian wu ||| shen ge ||| wei fan ||| 
2021 ||| multimodal transformer networks for pedestrian trajectory prediction. ||| ziyi yin ||| ruijin liu ||| zhiliang xiong ||| zejian yuan ||| 
2019 ||| bpam: recommendation based on bp neural network with attention mechanism. ||| wu-dong xi ||| ling huang ||| chang-dong wang ||| yin-yu zheng ||| jianhuang lai ||| 
2018 ||| get the point of my utterance! learning towards effective responses with multi-head attention mechanism. ||| chongyang tao ||| shen gao ||| mingyue shang ||| wei wu ||| dongyan zhao ||| rui yan ||| 
2020 ||| internal and contextual attention network for cold-start multi-channel matching in recommendation. ||| ruobing xie ||| zhijie qiu ||| jun rao ||| yi liu ||| bo zhang ||| leyu lin ||| 
2021 ||| gaen: graph attention evolving networks. ||| min shi ||| yu huang ||| xingquan zhu ||| yufei tang ||| yuan zhuang ||| jianxun liu ||| 
2017 ||| learning to read irregular text with attention mechanisms. ||| xiao yang ||| dafang he ||| zihan zhou ||| daniel kifer ||| c. lee giles ||| 
2019 ||| predicting the visual focus of attention in multi-person discussion videos. ||| chongyang bai ||| srijan kumar ||| jure leskovec ||| miriam metzger ||| jay f. nunamaker jr. ||| v. s. subrahmanian ||| 
2020 ||| deep interleaved network for single image super-resolution with asymmetric co-attention. ||| feng li ||| runming cong ||| huihui bai ||| yifan he ||| 
2020 ||| deep specification mining with attention. ||| zhi cao ||| nan zhang ||| 
2017 ||| testing the electrical insulation system of power transformer based on mesuring factor of dielectric losses. ||| vladimir yalentic ||| sanja grzinic ||| dean dobrec ||| 
2019 ||| application of the protective current transformers in measurement systems for energy management purposes. ||| dragana naumovic-vukovic ||| radoslav antic ||| vladimir vujicic ||| dragan v. pejic ||| 
2019 ||| external attention lstm models for cognitive load classification from speech. ||| ascensi ||| n gallardo-antol ||| n ||| juan manuel montero ||| 
2018 ||| restoring punctuation and capitalization using transformer models. ||| andris varavs ||| askars salimbajevs ||| 
2017 ||| attentional parallel rnns for generating punctuation in transcribed speech. ||| alp  ||| ktem ||| mireia farr ||| s ||| leo wanner ||| 
2021 ||| comparison of czech transformers on text classification tasks. ||| jan lehecka ||| jan svec ||| 
2019 ||| use of intelligent agent through low-cost brain-computer interface to analyze attention and meditation levels by gender. ||| bladimir serna ||| rosario baltazar ||| pedro cruz-parada ||| jorge meza ||| juan jos |||  manriquez santos ||| v ||| ctor zamudio ||| 
2020 ||| the wonderful wizard of loc: paying attention to the man behind the curtain of lines-of-code metrics. ||| kalev alpernas ||| yotam m. y. feldman ||| hila peleg ||| 
2021 ||| recurrent attention unit: a simple and effective method for traffic prediction. ||| zebing wei ||| zhishuai li ||| chunxiang wang ||| yuanyuan chen ||| qinghai miao ||| yisheng lv ||| fei-yue wang ||| 
2021 ||| ra-gat: repulsion and attraction graph attention for trajectory prediction. ||| zhezhang ding ||| ziyang yao ||| huijing zhao ||| 
2021 ||| tqam: temporal attention for cycle-wise queue length estimation using high-resolution loop detector data. ||| rahul sengupta ||| yashaswi karnati ||| anand rangarajan ||| sanjay ranka ||| 
2019 ||| vehicle speed prediction with rnn and attention model under multiple scenarios. ||| chi-sheng shih ||| pao-wei huang ||| e.-ton yen ||| pei-kuei tsung ||| 
2020 ||| graph attention convolutional network: spatiotemporal modeling for urban traffic prediction. ||| qingyu song ||| ruibo ming ||| jianming hu ||| haoyi niu ||| mingyang gao ||| 
2020 ||| detecting lane and road markings at a distance with perspective transformer layers. ||| zhuoping yu ||| xiaozhou ren ||| yuyao huang ||| wei tian ||| junqiao zhao ||| 
2019 ||| predicting time-series trajectories of human driven vehicles, using an unsupervised attention-based recurrent neural network. ||| ross walker ||| 
2021 ||| learning to drive at unsignalized intersections using attention-based deep reinforcement learning. ||| hyunki seong ||| chanyoung jung ||| seungwook lee ||| david hyunchul shim ||| 
2019 ||| a benchmark dataset and multi-scale attention network for semantic traffic light detection. ||| yang feng ||| deqian kong ||| ping wei ||| hongbin sun ||| nanning zheng ||| 
2021 ||| effects of cooperative vehicle infrastructure system on driver's attention with different personal attribute. ||| xuewei li ||| xiaohua zhao ||| zhenlong li ||| jian rong ||| 
2021 ||| dsa-gan: driving style attention generative adversarial network for vehicle trajectory prediction. ||| seungwon choi ||| nahyun kweon ||| chanuk yang ||| dongchan kim ||| hyukju shon ||| jaewoong choi ||| kunsoo huh ||| 
2021 ||| densepass: dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange. ||| chaoxiang ma ||| jiaming zhang ||| kailun yang ||| alina roitberg ||| rainer stiefelhagen ||| 
2021 ||| fusionpainting: multimodal fusion with adaptive attention for 3d object detection. ||| shaoqing xu ||| dingfu zhou ||| jin fang ||| junbo yin ||| bin zhou ||| liangjun zhang ||| 
2019 ||| neural-attention-based deep learning architectures for modeling traffic dynamics on lane graphs. ||| matthew a. wright ||| simon f. g. ehlers ||| roberto horowitz ||| 
2021 ||| attention-based vehicle self-localization with hd feature maps. ||| nico engel ||| vasileios belagiannis ||| klaus dietmayer ||| 
2019 ||| an efficient model for driving focus of attention prediction using deep learning. ||| minghao ning ||| chao lu ||| jianwei gong ||| 
2021 ||| rpfa-net: a 4d radar pillar feature attention network for 3d object detection. ||| baowei xu ||| xinyu zhang ||| li wang ||| xiaomei hu ||| zhiwei li ||| shuyue pan ||| jun li ||| yongqiang deng ||| 
2021 ||| spatio-temporal multi-task learning transformer for joint moving object detection and segmentation. ||| eslam mohamed ||| ahmad el sallab ||| 
2020 ||| top-down, spatio-temporal attentional guidance for on-road object detection. ||| jayani withanawasam ||| ehsan javanmardi ||| shunsuke kamijo ||| 
2021 ||| image transformer for explainable autonomous driving system. ||| jiqian dong ||| sikai chen ||| shuya zong ||| tiantian chen ||| samuel labi ||| 
2019 ||| driver attention level estimation using driver model identification. ||| morimichi nishigaki ||| tetsuro shirakata ||| 
2019 ||| detection of driver's inattention: a real-time deep learning approach. ||| s. tryhub ||| giovanni luca masala ||| 
2019 ||| safety criteria analysis for negotiating blind corners in personal mobility vehicles based on driver's attention simulation on 3d map. ||| naoki akai ||| takatsugu hirayama ||| luis yoichi morales ||| hiroshi murase ||| 
2020 ||| attention based graph bi-lstm networks for traffic forecasting. ||| han zhao ||| huan yang ||| yu wang ||| danwei wang ||| rong su ||| 
2021 ||| an attention-based spatial-temporal traffic flow prediction method with pattern similarity analysis. ||| liankun yang ||| yaying zhang ||| jiankai zuo ||| 
2021 ||| dau-net: dense attention u-net for pavement crack segmentation. ||| yung-an hsieh ||| yi-chang james tsai ||| 
2019 ||| attention-based gated recurrent unit for links traffic speed forecasting. ||| ghazaleh khodabandelou ||| mehdi katranji ||| sami kraiem ||| walid kheriji ||| fouad hadj-selem ||| 
2021 ||| centralized traffic signal control for multiple intersections based on sequence-to-sequence model and attention mechanism. ||| le ma ||| bo xue ||| jia wu ||| 
2021 ||| attention-based neural network for driving environment complexity perception. ||| ce zhang ||| azim eskandarian ||| xuelai du ||| 
2019 ||| attention neural baby talk: captioning of risk factors while driving. ||| yuki mori ||| hiroshi fukui ||| tsubasa hirakawa ||| jo nishiyama ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2019 ||| towards vr attention guidance: environment-dependent perceptual threshold for stereo inverse brightness modulation. ||| steve grogorick ||| georgia albuquerque ||| jan-philipp tauscher ||| marc kassubeck ||| marcus a. magnor ||| 
2019 ||| empirical evaluation of the interplay of emotion and visual attention in human-virtual human interaction. ||| matias volonte ||| reza ghaiumy anaraky ||| bart p. knijnenburg ||| andrew t. duchowski ||| sabarish v. babu ||| 
2019 ||| assessment of driver attention during a safety critical situation in vr to generate vr-based training. ||| efe bozkir ||| david geisler ||| enkelejda kasneci ||| 
2019 ||| real time attention based bidirectional long short-term memory networks for air pollution forecasting. ||| radhika dua ||| divyam madaan ||| prerana mukherjee ||| brejesh lall ||| 
2019 ||| person search based on attention mechanism. ||| zhongjie huang ||| songlin sun ||| yuhao liu ||| 
2019 ||| glosysic framework: transformer for image captioning with sequential attention. ||| srinivasan thanukrishnan ||| r. sai venkatesh ||| prasad rao vijay vignesh ||| 
2022 ||| graves-cpa: a graph-attention verifier selector (competition contribution). ||| will leeson ||| matthew b. dwyer ||| 
2018 ||| recurrent convolutional neural network with attention for twitter and yelp sentiment classification: arc model for sentiment classification. ||| shuang wen ||| jian li ||| 
2021 ||| mulrnn: an enhanced technique of multi-stage-attention network for complex time series prediction. ||| wenliang wang ||| jing li ||| hanyu rao ||| dong mao ||| 
2021 ||| region-level attention network for food and ingredient joint recognition. ||| yirong xue ||| kai niu ||| zhiqiang he ||| 
2018 ||| attresnet: attention-based resnet for image captioning. ||| yunmeng feng ||| long lan ||| xiang zhang ||| chuanfu xu ||| zhenghua wang ||| zhigang luo ||| 
2021 ||| multi-level convolutional transformer with adaptive ranking for semi-supervised crowd counting. ||| xin deng ||| songjian chen ||| yifan chen ||| jie-fang xu ||| 
2019 ||| single-shot object detector based on attention mechanism. ||| xinxin wang ||| hongwei zhu ||| 
2020 ||| leveraging different context for response generation through topic-guided multi-head attention. ||| weikang zhang ||| zhanzhe li ||| yupu guo ||| 
2021 ||| handwritten mathematical expression recognition with self-attention. ||| xueke chi ||| da-han wang ||| yuefeng wu ||| yun wu ||| 
2021 ||| residual networks with channel attention for single image super-resolution. ||| yadong wang ||| junmin wu ||| hui wang ||| 
2020 ||| a novel interactive recurrent attention network for emotion-cause pair extraction. ||| xiangyu jia ||| xinhai chen ||| qian wan ||| jie liu ||| 
2021 ||| attention-based multi-level network for text matching with feature fusion. ||| yixiao yang ||| chongyang zhang ||| 
2020 ||| probabilistic graph attention for relation extraction for domain of geography. ||| jiaorou yin ||| pengfei duan ||| weitao huang ||| shengwu xiong ||| 
2021 ||| research and application of recommendation algorithm based on bidirectional attention model. ||| jiahua wan ||| chengrui ji ||| yiwen zhang ||| 
2021 ||| deep joint convolutional neural network with double-level attention mechanism for multi-sensor bearing performance degradation assessment. ||| jiachen kuang ||| guanghua xu ||| tangfei tao ||| chongyue yang ||| fan wei ||| 
2019 ||| attention in software maintenance: an eye tracking study. ||| maike ahrens ||| kurt schneider ||| melanie busch ||| 
2019 ||| toward imitating visual attention of experts in software development tasks. ||| yoshiharu ikutani ||| nishanth koganti ||| hideaki hata ||| takatomi kubo ||| kenichi matsumoto ||| 
2019 ||| integration of laser scanning and photogrammetry in architecture survey. open issue in geomatics and attention to details. ||| gabriella caroti ||| andrea piemonte ||| 
2021 ||| transformer-based approaches for personality detection using the mbti model. ||| ricardo lazo v ||| squez ||| jos |||  ochoa luna ||| 
2021 ||| visual attention prediction model based on prominence maps, machine learning and biometric data. ||| helver novoa mendoza ||| william joseph giraldo ||| emilio granell ||| f ||| ber danilo giraldo ||| 
2018 ||| inference of the definition of the predicate transformer wp with occurrences of the predicate domain based on denotational semantics of gcl on zf set theory. ||| federico flaviani ||| 
2019 ||| tagging malware intentions by using attention-based sequence-to-sequence neural network. ||| yi-ting huang ||| yu-yuan chen ||| chih-chun yang ||| yeali s. sun ||| shun-wen hsiao ||| meng chang chen ||| 
2020 ||| sequence-to-set semantic tagging for complex query reformulation and automated text categorization in biomedical ir using self-attention. ||| manirupa das ||| juanxi li ||| eric fosler-lussier ||| simon m. lin ||| steve rust ||| yungui huang ||| rajiv ramnath ||| 
2019 ||| nlnde: enhancing neural sequence taggers with attention and noisy channel for robust pharmacological entity detection. ||| lukas lange ||| heike adel ||| jannik str ||| tgen ||| 
2019 ||| ncuee at mediqa 2019: medical text inference using ensemble bert-bilstm-attention model. ||| lung-hao lee ||| yi lu ||| po-han chen ||| po-lei lee ||| kuo-kai shyu ||| 
2021 ||| wbi at mediqa 2021: summarizing consumer health questions with generative transformers. ||| mario s ||| nger ||| leon weber ||| ulf leser ||| 
2019 ||| dut-bim at mediqa 2019: utilizing transformer network and medical domain-specific contextualized representations for question answering. ||| huiwei zhou ||| bizun lei ||| zhe liu ||| zhuang liu ||| 
2019 ||| saama research at mediqa 2019: pre-trained biobert with attention visualisation for medical natural language inference. ||| kamal raj kanakarajan ||| suriyadeepan ramamoorthy ||| vaidheeswaran archana ||| soham chatterjee ||| malaikannan sankarasubbu ||| 
2017 ||| extracting drug-drug interactions with attention cnns. ||| masaki asada ||| makoto miwa ||| yutaka sasaki ||| 
2019 ||| bionlp-ost 2019 rdoc tasks: multi-grain neural relevance ranking using topics and attention based query-document-sentence interactions. ||| pankaj gupta ||| yatin chaudhary ||| hinrich sch ||| tze ||| 
2021 ||| ncuee-nlp at mediqa 2021: health question summarization using pegasus transformers. ||| lung-hao lee ||| po-han chen ||| yu-xiang zeng ||| po-lei lee ||| kuo-kai shyu ||| 
2021 ||| biom-transformers: building large biomedical language models with bert, albert and electra. ||| sultan alrowili ||| vijay shanker ||| 
2019 ||| lasigebiotm at mediqa 2019: biomedical question answering using bidirectional transformers and named entity recognition. ||| andre lamurias ||| francisco m. couto ||| 
2019 ||| constructive type-logical supertagging with self-attention networks. ||| konstantinos kogkalidis ||| michael moortgat ||| tejaswini deoskar ||| 
2020 ||| staying true to your word: (how) can attention become explanation? ||| martin tutek ||| jan snajder ||| 
2017 ||| sequential attention: a context-aware alignment function for machine reading. ||| sebastian brarda ||| philip yeres ||| samuel r. bowman ||| 
2019 ||| multilingual nmt with a language-independent attention bridge. ||| ra ||| l v ||| zquez ||| alessandro raganato ||| j ||| rg tiedemann ||| mathias creutz ||| 
2018 ||| hierarchical convolutional attention networks for text classification. ||| shang gao ||| arvind ramanathan ||| georgia d. tourassi ||| 
2019 ||| an evaluation of language-agnostic inner-attention-based representations in machine translation. ||| alessandro raganato ||| ra ||| l v ||| zquez ||| mathias creutz ||| j ||| rg tiedemann ||| 
2020 ||| enhancing transformer with sememe knowledge. ||| yuhui zhang ||| chenghao yang ||| zhengping zhou ||| zhiyuan liu ||| 
2021 ||| an emotion analysis model based on fine-grained emoji attention mechanism for multi-modal. ||| chunxiao fan ||| siteng chang ||| yuexin wu ||| yitong wang ||| 
2019 ||| attention based speech model for japanese recognization. ||| deguo mu ||| tao zhu ||| guoliang xu ||| han li ||| dongbin guo ||| yongquan liu ||| 
2021 ||| research on vehicle detection model based on attention mechanism. ||| haitao zhang ||| jianmin bao ||| fei ding ||| guanyu mi ||| 
2018 ||| ship targets detection based on visual attention. ||| guiming shi ||| jidong suo ||| 
2020 ||| multi-head attention networks for nonintrusive load monitoring. ||| nan lin ||| binggui zhou ||| guanghua yang ||| shaodan ma ||| 
2021 ||| the video captioning method based on the spatial- temporal information and attention mechanism. ||| ou ye ||| tao liu ||| yan fu ||| jun deng ||| jian feng ||| yun zhang ||| 
2018 ||| enhancing human cross-linguistic comprehension via cognitive computation and selective attention. ||| will x. y. li ||| yizhou lan ||| 
2021 ||| airport small target algorithm based on convolution kernel attention mechanism. ||| shichong li ||| hao zhou ||| 
2021 ||| dynamic gesture recognition based on cnn-lstm-attention. ||| jinwei liu ||| baoguo wei ||| mingzhi cai ||| yong xu ||| 
2021 ||| a multiscale dual-attention based convolutional neural network for ship classification in sar image. ||| gaoyu zhou ||| gong zhang ||| zheng fang ||| qijun dai ||| 
2017 ||| scene emotion detection using closed caption based on hierarchical attention network. ||| chang-uk kwak ||| jeong woo son ||| alex lee ||| sun-joong kim ||| 
2019 ||| depth attention net. ||| hye-jin s. kim ||| seung-min choi ||| suyoung chi ||| 
2020 ||| robust keypoint normalization method for korean sign language translation using transformer. ||| san kim ||| chang jo kim ||| han-mu park ||| yoonyoung jeong ||| jin yea jang ||| hyedong jung ||| 
2021 ||| generating face images using vqgan and sparse transformer. ||| dong-hyuck im ||| yongseok seo ||| 
2021 ||| arrhythmia detection using convolutional neural networks with temporal attention mechanism. ||| muhammad zubair ||| sung-pil woo ||| sunhwan lim ||| chanwon park ||| 
2021 ||| korean traditional document translation using transformer in bidirectional-crf. ||| jungi lee ||| jongwon jang ||| jangwon lee ||| gil-jin jang ||| minho lee ||| 
2018 ||| frequency-aware attention based lstm networks for cardiovascular disease. ||| hwin dol park ||| youngwoong han ||| jae hun choi ||| 
2021 ||| transformer based prediction method for solar power generation data. ||| nac-woo kim ||| hyunyong lee ||| jun-gi lee ||| byung-tak lee ||| 
2018 ||| estimating attentional state of a driver: interacting effects of task demands and cognitive capacities. ||| seonggyu choe ||| hyun-jun jeon ||| oh-sang kwon ||| 
2021 ||| human pose estimation based on attention multi-resolution network. ||| congcong zhang ||| ning he ||| qixiang sun ||| xiaojie yin ||| ke lu ||| 
2019 ||| cross-modal video moment retrieval with spatial and language-temporal attention. ||| bin jiang ||| xin huang ||| chao yang ||| junsong yuan ||| 
2019 ||| improving what cross-modal retrieval models learn through object-oriented inter- and intra-modal attention networks. ||| po-yao huang ||| vaibhav ||| xiaojun chang ||| alexander g. hauptmann ||| 
2020 ||| dagc: employing dual attention and graph convolution for point cloud based place recognition. ||| qi sun ||| hongyan liu ||| jun he ||| zhaoxin fan ||| xiaoyong du ||| 
2020 ||| multi-attention multimodal sentiment analysis. ||| taeyong kim ||| bowon lee ||| 
2020 ||| deep discrete attention guided hashing for face image retrieval. ||| zhi xiong ||| dayan wu ||| wen gu ||| haisu zhang ||| bo li ||| weiping wang ||| 
2021 ||| reading scene text by fusing visual attention with semantic representations. ||| zhiguang liu ||| liangwei wang ||| jian qiao ||| 
2021 ||| gpt2mvs: generative pre-trained transformer-2 for multi-modal video summarization. ||| jia-hong huang ||| luka murn ||| marta mrak ||| marcel worring ||| 
2017 ||| frame-transformer emotion classification network. ||| jiarui gao ||| yanwei fu ||| yu-gang jiang ||| xiangyang xue ||| 
2021 ||| unsupervised training data generation of handwritten formulas using generative adversarial networks with self-attention. ||| matthias springstein ||| eric m ||| ller-budack ||| ralph ewerth ||| 
2021 ||| relation-aware hierarchical attention framework for video question answering. ||| fangtao li ||| ting bai ||| chenyu cao ||| zihe liu ||| chenghao yan ||| bin wu ||| 
2021 ||| be specific, be clear: bridging machine and human captions by scene-guided transformer. ||| yupan huang ||| zhaoyang zeng ||| yutong lu ||| 
2021 ||| teach: attention-aware deep cross-modal hashing. ||| hong-lei yao ||| yu-wei zhan ||| zhen-duo chen ||| xin luo ||| xin-shun xu ||| 
2021 ||| text-enhanced attribute-based attention for generalized zero-shot fine-grained image classification. ||| yan-he chen ||| mei-chen yeh ||| 
2020 ||| attention mechanisms, signal encodings and fusion strategies for improved ad-hoc video search with dual encoding networks. ||| damianos galanopoulos ||| vasileios mezaris ||| 
2020 ||| bidal-hcmus@lsc2020: an interactive multimodal lifelog retrieval with query-to-sample attention-based search engine. ||| anh-vu mai-nguyen ||| trong-dat phan ||| anh-khoa vo ||| van-luon tran ||| minh-son dao ||| koji zettsu ||| 
2021 ||| cross-modal self-attention with multi-task pre-training for medical visual question answering. ||| haifan gong ||| guanqi chen ||| sishuo liu ||| yizhou yu ||| guanbin li ||| 
2021 ||| scene text recognition with cascade attention network. ||| min zhang ||| meng ma ||| ping wang ||| 
2019 ||| relationship detection based on object semantic inference and attention mechanisms. ||| liang zhang ||| shuai zhang ||| peiyi shen ||| guangming zhu ||| syed afaq ali shah ||| mohammed bennamoun ||| 
2018 ||| class-aware self-attention for audio event recognition. ||| shizhe chen ||| jia chen ||| qin jin ||| alexander g. hauptmann ||| 
2021 ||| naster: non-local attentional scene text recognizer. ||| lei wu ||| xueliang liu ||| yanbin hao ||| yunjie ma ||| richang hong ||| 
2019 ||| weakly supervised image retrieval via coarse-scale feature fusion and multi-level attention blocks. ||| xinyao nie ||| hong lu ||| zijian wang ||| jingyuan liu ||| zehua guo ||| 
2018 ||| multimodal network embedding via attention based multi-view variational autoencoder. ||| feiran huang ||| xiaoming zhang ||| chaozhuo li ||| zhoujun li ||| yueying he ||| zhonghua zhao ||| 
2019 ||| stacked self-attention networks for visual question answering. ||| qiang sun ||| yanwei fu ||| 
2021 ||| nested dense attention network for single image super-resolution. ||| cheng qiu ||| yirong yao ||| yuntao du ||| 
2019 ||| a geographical-temporal awareness hierarchical attention network for next point-of-interest recommendation. ||| tongcun liu ||| jianxin liao ||| zhigen wu ||| yulong wang ||| jingyu wang ||| 
2021 ||| multi-attention audio-visual fusion network for audio spatialization. ||| wen zhang ||| jie shao ||| 
2021 ||| global relation-aware attention network for image-text retrieval. ||| jie cao ||| shengsheng qian ||| huaiwen zhang ||| quan fang ||| changsheng xu ||| 
2020 ||| a coordinated representation learning enhanced multimodal machine translation approach with multi-attention. ||| yifeng han ||| lin li ||| jianwei zhang ||| 
2021 ||| look back again: dual parallel attention network for accurate and robust scene text recognition. ||| zilong fu ||| hongtao xie ||| guoqing jin ||| junbo guo ||| 
2021 ||| fire detection using transformer network. ||| mohammad shahid ||| kai-lung hua ||| 
2021 ||| multi-feature graph attention network for cross-modal video-text retrieval. ||| xiaoshuai hao ||| yucan zhou ||| dayan wu ||| wanqian zhang ||| bo li ||| weiping wang ||| 
2019 ||| hierarchical attention based neural network for explainable recommendation. ||| dawei cong ||| yanyan zhao ||| bing qin ||| yu han ||| murray zhang ||| alden liu ||| nat chen ||| 
2021 ||| automatic music composition with transformers. ||| yi-hsuan yang ||| 
2021 ||| rgb-d scene recognition based on object-scene relation and semantics-preserving attention. ||| yuhui guo ||| xun liang ||| 
2020 ||| learning deep graph matching with channel-independent embedding and hungarian attention. ||| tianshu yu ||| runzhong wang ||| junchi yan ||| baoxin li ||| 
2020 ||| on the relationship between self-attention and convolutional layers. ||| jean-baptiste cordonnier ||| andreas loukas ||| martin jaggi ||| 
2021 ||| attentional constellation nets for few-shot learning. ||| weijian xu ||| yifan xu ||| huaijin wang ||| zhuowen tu ||| 
2021 ||| group equivariant stand-alone self-attention for vision. ||| david w. romero ||| jean-baptiste cordonnier ||| 
2020 ||| reducing transformer depth on demand with structured dropout. ||| angela fan ||| edouard grave ||| armand joulin ||| 
2017 ||| frustratingly short attention spans in neural language modeling. ||| michal daniluk ||| tim rockt ||| schel ||| johannes welbl ||| sebastian riedel ||| 
2021 ||| vtnet: visual transformer network for object goal navigation. ||| heming du ||| xin yu ||| liang zheng ||| 
2020 ||| compressive transformers for long-range sequence modelling. ||| jack w. rae ||| anna potapenko ||| siddhant m. jayakumar ||| chloe hillier ||| timothy p. lillicrap ||| 
2021 ||| an image is worth 16x16 words: transformers for image recognition at scale. ||| alexey dosovitskiy ||| lucas beyer ||| alexander kolesnikov ||| dirk weissenborn ||| xiaohua zhai ||| thomas unterthiner ||| mostafa dehghani ||| matthias minderer ||| georg heigold ||| sylvain gelly ||| jakob uszkoreit ||| neil houlsby ||| 
2021 ||| hypergrid transformers: towards a single model for multiple tasks. ||| yi tay ||| zhe zhao ||| dara bahri ||| donald metzler ||| da-cheng juan ||| 
2019 ||| marginalized average attentional network for weakly-supervised learning. ||| yuan yuan ||| yueming lyu ||| xi shen ||| ivor w. tsang ||| dit-yan yeung ||| 
2017 ||| deep biaffine attention for neural dependency parsing. ||| timothy dozat ||| christopher d. manning ||| 
2020 ||| tree-structured attention with hierarchical accumulation. ||| xuan-phi nguyen ||| shafiq r. joty ||| steven c. h. hoi ||| richard socher ||| 
2021 ||| transformer protein language models are unsupervised structure learners. ||| roshan rao ||| joshua meier ||| tom sercu ||| sergey ovchinnikov ||| alexander rives ||| 
2020 ||| u-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation. ||| junho kim ||| minjae kim ||| hyeonwoo kang ||| kwanghee lee ||| 
2019 ||| hyperbolic attention networks. ||| aglar g ||| l ||| ehre ||| misha denil ||| mateusz malinowski ||| ali razavi ||| razvan pascanu ||| karl moritz hermann ||| peter w. battaglia ||| victor bapst ||| david raposo ||| adam santoro ||| nando de freitas ||| 
2018 ||| dcn+: mixed objective and deep residual coattention for question answering. ||| caiming xiong ||| victor zhong ||| richard socher ||| 
2017 ||| dynamic coattention networks for question answering. ||| caiming xiong ||| victor zhong ||| richard socher ||| 
2019 ||| coarse-grain fine-grain coattention network for multi-evidence question answering. ||| victor zhong ||| caiming xiong ||| nitish shirish keskar ||| richard socher ||| 
2021 ||| iot: instance-wise layer reordering for transformer structures. ||| jinhua zhu ||| lijun wu ||| yingce xia ||| shufang xie ||| tao qin ||| wengang zhou ||| houqiang li ||| tie-yan liu ||| 
2020 ||| transformer-xh: multi-evidence reasoning with extra hop attention. ||| chen zhao ||| chenyan xiong ||| corby rosset ||| xia song ||| paul n. bennett ||| saurabh tiwary ||| 
2021 ||| hopper: multi-hop transformer for spatiotemporal reasoning. ||| honglu zhou ||| asim kadav ||| farley lai ||| alexandru niculescu-mizil ||| martin renqiang min ||| mubbasir kapadia ||| hans peter graf ||| 
2018 ||| qanet: combining local convolution with global self-attention for reading comprehension. ||| adams wei yu ||| david dohan ||| minh-thang luong ||| rui zhao ||| kai chen ||| mohammad norouzi ||| quoc v. le ||| 
2020 ||| reformer: the efficient transformer. ||| nikita kitaev ||| lukasz kaiser ||| anselm levskaya ||| 
2018 ||| designing efficient neural attention systems towards achieving human-level sharp vision. ||| abdul rahman abdul ghani ||| nishanth koganti ||| alfredo solano ||| yusuke iwasawa ||| kotaro nakayama ||| yutaka matsuo ||| 
2018 ||| compositional attention networks for machine reasoning. ||| drew a. hudson ||| christopher d. manning ||| 
2021 ||| pre-training text-to-text transformers for concept-centric common sense. ||| wangchunshu zhou ||| dong-ho lee ||| ravi kiran selvam ||| seyeon lee ||| xiang ren ||| 
2020 ||| robustness verification for transformers. ||| zhouxing shi ||| huan zhang ||| kai-wei chang ||| minlie huang ||| cho-jui hsieh ||| 
2018 ||| graph attention networks. ||| petar velickovic ||| guillem cucurull ||| arantxa casanova ||| adriana romero ||| pietro li ||| yoshua bengio ||| 
2017 ||| recurrent mixture density network for spatiotemporal visual attention. ||| loris bazzani ||| hugo larochelle ||| lorenzo torresani ||| 
2017 ||| structured attention networks. ||| yoon kim ||| carl denton ||| luong hoang ||| alexander m. rush ||| 
2018 ||| learn to pay attention. ||| saumya jetley ||| nicholas a. lord ||| namhoon lee ||| philip h. s. torr ||| 
2018 ||| polar transformer networks. ||| carlos esteves ||| christine allen-blanchette ||| xiaowei zhou ||| kostas daniilidis ||| 
2020 ||| monotonic multihead attention. ||| xutai ma ||| juan miguel pino ||| james cross ||| liezl puzon ||| jiatao gu ||| 
2019 ||| posterior attention models for sequence to sequence learning. ||| shiv shankar ||| sunita sarawagi ||| 
2021 ||| a trainable optimal transport embedding for feature aggregation and its relationship to attention. ||| gr ||| goire mialon ||| dexiong chen ||| alexandre d'aspremont ||| julien mairal ||| 
2018 ||| fusionnet: fusing via fully-aware attention with application to machine comprehension. ||| hsin-yuan huang ||| chenguang zhu ||| yelong shen ||| weizhu chen ||| 
2021 ||| random feature attention. ||| hao peng ||| nikolaos pappas ||| dani yogatama ||| roy schwartz ||| noah a. smith ||| lingpeng kong ||| 
2019 ||| music transformer: generating music with long-term structure. ||| cheng-zhi anna huang ||| ashish vaswani ||| jakob uszkoreit ||| ian simon ||| curtis hawthorne ||| noam shazeer ||| andrew m. dai ||| matthew d. hoffman ||| monica dinculescu ||| douglas eck ||| 
2020 ||| capsules with inverted dot-product attention routing. ||| yao-hung hubert tsai ||| nitish srivastava ||| hanlin goh ||| ruslan salakhutdinov ||| 
2020 ||| logic and the 2-simplicial transformer. ||| james clift ||| dmitry doryn ||| daniel murfet ||| james wallbridge ||| 
2020 ||| pay attention to features, transfer learn faster cnns. ||| kafeng wang ||| xitong gao ||| yiren zhao ||| xingjian li ||| dejing dou ||| cheng-zhong xu ||| 
2019 ||| delta: deep learning transfer using feature map with attention for convolutional networks. ||| xingjian li ||| haoyi xiong ||| hanchao wang ||| yuxuan rao ||| liping liu ||| jun huan ||| 
2021 ||| how to find your friendly neighborhood: graph attention design with self-supervision. ||| dongkwan kim ||| alice h. oh ||| 
2021 ||| a universal representation transformer layer for few-shot image classification. ||| lu liu ||| william l. hamilton ||| guodong long ||| jing jiang ||| hugo larochelle ||| 
2019 ||| attention, learn to solve routing problems! ||| wouter kool ||| herke van hoof ||| max welling ||| 
2020 ||| adaptive structural fingerprints for graph attention networks. ||| kai zhang ||| yaokang zhu ||| jun wang ||| jie zhang ||| 
2021 ||| lambdanetworks: modeling long-range interactions without attention. ||| irwan bello ||| 
2021 ||| deformable detr: deformable transformers for end-to-end object detection. ||| xizhou zhu ||| weijie su ||| lewei lu ||| bin li ||| xiaogang wang ||| jifeng dai ||| 
2021 ||| on the dynamics of training attention models. ||| haoye lu ||| yongyi mao ||| amiya nayak ||| 
2017 ||| paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. ||| sergey zagoruyko ||| nikos komodakis ||| 
2020 ||| on identifiability in transformers. ||| gino brunner ||| yang liu ||| damian pascual ||| oliver richter ||| massimiliano ciaramita ||| roger wattenhofer ||| 
2019 ||| universal transformers. ||| mostafa dehghani ||| stephan gouws ||| oriol vinyals ||| jakob uszkoreit ||| lukasz kaiser ||| 
2021 ||| bertology meets biology: interpreting attention in protein language models. ||| jesse vig ||| ali madani ||| lav r. varshney ||| caiming xiong ||| richard socher ||| nazneen fatema rajani ||| 
2020 ||| space: unsupervised object-oriented scene representation via spatial attention and decomposition. ||| zhixuan lin ||| yi-fu wu ||| skand vishwanath peri ||| weihao sun ||| gautam singh ||| fei deng ||| jindong jiang ||| sungjin ahn ||| 
2021 ||| multi-time attention networks for irregularly sampled time series. ||| satya narayan shukla ||| benjamin m. marlin ||| 
2021 ||| is attention better than matrix decomposition? ||| zhengyang geng ||| meng-hao guo ||| hongxu chen ||| xia li ||| ke wei ||| zhouchen lin ||| 
2021 ||| delight: deep and light-weight transformer. ||| sachin mehta ||| marjan ghazvininejad ||| srinivasan iyer ||| luke zettlemoyer ||| hannaneh hajishirzi ||| 
2018 ||| monotonic chunkwise attention. ||| chung-cheng chiu ||| colin raffel ||| 
2020 ||| are transformers universal approximators of sequence-to-sequence functions? ||| chulhee yun ||| srinadh bhojanapalli ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2021 ||| cross-attentional audio-visual fusion for weakly-supervised action localization. ||| jun-tae lee ||| mihir jain ||| hyoungwoo park ||| sungrack yun ||| 
2021 ||| updet: universal multi-agent rl via policy decoupling with transformers. ||| siyi hu ||| fengda zhu ||| xiaojun chang ||| xiaodan liang ||| 
2021 ||| parameter efficient multimodal transformers for video representation learning. ||| sangho lee ||| youngjae yu ||| gunhee kim ||| thomas m. breuel ||| jan kautz ||| yale song ||| 
2020 ||| lite transformer with long-short range attention. ||| zhanghao wu ||| zhijian liu ||| ji lin ||| yujun lin ||| song han ||| 
2021 ||| colorization transformer. ||| manoj kumar ||| dirk weissenborn ||| nal kalchbrenner ||| 
2021 ||| neural attention distillation: erasing backdoor triggers from deep neural networks. ||| yige li ||| xixiang lyu ||| nodens koren ||| lingjuan lyu ||| bo li ||| xingjun ma ||| 
2021 ||| rethinking attention with performers. ||| krzysztof marcin choromanski ||| valerii likhosherstov ||| david dohan ||| xingyou song ||| andreea gane ||| tam ||| s sarl ||| s ||| peter hawkins ||| jared quincy davis ||| afroz mohiuddin ||| lukasz kaiser ||| david benjamin belanger ||| lucy j. colwell ||| adrian weller ||| 
2019 ||| residual non-local attention networks for image restoration. ||| yulun zhang ||| kunpeng li ||| kai li ||| bineng zhong ||| yun fu ||| 
2019 ||| pay less attention with lightweight and dynamic convolutions. ||| felix wu ||| angela fan ||| alexei baevski ||| yann n. dauphin ||| michael auli ||| 
2021 ||| long range arena : a benchmark for efficient transformers. ||| yi tay ||| mostafa dehghani ||| samira abnar ||| yikang shen ||| dara bahri ||| philip pham ||| jinfeng rao ||| liu yang ||| sebastian ruder ||| donald metzler ||| 
2018 ||| bi-directional block self-attention for fast and memory-efficient sequence modeling. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2021 ||| efficient transformers in reinforcement learning using actor-learner distillation. ||| emilio parisotto ||| russ r. salakhutdinov ||| 
2020 ||| hyper-sagnn: a self-attention based graph neural network for hypergraphs. ||| ruochi zhang ||| yuesong zou ||| jian ma ||| 
2021 ||| deberta: decoding-enhanced bert with disentangled attention. ||| pengcheng he ||| xiaodong liu ||| jianfeng gao ||| weizhu chen ||| 
2020 ||| depth-adaptive transformer. ||| maha elbayad ||| jiatao gu ||| edouard grave ||| michael auli ||| 
2017 ||| bidirectional attention flow for machine comprehension. ||| min joon seo ||| aniruddha kembhavi ||| ali farhadi ||| hannaneh hajishirzi ||| 
2020 ||| deep and shallow feature fusion and recognition of recording devices based on attention mechanism. ||| chunyan zeng ||| dongliang zhu ||| zhifeng wang ||| yao yang ||| 
2020 ||| research on book recommendation system for people with visual impairment based on fusion of preference and user attention. ||| zhi yu ||| jiajun bu ||| sijie li ||| wei wang ||| lizhen tang ||| chuanwu zhao ||| 
2019 ||| spatial attention for pedestrian detection. ||| ujjwal ||| aziz dziri ||| bertrand leroy ||| fran ||| ois br ||| mond ||| 
2019 ||| video-based person re-identification using refined attention networks. ||| tanzila rahman ||| mrigank rochan ||| yang wang ||| 
2021 ||| from multimodal to unimodal attention in transformers using knowledge distillation. ||| dhruv agarwal ||| tanay agrawal ||| laura m. ferrari ||| fran ||| ois br ||| mond ||| 
2021 ||| dam: dissimilarity attention module for weakly-supervised video anomaly detection. ||| snehashis majhi ||| srijan das ||| fran ||| ois br ||| mond ||| 
2019 ||| self-attention temporal convolutional network for long-term daily living activity detection. ||| rui dai ||| luca minciullo ||| lorenzo garattoni ||| gianpiero francesca ||| fran ||| ois br ||| mond ||| 
2019 ||| inverse attention guided deep crowd counting network. ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2019 ||| multi-component spatiotemporal attention and its application to object detection in surveillance videos. ||| roman palenychka ||| rami s. abielmona ||| francesco rea ||| emil m. petriu ||| 
2020 ||| emphasis: an embedded public attention stress identification system. ||| jessica leoni ||| asia ciallella ||| luca stornaiuolo ||| marco d. santambrogio ||| donatella sciuto ||| 
2021 ||| super: sub-graph parallelism for transformers. ||| arpan jain ||| tim moon ||| tom benson ||| hari subramoni ||| sam ad |||  jacobs ||| dhabaleswar k. panda ||| brian van essen ||| 
2021 ||| encoder-attention-based automatic term recognition (ea-atr). ||| sampritha h. manjunath ||| john p. mccrae ||| 
2020 ||| applying multilingual and monolingual transformer-based models for dialect identification. ||| cristian popa ||| vlad stefanescu ||| 
2021 ||| hierarchical transformer for multilingual machine translation. ||| albina khusainova ||| adil khan ||| ad ||| n ram ||| rez rivera ||| vitaly romanov ||| 
2021 ||| sapn: spatial attention pyramid network for cross-domain person re-identification. ||| zhaoqian jia ||| wenchao wang ||| shaoqi hou ||| ye li ||| guangqiang yin ||| 
2021 ||| mfagcn: multi-feature based attention graph convolutional network for traffic prediction. ||| haoran li ||| jianbo li ||| zhiqiang lv ||| zhihao xu ||| 
2021 ||| aopl: attention enhanced oversampling and parallel deep learning model for attack detection in imbalanced network traffic. ||| leiqi wang ||| weiqing huang ||| qiujian lv ||| yan wang ||| haiyan chen ||| 
2019 ||| self-attention based collaborative neural network for recommendation. ||| shengchao ma ||| jinghua zhu ||| 
2021 ||| sequential recommendation via temporal self-attention and multi-preference learning. ||| wenchao wang ||| jinghua zhu ||| heran xi ||| 
2021 ||| person re-identification algorithm based on spatial attention network. ||| shaoqi hou ||| chunhui liu ||| kangning yin ||| guangqiang yin ||| 
2020 ||| attention-based dynamic preference model for next point-of-interest recommendation. ||| chenwang zheng ||| dan tao ||| 
2021 ||| multi-step domain adaption image classification network via attention mechanism and multi-level feature alignment. ||| yaoci xiang ||| chong zhao ||| xing wei ||| yang lu ||| shaofan liu ||| 
2021 ||| light field super-resolution based on spatial and angular attention. ||| donglin li ||| da yang ||| sizhe wang ||| hao sheng ||| 
2021 ||| temporal attention-based graph convolution network for taxi demand prediction in functional areas. ||| yue wang ||| jianbo li ||| aite zhao ||| zhiqiang lv ||| guangquan lu ||| 
2021 ||| dual attention network based on knowledge graph for news recommendation. ||| yang ren ||| xiaoming wang ||| guangyao pang ||| yaguang lin ||| pengfei wan ||| 
2017 ||| a preliminary study on the influence of automation over mind wandering frequency in sustained attention. ||| jonas gouraud ||| arnaud delorme ||| bruno berberian ||| 
2021 ||| imitations of immortality: learning from human imitative examples in transformer poetry generation. ||| ray lc ||| 
2021 ||| improving the efficiency of transformers for resource-constrained devices. ||| hamid tabani ||| ajay balasubramaniam ||| shabbir marzban ||| elahe arani ||| bahram zonooz ||| 
2021 ||| integrating channel context attention and regional association attention for kidney and tumor segmentation. ||| ying liu ||| hui cui ||| tiangang zhang ||| toshiya nakaguchi ||| ping xuan ||| 
2019 ||| attention-guided convolutional neural network for detecting pneumonia on chest x-rays. ||| bingchuan li ||| guixia kang ||| kai cheng ||| ningbo zhang ||| 
2020 ||| correlates of attention in the cingulate cortex during gambling in humans. ||| christopher taylor ||| patrick greene ||| raina d'aleo ||| macauley smith breault ||| cynthia r. steinhardt ||| jorge gonzalez-martinez ||| sridevi v. sarma ||| 
2020 ||| a haptic-based perception-empathy biofeedback system with vibration transition: verifying the attention amount. ||| jiayi ling ||| jing-chen hong ||| yuki hayashi ||| kazuhiro yasuda ||| yu kitaji ||| hiroaki harashima ||| hiroyasu iwata ||| 
2020 ||| development and evaluation of a new virtual reality-based audio-tactile cueing-system to guide visuo-spatial attention. ||| samuel e. j. knobel ||| nathan t. gyger ||| thomas nyffeler ||| dario cazzoli ||| ren |||  m. m ||| ri ||| tobias nef ||| 
2021 ||| hierarchical attentional feature fusion for surgical instrument segmentation. ||| xiaowei zhou ||| yue guo ||| wenhao he ||| haitao song ||| 
2021 ||| placental super micro-vessels segmentation based on resnext with convolutional block attention and u-net. ||| minsi chen ||| cheng zhao ||| xiaoxian tian ||| yujian liu ||| tianfu wang ||| baiying lei ||| 
2020 ||| metaheuristic spatial transformation (mst) for accurate detection of attention deficit hyperactivity disorder (adhd) using rs-fmri. ||| abhay m. s. aradhya ||| suresh sundaram ||| mahardhika pratama ||| 
2020 ||| a weighted graph attention network based method for multi-label classification of electrocardiogram abnormalities. ||| hongmei wang ||| wei zhao ||| zhenqi li ||| dongya jia ||| cong yan ||| jing hu ||| jiansheng fang ||| ming yang ||| 
2021 ||| ucatr: based on cnn and transformer encoding and cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. ||| chun luo ||| jing zhang ||| xinglin chen ||| yinhao tang ||| xiechuan weng ||| fan xu ||| 
2021 ||| introducing attention mechanism for eeg signals: emotion recognition with vision transformers. ||| arjun ||| aniket singh rajpoot ||| mahesh raveendranatha panicker ||| 
2019 ||| pandas: paediatric attention-deficit/hyperactivity disorder application software. ||| herv |||  m. mwamba ||| pieter r. fourie ||| dawie van den heever ||| 
2021 ||| towards interpretable attention networks for cervical cancer analysis. ||| ruiqi wang ||| mohammad ali armin ||| simon denman ||| lars petersson ||| david ahmedt-aristizabal ||| 
2021 ||| high-resolution magnetic resonance spectroscopic imaging using a multi-encoder attention u-net with structural and adversarial loss. ||| siyuan dong ||| gilbert hangel ||| wolfgang bogner ||| siegfried trattnig ||| karl r ||| ssler ||| georg widhalm ||| henk m. de feyter ||| robin a. de graaf ||| james s. duncan ||| 
2021 ||| transformer-based cnns: mining temporal context information for multi-sound covid-19 diagnosis. ||| yi chang ||| zhao ren ||| bj ||| rn w. schuller ||| 
2021 ||| weakly supervised attention map training for histological localization of colonoscopy images. ||| jangho kwon ||| kihwan choi ||| 
2018 ||| automatic sleep stage classification using single-channel eeg: learning sequential features with attention-based recurrent neural networks. ||| huy phan ||| fernando andreotti ||| navin cooray ||| oliver y. ch ||| n ||| maarten de vos ||| 
2019 ||| using soft attention mechanisms to classify heart sounds. ||| jorge oliveira ||| diogo marcelo nogueira ||| cl ||| ber ramos ||| francesco renna ||| carlos abreu ferreira ||| miguel t. coimbra ||| 
2019 ||| a blstm with attention network for predicting acute myeloid leukemia patient's prognosis using comprehensive clinical parameters. ||| chih-chuan lu ||| jeng-lin li ||| yu-fen wang ||| bor-sheng ko ||| jih-luh tang ||| chi-chun lee ||| 
2017 ||| brain connectivity networks at the basis of human attention components: an eeg study. ||| alessandra anzolin ||| donatella mattia ||| jlenia toppi ||| floriana pichiorri ||| angela riccio ||| laura astolfi ||| 
2021 ||| cross-subject eeg-based emotion recognition using adversarial domain adaption with attention mechanism. ||| yalan ye ||| xin zhu ||| yunxia li ||| tongjie pan ||| wenwen he ||| 
2021 ||| learning generalized representations of eeg between multiple cognitive attention tasks. ||| yi ding ||| nigel wei jun ang ||| aung aung phyo wai ||| cuntai guan ||| 
2018 ||| research of the regulation effect of transcranial alternating current stimulation on vigilant attention. ||| huanhuan cui ||| jinwen wei ||| yufeng ke ||| xingwei an ||| chang sun ||| minpeng xu ||| hongzhi qi ||| dong ming ||| peng zhou ||| 
2019 ||| prediction of response time and vigilance score in a sustained attention task from pre-trial phase synchrony using deep neural networks. ||| mastaneh torkamani-azar ||| sumeyra demir kanik ||| sara atito ali ahmed ||| serap aydin ||| m ||| jdat  ||| etin ||| 
2017 ||| personalized features for attention detection in children with attention deficit hyperactivity disorder. ||| fatemeh fahimi ||| cuntai guan ||| wooi boon goh ||| kai keng ang ||| choon guan lim ||| tih shih lee ||| 
2020 ||| a novel graph attention network architecture for modeling multimodal brain connectivity. ||| alexandru-catalin filip ||| tiago azevedo ||| luca passamonti ||| nicola toschi ||| pietro li ||| 
2019 ||| effect of english learning experience on young children's prefrontal cortex functioning for attentional control: an fnirs study. ||| chuanjiang li ||| mingming zhang ||| keya ding ||| jing zhou ||| dongchuan yu ||| 
2021 ||| gated transformer for decoding human brain eeg signals. ||| yunzhe tao ||| tao sun ||| aashiq muhamed ||| sahika genc ||| dylan jackson ||| ali arsanjani ||| suri yaddanapudi ||| liang li ||| prachi kumar ||| 
2021 ||| attention based deep multiple instance learning approach for lung cancer prediction using histopathological images. ||| jo ||| o moranguinho ||| t ||| nia pereira ||| bernardo ramos ||| joana morgado ||| jos |||  luis costa ||| h ||| lder p. oliveira ||| 
2020 ||| an attention-based deep learning method for schizophrenia patients classification using dna methylation data. ||| minmin zhang ||| changchun pan ||| haichun liu ||| qinting zhang ||| haozhe li ||| 
2021 ||| c3d-unet: a comprehensive 3d unet for covid-19 segmentation with intact encoding and local attention. ||| yiming bao ||| hexiang zeng ||| chengfeng zhou ||| chen liu ||| lichi zhang ||| dahong qian ||| jun wang ||| hongbing lu ||| 
2018 ||| assessment of attention demand for balance control using a smartphone: implementation and evaluation. ||| quentin mourcou ||| celine franco ||| bruno diot ||| nicolas vuillerme ||| 
2020 ||| learning a phenotypic-attribute attentional brain connectivity embedding for adhd classification using rs-fmri. ||| ming-shan gao ||| fu-sheng tsai ||| chi-chun lee ||| 
2020 ||| analysis of selective attention processing on experienced simultaneous interpreters using eeg phase synchronization. ||| haruko yagura ||| hiroki tanaka ||| taiki kinoshita ||| hiroki watanabe ||| shunnosuke motomura ||| katsuhito sudoh ||| satoshi nakamura ||| 
2021 ||| combined dynamic time warping and spatiotemporal attention for myoelectric control. ||| milad jabbari ||| rami n. khushaba ||| kianoush nazarpour ||| 
2018 ||| vulnerable plaque recognition based on attention model with deep convolutional neural network. ||| jingmin xin ||| sijie liu ||| yangyang deng ||| nanning zheng ||| 
2020 ||| adaptive brain-computer interface with attention alterations in patients with amyotrophic lateral sclerosis. ||| susan aliakbaryhosseinabadi ||| natalie mrachacz-kersting ||| 
2019 ||| neural activity from attention networks predicts movement errors. ||| macauley smith breault ||| jorge a. gonz ||| lez-mart ||| nez ||| john t. gale ||| sridevi v. sarma ||| 
2020 ||| decoding auditory attention from single-trial eeg for a high-efficiency brain-computer interface. ||| winko w. an ||| alexander pei ||| abigail l. noyce ||| barbara g. shinn-cunningham ||| 
2021 ||| spectrum power and brain functional connectivity of different eeg frequency bands in attention network tests. ||| cheng wang ||| xin wang ||| mingxing zhu ||| yao pi ||| xiaochen wang ||| feng wan ||| shixiong chen ||| guanglin li ||| 
2020 ||| dran: densely reversed attention based convolutional network for diabetic retinopathy detection. ||| cam-hao hua ||| thien huynh-the ||| sungyoung lee ||| 
2017 ||| feasibility study on the assessment of auditory sustained attention through walking motor parameters in mild cognitive impairments and healthy subjects. ||| laura fiorini ||| martina maselli ||| emanuela castro ||| stefania tocchini ||| marco t. sportiello ||| cecilia laschi ||| francesca cecchi ||| filippo cavallo ||| 
2021 ||| segmentation in diabetic retinopathy using deeply-supervised multiscalar attention. ||| sanhita basu ||| sushmita mitra ||| 
2020 ||| sequential attention-based detection of semantic incongruities from eeg while listening to speech. ||| shunnosuke motomura ||| hiroki tanaka ||| satoshi nakamura ||| 
2021 ||| a study of visual search based calibration protocol for eeg attention detection. ||| aung aung phyo wai ||| jee ern tchen ||| cuntai guan ||| 
2021 ||| self-attention based virtual staining for bright-field images of label-free human carotid atherosclerotic plaque tissue section. ||| guanghao zhang ||| hui hui ||| bin ning ||| di dong ||| jie tian ||| wen he ||| 
2020 ||| 40-hz rhythmic visual stimulation facilitates attention by reshaping the brain functional connectivity. ||| jia you ||| minpeng xu ||| rong li ||| zhongpeng wang ||| shuang liu ||| dong ming ||| 
2020 ||| multi-shell d-mri reconstruction via residual learning utilizing encoder-decoder network with attention (msr-net). ||| ranjeet ranjan jha ||| aditya nigam ||| arnav bhavsar ||| sudhir k. pathak ||| walter schneider ||| rathish kumar ||| 
2018 ||| regularized spatial filtering method (r-sfm) for detection of attention deficit hyperactivity disorder (adhd) from resting-state functional magnetic resonance imaging (rs-fmri). ||| abhay m. s. aradhya ||| vigneshwaran subbaraju ||| suresh sundaram ||| narasimhan sundararajan ||| 
2021 ||| dual attention convolutional neural network based on adaptive parametric relu for denoising ecg signals with strong noise. ||| zixiao he ||| xinwen liu ||| hao he ||| huan wang ||| 
2019 ||| real-time tracking of magnetoencephalographic neuromarkers during a dynamic attention-switching task. ||| alessandro presacco ||| sina miran ||| behtash babadi ||| jonathan z. simon ||| 
2021 ||| 3d attention m-net for short-axis left ventricular myocardium segmentation in mice mr cardiac images. ||| luojie huang ||| andrew jin ||| jinchi wei ||| dnyanesh tipre ||| chin-fu liu ||| robert g. weiss ||| siamak ardekani ||| 
2017 ||| eeg-based auditory attention decoding using unprocessed binaural signals in reverberant and noisy conditions? ||| ali aroudi ||| simon doclo ||| 
2021 ||| dual encoder attention u-net for nuclei segmentation. ||| abhishek vahadane ||| atheeth b ||| shantanu majumdar ||| 
2020 ||| automatic pulmonary vein and left atrium segmentation for tapvc preoperative evaluation using v-net with grouped attention. ||| jiang li ||| huai chen ||| fang zhu ||| chen wen ||| huiwen chen ||| lisheng wang ||| 
2021 ||| auditory attention detection with eeg channel attention. ||| enze su ||| siqi cai ||| peiwen li ||| longhan xie ||| haizhou li ||| 
2019 ||| a study of the midbrain network for covert attentional orienting in cervical dystonia patients using dynamic causal modelling. ||| oisin duggan ||| shruti narasimham ||| eavan mcgovern ||| owen killian ||| sean o'riordan ||| michael k. hutchinson ||| richard b. reilly ||| 
2021 ||| s and sequence transformer networks. ||| ryan chen ||| keshab k. parhi ||| 
2021 ||| low-latency auditory spatial attention detection based on spectro-spatial features from eeg. ||| siqi cai ||| pengcheng sun ||| tanja schultz ||| haizhou li ||| 
2021 ||| eeg emotion recognition via graph-based spatio-temporal attention neural networks. ||| shadi sartipi ||| mastaneh torkamani-azar ||| m ||| jdat  ||| etin ||| 
2020 ||| aec-net: attention and edge constraint network for medical image segmentation. ||| jingyi wang ||| xu zhao ||| qingtian ning ||| dahong qian ||| 
2021 ||| the synchronized enhancement effect of rhythmic visual stimulation of 40 hz on selective attention. ||| rong li ||| jia you ||| minpeng xu ||| dong ming ||| 
2021 ||| amf-net: attention-aware multi-scale fusion network for retinal vessel segmentation. ||| qi yang ||| bingqi ma ||| hui cui ||| jiquan ma ||| 
2017 ||| the timing of theta phase synchronization accords with vigilant attention. ||| jinwen wei ||| yufeng ke ||| chang sun ||| xingwei an ||| hongzhi qi ||| dong ming ||| peng zhou ||| 
2020 ||| breathing sound segmentation and detection using transfer learning techniques on an attention-based encoder-decoder architecture. ||| chiu-han hsiao ||| ting-wei lin ||| chii-wann lin ||| fu-shun hsu ||| frank yeong-sung lin ||| chung-wei chen ||| chi-ming chung ||| 
2020 ||| generalizability of eeg-based mental attention modeling with multiple cognitive tasks. ||| aung aung phyo wai ||| maokang dou ||| cuntai guan ||| 
2021 ||| zoome: efficient melanoma detection using zoom-in attention and metadata embedding deep neural network. ||| xiaoyan xing ||| pingping song ||| kai zhang ||| fang yang ||| yuhan dong ||| 
2018 ||| real-time decoding of auditory attention from eeg via bayesian filtering. ||| sina miran ||| sahar akram ||| alireza sheikhattar ||| jonathan z. simon ||| tao zhang ||| behtash babadi ||| 
2020 ||| malignancy detection in prostate multi-parametric mr images using u-net with attention. ||| archana machireddy ||| nicholas meermeier ||| fergus coakley ||| xubo song ||| 
2021 ||| generative adversarial training with dual-attention for vascular segmentation and topological analysis. ||| xueying wang ||| xiaoya liu ||| li lin ||| qiongyu guo ||| xiaoying tang ||| 
2019 ||| lstms and neural attention models for blood glucose prediction: comparative experiments on real and synthetic data. ||| sadegh mirshekarian ||| hui shen ||| razvan c. bunescu ||| cindy marling ||| 
2021 ||| brain tumors classification for mr images based on attention guided deep learning model. ||| yuhao zhang ||| shuhang wang ||| haoxiang wu ||| kejia hu ||| shufan ji ||| 
2020 ||| effects of stimulus spatial resolution on ssvep responses under overt and covert attention. ||| aung aung phyo wai ||| jun cong lee ||| tao yang ||| rosa q. so ||| cuntai guan ||| 
2020 ||| eeg-based depression detection using convolutional neural network with demographic attention mechanism. ||| xiaowei zhang ||| junlei li ||| kechen hou ||| bin hu ||| jian shen ||| jing pan ||| 
2020 ||| single fundus image super-resolution via cascaded channel-wise attention network. ||| zhihao fan ||| tingting dan ||| honghua yu ||| baoyi liu ||| hongmin cai ||| 
2019 ||| rasnet: segmentation for tracking surgical instruments in surgical videos using refined attention segmentation network. ||| zhen-liang ni ||| gui-bin bian ||| xiaoliang xie ||| zeng-guang hou ||| xiao-hu zhou ||| yan-jie zhou ||| 
2017 ||| neural decoding of attentional selection in multi-speaker environments without access to separated sources. ||| james o'sullivan ||| zhuo chen ||| sameer a. sheth ||| guy mckhann ||| ashesh d. mehta ||| nima mesgarani ||| 
2021 ||| full scale attention for automated covid-19 diagnosis from ct images. ||| zheng cao ||| cailin mu ||| haochao ying ||| jian wu ||| 
2020 ||| automatic quality assessment of reflectance confocal microscopy mosaics using attention-based deep neural network. ||| marek wodzinski ||| miroslawa pajak ||| andrzej skalski ||| alexander witkowski ||| giovanni pellacani ||| joanna ludzik ||| 
2021 ||| a novel approach to decode covert spatial attention using ssvep and single-frequency phase-coded stimuli. ||| alexandre armengol-urpi ||| andres f. salazar-gomez ||| sanjay e. sarma ||| 
2021 ||| attention-based multi-scale generative adversarial network for synthesizing contrast-enhanced mri. ||| meiqing pan ||| hui zhang ||| zhenchao tang ||| yinghua zhao ||| jie tian ||| 
2020 ||| neural correlates of attention lapses during continuous tasks. ||| mohamed hossam zaky ||| reza shoorangiz ||| govinda r. poudel ||| le yang ||| richard d. jones ||| 
2020 ||| an lmmse-based estimation of temporal response function in auditory attention decoding. ||| ivine kuruvila ||| eghart fischer ||| ulrich hoppe ||| 
2019 ||| attentional bias for emotional faces in depressed and non-depressed individuals: an eye-tracking study. ||| germano r. figueiredo ||| wagner l. ripka ||| eduardo f ||| lix ribeiro romaneli ||| leandra ulbricht ||| 
2020 ||| triple multi-scale adversarial learning with self-attention and quality loss for unpaired fundus fluorescein angiography synthesis. ||| zhuotong cai ||| jingmin xin ||| jiayi wu ||| sijie liu ||| weiliang zuo ||| nanning zheng ||| 
2020 ||| classification of auditory attention focuses during speech perception. ||| haiqing yu ||| minpeng xu ||| jiayuan meng ||| zhen ma ||| dong ming ||| 
2018 ||| applying entropy to human center of foot pressure data to assess attention investment in balance control. ||| celine franco ||| anthony fleury ||| bruno diot ||| nicolas vuillerme ||| 
2021 ||| low dose ct image denoising using boosting attention fusion gan with perceptual loss. ||| luella marcos ||| javad alirezaie ||| paul s. babyn ||| 
2021 ||| attentional bias towards high and low caloric food on repeated visual food stimuli: an erp study. ||| aruna duraisingam ||| ramaswamy palaniappan ||| daniele soria ||| 
2020 ||| deep learning with skip connection attention for choroid layer segmentation in oct images. ||| xiaoqian mao ||| yitian zhao ||| bang chen ||| yuhui ma ||| zaiwang gu ||| shenshen gu ||| jianlong yang ||| jun cheng ||| jiang liu ||| 
2018 ||| the effect of miniaturization and galvanic separation of eeg sensor devices in an auditory attention detection task. ||| abhijith mundanad narayanan ||| alexander bertrand ||| 
2020 ||| attention networks for multi-task signal analysis. ||| david ahmedt-aristizabal ||| mohammad ali armin ||| simon denman ||| clinton fookes ||| lars petersson ||| 
2020 ||| a novel approach for atrial fibrillation signal identification based on temporal attention mechanism. ||| yibo gao ||| huan wang ||| zuhao liu ||| 
2019 ||| spectral analysis versus signal complexity methods for assessing attention related activity in human eeg. ||| urszula malinowska ||| jakub wojciechowski ||| marek walig ||| ra ||| andrzej wr ||| bel ||| pawel niedbalski ||| jacek rogala ||| 
2019 ||| canet: a channel attention network to determine informative multi-channel for image classification from brain signals. ||| yangwoo kim ||| sehyeon jang ||| kyungho won ||| sung chan jun ||| 
2021 ||| 3d deep attentive u-net with transformer for breast tumor segmentation from automated breast volume scanner. ||| yiyao liu ||| yi yang ||| wei jiang ||| tianfu wang ||| baiying lei ||| 
2017 ||| the motion influence on respiration rate estimation from low-resolution thermal sequences during attention focusing tasks. ||| alicja kwasniewska ||| jacek ruminski ||| jerzy wtorek ||| 
2019 ||| improving crnn with efficientnet-like feature extractor and multi-head attention for text recognition. ||| dinh viet sang ||| le tran bao cuong ||| 
2017 ||| parallel multi-feature attention on neural sentiment classification. ||| pengcheng zhu ||| yujiu yang ||| 
2019 ||| session-based recommendation with self-attention. ||| pham hoang anh ||| ngo xuan bach ||| tu minh phuong ||| 
2021 ||| catching patient's attention at the right time to help them undergo behavioural change: stress classification experiment from blood volume pulse. ||| aneta lisowska ||| szymon wilk ||| mor peleg ||| 
2021 ||| transicd: transformer based code-wise attention model for explainable icd coding. ||| biplob biswas ||| thai-hoang pham ||| ping zhang ||| 
2021 ||| attention-based explanation in a deep learning model for classifying radiology reports. ||| luca putelli ||| alfonso emilio gerevini ||| alberto lavelli ||| roberto maroldi ||| ivan serina ||| 
2021 ||| transformers for multi-label classification of medical text: an empirical comparison. ||| vithya yogarajan ||| jacob montiel ||| tony smith ||| bernhard pfahringer ||| 
2021 ||| using event-based web-scraping methods and bidirectional transformers to characterize covid-19 outbreaks in food production and retail settings. ||| joseph miano ||| charity hilton ||| vasu gangrade ||| mary pomeroy ||| jacqueline siven ||| michael flynn ||| frances tilashalski ||| 
2021 ||| cbam-unet++: easier to find the target with the attention module "cbam". ||| zhengxuan zhao ||| kaixu chen ||| satoshi yamane ||| 
2021 ||| propagation-based fake news detection using graph neural networks with transformer. ||| hayato matsumoto ||| soh yoshida ||| mitsuji muneyasu ||| 
2020 ||| automatic detection of chewing and swallowing using hybrid ctc/attention. ||| akihiro nakamura ||| ken ohta ||| takato saito ||| hiroshi mineno ||| daizo ikeda ||| masafumi nishimura ||| 
2021 ||| text image super resolution using deep attention neural network. ||| yun liu ||| remina yano ||| hiroshi watanabe ||| takuya suzuki ||| takeshi chujoh ||| tomohiro ikai ||| 
2017 ||| an intelligent maintenance scheduling of distribution transformers in a smart grid. ||| pei-shuo chiu ||| shyh-jier huang ||| wei-fu su ||| kuan-te wu ||| che-jung hsu ||| 
2020 ||| self-attention based neural network for few shot classification. ||| ravi jain ||| hiroshi watanabe ||| 
2018 ||| user-centric visual attention estimation based on relationship between image and eye gaze data. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2020 ||| data augmentation using user attention for educational content recommendation based on ffm. ||| kazuma ohtomo ||| ryosuke harakawa ||| masaki iisaka ||| masahiro iwahashi ||| 
2019 ||| the design of wireless power transformer for electronic flower pot. ||| chia-yang liu ||| chih-lung hsiao ||| 
2020 ||| estimation of user-specific visual attention considering individual tendency toward gazed objects. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| a study of dqn using visiontransformer as an image extractor. ||| toshiki hatano ||| toi tsuneda ||| satoshi yamane ||| 
2021 ||| question generation using knowledge graphs with the t5 language model and masked self-attention. ||| kosuke aigo ||| takashi tsunakawa ||| masafumi nishida ||| masafumi nishimura ||| 
2019 ||| estimation of user-specific visual attention based on gaze information of similar users. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| automatic detection of chewing and swallowing using attention-based fusion. ||| akihiro nakamura ||| takato saito ||| daizo ikeda ||| ken ohta ||| hiroshi mineno ||| masafumi nishimura ||| 
2021 ||| four-way bidirectional attention for multiple-choice reading comprehension. ||| lei hu ||| dongsheng zou ||| xiwang guo ||| liang qi ||| ying tang ||| haohao song ||| jieying yuan ||| 
2021 ||| cthgat: category-aware and time-aware next point-of-interest via heterogeneous graph attention network. ||| chenchao wang ||| chao peng ||| mengdan wang ||| rui yang ||| wenhan wu ||| qilin rui ||| neal n. xiong ||| 
2021 ||| mix attention based convolutional neural network for clothing brand logo recognition and classification. ||| kuan-hsien liu ||| guan-hong chen ||| tsung-jung liu ||| 
2020 ||| a cnn model for herb identification based on part priority attention mechanism. ||| yangyang zhao ||| zhanquan sun ||| engang tian ||| chuanfei hu ||| hui zong ||| fan yang ||| 
2020 ||| decoding visual recognition of objects from eeg signals based on attention-driven convolutional neural network. ||| jenifer kalafatovich ||| minji lee ||| seong-whan lee ||| 
2021 ||| csagan: channel and spatial attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| rui yang ||| chao peng ||| chenchao wang ||| mengdan wang ||| yao chen ||| peng zheng ||| neal n. xiong ||| 
2019 ||| a pbci to predict attentional error before it happens in real flight conditions. ||| fr ||| d ||| ric dehais ||| imad rida ||| rapha ||| lle n. roy ||| john iversen ||| tim r. mullen ||| daniel e. callan ||| 
2020 ||| rna-net: residual nonlocal attention network for retinal vessel segmentation. ||| yixuan chen ||| yuhan dong ||| yi zhang ||| kai zhang ||| 
2021 ||| risc-vtf: risc-v based extended instruction set for transformer. ||| qiang jiao ||| wei hu ||| fang liu ||| yong dong ||| 
2021 ||| acnet: mask-aware attention with dynamic context enhancement for robust acne detection. ||| kyungseo min ||| gun-hee lee ||| seong-whan lee ||| 
2021 |||  adaptive attention convolutional neural network. ||| abenezer girma ||| abdollah homaifar ||| m. nabil mahmoud ||| xuyang yan ||| mrinmoy sarkar ||| 
2020 ||| bmi-vr based cognitive training improves attention switching processing speed. ||| christian i. penaloza ||| melanie segado ||| patricia debergue ||| 
2021 ||| box-driven weakly supervised images semantic segmentation algorithm based on attention model. ||| jianxin liu ||| yushui geng ||| jing zhao ||| wenxiao li ||| kang zhang ||| 
2018 ||| cognitive environment system by joint attention behaviors and relevance theory for robot partners. ||| naoyuki kubota ||| jinseok woo ||| ryosuke tanaka ||| 
2021 ||| ssvep-aided recognition of internally and externally directed attention from brain activity. ||| lisa-marie vortmann ||| jonas klaff ||| timo urban ||| felix putze ||| 
2020 ||| learning effective value function factorization via attentional communication. ||| bo wu ||| xiaoya yang ||| chuxiong sun ||| rui wang ||| xiaohui hu ||| yan hu ||| 
2021 ||| predictive attention allocation in supervising multiple robots for search and rescue tasks. ||| songpo li ||| xiaoli zhang ||| 
2019 ||| predicting auditory spatial attention from eeg using single- and multi-task convolutional neural networks. ||| zhentao liu ||| jeffrey mock ||| yufei huang ||| edward j. golob ||| 
2019 ||| conversation during partially automated driving: how attention arousal is effective on maintaining situation awareness. ||| jieun lee ||| toshiaki hirano ||| tomoya hano ||| makoto itoh ||| 
2018 ||| a study of ssvep responses in case of overt and covert visual attention with different view angles. ||| aung aung phyo wai ||| zhiyan goh ||| shi de foo ||| cuntai guan ||| 
2021 ||| transformer-xl with graph neural network for source code summarization. ||| xiaoling zhang ||| shouguo yang ||| luqian duan ||| zhe lang ||| zhiqiang shi ||| limin sun ||| 
2021 ||| non-strict attentional region annotation to improve image classification accuracy. ||| satoshi arai ||| shinichi shirakawa ||| tomoharu nagao ||| 
2021 ||| sarnet: self-attention assisted ranking network for temporal action proposal generation. ||| jiahao yu ||| jiang hong ||| 
2021 ||| crb-net: a sign language recognition deep learning strategy based on multi-modal fusion with attention mechanism. ||| feng xiao ||| cong shen ||| tiantian yuan ||| shengyong chen ||| 
2018 ||| covert visuospatial attention (vsa) for eeg-based asynchronous control of robot. ||| oh yoke chew ||| neethu robinson ||| smitha kavallur pisharath gopi ||| 
2021 ||| edge attention network for image deblurring and super-resolution. ||| jong-wook han ||| jun-ho choi ||| jun-hyuk kim ||| jong-seok lee ||| 
2017 ||| eeg-based auditory attention decoding: impact of reverberation, noise and interference reduction. ||| ali aroudi ||| simon doclo ||| 
2021 ||| tstrack: tracking by detector with target guidance and self-attention. ||| teng liu ||| aimin li ||| jiachuan li ||| 
2019 ||| morphological landmark detection on lobsters using attention networks. ||| parmeet singh ||| mae l. seto ||| 
2019 ||| attention prediction on webpage images using multilabel classification. ||| sandeep vidyapu ||| vijaya saradhi vedula ||| samit bhattacharya ||| 
2021 ||| malbert: malware detection using bidirectional encoder representations from transformers. ||| abir rahali ||| moulay a. akhloufi ||| 
2021 ||| one-stage attention-based network for image classification and segmentation on optical coherence tomography image. ||| xiaoming liu ||| yingjie bai ||| min jiang ||| 
2020 ||| ahac: actor hierarchical attention critic for multi-agent reinforcement learning. ||| yajie wang ||| dianxi shi ||| chao xue ||| hao jiang ||| gongju wang ||| peng gong ||| 
2019 ||| shared attention reflected in eeg, electrodermal activity and heart rate. ||| anne-marie brouwer ||| ivo v. stuldreher ||| nattapong thammasan ||| 
2021 ||| automatic misogyny detection in social media platforms using attention-based bidirectional-lstm. ||| abir rahali ||| moulay a. akhloufi ||| anne-marie therien-daniel ||| loi brassard-gourdeau ||| 
2020 ||| an auxiliary screening system for autism spectrum disorder based on emotion and attention analysis. ||| kai xu ||| bin ji ||| zhiyong wang ||| jingjing liu ||| honghai liu ||| 
2017 ||| visual attention control using peripheral vision stimulation. ||| yuta inoue ||| takuya tanizawa ||| akira utsumi ||| kenji susami ||| tadahisa kondo ||| kazuhiko takahashi ||| 
2021 ||| semantic segmentation method of 3d liver image based on contextual attention model. ||| sai shao ||| xiaolong zhang ||| ruoqin chen ||| chunhua deng ||| 
2017 ||| analysis of driver visual attention when driving with different levels of haptic steering guidance. ||| zheng wang ||| bo yang ||| rencheng zheng ||| tsutomu kaizuka ||| kimihiko nakano ||| 
2021 ||| multimodal sentiment analysis based on attention mechanism and tensor fusion network. ||| kang zhang ||| yushui geng ||| jing zhao ||| wenxiao li ||| jianxin liu ||| 
2021 ||| show why the answer is correct! towards explainable ai using compositional temporal attention. ||| nihar bendre ||| kevin desai ||| peyman najafirad ||| 
2021 ||| a fine-grained visual attention approach for fingerspelling recognition in the wild. ||| kamala gajurel ||| cuncong zhong ||| guanghui wang ||| 
2020 ||| proposal for visual attention level based on microsaccades after saccades. ||| soichiro yokoo ||| nobuyuki nishiuchi ||| kimihiro yamanaka ||| 
2020 ||| self-attention networks for human activity recognition using wearable devices. ||| carlos andres betancourt baca ||| wen-hui chen ||| chi-wei kuan ||| 
2020 ||| attention bidirectional lstm networks based mime speech recognition using semg data. ||| hongyi ye ||| haohong lin ||| zijun song ||| ming zhang ||| ruifen hu ||| nan li ||| guang li ||| 
2021 ||| temporally-aware convolutional block attention module for video text detection. ||| masato fujitake ||| hongpeng ge ||| 
2021 ||| assessing the significance of co-occurring terms in goods and services tax in india using co-occurrence graphs and attention based deep learning models. ||| pankaj dikshit ||| b. chandra ||| 
2018 ||| error backpropagation with attention control to learn imbalanced data for regression. ||| chang hwa lee ||| sang wan lee ||| 
2021 ||| default mode network and attention network in unconscious processing. ||| ying gao ||| 
2021 ||| eeg-based classification of drivers attention using convolutional neural network. ||| fred atilla ||| maryam alimardani ||| 
2020 ||| is autism, attention deficit hyperactivity disorder (adhd) and specific learning disorder linked to impaired emotion recognition in primary school aged children? ||| aliki economides ||| chiara baiano ||| isa zappullo ||| massimiliano conson ||| joulietta kalli-laouri ||| yiannis laouris ||| anna esposito ||| 
2022 ||| gsap: a hybrid gru and self-attention based model for dual medical nlp tasks. ||| huey-ing liu ||| meng-wei chen ||| wei-chun kao ||| yao-wen yeh ||| cheng-xuan yang ||| 
2019 ||| local features in angry faces capture attention in children with autism spectrum disorders exhibiting local-biased perceptual characteristics. ||| tomoko isomura ||| shino ogawa ||| nobuo masataka ||| 
2017 ||| visual attention is captured by task-irrelevant faces, but not by pareidolia faces. ||| atsunori ariga ||| katsuhiko arihara ||| 
2017 ||| impacts of cue reliability and explicit instruction on visual attention. ||| kanji tanaka ||| katsumi watanabe ||| 
2020 ||| an improved attention-based lstm for multi-step dissolved oxygen prediction in water environment. ||| jing bi ||| yongze lin ||| quanxi dong ||| haitao yuan ||| mengchu zhou ||| 
2021 ||| research on digital twin model and visualization of power transformer. ||| delong zhang ||| zhijun ye ||| huipeng chen ||| peng zhou ||| jiliang luo ||| 
2017 ||| salient locations search based on human visual attention: an experimental analysis. ||| wenting hu ||| pei yang ||| xianzhong zhou ||| zhen liu ||| huaxiong li ||| xianjun zhu ||| 
2021 ||| improved polarmask with attention for instance segmentation. ||| yang cao ||| guanjun liu ||| 
2017 ||| rssi-based attention target approach detection for a vehicle reminder system with beaconing devices. ||| yoshito watanabe ||| yozo shoji ||| 
2020 ||| a dual-attention-based neural network for see-through driving decision. ||| fanqi xu ||| jinglin li ||| quan yuan ||| guiyang luo ||| 
2021 ||| a time-efficient and attention-aware deployment strategy for uav networks driven by deep reinforcement learning. ||| jinyue wu ||| xiang cheng ||| xiaoyong ma ||| wei li ||| yi zhou ||| 
2019 ||| multimodal brain image segmentation and analysis with neuromorphic attention-based learning. ||| woo-sup han ||| il song han ||| 
2019 ||| brain tumor segmentation based on attention mechanism and multi-model fusion. ||| xutao guo ||| chushu yang ||| ting ma ||| pengzheng zhou ||| shangfeng lu ||| nan ji ||| deling li ||| tong wang ||| haiyan lv ||| 
2020 ||| brain tumor segmentation using dual-path attention u-net in 3d mri images. ||| wen jun ||| haoxiang xu ||| zhang wang ||| 
2020 ||| attention u-net with dimension-hybridized fast data density functional theory for automatic brain tumor image segmentation. ||| zi-jun su ||| tang-chen chang ||| yen-ling tai ||| shu-jung chang ||| chien-chang chen ||| 
2020 ||| automatic brain tumor segmentation with scale attention network. ||| yading yuan ||| 
2020 ||| multi-threshold attention u-net (mtau) based model for multimodal brain tumor segmentation in mri scans. ||| navchetan awasthi ||| rohit pardasani ||| swati gupta ||| 
2019 ||| brain tumor segmentation using attention-based network in 3d mri images. ||| xiaowei xu ||| wangyuan zhao ||| jun zhao ||| 
2020 ||| multimodal brain image analysis and survival prediction using neuromorphic attention-based neural networks. ||| il song han ||| 
2019 ||| brain tumor segmentation and survival prediction using 3d attention unet. ||| mobarakol islam ||| v. s. vibashan ||| v. jeya maria jose ||| navodini wijethilake ||| uppal utkarsh ||| hongliang ren ||| 
2020 ||| brain tumor segmentation network using attention-based fusion and spatial relationship constraint. ||| chenyu liu ||| wangbin ding ||| lei li ||| zhen zhang ||| chenhao pei ||| liqin huang ||| xiahai zhuang ||| 
2020 ||| a deep supervised u-attention net for pixel-wise brain tumor segmentation. ||| jiahua xu ||| wai po kevin teng ||| xiong jun wang ||| andreas n ||| rnberger ||| 
2020 ||| a two-stage cascade model with variational autoencoders and attention gates for mri brain tumor segmentation. ||| chenggang lyu ||| hai shu ||| 
2021 ||| evaluation of the bubble view metaphor for the crowdsourcing study of visual attention deployment in tone-mapped images. ||| waqas ellahi ||| toinon vigier ||| patrick le callet ||| 
2018 ||| viewport-aware omnidirectional video streaming using visual attention and dynamic tiles. ||| cagri ozcinar ||| juli ||| n cabrera ||| aljosa smolic ||| 
2020 ||| group-wise attention fusion network for choroid segmentation in oct images. ||| xuena cheng ||| xinjian chen ||| shuanglang feng ||| weifang zhu ||| dehui xiang ||| qiuying chen ||| xun xu ||| ying fan ||| fei shi ||| 
2020 ||| automatic epicardial fat segmentation in cardiac ct imaging using 3d deep attention u-net. ||| xiuxiu he ||| bangjun guo ||| yang lei ||| tonghe wang ||| tian liu ||| walter j. curran ||| longjiang zhang ||| xiaofeng yang ||| 
2020 ||| ganet: group attention network for diabetic retinopathy image segmentation. ||| lei ye ||| weifang zhu ||| shuanglang feng ||| xinjian chen ||| 
2020 ||| automated retinopathy of prematurity screening using deep neural network with attention mechanism. ||| yuanyuan peng ||| weifang zhu ||| feng chen ||| daoman xiang ||| xinjian chen ||| 
2020 ||| attention-guided channel to pixel convolution network for retinal layer segmentation with choroidal neovascularization. ||| xiaoling yang ||| xinjian chen ||| dehui xiang ||| 
2020 ||| automatic lung segmentation in low-dose ct image with contrastive attention module. ||| changxing yang ||| haihong tian ||| dehui xiang ||| fei shi ||| weifang zhu ||| xinjian chen ||| 
2019 ||| coronary calcium detection using 3d attention identical dual deep network based on weakly supervised learning. ||| yuankai huo ||| james g. terry ||| jiachen wang ||| vishwesh nath ||| camilo bermudez ||| shunxing bao ||| prasanna parvathaneni ||| j. jeffrey carr ||| bennett a. landman ||| 
2020 ||| super-resolution magnetic resonance imaging reconstruction using deep attention networks. ||| xiuxiu he ||| yang lei ||| yabo fu ||| hui mao ||| walter j. curran ||| tian liu ||| xiaofeng yang ||| 
2020 ||| attention multi-scale network for pigment epithelial detachment segmentation in oct images. ||| dengsen bao ||| xuena cheng ||| weifang zhu ||| fei shi ||| xinjian chen ||| 
2020 ||| enhancing infarct segmentation performance using domain-specific attention in acute ischemic stroke. ||| manikanda krishnan v. ||| srinivasa rao kundeti ||| arun h. shastry ||| shankar prasad gorthi ||| 
2020 ||| an attention model for mashup tag recommendation. ||| kenneth k. fletcher ||| 
2019 ||| neural-attention multi-instance learning for predicting user demographics from highly noisy tweets. ||| qing chen ||| mingxuan sun ||| jian zhang ||| 
2021 ||| implicit visual attention feedback system for wikipedia users. ||| neeru dubey ||| amit arjun verma ||| s. r. s. iyengar ||| simran setia ||| 
2019 ||| interpretable multi-task learning for product quality prediction with attention mechanism. ||| cheng-han yeh ||| yao-chung fan ||| wen-chih peng ||| 
2021 ||| gallat: a spatiotemporal graph attention network for passenger demand prediction. ||| yuandong wang ||| hongzhi yin ||| tong chen ||| chunyang liu ||| ben wang ||| tianyu wo ||| jie xu ||| 
2019 ||| context-aware co-attention neural network for service recommendations. ||| lei li ||| ruihai dong ||| li chen ||| 
2021 ||| purchase intent forecasting with convolutional hierarchical transformer networks. ||| chao huang ||| jiashu zhao ||| dawei yin ||| 
2019 ||| context-aware attention-based data augmentation for poi recommendation. ||| yang li ||| yadan luo ||| zheng zhang ||| shazia w. sadiq ||| peng cui ||| 
2020 ||| san : scale-space attention networks. ||| yash garg ||| k. sel ||| uk candan ||| maria luisa sapino ||| 
2021 ||| variational self-attention network for sequential recommendation. ||| jing zhao ||| pengpeng zhao ||| lei zhao ||| yanchi liu ||| victor s. sheng ||| xiaofang zhou ||| 
2019 ||| air: attentional intention-aware recommender systems. ||| tong chen ||| hongzhi yin ||| hongxu chen ||| rui yan ||| quoc viet hung nguyen ||| xue li ||| 
2021 ||| structure-aware parameter-free group query via heterogeneous information network transformer. ||| hsi-wen chen ||| hong-han shuai ||| de-nian yang ||| wang-chien lee ||| chuan shi ||| philip s. yu ||| ming-syan chen ||| 
2018 ||| joint bottleneck feature and attention model for speech recognition. ||| long xingyan ||| qu dan ||| 
2021 ||| integrating transformers and knowledge graphs for twitter stance detection. ||| thomas hikaru clark ||| costanza conforti ||| fangyu liu ||| zaiqiao meng ||| ehsan shareghi ||| nigel collier ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| joan plepi ||| lucie flek ||| 
2020 ||| tatl at wnut-2020 task 2: a transformer-based baseline system for identification of informative covid-19 english tweets. ||| anh tuan nguyen ||| 
2020 ||| upennhlp at wnut-2020 task 2 : transformer models for classification of covid19 posts on twitter. ||| arjun magge ||| varad pimpalkhute ||| divya rallapalli ||| david siguenza ||| graciela gonzalez-hernandez ||| 
2020 ||| punctuation restoration using transformer models for high-and low-resource languages. ||| tanvirul alam ||| akib khan ||| firoj alam ||| 
2021 ||| does it happen? multi-hop path structures for event factuality prediction with graph transformer networks. ||| duong le ||| thien huu nguyen ||| 
2020 ||| siva at wnut-2020 task 2: fine-tuning transformer neural networks for identification of informative covid-19 tweets. ||| siva sai ||| 
2021 ||| sequence-to-sequence lexical normalization with multilingual transformers. ||| ana-maria bucur ||| adrian cosma ||| liviu p. dinu ||| 
2019 ||| weakly supervised attention networks for fine-grained opinion mining and public health. ||| giannis karamanolakis ||| daniel hsu ||| luis gravano ||| 
2019 ||| geolocation with attention-based multitask learning models. ||| tommaso fornaciari ||| dirk hovy ||| 
2021 ||| description-based label attention classifier for explainable icd-9 classification. ||| malte feucht ||| zhiliang wu ||| sophia althammer ||| volker tresp ||| 
2020 ||| infominer at wnut-2020 task 2: transformer-based covid-19 informative tweet extraction. ||| hansi hettiarachchi ||| tharindu ranasinghe ||| 
2020 ||| edinburghnlp at wnut-2020 task 2: leveraging transformers with generalized augmentation for identifying informativeness in covid-19 tweets. ||| nickil maveli ||| 
2021 ||| aspect level sentiment classification with semantic distance attention networks. ||| xiaohong cai ||| hui cao ||| jin-gang ma ||| ming li ||| xuqiang zhuang ||| 
2018 ||| insertion-loss optimization of transformer-based matching networks for mm-wave applications. ||| david bierbuesse ||| renato negra ||| 
2019 ||| designing at millimeter-wave: lessons from a triple coil variable transformer. ||| alok sethi ||| rehman akbar ||| janne p. aikio ||| rana azhar shaheen ||| aarno p ||| rssinen ||| timo rahkonen ||| 
2020 ||| predicting of air pollutant concentrations based on spatio-temporal attention convolutional lstm networks. ||| peng jiang ||| igor v. bychkov ||| jun liu ||| alexei e. hmelnov ||| 
2020 ||| mild cognitive impairment classification convolutional neural network with attention mechanism. ||| jianing wei ||| wendong xiao ||| sen zhang ||| pengyun wang ||| 
2018 ||| multi-column spatial transformer convolution neural network for traffic sign recognition. ||| jin zhang ||| shukai duan ||| lidan wang ||| xianli zou ||| 
2019 ||| bidirectional gated recurrent unit networks for relation classification with multiple attentions and semantic information. ||| bixiao meng ||| baomin xu ||| erjing zhou ||| shuangyuan yu ||| hongfeng yin ||| 
2019 ||| graph convolution and self attention based non-maximum suppression. ||| zhe qiu ||| xiaodong gu ||| 
2019 ||| automatically generate hymns using variational attention models. ||| han k. cao ||| duyen t. ly ||| duy m. h. nguyen ||| binh t. nguyen ||| 
2019 ||| dynamic graph cnn with attention module for 3d hand pose estimation. ||| xu jiang ||| xiaohong ma ||| 
2021 ||| ambd: attention based multi-block deep learning model for warehouse dwell time prediction. ||| xingyi lv ||| wei zhao ||| jiali mao ||| ye guo ||| aoying zhou ||| 
2020 ||| nsa-net: a netflow sequence attention network for virtual private network traffic detection. ||| peipei fu ||| chang liu ||| qingya yang ||| zhenzhen li ||| gaopeng gou ||| gang xiong ||| zhen li ||| 
2020 ||| adfr: an attention-based deep learning model for flight ranking. ||| yuan yi ||| jian cao ||| yudong tan ||| qiangqiang nie ||| xiaoxi lu ||| 
2017 ||| connecting targets to tweets: semantic attention-based model for target-specific stance detection. ||| yiwei zhou ||| alexandra i. cristea ||| lei shi ||| 
2021 ||| enhancing both local and global entity linking models with attention. ||| jinliang li ||| haoyu liu ||| yulong zhang ||| li zhang ||| qiang yang ||| jianfeng qu ||| zhixu li ||| 
2018 ||| dual: a deep unified attention model with latent relation representations for fake news detection. ||| manqing dong ||| lina yao ||| xianzhi wang ||| boualem benatallah ||| quan z. sheng ||| hao huang ||| 
2019 ||| multiple interaction attention model for open-world knowledge graph completion. ||| chenpeng fu ||| zhixu li ||| qiang yang ||| zhigang chen ||| junhua fang ||| pengpeng zhao ||| jiajie xu ||| 
2021 ||| cross-modal attention network with orthogonal latent memory for rumor detection. ||| zekai wu ||| jiaxing chen ||| zhenguo yang ||| haoran xie ||| fu lee wang ||| wenyin liu ||| 
2018 ||| combining contextual information by self-attention mechanism in convolutional neural networks for text classification. ||| xin wu ||| yi cai ||| qing li ||| jingyun xu ||| ho-fung leung ||| 
2021 ||| multi-task learning with personalized transformer for review recommendation. ||| haiming wang ||| wei liu ||| jian yin ||| 
2021 ||| interactive pose attention network for human pose transfer. ||| di luo ||| guipeng zhang ||| zhenguo yang ||| minzheng yuan ||| tao tao ||| liangliang xu ||| qing li ||| wenyin liu ||| 
2020 ||| attention-based high-order feature interactions to enhance the recommender system for web-based knowledge-sharing service. ||| jiayin lin ||| geng sun ||| jun shen ||| tingru cui ||| david pritchard ||| dongming xu ||| li li ||| wei wei ||| ghassan beydoun ||| shiping chen ||| 
2021 ||| mgsan: a multi-granularity self-attention network for next poi recommendation. ||| yepeng li ||| xuefeng xian ||| pengpeng zhao ||| yanchi liu ||| victor s. sheng ||| 
2019 ||| entity disambiguation based on parse tree neighbours on graph attention network. ||| kexuan xin ||| wen hua ||| yu liu ||| xiaofang zhou ||| 
2019 ||| memory-augmented attention network for sequential recommendation. ||| cheng hu ||| peijian he ||| chaofeng sha ||| junyu niu ||| 
2019 ||| pattern filtering attention for distant supervised relation extraction via online clustering. ||| min peng ||| qingwen liao ||| weilong hu ||| gang tian ||| hua wang ||| yanchun zhang ||| 
2021 ||| rau: an interpretable automatic infection diagnosis of covid-19 pneumonia with residual attention u-net. ||| xiaocong chen ||| lina yao ||| yu zhang ||| 
2021 ||| efficient feature interactions learning with gated attention transformer. ||| chao long ||| yanmin zhu ||| haobing liu ||| jiadi yu ||| 
2021 ||| jurassic mark: inattentional blindness for a datasaurus reveals that visualizations are explored, not seen. ||| tal boger ||| steven b. most ||| steven l. franconeri ||| 
2019 ||| sanvis: visual analytics for understanding self-attention networks. ||| cheonbok park ||| jaegul choo ||| inyoup na ||| yongjang jo ||| sungbok shin ||| jaehyo yoo ||| bum chul kwon ||| jian zhao ||| hyungjong noh ||| yeonsoo lee ||| 
2021 ||| named entity recognition in cyber threat intelligence using transformer-based models. ||| pavlos evangelatos ||| christos iliou ||| thanassis mavropoulos ||| konstantinos apostolou ||| theodora tsikrika ||| stefanos vrochidis ||| ioannis kompatsiaris ||| 
2021 ||| ai-assisted stanford classification of aortic dissection in ct imaging using volumetric 3d cnn with external guided attention. ||| cheng-fu liou ||| li-ting huang ||| paul kuo ||| chien-kuo wang ||| jiun-in guo ||| 
2017 ||| measurement of energy transmission efficiency of transcutaneous energy transformer in nacl solution for ventricular assist devices by reducing common-mode current in the range of 200-1500 khz. ||| tadashi kaga ||| kenji shiba ||| 
2021 ||| video based heart rate extraction using skin roi segmentation and attention cnn. ||| hongbo guo ||| yang zhao ||| yong lian ||| 
2021 ||| residual learning attention cnn for motion intention recognition based on eeg data. ||| ting wang ||| jingna mao ||| ruozhou xiao ||| wuqi wang ||| guangxin ding ||| zhiwei zhang ||| 
2021 ||| attention state classification with in-ear eeg. ||| akshay paul ||| gopabandhu hota ||| behnam khaleghi ||| yuchen xu ||| tajana rosing ||| gert cauwenberghs ||| 
2021 ||| light field visual attention prediction using fourier disparity layers. ||| ailbhe gill ||| mikael le pendu ||| martin alain ||| emin zerman ||| aljosa smolic ||| 
2017 ||| fine-grained vehicle classificationusing deep residual networks with multiscale attention windows. ||| sina ghassemi ||| attilio fiandrotti ||| enrico magli ||| gianluca francini ||| 
2021 ||| a machine-learning framework to predict tmo preference based on image and visual attention features. ||| waqas ellahi ||| toinon vigier ||| patrick le callet ||| 
2019 ||| one-shot video object segmentation using attention transfer. ||| omit chanda ||| yang wang ||| 
2021 ||| dual attention network for heart rate and respiratory rate estimation. ||| yuzhuo ren ||| braeden syrnyk ||| niranjan avadhanam ||| 
2019 ||| accurate small bowel lesions detection in wireless capsule endoscopy images using deep recurrent attention neural network. ||| r ||| mi vall ||| e ||| astrid de maissin ||| antoine coutrot ||| nicolas normand ||| arnaud bourreille ||| harold mouch ||| re ||| 
2020 ||| profiling actions for sport video summarization: an attention signal analysis. ||| melissa sanabria ||| fr ||| d ||| ric precioso ||| thomas menguy ||| 
2019 ||| from speech to facial activity: towards cross-modal sequence-to-sequence attention networks. ||| lukas stappen ||| vincent karas ||| nicholas cummins ||| fabien ringeval ||| klaus r. scherer ||| bj ||| rn w. schuller ||| 
2020 ||| multianet: a multi-attention network for defocus blur detection. ||| zeyu jiang ||| xun xu ||| chao zhang ||| ce zhu ||| 
2021 ||| spatial cross-attention rgb-d fusion module for object detection. ||| shangyin gao ||| lev markhasin ||| bi wang ||| 
2021 ||| attention-based stylisation for exemplar image colourisation. ||| marc g ||| rriz blanch ||| issa khalifeh ||| noel e. o'connor ||| marta mrak ||| 
2020 ||| station correlation attention learning for data-driven bike sharing system usage prediction. ||| xi yang ||| suining he ||| huiqun huang ||| 
2020 ||| gsan: graph self-attention network for interaction measurement in autonomous driving. ||| luyao ye ||| zezhong wang ||| xinhong chen ||| jianping wang ||| kui wu ||| kejie lu ||| 
2020 ||| the problem of tracking the center of attention in eye tracking systems. ||| marina boronenko ||| vladimir zelensky ||| oksana isaeva ||| elizaveta kiseleva ||| 
2019 ||| vr experience from data science point of view: how to measure inter-subject dependence in visual attention and spatial behavior. ||| pawel kobylinski ||| grzegorz pochwatko ||| cezary biele ||| 
2020 ||| the car as a transformer. ||| jeremy aston ||| rui pedro freire ||| 
2021 ||| do syntax trees help pre-trained transformers extract information? ||| devendra singh sachan ||| yuhao zhang ||| peng qi ||| william l. hamilton ||| 
2021 ||| how certain is your transformer? ||| artem shelmanov ||| evgenii tsymbalov ||| dmitry puzyrev ||| kirill fedyanin ||| alexander panchenko ||| maxim panov ||| 
2021 ||| civil rephrases of toxic texts with self-supervised transformers. ||| leo laugier ||| john pavlopoulos ||| jeffrey sorensen ||| lucas dixon ||| 
2021 ||| trankit: a light-weight transformer-based toolkit for multilingual natural language processing. ||| minh van nguyen ||| viet dac lai ||| amir pouran ben veyseh ||| thien huu nguyen ||| 
2021 ||| telling bert's full story: from local attention to global aggregation. ||| damian pascual ||| gino brunner ||| roger wattenhofer ||| 
2021 ||| interpret: an interactive visualization tool for interpreting transformers. ||| vasudev lal ||| arden ma ||| estelle aflalo ||| phillip howard ||| ana simoes ||| daniel korat ||| oren pereg ||| gadi singer ||| moshe wasserblat ||| 
2021 ||| attention can reflect syntactic structure (if you let it). ||| vinit ravishankar ||| artur kulmizev ||| mostafa abdou ||| anders s ||| gaard ||| joakim nivre ||| 
2017 ||| large-scale categorization of japanese product titles using neural attention models. ||| yandi xia ||| aaron levine ||| pradipto das ||| giuseppe di fabbrizio ||| keiji shinzato ||| ankur datta ||| 
2021 ||| multi-split reversible transformers can enhance neural machine translation. ||| yuekai zhao ||| shuchang zhou ||| zhihua zhang ||| 
2021 ||| conversational question answering over knowledge graphs with transformer and graph attention networks. ||| endri kacupaj ||| joan plepi ||| kuldeep singh ||| harsh thakkar ||| jens lehmann ||| maria maleshkova ||| 
2021 ||| enriching non-autoregressive transformer with syntactic and semantic structures for neural machine translation. ||| ye liu ||| yao wan ||| jianguo zhang ||| wenting zhao ||| philip s. yu ||| 
2017 ||| attention modeling for targeted sentiment. ||| jiangming liu ||| yue zhang ||| 
2021 ||| attention-based relational graph convolutional network for target-oriented opinion words extraction. ||| junfeng jiang ||| an wang ||| akiko aizawa ||| 
2021 ||| t2ner: transformers based transfer learning framework for named entity recognition. ||| saadullah amin ||| guenter neumann ||| 
2021 ||| bert meets shapley: extending shap explanations to transformer-based classifiers. ||| enja kokalj ||| blaz skrlj ||| nada lavrac ||| senja pollak ||| marko robnik-sikonja ||| 
2017 ||| exploring different dimensions of attention for uncertainty detection. ||| heike adel ||| hinrich sch ||| tze ||| 
2021 ||| exploring neural language models via analysis of local and global self-attention spaces. ||| blaz skrlj ||| shane sheehan ||| nika erzen ||| marko robnik-sikonja ||| saturnino luz ||| senja pollak ||| 
2021 ||| t-ner: an all-round python library for transformer-based named entity recognition. ||| asahi ushio ||| jos |||  camacho-collados ||| 
2021 ||| dynamic graph transformer for implicit tag recognition. ||| yi-ting liou ||| chung-chi chen ||| hen-hsen huang ||| hsin-hsi chen ||| 
2021 ||| applying the transformer to character-level transduction. ||| shijie wu ||| ryan cotterell ||| mans hulden ||| 
2021 ||| have attention heads in bert learned constituency grammar? ||| ziyang luo ||| 
2021 ||| "killing me" is not a spoiler: spoiler detection model using graph neural networks with dependency relation-aware attention mechanism. ||| buru chang ||| inggeol lee ||| hyunjae kim ||| jaewoo kang ||| 
2017 ||| neural machine translation with recurrent attention modeling. ||| zichao yang ||| zhiting hu ||| yuntian deng ||| chris dyer ||| alexander j. smola ||| 
2017 ||| structural attention neural networks for improved sentiment analysis. ||| filippos kokkinos ||| alexandros potamianos ||| 
2021 ||| measuring and improving faithfulness of attention in neural machine translation. ||| pooya moradi ||| nishant kambhatla ||| anoop sarkar ||| 
2021 ||| syntax-bert: improving pre-trained transformers with syntax trees. ||| jiangang bai ||| yujing wang ||| yiren chen ||| yaming yang ||| jing bai ||| jing yu ||| yunhai tong ||| 
2021 ||| benchmarking a transformer-free model for ad-hoc retrieval. ||| tiago almeida ||| s ||| rgio matos ||| 
2021 ||| globalizing bert-based transformer architectures for long document summarization. ||| quentin grail ||| julien perez ||| ric gaussier ||| 
2021 ||| grit: generative role-filler transformers for document-level event entity extraction. ||| xinya du ||| alexander m. rush ||| claire cardie ||| 
2021 ||| enconter: entity constrained progressive sequence generation via insertion-based transformer. ||| lee-hsun hsieh ||| yang-yin lee ||| ee-peng lim ||| 
2021 ||| proformer: towards on-device lsh projection based transformers. ||| chinnadhurai sankar ||| sujith ravi ||| zornitsa kozareva ||| 
2021 ||| changing the mind of transformers for topically-controllable language generation. ||| haw-shiuan chang ||| jiaming yuan ||| mohit iyyer ||| andrew mccallum ||| 
2021 ||| bert prescriptions to avoid unwanted headaches: a comparison of transformer architectures for adverse drug event detection. ||| beatrice portelli ||| edoardo lenzi ||| emmanuele chersoni ||| giuseppe serra ||| enrico santus ||| 
2020 ||| malware classification using attention-based transductive learning network. ||| liting deng ||| hui wen ||| mingfeng xin ||| yue sun ||| limin sun ||| hongsong zhu ||| 
2019 ||| spatial attention mechanism for weakly supervised fire and traffic accident scene classification. ||| md. moniruzzaman ||| zhaozheng yin ||| ruwen qin ||| 
2021 ||| mldt: multi-task learning with denoising transformer for gait identity and emotion recognition. ||| weijie sheng ||| xiaoyan lu ||| xinde li ||| 
2021 ||| deep-learning-based diagnosis of cassava leaf diseases using vision transformer. ||| lipeng zhuang ||| 
2020 ||| malware classification on imbalanced data through self-attention. ||| yu ding ||| shupeng wang ||| jian xing ||| xiaoyu zhang ||| zisen oi ||| ge fu ||| qian qiang ||| haoliang sun ||| jianyu zhang ||| 
2021 ||| gea-net: global embedded attention neural network for image classification. ||| yajie wang ||| zhiyang wan ||| zhao pei ||| chengcai leng ||| yuli chen ||| 
2020 ||| pyramid pooling channel attention network for esophageal tissue segmentation on oct images. ||| deyin li ||| miao zhang ||| wei shi ||| huimin zhang ||| duoduo wang ||| lirong wang ||| 
2019 ||| phishing url detection via cnn and attention-based hierarchical rnn. ||| yongjie huang ||| qiping yang ||| jinghui qin ||| wushao wen ||| 
2021 ||| simple online unmanned aerial vehicle tracking with transformer. ||| yang liu ||| ershen wang ||| song xu ||| zhi wang ||| meizhi liu ||| wansen shu ||| 
2020 ||| exploit internal structural information for iot malware detection based on hierarchical transformer model. ||| xiaohui hu ||| rui sun ||| kejia xu ||| yongzheng zhang ||| peng chang ||| 
2020 ||| hypergraph attention networks. ||| chaofan chen ||| zelei cheng ||| zuotian li ||| manyi wang ||| 
2019 ||| solving a tool-based interaction task using deep reinforcement learning with visual attention. ||| sascha fleer ||| helge j. ritter ||| 
2020 ||| can nao robot influence the eye gaze and joint attention of mentally impaired young adults? ||| ana freire ||| ant ||| nio valente ||| v ||| tor filipe ||| 
2021 ||| turbotransformers: an efficient gpu serving system for transformer models. ||| jiarui fang ||| yang yu ||| chengduo zhao ||| jie zhou ||| 
2021 ||| a transformer architecture for stress detection from ecg. ||| behnam behinaein ||| anubhav bhatti ||| dirk rodenburg ||| paul hungler ||| ali etemad ||| 
2017 ||| wearable social prosthetics: supporting joint attention during communication with artificial eyes. ||| hirotaka osawa ||| takeomi goto ||| bohao wang ||| 
2021 ||| a pilot study using covert visuospatial attention as an eeg-based brain computer interface to enhance ar interaction. ||| nataliya kosmyna ||| chi-yun hu ||| yujie wang ||| qiuxuan wu ||| cassandra scheirer ||| pattie maes ||| 
2020 ||| generative text steganography based on lstm network and attention mechanism with keywords. ||| huixian kang ||| hanzhou wu ||| xinpeng zhang ||| 
2019 ||| attention based residual-time delay neural network for indian language identification. ||| tirusha mandava ||| anil kumar vuppala ||| 
2019 ||| multi-head self-attention networks for language identification. ||| ravi kumar vuddagiri ||| tirusha mandava ||| hari krishna vydana ||| anil kumar vuppala ||| 
2021 ||| evaluation of the transformer architecture for univariate time series forecasting. ||| pedro lara-ben ||| tez ||| luis gallego-ledesma ||| manuel carranza-garc ||| a ||| jos |||  mar ||| a luna-romera ||| 
2019 ||| a novel three-stage power electronic transformer for ac/dc conversion. ||| hang zhang ||| yaohua li ||| zixin li ||| fanqiang gao ||| cong zhao ||| yujie hu ||| ping wang ||| 
2019 ||| a multi path feedforward control of load current for three-phase inverter with transformer. ||| lin bowei ||| peng li ||| xu ke ||| 
2019 ||| a global redundancy scheme for medium-voltage modular multilevel converter based solid-state transformer. ||| jiajie zang ||| jiacheng wang ||| jianwen zhang ||| jianqiao zhou ||| 
2017 ||| asymmetrical two-phase induction motor speed controlled by multilevel inverter employing cascaded transformers. ||| vittaya tipsuwanporn ||| khomkrit kaenthong ||| arjin numsomran ||| anuchit charean ||| winyu sawaengsinkasikit ||| 
2020 ||| secondary-side center-tapped transformer structure with one-turn secondary coils integrating rectifier for reducing copper loss of forward converter. ||| tomohide shirakawa ||| kazuhiro umetani ||| eiji hiraki ||| wilmar martinez ||| 
2021 ||| current and voltage model predictive control for a three-stage smart transformer. ||| giampaolo buticchi ||| luca tarisciotti ||| pat wheeler ||| shuai shao ||| linglin chen ||| 
2021 ||| disturbance property of high-frequency transformer model for photovoltaic applications. ||| duc-thanh do ||| holger hirsch ||| 
2017 ||| series transistor array-based linear ac regulator: role of multiple buck-boost transformers in efficiency improvements. ||| priyanwada nimesha wijesooriya ||| nihal kularatna ||| jayathu fernando ||| d. alistair steyn-ross ||| 
2019 ||| control of solid state transformer based on modular multilevel converters with interconnecting dual active bridges. ||| marcelo a. p ||| rez ||| felipe ruiz ||| jos |||  r. espinoza ||| mariusz malinowski ||| 
2019 ||| improving the diagnosis of incipient faults in power transformers using dissolved gas analysis and multi layer perceptron. ||| s. nkosi ||| pitshou n. bokoro ||| 
2020 ||| an orthogonal decoupled transformer design for inductive power transfer applications. ||| zhuhaobo zhang ||| shaoting zheng ||| dehong xu ||| philip t. krein ||| hao ma ||| 
2019 ||| advanced power management algorithm in dc microgrid subsystem controlled by smart transformer. ||| adam milczarek ||| michal gl ||| wczyk ||| sebastian stynski ||| 
2019 ||| novel zcs transformerless high gain dc-dc converters for renewable energy conversion systems. ||| v. v. subrahmanya kumar bhajana ||| pavel dr ||| bek ||| 
2021 ||| disturbance rejection and harmonic mitigation for solid state transformer through passivity based control. ||| m. monika ||| ragini v. meshram ||| sushama wagh ||| 
2021 ||| a novel interleaved transformerless ultra-high step-up dc/dc converter. ||| sung-pei yang ||| shin-ju chen ||| chao-ming huang ||| jia-yao lin ||| 
2019 ||| implementation of fpga and dsp combined algorithms for modular single-phase matrix converter with medium frequency transformer for traction drive application. ||| bedrich bedn ||| r ||| pavel dr ||| bek ||| martin pittermann ||| 
2021 ||| unsupervised person re-identification with transformer-based network for intelligent surveillance systems. ||| ge cao ||| kang-hyun jo ||| 
2021 ||| vt-adl: a vision transformer network for image anomaly detection and localization. ||| pankaj mishra ||| riccardo verk ||| daniele fornasier ||| claudio piciarelli ||| gian luca foresti ||| 
2020 ||| quadratic dc/dc converter with autotransformer at the output side. ||| felix a. himmelstoss ||| helmut l. votzi ||| michael windisch ||| 
2021 ||| a novel framework of cnn for image super-resolution based on attention module. ||| jiahao tan ||| hiroaki mukaidani ||| 
2021 ||| modeling of cross-circulating currents in a mmc with parallel connected submodules in solid state transformers. ||| felipe ruiz allende ||| marcelo a. perez ||| freddy flores-bahamonde ||| mariusz malinowski ||| 
2017 ||| enhancing the provision of ancillary services from storage systems using smart transformer and smart meters. ||| fabrizio sossan ||| konstantina christakou ||| mario paolone ||| xiang gao ||| marco liserre ||| 
2021 ||| analysis and validation of variable transformers. ||| camilo suarez ||| diego bernal cobaleda ||| wilmar martinez ||| 
2017 ||| design of transformer load monitoring systems by utilizing smart meter data. ||| chun-lien su ||| yu-chi pu ||| hai-ming ching ||| chao-lin kuo ||| jheng-he kuo ||| 
2017 ||| study on operating performance of transformer and scaling model with dc bias. ||| jiantao sun ||| jinzhong li ||| ke wang ||| chao wu ||| yuzhou qiu ||| xinru yu ||| 
2018 ||| a smart transformer-rectifier unit for the more electric aircraft. ||| giampaolo buticchi ||| chris gerada ||| youngjong ko ||| marco liserre ||| 
2019 ||| multi-winding transformer based high resolution power flow controller. ||| diksha kumari ||| sumit k. chattopadhyay ||| ashu verma ||| 
2019 ||| on design of 60 ghz solid-state transformers modeled as coupled bitter coils. ||| igor m. filanovsky ||| 
2021 ||| an efficient implementation of fpga-based object detection using multi-scale attention. ||| masanori furuta ||| koichiro ban ||| daisuke kobayashi ||| tomoyuki shibata ||| 
2018 ||| impact assessment of electric vehicle charging on distribution transformers including state-of-charge. ||| dima alame ||| maher azzouz ||| narayan c. kar ||| 
2018 ||| a 2: 8 ghz to 12: 8 ghz uwb lna using transformer wide-band input matching for ir-uwb radar applications. ||| xin an ||| jens wagner ||| frank ellinger ||| 
2017 ||| a wideband simplified transformer-based vco with digital amplitude calibration. ||| xinchi gao ||| licheng xu ||| jing jin ||| naifeng jing ||| jianjun zhou ||| 
2020 ||| iot device auto-tagging using transformers. ||| hyuncheol jang ||| soobin choi ||| eunhye kwon ||| chongwook kwon ||| 
2018 ||| jellys: towards a videogame that trains rhythm and visual attention for dyslexia. ||| mikel ostiz-blanco ||| marie lallier ||| sergi grau carri ||| n ||| luz rello ||| jeffrey p. bigham ||| manuel carreiras ||| 
2020 ||| transformer puf : a highly flexible configurable ro puf based on fpga. ||| ziwei wei ||| yijun cui ||| yunpeng chen ||| chenghua wang ||| chongyan gu ||| weiqiang liu ||| 
2018 ||| simulation analysis of railway co-phase power supply system based on inequilateral scott connection transformer. ||| liang min ||| xiaochun mou ||| 
2019 ||| idmn: a two-pass attention mechanism in dynamic memory network. ||| zheng zhou ||| xin kang ||| shun nishide ||| fuji ren ||| 
2020 ||| a cross-cultural study on information architecture: culture differences on attention allocation to web components. ||| gao-ming tang ||| hsin-yuan hu ||| shih-yi chen ||| wei jeng ||| 
2021 ||| attracting attention in online health forums: studies of r/alzheimers and r/dementia. ||| olivia a. flynn ||| abinav murugadass ||| lu xiao ||| 
2022 ||| exploiting transformer-based multitask learning for the detection of media bias in news articles. ||| timo spinde ||| jan-david krieger ||| terry ruas ||| jelena mitrovic ||| franz g ||| tz-hahn ||| akiko aizawa ||| bela gipp ||| 
2021 ||| attention: to better stand on the shoulders of giants. ||| sha yuan ||| zhou shao ||| yu zhang ||| tong xiao ||| yifan wang ||| 
2021 ||| syntax-aware transformers for neural machine translation: the case of text to sign gloss translation. ||| santiago egea g ||| mez ||| euan mcgill ||| horacio saggion ||| 
2020 ||| self-attention network for next poi recommendation. ||| jiacheng ni ||| pengpeng zhao ||| jiajie xu ||| junhua fang ||| zhixu li ||| xuefeng xian ||| zhiming cui ||| victor s. sheng ||| 
2021 ||| co-authorship prediction based on temporal graph attention. ||| dongdong jin ||| peng cheng ||| xuemin lin ||| lei chen ||| 
2021 ||| grham: towards group recommendation using hierarchical attention mechanism. ||| nanzhou lin ||| juntao zhang ||| xiandi yang ||| wei song ||| zhiyong peng ||| 
2018 ||| improving clinical named entity recognition with global neural attention. ||| guohai xu ||| chengyu wang ||| xiaofeng he ||| 
2021 ||| sqkt: a student attention-based and question-aware model for knowledge tracing. ||| qize xie ||| liping wang ||| peidong song ||| xuemin lin ||| 
2020 ||| aligning users across social networks via intra and inter attentions. ||| zhichao huang ||| xutao li ||| yunming ye ||| 
2020 ||| fhan: feature-level hierarchical attention network for group event recommendation. ||| guoqiong liao ||| xiaobin deng ||| xiaomei huang ||| changxuan wan ||| 
2020 ||| predicting human mobility with self-attention and feature interaction. ||| jingang jiang ||| shuo tao ||| defu lian ||| zhenya huang ||| enhong chen ||| 
2020 ||| turn-level recurrence self-attention for joint dialogue action prediction and response generation. ||| yanxin tan ||| zhonghong ou ||| kemeng liu ||| yanan shi ||| meina song ||| 
2019 ||| transformer and multi-scale convolution for target-oriented sentiment analysis. ||| yinxu pan ||| binheng song ||| ningqi luo ||| xiaojun chen ||| hengbin cui ||| 
2019 ||| medical treatment migration prediction in healthcare via attention-based bidirectional gru. ||| lin cheng ||| yongjian ren ||| kun zhang ||| yuliang shi ||| 
2021 ||| self-supervised learning for semantic sentence matching with dense transformer inference network. ||| fengying yu ||| jianzong wang ||| dewei tao ||| ning cheng ||| jing xiao ||| 
2020 ||| knowledge graph attention network enhanced sequential recommendation. ||| xingwei zhu ||| pengpeng zhao ||| jiajie xu ||| junhua fang ||| lei zhao ||| xuefeng xian ||| zhiming cui ||| victor s. sheng ||| 
2021 ||| multi-interest network based on double attention for click-through rate prediction. ||| xiaoling xia ||| wenjian fang ||| xiujin shi ||| 
2020 ||| global and local attention embedding network for few-shot fine-grained image classification. ||| jiayuan hu ||| chung-ming own ||| wenyuan tao ||| 
2020 ||| graph attentive network for region recommendation with poi- and roi-level attention. ||| hengpeng xu ||| jinmao wei ||| zhenglu yang ||| jun wang ||| 
2019 ||| enhancing joint entity and relation extraction with language modeling and hierarchical attention. ||| renjun chi ||| bin wu ||| linmei hu ||| yunlei zhang ||| 
2019 ||| boundary detector encoder and decoder with soft attention for video captioning. ||| tangming chen ||| qike zhao ||| jingkuan song ||| 
2018 ||| attention-based recurrent neural network for sequence labeling. ||| bofang li ||| tao liu ||| zhe zhao ||| xiaoyong du ||| 
2021 ||| a graph attention network model for gmv forecast on online shopping festival. ||| qianyu yu ||| shuo yang ||| zhiqiang zhang ||| ya-lin zhang ||| binbin hu ||| ziqi liu ||| kai huang ||| xingyu zhong ||| jun zhou ||| yanming fang ||| 
2020 ||| densely-connected transformer with co-attentive information for matching text sequences. ||| minxu zhang ||| yingxia shao ||| kai lei ||| yuesheng zhu ||| bin cui ||| 
2020 ||| natural answer generation via graph transformer. ||| xiangyu li ||| sen hu ||| lei zou ||| 
2018 ||| neural indexes of attention extracted from eeg correlate with elderly reaction time in response to an attentional task. ||| fatemeh fahimi ||| wooi boon goh ||| tih shih lee ||| cuntai guan ||| 
2021 ||| research on transformer fault diagnosis method based on neighborhood rough set and grey wolf algorithm optimized support vector machine. ||| shunjie han ||| zhen wu ||| wenqing ding ||| 
2021 ||| multi-attention parallel cnn-gru fault diagnosis method for rolling bearing. ||| zhihao huo ||| xiaoyu yang ||| tao zhang ||| yanwei wang ||| ying zheng ||| 
2021 ||| remaining useful life prediction of tool with bigru-attention and improved particle filter. ||| yi li ||| yong zhang ||| yang chang ||| zhang liu ||| zhenxing liu ||| 
2021 ||| a bearing remaining useful life prediction method based on inception-resnet module and attention mechanism. ||| renpeng mo ||| tianmei li ||| xu zhu ||| xiaosheng si ||| hanxiao mu ||| baokui yang ||| 
2021 ||| life prediction of rolling bearing using temporal convolution network and attention mechanism. ||| gengwei zhang ||| yuxing gu ||| zehui mao ||| bin jiang ||| chengrui liu ||| 
2019 ||| -harmonic-shorting four-way transformer and integrated thermal sensors. ||| inchan ju ||| michael j. mcpartlin ||| chun-wen paul huang ||| clifford d. y. cheon ||| mark doherty ||| john d. cressler ||| 
2019 ||| a 0.1-to-0.2v transformer-based switched-mode folded dco in 22nm fdsoi with active step-down impedance achieving 197dbc/hz peak fom and 40mhz/v frequency pushing. ||| omar el-aassar ||| gabriel m. rebeiz ||| 
2018 ||| an 82-to-108ghz -181db-fomt adpll employing a dco with split-transformer and dual-path switched-capacitor ladder and a clock-skew-sampling delta-sigma tdc. ||| zhiqiang huang ||| howard cam luong ||| 
2022 ||| a 28nm 27.5tops/w approximate-computing-based transformer processor with asymptotic sparsity speculating and out-of-order computing. ||| yang wang ||| yubin qin ||| dazheng deng ||| jingchuan wei ||| yang zhou ||| yuanqi fan ||| tianbao chen ||| hao sun ||| leibo liu ||| shaojun wei ||| shouyi yin ||| 
2019 ||| a 60ghz cmos power amplifier with cascaded asymmetric distributed-active-transformer achieving watt-level peak output power with 20.8% pae and supporting 2gsym/s 64-qam modulation. ||| huy thong nguyen ||| doohwan jung ||| hua wang ||| 
2018 ||| a compact dual-band digital doherty power amplifier using parallel-combining transformer for cellular nb-iot applications. ||| yun yin ||| liang xiong ||| yiting zhu ||| bowen chen ||| hao min ||| hongtao xu ||| 
2021 ||| a 1.25w 46.5%-peak-efficiency transformer-in-package isolated dc-dc converter using glass-based fan-out wafer-level packaging achieving 50mw/mm2 power density. ||| dongfang pan ||| guolong li ||| fangting miao ||| biao deng ||| junying wei ||| daquan yu ||| ming liu ||| lin cheng ||| 
2022 ||| a 28ghz compact 3-way transformer-based parallel-series doherty power amplifier with 20.4%/14.2% pae at 6-/12-db power back-off and 25.5dbm psat in 55nm bulk cmos. ||| zonglin ma ||| kaixue ma ||| keping wang ||| fanyi meng ||| 
2021 ||| a 60ghz 186.5dbc/hz fom quad-core fundamental vco using circular triple-coupled transformer with no mode ambiguity in 65nm cmos. ||| haikun jia ||| wei deng ||| pingda guan ||| zhihua wang ||| baoyong chi ||| 
2021 ||| 270-to-300ghz double-balanced parametric upconverter using asymmetric mos varactors and a power-splitting- transformer hybrid in 65nm cmos. ||| zhiyu chen ||| wooyeol choi ||| kenneth k. o ||| 
2018 ||| a 28ghz 41%-pae linear cmos power amplifier using a transformer-based am-pm distortion-correction technique for 5g phased arrays. ||| sheikh nijam ali ||| pawan agarwal ||| joe baylon ||| srinivasan gopal ||| luke renaud ||| deukhyoun heo ||| 
2019 |||  loop antenna and transformer-boost power oscillator. ||| yao shi ||| xing chen ||| hun-seok kim ||| david t. blaauw ||| david d. wentzloff ||| 
2022 ||| j/token full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes. ||| fengbin tu ||| zihan wu ||| yiqi wang ||| ling liang ||| liu liu ||| yufei ding ||| leibo liu ||| shaojun wei ||| yuan xie ||| shouyi yin ||| 
2020 ||| 24.5 a 15b quadrature digital power amplifier with transformer-based complex-domain power-efficiency enhancement. ||| diyang zheng ||| yun yin ||| yiting zhu ||| liang xiong ||| yicheng li ||| na yan ||| hongtao xu ||| 
2019 ||| a broadband switched-transformer digital power amplifier for deep back-off efficiency enhancement. ||| liang xiong ||| tong li ||| yun yin ||| hao min ||| na yan ||| hongtao xu ||| 
2018 ||| a 0.3ppm dual-resonance transformer-based drift-cancelling reference-free magnetic sensor for biosensing applications. ||| constantine sideris ||| parham porsandeh khial ||| bill ling ||| ali hajimiri ||| 
2019 ||| 1w isolated power transfer system using fully integrated magnetic-core transformer. ||| yue zhuo ||| shaoyu ma ||| tianting zhao ||| wenhui qin ||| yuanyuan zhao ||| yingjie guo ||| baoxing chen ||| 
2021 ||| 26.5 a watt-level quadrature switched/floated-capacitor power amplifier with back-off efficiency enhancement in complex domain using reconfigurable self-coupling canceling transformer. ||| bingzheng yang ||| huizhen jenny qian ||| xun luo ||| 
2021 |||  soc for iot devices with 18ms noise-robust speech-to-text latency via bayesian speech denoising and attention-based sequence-to-sequence dnn speech recognition in 16nm finfet. ||| thierry tambe ||| en-yu yang ||| glenn g. ko ||| yuji chai ||| coleman hooper ||| marco donato ||| paul n. whatmough ||| alexander m. rush ||| david brooks ||| gu-yeon wei ||| 
2021 ||| a 1.75db-nf 25mw 5ghz transformer-based noise- cancelling cmos receiver front-end. ||| kaituo yang ||| chirn chye boon ||| guangyin feng ||| chenyang li ||| zhe liu ||| ting guo ||| xiang yi ||| yangtao dong ||| ao zhou ||| xiaoying wang ||| 
2020 ||| explainable and adaptable augmentation in knowledge attention network for multi-agent deep reinforcement learning systems. ||| joshua ho ||| chien-min wang ||| 
2018 ||| emotion recognition from human behaviors using attention model. ||| james jie deng ||| clement ho cheung leung ||| paolo mengoni ||| yuanxi li ||| 
2019 ||| poster abstract: a wearable diagnostic assessment system for attention deficit hyperactivity disorder. ||| xinlong jiang ||| yunbing xing ||| teng zhang ||| wuliang huang ||| chenlong gao ||| yiqiang chen ||| 
2021 ||| sensor-based human activity recognition for elderly in-patients with a luong self-attention network. ||| g. r. nithin ||| mihika chhabra ||| yujiao hao ||| boyu wang ||| rong zheng ||| 
2021 ||| a transformer-based framework for multivariate time series representation learning. ||| george zerveas ||| srideepika jayaraman ||| dhaval patel ||| anuradha bhamidipaty ||| carsten eickhoff ||| 
2019 ||| multiple relational attention network for multi-task learning. ||| jiejie zhao ||| bowen du ||| leilei sun ||| fuzhen zhuang ||| weifeng lv ||| hui xiong ||| 
2018 ||| r-vqa: learning visual relation facts with semantic attention for visual question answering. ||| pan lu ||| lei ji ||| wei zhang ||| nan duan ||| ming zhou ||| jianyong wang ||| 
2020 ||| attentional multi-graph convolutional network for regional economy prediction with open migration data. ||| fengli xu ||| yong li ||| shusheng xu ||| 
2021 ||| accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts. ||| jaemin yoo ||| yejun soun ||| yong-chan park ||| u kang ||| 
2018 ||| stamp: short-term attention/memory priority model for session-based recommendation. ||| qiao liu ||| yifu zeng ||| refuoe mokhosi ||| haibin zhang ||| 
2021 ||| intention-aware heterogeneous graph attention networks for fraud transactions detection. ||| can liu ||| li sun ||| xiang ao ||| jinghua feng ||| qing he ||| hao yang ||| 
2019 ||| conversion prediction using multi-task conditional attention networks to support the creation of effective ad creatives. ||| shunsuke kitada ||| hitoshi iyatomi ||| yoshifumi seki ||| 
2020 ||| attention realignment and pseudo-labelling for interpretable cross-lingual classification of crisis tweets. ||| jitin krishnan ||| hemant purohit ||| huzefa rangwala ||| 
2021 ||| rapt: pre-training of time-aware transformer for learning robust healthcare representation. ||| houxing ren ||| jingyuan wang ||| wayne xin zhao ||| ning wu ||| 
2021 ||| tuta: tree-based transformers for generally structured table pre-training. ||| zhiruo wang ||| haoyu dong ||| ran jia ||| jia li ||| zhiyi fu ||| shi han ||| dongmei zhang ||| 
2019 ||| understanding consumer journey using attention based recurrent neural networks. ||| yichao zhou ||| shaunak mishra ||| jelena gligorijevic ||| tarun bhatia ||| narayan bhamidipati ||| 
2017 ||| a context-aware attention network for interactive question answering. ||| huayu li ||| martin renqiang min ||| yong ge ||| asim kadav ||| 
2018 ||| graph classification using structural attention. ||| john boaz lee ||| ryan a. rossi ||| xiangnan kong ||| 
2019 ||| real-time attention based look-alike model for recommender system. ||| yudan liu ||| kaikai ge ||| xu zhang ||| leyu lin ||| 
2019 ||| daml: dual attention mutual learning between ratings and reviews for item recommendation. ||| donghua liu ||| jing li ||| bo du ||| jun chang ||| rong gao ||| 
2020 ||| attention based multi-modal new product sales time-series forecasting. ||| vijay ekambaram ||| kushagra manglik ||| sumanta mukherjee ||| surya shravan kumar sajja ||| satyam dwivedi ||| vikas raykar ||| 
2021 ||| hgamn: heterogeneous graph attention matching network for multilingual poi retrieval at baidu maps. ||| jizhou huang ||| haifeng wang ||| yibo sun ||| miao fan ||| zhengjie huang ||| chunyuan yuan ||| yawen li ||| 
2019 ||| akupm: attention-enhanced knowledge-aware user preference model for recommendation. ||| xiaoli tang ||| tengyun wang ||| haizhi yang ||| hengjie song ||| 
2021 ||| heterogeneous temporal graph transformer: an intelligent system for evolving android malware detection. ||| yujie fan ||| mingxuan ju ||| shifu hou ||| yanfang ye ||| wenqiang wan ||| kui wang ||| yinming mei ||| qi xiong ||| 
2020 ||| combo-attention network for baidu video advertising. ||| tan yu ||| yi yang ||| yi li ||| xiaodong chen ||| mingming sun ||| ping li ||| 
2017 ||| dipole: diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. ||| fenglong ma ||| radha chitta ||| jing zhou ||| quanzeng you ||| tong sun ||| jing gao ||| 
2020 ||| hitanet: hierarchical time-aware attention networks for risk prediction on electronic health records. ||| junyu luo ||| muchao ye ||| cao xiao ||| fenglong ma ||| 
2018 ||| multi-pointer co-attention networks for recommendation. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| 
2020 ||| deterrent: knowledge guided graph attention network for detecting healthcare misinformation. ||| limeng cui ||| haeseung seo ||| maryam tabar ||| fenglong ma ||| suhang wang ||| dongwon lee ||| 
2019 ||| npa: neural news recommendation with personalized attention. ||| chuhan wu ||| fangzhao wu ||| mingxiao an ||| jianqiang huang ||| yongfeng huang ||| xing xie ||| 
2019 ||| social recommendation with optimal limited attention. ||| xin wang ||| wenwu zhu ||| chenghao liu ||| 
2019 ||| kgat: knowledge graph attention network for recommendation. ||| xiang wang ||| xiangnan he ||| yixin cao ||| meng liu ||| tat-seng chua ||| 
2020 ||| hierarchical attention propagation for healthcare representation learning. ||| muhan zhang ||| christopher r. king ||| michael avidan ||| yixin chen ||| 
2018 ||| detection of paroxysmal atrial fibrillation using attention-based bidirectional recurrent neural networks. ||| supreeth prajwal shashikumar ||| amit j. shah ||| gari d. clifford ||| shamim nemati ||| 
2020 ||| recurrent networks for guided multi-attention classification. ||| xin dai ||| xiangnan kong ||| tian guo ||| john boaz lee ||| xinyue liu ||| constance m. moore ||| 
2020 ||| graph attention networks over edge content-based channels. ||| lu lin ||| hongning wang ||| 
2020 ||| preserving dynamic attention for long-term spatial-temporal prediction. ||| haoxing lin ||| rufan bai ||| weijia jia ||| xinyu yang ||| yongjian you ||| 
2021 ||| m6: multi-modality-to-multi-modality multitask mega-transformer for unified pretraining. ||| junyang lin ||| rui men ||| an yang ||| chang zhou ||| yichang zhang ||| peng wang ||| jingren zhou ||| jie tang ||| hongxia yang ||| 
2019 ||| buying or browsing?: predicting real-time purchasing intent using attention-based deep network with multiple behavior. ||| long guo ||| lifeng hua ||| rongfei jia ||| binqiang zhao ||| xiaobo wang ||| bin cui ||| 
2019 ||| alphastock: a buying-winners-and-selling-losers investment strategy using interpretable deep reinforcement attention networks. ||| jingyuan wang ||| yang zhang ||| ke tang ||| junjie wu ||| zhang xiong ||| 
2017 ||| learning to generate rock descriptions from multivariate well logs with hierarchical attention. ||| bin tong ||| martin klinkigt ||| makoto iwayama ||| toshihiko yanase ||| yoshiyuki kobayashi ||| anshuman sahu ||| ravigopal vennelakanti ||| 
2021 ||| why attentions may not be interpretable? ||| bing bai ||| jian liang ||| guanhua zhang ||| hao li ||| kun bai ||| fei wang ||| 
2020 ||| a dual heterogeneous graph attention network to improve long-tail performance for shop search in e-commerce. ||| xichuan niu ||| bofang li ||| chenliang li ||| rong xiao ||| haochuan sun ||| hongbo deng ||| zhenzhong chen ||| 
2019 ||| hats: a hierarchical sequence-attention framework for inductive set-of-sets embeddings. ||| changping meng ||| jiasen yang ||| bruno ribeiro ||| jennifer neville ||| 
2018 ||| multi-cast attention networks. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2017 ||| gram: graph-based attention model for healthcare representation learning. ||| edward choi ||| mohammad taha bahadori ||| le song ||| walter f. stewart ||| jimeng sun ||| 
2020 ||| attention and memory-augmented networks for dual-view sequential learning. ||| yong he ||| cheng wang ||| nan li ||| zhenyu zeng ||| 
2019 ||| graph representation learning via hard and channel-wise attention networks. ||| hongyang gao ||| shuiwang ji ||| 
2017 ||| dynamic attention deep model for article recommendation by learning human editors' demonstration. ||| xuejian wang ||| lantao yu ||| kan ren ||| guanyu tao ||| weinan zhang ||| yong yu ||| jun wang ||| 
2019 ||| multi-horizon time series forecasting with temporal attention learning. ||| chenyou fan ||| yuze zhang ||| yi pan ||| xiaoyue li ||| chi zhang ||| rong yuan ||| di wu ||| wensheng wang ||| jian pei ||| heng huang ||| 
2020 ||| rationale-based human-in-the-loop via supervised attention. ||| teja kanchinadam ||| keith westpfahl ||| qian you ||| glenn fung ||| 
2020 ||| kronecker attention networks. ||| hongyang gao ||| zhengyang wang ||| shuiwang ji ||| 
2020 ||| taming pretrained transformers for extreme multi-label text classification. ||| wei-cheng chang ||| hsiang-fu yu ||| kai zhong ||| yiming yang ||| inderjit s. dhillon ||| 
2018 ||| leveraging meta-path based context for top- n recommendation with a neural co-attention model. ||| binbin hu ||| chuan shi ||| wayne xin zhao ||| philip s. yu ||| 
2019 ||| mvan: multi-view attention networks for real money trading detection in online games. ||| jianrong tao ||| jianshi lin ||| shize zhang ||| sha zhao ||| runze wu ||| changjie fan ||| peng cui ||| 
2021 ||| triplet attention: rethinking the similarity in transformers. ||| haoyi zhou ||| jianxin li ||| jieqi peng ||| shuai zhang ||| shanghang zhang ||| 
2020 ||| constgat: contextual spatial-temporal graph attention network for travel time estimation at baidu maps. ||| xiaomin fang ||| jizhou huang ||| fan wang ||| lingke zeng ||| haijin liang ||| haifeng wang ||| 
2021 ||| attention guidance technique using visual subliminal cues and its application on videos. ||| eugene hwang ||| jeongmi lee ||| 
2020 ||| joint attention for automated video editing. ||| hui-yin wu ||| trevor santarra ||| michael leece ||| rolando vargas ||| arnav jhala ||| 
2019 ||| understanding user attention in vr using gaze controlled games. ||| murtada dohan ||| mu mu ||| 
2017 ||| two-step joint attention network for visual question answering. ||| weiming zhang ||| chunhong zhang ||| pei liu ||| zhiqiang zhan ||| xiaofeng qiu ||| 
2021 ||| hybrid attention cascaded u-net for building extraction from aerial images. ||| chaohui li ||| yingjian liu ||| haoyu yin ||| yue li ||| pengting du ||| limin zhang ||| qingxiang guo ||| 
2020 ||| recurrent factorization machine with self-attention for time-aware service recommendation. ||| jiao zhou ||| xing guo ||| chunhui yin ||| 
2019 ||| qos attributes prediction with attention-based lstm network for mobile services. ||| qing wang ||| xiaodong wang ||| xiangqian ding ||| 
2019 ||| mining aspects in online comments with attention and bi-lstm. ||| lan yao ||| heting rong ||| chao chu ||| fuxiang gao ||| 
2020 ||| joint self-attention and multi-embeddings for chinese named entity recognition. ||| cijian song ||| yan xiong ||| wenchao huang ||| lu ma ||| 
2018 ||| emotionx-area66: predicting emotions in dialogues using hierarchical attention network with sequence labeling. ||| rohit saxena ||| savita bhat ||| niranjan pedanekar ||| 
2021 ||| self-contextualized attention for abusive language identification. ||| horacio jes ||| s jarqu ||| n-v ||| squez ||| hugo jair escalante ||| manuel montes-y-g ||| mez ||| 
2018 ||| emotionx-jtml: detecting emotions with attention. ||| johnny torres ||| 
2018 ||| auditory attention, implications for serious game design. ||| nikesh bajaj ||| francesco bellotti ||| riccardo berta ||| jes ||| s requena-carri ||| n ||| alessandro de gloria ||| 
2020 ||| how do students and researchers behave and feel while playing the "online attention game"?: an exploratory study on digital identity education. ||| tadao obana ||| miha takubo ||| yohko orito ||| kiyoshi murata ||| hidenobu sai ||| tadayuki okamoto ||| 
2018 ||| cran: a hybrid cnn-rnn attention-based model for text classification. ||| long guo ||| dongxiang zhang ||| lei wang ||| han wang ||| bin cui ||| 
2021 ||| fake speech detection using residual network with transformer encoder. ||| zhenyu zhang ||| xiaowei yi ||| xianfeng zhao ||| 
2020 ||| left ventricular myocardium segmentation in coronary computed tomography angiography using 3d deep attention u-net. ||| xiuxiu he ||| bangjun guo ||| yang lei ||| joseph harms ||| tonghe wang ||| tian liu ||| longjiang zhang ||| xiaofeng yang ||| 
2020 ||| classification of attention-deficit/hyperactivity disorder from resting-state functional mri with mutual connectivity analysis. ||| seyed saman saboksayr ||| adora m. dsouza ||| john j. foxe ||| axel wism ||| ller ||| 
2020 ||| the impact of a social robot public speaker on audience attention. ||| marie-luce bourguet ||| minghe xu ||| shiyu zhang ||| jacqueline urakami ||| gentiane venture ||| 
2017 ||| proposal of a model to determine the attention target for an agent in group discussion with non-verbal features. ||| seiya kimura ||| hung-hsuan huang ||| qi zhang ||| shogo okada ||| naoki ohta ||| kazuhiro kuwabara ||| 
2019 ||| a speech promotion system by using embodied entrainment objects of spoken words and a listener character for joint attention. ||| masakatsu kubota ||| tomio watanabe ||| yutaka ishii ||| 
2019 ||| acting as if being aware of visitors' attention strengthens a robotic salesperson's social presence. ||| masaya iwasaki ||| jian zhou ||| mizuki ikeda ||| yuya onishi ||| tatsuyuki kawamura ||| hideyuki nakanishi ||| 
2021 ||| enhancing sense of attention from a communication robot by drawing the user's face on its thought bubble in the video conferencing system. ||| yuta nitada ||| yuichiro yoshikawa ||| alexis meneses ||| hiroshi ishiguro ||| 
2019 ||| gender differences in allocation of attention and read time in an educational history game. ||| betty t ||| rning ||| trond a. tj ||| stheim ||| 
2018 ||| attracting attention and changing behavior toward wall advertisements with a walking virtual agent. ||| naoto yoshida ||| sho hanasaki ||| tomoko yonezawa ||| 
2017 ||| multi-level attention-based neural networks for distant supervised relation extraction. ||| linyi yang ||| tin lok james ng ||| catherine mooney ||| ruihai dong ||| 
2017 ||| deep semantic indexing using convolutional localization network with region-based visual attention for image database. ||| mingxing zhang ||| yang yang ||| hanwang zhang ||| yanli ji ||| ning xie ||| heng tao shen ||| 
2017 ||| jointly learning attentions with semantic cross-modal correlation for visual question answering. ||| liangfu cao ||| lianli gao ||| jingkuan song ||| xing xu ||| heng tao shen ||| 
2021 ||| contrastively learning visual attention as affordance cues from demonstrations for robotic grasping. ||| yantian zha ||| siddhant bhambri ||| lin guan ||| 
2019 ||| ronet: real-time range-only indoor localization via stacked bidirectional lstm with residual attention. ||| hyungtae lim ||| changgue park ||| hyun myung ||| 
2020 ||| self-supervised attention learning for depth and ego-motion estimation. ||| assem sadek ||| boris chidlovskii ||| 
2021 ||| ptt: point-track-transformer module for 3d single object tracking in point clouds. ||| jiayao shan ||| sifan zhou ||| zheng fang ||| yubo cui ||| 
2018 ||| attention-aware cross-modal cross-level fusion network for rgb-d salient object detection. ||| hao chen ||| youfu li ||| dan su ||| 
2019 ||| goal-directed behavior under variational predictive coding: dynamic organization of visual attention and working memory. ||| minju jung ||| takazumi matsumoto ||| jun tani ||| 
2020 ||| few-shot relation learning with attention for eeg-based motor imagery classification. ||| sion an ||| soopil kim ||| philip chikontwe ||| sang hyun park ||| 
2021 ||| latent attention augmentation for robust autonomous driving policies. ||| ran cheng ||| christopher agia ||| florian shkurti ||| david meger ||| gregory dudek ||| 
2017 ||| measurement and prediction of situation awareness in human-robot interaction based on a framework of probabilistic attention. ||| amir dini ||| cornelia murko ||| saeed yahyanejad ||| ursula h. augsd ||| rfer ||| michael w. hofbaur ||| lucas paletta ||| 
2021 ||| transformer-based deep imitation learning for dual-arm robot manipulation. ||| heecheol kim ||| yoshiyuki ohmura ||| yasuo kuniyoshi ||| 
2019 ||| a deep learning approach for multi-view engagement estimation of children in a child-robot joint attention task. ||| jack hadfield ||| georgia chalvatzaki ||| petros koutras ||| mehdi khamassi ||| costas s. tzafestas ||| petros maragos ||| 
2021 ||| joint intention and trajectory prediction based on transformer. ||| ze sui ||| yue zhou ||| xu zhao ||| ao chen ||| yiyang ni ||| 
2021 ||| trajectory-constrained deep latent visual attention for improved local planning in presence of heterogeneous terrain. ||| stefan wapnick ||| travis manderson ||| david meger ||| gregory dudek ||| 
2018 ||| calibnet: geometrically supervised extrinsic calibration using 3d spatial transformer networks. ||| ganesh iyer ||| karnik ram r. ||| j. krishna murthy ||| k. madhava krishna ||| 
2021 ||| feanet: feature-enhanced attention network for rgb-thermal real-time semantic segmentation. ||| fuqin deng ||| hua feng ||| mingjian liang ||| hongmin wang ||| yong yang ||| yuan gao ||| junfeng chen ||| junjie hu ||| xiyue guo ||| tin lun lam ||| 
2021 ||| attention augmented convlstm for environment prediction. ||| bernard lange ||| masha itkina ||| mykel j. kochenderfer ||| 
2020 ||| probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles. ||| chenxu luo ||| lin sun ||| dariush dabiri ||| alan l. yuille ||| 
2021 ||| self attention guided depth completion using rgb and sparse lidar point clouds. ||| siddharth srivastava ||| gaurav sharma ||| 
2021 ||| multi-scale aggregation with self-attention network for modeling electrical motor dynamics. ||| kuan-chih huang ||| hao-hsiang yang ||| wei-ting chen ||| 
2020 ||| learning accurate and human-like driving using semantic maps and attention. ||| simon hecker ||| dengxin dai ||| alexander liniger ||| martin hahner ||| luc van gool ||| 
2021 ||| local memory attention for fast video semantic segmentation. ||| matthieu paul ||| martin danelljan ||| luc van gool ||| radu timofte ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| yilun chen ||| chiyu dong ||| praveen palanisamy ||| priyantha mudalige ||| katharina muelling ||| john m. dolan ||| 
2020 ||| diagnose like a clinician: third-order attention guided lesion amplification network for wce image classification. ||| xiaohan xing ||| yixuan yuan ||| max q.-h. meng ||| 
2020 ||| dr-spaam: a spatial-attention and auto-regressive model for person detection in 2d range data. ||| dan jia ||| alexander hermans ||| bastian leibe ||| 
2021 ||| re-attention is all you need: memory-efficient scene text detection via re-attention on uncertain regions. ||| hsiang-chun chang ||| hung-jen chen ||| yu-chia shen ||| hong-han shuai ||| wen-huang cheng ||| 
2020 ||| hamlet: a hierarchical multimodal attention-based human activity recognition algorithm. ||| md mofijul islam ||| tariq iqbal ||| 
2021 ||| design of an ssvep-based bci stimuli system for attention-based robot navigation in robotic telepresence. ||| xingchao wang ||| xiaopeng huang ||| yi lin ||| liguang zhou ||| zhenglong sun ||| yangsheng xu ||| 
2018 ||| tssd: temporal single-shot detector based on attention and lstm. ||| xingyu chen ||| zhengxing wu ||| junzhi yu ||| 
2020 ||| dynamic attention-based visual odometry. ||| xin-yu kuo ||| chien liu ||| kai-chen lin ||| evan luo ||| yu-wen chen ||| chun-yi lee ||| 
2021 ||| marine autonomous navigation for biomimetic underwater robots based on deep stereo attention network. ||| shuaizheng yan ||| zhengxing wu ||| jian wang ||| min tan ||| junzhi yu ||| 
2017 ||| attentional masking for pre-trained deep networks. ||| marcus wallenberg ||| per-erik forss ||| n ||| 
2019 ||| responsive joint attention in human-robot interaction. ||| andr |||  pereira ||| catharine oertel ||| leonor fermoselle ||| joe mendelson ||| joakim gustafson ||| 
2019 ||| local pose optimization with an attention-based neural network. ||| yiling liu ||| hesheng wang ||| fan xu ||| yong wang ||| weidong chen ||| qirong tang ||| 
2020 ||| lane-attention: predicting vehicles' moving trajectories by learning their attention over lanes. ||| jiacheng pan ||| hongyi sun ||| kecheng xu ||| yifei jiang ||| xiangquan xiao ||| jiangtao hu ||| jinghao miao ||| 
2018 ||| policy shaping with supervisory attention driven exploration. ||| taylor kessler faulkner ||| elaine schaertl short ||| andrea lockerd thomaz ||| 
2020 ||| towards understanding and inferring the crowd: guided second order attention networks and re-identification for multi-object tracking. ||| niraj bhujel ||| li jun ||| wei-yun yau ||| han wang ||| 
2021 ||| pctma-net: point cloud transformer with morphing atlas-based point generation network for dense point cloud completion. ||| jianjie lin ||| markus rickert ||| alexander perzylo ||| alois c. knoll ||| 
2020 ||| planning on the fast lane: learning to interact using attention mechanisms in path integral inverse reinforcement learning. ||| sascha rosbach ||| xing li ||| simon gro ||| johann ||| silviu homoceanu ||| stefan roth ||| 
2020 ||| spatio-temporal attention model for tactile texture recognition. ||| guanqun cao ||| yi zhou ||| danushka bollegala ||| shan luo ||| 
2021 ||| stereo waterdrop removal with row-wise dilated attention. ||| zifan shi ||| na fan ||| dit-yan yeung ||| qifeng chen ||| 
2021 ||| siamapn++: siamese attentional aggregation network for real-time uav tracking. ||| ziang cao ||| changhong fu ||| junjie ye ||| bowen li ||| yiming li ||| 
2020 ||| end-to-end contextual perception and prediction with interaction transformer. ||| lingyun luke li ||| bin yang ||| ming liang ||| wenyuan zeng ||| mengye ren ||| sean segal ||| raquel urtasun ||| 
2018 ||| i can see your aim: estimating user attention from gaze for handheld robot collaboration. ||| janis stolzenwald ||| walterio w. mayol-cuevas ||| 
2020 ||| towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters. ||| yujie he ||| changhong fu ||| fuling lin ||| yiming li ||| peng lu ||| 
2020 ||| italian transformers under the linguistic lens. ||| alessio miaschi ||| gabriele sarti ||| dominique brunato ||| felice dell'orletta ||| giulia venturi ||| 
2020 ||| exploring attention in a multimodal corpus of guided tours. ||| andrea amelio ravelli ||| antonio origlia ||| felice dell'orletta ||| 
2019 ||| deep bidirectional transformers for italian question answering. ||| danilo croce ||| giorgio brandi ||| roberto basili ||| 
2021 ||| tackling italian university assessment tests with transformer-based language models. ||| daniele puccinelli ||| silvia demartini ||| pier luigi ferrari ||| 
2019 ||| the impact of self-interaction attention on the extraction of drug-drug interactions. ||| luca putelli ||| alfonso emilio gerevini ||| alberto lavelli ||| ivan serina ||| 
2018 ||| multi-source transformer for automatic post-editing. ||| amirhossein tebbifakhr ||| ruchit agrawal ||| matteo negri ||| marco turchi ||| 
2020 ||| cross-language transformer adaptation for frequently asked questions. ||| luca di liello ||| daniele bonadiman ||| alessandro moschitti ||| cristina giannone ||| andrea favalli ||| raniero romagnoli ||| 
2017 ||| a reconfigurable analog baseband transformer for multistandard applications in 14nm finfet cmos. ||| jongmi lee ||| jongwoo lee ||| chilun lo ||| jaehoon lee ||| in-young lee ||| byungki han ||| seunghyun oh ||| thomas byunghak cho ||| 
2021 ||| a 196.2 dbc/hz fomt 16.8-to-21.6 ghz class-f23 vco with transformer-based optimal q-factor tank in 65-nm cmos. ||| feifan hong ||| tianao ding ||| dixian zhao ||| 
2021 ||| a 7.9-14.3ghz -243.3db fomt sub-sampling pll with transformer-based dual-mode vco in 40nm cmos. ||| yizhuo wang ||| tenghao zou ||| bowen chen ||| shujiang ji ||| chao zhang ||| na yan ||| 
2020 ||| attention models for pm2.5 prediction. ||| jovan kalajdjieski ||| georgina mirceva ||| slobodan kalajdziski ||| 
2017 ||| feature engineering and classification models for partial discharge events in power transformers. ||| jonathan wang ||| kesheng wu ||| alex sim ||| seongwook hwangbo ||| 
2019 ||| diagnosing and classifying the fault of transformer with deep belief network. ||| li-peng zhu ||| wei rao ||| junfeng qiao ||| sen pan ||| 
2017 ||| mining association rules from multidimensional transformer defect records. ||| yi yang ||| yujie geng ||| yi ju ||| xuan zhao ||| danfeng yan ||| 
2018 ||| videogame-based case studies for improving communication and attention in children with asd. ||| sandra baldassarri ||| liliana passerino ||| silvia ramis ||| inma riquelme ||| francisco jos |||  perales l ||| pez ||| 
2020 ||| sentiment analysis of online reviews with a hierarchical attention network. ||| jingren zhou ||| peiquan jin ||| 
2020 ||| data-sparsity service discovery using enriched neural topic model and attentional bi-lstm. ||| li yao ||| bing li ||| jian wang ||| 
2020 ||| collaborative denoising graph attention autoencoders for social recommendation. ||| nan mu ||| daren zha ||| lin zhao ||| rui gong ||| 
2019 ||| a convolutional neural network pruning method based on attention mechanism. ||| xiao-jie wang ||| wenbin yao ||| huiyuan fu ||| 
2020 ||| deep graph attention neural network for click-through rate prediction. ||| wen fang ||| lu lu ||| 
2020 ||| a combined model for extractive and abstractive summarization based on transformer model. ||| xin liu ||| liutong xu ||| 
2019 ||| acnet: attention-based convolution network with additional discriminative features for dcm classification (s). ||| chao luo ||| xin wang ||| xiaojie li ||| yucheng chen ||| jiliu zhou ||| kunlin cao ||| qi song ||| xi wu ||| youbing yin ||| 
2019 ||| multistep flow prediction on car-sharing systems: a multi-graph convolutional neural network with attention mechanism. ||| yi luo ||| qin liu ||| hongming zhu ||| hongfei fan ||| tianyou song ||| chang wu yu ||| bowen du ||| 
2020 ||| a novel self-attention based automatic code completion neural network. ||| bohao wang ||| wanyou lv ||| jianqi shi ||| yanhong huang ||| 
2021 ||| streaming transformer asr with blockwise synchronous beam search. ||| emiru tsunoo ||| yosuke kashiwagi ||| shinji watanabe ||| 
2021 ||| streaming attention-based models with augmented memory for end-to-end speech recognition. ||| ching-feng yeh ||| yongqiang wang ||| yangyang shi ||| chunyang wu ||| frank zhang ||| julian chan ||| michael l. seltzer ||| 
2021 ||| a light transformer for speech-to-intent applications. ||| pu wang ||| hugo van hamme ||| 
2021 ||| convolution-based attention model with positional encoding for streaming speech recognition on embedded devices. ||| jinhwan park ||| chanwoo kim ||| wonyong sung ||| 
2018 ||| audio-visual speech recognition with a hybrid ctc/attention architecture. ||| stavros petridis ||| themos stafylakis ||| pingchuan ma ||| georgios tzimiropoulos ||| maja pantic ||| 
2018 ||| multi-scale alignment and contextual history for attention mechanism in sequence-to-sequence model. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| multimodal attention fusion for target speaker extraction. ||| hiroshi sato ||| tsubasa ochiai ||| keisuke kinoshita ||| marc delcroix ||| tomohiro nakatani ||| shoko araki ||| 
2021 ||| on the usefulness of self-attention for automatic speech recognition with transformers. ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2018 ||| exploring end-to-end attention-based neural networks for native language identification. ||| rutuja ubale ||| yao qian ||| keelan evanini ||| 
2018 ||| exploring layer trajectory lstm with depth processing units and attention. ||| jinyu li ||| liang lu ||| changliang liu ||| yifan gong ||| 
2021 ||| supervised attention for speaker recognition. ||| seong min kye ||| joon son chung ||| hoirin kim ||| 
2021 ||| self-supervised learning with cross-modal transformers for emotion recognition. ||| aparna khare ||| srinivas parthasarathy ||| shiva sundaram ||| 
2021 ||| transformer-based online speech recognition with decoder-end adaptive computation steps. ||| mohan li ||| catalin zorila ||| rama doddipatla ||| 
2018 ||| attention mechanism in speaker recognition: what does it learn in deep speaker embedding? ||| qiongqiong wang ||| koji okabe ||| kong aik lee ||| hitoshi yamamoto ||| takafumi koshinaka ||| 
2021 ||| emotion recognition in public speaking scenarios utilising an lstm-rnn approach with attention. ||| alice baird ||| shahin amiriparian ||| manuel milling ||| bj ||| rn w. schuller ||| 
2018 ||| context-aware attention mechanism for speech emotion recognition. ||| gaetan ramet ||| philip n. garner ||| michael baeriswyl ||| alexandros lazaridis ||| 
2018 ||| improving attention-based end-to-end asr systems with sequence-based loss functions. ||| jia cui ||| chao weng ||| guangsen wang ||| jun wang ||| peidong wang ||| chengzhu yu ||| dan su ||| dong yu ||| 
2021 ||| transformer based deliberation for two-pass speech recognition. ||| ke hu ||| ruoming pang ||| tara n. sainath ||| trevor strohman ||| 
2021 ||| controllable emphatic speech synthesis based on forward attention for expressive speech synthesis. ||| liangqi liu ||| jiankun hu ||| zhiyong wu ||| song yang ||| songfan yang ||| jia jia ||| helen meng ||| 
2021 ||| detecting expressions with multimodal transformers. ||| srinivas parthasarathy ||| shiva sundaram ||| 
2021 ||| automated scoring of spontaneous speech from young learners of english using transformers. ||| xinhao wang ||| keelan evanini ||| yao qian ||| matthew mulholland ||| 
2021 ||| simplified self-attention for transformer-based end-to-end speech recognition. ||| haoneng luo ||| shiliang zhang ||| ming lei ||| lei xie ||| 
2021 ||| transformer-based direct speech-to-speech translation with transcoder. ||| takatomo kano ||| sakriani sakti ||| satoshi nakamura ||| 
2018 ||| improving very deep time-delay neural network with vertical-attention for effectively training ctc-based asr systems. ||| sheng li ||| xugang lu ||| ryoichi takashima ||| peng shen ||| tatsuya kawahara ||| hisashi kawai ||| 
2021 ||| operator-adapted evolutionary large-scale multiobjective optimization for voltage transformer ratio error estimation. ||| changwu huang ||| lianghao li ||| cheng he ||| ran cheng ||| xin yao ||| 
2019 ||| plant identification based on multi-branch convolutional neural network with attention. ||| pengxi li ||| xiaoqing gong ||| xu hu ||| lianqi shi ||| xiaoting xue ||| jun guo ||| pengfei xu ||| daguang gan ||| 
2019 ||| pixel and channel attention network for person re-identification. ||| minjie wang ||| xian li ||| jiahuan zhang ||| haoyu zhou ||| lei lei ||| banghua yang ||| 
2019 ||| attention-based gan for single image super-resolution. ||| dongqi huo ||| rong wang ||| jianwei ding ||| 
2019 ||| multi-attention network for 2d face alignment in the wild. ||| xin liu ||| huabin wang ||| rui cheng ||| xiang yan ||| liang tao ||| 
2019 ||| sar ship detection under complex background based on attention mechanism. ||| chen chen ||| changhua hu ||| chuan he ||| hong pei ||| zhenan pang ||| tong zhao ||| 
2020 ||| eyebrow deserves attention: upper periocular biometrics. ||| hoang (mark) nguyen ||| ajita rattani ||| reza derakhshani ||| 
2019 ||| from material objects to social objects: researching the material-dialogic spaces of joint attention in a school-based makerspace. ||| kristiina kumpulainen ||| anu kajamaa ||| 
2019 ||| a qualitative analysis of joint visual attention and collaboration with high- and low-achieving groups in computer-mediated learning. ||| tonya bryant ||| iulian radu ||| bertrand schneider ||| 
2018 ||| message-efficient self-stabilizing transformer using snap-stabilizing quiescence detection. ||| ana ||| s durand ||| shay kutten ||| 
2019 ||| keep attention: a personalized serious game for attention training. ||| nadia hocine ||| mohamed ameur ||| wafaa ziani ||| 
2021 ||| attention enhanced hierarchical feature representation for three-way decision boundary processing. ||| jie chen ||| yue chen ||| yang xu ||| shu zhao ||| yanping zhang ||| 
2019 ||| design of a d-band transformer-based neutralized class-ab power amplifier in silicon technologies. ||| xinyan tang ||| alaaeldien medra ||| johan nguyen ||| khaled khalaf ||| bj ||| rn debaillie ||| piet wambacq ||| 
2017 ||| self-aware sensing and attention-based data collection in multi-processor system-on-chips. ||| nima taherinejad ||| muhammad ali shami ||| sai manoj p. d. ||| 
2021 ||| cloud detection method using convolutional neural network based on cascaded color and texture feature attention. ||| jing zhang ||| jun wu ||| yuchen wang ||| hui wang ||| yunsong li ||| 
2021 ||| remaining useful life estimation for turbofan engine with transformer-based deep architecture. ||| qianxia ma ||| ming zhang ||| yuchun xu ||| jingyan song ||| tao zhang ||| 
2021 ||| 6d object pose estimation with attention networks. ||| tao chen ||| dongbing gu ||| 
2021 ||| accurate visual tracking with attention feature fusion. ||| shuo hu ||| linna sun ||| hui yu ||| 
2018 ||| powering up attentional focus: validating a school-based deep breathing intervention with mobile eeg - a pilot exploration. ||| kiat hui khng ||| ravikiran mane ||| 
2021 ||| effectiveness of decoder transformer network in breaking low-resource real-time text captcha system. ||| rajat subhra bhowmick ||| isha ganguli ||| jayanta paul ||| jaya sil ||| 
2020 ||| wellhead compressor failure prediction using attention-based bidirectional lstms with data reduction techniques. ||| wirasak chomphu ||| boonserm kijsirikul ||| 
2020 ||| robust speaker recognition using speech enhancement and attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2020 ||| speaker characterization using tdnn, tdnn-lstm, tdnn-lstm-attention based speaker embeddings for nist sre 2019. ||| chien-lin huang ||| 
2020 ||| learning mixture representation for deep speaker embedding using attention. ||| weiwei lin ||| man-wai mak ||| lu yi ||| 
2019 ||| state evaluation of power transformer based on digital twin. ||| yong yang ||| zhu chen ||| jing yan ||| zhi xiong ||| jun zhang ||| hongxia yuan ||| yali tu ||| tianyun zhang ||| 
2020 ||| an approach for neural machine translation with graph attention network. ||| linjie chen ||| jianzong wang ||| zhangcheng huang ||| jing xiao ||| 
2019 ||| attention-based feature pyramid network for object detection. ||| ziyuan liu ||| ping gong ||| jing wang ||| 
2020 ||| gaze estimation with multi-scale channel and spatial attention. ||| song liu ||| danping liu ||| haiyang wu ||| 
2019 ||| a multi-scale network based on attention mechanism for hyperspectral image classification. ||| xin luo ||| baiyi shu ||| haoyang yu ||| hongjie wang ||| jiong mu ||| 
2021 ||| lightcvt: audio forgery detection via fusion of light cnn and transformer. ||| chenyu liu ||| jia li ||| junxian duan ||| haifeng shen ||| huaibo huang ||| 
2019 ||| multigraph attention network for analyzing company relations. ||| natraj raman ||| grace bang ||| azadeh nematzadeh ||| 
2021 ||| research on general text classification model integrating character-level attention and multi-scale features. ||| congcong zhang ||| haifeng zhao ||| mingwei cao ||| 
2021 ||| amdet: an efficient infrared small object detection model based on visual attention and multi-dilation feature. ||| cuicao zhang ||| yan dong ||| huanyu li ||| chunlei li ||| zhoufeng liu ||| 
2020 ||| weakly supervised fine-grained image recognition based on multi-channel attention and object localization. ||| sibo li ||| jianming liu ||| simin chen ||| 
2020 ||| attention mechanism balances semantic representation and syntactic representation. ||| ci liu ||| xuetong zhao ||| xiang li ||| yawei zhao ||| 
2019 ||| fabric defect detection using fully convolutional network with attention mechanism. ||| zhoufeng liu ||| jinjin wang ||| chunlei li ||| bicao li ||| ruimin yang ||| 
2021 ||| explicit attention network for face alignment. ||| shuangping jin ||| qiang wang ||| wankou yang ||| 
2019 ||| a graph-enhanced convolution network with attention gate for skeleton based action recognition. ||| kai yang ||| xiaolu ding ||| wai chen ||| 
2020 ||| aminn: attention-based multi-information neural network for emotion recognition. ||| ke xu ||| bin liu ||| jianhua tao ||| zhao lv ||| qifei li ||| cunhang fan ||| 
2019 ||| few-shot knowledge reasoning method based on attention mechanism. ||| haocheng xie ||| aiping li ||| yan jia ||| 
2019 ||| se-densenet: attention-based network for detecting pathological images of metastatic breast cancer. ||| yang duan ||| lingling sun ||| yaqi wang ||| 
2020 ||| attention-based end-to-end keywords spotting. ||| hengbo hu ||| wenlin zhang ||| li feng ||| ziwei wei ||| qi chen ||| 
2019 ||| combining attentional cnn and gru networks for ocean current prediction based on hf radar observations. ||| nathachai thongniran ||| kulsawasd jitkajornwanich ||| siam lawawirojwong ||| panu srestasathiern ||| peerapon vateekul ||| 
2019 ||| prediction of short-term precipitation in qinghai lake based on bilstm-attention method. ||| zhenye wang ||| chengxu ye ||| yuchao wang ||| 
2018 ||| assessing consequences of the final failure of a power transformer using fuzzy logic and expert criteria. ||| christian adrian farfan ||| diego m. mar ||| n ||| diego p. chac ||| n-troya ||| ricardo medina ||| 
2018 ||| automated protection of power transformers at distribution grid connected to green energy sources. ||| ali asghar memon ||| aneel kumar ||| ghan sham ||| urooj yasir ||| 
2017 ||| optimization of miniaturized single- and multiband cpw-based matching transformers for rf circuitry on lcp substrates. ||| osama hussein ||| khair alshamaileh ||| abhishek sahu ||| blen keneni ||| vijay kumar devabhaktuni ||| 
2017 ||| application of ensemble classification method for power transformers condition assessment. ||| ayman othman ||| monsef tahir ||| ramadan el shatshat ||| khaled bashir shaban ||| 
2017 ||| a three-phase ac-dc converter with transformer isolation and reduced number of switches. ||| javad khodabakhsh ||| gerry moschopoulos ||| 
2019 ||| reducing the loss of life of distribution transformers affected by plug-in electric vehicles using electric water heaters. ||| majid moradzadeh ||| morad abdelaziz ||| 
2021 ||| exploring how saliency affects attention in virtual reality. ||| radiah rivu ||| ville m ||| kel ||| mariam hassib ||| yomna abdelrahman ||| florian alt ||| 
2019 ||| gazelens: guiding attention to improve gaze interpretation in hub-satellite collaboration. ||| khanh-duy le ||| ignacio avellino ||| c ||| dric fleury ||| morten fjeld ||| andreas m. kunz ||| 
2019 ||| p(l)ay attention! co-designing for and with children with attention deficit hyperactivity disorder (adhd). ||| gy ||| ngyi fekete ||| andr ||| s lucero ||| 
2021 ||| attngan: realistic text-to-image synthesis with attentional generative adversarial networks. ||| shubham mathesul ||| ganesh bhutkar ||| ayush rambhad ||| 
2019 ||| you talkin' to me? a practical attention-aware embodied agent. ||| rahul r. divekar ||| jeffrey o. kephart ||| xiangyang mou ||| lisha chen ||| hui su ||| 
2021 ||| effect of attention saturating and cognitive load on tactile texture recognition for mobile surface. ||| guettaf adnane ||| yosra rekik ||| laurent grisoni ||| 
2017 ||| feedback modulated attention within a predictive framework. ||| benjamin cowley ||| john thornton ||| 
2021 ||| a study on email topic identification using latent dirichlet allocation integrated with visual attention. ||| tzuhang chiang ||| yungyu lin ||| yukari nagai ||| huako chiang ||| 
2018 ||| visual attention in omnidirectional video for virtual reality applications. ||| cagri ozcinar ||| aljosa smolic ||| 
2020 ||| a code-description representation learning model based on attention. ||| qing huang ||| an qiu ||| maosheng zhong ||| yuan wang ||| 
2021 ||| two-stage attention-based model for code search with textual and structural features. ||| ling xu ||| huanhuan yang ||| chao liu ||| jianhang shuai ||| meng yan ||| yan lei ||| zhou xu ||| 
2018 ||| machine learning of user attentions in sensor data visualization. ||| keita fujino ||| sozo inoue ||| tomohiro shibata ||| 
2019 ||| transformer based memory network for sentiment analysis of chinese weibo texts. ||| junlei wu ||| ming jiang ||| min zhang ||| 
2018 ||| tweetit: analyzing topics for twitter users to garner maximum attention. ||| dhanasekar sundararaman ||| priya arora ||| vishwanath seshagiri ||| 
2021 ||| mix-groups attention network for object detection. ||| yuqiang he ||| yinfeng xia ||| yizhen wang ||| baoqun yin ||| 
2020 ||| multimodal sentiment analysis based on multi-head attention mechanism. ||| chen xi ||| guanming lu ||| jingjie yan ||| 
2021 ||| channel attention module and weighted local feature person re-id network. ||| xiaolei zhou ||| leilei rong ||| linghui li ||| yongquan li ||| yan xu ||| 
2019 ||| the role of attention mechanism and multi-feature in image captioning. ||| tien x. dang ||| aran oh ||| in seop na ||| soo-hyung kim ||| 
2021 ||| transformer-based machine translation for low-resourced languages embedded with language identification. ||| tshephisho j. sefara ||| skhumbuzo g. zwane ||| nelisiwe gama ||| hlawulani sibisi ||| phillemon n. senoamadi ||| vukosi marivate ||| 
2018 ||| experimental demonstration of a 5g network slice deployment through the 5g-transformer architecture. ||| marco capitani ||| francesco giannone ||| silvia fichera ||| andrea sgambelluri ||| koteswararao kondepu ||| elian kraja ||| barbara martini ||| giada landi ||| luca valcarenghi ||| 
2021 ||| programmable integrated microwave photonic filter using a modulation transformer and a double-injection ring resonator. ||| okky daulay ||| gaojian liu ||| roel botter ||| marcel hoekman ||| edwin j. klein ||| chris roeloffzen ||| jos |||  capmany ||| david marpaung ||| 
2021 ||| transformer-based alarm context-vectorization representation for reliable alarm root cause identification in optical networks. ||| jinwei jia ||| danshi wang ||| chunyu zhang ||| hui yang ||| luyao guan ||| xue chen ||| min zhang ||| 
2021 ||| iterative se(3)-transformers. ||| fabian b. fuchs ||| edward wagstaff ||| justas dauparas ||| ingmar posner ||| 
2019 ||| enhanced air quality inference with mobile sensing attention mechanism: poster abstract. ||| ning liu ||| yue wang ||| jiayi huang ||| rui ma ||| lin zhang ||| 
2018 ||| speech emotion recognition via attention-based dnn from multi-task learning. ||| fei ma ||| weixi gu ||| wei zhang ||| shiguang ni ||| shao-lun huang ||| lin zhang ||| 
2018 ||| attention-based lstm-cnns for time-series classification. ||| qianjin du ||| weixi gu ||| lin zhang ||| shao-lun huang ||| 
2020 ||| bert4nilm: a bidirectional transformer model for non-intrusive load monitoring. ||| zhenrui yue ||| camilo requena witzig ||| daniel jorde ||| hans-arno jacobsen ||| 
2021 ||| detecting online risks and supportive interaction in instant messenger conversations using czech transformers. ||| ondrej sotol ||| r ||| jarom ||| r plh ||| k ||| michal tkaczyk ||| michaela lebed ||| kov ||| david smahel ||| 
2019 ||| dynamic fusion: attentional language model for neural machine translation. ||| michiki kurosawa ||| mamoru komachi ||| 
2020 ||| ram-net: a residual attention mobilenet to detect covid-19 cases from chest x-ray images. ||| md. aminur rab ratul ||| maryam tavakol elahi ||| kun yuan ||| won-sook lee ||| 
2019 ||| using bidirectional long short term memory with attention layer to estimate driver behavior. ||| shokoufeh monjezi kouchak ||| ashraf gaffar ||| 
2021 ||| predicting real-time scientific experiments using transformer models and reinforcement learning. ||| juan manuel parrilla gutierrez ||| 
2018 ||| a novel neural sequence model with multiple attentions for word sense disambiguation. ||| mahtab ahmed ||| muhammad rifayat samee ||| robert e. mercer ||| 
2021 ||| a transformer-based approach for translating natural language to bash commands. ||| quchen fu ||| zhongwei teng ||| jules white ||| douglas c. schmidt ||| 
2020 ||| gcn with clustering coefficients and attention module. ||| rakesh kumar yadav ||| abhishek ||| sourav s ||| shekhar verma ||| 
2018 ||| parallel attention mechanisms in neural machine translation. ||| julian richard medina ||| jugal kalita ||| 
2020 ||| on analyzing covid-19-related hate speech using bert attention. ||| nishant vishwamitra ||| ruijia (roger) hu ||| feng luo ||| long cheng ||| matthew costello ||| yin yang ||| 
2021 ||| super resolution with sparse gradient-guided attention for suppressing structural distortion. ||| geonhak song ||| tien-dung nguyen ||| junghyun bum ||| hwijong yi ||| chang-hwan son ||| hyunseung choo ||| 
2018 ||| inner attention based bi-lstms with indexing for non-factoid question answering. ||| akshay sharma ||| chetan harithas ||| 
2020 ||| dat-rnn: trajectory prediction with diverse attention. ||| zheng li ||| xiaocong du ||| yu cao ||| 
2021 ||| medical code prediction from discharge summary: document to sequence bert using sequence attention. ||| tak-sung heo ||| yongmin yoo ||| yeongjoon park ||| byeong-cheol jo ||| kyounguk lee ||| kyungsun kim ||| 
2020 ||| safl: a self-attention scene text recognizer with focal loss. ||| bao hieu tran ||| thanh le-cong ||| huu manh nguyen ||| duc anh le ||| thanh-hung nguyen ||| phi le nguyen ||| 
2018 ||| an attention-based air quality forecasting method. ||| bo liu ||| shuo yan ||| jianqiang li ||| guangzhi qu ||| yong li ||| jianlei lang ||| rentao gu ||| 
2021 ||| a study of the plausibility of attention between rnn encoders in natural language inference. ||| duc hau nguyen ||| guillaume gravier ||| pascale s ||| billot ||| 
2021 ||| leveraging transformers for starcraft macromanagement prediction. ||| muhammad junaid khan ||| shah hassan ||| gita sukthankar ||| 
2021 ||| sentiment analysis of stocktwits using transformer models. ||| aysun bozanta ||| sabrina angco ||| mucahit cevik ||| ayse basar ||| 
2021 ||| semi-supervised graph instance transformer for mental health inference. ||| guimin dong ||| mingyue tang ||| lihua cai ||| laura e. barnes ||| mehdi boukhechba ||| 
2020 ||| estimating the effect of general health checkup using uncertainty aware attention of deep instrument variable 2-stage network. ||| sangkyun jo ||| duk bin jun ||| sungho park ||| 
2021 ||| a physics-informed graph attention-based approach for power flow analysis. ||| ashkan b. jeddi ||| abdollah shafieezadeh ||| 
2019 ||| gram: gradient rescaling attention model for data uncertainty estimation in single image super resolution. ||| ki-seok chung ||| changwoo lee ||| 
2020 ||| a survey of biometric and machine learning methods for tracking students' attention and engagement. ||| maria villa ||| mikhail i. gofman ||| sinjini mitra ||| ali almadan ||| anoop krishnan ||| ajita rattani ||| 
2020 ||| covid-19 screening using residual attention network an artificial intelligence approach. ||| vishal sharma ||| curtis e. dyreson ||| 
2020 ||| interpreting attention models: lstm vs. cnn : a case study on customer activation. ||| koen weterings ||| shir-lee kimelman ||| stefano bromuri ||| marko c. j. d. van eekelen ||| 
2018 ||| unsupervised anomaly detection in energy time series data using variational recurrent autoencoders with attention. ||| jo ||| o pereira ||| margarida silveira ||| 
2021 ||| decoder transformer for temporally-embedded health outcome predictions. ||| omar boursalie ||| reza samavi ||| thomas e. doyle ||| 
2021 ||| transformer based bengali chatbot using general knowledge dataset. ||| abu kaisar mohammad masum ||| sheikh abujar ||| sharmin akter ||| nushrat jahan ria ||| syed akhter hossain ||| 
2019 ||| radar gesture recognition system in presence of interference using self-attention neural network. ||| souvik hazra ||| avik santra ||| 
2021 ||| active learning and machine teaching for online learning: a study of attention and labelling cost. ||| agnes tegen ||| paul davidsson ||| jan a. persson ||| 
2021 ||| self-attention mechanism in gans for molecule generation. ||| sandeep chinnareddy ||| pranav grandhi ||| apurva narayan ||| 
2021 ||| improving next-application prediction with deep personalized-attention neural network. ||| jun zhu ||| gautier viaud ||| c ||| line hudelot ||| 
2021 ||| attention on classification for fire segmentation. ||| milad niknejad ||| alexandre bernardino ||| 
2021 ||| continuous multi-modal emotion prediction in video based on recurrent neural network variants with attention. ||| joyal raju ||| yona falinie a. gaus ||| toby p. breckon ||| 
2021 ||| temporal bottleneck attention for video recognition. ||| schubert r. carvalho ||| nicolas m. bertagnolli ||| tyler folkman ||| 
2020 ||| disease state prediction from single-cell data using graph attention networks. ||| neal g. ravindra ||| arijit sehanobish ||| jenna l. pappalardo ||| david a. hafler ||| david van dijk ||| 
2020 ||| deidentification of free-text medical records using pre-trained bidirectional transformers. ||| alistair e. w. johnson ||| lucas bulgarelli ||| tom j. pollard ||| 
2020 ||| hhh: an online medical chatbot system based on knowledge graph and hierarchical bi-directional attention. ||| qiming bao ||| lin ni ||| jiamou liu ||| 
2020 ||| attentional matrix factorization with document-context awareness and implicit api relationship for service recommendation. ||| mo nguyen ||| jian yu ||| quan bai ||| sira yongchareon ||| yanbo han ||| 
2017 ||| character-level intra attention network for natural language inference. ||| han yang ||| marta r. costa-juss ||| jos |||  a. r. fonollosa ||| 
2017 ||| refining raw sentence representations for textual entailment recognition via attention. ||| jorge a. balazs ||| edison marrese-taylor ||| pablo loyola ||| yutaka matsuo ||| 
2017 ||| recurrent neural network-based sentence encoder with gated attention for natural language inference. ||| qian chen ||| xiaodan zhu ||| zhen-hua ling ||| si wei ||| hui jiang ||| diana inkpen ||| 
2017 ||| neural machine translation with phrasal attention. ||| yachao li ||| deyi xiong ||| min zhang ||| 
2020 ||| attention tracking for developers. ||| rozaliya amirova ||| 
2021 ||| empirical study of transformers for source code. ||| nadezhda chirkova ||| sergey troshin ||| 
2021 ||| fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. ||| sumon biswas ||| hridesh rajan ||| 
2020 ||| intellicode compose: code generation using transformer. ||| alexey svyatkovskiy ||| shao kun deng ||| shengyu fu ||| neel sundaresan ||| 
2018 ||| effects of e-games on the development of saudi children with attention deficit hyperactivity disorder cognitively, behaviourally and socially: an experimental study. ||| doaa sinnari ||| paul krause ||| maysoon f. abulkhair ||| 
2017 ||| modifications of driver attention post-distraction: a detection response task study. ||| oliver m. winzer ||| antonia s. conti ||| cristina olaverri-monreal ||| klaus bengler ||| 
2019 ||| automated behavioral modeling and pattern analysis of children with autism in a joint attention training application: a preliminary study. ||| tiffany ya tang ||| pinata winoto ||| 
2018 ||| automatic low-level overlays on presentations to support regaining an audience's attention. ||| walter ritter ||| guido kempter ||| isabella h ||| mmerle ||| andreas wohlgenannt ||| 
2021 ||| comparing eye tracking and head tracking during a visual attention task in immersive virtual reality. ||| jose llanes-jurado ||| javier mar ||| n-morales ||| masoud moghaddasi ||| jaikishan khatri ||| jaime guixeres ||| mariano alca ||| iz ||| 
2017 ||| attentional trade-offs under resource scarcity. ||| jiaying zhao ||| brandon m. tomm ||| 
2017 ||| interpretable feature maps for robot attention. ||| kasim terzic ||| j. m. hans du buf ||| 
2019 ||| hmi design for autonomous cars: investigating on driver's attention distribution. ||| weizhe chen ||| wei liu ||| 
2017 ||| guiding visual attention based on visual saliency map with projector-camera system. ||| hironori takimoto ||| katsumi yamamoto ||| akihiro kanagawa ||| mitsuyoshi kishihara ||| kensuke okubo ||| 
2021 ||| towards a computerized approach to identify attentional states of online learners. ||| indika karunaratne ||| ajantha s. atukorale ||| 
2020 ||| quantifying museum visitor attention using bluetooth proximity beacons. ||| jonathan d. l. casano ||| jenilyn l. agapito ||| abigail moreno ||| ma. mercedes t. rodrigo ||| 
2019 ||| eeg acquisition during the vr administration of resting state, attention, and image recognition tasks: a feasibility study. ||| greg rupp ||| chris berka ||| amir h. meghdadi ||| marissa c. mcconnell ||| mike storm ||| thomas zo ||| ga rams ||| y ||| ajay verma ||| 
2019 ||| attention assessment: evaluation of facial expressions of children with autism spectrum disorder. ||| bilikis banire ||| dena al-thani ||| mustapha makki ||| marwa k. qaraqe ||| kruthika anand ||| olcay bilge connor ||| kamran khowaja ||| bilal mansoor ||| 
2020 ||| basic study on incidence of micro-error in visual attention-controlled environment. ||| taisei ando ||| takehiko yamaguchi ||| tania giovannetti ||| maiko sakamoto ||| 
2021 ||| comparison study of attention between training in a simulator vs. live-fire range. ||| gregory p. kr ||| tzig ||| chet c. hembroff ||| billea ahlgrim ||| 
2018 ||| cyber vulnerability: an attentional dilemma. ||| joseph b. lyons ||| mark a. roebke ||| philip bobko ||| craig a. cox ||| 
2017 ||| attention value of motion graphics on digital signages. ||| tsubasa kato ||| nahomi maki ||| 
2017 ||| tactile acoustic devices: the effect on drowsiness during prolonged attentional tasks. ||| patrick m. langdon ||| maria karam ||| 
2017 ||| attention sharing in a virtual environment attracts others. ||| takuji narumi ||| yuta sakakibara ||| tomohiro tanikawa ||| michitaka hirose ||| 
2021 ||| attention-based design and selective exposure amid covid-19 misinformation sharing. ||| zaid amin ||| nazlena mohamad ali ||| alan f. smeaton ||| 
2018 ||| a robot-based cognitive assessment model based on visual working memory and attention level. ||| ali sharifara ||| ashwin ramesh babu ||| akilesh rajavenkatanarayanan ||| christopher collander ||| fillia makedon ||| 
2020 ||| visual attention of young and older drivers in takeover tasks of highly automated driving. ||| qijia peng ||| sunao iwaki ||| 
2021 ||| influence of a video game on children's attention to food: should games be served with a character during mealtime? ||| weiwei ma ||| bo liu ||| zhao liu ||| 
2021 ||| monitoring attention of crane operators during load oscillations using gaze entropy measures. ||| jouh yeong chew ||| koichi ohtomi ||| hiromasa suzuki ||| 
2020 ||| brain activation in virtual reality for attention guidance. ||| philipp ulsamer ||| kevin pfeffel ||| nicholas h. m ||| ller ||| 
2018 ||| a novel way of estimating a user's focus of attention in a virtual environment. ||| xuanchao he ||| zhejun liu ||| 
2018 ||| a method of evaluating user visual attention to moving objects in head mounted virtual reality. ||| shi huang ||| 
2021 ||| a comparative analysis of attention to facial recognition payment between china and south korea: a news analysis using latent dirichlet allocation. ||| shaopeng che ||| dongyan nan ||| pim kamphuis ||| jang hyun kim ||| 
2020 ||| the relation between video game experience and children's attentional networks. ||| hui li ||| muyun long ||| kaveri subrahmanyam ||| 
2017 ||| a comparison of attention estimation techniques in a public display scenario. ||| wolfgang narzt ||| 
2018 ||| product web page design: a psychophysiological investigation of the influence of product similarity, visual proximity on attention and performance. ||| carolane juan ||| da ||| sylvain s ||| n ||| cal ||| pierre-majorique l ||| ger ||| 
2019 ||| semi-automatic aggregation of multiple models of visual attention for model-based user interface evaluation. ||| dennis knoop ||| bertram wortelen ||| marcus behrendt ||| 
2020 ||| information visualization design of nuclear power control system based on attention capture mechanism. ||| xiaoli wu ||| panpan xu ||| 
2017 ||| psychophysiological and intraoperative aeps and seps monitoring for perception, attention and cognition. ||| sergey lytaev ||| mikhail aleksandrov ||| aleksei ulitin ||| 
2020 ||| age-related differences in takeover request modality preferences and attention allocation during semi-autonomous driving. ||| gaojian huang ||| brandon pitts ||| 
2018 ||| use bci to generate attention-based metadata for the assessment of effective learning duration. ||| yang ting shen ||| xin mao chen ||| pei wen lu ||| ju chuan wu ||| 
2017 ||| using portable eeg to assess human visual attention. ||| olave e. krigolson ||| chad c. williams ||| francisco l. colino ||| 
2020 ||| investigating the relation between sense of presence, attention and performance: virtual reality versus web. ||| aliane loureiro krassmann ||| fabr ||| cio herpich ||| liane margarida rockenbach tarouco ||| magda bercht ||| 
2020 ||| attention! designing a target group-oriented risk communication strategy. ||| lara raffel ||| patrick bours ||| sashidharan komandur ||| 
2017 ||| mentally imagined item captures attention during visual search. ||| haifeng li ||| xiaomei li ||| 
2019 ||| attentional dynamics after take-over requests: the need for handover assistance systems in highly automated vehicles. ||| tobias vogelpohl ||| mark vollrath ||| 
2017 ||| a comparison of an attention acknowledgement measure and eye tracking: application of the as low as reasonable assessment (alara) discount usability principle for control system studies. ||| thomas a. ulrich ||| ronald l. boring ||| steffen werner ||| roger t. lew ||| 
2020 ||| the impact of advertisements on user attention during permission authorization. ||| yousra javed ||| elham al qahtani ||| mohamed shehab ||| 
2017 ||| testing the specificity of eeg neurofeedback training on first- and second-order measures of attention. ||| eddy j. davelaar ||| 
2019 ||| visual symbol attention and cross-cultural communication - a case study of catering commercial graphic advertising. ||| huang zhang ||| li zhang ||| 
2017 ||| driver's multi-attribute task battery performance and attentional switch cost are correlated with speeding behavior in simulated driving. ||| jie zhang ||| mengnuo dai ||| feng du ||| 
2018 ||| cognitos: a student-centric working environment for an attention-aware intelligent classroom. ||| anastasia ntagianta ||| maria korozi ||| asterios leonidis ||| margherita antona ||| constantine stephanidis ||| 
2020 ||| nature at your service - nature inspired representations combined with eye-gaze features to infer user attention and provide contextualized support. ||| carla barreiros ||| nelson silva ||| viktoria pammer-schindler ||| eduardo e. veas ||| 
2019 ||| breaking down the "wall of text" - software tool to address complex assignments for students with attention disorders. ||| breanna desrochers ||| ella tuson ||| syed asad r. rizvi ||| john j. magee ||| 
2018 ||| measuring focused attention using fixation inner-density. ||| wen liu ||| soussan djamasbi ||| andrew c. trapp ||| mina shojaeizadeh ||| 
2020 ||| software log anomaly detection through one class clustering of transformer encoder representation. ||| rin hirakawa ||| keitaro tominaga ||| yoshihisa nakatoh ||| 
2021 ||| attention to breathing in response to vibrational and verbal cues in mindfulness meditation mediated by wearable devices. ||| eunseong kim ||| jeongyun heo ||| jeongmin han ||| 
2021 ||| body-part attention probability for measuring gaze during impression word evaluation. ||| ken kinoshita ||| michiko inoue ||| masashi nishiyama ||| yoshio iwai ||| 
2019 ||| effect of mental fatigue on visual selective attention. ||| qian-yiang zhou ||| jiaxuan li ||| zhongqi liu ||| 
2020 ||| the storm project: using video game to promote completion of morning routine for children with attention deficit hyperactivity disorder and autism spectrum disorder. ||| laurence p ||| pin-beauchesne ||| dany lussier-desrochers ||| annie-claude villeneuve ||| marie- ||| ve dupont ||| line mass ||| annie martineau ||| 
2017 ||| patterns of attention: how data visualizations are read. ||| laura e. matzen ||| michael j. haass ||| kristin m. divis ||| mallory c. stites ||| 
2020 ||| adapting interaction to address critical user states of high workload and incorrect attentional focus - an evaluation of five adaptation strategies. ||| sven fuchs ||| stephanie hochgeschurz ||| alina schmitz-h ||| bsch ||| lerke thiele ||| 
2017 ||| design and evaluation of an assistive window for soft keyboards of tablet pcs that reduces visual attention shifts. ||| bomyeong kim ||| kyungdoh kim ||| jinho ahn ||| robert w. proctor ||| 
2021 ||| preliminary findings from a single session of virtual reality attentional bias modification training in healthy women. ||| bruno porras-garcia ||| alana singh ||| helena miquel ||| marta ferrer-garc ||| a ||| sergio lopez ||| guillem hopmans ||| jesus fleta ||| jos |||  guti ||| rrez-maldonado ||| 
2020 ||| attentional autoencoder for course recommendation in mooc with course relevance. ||| jindan tan ||| liang chang ||| tieyuan liu ||| xuemei zhao ||| 
2018 ||| self-information of the variation in spatial parameter based audio attention model. ||| hua-dong zhu ||| cong zhang ||| bo hang ||| song wang ||| yu liu ||| 
2020 ||| lar: a user behavior prediction model in server log based on lstm-attention network and rsc algorithm. ||| yingying shang ||| 
2020 ||| hierarchy spatial-temporal transformer for action recognition in short videos. ||| guoyong cai ||| yumeng cai ||| 
2018 ||| the application of big data to improve the users' attention to we-media. ||| jianghui liu ||| weibo huang ||| wen-jing he ||| lingxi ruan ||| xiaodan li ||| 
2019 ||| differential item functioning for boys and girls in a screening instrument for attention deficit hyperactivity disorder. ||| sebastian appelbaum ||| rolf lefering ||| christian wolff ||| martin j. tomasik ||| thomas ostermann ||| 
2021 ||| transformer based spatial-temporal fusion network for metro passenger flow forecasting. ||| weiqi zhang ||| chen zhang ||| fugee tsung ||| 
2021 ||| a novel approach of fault diagnosis based on multi-source signals and attention mechanism. ||| liuen guan ||| xiaodong zhai ||| xuan tu ||| fei qiao ||| 
2020 ||| a soft graph attention reinforcement learning for multi-agent cooperation. ||| huimu wang ||| zhiqiang pu ||| zhen liu ||| jianqiang yi ||| tenghai qiu ||| 
2021 ||| an attention transfer model for human-assisted failure avoidance in robot manipulations. ||| boyi song ||| yuntao peng ||| ruijiao luo ||| rui liu ||| 
2021 ||| repairing human trust by promptly correcting robot mistakes with an attention transfer model. ||| ruijiao luo ||| chao huang ||| yuntao peng ||| boyi song ||| rui liu ||| 
2020 ||| weak scratch detection of optical components using attention fusion network. ||| xian tao ||| dapeng zhang ||| avinash kumar singh ||| mukesh prasad ||| chin-teng lin ||| de xu ||| 
2021 ||| multi-zone indoor temperature prediction based on graph attention network and gated recurrent unit. ||| chunxiang zhou ||| zhanbo xu ||| jiang wu ||| kun liu ||| xiaohong guan ||| 
2020 ||| attention based graph covolution networks for intelligent traffic flow analysis. ||| hongxin zhang ||| jiaxin liu ||| ying tang ||| gang xiong ||| 
2020 ||| mobile agents' dynamic small-world network based on attention mechanism. ||| rong xie ||| 
2020 ||| how self-attention improves rare class performance in a question-answering dialogue agent. ||| adam stiff ||| qi song ||| eric fosler-lussier ||| 
2021 ||| domain-independent user simulation with transformers for task-oriented dialogue systems. ||| hsien-chin lin ||| nurul lubis ||| songbo hu ||| carel van niekerk ||| christian geishauser ||| michael heck ||| shutong feng ||| milica gasic ||| 
2021 ||| a task-oriented dialogue architecture via transformer neural language models and symbolic injection. ||| oscar j. romero ||| antian wang ||| john zimmerman ||| aaron steinfeld ||| anthony tomasic ||| 
2020 ||| counseling-style reflection generation using generative pretrained transformers with augmented context. ||| siqi shen ||| charles welch ||| rada mihalcea ||| ver ||| nica p ||| rez-rosas ||| 
2021 ||| weakly supervised extractive summarization with attention. ||| yingying zhuang ||| yichao lu ||| simi wang ||| 
2017 ||| it takes two to tango: modification of siamese long short term memory network with attention mechanism in recognizing argumentative relations in persuasive essay. ||| aryo pradipta gema ||| suhendro winton ||| theodorus david ||| derwin suhartono ||| muhsin shodiq ||| wikaria gazali ||| 
2020 ||| automatic content analysis of computer-supported collaborative inquiry-based learning using deep networks and attention mechanisms. ||| pablo uribe ||| abelino jim ||| nez ||| roberto araya ||| joni l ||| ms ||| raija h ||| m ||| l ||| inen ||| jouni viiri ||| 
2019 ||| how can a robot calculate the level of visual focus of human's attention. ||| partha chakraborty ||| mohammad abu yousuf ||| md. zahidur rahman ||| nuruzzaman faruqui ||| 
2020 ||| an attention-based approach for traffic conditions forecasting considering spatial-temporal features. ||| lu tao ||| yuanli gu ||| wenqi lu ||| xiaoping rui ||| tian zhou ||| ying ding ||| 
2018 ||| adversarial transfer learning for chinese named entity recognition with self-attention mechanism. ||| pengfei cao ||| yubo chen ||| kang liu ||| jun zhao ||| shengping liu ||| 
2021 ||| discodvt: generating long text with discourse-aware discrete variational transformer. ||| haozhe ji ||| minlie huang ||| 
2020 ||| factorized transformer for multi-domain neural machine translation. ||| yongchao deng ||| hongfei yu ||| heng yu ||| xiangyu duan ||| weihua luo ||| 
2021 ||| gradient-based adversarial attacks against text transformers. ||| chuan guo ||| alexandre sablayrolles ||| herv |||  j ||| gou ||| douwe kiela ||| 
2019 ||| self-attention enhanced cnns and collaborative curriculum learning for distantly supervised relation extraction. ||| yuyun huang ||| jinhua du ||| 
2020 ||| cascaded semantic and positional self-attention network for document classification. ||| juyong jiang ||| jie zhang ||| kai zhang ||| 
2019 ||| humor detection: a transformer gets the last laugh. ||| orion weller ||| kevin d. seppi ||| 
2017 ||| satirical news detection and analysis using attention mechanism and linguistic features. ||| fan yang ||| arjun mukherjee ||| eduard constantin dragut ||| 
2018 ||| a genre-aware attention model to improve the likability prediction of books. ||| suraj maharjan ||| manuel montes-y-g ||| mez ||| fabio a. gonz ||| lez ||| thamar solorio ||| 
2018 ||| an analysis of encoder representations in transformer-based machine translation. ||| alessandro raganato ||| j ||| rg tiedemann ||| 
2019 ||| event detection with multi-order graph convolution and aggregated attention. ||| haoran yan ||| xiaolong jin ||| xiangbin meng ||| jiafeng guo ||| xueqi cheng ||| 
2017 ||| coarse-to-fine attention models for document summarization. ||| jeffrey ling ||| alexander m. rush ||| 
2017 ||| towards bidirectional hierarchical representations for attention-based neural machine translation. ||| baosong yang ||| derek f. wong ||| tong xiao ||| lidia s. chao ||| jingbo zhu ||| 
2021 ||| cross-attention is all you need: adapting pretrained transformers for machine translation. ||| mozhdeh gheini ||| xiang ren ||| jonathan may ||| 
2019 ||| a gated self-attention memory network for answer selection. ||| tuan manh lai ||| quan hung tran ||| trung bui ||| daisuke kihara ||| 
2020 ||| blockwise self-attention for long document understanding. ||| jiezhong qiu ||| hao ma ||| omer levy ||| wen-tau yih ||| sinong wang ||| jie tang ||| 
2019 ||| jointly learning to align and translate with transformer models. ||| sarthak garg ||| stephan peitz ||| udhyakumar nallasamy ||| matthias paulik ||| 
2021 ||| the devil is in the detail: simple tricks improve systematic generalization of transformers. ||| r ||| bert csord ||| s ||| kazuki irie ||| j ||| rgen schmidhuber ||| 
2018 ||| a hierarchical neural attention-based text classifier. ||| koustuv sinha ||| yue dong ||| jackie chi kit cheung ||| derek ruths ||| 
2018 ||| integrating transformer and paraphrase rules for sentence simplification. ||| sanqiang zhao ||| rui meng ||| daqing he ||| andi saptono ||| bambang parmanto ||| 
2018 ||| exploiting attention to reveal shortcomings in memory models. ||| kaylee burns ||| aida nematzadeh ||| erin grant ||| alison gopnik ||| thomas l. griffiths ||| 
2021 ||| pushing on text readability assessment: a transformer meets handcrafted linguistic features. ||| bruce w. lee ||| yoo sung jang ||| jason hyung-jong lee ||| 
2019 ||| the bottom-up evolution of representations in the transformer: a study with machine translation and language modeling objectives. ||| elena voita ||| rico sennrich ||| ivan titov ||| 
2020 ||| losing heads in the lottery: pruning transformer attention in neural machine translation. ||| maximiliana behnke ||| kenneth heafield ||| 
2021 ||| universal-kd: attention-based output-grounded intermediate layer knowledge distillation. ||| yimeng wu ||| mehdi rezagholizadeh ||| abbas ghaddar ||| md. akmal haidar ||| ali ghodsi ||| 
2020 ||| social commonsense reasoning with multi-head knowledge attention. ||| debjit paul ||| anette frank ||| 
2019 ||| making asynchronous stochastic gradient descent work for transformers. ||| alham fikri aji ||| kenneth heafield ||| 
2021 ||| veealign: multifaceted context representation using dual attention for ontology alignment. ||| vivek iyer ||| arvind agarwal ||| harshit kumar ||| 
2018 ||| why self-attention? a targeted evaluation of neural machine translation architectures. ||| gongbo tang ||| mathias m ||| ller ||| annette rios ||| rico sennrich ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| joan plepi ||| lucie flek ||| 
2020 ||| transformers: state-of-the-art natural language processing. ||| thomas wolf ||| lysandre debut ||| victor sanh ||| julien chaumond ||| clement delangue ||| anthony moi ||| pierric cistac ||| tim rault ||| r ||| mi louf ||| morgan funtowicz ||| joe davison ||| sam shleifer ||| patrick von platen ||| clara ma ||| yacine jernite ||| julien plu ||| canwen xu ||| teven le scao ||| sylvain gugger ||| mariama drame ||| quentin lhoest ||| alexander m. rush ||| 
2021 ||| contrastive document representation learning with graph attention networks. ||| peng xu ||| xinchi chen ||| xiaofei ma ||| zhiheng huang ||| bing xiang ||| 
2018 ||| paragraph-level neural question generation with maxout pointer and gated self-attention networks. ||| yao zhao ||| xiaochuan ni ||| yuanyuan ding ||| qifa ke ||| 
2021 ||| transformer feed-forward layers are key-value memories. ||| mor geva ||| roei schuster ||| jonathan berant ||| omer levy ||| 
2020 ||| calibration of pre-trained transformers. ||| shrey desai ||| greg durrett ||| 
2018 ||| learning universal sentence representations with mean-max attention autoencoder. ||| minghua zhang ||| yunfang wu ||| weikang li ||| wei li ||| 
2021 ||| melt: message-level transformer with masked document representations as pre-training for stance detection. ||| matthew matero ||| nikita soni ||| niranjan balasubramanian ||| h. andrew schwartz ||| 
2021 ||| when attention meets fast recurrence: training language models with reduced compute. ||| tao lei ||| 
2017 ||| cascaded attention based unsupervised information distillation for compressive summarization. ||| piji li ||| wai lam ||| lidong bing ||| weiwei guo ||| hang li ||| 
2021 ||| inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks. ||| yichen jiang ||| mohit bansal ||| 
2017 ||| multi-task attention-based neural networks for implicit discourse relationship representation and identification. ||| man lan ||| jianxiang wang ||| yuanbin wu ||| zheng-yu niu ||| haifeng wang ||| 
2019 ||| cloze-driven pretraining of self-attention networks. ||| alexei baevski ||| sergey edunov ||| yinhan liu ||| luke zettlemoyer ||| michael auli ||| 
2020 ||| attention is all you need for chinese word segmentation. ||| sufeng duan ||| hai zhao ||| 
2019 ||| neural news recommendation with multi-head self-attention. ||| chuhan wu ||| fangzhao wu ||| suyu ge ||| tao qi ||| yongfeng huang ||| xing xie ||| 
2019 ||| syntax-enhanced self-attention-based semantic role labeling. ||| yue zhang ||| rui wang ||| luo si ||| 
2020 ||| investigating african-american vernacular english in transformer-based text generation. ||| sophie groenwold ||| lily ou ||| aesha parekh ||| samhita honnavalli ||| sharon levy ||| diba mirza ||| william yang wang ||| 
2021 ||| value-aware approximate attention. ||| ankit gupta ||| jonathan berant ||| 
2021 ||| what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers. ||| boseop kim ||| hyoungseok kim ||| sang-woo lee ||| gichang lee ||| dong-hyun kwak ||| dong hyeon jeon ||| sunghyun park ||| sungju kim ||| seonhoon kim ||| dongpil seo ||| heungsub lee ||| minyoung jeong ||| sungjae lee ||| minsub kim ||| sukhyun ko ||| seokhun kim ||| taeyong park ||| jinuk kim ||| soyoung kang ||| na-hyeon ryu ||| kang min yoo ||| minsuk chang ||| soobin suh ||| sookyo in ||| jinseong park ||| kyungduk kim ||| hiun kim ||| jisu jeong ||| yong goo yeo ||| donghoon ham ||| dongju park ||| min young lee ||| jaewook kang ||| inho kang ||| jung-woo ha ||| woo-myoung park ||| nako sung ||| 
2020 ||| towards reasonably-sized character-level transformer nmt by finetuning subword systems. ||| jindrich libovick ||| alexander fraser ||| 
2018 ||| attention-based capsule network with dynamic routing for relation extraction. ||| ningyu zhang ||| shumin deng ||| zhanling sun ||| xi chen ||| wei zhang ||| huajun chen ||| 
2020 ||| an empirical study of pre-trained transformers for arabic information extraction. ||| wuwei lan ||| yang chen ||| wei xu ||| alan ritter ||| 
2018 ||| phrase-level self-attention networks for universal sentence encoding. ||| wei wu ||| houfeng wang ||| tianyu liu ||| shuming ma ||| 
2021 ||| sentence bottleneck autoencoders from transformer language models. ||| ivan montero ||| nikolaos pappas ||| noah a. smith ||| 
2021 ||| on pursuit of designing multi-modal transformer for video grounding. ||| meng cao ||| long chen ||| mike zheng shou ||| can zhang ||| yuexian zou ||| 
2020 ||| learning to fuse sentences with transformers for summarization. ||| logan lebanoff ||| franck dernoncourt ||| doo soon kim ||| lidan wang ||| walter chang ||| fei liu ||| 
2020 ||| from zero to hero: on the limitations of zero-shot language transfer with multilingual transformers. ||| anne lauscher ||| vinit ravishankar ||| ivan vulic ||| goran glavas ||| 
2020 ||| transformer-gcrf: recovering chinese dropped pronouns with general conditional random fields. ||| jingxuan yang ||| kerui xu ||| jun xu ||| si li ||| sheng gao ||| jun guo ||| ji-rong wen ||| nianwen xue ||| 
2020 ||| adaptive attentional network for few-shot knowledge graph completion. ||| jiawei sheng ||| shu guo ||| zhenyu chen ||| juwei yue ||| lihong wang ||| tingwen liu ||| hongbo xu ||| 
2018 ||| a visual attention grounding neural model for multimodal machine translation. ||| mingyang zhou ||| runxiang cheng ||| yong jae lee ||| zhou yu ||| 
2021 ||| rumor detection on twitter with claim-guided hierarchical graph attention networks. ||| hongzhan lin ||| jing ma ||| mingfei cheng ||| zhiwei yang ||| liangliang chen ||| guang chen ||| 
2021 ||| translation as cross-domain knowledge: attention augmentation for unsupervised cross-domain segmenting and labeling tasks. ||| ruixuan luo ||| yi zhang ||| sishuo chen ||| xu sun ||| 
2019 ||| mixed multi-head self-attention for neural machine translation. ||| hongyi cui ||| shohei iida ||| po-hsuan hung ||| takehito utsuro ||| masaaki nagata ||| 
2020 ||| a dual-attention network for joint named entity recognition and sentence classification of adverse drug events. ||| susmitha wunnava ||| xiao qin ||| tabassum kakar ||| xiangnan kong ||| elke a. rundensteiner ||| 
2019 ||| heterogeneous graph attention networks for semi-supervised short text classification. ||| linmei hu ||| tianchi yang ||| chuan shi ||| houye ji ||| xiaoli li ||| 
2019 ||| context-aware interactive attention for multi-modal sentiment and emotion analysis. ||| dushyant singh chauhan ||| md. shad akhtar ||| asif ekbal ||| pushpak bhattacharyya ||| 
2019 ||| contrastive attention mechanism for abstractive sentence summarization. ||| xiangyu duan ||| hongfei yu ||| mingming yin ||| min zhang ||| weihua luo ||| yue zhang ||| 
2019 ||| negative focus detection via contextual attention mechanism. ||| longxiang shen ||| bowei zou ||| yu hong ||| guodong zhou ||| qiaoming zhu ||| aiti aw ||| 
2021 ||| synchronous dual network with cross-type attention for joint entity and relation extraction. ||| hui wu ||| xiaodong shi ||| 
2020 ||| analyzing redundancy in pretrained transformer models. ||| fahim dalvi ||| hassan sajjad ||| nadir durrani ||| yonatan belinkov ||| 
2020 ||| improve transformer models with better relative position embeddings. ||| zhiheng huang ||| davis liang ||| peng xu ||| bing xiang ||| 
2020 ||| table fact verification with structure-aware transformer. ||| hongzhi zhang ||| yingyao wang ||| sirui wang ||| xuezhi cao ||| fuzheng zhang ||| zhongyuan wang ||| 
2021 ||| multi-vector attention models for deep re-ranking. ||| giulio zhou ||| jacob devlin ||| 
2020 ||| less is more: attention supervision with counterfactuals for text classification. ||| seungtaek choi ||| haeju park ||| jinyoung yeo ||| seung-won hwang ||| 
2018 ||| a wordnet-encoded collocation-attention network for homographic pun recognition. ||| yufeng diao ||| hongfei lin ||| di wu ||| liang yang ||| kan xu ||| zhihao yang ||| jian wang ||| shaowu zhang ||| bo xu ||| dongyu zhang ||| 
2020 ||| kermit: complementing transformer architectures with encoders of explicit syntactic interpretations. ||| fabio massimo zanzotto ||| andrea santilli ||| leonardo ranaldi ||| dario onorati ||| pierfrancesco tommasino ||| francesca fallucchi ||| 
2021 ||| sparse attention with linear units. ||| biao zhang ||| ivan titov ||| rico sennrich ||| 
2020 ||| be more with less: hypergraph attention networks for inductive text classification. ||| kaize ding ||| jianling wang ||| jundong li ||| dingcheng li ||| huan liu ||| 
2020 ||| luke: deep contextualized entity representations with entity-aware self-attention. ||| ikuya yamada ||| akari asai ||| hiroyuki shindo ||| hideaki takeda ||| yuji matsumoto ||| 
2020 ||| x-lxmert: paint, caption and answer questions with multi-modal transformers. ||| jaemin cho ||| jiasen lu ||| dustin schwenk ||| hannaneh hajishirzi ||| aniruddha kembhavi ||| 
2019 ||| latent suicide risk detection on microblog via suicide-oriented word embeddings and layered attention. ||| lei cao ||| huijun zhang ||| ling feng ||| zihan wei ||| xin wang ||| ningyun li ||| xiaohao he ||| 
2021 ||| bidirectional hierarchical attention networks based on document-level context for emotion cause extraction. ||| guimin hu ||| guangming lu ||| yi zhao ||| 
2020 ||| multilevel text alignment with cross-document attention. ||| xuhui zhou ||| nikolaos pappas ||| noah a. smith ||| 
2021 ||| structure-aware fine-tuning of sequence-to-sequence transformers for transition-based amr parsing. ||| jiawei zhou ||| tahira naseem ||| ram ||| n fernandez astudillo ||| young-suk lee ||| radu florian ||| salim roukos ||| 
2021 ||| deep attention diffusion graph neural networks for text classification. ||| yonghao liu ||| renchu guan ||| fausto giunchiglia ||| yanchun liang ||| xiaoyue feng ||| 
2021 ||| stanker: stacking network based on level-grained attention-masked bert for rumor detection on social media. ||| dongning rao ||| xin miao ||| zhihua jiang ||| ran li ||| 
2021 ||| what's in your head? emergent behaviour in multi-task transformer models. ||| mor geva ||| uri katz ||| aviv ben-arie ||| jonathan berant ||| 
2020 ||| scheduled drophead: a regularization method for transformer models. ||| wangchunshu zhou ||| tao ge ||| furu wei ||| ming zhou ||| ke xu ||| 
2021 ||| contrastive out-of-distribution detection for pretrained transformers. ||| wenxuan zhou ||| fangyu liu ||| muhao chen ||| 
2021 ||| attention weights in transformer nmt fail aligning words between sequences but largely explain model predictions. ||| javier ferrando ||| marta r. costa-juss ||| 
2019 ||| fine-tune bert with sparse self-attention mechanism. ||| baiyun cui ||| yingming li ||| ming chen ||| zhongfei zhang ||| 
2018 ||| interpretable structure induction via sparse attention. ||| ben peters ||| vlad niculae ||| andr |||  f. t. martins ||| 
2021 ||| gated transformer for robust de-noised sequence-to-sequence modelling. ||| ayan sengupta ||| amit kumar ||| sourabh kumar bhattacharjee ||| suman roy ||| 
2017 ||| no need to pay attention: simple recurrent neural networks work! ||| ferhan t ||| re ||| oliver jojic ||| 
2021 ||| effective convolutional attention network for multi-label clinical document classification. ||| yang liu ||| hua cheng ||| russell klopfer ||| matthew r. gormley ||| thomas schaaf ||| 
2020 ||| unsupervised extractive summarization by pre-training hierarchical transformers. ||| shusheng xu ||| xingxing zhang ||| yi wu ||| furu wei ||| ming zhou ||| 
2018 ||| supervised domain enablement attention for personalized domain classification. ||| joo-kyung kim ||| young-bum kim ||| 
2018 ||| co-stack residual affinity networks with multi-level attention refinement for matching text sequences. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| 
2018 ||| improving multi-label emotion classification via sentiment classification with dual attention transfer network. ||| jianfei yu ||| lu ||| s marujo ||| jing jiang ||| pradeep karuturi ||| william brendel ||| 
2018 ||| hierarchical relation extraction with coarse-to-fine grained attention. ||| xu han ||| pengfei yu ||| zhiyuan liu ||| maosong sun ||| peng li ||| 
2020 ||| how can self-attention networks recognize dyck-n languages? ||| javid ebrahimi ||| dhruv gelda ||| wei zhang ||| 
2019 ||| enhancing dialogue symptom diagnosis with global attention and symptom graph. ||| xinzhu lin ||| xiahui he ||| qin chen ||| huaixiao tou ||| zhongyu wei ||| ting chen ||| 
2017 ||| deeper attention to abusive user content moderation. ||| john pavlopoulos ||| prodromos malakasiotis ||| ion androutsopoulos ||| 
2021 ||| enjoy the salience: towards better transformer-based faithful explanations with word salience. ||| george chrysostomou ||| nikolaos aletras ||| 
2019 ||| improving relation extraction with knowledge-attention. ||| pengfei li ||| kezhi mao ||| xuefeng yang ||| qi li ||| 
2018 ||| extracting syntactic trees from transformer encoder self-attentions. ||| david marecek ||| rudolf rosa ||| 
2020 ||| pre-training transformers as energy-based cloze models. ||| kevin clark ||| minh-thang luong ||| quoc v. le ||| christopher d. manning ||| 
2020 ||| on the ability and limitations of transformers to recognize formal languages. ||| satwik bhattamishra ||| kabir ahuja ||| navin goyal ||| 
2021 ||| subformer: exploring weight sharing for parameter efficiency in generative transformers. ||| machel reid ||| edison marrese-taylor ||| yutaka matsuo ||| 
2018 ||| training deeper neural machine translation models with transparent attention. ||| ankur bapna ||| mia xu chen ||| orhan firat ||| yuan cao ||| yonghui wu ||| 
2018 ||| deriving machine attention from human rationales. ||| yujia bao ||| shiyu chang ||| mo yu ||| regina barzilay ||| 
2020 ||| transfer learning and distant supervision for multilingual transformer models: a study on african languages. ||| michael a. hedderich ||| david ifeoluwa adelani ||| dawei zhu ||| jesujoba o. alabi ||| udia markus ||| dietrich klakow ||| 
2020 ||| efficient transformer-based large scale language representations using hardware-friendly block structured pruning. ||| bingbing li ||| zhenglun kong ||| tianyun zhang ||| ji li ||| zhengang li ||| hang liu ||| caiwen ding ||| 
2021 ||| vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers. ||| stella frank ||| emanuele bugliarello ||| desmond elliott ||| 
2020 ||| extremely low bit transformer quantization for on-device neural machine translation. ||| insoo chung ||| byeongwook kim ||| yoonjung choi ||| se jung kwon ||| yongkweon jeon ||| baeseong park ||| sangha kim ||| dongsoo lee ||| 
2021 ||| self-attention graph residual convolutional networks for event detection with dependency relations. ||| anan liu ||| ning xu ||| haozhe liu ||| 
2021 ||| haconvgnn: hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks. ||| xuye liu ||| dakuo wang ||| april yi wang ||| yufang hou ||| lingfei wu ||| 
2020 ||| vd-bert: a unified vision and dialog transformer with bert. ||| yue wang ||| shafiq r. joty ||| michael r. lyu ||| irwin king ||| caiming xiong ||| steven c. h. hoi ||| 
2021 ||| on the effects of transformer size on in- and out-of-domain calibration. ||| soham dan ||| dan roth ||| 
2020 ||| a compare aggregate transformer for understanding document-grounded dialogue. ||| longxuan ma ||| wei-nan zhang ||| runxin sun ||| ting liu ||| 
2020 ||| multi-unit transformers for neural machine translation. ||| jianhao yan ||| fandong meng ||| jie zhou ||| 
2021 ||| consistent accelerated inference via confident adaptive transformers. ||| tal schuster ||| adam fisch ||| tommi s. jaakkola ||| regina barzilay ||| 
2020 ||| a time-aware transformer based model for suicide ideation detection on social media. ||| ramit sawhney ||| harshit joshi ||| saumya gandhi ||| rajiv ratn shah ||| 
2020 ||| discourse self-attention for discourse element identification in argumentative student essays. ||| wei song ||| ziyao song ||| ruiji fu ||| lizhen liu ||| miaomiao cheng ||| ting liu ||| 
2017 ||| stack-based multi-layer attention for transition-based dependency parsing. ||| zhirui zhang ||| shujie liu ||| mu li ||| ming zhou ||| enhong chen ||| 
2021 ||| recurrent attention for neural machine translation. ||| jiali zeng ||| shuangzhi wu ||| yongjing yin ||| yufan jiang ||| mu li ||| 
2021 ||| the stem cell hypothesis: dilemma behind multi-task learning with transformer encoders. ||| han he ||| jinho d. choi ||| 
2017 ||| interactive visualization and manipulation of attention-based neural machine translation. ||| jaesong lee ||| joong-hwi shin ||| jun-seok kim ||| 
2018 ||| attentive gated lexicon reader with contrastive contextual co-attention for sentiment classification. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| jian su ||| 
2021 ||| frequency effects on syntactic rule learning in transformers. ||| jason wei ||| dan garrette ||| tal linzen ||| ellie pavlick ||| 
2019 ||| low-rank hoca: efficient high-order cross-modal attention for video captioning. ||| tao jin ||| siyu huang ||| yingming li ||| zhongfei zhang ||| 
2020 ||| powertransformer: unsupervised controllable revision for biased language correction. ||| xinyao ma ||| maarten sap ||| hannah rashkin ||| yejin choi ||| 
2018 ||| a co-attention neural network model for emotion cause analysis with emotional context awareness. ||| xiangju li ||| kaisong song ||| shi feng ||| daling wang ||| yifei zhang ||| 
2021 ||| finetuning pretrained transformers into rnns. ||| jungo kasai ||| hao peng ||| yizhe zhang ||| dani yogatama ||| gabriel ilharco ||| nikolaos pappas ||| yi mao ||| weizhu chen ||| noah a. smith ||| 
2019 ||| prado: projection attention networks for document classification on-device. ||| prabhu kaliamoorthi ||| sujith ravi ||| zornitsa kozareva ||| 
2020 ||| pymt5: multi-mode translation of natural language and python code with transformers. ||| colin b. clement ||| dawn drain ||| jonathan timcheck ||| alexey svyatkovskiy ||| neel sundaresan ||| 
2018 ||| jointly multiple events extraction via attention-based graph information aggregation. ||| xiao liu ||| zhunchen luo ||| heyan huang ||| 
2020 ||| attnio: knowledge graph exploration with in-and-out attention flow for knowledge-grounded dialogue. ||| jaehun jung ||| bokyung son ||| sungwon lyu ||| 
2017 ||| a cognition based attention model for sentiment analysis. ||| yunfei long ||| qin lu ||| rong xiang ||| minglei li ||| chu-ren huang ||| 
2018 ||| hard non-monotonic attention for character-level transduction. ||| shijie wu ||| pamela shapiro ||| ryan cotterell ||| 
2020 |||  job description matching using context-aware transformer models. ||| changmao li ||| elaine fisher ||| rebecca thomas ||| steve pittard ||| vicki hertzberg ||| jinho d. choi ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| marius mosbach ||| anna khokhlova ||| michael a. hedderich ||| dietrich klakow ||| 
2020 ||| autoeter: automated entity type representation with relation-aware attention for knowledge graph embedding. ||| guanglin niu ||| bo li ||| yongfei zhang ||| shiliang pu ||| jingyang li ||| 
2020 ||| point to the expression: solving algebraic word problems using the expression-pointer transformer model. ||| bugeun kim ||| kyung seo ki ||| donggeon lee ||| gahgene gweon ||| 
2019 ||| transformer-based model for single documents neural summarization. ||| elozino egonmwan ||| yllias chali ||| 
2021 ||| hitrans: a hierarchical transformer network for nested named entity recognition. ||| zhiwei yang ||| jing ma ||| hechang chen ||| yunke zhang ||| yi chang ||| 
2020 ||| coupled hierarchical transformer for stance-aware rumor verification in social media conversations. ||| jianfei yu ||| jing jiang ||| ling min serena khoo ||| hai leong chieu ||| rui xia ||| 
2020 ||| convert: efficient and accurate conversational representations from transformers. ||| matthew henderson ||| i ||| igo casanueva ||| nikola mrksic ||| pei-hao su ||| tsung-hsien wen ||| ivan vulic ||| 
2020 ||| using pre-trained transformer for better lay summarization. ||| seungwon kim ||| 
2020 ||| text graph transformer for document classification. ||| haopeng zhang ||| jiawei zhang ||| 
2019 ||| modeling graph structure in transformer for better amr-to-text generation. ||| jie zhu ||| junhui li ||| muhua zhu ||| longhua qian ||| min zhang ||| guodong zhou ||| 
2019 ||| improving deep transformer with depth-scaled initialization and merged attention. ||| biao zhang ||| ivan titov ||| rico sennrich ||| 
2019 ||| enhanced transformer model for data-to-text generation. ||| li gong ||| josep maria crego ||| jean senellart ||| 
2020 ||| pair: planning and iterative refinement in pre-trained transformers for long text generation. ||| xinyu hua ||| lu wang ||| 
2018 ||| collective event detection via a hierarchical and bias tagging networks with gated multi-level attention mechanisms. ||| yubo chen ||| hang yang ||| kang liu ||| jun zhao ||| yantao jia ||| 
2020 ||| a concise model for multi-criteria chinese word segmentation with transformer encoder. ||| xipeng qiu ||| hengzhi pei ||| hang yan ||| xuanjing huang ||| 
2020 ||| structured attention for unsupervised dialogue structure induction. ||| liang qiu ||| yizhou zhao ||| weiyan shi ||| yuan liang ||| feng shi ||| tao yuan ||| zhou yu ||| song-chun zhu ||| 
2018 ||| surprisingly easy hard-attention for sequence to sequence learning. ||| shiv shankar ||| siddhant garg ||| sunita sarawagi ||| 
2021 ||| hitter: hierarchical transformers for knowledge graph embeddings. ||| sanxing chen ||| xiaodong liu ||| jianfeng gao ||| jian jiao ||| ruofei zhang ||| yangfeng ji ||| 
2020 ||| text segmentation by cross segment attention. ||| michal lukasik ||| boris dadachev ||| kishore papineni ||| gon ||| alo sim ||| es ||| 
2019 ||| hierarchically-refined label attention network for sequence labeling. ||| leyang cui ||| yue zhang ||| 
2020 ||| oie: multilingual open information extraction based on multi-head attention with bert. ||| youngbin ro ||| yukyung lee ||| pilsung kang ||| 
2020 ||| mmft-bert: multimodal fusion transformer with bert encodings for visual question answering. ||| aisha urooj khan ||| amir mazaheri ||| niels da vitoria lobo ||| mubarak shah ||| 
2018 ||| abstractive text-image summarization using multi-modal attentional hierarchical rnn. ||| jingqiang chen ||| hai zhuge ||| 
2018 ||| contextual inter-modal attention for multi-modal sentiment analysis. ||| deepanway ghosal ||| md. shad akhtar ||| dushyant singh chauhan ||| soujanya poria ||| asif ekbal ||| pushpak bhattacharyya ||| 
2019 ||| syntax-aware aspect level sentiment classification with graph attention networks. ||| binxuan huang ||| kathleen m. carley ||| 
2021 ||| are transformers a modern version of eliza? observations on french object verb agreement. ||| bingzhi li ||| guillaume wisniewski ||| beno ||| t crabb ||| 
2019 ||| interpretable relevant emotion ranking with event-driven attention. ||| yang yang ||| deyu zhou ||| yulan he ||| meng zhang ||| 
2020 ||| question directed graph attention network for numerical reasoning over text. ||| kunlong chen ||| weidi xu ||| xingyi cheng ||| zou xiaochuan ||| yuyu zhang ||| le song ||| taifeng wang ||| yuan qi ||| wei chu ||| 
2021 ||| contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems. ||| yanjie gou ||| yinjie lei ||| lingqiao liu ||| yong dai ||| chunxu shen ||| 
2019 ||| attention optimization for abstractive document summarization. ||| min gui ||| junfeng tian ||| rui wang ||| zhenglu yang ||| 
2021 ||| tag: gradient attack on transformer-based language models. ||| jieren deng ||| yijue wang ||| ji li ||| chenghong wang ||| chao shang ||| hang liu ||| sanguthevar rajasekaran ||| caiwen ding ||| 
2020 ||| etc: encoding long and structured inputs in transformers. ||| joshua ainslie ||| santiago onta ||| n ||| chris alberti ||| vaclav cvicek ||| zachary fisher ||| philip pham ||| anirudh ravula ||| sumit sanghai ||| qifan wang ||| li yang ||| 
2021 ||| narrative embedding: re-contextualization through attention. ||| sean wilner ||| daniel woolridge ||| madeleine glick ||| 
2018 ||| learning when to concentrate or divert attention: self-adaptive attention temperature for neural machine translation. ||| junyang lin ||| xu sun ||| xuancheng ren ||| muyu li ||| qi su ||| 
2021 ||| t3-vis: visual analytic for training and fine-tuning transformers in nlp. ||| raymond li ||| wen xiao ||| lanjun wang ||| hyeju jang ||| giuseppe carenini ||| 
2017 ||| efficient attention using a fixed-size memory representation. ||| denny britz ||| melody y. guan ||| minh-thang luong ||| 
2018 ||| document-level neural machine translation with hierarchical attention networks. ||| lesly miculicich werlen ||| dhananjay ram ||| nikolaos pappas ||| james henderson ||| 
2018 ||| linguistically-informed self-attention for semantic role labeling. ||| emma strubell ||| patrick verga ||| daniel andor ||| david weiss ||| andrew mccallum ||| 
2020 ||| transition-based parsing with stack-transformers. ||| ram ||| n fernandez astudillo ||| miguel ballesteros ||| tahira naseem ||| austin blodgett ||| radu florian ||| 
2019 ||| adaptively sparse transformers. ||| gon ||| alo m. correia ||| vlad niculae ||| andr |||  f. t. martins ||| 
2021 ||| towards incremental transformers: an empirical analysis of transformer models for incremental nlu. ||| patrick kahardipraja ||| brielen madureira ||| david schlangen ||| 
2021 ||| mt6: multilingual pretrained text-to-text transformer with translation pairs. ||| zewen chi ||| li dong ||| shuming ma ||| shaohan huang ||| saksham singhal ||| xian-ling mao ||| heyan huang ||| xia song ||| furu wei ||| 
2020 ||| compressing transformer-based semantic parsing models using compositional code embeddings. ||| prafull prakash ||| saurabh kumar shashidhar ||| wenlong zhao ||| subendhu rongali ||| haidar khan ||| michael kayser ||| 
2021 ||| progressive transformer-based generation of radiology reports. ||| farhad nooralahzadeh ||| nicolas perez gonzalez ||| thomas frauenfelder ||| koji fujimoto ||| michael krauthammer ||| 
2020 ||| inserting information bottleneck for attribution in transformers. ||| zhiying jiang ||| raphael tang ||| ji xin ||| jimmy lin ||| 
2020 ||| focus-constrained attention mechanism for cvae-based response generation. ||| zhi cui ||| yanran li ||| jiayi zhang ||| jianwei cui ||| chen wei ||| bin wang ||| 
2020 ||| on extractive and abstractive neural document summarization with transformer language models. ||| jonathan pilault ||| raymond li ||| sandeep subramanian ||| chris pal ||| 
2021 ||| lamad: a linguistic attentional model for arabic text diacritization. ||| raeed al-sabri ||| jianliang gao ||| 
2021 ||| mate: multi-view attention for table transformer efficiency. ||| julian eisenschlos ||| maharshi gor ||| thomas m ||| ller ||| william w. cohen ||| 
2020 ||| on the weak link between importance and prunability of attention heads. ||| aakriti budhraja ||| madhura pande ||| preksha nema ||| pratyush kumar ||| mitesh m. khapra ||| 
2020 ||| friendly topic assistant for transformer based abstractive summarization. ||| zhengjue wang ||| zhibin duan ||| hao zhang ||| chaojie wang ||| long tian ||| bo chen ||| mingyuan zhou ||| 
2020 ||| generating radiology reports via memory-driven transformer. ||| zhihong chen ||| yan song ||| tsung-hui chang ||| xiang wan ||| 
2020 ||| transformer based multi-source domain adaptation. ||| dustin wright ||| isabelle augenstein ||| 
2021 ||| grouped-attention for content-selection and content-plan generation. ||| bayu distiawan trisedya ||| xiaojie wang ||| jianzhong qi ||| rui zhang ||| qingjun cui ||| 
2020 ||| assessing phrasal representation and composition in transformers. ||| lang yu ||| allyson ettinger ||| 
2019 ||| video dialog via progressive inference and cross-transformer. ||| weike jin ||| zhou zhao ||| mao gu ||| jun xiao ||| furu wei ||| yueting zhuang ||| 
2020 ||| biomedical event extraction on graph edge-conditioned attention networks with hierarchical knowledge graphs. ||| kung-hsiang huang ||| mu yang ||| nanyun peng ||| 
2020 ||| summarizing chinese medical answer with graph convolution networks and question-focused dual attention. ||| ningyu zhang ||| shumin deng ||| juan li ||| xi chen ||| wei zhang ||| huajun chen ||| 
2019 ||| capsule network with interactive attention for aspect-level sentiment classification. ||| chunning du ||| haifeng sun ||| jingyu wang ||| qi qi ||| jianxin liao ||| tong xu ||| ming liu ||| 
2019 ||| weakly supervised attention networks for entity recognition. ||| barun patra ||| joel ruben antony moniz ||| 
2021 ||| arabictransformer: efficient large arabic language model with funnel transformer and electra objective. ||| sultan alrowili ||| vijay shanker ||| 
2020 ||| cross-media keyphrase prediction: a unified framework with multi-modality multi-head attention and image wordings. ||| yue wang ||| jing li ||| michael r. lyu ||| irwin king ||| 
2021 ||| transformer-based lexically constrained headline generation. ||| kosuke yamada ||| yuta hitomi ||| hideaki tamori ||| ryohei sasano ||| naoaki okazaki ||| kentaro inui ||| koichi takeda ||| 
2021 ||| enlivening redundant heads in multi-head self-attention for machine translation. ||| tianfu zhang ||| heyan huang ||| chong feng ||| longbing cao ||| 
2019 ||| transformer and seq2seq model for paraphrase generation. ||| elozino egonmwan ||| yllias chali ||| 
2020 ||| repulsive attention: rethinking multi-head attention as bayesian inference. ||| bang an ||| jie lyu ||| zhenyi wang ||| chunyuan li ||| changwei hu ||| fei tan ||| ruiyi zhang ||| yifan hu ||| changyou chen ||| 
2020 ||| graph-to-graph transformer for transition-based dependency parsing. ||| alireza mohammadshahi ||| james henderson ||| 
2021 ||| disentangling representations of text by masking transformers. ||| xiongyi zhang ||| jan-willem van de meent ||| byron c. wallace ||| 
2020 ||| how effective is task-agnostic data augmentation for pretrained transformers? ||| shayne longpre ||| yu wang ||| chris dubois ||| 
2021 ||| attention-based contrastive learning for winograd schemas. ||| tassilo klein ||| moin nabi ||| 
2018 ||| improving large-scale fact-checking using decomposable attention models and lexical tagging. ||| nayeon lee ||| chien-sheng wu ||| pascale fung ||| 
2019 ||| multi-granularity self-attention for neural machine translation. ||| jie hao ||| xing wang ||| shuming shi ||| jinfeng zhang ||| zhaopeng tu ||| 
2019 ||| transformer dissection: an unified understanding for transformer's attention via the lens of kernel. ||| yao-hung hubert tsai ||| shaojie bai ||| makoto yamada ||| louis-philippe morency ||| ruslan salakhutdinov ||| 
2017 ||| position-aware attention and supervised data improve slot filling. ||| yuhao zhang ||| victor zhong ||| danqi chen ||| gabor angeli ||| christopher d. manning ||| 
2019 ||| original semantics-oriented attention and deep fusion network for sentence matching. ||| mingtong liu ||| yujie zhang ||| jinan xu ||| yufeng chen ||| 
2021 ||| block pruning for faster transformers. ||| fran ||| ois lagunas ||| ella charlaix ||| victor sanh ||| alexander m. rush ||| 
2020 ||| query-key normalization for transformers. ||| alex henry ||| prudhvi raj dachapally ||| shubham shantaram pawar ||| yuxuan chen ||| 
2021 ||| artificial text detection via examining the topology of attention maps. ||| laida kushnareva ||| daniil cherniavskii ||| vladislav mikhailov ||| ekaterina artemova ||| serguei barannikov ||| alexander bernstein ||| irina piontkovskaya ||| dmitri piontkovski ||| evgeny burnaev ||| 
2019 ||| self-attention with structural position representations. ||| xing wang ||| zhaopeng tu ||| longyue wang ||| shuming shi ||| 
2019 ||| lxmert: learning cross-modality encoder representations from transformers. ||| hao tan ||| mohit bansal ||| 
2018 ||| importance of self-attention for sentiment analysis. ||| ga ||| l letarte ||| fr ||| d ||| rik paradis ||| philippe gigu ||| re ||| fran ||| ois laviolette ||| 
2018 ||| iterative recursive attention model for interpretable sequence classification. ||| martin tutek ||| jan snajder ||| 
2021 ||| do transformer modifications transfer across implementations and applications? ||| sharan narang ||| hyung won chung ||| yi tay ||| liam fedus ||| thibault f ||| vry ||| michael matena ||| karishma malkan ||| noah fiedel ||| noam shazeer ||| zhenzhong lan ||| yanqi zhou ||| wei li ||| nan ding ||| jake marcus ||| adam roberts ||| colin raffel ||| 
2019 ||| auto-sizing the transformer network: improving speed, efficiency, and performance for low-resource machine translation. ||| kenton murray ||| jeffery kinnison ||| toan q. nguyen ||| walter j. scheirer ||| david chiang ||| 
2021 ||| multimodal phased transformer for sentiment analysis. ||| junyan cheng ||| iordanis fostiropoulos ||| barry w. boehm ||| mohammad soleymani ||| 
2018 ||| visual interrogation of attention-based models for natural language inference and machine comprehension. ||| shusen liu ||| tao li ||| zhimin li ||| vivek srikumar ||| valerio pascucci ||| peer-timo bremer ||| 
2020 ||| fixed encoder self-attention patterns in transformer-based machine translation. ||| alessandro raganato ||| yves scherrer ||| j ||| rg tiedemann ||| 
2018 ||| multi-level structured self-attentions for distantly supervised relation extraction. ||| jinhua du ||| jingguang han ||| andy way ||| dadong wan ||| 
2019 ||| towards better modeling hierarchical structure for self-attention with ordered neurons. ||| jie hao ||| xing wang ||| shuming shi ||| jinfeng zhang ||| zhaopeng tu ||| 
2021 ||| effects of parameter norm growth during transformer training: inductive bias from gradient descent. ||| william merrill ||| vivek ramanujan ||| yoav goldberg ||| roy schwartz ||| noah a. smith ||| 
2021 ||| attentionrank: unsupervised keyphrase extraction using self and cross attentions. ||| haoran ding ||| xiao luo ||| 
2019 ||| can: constrained attention networks for multi-aspect sentiment analysis. ||| mengting hu ||| shiwan zhao ||| li zhang ||| keke cai ||| zhong su ||| renhong cheng ||| xiaowei shen ||| 
2021 ||| hetformer: heterogeneous transformer with sparse attention for long-text extractive summarization. ||| ye liu ||| jianguo zhang ||| yao wan ||| congying xia ||| lifang he ||| philip s. yu ||| 
2020 ||| improving constituency parsing with span attention. ||| yuanhe tian ||| yan song ||| fei xia ||| tong zhang ||| 
2020 ||| guiding attention for self-supervised learning with transformers. ||| ameet deshpande ||| karthik narasimhan ||| 
2018 ||| leveraging gloss knowledge in neural word sense disambiguation by hierarchical co-attention. ||| fuli luo ||| tianyu liu ||| zexue he ||| qiaolin xia ||| zhifang sui ||| baobao chang ||| 
2021 ||| exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media. ||| shivam raval ||| hooman sedghamiz ||| enrico santus ||| tuka alhanai ||| mohammad m. ghassemi ||| emmanuele chersoni ||| 
2021 ||| bag of tricks for optimizing transformer efficiency. ||| ye lin ||| yanyang li ||| tong xiao ||| jingbo zhu ||| 
2021 ||| adapterdrop: on the efficiency of adapters in transformers. ||| andreas r ||| ckl ||| gregor geigle ||| max glockner ||| tilman beck ||| jonas pfeiffer ||| nils reimers ||| iryna gurevych ||| 
2019 ||| discourse-aware semantic self-attention for narrative reading comprehension. ||| todor mihaylov ||| anette frank ||| 
2021 ||| mirtt: learning multimodal interaction representations from trilinear transformers for visual question answering. ||| junjie wang ||| yatai ji ||| jiaqi sun ||| yujiu yang ||| tetsuya sakai ||| 
2019 ||| incorporating graph attention mechanism into knowledge graph reasoning based on deep reinforcement learning. ||| heng wang ||| shuangyin li ||| rong pan ||| mingzhi mao ||| 
2021 ||| cross attention augmented transducer networks for simultaneous translation. ||| dan liu ||| mengge du ||| xiaoxi li ||| ya li ||| enhong chen ||| 
2020 ||| slot attention with value normalization for multi-domain dialogue state tracking. ||| yexiang wang ||| yi guo ||| siqi zhu ||| 
2018 ||| neural related work summarization with a joint context-driven attention mechanism. ||| yongzhen wang ||| xiaozhong liu ||| zheng gao ||| 
2018 ||| multi-head attention with disagreement regularization. ||| jian li ||| zhaopeng tu ||| baosong yang ||| michael r. lyu ||| tong zhang ||| 
2019 ||| hierarchical attention prototypical networks for few-shot text classification. ||| shengli sun ||| qingfeng sun ||| kevin zhou ||| tengchao lv ||| 
2020 ||| relation-aware graph attention networks with relational position encodings for emotion recognition in conversations. ||| taichi ishiwatari ||| yuki yasuda ||| taro miyazaki ||| jun goto ||| 
2021 ||| all bark and no bite: rogue dimensions in transformer language models obscure representational quality. ||| william timkey ||| marten van schijndel ||| 
2019 ||| knowledge-enriched transformer for emotion detection in textual conversations. ||| peixiang zhong ||| di wang ||| chunyan miao ||| 
2020 ||| understanding the difficulty of training transformers. ||| liyuan liu ||| xiaodong liu ||| jianfeng gao ||| weizhu chen ||| jiawei han ||| 
2019 ||| multi-head attention with diversity for learning grounded multilingual multimodal representations. ||| po-yao huang ||| xiaojun chang ||| alexander g. hauptmann ||| 
2020 ||| long document ranking with query-directed sparse transformer. ||| jyun-yu jiang ||| chenyan xiong ||| chia-jung lee ||| wei wang ||| 
2020 ||| retrofitting structure-aware transformer language model for end tasks. ||| hao fei ||| yafeng ren ||| donghong ji ||| 
2021 ||| tsdae: using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. ||| kexin wang ||| nils reimers ||| iryna gurevych ||| 
2021 ||| shape : shifted absolute position embedding for transformers. ||| shun kiyono ||| sosuke kobayashi ||| jun suzuki ||| kentaro inui ||| 
2021 ||| learning hard retrieval decoder attention for transformers. ||| hongfei xu ||| qiuhui liu ||| josef van genabith ||| deyi xiong ||| 
2021 ||| neural attention-aware hierarchical topic model. ||| yuan jin ||| he zhao ||| ming liu ||| lan du ||| wray l. buntine ||| 
2018 ||| modeling localness for self-attention networks. ||| baosong yang ||| zhaopeng tu ||| derek f. wong ||| fandong meng ||| lidia s. chao ||| tong zhang ||| 
2017 ||| deep joint entity disambiguation with local neural attention. ||| octavian-eugen ganea ||| thomas hofmann ||| 
2020 ||| turngpt: a transformer-based language model for predicting turn-taking in spoken dialog. ||| erik ekstedt ||| gabriel skantze ||| 
2019 ||| recognizing conflict opinions in aspect-level sentiment classification with dual attention networks. ||| xingwei tan ||| yi cai ||| changxi zhu ||| 
2019 ||| tree transformer: integrating tree structures into self-attention. ||| yau-shian wang ||| hung-yi lee ||| yun-nung chen ||| 
2020 ||| rethinking self-attention: towards interpretability in neural parsing. ||| khalil mrini ||| franck dernoncourt ||| quan hung tran ||| trung bui ||| walter chang ||| ndapa nakashole ||| 
2019 ||| effective use of transformer networks for entity tracking. ||| aditya gupta ||| greg durrett ||| 
2018 ||| attention-guided answer distillation for machine reading comprehension. ||| minghao hu ||| yuxing peng ||| furu wei ||| zhen huang ||| dongsheng li ||| nan yang ||| ming zhou ||| 
2019 ||| interrogating the explanatory power of attention in neural machine translation. ||| pooya moradi ||| nishant kambhatla ||| anoop sarkar ||| 
2021 ||| gradts: a gradient-based automatic auxiliary task selection method based on transformer networks. ||| weicheng ma ||| renze lou ||| kai zhang ||| lili wang ||| soroush vosoughi ||| 
2020 ||| h2kgat: hierarchical hyperbolic knowledge graph attention network. ||| shen wang ||| xiaokai wei ||| c ||| cero nogueira dos santos ||| zhiguo wang ||| ramesh nallapati ||| andrew o. arnold ||| bing xiang ||| philip s. yu ||| 
2021 ||| a simple and effective positional encoding for transformers. ||| pu-chin chen ||| henry tsai ||| srinadh bhojanapalli ||| hyung won chung ||| yin-wen chang ||| chun-sung ferng ||| 
2021 ||| sequence length is a domain: length-based overfitting in transformer models. ||| dusan varis ||| ondrej bojar ||| 
2021 ||| transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence. ||| kelvin lo ||| yuan jin ||| weicong tan ||| ming liu ||| lan du ||| wray l. buntine ||| 
2021 ||| a label-aware bert attention network for zero-shot multi-intent detection in spoken language understanding. ||| ting-wei wu ||| ruolin su ||| biing-hwang juang ||| 
2021 ||| revisiting robust neural machine translation: a transformer case study. ||| peyman passban ||| puneeth s. m. saladi ||| qun liu ||| 
2020 ||| on the sub-layer functionalities of transformer decoder. ||| yilin yang ||| longyue wang ||| shuming shi ||| prasad tadepalli ||| stefan lee ||| zhaopeng tu ||| 
2021 ||| considering nested tree structure in sentence extractive summarization with pre-trained transformer. ||| jingun kwon ||| naoki kobayashi ||| hidetaka kamigaito ||| manabu okumura ||| 
2020 ||| a bilingual generative transformer for semantic sentence embedding. ||| john wieting ||| graham neubig ||| taylor berg-kirkpatrick ||| 
2018 ||| interpreting recurrent and attention-based neural models: a case study on natural language inference. ||| reza ghaeini ||| xiaoli z. fern ||| prasad tadepalli ||| 
2018 ||| multi-grained attention network for aspect-level sentiment classification. ||| feifan fan ||| yansong feng ||| dongyan zhao ||| 
2017 ||| recurrent attention network on memory for aspect sentiment analysis. ||| peng chen ||| zhongqian sun ||| lidong bing ||| wei yang ||| 
2021 ||| #howyoutagtweets: learning user hashtagging preferences via personalized topic attention. ||| yuji zhang ||| yubo zhang ||| chunpu xu ||| jing li ||| ziyan jiang ||| baolin peng ||| 
2017 ||| incorporating global visual features into attention-based neural machine translation. ||| iacer calixto ||| qun liu ||| 
2020 ||| pruning redundant mappings in transformer models via spectral-normalized identity prior. ||| zi lin ||| jeremiah z. liu ||| zi yang ||| nan hua ||| dan roth ||| 
2021 ||| extracting fine-grained knowledge graphs of scientific claims: dataset and transformer-based results. ||| ian h. magnusson ||| scott e. friedman ||| 
2020 ||| program enhanced fact verification with verbalization and graph attention network. ||| xiaoyu yang ||| feng nie ||| yufei feng ||| quan liu ||| zhigang chen ||| xiaodan zhu ||| 
2020 ||| graph transformer networks with syntactic and semantic structures for event argument extraction. ||| amir pouran ben veyseh ||| tuan ngo nguyen ||| thien huu nguyen ||| 
2020 ||| tnt: text normalization based pre-training of transformers for content moderation. ||| fei tan ||| yifan hu ||| changwei hu ||| keqian li ||| kevin yen ||| 
2021 ||| modeling concentrated cross-attention for neural machine translation with gaussian mixture model. ||| shaolei zhang ||| yang feng ||| 
2019 ||| attention is not not explanation. ||| sarah wiegreffe ||| yuval pinter ||| 
2020 ||| adapterhub: a framework for adapting transformers. ||| jonas pfeiffer ||| andreas r ||| ckl ||| clifton poth ||| aishwarya kamath ||| ivan vulic ||| sebastian ruder ||| kyunghyun cho ||| iryna gurevych ||| 
2021 ||| what's hidden in a one-layer randomly weighted transformer? ||| sheng shen ||| zhewei yao ||| douwe kiela ||| kurt keutzer ||| michael w. mahoney ||| 
2018 ||| improving the transformer translation model with document-level context. ||| jiacheng zhang ||| huanbo luan ||| maosong sun ||| feifei zhai ||| jingfang xu ||| min zhang ||| yang liu ||| 
2017 ||| towards neural machine translation with latent tree attention. ||| james bradbury ||| richard socher ||| 
2021 ||| ctal: pre-training cross-modal transformer for audio-and-language representations. ||| hang li ||| wenbiao ding ||| yu kang ||| tianqiao liu ||| zhongqin wu ||| zitao liu ||| 
2020 ||| persian ezafe recognition using transformers and its role in part-of-speech tagging. ||| ehsan doostmohammadi ||| minoo nassajian ||| adel rahimi ||| 
2021 ||| sparsity and sentence structure in encoder-decoder attention of summarization systems. ||| potsawee manakul ||| mark j. f. gales ||| 
2020 ||| stepwise extractive summarization and planning with structured transformers. ||| shashi narayan ||| joshua maynez ||| jakub ad ||| mek ||| daniele pighin ||| blaz bratanic ||| ryan t. mcdonald ||| 
2021 ||| irene-viz: visualizing energy consumption of transformer models. ||| yash kumar lal ||| reetu singh ||| harsh trivedi ||| qingqing cao ||| aruna balasubramanian ||| niranjan balasubramanian ||| 
2021 ||| fbert: a neural transformer for identifying offensive content. ||| diptanu sarkar ||| marcos zampieri ||| tharindu ranasinghe ||| alexander g. ororbia ||| 
2021 ||| skim-attention: learning to focus via document layout. ||| laura nguyen ||| thomas scialom ||| jacopo staiano ||| benjamin piwowarski ||| 
2020 ||| long-short term masking transformer: a simple but effective baseline for document-level neural machine translation. ||| pei zhang ||| boxing chen ||| niyu ge ||| kai fan ||| 
2020 ||| fully quantized transformer for machine translation. ||| gabriele prato ||| ella charlaix ||| mehdi rezagholizadeh ||| 
2020 ||| stl-cqa: structure-based transformers with localization and encoding for chart question answering. ||| hrituraj singh ||| sumit shekhar ||| 
2021 ||| explore better relative position embeddings from encoding perspective for transformer models. ||| anlin qu ||| jianwei niu ||| shasha mo ||| 
2019 ||| dual attention networks for visual reference resolution in visual dialog. ||| gi-cheon kang ||| jaeseo lim ||| byoung-tak zhang ||| 
2018 ||| hybrid neural attention for agreement/disagreement inference in online debates. ||| di chen ||| jiachen du ||| lidong bing ||| ruifeng xu ||| 
2018 ||| interpretable emoji prediction via label-wise attention lstms. ||| francesco barbieri ||| luis espinosa anke ||| jos |||  camacho-collados ||| steven schockaert ||| horacio saggion ||| 
2020 ||| isaaq - mastering textbook questions with pre-trained transformers and bottom-up and top-down attention. ||| jos |||  manu ||| l g ||| mez-p ||| rez ||| ra ||| l ortega ||| 
2021 ||| understanding and overcoming the challenges of efficient transformer quantization. ||| yelysei bondarenko ||| markus nagel ||| tijmen blankevoort ||| 
2020 ||| attention is not only a weight: analyzing transformers with vector norms. ||| goro kobayashi ||| tatsuki kuribayashi ||| sho yokoi ||| kentaro inui ||| 
2018 ||| hierarchical attention network for context-aware query suggestion. ||| xiangsheng li ||| yiqun liu ||| xin li ||| cheng luo ||| jian-yun nie ||| min zhang ||| shaoping ma ||| 
2020 ||| x-aware: context-aware human-environment attention fusion for driver gaze prediction in the wild. ||| lukas stappen ||| georgios rizos ||| bj ||| rn w. schuller ||| 
2019 ||| understanding the attention demand of touch and tangible interaction on a composite task. ||| yosra rekik ||| walid merrad ||| christophe kolski ||| 
2020 ||| temporal attention and consistency measuring for video question answering. ||| lingyu zhang ||| richard j. radke ||| 
2019 ||| attention-driven interaction systems for augmented reality. ||| lisa-marie vortmann ||| 
2019 ||| improved visual focus of attention estimation and prosodic features for analyzing group interactions. ||| lingyu zhang ||| mallory morgan ||| indrani bhattacharya ||| michael foley ||| jonas braasch ||| christoph riedl ||| brooke foucault welles ||| richard j. radke ||| 
2020 ||| model-based prediction of exogeneous and endogeneous attention shifts during an everyday activity. ||| felix putze ||| merlin burri ||| lisa-marie vortmann ||| tanja schultz ||| 
2020 ||| mebal: a multimodal database for eye blink detection and attention level estimation. ||| roberto daza ||| aythami morales ||| julian fi ||| rrez ||| rub ||| n tolosana ||| 
2020 ||| multi-rate attention based gru model for engagement prediction. ||| bin zhu ||| xinjie lan ||| xin guo ||| kenneth e. barner ||| charles boncelet ||| 
2019 ||| multi-attention fusion network for video-based emotion recognition. ||| yanan wang ||| jianming wu ||| keiichiro hoashi ||| 
2019 ||| modeling emotion influence using attention-based graph convolutional recurrent network. ||| yulan chen ||| jia jia ||| zhiyong wu ||| 
2021 ||| what's fair is fair: detecting and mitigating encoded bias in multimodal models of museum visitor attention. ||| halim acosta ||| nathan l. henderson ||| jonathan p. rowe ||| wookhee min ||| james minogue ||| james c. lester ||| 
2020 ||| implicit knowledge injectable cross attention audiovisual model for group emotion recognition. ||| yanan wang ||| jianming wu ||| panikos heracleous ||| shinya wada ||| rui kimura ||| satoshi kurihara ||| 
2018 ||| looking beyond a clever narrative: visual context and attention are primary drivers of affect in video advertisements. ||| abhinav shukla ||| harish katti ||| mohan s. kankanhalli ||| ramanathan subramanian ||| 
2018 ||| group-level emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions. ||| xin guo ||| bin zhu ||| luisa f. polan ||| a ||| charles boncelet ||| kenneth e. barner ||| 
2018 ||| an attention model for group-level emotion recognition. ||| aarush gupta ||| dakshit agrawal ||| hardik chauhan ||| jose dolz ||| marco pedersoli ||| 
2018 ||| attention network for engagement prediction in the wild. ||| amanjot kaur ||| 
2021 ||| attention-based multimodal feature fusion for dance motion generation. ||| kosmas kritsis ||| aggelos gkiokas ||| aggelos pikrakis ||| vassilis katsouros ||| 
2018 ||| cascade attention networks for group emotion recognition with face, body and image cues. ||| kai wang ||| xiaoxing zeng ||| jianfei yang ||| debin meng ||| kaipeng zhang ||| xiaojiang peng ||| yu qiao ||| 
2020 ||| multimodal physiological synchrony as measure of attentional engagement. ||| ivo v. stuldreher ||| 
2018 ||| sensing arousal and focal attention during visual interaction. ||| oludamilare matthews ||| markel vigo ||| simon harper ||| 
2019 ||| real-time multimodal classification of internal and external attention. ||| lisa-marie vortmann ||| moritz schult ||| mathias benedek ||| sonja walcher ||| felix putze ||| 
2017 ||| multimodal language grounding for improved human-robot collaboration: exploring spatial semantic representations in the shared space of attention. ||| dimosthenis kontogiorgos ||| 
2020 ||| attention sensing through multimodal user modeling in an augmented reality guessing game. ||| felix putze ||| dennis k ||| ster ||| timo urban ||| alexander zastrow ||| marvin kampen ||| 
2018 ||| using interlocutor-modulated attention blstm to predict personality traits in small group interaction. ||| yun-shao lin ||| chi-chun lee ||| 
2018 ||| dozing off or thinking hard?: classifying multi-dimensional attentional states in the classroom from video. ||| felix putze ||| dennis k ||| ster ||| sonja annerer-walcher ||| mathias benedek ||| 
2018 ||| attention-based audio-visual fusion for robust automatic speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2018 ||| estimating visual focus of attention in multiparty meetings using deep convolutional neural networks. ||| kazuhiro otsuka ||| keisuke kasuga ||| martina k ||| hler ||| 
2020 ||| detection of micro-expression recognition based on spatio-temporal modelling and spatial attention. ||| mengjiong bai ||| 
2020 ||| smarthelm: towards multimodal detection of attention in an outdoor augmented reality biking scenario. ||| sromona chatterjee ||| kevin scheck ||| dennis k ||| ster ||| felix putze ||| harish moturu ||| johannes schering ||| jorge marx g ||| mez ||| tanja schultz ||| 
2020 ||| graph self-attention network for image captioning. ||| qitong zheng ||| yuping wang ||| 
2021 ||| social media adverse drug reaction detection based on bi-lstm with multi-head attention mechanism. ||| xuqi wang ||| wenzhun huang ||| shanwen zhang ||| 
2019 ||| hierarchical attention network for predicting dna-protein binding sites. ||| wenbo yu ||| chang-an yuan ||| xiao qin ||| zhi-kai huang ||| li shang ||| 
2020 ||| real-time object detection based on convolutional block attention module. ||| ming-yang ban ||| weidong tian ||| zhong-qiu zhao ||| 
2021 ||| efficient face detector using spatial attention module in real-time application on an edge device. ||| muhamad dwisnanto putro ||| duy-linh nguyen ||| kang-hyun jo ||| 
2020 ||| hgalinker: drug-disease association prediction based on attention mechanism of heterogeneous graph. ||| xiaozhu jing ||| wei jiang ||| zhongqing zhang ||| yadong wang ||| junyi li ||| 
2021 ||| attention-based deep multi-scale network for plant leaf recognition. ||| xiao qin ||| yu shi ||| xiao huang ||| huiting li ||| jiangtao huang ||| changan yuan ||| chunxia liu ||| 
2020 ||| aggregated deep saliency prediction by self-attention network. ||| ge cao ||| qing tang ||| kang-hyun jo ||| 
2021 ||| detection of drug-drug interactions through knowledge graph integrating multi-attention with capsule network. ||| xiao-rui su ||| zhu-hong you ||| hai-cheng yi ||| bo-wei zhao ||| 
2021 ||| graph semantics based neighboring attentional entity alignment for knowledge graphs. ||| hanchen wang ||| jianfeng li ||| tao luo ||| 
2021 ||| a diabetic retinopathy classification method based on novel attention mechanism. ||| jinfan zou ||| xiaolong zhang ||| xiaoli lin ||| 
2020 ||| inferring disease-associated piwi-interacting rnas via graph attention networks. ||| kai zheng ||| zhu-hong you ||| lei wang ||| leon wong ||| zhan-heng chen ||| 
2020 ||| position attention-guided learning for infrared-visible person re-identification. ||| yong wu ||| si-zhe wan ||| di wu ||| chao wang ||| changan yuan ||| xiao qin ||| hongjie wu ||| xingming zhao ||| 
2018 ||| mandarin prosody prediction based on attention mechanism and multi-model ensemble. ||| kun xie ||| wei pan ||| 
2021 ||| multi-class text classification model based on weighted word vector and bilstm-attention optimization. ||| hao wu ||| zhuangzhuang he ||| weitao zhang ||| yunsheng hu ||| yunzhi wu ||| yi yue ||| 
2021 ||| a lightweight attention fusion module for multi-sensor 3-d object detection. ||| li-hua wen ||| ting-yue xu ||| kang-hyun jo ||| 
2017 ||| recognizing text entailment via bidirectional lstm model with inner-attention. ||| chengjie sun ||| yang liu ||| chang'e jia ||| bingquan liu ||| lei lin ||| 
2019 ||| flower species recognition system combining object detection and attention mechanism. ||| wei qin ||| xue cui ||| chang-an yuan ||| xiao qin ||| li shang ||| zhi-kai huang ||| si-zhe wan ||| 
2019 ||| missing data imputation for operation data of transformer based on functional principal component analysis and wavelet transform. ||| jiafeng qin ||| yi yang ||| zijing hong ||| hongyi du ||| 
2021 ||| fine-grained recognition of crop pests based on capsule network with attention mechanism. ||| xianfeng wang ||| xuqi wang ||| wenzhun huang ||| shanwen zhang ||| 
2020 ||| paying deep attention to both neighbors and multiple tasks. ||| gaoyuan liang ||| haoran mo ||| ying qiao ||| chuxin wang ||| jingyan wang ||| 
2017 ||| mining implicit intention using attention-based rnn encoder-decoder model. ||| chenxing li ||| yajun du ||| sida wang ||| 
2020 ||| depth guided attention for person re-identification. ||| md. kamal uddin ||| antony lam ||| hisato fukuda ||| yoshinori kobayashi ||| yoshinori kuno ||| 
2021 ||| mitt: musical instrument timbre transfer based on the multichannel attention-guided mechanism. ||| huayuan chen ||| yanxiang chen ||| 
2019 ||| using attention-based bidirectional lstm to identify different categories of offensive language directed toward female celebrities. ||| sima sharifirad ||| stan matwin ||| 
2020 ||| sam: self-attention based deep learning method for online traffic classification. ||| guorui xie ||| qing li ||| yong jiang ||| tao dai ||| gengbiao shen ||| rui li ||| richard o. sinnott ||| shutao xia ||| 
2019 ||| conditional dilated convolution attention tracking model. ||| tyler highlander ||| bernard abayowa ||| mateen m. rizki ||| hamilton scott clouse ||| 
2018 ||| multi-robot scheduling and path-planning for non-overlapping operator attention. ||| sebasti ||| n a. zanlongo ||| franklin abodo ||| philip long ||| taskin padir ||| leonardo bobadilla ||| 
2017 ||| multi-robot planning for non-overlapping operator attention allocation. ||| sebasti ||| n a. zanlongo ||| md. mahbubur rahman ||| franklin abodo ||| leonardo bobadilla ||| 
2020 ||| dualanet: dual lesion attention network for thoracic disease classification in chest x-rays. ||| vinicius teixeira ||| leod ||| cio braz ||| h ||| lio pedrini ||| zanoni dias ||| 
2021 ||| graph attention autoencoder for collaborative pair-wise ranking. ||| nan mu ||| daren zha ||| yuanye he ||| lin zhao ||| 
2021 ||| knowledge graph construction and decision support towards transformer fault maintenance. ||| xiaoying liu ||| hongwei wang ||| 
2021 ||| research on the impacts of feedback in instructional videos on college students' attention and learning effects. ||| xin lu ||| qing li ||| xue wang ||| 
2021 ||| incorporating background knowledge into dialogue generation using multi-task transformer learning. ||| yiming yuan ||| xiantao cai ||| 
2019 ||| multilingual transformer ensembles for portuguese natural language tasks. ||| ruan chaves rodrigues ||| j ||| ssica rodrigues da silva ||| pedro vitor quinta de castro ||| n ||| dia f ||| lix felipe da silva ||| anderson da silva soares ||| 
2019 ||| multilingual transformer ensembles for portuguese natural language tasks. ||| evandro fonseca ||| jo ||| o paulo reis alvarenga ||| 
2022 ||| view-invariant 3d skeleton-based human activity recognition based on transformer and spatio-temporal features. ||| ahmed snoun ||| tahani bouchrika ||| olfa jemai ||| 
2020 ||| dimensionality reduction and attention mechanisms for extracting affective state from sound spectrograms. ||| george pikramenos ||| konstantinos kechagias ||| theodoros psallidas ||| georgios smyrnis ||| evaggelos spyrou ||| stavros j. perantonis ||| 
2021 ||| a blended attention-ctc network architecture for amharic text-image recognition. ||| birhanu hailu belay ||| tewodros habtegebrial ||| marcus liwicki ||| gebeyehu belay ||| didier stricker ||| 
2021 ||| upgraded w-net with attention gates and its application in unsupervised 3d liver segmentation. ||| dhanunjaya mitta ||| soumick chatterjee ||| oliver speck ||| andreas n ||| rnberger ||| 
2021 ||| neurofeedback video games in the rehabilitative treatment of hyperactivity and attention deficit disorder for attention enhancement and hyperactivity reduction. ||| giuliana nardacchione ||| federica doronzo ||| guendalina peconio ||| 
2021 ||| comparison between active and passive attention using eeg waves and deep neural network. ||| sumit chakravarty ||| ying xie ||| linh le ||| john johnson ||| michael hales ||| 
2021 ||| an attention-based mood controlling framework for social media users. ||| tapotosh ghosh ||| md. hasan al banna ||| tazkia mim angona ||| m. jaber al nahian ||| mohammed nasir uddin ||| m. shamim kaiser ||| mufti mahmud ||| 
2021 ||| towards learning a joint representation from transformer in multimodal emotion recognition. ||| james j. deng ||| clement h. c. leung ||| 
2018 ||| functional connectivity analysis using the oddball auditory paradigm for attention tasks. ||| juana valeria hurtado-rinc ||| n ||| francia restrepo ||| jorge ivan padilla ||| h ||| ctor fabio torres ||| germ ||| n castellanos-dom ||| nguez ||| 
2021 ||| the influence of clutter on search-based learning, long-term memory, and memory-guided attention in real-world scenes: an eye-movement research protocol. ||| christos gkoumas ||| andria shimi ||| 
2019 ||| deep learning investigation for chess player attention prediction using eye-tracking and game data. ||| justin le louedec ||| thomas guntz ||| james l. crowley ||| dominique vaufreydaz ||| 
2020 ||| eye-tracking and virtual reality in 360-degrees: exploring two ways to assess attentional orienting in rear space. ||| r ||| ba |||  soret ||| pom charras ||| ines khazar ||| christophe hurter ||| vsevolod peysakhovich ||| 
2019 ||| attention towards privacy notifications on web pages. ||| agnieszka ozimek ||| paulina lewandowska ||| krzysztof krejtz ||| andrew t. duchowski ||| 
2021 ||| what the eyes can tell: analyzing visual attention with an educational video game. ||| wenyi lu ||| hao he ||| alex urban ||| joe griffin ||| 
2018 ||| how many words is a picture worth?: attention allocation on thumbnails versus title text regions. ||| chaitra yangandul ||| sachin paryani ||| madison le ||| eakta jain ||| 
2019 ||| characterizing joint attention behavior during real world interactions using automated object and gaze detection. ||| pranav venuprasad ||| tushar dobhal ||| anurag paul ||| tu n. m. nguyen ||| andrew gilman ||| pamela c. cosman ||| leanne chukoskie ||| 
2020 ||| how shared visual attention patterns of pairs unfold over time when workload changes. ||| shannon patricia devlin ||| jake ryan flynn ||| sara lu riggs ||| 
2021 ||| gaze interactive and attention aware low vision aids as future smart glasses. ||| fiona br ||| d mulvey ||| marek mikitovic ||| mateusz sadowski ||| baosheng james hou ||| nils david rasamoel ||| john paulin hansen ||| per b ||| kgaard ||| 
2020 ||| attention-based cross-modal unification of visualized text and image features: understanding the influence of interface and user idiosyncrasies on unification for free-viewing. ||| sandeep vidyapu ||| vijaya saradhi vedula ||| michael burch ||| samit bhattacharya ||| 
2019 ||| quantitative visual attention prediction on webpage images using multiclass svm. ||| sandeep vidyapu ||| vijaya saradhi vedula ||| samit bhattacharya ||| 
2019 ||| the vision and interpretation of paintings: bottom-up visual processes, top-down culturally informed attention, and aesthetic experience. ||| pablo fontoura ||| jean-marie schaeffer ||| michel menu ||| 
2020 ||| towards capturing focal/ambient attention during dynamic wayfinding. ||| jakub krukar ||| panagiotis mavros ||| christoph hoelscher ||| 
2021 ||| estimation of visual attention using microsaccades in response to vibrations in the peripheral field of vision. ||| takahiro ueno ||| minoru nakayama ||| 
2020 ||| giuplayer: a gaze immersive youtube player enabling eye control and attention analysis. ||| ramin hedeshy ||| chandan kumar ||| raphael menges ||| steffen staab ||| 
2019 ||| attentional orienting in real and virtual 360-degree environments: applications to aeronautics. ||| r ||| ba |||  soret ||| christophe hurter ||| vsevolod peysakhovich ||| 
2020 ||| toward a pervasive gaze-contingent assistance system: attention and context-awareness in augmented reality. ||| kenan bektas ||| 
2021 ||| climate change overlooked. the role of attitudes and mood regulation in visual attention to global warming. ||| anna mazurowska ||| 
2019 ||| attentional orienting in virtual reality using endogenous and exogenous cues in auditory and visual modalities. ||| r ||| ba |||  soret ||| pom charras ||| christophe hurter ||| vsevolod peysakhovich ||| 
2021 ||| 55 rides: attention annotated head and gaze data during naturalistic driving. ||| thomas c. k ||| bler ||| wolfgang fuhl ||| elena wagner ||| enkelejda kasneci ||| 
2021 ||| saccades, attentional orienting and disengagement: the effects of anodal tdcs over right posterior parietal cortex (ppc) and frontal eye field (fef). ||| lorenzo diana ||| patrick pilastro ||| edoardo n. aiello ||| aleksandra k. eberhard-moscicka ||| ren |||  m. m ||| ri ||| nadia bolognini ||| 
2018 ||| virtual reality as a proxy for real-life social attention? ||| marius rubo ||| matthias gamer ||| 
2020 ||| detecting ambient/focal visual attention in professional airline pilots with a modified coefficient k: a full flight simulator study. ||| christophe antony lounis ||| almoctar hassoumi ||| olivier lefrancois ||| vsevolod peysakhovich ||| micka ||| l causse ||| 
2020 ||| attention-based graph neural network enabled method to predict short-term metro passenger flow. ||| lin li ||| jun xu ||| s. thomas ng ||| jiajian zhang ||| shenghua zhou ||| yifan yang ||| 
2017 ||| attention retargeting in real space with projector camera system. ||| katsumi yamamoto ||| hironori takimoto ||| akihiro kanagawa ||| 
2021 ||| attention based multi-unit spatial-temporal network for traffic flow forecasting. ||| chuang ma ||| xinting hu ||| shuaiwu liu ||| lei liu ||| 
2020 ||| research on load forecasting method of distribution transformer based on deep learning. ||| lei chen ||| huihua yu ||| li tong ||| xu huai ||| peipei jin ||| yu huang ||| chengfeng dou ||| 
2021 ||| electric transformer oil leakage visual detection as service based on lstm and genetic algorithm. ||| mingliang gao ||| cihang zhang ||| chongyao xu ||| qi gao ||| jiwei gao ||| jing yan ||| weidong liu ||| xiaohu fan ||| hao tu ||| 
2020 ||| deriving interpretable rules for iot discovery through attention. ||| franck le ||| mudhakar srivatsa ||| 
2020 ||| attention-based robot learning of haptic interaction. ||| alexandra moringen ||| sascha fleer ||| guillaume walck ||| helge j. ritter ||| 
2021 ||| uzh onpoint at swisstext-2021: sentence end and punctuation prediction in nlg text through ensembling of different transformers (short paper). ||| adrianos michail ||| silvan wehrli ||| ter ||| zia buckov ||| 
2020 ||| evaluating german transformer language models with syntactic agreement tests. ||| karolina zaczynska ||| nils feldhus ||| robert schwarzenberg ||| aleksandra gabryszak ||| sebastian m ||| ller ||| 
2021 ||| crisisbert: a robust transformer for crisis classification and contextual crisis embedding. ||| junhua liu ||| trisha singhal ||| luci ||| nne t. m. blessing ||| kristin l. wood ||| kwan hui lim ||| 
2018 ||| intellieye: enhancing mooc learners' video watching experience through real-time attention tracking. ||| tarmo robal ||| yue zhao ||| christoph lofi ||| claudia hauff ||| 
2019 ||| human attention span modeling using 2d visualization plots for gaze progression and gaze sustenance. ||| seba susan ||| ankur agarwal ||| chetan gulati ||| sunpreet singh ||| vaibhav chauhan ||| 
2019 ||| understanding and modelling human attention for soft biometrics purposes. ||| dario cazzato ||| marco leo ||| pierluigi carcagn ||| claudio cimarelli ||| holger voos ||| 
2018 ||| enhanced character segmentation for multi-language data plate in substation transformer based on connected component analysis. ||| jieling zheng ||| xiren miao ||| shih-hau fang ||| jing chen ||| hao jiang ||| 
2018 ||| impact of harmonic distortion on the energization of energy distribution transformers integrated in virtual power plants. ||| vicente tiburcio dos santos junior ||| benedito donizeti bonatto ||| cl ||| udio ferreira ||| antonio carlos zambroni de souza ||| 
2020 ||| hybrid feature network driven by attention and graph features for multiple sclerosis lesion segmentation from mr images. ||| zhanlan chen ||| xiuying wang ||| jiangbin zheng ||| 
2020 ||| 6d object pose estimation with color/geometry attention fusion. ||| honglin yuan ||| remco c. veltkamp ||| 
2020 ||| self-intersection attention pooling based classification for rock recognition. ||| liang chen ||| qian zheng ||| zhitao liu ||| weijie mao ||| fulong lin ||| 
2020 ||| a residual network for de novo peptide sequencing with attention mechanism. ||| zihang liu ||| chunhui zhao ||| 
2018 ||| visual dialog with multi-turn attentional memory network. ||| dejiang kong ||| fei wu ||| 
2018 ||| multi-decoder based co-attention for image captioning. ||| zhen sun ||| xin lin ||| zhaohui wang ||| yi ji ||| chunping liu ||| 
2018 ||| attention to refine through multi scales for semantic segmentation. ||| shiqi yang ||| gang peng ||| 
2017 ||| attention window aware encoder-decoder model for spoken language understanding. ||| yiming wang ||| wenge rong ||| jingshuang liu ||| jingfei han ||| zhang xiong ||| 
2018 ||| spatial-temporal attention for action recognition. ||| dengdi sun ||| hanqing wu ||| zhuanlian ding ||| bin luo ||| jin tang ||| 
2018 ||| val: visual-attention action localizer. ||| xiaomeng song ||| yahong han ||| 
2018 ||| spatial attention network for head detection. ||| rongchun li ||| biao zhang ||| zhen huang ||| xiang zhao ||| peng qiao ||| yong dou ||| 
2018 ||| intra-view and inter-view attention for multi-view network embedding. ||| yueyang wang ||| liang hu ||| yueting zhuang ||| fei wu ||| 
2018 ||| latitude-based visual attention in 360-degree video display. ||| huiwen huang ||| yiwen xu ||| jinling chen ||| yanjie song ||| tiesong zhao ||| 
2018 ||| temporal-contextual attention network for video-based person re-identification. ||| di chen ||| zheng-jun zha ||| jiawei liu ||| hongtao xie ||| yongdong zhang ||| 
2018 ||| hand pose estimation with attention-and-sequence network. ||| tianping hu ||| wenhai wang ||| tong lu ||| 
2018 ||| reading document and answering question via global attentional inference. ||| jun song ||| siliang tang ||| tianchi qian ||| wenwu zhu ||| fei wu ||| 
2018 ||| context and temporal aware attention model for flood prediction. ||| zhaoyang liu ||| yirui wu ||| yukai ding ||| jun feng ||| tong lu ||| 
2017 ||| multi-modal emotion recognition with temporal-band attention based on lstm-rnn. ||| jiamin liu ||| yuanqi su ||| yuehu liu ||| 
2018 ||| multi-modal sequence to sequence learning with content attention for hotspot traffic speed prediction. ||| binbing liao ||| siliang tang ||| shengwen yang ||| wenwu zhu ||| fei wu ||| 
2018 ||| scan: spatial and channel attention network for vehicle re-identification. ||| shangzhi teng ||| xiaobin liu ||| shiliang zhang ||| qingming huang ||| 
2018 ||| text-guided dual-branch attention network for visual question answering. ||| mengfei li ||| li gu ||| yi ji ||| chunping liu ||| 
2018 ||| contextual attention model for social recommendation. ||| hongfeng bao ||| le wu ||| peijie sun ||| 
2020 ||| semantic segmentation of nuclei from breast histopathological images by incorporating attention in u-net. ||| r. rashmi ||| keerthana prasad ||| chethana babu k. udupa ||| 
2020 ||| ecasr: efficient channel attention based super-resolution. ||| sameeran borah ||| nilkanta sahu ||| 
2019 ||| automatic report generation for chest x-ray images: a multilevel multi-attention approach. ||| gaurav o. gajbhiye ||| abhijeet v. nandedkar ||| ibrahima faye ||| 
2019 ||| traffic sign recognition using color and spatial transformer network on gpu embedded development board. ||| bhaumik vaidya ||| chirag n. paunwala ||| 
2020 ||| spatial attention for autonomous decision-making in highway scene. ||| shuwei zhang ||| yutian wu ||| harutoshi ogai ||| 
2020 ||| joint learning with pre-trained transformer on named entity recognition and relation extraction tasks for clinical analytics. ||| miao chen ||| ganhui lan ||| fang du ||| victor s. lobanov ||| 
2020 ||| dilated convolutional attention network for medical code assignment from clinical text. ||| shaoxiong ji ||| erik cambria ||| pekka marttinen ||| 
2020 ||| information extraction from swedish medical prescriptions with sig-transformer encoder. ||| john pougue biyong ||| bo wang ||| terry j. lyons ||| alejo j. nevado-holgado ||| 
2020 ||| incorporating risk factor embeddings in pre-trained transformers improves sentiment prediction in psychiatric discharge summaries. ||| xiyu ding ||| mei-hua hall ||| timothy miller ||| 
2020 ||| 3d medical image registration based on spatial attention. ||| shanshan li ||| yingjun ma ||| hongyan wang ||| 
2019 ||| attentional bi-directional lstm for semantic attribute prediction. ||| mengling shen ||| xianlin zhang ||| xueming li ||| 
2019 ||| improving variational auto-encoder with self-attention and mutual information for image generation. ||| lizhi lin ||| xinyue liu ||| wenxin liang ||| 
2020 ||| attention enhanced multi-patch deformable network for image deblurring. ||| jia li ||| xueming li ||| 
2021 ||| unsupervised monocular depth estimation with attention based inception pipe and overlap regularized loss. ||| xiaoyuan jiang ||| xihai chen ||| zhao zhang ||| 
2019 ||| dangerous driving behavior detection with attention mechanism. ||| kun wang ||| xianqiao chen ||| rui gao ||| 
2018 ||| template attentional siamese network for object tracking. ||| junyan gao ||| zhenguo yang ||| wenyin liu ||| 
2021 ||| construction site safety detection based on object detection with channel-wise attention. ||| weihong jiang ||| changzhen qiu ||| chunze li ||| dengxiang li ||| weibiao chen ||| zhiyong zhang ||| luping wang ||| liang wang ||| 
2021 ||| a fine-grained classification method based on self-attention siamese network. ||| he can ||| yuan guo wu ||| wu hao ||| 
2019 ||| attention-based generative graph convolutional network for skeleton-based human action recognition. ||| kai yang ||| xiaolu ding ||| wai chen ||| 
2021 ||| u-shaped network based on transformer for 3d point clouds semantic segmentation. ||| jiazhe zhang ||| xingwei li ||| xianfa zhao ||| yizhi ge ||| zheng zhang ||| 
2020 ||| image super-resolution using hybrid attention mechanism. ||| jie chen ||| yaoyao ye ||| ming kang ||| 
2020 ||| siamese region proposal networks and attention module for real-time visual tracking. ||| hang dong ||| yuan zeng ||| yi gong ||| 
2019 ||| mdanet: multiple fusion network with double attention for visual question answering. ||| junyi feng ||| ping gong ||| guanghui qiu ||| 
2021 ||| transformer-based bidirectional encoder representations for emotion detection from text. ||| ashok kumar j ||| erik cambria ||| tina esther trueman ||| 
2020 ||| detecting proper mask usage with soft attention. ||| thomas truong ||| dhyey lalseta ||| ryan ittyipe ||| svetlana n. yanushkevich ||| 
2021 ||| dibert: dependency injected bidirectional encoder representations from transformers. ||| abdul wahab ||| rafet sifa ||| 
2020 ||| low-rank temporal attention-augmented bilinear network for financial time-series forecasting. ||| mostafa shabani ||| alexandros iosifidis ||| 
2021 ||| improving transformer model translation for low resource south african languages using bert. ||| paddington chiguvare ||| christopher w. cleghorn ||| 
2019 ||| weakly supervised attention inference generative adversarial network for text-to-image. ||| lingrui mei ||| xuming ran ||| jin hu ||| 
2020 ||| action detection based on 3d convolution neural network with channel attention mechanism. ||| yan gao ||| huilai liang ||| bao-di liu ||| yanjiang wang ||| 
2018 ||| self-attention enhanced recurrent neural networks for sentence classification. ||| ankit kumar ||| reshma rastogi ||| 
2020 ||| inpainting electrical logging images based on deep cnn with attention mechanisms. ||| chunyu du ||| qiang xing ||| jinyan zhang ||| jun wang ||| bao-di liu ||| yanjiang wang ||| 
2020 ||| improvement of mixture-of-experts-type model to construct dynamic saliency maps for predicting drivers' attention. ||| sorachi nakazawa ||| yohei nakada ||| 
2021 ||| multistream graph attention networks for wind speed forecasting. ||| dogan aykas ||| siamak mehrkanoon ||| 
2021 ||| electrode selection and convolutional attention network for recognition of silently spoken words from eeg signals. ||| sahil datta ||| jorunn jo holmberg ||| elena antonova ||| 
2020 ||| interpretable multivariate time series forecasting with temporal attention convolutional neural networks. ||| leonardos pantiskas ||| kees verstoep ||| henri e. bal ||| 
2021 ||| a daily tourism demand prediction framework based on multi-head attention cnn: the case of the foreign entrant in south korea. ||| dong-keon kim ||| sung kuk shyn ||| donghee kim ||| seungwoo jang ||| kwangsu kim ||| 
2017 ||| diversity-guided generalized extremal optimization for transformer design problem. ||| leandro dos santos coelho ||| viviana cocco mariani ||| rafael bartnik grebogi ||| emerson hochsteiner de vasconcelos segundo ||| mauricio valencia ferreira da luz ||| jean vianei leite ||| roberto zanetti freire ||| 
2019 ||| on machine learning apporaches towards dissolved gases analyses of power transformer oil chromatography. ||| lei su ||| yixuan yang ||| haobo xing ||| bengang wei ||| ping ling ||| wenlian lu ||| 
2018 ||| gaan: gated attention networks for learning on large and spatiotemporal graphs. ||| jiani zhang ||| xingjian shi ||| junyuan xie ||| hao ma ||| irwin king ||| dit-yan yeung ||| 
2019 ||| compatibility study of paper and epoxy with natural ester- a substitute for transformer mineral oil. ||| joyce jacob ||| uppu navya rashmika ||| archana jha ||| chelli divya sai ||| katta lakshmi sai sravani ||| p. preetha ||| 
2018 ||| analyzing the single-line to ground fault current contribution by the type of transformer and distributed generator. ||| sangwon yun ||| jaesung jung ||| namhun cho ||| 
2019 ||| multilingual cyber abuse detection using advanced transformer architecture. ||| aditya malte ||| pratik ratadiya ||| 
2019 ||| deep recurrent architecture with attention for remaining useful life estimation. ||| ankit das ||| shaista hussain ||| feng yang ||| mohamed salahuddin habibullah ||| arun kumar ||| 
2019 ||| fork and d/y/d connected transformer supplied 12-pulse uncontrolled converter fed retrofit vector controlled induction motor drive: a comparative study. ||| aekamdeep kaur ||| yugal gupta ||| sumit ghatak choudhuri ||| 
2021 ||| convolutional neural network or vision transformer? benchmarking various machine learning models for distracted driver detection. ||| hong vin koay ||| joon huang chuah ||| chee-onn chow ||| 
2019 ||| isolated switched boost dc-dc converter with coupled inductor and transformer. ||| preenu paul ||| babita r. jose ||| shahana thottathikkulam kassim ||| chikku abraham ||| jimson mathew ||| 
2021 ||| hyperspectral image classification based on multi-stage vision transformer with stacked samples. ||| xiaoyue chen ||| sei-ichiro kamata ||| weilian zhou ||| 
2018 ||| a method of modeling tap-changing transformers for power-flow and short-circuit analysis studies. ||| insu kim ||| 
2021 ||| investigating the vision transformer model for image retrieval tasks. ||| socratis gkelios ||| yiannis s. boutalis ||| savvas a. chatzichristofis ||| 
2020 ||| music genre classification with transformer classifier. ||| yingying zhuang ||| yuezhang chen ||| jie zheng ||| 
2021 ||| a real-time inference method of graph attention network based on knowledge graph for lung cancer. ||| mei yong zhang ||| rong zhi du ||| 
2020 ||| at-gan: attention transfer gan for image-to-image translation. ||| xinxiong xie ||| yuhan dong ||| yali li ||| shengjin wang ||| 
2021 ||| boundary-aware transformers for skin lesion segmentation. ||| jiacheng wang ||| lan wei ||| liansheng wang ||| qichao zhou ||| lei zhu ||| jing qin ||| 
2021 ||| ccut-net: pixel-wise global context channel attention ut-net for head and neck tumor segmentation. ||| jiao wang ||| yanjun peng ||| yanfei guo ||| dapeng li ||| jindong sun ||| 
2021 ||| joint segmentation and quantification of main coronary vessels using dual-branch multi-scale attention network. ||| hongwei zhang ||| dong zhang ||| zhifan gao ||| heye zhang ||| 
2021 ||| mil-vt: multiple instance learning enhanced vision transformer for fundus image classification. ||| shuang yu ||| kai ma ||| qi bi ||| cheng bian ||| munan ning ||| nanjun he ||| yuexiang li ||| hanruo liu ||| yefeng zheng ||| 
2021 ||| priori and posteriori attention for generalizing head and neck tumors segmentation. ||| jiangshan lu ||| wenhui lei ||| ran gu ||| guotai wang ||| 
2021 ||| convolutional nets versus vision transformers for diabetic foot ulcer classification. ||| adrian galdran ||| gustavo carneiro ||| miguel  ||| ngel gonz ||| lez ballester ||| 
2021 ||| gt u-net: a u-net like group transformer network for tooth root segmentation. ||| yunxiang li ||| shuai wang ||| jun wang ||| guodong zeng ||| wenjun liu ||| qianni zhang ||| qun jin ||| yaqi wang ||| 
2019 ||| comparing deep learning strategies and attention mechanisms of discrete registration for multimodal image-guided interventions. ||| in young ha ||| mattias p. heinrich ||| 
2020 ||| anatomy prior based u-net for pathology segmentation with attention. ||| yuncheng zhou ||| ke zhang ||| xinzhe luo ||| sihan wang ||| xiahai zhuang ||| 
2021 ||| u-net transformer: self and cross attention for medical image segmentation. ||| olivier petit ||| nicolas thome ||| clement rambour ||| loic themyr ||| toby collins ||| luc soler ||| 
2021 ||| multi-frame attention network for left ventricle segmentation in 3d echocardiography. ||| shawn s. ahn ||| kevinminh ta ||| stephanie thorn ||| jonathan langdon ||| albert j. sinusas ||| james s. duncan ||| 
2020 ||| paying per-label attention for multi-label extraction from radiology reports. ||| patrick schrempf ||| hannah watson ||| shadia mikhael ||| maciej pajak ||| mat ||| s falis ||| aneta lisowska ||| keith w. muir ||| david harris-birtill ||| alison q. o'neil ||| 
2020 ||| attention-guided deep domain adaptation for brain dementia identification with multi-site neuroimaging data. ||| hao guan ||| erkun yang ||| pew-thian yap ||| dinggang shen ||| mingxia liu ||| 
2020 ||| learning bronchiole-sensitive airway segmentation cnns by feature recalibration and attention distillation. ||| yulei qin ||| hao zheng ||| yun gu ||| xiaolin huang ||| jie yang ||| lihui wang ||| yue-min zhu ||| 
2021 ||| soft attention improves skin cancer classification performance. ||| soumyya kanti datta ||| mohammad abuzar shaikh ||| sargur n. srihari ||| mingchen gao ||| 
2019 ||| deep cascaded attention network for multi-task brain tumor segmentation. ||| hai xu ||| hongtao xie ||| yizhi liu ||| chuandong cheng ||| chaoshi niu ||| yongdong zhang ||| 
2021 ||| stacked hourglass network with a multi-level attention mechanism: where to look for intervertebral disc labeling. ||| reza azad ||| lucas rouhier ||| julien cohen-adad ||| 
2020 ||| inside: steering spatial attention with non-imaging information in cnns. ||| grzegorz jacenk ||| w ||| alison q. o'neil ||| brian mohr ||| sotirios a. tsaftaris ||| 
2020 ||| pranet: parallel reverse attention network for polyp segmentation. ||| deng-ping fan ||| ge-peng ji ||| tao zhou ||| geng chen ||| huazhu fu ||| jianbing shen ||| ling shao ||| 
2021 ||| imaging biomarker knowledge transfer for attention-based diagnosis of covid-19 in lung ultrasound videos. ||| tyler lum ||| mobina mahdavi ||| oron frenkel ||| christopher lee ||| mohammad h. jafari ||| fatemeh taheri dezaki ||| nathan van woudenberg ||| ang nan gu ||| purang abolmaesumi ||| teresa tsang ||| 
2020 ||| attention, suggestion and annotation: a deep active learning framework for biomedical image segmentation. ||| haohan li ||| zhaozheng yin ||| 
2020 ||| lightweight double attention-fused networks for intraoperative stent segmentation. ||| yan-jie zhou ||| xiao-liang xie ||| zeng-guang hou ||| xiao-hu zhou ||| gui-bin bian ||| shi-qi liu ||| 
2019 ||| deep learning via fused bidirectional attention stacked long short-term memory for obsessive-compulsive disorder diagnosis and risk screening. ||| chiyu feng ||| lili jin ||| chuangyong xu ||| peng yang ||| tianfu wang ||| baiying lei ||| ziwen peng ||| 
2021 ||| attention based cnn-lstm network for pulmonary embolism prediction on chest computed tomography pulmonary angiograms. ||| sudhir suman ||| gagandeep singh ||| nicole sakla ||| rishabh gattu ||| jeremy green ||| tej phatak ||| dimitris samaras ||| prateek prasanna ||| 
2019 ||| dense-residual attention network for skin lesion segmentation. ||| lei song ||| jianzhe lin ||| z. jane wang ||| haoqian wang ||| 
2018 ||| lstm spatial co-transformer networks for registration of 3d fetal us and mr brain images. ||| robert wright ||| bishesh khanal ||| alberto g ||| mez ||| emily skelton ||| jacqueline matthew ||| joseph v. hajnal ||| daniel rueckert ||| julia a. schnabel ||| 
2021 ||| kidney and kidney tumor segmentation using spatial and channel attention enhanced u-net. ||| sajan gohil ||| abhi lad ||| 
2021 ||| combining attention-based multiple instance learning and gaussian processes for ct hemorrhage detection. ||| yunan wu ||| arne schmidt ||| enrique hern ||| ndez-s ||| nchez ||| rafael molina ||| aggelos k. katsaggelos ||| 
2021 ||| spatial attention-based deep learning system for breast cancer pathological complete response prediction with serial histopathology images in multiple stains. ||| hongyi duanmu ||| shristi bhattarai ||| hongxiao li ||| chia cheng cheng ||| fusheng wang ||| george teodoro ||| emiel a. m. janssen ||| keerthi gogineni ||| preeti subhedar ||| ritu aneja ||| jun kong ||| 
2019 ||| reinforced transformer for medical image captioning. ||| yuxuan xiong ||| bo du ||| pingkun yan ||| 
2021 ||| spine-transformers: vertebra detection and localization in arbitrary field-of-view spine ct with transformers. ||| rong tao ||| guoyan zheng ||| 
2019 ||| residual attention generative adversarial networks for nuclei detection on routine colon cancer histology images. ||| junwei li ||| wei shao ||| zhongnian li ||| weida li ||| daoqiang zhang ||| 
2019 ||| deep angular embedding and feature correlation attention for breast mri cancer analysis. ||| luyang luo ||| hao chen ||| xi wang ||| qi dou ||| huangjing lin ||| juan zhou ||| gongjie li ||| pheng-ann heng ||| 
2018 ||| a framework for identifying diabetic retinopathy based on anti-noise detection and attention-based fusion. ||| zhiwen lin ||| ruoqian guo ||| yanjie wang ||| bian wu ||| tingting chen ||| wenzhe wang ||| danny z. chen ||| jian wu ||| 
2021 ||| less is more: contrast attention assisted u-net for kidney, tumor and cyst segmentations. ||| mengran wu ||| zhiyang liu ||| 
2021 ||| surgical instruction generation with transformers. ||| jinglu zhang ||| yinyu nie ||| jian chang ||| jian-jun zhang ||| 
2019 ||| a cascade attention network for liver lesion classification in weakly-labeled multi-phase ct images. ||| xiao chen ||| lanfen lin ||| hongjie hu ||| qiaowei zhang ||| yutaro iwamoto ||| xianhua han ||| yen-wei chen ||| ruofeng tong ||| jian wu ||| 
2019 ||| permutohedral attention module for efficient non-local neural networks. ||| samuel joutard ||| reuben dorent ||| amanda isaac ||| s ||| bastien ourselin ||| tom vercauteren ||| marc modat ||| 
2021 ||| co-segmentation of multi-modality spinal image using channel and spatial attention. ||| yaocong zou ||| yonghong shi ||| 
2021 ||| ted-net: convolution-free t2t vision transformer-based encoder-decoder dilation network for low-dose ct denoising. ||| dayang wang ||| zhan wu ||| hengyong yu ||| 
2021 ||| teds-net: enforcing diffeomorphisms in spatial transformers to guarantee topology preservation in segmentations. ||| madeleine k. wyburd ||| nicola k. dinsdale ||| ana i. l. namburete ||| mark jenkinson ||| 
2021 ||| ultrasound video transformers for cardiac ejection fraction estimation. ||| hadrien reynaud ||| athanasios vlontzos ||| benjamin hou ||| arian beqiri ||| paul leeson ||| bernhard kainz ||| 
2020 ||| automatic localization of landmarks in craniomaxillofacial cbct images using a local attention-based graph convolution network. ||| yankun lang ||| chunfeng lian ||| deqiang xiao ||| hannah h. deng ||| peng yuan ||| jaime gateno ||| steve guo-fang shen ||| david m. alfi ||| pew-thian yap ||| james j. xia ||| dinggang shen ||| 
2020 ||| weakly supervised organ localization with attention maps regularized by local area reconstruction. ||| heng guo ||| minfeng xu ||| ying chi ||| lei zhang ||| xian-sheng hua ||| 
2018 ||| iterative attention mining for weakly supervised thoracic disease pattern localization in chest x-rays. ||| jinzheng cai ||| le lu ||| adam p. harrison ||| xiaoshuang shi ||| pingjun chen ||| lin yang ||| 
2018 ||| deep attentional features for prostate segmentation in ultrasound. ||| yi wang ||| zijun deng ||| xiaowei hu ||| lei zhu ||| xin yang ||| xuemiao xu ||| pheng-ann heng ||| dong ni ||| 
2021 ||| learning with noise: mask-guided attention model for weakly supervised nuclei segmentation. ||| ruoyu guo ||| maurice pagnucco ||| yang song ||| 
2019 ||| feature transformers: privacy preserving lifelong learners for medical imaging. ||| hariharan ravishankar ||| rahul venkataramani ||| saihareesh anamandra ||| prasad sudhakar ||| pavan annangi ||| 
2021 ||| e-dssr: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception. ||| yonghao long ||| zhaoshuo li ||| chi hang yee ||| chi-fai ng ||| russell h. taylor ||| mathias unberath ||| qi dou ||| 
2020 ||| error attention interactive segmentation of medical image through matting and fusion. ||| weifeng hu ||| xiaofen yao ||| zhou zheng ||| xiaoyun zhang ||| yu-min zhong ||| xiaoxia wang ||| ya zhang ||| yanfeng wang ||| 
2020 ||| prediction of plantar shear stress distribution by conditional gan with attention mechanism. ||| jinghui guo ||| ali ersen ||| yang gao ||| yu lin ||| latifur khan ||| metin yavuz ||| 
2020 ||| learning rich attention for pediatric bone age assessment. ||| chuanbin liu ||| hongtao xie ||| yunyan yan ||| zhendong mao ||| yongdong zhang ||| 
2020 ||| model-driven deep attention network for ultra-fast compressive sensing mri guided by cross-contrast mr image. ||| yan yang ||| na wang ||| heran yang ||| jian sun ||| zongben xu ||| 
2021 ||| instance-based vision transformer for subtyping of papillary renal cell carcinoma in histopathological image. ||| zeyu gao ||| bangyang hong ||| xianli zhang ||| yang li ||| chang jia ||| jialun wu ||| chunbao wang ||| deyu meng ||| chen li ||| 
2021 ||| utnet: a hybrid transformer architecture for medical image segmentation. ||| yunhe gao ||| mu zhou ||| dimitris n. metaxas ||| 
2021 ||| a hybrid attention ensemble framework for zonal prostate segmentation. ||| mingyan qiu ||| chenxi zhang ||| zhijian song ||| 
2020 ||| self-supervised nuclei segmentation in histopathological images using attention. ||| mihir sahasrabudhe ||| stergios christodoulidis ||| roberto salgado ||| stefan michiels ||| sherene loi ||| fabrice andr ||| nikos paragios ||| maria vakalopoulou ||| 
2019 ||| patch transformer for multi-tagging whole slide histopathology images. ||| weijian li ||| viet-duy nguyen ||| haofu liao ||| matt wilder ||| ke cheng ||| jiebo luo ||| 
2021 ||| predicting symptoms from multiphasic mri via multi-instance attention learning for hepatocellular carcinoma grading. ||| zelin qiu ||| yongsheng pan ||| jie wei ||| dijia wu ||| yong xia ||| dinggang shen ||| 
2020 ||| dual attention u-net for multi-sequence cardiac mr images segmentation. ||| hong yu ||| sen zha ||| yubin huangfu ||| chen chen ||| meng ding ||| jiangyun li ||| 
2018 ||| multi-task sonoeyenet: detection of fetal standardized planes assisted by generated sonographer attention maps. ||| yifan cai ||| harshita sharma ||| pierre chatelain ||| j. alison noble ||| 
2019 ||| weakly supervised brain lesion segmentation via attentional representation learning. ||| kai wu ||| bowen du ||| man luo ||| hongkai wen ||| yiran shen ||| jianfeng feng ||| 
2019 ||| an attention-guided deep regression model for landmark detection in cephalograms. ||| zhusi zhong ||| jie li ||| zhenxi zhang ||| zhicheng jiao ||| xinbo gao ||| 
2019 ||| interpretable multimodality embedding of cerebral cortex using attention graph network for identifying bipolar disorder. ||| huzheng yang ||| xiaoxiao li ||| yifan wu ||| siyi li ||| su lu ||| james s. duncan ||| james c. gee ||| shi gu ||| 
2020 ||| a generalizable deep-learning approach for cardiac magnetic resonance image segmentation using image augmentation and attention u-net. ||| fanwei kong ||| shawn c. shadden ||| 
2020 ||| asymmetrical multi-task attention u-net for the segmentation of prostate bed in ct image. ||| xuanang xu ||| chunfeng lian ||| shuai wang ||| andrew z. wang ||| trevor j. royce ||| ronald c. chen ||| jun lian ||| dinggang shen ||| 
2019 ||| multi-task attention-based semi-supervised learning for medical image segmentation. ||| shuai chen ||| gerda bortsova ||| antonio garc ||| a-uceda ju ||| rez ||| gijs van tulder ||| marleen de bruijne ||| 
2021 ||| few-shot domain adaptation with polymorphic transformers. ||| shaohua li ||| xiuchao sui ||| jie fu ||| huazhu fu ||| xiangde luo ||| yangqin feng ||| xinxing xu ||| yong liu ||| daniel shu wei ting ||| rick siow mong goh ||| 
2020 ||| attention-guided quality assessment for automated cryo-em grid screening. ||| hong xu ||| david e. timm ||| shireen y. elhabian ||| 
2021 ||| multi-view surgical video action detection via mixed global view attention. ||| adam schmidt ||| aidean sharghi ||| helene haugerud ||| daniel oh ||| omid mohareri ||| 
2020 ||| microscopic fine-grained instance classification through deep attention. ||| mengran fan ||| tapabrata chakraborti ||| eric i-chao chang ||| yan xu ||| jens rittscher ||| 
2021 ||| ccbanet: cascading context and balancing attention for polyp segmentation. ||| tan-cong nguyen ||| tien-phat nguyen ||| gia-han diep ||| anh-huy tran-dinh ||| tam v. nguyen ||| minh-triet tran ||| 
2021 ||| convolution-free medical image segmentation using transformers. ||| davood karimi ||| serge vasylechko ||| ali gholipour ||| 
2020 ||| domain-invariant prior knowledge guided attention networks for robust skull stripping of developing macaque brains. ||| tao zhong ||| yu zhang ||| fenqiang zhao ||| yuchen pei ||| lufan liao ||| zhenyuan ning ||| li wang ||| dinggang shen ||| gang li ||| 
2021 ||| learning dual transformer network for diffeomorphic registration. ||| yungeng zhang ||| yuru pei ||| hongbin zha ||| 
2019 ||| mvp-net: multi-view fpn with position-aware attention for deep universal lesion detection. ||| zihao li ||| shu zhang ||| junge zhang ||| kaiqi huang ||| yizhou wang ||| yizhou yu ||| 
2021 ||| self-guided multi-attention network for periventricular leukomalacia recognition. ||| zhuochen wang ||| tingting huang ||| bin xiao ||| jiayu huo ||| sheng wang ||| haoxiang jiang ||| heng liu ||| fan wu ||| xiang sean zhou ||| zhong xue ||| jian yang ||| qian wang ||| 
2021 ||| multi-head gagnn: a multi-head guided attention graph neural network for modeling spatio-temporal patterns of holistic brain functional networks. ||| jiadong yan ||| yuzhong chen ||| shimin yang ||| shu zhang ||| mingxin jiang ||| zhongbo zhao ||| tuo zhang ||| yu zhao ||| benjamin becker ||| tianming liu ||| keith m. kendrick ||| xi jiang ||| 
2020 ||| nas-scam: neural architecture search-based spatial and channel joint attention module for nuclei semantic segmentation and classification. ||| zuhao liu ||| huan wang ||| shaoting zhang ||| guotai wang ||| jin qi ||| 
2021 ||| cross-modal attention for mri and ultrasound volume registration. ||| xinrui song ||| hengtao guo ||| xuanang xu ||| hanqing chao ||| sheng xu ||| baris turkbey ||| bradford j. wood ||| ge wang ||| pingkun yan ||| 
2021 ||| transformer network for significant stenosis detection in ccta of coronary arteries. ||| xinghua ma ||| gongning luo ||| wei wang ||| kuanquan wang ||| 
2019 ||| multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures. ||| yuyin zhou ||| david dreizin ||| yingwei li ||| zhishuai zhang ||| yan wang ||| alan l. yuille ||| 
2021 ||| aligntransformer: hierarchical alignment of visual regions and disease tags for medical report generation. ||| di you ||| fenglin liu ||| shen ge ||| xiaoxia xie ||| jing zhang ||| xian wu ||| 
2020 ||| end-to-end coordinate regression model with attention-guided mechanism for landmark localization in 3d medical images. ||| jupeng li ||| yinghui wang ||| junbo mao ||| gang li ||| ruohan ma ||| 
2019 ||| rca-u-net: residual channel attention u-net for fast tissue quantification in magnetic resonance fingerprinting. ||| zhenghan fang ||| yong chen ||| dong nie ||| weili lin ||| dinggang shen ||| 
2020 ||| multimodality biomedical image registration using free point transformer networks. ||| zachary m. c. baum ||| yipeng hu ||| dean c. barratt ||| 
2021 ||| integration of patch features through self-supervised learning and transformer for survival analysis on whole slide images. ||| ziwang huang ||| hua chai ||| ruoqi wang ||| haitao wang ||| yuedong yang ||| hejun wu ||| 
2019 ||| multi-task learning for neonatal brain segmentation using 3d dense-unet with dense attention guided by geodesic distance. ||| toan duc bui ||| li wang ||| jian chen ||| weili lin ||| gang li ||| dinggang shen ||| 
2021 ||| multimodal spatial attention network for automatic head and neck tumor segmentation in fdg-pet and ct images. ||| minjeong cho ||| yujin choi ||| donghwi hwang ||| si young yie ||| hanvit kim ||| jae sung lee ||| 
2018 ||| multimodal recurrent model with attention for automated radiology report generation. ||| yuan xue ||| tao xu ||| l. rodney long ||| zhiyun xue ||| sameer k. antani ||| george r. thoma ||| xiaolei huang ||| 
2018 ||| multiview two-task recursive attention model for left atrium and atrial scars segmentation. ||| jun chen ||| guang yang ||| zhifan gao ||| hao ni ||| elsa d. angelini ||| raad mohiaddin ||| tom wong ||| yanping zhang ||| xiuquan du ||| heye zhang ||| jennifer keegan ||| david n. firmin ||| 
2020 ||| graph attention multi-instance learning for accurate colorectal cancer staging. ||| ashwin raju ||| jiawen yao ||| mohammad minhazul haq ||| jitendra jonnagaddala ||| junzhou huang ||| 
2021 ||| co-graph attention reasoning based imaging and clinical features integration for lymph node metastasis prediction. ||| hui cui ||| ping xuan ||| qiangguo jin ||| mingjun ding ||| butuo li ||| bing zou ||| yiyue xu ||| bingjie fan ||| wanlong li ||| jinming yu ||| linlin wang ||| henry b. l. duh ||| 
2021 ||| sama: spatially-aware multimodal network with attention for early lung cancer diagnosis. ||| mafe roa ||| laura alexandra daza ||| mar ||| a escobar ||| angela castillo ||| pablo arbelaez ||| 
2019 ||| triple anet: adaptive abnormal-aware attention network for wce image classification. ||| xiaoqing guo ||| yixuan yuan ||| 
2019 ||| rsanet: recurrent slice-wise attention network for multiple sclerosis lesion segmentation. ||| hang zhang ||| jinwei zhang ||| qihao zhang ||| jeremy kim ||| shun zhang ||| susan a. gauthier ||| pascal spincemaille ||| thanh d. nguyen ||| mert r. sabuncu ||| yi wang ||| 
2021 ||| deep mri reconstruction with generative vision transformers. ||| yilmaz korkmaz ||| mahmut yurt ||| salman ul hassan dar ||| muzaffer  ||| zbey ||| tolga  ||| ukur ||| 
2021 ||| multimodal pet/ct tumour segmentation and prediction of progression-free survival using a full-scale unet with attention. ||| emmanuelle bourigault ||| daniel r. mcgowan ||| abolfazl mehranian ||| bartlomiej w. papiez ||| 
2021 ||| opera: attention-regularized transformers for surgical phase recognition. ||| tobias czempiel ||| magdalini paschali ||| daniel ostler ||| seong tae kim ||| benjamin busam ||| nassir navab ||| 
2021 ||| efficient multi-model vision transformer based on feature fusion for classification of dfuc2021 challenge. ||| abdul qayyum ||| abdesslam benzinou ||| moona mazher ||| fabrice m ||| riaudeau ||| 
2021 ||| focusing on clinically interpretable features: selective attention regularization for liver biopsy image classification. ||| chong yin ||| siqi liu ||| rui shao ||| pong c. yuen ||| 
2020 ||| detect and identify aneurysms based on adjusted 3d attention unet. ||| yizhuan jia ||| weibin liao ||| yi lv ||| ziyu su ||| jiaqi dou ||| zhongwei sun ||| xuesong li ||| 
2021 ||| end-to-end ugly duckling sign detection for melanoma identification with transformers. ||| zhen yu ||| victoria mar ||| anders eriksson ||| shakes chandra ||| c. paul bonnington ||| lei zhang ||| zongyuan ge ||| 
2021 ||| transbridge: a lightweight transformer for left ventricle segmentation in echocardiography. ||| kaizhong deng ||| yanda meng ||| dongxu gao ||| joshua bridge ||| yaochun shen ||| gregory y. h. lip ||| yitian zhao ||| yalin zheng ||| 
2021 ||| the head and neck tumor segmentation in pet/ct based on multi-channel attention network. ||| guoshuai wang ||| zhengyong huang ||| hao shen ||| zhanli hu ||| 
2021 ||| lesion segmentation and recist diameter prediction via click-driven attention and dual-path connection. ||| youbao tang ||| ke yan ||| jinzheng cai ||| lingyun huang ||| guotong xie ||| jing xiao ||| jingjing lu ||| gigin lin ||| le lu ||| 
2020 ||| cerebrovascular segmentation in mra via reverse edge attention network. ||| hao zhang ||| likun xia ||| ran song ||| jianlong yang ||| huaying hao ||| jiang liu ||| yitian zhao ||| 
2019 ||| encoder-decoder attention network for lesion segmentation of diabetic retinopathy. ||| shuanglang feng ||| weifang zhu ||| heming zhao ||| fei shi ||| zuoyong li ||| xinjian chen ||| 
2019 ||| the channel attention based context encoder network for inner limiting membrane detection. ||| hao qiu ||| zaiwang gu ||| lei mou ||| xiaoqian mao ||| liyang fang ||| yitian zhao ||| jiang liu ||| jun cheng ||| 
2021 ||| dual-branch attention network and atrous spatial pyramid pooling for diabetic retinopathy classification using ultra-widefield images. ||| zhihui tian ||| haijun lei ||| hai xie ||| xianlu zeng ||| xinyu zhao ||| miaohong chen ||| guoming zhang ||| baiying lei ||| 
2021 ||| 3d transformer-gan for high-quality pet reconstruction. ||| yanmei luo ||| yan wang ||| chen zu ||| bo zhan ||| xi wu ||| jiliu zhou ||| dinggang shen ||| luping zhou ||| 
2021 ||| attention-based multi-scale gated recurrent encoder with novel correlation loss for covid-19 progression prediction. ||| aishik konwer ||| joseph bae ||| gagandeep singh ||| rishabh gattu ||| syed ali ||| jeremy green ||| tej phatak ||| prateek prasanna ||| 
2021 ||| dllnet: an attention-based deep learning method for dental landmark localization on high-resolution 3d digital dental models. ||| yankun lang ||| hannah h. deng ||| deqiang xiao ||| chunfeng lian ||| tianshu kuang ||| jaime gateno ||| pew-thian yap ||| james j. xia ||| 
2020 ||| automatic head and neck tumor segmentation in pet/ct with scale attention network. ||| yading yuan ||| 
2019 ||| automated segmentation of skin lesion based on pyramid attention network. ||| huan wang ||| guotai wang ||| ze sheng ||| shaoting zhang ||| 
2021 ||| transbts: multimodal brain tumor segmentation using transformer. ||| wenxuan wang ||| chen chen ||| meng ding ||| hong yu ||| sen zha ||| jiangyun li ||| 
2021 ||| multi-compound transformer for accurate biomedical image segmentation. ||| yuanfeng ji ||| ruimao zhang ||| huijie wang ||| zhen li ||| lingyun wu ||| shaoting zhang ||| ping luo ||| 
2021 ||| task transformer network for joint mri reconstruction and super-resolution. ||| chun-mei feng ||| yunlu yan ||| huazhu fu ||| li chen ||| yong xu ||| 
2019 ||| volumetric attention for 3d medical image segmentation and detection. ||| xudong wang ||| shizhong han ||| yunqiang chen ||| dashan gao ||| nuno vasconcelos ||| 
2021 ||| predicting esophageal fistula risks using a multimodal self-attention network. ||| yulu guan ||| hui cui ||| yiyue xu ||| qiangguo jin ||| tian feng ||| huawei tu ||| ping xuan ||| wanlong li ||| linlin wang ||| henry been-lirn duh ||| 
2019 ||| arrhythmia classification with attention-based res-bilstm-net. ||| chengbin huang ||| renjie zhao ||| weiting chen ||| huazheng li ||| 
2019 ||| novel iterative attention focusing strategy for joint pathology localization and prediction of mci progression. ||| qingfeng li ||| xiaodan xing ||| ying sun ||| bin xiao ||| hao wei ||| quan huo ||| minqing zhang ||| xiang sean zhou ||| yiqiang zhan ||| zhong xue ||| feng shi ||| 
2020 ||| infant cognitive scores prediction with multi-stream attention-based temporal path signature features. ||| xin zhang ||| jiale cheng ||| hao ni ||| chenyang li ||| xiangmin xu ||| zhengwang wu ||| li wang ||| weili lin ||| dinggang shen ||| gang li ||| 
2021 ||| symmetry-enhanced attention network for acute ischemic infarct segmentation with non-contrast ct images. ||| kongming liang ||| kai han ||| xiuli li ||| xiaoqing cheng ||| yiming li ||| yizhou wang ||| yizhou yu ||| 
2019 ||| dpanet: a novel network based on dense pyramid feature extractor and dual correlation analysis attention modules for colon glands segmentation. ||| shuting liu ||| baochang zhang ||| xi li ||| yiqing liu ||| mengying hu ||| tian guan ||| yonghong he ||| 
2021 ||| skip-scse multi-scale attention and co-learning method for oropharyngeal tumor segmentation on multi-modal pet-ct images. ||| alessia de biase ||| wei tang ||| nikos sourlos ||| baoqiang ma ||| jiapan guo ||| nanna maria sijtsema ||| peter m. a. van ooijen ||| 
2021 ||| false positive suppression in cervical cell screening via attention-guided semi-supervised learning. ||| xiaping du ||| jiayu huo ||| yuanfang qiao ||| qian wang ||| lichi zhang ||| 
2020 ||| pay more attention to discontinuity for medical image segmentation. ||| jiajia chu ||| yajie chen ||| wei zhou ||| heshui shi ||| yukun cao ||| dandan tu ||| ri-chu jin ||| yongchao xu ||| 
2019 ||| end-to-end dementia status prediction from brain mri using multi-task weakly-supervised attention network. ||| chunfeng lian ||| mingxia liu ||| li wang ||| dinggang shen ||| 
2021 ||| multi-view analysis of unregistered medical images using cross-view transformers. ||| gijs van tulder ||| yao tong ||| elena marchiori ||| 
2020 ||| high-order attention networks for medical image segmentation. ||| fei ding ||| gang yang ||| jun wu ||| dayong ding ||| jie xv ||| gangwei cheng ||| xirong li ||| 
2019 ||| improving deep lesion detection using 3d contextual and spatial attention. ||| qingyi tao ||| zongyuan ge ||| jianfei cai ||| jianxiong yin ||| simon see ||| 
2021 ||| shallow attention network for polyp segmentation. ||| jun wei ||| yiwen hu ||| ruimao zhang ||| zhen li ||| s. kevin zhou ||| shuguang cui ||| 
2019 ||| attention-guided decoder in dilated residual network for accurate aortic valve segmentation in 3d ct scans. ||| bowen fan ||| naoki tomii ||| hiroyuki tsukihara ||| eriko maeda ||| haruo yamauchi ||| kan nawata ||| asuka hatano ||| shu takagi ||| ichiro sakuma ||| minoru ono ||| 
2021 ||| transformesh: a transformer network for longitudinal modeling of anatomical meshes. ||| ignacio sarasua ||| sebastian p ||| lsterl ||| christian wachinger ||| 
2020 ||| max-fusion u-net for multi-modal pathology segmentation with attention and dynamic resampling. ||| haochuan jiang ||| chengjia wang ||| agisilaos chartsias ||| sotirios a. tsaftaris ||| 
2017 ||| automatic classification of proximal femur fractures based on attention models. ||| anees kazi ||| shadi albarqouni ||| amelia jim ||| nez-s ||| nchez ||| sonja kirchhoff ||| peter biberthaler ||| nassir navab ||| diana mateus ||| 
2021 ||| transct: dual-path transformer for low dose computed tomography. ||| zhicheng zhang ||| lequan yu ||| xiaokun liang ||| wei zhao ||| lei xing ||| 
2021 ||| medical transformer: gated axial-attention for medical image segmentation. ||| jeya maria jose valanarasu ||| poojan oza ||| ilker hacihaliloglu ||| vishal m. patel ||| 
2019 ||| unified attentional generative adversarial network for brain tumor segmentation from multimodal unpaired images. ||| wenguang yuan ||| jia wei ||| jiabing wang ||| qianli ma ||| tolga tasdizen ||| 
2019 ||| multi-label thoracic disease image classification with cross-attention networks. ||| congbo ma ||| hu wang ||| steven c. h. hoi ||| 
2020 ||| joint left atrial segmentation and scar quantification based on a dnn with spatial encoding and shape attention. ||| lei li ||| xin weng ||| julia a. schnabel ||| xiahai zhuang ||| 
2018 ||| asdnet: attention based semi-supervised deep networks for medical image segmentation. ||| dong nie ||| yaozong gao ||| li wang ||| dinggang shen ||| 
2017 ||| attention-driven deep learning for pathological spine segmentation. ||| anjany sekuboyina ||| jan kukacka ||| jan s. kirschke ||| bjoern h. menze ||| alexander valentinitsch ||| 
2021 ||| hierarchical attention guided framework for multi-resolution collaborative whole slide image segmentation. ||| jiangpeng yan ||| hanbo chen ||| kang wang ||| yan ji ||| yuyao zhu ||| jingjing li ||| dong xie ||| zhe xu ||| junzhou huang ||| shuqun cheng ||| xiu li ||| jianhua yao ||| 
2021 ||| a topological-attention convlstm network and its application to em images. ||| jiaqi yang ||| xiaoling hu ||| chao chen ||| chialing tsai ||| 
2021 ||| improving pneumonia localization via cross-attention on medical images and reports. ||| riddhish bhalodia ||| ali hatamizadeh ||| leo k. tam ||| ziyue xu ||| xiaosong wang ||| evrim turkbey ||| daguang xu ||| 
2021 ||| integrating multimodal mris for adult adhd identification with heterogeneous graph attention convolutional network. ||| dongren yao ||| erkun yang ||| li sun ||| jing sui ||| mingxia liu ||| 
2019 ||| et-net: a generic edge-attention guidance network for medical image segmentation. ||| zhijie zhang ||| huazhu fu ||| hang dai ||| jianbing shen ||| yanwei pang ||| ling shao ||| 
2019 ||| automatic detection of ecg abnormalities by using an ensemble of deep residual networks with attention. ||| yang liu ||| runnan he ||| kuanquan wang ||| qince li ||| qiang sun ||| na zhao ||| henggui zhang ||| 
2020 ||| attention-guided deep graph neural network for longitudinal alzheimer's disease analysis. ||| junbo ma ||| xiaofeng zhu ||| defu yang ||| jiazhou chen ||| guorong wu ||| 
2021 ||| domain composition and attention for unseen-domain generalizable medical image segmentation. ||| ran gu ||| jingyang zhang ||| rui huang ||| wenhui lei ||| guotai wang ||| shaoting zhang ||| 
2021 ||| u-net with hierarchical bottleneck attention for landmark detection in fundus images of the degenerated retina. ||| shuyun tang ||| ziming qi ||| jacob granley ||| michael beyeler ||| 
2020 ||| image-level harmonization of multi-site data using image-and-spatial transformer networks. ||| robert robinson ||| qi dou ||| daniel coelho de castro ||| konstantinos kamnitsas ||| marius de groot ||| ronald m. summers ||| daniel rueckert ||| ben glocker ||| 
2019 ||| attention guided network for retinal image segmentation. ||| shihao zhang ||| huazhu fu ||| yuguang yan ||| yubing zhang ||| qingyao wu ||| ming yang ||| mingkui tan ||| yanwu xu ||| 
2019 ||| achieving accurate segmentation of nasopharyngeal carcinoma in mr images through recurrent attention. ||| jia-bin huang ||| enhong zhuo ||| haojiang li ||| lizhi liu ||| hongmin cai ||| yangming ou ||| 
2021 ||| mftrans-net: quantitative measurement of hepatocellular carcinoma via multi-function transformer regression network. ||| jianfeng zhao ||| xiaojiao xiao ||| dengwang li ||| jaron chong ||| zahra kassam ||| bo chen ||| shuo li ||| 
2021 ||| transfuse: fusing transformers and cnns for medical image segmentation. ||| yundong zhang ||| huiye liu ||| qiang hu ||| 
2019 ||| automatic segmentation of vestibular schwannoma from t2-weighted mri by deep spatial attention with hardness-weighted loss. ||| guotai wang ||| jonathan shapey ||| wenqi li ||| reuben dorent ||| alex demitriadis ||| sotirios bisdas ||| ian paddick ||| robert bradford ||| shaoting zhang ||| s ||| bastien ourselin ||| tom vercauteren ||| 
2020 ||| generalizing spatial transformers to projective geometry with applications to 2d/3d registration. ||| cong gao ||| xingtong liu ||| wenhao gu ||| benjamin killeen ||| mehran armand ||| russell h. taylor ||| mathias unberath ||| 
2021 ||| pay attention with focus: a novel learning scheme for classification of whole slide images. ||| shivam kalra ||| mohammed adnan ||| sobhan hemati ||| taher dehkharghanian ||| shahryar rahnamayan ||| hamid r. tizhoosh ||| 
2021 ||| multi-scale hybrid transformer networks: application to prostate disease classification. ||| ainkaran santhirasekaram ||| karen pinto ||| mathias winkler ||| eric o. aboagye ||| ben glocker ||| andrea g. rockall ||| 
2020 ||| multimodal priors guided segmentation of liver lesions in mri using mutual information based graph co-attention networks. ||| shaocong mo ||| ming cai ||| lanfen lin ||| ruofeng tong ||| qingqing chen ||| fang wang ||| hongjie hu ||| yutaro iwamoto ||| xian-hua han ||| yen-wei chen ||| 
2021 ||| conditional gan with an attention-based generator and a 3d discriminator for 3d medical image generation. ||| euijin jung ||| miguel luna ||| sang hyun park ||| 
2021 ||| cotr: efficiently bridging cnn and transformer for 3d medical image segmentation. ||| yutong xie ||| jianpeng zhang ||| chunhua shen ||| yong xia ||| 
2020 ||| 3d attention u-net with pretraining: a solution to cada-aneurysm segmentation challenge. ||| ziyu su ||| yizhuan jia ||| weibin liao ||| yi lv ||| jiaqi dou ||| zhongwei sun ||| xuesong li ||| 
2021 ||| effective pancreatic cancer screening on non-contrast ct scans via anatomy-aware transformers. ||| yingda xia ||| jiawen yao ||| le lu ||| lingyun huang ||| guotong xie ||| jing xiao ||| alan l. yuille ||| kai cao ||| ling zhang ||| 
2020 ||| spatiotemporal attention autoencoder (staae) for adhd classification. ||| qinglin dong ||| ning qiang ||| jinglei lv ||| xiang li ||| tianming liu ||| quanzheng li ||| 
2020 ||| demographic-guided attention in recurrent neural networks for modeling neuropathophysiological heterogeneity. ||| nicha c. dvornek ||| xiaoxiao li ||| juntang zhuang ||| pamela ventola ||| james s. duncan ||| 
2017 ||| ssemnet: serial-section electron microscopy image registration using a spatial transformer network with learned features. ||| inwan yoo ||| david g. c. hildebrand ||| willie f. tobin ||| wei-chung allen lee ||| won-ki jeong ||| 
2020 ||| attention based multiple instance learning for classification of blood cell disorders. ||| ario sadafi ||| asya makhro ||| anna bogdanova ||| nassir navab ||| tingying peng ||| shadi albarqouni ||| carsten marr ||| 
2021 ||| 3d u-net applied to simple attention module for head and neck tumor segmentation in pet and ct images. ||| tao liu ||| yixin su ||| jiabao zhang ||| tianqi wei ||| zhiyong xiao ||| 
2021 ||| covid-net us: a tailored, highly efficient, self-attention deep convolutional neural network design for detection of covid-19 patient cases from point-of-care ultrasound imaging. ||| alexander maclean ||| saad abbasi ||| ashkan ebadi ||| andy zhao ||| maya pavlova ||| hayden gunraj ||| pengcheng xi ||| sonny kohli ||| alexander wong ||| 
2021 ||| trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer. ||| xiaojie gao ||| yueming jin ||| yonghao long ||| qi dou ||| pheng-ann heng ||| 
2018 ||| attention based hierarchical aggregation network for 3d left atrial segmentation. ||| caizi li ||| qianqian tong ||| xiangyun liao ||| weixin si ||| yinzi sun ||| qiong wang ||| pheng-ann heng ||| 
2018 ||| attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ||| yuxing tang ||| xiaosong wang ||| adam p. harrison ||| le lu ||| jing xiao ||| ronald m. summers ||| 
2017 ||| agnet: attention-guided network for surgical tool presence detection. ||| xiaowei hu ||| lequan yu ||| hao chen ||| jing qin ||| pheng-ann heng ||| 
2019 ||| msafusionnet: multiple subspace attention based deep multi-modal fusion network. ||| sen zhang ||| changzheng zhang ||| lanjun wang ||| cixing li ||| dandan tu ||| rui luo ||| guojun qi ||| jiebo luo ||| 
2019 ||| cross-modal attention-guided convolutional network for multi-modal cardiac segmentation. ||| ziqi zhou ||| xinna guo ||| wanqi yang ||| yinghuan shi ||| luping zhou ||| lei wang ||| ming yang ||| 
2021 ||| graph transformers for characterization and interpretation of surgical margins. ||| amoon jamzad ||| alice m. l. santilli ||| faranak akbarifar ||| martin kaufmann ||| kathryn logan ||| julie wallis ||| kevin yi mi ren ||| shaila merchant ||| c. jay engel ||| sonal varma ||| gabor fichtinger ||| john f. rudan ||| parvin mousavi ||| 
2019 ||| cs-net: channel and spatial attention network for curvilinear structure segmentation. ||| lei mou ||| yitian zhao ||| li chen ||| jun cheng ||| zaiwang gu ||| huaying hao ||| hong qi ||| yalin zheng ||| alejandro f. frangi ||| jiang liu ||| 
2020 ||| cascaded attention guided network for retinal vessel segmentation. ||| mingxing li ||| yueyi zhang ||| zhiwei xiong ||| dong liu ||| 
2019 ||| image-and-spatial transformer networks for structure-guided image registration. ||| matthew c. h. lee ||| ozan oktay ||| andreas schuh ||| michiel schaap ||| ben glocker ||| 
2020 ||| collaborative learning of cross-channel clinical attention for radiotherapy-related esophageal fistula prediction from ct. ||| hui cui ||| yiyue xu ||| wanlong li ||| linlin wang ||| henry b. l. duh ||| 
2021 ||| attention guided slit lamp image quality assessment. ||| mingchao li ||| yerui chen ||| kun huang ||| wen fang ||| qiang chen ||| 
2020 ||| generalisable cardiac structure segmentation via attentional and stacked image adaptation. ||| hongwei li ||| jianguo zhang ||| bjoern h. menze ||| 
2021 ||| m-seam-nam: multi-instance self-supervised equivalent attention mechanism with neighborhood affinity module for double weakly supervised segmentation of covid-19. ||| wen tang ||| han kang ||| ying cao ||| pengxin yu ||| hu han ||| rongguo zhang ||| kuan chen ||| 
2019 ||| graph convolution based attention model for personalized disease prediction. ||| anees kazi ||| shayan shekarforoush ||| s. arvind krishna ||| hendrik burwinkel ||| gerome vivar ||| benedikt wiestler ||| karsten kort ||| m ||| seyed-ahmad ahmadi ||| shadi albarqouni ||| nassir navab ||| 
2018 ||| predicting cancer with a recurrent visual attention model for histopathology images. ||| a ||| cha bentaieb ||| ghassan hamarneh ||| 
2019 ||| biomedical image segmentation by retina-like sequential attention mechanism using only a few training images. ||| shohei hayashi ||| bisser raytchev ||| toru tamaki ||| kazufumi kaneda ||| 
2021 ||| ratchet: medical transformer for chest x-ray diagnosis and reporting. ||| benjamin hou ||| georgios kaissis ||| ronald m. summers ||| bernhard kainz ||| 
2018 ||| a diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism. ||| jiang tian ||| cong li ||| zhongchao shi ||| feiyu xu ||| 
2021 ||| automated kidney tumor segmentation with convolution and transformer network. ||| zhiqiang shen ||| hua yang ||| zhen zhang ||| shaohua zheng ||| 
2019 ||| brain segmentation from k-space with end-to-end recurrent attention network. ||| qiaoying huang ||| xiao chen ||| dimitris n. metaxas ||| mariappan s. nadar ||| 
2021 ||| transpath: transformer-based self-supervised learning for histopathological image classification. ||| xiyue wang ||| sen yang ||| jun zhang ||| minghui wang ||| jing zhang ||| junzhou huang ||| wei yang ||| xiao han ||| 
2021 ||| 3dmet: 3d medical image transformer for knee cartilage defect assessment. ||| sheng wang ||| zixu zhuang ||| kai xuan ||| dahong qian ||| zhong xue ||| jia xu ||| ying liu ||| yiming chai ||| lichi zhang ||| qian wang ||| dinggang shen ||| 
2021 ||| dt-mil: deformable transformer for multi-instance learning on histopathological image. ||| hang li ||| fan yang ||| yu zhao ||| xiaohan xing ||| jun zhang ||| mingxuan gao ||| junzhou huang ||| liansheng wang ||| jianhua yao ||| 
2021 ||| a multi-branch hybrid transformer network for corneal endothelial cell segmentation. ||| yinglin zhang ||| risa higashita ||| huazhu fu ||| yanwu xu ||| yang zhang ||| haofeng liu ||| jian zhang ||| jiang liu ||| 
2021 ||| progressively normalized self-attention network for video polyp segmentation. ||| ge-peng ji ||| yu-cheng chou ||| deng-ping fan ||| geng chen ||| huazhu fu ||| debesh jha ||| ling shao ||| 
2019 ||| feature pyramid based attention for cervical image classification. ||| hongfeng li ||| jian zhao ||| li zhang ||| jie zhao ||| li yang ||| quanzheng li ||| 
2020 ||| learned deep radiomics for survival analysis with attention. ||| ludivine morvan ||| cristina nanni ||| anne-victoire michaud ||| bastien jamet ||| cl ||| ment bailly ||| caroline bodet-milin ||| stephane chauvie ||| cyrille touzeau ||| philippe moreau ||| elena zamagni ||| fran ||| oise kraeber-bod ||| r ||| thomas carlier ||| diana mateus ||| 
2020 ||| multi-task dynamic transformer network for concurrent bone segmentation and large-scale landmark localization with dental cbct. ||| chunfeng lian ||| fan wang ||| hannah h. deng ||| li wang ||| deqiang xiao ||| tianshu kuang ||| hung-ying lin ||| jaime gateno ||| steve guo-fang shen ||| pew-thian yap ||| james j. xia ||| dinggang shen ||| 
2017 ||| attention-based lstm-cnns for uncertainty identification on chinese social media texts. ||| binyang li ||| kaiming zhou ||| wei gao ||| xu han ||| linna zhou ||| 
2017 ||| modeling anomalous attention over an online social network through read/post analytics. ||| zijian zhang ||| jiamou liu ||| 
2021 ||| design and research of transformer fault diagnosis method based on data-driven. ||| shoupeng wang ||| yajuan liu ||| jingzhong yuan ||| yu jiang ||| fang fang ||| libin zhang ||| yang gao ||| 
2017 ||| a convolutional attentional neural network for sentiment classification. ||| jiachen du ||| lin gui ||| yulan he ||| ruifeng xu ||| 
2021 ||| math word problem solver based on text-to-text transformer model. ||| chuanzhi yang ||| runze huang ||| xinguo yu ||| rao peng ||| 
2019 ||| design of online learning mobile app for the elderly based on attention, relevance, confidence, and satisfaction (arcs) motivation model. ||| wei xiong ||| weishan he ||| zhen liu ||| 
2020 ||| interactive attention model explorer for natural language processing tasks with unbalanced data sizes. ||| zhihang dong ||| tongshuang wu ||| sicheng song ||| mingrui zhang ||| 
2021 ||| keywordmap: attention-based visual exploration for keyword analysis. ||| yamei tu ||| jiayi xu ||| han-wei shen ||| 
2019 ||| simulation of spatial memory for human navigation based on visual attention in floorplan review. ||| yangming shi ||| jing du ||| 
2020 ||| spotnet: self-attention multi-task network for object detection. ||| hughes perreault ||| guillaume-alexandre bilodeau ||| nicolas saunier ||| maguelonne h ||| ritier ||| 
2019 ||| traffic risk assessment: a two-stream approach using dynamic-attention. ||| gary-patrick corcoran ||| james clark ||| 
2021 ||| 2lspe: 2d learnable sinusoidal positional encoding using transformer for scene text recognition. ||| zobeir raisi ||| mohamed a. naiel ||| georges younes ||| steven wardell ||| john s. zelek ||| 
2018 ||| estimation of scenes contributing to score in tennis video using attention. ||| ryunosuke kurose ||| masaki hayashi ||| yoshimitsu aoki ||| 
2019 ||| translation of sign language glosses to text using sequence-to-sequence attention models. ||| nikolaos arvanitis ||| constantinos constantinopoulos ||| dimitrios i. kosmopoulos ||| 
2019 ||| fine-grained action recognition in assembly work scenes by drawing attention to the hands. ||| takuya kobayashi ||| yoshimitsu aoki ||| shogo shimizu ||| katsuhiro kusano ||| seiji okumura ||| 
2021 ||| automatic text summarization using transformers. ||| siwar abbes ||| sarra ben abb ||| s ||| rim hantach ||| philippe calvez ||| 
2019 ||| learning contextual features with multi-head self-attention for fake news detection. ||| yangqian wang ||| hao han ||| ye ding ||| xuan wang ||| qing liao ||| 
2020 ||| end-to-end nested multi-attention network for 3d brain tumor segmentation. ||| xinrui zhuang ||| yujiu yang ||| 
2019 ||| a neural rumor detection framework by incorporating uncertainty attention on social media texts. ||| yan gao ||| xu han ||| binyang li ||| 
2018 ||| depth in the visual attention modelling from the egocentric perspective of view. ||| miroslav laco ||| wanda benesova ||| 
2018 ||| visual attention for behavioral cloning in autonomous driving. ||| tharun mohandoss ||| sourav pal ||| pabitra mitra ||| 
2019 ||| performance of bottom-up visual attention models when compared in contextless and context awareness scenarios. ||| juan anaya-jaimes ||| angie garc ||| a-castro ||| ricardo enrique guti ||| rrez-carvajal ||| 
2019 ||| a deep interactive segmentation method with user interaction-based attention module and polar transformation. ||| jee-young sun ||| sang-won lee ||| ye-won kim ||| bo-sang kim ||| sung-jea ko ||| 
2019 ||| phrase-guided attention web article recommendation for next clicks and views. ||| chia-wei chen ||| sheng-chuan chou ||| chang-you tai ||| lun-wei ku ||| 
2020 ||| fine-tuning techniques and data augmentation on transformer-based models for conversational texts and noisy user-generated content. ||| mike tian-jian jiang ||| shih-hung wu ||| yi-kun chen ||| zhao-xian gu ||| cheng-jhe chiang ||| yueh-chia wu ||| yu-chen huang ||| cheng-han chiu ||| sheng-ru shaw ||| min-yuh day ||| 
2020 ||| vstreamdrls: dynamic graph representation learning with self-attention for enterprise distributed video streaming solutions. ||| stefanos antaris ||| dimitrios rafailidis ||| 
2021 ||| scate: shared cross attention transformer encoders for multimodal fake news detection. ||| tanmay sachan ||| nikhil pinnaparaju ||| manish gupta ||| vasudeva varma ||| 
2021 ||| group-node attention for community evolution prediction. ||| matt revelle ||| carlotta domeniconi ||| ben u. gelman ||| 
2021 ||| analyzing topic attention in online small groups. ||| josemar alves caetano ||| jussara m. almeida ||| marcos andr |||  gon ||| alves ||| wagner meira jr. ||| humberto torres marques-neto ||| virg ||| lio a. f. almeida ||| 
2021 ||| care: learning convolutional attentional recurrent embedding for sequential recommendation. ||| yu-che tsai ||| cheng-te li ||| 
2021 ||| hgats: hierarchical graph attention networks for multiple comments integration. ||| huixin zhan ||| kun zhang ||| chenyi hu ||| victor s. sheng ||| 
2020 ||| co-refining user and item representations with feature-level self-attention for enhanced recommendation. ||| zikai guo ||| deqing yang ||| baichuan liu ||| lyuxin xue ||| yanghua xiao ||| 
2021 ||| ian: interpretable attention network for churn prediction in lbsns. ||| liang-yu chen ||| yutong chen ||| young d. kwon ||| youwen kang ||| pan hui ||| 
2020 ||| language identification on massive datasets of short messages using an attention mechanism cnn. ||| duy-tin vo ||| richard khoury ||| 
2017 ||| multiplex media attention and disregard network among 129 countries. ||| haewoon kwak ||| jisun an ||| 
2019 ||| attention and vigilance detection based on electroencephalography - a summary of a literature review. ||| rozaliya amirova ||| anastasiia repryntseva ||| herman tarasau ||| artem v. kruglov ||| sara busechianv ||| 
2021 ||| a multi-scale deep learning attention-based feature method for rolling elements bearing fault detection in industrial motor drives. ||| yannis l. karnavas ||| spyridon plakias ||| ioannis d. chasiotis ||| 
2017 ||| a methodology for integrated transformer compact modeling. ||| yiannis moisiadis ||| konstantinos nikelis ||| padelis papadopoulos ||| 
2021 ||| the role of diodes in the leakage current suppression mechanism of decoupling transformerless pv inverter topologies. ||| georgios i. orfanoudakis ||| eftichios koutroulis ||| georgios foteinopoulos ||| 
2020 ||| accuracy improvement of instantaneous frequency estimation by finite order fir hilbert transformer using notch filter. ||| keisuke takao ||| takahiro natori ||| toma miyata ||| naoyuki aikawa ||| 
2017 ||| questions classification with attention machine. ||| yunlu liaozheng ||| jin liu ||| jin wang ||| 
2019 ||| tagattention: mobile object tracing without object appearance information by vision-rfid fusion. ||| xiaofeng shi ||| minmei wang ||| ge wang ||| baiwen huang ||| haofan cai ||| junjie xie ||| chen qian ||| 
2020 ||| chinese punctuation prediction with adaptive attention and dependency tree. ||| zelong yan ||| jianzong wang ||| ning cheng ||| tianbo wu ||| jing xiao ||| 
2021 ||| structural dependency self-attention based hierarchical event model for chinese financial event extraction. ||| zhi liu ||| hao xu ||| haitao wang ||| dan zhou ||| guilin qi ||| wanqi sun ||| shirong shen ||| jiawei zhao ||| 
2020 ||| obstetric diagnosis assistant via knowledge powered attention and information-enhanced strategy. ||| kunli zhang ||| xu zhao ||| lei zhuang ||| hongying zan ||| qi xie ||| 
2017 ||| attention-based event relevance model for stock price movement prediction. ||| jian liu ||| yubo chen ||| kang liu ||| jun zhao ||| 
2018 ||| an enhanced esim model for sentence pair matching with self-attention. ||| yongkang liu ||| xiaobo liang ||| feiliang ren ||| yan li ||| yining hou ||| yi zhang ||| lingfeng pan ||| 
2021 ||| a biaffine attention-based approach for event factor extraction. ||| jiangzhou ji ||| yaohan he ||| jinlong li ||| 
2021 ||| dependency to semantics: structure transformation and syntax-guided attention for neural semantic parsing. ||| shan wu ||| bo chen ||| xianpei han ||| le sun ||| 
2019 ||| adaptive multilingual representations for cross-lingual entity linking with attention on entity descriptions. ||| chenhao wang ||| yubo chen ||| kang liu ||| jun zhao ||| 
2019 ||| fast neural chinese named entity recognition with multi-head self-attention. ||| tao qi ||| chuhan wu ||| fangzhao wu ||| suyu ge ||| junxin liu ||| yongfeng huang ||| xing xie ||| 
2019 ||| reka: relation extraction with knowledge-aware attention. ||| peiyi wang ||| hongtao liu ||| fangzhao wu ||| jinduo song ||| hongyan xu ||| wenjun wang ||| 
2021 ||| patentminer: patent vacancy mining via context-enhanced and knowledge-guided graph attention. ||| gaochen wu ||| bin xu ||| yuxin qin ||| fei kong ||| bangchang liu ||| hongwen zhao ||| dejie chang ||| 
2018 ||| adversarial training for relation classification with attention based gate mechanism. ||| pengfei cao ||| yubo chen ||| kang liu ||| jun zhao ||| 
2019 ||| discriminative spectral-spatial attention-aware residual network for hyperspectral image classification. ||| yaoming cai ||| zhimin dong ||| zhihua cai ||| xiaobo liu ||| guangjun wang ||| 
2019 ||| hyperspectral and multispectral image fusion based on deep attention network. ||| qing yang ||| yang xu ||| zebin wu ||| zhihui wei ||| 
2021 ||| spectral-spatial-temporal attention network for hyperspectral tracking. ||| zhuanfeng li ||| xinhai ye ||| fengchao xiong ||| jianfeng lu ||| jun zhou ||| yuntao qian ||| 
2021 ||| convcatb: an attention-based cnn-catboost risk prediction model for driving safety. ||| xinhong hei ||| hao zhang ||| wenjiang ji ||| yichuan wang ||| lei zhu ||| yuan qiu ||| 
2021 ||| dual attention mechanism object tracking algorithm based on fully-convolutional siamese network. ||| sugang ma ||| zixian zhang ||| lei zhang ||| yanping chen ||| xiaobao yang ||| lei pu ||| zhiqiang hou ||| 
2021 ||| attentionae: autoencoder for anomaly detection in attributed networks. ||| kenan qin ||| yihui zhou ||| bo tian ||| rui wang ||| 
2019 ||| question generation for reading comprehension of language learning test : -a method using seq2seq approach with transformer model-. ||| junjie shan ||| yoko nishihara ||| ryosuke yamanishi ||| akira maeda ||| 
2020 ||| extraction of question-related sentences for reading comprehension tests via attention mechanism. ||| junjie shan ||| yoko nishihara ||| akira maeda ||| ryosuke yamanishi ||| 
2019 ||| deep residual attention reinforcement learning. ||| hanhua zhu ||| tomoyuki kaneko ||| 
2020 ||| aspect-based sentiment analysis on convolution neural network and multi-hierarchical attention. ||| zhixiong lou ||| yuexin wu ||| chunxiao fan ||| wentong chen ||| 
2019 ||| currency exchange rate prediction with long short-term memory networks based on attention and news sentiment analysis. ||| ching-i lee ||| chia-hui chang ||| feng-nan hwang ||| 
2018 ||| interactive interface design for the evaluation of attention deficiencies in preschool children. ||| alberto s ||| nchez-morales ||| claudia lizbeth mart ||| nez-gonz ||| lez ||| franceli l. cibrian ||| monica tentori ||| 
2017 ||| entropic brain-computer interfaces - using fnirs and eeg to measure attentional states in a bayesian framework. ||| samuel w. hincks ||| sarah bratt ||| sujit poudel ||| vir v. phoha ||| robert j. k. jacob ||| daniel c. dennett ||| leanne m. hirshfield ||| 
2017 ||| a hardware/software platform to acquire bioelectrical signals. a case study: characterizing computer access through attention. ||| alberto j. molina ||| isabel m. g ||| mez ||| jaime guerrero ||| manuel merino monge ||| juan a. castro-garc ||| a ||| royl ||| n quesada ||| santiago berrazueta ||| mar ||| a hermoso-de-mendoza ||| 
2021 ||| gates: using graph attention networks for entity summarization. ||| asep fajar firmansyah ||| diego moussallem ||| axel-cyrille ngonga ngomo ||| 
2019 ||| contextual graph attention for answering logical queries over incomplete knowledge graphs. ||| gengchen mai ||| krzysztof janowicz ||| bo yan ||| rui zhu ||| ling cai ||| ni lao ||| 
2021 ||| application of transfer learning in field verification for children in attention deficit hyperactivity disorder. ||| jo-wei lin ||| yang chang ||| chih-hao chang ||| li-wei ko ||| 
2017 ||| detecting driver's visual attention area by using vehicle-mounted device. ||| nobuhiro mizuno ||| akira yoshizawa ||| akihiro hayashi ||| takahiro ishikawa ||| 
2018 ||| visual cognitive attention based bag-of-words image representation for object discovery. ||| zhong ma ||| zhuping wang ||| 
2019 ||| measuring and controlling cognitive process of visual attention in forest fire monitoring system. ||| ljiljana seric ||| damir krstinic ||| pero bogunovic ||| 
2018 ||| cognitive training modulates cognitive processes of the brain: the response inhibition improved by attention training. ||| yang wang ||| yi wang ||| xuebing li ||| 
2017 ||| question answering over knowledgebase with attention-based lstm networks and knowledge embeddings. ||| liu chen ||| guangping zeng ||| qingchuan zhang ||| xingyu chen ||| danfeng wu ||| 
2019 ||| cognitive attention in autism using virtual reality learning tool. ||| vidhusha s ||| b. divya ||| kavitha anandan ||| r. viswath narayanan ||| d. yaamini ||| 
2020 ||| sapcgan: self-attention based generative adversarial network for point clouds. ||| yushi li ||| george baciu ||| 
2017 ||| analysis of driver's visual attention using near-miss incidents. ||| akira yoshizawa ||| hirotoshi iwasaki ||| 
2021 ||| scnet: a generalized attention-based model for crack fault segmentation. ||| hrishikesh sharma ||| prakhar pradhan ||| balamuralidhar p. ||| 
2021 ||| meranet: facial micro-expression recognition using 3d residual attention network. ||| gajjala viswanatha reddy ||| sai prasanna teja reddy ||| snehasis mukherjee ||| shiv ram dubey ||| 
2018 ||| a bottom-up and top-down approach for image captioning using transformer. ||| sandeep narayan parameswaran ||| sukhendu das ||| 
2021 ||| attention guided complementary feature integration for latent image recovery from noisy/blurry pairs. ||| green rosh k. s ||| sachin deepak lomte ||| nikhil krishnan ||| b. h. pawan prasad ||| 
2020 ||| scaling language data import/export with a data transformer interface. ||| nicholas buckeridge ||| ben foley ||| 
2017 ||| automatic assessment of engagement and attention of the student by means of facial expressions. ||| catalina alejandra v ||| zquez rodr ||| guez ||| ra ||| l pinto-el ||| as ||| 
2021 ||| small geodetic datasets and deep networks: attention-based residual lstm autoencoder stacking for geodetic time series. ||| mostafa kiani shahvandi ||| benedikt soja ||| 
2021 ||| synthesizing object state transformers for dynamic software updates. ||| zelin zhao ||| yanyan jiang ||| chang xu ||| tianxiao gu ||| xiaoxing ma ||| 
2021 ||| pasta: synthesizing object state transformers for dynamic software updates. ||| zelin zhao ||| yanyan jiang ||| chang xu ||| tianxiao gu ||| xiaoxing ma ||| 
2021 ||| studying the usage of text-to-text transfer transformer to support code-related tasks. ||| antonio mastropaolo ||| simone scalabrino ||| nathan cooper ||| david nader-palacio ||| denys poshyvanyk ||| rocco oliveto ||| gabriele bavota ||| 
2021 ||| code prediction by feeding trees to transformers. ||| seohyun kim ||| jinman zhao ||| yuchi tian ||| satish chandra ||| 
2019 ||| eeg based driver inattention identification via feature profiling and dimensionality reduction. ||| omid dehzangi ||| mojtaba taherisadr ||| 
2019 ||| the attention pattern emerging from information technology: a structural perspective. ||| isabel ramos ||| 
2018 ||| attention models for motor coordination and resulting interface design. ||| bettina wollesen ||| laura l. bischoff ||| johannes r ||| nnfeldt ||| klaus mattes ||| 
2022 ||| universal adversarial perturbation generated by using attention information. ||| zifei wang ||| xiaolin huang ||| jie yang ||| nikola k. kasabov ||| 
2022 ||| 3d cnn architectures and attention mechanisms for deepfake detection. ||| ritaban roy ||| indu joshi ||| abhijit das ||| antitza dantcheva ||| 
2021 ||| theory of mind and joint attention. ||| jairo p ||| rez-osorio ||| eva wiese ||| agnieszka wykowska ||| 
2022 ||| graph attention lstm: a spatiotemporal approach for traffic flow forecasting. ||| tianqi zhang ||| ge guo ||| 
2022 ||| vehicle trajectory prediction using lstms with spatial-temporal attention mechanisms. ||| lei lin ||| weizi li ||| huikun bi ||| lingqiao qin ||| 
2017 ||| connectivity preserving network transformers. ||| othon michail ||| paul g. spirakis ||| 
2021 ||| attention based multi-component spatiotemporal cross-domain neural network model for wireless cellular network traffic prediction. ||| qingtian zeng ||| qiang sun ||| geng chen ||| hua duan ||| 
2017 ||| effects of animation on attentional resources of online consumers. ||| muller y. m. cheung ||| weiyin hong ||| james y. l. thong ||| 
2019 ||| when risks need attention: adoption of green supply chain initiatives in the pharmaceutical industry. ||| anil kumar ||| edmundas kazimieras zavadskas ||| sachin kumar mangla ||| varun agrawal ||| kartik sharma ||| divyanshu gupta ||| 
2020 ||| chinese microblog sentiment detection based on cnn-bigru and multihead attention mechanism. ||| hong qiu ||| chongdi fan ||| jie yao ||| xiaohan ye ||| michele risi ||| 
2021 ||| network course recommendation system based on double-layer attention mechanism. ||| qianyao zhu ||| 
2021 ||| neighborhood attentional memory networks for recommendation systems. ||| tianlong gu ||| hongliang chen ||| chenzhong bin ||| liang chang ||| wei chen ||| 
2020 ||| adaptive residual channel attention network for single image super-resolution. ||| kerang cao ||| yuqing liu ||| lini duan ||| tian xie ||| 
2021 ||| english machine translation model based on an improved self-attention technology. ||| wenxia pan ||| 
2021 ||| facial expression recognition based on attention mechanism. ||| jiang dai-hong ||| yuanzheng hu ||| dai lei ||| peng jin ||| 
2021 ||| a multifeature complementary attention mechanism for image topic representation in social networks. ||| lei shi ||| jia luo ||| gang cheng ||| xia liu ||| gang xie ||| 
2020 ||| face detection and recognition based on visual attention mechanism guidance model in unrestricted posture. ||| zhenguo yuan ||| 
2021 ||| multi-scale guided attention network for crowd counting. ||| pengfei li ||| min zhang ||| jian wan ||| ming jiang ||| 
2021 ||| research on intelligent english translation method based on the improved attention mechanism model. ||| rong wang ||| 
2021 ||| performance analysis of hybrid deep learning models with attention mechanism positioning and focal loss for text classification. ||| sunil kumar prabhakar ||| harikumar rajaguru ||| dong-ok won ||| 
2021 ||| innovative research on the development of online education mode of internet thinking based on the discrimination of learning attention under the analysis of head posture. ||| su song ||| fangzheng wang ||| 
2021 ||| voice keyword retrieval method using attention mechanism and multimodal information fusion. ||| hongli zhang ||| 
2021 ||| spatial transformer network-based automatic modulation recognition of blind signals. ||| yuxin huang ||| 
2021 ||| learning deep attention network from incremental and decremental features for evolving features. ||| chuxin wang ||| haoran mo ||| 
2019 ||| software defect prediction via attention-based recurrent neural network. ||| guisheng fan ||| xuyang diao ||| huiqun yu ||| kang yang ||| liqiong chen ||| 
2021 ||| multi-instance deep learning based on attention mechanism for failure prediction of unlabeled hard disk drives. ||| guochao wang ||| yu wang ||| xiaojie sun ||| 
2019 ||| detection of hv winding radial deformation and pd in power transformer using stepped-frequency hyperboloid method. ||| hamidreza tabarsa ||| maryam a. hejazi ||| gevork b. gharehpetian ||| 
2021 ||| qscgan: an un-supervised quick self-attention convolutional gan for lre bearing fault diagnosis under limited label-lacked data. ||| wenqing wan ||| shuilong he ||| jinglong chen ||| aimin li ||| yong feng ||| 
2017 ||| an ac power standard for loss measurement systems for testing power transformers. ||| enrico mohns ||| peter r ||| ther ||| henrik badura ||| 
2021 ||| high-frequency current transformer design and implementation considerations for wideband partial discharge applications. ||| joni v. kl ||| ss ||| alf-peter elg ||| claes wingqvist ||| 
2021 ||| attention recurrent autoencoder hybrid model for early fault diagnosis of rotating machinery. ||| xiangwei kong ||| xueyi li ||| qingzhao zhou ||| zhiyong hu ||| cheng shi ||| 
2022 ||| an auxiliary framework to mitigate measurement inaccuracies caused by capacitive voltage transformers. ||| amir ameli ||| mohsen ghafouri ||| magdy m. a. salama ||| ehab f. el-saadany ||| 
2021 ||| transformer fault prognosis using deep recurrent neural network over vibration signals. ||| amin zollanvari ||| kassymzhomart kunanbayev ||| saeid akhavan bitaghsir ||| mehdi bagheri ||| 
2020 ||| a comb-type capacitive 2-fal sensor for transformer oil with improved sensitivity. ||| md manzar nezami ||| shufali ashraf wani ||| shakeb ahmad khan ||| neeraj khera ||| shiraz sohail ||| 
2021 ||| a miniature transformer-coupled low-noise preamplifier for low source resistance sensors at low frequency. ||| fan liu ||| xingfei li ||| ganming xia ||| 
2019 ||| a low-cost generator for testing and calibrating current transformers. ||| loredana cristaldi ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2019 ||| transformer health management based on self-powered rfid sensor and multiple kernel rvm. ||| tao wang ||| yigang he ||| tiancheng shi ||| jin tong ||| bing li ||| 
2022 ||| toward small sample challenge in intelligent fault diagnosis: attention-weighted multidepth feature fusion net with signals augmentation. ||| tianci zhang ||| shuilong he ||| jinglong chen ||| tongyang pan ||| zitong zhou ||| 
2021 ||| feature analysis of oscillating wave signal for axial displacement in autotransformer. ||| zhenyu wu ||| lijun zhou ||| dongyang wang ||| meng zhou ||| feiming jiang ||| xingyu yu ||| huiling tang ||| haiquan zhao ||| 
2018 ||| characterization of voltage instrument transformers under nonsinusoidal conditions based on the best linear approximation. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2020 ||| performances evaluation of on-chip large-size-tapped transformer for mems applications. ||| fares tounsi ||| denis flandre ||| libor rufer ||| laurent a. francis ||| 
2021 ||| vit-p: classification of genitourinary syndrome of menopause from oct images based on vision transformer models. ||| haoran wang ||| yanju ji ||| kaiwen song ||| mingyang sun ||| peitong lv ||| tianyu zhang ||| 
2020 ||| recovery of partial discharge signal and noise cancellation in power transformer using radial basis function. ||| ehsan khavari ||| seyed mohammad hassan hosseini ||| gevork b. gharehpetian ||| 
2021 ||| 3-d facial expression recognition via attention-based multichannel data fusion network. ||| yu gu ||| huan yan ||| xiang zhang ||| zhi liu ||| fuji ren ||| 
2021 ||| retinanet with difference channel attention and adaptively spatial feature fusion for steel surface defect detection. ||| xun cheng ||| jianbo yu ||| 
2021 ||| visual landmark learning via attention-based deep neural networks. ||| mingyu you ||| chaoxian luo ||| hongjun zhou ||| 
2021 ||| lightweight attention module for deep learning on classification and segmentation of 3-d point clouds. ||| yunhao cui ||| yi an ||| wei sun ||| huosheng hu ||| xueguan song ||| 
2019 ||| harmonic distortion compensation in voltage transformers for improved power quality measurements. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2022 ||| effective fault diagnosis based on wavelet and convolutional attention neural network for induction motors. ||| minh-quang tran ||| meng-kun liu ||| quoc-viet tran ||| toan-khoa nguyen ||| 
2019 ||| mechanical fault diagnostics of power transformer on-load tap changers using dynamic time warping. ||| ruilin yang ||| dandan zhang ||| zhenbiao li ||| kai yang ||| shi mo ||| li li ||| 
2021 ||| 3-kv two-stage voltage transformer with high-voltage excitation. ||| hao liu ||| feng zhou ||| lixue chen ||| min lei ||| xiaodong yin ||| chunyang jiang ||| jian liu ||| 
2021 ||| 1100-kv uhvdc all fiber current transformer. ||| jun zhao ||| lei shi ||| xiaohan sun ||| 
2021 ||| linking attention-based multiscale cnn with dynamical gcn for driving fatigue detection. ||| hongtao wang ||| linfeng xu ||| anastasios bezerianos ||| chuangquan chen ||| zhi guo zhang ||| 
2020 ||| a new testing method for the diagnosis of winding faults in transformer. ||| zhenyu wu ||| lijun zhou ||| tong lin ||| xiangyu zhou ||| dongyang wang ||| shibin gao ||| feiming jiang ||| 
2021 ||| fds measurement-based moisture estimation model for transformer oil-paper insulation including the aging effect. ||| xianhao fan ||| jiefeng liu ||| benghui lai ||| yiyi zhang ||| chaohai zhang ||| 
2021 ||| voltage dependence of the reference system in medium- and high-voltage current transformer calibrations. ||| helko e. van den brom ||| ronald van leeuwen ||| gert rietveld ||| ernest houtzager ||| 
2021 ||| dual attention-based temporal convolutional network for fault prognosis under time-varying operating conditions. ||| chongdang liu ||| linxuan zhang ||| rong yao ||| cheng wu ||| 
2017 ||| voltage ratio traceability of 10 kv low-voltage excited two-stage voltage transformer. ||| huanghui zhang ||| haiming shao ||| jiafu wang ||| wei wang ||| feipeng lin ||| tianyu sun ||| wei zhao ||| chuansheng li ||| yiqian wu ||| 
2022 ||| performance improvement of transformer differential protection during cross-country fault using hyperbolic s-transform. ||| nassim shahbazi ||| sajad bagheri ||| gevork b. gharehpetian ||| 
2021 ||| attention and feature fusion ssd for remote sensing object detection. ||| xiaocong lu ||| jian ji ||| zhiqi xing ||| qiguang miao ||| 
2021 ||| multipath fusion mask r-cnn with double attention and its application into gear pitting detection. ||| dejun xi ||| yi qin ||| jun luo ||| huayan pu ||| zhiwen wang ||| 
2019 ||| detection of winding faults based on a characterization of the nonlinear dynamics of transformers. ||| jing zheng ||| hai huang ||| jie pan ||| 
2021 ||| self-attention convlstm and its application in rul prediction of rolling bearings. ||| biao li ||| baoping tang ||| lei deng ||| minghang zhao ||| 
2017 ||| frequency response of mv voltage transformer under actual waveforms. ||| gabriella crotti ||| daniele gallo ||| domenico giordano ||| carmine landi ||| mario luiso ||| mohammad modarres ||| 
2021 ||| gas volume fraction measurement of oil-gas-water three-phase flows in vertical pipe by combining ultrasonic sensor and deep attention network. ||| weikai ren ||| ningde jin ||| lei ouyang ||| lusheng zhai ||| yingyu ren ||| 
2020 ||| detecting trees in street images via deep learning with attention module. ||| qian xie ||| dawei li ||| zhenghao yu ||| jun zhou ||| jun wang ||| 
2022 ||| -net: triple-attention semantic segmentation network for small surface defect detection. ||| taiheng liu ||| zhaoshui he ||| 
2021 ||| plane-wave image reconstruction via generative adversarial network and attention mechanism. ||| jiahua tang ||| bao zou ||| chang li ||| shuai feng ||| hu peng ||| 
2022 ||| dual-aspect self-attention based on transformer for remaining useful life prediction. ||| zhizheng zhang ||| wen song ||| qiqiang li ||| 
2022 ||| res2fusion: infrared and visible image fusion based on dense res2net and double nonlocal attention models. ||| zhishe wang ||| yuanyuan wu ||| junyao wang ||| jiawei xu ||| wenyu shao ||| 
2021 ||| anchor-based spatio-temporal attention 3-d convolutional networks for dynamic 3-d point cloud sequences. ||| guangming wang ||| hanwen liu ||| muyao chen ||| yehui yang ||| zhe liu ||| hesheng wang ||| 
2021 ||| self-detecting the measurement error of electronic voltage transformer based on principal component analysis-wavelet packet decomposition. ||| binbin li ||| zhu zhang ||| lijian ding ||| 
2021 ||| accurate fault diagnosis in transformers using an auxiliary current-compensation-based framework for differential relays. ||| amir ameli ||| mohsen ghafouri ||| hatem h. zeineldin ||| magdy m. a. salama ||| ehab f. el-saadany ||| 
2019 ||| overcoming frequency response measurements of voltage transformers: an approach based on quasi-sinusoidal volterra models. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| gabriella crotti ||| domenico giordano ||| luca barbieri ||| marco gondola ||| paolo mazza ||| 
2021 ||| transformer winding faults detection based on time series analysis. ||| alireza abbasi ||| mohammad reza mahmoudi ||| mohammad mehdi arefi ||| 
2017 ||| an ac current transformer standard measuring system for power frequencies. ||| enrico mohns ||| gunter roeissle ||| soeren fricke ||| florian pauling ||| 
2021 ||| classify and localize threat items in x-ray imagery with multiple attention mechanism and high-resolution and high-semantic features. ||| ruiyang xia ||| guoquan li ||| zhengwen huang ||| lingyun wen ||| yu pang ||| 
2020 ||| nestfuse: an infrared and visible image fusion architecture based on nest connection and spatial/channel attention models. ||| hui li ||| xiao-jun wu ||| tariq s. durrani ||| 
2021 ||| a novel application of the cross-capacitive sensor in real-time condition monitoring of transformer oil. ||| obaidur rahman ||| tarikul islam ||| neeraj khera ||| shakeb ahmad khan ||| 
2020 ||| a defect inspection for explosive cartridge using an improved visual attention and image-weighted eigenvalue. ||| liang xu ||| haibo xu ||| xiuxi li ||| ming pan ||| 
2022 ||| dense attention-guided cascaded network for salient object detection of strip steel surface defects. ||| xiaofei zhou ||| hao fang ||| zhi liu ||| bolun zheng ||| yaoqi sun ||| jiyong zhang ||| chenggang yan ||| 
2020 ||| an innovative approach to express uncertainty introduced by voltage transformers. ||| marco faifer ||| alessandro ferrero ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2021 ||| high-precision self-calibrating current transformer with stray capacitances control. ||| daniel slomovitz ||| alejandro santos ||| rogelio sandler ||| gabriela barreto ||| 
2021 ||| measuring harmonics with inductive voltage transformers in presence of subharmonics. ||| gabriella crotti ||| giovanni d'avanzo ||| palma sara letizia ||| mario luiso ||| 
2020 ||| effects of multiple influence quantities on rogowski-coil-type current transformers. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2017 ||| a new calibration transformer and measurement setup for bridge standard calibrations up to 5 khz. ||| marc florian beug ||| axel kolling ||| harald moser ||| 
2019 ||| compensation of current transformers' nonlinearities by tensor linearization. ||| adam j. collin ||| antonio delle femine ||| daniele gallo ||| roberto langella ||| mario luiso ||| 
2021 ||| eta-rppgnet: effective time-domain attention network for remote heart rate measurement. ||| min hu ||| fei qian ||| dong guo ||| xizhao wang ||| lei he ||| fuji ren ||| 
2021 ||| a modified simulation model for predicting the fds of transformer oil-paper insulation under nonuniform aging. ||| jiefeng liu ||| tengyue sun ||| xianhao fan ||| yiyi zhang ||| lai benhui ||| 
2017 ||| wiener filtering for real-time dsp compensation of current transformers over a wide frequency range. ||| martin dadic ||| petar mostarac ||| roman malaric ||| 
2021 ||| transportation monitoring of geo-location, speed, vibration, and shock acceleration for 110-kv vehicular mobile transformers. ||| jinglu wu ||| yadong fan ||| jianguo wang ||| zeting liu ||| li cai ||| mi zhou ||| zhenpeng tang ||| 
2021 ||| a new method for transformer fault prediction based on multifeature enhancement and refined long short-term memory. ||| xin ma ||| hao hu ||| yizi shang ||| 
2020 ||| improved stepup method to determine the errors of voltage instrument transformer with high accuracy. ||| feng zhou ||| chunyang jiang ||| min lei ||| fuchang lin ||| 
2021 ||| rcag-net: residual channelwise attention gate network for hot spot defect detection of photovoltaic farms. ||| binyi su ||| haiyong chen ||| kun liu ||| weipeng liu ||| 
2017 ||| a computer-controlled calibrator for instrument transformer test sets. ||| stefan siegenthaler ||| christian mester ||| 
2021 ||| comparing the new improved rlc and cmtl models for measuring partial discharge in transformer winding. ||| peyman rezaei baravati ||| seyed mohammad hassan hosseini ||| majid moazzami ||| 
2019 ||| a high-precision current transformer for loss measurements of ehv shunt reactors. ||| eddy so ||| rob verhoeven ||| bart simons ||| harold v. parks ||| dave angelo ||| 
2019 ||| compensation of nonlinearity of voltage and current instrument transformers. ||| antonio cataliotti ||| valentina cosentino ||| gabriella crotti ||| antonio delle femine ||| dario di cara ||| daniele gallo ||| domenico giordano ||| carmine landi ||| mario luiso ||| mohammad modarres ||| giovanni tin ||| 
2019 ||| comparison of reference setups for calibrating power transformer loss measurement systems. ||| gert rietveld ||| enrico mohns ||| ernest houtzager ||| henrik badura ||| dennis hoogenboom ||| 
2021 ||| research on residual flux density measurement for single-phase transformer core based on energy changes. ||| cailing huo ||| youhua wang ||| shipu wu ||| chengcheng liu ||| 
2021 ||| dftnet: deep fish tracker with attention mechanism in unconstrained marine environments. ||| shilpi gupta ||| prerana mukherjee ||| santanu chaudhury ||| brejesh lall ||| hemanth sanisetty ||| 
2022 ||| a novel anti-dc bias energy meter based on magnetic-valve-type current transformer. ||| baichao chen ||| zhexuan zhang ||| yuwen wu ||| cuihua tian ||| yaojun chen ||| 
2021 ||| joint attention network for finger vein authentication. ||| junduan huang ||| mo tu ||| weili yang ||| wenxiong kang ||| 
2021 ||| rotating machinery fault diagnosis through a transformer convolution network subjected to transfer learning. ||| xinglong pei ||| xiaoyang zheng ||| jinliang wu ||| 
2022 ||| selective multibranch attention network with material constraint for baggage reidentification. ||| hui zhang ||| ruibo chen ||| chen li ||| yurong chen ||| yaonan wang ||| q. m. jonathan wu ||| 
2021 ||| harmonic synchrophasors measurement algorithms with embedded compensation of voltage transformer frequency response. ||| paolo castello ||| christian laurano ||| carlo muscas ||| paolo attilio pegoraro ||| sergio toscani ||| michele zanoni ||| 
2021 ||| a sequence-to-sequence model with attention and monotonicity loss for tool wear monitoring and prediction. ||| gang wang ||| feng zhang ||| 
2022 ||| surface defect detection of steel strips based on anchor-free network with channel attention and bidirectional feature fusion. ||| jianbo yu ||| xun cheng ||| qingfeng li ||| 
2020 ||| condition assessment of power transformer insulation using short-duration time-domain dielectric spectroscopy measurement data. ||| deepak mishra ||| arijit baral ||| nasirul haque ||| sivaji chakravorti ||| 
2019 ||| a variational mode decomposition approach for degradation assessment of power transformer windings. ||| kaixing hong ||| ling wang ||| suan xu ||| 
2020 ||| detrapped charge-affected depolarization-current estimation using short-duration dielectric response for diagnosis of transformer insulation. ||| chandra madhab banerjee ||| arijit baral ||| sivaji chakravorti ||| 
2022 ||| eanet: edge-attention 6d pose estimation network for texture-less objects. ||| yuqi zhang ||| yuanpeng liu ||| qiaoyun wu ||| jun zhou ||| xiaoxi gong ||| jun wang ||| 
2019 ||| a fundamental step-up method for standard voltage transformers based on an active capacitive high-voltage divider. ||| enrico mohns ||| chunyang jiang ||| henrik badura ||| peter raether ||| 
2021 ||| automatic attention learning using neural architecture search for detection of cardiac abnormality in 12-lead ecg. ||| zuhao liu ||| huan wang ||| yibo gao ||| shunchen shi ||| 
2022 ||| clformer: a lightweight transformer based on convolutional embedding and linear self-attention with strong robustness for bearing fault diagnosis under limited sample conditions. ||| hairui fang ||| jin deng ||| yaoxu bai ||| bo feng ||| sheng li ||| siyu shao ||| dongsheng chen ||| 
2021 ||| sps-net: self-attention photometric stereo network. ||| huiyu liu ||| yunhui yan ||| kechen song ||| han yu ||| 
2021 ||| rotating machine systems fault diagnosis using semisupervised conditional random field-based graph attention network. ||| yao tang ||| xiaofei zhang ||| yujia zhai ||| guojun qin ||| dianyi song ||| shoudao huang ||| zhuo long ||| 
2021 ||| defect detection method of aluminum profile surface using deep self-attention mechanism under hybrid noise conditions. ||| renxiang chen ||| dongyin cai ||| xiaolin hu ||| zan zhan ||| shuai wang ||| 
2017 ||| a comparison of two current transformer calibration systems at nrc canada. ||| branislav v. djokic ||| harold parks ||| nicholas wise ||| dragana naumovic-vukovic ||| slobodan p. skundric ||| aleksandar d. zigic ||| vladimir poluzanski ||| 
2022 ||| a novel interpretable method based on dual-level attentional deep neural network for actual multilabel arrhythmia detection. ||| yanrui jin ||| jinlei liu ||| yunqing liu ||| chengjin qin ||| zhiyuan li ||| dengyu xiao ||| liqun zhao ||| chengliang liu ||| 
2021 ||| multigrained attention network for infrared and visible image fusion. ||| jing li ||| hongtao huo ||| chang li ||| renhua wang ||| chenhong sui ||| zhao liu ||| 
2020 ||| temperature measuring-based decision-making prognostic approach in electric power transformers winding failures. ||| milad soleimani ||| jawad faiz ||| pedram shahriari nasab ||| mehdi moallem ||| 
2019 ||| impact of coreless current transformer position on current measurement. ||| xiaoyu ma ||| yi guo ||| xianan chen ||| yukai xiang ||| kun-long chen ||| 
2021 ||| deep rational attention network with threshold strategy embedded for mechanical fault diagnosis. ||| dongfang zhao ||| hongli zhang ||| shulin liu ||| yuan wei ||| shungen xiao ||| 
2021 ||| acr-net: attention integrated and cross-spatial feature fused rotation network for tubular solder joint detection. ||| chenlin zhou ||| daheng li ||| peng wang ||| jia sun ||| yikun huang ||| wanyi li ||| 
2022 ||| improving the temperature and vibration robustness of fiber optic current transformer using fiber polarization rotator. ||| wenxia sima ||| li zeng ||| ming yang ||| tao yuan ||| potao sun ||| 
2021 ||| unsupervised anomaly segmentation via multilevel image reconstruction and adaptive attention-level transition. ||| yi yan ||| deming wang ||| guangliang zhou ||| qijun chen ||| 
2020 ||| nonlinear behavioral modeling of voltage transformers in the frequency domain: comparing different approaches. ||| marco faifer ||| christian laurano ||| roberto ottoboni ||| sergio toscani ||| michele zanoni ||| 
2019 ||| a new vibration testing platform for electronic current transformers. ||| zhenhua li ||| yuan tao ||| ahmed abu-siada ||| mohammad a. s. masoum ||| zhenxing li ||| yanchun xu ||| xiaozhen zhao ||| 
2021 ||| integrated multiple directed attention-based deep learning for improved air pollution forecasting. ||| abdelkader dairi ||| fouzi harrou ||| sofiane khadraoui ||| ying sun ||| 
2020 ||| image captioning using facial expression and attention. ||| omid mohamad nezami ||| mark dras ||| stephen wan ||| c ||| cile paris ||| 
2019 ||| interpretable charge prediction for criminal cases with dynamic rationale attention. ||| wenhan chao ||| xin jiang ||| zhunchen luo ||| yakun hu ||| wenjia ma ||| 
2021 ||| multi-document summarization with determinantal point process attention. ||| laura perez-beltrachini ||| mirella lapata ||| 
2020 ||| bi-directional recurrent attentional topic model. ||| shuangyin li ||| yu zhang ||| rong pan ||| 
2020 ||| a deep multi-task contextual attention framework for multi-modal affect analysis. ||| md. shad akhtar ||| dushyant singh chauhan ||| asif ekbal ||| 
2022 ||| kran: knowledge refining attention network for recommendation. ||| zhenyu zhang ||| lei zhang ||| dingqi yang ||| liu yang ||| 
2022 ||| dimbert: learning vision-language grounded representations with disentangled multimodal-attention. ||| fenglin liu ||| xian wu ||| shen ge ||| xuancheng ren ||| wei fan ||| xu sun ||| yuexian zou ||| 
2022 ||| knowledge distillation with attention for deep transfer learning of convolutional networks. ||| xingjian li ||| haoyi xiong ||| zeyu chen ||| jun huan ||| ji liu ||| cheng-zhong xu ||| dejing dou ||| 
2022 ||| graph-based stock recommendation by time-aware relational attention network. ||| jianliang gao ||| xiaoting ying ||| cong xu ||| jianxin wang ||| shichao zhang ||| zhao li ||| 
2021 ||| harp: a novel hierarchical attention model for relation prediction. ||| yashen wang ||| huanhuan zhang ||| 
2019 ||| attention models in graphs: a survey. ||| john boaz lee ||| ryan a. rossi ||| sungchul kim ||| nesreen k. ahmed ||| eunyee koh ||| 
2021 ||| ggatb-lstm: grouping and global attention-based time-aware bidirectional lstm medical treatment behavior prediction. ||| lin cheng ||| yuliang shi ||| kun zhang ||| xinjun wang ||| zhiyong chen ||| 
2020 ||| nguard+: an attention-based game bot detection framework via player behavior sequences. ||| jiarong xu ||| yifan luo ||| jianrong tao ||| changjie fan ||| zhou zhao ||| jiangang lu ||| 
2018 ||| the effect of attention cueing on science text learning. ||| 
2021 ||| attention based vehicle trajectory prediction. ||| kaouther messaoud ||| itheri yahiaoui ||| anne verroust-blondet ||| fawzi nashashibi ||| 
2021 ||| attention-based gated recurrent unit for gesture recognition. ||| ghazaleh khodabandelou ||| pyeong-gook jung ||| yacine amirat ||| samer mohammed ||| 
2019 ||| salientdso: bringing attention to direct sparse odometry. ||| huai-jen liang ||| nitin j. sanket ||| cornelia ferm ||| ller ||| yiannis aloimonos ||| 
2021 ||| needs-based product configurator design for mass customization using hierarchical attention network. ||| yue wang ||| wenlong zhao ||| wayne xinwei wan ||| 
2021 ||| fine-grained user location prediction using meta-path context with attention mechanism. ||| zhixiao wang ||| wenyao yan ||| ang gao ||| 
2022 ||| hybrid ctc-attention network-based end-to-end speech recognition system for korean language. ||| hosung park ||| changmin kim ||| hyunsoo son ||| soonshin seo ||| ji-hwan kim ||| 
2020 ||| sara-gan: self-attention and relative average discriminator based generative adversarial networks for fast compressed sensing mri reconstruction. ||| zhenmou yuan ||| mingfeng jiang ||| yaming wang ||| bo wei ||| yongming li ||| pin wang ||| wade menpes-smith ||| zhangming niu ||| guang yang ||| 
2019 ||| transcranial magnetic stimulation to the middle frontal gyrus during attention modes induced dynamic module reconfiguration in brain networks. ||| penghui song ||| hua lin ||| chunyan liu ||| yuanling jiang ||| yicong lin ||| qing xue ||| peng xu ||| yuping wang ||| 
2020 ||| machine learning methods for diagnosing autism spectrum disorder and attention- deficit/hyperactivity disorder using functional and structural mri: a survey. ||| taban eslami ||| fahad almuqhim ||| joseph s. raiker ||| fahad saeed ||| 
2020 ||| developmental designs and adult functions of cortical maps in multiple modalities: perception, attention, navigation, numbers, streaming, speech, and cognition. ||| stephen grossberg ||| 
2021 ||| dr-iixrn : detection algorithm of diabetic retinopathy based on deep ensemble learning and attention mechanism. ||| zhuang ai ||| xuan huang ||| yuan fan ||| jing feng ||| fanxin zeng ||| yaping lu ||| 
2020 ||| effects of visual attentional load on the tactile sensory memory indexed by somatosensory mismatch negativity. ||| xin he ||| jian zhang ||| zhilin zhang ||| ritsu go ||| jinglong wu ||| chunlin li ||| kai gan ||| duanduan chen ||| 
2019 ||| fuzzy sliding mode control with state estimation for velocity control system of hydraulic cylinder using a new hydraulic transformer. ||| wei shen ||| jiehao wang ||| honglei huang ||| junyi he ||| 
2021 ||| short-term wind speed prediction with a two-layer attention-based lstm. ||| jingcheng qian ||| mingfang zhu ||| yingnan zhao ||| xiangjian he ||| 
2022 ||| facial expression recognition using enhanced convolution neural network with attention mechanism. ||| k. prabhu ||| s. sathishkumar ||| m. sivachitra ||| s. dineshkumar ||| p. sathiyabama ||| 
2021 ||| mixed attention densely residual network for single image super-resolution. ||| jingjun zhou ||| jing liu ||| jingbing li ||| mengxing huang ||| jieren cheng ||| yen-wei chen ||| yingying xu ||| saqib ali nawaz ||| 
2020 ||| predicting mobile trading system discontinuance: the role of attention. ||| dongyeon kim ||| kyuhong park ||| dong-joo lee ||| yongkil ahn ||| 
2019 ||| attention to online channels across the path to purchase: an eye-tracking study. ||| m ||| nica corti ||| as ||| rafael cabeza ||| raquel chocarro ||| arantxa villanueva ||| 
2020 ||| a multi-attention matching model for multiple-choice reading comprehension. ||| liguo duan ||| jianying gao ||| aiping li ||| 
2021 ||| transformer models for text-based emotion detection: a review of bert-based approaches. ||| francisca adoma acheampong ||| henry nunoo-mensah ||| wenyu chen ||| 
2020 ||| selective attention to historical comparison or social comparison in the evolutionary iterated prisoner's dilemma game. ||| weijun zeng ||| minqiang li ||| 
2022 ||| attention-based neural joint source-channel coding of text for point to point and broadcast channel. ||| ting liu ||| xuechen chen ||| 
2021 ||| a novel hybrid network model based on attentional multi-feature fusion for deception detection. ||| yuanbo fang ||| hongliang fu ||| huawei tao ||| ruiyu liang ||| li zhao ||| 
2021 ||| self-channel attention weighted part for person re-identification. ||| lin du ||| chang tian ||| mingyong zeng ||| jiabao wang ||| shanshan jiao ||| qing shen ||| wei bai ||| aihong lu ||| 
2020 ||| siamese attention-based lstm for speech emotion recognition. ||| tashpolat nizamidin ||| li zhao ||| ruiyu liang ||| yue xie ||| askar hamdulla ||| 
2017 ||| attentional control and other executive functions. ||| athanasios drigas ||| maria karyotaki ||| 
2017 ||| the role of perceived relevance and attention in teachers' attitude and intention to use educational video games. ||| antonio s ||| nchez-mena ||| jos |||  mart |||  parre ||| o ||| joaqu ||| n ald ||| s-manzano ||| 
2019 ||| attention and its role: theories and models. ||| athanasios drigas ||| maria karyotaki ||| 
2020 ||| joint model-based attention for spoken language understanding task. ||| xin liu ||| ruihua qi ||| lin shao ||| 
2018 ||| a digital phase shift method for phase compensation of electronic transformer. ||| wei wei ||| han-miao cheng ||| fan li ||| dengping tang ||| shui-bin xia ||| 
2021 ||| experimental investigation on creep characteristic of the spacer between winding turns of power transformers. ||| ahmet yigit arabul ||| celal fadil kumru ||| fatma keskin arabul ||| ibrahim senol ||| 
2018 ||| driver inattention monitoring system based on multimodal fusion with visual cues to improve driving safety. ||| xuanpeng li ||| emmanuel seignez ||| 
2019 ||| study of an accurate electronic power measurement technique using modified current transformer and potential transformer. ||| sirshendu saha ||| saikat kumar bera ||| hiranmoy mandal ||| pradip kumar sadhu ||| satish chandra bera ||| 
2018 ||| high accuracy optical voltage transformer with digital output based on coaxial capacitor voltage divider. ||| zhen-hua li ||| shuang zhao ||| 
2020 ||| a foot in two camps or your undivided attention? the impact of intra- and inter-community collaboration on firm innovation performance. ||| shuhui zhang ||| na zhang ||| shanshan zhu ||| fengchao liu ||| 
2021 ||| cross-scale global attention feature pyramid network for person search. ||| yang li ||| huahu xu ||| minjie bian ||| junsheng xiao ||| 
2021 ||| dct-net: a deep co-interactive transformer network for video temporal grounding. ||| wen wang ||| jian cheng ||| siyu liu ||| 
2021 ||| unsupervised cross-domain person re-identification with self-attention and joint-flexible optimization. ||| haopeng hou ||| yong zhou ||| jiaqi zhao ||| rui yao ||| ying chen ||| yi zheng ||| abdulmotaleb el-saddik ||| 
2021 ||| atcc: accurate tracking by criss-cross location attention. ||| yong wu ||| zhi liu ||| xiaofei zhou ||| linwei ye ||| yang wang ||| 
2021 ||| attention-guided aggregation stereo matching network. ||| yaru zhang ||| yaqian li ||| chao wu ||| bin liu ||| 
2021 ||| multi-tier attention network using term-weighted question features for visual question answering. ||| sruthy manmadhan ||| binsu c. kovoor ||| 
2021 ||| visual question answering model based on graph neural network and contextual attention. ||| himanshu sharma ||| anand singh jalal ||| 
2021 ||| a study on attention-based lstm for abnormal behavior recognition with variable pooling. ||| kai zhou ||| bei hui ||| junfeng wang ||| chunyu wang ||| tingting wu ||| 
2022 ||| facial expression recognition using densely connected convolutional neural network and hierarchical spatial attention. ||| chenquan gan ||| junhao xiao ||| zhangyi wang ||| zufan zhang ||| qingyi zhu ||| 
2020 ||| variance-guided attention-based twin deep network for cross-spectral periocular recognition. ||| sushree sangeeta behera ||| sapna s. mishra ||| bappaditya mandal ||| niladri b. puhan ||| 
2021 ||| roi tanh-polar transformer network for face parsing in the wild. ||| yiming lin ||| jie shen ||| yujiang wang ||| maja pantic ||| 
2020 ||| a calibration method of computer vision system based on dual attention mechanism. ||| youling li ||| 
2022 ||| improving image captioning with pyramid attention and sc-gan. ||| tianyu chen ||| zhixin li ||| jingli wu ||| huifang ma ||| bianping su ||| 
2020 ||| cross-correlated attention networks for person re-identification. ||| jieming zhou ||| soumava kumar roy ||| pengfei fang ||| mehrtash harandi ||| lars petersson ||| 
2021 ||| point cloud completion using multiscale feature fusion and cross-regional attention. ||| hang wu ||| yubin miao ||| ruochong fu ||| 
2021 ||| graph-based reasoning attention pooling with curriculum design for content-based image retrieval. ||| xiaoguang zhu ||| haoyu wang ||| peilin liu ||| zhantao yang ||| jiuchao qian ||| 
2022 ||| ltst: long-term segmentation tracker with memory attention network. ||| lang yu ||| baojun qiao ||| huanlong zhang ||| junyang yu ||| xin he ||| 
2020 ||| an attention-based deep learning model for multiple pedestrian attributes recognition. ||| ehsan yaghoubi ||| diana borza ||| jo ||| o c. neves ||| aruna kumar ||| hugo proen ||| a ||| 
2021 ||| spatiotemporal module for video saliency prediction based on self-attention. ||| yuhao wang ||| zhuoran liu ||| yibo xia ||| chunbo zhu ||| danpei zhao ||| 
2021 ||| attention-guided chained context aggregation for semantic segmentation. ||| quan tang ||| fagui liu ||| tong zhang ||| jun jiang ||| yu zhang ||| 
2020 ||| pcanet: pyramid convolutional attention network for semantic segmentation. ||| haiwei sang ||| qiuhao zhou ||| yong zhao ||| 
2020 ||| non-local attention association scheme for online multi-object tracking. ||| haidong wang ||| saizhou wang ||| jingyi lv ||| chenming hu ||| zhiyong li ||| 
2021 ||| multi-information-based convolutional neural network with attention mechanism for pedestrian trajectory prediction. ||| ruiping wang ||| yong cui ||| xiao song ||| kai chen ||| hong fang ||| 
2021 ||| multimodal assessment of apparent personality using feature attention and error consistency constraint. ||| s ||| leyman aslan ||| ugur g ||| d ||| kbay ||| hamdi dibeklioglu ||| 
2021 ||| short-term anchor linking and long-term self-guided attention for video object detection. ||| daniel cores ||| v ||| ctor m. brea ||| manuel mucientes ||| 
2022 ||| attention guided contextual feature fusion network for salient object detection. ||| jin zhang ||| yanjiao shi ||| qing zhang ||| liu cui ||| ying chen ||| yugen yi ||| 
2021 ||| flow guided mutual attention for person re-identification. ||| madhu kiran ||| amran bhuiyan ||| le thanh nguyen-meidine ||| louis-antoine blais-morin ||| ismail ben ayed ||| eric granger ||| 
2021 ||| da-sacot: domain adaptive-segmentation guided attention for correlation based object tracking. ||| priya mariam raju ||| deepak mishra ||| prerana mukherjee ||| 
2020 ||| crossatnet - a novel cross-attention based framework for sketch-based image retrieval. ||| ushasi chaudhuri ||| biplab banerjee ||| avik bhattacharya ||| mihai datcu ||| 
2021 ||| transformer models for enhancing attngan based text to image generation. ||| s. naveen ||| m. s. s. ram kiran ||| m. indupriya ||| t. v. manikanta ||| p. v. sudeep ||| 
2020 ||| a novel co-attention computation block for deep learning based image co-segmentation. ||| xiaopeng gong ||| xiabi liu ||| yushuo li ||| huiyu li ||| 
2020 ||| attention-guided rgbd saliency detection using appearance information. ||| xiaofei zhou ||| gongyang li ||| chen gong ||| zhi liu ||| jiyong zhang ||| 
2020 ||| cam: a fine-grained vehicle model recognition method based on visual attention model. ||| ye yu ||| longdao xu ||| wei jia ||| wenjia zhu ||| yunxiang fu ||| qiang lu ||| 
2021 ||| exploring region relationships implicitly: image captioning with visual relationship attention. ||| zongjian zhang ||| qiang wu ||| yang wang ||| fang chen ||| 
2022 ||| pu-gacnet: graph attention convolution network for point cloud upsampling. ||| bing han ||| xinyun zhang ||| shuang ren ||| 
2020 ||| spatial biases in crowdsourced data: social media content attention concentrates on populous areas in disasters. ||| chao fan ||| miguel esparza ||| jennifer dargin ||| fangsheng wu ||| bora oztekin ||| ali mostafavi ||| 
2020 ||| capturing what human eyes perceive: a visual hierarchy generation approach to emulating saliency-based visual attention for grid-like urban street networks. ||| wenjie zhen ||| lin yang ||| mei-po kwan ||| zejun zuo ||| bo wan ||| shunping zhou ||| shengwen li ||| yaqin ye ||| haoyue qian ||| xiaofang pan ||| 
2019 ||| functional and contextual attention-based lstm for service recommendation in mashup creation. ||| min shi ||| yufei tang ||| jianxun liu ||| 
2021 ||| multi-deformation aware attention learning for concrete structural defect classification. ||| gaurab bhattacharya ||| bappaditya mandal ||| niladri b. puhan ||| 
2022 ||| action-centric relation transformer network for video question answering. ||| jipeng zhang ||| jie shao ||| rui cao ||| lianli gao ||| xing xu ||| heng tao shen ||| 
2021 ||| multi-level fusion and attention-guided cnn for image dehazing. ||| xiaoqin zhang ||| tao wang ||| wenhan luo ||| pengcheng huang ||| 
2020 ||| recurrent prediction with spatio-temporal attention for crowd attribute recognition. ||| qiaozhe li ||| xin zhao ||| ran he ||| kaiqi huang ||| 
2020 ||| complementation-reinforced attention network for person re-identification. ||| chuchu han ||| ruochen zheng ||| changxin gao ||| nong sang ||| 
2020 ||| video summarization with attention-based encoder-decoder networks. ||| zhong ji ||| kailin xiong ||| yanwei pang ||| xuelong li ||| 
2020 ||| porn streamer recognition in live video streaming via attention-gated multimodal deep features. ||| liyuan wang ||| jing zhang ||| qi tian ||| chenhao li ||| li zhuo ||| 
2022 ||| iid-net: image inpainting detection network via neural architecture search and attention. ||| haiwei wu ||| jiantao zhou ||| 
2017 ||| attention-weighted texture and depth bit-allocation in general-geometry free-viewpoint television. ||| camilo c. dorea ||| ricardo l. de queiroz ||| 
2020 ||| video dialog via multi-grained convolutional self-attention context multi-modal networks. ||| mao gu ||| zhou zhao ||| weike jin ||| deng cai ||| fei wu ||| 
2019 ||| two-stream collaborative learning with spatial-temporal attention for video classification. ||| yuxin peng ||| yunzhen zhao ||| junchao zhang ||| 
2021 ||| deep convolutional-neural-network-based channel attention for single image dynamic scene blind deblurring. ||| shengdao wan ||| shu tang ||| xianzhong xie ||| jia gu ||| rong huang ||| bin ma ||| lei luo ||| 
2020 ||| driver drowsiness recognition via 3d conditional gan and two-level attention bi-lstm. ||| yaocong hu ||| mingqi lu ||| chao xie ||| xiaobo lu ||| 
2021 ||| attention-aligned network for person re-identification. ||| sicheng lian ||| weitao jiang ||| haifeng hu ||| 
2020 ||| optical flow estimation using dual self-attention pyramid networks. ||| mingliang zhai ||| xuezhi xiang ||| rongfang zhang ||| ning lv ||| abdulmotaleb el-saddik ||| 
2022 ||| transformer-based language-person search with multiple region slicing. ||| hui li ||| jimin xiao ||| mingjie sun ||| eng gee lim ||| yao zhao ||| 
2020 ||| multimodal transformer with multi-view visual representation for image captioning. ||| jun yu ||| jing li ||| zhou yu ||| qingming huang ||| 
2021 ||| multi-view spatial attention embedding for vehicle re-identification. ||| shangzhi teng ||| shiliang zhang ||| qingming huang ||| nicu sebe ||| 
2021 ||| a novel just-noticeable-difference-based saliency-channel attention residual network for full-reference image quality predictions. ||| soomin seo ||| sehwan ki ||| munchurl kim ||| 
2022 ||| dahp: deep attention-guided hashing with pairwise labels. ||| xue li ||| jiong yu ||| yongqiang wang ||| jia-ying chen ||| peng-xiao chang ||| ziyang li ||| 
2021 ||| cross-view gait recognition using pairwise spatial transformer networks. ||| chi xu ||| yasushi makihara ||| xiang li ||| yasushi yagi ||| jianfeng lu ||| 
2021 ||| multi-turn video question generation via reinforced multi-choice attention network. ||| zhaoyu guo ||| zhou zhao ||| weike jin ||| zhicheng wei ||| min yang ||| nannan wang ||| nicholas jing yuan ||| 
2020 ||| probabilistic topic model for context-driven visual attention understanding. ||| miguel angel fernandez-torres ||| iv ||| n gonz ||| lez-d ||| az ||| fernando d ||| az-de-mar ||| a ||| 
2020 ||| fast and accurate action detection in videos with motion-centric attention model. ||| jinzhuo wang ||| wenmin wang ||| wen gao ||| 
2021 ||| dynamic attention guided multi-trajectory analysis for single object tracking. ||| xiao wang ||| zhe chen ||| jin tang ||| bin luo ||| yaowei wang ||| yonghong tian ||| feng wu ||| 
2022 ||| tagnet: triplet-attention graph networks for hashtag recommendation. ||| yu-chi chen ||| kuan-ting lai ||| dong liu ||| ming-syan chen ||| 
2021 ||| multi-grained attention networks for single image super-resolution. ||| huapeng wu ||| zhengxia zou ||| jie gui ||| wen-jun zeng ||| jieping ye ||| jun zhang ||| hongyi liu ||| zhihui wei ||| 
2022 ||| stacked multimodal attention network for context-aware video captioning. ||| yi zheng ||| yuejie zhang ||| rui feng ||| tao zhang ||| weiguo fan ||| 
2022 ||| hierarchical feature fusion with mixed convolution attention for single image dehazing. ||| xiaoqin zhang ||| jinxin wang ||| tao wang ||| runhua jiang ||| 
2021 ||| multimodal local-global attention network for affective video content analysis. ||| yangjun ou ||| zhenzhong chen ||| feng wu ||| 
2019 ||| sharp attention network via adaptive sampling for person re-identification. ||| chen shen ||| guo-jun qi ||| rongxin jiang ||| zhongming jin ||| hongwei yong ||| yaowu chen ||| xian-sheng hua ||| 
2022 ||| self-paced feature attention fusion network for concealed object detection in millimeter-wave image. ||| xinlin wang ||| shuiping gou ||| jichao li ||| yinghai zhao ||| zhen liu ||| changzhe jiao ||| shasha mao ||| 
2019 ||| multi-scale attention deep neural network for fast accurate object detection. ||| kaiyou song ||| hua yang ||| zhouping yin ||| 
2020 ||| three-dimension transmissible attention network for person re-identification. ||| yewen huang ||| sicheng lian ||| suian zhang ||| haifeng hu ||| dihu chen ||| tao su ||| 
2022 ||| probabilistic spatial distribution prior based attentional keypoints matching network. ||| xiaoming zhao ||| jingmeng liu ||| xingming wu ||| weihai chen ||| fanghong guo ||| zhengguo li ||| 
2021 ||| decomposition makes better rain removal: an improved attention-guided deraining network. ||| kui jiang ||| zhongyuan wang ||| peng yi ||| chen chen ||| zhen han ||| tao lu ||| baojin huang ||| junjun jiang ||| 
2022 ||| learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion. ||| jinyuan liu ||| xin fan ||| ji jiang ||| risheng liu ||| zhongxuan luo ||| 
2020 ||| aggregating attentional dilated features for salient object detection. ||| lei zhu ||| jiaxing chen ||| xiaowei hu ||| chi-wing fu ||| xuemiao xu ||| jing qin ||| pheng-ann heng ||| 
2022 ||| lightweight image super-resolution with expectation-maximization attention mechanism. ||| xiangyuan zhu ||| kehua guo ||| sheng ren ||| bin hu ||| min hu ||| hui fang ||| 
2021 ||| co-saliency detection with co-attention fully convolutional network. ||| guangshuai gao ||| wenting zhao ||| qingjie liu ||| yunhong wang ||| 
2021 ||| learning dual semantic relations with graph attention for image-text matching. ||| keyu wen ||| xiaodong gu ||| qingrong cheng ||| 
2022 ||| feature aggregation networks based on dual attention capsules for visual object tracking. ||| yi cao ||| hongbing ji ||| wenbo zhang ||| shahram shirani ||| 
2019 ||| attention-based 3d-cnns for large-vocabulary sign language recognition. ||| jie huang ||| wengang zhou ||| houqiang li ||| weiping li ||| 
2021 ||| attention transfer network for nature image matting. ||| fenfen zhou ||| yingjie tian ||| zhiquan qi ||| 
2022 ||| syntax-guided hierarchical attention network for video captioning. ||| jincan deng ||| liang li ||| beichen zhang ||| shuhui wang ||| zhengjun zha ||| qingming huang ||| 
2020 ||| task-aware attention model for clothing attribute prediction. ||| sanyi zhang ||| zhanjie song ||| xiaochun cao ||| hua zhang ||| jie zhou ||| 
2022 ||| tsan: synthesized view quality enhancement via two-stream attention network for 3d-hevc. ||| zhaoqing pan ||| weijie yu ||| jianjun lei ||| nam ling ||| sam kwong ||| 
2020 ||| fine-grained age estimation in the wild with attention lstm networks. ||| ke zhang ||| na liu ||| xingfang yuan ||| xinyao guo ||| ce gao ||| zhenbing zhao ||| zhanyu ma ||| 
2021 ||| where to look and how to describe: fashion image retrieval with an attentional heterogeneous bilinear network. ||| haibo su ||| peng wang ||| lingqiao liu ||| hui li ||| zhen li ||| yanning zhang ||| 
2021 ||| attentional kernel encoding networks for fine-grained visual categorization. ||| yutao hu ||| yandan yang ||| jun zhang ||| xianbin cao ||| xiantong zhen ||| 
2022 ||| task-adaptive attention for image captioning. ||| chenggang yan ||| yiming hao ||| liang li ||| jian yin ||| anan liu ||| zhendong mao ||| zhenyu chen ||| xingyu gao ||| 
2021 ||| multiscale omnibearing attention networks for person re-identification. ||| yewen huang ||| sicheng lian ||| haifeng hu ||| dihu chen ||| tao su ||| 
2022 ||| wide weighted attention multi-scale network for accurate mr image super-resolution. ||| haoqian wang ||| xiaowan hu ||| xiaole zhao ||| yulun zhang ||| 
2021 ||| transformer3d-det: improving 3d object detection by vote refinement. ||| lichen zhao ||| jinyang guo ||| dong xu ||| lu sheng ||| 
2019 ||| action recognition with spatio-temporal visual attention on skeleton image sequences. ||| zhengyuan yang ||| yuncheng li ||| jianchao yang ||| jiebo luo ||| 
2017 ||| visual-attention-based background modeling for detecting infrequently moving objects. ||| yuewei lin ||| yan tong ||| yu cao ||| youjie zhou ||| song wang ||| 
2022 ||| pman: progressive multi-attention network for human pose transfer. ||| baoyu chen ||| yi zhang ||| hongchen tan ||| baocai yin ||| xiuping liu ||| 
2020 ||| attention-driven loss for anomaly detection in video surveillance. ||| joey tianyi zhou ||| le zhang ||| zhiwen fang ||| jiawei du ||| xi peng ||| yang xiao ||| 
2022 ||| ptpgc: pedestrian trajectory prediction by graph attention network with convlstm. ||| juan yang ||| xu sun ||| ronggui wang ||| lixia xue ||| 
2018 ||| visual attention and object naming in humanoid robots using a bio-inspired spiking neural network. ||| daniel hern ||| ndez garc ||| a ||| samantha v. adams ||| alexander d. rast ||| thomas wennekers ||| steve b. furber ||| angelo cangelosi ||| 
2020 ||| visual-spatial attention as a comfort measure in human-robot collaborative tasks. ||| kevin dufour ||| jorge ocampo-jim ||| nez ||| wael suleiman ||| 
2022 ||| joint disease classification and lesion segmentation via one-stage attention-based convolutional neural network in oct images. ||| xiaoming liu ||| yingjie bai ||| jun cao ||| junping yao ||| ying zhang ||| man wang ||| 
2021 ||| attention graph convolutional nets for esophageal contraction pattern recognition in high-resolution manometries. ||| zheng wang ||| lu yan ||| yuzhuo dai ||| fanggen lu ||| jie zhang ||| muzhou hou ||| xiaowei liu ||| 
2021 ||| bascnet: bilateral adaptive spatial and channel attention network for breast density classification in the mammogram. ||| wenwei zhao ||| runze wang ||| yunliang qi ||| meng lou ||| yiming wang ||| yang yang ||| xiangyu deng ||| yide ma ||| 
2021 ||| ffanet: feature fusion attention network to medical image segmentation. ||| jiankang yu ||| dedong yang ||| hanshuo zhao ||| 
2021 ||| deep learning models for cuffless blood pressure monitoring from ppg signals using attention mechanism. ||| chadi el hajj ||| panayiotis a. kyriacou ||| 
2021 ||| deep connected attention (dca) resnet for robust voice pathology detection and classification. ||| huijun ding ||| zixiong gu ||| peng dai ||| zhou zhou ||| lu wang ||| xiaoxiao wu ||| 
2022 ||| a novel sleep staging network based on multi-scale dual attention. ||| huafeng wang ||| chonggang lu ||| qi zhang ||| zhimin hu ||| xiaodong yuan ||| pingshu zhang ||| wanquan liu ||| 
2022 ||| attention gated tensor neural network architectures for speech emotion recognition. ||| sandeep kumar pandey ||| hanumant singh shekhawat ||| s. r. m. prasanna ||| 
2020 ||| identifying heart-brain interactions during internally and externally operative attention using conditional entropy. ||| mukesh kumar ||| dilbag singh ||| k. k. deepak ||| 
2021 ||| anterior chamber angle classification in anterior segment optical coherence tomography images using hybrid attention based pyramidal convolutional network. ||| quan zhou ||| junming wang ||| jingmin guo ||| zhiwen huang ||| mingyue ding ||| ming yuchi ||| xuming zhang ||| 
2021 ||| multi-scale attention-guided network for mammograms classification. ||| chunbo xu ||| meng lou ||| yunliang qi ||| yiming wang ||| jiande pi ||| yide ma ||| 
2022 ||| estimating finger joint angles on surface emg using manifold learning and long short-term memory with attention mechanism. ||| cries avian ||| setya widyawan prakosa ||| muhamad faisal ||| jenq-shiou leu ||| 
2022 ||| adjacent slices feature transformer network for single anisotropic 3d brain mri image super-resolution. ||| lulu wang ||| huazheng zhu ||| zhongshi he ||| yuanyuan jia ||| jinglong du ||| 
2021 ||| a multiscale residual pyramid attention network for medical image fusion. ||| jun fu ||| weisheng li ||| jiao du ||| yuping huang ||| 
2022 ||| brain tumor segmentation with corner attention and high-dimensional perceptual loss. ||| weijin xu ||| huihua yang ||| mingying zhang ||| zhiwei cao ||| xipeng pan ||| wentao liu ||| 
2021 ||| dense gan and multi-layer attention based lesion segmentation method for covid-19 ct images. ||| ju zhang ||| lunduan yu ||| decheng chen ||| weidong pan ||| chao shi ||| yan niu ||| xinwei yao ||| xiaobin xu ||| yun cheng ||| 
2021 ||| identification of autism spectrum disorder using multi-regional resting-state data through an attention learning approach. ||| yaya liu ||| lingyu xu ||| jie yu ||| jun li ||| xuan yu ||| 
2022 ||| a novel analytical method to measure intra-individual variability of steady-state evoked potentials; new insights into attention deficit. ||| amir norouzpour ||| stanley a. klein ||| 
2022 ||| saa-net: u-shaped network with scale-axis-attention for liver tumor segmentation. ||| chi zhang ||| jingben lu ||| qianqian hua ||| chunguo li ||| pengwei wang ||| 
2022 ||| dual attention based network for skin lesion classification with auxiliary learning. ||| zenghui wei ||| qiang li ||| hong song ||| 
2017 ||| efficient visual attention driven framework for key frames extraction from hysteroscopy videos. ||| khan muhammad ||| muhammad sajjad ||| mi young lee ||| sung wook baik ||| 
2022 ||| attention deficit/hyperactivity disorder classification based on deep spatio-temporal features of functional magnetic resonance imaging. ||| shuaiqi liu ||| ling zhao ||| jie zhao ||| bing li ||| shui-hua wang ||| 
2022 ||| automatic detection of multiple types of pneumonia: open dataset and a multi-scale attention network. ||| pak kin wong ||| tao yan ||| huaqiao wang ||| in neng chan ||| jiangtao wang ||| yang li ||| hao ren ||| chi hong wong ||| 
2022 ||| eff2net: an efficient channel attention-based convolutional neural network for skin disease classification. ||| r. karthik ||| tejas sunil vaichole ||| sanika kiran kulkarni ||| ojaswa yadav ||| faiz khan ||| 
2022 ||| broad learning system stacking with multi-scale attention for the diagnosis of gastric intestinal metaplasia. ||| pak-kin wong ||| liang yao ||| tao yan ||| i. cheong choi ||| hon ho yu ||| ying hu ||| 
2021 ||| auditory attention decoding from electroencephalography based on long short-term memory networks. ||| yun lu ||| mingjiang wang ||| longxin yao ||| hongcai shen ||| wanqing wu ||| qiquan zhang ||| lu zhang ||| moran chen ||| hao liu ||| rongchao peng ||| ming liu ||| shixiong chen ||| 
2021 ||| gcaunet: a group cross-channel attention residual unet for slice based brain tumor segmentation. ||| zheng huang ||| yiwen zhao ||| yunhui liu ||| guoli song ||| 
2019 ||| comparison of brain effective connectivity in different states of attention and consciousness based on eeg signals. ||| masoomeh rahimi ||| mohammad hassan moradi ||| farnaz ghassemi ||| 
2021 ||| a self-attention based faster r-cnn for polyp detection from colonoscopy images. ||| bo-lun chen ||| jing-jing wan ||| tai-yue chen ||| yong-tao yu ||| min ji ||| 
2018 ||| a visual attention guided unsupervised feature learning for robust vessel delineation in retinal images. ||| chetan l. srinidhi ||| p. aparna ||| jeny rajan ||| 
2022 ||| corrigendum to "identification of autism spectrum disorder using multi-regional resting-state data through an attention learning approach" [biomed. signal process. control 69 (2021) 102833]. ||| yaya liu ||| lingyu xu ||| jie yu ||| jun li ||| xuan yu ||| 
2022 ||| attention res-unet with guided decoder for semantic segmentation of brain tumors. ||| dhiraj maji ||| prarthana sigedar ||| munendra singh ||| 
2018 ||| investigation of brain networks in children with attention deficit/hyperactivity disorder using a graph theoretical approach. ||| jianqin cao ||| yang li ||| hong yu ||| xiwu zhao ||| yingli li ||| suhong wang ||| 
2021 ||| automated skin lesion segmentation using attention-based deep convolutional neural network. ||| ridhi arora ||| balasubramanian raman ||| kritagya nayyar ||| ruchi awasthi ||| 
2021 ||| an attention-based bi-lstm method for visual object classification via eeg. ||| xiao zheng ||| wanzhong chen ||| 
2019 ||| chemical-protein interaction extraction via contextualized word representations and multihead attention. ||| yijia zhang ||| hongfei lin ||| zhihao yang ||| jian wang ||| yuanyuan sun ||| 
2018 ||| hierarchical bi-directional attention-based rnns for supporting document classification on protein-protein interactions affected by genetic mutations. ||| aris fergadis ||| christos baziotis ||| dimitris pappas ||| haris papageorgiou ||| alexandros potamianos ||| 
2018 ||| extracting chemical-protein relations using attention-based neural networks. ||| sijia liu ||| feichen shen ||| ravikumar komandur elayavilli ||| yanshan wang ||| majid rastegar-mojarad ||| vipin chaudhary ||| hongfang liu ||| 
2018 ||| bio-inspired model learning visual goals and attention skills through contingencies and intrinsic motivations. ||| valerio sperati ||| gianluca baldassarre ||| 
2019 ||| brain-inspired cognitive model with attention for self-driving cars. ||| shi-tao chen ||| songyi zhang ||| jinghao shang ||| badong chen ||| nanning zheng ||| 
2020 ||| toward improving engagement in neural rehabilitation: attention enhancement based on brain-computer interface and audiovisual feedback. ||| jiaxing wang ||| weiqun wang ||| zeng-guang hou ||| 
2017 ||| flexible task execution and attentional regulations in human-robot interaction. ||| riccardo caccavale ||| alberto finzi ||| 
2017 ||| deep reinforcement learning with visual attention for vehicle classification. ||| dongbin zhao ||| yaran chen ||| le lv ||| 
2020 ||| joint attention in hearing parent-deaf child and hearing parent-hearing child dyads. ||| heather bortfeld ||| john oghalai ||| 
2021 ||| color facial expression recognition by quaternion convolutional neural network with gabor attention. ||| yu zhou ||| lianghai jin ||| hong liu ||| enmin song ||| 
2021 ||| attention-based video hashing for large-scale video retrieval. ||| yingxin wang ||| xiushan nie ||| yang shi ||| xin zhou ||| yilong yin ||| 
2021 ||| understanding the role of objects in joint attention task framework for children with autism. ||| vishav jyoti ||| sanika gupta ||| uttama lahiri ||| 
2018 ||| multibranch attention networks for action recognition in still images. ||| shiyang yan ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2022 ||| attention-net: an ensemble sketch recognition approach using vector images. ||| gaurav jain ||| shivang chopra ||| suransh chopra ||| anil singh parihar ||| 
2022 ||| trear: transformer-based rgb-d egocentric action recognition. ||| xiangyu li ||| yonghong hou ||| pichao wang ||| zhimin gao ||| mingliang xu ||| wanqing li ||| 
2018 ||| news attention in a mobile era. ||| johanna dunaway ||| kathleen searles ||| mingxiao sui ||| newly paul ||| 
2021 ||| method based on the cross-layer attention mechanism and multiscale perception for safety helmet-wearing detection. ||| guang han ||| mengcheng zhu ||| xuechen zhao ||| hua gao ||| 
2021 ||| intelligent real-time arabic sign language classification using attention-based inception and bilstm. ||| wadood abdul ||| mansour alsulaiman ||| syed umar amin ||| mohammed faisal ||| ghulam muhammad ||| fahad r. albogamy ||| mohamed abdelkader bencherif ||| hamid ghaleb ||| 
2021 ||| image captioning in hindi language using transformer networks. ||| santosh kumar mishra ||| rijul dhir ||| sriparna saha ||| pushpak bhattacharyya ||| amit kumar singh ||| 
2020 ||| self-attention guided model for defect detection of aluminium alloy casting on x-ray image. ||| yongxiong wang ||| chuanfei hu ||| kai chen ||| zhong yin ||| 
2022 ||| attention meta-transfer learning approach for few-shot iris recognition. ||| songze lei ||| baihua dong ||| aokui shan ||| yonggang li ||| wenjuan zhang ||| feng xiao ||| 
2021 ||| self-attention and adversary learning deep hashing network for cross-modal retrieval. ||| shubai chen ||| song wu ||| li wang ||| zhenyang yu ||| 
2021 ||| pooling attention-based encoder-decoder network for semantic segmentation. ||| haixia xu ||| yunjia huang ||| edwin r. hancock ||| shuailong wang ||| qijun xuan ||| wei zhou ||| 
2018 ||| highly reliable inverter topology with a novel soft computing technique to eliminate leakage current in grid-connected transformerless photovoltaic systems. ||| s. kirthiga ||| n. m. jothi swaroopan ||| 
2022 ||| predicate-attention neural model for chinese semantic role labeling. ||| heng song ||| shi wang ||| yu liu ||| ya wang ||| 
2022 ||| laednet: a lightweight attention encoder-decoder network for ultrasound medical image segmentation. ||| quan zhou ||| qianwen wang ||| yunchao bao ||| lingjun kong ||| xin jin ||| weihua ou ||| 
2022 ||| deep hierarchical lstm networks with attention for video summarization. ||| jingxu lin ||| sheng-hua zhong ||| ahmed fares ||| 
2022 ||| regional attention reinforcement learning for rapid object detection. ||| hongge yao ||| peng dong ||| siyi cheng ||| jun yu ||| 
2022 ||| attention-inception-based u-net for retinal vessel segmentation with advanced residual. ||| huadeng wang ||| guang xu ||| xipeng pan ||| zhenbing liu ||| ningning tang ||| rushi lan ||| xiaonan luo ||| 
2019 ||| deep attention network for person re-identification with multi-loss. ||| rui li ||| baopeng zhang ||| dong-joong kang ||| zhu teng ||| 
2017 ||| commentary: interactivity - agency, pace and attention. ||| alan j. dix ||| 
2019 ||| a joint approach to detect malicious url based on attention mechanism. ||| yongfang peng ||| shengwei tian ||| long yu ||| yalong lv ||| ruijin wang ||| 
2020 ||| spatial relational attention using fully convolutional networks for image caption generation. ||| teng jiang ||| liang gong ||| yupu yang ||| 
2017 ||| software-defined networking model for smart transformers with iso/iec/ieee 21451 sensors. ||| longhua guo ||| jun wu ||| gaolei li ||| jianhua li ||| jie wu ||| 
2022 ||| improving communication protocols in smart cities with transformers. ||| edgar romo-montiel ||| ricardo menchaca-mendez ||| mario eduardo rivero-angeles ||| rolando menchaca-m ||| ndez ||| 
2021 ||| intelligent short-term voltage stability assessment via spatial attention rectified rnn learning. ||| lipeng zhu ||| david j. hill ||| chao lu ||| 
2021 ||| remaining useful life prediction using a novel feature-attention-based end-to-end approach. ||| hui liu ||| zhenyu liu ||| weiqiang jia ||| xianke lin ||| 
2019 ||| distribution transformer's loss of life considering residential prosumers owning solar shingles, high-power fast chargers and second-generation battery energy storage. ||| shady a. el batawy ||| walid g. morsi ||| 
2022 ||| attention network for rail surface defect detection via consistency of intersection-over-union(iou)-guided center-point estimation. ||| xuefeng ni ||| ziji ma ||| jianwei liu ||| bo shi ||| hongli liu ||| 
2021 ||| localization of partial discharge in electrical transformer considering multimedia refraction and diffraction. ||| jun jia ||| chengbo hu ||| qiang yang ||| yuncai lu ||| bo wang ||| hengyang zhao ||| 
2017 ||| hierarchical system design and control of an mmc-based power-electronic transformer. ||| boran fan ||| yongdong li ||| kui wang ||| zedong zheng ||| lie xu ||| 
2017 ||| efficient control of active transformers for increasing the pv hosting capacity of lv grids. ||| seyedmostafa hashemi ||| jacob  ||| stergaard ||| thomas degner ||| ron brandl ||| wolfram heckmann ||| 
2020 ||| understanding and learning discriminant features based on multiattention 1dcnn for wheelset bearing fault diagnosis. ||| huan wang ||| zhiliang liu ||| dandan peng ||| yong qin ||| 
2021 ||| gated dual attention unit neural networks for remaining useful life prediction of rolling bearings. ||| yi qin ||| dingliang chen ||| sheng xiang ||| caichao zhu ||| 
2020 ||| ar-net: adaptive attention and residual refinement network for copy-move forgery detection. ||| ye zhu ||| chaofan chen ||| gang yan ||| yingchun guo ||| yongfeng dong ||| 
2020 ||| visual attention assessment for expert-in-the-loop training in a maritime operation simulator. ||| guoyuan li ||| runze mao ||| hans petter hildre ||| houxiang zhang ||| 
2020 ||| interactive grid interfacing system by matrix-converter-based solid state transformer with model predictive control. ||| yupeng liu ||| yushan liu ||| baoming ge ||| haitham abu-rub ||| 
2021 ||| three-attention mechanisms for one-stage 3-d object detection based on lidar and camera. ||| li-hua wen ||| kang-hyun jo ||| 
2021 ||| a new diagnostic technique for reliable decision-making on transformer fra data in interturn short-circuit condition. ||| yerbol akhmetov ||| venera nurmanova ||| mehdi bagheri ||| amin zollanvari ||| gevork b. gharehpetian ||| 
2017 ||| self-repairable smart grids via online coordination of smart transformers. ||| evangelos pournaras ||| jose espejo-uribe ||| 
2020 ||| integration of accelerated deep neural network into power transformer differential protection. ||| shahabodin afrasiabi ||| mousa afrasiabi ||| benyamin parang ||| mohammad mohammadi ||| 
2022 ||| parallel operation of transformer-based improved z-source inverter with high boost and interleaved control. ||| zeeshan aleem ||| simon lucas winberg ||| hafiz furqan ahmed ||| jung-wook park ||| 
2019 ||| self-triggered reduced-attention output feedback control for linear networked control systems. ||| erick j. rodr ||| guez-seda ||| 
2020 ||| a surface defect detection framework for glass bottle bottom using visual attention model and wavelet transform. ||| xianen zhou ||| yaonan wang ||| qing zhu ||| jianxu mao ||| changyan xiao ||| xiao lu ||| hui zhang ||| 
2021 ||| wide-attention and deep-composite model for traffic flow prediction in transportation cyber-physical systems. ||| junhao zhou ||| hong-ning dai ||| hao wang ||| tian wang ||| 
2022 ||| multilevel attention based u-shape graph neural network for point clouds learning. ||| xiaofeng zou ||| kenli li ||| cen chen ||| 
2022 ||| graph cardinality preserved attention network for fault diagnosis of induction motor under varying speed and load condition. ||| yao tang ||| xiaofei zhang ||| guojun qin ||| zhuo long ||| shoudao huang ||| dianyi song ||| haidong shao ||| 
2021 ||| parallel deep learning algorithms with hybrid attention mechanism for image segmentation of lung tumors. ||| hexuan hu ||| qingqiu li ||| yunfeng zhao ||| ye zhang ||| 
2020 ||| fault-attention generative probabilistic adversarial autoencoder for machine anomaly detection. ||| jingyao wu ||| zhibin zhao ||| chuang sun ||| ruqiang yan ||| xuefeng chen ||| 
2021 ||| locating inter-turn faults in transformer windings using isometric feature mapping of frequency response traces. ||| arash moradzadeh ||| kazem pourhossein ||| behnam mohammadi-ivatloo ||| fazel mohammadi ||| 
2017 ||| autonomous energy management strategy for solid-state transformer to integrate pv-assisted ev charging station participating in ancillary service. ||| qifang chen ||| nian liu ||| cungang hu ||| lingfeng wang ||| jianhua zhang ||| 
2021 ||| attention-aware encoder-decoder neural networks for heterogeneous graphs of things. ||| yangfan li ||| cen chen ||| mingxing duan ||| zeng zeng ||| kenli li ||| 
2019 ||| fpga implementation of passivity-based control and output load algebraic estimation for transformerless multilevel active rectifier. ||| jose antonio juarez-abad ||| arturo pablo sandoval-garcia ||| jes ||| s linares-flores ||| jos ||| -fermi guerrero-castellanos ||| pedro ba ||| uelos s ||| nchez ||| marco antonio contreras-ordaz ||| 
2017 ||| novel calculation method of indices to improve classification of transformer winding fault type, location, and extent. ||| hadi tarimoradi ||| gevork b. gharehpetian ||| 
2021 ||| active object discovery and localization using sound-induced attention. ||| huaping liu ||| feng wang ||| di guo ||| xinzhu liu ||| xinyu zhang ||| fuchun sun ||| 
2021 ||| lightweight attention convolutional neural network for retinal vessel image segmentation. ||| xiang li ||| yuchen jiang ||| minglei li ||| shen yin ||| 
2022 ||| novel transformer based on gated convolutional neural network for dynamic soft sensor modeling of industrial processes. ||| zhiqiang geng ||| zhiwei chen ||| qingchao meng ||| yongming han ||| 
2021 ||| clothing sale forecasting by a composite gru-prophet model with an attention mechanism. ||| yuanjiang li ||| yi yang ||| kai zhu ||| jinglin zhang ||| 
2017 ||| application of a recursive phasor estimation method for adaptive fault component based differential protection of power transformers. ||| abdolaziz ashrafian ||| mojtaba mirsalim ||| mohammad a. s. masoum ||| 
2021 ||| attentional residual network for necking predictions in hot strip mills. ||| hongseok choi ||| youngmin kim ||| hyunju lee ||| 
2020 ||| a temporally irreversible visual attention model inspired by motion sensitive neurons. ||| jiawei xu ||| seop hyeong park ||| xiaoqin zhang ||| 
2022 ||| effective meta-attention dehazing networks for vision-based outdoor industrial systems. ||| tongyao jia ||| jiafeng li ||| li zhuo ||| guoqiang li ||| 
2022 ||| path enhanced bidirectional graph attention network for quality prediction in multistage manufacturing process. ||| donghao zhang ||| zhenyu liu ||| weiqiang jia ||| hui liu ||| jianrong tan ||| 
2017 ||| application of dip to detect power transformers axial displacement and disk space variation using fra polar plot signature. ||| omar m. aljohani ||| ahmed abu-siada ||| 
2021 ||| nondestructive defect detection in castings by using spatial attention bilinear convolutional neural network. ||| zhenhui tang ||| engang tian ||| yongxiong wang ||| licheng wang ||| taicheng yang ||| 
2020 ||| feasibility study on simultaneous detection of partial discharge and axial displacement of hv transformer winding using electromagnetic waves. ||| hossein karami ||| hamidreza tabarsa ||| gevork b. gharehpetian ||| yaser norouzi ||| maryam a. hejazi ||| 
2018 ||| classification and discrimination among winding mechanical defects, internal and external electrical faults, and inrush current of transformer. ||| sajad bagheri ||| zahra moravej ||| gevork b. gharehpetian ||| 
2021 ||| deephealth: a self-attention based method for instant intelligent predictive maintenance in industrial internet of things. ||| weiting zhang ||| dong yang ||| youzhi xu ||| xuefeng huang ||| jun zhang ||| mikael gidlund ||| 
2021 ||| deep learning-based solar-cell manufacturing defect detection with complementary attention network. ||| binyi su ||| haiyong chen ||| peng chen ||| gui-bin bian ||| kun liu ||| weipeng liu ||| 
2021 ||| softwarized attention-based context-aware group recommendation technology in event-based industrial cyber-physical systems. ||| guoqiong liao ||| xiaomei huang ||| naixue xiong ||| changxuan wan ||| mingsong mao ||| 
2020 ||| pga-net: pyramid feature fusion and global context attention network for automated surface defect detection. ||| hongwen dong ||| kechen song ||| yu he ||| jing xu ||| yunhui yan ||| qinggang meng ||| 
2020 ||| analysis and experimental validation of current-fed switched capacitor-based modular dc transformer. ||| qianhao sun ||| yalou li ||| xiaolin shen ||| fan cheng ||| gen li ||| jun liang ||| qing mu ||| jingwei meng ||| 
2022 ||| integrating multihub driven attention mechanism and big data analytics for virtual representation of visual scenes. ||| yang yao ||| bo gu ||| mamoun alazab ||| neeraj kumar ||| yu han ||| 
2021 ||| moisture diagnosis of transformer oil-immersed insulation with intelligent technique and frequency-domain spectroscopy. ||| jiefeng liu ||| xianhao fan ||| chaohai zhang ||| chun sing lai ||| yiyi zhang ||| hanbo zheng ||| loi lei lai ||| enze zhang ||| 
2021 ||| covtanet: a hybrid tri-level attention-based network for lesion segmentation, diagnosis, and severity prediction of covid-19 chest ct scans. ||| tanvir mahmud ||| md. jahin alam ||| sakib chowdhury ||| shams nafisa ali ||| md maisoon rahman ||| shaikh anowarul fattah ||| mohammad saquib ||| 
2022 ||| special issue: operation and control of modular dc transformer applied to energy internet. ||| zhipeng lv ||| ming wu ||| zhenhao song ||| shan zhou ||| 
2020 ||| a skill-based approach to modeling the attentional blink. ||| corn |||  hoekstra ||| sander martens ||| niels a. taatgen ||| 
2019 ||| visual search without selective attention: a cognitive architecture account. ||| david e. kieras ||| 
2020 ||| sarcasm detection in mash-up language using soft-attention based bi-directional lstm and feature-rich cnn. ||| deepak kumar jain ||| akshi kumar ||| geetanjali garg ||| 
2021 ||| semantic boundary enhancement and position attention network with long-range dependency for semantic segmentation. ||| xi chen ||| zhen han ||| xiaoping liu ||| zhiqiang li ||| tao fang ||| hong huo ||| qingli li ||| min zhu ||| min liu ||| haolei yuan ||| 
2022 ||| multi-scale sparse network with cross-attention mechanism for image-based butterflies fine-grained classification. ||| maopeng li ||| guoxiong zhou ||| weiwei cai ||| jiayong li ||| mingxuan li ||| mingfang he ||| yahui hu ||| liujun li ||| 
2021 ||| sentiment classification using attention mechanism and bidirectional long short-term memory network. ||| peng wu ||| xiaotong li ||| chen ling ||| shengchun ding ||| si shen ||| 
2021 ||| danhar: dual attention network for multimodal human activity recognition using wearable sensors. ||| wenbin gao ||| lei zhang ||| qi teng ||| jun he ||| hao wu ||| 
2019 ||| improved power transformer condition monitoring under uncertainty through soft computing and probabilistic health index. ||| jose ignacio aizpurua ||| brian g. stewart ||| stephen d. j. mcarthur ||| brandon lambert ||| james g. cross ||| victoria m. catterson ||| 
2021 ||| cross-sean: a cross-stitch semi-supervised neural attention model for covid-19 fake news detection. ||| william scott paka ||| rachit bansal ||| abhay kaushik ||| shubhashis sengupta ||| tanmoy chakraborty ||| 
2021 ||| a novel rule-based evolving fuzzy system applied to the thermal modeling of power transformers. ||| kaike sa teles rocha alves ||| eduardo pestana de aguiar ||| 
2021 ||| predictive intelligence powered attentional stacking matrix factorization algorithm for the computational drug repositioning. ||| shaohong yan ||| aimin yang ||| shanshan kong ||| bin bai ||| xiaoyu li ||| 
2021 ||| channel pruning guided by spatial and channel attention for dnns in intelligent edge computing. ||| mengran liu ||| weiwei fang ||| xiaodong ma ||| wenyuan xu ||| naixue xiong ||| yi ding ||| 
2020 ||| designing a composite deep learning based differential protection scheme of power transformers. ||| shahabodin afrasiabi ||| mousa afrasiabi ||| benyamin parang ||| mohammad mohammadi ||| 
2021 ||| application of particle swarm optimization for optimal setting of phase shifting transformers to minimize unscheduled active power flows. ||| roman korab ||| marcin polomski ||| robert owczarek ||| 
2020 ||| bg-sac: entity relationship classification model based on self-attention supported capsule networks. ||| dunlu peng ||| dongdong zhang ||| cong liu ||| jing lu ||| 
2021 ||| attention induced multi-head convolutional neural network for human activity recognition. ||| zanobya n. khan ||| jamil ahmad ||| 
2022 ||| an attention based dual learning approach for video captioning. ||| wanting ji ||| ruili wang ||| yan tian ||| xun wang ||| 
2021 ||| video salient object detection using dual-stream spatiotemporal attention. ||| chenchu xu ||| zhifan gao ||| heye zhang ||| shuo li ||| victor hugo c. de albuquerque ||| 
2021 ||| hybrid attention-based long short-term memory network for sarcasm identification. ||| rajnish pandey ||| abhinav kumar ||| jyoti prakash singh ||| sudhakar tripathi ||| 
2021 ||| correlational graph attention-based long short-term memory network for multivariate time series prediction. ||| shuang han ||| hongbin dong ||| xuyang teng ||| xiaohui li ||| xiaowei wang ||| 
2020 ||| human action recognition using two-stream attention based lstm networks. ||| cheng dai ||| xingang liu ||| jinfeng lai ||| 
2020 ||| stockwell transform of time-series of fmri data for diagnoses of attention deficit hyperactive disorder. ||| shadi sartipi ||| hashem kalbkhani ||| peyman ghasemzadeh ||| mahrokh g. shayesteh ||| 
2020 ||| a spatio-temporal attention-based spot-forecasting framework for urban traffic prediction. ||| rodrigo de medrano ||| jos |||  luis aznarte ||| 
2020 ||| visual fixation prediction with incomplete attention map based on brain storm optimization. ||| jian yang ||| yang shen ||| yuhui shi ||| 
2022 ||| multi-scale attention recalibration network for crowd counting. ||| jinyang xie ||| chen pang ||| yanjun zheng ||| liang li ||| chen lyu ||| lei lyu ||| hong liu ||| 
2019 ||| global optimization algorithms applied to solve a multi-variable inverse artificial neural network to improve the performance of an absorption heat transformer with energy recycling. ||| jes ||| s emmanuel sol ||| s p ||| rez ||| jos |||  francisco g ||| mez-aguilar ||| j. a. hern ||| ndez ||| ricardo fabricio escobar-jim ||| nez ||| e. viera-martin ||| r. a. conde-guti ||| rrez ||| u. cruz-jacobo ||| 
2021 ||| acomnn: attention enhanced compound neural network for financial time-series forecasting with cross-regional features. ||| zhen yang ||| jacky keung ||| md. alamgir kabir ||| xiao yu ||| yutian tang ||| miao zhang ||| shuo feng ||| 
2022 ||| tcran: multivariate time series classification using residual channel attention networks with time correction. ||| hegui zhu ||| jiapeng zhang ||| hao cui ||| kai wang ||| qingsong tang ||| 
2021 ||| att-net: enhanced emotion recognition system using lightweight self-attention module. ||| mustaqeem ||| soonil kwon ||| 
2021 ||| adversarial transfer network with bilinear attention for the detection of adverse drug reactions from social media. ||| tongxuan zhang ||| hongfei lin ||| yuqi ren ||| zhihao yang ||| jian wang ||| shaowu zhang ||| bo xu ||| xiaodong duan ||| 
2020 ||| a novel deep learning method based on attention mechanism for bearing remaining useful life prediction. ||| yuanhang chen ||| gaoliang peng ||| zhiyu zhu ||| sijue li ||| 
2021 ||| biomedical cross-sentence relation extraction via multihead attention and graph convolutional networks. ||| di zhao ||| jian wang ||| hongfei lin ||| xin wang ||| zhihao yang ||| yijia zhang ||| 
2021 ||| attention augmented convolutional neural network for acoustics based machine state estimation. ||| jiannan tan ||| john oyekan ||| 
2020 ||| multi-scale channel importance sorting and spatial attention mechanism for retinal vessels segmentation. ||| xianlun tang ||| bing zhong ||| jiangping peng ||| bohui hao ||| jie li ||| 
2020 ||| attention distribution guided information transfer networks for recommendation in practice. ||| gang sun ||| yu li ||| hongfang yu ||| victor chang ||| 
2020 ||| efficient point-of-interest recommendation with hierarchical attention mechanism. ||| guangyao pang ||| xiaoming wang ||| fei hao ||| liang wang ||| xinyan wang ||| 
2022 ||| computational intelligence for preventive maintenance of power transformers. ||| shen yuong wong ||| xiaofeng ye ||| fengkai guo ||| hui hwang goh ||| 
2018 ||| multi-objective ensemble forecasting with an application to power transformers. ||| abdolrahman peimankar ||| stephen john weddell ||| thahirah jalal ||| andrew craig lapthorn ||| 
2020 ||| music auto-tagging using scattering transform and convolutional neural network with self-attention. ||| guangxiao song ||| zhijie wang ||| fang han ||| shenyi ding ||| xiaochun gu ||| 
2020 ||| attention embedded residual cnn for disease detection in tomato leaves. ||| karthik r ||| m. hariharan ||| sundar anand ||| priyanka mathikshara ||| annie johnson ||| menaka r ||| 
2021 ||| transformer-based identification of stochastic information cascades in social networks using text and image similarity. ||| panagiotis kasnesis ||| ryan heartfield ||| xing liang ||| lazaros toumanidis ||| georgia sakellari ||| charalampos z. patrikakis ||| george loukas ||| 
2021 ||| deep multi-scale attentional features for medical image segmentation. ||| sahadev poudel ||| sang-woong lee ||| 
2019 ||| visual question answering via attention-based syntactic structure tree-lstm. ||| yun liu ||| xiaoming zhang ||| feiran huang ||| xianghong tang ||| zhoujun li ||| 
2020 ||| denseattentionseg: segment hands from interacted objects using depth input. ||| zihao bo ||| hao zhang ||| jun-hai yong ||| hao gao ||| feng xu ||| 
2020 ||| aglnet: towards real-time semantic segmentation of self-driving images via attention-guided lightweight network. ||| quan zhou ||| yu wang ||| yawen fan ||| xiaofu wu ||| suofei zhang ||| bin kang ||| longin jan latecki ||| 
2021 ||| attention-based c-bilstm for fake news detection. ||| tina esther trueman ||| ashok kumar j. ||| narayanasamy p. ||| vidya j. ||| 
2018 ||| optimal multivariable conditions in the operation of an absorption heat transformer with energy recycling solved by the genetic algorithm in artificial neural network inverse. ||| r. a. conde-guti ||| rrez ||| u. cruz-jacobo ||| armando huicochea ||| s. r. casolco ||| j. a. hern ||| ndez ||| 
2021 ||| attention-based dynamic user preference modeling and nonlinear feature interaction learning for collaborative filtering recommendation. ||| ruiqin wang ||| yunliang jiang ||| jungang lou ||| 
2020 ||| a fast self-attention cascaded network for object detection in large scene remote sensing images. ||| xia hua ||| xinqing wang ||| ting rui ||| haitao zhang ||| dong wang ||| 
2022 ||| gabor log-euclidean gaussian and its fusion with deep network based on self-attention for face recognition. ||| chaorong li ||| wei huang ||| yuanyuan huang ||| 
2022 ||| interpretable cognitive learning with spatial attention for high-volatility time series prediction. ||| fengqian ding ||| chao luo ||| 
2020 ||| ed-acnn: novel attention convolutional neural network based on encoder-decoder framework for human traffic prediction. ||| bin pu ||| yuan liu ||| ningbo zhu ||| kenli li ||| keqin li ||| 
2020 ||| interpreting network knowledge with attention mechanism for bearing fault diagnosis. ||| zhibo yang ||| jun-peng zhang ||| zhibin zhao ||| zhi zhai ||| xuefeng chen ||| 
2020 ||| dual path attention net for remote sensing semantic image segmentation. ||| jinglun li ||| jiapeng xiu ||| zhengqiu yang ||| chen liu ||| 
2021 ||| a visual attention model based on eye tracking in 3d scene maps. ||| bincheng yang ||| hongwei li ||| 
2019 ||| incorporating graph attention and recurrent architectures for city-wide taxi demand prediction. ||| ying xu ||| dongsheng li ||| 
2021 ||| cascaded attention denseunet (cadunet) for road extraction from very-high-resolution images. ||| jing li ||| yong liu ||| yindan zhang ||| yang zhang ||| 
2021 ||| detection of schools in remote sensing images based on attention-guided dense network. ||| han fu ||| xiangtao fan ||| zhenzhen yan ||| xiaoping du ||| 
2021 ||| a dynamic and static context-aware attention network for trajectory prediction. ||| jian yu ||| meng zhou ||| xin wang ||| guoliang pu ||| chengqi cheng ||| bo chen ||| 
2022 ||| cascaded residual attention enhanced road extraction from remote sensing images. ||| shengfu li ||| cheng liao ||| yulin ding ||| han hu ||| yang jia ||| min chen ||| bo xu ||| xuming ge ||| tianyang liu ||| di wu ||| 
2021 ||| a3t-gcn: attention temporal graph convolutional network for traffic forecasting. ||| jiandong bai ||| jiawei zhu ||| yujiao song ||| ling zhao ||| zhixiang hou ||| ronghua du ||| haifeng li ||| 
2022 ||| end-to-end pedestrian trajectory forecasting with transformer network. ||| hai-yan yao ||| wang-gen wan ||| xiang li ||| 
2018 ||| reduction of map information regulates visual attention without affecting route recognition performance. ||| julian keil ||| franz-benjamin mocnik ||| dennis edler ||| frank dickmann ||| lars kuchinke ||| 
2021 ||| residual multi-attention classification network for a forest dominated tropical landscape using high-resolution remote sensing imagery. ||| tong yu ||| wenjin wu ||| chen gong ||| xinwu li ||| 
2021 ||| high-resolution remote sensing image segmentation framework based on attention mechanism and adaptive weighting. ||| yifan liu ||| qigang zhu ||| feng cao ||| junke chen ||| gang lu ||| 
2019 ||| an attention-based spatiotemporal gated recurrent unit network for point-of-interest recommendation. ||| chunyang liu ||| jiping liu ||| jian wang ||| shenghua xu ||| houzeng han ||| yang chen ||| 
2020 ||| dem void filling based on context attention generation model. ||| chunsen zhang ||| shu shi ||| yingwei ge ||| hengheng liu ||| weihong cui ||| 
2020 ||| neur and smartphone zombie: smartphone users' altering visual attention and walking behavior in public space. ||| gorsev argin ||| burak pak ||| handan turkoglu ||| 
2021 ||| neurogrid simulates cortical cell-types, active dendrites, and top-down attention. ||| ben varkey benjamin ||| nicholas a. steinmetz ||| nick n. oza ||| jose j. aguayo ||| kwabena boahen ||| 
2021 ||| a scalable off-the-shelf framework for measuring patterns of attention in young children and its application in autism spectrum disorder. ||| matthieu bovery ||| geraldine dawson ||| jordan hashemi ||| guillermo sapiro ||| 
2021 ||| improving attention model based on cognition grounded data for sentiment analysis. ||| yunfei long ||| rong xiang ||| qin lu ||| chu-ren huang ||| minglei li ||| 
2021 ||| a computational model of focused attention meditation and its transfer to a sustained attention task. ||| amir j. moye ||| marieke k. van vugt ||| 
2022 ||| attention-based skill translation models for expert finding. ||| zohreh fallahnejad ||| hamid beigy ||| 
2022 ||| multi-view graph attention network for travel recommendation. ||| lei chen ||| jie cao ||| youquan wang ||| weichao liang ||| guixiang zhu ||| 
2021 ||| data augmentation for skin lesion using self-attention based progressive generative adversarial network. ||| ibrahim saad aly abdelhalim ||| mamdouh farouk mohamed ||| youssef bassyouni mahdy ||| 
2021 ||| representation learning using attention network and cnn for heterogeneous networks. ||| ning tong ||| ying tang ||| bo chen ||| lirong xiong ||| 
2020 ||| multiple premises entailment recognition based on attention and gate mechanism. ||| pin wu ||| zhidan lei ||| quan zhou ||| rukang zhu ||| xuting chang ||| junwu sun ||| wenjie zhang ||| yike guo ||| 
2021 ||| dynamic network embedding via structural attention. ||| chen zhang ||| yiming fan ||| yu xie ||| bin yu ||| chunyi li ||| ke pan ||| 
2021 ||| image super-resolution based on adaptive cascading attention network. ||| dengwen zhou ||| yiming chen ||| wenbin li ||| jinxin li ||| 
2021 ||| learning attention embeddings based on memory networks for neural collaborative recommendation. ||| yihao zhang ||| xiaoyang liu ||| 
2021 ||| an attention enhanced sentence feature network for subtitle extraction and summarization. ||| chalothon chootong ||| timothy k. shih ||| ankhtuya ochirbat ||| worapot sommool ||| yung-yu zhuang ||| 
2021 ||| an attention-based cnn-bilstm hybrid neural network enhanced with features of discrete wavelet transformation for fetal acidosis classification. ||| mujun liu ||| yaosheng lu ||| shun long ||| jieyun bai ||| wanmin lian ||| 
2018 ||| curriculum learning based approach for noise robust language identification using dnn with attention. ||| ravi kumar vuddagiri ||| hari krishna vydana ||| anil kumar vuppala ||| 
2020 ||| ilwaanet: an interactive lexicon-aware word-aspect attention network for aspect-level sentiment classification on social networking. ||| huy-thanh nguyen ||| le-minh nguyen ||| 
2020 ||| a two-step hybrid unsupervised model with attention mechanism for aspect extraction. ||| ganpat singh chauhan ||| yogesh kumar meena ||| dinesh gopalani ||| ravi nahta ||| 
2022 ||| distinguishing between fake news and satire with transformers. ||| jwen fai low ||| benjamin c. m. fung ||| farkhund iqbal ||| shih-chia huang ||| 
2019 ||| detecting user attention to video segments using interval eeg features. ||| jinyoung moon ||| yongjin kwon ||| jongyoul park ||| wan chul yoon ||| 
2020 ||| rumor detection based on propagation graph neural network with attention mechanism. ||| zhiyuan wu ||| dechang pi ||| junfu chen ||| meng xie ||| jianjun cao ||| 
2021 ||| speech emotion recognition using recurrent neural networks with directional self-attention. ||| dongdong li ||| jinlin liu ||| zhuo yang ||| linyu sun ||| zhe wang ||| 
2019 ||| word n-gram attention models for sentence similarity and inference. ||| i ||| igo lopez-gazpio ||| montse maritxalar ||| mirella lapata ||| eneko agirre ||| 
2020 ||| friend recommendation for cross marketing in online brand community based on intelligent attention allocation link prediction algorithm. ||| shugang li ||| xuewei song ||| hanyu lu ||| linyi zeng ||| miaojing shi ||| fang liu ||| 
2021 ||| improved few-shot learning method for transformer fault diagnosis based on approximation space and belief functions. ||| yaoyu xu ||| yuan li ||| yijing wang ||| dexing zhong ||| guanjun zhang ||| 
2021 ||| attention-aware metapath-based network embedding for hin based recommendation. ||| surong yan ||| haosen wang ||| yixiao li ||| yuan zheng ||| long han ||| 
2022 ||| noun-based attention mechanism for fine-grained named entity recognition. ||| alejandro jes ||| s casta ||| eira rodr ||| guez ||| daniel castro castro ||| silena herold garc ||| a ||| 
2020 ||| dstp-rnn: a dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction. ||| yeqi liu ||| chuanyang gong ||| ling yang ||| yingyi chen ||| 
2021 ||| dsanet: dilated spatial attention for real-time semantic segmentation in urban street scenes. ||| mohammed a. m. elhassan ||| chenxi huang ||| chenhui yang ||| tewodros legesse munea ||| 
2022 ||| la-hcn: label-based attention for hierarchical multi-label text classification neural network. ||| xinyi zhang ||| jiahao xu ||| charlie soh ||| lihui chen ||| 
2021 ||| driver stress detection via multimodal fusion using attention-based cnn-lstm. ||| luntian mou ||| chao zhou ||| pengfei zhao ||| bahareh nakisa ||| mohammad naim rastgoo ||| ramesh c. jain ||| wen gao ||| 
2021 ||| identification of rice plant diseases using lightweight attention networks. ||| junde chen ||| defu zhang ||| adnan zeb ||| yaser ahangari nanehkaran ||| 
2021 ||| mffnet: multi-dimensional feature fusion network based on attention mechanism for semg analysis to detect muscle fatigue. ||| yongqing zhang ||| siyu chen ||| wenpeng cao ||| peng guo ||| dongrui gao ||| manqing wang ||| jiliu zhou ||| ting wang ||| 
2021 ||| candidate point selection using a self-attention mechanism for generating a smooth volatility surface under the sabr model. ||| hyeonuk kim ||| kyunghyun park ||| junkee jeon ||| changhoon song ||| jungwoo bae ||| yongsik kim ||| myungjoo kang ||| 
2021 ||| amfb: attention based multimodal factorized bilinear pooling for multimodal fake news detection. ||| rina kumari ||| asif ekbal ||| 
2020 ||| path-based reasoning approach for knowledge graph completion using cnn-bilstm with attention mechanism. ||| batselem jagvaral ||| wan-kon lee ||| jae-seung roh ||| min-sung kim ||| young-tack park ||| 
2022 ||| relation-aware heterogeneous graph transformer based drug repurposing. ||| xin mei ||| xiaoyan cai ||| libin yang ||| nanxin wang ||| 
2021 ||| rice diseases detection and classification using attention based neural network and bayesian optimization. ||| yibin wang ||| haifeng wang ||| zhaohua peng ||| 
2020 ||| learning competitive channel-wise attention in residual network with masked regularization and signal boosting. ||| mingnan luo ||| guihua wen ||| yang hu ||| dan dai ||| jiajiong ma ||| 
2021 ||| detection of tuberculosis from chest x-ray images: boosting the performance with vision transformer and transfer learning. ||| linh t. duong ||| nhi h. le ||| toan b. tran ||| vuong m. ngo ||| phuong t. nguyen ||| 
2021 ||| deep multi-scale separable convolutional network with triple attention mechanism: a novel multi-task domain adaptation method for intelligent fault diagnosis. ||| bo zhao ||| xianmin zhang ||| zhenhui zhan ||| qiqiang wu ||| 
2019 ||| distinguishing mental attention states of humans via an eeg-based passive bci using machine learning methods. ||| igdem inan aci ||| murat kaya ||| yuriy mishchenko ||| 
2022 ||| attention-based dynamic user modeling and deep collaborative filtering recommendation. ||| ruiqin wang ||| zongda wu ||| jungang lou ||| yunliang jiang ||| 
2021 ||| attentional matrix factorization with context and co-invocation for service recommendation. ||| mo nguyen ||| jian yu ||| tung nguyen ||| yanbo han ||| 
2020 ||| attention-based deep neural network for internet platform group users' dynamic identification and recommendation. ||| xuna wang ||| qingmei tan ||| mark goh ||| 
2020 ||| agcn: attention-based graph convolutional networks for drug-drug interaction extraction. ||| chanhee park ||| jinuk park ||| sanghyun park ||| 
2022 ||| attention based cnn model for fire detection and localization in real-world images. ||| saima majid ||| fayadh alenezi ||| sarfaraz masood ||| musheer ahmad ||| emine selda g ||| nd ||| z ||| kemal polat ||| 
2022 ||| deep multi-graph neural networks with attention fusion for recommendation. ||| yuzhi song ||| hailiang ye ||| ming li ||| feilong cao ||| 
2021 ||| an end-to-end framework combining time-frequency expert knowledge and modified transformer networks for vibration signal classification. ||| can-can jin ||| xi chen ||| 
2021 ||| an attention-driven convolutional neural network-based multi-level spectral-spatial feature learning for hyperspectral image classification. ||| chunyu pu ||| hong huang ||| liping yang ||| 
2021 ||| deep learning with multiple scale attention and direction regularization for asset price prediction. ||| fucui xu ||| shan tan ||| 
2021 ||| dual-path attention network for single image super-resolution. ||| zhiyong huang ||| wenbin li ||| jinxin li ||| dengwen zhou ||| 
2021 ||| fast prediction of complicated temperature field using conditional multi-attention generative adversarial networks (cmagan). ||| jincheng chen ||| feiding zhu ||| yuge han ||| chen chen ||| 
2022 ||| weakly supervised attention-based models using activation maps for citrus mite and insect pest classification. ||| edson r. bollis ||| helena almeida maia ||| h ||| lio pedrini ||| sandra avila ||| 
2022 ||| identification method of vegetable diseases based on transfer learning and attention mechanism. ||| xue zhao ||| kaiyu li ||| yunxia li ||| juncheng ma ||| lingxian zhang ||| 
2021 ||| reflectance images of effective wavelengths from hyperspectral imaging for identification of fusarium head blight-infected wheat kernels combined with a residual attention convolution neural network. ||| shizhuang weng ||| kaixuan han ||| zhaojie chu ||| gongqin zhu ||| cunchuan liu ||| zede zhu ||| zixi zhang ||| ling zheng ||| linsheng huang ||| 
2022 ||| ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism. ||| yun zhao ||| cheng sun ||| xing xu ||| jiagui chen ||| 
2021 ||| dual attention-guided feature pyramid network for instance segmentation of group pigs. ||| zhiwei hu ||| hua yang ||| tiantian lou ||| 
2022 ||| an improved yolov5 model based on visual attention mechanism: application to recognition of tomato virus disease. ||| jiangtao qi ||| xiangnan liu ||| kai liu ||| farong xu ||| hui guo ||| xinliang tian ||| mao li ||| zhiyuan bao ||| yang li ||| 
2021 ||| a dual attention network based on efficientnet-b2 for short-term fish school feeding behavior analysis in aquaculture. ||| ling yang ||| huihui yu ||| yuelan cheng ||| siyuan mei ||| yanqing duan ||| daoliang li ||| yingyi chen ||| 
2020 ||| crop leaf disease recognition based on self-attention convolutional neural network. ||| weihui zeng ||| miao li ||| 
2021 ||| a new attention-based cnn approach for crop mapping using time series sentinel-2 images. ||| yumiao wang ||| zhou zhang ||| luwei feng ||| yuchi ma ||| qingyun du ||| 
2019 ||| attention-based recurrent neural networks for accurate short-term and long-term dissolved oxygen prediction. ||| yeqi liu ||| qian zhang ||| lihua song ||| yingyi chen ||| 
2022 ||| canopy-attention-yolov4-based immature/mature apple fruit detection on dense-foliage tree architectures for early crop load estimation. ||| shenglian lu ||| wenkang chen ||| xin zhang ||| manoj karkee ||| 
2021 ||| a dual-head attention model for time series data imputation. ||| yi-fan zhang ||| peter j. thorburn ||| 
2021 ||| semantic segmentation model of cotton roots in-situ image based on attention mechanism. ||| jia kang ||| liantao liu ||| fucheng zhang ||| chen shen ||| nan wang ||| limin shao ||| 
2020 ||| chinese agricultural diseases and pests named entity recognition with multi-scale local context features and self-attention mechanism. ||| xuchao guo ||| han zhou ||| jie su ||| xia hao ||| zhan tang ||| lei diao ||| lin li ||| 
2021 ||| feature detection method for hind leg segmentation of sheep carcass based on multi-scale dual attention u-net. ||| bin xie ||| weipeng jiao ||| changkai wen ||| songtao hou ||| fan zhang ||| kaidong liu ||| junlin li ||| 
2021 ||| visual classification of apple bud-types via attention-guided data enrichment network. ||| xue xia ||| xiujuan chai ||| ning zhang ||| tan sun ||| 
2021 ||| dual-branch, efficient, channel attention-based crop disease identification. ||| ronghua gao ||| rong wang ||| lu feng ||| qifeng li ||| huarui wu ||| 
2020 ||| grape disease image classification based on lightweight convolution neural networks and channelwise attention. ||| zhe tang ||| jialing yang ||| zhe li ||| fang qi ||| 
2021 ||| a three-dimensional prediction method of dissolved oxygen in pond culture based on attention-gru-gbrt. ||| xinkai cao ||| ni ren ||| ganglu tian ||| yuxing fan ||| qingling duan ||| 
2020 ||| detection of unregistered electric distribution transformers in agricultural fields with the aid of sentinel-1 sar images by machine learning approaches. ||| emrullah acar ||| 
2020 ||| two-level attention and score consistency network for plant segmentation. ||| lele xu ||| ye li ||| jinzhong xu ||| lili guo ||| 
2022 ||| an efficient attention module for instance segmentation network in pest monitoring. ||| hanxiang wang ||| yanfen li ||| l. minh dang ||| hyeonjoon moon ||| 
2021 ||| autonomous underwater robot for underwater image enhancement via multi-scale deformable convolution network with attention mechanism. ||| yi lin ||| jingchun zhou ||| wenqi ren ||| weishi zhang ||| 
2018 ||| neurofeedback training for enhancement of the focused attention related to athletic performance in elite rifle shooters. ||| yisi liu ||| salem chandrasekaran harihara subramaniam ||| olga sourina ||| eesha shah ||| joshua chua ||| kirill ivanov ||| 
2020 ||| classification of visual attention level during target gazing using microsaccades. ||| soichiro yokoo ||| nobuyuki nishiuchi ||| kimihiro yamanaka ||| 
2017 ||| sensorimotor accounts of joint attention. ||| alexander maye ||| carme isern-mas ||| pamela barone ||| john michael ||| 
2019 ||| applying siamese hierarchical attention neural networks for multi-document summarization. ||| jos ||| - ||| ngel gonz ||| lez ||| julien delonca ||| emilio sanchis ||| fernando garc ||| a-granada ||| encarna segarra ||| 
2021 ||| consumer cynicism identification for spanish reviews using a spanish transformer model. ||| samuel gonz ||| lez l ||| pez ||| steven bethard ||| francisca cecilia encinas orozco ||| adri ||| n pastor l ||| pez-monroy ||| 
2018 ||| experimental research on encoder-decoder architectures with attention for chatbots. ||| marta r. costa-juss ||| lvaro nuez ||| carlos segura ||| 
2021 ||| transformer-based extractive social media question answering on tweetqa. ||| sabur butt ||| noman ashraf ||| muhammad hammad fahim siddiqui ||| grigori sidorov ||| alexander f. gelbukh ||| 
2017 ||| hybrid attention networks for chinese short text classification. ||| yujun zhou ||| jiaming xu ||| jie cao ||| bo xu ||| changliang li ||| bo xu ||| 
2019 ||| promoting the knowledge of source syntax in transformer nmt is not needed. ||| thuong-hai pham ||| dominik mach ||| cek ||| ondrej bojar ||| 
2019 ||| multi-head multi-layer attention to deep language representations for grammatical error detection. ||| masahiro kaneko ||| mamoru komachi ||| 
2019 ||| a deep attention based framework for image caption generation in hindi language. ||| rijul dhir ||| santosh kumar mishra ||| sriparna saha ||| pushpak bhattacharyya ||| 
2020 ||| anomaly-based web attack detection: the application of deep neural network seq2seq with attention mechanism. ||| shahriar mohammadi ||| amin namadchian ||| 
2018 ||| decoding covert somatosensory attention by a bci system calibrated with tactile sensation. ||| lin yao ||| xinjun sheng ||| natalie mrachacz-kersting ||| xiangyang zhu ||| dario farina ||| ning jiang ||| 
2021 ||| fast eeg-based decoding of the directional focus of auditory attention using common spatial patterns. ||| simon geirnaert ||| tom francart ||| alexander bertrand ||| 
2019 ||| sensory stimulation training for bci system based on somatosensory attentional orientation. ||| lin yao ||| xinjun sheng ||| natalie mrachacz-kersting ||| xiangyang zhu ||| dario farina ||| ning jiang ||| 
2022 ||| ma-net: cross-modal cross-attention network for acute ischemic stroke lesion segmentation based on ct perfusion scans. ||| tianyu shi ||| huiyan jiang ||| bin zheng ||| 
2021 ||| inference of the selective auditory attention using sequential lmmse estimation. ||| ivine kuruvila ||| kubilay can demir ||| eghart fischer ||| ulrich hoppe ||| 
2018 ||| three-dimensional brain-computer interface control through simultaneous overt spatial attentional and motor imagery tasks. ||| jianjun meng ||| taylor d. streitz ||| nicholas s. gulachek ||| daniel suma ||| bin he ||| 
2020 ||| analysis of miniaturization effects and channel selection strategies for eeg sensor networks with application to auditory attention detection. ||| abhijith mundanad narayanan ||| alexander bertrand ||| 
2020 ||| toward decoding selective attention from single-trial eeg data in cochlear implant users. ||| waldo nogueira ||| giulio cosatti ||| irina schierholz ||| maria egger ||| bojana mirkovic ||| andreas b ||| chner ||| 
2022 ||| channel attention networks for robust mr fingerprint matching. ||| refik soyak ||| ebru navruz ||| eda ozgu ersoy ||| gast ||| o cruz ||| claudia prieto ||| andrew p. king ||| devrim  ||| nay ||| ilkay  ||| ks ||| z ||| 
2020 ||| skin lesion classification using cnns with patch-based attention and diagnosis-guided loss weighting. ||| nils gessert ||| thilo sentker ||| frederic madesta ||| r ||| diger schmitz ||| helge kniep ||| ivo m. baltruschat ||| ren |||  werner ||| alexander schlaefer ||| 
2019 ||| foot inertial sensing for combined cognitive-motor exercise of the sustained attention domain. ||| laura fiorini ||| martina maselli ||| raffaele esposito ||| emanuela castro ||| gianmaria mancioppi ||| francesca cecchi ||| cecilia laschi ||| saverio ottino ||| chiara rossi ||| francesca pinori ||| stefania tocchini ||| marco t. sportiello ||| paolo dario ||| filippo cavallo ||| 
2019 ||| ultrasound image segmentation: a deeply supervised network with attention to boundaries. ||| deepak mishra ||| santanu chaudhury ||| mukul sarkar ||| arvinder singh soin ||| 
2020 ||| multi-attention mechanism medical image segmentation combined with word embedding technology. ||| junlong cheng ||| shengwei tian ||| long yu ||| hongfeng you ||| 
2020 ||| drug adverse reaction discovery based on attention mechanism and fusion of emotional information. ||| keming kang ||| shengwei tian ||| long yu ||| 
2020 ||| identification of local adverse drug reactions in xinjiang based on attention mechanism and bilstm-cnn hybrid network. ||| xiaozhuo wang ||| shengwei tian ||| long yu ||| qimeng yang ||| 
2020 ||| personal-bullying detection based on multi-attention and cognitive feature. ||| m. niu ||| l. yu ||| s. tian ||| x. wang ||| q. zhang ||| 
2020 ||| coordination motives and competition for attention in information markets. ||| simone galperti ||| isabel trevino ||| 
2021 ||| attentional role of quota implementation. ||| andrei matveenko ||| sergei mikhalishchev ||| 
2017 ||| limited attention and status quo bias. ||| mark dean ||| zg ||| r kibris ||| yusufcan masatlioglu ||| 
2019 ||| inattention and belief polarization. ||| kristoffer p. nimark ||| savitar sundaresan ||| 
2021 ||| rational inattention and the monotone likelihood ratio property. ||| jeffrey mensch ||| 
2020 ||| estimating information cost functions in models of rational inattention. ||| ambuj dewan ||| nathaniel neligh ||| 
2019 ||| directed attention and nonparametric learning. ||| ian dew-becker ||| charles g. nathanson ||| 
2018 ||| dynamic rational inattention: analytical results. ||| bartosz mackowiak ||| filip matejka ||| mirko wiederholt ||| 
2018 ||| foundations for optimal inattention. ||| andrew ellis ||| 
2018 ||| limited attention, competition and welfare. ||| andreas hefti ||| 
2017 ||| rational inattention and the dynamics of consumption and wealth in general equilibrium. ||| yulei luo ||| jun nie ||| gaowang wang ||| eric r. young ||| 
2018 ||| the prevalence and gratification of nude self-presentation of men who have sex with men in online-dating environments: attracting attention, empowerment, and self-verification. ||| richard lemke ||| simon merz ||| 
2021 ||| internet addiction and attention in adolescents: a systematic review. ||| ma ||| sa gelain marin ||| xiomara nu ||| ez ||| rosa maria martins de almeida ||| 
2020 ||| short-term prosocial video game exposure influences attentional bias toward prosocial stimuli. ||| boyu qiu ||| shuangju zhen ||| cui zhou ||| jianping hu ||| wei zhang ||| 
2021 ||| whose tweets on covid-19 gain the most attention: celebrities, political, or scientific authorities? ||| mikolaj kaminski ||| cyntia szymanska ||| krzysztof jan nowak ||| 
2022 ||| trends, limits, and challenges of computer technologies in attention deficit hyperactivity disorder diagnosis and treatment. ||| renato montale ||| o brum alves ||| m ||| nica ferreira da silva ||| eber assis schmitz ||| antonio juarez alencar ||| 
2019 ||| interactive avatar boosts the performances of children with attention deficit hyperactivity disorder in dynamic measures of intelligence. ||| rosa angela fabio ||| tindara capr ||| giancarlo iannizzotto ||| andrea nucita ||| nasrin mohammadhasani ||| 
2019 ||| development of virtual reality continuous performance test utilizing social cues for children and adolescents with attention-deficit/hyperactivity disorder. ||| hyojung eom ||| kwanguk (kenny) kim ||| sungmi lee ||| yeon-ju hong ||| jiwoong heo ||| jae-jin kim ||| eunjoo kim ||| 
2020 ||| generative attention learning: a "general" framework for high-performance multi-fingered grasping in clutter. ||| bohan wu ||| iretiayo akinola ||| abhi gupta ||| feng xu ||| jacob varley ||| david watkins-valls ||| peter k. allen ||| 
2022 ||| heterogeneous graph attention networks for scalable multi-robot scheduling with temporospatial constraints. ||| zheyuan wang ||| chen liu ||| matthew c. gombolay ||| 
2019 ||| learning attentional regulations for structured tasks execution in robotic cognitive control. ||| riccardo caccavale ||| alberto finzi ||| 
2020 ||| attention-based active visual search for mobile robots. ||| amir rasouli ||| pablo lanillos ||| gordon cheng ||| john k. tsotsos ||| 
2019 ||| kinesthetic teaching and attentional supervision of structured tasks in human-robot interaction. ||| riccardo caccavale ||| matteo saveriano ||| alberto finzi ||| dongheui lee ||| 
2019 ||| a driving simulation study on visual cue presented in the peripheral visual field for prompting driver's attention. ||| hiroshi takahashi ||| makoto itoh ||| 
2021 ||| design and evaluation of attention guidance through eye gazing of "namida" driving agent. ||| shintaro tamura ||| naoki ohshima ||| komei hasegawa ||| michio okada ||| 
2021 ||| an attention-based deep learning model for traffic flow prediction using spatiotemporal features towards sustainable smart city. ||| balachandran vijayalakshmi ||| kadarkarayandi ramar ||| n. z. jhanjhi ||| sahil verma ||| madasamy kaliappan ||| kandasamy vijayalakshmi ||| shanmuganathan vimal ||| kavita ||| uttam ghosh ||| 
2017 ||| measurements and characterization of power transformer and low voltage access network for nb-plc. ||| bilal masood ||| muhammad usman ||| muhammad usman gul ||| waheed aftab khan ||| 
2021 ||| the dynamic effect of public information on liquidity: from the perspective of limited attention. ||| tao bing ||| yian cui ||| 
2021 ||| gaitvision: real-time extraction of gait parameters using residual attention network. ||| mohammad farukh hashmi ||| b. kiran kumar ashish ||| prabhu chaitanya ||| avinash g. keskar ||| sinan q. salih ||| neeraj dhanraj bokde ||| 
2020 ||| adaptive attention with consumer sentinel for movie box office prediction. ||| kaicheng feng ||| xiaobing liu ||| 
2021 ||| aemf: an attention-based efficient and multifeature fast text detector. ||| wanqi ma ||| chaoyu yang ||| jie yang ||| jian wu ||| 
2021 ||| multiscale efficient channel attention for fusion lane line segmentation. ||| kang liu ||| xin gao ||| 
2021 ||| multiscale receptive fields graph attention network for point cloud classification. ||| xi-an li ||| li-yan wang ||| jian lu ||| 
2020 ||| a tri-attention neural network model-basedrecommendation. ||| nanxin wang ||| libin yang ||| yu zheng ||| xiaoyan cai ||| xin mei ||| hang dai ||| 
2020 ||| fine-grained lung cancer classification from pet and ct images based on multidimensional attention mechanism. ||| ruoxi qin ||| zhenzhen wang ||| lingyun jiang ||| kai qiao ||| jinjin hai ||| jian chen ||| junling xu ||| dapeng shi ||| bin yan ||| 
2021 ||| time- and quantile-varying causality between investor attention and bitcoin returns: a rolling-window causality-in-quantiles approach. ||| jianqin hang ||| xu zhang ||| 
2021 ||| improving transformer-based neural machine translation with prior alignments. ||| thien nguyen ||| lam nguyen ||| phuoc tran ||| huu nguyen ||| 
2019 ||| decoding attentional state to faces and scenes using eeg brainwaves. ||| reza abiri ||| soheil borhani ||| yang jiang ||| xiaopeng zhao ||| 
2020 ||| multitask learning with local attention for tibetan speech recognition. ||| hui wang ||| fei gao ||| yue zhao ||| li yang ||| jianjian yue ||| huilin ma ||| 
2020 ||| attention with long-term interval-based deep sequential learning for recommendation. ||| zhao li ||| long zhang ||| chenyi lei ||| xia chen ||| jianliang gao ||| jun gao ||| 
2020 ||| dtfa-net: dynamic and texture features fusion attention network for face antispoofing. ||| xin cheng ||| hongfei wang ||| jingmei zhou ||| hui chang ||| xiangmo zhao ||| yilin jia ||| 
2018 ||| weibo attention and stock market performance: some empirical evidence. ||| minghua dong ||| xiong xiong ||| xiao li ||| dehua shen ||| 
2021 ||| application of multiattention mechanism in power system branch parameter identification. ||| zhiwei wang ||| liguo weng ||| min lu ||| jun liu ||| lingling pan ||| 
2020 ||| a hierarchical attention recommender system based on cross-domain social networks. ||| rongmei zhao ||| xi xiong ||| xia zu ||| shenggen ju ||| zhongzhi li ||| binyong li ||| 
2020 ||| complexity to forecast flood: problem definition and spatiotemporal attention lstm solution. ||| yirui wu ||| yukai ding ||| yuelong zhu ||| jun feng ||| sifeng wang ||| 
2020 ||| a bichannel transformer with context encoding for document-driven conversation generation in social media. ||| yuanyuan cai ||| min zuo ||| qingchuan zhang ||| haitao xiong ||| ke li ||| 
2021 ||| a spatial-temporal self-attention network (stsan) for location prediction. ||| shuang wang ||| anliang li ||| shuai xie ||| wenzhu li ||| bowei wang ||| shuai yao ||| muhammad asif ||| 
2022 ||| river segmentation of remote sensing images based on composite attention network. ||| zhiyong fan ||| jianmin hou ||| qiang zang ||| yunjie chen ||| fei yan ||| 
2020 ||| multichannel deep attention neural networks for the classification of autism spectrum disorder using neuroimaging and personal characteristic data. ||| ke niu ||| jiayang guo ||| yijie pan ||| xin gao ||| xueping peng ||| ning li ||| hailong li ||| 
2020 ||| identification and classification of atmospheric particles based on sem images using convolutional neural network with attention mechanism. ||| changchang yin ||| xuezhen cheng ||| xilu liu ||| meng zhao ||| 
2020 ||| articulatory-to-acoustic conversion using bilstm-cnn word-attention-based method. ||| guofeng ren ||| guicheng shao ||| jianmei fu ||| 
2021 ||| comparative efficacy and acceptability of nonpharmacotherapy in the treatment of inattention for adhd: a network meta-analysis. ||| xusheng che ||| choi jong-hwan ||| xiuhai shang ||| 
2020 ||| phonetics and ambiguity comprehension gated attention network for humor recognition. ||| xiaochao fan ||| hongfei lin ||| liang yang ||| yufeng diao ||| chen shen ||| yonghe chu ||| tongxuan zhang ||| 
2021 ||| multi-indices quantification for left ventricle via densenet and gru-based encoder-decoder with attention. ||| zhi liu ||| yunhua lu ||| xiaochuan zhang ||| sen wang ||| shuo li ||| bo chen ||| 
2021 ||| arabic fake news detection: comparative study of neural networks and transformer-based approaches. ||| maha al-yahya ||| hend s. al-khalifa ||| heyam h. al-baity ||| duaa h. alsaeed ||| amr essam ||| 
2019 ||| dynamic cross-correlations between participants' attentions to p2p lending and offline loan in the private lending market. ||| yingxiu zhao ||| wei zhang ||| xiangyu kong ||| 
2021 ||| person reidentification model based on multiattention modules and multiscale residuals. ||| yongyi li ||| shiqi wang ||| shuang dong ||| xueling lv ||| changzhi lv ||| di fan ||| 
2021 ||| erratum to "a hierarchical attention recommender system based on cross-domain social networks". ||| rongmei zhao ||| xi xiong ||| xia zu ||| shenggen ju ||| zhongzhi li ||| binyong li ||| 
2020 ||| load estimation of complex power networks from transformer measurements and forecasted loads. ||| haina rong ||| francisco de leon ||| 
2020 ||| recommendation algorithm in double-layer network based on vector dynamic evolution clustering and attention mechanism. ||| jianrui chen ||| zhihui wang ||| tingting zhu ||| fernando e. rosas ||| 
2019 ||| dynamic transmission of correlation between investor attention and stock price: evidence from china's energy industry typical stocks. ||| yajie qi ||| huajiao li ||| sui guo ||| sida feng ||| 
2020 ||| a deep learning approach for a source code detection model using self-attention. ||| yao meng ||| long liu ||| 
2021 ||| cross-model transformer method for medical image synthesis. ||| zebin hu ||| hao liu ||| zhendong li ||| zekuan yu ||| 
2021 ||| influencing factors of social service satisfaction of the elderly under the background of internet attention. ||| miao lin ||| 
2021 ||| collaborative big data management and analytics in complex systems with edge 2021 eacamera: a case study on ai-based complex attention analysis with edge system. ||| chaopeng guo ||| peimeng zhu ||| feng li ||| jie song ||| 
2020 ||| an easy to implement and robust design control method dedicated to multi-cell converters using inter cell transformers. ||| khaled tamizi ||| olivier bethoux ||| eric labour ||| 
2017 ||| design and fault-operation analysis of a modular cyclic cascade inter-cell transformer (ict) for parallel multicell converters. ||| s ||| bastien sanchez ||| fr ||| d ||| ric richardeau ||| damien risaletto ||| 
2020 ||| realization of extremely high and low impedance transforming ratios using cross-shaped impedance transformer. ||| hong-xu zhu ||| pedro cheong ||| sut-kam ho ||| kam-weng tam ||| wai-wa choi ||| 
2017 ||| a novel ch5 inverter for single-phase transformerless photovoltaic system applications. ||| xiaoqiang guo ||| 
2018 ||| a -8 mv/+15 mv double polarity piezoelectric transformer-based step-up oscillator for energy harvesting applications. ||| antonio camarda ||| marco tartagni ||| aldo romani ||| 
2018 ||| a k-ka-band concurrent dual-band single-ended input to differential output low-noise amplifier employing a novel transformer feedback dual-band load. ||| jaeyoung lee ||| cam nguyen ||| 
2020 ||| a digitally programmable wide tuning-range active transformer for inductorless bpf and dcos. ||| saeed abolhassani ||| mostafa shaterian ||| 
2018 ||| transformer-based input integrated matching in cascode amplifiers: analytical proofs. ||| domenico pepe ||| ilias chlis ||| domenico zito ||| 
2018 ||| an on-chip transformer-based self-startup hybrid sidito converter for thermoelectric energy harvesting. ||| yao qian ||| danzhu lu ||| jie he ||| zhiliang hong ||| 
2020 ||| high-selectivity single-ended/balanced dc-block filtering impedance transformer and its application on power amplifier. ||| zheng zhuang ||| yongle wu ||| mengdan kong ||| weimin wang ||| 
2017 ||| tuning range extension of a transformer-based oscillator through common-mode colpitts resonance. ||| mina shahmohammadi ||| masoud babaie ||| robert bogdan staszewski ||| 
2017 ||| design and analysis of cmos lnas with transformer feedback for wideband input matching and noise cancellation. ||| liang wu ||| hiu fai leung ||| howard c. luong ||| 
2020 ||| transformerless grid-connected pv inverter without common mode leakage current and shoot-through problems. ||| zhilei yao ||| yubo zhang ||| xuefeng hu ||| 
2019 ||| a v-band power amplifier with integrated wilkinson power dividers-combiners and transformers in 0.18- $\mu$ m sige bicmos. ||| kyoungwoon kim ||| cam nguyen ||| 
2020 ||| galvanically isolated dc-dc converter using a single isolation transformer for multi-channel communication. ||| alessandro parisi ||| egidio ragonese ||| nunzio spina ||| giuseppe palmisano ||| 
2020 ||| a 0.2-v three-winding transformer-based dco in 16-nm finfet cmos. ||| chao-chieh li ||| min-shueh yuan ||| yu-tso lin ||| chia-chun liao ||| chih-hsien chang ||| robert bogdan staszewski ||| 
2018 ||| cmos switched-capacitor dsb-ssb converter using a hilbert transformer. ||| fabio de lacerda ||| antonio petraglia ||| jos |||  gabriel rodr ||| guez carneiro gomes ||| 
2018 ||| synthesis of low-pass real-to-real impedance transformer with coupled-inductors. ||| hiromitsu uchida ||| masao nakashima ||| seiichi handa ||| takehiko sagara ||| kenichi hariu ||| 
2017 ||| transformer-feedback dual-band neutralization technique. ||| gholamreza nikandish ||| ali medi ||| 
2020 ||| a multiport power electronic transformer based on modular multilevel converter and mixed-frequency modulation. ||| dajun ma ||| wu chen ||| liangcai shu ||| xiaohui qu ||| xin zhan ||| zhong liu ||| 
2018 ||| a transformer-based current-reuse qvco with an fom up to -200.5 dbc/hz. ||| ping-yi wang ||| guan-yu su ||| yin-cheng chang ||| da-chiang chang ||| shawn s. h. hsu ||| 
2018 ||| second-order equivalent circuits for the design of doubly-tuned transformer matching networks. ||| andrea mazzanti ||| andrea bevilacqua ||| 
2020 ||| -boosted 10-ghz vco with center-tap transformer and stacked transistor. ||| hee sung lee ||| dong min kang ||| seong jun cho ||| chul woo byeon ||| chul soon park ||| 
2019 ||| a 2-ghz fbar-based transformer coupled oscillator design with phase noise reduction. ||| jabeom koo ||| keping wang ||| richard c. ruby ||| brian p. otis ||| 
2020 ||| design of compact coupled-line complex impedance transformers with the series susceptance component. ||| shaojun fang ||| xiao jia ||| hongmei liu ||| zhongbao wang ||| 
2020 ||| an improved pwm technique to achieve continuous input current in common-ground transformerless boost inverter. ||| sze sing lee ||| yam prasad siwakoti ||| chee shen lim ||| kyo-beum lee ||| 
2020 ||| a 65-81 ghz cmos dual-mode vco using high quality factor transformer-based inductors. ||| ali basaligheh ||| parvaneh saffari ||| igor m. filanovsky ||| kambiz moez ||| 
2018 ||| a transformer-based 3-db differential coupler. ||| yongqiang wang ||| kaixue ma ||| shouxian mou ||| 
2018 ||| a low-voltage low-phase-noise 25-ghz two-tank transformer-feedback vco. ||| shita guo ||| ping gui ||| tianwei liu ||| tao zhang ||| tianzuo xi ||| guoying wu ||| yanli fan ||| mark morgan ||| 
2017 ||| an efficient digital background control for hybrid transformer-based receivers. ||| gerardo castellano ||| daniele montanari ||| davide de caro ||| danilo manstretta ||| antonio giuseppe maria strollo ||| 
2020 ||| design of d-band transformer-based gain-boosting class-ab power amplifiers in silicon technologies. ||| xinyan tang ||| johan nguyen ||| alaaeldien medra ||| khaled khalaf ||| akshay visweswaran ||| bj ||| rn debaillie ||| piet wambacq ||| 
2018 ||| adaptive spatiotemporal feature extraction and dynamic combining methods for selective visual attention system. ||| kyung joo cheoi ||| mi-hye kim ||| 
2018 ||| a neural attention based model for morphological segmentation. ||| shunle zhu ||| 
2018 ||| evaluation of the forecast models of chinese tourists to thailand based on search engine attention: a case study of baidu. ||| junjian tang ||| 
2020 ||| science map of cochrane systematic reviews receiving the most altmetric attention score: a network analysis. ||| jafar kolahi ||| saber khazaei ||| elham bidram ||| roya kelishadi ||| pedram iranmanesh ||| abbasali khademi ||| mohammad h. nekoofar ||| paul m. h. dummer ||| 
2020 ||| the balance of attention: the challenges of creating locative cultural storytelling experiences. ||| david e. millard ||| heather s. packer ||| yvonne margaret howard ||| charlie hargood ||| 
2019 ||| attention-based sentiment reasoner for aspect-based sentiment analysis. ||| ning liu ||| bo shen ||| zhenjiang zhang ||| zhiyuan zhang ||| kun mi ||| 
2019 ||| an audio attention computational model based on information entropy of two channels and exponential moving average. ||| yu liu ||| cong zhang ||| bo hang ||| song wang ||| han-chieh chao ||| 
2020 ||| facial uv map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual unets. ||| in seop na ||| chung tran ||| dung nguyen ||| sang dinh ||| 
2020 ||| information cascades prediction with attention neural network. ||| yun liu ||| zemin bao ||| zhenjiang zhang ||| di tang ||| fei xiong ||| 
2022 ||| blind image separation based on attentional generative adversarial network. ||| xiao sun ||| jindong xu ||| yongli ma ||| tianyu zhao ||| shifeng ou ||| lizhi peng ||| 
2021 ||| attention-based hierarchical recurrent neural networks for mooc forum posts analysis. ||| nicola capuano ||| santi caball ||| jordi conesa ||| antonio greco ||| 
2021 ||| sentiment analysis of student feedback using multi-head attention fusion model of word and context embedding for lstm. ||| k. sangeetha ||| d. prabha ||| 
2021 ||| composite deep neural network with gated-attention mechanism for diabetic retinopathy severity classification. ||| jyostna devi bodapati ||| shaik nagur shareef ||| naralasetti veeranjaneyulu ||| 
2022 ||| improving time series forecasting using lstm and attention models. ||| hossein abbasimehr ||| reza paki ||| 
2022 ||| serial attention network for skin lesion segmentation. ||| yuan ren ||| long yu ||| shengwei tian ||| junlong cheng ||| zhiqi guo ||| yanhan zhang ||| 
2021 ||| textspamdetector: textual content based deep learning framework for social spam detection using conjoint attention mechanism. ||| e. elakkiya ||| s. selvakumar ||| r. leela velusamy ||| 
2022 ||| single-channel blind source separation based on attentional generative adversarial network. ||| xiao sun ||| jindong xu ||| yongli ma ||| tianyu zhao ||| shifeng ou ||| 
2020 ||| development of virtual reality rehabilitation games for children with attention-deficit hyperactivity disorder. ||| yang-kun ou ||| yu-lin wang ||| hua-cheng chang ||| shih-yin yen ||| yu-hua zheng ||| bih-o. lee ||| 
2022 ||| three-stream spatio-temporal attention network for first-person action and interaction recognition. ||| javed imran ||| balasubramanian raman ||| 
2020 ||| migrating a software factory to design thinking: paying attention to people and mind-sets. ||| nolwen mahe ||| bram adams ||| josianne marsan ||| mathieu templier ||| sylvie bissonnette ||| 
2022 ||| an attention-aided deep learning framework for massive mimo channel estimation. ||| jiabao gao ||| mu hu ||| caijun zhong ||| geoffrey ye li ||| zhaoyang zhang ||| 
2018 ||| scalable lumped models of integrated transformers for galvanically isolated power transfer systems. ||| nunzio greco ||| alessandro parisi ||| nunzio spina ||| egidio ragonese ||| giuseppe palmisano ||| 
2021 ||| a transformer with high coupling coefficient and small area based on tsv. ||| fengjuan wang ||| ruinan ren ||| xiangkun yin ||| ningmei yu ||| yuan yang ||| 
2017 ||| design of low-power wideband frequency quadruplers based on transformer-coupled resonators for e-band backhaul applications. ||| lorenzo iotti ||| matteo bassi ||| andrea mazzanti ||| francesco svelto ||| 
2018 ||| made in academia: the effect of institutional origin on inventors' attention to science. ||| micha ||| l bikard ||| 
2018 ||| tasks interrupted: how anticipating time pressure on resumption of an interrupted task causes attention residue and low performance on interrupting tasks and how a "ready-to-resume" plan mitigates the effects. ||| sophie leroy ||| theresa m. glomb ||| 
2019 ||| visual focus of attention and spontaneous smile recognition based on continuous head pose estimation by cascaded multi-task learning. ||| yuanyuan liu ||| xingmei li ||| fang fang ||| fayong zhang ||| jingying chen ||| zhizhong zeng ||| 
2021 ||| caffnet: channel attention and feature fusion network for multi-target traffic sign detection. ||| feng liu ||| yurong qian ||| hua li ||| yongqiang wang ||| hao zhang ||| 
2021 ||| baggage image retrieval with attention-based network for security checks. ||| gan huang ||| li yang ||| ding zhang ||| xiaofeng wang ||| yanfu wang ||| 
2018 ||| decentralized robust dynamic state estimation in power systems using instrument transformers. ||| abhinav kumar singh ||| bikash c. pal ||| 
2022 ||| toast: automated testing of object transformers in dynamic software updates. ||| zelin zhao ||| di huang ||| xiaoxing ma ||| 
2021 ||| watuning: a workload-aware tuning system with attention-based deep reinforcement learning. ||| jia-ke ge ||| yanfeng chai ||| yunpeng chai ||| 
2019 ||| adversarial heterogeneous network embedding with metapath attention mechanism. ||| chunyang ruan ||| ye wang ||| jiangang ma ||| yanchun zhang ||| xintian chen ||| 
2017 ||| deep multimodal reinforcement network with contextually guided recurrent attention for image question answering. ||| aiwen jiang ||| bo liu ||| ming-wen wang ||| 
2020 ||| atlrec: an attentional adversarial transfer learning network for cross-domain recommendation. ||| ying li ||| jia-jie xu ||| pengpeng zhao ||| junhua fang ||| wei chen ||| lei zhao ||| 
2017 ||| type-aware question answering over knowledge base with attention-based tree-structured neural networks. ||| jun yin ||| wayne xin zhao ||| xiaoming li ||| 
2020 ||| word-pair relevance modeling with multi-view neural attention mechanism for sentence alignment. ||| ying ding ||| junhui li ||| zhengxian gong ||| guodong zhou ||| 
2020 ||| reference image guided super-resolution via progressive channel attention networks. ||| huanjing yue ||| sheng shen ||| jing-yu yang ||| haofeng hu ||| yan-fang chen ||| 
2021 ||| attend to chords: improving harmonic analysis of symbolic music using transformer-based models. ||| tsung-ping chen ||| li su ||| 
2021 ||| impact of mainstream classroom setting on attention of children with autism spectrum disorder: an eye-tracking study. ||| bilikis banire ||| dena al-thani ||| marwa k. qaraqe ||| bilal mansoor ||| mustapha makki ||| 
2022 ||| effects of a cognitive stimulation software on attention, memory, and activities of daily living in mexican older adults. ||| christian o. acosta ||| ramon r. palacio ||| joaqu ||| n cortez gonz ||| lez ||| sonia b. echeverr ||| a ||| mar ||| a jos |||  rodr ||| guez-f ||| rtiz ||| 
2019 ||| enhancement of english learning performance by using an attention-based diagnosing and review mechanism in paper-based learning context with digital pen support. ||| ming chen ||| jung-ying wang ||| mi lin ||| 
2022 ||| ms-transformer: introduce multiple structural priors into a unified transformer for encoding sentences. ||| le qi ||| yu zhang ||| qingyu yin ||| ting liu ||| 
2020 ||| investigating topics, audio representations and attention for multimodal scene-aware dialog. ||| shachi h. kumar ||| eda okur ||| saurav sahay ||| jonathan huang ||| lama nachman ||| 
2020 ||| rap-net: recurrent attention pooling networks for dialogue response selection. ||| chao-wei huang ||| ting-rui chiang ||| shang-yu su ||| yun-nung chen ||| 
2021 ||| attention-based bilstm fused cnn with gating mechanism model for chinese long text classification. ||| jianfeng deng ||| lianglun cheng ||| zhuowei wang ||| 
2022 ||| talking-heads attention-based knowledge representation for link prediction. ||| shirui wang ||| wen'an zhou ||| qiang zhou ||| 
2020 ||| learning multi-level information for dialogue response selection by highway recurrent transformer. ||| ting-rui chiang ||| chao-wei huang ||| shang-yu su ||| yun-nung chen ||| 
2020 ||| hierarchical multimodal attention for end-to-end audio-visual scene-aware dialogue response generation. ||| hung le ||| doyen sahoo ||| nancy f. chen ||| steven c. h. hoi ||| 
2020 ||| speaker-informed time-and-content-aware attention for spoken language understanding. ||| jonggu kim ||| yewon jeong ||| jong-hyeok lee ||| 
2019 ||| joint dialog act segmentation and recognition in human conversations using attention to dialog context. ||| tianyu zhao ||| tatsuya kawahara ||| 
2022 ||| combining context-relevant features with multi-stage attention network for short text classification. ||| yingying liu ||| pei-pei li ||| xuegang hu ||| 
2020 ||| knowledge-grounded response generation with deep attentional latent-variable model. ||| hao-tong ye ||| kai-lin lo ||| shang-yu su ||| yun-nung chen ||| 
2021 ||| a korean named entity recognition method using bi-lstm-crf and masked self-attention. ||| guozhe jin ||| zhezhou yu ||| 
2017 ||| focusstack: orchestrating edge clouds using focus of attention. ||| brian amento ||| robert j. hall ||| kaustubh joshi ||| k. hal purdy ||| 
2020 ||| residual dense network based on channel-spatial attention for the scene classification of a high-resolution remote sensing image. ||| xiaolei zhao ||| jing zhang ||| jimiao tian ||| li zhuo ||| jie zhang ||| 
2021 ||| integrating weighted feature fusion and the spatial attention module with convolutional neural networks for automatic aircraft detection from sar images. ||| jielan wang ||| hongguang xiao ||| lifu chen ||| jin xing ||| zhouhao pan ||| ru luo ||| xingmin cai ||| 
2021 ||| erratum: liu et al. nightlight as a proxy of economic indicators: fine-grained gdp inference around chinese mainland via attention-augmented cnn from daytime satellite imagery. remote sens. 2021, 13, 2067. ||| haoyu liu ||| xianwen he ||| yanbing bai ||| xing liu ||| yilin wu ||| yanyun zhao ||| hanfang yang ||| 
2019 ||| semantic segmentation on remotely sensed images using an enhanced global convolutional network with channel attention and domain specific transfer learning. ||| teerapong panboonyuen ||| kulsawasd jitkajornwanich ||| siam lawawirojwong ||| panu srestasathiern ||| peerapon vateekul ||| 
2021 ||| knowledge and spatial pyramid distance-based gated graph attention network for remote sensing semantic segmentation. ||| wei cui ||| xin he ||| meng yao ||| ziwei wang ||| yuanjie hao ||| jie li ||| weijie wu ||| huiling zhao ||| cong xia ||| jin li ||| wenqi cui ||| 
2021 ||| trs: transformers for remote sensing scene classification. ||| jianrong zhang ||| hongwei zhao ||| jiao li ||| 
2021 ||| target detection network for sar images based on semi-supervised learning and attention mechanism. ||| di wei ||| yuang du ||| lan du ||| lu li ||| 
2022 ||| transformer for tree counting in aerial images. ||| guang chen ||| yi shang ||| 
2021 ||| domain-adversarial training of self-attention-based networks for land cover classification using multi-temporal sentinel-2 satellite imagery. ||| mauro martini ||| vittorio mazzia ||| aleem khaliq ||| marcello chiaberge ||| 
2020 ||| multi-temporal unmanned aerial vehicle remote sensing for vegetable mapping using an attention-based recurrent convolutional neural network. ||| quanlong feng ||| jianyu yang ||| yiming liu ||| cong ou ||| dehai zhu ||| bowen niu ||| jiantao liu ||| baoguo li ||| 
2021 ||| split-attention networks with self-calibrated convolution for moon impact crater detection from multi-source data. ||| yutong jia ||| gang wan ||| lei liu ||| jue wang ||| yitian wu ||| naiyang xue ||| ying wang ||| rixin yang ||| 
2022 ||| a spatiotemporal fusion method based on multiscale feature extraction and spatial channel attention mechanism. ||| dajiang lei ||| gangsheng ran ||| liping zhang ||| weisheng li ||| 
2022 ||| swin transformer and deep convolutional neural networks for coastal wetland classification using sentinel-1, sentinel-2, and lidar data. ||| ali jamali ||| masoud mahdianpari ||| 
2021 ||| semantic segmentation of aerial imagery via split-attention networks with disentangled nonlocal and edge supervision. ||| cheng zhang ||| wanshou jiang ||| qing zhao ||| 
2021 ||| attention multi-scale network for automatic layer extraction of ice radar topological sequences. ||| yiheng cai ||| dan liu ||| jin xie ||| jingxian yang ||| xiangbin cui ||| shinan lang ||| 
2021 ||| a 3d cascaded spectral-spatial element attention network for hyperspectral image classification. ||| huaiping yan ||| jun wang ||| lei tang ||| erlei zhang ||| kun yan ||| kai yu ||| jinye peng ||| 
2020 ||| a multiscale self-adaptive attention network for remote sensing scene classification. ||| lingling li ||| pujiang liang ||| jingjing ma ||| licheng jiao ||| xiaohui guo ||| fang liu ||| chen sun ||| 
2022 ||| uatnet: u-shape attention-based transformer net for meteorological satellite cloud recognition. ||| zhanjie wang ||| jianghua zhao ||| ran zhang ||| zheng li ||| qinghui lin ||| xuezhi wang ||| 
2020 ||| super-resolution for hyperspectral remote sensing images based on the 3d attention-srgan network. ||| xinyu dou ||| chenyu li ||| qian shi ||| mengxi liu ||| 
2021 ||| multi-modality and multi-scale attention fusion network for land cover classification from vhr remote sensing images. ||| tao lei ||| linze li ||| zhiyong lv ||| mingzhe zhu ||| xiaogang du ||| asoke k. nandi ||| 
2021 ||| memory-augmented transformer for remote sensing image semantic segmentation. ||| xin zhao ||| jiayi guo ||| yueting zhang ||| yirong wu ||| 
2021 ||| ship object detection of remote sensing image based on visual attention. ||| yuxin dong ||| fukun chen ||| shuang han ||| hao liu ||| 
2019 ||| a mutiscale residual attention network for multitask learning of human activity using radar micro-doppler signatures. ||| yuan he ||| xinyu li ||| xiaojun jing ||| 
2019 ||| hdranet: hybrid dilated residual attention network for sar image despeckling. ||| jingyu li ||| ying li ||| yayuan xiao ||| yunpeng bai ||| 
2021 ||| cross-dimension attention guided self-supervised remote sensing single-image super-resolution. ||| wenzong jiang ||| lifei zhao ||| yanjiang wang ||| weifeng liu ||| bao-di liu ||| 
2021 ||| self-attention-based conditional variational auto-encoder generative adversarial networks for hyperspectral classification. ||| zhitao chen ||| lei tong ||| bin qian ||| jing yu ||| chuangbai xiao ||| 
2020 ||| sample generation with self-attention generative adversarial adaptation network (sagaan) for hyperspectral image classification. ||| wenzhi zhao ||| xi chen ||| jiage chen ||| yang qu ||| 
2021 ||| sga-net: self-constructing graph attention neural network for semantic segmentation of remote sensing images. ||| wenjie zi ||| wei xiong ||| hao chen ||| jun li ||| ning jing ||| 
2022 ||| a dual attention convolutional neural network for crop classification using time-series sentinel-2 imagery. ||| seyd teymoor seydi ||| meisam amani ||| arsalan ghorbanian ||| 
2021 ||| hybrid attention based residual network for pansharpening. ||| qin liu ||| letong han ||| rui tan ||| hongfei fan ||| weiqi li ||| hongming zhu ||| bowen du ||| sicong liu ||| 
2019 ||| building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms. ||| xuran pan ||| fan yang ||| lianru gao ||| zhengchao chen ||| bing zhang ||| hairui fan ||| jinchang ren ||| 
2020 ||| generating anchor boxes based on attention mechanism for object detection in remote sensing images. ||| zhuangzhuang tian ||| ronghui zhan ||| jiemin hu ||| wei wang ||| zhiqiang he ||| zhaowen zhuang ||| 
2022 ||| pan-sharpening based on cnn+ pyramid transformer by using no-reference loss. ||| sijia li ||| qing guo ||| an li ||| 
2022 ||| an investigation of a multidimensional cnn combined with an attention mechanism model to resolve small-sample problems in hyperspectral image classification. ||| jinxiang liu ||| kefei zhang ||| suqin wu ||| hongtao shi ||| yindi zhao ||| yaqin sun ||| huifu zhuang ||| erjiang fu ||| 
2021 ||| improving yolov5 with attention mechanism for detecting boulders from planetary images. ||| linlin zhu ||| xun geng ||| zheng li ||| chun liu ||| 
2021 ||| land use and land cover mapping using rapideye imagery based on a novel band attention deep learning method in the three gorges reservoir area. ||| xin zhang ||| ling du ||| shen tan ||| fangming wu ||| liang zhu ||| yuan zeng ||| bingfang wu ||| 
2022 ||| superpixel-based attention graph neural network for semantic segmentation in aerial images. ||| qi diao ||| yaping dai ||| ce zhang ||| yan wu ||| xiaoxue feng ||| feng pan ||| 
2021 ||| remote sensing image defogging networks based on dual self-attention boost residual octave convolution. ||| zhiqin zhu ||| yaqin luo ||| guanqiu qi ||| jun meng ||| yong li ||| neal mazur ||| 
2021 ||| saffnet: self-attention-based feature fusion network for remote sensing few-shot scene classification. ||| joseph kim ||| mingmin chi ||| 
2019 ||| an improved grabcut method based on a visual attention model for rare-earth ore mining area recognition with high-resolution remote sensing images. ||| yan peng ||| zhaoming zhang ||| guojin he ||| mingyue wei ||| 
2019 ||| spectral-spatial attention networks for hyperspectral image classification. ||| xiaoguang mei ||| erting pan ||| yong ma ||| xiaobing dai ||| jun huang ||| fan fan ||| qinglei du ||| hong zheng ||| jiayi ma ||| 
2021 ||| effect of attention mechanism in deep learning-based remote sensing image processing: a systematic literature review. ||| saman ghaffarian ||| jo ||| o valente ||| mariska van der voort ||| bedir tekinerdogan ||| 
2021 ||| attention-based spatial and spectral network with pca-guided self-supervised feature extraction for change detection in hyperspectral images. ||| zhao wang ||| fenlong jiang ||| tongfei liu ||| fei xie ||| peng li ||| 
2020 ||| a hybrid attention-aware fusion network (hafnet) for building extraction from high-resolution imagery and lidar data. ||| peng zhang ||| peijun du ||| cong lin ||| xin wang ||| erzhu li ||| zhaohui xue ||| xuyu bai ||| 
2020 ||| a slimmer network with polymorphic and group attention modules for more efficient object detection in aerial images. ||| wei guo ||| weihong li ||| zhenghao li ||| weiguo gong ||| jinkai cui ||| xinran wang ||| 
2019 ||| attentionbased deep feature fusion for the scene classification of highresolution remote sensing images. ||| 
2022 ||| fusion classification of hsi and msi using a spatial-spectral vision transformer for wetland biodiversity estimation. ||| yunhao gao ||| xiukai song ||| wei li ||| jianbu wang ||| jianlong he ||| xiangyang jiang ||| yinyin feng ||| 
2021 ||| deanet: dual encoder with attention network for semantic segmentation of remote sensing imagery. ||| haoran wei ||| xiangyang xu ||| ni ou ||| xinru zhang ||| yaping dai ||| 
2022 ||| gansformer: a detection network for aerial images with high performance combining convolutional network and transformer. ||| yan zhang ||| xi liu ||| shiyun wa ||| shuyu chen ||| qin ma ||| 
2022 ||| reflective noise filtering of large-scale point cloud using transformer. ||| rui gao ||| mengyu li ||| seungjun yang ||| kyungeun cho ||| 
2022 ||| multiscale feature fusion network incorporating 3d self-attention for hyperspectral image classification. ||| yuhao qing ||| quanzhen huang ||| liuyan feng ||| yueyan qi ||| wenyi liu ||| 
2019 ||| deep feature fusion with integration of residual connection and attention model for classification of vhr remote sensing images. ||| jicheng wang ||| li shen ||| wenfan qiao ||| yanshuai dai ||| zhilin li ||| 
2020 ||| generative adversarial networks based on collaborative learning and attention mechanism for hyperspectral image classification. ||| jie feng ||| xueliang feng ||| jiantong chen ||| xianghai cao ||| xiangrong zhang ||| licheng jiao ||| tao yu ||| 
2022 ||| a multi-domain collaborative transfer learning method with multi-scale repeated attention mechanism for underwater side-scan sonar image classification. ||| zhen cheng ||| guanying huo ||| haisen li ||| 
2021 ||| u2-onet: a two-level nested octave u-structure network with a multi-scale attention mechanism for moving object segmentation. ||| chenjie wang ||| chengyuan li ||| jun liu ||| bin luo ||| xin su ||| yajun wang ||| yan gao ||| 
2021 ||| improved transformer net for hyperspectral image classification. ||| yuhao qing ||| wenyi liu ||| liuyan feng ||| wanjia gao ||| 
2019 ||| multi-scale semantic segmentation and spatial relationship recognition of remote sensing images based on an attention model. ||| wei cui ||| fei wang ||| xin he ||| dongyou zhang ||| xuxiang xu ||| meng yao ||| ziwei wang ||| jiejun huang ||| 
2021 ||| mare: self-supervised multi-attention resu-net for semantic segmentation in remote sensing. ||| valerio marsocci ||| simone scardapane ||| nikos komodakis ||| 
2021 ||| an attention-guided multilayer feature aggregation network for remote sensing image scene classification. ||| ming li ||| lin lei ||| yuqi tang ||| yuli sun ||| gangyao kuang ||| 
2021 ||| double-branch network with pyramidal convolution and iterative attention for hyperspectral image classification. ||| hao shi ||| guo cao ||| zixian ge ||| youqiang zhang ||| peng fu ||| 
2022 ||| a transformer-based coarse-to-fine wide-swath sar image registration method under weak texture conditions. ||| yibo fan ||| feng wang ||| haipeng wang ||| 
2022 ||| agnet: an attention-based graph network for point cloud classification and segmentation. ||| weipeng jing ||| wenjun zhang ||| linhui li ||| donglin di ||| guangsheng chen ||| jian wang ||| 
2021 ||| hcnet: a point cloud object detection network based on height and channel attention. ||| jing zhang ||| jiajun wang ||| da xu ||| yunsong li ||| 
2021 ||| deep residual dual-attention network for super-resolution reconstruction of remote sensing images. ||| bo huang ||| boyong he ||| liaoni wu ||| zhiming guo ||| 
2021 ||| revise-net: exploiting reverse attention mechanism for salient object detection. ||| rukhshanda hussain ||| yash karbhari ||| muhammad fazal ijaz ||| marcin wozniak ||| pawan kumar singh ||| ram sarkar ||| 
2020 ||| a new framework for automatic airports extraction from sar images using multi-level dual attention mechanism. ||| lifu chen ||| siyu tan ||| zhouhao pan ||| jin xing ||| zhihui yuan ||| xuemin xing ||| peng zhang ||| 
2021 ||| gsap: a global structure attention pooling method for graph-based visual place recognition. ||| yukun yang ||| bo ma ||| xiangdong liu ||| liang zhao ||| shoudong huang ||| 
2021 ||| multiscale and multitemporal road detection from high resolution sar images using attention mechanism. ||| xiaochen wei ||| xikai fu ||| ye yun ||| xiaolei lv ||| 
2021 ||| pag-yolo: a portable attention-guided yolo network for small ship detection. ||| jianming hu ||| xiyang zhi ||| tianjun shi ||| wei zhang ||| yang cui ||| shenggang zhao ||| 
2021 ||| improved yolov3 based on attention mechanism for fast and accurate ship detection in optical remote sensing images. ||| liqiong chen ||| wenxuan shi ||| dexiang deng ||| 
2021 ||| combinational fusion and global attention of the single-shot method for synthetic aperture radar ship detection. ||| libo xu ||| chaoyi pang ||| yan guo ||| zhenyu shu ||| 
2022 ||| remote sensing image denoising based on deep and shallow feature fusion and attention mechanism. ||| lintao han ||| yuchen zhao ||| hengyi lv ||| yisa zhang ||| hailong liu ||| guoling bi ||| 
2021 ||| vision transformers for remote sensing image classification. ||| yakoub bazi ||| laila bashmal ||| mohamad mahmoud al rahhal ||| reham al dayil ||| naif al ajlan ||| 
2022 ||| hyperspectral image classification based on 3d coordination attention mechanism network. ||| cuiping shi ||| diling liao ||| tianyu zhang ||| liguo wang ||| 
2021 ||| attention-guided siamese fusion network for change detection of remote sensing images. ||| puhua chen ||| lei guo ||| xiangrong zhang ||| alex kai qin ||| wentao ma ||| licheng jiao ||| 
2021 ||| transformer-based decoder designs for semantic segmentation on remotely sensed images. ||| teerapong panboonyuen ||| kulsawasd jitkajornwanich ||| siam lawawirojwong ||| panu srestasathiern ||| peerapon vateekul ||| 
2021 ||| looking for change? roll the dice and demand attention. ||| foivos i. diakogiannis ||| fran ||| ois waldner ||| peter caccetta ||| 
2021 ||| hybrid dense network with dual attention for hyperspectral image classification. ||| jinling zhao ||| lei hu ||| yingying dong ||| linsheng huang ||| 
2022 ||| da-imrn: dual-attention-guided interactive multi-scale residual network for hyperspectral image classification. ||| liang zou ||| zhifan zhang ||| haijia du ||| meng lei ||| yong xue ||| z. jane wang ||| 
2021 ||| a spatial-channel collaborative attention network for enhancement of multiresolution classification. ||| wenping ma ||| jiliang zhao ||| hao zhu ||| jianchao shen ||| licheng jiao ||| yue wu ||| biao hou ||| 
2021 ||| efficient transformer for remote sensing image segmentation. ||| zhiyong xu ||| weicun zhang ||| tianxiang zhang ||| zhifang yang ||| jiangyun li ||| 
2019 ||| building extraction from very high resolution aerial imagery using joint attention deep neural network. ||| ziran ye ||| yongyong fu ||| muye gan ||| jinsong deng ||| alexis j. comber ||| ke wang ||| 
2020 ||| transboundary basins need more attention: anthropogenic impacts on land cover changes in aras river basin, monitoring and prediction. ||| sajad khoshnoodmotlagh ||| jochem verrelst ||| alireza daneshi ||| mohsen mirzaei ||| hossein azadi ||| mohammad haghighi ||| masoud hatamimanesh ||| safar marofi ||| 
2022 ||| voids filling of dem with multiattention generative adversarial network model. ||| guoqing zhou ||| bo song ||| peng liang ||| jiasheng xu ||| tao yue ||| 
2021 ||| real-time underwater maritime object detection in side-scan sonar images based on transformer-yolov5. ||| yongcan yu ||| jianhu zhao ||| quanhua gong ||| chao huang ||| gen zheng ||| jinye ma ||| 
2021 ||| wildfire segmentation using deep vision transformers. ||| rafik ghali ||| moulay a. akhloufi ||| marwa jmal ||| wided souid ||| ne mseddi ||| rabah attia ||| 
2020 ||| da-capsunet: a dual-attention capsule u-net for road extraction from remote sensing imagery. ||| yongfeng ren ||| yongtao yu ||| haiyan guan ||| 
2021 ||| building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets. ||| chuyi wu ||| feng zhang ||| junshi xia ||| yichen xu ||| guoqing li ||| jibo xie ||| zhenhong du ||| renyi liu ||| 
2021 ||| building extraction from remote sensing images with sparse token transformers. ||| keyan chen ||| zhengxia zou ||| zhenwei shi ||| 
2021 ||| light-weight cloud detection network for optical remote sensing images with attention-based deeplabv3+ architecture. ||| xudong yao ||| qing guo ||| an li ||| 
2019 ||| description generation for remote sensing images using attribute attention mechanism. ||| xiangrong zhang ||| xin wang ||| xu tang ||| huiyu zhou ||| chen li ||| 
2020 ||| a new deep learning network for automatic bridge detection from sar images based on balanced and attention mechanism. ||| lifu chen ||| ting weng ||| jin xing ||| zhouhao pan ||| zhihui yuan ||| xuemin xing ||| peng zhang ||| 
2021 ||| residual augmented attentional u-shaped network for spectral reconstruction from rgb images. ||| jiaojiao li ||| chaoxiong wu ||| rui song ||| yunsong li ||| weiying xie ||| 
2021 ||| converting optical videos to infrared videos using attention gan and its impact on target detection and classification performance. ||| mohammad shahabuddin ||| md reshad ul hoque ||| kazi aminul islam ||| chiman kwan ||| david gribben ||| jiang li ||| 
2021 ||| hyperspectral image classification based on multi-scale residual network with attention mechanism. ||| yuhao qing ||| wenyi liu ||| 
2020 ||| radet: refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images. ||| yangyang li ||| qin huang ||| xuan pei ||| licheng jiao ||| ronghua shang ||| 
2021 ||| compound multiscale weak dense network with hybrid attention for hyperspectral image classification. ||| zixian ge ||| guo cao ||| hao shi ||| youqiang zhang ||| xuesong li ||| peng fu ||| 
2021 ||| eaau-net: enhanced asymmetric attention u-net for infrared small target detection. ||| xiaozhong tong ||| bei sun ||| junyu wei ||| zhen zuo ||| shaojing su ||| 
2019 ||| lam: remote sensing image captioning with label-attention mechanism. ||| zhengyuan zhang ||| wenhui diao ||| wenkai zhang ||| menglong yan ||| xin gao ||| xian sun ||| 
2019 ||| smokenet: satellite smoke scene detection using convolutional neural network with spatial and channel-wise attention. ||| rui ba ||| chen chen ||| jing yuan ||| weiguo song ||| siuming lo ||| 
2020 ||| classification of hyperspectral image based on double-branch dual-attention mechanism network. ||| rui li ||| shunyi zheng ||| chenxi duan ||| yang yang ||| xiqi wang ||| 
2020 ||| kda3d: key-point densification and multi-attention guidance for 3d object detection. ||| jiarong wang ||| ming zhu ||| bo wang ||| deyao sun ||| hua wei ||| changji liu ||| hai-tao nie ||| 
2020 ||| compact cloud detection with bidirectional self-attention knowledge distillation. ||| yajie chai ||| kun fu ||| xian sun ||| wenhui diao ||| zhiyuan yan ||| yingchao feng ||| lei wang ||| 
2021 ||| lightweight underwater object detection based on yolo v4 and multi-scale attentional feature fusion. ||| minghua zhang ||| shubo xu ||| wei song ||| qi he ||| quanmiao wei ||| 
2020 ||| deep discriminative representation learning with attention map for scene classification. ||| jun li ||| daoyu lin ||| yang wang ||| guangluan xu ||| yunyan zhang ||| chibiao ding ||| yanhai zhou ||| 
2021 ||| subtask attention based object detection in remote sensing images. ||| shengzhou xiong ||| yihua tan ||| yansheng li ||| cai wen ||| pei yan ||| 
2019 ||| attention graph convolution network for image segmentation in big sar imagery data. ||| fei ma ||| fei gao ||| jinping sun ||| huiyu zhou ||| amir hussain ||| 
2021 ||| semantic-guided attention refinement network for salient object detection in optical remote sensing images. ||| zhou huang ||| huai-xin chen ||| bi-yuan liu ||| zhixi wang ||| 
2021 ||| ship detection via dilated rate search and attention-guided feature representation. ||| jianming hu ||| xiyang zhi ||| tianjun shi ||| lijian yu ||| wei zhang ||| 
2021 ||| a novel lstm model with interaction dual attention for radar echo extrapolation. ||| chuyao luo ||| xutao li ||| yongliang wen ||| yunming ye ||| xiaofeng zhang ||| 
2021 ||| a fast aircraft detection method for sar images based on efficient bidirectional path aggregated attention network. ||| ru luo ||| lifu chen ||| jin xing ||| zhihui yuan ||| siyu tan ||| xingmin cai ||| jielan wang ||| 
2019 ||| double-branch multi-attention mechanism network for hyperspectral image classification. ||| wenping ma ||| qifan yang ||| yue wu ||| wei zhao ||| xiangrong zhang ||| 
2022 ||| an adaptive attention fusion mechanism convolutional network for object detection in remote sensing images. ||| yuanxin ye ||| xiaoyue ren ||| bai zhu ||| tengfeng tang ||| xin tan ||| yang gui ||| qin yao ||| 
2021 ||| ams-net: an attention-based multi-scale network for classification of 3d terracotta warrior fragments. ||| jie liu ||| xin cao ||| pingchuan zhang ||| xueli xu ||| yangyang liu ||| guohua geng ||| fengjun zhao ||| kang li ||| mingquan zhou ||| 
2021 ||| a multi-scale spatial attention region proposal network for high-resolution optical remote sensing imagery. ||| ruchan dong ||| licheng jiao ||| yan zhang ||| jin zhao ||| weiyan shen ||| 
2021 ||| mra-snet: siamese networks of multiscale residual and attention for change detection in high-resolution remote sensing images. ||| xin yang ||| lei hu ||| yongmei zhang ||| yunqing li ||| 
2021 ||| attention enhanced u-net for building extraction from farmland based on google and worldview-2 remote sensing images. ||| chuangnong li ||| lin fu ||| qing zhu ||| jun zhu ||| zheng fang ||| yakun xie ||| yukun guo ||| yuhang gong ||| 
2018 ||| attention-mechanism-containing neural networks for high-resolution remote sensing image classification. ||| rudong xu ||| yiting tao ||| zhongyuan lu ||| yanfei zhong ||| 
2020 ||| a multi-level attention model for remote sensing image captions. ||| yangyang li ||| shuangkang fang ||| licheng jiao ||| ruijiao liu ||| ronghua shang ||| 
2021 ||| a spectral spatial attention fusion with deformable convolutional residual network for hyperspectral image classification. ||| tianyu zhang ||| cuiping shi ||| diling liao ||| liguo wang ||| 
2020 ||| csa-mso3dcnn: multiscale octave 3d cnn with channel and spatial attention for hyperspectral image classification. ||| qin xu ||| yong xiao ||| dongyue wang ||| bin luo ||| 
2021 ||| ecap-yolo: efficient channel attention pyramid yolo for small object detection in aerial image. ||| munhyeong kim ||| jongmin jeong ||| sungho kim ||| 
2020 ||| residual group channel and space attention network for hyperspectral image classification. ||| peida wu ||| ziguan cui ||| zongliang gan ||| feng liu ||| 
2021 ||| pcan - part-based context attention network for thermal power plant detection in remote sensing imagery. ||| wenxin yin ||| wenhui diao ||| peijin wang ||| xin gao ||| ya li ||| xian sun ||| 
2021 ||| dual attention feature fusion and adaptive context for accurate segmentation of very high-resolution remote sensing images. ||| hao shi ||| jiahe fan ||| yupei wang ||| liang chen ||| 
2019 ||| hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism. ||| bei fang ||| ying li ||| haokui zhang ||| jonathan cheung-wai chan ||| 
2022 ||| a lightweight convolutional neural network based on group-wise hybrid attention for remote sensing scene classification. ||| cuiping shi ||| xinlei zhang ||| jingwei sun ||| liguo wang ||| 
2021 ||| hyperspectral image classification based on two-branch spectral-spatial-feature attention network. ||| hanjie wu ||| dan li ||| yujian wang ||| xiaojun li ||| fanqiang kong ||| qiang wang ||| 
2021 ||| a residual attention and local context-aware network for road extraction from high-resolution remote sensing imagery. ||| ziwei liu ||| mingchang wang ||| fengyan wang ||| xue ji ||| 
2021 ||| nightlight as a proxy of economic indicators: fine-grained gdp inference around mainland china via attention-augmented cnn from daytime satellite imagery. ||| haoyu liu ||| xianwen he ||| yanbing bai ||| xing liu ||| yilin wu ||| yanyun zhao ||| hanfang yang ||| 
2022 ||| multiscale object detection in remote sensing images combined with multi-receptive-field features and relation-connected attention. ||| jiahang liu ||| donghao yang ||| fei hu ||| 
2021 ||| a novel ensemble architecture of residual attention-based deep metric learning for remote sensing image retrieval. ||| qimin cheng ||| deqiao gan ||| peng fu ||| haiyan huang ||| yuzhuo zhou ||| 
2020 ||| attention-guided multi-scale segmentation neural network for interactive extraction of region objects from high-resolution satellite imagery. ||| kun li ||| xiangyun hu ||| huiwei jiang ||| zhen shu ||| mi zhang ||| 
2021 ||| remote sensing image scene classification based on global self-attention module. ||| qingwen li ||| dongmei yan ||| wanrong wu ||| 
2021 ||| gacm: a graph attention capsule model for the registration of tls point clouds in the urban scene. ||| jianjun zou ||| zhenxin zhang ||| dong chen ||| qinghua li ||| lan sun ||| ruofei zhong ||| liqiang zhang ||| jinghan sha ||| 
2022 ||| a bidirectional deep-learning-based spectral attention mechanism for hyperspectral data classification. ||| bishwas praveen ||| vineetha menon ||| 
2020 ||| object tracking in unmanned aerial vehicle videos via multifeature discrimination and instance-aware attention network. ||| shiyu zhang ||| li zhuo ||| hui zhang ||| jiafeng li ||| 
2021 ||| transformer meets convolution: a bilateral awareness network for semantic segmentation of very fine resolution urban scene images. ||| libo wang ||| rui li ||| dongzhi wang ||| chenxi duan ||| teng wang ||| xiaoliang meng ||| 
2021 ||| crop type mapping from optical and radar time series using attention-based deep learning. ||| stella ofori-ampofo ||| charlotte pelletier ||| stefan lang ||| 
2021 ||| a dual-model architecture with grouping-attention-fusion for remote sensing scene classification. ||| junge shen ||| tong zhang ||| yichen wang ||| ruxin wang ||| qi wang ||| min qi ||| 
2018 ||| building extraction in very high resolution imagery by dense-attention networks. ||| hui yang ||| penghai wu ||| xuedong yao ||| yanlan wu ||| biao wang ||| yongyang xu ||| 
2021 ||| dganet: a dilated graph attention-based network for local feature extraction on 3d point clouds. ||| jie wan ||| zhong xie ||| yongyang xu ||| ziyin zeng ||| ding yuan ||| qinjun qiu ||| 
2019 ||| mask obb: a semantic attention-based mask oriented bounding box representation for multi-category object detection in aerial images. ||| jinwang wang ||| jian ding ||| haowen guo ||| wensheng cheng ||| ting pan ||| wen yang ||| 
2021 ||| triple-attention-based parallel network for hyperspectral image classification. ||| lei qu ||| xingliang zhu ||| jiannan zheng ||| liang zou ||| 
2021 ||| semantic segmentation of urban buildings using a high-resolution network (hrnet) with channel and spatial attention gates. ||| seonkyeong seong ||| jaewan choi ||| 
2021 ||| flood discharge prediction based on remote-sensed spatiotemporal features fusion and graph attention. ||| chen chen ||| dingbin luan ||| song zhao ||| zhan liao ||| yang zhou ||| jiange jiang ||| qingqi pei ||| 
2020 ||| a cloud detection method using convolutional neural network based on gabor transform and attention mechanism with dark channel subnet for remote sensing image. ||| jing zhang ||| qin zhou ||| jun wu ||| yuchen wang ||| hui wang ||| yunsong li ||| yuzhou chai ||| yang liu ||| 
2021 ||| msnet: a multi-stream fusion network for remote sensing spatiotemporal fusion based on transformer and convolution. ||| weisheng li ||| dongwen cao ||| yidong peng ||| chao yang ||| 
2021 ||| ha-net: a lake water body extraction network based on hybrid-scale attention and transfer learning. ||| zhaobin wang ||| xiong gao ||| yaonan zhang ||| 
2020 ||| retraction: zhu r. et al. attention-based deep feature fusion for the scene classification of high-resolution remote sensing images. remote sensing. 2019 11(17), 1996. ||| 
2018 ||| remote sensing scene classification based on convolutional neural networks pre-trained using attention-guided sparse filters. ||| jingbo chen ||| chengyi wang ||| zhong ma ||| jiansheng chen ||| dong-xu he ||| stephen ackland ||| 
2020 ||| an efficient method for infrared and visual images fusion based on visual attention technique. ||| yaochen liu ||| lili dong ||| yang chen ||| wenhai xu ||| 
2020 ||| multi-image super resolution of remotely sensed images using residual attention deep neural networks. ||| francesco salvetti ||| vittorio mazzia ||| aleem khaliq ||| marcello chiaberge ||| 
2022 ||| transformer neural network for weed and crop classification of high resolution uav images. ||| reenul reedha ||| eric dericquebourg ||| rapha ||| l canals ||| adel hafiane ||| 
2020 ||| attention-based residual network with scattering transform features for hyperspectral unmixing with limited training samples. ||| yiliang zeng ||| christian ritz ||| jiahong zhao ||| jinhui lan ||| 
2020 ||| attention-based pyramid network for segmentation and classification of high-resolution and hyperspectral remote sensing images. ||| qingsong xu ||| xin yuan ||| chaojun ouyang ||| yue zeng ||| 
2022 ||| transformer with transfer cnn for remote-sensing-image object detection. ||| qingyun li ||| yushi chen ||| ying zeng ||| 
2021 ||| few-shot object detection on remote sensing images via shared attention module and balanced fine-tuning strategy. ||| xu huang ||| bokun he ||| ming tong ||| dingwen wang ||| chu he ||| 
2021 ||| ebarec-bs: effective band attention reconstruction network for hyperspectral imagery band selection. ||| yufei liu ||| xiaorun li ||| ziqiang hua ||| liaoying zhao ||| 
2019 ||| hyperspectral image super-resolution with 1d-2d attentional convolutional neural network. ||| jiaojiao li ||| ruxing cui ||| bo li ||| rui song ||| yunsong li ||| qian du ||| 
2021 ||| improved singan integrated with an attentional mechanism for remote sensing image classification. ||| songwei gu ||| rui zhang ||| hongxia luo ||| mengyao li ||| huamei feng ||| xuguang tang ||| 
2022 ||| structural attention enhanced continual meta-learning for graph edge labeling based few-shot remote sensing scene classification. ||| feimo li ||| shuaibo li ||| xinxin fan ||| xiong li ||| hongxing chang ||| 
2021 ||| self-attention in reconstruction bias u-net for semantic segmentation of building rooftops in optical remote sensing images. ||| ziyi chen ||| dilong li ||| wentao fan ||| haiyan guan ||| cheng wang ||| jonathan li ||| 
2021 ||| msst-net: a multi-scale adaptive network for building extraction from remote sensing images based on swin transformer. ||| wei yuan ||| wenbo xu ||| 
2020 ||| a spatial-temporal attention-based method and a new dataset for remote sensing image change detection. ||| hao chen ||| zhenwei shi ||| 
2021 ||| spectral and spatial global context attention for hyperspectral image classification. ||| zhongwei li ||| xingshuai cui ||| leiquan wang ||| hao zhang ||| xue zhu ||| yajing zhang ||| 
2021 ||| a multi-branch feature fusion strategy based on an attention mechanism for remote sensing image scene classification. ||| cuiping shi ||| xin zhao ||| liguo wang ||| 
2021 ||| an improved swin transformer-based model for remote sensing object detection and instance segmentation. ||| xiangkai xu ||| zhejun feng ||| changqing cao ||| mengyuan li ||| jin wu ||| zengyan wu ||| yajie shang ||| shubing ye ||| 
2021 ||| synergistic attention for ship instance segmentation in sar images. ||| danpei zhao ||| chunbo zhu ||| jing qi ||| xinhu qi ||| zhenhua su ||| zhenwei shi ||| 
2020 ||| building extraction based on u-net with an attention block and multiple losses. ||| mingqiang guo ||| heng liu ||| yongyang xu ||| ying huang ||| 
2021 ||| aau-net: attention-based asymmetric u-net for subject-sensitive hashing of remote sensing images. ||| kaimeng ding ||| shiping chen ||| yu wang ||| yueming liu ||| yue zeng ||| jin tian ||| 
2021 ||| remote sensing time series classification based on self-attention mechanism and time sequence enhancement. ||| jingwei liu ||| jining yan ||| lizhe wang ||| liang huang ||| haixu he ||| hong liu ||| 
2020 ||| pga-siamnet: pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection. ||| huiwei jiang ||| xiangyun hu ||| kun li ||| jinming zhang ||| jinqi gong ||| mi zhang ||| 
2021 ||| attention-guided multispectral and panchromatic image classification. ||| cheng shi ||| yenan dang ||| li fang ||| zhiyong lv ||| huifang shen ||| 
2020 ||| mffa-sarnet: deep transferred multi-level feature fusion attention network with dual optimized loss for small-sample sar atr. ||| yikui zhai ||| wenbo deng ||| tian lan ||| bing sun ||| zilu ying ||| junying gan ||| chaoyun mai ||| jingwen li ||| ruggero donida labati ||| vincenzo piuri ||| fabio scotti ||| 
2021 ||| adt-det: adaptive dynamic refined single-stage transformer detector for arbitrary-oriented object detection in satellite optical imagery. ||| yongbin zheng ||| peng sun ||| zongtan zhou ||| wanying xu ||| qiang ren ||| 
2020 ||| anchor-free convolutional network with dense attention feature aggregation for ship detection in sar images. ||| fei gao ||| yishan he ||| jun wang ||| amir hussain ||| huiyu zhou ||| 
2019 ||| transferred multi-perception attention networks for remote sensing image super-resolution. ||| xiaoyu dong ||| zhihong xi ||| xu sun ||| lianru gao ||| 
2020 ||| gsca-unet: towards automatic shadow detection in urban aerial imagery with global-spatial-context attention module. ||| yuwei jin ||| wenbo xu ||| zhongwen hu ||| haitao jia ||| xin luo ||| donghang shao ||| 
2021 ||| pyramid information distillation attention network for super-resolution reconstruction of remote sensing images. ||| bo huang ||| zhiming guo ||| liaoni wu ||| boyong he ||| xianjiang li ||| yuxing lin ||| 
2020 ||| amn: attention metric network for one-shot remote sensing image scene classification. ||| xirong li ||| fangling pu ||| rui yang ||| rong gui ||| xin xu ||| 
2021 ||| remote sensing image super-resolution based on dense channel attention network. ||| yunchuan ma ||| pengyuan lv ||| hao liu ||| xuehong sun ||| yanfei zhong ||| 
2021 ||| spatial-spectral transformer for hyperspectral image classification. ||| xin he ||| yushi chen ||| zhouhan lin ||| 
2021 ||| a deformable convolutional neural network with spatial-channel attention for remote sensing scene classification. ||| di wang ||| jinhui lan ||| 
2019 ||| attention matters: an exploration of relationship between google search behaviors and crude oil prices. ||| xin li ||| xun zhang ||| shouyang wang ||| jian ma ||| 
2021 ||| motor fault diagnosis algorithm based on wavelet and attention mechanism. ||| yong yan ||| qiang liu ||| xiaoqin gao ||| 
2021 ||| pedestrian reidentification algorithm based on deconvolution network feature extraction-multilayer attention mechanism convolutional neural network. ||| fengping an ||| jun-e liu ||| lei bai ||| 
2019 ||| reverse engineering for the design patterns extraction of android mobile applications for attention deficit disorder. ||| ngela villareal-freire ||| andr ||| s f. aguirre ||| c ||| sar alberto collazos ord ||| ez ||| 
2022 ||| flicker phase-noise reduction using gate-drain phase shift in transformer-based oscillators. ||| xi chen ||| yizhe hu ||| teerachot siriburanon ||| jianglin du ||| robert bogdan staszewski ||| anding zhu ||| 
2021 ||| frequency selective impedance transformer with high-impedance transforming ratio and extremely high/low termination impedances. ||| yongchae jeong ||| girdhari chaudhary ||| phirun kim ||| 
2021 ||| a compact transformer-based fractional-n adpll in 10-nm finfet cmos. ||| chao-chieh li ||| min-shueh yuan ||| chia-chun liao ||| chih-hsien chang ||| yu-tso lin ||| tsung-hsien tsai ||| tien-chien huang ||| hsien-yuan liao ||| chung-ting lu ||| hung-yi kuo ||| augusto ronchini ximenes ||| robert bogdan staszewski ||| 
2021 ||| evaluation of static synchronous compensator and rail power conditioner in electrified railway systems using v/v and scott power transformers. ||| luis a. m. barros ||| mohamed tanta ||| ant ||| nio p. martins ||| jo ||| o luiz afonso ||| j. g. pinto ||| 
2020 ||| graph attention network for detecting license plates in crowded street scenes. ||| pinaki nath chowdhury ||| palaiahnakote shivakumara ||| swati kanchan ||| ramachandra raghavendra ||| umapada pal ||| tong lu ||| daniel lopresti ||| 
2018 ||| explicit ensemble attention learning for improving visual question answering. ||| vasileios lioutas ||| nikolaos passalis ||| anastasios tefas ||| 
2020 ||| spatio-temporal fall event detection in complex scenes using attention guided lstm. ||| qi feng ||| chenqiang gao ||| lan wang ||| yue zhao ||| tiecheng song ||| qiang li ||| 
2020 ||| partial attention and multi-attribute learning for vehicle re-identification. ||| saifullah tumrani ||| zhiyi deng ||| haoyang lin ||| jie shao ||| 
2020 ||| video captioning with text-based dynamic attention and step-by-step learning. ||| huanhou xiao ||| jinglun shi ||| 
2021 ||| attention fusion network for multi-spectral semantic segmentation. ||| jiangtao xu ||| kaige lu ||| han wang ||| 
2019 ||| dual-supervised attention network for deep cross-modal hashing. ||| hanyu peng ||| junjun he ||| shifeng chen ||| yali wang ||| yu qiao ||| 
2018 ||| boosting image classification through semantic attention filtering strategies. ||| eduardo fidalgo ||| enrique alegre ||| v ||| ctor gonz ||| lez-castro ||| laura fern ||| ndez-robles ||| 
2021 ||| image captioning with transformer and knowledge graph. ||| yu zhang ||| xinyu shi ||| siya mi ||| xu yang ||| 
2019 ||| learning part-aware attention networks for kinship verification. ||| haibin yan ||| shiwei wang ||| 
2021 ||| msar-net: multi-scale attention based light-weight image super-resolution. ||| nancy mehta ||| subrahmanyam murala ||| 
2022 ||| hmfca-net: hierarchical multi-frequency based channel attention net for mobile phone surface defect detection. ||| ying zhu ||| runwei ding ||| weibo huang ||| peng wei ||| ge yang ||| yong wang ||| 
2021 ||| snipedet: attention-guided pyramidal prediction kernels for generic object detection. ||| suting chen ||| zehua cheng ||| liangchen zhang ||| yujie zheng ||| 
2018 ||| fusion-attention network for person search with free-form natural language. ||| zhong ji ||| shengjia li ||| yanwei pang ||| 
2021 ||| self-attention binary neural tree for video summarization. ||| hao fu ||| hongxing wang ||| 
2021 ||| clothes image caption generation with attribute detection and visual attention model. ||| xianrui li ||| zhiling ye ||| zhao zhang ||| mingbo zhao ||| 
2020 ||| attention-aware invertible hashing network with skip connections. ||| shanshan li ||| qiang cai ||| zhuangzi li ||| haisheng li ||| naiguang zhang ||| xiaoyu zhang ||| 
2021 ||| trseg: transformer for semantic segmentation. ||| youngsaeng jin ||| david k. han ||| hanseok ko ||| 
2020 ||| can-gan: conditioned-attention normalized gan for face age synthesis. ||| chenglong shi ||| jiachao zhang ||| yazhou yao ||| yunlian sun ||| huaming rao ||| xiangbo shu ||| 
2021 ||| self-attention-based conditional random fields latent variables model for sequence labeling. ||| yinan shao ||| jerry chun-wei lin ||| gautam srivastava ||| alireza jolfaei ||| dongdong guo ||| yi hu ||| 
2021 ||| laryngoscope8: laryngeal image dataset and classification of laryngeal disease based on attention mechanism. ||| li yin ||| yang liu ||| mingtao pei ||| jinrang li ||| mukun wu ||| yuanyuan jia ||| 
2018 ||| multimodal architecture for video captioning with memory networks and an attention mechanism. ||| wei li ||| dashan guo ||| xiangzhong fang ||| 
2020 ||| attention guided neural network models for occluded pedestrian detection. ||| tengtao zou ||| shangming yang ||| yun zhang ||| mao ye ||| 
2020 ||| object detection with class aware region proposal network and focused attention objective. ||| xiaoyu tao ||| yihong gong ||| weiwei shi ||| de cheng ||| 
2020 ||| learning cross-modal correlations by exploring inter-word semantics and stacked co-attention. ||| jing yu ||| yuhang lu ||| weifeng zhang ||| zengchang qin ||| yanbing liu ||| yue hu ||| 
2020 ||| enhanced factorization machine via neural pairwise ranking and attention networks. ||| yonghong yu ||| lihong jiao ||| ningning zhou ||| li zhang ||| hongzhi yin ||| 
2020 ||| single-image raindrop removal using concurrent channel-spatial attention and long-short skip connections. ||| jiayi peng ||| yong xu ||| tianyi chen ||| yan huang ||| 
2021 ||| document-level relation extraction via graph transformer networks and temporal convolutional networks. ||| yong shi ||| yang xiao ||| pei quan ||| minglong lei ||| lingfeng niu ||| 
2020 ||| sahan: scale-aware hierarchical attention network for scene text recognition. ||| jiaxin zhang ||| canjie luo ||| lianwen jin ||| tianwei wang ||| ziyan li ||| weiying zhou ||| 
2017 ||| human attribute recognition by refining attention heat map. ||| hao guo ||| xiaochuan fan ||| song wang ||| 
2020 ||| semantically consistent text to fashion image synthesis with an enhanced attentional generative adversarial network. ||| kenan e. ak ||| joo hwee lim ||| jo yew tham ||| ashraf a. kassim ||| 
2020 ||| diablo: dictionary-based attention block for deep metric learning. ||| pierre jacob ||| david picard ||| aymeric histace ||| edouard klein ||| 
2021 ||| triplet interactive attention network for cross-modality person re-identification. ||| chenrui zhang ||| ping chen ||| tao lei ||| hongying meng ||| 
2021 ||| landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition. ||| darshan gera ||| s. balasubramanian ||| 
2020 ||| hierarchical attention network for action segmentation. ||| harshala gammulle ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2021 ||| scanet: a spatial and channel attention based network for partial-to-partial point cloud registration. ||| ruqin zhou ||| xixing li ||| wanshou jiang ||| 
2020 ||| adversarial learning based attentional scene text recognizer. ||| jinyuan zhao ||| yanna wang ||| baihua xiao ||| cunzhao shi ||| jingzhong jiang ||| chunheng wang ||| 
2019 ||| video-based person re-identification via spatio-temporal attentional and two-stream fusion convolutional networks. ||| deqiang ouyang ||| yonghui zhang ||| jie shao ||| 
2021 ||| smaat-unet: precipitation nowcasting using a small attention-unet architecture. ||| kevin trebing ||| tomasz stanczyk ||| siamak mehrkanoon ||| 
2021 ||| human motion reconstruction using deep transformer networks. ||| seong uk kim ||| hanyoung jang ||| hyeonseung im ||| jongmin kim ||| 
2022 ||| deep relational self-attention networks for scene graph generation. ||| ping li ||| zhou yu ||| yibing zhan ||| 
2021 ||| midcan: a multiple input deep convolutional attention network for covid-19 diagnosis based on chest ct and chest x-ray. ||| yu-dong zhang ||| zheng zhang ||| xin zhang ||| shui-hua wang ||| 
2019 ||| quantifying patterns of joint attention during human-robot interactions: an application for autism spectrum disorder assessment. ||| salvatore maria anzalone ||| jean xavier ||| sofiane boucenna ||| lucia billeci ||| antonio narzisi ||| filippo muratori ||| david cohen ||| mohamed chetouani ||| 
2021 ||| adversarial robustness via attention transfer. ||| zhuorong li ||| chao feng ||| minghui wu ||| hongchuan yu ||| jianwei zheng ||| fanwei zhu ||| 
2018 ||| move, attend and predict: an attention-based neural model for people's movement prediction. ||| abdulrahman al-molegi ||| mohammed jabreel ||| antoni mart ||| nez-ballest ||| 
2020 ||| an attention-based row-column encoder-decoder model for text recognition in japanese historical documents. ||| nam tuan ly ||| cuong tuan nguyen ||| masaki nakagawa ||| 
2019 ||| image-attribute reciprocally guided attention network for pedestrian attribute recognition. ||| zhong ji ||| erlu he ||| haoran wang ||| aiping yang ||| 
2019 ||| multi-level attention model for person re-identification. ||| yichao yan ||| bingbing ni ||| jinxian liu ||| xiaokang yang ||| 
2020 ||| constraint saliency based intelligent camera for enhancing viewers attention towards intended face. ||| ravi kant kumar ||| jogendra garain ||| dakshina ranjan kisku ||| goutam sanyal ||| 
2020 ||| movie fill in the blank by joint learning from video and text with adaptive temporal attention. ||| jie chen ||| jie shao ||| chengkun he ||| 
2020 ||| pedestrian attribute recognition based on multiple time steps attention. ||| zhong ji ||| zhenfei hu ||| erlu he ||| jungong han ||| yanwei pang ||| 
2022 ||| attention guided deep features for accurate body mass index estimation. ||| zhi jin ||| junjia huang ||| aolin xiong ||| yuxian pang ||| wenjin wang ||| beichen ding ||| 
2022 ||| sam-net: semantic probabilistic and attention mechanisms of dynamic objects for self-supervised depth and camera pose estimation in visual odometry applications. ||| binchao yang ||| xinying xu ||| jinchang ren ||| lan cheng ||| lei guo ||| zhe zhang ||| 
2022 ||| edge detection with attention: from global view to local focus. ||| huajun liu ||| zuyuan yang ||| haofeng zhang ||| cailing wang ||| 
2019 ||| assessment of feature fusion strategies in visual attention mechanism for saliency detection. ||| muwei jian ||| quan zhou ||| chaoran cui ||| xiushan nie ||| hanjiang luo ||| jianli zhao ||| yilong yin ||| 
2022 ||| category attention transfer for efficient fine-grained visual categorization. ||| qiyu liao ||| dadong wang ||| min xu ||| 
2020 ||| thorax disease classification with attention guided convolutional neural network. ||| qingji guan ||| yaping huang ||| zhun zhong ||| zhedong zheng ||| liang zheng ||| yi yang ||| 
2020 ||| visual question answering with attention transfer and a cross-modal gating mechanism. ||| wei li ||| jianhui sun ||| ge liu ||| linglan zhao ||| xiangzhong fang ||| 
2021 ||| adaptive hybrid attention network for hyperspectral image classification. ||| shivam pande ||| biplab banerjee ||| 
2020 ||| feature fusion network based on attention mechanism for 3d semantic segmentation of point clouds. ||| heng zhou ||| zhijun fang ||| yongbin gao ||| bo huang ||| cengsi zhong ||| ruoxi shang ||| 
2021 ||| mixed pooling and richer attention feature fusion for crack detection. ||| qiang zhou ||| zhong qu ||| chong cao ||| 
2018 ||| joint spatial-temporal attention for action recognition. ||| tingzhao yu ||| chaoxu guo ||| lingfeng wang ||| huxiang gu ||| shiming xiang ||| chunhong pan ||| 
2020 ||| multi-label chest x-ray image classification via category-wise residual attention learning. ||| qingji guan ||| yaping huang ||| 
2021 ||| 3d dental model segmentation with graph attentional convolution network. ||| yue zhao ||| lingming zhang ||| chongshi yang ||| yingyun tan ||| yang liu ||| pengcheng li ||| tianhao huang ||| chenqiang gao ||| 
2020 ||| refineu-net: improved u-net with progressive global feedbacks and residual attention guided local refinement for medical image segmentation. ||| dongyun lin ||| yiqun li ||| tin lay nwe ||| sheng dong ||| zaw min oo ||| 
2022 ||| bert-kgly: a bidirectional encoder representations from transformers (bert)-based model for predicting lysine glycation site for homo sapiens. ||| yinbo liu ||| yufeng liu ||| gang-ao wang ||| yinchu cheng ||| shoudong bi ||| xiaolei zhu ||| 
2019 ||| pyramid predictive attention network for medical image segmentation. ||| tingxiao yang ||| yuichiro yoshimura ||| akira morita ||| takao namiki ||| toshiya nakaguchi ||| 
2018 ||| deep attention residual hashing. ||| yang li ||| zhuang miao ||| ming he ||| yafei zhang ||| hang li ||| 
2019 ||| attention-guided spatial transformer networks for fine-grained visual recognition. ||| dichao liu ||| yu wang ||| jien kato ||| 
2019 ||| multi-level attention based blstm neural network for biomedical event extraction. ||| xinyu he ||| lishuang li ||| xingchen song ||| degen huang ||| fuji ren ||| 
2020 ||| neural machine translation with target-attention model. ||| mingming yang ||| min zhang ||| kehai chen ||| rui wang ||| tiejun zhao ||| 
2017 ||| an attention-based hybrid neural network for document modeling. ||| dengchao he ||| hongjun zhang ||| wenning hao ||| rui zhang ||| huan hao ||| 
2018 ||| ultra-low field mri food inspection system using hts-squid with flux transformer. ||| saburo tanaka ||| satoshi kawagoe ||| kazuma demachi ||| junichi hatta ||| 
2018 ||| winding ratio design of transformer in equivalent circuit of circular patch array absorber. ||| ryosuke suga ||| tomohiko nakamura ||| daisuke kitahara ||| kiyomichi araki ||| osamu hashimoto ||| 
2018 ||| fully integrated cmos pas with two-winding and single-winding combined transformer for wlan applications. ||| se-eun choi ||| hyunjin ahn ||| hyunsik ryu ||| ilku nam ||| ockgoo lee ||| 
2019 ||| attention-guided region proposal network for pedestrian detection. ||| rui sun ||| huihui wang ||| jun zhang ||| xudong zhang ||| 
2019 ||| channel and frequency attention module for diverse animal sound classification. ||| kyungdeuk ko ||| jaihyun park ||| david k. han ||| hanseok ko ||| 
2019 ||| attention-based dense lstm for speech emotion recognition. ||| yue xie ||| ruiyu liang ||| zhenlin liang ||| li zhao ||| 
2019 ||| spectra restoration of bone-conducted speech via attention-based contextual information and spectro-temporal structure constraint. ||| changyan zheng ||| tieyong cao ||| jibin yang ||| xiongwei zhang ||| meng sun ||| 
2017 ||| image modification based on spatial frequency components for visual attention retargeting. ||| hironori takimoto ||| syuhei hitomi ||| hitoshi yamauchi ||| mitsuyoshi kishihara ||| kensuke okubo ||| 
2021 ||| dissolved gas analysis for transformer fault based on learning spiking neural p system with belief adaboost. ||| xihai zhang ||| gexiang zhang ||| pirthwineel paul ||| jinquan zhang ||| tianbao wu ||| songhai fan ||| xingzhong xiong ||| 
2020 ||| cab u-net: an end-to-end category attention boosting algorithm for segmentation. ||| xiaofeng ding ||| yaxin peng ||| chaomin shen ||| tieyong zeng ||| 
2021 ||| a category attention instance segmentation network for four cardiac chambers segmentation in fetal echocardiography. ||| shan an ||| haogang zhu ||| yuanshuai wang ||| fangru zhou ||| xiaoxue zhou ||| xu yang ||| yingying zhang ||| xiangyu liu ||| zhicheng jiao ||| yihua he ||| 
2020 ||| fusion based on attention mechanism and context constraint for multi-modal brain tumor segmentation. ||| tongxue zhou ||| st ||| phane canu ||| su ruan ||| 
2020 ||| pyramid attention recurrent networks for real-time guidewire segmentation and tracking in intraoperative x-ray fluoroscopy. ||| yan-jie zhou ||| xiao-liang xie ||| xiao-hu zhou ||| shi-qi liu ||| gui-bin bian ||| zeng-guang hou ||| 
2021 ||| three-dimensional breast tumor segmentation on dce-mri with a multilabel attention-guided joint-phase-learning network. ||| mengyun qiao ||| shiteng suo ||| fang cheng ||| jia hua ||| dan xue ||| yi guo ||| jianrong xu ||| yuanyuan wang ||| 
2020 ||| mri image synthesis with dual discriminator adversarial learning and difficulty-aware attention mechanism for hippocampal subfields segmentation. ||| baoqiang ma ||| yan zhao ||| yujing yang ||| xiaohui zhang ||| xiaoxi dong ||| debin zeng ||| siyu ma ||| shuyu li ||| 
2020 ||| multiple sclerosis lesion activity segmentation with attention-guided two-path cnns. ||| nils gessert ||| julia kr ||| ger ||| roland opfer ||| ann-christin ostwaldt ||| praveena manogaran ||| hagen h. kitzler ||| sven schippling ||| alexander schlaefer ||| 
2021 ||| human papilloma virus detection in oropharyngeal carcinomas with in situ hybridisation using hand crafted morphological features and deep central attention residual networks. ||| shereen fouad ||| gabriel landini ||| max robinson ||| tzu-hsi song ||| hisham mehanna ||| 
2021 ||| sentiment classification with adversarial learning and attention mechanism. ||| yueshen xu ||| lei li ||| honghao gao ||| lei hei ||| rui li ||| yihao wang ||| 
2019 ||| a hierarchical recurrent approach to predict scene graphs from a visual-attention-oriented perspective. ||| wenjing gao ||| yonghua zhu ||| wenjun zhang ||| ke zhang ||| honghao gao ||| 
2018 ||| attention recognition in eeg-based affective learning research using cfs+knn algorithm. ||| bin hu ||| xiaowei li ||| shuting sun ||| martyn ratcliffe ||| 
2022 ||| sacall: a neural network basecaller for oxford nanopore sequencing data based on self-attention mechanism. ||| neng huang ||| fan nie ||| peng ni ||| feng luo ||| jianxin wang ||| 
2021 ||| multi-view mammographic density classification by dilated and attention-guided residual learning. ||| cheng li ||| jingxu xu ||| qiegen liu ||| yongjin zhou ||| lisha mou ||| zuhui pu ||| yong xia ||| hairong zheng ||| shanshan wang ||| 
2021 ||| a deep segmentation network of multi-scale feature fusion based on attention mechanism for ivoct lumen contour. ||| chenxi huang ||| yisha lan ||| gaowei xu ||| xiaojun zhai ||| jipeng wu ||| fan lin ||| nianyin zeng ||| qingqi hong ||| e. y. k. ng ||| yonghong peng ||| fei chen ||| guokai zhang ||| 
2021 ||| high-risk prediction of cardiovascular diseases via attention-based deep neural networks. ||| ying an ||| nengjun huang ||| xianlai chen ||| fang-xiang wu ||| jianxin wang ||| 
2022 ||| novel transformer networks for improved sequence labeling in genomics. ||| jim clauwaert ||| willem waegeman ||| 
2021 ||| attention based simplified deep residual network for citywide crowd flows prediction. ||| genan dai ||| xiaoyang hu ||| youming ge ||| zhiqing ning ||| yubao liu ||| 
2022 ||| distant supervised relation extraction based on residual attention. ||| zhiyun zheng ||| yun liu ||| dun li ||| xingjin zhang ||| 
2022 ||| referring image segmentation with attention guided cross modal fusion for semantic oriented languages. ||| qianli zhou ||| rong wang ||| hai-miao hu ||| quange tan ||| wenjin zhang ||| 
2021 ||| using bilstm with attention mechanism to automatically detect self-admitted technical debt. ||| dongjin yu ||| lin wang ||| xin chen ||| jie chen ||| 
2022 ||| polynomial stacked-attention network for nationality classification. ||| kunyan li ||| jie zhang ||| shiguang shan ||| 
2019 ||| codeattention: translating source code to comments by exploiting the code constructs. ||| wenhao zheng ||| hong-yu zhou ||| ming li ||| jianxin wu ||| 
2022 ||| speech-driven facial animation with spectral gathering and temporal attention. ||| yujin chai ||| yanlin weng ||| lvdi wang ||| kun zhou ||| 
2022 ||| visual saliency prediction using multi-scale attention gated network. ||| yubao sun ||| mengyang zhao ||| kai hu ||| shaojing fan ||| 
2022 ||| attention based video captioning framework for hindi. ||| alok singh ||| thoudam doren singh ||| sivaji bandyopadhyay ||| 
2022 ||| scale-aware attention-based multi-resolution representation for multi-person pose estimation. ||| honghong yang ||| longfei guo ||| xiaojun wu ||| yumei zhang ||| 
2022 ||| nasmamsr: a fast image super-resolution network based on neural architecture search and multiple attention mechanism. ||| xin yang ||| jiangfeng fan ||| chenhuan wu ||| dake zhou ||| tao li ||| 
2022 ||| code generation from a graphical user interface via attention-based encoder-decoder model. ||| wen-yin chen ||| pavol podstreleny ||| wen-huang cheng ||| yung-yao chen ||| kai-lung hua ||| 
2019 ||| panorama based on multi-channel-attention cnn for 3d model recognition. ||| weizhi nie ||| kun wang ||| qi liang ||| roubing he ||| 
2020 ||| evaluation of uhf transfer function in a power transformer for real-time partial discharge detection. ||| sathaporn promwong ||| thanadol tiengthong ||| 
2020 ||| library and information science papers discussed on twitter: a new network-based approach for measuring public attention. ||| robin haunschild ||| loet leydesdorff ||| lutz bornmann ||| 
2017 ||| understanding the correlations between social attention and topic trends of scientific publications. ||| xianlei dong ||| jian xu ||| ying ding ||| chenwei zhang ||| kunpeng zhang ||| min song ||| 
2021 ||| a topic detection method based on word-attention networks. ||| zheng xie ||| 
2021 ||| modeling temporal patterns of cyberbullying detection with hierarchical attention networks. ||| lu cheng ||| ruocheng guo ||| yasin n. silva ||| deborah l. hall ||| huan liu ||| 
2018 ||| multi-dgas: a pattern based educational framework design for power transformers faults interpretation and comparative performance analysis. ||| metin varan ||| ulas yurtsever ||| 
2018 ||| gaze-based cursor control impairs performance in divided attention. ||| r ||| bert adrian rill ||| kinga bettina farag ||| 
2019 ||| learners' attention preferences of information in online learning. ||| su mu ||| meng cui ||| xiao jin wang ||| jin xiu qiao ||| dong mei tang ||| 
2022 ||| fetal ecg extraction from maternal ecg using attention-based cyclegan. ||| mohammadreza mohebbian ||| seyed shahim vedaei ||| khan a. wahid ||| anh dinh ||| hamid reza marateb ||| kouhyar tavakolian ||| 
2021 ||| multiscale attention guided network for covid-19 diagnosis using chest x-ray images. ||| jingxiong li ||| yaqi wang ||| shuai wang ||| jun wang ||| jun liu ||| qun jin ||| lingling sun ||| 
2022 ||| deep attention and graphical neural network for multiple sclerosis lesion segmentation from mr imaging sequences. ||| zhanlan chen ||| xiuying wang ||| jing huang ||| jie lu ||| jiangbin zheng ||| 
2022 ||| task-induced pyramid and attention gan for multimodal brain image imputation and classification in alzheimer's disease. ||| xingyu gao ||| feng shi ||| dinggang shen ||| manhua liu ||| 
2021 ||| unsupervised self-adaptive auditory attention decoding. ||| simon geirnaert ||| tom francart ||| alexander bertrand ||| 
2021 ||| graph convolutional autoencoder and fully-connected autoencoder with attention mechanism based method for predicting drug-disease associations. ||| ping xuan ||| ling gao ||| nan sheng ||| tiangang zhang ||| toshiya nakaguchi ||| 
2020 ||| hard attention net for automatic retinal vessel segmentation. ||| dongyi wang ||| ayman haytham ||| jessica pottenburgh ||| osamah saeedi ||| yang tao ||| 
2022 ||| aprnet: a 3d anisotropic pyramidal reversible network with multi-modal cross-dimension attention for brain tissue segmentation in mr images. ||| yuzhou zhuang ||| hong liu ||| enmin song ||| guangzhi ma ||| xiangyang xu ||| chih-cheng hung ||| 
2020 ||| prediction of reaction time and vigilance variability from spatio-spectral features of resting-state eeg in a long sustained attention task. ||| mastaneh torkamani-azar ||| sumeyra demir kanik ||| serap aydin ||| m ||| jdat  ||| etin ||| 
2022 ||| cross-model attention-guided tumor segmentation for 3d automated breast ultrasound (abus) images. ||| yue zhou ||| houjin chen ||| yanfeng li ||| xuyang cao ||| shu wang ||| dinggang shen ||| 
2021 ||| bidirectional representation learning from transformers using multimodal electronic health record data to predict depression. ||| yiwen meng ||| william speier ||| michael k. ong ||| corey w. arnold ||| 
2021 ||| attention-aware residual network based manifold learning for white blood cells classification. ||| pu huang ||| jing wang ||| jian zhang ||| yajuan shen ||| cong liu ||| weiqing song ||| shangshang wu ||| yuwei zuo ||| zhiming lu ||| dengwang li ||| 
2021 ||| multi-scale self-guided attention for medical image segmentation. ||| ashish sinha ||| jose dolz ||| 
2019 ||| photoplethysmographic waveform versus heart rate variability to identify low-stress states: attention test. ||| maria dolores pel ||| ez-coca ||| mar ||| a teresa lozano albalate ||| alberto hernando sanz ||| montserrat aiger ||| eduardo gil ||| 
2022 ||| attention-guided discriminative region localization and label distribution learning for bone age assessment. ||| chao chen ||| zhihong chen ||| xinyu jin ||| lanjuan li ||| william speier ||| corey w. arnold ||| 
2021 ||| an attention based cnn-lstm approach for sleep-wake detection with heterogeneous sensors. ||| zhenghua chen ||| min wu ||| wei cui ||| chengyu liu ||| xiaoli li ||| 
2021 ||| accurate retinal vessel segmentation in color fundus images via fully attention-based networks. ||| kaiqi li ||| xingqun qi ||| yiwen luo ||| zeyi yao ||| xiaoguang zhou ||| muyi sun ||| 
2022 ||| multi-level attention network for retinal vessel segmentation. ||| yuchen yuan ||| lei zhang ||| lituan wang ||| haiying huang ||| 
2020 ||| a residual based attention model for eeg based sleep staging. ||| wei qu ||| zhiyong wang ||| hong hong ||| zheru chi ||| david dagan feng ||| ron grunstein ||| christopher gordon ||| 
2021 ||| attention-guided multi-branch convolutional neural network for mitosis detection from histopathological images. ||| haijun lei ||| shaomin liu ||| ahmed elazab ||| xuehao gong ||| baiying lei ||| 
2020 ||| heart sound segmentation using bidirectional lstms with attention. ||| tharindu fernando ||| houman ghaemmaghami ||| simon denman ||| sridha sridharan ||| nayyar hussain ||| clinton fookes ||| 
2020 ||| pulmonary textures classification via a multi-scale attention network. ||| rui xu ||| zhen cong ||| xinchen ye ||| yasushi hirano ||| shoji kido ||| tomoko gyobu ||| yutaka kawata ||| osamu honda ||| noriyuki tomiyama ||| 
2022 ||| exploiting icd hierarchy for classification of ehrs in spanish through multi-task transformers. ||| alberto blanco ||| alicia p ||| rez ||| arantza casillas ||| 
2020 ||| lesion location attention guided network for multi-label thoracic disease classification in chest x-rays. ||| bingzhi chen ||| jinxing li ||| guangming lu ||| david zhang ||| 
2021 ||| gcsba-net: gabor-based and cascade squeeze bi-attention network for gland segmentation. ||| zhijie wen ||| ru feng ||| jingxin liu ||| ying li ||| shihui ying ||| 
2021 ||| discriminative feature network based on a hierarchical attention mechanism for semantic hippocampus segmentation. ||| jiali shi ||| rong zhang ||| lijun guo ||| linlin gao ||| huifang ma ||| jianhua wang ||| 
2021 ||| attention-guided deep neural network with multi-scale feature fusion for liver vessel segmentation. ||| qingsen yan ||| bo wang ||| wei zhang ||| chuan luo ||| wei xu ||| zhengqing xu ||| yanning zhang ||| qinfeng shi ||| liang zhang ||| zheng you ||| 
2021 ||| attention-refnet: interactive attention refinement network for infected area segmentation of covid-19. ||| titinunt kitrungrotsakul ||| qingqing chen ||| huitao wu ||| yutaro iwamoto ||| hongjie hu ||| wenchao zhu ||| chao chen ||| fangyi xu ||| yong zhou ||| lanfen lin ||| ruofeng tong ||| jingsong li ||| yen-wei chen ||| 
2021 ||| automatic acetowhite lesion segmentation via specular reflection removal and deep attention network. ||| zijie yue ||| shuai ding ||| xiaojian li ||| shanlin yang ||| youtao zhang ||| 
2021 ||| an attention-based mechanism to combine images and metadata in deep learning models applied to skin cancer classification. ||| andr |||  g. c. pacheco ||| renato a. krohling ||| 
2020 ||| using a multi-task recurrent neural network with attention mechanisms to predict hospital mortality of patients. ||| ruoxi yu ||| yali zheng ||| ruikai zhang ||| yuqi jiang ||| carmen c. y. poon ||| 
2021 ||| limitations of transformers on clinical text classification. ||| shang gao ||| mohammed m. alawad ||| m. todd young ||| john gounley ||| noah schaefferkoetter ||| hong-jun yoon ||| xiao-cheng wu ||| eric b. durbin ||| jennifer a. doherty ||| antoinette stroup ||| linda coyle ||| georgia d. tourassi ||| 
2021 ||| attention-based lstm for non-contact sleep stage classification using ir-uwb radar. ||| hyunbin kwon ||| sang ho choi ||| dongseok lee ||| dongyeon son ||| heenam yoon ||| mi hyun lee ||| yu jin lee ||| kwang suk park ||| 
2020 ||| thorax-net: an attention regularized deep neural network for classification of thoracic diseases on chest radiography. ||| hongyu wang ||| haozhe jia ||| le lu ||| yong xia ||| 
2020 ||| attention-guided 3d-cnn framework for glaucoma detection and structural-functional association using volumetric images. ||| yasmeen m. george ||| bhavna j. antony ||| hiroshi ishikawa ||| gadi wollstein ||| joel s. schuman ||| rahil garnavi ||| 
2021 ||| multi-res-attention unet: a cnn model for the segmentation of focal cortical dysplasia lesions from magnetic resonance images. ||| edwin thomas ||| pawan s. jogi ||| shushant kumar ||| anmol horo ||| s. niyas ||| s. vinayagamani ||| chandrasekharan kesavadas ||| jeny rajan ||| 
2021 ||| a spatio-temporal attention-based model for infant movement assessment from videos. ||| binh nguyen-thai ||| vuong le ||| catherine morgan ||| nadia badawi ||| truyen tran ||| svetha venkatesh ||| 
2021 ||| attention-based parallel multiscale convolutional neural network for visual evoked potentials eeg classification. ||| zhongke gao ||| xinlin sun ||| mingxu liu ||| wei-dong dang ||| chao ma ||| guanrong chen ||| 
2020 ||| motor imagery classification via temporal attention cues of graph embedded eeg signals. ||| dalin zhang ||| kaixuan chen ||| debao jian ||| lina yao ||| 
2021 ||| covid-19 automatic diagnosis with radiographic imaging: explainable attention transfer deep neural networks. ||| wenqi shi ||| li tong ||| yuanda zhu ||| may d. wang ||| 
2022 ||| self-attention-based deep learning network for regional influenza forecasting. ||| seungwon jung ||| jaeuk moon ||| sungwoo park ||| eenjun hwang ||| 
2021 ||| learning a deep cnn denoising approach using anatomical prior information implemented with attention mechanism for low-dose ct imaging on clinical patient data from multiple anatomical sites. ||| zhenxing huang ||| xinfeng liu ||| rongpin wang ||| zixiang chen ||| yongfeng yang ||| xin liu ||| hairong zheng ||| dong liang ||| zhanli hu ||| 
2021 ||| medication combination prediction using temporal attention mechanism and simple graph convolution. ||| haiqiang wang ||| yinying wu ||| chao gao ||| yue deng ||| fan zhang ||| jiajin huang ||| jiming liu ||| 
2022 ||| an o-shape neural network with attention modules to detect junctions in biomedical images without segmentation. ||| yuqiang zhang ||| min liu ||| fuhao yu ||| tieyong zeng ||| yaonan wang ||| 
2020 ||| computer-aided diagnosis in histopathological images of the endometrium using a convolutional neural network and attention mechanisms. ||| hao sun ||| xianxu zeng ||| tao xu ||| gang peng ||| yutao ma ||| 
2021 ||| multimodal spatial attention module for targeting multimodal pet-ct lung tumor segmentation. ||| xiaohang fu ||| lei bi ||| ashnil kumar ||| michael j. fulham ||| jinman kim ||| 
2021 ||| long-term prediction for temporal propagation of seasonal influenza using transformer-based model. ||| liang li ||| yuewen jiang ||| biqing huang ||| 
2021 ||| adversarial neural network with sentiment-aware attention for detecting adverse drug reactions. ||| tongxuan zhang ||| hongfei lin ||| bo xu ||| liang yang ||| jian wang ||| xiaodong duan ||| 
2021 ||| drug-drug interaction extraction using a position and similarity fusion-based attention mechanism. ||| mohsen fatehifar ||| hossein karshenas ||| 
2021 ||| traditional chinese medicine symptom normalization approach leveraging hierarchical semantic information and text matching with attention mechanism. ||| qi jia ||| dezheng zhang ||| shibing yang ||| chao xia ||| yingjie shi ||| hu tao ||| cong xu ||| xiong luo ||| yuekun ma ||| yonghong xie ||| 
2021 ||| attention-based bidirectional long short-term memory networks for extracting temporal relationships from clinical discharge summaries. ||| ghada alfattni ||| niels peek ||| goran nenadic ||| 
2021 ||| nd transformers for evidence-based medicine and argument mining in medical literature. ||| nikolaos stylianou ||| ioannis p. vlahavas ||| 
2021 ||| explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation. ||| hang dong ||| v ||| ctor su ||| rez-paniagua ||| william whiteley ||| honghan wu ||| 
2021 ||| extracting chemical-induced disease relation by integrating a hierarchical concentrative attention and a hybrid graph-based neural network. ||| hongbin lu ||| lishuang li ||| zuocheng li ||| shiyi zhao ||| 
2019 ||| associative attention networks for temporal relation extraction from electronic health records. ||| shiyi zhao ||| lishuang li ||| hongbin lu ||| anqiao zhou ||| shuang qian ||| 
2022 ||| ammu: a survey of transformer-based biomedical pretrained language models. ||| katikapalli subramanyam kalyan ||| ajit rajasekharan ||| sivanesan sangeetha ||| 
2018 ||| using neural attention networks to detect adverse medical events from electronic health records. ||| jiebin chu ||| wei dong ||| kunlun he ||| huilong duan ||| zhengxing huang ||| 
2021 ||| improved biomedical word embeddings in the transformer era. ||| jiho noh ||| ramakanth kavuluru ||| 
2022 ||| an attentive joint model with transformer-based weighted graph convolutional network for extracting adverse drug event relation. ||| ed-drissiya el-allaly ||| mourad sarrouti ||| noureddine en-nahnahi ||| said ouatik el alaoui ||| 
2019 ||| chinese clinical named entity recognition with radical-level feature and self-attention mechanism. ||| mingwang yin ||| chengjie mou ||| kaineng xiong ||| jiangtao ren ||| 
2021 ||| a deep attention model to forecast the length of stay and the in-hospital mortality right on admission from icd codes and demographic data. ||| gaspard harerimana ||| jong wook kim ||| beakcheol jang ||| 
2019 ||| automatic icd code assignment of chinese clinical notes based on multilayer attention birnn. ||| ying yu ||| min li ||| liangliang liu ||| zhihui fei ||| fang-xiang wu ||| jianxin wang ||| 
2020 ||| s from texts with biobert and multiple entity-aware attentions. ||| yu zhu ||| lishuang li ||| hongbin lu ||| anqiao zhou ||| xueyang qin ||| 
2021 ||| supervised line attention for tumor attribute classification from pathology reports: higher performance with less data. ||| nicholas altieri ||| briton park ||| mara olson ||| john denero ||| anobel y. odisho ||| bin yu ||| 
2019 ||| knowledge-aware attention network for protein-protein interaction extraction. ||| huiwei zhou ||| zhuang liu ||| shixian ning ||| chengkun lang ||| yingyu lin ||| lei du ||| 
2020 ||| multi-view self-attention for interpretable drug-target interaction prediction. ||| brighter agyemang ||| wei-ping wu ||| michael yelpengne kpiebaareh ||| zhihua lei ||| ebenezer nanor ||| lei chen ||| 
2021 ||| enhancing biomedical relation extraction with transformer models using shortest dependency path features and triplet information. ||| vani kanjirangat ||| fabio rinaldi ||| 
2021 ||| gla-net: a global-local attention network for automatic cataract classification. ||| xi xu ||| jianqiang li ||| yu guan ||| linna zhao ||| qing zhao ||| li zhang ||| li li ||| 
2020 ||| an attention-based multi-task model for named entity recognition and intent analysis of chinese online medical questions. ||| chaochen wu ||| guan luo ||| chao guo ||| yin ren ||| anni zheng ||| cheng yang ||| 
2020 ||| attention guided capsule networks for chemical-protein interaction extraction. ||| cong sun ||| zhihao yang ||| lei wang ||| yin zhang ||| hongfei lin ||| jian wang ||| 
2021 ||| incorporating multi-level cnn and attention mechanism for chinese clinical named entity recognition. ||| jun kong ||| leixin zhang ||| min jiang ||| tianshan liu ||| 
2021 ||| dynamic deformable attention network (ddanet) for covid-19 lesions semantic segmentation. ||| kumar t. rajamani ||| hanna siebert ||| mattias p. heinrich ||| 
2021 ||| graph convolutional and attention models for entity classification in multilayer networks. ||| lorenzo zangari ||| roberto interdonato ||| antonio cali ||| andrea tagarelli ||| 
2020 ||| automated diagnosis of bone metastasis based on multi-view bone scans using attention-augmented deep neural networks. ||| yong pi ||| zhen zhao ||| yongzhao xiang ||| yuhao li ||| huawei cai ||| zhang yi ||| 
2021 ||| cross-attention multi-branch network for fundus diseases classification using slo images. ||| hai xie ||| xianlu zeng ||| haijun lei ||| jie du ||| jiantao wang ||| guoming zhang ||| jiuwen cao ||| tianfu wang ||| baiying lei ||| 
2021 ||| end-to-end prostate cancer detection in bpmri via 3d cnns: effects of attention mechanisms, clinical priori and decoupled false positive reduction. ||| anindo saha ||| matin hosseinzadeh ||| henkjan j. huisman ||| 
2020 ||| whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. ||| jiawen yao ||| xinliang zhu ||| jitendra jonnagaddala ||| nicholas j. hawkins ||| junzhou huang ||| 
2022 ||| global-local attention network with multi-task uncertainty loss for abnormal lymph node detection in mr images. ||| shuai wang ||| yingying zhu ||| sungwon lee ||| daniel c. elton ||| thomas c. shen ||| youbao tang ||| yifan peng ||| zhiyong lu ||| ronald m. summers ||| 
2021 ||| deep pyramid local attention neural network for cardiac structure segmentation in two-dimensional echocardiography. ||| fei liu ||| kun wang ||| dan liu ||| xin yang ||| jie tian ||| 
2022 ||| resganet: residual group attention network for medical image classification and segmentation. ||| junlong cheng ||| shengwei tian ||| long yu ||| chengrui gao ||| xiaojing kang ||| xiang ma ||| weidong wu ||| shijia liu ||| hongchun lu ||| 
2021 ||| a novel attention-guided convolutional network for the detection of abnormal cervical cells in cervical cancer screening. ||| lei cao ||| jinying yang ||| zhiwei rong ||| lulu li ||| bairong xia ||| chong you ||| ge lou ||| lei jiang ||| chun du ||| hongxue meng ||| wenjie wang ||| meng wang ||| kang li ||| yan hou ||| 
2019 ||| learning to detect chest radiographs containing pulmonary lesions using visual attention networks. ||| emanuele pesce ||| samuel withey ||| petros-pavlos ypsilantis ||| robert bakewell ||| vicky goh ||| giovanni montana ||| 
2022 ||| spatio-temporal directed acyclic graph learning with attention mechanisms on brain functional time series and connectivity. ||| shih-gu huang ||| jing xia ||| liyuan xu ||| anqi qiu ||| 
2022 ||| fat-net: feature adaptive transformers for automated skin lesion segmentation. ||| huisi wu ||| shihuai chen ||| guilian chen ||| wei wang ||| baiying lei ||| zhenkun wen ||| 
2019 ||| a collaborative computer aided diagnosis (c-cad) system with eye-tracking, sparse attentional model, and deep learning. ||| naji khosravan ||| haydar celik ||| baris turkbey ||| elizabeth jones ||| bradford j. wood ||| ulas bagci ||| 
2020 ||| prediction of in-plane organ deformation during free-breathing radiotherapy via discriminative spatial transformer networks. ||| liset v ||| zquez romaguera ||| rosalie plantef ||| ve ||| francisco perdig ||| n romero ||| fran ||| ois h ||| bert ||| jean-fran ||| ois carrier ||| samuel kadoury ||| 
2022 ||| multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. ||| sangjoon park ||| gwanghyun kim ||| yujin oh ||| joon beom seo ||| sang min lee ||| jin hwan kim ||| sungjun moon ||| jae-kwang lim ||| jong chul ye ||| 
2020 ||| spatio-temporal visual attention modelling of standard biometry plane-finding navigation. ||| yifan cai ||| richard droste ||| harshita sharma ||| pierre chatelain ||| lior drukker ||| aris t. papageorghiou ||| j. alison noble ||| 
2020 ||| automatic ischemic stroke lesion segmentation from computed tomography perfusion images by image synthesis and attention-based deep neural networks. ||| guotai wang ||| tao song ||| qiang dong ||| mei cui ||| ning huang ||| shaoting zhang ||| 
2021 ||| asymmetric multi-task attention network for prostate bed segmentation in computed tomography images. ||| xuanang xu ||| chunfeng lian ||| shuai wang ||| tong zhu ||| ronald c. chen ||| andrew z. wang ||| trevor j. royce ||| pew-thian yap ||| dinggang shen ||| jun lian ||| 
2020 ||| fully automatic brain tumor segmentation with deep learning-based selective attention using overlapping patches and multi-class weighted cross-entropy. ||| mostefa ben naceur ||| mohamed akil ||| rachida saouli ||| rostom kachouri ||| 
2022 ||| fully transformer network for skin lesion analysis. ||| xinzi he ||| ee-leng tan ||| hanwen bi ||| xuzhe zhang ||| shijie zhao ||| baiying lei ||| 
2020 ||| semi-supervised wce image classification with adaptive aggregated attention. ||| xiaoqing guo ||| yixuan yuan ||| 
2021 ||| anatomical-guided attention enhances unsupervised pet image denoising performance. ||| yuya onishi ||| fumio hashimoto ||| kibo ote ||| hiroyuki ohba ||| ryosuke ota ||| etsuji yoshikawa ||| yasuomi ouchi ||| 
2019 ||| attention gated networks: learning to leverage salient regions in medical images. ||| jo schlemper ||| ozan oktay ||| michiel schaap ||| mattias p. heinrich ||| bernhard kainz ||| ben glocker ||| daniel rueckert ||| 
2022 ||| awsnet: an auto-weighted supervision attention network for myocardial scar and edema segmentation in multi-sequence cardiac magnetic resonance images. ||| kai-ni wang ||| xin yang ||| juzheng miao ||| lei li ||| jing yao ||| ping zhou ||| wufeng xue ||| guang-quan zhou ||| xiahai zhuang ||| dong ni ||| 
2021 ||| dual attention multiple instance learning with unsupervised complementary loss for covid-19 screening. ||| philip chikontwe ||| miguel luna ||| myeongkyun kang ||| kyung soo hong ||| june hong ahn ||| sang hyun park ||| 
2022 ||| uncertainty-guided graph attention network for parapneumonic effusion diagnosis. ||| jinkui hao ||| jiang liu ||| ella pereira ||| ri liu ||| jiong zhang ||| yangfan zhang ||| kun yan ||| yan gong ||| jianjun zheng ||| jingfeng zhang ||| yonghuai liu ||| yitian zhao ||| 
2021 ||| multi-site mri harmonization via attention-guided deep domain adaptation for brain disorder identification. ||| hao guan ||| yunbi liu ||| erkun yang ||| pew-thian yap ||| dinggang shen ||| mingxia liu ||| 
2022 ||| m3net: a multi-scale multi-view framework for multi-phase pancreas segmentation based on cross-phase non-local attention. ||| taiping qu ||| xiheng wang ||| chaowei fang ||| li mao ||| juan li ||| ping li ||| jinrong qu ||| xiuli li ||| huadan xue ||| yizhou yu ||| zhengyu jin ||| 
2021 ||| rigid and non-rigid motion artifact reduction in x-ray ct using attention module. ||| youngjun ko ||| seunghyuk moon ||| jongduk baek ||| hyunjung shim ||| 
2022 ||| spine-transformers: vertebra labeling and segmentation in arbitrary field-of-view spine cts via 3d transformers. ||| rong tao ||| wenyong liu ||| guoyan zheng ||| 
2021 ||| automated cardiac segmentation of cross-modal medical images using unsupervised multi-domain adaptation and spatial neural attention structure. ||| jinping liu ||| hui liu ||| subo gong ||| zhaohui tang ||| yongfang xie ||| huazhan yin ||| jean paul niyoyita ||| 
2021 ||| triple attention learning for classification of 14 thoracic diseases using chest radiography. ||| hongyu wang ||| shanshan wang ||| zibo qin ||| yanning zhang ||| ruijiang li ||| yong xia ||| 
2020 ||| self-co-attention neural network for anatomy segmentation in whole breast ultrasound. ||| baiying lei ||| shan huang ||| hang li ||| ran li ||| cheng bian ||| yi-hong chou ||| jie du ||| peng zhou ||| xuehao gong ||| jie-zhi cheng ||| 
2019 ||| abdominal multi-organ segmentation with organ-attention networks and statistical fusion. ||| yan wang ||| yuyin zhou ||| wei shen ||| seyoun park ||| elliot k. fishman ||| alan l. yuille ||| 
2022 ||| surginet: pyramid attention aggregation and class-wise self-distillation for surgical instrument segmentation. ||| zhen-liang ni ||| xiao-hu zhou ||| guan'an wang ||| wen-qian yue ||| zhen li ||| gui-bin bian ||| zeng-guang hou ||| 
2021 ||| multi-channel attention-fusion neural network for brain age estimation: accuracy, generality, and interpretation with 16, 705 healthy mris across lifespan. ||| sheng he ||| diana pereira ||| juan david perez ||| randy l. gollub ||| shawn n. murphy ||| sanjay prabhu ||| rudolph pienaar ||| richard l. robertson ||| patricia ellen grant ||| yangming ou ||| 
2020 ||| attention convolutional neural network for accurate segmentation and quantification of lesions in ischemic stroke disease. ||| liangliang liu ||| lukasz a. kurgan ||| fang-xiang wu ||| jianxin wang ||| 
2021 ||| dual attention enhancement feature fusion network for segmentation and quantitative analysis of paediatric echocardiography. ||| libao guo ||| baiying lei ||| weiling chen ||| jie du ||| alejandro f. frangi ||| jing qin ||| cheng zhao ||| pengpeng shi ||| bei xia ||| tianfu wang ||| 
2022 ||| prostattention-net: a deep attention model for prostate cancer segmentation by aggressiveness in mri scans. ||| audrey duran ||| gaspard dussert ||| olivier rouvi ||| re ||| tristan jaouen ||| pierre-marc jodoin ||| carole lartizien ||| 
2020 ||| attentional reinforcement learning in the brain. ||| hiroshi yamakawa ||| 
2017 ||| erratum to: visualizing collective attention using association networks. ||| kazutoshi sasahara ||| 
2019 ||| development and application research of smart distribution district based on idtt-b new-type transformer terminal unit. ||| aidong xu ||| lefeng cheng ||| xiaobin guo ||| ganyang jian ||| tao yu ||| wenxiao wei ||| li yu ||| 
2022 ||| improving wireless indoor non-intrusive load disaggregation using attention-based deep learning networks. ||| qi liu ||| jing zhang ||| xiaodong liu ||| yonghong zhang ||| xiaolong xu ||| mohammad khosravi ||| muhammad bilal ||| 
2021 ||| effcdnet: transfer learning with deep attention network for change detection in high spatial resolution satellite images. ||| parmeshwar s. patil ||| raghunath s. holambe ||| laxman m. waghmare ||| 
2021 ||| dual attention residual group networks for single image deraining. ||| hai zhang ||| qiangqiang xie ||| bei lu ||| shan gai ||| 
2022 ||| the multi-level classification and regression network for visual tracking via residual channel attention. ||| junyang yu ||| mengle zuo ||| lifeng dong ||| huanlong zhang ||| xin he ||| 
2021 ||| dual attention per-pixel filter network for spatially varying image deblurring. ||| yanfang zhang ||| weihong li ||| zhenghao li ||| taigong ning ||| 
2020 ||| cross-modal feature alignment based hybrid attentional generative adversarial networks for text-to-image synthesis. ||| qingrong cheng ||| xiaodong gu ||| 
2022 ||| a novel multi-scale convolution model based on multi-dilation rates and multi-attention mechanism for mechanical fault diagnosis. ||| caiyuan chu ||| yongxin ge ||| quan qian ||| boyu hua ||| jie guo ||| 
2022 ||| pedestrian detection algorithm based on multi-scale feature extraction and attention feature fusion. ||| hao xia ||| jun ma ||| jiayu ou ||| xinyao lv ||| chengjie bai ||| 
2022 ||| multi-scale residual attention network for single image dehazing. ||| jiechao sheng ||| guoqiang lv ||| gang du ||| zi wang ||| qibin feng ||| 
2021 ||| structural pixel-wise target attention for robust object tracking. ||| huanlong zhang ||| liyun cheng ||| jianwei zhang ||| wanwei huang ||| xiulei liu ||| junyang yu ||| 
2021 ||| boundary-aware pyramid attention network for detecting salient objects in rgb-d images. ||| wujie zhou ||| yuzhen chen ||| jingsheng lei ||| lu yu ||| xi zhou ||| ting luo ||| 
2021 ||| spatial-temporal attention network for multistep-ahead forecasting of chlorophyll. ||| xiaoyu he ||| suixiang shi ||| xiulin geng ||| lingyu xu ||| xiaolin zhang ||| 
2021 ||| infrared image super-resolution reconstruction by using generative adversarial network with an attention mechanism. ||| qing-ming liu ||| ruisheng jia ||| yan-bo liu ||| hai-bin sun ||| jian-zhi yu ||| hong-mei sun ||| 
2022 ||| triplet attention multiple spacetime-semantic graph convolutional network for skeleton-based action recognition. ||| yanjing sun ||| han huang ||| xiao yun ||| bin yang ||| kaiwen dong ||| 
2020 ||| multi-branch cross attention model for prediction of kras mutation in rectal cancer with t2-weighted mri. ||| jiawen wang ||| yanfen cui ||| guohua shi ||| juanjuan zhao ||| xiaotang yang ||| yan qiang ||| qianqian du ||| yue ma ||| ntikurako guy-fernand kazihise ||| 
2020 ||| two-dimensional discrete feature based spatial attention capsnet for semg signal recognition. ||| guoqi chen ||| wanliang wang ||| zheng wang ||| honghai liu ||| zelin zang ||| weikun li ||| 
2021 ||| crowd counting method based on the self-attention residual network. ||| yan-bo liu ||| ruisheng jia ||| qing-ming liu ||| xing-li zhang ||| hong-mei sun ||| 
2022 ||| self attention mechanism of bidirectional information enhancement. ||| qibin li ||| nianmin yao ||| jian zhao ||| yanan zhang ||| 
2018 ||| recurrent networks with attention and convolutional networks for sentence representation and classification. ||| tengfei liu ||| shuangyuan yu ||| baomin xu ||| hongfeng yin ||| 
2022 ||| multi-stage attention and center triplet loss for person re-identication. ||| dandan zhao ||| chunyu chen ||| dongfang li ||| 
2022 ||| a temporal attention based appearance model for video object segmentation. ||| hui wang ||| weibin liu ||| weiwei xing ||| 
2021 ||| residual attention network using multi-channel dense connections for image super-resolution. ||| zhiwei liu ||| ji huang ||| chengjia zhu ||| xiaoyu peng ||| xinyu du ||| 
2022 ||| adaptive spatial-temporal graph attention networks for traffic flow forecasting. ||| xiangyuan kong ||| jian zhang ||| xiang wei ||| weiwei xing ||| wei lu ||| 
2021 ||| collaborative attention neural network for multi-domain sentiment classification. ||| chunyi yue ||| hanqiang cao ||| guoping xu ||| youli dong ||| 
2022 ||| egsanet: edge-guided sparse attention network for improving license plate detection in the wild. ||| jing liang ||| guancheng chen ||| yan wang ||| huabiao qin ||| 
2021 ||| subtler mixed attention network on fine-grained image classification. ||| chao liu ||| lei huang ||| zhiqiang wei ||| wenfeng zhang ||| 
2021 ||| an efficient attention module for 3d convolutional neural networks in action recognition. ||| guanghao jiang ||| xiaoyan jiang ||| zhijun fang ||| shanshan chen ||| 
2022 ||| self-attention enhanced cnns with average margin loss for chinese zero pronoun resolution. ||| shi-jun sun ||| 
2017 ||| improving human-robot interaction based on joint attention. ||| evren daglarli ||| sare funda daglarli ||| g ||| lay  ||| ke g ||| nel ||| hatice k ||| se ||| 
2021 ||| mixed attention dense network for sketch classification. ||| ming zhu ||| chun chen ||| nian wang ||| jun tang ||| chen zhao ||| 
2021 ||| vehicle theft recognition from surveillance video based on spatiotemporal attention. ||| lijun he ||| shuai wen ||| liejun wang ||| fan li ||| 
2021 ||| attention-based vgg-16 model for covid-19 chest x-ray image classification. ||| chiranjibi sitaula ||| mohammad belayet hossain ||| 
2022 ||| mc-net: multi-scale context-attention network for medical ct image segmentation. ||| haiying xia ||| mingjun ma ||| haisheng li ||| shuxiang song ||| 
2021 ||| unsupervised medical images denoising via graph attention dual adversarial network. ||| tianxu lv ||| xiang pan ||| yazhou zhu ||| lihua li ||| 
2021 ||| multi-attention based semantic deep hashing for cross-modal retrieval. ||| liping zhu ||| gangyi tian ||| bingyao wang ||| wenjie wang ||| di zhang ||| chengyang li ||| 
2021 ||| structural attention network for graph. ||| anzhong zhou ||| yifen li ||| 
2020 ||| elective future: the influence factor mining of students' graduation development based on hierarchical attention neural network model with graph. ||| yong ouyang ||| yawen zeng ||| rong gao ||| yonghong yu ||| chunzhi wang ||| 
2022 ||| image super-resolution via channel attention and spatial attention. ||| enmin lu ||| xiaoxiao hu ||| 
2021 ||| memory network with hierarchical multi-head attention for aspect-based sentiment analysis. ||| yuzhong chen ||| tianhao zhuang ||| kun guo ||| 
2022 ||| efficient residual attention network for single image super-resolution. ||| fangwei hao ||| taiping zhang ||| linchang zhao ||| yuanyan tang ||| 
2021 ||| attention augmented multi-scale network for single image super-resolution. ||| chengyi xiong ||| xiaodi shi ||| zhirong gao ||| ge wang ||| 
2022 ||| convolutional neural network based on attention mechanism and bi-lstm for bearing remaining life prediction. ||| jiahang luo ||| xu zhang ||| 
2021 ||| correction to: non-parallel text style transfer with domain adaptation and an attention model. ||| mingxuan hu ||| min he ||| 
2022 ||| an attention network via pronunciation, lexicon and syntax for humor recognition. ||| lu ren ||| bo xu ||| hongfei lin ||| jinhui zhang ||| liang yang ||| 
2022 ||| neural tv program recommendation with label and user dual attention. ||| fulian yin ||| sitong li ||| meiqi ji ||| yanyan wang ||| 
2021 ||| exploiting multi-attention network with contextual influence for point-of-interest recommendation. ||| liang chang ||| wei chen ||| jianbo huang ||| chenzhong bin ||| wenkai wang ||| 
2022 ||| focus on temporal graph convolutional networks with unified attention for skeleton-based action recognition. ||| bingkun gao ||| le dong ||| hongbo bi ||| yun-ze bi ||| 
2021 ||| co-attention fusion based deep neural network for chinese medical answer selection. ||| xichen chen ||| zuyuan yang ||| naiyao liang ||| zhenni li ||| weijun sun ||| 
2022 ||| knowledge-aware recommendation model with dynamic co-attention and attribute regularize. ||| guisheng yin ||| fukun chen ||| yuxin dong ||| gesu li ||| 
2021 ||| modeling low- and high-order feature interactions with fm and self-attention network. ||| cairong yan ||| yizhou chen ||| yongquan wan ||| pengwei wang ||| 
2021 ||| link traffic speed forecasting using convolutional attention-based gated recurrent unit. ||| ghazaleh khodabandelou ||| walid kheriji ||| fouad hadj-selem ||| 
2022 ||| piecewise convolutional neural networks with position attention and similar bag attention for distant supervision relation extraction. ||| weijiang li ||| qing wang ||| jiekun wu ||| zhengtao yu ||| 
2022 ||| a multi-focus image fusion method based on attention mechanism and supervised learning. ||| limai jiang ||| hui fan ||| jinjiang li ||| 
2020 ||| hierarchical graph attention networks for semi-supervised node classification. ||| kangjie li ||| yixiong feng ||| yicong gao ||| jian qiu ||| 
2021 ||| spatio-temporal attention on manifold space for 3d human action recognition. ||| chongyang ding ||| kai liu ||| fei cheng ||| evgeny belyaev ||| 
2021 ||| an enhanced siamese angular softmax network with dual joint-attention for person re-identification. ||| jie su ||| xiaohai he ||| linbo qing ||| yongqiang cheng ||| yonghong peng ||| 
2020 ||| deep spatial-temporal networks for crowd flows prediction by dilated convolutions and region-shifting attention mechanism. ||| chujie tian ||| xinning zhu ||| zheng hu ||| jian ma ||| 
2022 ||| rman: relational multi-head attention neural network for joint extraction of entities and relations. ||| taiqu lai ||| lianglun cheng ||| depei wang ||| haiming ye ||| weiwen zhang ||| 
2022 ||| a novel deep pixel restoration video prediction algorithm integrating attention mechanism. ||| muxuan yuan ||| qun dai ||| 
2022 ||| pyramid-attention based multi-scale feature fusion network for multispectral pan-sharpening. ||| yang chi ||| jinjiang li ||| hui fan ||| 
2022 ||| deca: a novel multi-scale efficient channel attention module for object detection in real-life fire images. ||| junjie wang ||| jiong yu ||| zhu he ||| 
2021 ||| non-parallel text style transfer with domain adaptation and an attention model. ||| mingxuan hu ||| min he ||| 
2022 ||| residual deep attention mechanism and adaptive reconstruction network for single image super-resolution. ||| hongjuan wang ||| mingrun wei ||| ru cheng ||| yue yu ||| xingli zhang ||| 
2021 ||| saliency prediction on omnidirectional images with attention-aware feature fusion network. ||| dandan zhu ||| yongqing chen ||| defang zhao ||| qiangqiang zhou ||| xiaokang yang ||| 
2021 ||| neural attention model for recommendation based on factorization machines. ||| peng wen ||| weihua yuan ||| qianqian qin ||| sheng sang ||| zhijun zhang ||| 
2021 ||| sta-net: spatial-temporal attention network for video salient object detection. ||| hongbo bi ||| di lu ||| hui-hui zhu ||| lina yang ||| huaping guan ||| 
2022 ||| joint pyramid attention network for real-time semantic segmentation of urban scenes. ||| xuegang hu ||| liyuan jing ||| uroosa sehar ||| 
2022 ||| a multi-task dual attention deep recommendation model using ratings and review helpfulness. ||| zhen liu ||| baoxin yuan ||| ying ma ||| 
2022 ||| dynamic-boosting attention for self-supervised video representation learning. ||| zhipeng wang ||| chunping hou ||| guanghui yue ||| qingyuan yang ||| 
2022 ||| civil airline fare prediction with a multi-attribute dual-stage attention mechanism. ||| zhichao zhao ||| jinguo you ||| guoyu gan ||| xiaowu li ||| jiaman ding ||| 
2020 ||| a ctr prediction model based on user interest via attention mechanism. ||| hao li ||| huichuan duan ||| yuanjie zheng ||| qianqian wang ||| yu wang ||| 
2021 ||| image super-resolution reconstruction based on feature map attention mechanism. ||| yuantao chen ||| linwu liu ||| volachith phonevilay ||| ke gu ||| runlong xia ||| jingbo xie ||| qian zhang ||| kai yang ||| 
2020 ||| gama: graph attention multi-agent reinforcement learning algorithm for cooperation. ||| haoqiang chen ||| yadong liu ||| zongtan zhou ||| dewen hu ||| ming zhang ||| 
2021 ||| attributed network representation learning via improved graph attention with robust negative sampling. ||| huilian fan ||| yuanchang zhong ||| guangpu zeng ||| lili sun ||| 
2018 ||| a human-like visual-attention-based artificial vision system for wildland firefighting assistance. ||| kurosh madani ||| viachaslau kachurka ||| christophe sabourin ||| v ||| ronique amarger ||| vladimir a. golovko ||| lucile rossi ||| 
2020 ||| end-to-end multitask siamese network with residual hierarchical attention for real-time object tracking. ||| wenhui huang ||| jason gu ||| xin ma ||| yibin li ||| 
2021 ||| sat-net: a side attention network for retinal image segmentation. ||| huilin tong ||| zhijun fang ||| ziran wei ||| qingping cai ||| yongbin gao ||| 
2022 ||| reference-guided deep deblurring via a selective attention network. ||| yaowei li ||| ye luo ||| jianwei lu ||| 
2019 ||| hierarchical attention based long short-term memory for chinese lyric generation. ||| xing wu ||| zhikang du ||| yike guo ||| hamido fujita ||| 
2022 ||| sf-ann: leveraging structural features with an attention neural network for candidate fact ranking. ||| yanan zhang ||| li jin ||| zequn zhang ||| xiaoyu li ||| qing liu ||| hongqi wang ||| 
2021 ||| cropping and attention based approach for masked face recognition. ||| yande li ||| kun guo ||| yonggang lu ||| li liu ||| 
2022 ||| reinforcement-learning-guided source code summarization using hierarchical attention. ||| wenhua wang ||| yuqun zhang ||| yulei sui ||| yao wan ||| zhou zhao ||| jian wu ||| philip s. yu ||| guandong xu ||| 
2020 ||| an end-to-end framework for biomedical event trigger identification with hierarchical attention and adaptive cost learning. ||| jinyong zhang ||| dandan fang ||| weizhong zhao ||| jincai yang ||| wen zou ||| xingpeng jiang ||| tingting he ||| 
2020 ||| drug target interaction prediction via multi-task co-attention. ||| yuyou weng ||| xinyi liu ||| hui li ||| chen lin ||| yun liang ||| 
2018 ||| pcb-planar transformers equivalent circuit model identification using genetic algorithm. ||| aymen ammouri ||| tarek ben salah ||| ferid kourda ||| 
2022 ||| identification of encrypted traffic through attention mechanism based long short term memory. ||| haipeng yao ||| chong liu ||| peiying zhang ||| sheng wu ||| chunxiao jiang ||| shui yu ||| 
2021 ||| universal transformer hawkes process with adaptive recursive iteration. ||| lu-ning zhang ||| jian-wei liu ||| zhi-yan song ||| xin zuo ||| 
2020 ||| an attention long short-term memory based system for automatic classification of speech intelligibility. ||| miguel fern ||| ndez-d ||| az ||| ascensi ||| n gallardo-antol ||| n ||| 
2021 ||| a novel dual attention-based blstm with hybrid features in speech emotion recognition. ||| qiupu chen ||| guimin huang ||| 
2020 ||| purifying real images with an attention-guided style transfer network for gaze estimation. ||| xianping fu ||| yuxiao yan ||| yang yan ||| jinjia peng ||| huibing wang ||| 
2021 ||| transformer based network for open information extraction. ||| jiabao han ||| hongzhi wang ||| 
2022 ||| sequential transformer via an outside-in attention for image captioning. ||| yiwei wei ||| chunlei wu ||| guohe li ||| haitao shi ||| 
2021 ||| computer vision detection of foreign objects in coal processing using attention cnn. ||| kanghui zhang ||| weidong wang ||| ziqi lv ||| yuhan fan ||| yang song ||| 
2021 ||| attention-based learning of self-media data for marketing intention detection. ||| zhihao hou ||| kun ma ||| yufeng wang ||| jia yu ||| ke ji ||| zhenxiang chen ||| ajith abraham ||| 
2021 ||| matching images and texts with multi-head attention network for cross-media hashing retrieval. ||| zhixin li ||| xiumin xie ||| feng ling ||| huifang ma ||| zhongzhi shi ||| 
2021 ||| ptanet: triple attention network for point cloud semantic segmentation. ||| haozhe cheng ||| jian lu ||| mao-xin luo ||| wei liu ||| kaibing zhang ||| 
2020 ||| automatic epileptic eeg classification based on differential entropy and attention model. ||| jian zhang ||| zuochen wei ||| junzhong zou ||| hao fu ||| 
2021 ||| transformers-based information extraction with limited data for domain-specific business documents. ||| minh-tien nguyen ||| dung tien le ||| le thai linh ||| 
2017 ||| the impact of a model-based clinical regional registry for attention-deficit hyperactivity disorder. ||| michele zanetti ||| massimo cartabia ||| anna didoni ||| filomena fortinguerra ||| laura reale ||| matteo mondini ||| maurizio bonati ||| 
2020 ||| predicting substance use disorder using long-term attention deficit hyperactivity disorder medication records in truven. ||| sajjad fouladvand ||| emily r. hankosky ||| heather bush ||| jin chen ||| linda p. dwoskin ||| patricia r. freeman ||| darren w. henderson ||| kathleen kantak ||| jeffery c. talbert ||| shiqiang tao ||| guo-qiang zhang ||| 
2019 ||| perceptual presence: an attentional account. ||| mattia riccardi ||| 
2021 ||| joint attention and perceptual experience. ||| lucas battich ||| bart geurts ||| 
2017 ||| perceptual content is indexed to attention. ||| adrienne prettyman ||| 
2021 ||| the transparency of experience and the neuroscience of attention. ||| assaf weksler ||| hilla jacobson ||| zohar z. bronfman ||| 
2021 ||| joint attention to mental content and the social origin of reasoning. ||| cathal o'madagain ||| michael tomasello ||| 
2022 ||| multi-scale graph capsule with influence attention for information cascades prediction. ||| xueqin chen ||| fengli zhang ||| fan zhou ||| marcello m. bonsangue ||| 
2022 ||| dadcnet: dual attention densely connected network for more accurate real iris region segmentation. ||| ying chen ||| huimin gan ||| zhuang zeng ||| huiling chen ||| 
2021 ||| an attention-based category-aware gru model for the next poi recommendation. ||| yuwen liu ||| aixiang pei ||| fan wang ||| yihong yang ||| xuyun zhang ||| hao wang ||| hongning dai ||| lianyong qi ||| rui ma ||| 
2021 ||| a dual-stage attention-based conv-lstm network for spatio-temporal correlation and multivariate time series prediction. ||| yuteng xiao ||| hongsheng yin ||| yudong zhang ||| honggang qi ||| yundong zhang ||| zhaoyang liu ||| 
2021 ||| an effective framework for semistructured document classification via hierarchical attention model. ||| weizhong zhao ||| dandan fang ||| jinyong zhang ||| yao zhao ||| xiaowei xu ||| xingpeng jiang ||| xiaohua hu ||| tingting he ||| 
2021 ||| topology and channel affinity reinforced global attention for person re-identification. ||| xile wang ||| chengcheng gao ||| ming xin ||| sihan zhang ||| miaohui zhang ||| 
2019 ||| an experimental study of the attention-based view of idea integration: the need for a multi-level dependent variable. ||| elahe javadi ||| judith gebauer ||| 
2020 ||| knowledge graph completion for the chinese text of cultural relics based on bidirectional encoder representations from transformers with entity-type information. ||| min zhang ||| guohua geng ||| sheng zeng ||| huaping jia ||| 
2021 ||| hierarchical classification of event-related potentials for the recognition of gender differences in the attention task. ||| karina maciejewska ||| wojciech froelich ||| 
2019 ||| convolutional recurrent neural networks with a self-attention mechanism for personnel performance prediction. ||| xia xue ||| jun feng ||| yi gao ||| meng liu ||| wenyu zhang ||| xia sun ||| aiqi zhao ||| shou xi guo ||| 
2021 ||| a lightweight yolov4-based forestry pest detection method using coordinate attention and feature fusion. ||| mingfeng zha ||| wenbin qian ||| wenlong yi ||| jing hua ||| 
2021 ||| a novel feature extraction method for power transformer vibration signal based on ceemdan and multi-scale dispersion entropy. ||| haikun shang ||| junyan xu ||| yucai li ||| wei lin ||| jinjuan wang ||| 
2021 ||| personal interest attention graph neural networks for session-based recommendation. ||| xiangde zhang ||| yuan zhou ||| jianping wang ||| xiaojun lu ||| 
2019 ||| attention to the variation of probabilistic events: information processing with message importance measure. ||| rui she ||| shanyun liu ||| pingyi fan ||| 
2021 ||| attention-based fault-tolerant approach for multi-agent reinforcement learning systems. ||| shanzhi gu ||| mingyang geng ||| long lan ||| 
2019 ||| quantum information remote carnot engines and voltage transformers. ||| jose m. diaz de la cruz ||| miguel-angel martin-delgado ||| 
2021 ||| diabetic retinal grading using attention-based bilinear convolutional neural network and complement cross entropy. ||| pingping liu ||| xiaokang yang ||| baixin jin ||| qiuzhan zhou ||| 
2019 ||| joint deep model with multi-level attention and hybrid-prediction for recommendation. ||| zhipeng lin ||| yuhua tang ||| yongjun zhang ||| 
2021 ||| benchmarking attention-based interpretability of deep learning in multivariate time series predictions. ||| domjan baric ||| petar fumic ||| davor horvatic ||| tomislav lipic ||| 
2019 ||| supervisors' visual attention allocation modeling using hybrid entropy. ||| haifeng bao ||| weining fang ||| beiyuan guo ||| peng wang ||| 
2021 ||| interpretable multi-head self-attention architecture for sarcasm detection in social media. ||| ramya akula ||| ivan garibay ||| 
2019 ||| learning to cooperate via an attention-based communication neural network in decentralized multi-robot exploration. ||| mingyang geng ||| kele xu ||| xing zhou ||| bo ding ||| huaimin wang ||| lei zhang ||| 
2022 ||| mfan: multi-level features attention network for fake certificate image detection. ||| yu sun ||| rongrong ni ||| yao zhao ||| 
2021 ||| an information gain-based model and an attention-based rnn for wearable human activity recognition. ||| leyuan liu ||| jian he ||| keyan ren ||| jonathan lungu ||| yibin hou ||| ruihai dong ||| 
2022 ||| regularity normalization: neuroscience-inspired unsupervised attention across neural network layers. ||| baihan lin ||| 
2021 ||| s2a: scale-attention-aware networks for video super-resolution. ||| taian guo ||| tao dai ||| ling liu ||| zexuan zhu ||| shu-tao xia ||| 
2021 ||| learning numerosity representations with transformers: number generation tasks and out-of-distribution generalization. ||| tommaso boccato ||| alberto testolin ||| marco zorzi ||| 
2020 ||| optic disc segmentation using attention-based u-net and the improved cross-entropy convolutional neural network. ||| baixin jin ||| pingping liu ||| peng wang ||| lida shi ||| jing zhao ||| 
2020 ||| separated channel attention convolutional neural network (sc-cnn-attention) to identify adhd in multi-site rs-fmri dataset. ||| tao zhang ||| cunbo li ||| peiyang li ||| yueheng peng ||| xiaodong kang ||| chenyang jiang ||| fali li ||| xuyang zhu ||| dezhong yao ||| bharat b. biswal ||| peng xu ||| 
2021 ||| multi-level fusion temporal-spatial co-attention for video-based person re-identification. ||| shengyu pei ||| xiaoping fan ||| 
2018 ||| identification of auditory object-specific attention from single-trial electroencephalogram signals via entropy measures and machine learning. ||| yun lu ||| mingjiang wang ||| qiquan zhang ||| yufei han ||| 
2021 ||| part-aware mask-guided attention for thorax disease classification. ||| ruihua zhang ||| fan yang ||| yan luo ||| jianyi liu ||| jinbin li ||| cong wang ||| 
2021 ||| a transformer-based hierarchical variational autoencoder combined hidden markov model for long text generation. ||| kun zhao ||| hongwei ding ||| kai ye ||| xiaohui cui ||| 
2021 ||| attention mechanisms and their applications to complex systems. ||| adri ||| n hern ||| ndez ||| jos |||  m. amig ||| 
2021 ||| a pi+passivity-based control of a wind energy conversion system enabled with a solid-state transformer. ||| rafael cisneros ||| rui gao ||| romeo ortega ||| iqbal husain ||| 
2019 ||| consumer choice under limited attention when alternatives have different information costs. ||| frank huettner ||| tamer boyaci ||| yal ||| in ak ||| ay ||| 
2017 ||| smart testing environment for the evaluation of students' attention. ||| manuella kadar ||| paul nicolae borza ||| mihai romanca ||| dan iordachescu ||| teodora iordachescu ||| 
2020 ||| multi-level context extraction and attention-based contextual inter-modal fusion for multimodal sentiment analysis and emotion classification. ||| mahesh g. huddar ||| sanjeev s. sannakki ||| vijay s. rajpurohit ||| 
2020 ||| a semisupervised recurrent convolutional attention model for human activity recognition. ||| kaixuan chen ||| lina yao ||| dalin zhang ||| xianzhi wang ||| xiaojun chang ||| feiping nie ||| 
2021 ||| global and local knowledge-aware attention network for action recognition. ||| zhenxing zheng ||| gaoyun an ||| dapeng wu ||| qiuqi ruan ||| 
2019 ||| attention inspiring receptive-fields network for learning invariant representations. ||| lu yang ||| qing song ||| yingqi wu ||| mengjie hu ||| 
2021 ||| casnet: a cross-attention siamese network for video salient object detection. ||| yuzhu ji ||| haijun zhang ||| zequn jie ||| lin ma ||| q. m. jonathan wu ||| 
2022 ||| multitask attention network for lane detection and fitting. ||| qi wang ||| tao han ||| zequn qin ||| junyu gao ||| xuelong li ||| 
2021 ||| deep coattention-based comparator for relative representation learning in person re-identification. ||| lin wu ||| yang wang ||| junbin gao ||| meng wang ||| zheng-jun zha ||| dacheng tao ||| 
2020 ||| neural machine translation with gru-gated attention model. ||| biao zhang ||| deyi xiong ||| jun xie ||| jinsong su ||| 
2017 ||| quantized attention-gated kernel reinforcement learning for brain-machine interface decoding. ||| fang wang ||| yiwen wang ||| kai xu ||| hongbao li ||| yuxi liao ||| qiaosheng zhang ||| shaomin zhang ||| xiaoxiang zheng ||| jos |||  c. pr ||| ncipe ||| 
2022 |||  clnn: spatial, spectral and multiscale attention convlstm neural network for multisource remote sensing data classification. ||| heng-chao li ||| wen-shuai hu ||| wei li ||| jun li ||| qian du ||| antonio plaza ||| 
2021 ||| automated social text annotation with joint multilabel attention networks. ||| hang dong ||| wei wang ||| kaizhu huang ||| frans coenen ||| 
2021 ||| scene segmentation with dual relation-aware attention network. ||| jun fu ||| jing liu ||| jie jiang ||| yong li ||| yongjun bao ||| hanqing lu ||| 
2021 ||| neighborhood attention networks with adversarial learning for link prediction. ||| zhitao wang ||| yu lei ||| wenjie li ||| 
2021 ||| dual attention-based encoder-decoder: a customized sequence-to-sequence learning for soft sensor development. ||| liangjun feng ||| chunhui zhao ||| youxian sun ||| 
2020 ||| pay attention to them: deep reinforcement learning-based cascade object detection. ||| songtao liu ||| di huang ||| yunhong wang ||| 
2020 ||| cross-modal attention with semantic consistence for image-text matching. ||| xing xu ||| tan wang ||| yang yang ||| lin zuo ||| fumin shen ||| heng tao shen ||| 
2020 ||| robust deep co-saliency detection with group semantic and pyramid attention. ||| zheng-jun zha ||| chong wang ||| dong liu ||| hongtao xie ||| yongdong zhang ||| 
2019 ||| temporal attention-augmented bilinear network for financial time-series data analysis. ||| dat thanh tran ||| alexandros iosifidis ||| juho kanniainen ||| moncef gabbouj ||| 
2021 ||| aanet: adaptive attention network for covid-19 detection from chest x-ray images. ||| zhijie lin ||| zhaoshui he ||| shengli xie ||| xu wang ||| ji tan ||| jun lu ||| beihai tan ||| 
2021 ||| adversarial learning with multi-modal attention for visual question answering. ||| yun liu ||| xiaoming zhang ||| feiran huang ||| lei cheng ||| zhoujun li ||| 
2020 ||| gramme: semisupervised learning using multilayered graph attention models. ||| uday shankar shanthamallu ||| jayaraman j. thiagarajan ||| huan song ||| andreas spanias ||| 
2021 ||| attention-based road registration for gps-denied uas navigation. ||| teng wang ||| ye zhao ||| jiawei wang ||| arun k. somani ||| changyin sun ||| 
2021 ||| attention in natural language processing. ||| andrea galassi ||| marco lippi ||| paolo torroni ||| 
2020 ||| an efficient group recommendation model with multiattention-based neural networks. ||| zhenhua huang ||| xin xu ||| honghao zhu ||| mengchu zhou ||| 
2020 ||| redundancy and attention in convolutional lstm for gesture recognition. ||| guangming zhu ||| liang zhang ||| lu yang ||| lin mei ||| syed afaq ali shah ||| mohammed bennamoun ||| peiyi shen ||| 
2019 ||| development of a multi-user system to identify the level of attention in people. ||| alfredo garcia ||| juan manuel gonz ||| lez ||| amparo dora palomino ||| 
2020 ||| frequency of mind-wandering in a sustained attention to response task: a cognitive model of distraction. ||| renzo cuadra ||| 
2020 ||| extraction of body posture characteristics as a correlation variable with the level of attention. ||| alfredo garcia ||| juan manuel gonz ||| lez ||| amparo dora palomino ||| 
2017 ||| bubbleview: an interface for crowdsourcing image importance maps and tracking visual attention. ||| nam wook kim ||| zoya bylinskii ||| michelle a. borkin ||| krzysztof z. gajos ||| aude oliva ||| fr ||| do durand ||| hanspeter pfister ||| 
2019 ||| attention regulation framework: designing self-regulated mindfulness technologies. ||| kavous salehzadeh niksirat ||| chaklam silpasuwanchai ||| peng cheng ||| xiangshi ren ||| 
2017 ||| gaze-contingent auditory displays for improved spatial attention in virtual reality. ||| margarita vinnikov ||| robert s. allison ||| suzette fernandes ||| 
2021 ||| a new perspective on online malicious comments: effects of attention and neutralization. ||| han-min kim ||| gee-woo bock ||| hyung su kim ||| 
2019 ||| transfer hierarchical attention network for generative dialog system. ||| xiang zhang ||| qiang yang ||| 
2018 ||| a selective attention guided initiative semantic cognition algorithm for service robot. ||| huanzhao chen ||| guohui tian ||| guo-liang liu ||| 
2021 ||| fault classification for on-board equipment of high-speed railway based on attention capsule network. ||| lujie zhou ||| jian-wu dang ||| zhen-hai zhang ||| 
2021 ||| encoding-decoding network with pyramid self-attention module for retinal vessel segmentation. ||| congzhong wu ||| jun sun ||| jing wang ||| liang-feng xu ||| shu zhan ||| 
2021 ||| correction to: transfer hierarchical attention network for generative dialog system. ||| xiang zhang ||| qiang yang ||| 
2021 ||| attention-based deep recurrent model for survival prediction. ||| zhaohong sun ||| wei dong ||| jinlong shi ||| kunlun he ||| zhengxing huang ||| 
2021 ||| attention-gated graph convolutions for extracting drug interaction information from drug labels. ||| tung tran ||| ramakanth kavuluru ||| halil kilicoglu ||| 
2022 ||| attention-based unsupervised keyphrase extraction and phrase graph for covid-19 medical literature retrieval. ||| haoran ding ||| xiao luo ||| 
2018 ||| intelligent virtual security system using attention mechanism. ||| wasim ahmad khan ||| hafiz usman akmal ||| ahmad ullah ||| aqdas malik ||| sagheer abbas ||| abdullah ahmad ||| abdullah farooq ||| 
2022 ||| hybrid neural network model based on multi-head attention for english text emotion analysis. ||| ping li ||| 
2022 ||| a spatio-temporal attention fusion model for students behaviour recognition. ||| xiaoli wang ||| 
2020 ||| classification of fake news by fine-tuning deep bidirectional transformers based language model. ||| akshay aggarwal ||| aniruddha chauhan ||| deepika kumar ||| mamta mittal ||| sharad verma ||| 
2019 ||| understanding and improving deep learning-based rolling bearing fault diagnosis with attention mechanism. ||| xiang li ||| wei zhang ||| qian ding ||| 
2018 ||| recurrent attention network using spatial-temporal relations for action recognition. ||| mingxing zhang ||| yang yang ||| yanli ji ||| ning xie ||| fumin shen ||| 
2020 ||| rafnet: recurrent attention fusion network of hyperspectral and multispectral images. ||| ruiying lu ||| bo chen ||| ziheng cheng ||| penghui wang ||| 
2019 ||| target-aware recurrent attentional network for radar hrrp target recognition. ||| bin xu ||| bo chen ||| jinwei wan ||| hongwei liu ||| lin jin ||| 
2021 ||| multi-semantic crf-based attention model for image forgery detection and localization. ||| yuan rao ||| jiangqun ni ||| hao xie ||| 
2020 ||| image captioning via hierarchical attention mechanism and policy gradient optimization. ||| shiyang yan ||| yuan xie ||| fangyu wu ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2021 ||| region-factorized recurrent attentional network with deep clustering for radar hrrp target recognition. ||| chuan du ||| long tian ||| bo chen ||| lei zhang ||| wenchao chen ||| hongwei liu ||| 
2019 ||| monitoring driver attention distraction with binocular vision. ||| hai yan sun ||| li guo zang ||| 
2018 ||| effect of visual attention guidance by camera work in visualization using dome display. ||| tetsuro ogi ||| takeshi yokota ||| 
2017 ||| look before you authorize: using eye-tracking to enforce user attention towards application permissions. ||| yousra javed ||| mohamed shehab ||| 
2019 ||| fault prediction of a transformer bushing based on entropy weight topsis and gray theory. ||| chen jin-qiang ||| 
2020 ||| super resolution with kernel estimation and dual attention mechanism. ||| huan liang ||| youdong ding ||| fei wang ||| yuzhen gao ||| xiaofeng qiu ||| 
2020 ||| attention-based seriesnet: an attention-based hybrid neural network model for conditional time series forecasting. ||| yepeng cheng ||| zuren liu ||| yasuhiko morimoto ||| 
2020 ||| emotion-semantic-enhanced bidirectional lstm with multi-head attention mechanism for microblog sentiment analysis. ||| shaoxiu wang ||| yonghua zhu ||| wenjing gao ||| meng cao ||| mengyao li ||| 
2019 ||| interactional and informational attention on twitter. ||| agathe baltzer ||| m ||| rton karsai ||| camille roth ||| 
2021 ||| improving amharic speech recognition system using connectionist temporal classification with attention model and phoneme-based byte-pair-encodings. ||| eshete derb emiru ||| shengwu xiong ||| yaxing li ||| awet fesseha ||| moussa diallo ||| 
2021 ||| combating fake news with transformers: a comparative analysis of stance detection and subjectivity analysis. ||| panagiotis kasnesis ||| lazaros toumanidis ||| charalampos z. patrikakis ||| 
2020 ||| semantic enhanced distantly supervised relation extraction via graph attention network. ||| xiaoye ouyang ||| shudong chen ||| rong wang ||| 
2021 ||| attention paid to privacy policy statements. ||| tom ||| s sigmund ||| 
2020 ||| attentional colorization networks with adaptive group-instance normalization. ||| yuzhen gao ||| youdong ding ||| fei wang ||| huan liang ||| 
2018 ||| chinese knowledge base question answering by attention-based multi-granularity model. ||| cun shen ||| tinglei huang ||| xiao liang ||| feng li ||| kun fu ||| 
2022 ||| dual co-attention-based multi-feature fusion method for rumor detection. ||| changsong bing ||| yirong wu ||| fangmin dong ||| shouzhi xu ||| xiaodi liu ||| shuifa sun ||| 
2020 ||| outpatient text classification using attention-based bidirectional lstm for robot-assisted servicing in hospital. ||| che-wen chen ||| shih-pang tseng ||| ta-wen kuan ||| jhing-fa wang ||| 
2021 ||| canet: a combined attention network for remote sensing image change detection. ||| di lu ||| liejun wang ||| shuli cheng ||| yongming li ||| anyu du ||| 
2020 ||| crowd counting guided by attention network. ||| pei nie ||| cien fan ||| lian zou ||| liqiong chen ||| xiaopeng li ||| 
2020 ||| an attention-based model using character composition of entities in chinese relation extraction. ||| xiaoyu han ||| yue zhang ||| wenkai zhang ||| tinglei huang ||| 
2020 ||| adversarial hard attention adaptation. ||| hui tao ||| jun he ||| quan-jie cao ||| lei zhang ||| 
2019 ||| attention and signal detection. ||| adam j. reeves ||| 
2020 ||| cwpc_biatt: character-word-position combined bilstm-attention for chinese named entity recognition. ||| shardrom johnson ||| sherlock shen ||| yuanchen liu ||| 
2020 ||| information needs and visual attention during urban, highly automated driving - an investigation of potential influencing factors. ||| alexander feierle ||| simon danner ||| sarah steininger ||| klaus bengler ||| 
2021 ||| financial volatility forecasting: a sparse multi-head attention neural network. ||| hualing lin ||| qiubi sun ||| 
2020 ||| triadic automata and machines as information transformers. ||| mark burgin ||| 
2020 ||| a novel method for twitter sentiment analysis based on attentional-graph neural network. ||| mingda wang ||| guangmin hu ||| 
2021 ||| feature extraction network with attention mechanism for data enhancement and recombination fusion for multimodal sentiment analysis. ||| qingfu qi ||| liyuan lin ||| rui zhang ||| 
2021 ||| deep hash with improved dual attention for image retrieval. ||| wenjing yang ||| liejun wang ||| shuli cheng ||| yongming li ||| anyu du ||| 
2020 ||| vehicle pedestrian detection method based on spatial pyramid pooling and attention mechanism. ||| mingtao guo ||| donghui xue ||| peng li ||| he xu ||| 
2022 ||| a bidirectional context embedding transformer for automatic speech recognition. ||| lyuchao liao ||| francis afedzie kwofie ||| zhifeng chen ||| guangjie han ||| yongqiang wang ||| yuyuan lin ||| dongmei hu ||| 
2021 ||| does salience of neighbor-comparison information attract attention and conserve energy? eye-tracking experiment and interview with korean local apartment residents. ||| sunghee choi ||| 
2020 ||| antonyms: a computer game to improve inhibitory control of impulsivity in children with attention deficit/hyperactivity disorder (adhd). ||| maura crepaldi ||| vera colombo ||| stefano mottura ||| davide baldassini ||| marco sacco ||| alice cancer ||| alessandro antonietti ||| 
2022 ||| object detection of road assets using transformer-based yolox with feature pyramid decoder on thai highway panorama. ||| teerapong panboonyuen ||| sittinun thongbai ||| weerachai wongweeranimit ||| phisan santitamnont ||| kittiwan suphan ||| chaiyut charoenphon ||| 
2020 ||| multilingual transformer-based personality traits estimation. ||| simone leonardi ||| diego monti ||| giuseppe rizzo ||| maurizio morisio ||| 
2019 ||| attention-based joint entity linking with entity embedding. ||| chen liu ||| feng li ||| xian sun ||| hongzhe han ||| 
2021 ||| cyberbullying detection in social networks using bi-gru with self-attention mechanism. ||| yong fang ||| shaoshuai yang ||| bin zhao ||| cheng huang ||| 
2021 ||| short-term load forecasting based on the transformer model. ||| zezheng zhao ||| chunqiu xia ||| lian chi ||| xiaomin chang ||| wei li ||| ting yang ||| albert y. zomaya ||| 
2020 ||| spatiotemporal convolutional neural network with convolutional block attention module for micro-expression recognition. ||| boyu chen ||| zhihao zhang ||| nian liu ||| yang tan ||| xinyu liu ||| tong chen ||| 
2021 ||| a transformer-based framework for neutralizing and reversing the political polarity of news articles. ||| ruibo liu ||| chenyan jia ||| soroush vosoughi ||| 
2017 ||| credibility and the dynamics of collective attention. ||| tanushree mitra ||| graham p. wright ||| eric gilbert ||| 
2020 ||| gestatten: estimation of user's attention in mobile moocs from eye gaze and gaze gesture tracking. ||| pragma kar ||| samiran chattopadhyay ||| sandip chakraborty ||| 
2020 ||| looking for a deal?: visual social attention during negotiations via mixed media videoconferencing. ||| hana vrzakova ||| mary jean amon ||| mckenzie rees ||| myrthe faber ||| sidney d'mello ||| 
2019 ||| estimating attention flow in online video networks. ||| siqi wu ||| marian-andrei rizoiu ||| lexing xie ||| 
2020 ||| quality of and attention to instructions in telementoring. ||| azin semsar ||| hannah mcgowan ||| yuanyuan feng ||| hamid reza zahiri ||| adrian park ||| andrea kleinsmith ||| helena m. mentis ||| 
2017 ||| types of motivation affect study selection, attention, and dropouts in online experiments. ||| eunice jun ||| gary hsieh ||| katharina reinecke ||| 
2019 ||| smac: a simplified model of attention and capture in multi-device desk-centric environments. ||| zhen li ||| michelle annett ||| ken hinckley ||| daniel wigdor ||| 
2019 ||| ba-pnn-based methods for power transformer fault diagnosis. ||| xiaohui yang ||| wenkai chen ||| anyi li ||| chunsheng yang ||| zihao xie ||| huanyu dong ||| 
2022 ||| heterogeneous star graph attention network for product attributes prediction. ||| xuejiao zhao ||| yong liu ||| yonghui xu ||| yonghua yang ||| xusheng luo ||| chunyan miao ||| 
2020 ||| a cnn-based personalized system for attention detection in wayfinding tasks. ||| yanchao wang ||| yangming shi ||| jing du ||| yingzi lin ||| qi wang ||| 
2020 ||| aicf: attention-based item collaborative filtering. ||| yanxia lv ||| ying zheng ||| fangna wei ||| cong wang ||| cuirong wang ||| 
2021 ||| driver behavior detection via adaptive spatial attention mechanism. ||| lei zhao ||| fei yang ||| lingguo bu ||| su han ||| guoxin zhang ||| ying luo ||| 
2021 ||| a newly-designed fault diagnostic method for transformers via improved empirical wavelet transform and kernel extreme learning machine. ||| sijia lu ||| wei gao ||| cui hong ||| yiqun sun ||| 
2021 ||| research on hybrid feature selection method of power transformer based on fuzzy information entropy. ||| song yu ||| weimin tan ||| chengming zhang ||| yun fang ||| chao tang ||| dong hu ||| 
2019 ||| recognizing people's identity in construction sites with computer vision: a spatial and temporal attention pooling network. ||| ran wei ||| peter e. d. love ||| weili fang ||| hanbin luo ||| shuangjie xu ||| 
2020 ||| review visual attention and spatial memory in building inspection: toward a cognition-driven information system. ||| yangming shi ||| jing du ||| eric d. ragan ||| 
2021 ||| a novel deep learning prediction model for concrete dam displacements using interpretable mixed attention mechanism. ||| qiubing ren ||| mingchao li ||| heng li ||| yang shen ||| 
2021 ||| an autoencoder wavelet based deep neural network with attention mechanism for multi-step prediction of plant growth. ||| bashar alhnaity ||| stefanos d. kollias ||| georgios leontidis ||| shouyong jiang ||| bert schamp ||| simon pearson ||| 
2018 ||| deep attention network for joint hand gesture localization and recognition using static rgb-d images. ||| yuan li ||| xinggang wang ||| wenyu liu ||| bin feng ||| 
2019 ||| self-attention convolutional neural network for improved mr image reconstruction. ||| yan wu ||| yajun ma ||| jing liu ||| jiang du ||| lei xing ||| 
2020 ||| dispatched attention with multi-task learning for nested mention recognition. ||| hao fei ||| yafeng ren ||| donghong ji ||| 
2022 ||| knowledge graph embedding by logical-default attention graph convolution neural network for link prediction. ||| jiarui zhang ||| jian huang ||| jialong gao ||| runhai han ||| cong zhou ||| 
2021 ||| attention guided for partial domain adaptation. ||| changchun zhang ||| qingjie zhao ||| 
2021 ||| collaborative filtering with a deep adversarial and attention network for cross-domain recommendation. ||| huiting liu ||| lingling guo ||| peipei li ||| peng zhao ||| xindong wu ||| 
2021 ||| cross-domain sentiment classification via parameter transferring and attention sharing mechanism. ||| chuanjun zhao ||| suge wang ||| deyu li ||| xianzhi liu ||| xinyi yang ||| jinfeng liu ||| 
2021 ||| dual attention guided multi-scale cnn for fine-grained image classification. ||| xiaozhang liu ||| lifeng zhang ||| tao li ||| dejian wang ||| zhaojie wang ||| 
2020 ||| semantic relation extraction using sequential and tree-structured lstm with attention. ||| zhiqiang geng ||| guofei chen ||| yongming han ||| gang lu ||| fang li ||| 
2021 ||| facial expression recognition with grid-wise attention and visual transformer. ||| qionghao huang ||| changqin huang ||| xizhe wang ||| fan jiang ||| 
2022 ||| a pattern-aware self-attention network for distant supervised relation extraction. ||| yuming shang ||| heyan huang ||| xin sun ||| wei wei ||| xian-ling mao ||| 
2021 ||| fusion of heterogeneous attention mechanisms in multi-view convolutional neural network for text classification. ||| yunji liang ||| huihui li ||| bin guo ||| zhiwen yu ||| xiaolong zheng ||| sagar samtani ||| daniel dajun zeng ||| 
2017 ||| visual attention analysis and prediction on human faces. ||| xiongkuo min ||| guangtao zhai ||| ke gu ||| jing liu ||| shiqi wang ||| xinfeng zhang ||| xiaokang yang ||| 
2020 ||| learning reinforced attentional representation for end-to-end visual tracking. ||| peng gao ||| qiquan zhang ||| fei wang ||| liyi xiao ||| hamido fujita ||| yan zhang ||| 
2021 ||| attention enhanced long short-term memory network with multi-source heterogeneous information fusion: an application to bgi genomics. ||| qun zhang ||| lijun yang ||| feng zhou ||| 
2021 ||| dig users' intentions via attention flow network for personalized recommendation. ||| yan chen ||| yongfang dai ||| xiulong han ||| yi ge ||| hong yin ||| ping li ||| 
2022 ||| hybrid attention network based on progressive embedding scale-context for crowd counting. ||| fusen wang ||| jun sang ||| zhongyuan wu ||| qi liu ||| nong sang ||| 
2020 ||| a spatiotemporal attention mechanism-based model for multi-step citywide passenger demand prediction. ||| yirong zhou ||| jun li ||| hao chen ||| ye wu ||| jiangjiang wu ||| luo chen ||| 
2020 ||| stmag: a spatial-temporal mixed attention graph-based convolution model for multi-data flow safety prediction. ||| jingjuan wang ||| qingkui chen ||| huilin gong ||| 
2021 ||| claver: an integrated framework of convolutional layer, bidirectional lstm with attention mechanism based scholarly venue recommendation. ||| tribikram pradhan ||| prashant kumar ||| sukomal pal ||| 
2020 ||| attention-aware perceptual enhancement nets for low-resolution image classification. ||| xiaobin zhu ||| zhuangzi li ||| xianbo li ||| shanshan li ||| feng dai ||| 
2020 ||| transferable attention networks for adversarial domain adaptation. ||| changchun zhang ||| qingjie zhao ||| yu wang ||| 
2020 ||| attention-based bidirectional gru networks for efficient https traffic classification. ||| xun liu ||| junling you ||| yulei wu ||| tong li ||| liangxiong li ||| zheyuan zhang ||| jingguo ge ||| 
2022 ||| global and local attention-based multi-label learning with missing labels. ||| yusheng cheng ||| kun qian ||| fan min ||| 
2021 ||| attention-based word embeddings using artificial bee colony algorithm for aspect-level sentiment classification. ||| ming zhang ||| vasile palade ||| yan wang ||| zhicheng ji ||| 
2021 ||| taert: triple-attentional explainable recommendation with temporal convolutional network. ||| siyuan guo ||| ying wang ||| hao yuan ||| zeyu huang ||| jianwei chen ||| xin wang ||| 
2020 ||| attention-based context-aware sequential recommendation model. ||| weihua yuan ||| hong wang ||| xiaomei yu ||| nan liu ||| zhenghao li ||| 
2019 ||| attention-based spatio-temporal dependence learning network. ||| qianli ma ||| shuai tian ||| jia wei ||| jiabing wang ||| wing w. y. ng ||| 
2021 ||| a spatiotemporal hierarchical attention mechanism-based model for multi-step station-level crowd flow prediction. ||| yirong zhou ||| jun li ||| hao chen ||| ye wu ||| jiangjiang wu ||| luo chen ||| 
2021 ||| pgra: projected graph relation-feature attention network for heterogeneous information network embedding. ||| nuttapong chairatanakul ||| xin liu ||| tsuyoshi murata ||| 
2021 ||| document-level relation extraction with entity-selection attention. ||| changsen yuan ||| heyan huang ||| chong feng ||| ge shi ||| xiaochi wei ||| 
2021 ||| attention-adaptive and deformable convolutional modules for dynamic scene deblurring. ||| lei chen ||| quansen sun ||| fanhai wang ||| 
2021 ||| integrating object proposal with attention networks for video saliency detection. ||| muwei jian ||| jiaojin wang ||| hui yu ||| gai-ge wang ||| 
2021 ||| attention based consistent semantic learning for micro-video scene recognition. ||| jie guo ||| xiushan nie ||| yuling ma ||| kashif shaheed ||| inam ullah ||| yilong yin ||| 
2021 ||| hybrid-attention guided network with multiple resolution features for person re-identification. ||| guoqing zhang ||| junchuan yang ||| yuhui zheng ||| ye wang ||| yi wu ||| shengyong chen ||| 
2021 ||| tagcn: station-level demand prediction for bike-sharing system via a temporal attention graph convolution network. ||| wenjie zi ||| wei xiong ||| hao chen ||| luo chen ||| 
2021 ||| interpretable duplicate question detection models based on attention mechanism. ||| qifeng zhou ||| xiang liu ||| qing wang ||| 
2021 ||| dsagan: a generative adversarial network based on dual-stream attention mechanism for anatomical and functional image fusion. ||| jun fu ||| weisheng li ||| jiao du ||| liming xu ||| 
2021 ||| joint extraction of entities and relations via an entity correlated attention neural model. ||| ren li ||| dong li ||| jianxi yang ||| fangyue xiang ||| hao ren ||| shixin jiang ||| luyi zhang ||| 
2021 ||| hierarchical segment-channel attention network for explainable multichannel signal classification. ||| jiyoon lee ||| hyungrok do ||| mingu kwak ||| hyungu kahng ||| seoung bum kim ||| 
2019 ||| learning peer recommendation using attention-driven cnn with interaction tripartite graph. ||| qintai hu ||| zhongmei han ||| xiaofan lin ||| qionghao huang ||| xiaomei zhang ||| 
2018 ||| attention driven multi-modal similarity learning. ||| xinjian gao ||| tingting mu ||| john yannis goulermas ||| meng wang ||| 
2021 ||| pa-net: learning local features using by pose attention for short-term person re-identification. ||| kai wang ||| shichao dong ||| nian liu ||| junhui yang ||| tao li ||| qinghua hu ||| 
2021 ||| uda: a user-difference attention for group recommendation. ||| shuxun zan ||| yujie zhang ||| xiangwu meng ||| pengtao lv ||| yulu du ||| 
2021 ||| fine-grained learning performance prediction via adaptive sparse self-attention networks. ||| xizhe wang ||| xiaoyong mei ||| qionghao huang ||| zhongmei han ||| changqin huang ||| 
2022 ||| mgat-esm: multi-channel graph attention neural network with event-sharing module for rumor detection. ||| hongyan ran ||| caiyan jia ||| pengfei zhang ||| xuanya li ||| 
2021 ||| learning sentiment sentence representation with multiview attention model. ||| you zhang ||| jin wang ||| xuejie zhang ||| 
2020 ||| cross-attentional bracket-shaped convolutional network for semantic image segmentation. ||| cam-hao hua ||| thien huynh-the ||| sung-ho bae ||| sungyoung lee ||| 
2022 ||| deep multi-scale attention network for rna-binding proteins prediction. ||| bo du ||| ziyi liu ||| fulin luo ||| 
2022 ||| self-attention-based multi-agent continuous control method in cooperative environments. ||| kai liu ||| yuyang zhao ||| gang wang ||| bei peng ||| 
2021 ||| exploiting dynamic spatio-temporal correlations for citywide traffic flow prediction using attention based neural networks. ||| ahmad ali ||| yanmin zhu ||| muhammad zakarya ||| 
2022 ||| convolutional attention neural network over graph structures for improving the performance of aspect-level sentiment analysis. ||| huyen trang phan ||| ngoc thanh nguyen ||| dosam hwang ||| 
2021 ||| dfiam: deep factorization integrated attention mechanism for smart tv recommendation. ||| yijie zhou ||| xuewen shen ||| suiyu zhang ||| dingguo yu ||| guandong xu ||| 
2021 ||| open-world knowledge graph completion with multiple interaction attention. ||| lei niu ||| chenpeng fu ||| qiang yang ||| zhixu li ||| zhigang chen ||| qingsheng liu ||| kai zheng ||| 
2020 ||| incorporating word attention with convolutional neural networks for abstractive summarization. ||| chengzhe yuan ||| zhifeng bao ||| mark sanderson ||| yong tang ||| 
2019 ||| attention based hierarchical lstm network for context-aware microblog sentiment classification. ||| shi feng ||| yang wang ||| liran liu ||| daling wang ||| ge yu ||| 
2021 ||| emochannel-sa: exploring emotional dependency towards classification task with self-attention mechanism. ||| zongxi li ||| xinhong chen ||| haoran xie ||| qing li ||| xiaohui tao ||| gary cheng ||| 
2021 ||| modalnet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network. ||| zhe zhang ||| zhu wang ||| xiaona li ||| nannan liu ||| bin guo ||| zhiwen yu ||| 
2020 ||| hybrid graph convolutional networks with multi-head attention for location recommendation. ||| ting zhong ||| shengming zhang ||| fan zhou ||| kunpeng zhang ||| goce trajcevski ||| jin wu ||| 
2019 ||| residual attention-based lstm for video captioning. ||| xiangpeng li ||| zhilong zhou ||| lijiang chen ||| lianli gao ||| 
2021 ||| aspect-based sentiment analysis for online reviews with hybrid attention networks. ||| yuming lin ||| yu fu ||| you li ||| guoyong cai ||| aoying zhou ||| 
2021 ||| a multi-view attention-based deep learning system for online deviant content detection. ||| yunji liang ||| bin guo ||| zhiwen yu ||| xiaolong zheng ||| zhu wang ||| lei tang ||| 
2021 ||| an approximation theorist's view on solving operator equations - with special attention to trefftz, mfs, mps, and drm methods. ||| robert schaback ||| 
2021 ||| power electronic transformer design with dual-pwm based on matlab/simulink. ||| en fang ||| lulu bei ||| jiu-yang mu ||| guan-bao zhang ||| song-hai zhou ||| 
2021 ||| ethics, rules of engagement, and ai: neural narrative mapping using large transformer language models. ||| philip feldman ||| aaron dant ||| david rosenbluth ||| 
2020 ||| an approach on lifetime estimation of distribution transformers based on degree of polymerization. ||| mohamadreza ariannik ||| ali asghar razi kazemi ||| matti lehtonen ||| 
2022 ||| attention-based deep survival model for time series data. ||| xingyu li ||| vasiliy krivtsov ||| karunesh arora ||| 
2021 ||| hierarchical attention graph convolutional network to fuse multi-sensor signals for remaining useful life prediction. ||| tianfu li ||| zhibin zhao ||| chuang sun ||| ruqiang yan ||| xuefeng chen ||| 
2021 ||| a novel temporal convolutional network with residual self-attention mechanism for remaining useful life prediction of rolling bearings. ||| yudong cao ||| yifei ding ||| minping jia ||| rushuai tian ||| 
2019 ||| an improved polynomial-based nonlinear variable importance measure and its application to degradation assessment for high-voltage transformer under imbalance data. ||| jin cheng ||| jian wang ||| xuezhou wu ||| shuo wang ||| 
2018 ||| top-down neural attention by excitation backprop. ||| jianming zhang ||| sarah adel bargal ||| zhe lin ||| jonathan brandt ||| xiaohui shen ||| stan sclaroff ||| 
2021 ||| deepvs2.0: a saliency-structured deep learning method for predicting dynamic visual attention. ||| lai jiang ||| mai xu ||| zulin wang ||| leonid sigal ||| 
2021 ||| talk2nav: long-range vision-and-language navigation with dual attention and spatial memory. ||| arun balajee vasudevan ||| dengxin dai ||| luc van gool ||| 
2022 ||| dual-attention-guided network for ghost-free high dynamic range imaging. ||| qingsen yan ||| dong gong ||| qinfeng (javen) shi ||| anton van den hengel ||| chunhua shen ||| ian d. reid ||| yanning zhang ||| 
2021 ||| a-net: joint facial action unit detection and face alignment via adaptive attention. ||| zhiwen shao ||| zhilei liu ||| jianfei cai ||| lizhuang ma ||| 
2021 ||| attention guided low-light image enhancement with a large scale low-light simulation dataset. ||| feifan lv ||| yu li ||| feng lu ||| 
2019 ||| zoom out-and-in network with map attention decision for region proposal and object detection. ||| hongyang li ||| yu liu ||| wanli ouyang ||| xiaogang wang ||| 
2019 ||| hierarchical attention for part-aware face detection. ||| shuzhe wu ||| meina kan ||| shiguang shan ||| xilin chen ||| 
2021 ||| enhanced 3d human pose estimation from videos by using attention-based neural network with dilated convolutions. ||| ruixu liu ||| ju shen ||| he wang ||| chen chen ||| sen-ching samson cheung ||| vijayan k. asari ||| 
2020 ||| a simple and light-weight attention module for convolutional neural networks. ||| jongchan park ||| sanghyun woo ||| joon-young lee ||| in so kweon ||| 
2020 ||| robust attentional aggregation of deep feature sets for multi-view 3d reconstruction. ||| bo yang ||| sen wang ||| andrew markham ||| niki trigoni ||| 
2022 ||| codon: on orchestrating cross-domain attentions for depth super-resolution. ||| yuxiang yang ||| qi cao ||| jing zhang ||| dacheng tao ||| 
2020 ||| scalable person re-identification by harmonious attention. ||| wei li ||| xiatian zhu ||| shaogang gong ||| 
2020 ||| simultaneous deep stereo matching and dehazing with feature attention. ||| taeyong song ||| youngjung kim ||| changjae oh ||| hyunsung jang ||| namkoo ha ||| kwanghoon sohn ||| 
2021 ||| guided attention in cnns for occluded pedestrian detection and re-identification. ||| shanshan zhang ||| di chen ||| jian yang ||| bernt schiele ||| 
2020 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in cnns. ||| timo hackel ||| mikhail usvyatsov ||| silvano galliani ||| jan d. wegner ||| konrad schindler ||| 
2022 ||| perspectives and prospects on transformer architecture for cross-modal tasks with language and vision. ||| andrew shin ||| masato ishii ||| takuya narihira ||| 
2021 ||| selective wavelet attention learning for single image deraining. ||| huaibo huang ||| aijing yu ||| zhenhua chai ||| ran he ||| tieniu tan ||| 
2021 ||| continuous 3d multi-channel sign language production via progressive transformers and mixture density networks. ||| ben saunders ||| necati cihan camg ||| z ||| richard bowden ||| 
2021 ||| multi-level motion attention for human motion prediction. ||| wei mao ||| miaomiao liu ||| mathieu salzmann ||| hongdong li ||| 
2018 ||| traffic signal detection and classification in street views using an attention model. ||| yi-fan lu ||| jiaming lu ||| song-hai zhang ||| peter hall ||| 
2021 ||| pct: point cloud transformer. ||| meng-hao guo ||| junxiong cai ||| zheng-ning liu ||| tai-jiang mu ||| ralph r. martin ||| shi-min hu ||| 
2021 ||| can attention enable mlps to catch up with cnns? ||| meng-hao guo ||| zheng-ning liu ||| tai-jiang mu ||| dun liang ||| ralph r. martin ||| shi-min hu ||| 
2022 ||| transformers in computational visual media: a survey. ||| yifan xu ||| huapeng wei ||| minxuan lin ||| yingying deng ||| kekai sheng ||| mengdan zhang ||| fan tang ||| weiming dong ||| feiyue huang ||| changsheng xu ||| 
2019 ||| recurrent 3d attentional networks for end-to-end active object recognition. ||| min liu ||| yifei shi ||| lintao zheng ||| kai xu ||| hui huang ||| dinesh manocha ||| 
2020 ||| high frequency transformer in electric traction with bidirectional dc-dc converter using customized embedded system. ||| sachin gee paul ||| c. s. ravichandran ||| 
2018 ||| efficient one-dimensional forward and inverse discrete wavelet transformers. ||| goran savic ||| milan prokin ||| vladimir m. rajovic ||| dragana prokin ||| 
2020 ||| a cnn-lstm network with attention approach for learning universal sentence representation in embedded system. ||| qunchao fu ||| cong wang ||| xu han ||| 
2021 ||| fine grained sentiment polarity classification using augmented knowledge sequence-attention mechanism. ||| sindhu c ||| vadivu g ||| 
2021 ||| design and optimization for current transformer core based on magnetic field analysis. ||| guoyong zhang ||| jing luo ||| lifu he ||| xiudong zhou ||| shaosheng fan ||| 
2019 ||| an lstm-cnn attention approach for aspect-level sentiment classification. ||| ming jiang ||| wen zhang ||| min zhang ||| jianping wu ||| tao wen ||| 
2021 ||| multiway dynamic mask attention networks for natural language inference. ||| jingfan tang ||| xinqiang wu ||| min zhang ||| xiujie zhang ||| ming jiang ||| 
2021 ||| self-attention networks for code search. ||| sen fang ||| youshuai tan ||| tao zhang ||| yepang liu ||| 
2021 ||| improving requirements specification use by transferring attention with eye tracking data. ||| maike ahrens ||| kurt schneider ||| 
2018 ||| semantic parsing natural language into sparql: improving target language representation with neural attention. ||| fabiano ferreira luz ||| marcelo finger ||| 
2018 ||| channel attention and multi-level features fusion for single image super-resolution. ||| yue lu ||| yun zhou ||| zhuqing jiang ||| xiaoqiang guo ||| zixuan yang ||| 
2021 ||| cltr: an end-to-end, transformer-based system for cell level table retrieval and table question answering. ||| feifei pan ||| mustafa canim ||| michael r. glass ||| alfio gliozzo ||| peter fox ||| 
2020 ||| self-attention-based bigru and capsule network for named entity recognition. ||| jianfeng deng ||| lianglun cheng ||| zhuowei wang ||| 
2017 ||| identity-aware textual-visual matching with latent co-attention. ||| shuang li ||| tong xiao ||| hongsheng li ||| wei yang ||| xiaogang wang ||| 
2021 ||| ammu - a survey of transformer-based biomedical pretrained language models. ||| katikapalli subramanyam kalyan ||| ajit rajasekharan ||| sivanesan sangeetha ||| 
2020 ||| attention-guided discriminative region localization for bone age assessment. ||| chao chen ||| zhihong chen ||| xinyu jin ||| lanjuan li ||| william speier ||| corey w. arnold ||| 
2021 ||| tcl: transformer-based dynamic graph modelling via contrastive learning. ||| lu wang ||| xiaofu chang ||| shuang li ||| yunfei chu ||| hui li ||| wei zhang ||| xiaofeng he ||| le song ||| jingren zhou ||| hongxia yang ||| 
2020 ||| end-to-end video instance segmentation with transformers. ||| yuqing wang ||| zhaoliang xu ||| xinlong wang ||| chunhua shen ||| baoshan cheng ||| hao shen ||| huaxia xia ||| 
2018 ||| summarizing videos with attention. ||| jiri fajtl ||| hajar sadeghi sokeh ||| vasileios argyriou ||| dorothy monekosso ||| paolo remagnino ||| 
2019 ||| computer-aided diagnosis in histopathological images of the endometrium using a convolutional neural network and attention mechanisms. ||| hao sun ||| xianxu zeng ||| tao xu ||| gang peng ||| yutao ma ||| 
2019 ||| attention-based modeling for emotion detection and classification in textual conversations. ||| waleed ragheb ||| j ||| r ||| me az ||| sandra bringay ||| maximilien servajean ||| 
2020 ||| image-based vehicle re-identification model with adaptive attention modules and metadata re-ranking. ||| quang truong ||| hy dang ||| zhankai ye ||| minh nguyen ||| bo mei ||| 
2021 ||| aaformer: auto-aligned transformer for person re-identification. ||| kuan zhu ||| haiyun guo ||| shiliang zhang ||| yaowei wang ||| gaopan huang ||| honglin qiao ||| jing liu ||| jinqiao wang ||| ming tang ||| 
2022 ||| a text attention network for spatial deformation robust scene text image super-resolution. ||| jianqi ma ||| zhetong liang ||| lei zhang ||| 
2021 ||| encoder fusion network with co-attention embedding for referring image segmentation. ||| guang feng ||| zhiwei hu ||| lihe zhang ||| huchuan lu ||| 
2019 ||| constructive type-logical supertagging with self-attention networks. ||| konstantinos kogkalidis ||| michael moortgat ||| tejaswini deoskar ||| 
2021 ||| situation-aware environment perception using a multi-layer attention map. ||| matti henning ||| johannes m ||| ller ||| fabian gies ||| michael buchholz ||| klaus dietmayer ||| 
2021 ||| pose discrepancy spatial transformer based feature disentangling for partial aspect angles sar target recognition. ||| zaidao wen ||| jiaxiang liu ||| zhunga liu ||| quan pan ||| 
2021 ||| vidface: a full-transformer solver for video facehallucination with unaligned tiny snapshots. ||| yuan gan ||| yawei luo ||| xin yu ||| bang zhang ||| yi yang ||| 
2021 ||| using self-supervised feature extractors with attention for automatic covid-19 detection from speech. ||| john mendon ||| a ||| rub ||| n solera-ure ||| a ||| alberto abad ||| isabel trancoso ||| 
2021 ||| pama-tts: progression-aware monotonic attention for stable seq2seq tts with accurate phoneme duration control. ||| yunchao he ||| jian luan ||| yujun wang ||| 
2020 ||| optimizing deeper transformers on small datasets: an application on text-to-sql semantic parsing. ||| peng xu ||| wei yang ||| wenjie zi ||| keyi tang ||| chengyang huang ||| jackie chi kit cheung ||| yanshuai cao ||| 
2021 ||| adversarial token attacks on vision transformers. ||| ameya joshi ||| gauri jagatap ||| chinmay hegde ||| 
2021 ||| sparse spatial attention network for semantic segmentation. ||| mengyu liu ||| hujun yin ||| 
2020 ||| tado: time-varying attention with dual-optimizer model. ||| yuexin wu ||| tianyu gao ||| sihao wang ||| zhongmin xiong ||| 
2019 ||| an empirical study of spatial attention mechanisms in deep networks. ||| xizhou zhu ||| dazhi cheng ||| zheng zhang ||| stephen lin ||| jifeng dai ||| 
2021 ||| combining cnns with transformer for multimodal 3d mri brain tumor segmentation with self-supervised pretraining. ||| mariia dobko ||| danylo-ivan kolinko ||| ostap viniavskyi ||| yurii yelisieiev ||| 
2021 ||| trouspi-net: spatio-temporal attention on parallel atrous convolutions and u-grus for skeletal pedestrian crossing prediction. ||| joseph gesnouin ||| steve pechberti ||| bogdan stanciulescu ||| fabien moutarde ||| 
2021 ||| doodleformer: creative sketch drawing with transformers. ||| ankan kumar bhunia ||| salman khan ||| hisham cholakkal ||| rao muhammad anwer ||| fahad shahbaz khan ||| jorma laaksonen ||| michael felsberg ||| 
2018 ||| development of spatial suppression surrounding the focus of visual attention. ||| audrey m. b. wong-kee-you ||| john k. tsotsos ||| scott a. adler ||| 
2019 ||| compressive transformers for long-range sequence modelling. ||| jack w. rae ||| anna potapenko ||| siddhant m. jayakumar ||| timothy p. lillicrap ||| 
2017 ||| attentional factorization machines: learning the weight of feature interactions via attention networks. ||| jun xiao ||| hao ye ||| xiangnan he ||| hanwang zhang ||| fei wu ||| tat-seng chua ||| 
2018 ||| show, attend and translate: unpaired multi-domain image-to-image translation with visual attention. ||| honglun zhang ||| wenqing chen ||| jidong tian ||| yongkun wang ||| yaohui jin ||| 
2020 ||| answer-checking in context: a multi-modal fullyattention network for visual question answering. ||| hantao huang ||| tao han ||| wei han ||| deep yap ||| cheng-ming chiang ||| 
2021 ||| cat: cross-attention transformer for one-shot object detection. ||| weidong lin ||| yuyan deng ||| yang gao ||| ning wang ||| jinghao zhou ||| lingqiao liu ||| lei zhang ||| peng wang ||| 
2020 ||| chroma intra prediction with attention-based cnn architectures. ||| marc g ||| rriz ||| saverio g. blasi ||| alan f. smeaton ||| noel e. o'connor ||| marta mrak ||| 
2018 ||| self-attention: a better building block for sentiment analysis neural network classifiers. ||| artaches ambartsoumian ||| fred popowich ||| 
2020 ||| catching attention with automatic pull quote selection. ||| tanner a. bohn ||| charles x. ling ||| 
2020 ||| complaint identification in social media with transformer networks. ||| mali jin ||| nikolaos aletras ||| 
2022 ||| kformer: knowledge injection in transformer feed-forward layers. ||| yunzhi yao ||| shaohan huang ||| ningyu zhang ||| li dong ||| furu wei ||| huajun chen ||| 
2022 ||| scorenet: learning non-uniform attention and augmentation for transformer-based histopathological image classification. ||| thomas stegm ||| ller ||| antoine spahr ||| behzad bozorgtabar ||| jean-philippe thiran ||| 
2021 ||| clta: contents and length-based temporal attention for few-shot action recognition. ||| yang bo ||| yangdi lu ||| wenbo he ||| 
2021 ||| predicting opioid use disorder from longitudinal healthcare data using multi-stream transformer. ||| sajjad fouladvand ||| jeffery c. talbert ||| linda p. dwoskin ||| heather bush ||| amy lynn meadows ||| lars e. peterson ||| ramakanth kavuluru ||| jin chen ||| 
2021 ||| compositional generalization in semantic parsing with pretrained transformers. ||| a. emin orhan ||| 
2018 ||| sarn: relational reasoning through sequential attention. ||| jinwon an ||| sungwon lyu ||| sungzoon cho ||| 
2018 ||| conditional transfer with dense residual attention: synthesizing traffic signs from street-view imagery. ||| clint sebastian ||| ries uittenbogaard ||| julien a. vijverberg ||| bas boom ||| peter h. n. de with ||| 
2021 ||| subformer: exploring weight sharing for parameter efficiency in generative transformers. ||| machel reid ||| edison marrese-taylor ||| yutaka matsuo ||| 
2022 ||| the dual form of neural networks revisited: connecting test time predictions to training patterns via spotlights of attention. ||| kazuki irie ||| r ||| bert csord ||| s ||| j ||| rgen schmidhuber ||| 
2021 ||| 3d medical point transformer: introducing convolution to attention networks for medical point cloud analysis. ||| jianhui yu ||| chaoyi zhang ||| heng wang ||| dingxin zhang ||| yang song ||| tiange xiang ||| dongnan liu ||| weidong cai ||| 
2021 ||| do time constraints re-prioritize attention to shapes during visual photo inspection? ||| yiyuan yang ||| kenneth li ||| fernanda monteiro eliott ||| maithilee kunda ||| 
2020 ||| attentional biased stochastic gradient for imbalanced classification. ||| qi qi ||| yi xu ||| rong jin ||| wotao yin ||| tianbao yang ||| 
2021 ||| mutualformer: multi-modality representation learning via mutual transformer. ||| xixi wang ||| bo jiang ||| xiao wang ||| bin luo ||| 
2021 ||| karl-trans-ner: knowledge aware representation learning for named entity recognition using transformers. ||| avi chawla ||| nidhi mulay ||| vikas bishnoi ||| gaurav dhama ||| 
2021 ||| stage conscious attention network (scan) : a demonstration-conditioned policy for few-shot imitation. ||| jia-fong yeh ||| chi-ming chung ||| hung-ting su ||| yi-ting chen ||| winston h. hsu ||| 
2019 ||| conversational emotion analysis via attention mechanisms. ||| zheng lian ||| jianhua tao ||| bin liu ||| jian huang ||| 
2021 ||| an exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers. ||| tharindu ranasinghe ||| constantin orasan ||| ruslan mitkov ||| 
2020 ||| linear attention mechanism: an efficient attention for semantic segmentation. ||| rui li ||| jianlin su ||| chenxi duan ||| shunyi zheng ||| 
2022 ||| multimodal pre-training based on graph attention network for document understanding. ||| zhenrong zhang ||| jiefeng ma ||| jun du ||| licheng wang ||| jianshu zhang ||| 
2017 ||| human action recognition: pose-based attention draws focus to hands. ||| fabien baradel ||| christian wolf ||| julien mille ||| 
2021 ||| residual attention based network for automatic classification of phonation modes. ||| xiaoheng sun ||| yiliang jiang ||| wei li ||| 
2021 ||| transformer-based deep image matching for generalizable person re-identification. ||| shengcai liao ||| ling shao ||| 
2020 ||| delight: very deep and light-weight transformer. ||| sachin mehta ||| marjan ghazvininejad ||| srinivasan iyer ||| luke zettlemoyer ||| hannaneh hajishirzi ||| 
2019 ||| spatio-temporal attention pooling for audio scene classification. ||| huy phan ||| oliver y. ch ||| n ||| lam dang pham ||| philipp koch ||| maarten de vos ||| ian mcloughlin ||| alfred mertins ||| 
2020 ||| progressive transformers for end-to-end sign language production. ||| ben saunders ||| necati cihan camg ||| z ||| richard bowden ||| 
2020 ||| encoding syntactic knowledge in transformer encoder for intent detection and slot filling. ||| jixuan wang ||| kai wei ||| martin radfar ||| weiwei zhang ||| clement chung ||| 
2020 ||| the monte carlo transformer: a stochastic self-attention model for sequence prediction. ||| alice martin ||| charles ollion ||| florian strub ||| sylvain le corff ||| olivier pietquin ||| 
2021 ||| transformer-based spatial-temporal feature learning for eeg decoding. ||| yonghao song ||| xueyu jia ||| lie yang ||| longhan xie ||| 
2017 ||| atrank: an attention-based user behavior modeling framework for recommendation. ||| chang zhou ||| jinze bai ||| junshuai song ||| xiaofei liu ||| zhengchao zhao ||| xiusi chen ||| jun gao ||| 
2018 ||| band selection from hyperspectral images using attention-based convolutional neural networks. ||| pablo ribalta lorenzo ||| lukasz tulczyjew ||| michal marcinkiewicz ||| jakub nalepa ||| 
2018 ||| generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization. ||| preksha nema ||| shreyas shetty ||| parag jain ||| anirban laha ||| karthik sankaranarayanan ||| mitesh m. khapra ||| 
2021 ||| localtrans: a multiscale local transformer network for cross-resolution homography estimation. ||| ruizhi shao ||| gaochang wu ||| yuemei zhou ||| ying fu ||| yebin liu ||| 
2020 ||| fine-grained visual textual alignment for cross-modal retrieval using transformer encoders. ||| nicola messina ||| giuseppe amato ||| andrea esuli ||| fabrizio falchi ||| claudio gennaro ||| st ||| phane marchand-maillet ||| 
2020 ||| privileged pooling: supervised attention-based pooling for compensating dataset bias. ||| andr ||| s c. rodr ||| guez ||| stefano d'aronco ||| konrad schindler ||| jan dirk wegner ||| 
2021 ||| lmms reloaded: transformer-based sense embeddings for disambiguation and beyond. ||| daniel loureiro ||| al ||| pio m ||| rio jorge ||| jos |||  camacho-collados ||| 
2021 ||| tcct: tightly-coupled convolutional transformer on time series forecasting. ||| li shen ||| yangzhu wang ||| 
2021 ||| searching for efficient multi-stage vision transformers. ||| yi-lun liao ||| sertac karaman ||| vivienne sze ||| 
2017 ||| attention is all you need. ||| ashish vaswani ||| noam shazeer ||| niki parmar ||| jakob uszkoreit ||| llion jones ||| aidan n. gomez ||| lukasz kaiser ||| illia polosukhin ||| 
2021 ||| billion-scale pretraining with vision transformers for multi-task visual representations. ||| josh beal ||| hao-yu wu ||| dong huk park ||| andrew zhai ||| dmitry kislyuk ||| 
2021 ||| dprost: 6-dof object pose estimation using space carving and dynamic projective spatial transformer. ||| jaewoo park ||| nam ik cho ||| 
2021 ||| a pressure ulcer care system for remote medical assistance: residual u-net with an attention model based for wound area segmentation. ||| jinyeong chae ||| ki yong hong ||| jihie kim ||| 
2020 ||| marathi to english neural machine translation with near perfect corpus and transformers. ||| swapnil ashok jadhav ||| 
2021 ||| stformer: a noise-aware efficient spatio-temporal transformer architecture for traffic forecasting. ||| yanjun qin ||| yuchen fang ||| haiyong luo ||| liang zeng ||| fang zhao ||| chenxing wang ||| 
2022 ||| a transformer-based siamese network for change detection. ||| wele gedara chaminda bandara ||| vishal m. patel ||| 
2021 ||| transformers: "the end of history" for nlp? ||| anton chernyavskiy ||| dmitry ilvovsky ||| preslav nakov ||| 
2021 ||| e-dssr: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception. ||| yonghao long ||| zhaoshuo li ||| chi hang yee ||| chi-fai ng ||| russell h. taylor ||| mathias unberath ||| qi dou ||| 
2021 ||| a new rotating machinery fault diagnosis method based on the time series transformer. ||| yuhong jin ||| lei hou ||| yushu chen ||| 
2019 ||| learning to dynamically coordinate multi-robot teams in graph attention networks. ||| zheyuan wang ||| matthew c. gombolay ||| 
2021 ||| hydra - hyper dependency representation attentions. ||| ha-thanh nguyen ||| vu tran ||| tran binh dang ||| minh-quan bui ||| minh phuong nguyen ||| le-minh nguyen ||| 
2021 ||| dual aspect self-attention based on transformer for remaining useful life prediction. ||| zhizheng zhang ||| wen song ||| qiqiang li ||| 
2019 ||| herding effect based attention for personalized time-sync video recommendation. ||| wenmian yang ||| wenyuan gao ||| xiaojie zhou ||| weijia jia ||| shaohua zhang ||| yutao luo ||| 
2020 ||| controllable time-delay transformer for real-time punctuation prediction and disfluency detection. ||| qian chen ||| mengzhe chen ||| bo li ||| wen wang ||| 
2021 ||| transformer-based conditional variational autoencoder for controllable story generation. ||| le fang ||| tao zeng ||| chaochun liu ||| liefeng bo ||| wen dong ||| changyou chen ||| 
2021 ||| do vision transformers see like convolutional neural networks? ||| maithra raghu ||| thomas unterthiner ||| simon kornblith ||| chiyuan zhang ||| alexey dosovitskiy ||| 
2021 ||| transunet: transformers make strong encoders for medical image segmentation. ||| jieneng chen ||| yongyi lu ||| qihang yu ||| xiangde luo ||| ehsan adeli ||| yan wang ||| le lu ||| alan l. yuille ||| yuyin zhou ||| 
2020 ||| pair: planning and iterative refinement in pre-trained transformers for long text generation. ||| xinyu hua ||| lu wang ||| 
2021 ||| ad text classification with transformer-based natural language processing methods. ||| umut  ||| zdil ||| b ||| sra arslan ||| d. emre tasar ||| g ||| k ||| e polat ||| s ||| kr |||  ozan ||| 
2019 ||| attribute-aware attention model for fine-grained representation learning. ||| kai han ||| jianyuan guo ||| chao zhang ||| mingjian zhu ||| 
2019 ||| tanet: robust 3d object detection from point clouds with triple attention. ||| zhe liu ||| xin zhao ||| tengteng huang ||| ruolan hu ||| yu zhou ||| xiang bai ||| 
2017 ||| segmentation-aware convolutional networks using local attention masks. ||| adam w. harley ||| konstantinos g. derpanis ||| iasonas kokkinos ||| 
2021 ||| maast: map attention with semantic transformersfor efficient visual navigation. ||| zachary seymour ||| kowshik thopalli ||| niluthpol chowdhury mithun ||| han-pang chiu ||| supun samarasekera ||| rakesh kumar ||| 
2021 ||| spatial graph attention and curiosity-driven policy for antiviral drug discovery. ||| yulun wu ||| nicholas choma ||| andrew chen ||| mikaela cashman ||| rica t. prates ||| manesh shah ||| ver ||| nica g. melesse vergara ||| austin clyde ||| thomas s. brettin ||| wibe a. de jong ||| neeraj kumar ||| martha s. head ||| rick l. stevens ||| peter nugent ||| daniel a. jacobson ||| james b. brown ||| 
2020 ||| space: unsupervised object-oriented scene representation via spatial attention and decomposition. ||| zhixuan lin ||| yi-fu wu ||| skand vishwanath peri ||| weihao sun ||| gautam singh ||| fei deng ||| jindong jiang ||| sungjin ahn ||| 
2021 ||| groupformer: group activity recognition with clustered spatial-temporal transformer. ||| shuaicheng li ||| qianggang cao ||| lingbo liu ||| kunlin yang ||| shinan liu ||| jun hou ||| shuai yi ||| 
2020 ||| joint self-attention and scale-aggregation for self-calibrated deraining network. ||| cong wang ||| yutong wu ||| zhixun su ||| junyang chen ||| 
2018 ||| couplenet: paying attention to couples with coupled attention for relationship recommendation. ||| yi tay ||| anh tuan luu ||| siu cheung hui ||| 
2021 ||| ms-tct: multi-scale temporal convtransformer for action detection. ||| rui dai ||| srijan das ||| kumara kahatapitiya ||| michael s. ryoo ||| fran ||| ois br ||| mond ||| 
2021 ||| end-to-end human object interaction detection with hoi transformer. ||| cheng zou ||| bohan wang ||| yue hu ||| junqi liu ||| qian wu ||| yu zhao ||| boxun li ||| chenguang zhang ||| chi zhang ||| yichen wei ||| jian sun ||| 
2018 ||| a neural attention model for speech command recognition. ||| douglas coimbra de andrade ||| sabato leo ||| martin loesener da silva viana ||| christoph bernkopf ||| 
2021 ||| temporal transformer networks with self-supervision for action recognition. ||| yongkang zhang ||| jun li ||| guoming wu ||| han zhang ||| zhiping shi ||| zhaoxun liu ||| zizhang wu ||| 
2020 ||| structured multimodal attentions for textvqa. ||| chenyu gao ||| qi zhu ||| peng wang ||| hui li ||| yuliang liu ||| anton van den hengel ||| qi wu ||| 
2021 ||| mm-vit: multi-modal video transformer for compressed video action recognition. ||| jiawei chen ||| chiu man ho ||| 
2022 ||| tgfuse: an infrared and visible image fusion approach based on transformer and generative adversarial network. ||| dongyu rao ||| xiao-jun wu ||| tianyang xu ||| 
2018 ||| decoupled spatial neural attention for weakly supervised semantic segmentation. ||| tianyi zhang ||| guosheng lin ||| jianfei cai ||| tong shen ||| chunhua shen ||| alex c. kot ||| 
2021 ||| traffic flow forecasting with maintenance downtime via multi-channel attention-based spatio-temporal graph convolutional networks. ||| yuanjie lu ||| parastoo kamranfar ||| david lattanzi ||| amarda shehu ||| 
2020 ||| multi-modal automated speech scoring using attention fusion. ||| manraj singh grover ||| yaman kumar ||| sumit sarin ||| payman vafaee ||| mika hama ||| rajiv ratn shah ||| 
2021 ||| agstn: learning attention-adjusted graph spatio-temporal networks for short-term urban sensor value forecasting. ||| yi-ju lu ||| cheng-te li ||| 
2020 ||| improving multimodal accuracy through modality pre-training and attention. ||| aya abdelsalam ismail ||| mahmudul hasan ||| faisal ishtiaq ||| 
2019 ||| behrt: transformer for electronic health records. ||| yikuan li ||| shishir rao ||| jos |||  roberto ayala solares ||| abdelaali hassa ||| ne ||| dexter canoy ||| yajie zhu ||| kazem rahimi ||| gholamreza salimi khorshidi ||| 
2021 ||| prototransformer: a meta-learning approach to providing student feedback. ||| mike wu ||| noah d. goodman ||| chris piech ||| chelsea finn ||| 
2022 ||| transformer-based sar image despeckling. ||| malsha v. perera ||| wele gedara chaminda bandara ||| jeya maria jose valanarasu ||| vishal m. patel ||| 
2019 ||| ognet: salient object detection with output-guided attention module. ||| shiping zhu ||| lanyun zhu ||| 
2019 ||| bert and pals: projected attention layers for efficient adaptation in multi-task learning. ||| asa cooper stickland ||| iain murray ||| 
2021 ||| extracting fine-grained knowledge graphs of scientific claims: dataset and transformer-based results. ||| ian h. magnusson ||| scott e. friedman ||| 
2021 ||| transhash: transformer-based hamming hashing for efficient image retrieval. ||| yongbiao chen ||| sheng zhang ||| fangxin liu ||| zhigang chang ||| mang ye ||| zhengwei qi ||| 
2019 ||| contrastive attention mechanism for abstractive sentence summarization. ||| xiangyu duan ||| hongfei yu ||| mingming yin ||| min zhang ||| weihua luo ||| yue zhang ||| 
2020 ||| a financial service chatbot based on deep bidirectional transformers. ||| shi yu ||| yuxin chen ||| hussain zaidi ||| 
2021 ||| trocr: transformer-based optical character recognition with pre-trained models. ||| minghao li ||| tengchao lv ||| lei cui ||| yijuan lu ||| dinei a. f. flor ||| ncio ||| cha zhang ||| zhoujun li ||| furu wei ||| 
2020 ||| unsupervised pansharpening based on self-attention mechanism. ||| ying qu ||| razieh kaviani baghbaderani ||| hairong qi ||| chiman kwan ||| 
2021 ||| adavit: adaptive vision transformers for efficient image recognition. ||| lingchen meng ||| hengduo li ||| bor-chun chen ||| shiyi lan ||| zuxuan wu ||| yu-gang jiang ||| ser-nam lim ||| 
2019 ||| improving transformer-based speech recognition using unsupervised pre-training. ||| dongwei jiang ||| xiaoning lei ||| wubo li ||| n ||| e luo ||| yuxuan hu ||| wei zou ||| xiangang li ||| 
2017 ||| latent attention networks. ||| christopher grimm ||| dilip arumugam ||| siddharth karamcheti ||| david abel ||| lawson l. s. wong ||| michael l. littman ||| 
2020 ||| deberta: decoding-enhanced bert with disentangled attention. ||| pengcheng he ||| xiaodong liu ||| jianfeng gao ||| weizhu chen ||| 
2021 ||| attention-based clinical note summarization. ||| neel kanwal ||| giuseppe rizzo ||| 
2021 ||| interaction detection between vehicles and vulnerable road users: a deep generative approach with attention. ||| hao cheng ||| li feng ||| hailong liu ||| takatsugu hirayama ||| hiroshi murase ||| monika sester ||| 
2020 ||| cosea: convolutional code search with layer-wise attention. ||| hao wang ||| jia zhang ||| yingce xia ||| jiang bian ||| chao zhang ||| tie-yan liu ||| 
2020 ||| selective attention encoders by syntactic graph convolutional networks for document summarization. ||| haiyang xu ||| yun wang ||| kun han ||| baochang ma ||| junwen chen ||| xiangang li ||| 
2018 ||| focusing on what is relevant: time-series learning and understanding using attention. ||| phongtharin vinayavekhin ||| subhajit chaudhury ||| asim munawar ||| don joven agravante ||| giovanni de magistris ||| daiki kimura ||| ryuki tachibana ||| 
2020 ||| cluster-former: clustering-based sparse transformer for long-range dependency encoding. ||| shuohang wang ||| luowei zhou ||| zhe gan ||| yen-chun chen ||| yuwei fang ||| siqi sun ||| yu cheng ||| jingjing liu ||| 
2021 ||| on isotropy calibration of transformers. ||| yue ding ||| karolis martinkus ||| damian pascual ||| simon clematide ||| roger wattenhofer ||| 
2021 ||| a fine-grained visual attention approach for fingerspelling recognition in the wild. ||| kamala gajurel ||| cuncong zhong ||| guanghui wang ||| 
2021 ||| interflow: aggregating multi-layer feature mappings with attention mechanism. ||| zhicheng cai ||| 
2021 ||| token pooling in vision transformers. ||| dmitrii marin ||| jen-hao rick chang ||| anurag ranjan ||| anish prabhu ||| mohammad rastegari ||| oncel tuzel ||| 
2020 ||| a generalization of transformer networks to graphs. ||| vijay prakash dwivedi ||| xavier bresson ||| 
2021 ||| laughing heads: can transformers detect what makes a sentence funny? ||| maxime peyrard ||| beatriz borges ||| kristina gligoric ||| robert west ||| 
2020 ||| scheduled drophead: a regularization method for transformer models. ||| wangchunshu zhou ||| tao ge ||| ke xu ||| furu wei ||| ming zhou ||| 
2021 ||| thank you for attention: a survey on attention-based artificial neural networks for automatic speech recognition. ||| priyabrata karmakar ||| shyh wei teng ||| guojun lu ||| 
2019 ||| focus your attention: a bidirectional focal attention network for image-text matching. ||| chunxiao liu ||| zhendong mao ||| an-an liu ||| tianzhu zhang ||| bin wang ||| yongdong zhang ||| 
2021 ||| an attention-driven hierarchical multi-scale representation for visual recognition. ||| zachary wharton ||| ardhendu behera ||| asish bera ||| 
2017 ||| star-rt: visual attention for real-time video game playing. ||| iuliia kotseruba ||| john k. tsotsos ||| 
2019 ||| action recognition in untrimmed videos with composite self-attention two-stream framework. ||| dong cao ||| lisha xu ||| haibo chen ||| 
2021 ||| predictive maintenance for general aviation using convolutional transformers. ||| hong yang ||| aidan labella ||| travis desell ||| 
2021 ||| recursive fusion and deformable spatiotemporal attention for video compression artifact reduction. ||| minyi zhao ||| yi xu ||| shuigeng zhou ||| 
2020 ||| combining predicate transformer semantics for effects: a case study in parsing regular languages. ||| anne baanen ||| wouter swierstra ||| 
2022 ||| attention based memory video portrait matting. ||| shufeng song ||| 
2020 ||| parallel machine translation with disentangled context transformer. ||| jungo kasai ||| james cross ||| marjan ghazvininejad ||| jiatao gu ||| 
2020 ||| depth potentiality-aware gated attention network for rgb-d salient object detection. ||| zuyao chen ||| qingming huang ||| 
2022 ||| towards unbiased multi-label zero-shot learning with pyramid and semantic attention. ||| ziming liu ||| song guo ||| jingcai guo ||| yuanyuan xu ||| fushuo huo ||| 
2021 ||| : interpretability-aware redundancy reduction for vision transformers. ||| bowen pan ||| yifan jiang ||| rameswar panda ||| zhangyang wang ||| rog ||| rio feris ||| aude oliva ||| 
2021 ||| attention toward neighbors: a context aware framework for high resolution image segmentation. ||| fahim faisal niloy ||| m. ashraful amin ||| amin ahsan ali ||| a. k. m. mahbubur rahman ||| 
2020 ||| zero-shot reinforcement learning with deep attention convolutional neural networks. ||| sahika genc ||| sunil mallya ||| sravan bodapati ||| tao sun ||| yunzhe tao ||| 
2020 ||| a hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention. ||| maria mihaela trusca ||| daan wassenberg ||| flavius frasincar ||| rommert dekker ||| 
2021 ||| supermeshing: a new deep learning architecture for increasing the mesh density of metal forming stress field with attention mechanism and perceptual features. ||| qingfeng xu ||| zhenguo nie ||| handing xu ||| haosu zhou ||| xinjun liu ||| 
2022 ||| tisat: time series anomaly transformer. ||| keval doshi ||| shatha abudalou ||| yasin yilmaz ||| 
2022 ||| cnn attention guidance for improved orthopedics radiographic fracture classification. ||| zhibin liao ||| kewen liao ||| haifeng shen ||| marouska f. van boxel ||| jasper prijs ||| ruurd l. jaarsma ||| job n. doornberg ||| anton van den hengel ||| johan w. verjans ||| 
2020 ||| sahdl: sparse attention hypergraph regularized dictionary learning. ||| shuai shao ||| rui xu ||| yanjiang wang ||| weifeng liu ||| bao-di liu ||| 
2021 ||| swinir: image restoration using swin transformer. ||| jingyun liang ||| jiezhang cao ||| guolei sun ||| kai zhang ||| luc van gool ||| radu timofte ||| 
2021 ||| scnet: a generalized attention-based model for crack fault segmentation. ||| hrishikesh sharma ||| prakhar pradhan ||| balamuralidhar purushothaman ||| 
2021 ||| weakly supervised attention model for rv strainclassification from volumetric ctpa scans. ||| noa cahan ||| edith m. marom ||| shelly soffer ||| yiftach barash ||| eli konen ||| eyal klang ||| hayit greenspan ||| 
2022 ||| multi-class token transformer for weakly supervised semantic segmentation. ||| lian xu ||| wanli ouyang ||| mohammed bennamoun ||| farid boussa ||| d ||| dan xu ||| 
2021 ||| deepfake detection scheme based on vision transformer and distillation. ||| young jin heo ||| young ju choi ||| young-woon lee ||| byung-gyu kim ||| 
2018 ||| attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions. ||| masanori suganuma ||| xing liu ||| takayuki okatani ||| 
2021 ||| real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network. ||| midia yousefi ||| john h. l. hansen ||| 
2021 ||| soft: softmax-free transformer with linear complexity. ||| jiachen lu ||| jinghan yao ||| junge zhang ||| xiatian zhu ||| hang xu ||| weiguo gao ||| chunjing xu ||| tao xiang ||| li zhang ||| 
2021 ||| attention based end to end speech recognition for voice search in hindi and english. ||| raviraj joshi ||| venkateshan kannan ||| 
2021 ||| introduce the result into self-attention. ||| chengcheng ye ||| 
2021 ||| lattention: lattice-attention in asr rescoring. ||| prabhat pandey ||| sergio duarte torres ||| ali orkan bayer ||| ankur gandhe ||| volker leutnant ||| 
2021 ||| sketching as a tool for understanding and accelerating self-attention for long sequences. ||| yifan chen ||| qi zeng ||| dilek hakkani-tur ||| di jin ||| heng ji ||| yun yang ||| 
2020 ||| public sentiment toward solar energy: opinion mining of twitter using a transformer-based language model. ||| serena y. kim ||| koushik ganesan ||| princess dickens ||| soumya panda ||| 
2020 ||| mqtransformer: multi-horizon forecasts with context dependent and feedback-aware attention. ||| carson eisenach ||| yagna patel ||| dhruv madeka ||| 
2020 ||| directed graph attention neural network utilizing 3d coordinates for molecular property prediction. ||| chen qian ||| yunhai xiong ||| xiang chen ||| 
2018 ||| two-level attention with two-stage multi-task learning for facial emotion recognition. ||| xiaohua wang ||| muzi peng ||| lijuan pan ||| min hu ||| chunhua jin ||| fuji ren ||| 
2020 ||| attentionanatomy: a unified framework for whole-body organs at risk segmentation using multiple partially annotated datasets. ||| shanlin sun ||| yang liu ||| narisu bai ||| hao tang ||| xuming chen ||| qian huang ||| yong liu ||| xiaohui xie ||| 
2021 ||| a study of latent monotonic attention variants. ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| gdca: gan-based single image super resolution with dual discriminators and channel attention. ||| thanh nguyen ||| hieu hoang ||| chang d. yoo ||| 
2021 ||| learning of frequency-time attention mechanism for automatic modulation recognition. ||| shangao lin ||| yuan zeng ||| yi gong ||| 
2021 ||| vtnet: visual transformer network for object goal navigation. ||| heming du ||| xin yu ||| liang zheng ||| 
2018 ||| training tips for the transformer model. ||| martin popel ||| ondrej bojar ||| 
2019 ||| deep discriminative representation learning with attention map for scene classification. ||| jun li ||| daoyu lin ||| yang wang ||| guangluan xu ||| chibiao ding ||| 
2020 ||| interpretable detail-fidelity attention network for single image super-resolution. ||| yuanfei huang ||| jie li ||| xinbo gao ||| yanting hu ||| wen lu ||| 
2020 ||| uld@nuig at semeval-2020 task 9: generative morphemes with an attention model for sentiment analysis in code-mixed text. ||| koustava goswami ||| priya rani ||| bharathi raja chakravarthi ||| theodorus fransen ||| john p. mccrae ||| 
2021 ||| can transformer language models predict psychometric properties? ||| antonio laverghetta jr. ||| animesh nighojkar ||| jamshidbek mirzakhalov ||| john licato ||| 
2019 ||| deep contextual attention for human-object interaction detection. ||| tiancai wang ||| rao muhammad anwer ||| muhammad haris khan ||| fahad shahbaz khan ||| yanwei pang ||| ling shao ||| jorma laaksonen ||| 
2020 ||| learning to pay attention to mistakes. ||| mou-cheng xu ||| neil oxtoby ||| daniel c. alexander ||| joseph jacob ||| 
2019 ||| data augmentation for skin lesion using self-attention based progressive generative adversarial network. ||| ibrahim saad ali ||| mamdouh farouk mohamed ||| youssef bassyouni mahdy ||| 
2020 |||  job description matching using context-aware transformer models. ||| changmao li ||| elaine fisher ||| rebecca thomas ||| steve pittard ||| vicki hertzberg ||| jinho d. choi ||| 
2020 ||| the benefit of distraction: denoising remote vitals measurements using inverse attention. ||| ewa magdalena nowara ||| daniel j. mcduff ||| ashok veeraraghavan ||| 
2021 ||| fusionpainting: multimodal fusion with adaptive attention for 3d object detection. ||| shaoqing xu ||| dingfu zhou ||| jin fang ||| junbo yin ||| bin zhou ||| liangjun zhang ||| 
2018 ||| unsupervised attention-guided image to image translation. ||| youssef a. mejjati ||| christian richardt ||| james tompkin ||| darren cosker ||| kwang in kim ||| 
2020 ||| from graph low-rank global attention to 2-fwl approximation. ||| omri puny ||| heli ben-hamu ||| yaron lipman ||| 
2018 ||| structured triplet learning with pos-tag guided attention for visual question answering. ||| zhe wang ||| xiaoyi liu ||| liangjian chen ||| limin wang ||| yu qiao ||| xiaohui xie ||| charless c. fowlkes ||| 
2018 ||| attention-based encoder-decoder networks for spelling and grammatical error correction. ||| sina ahmadi ||| 
2020 ||| leveraging bottom-up and top-down attention for few-shot object detection. ||| xianyu chen ||| ming jiang ||| qi zhao ||| 
2019 ||| residual non-local attention networks for image restoration. ||| yulun zhang ||| kunpeng li ||| kai li ||| bineng zhong ||| yun fu ||| 
2019 ||| cognitive functions of the brain: perception, attention and memory. ||| jiawei zhang ||| 
2020 ||| normalized and geometry-aware self-attention network for image captioning. ||| longteng guo ||| jing liu ||| xinxin zhu ||| peng yao ||| shichen lu ||| hanqing lu ||| 
2022 ||| graph self-attention for learning graph representation with transformer. ||| wonpyo park ||| woonggi chang ||| donggeon lee ||| juntae kim ||| 
2022 ||| pointattn: you only need attention for point cloud completion. ||| jun wang ||| ying cui ||| dongyan guo ||| junxia li ||| qingshan liu ||| chunhua shen ||| 
2018 ||| an attention-based bi-gru-capsnet model for hypernymy detection between compound entities. ||| qi wang ||| tong ruan ||| yangming zhou ||| chenming xu ||| daqi gao ||| ping he ||| 
2020 ||| residual spatial attention network for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yugen yi ||| wei zhou ||| haodong bian ||| 
2021 ||| from augmented microscopy to the topological transformer: a new approach in cell image analysis for alzheimer's research. ||| wooseok jung ||| 
2020 ||| predicting goal-directed attention control using inverse-reinforcement learning. ||| gregory j. zelinsky ||| yupei chen ||| seoyoung ahn ||| hossein adeli ||| zhibo yang ||| lihan huang ||| dimitris samaras ||| minh hoai ||| 
2021 ||| transrefer3d: entity-and-relation aware transformer for fine-grained 3d visual grounding. ||| dailan he ||| yusheng zhao ||| junyu luo ||| tianrui hui ||| shaofei huang ||| aixi zhang ||| si liu ||| 
2020 ||| attend and segment: attention guided active semantic segmentation. ||| soroush seifi ||| tinne tuytelaars ||| 
2019 ||| robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts. ||| mutian he ||| yan deng ||| lei he ||| 
2021 ||| convdysat: deep neural representation learning on dynamic graphs via self-attention and convolutional neural networks. ||| ahmad hafez ||| atulya praphul ||| yousef jaradt ||| ezani godwin ||| 
2021 ||| bdanet: multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images. ||| yu shen ||| sijie zhu ||| taojiannan yang ||| chen chen ||| delu pan ||| jianyu chen ||| liang xiao ||| qian du ||| 
2021 ||| sparse attention guided dynamic value estimation for single-task multi-scene reinforcement learning. ||| jaskirat singh ||| liang zheng ||| 
2020 ||| learning to encode position for transformer with continuous dynamical model. ||| xuanqing liu ||| hsiang-fu yu ||| inderjit s. dhillon ||| cho-jui hsieh ||| 
2020 ||| transformer networks for trajectory forecasting. ||| francesco giuliari ||| irtiza hasan ||| marco cristani ||| fabio galasso ||| 
2017 ||| cham: action recognition using convolutional hierarchical attention model. ||| shiyang yan ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2019 ||| contextual attention for hand detection in the wild. ||| supreeth narasimhaswamy ||| zhengwei wei ||| yang wang ||| justin zhang ||| minh hoai ||| 
2019 ||| frequency domain transformer networks for video prediction. ||| hafez farazi ||| sven behnke ||| 
2022 ||| vista: boosting 3d object detection via dual cross-view spatial attention. ||| shengheng deng ||| zhihao liang ||| lin sun ||| kui jia ||| 
2021 ||| 3d human pose estimation with spatial and temporal transformers. ||| ce zheng ||| sijie zhu ||| mat ||| as mendieta ||| taojiannan yang ||| chen chen ||| zhengming ding ||| 
2021 ||| generative chemical transformer: attention makes neural machine learn molecular geometric structures via text. ||| hyunseung kim ||| jonggeol na ||| won bo lee ||| 
2021 ||| benchmarking detection transfer learning with vision transformers. ||| yanghao li ||| saining xie ||| xinlei chen ||| piotr doll ||| r ||| kaiming he ||| ross b. girshick ||| 
2021 ||| bornon: bengali image captioning with transformer-based deep learning approach. ||| faisal muhammad shah ||| mayeesha humaira ||| md abidur rahman khan jim ||| amit saha ami ||| shimul paul ||| 
2020 ||| multitask learning and joint optimization for transformer-rnn-transducer speech recognition. ||| jae-jin jeon ||| eesung kim ||| 
2020 ||| length-adaptive transformer: train once with length drop, use anytime with search. ||| gyuwan kim ||| kyunghyun cho ||| 
2022 ||| a transformer-based network for deformable medical image registration. ||| yibo wang ||| wen qian ||| xuming zhang ||| 
2017 ||| lyrics-based music genre classification using a hierarchical attention network. ||| alexandros tsaptsinos ||| 
2018 ||| bidirectional attention for sql generation. ||| tong guo ||| huilin gao ||| 
2020 ||| channel recurrent attention networks for video pedestrian retrieval. ||| pengfei fang ||| pan ji ||| jieming zhou ||| lars petersson ||| mehrtash harandi ||| 
2020 ||| cross-media keyphrase prediction: a unified framework with multi-modality multi-head attention and image wordings. ||| yue wang ||| jing li ||| michael r. lyu ||| irwin king ||| 
2021 ||| short-term electricity price forecasting based on graph convolution network and attention mechanism. ||| yuyun yang ||| zhenfei tan ||| haitao yang ||| guangchun ruan ||| haiwang zhong ||| 
2019 ||| unraveling the origin of social bursts in collective attention. ||| manlio de domenico ||| eduardo g. altmann ||| 
2021 ||| autoformer: decomposition transformers with auto-correlation for long-term series forecasting. ||| haixu wu ||| jiehui xu ||| jianmin wang ||| mingsheng long ||| 
2022 ||| the dark side of the language: pre-trained transformers in the darknet. ||| leonardo ranaldi ||| aria nourbakhsh ||| arianna patrizi ||| elena sofia ruzzetti ||| dario onorati ||| francesca fallucchi ||| fabio massimo zanzotto ||| 
2021 ||| an end-to-end khmer optical character recognition using sequence-to-sequence with attention. ||| rina buoy ||| sokchea kor ||| nguonly taing ||| 
2018 ||| a shared attention mechanism for interpretation of neural automatic post-editing systems. ||| inigo jauregi unanue ||| ehsan zare borzeshi ||| massimo piccardi ||| 
2017 ||| stream attention for far-field multi-microphone asr. ||| xiaofei wang ||| yonghong yan ||| hynek hermansky ||| 
2021 ||| roformer: enhanced transformer with rotary position embedding. ||| jianlin su ||| yu lu ||| shengfeng pan ||| bo wen ||| yunfeng liu ||| 
2021 ||| trade when opportunity comes: price movement forecasting via locality-aware attention and adaptive refined labeling. ||| liang zeng ||| lei wang ||| hui niu ||| jian li ||| ruchen zhang ||| zhonghao dai ||| dewei zhu ||| ling wang ||| 
2021 ||| psg@hasoc-dravidian codemixfire2021: pretrained transformers for offensive language identification in tanglish. ||| sean benhur ||| kanchana sivanraju ||| 
2021 ||| thundr: transformer-based 3d human reconstruction with markers. ||| mihai zanfir ||| andrei zanfir ||| eduard gabriel bazavan ||| william t. freeman ||| rahul sukthankar ||| cristian sminchisescu ||| 
2021 ||| hit: hierarchical transformer with momentum contrast for video-text retrieval. ||| song liu ||| haoqi fan ||| shengsheng qian ||| yiru chen ||| wenkui ding ||| zhongyuan wang ||| 
2020 ||| feature based sequential classifier with attention mechanism. ||| sudhir sornapudi ||| r. joe stanley ||| william v. stoecker ||| l. rodney long ||| zhiyun xue ||| rosemary zuna ||| shelliane r. frazier ||| sameer k. antani ||| 
2017 ||| advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm. ||| takaaki hori ||| shinji watanabe ||| yu zhang ||| william chan ||| 
2022 ||| a spatial-temporal attention multi-graph convolution network for ride-hailing demand prediction based on periodicity with offset. ||| dong xing ||| chenguang zhao ||| gang wang ||| 
2021 ||| an attention ensemble approach for efficient text classification of indian languages. ||| atharva kulkarni ||| amey hengle ||| rutuja udyawar ||| 
2018 ||| bidirectional attentional encoder-decoder model and bidirectional beam search for abstractive summarization. ||| kamal al-sabahi ||| zuping zhang ||| kang yang ||| 
2018 ||| sfa: small faces attention face detector. ||| shi luo ||| xiongfei li ||| rui zhu ||| xiaoli zhang ||| 
2018 ||| stream attention-based multi-array end-to-end speech recognition. ||| xiaofei wang ||| ruizhi li ||| sri harish mallidi ||| takaaki hori ||| shinji watanabe ||| hynek hermansky ||| 
2018 ||| discriminative feature learning with foreground attention for person re-identification. ||| sanping zhou ||| jinjun wang ||| deyu meng ||| yudong liang ||| yihong gong ||| nanning zheng ||| 
2021 ||| contextual hate speech detection in code mixed text using transformer based approaches. ||| ravindra nayak ||| raviraj joshi ||| 
2021 ||| transformerfusion: monocular rgb scene reconstruction using transformers. ||| aljaz bozic ||| pablo r. palafox ||| justus thies ||| angela dai ||| matthias nie ||| ner ||| 
2021 ||| probing for bridging inference in transformer language models. ||| onkar pandit ||| yufang hou ||| 
2019 ||| dagcn: dual attention graph convolutional networks. ||| fengwen chen ||| shirui pan ||| jing jiang ||| huan huo ||| guodong long ||| 
2020 ||| relation transformer network. ||| rajat koner ||| poulami sinhamahapatra ||| volker tresp ||| 
2020 ||| deep entwined learning head pose and face alignment inside an attentional cascade with doubly-conditional fusion. ||| arnaud dapogny ||| k ||| vin bailly ||| matthieu cord ||| 
2017 ||| automatic music highlight extraction using convolutional recurrent attention networks. ||| jung-woo ha ||| adrian kim ||| chanju kim ||| jangyeon park ||| sunghun kim ||| 
2022 ||| transformer-based models of text normalization for speech applications. ||| jae hun ro ||| felix stahlberg ||| ke wu ||| shankar kumar ||| 
2020 ||| adaptive graph diffusion networks with hop-wise attention. ||| chuxiong sun ||| guoshi wu ||| 
2021 ||| bert-gt: cross-sentence n-ary relation extraction with bert and graph transformer. ||| po-ting lai ||| zhiyong lu ||| 
2022 ||| agent-temporal attention for reward redistribution in episodic multi-agent reinforcement learning. ||| baicen xiao ||| bhaskar ramasubramanian ||| radha poovendran ||| 
2021 ||| enriching non-autoregressive transformer with syntactic and semanticstructures for neural machine translation. ||| ye liu ||| yao wan ||| jianguo zhang ||| wenting zhao ||| philip s. yu ||| 
2017 ||| towards bidirectional hierarchical representations for attention-based neural machine translation. ||| baosong yang ||| derek f. wong ||| tong xiao ||| lidia s. chao ||| jingbo zhu ||| 
2021 ||| hrformer: high-resolution transformer for dense prediction. ||| yuhui yuan ||| rao fu ||| lang huang ||| weihong lin ||| chao zhang ||| xilin chen ||| jingdong wang ||| 
2021 ||| attention-guided black-box adversarial attacks with large-scale multiobjective evolutionary optimization. ||| jie wang ||| zhaoxia yin ||| jing jiang ||| yang du ||| 
2022 ||| dct-former: efficient self-attention with discrete cosine transform. ||| carmelo scribano ||| giorgia franchini ||| marco prato ||| marko bertogna ||| 
2021 ||| vsec: transformer-based model for vietnamese spelling correction. ||| dinh-truong do ||| ha thanh nguyen ||| thang ngoc bui ||| hieu dinh vo ||| 
2019 ||| m3d-gan: multi-modal multi-domain translation with universal attention. ||| shuang ma ||| daniel j. mcduff ||| yale song ||| 
2021 ||| uction via attention-guided deep fusion of hybrid lenses. ||| jing jin ||| hui liu ||| junhui hou ||| hongkai xiong ||| 
2020 ||| how can self-attention networks recognize dyck-n languages? ||| javid ebrahimi ||| dhruv gelda ||| wei zhang ||| 
2021 ||| semi-autoregressive transformer for image captioning. ||| yuanen zhou ||| yong zhang ||| zhenzhen hu ||| meng wang ||| 
2019 ||| self-attention and ingredient-attention based model for recipe retrieval from image queries. ||| matthias fontanellaz ||| stergios christodoulidis ||| stavroula g. mougiakakou ||| 
2020 ||| pedestrian trajectory prediction using context-augmented transformer networks. ||| khaled saleh ||| 
2020 ||| improving constituency parsing with span attention. ||| yuanhe tian ||| yan song ||| fei xia ||| tong zhang ||| 
2022 ||| brain cancer survival prediction on treatment-na ive mri using deep anchor attention learning with vision transformer. ||| xuan xu ||| prateek prasanna ||| 
2021 ||| differentiable spatial planning using transformers. ||| devendra singh chaplot ||| deepak pathak ||| jitendra malik ||| 
2021 ||| towards emotion recognition in hindi-english code-mixed data: a transformer based approach. ||| anshul wadhawan ||| akshita aggarwal ||| 
2021 ||| fine-tuned transformers show clusters of similar representations across layers. ||| jason phang ||| haokun liu ||| samuel r. bowman ||| 
2020 ||| when do drivers concentrate? attention-based driver behavior modeling with deep reinforcement learning. ||| xingbo fu ||| xuan di ||| zhaobin mo ||| 
2017 ||| distance-based self-attention network for natural language inference. ||| jinbae im ||| sungzoon cho ||| 
2019 ||| atloc: attention guided camera localization. ||| bing wang ||| changhao chen ||| chris xiaoxuan lu ||| peijun zhao ||| niki trigoni ||| andrew markham ||| 
2021 ||| ptnet: a high-resolution infant mri synthesizer based on transformer. ||| xuzhe zhang ||| xinzi he ||| jia guo ||| nabil ettehadi ||| natalie aw ||| david semanek ||| jonathan posner ||| andrew laine ||| yun wang ||| 
2019 ||| a simple and robust convolutional-attention network for irregular text recognition. ||| peng wang ||| lu yang ||| hui li ||| yuyan deng ||| chunhua shen ||| yanning zhang ||| 
2020 ||| max-fusion u-net for multi-modal pathology segmentation with attention and dynamic resampling. ||| haochuan jiang ||| chengjia wang ||| agisilaos chartsias ||| sotirios a. tsaftaris ||| 
2021 ||| dynamic clone transformer for efficient convolutional neural netwoks. ||| longqing ye ||| 
2021 ||| deepvit: towards deeper vision transformer. ||| daquan zhou ||| bingyi kang ||| xiaojie jin ||| linjie yang ||| xiaochen lian ||| qibin hou ||| jiashi feng ||| 
2021 |||  cognition: the role of attention. ||| hugo latapie ||| zkan kili ||| kristinn r. th ||| risson ||| pei wang ||| patrick hammer ||| 
2020 ||| unsupervised extraction of market moving events with neural attention. ||| luciano del corro ||| johannes hoffart ||| 
2021 ||| a non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. ||| rongyi sun ||| borun chen ||| qingyu zhou ||| yinghui li ||| yunbo cao ||| hai-tao zheng ||| 
2021 ||| high-resolution depth maps imaging via attention-based hierarchical multi-modal fusion. ||| zhiwei zhong ||| xianming liu ||| junjun jiang ||| debin zhao ||| zhiwen chen ||| xiangyang ji ||| 
2020 ||| learning accurate integer transformer machine-translation models. ||| ephrem wu ||| 
2021 ||| recurrent vision transformer for solving visual reasoning problems. ||| nicola messina ||| giuseppe amato ||| fabio carrara ||| claudio gennaro ||| fabrizio falchi ||| 
2021 ||| adaattn: revisit attention mechanism in arbitrary neural style transfer. ||| songhua liu ||| tianwei lin ||| dongliang he ||| fu li ||| meiling wang ||| xin li ||| zhengxing sun ||| qian li ||| errui ding ||| 
2018 ||| diversity regularized spatiotemporal attention for video-based person re-identification. ||| shuang li ||| slawomir bak ||| peter carr ||| xiaogang wang ||| 
2021 ||| how does a pre-trained transformer integrate contextual keywords? application to humanitarian computing. ||| valentin barriere ||| guillaume jacquet ||| 
2020 ||| short text classification via knowledge powered attention with similarity matrix based cnn. ||| 
2018 ||| quantum statistics-inspired neural attention. ||| aristotelis charalampous ||| sotirios chatzis ||| 
2020 ||| deepbrain: towards personalized eeg interaction through attentional and embedded lstm learning. ||| di wu ||| huayan wan ||| siping liu ||| weiren yu ||| zhanpeng jin ||| dakuo wang ||| 
2020 ||| attention-based self-supervised feature learning for security data. ||| i-ta lee ||| manish marwah ||| martin f. arlitt ||| 
2021 ||| vision transformer for small-size datasets. ||| seung hoon lee ||| seunghyun lee ||| byung cheol song ||| 
2022 ||| memvit: memory-augmented multiscale vision transformer for efficient long-term video recognition. ||| chao-yuan wu ||| yanghao li ||| karttikeya mangalam ||| haoqi fan ||| bo xiong ||| jitendra malik ||| christoph feichtenhofer ||| 
2021 ||| learning the physics of particle transport via transformers. ||| oscar pastor-serrano ||| zolt ||| n perk ||| 
2021 ||| dfcanet: dense feature calibration-attention guided network for cross domain iris presentation attack detection. ||| gaurav jaswal ||| aman verma ||| sumantra dutta roy ||| raghavendra ramachandra ||| 
2021 ||| attention-based domain adaptation for single stage detectors. ||| vidit ||| mathieu salzmann ||| 
2020 ||| a better use of audio-visual cues: dense video captioning with bi-modal transformer. ||| vladimir iashin ||| esa rahtu ||| 
2019 ||| syntax-aware aspect level sentiment classification with graph attention networks. ||| binxuan huang ||| kathleen m. carley ||| 
2019 ||| multilingual named entity recognition using pretrained embeddings, attention mechanism and ncrf. ||| anton a. emelyanov ||| ekaterina artemova ||| 
2020 ||| distant supervision for e-commerce query segmentation via attention network. ||| zhao li ||| donghui ding ||| pengcheng zou ||| yu gong ||| xi chen ||| ji zhang ||| jianliang gao ||| youxi wu ||| yucong duan ||| 
2017 ||| local monotonic attention mechanism for end-to-end speech recognition. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| gpt2mvs: generative pre-trained transformer-2 for multi-modal video summarization. ||| jia-hong huang ||| luka murn ||| marta mrak ||| marcel worring ||| 
2021 ||| going beyond linear transformers with recurrent fast weight programmers. ||| kazuki irie ||| imanol schlag ||| r ||| bert csord ||| s ||| j ||| rgen schmidhuber ||| 
2018 ||| improving distantly supervised relation extraction using word and entity based attention. ||| sharmistha jat ||| siddhesh khandelwal ||| partha p. talukdar ||| 
2021 ||| accelerated multi-modal mr imaging with transformers. ||| chun-mei feng ||| yunlu yan ||| geng chen ||| huazhu fu ||| yong xu ||| ling shao ||| 
2021 ||| pvtv2: improved baselines with pyramid vision transformer. ||| wenhai wang ||| enze xie ||| xiang li ||| deng-ping fan ||| kaitao song ||| ding liang ||| tong lu ||| ping luo ||| ling shao ||| 
2017 ||| attention allocation aid for visual search. ||| arturo deza ||| jeffrey r. peters ||| grant s. taylor ||| amit surana ||| miguel p. eckstein ||| 
2020 ||| synthesizer: rethinking self-attention in transformer models. ||| yi tay ||| dara bahri ||| donald metzler ||| da-cheng juan ||| zhe zhao ||| che zheng ||| 
2021 ||| rap-net: region attention predictive network for precipitation nowcasting. ||| chuyao luo ||| zhengzhang ||| rui ye ||| xutao li ||| yunming ye ||| 
2020 ||| efficient transformer-based large scale language representations using hardware-friendly block structured pruning. ||| bingbing li ||| zhenglun kong ||| tianyun zhang ||| ji li ||| zhengang li ||| hang liu ||| caiwen ding ||| 
2022 ||| global matching with overlapping attention for optical flow estimation. ||| shiyu zhao ||| long zhao ||| zhixing zhang ||| enyu zhou ||| dimitris n. metaxas ||| 
2021 ||| ultrasound video transformers for cardiac ejection fraction estimation. ||| hadrien reynaud ||| athanasios vlontzos ||| benjamin hou ||| arian beqiri ||| paul leeson ||| bernhard kainz ||| 
2021 ||| fully transformer networks for semantic image segmentation. ||| sitong wu ||| tianyi wu ||| fangjian lin ||| shengwei tian ||| guodong guo ||| 
2019 ||| a hybrid text normalization system using multi-head self-attention for mandarin. ||| junhui zhang ||| junjie pan ||| xiang yin ||| chen li ||| shichao liu ||| yang zhang ||| yuxuan wang ||| zejun ma ||| 
2021 ||| attention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario. ||| chiang-jen peng ||| yun-ju chan ||| cheng yu ||| syu-siang wang ||| yu tsao ||| tai-shih chi ||| 
2017 ||| fusionnet: fusing via fully-aware attention with application to machine comprehension. ||| hsin-yuan huang ||| chenguang zhu ||| yelong shen ||| weizhu chen ||| 
2018 ||| automatic graphics program generation using attention-based hierarchical decoder. ||| zhihao zhu ||| zhan xue ||| zejian yuan ||| 
2021 ||| knowledge-enhanced session-based recommendation with temporal transformer. ||| rongzhi zhang ||| yulong gu ||| xiaoyu shen ||| hui su ||| 
2020 ||| successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. ||| christopher brix ||| parnia bahar ||| hermann ney ||| 
2022 ||| a survey of controllable text generation using transformer-based pre-trained language models. ||| hanqing zhang ||| haolin song ||| shaoyu li ||| ming zhou ||| dawei song ||| 
2018 ||| optimal tap setting of voltage regulation transformers using batch reinforcement learning. ||| hanchen xu ||| alejandro d. dom ||| nguez-garc ||| a ||| peter w. sauer ||| 
2018 ||| ms-uedin submission to the wmt2018 ape shared task: dual-source transformer for automatic post-editing. ||| marcin junczys-dowmunt ||| roman grundkiewicz ||| 
2021 ||| scaled relu matters for training vision transformers. ||| pichao wang ||| xue wang ||| hao luo ||| jingkai zhou ||| zhipeng zhou ||| fan wang ||| hao li ||| rong jin ||| 
2021 ||| span: subgraph prediction attention network for dynamic graphs. ||| yuan li ||| chuanchang chen ||| yubo tao ||| hai lin ||| 
2022 ||| contextformer: a transformer with spatio-channel attention for context modeling in learned image compression. ||| ahmet burakhan koyuncu ||| han gao ||| eckehard g. steinbach ||| 
2020 ||| deep learning with attention mechanism for predicting driver intention at intersection. ||| abenezer girma ||| seifemichael b. amsalu ||| abrham workineh ||| mubbashar altaf khan ||| abdollah homaifar ||| 
2019 ||| pairwise interactive graph attention network for context-aware recommendation. ||| yahui liu ||| furao shen ||| jian zhao ||| 
2022 ||| video instance segmentation via multi-scale spatio-temporal split attention transformer. ||| omkar thawakar ||| sanath narayan ||| jiale cao ||| hisham cholakkal ||| rao muhammad anwer ||| muhammad haris khan ||| salman khan ||| michael felsberg ||| fahad shahbaz khan ||| 
2021 ||| not all attention is all you need. ||| hongqiu wu ||| hai zhao ||| min zhang ||| 
2020 ||| memory transformer. ||| mikhail s. burtsev ||| grigory v. sapunov ||| 
2020 ||| parsbert: transformer-based model for persian language understanding. ||| mehrdad farahani ||| mohammad gharachorloo ||| marzieh farahani ||| mohammad manthouri ||| 
2021 ||| git: graph interactive transformer for vehicle re-identification. ||| fei shen ||| yi xie ||| jianqing zhu ||| xiaobin zhu ||| huanqiang zeng ||| 
2020 ||| multispeech: multi-speaker text to speech with transformer. ||| mingjian chen ||| xu tan ||| yi ren ||| jin xu ||| hao sun ||| sheng zhao ||| tao qin ||| 
2020 ||| learning accurate and human-like driving using semantic maps and attention. ||| simon hecker ||| dengxin dai ||| alexander liniger ||| luc van gool ||| 
2021 ||| hawk: rapid android malware detection through heterogeneous graph attention networks. ||| yiming hei ||| renyu yang ||| hao peng ||| lihong wang ||| xiaolin xu ||| jianwei liu ||| hong liu ||| jie xu ||| lichao sun ||| 
2021 ||| disentangled attention as intrinsic regularization for bimanual multi-object manipulation. ||| minghao zhang ||| pingcheng jian ||| yi wu ||| huazhe xu ||| xiaolong wang ||| 
2020 ||| how far does bert look at: distance-based clustering and analysis of bert's attention. ||| yue guan ||| jingwen leng ||| chao li ||| quan chen ||| minyi guo ||| 
2020 ||| modality-agnostic attention fusion for visual search with text feedback. ||| eric dodds ||| jack culpepper ||| simao herdade ||| yang zhang ||| kofi boakye ||| 
2021 ||| attention-gated convolutional neural networks for off-resonance correction of spiral real-time mri. ||| yongwan lim ||| shrikanth s. narayanan ||| krishna s. nayak ||| 
2021 ||| switch transformers: scaling to trillion parameter models with simple and efficient sparsity. ||| william fedus ||| barret zoph ||| noam shazeer ||| 
2018 ||| what increases (social) media attention: research impact, author prominence or title attractiveness? ||| olga zagovora ||| katrin weller ||| milan janosov ||| claudia wagner ||| isabella peters ||| 
2022 ||| the nlp task effectiveness of long-range transformers. ||| guanghui qin ||| yukun feng ||| benjamin van durme ||| 
2022 ||| when shift operation meets vision transformer: an extremely simple alternative to attention mechanism. ||| guangting wang ||| yucheng zhao ||| chuanxin tang ||| chong luo ||| wenjun zeng ||| 
2022 ||| attention over self-attention: intention-aware re-ranking with dynamic transformer encoders for recommendation. ||| zhuoyi lin ||| sheng zang ||| rundong wang ||| zhu sun ||| chi xu ||| chee keong kwoh ||| 
2020 ||| improving long-tail relation extraction with collaborating relation-augmented attention. ||| yang li ||| tao shen ||| guodong long ||| jing jiang ||| tianyi zhou ||| chengqi zhang ||| 
2021 ||| triple attention network architecture for movieqa. ||| ankit shah ||| tzu-hsiang lin ||| shijie wu ||| 
2021 ||| understanding attention in machine reading comprehension. ||| yiming cui ||| wei-nan zhang ||| wanxiang che ||| ting liu ||| zhigang chen ||| 
2017 ||| hashgan: attention-aware deep adversarial hashing for cross modal retrieval. ||| xi zhang ||| siyu zhou ||| jiashi feng ||| hanjiang lai ||| bo li ||| yan pan ||| jian yin ||| shuicheng yan ||| 
2019 ||| conditionally learn to pay attention for sequential visual task. ||| jun he ||| quan-jie cao ||| lei zhang ||| 
2019 ||| a symmetric equilibrium generative adversarial network with attention refine block for retinal vessel segmentation. ||| yukun zhou ||| zailiang chen ||| hailan shen ||| peng peng ||| ziyang zeng ||| xianxian zheng ||| 
2020 ||| volumetric transformer networks. ||| seungryong kim ||| sabine s ||| sstrunk ||| mathieu salzmann ||| 
2019 ||| language identification on massive datasets of short message using an attention mechanism cnn. ||| duy-tin vo ||| richard khoury ||| 
2021 ||| conversational question answering over knowledge graphs with transformer and graph attention networks. ||| endri kacupaj ||| joan plepi ||| kuldeep singh ||| harsh thakkar ||| jens lehmann ||| maria maleshkova ||| 
2020 ||| resnest: split-attention networks. ||| hang zhang ||| chongruo wu ||| zhongyue zhang ||| yi zhu ||| zhi zhang ||| haibin lin ||| yue sun ||| tong he ||| jonas mueller ||| r. manmatha ||| mu li ||| alexander j. smola ||| 
2020 ||| sparse and continuous attention mechanisms. ||| andr |||  f. t. martins ||| marcos v. treviso ||| ant ||| nio farinhas ||| vlad niculae ||| m ||| rio a. t. figueiredo ||| pedro m. q. aguiar ||| 
2021 ||| parallel scale-wise attention network for effective scene text recognition. ||| usman sajid ||| michael chow ||| jin zhang ||| taejoon kim ||| guanghui wang ||| 
2022 ||| mutual attention-based hybrid dimensional network for multimodal imaging computer-aided diagnosis. ||| yin dai ||| yifan gao ||| fayu liu ||| jun fu ||| 
2018 ||| visual attention driven by convolutional features. ||| dario zanca ||| marco gori ||| 
2021 ||| sparsebert: rethinking the importance analysis in self-attention. ||| han shi ||| jiahui gao ||| xiaozhe ren ||| hang xu ||| xiaodan liang ||| zhenguo li ||| james t. kwok ||| 
2021 ||| tensor-to-image: image-to-image translation with vision transformers. ||| yigit g ||| nd ||| 
2021 ||| attention models for point clouds in deep learning: a survey. ||| xu wang ||| yi jin ||| yigang cen ||| tao wang ||| yidong li ||| 
2021 ||| semi-supervised music tagging transformer. ||| minz won ||| keunwoo choi ||| xavier serra ||| 
2019 ||| learning attention-based embeddings for relation prediction in knowledge graphs. ||| deepak nathani ||| jatin chauhan ||| charu sharma ||| manohar kaul ||| 
2020 ||| talking-heads attention. ||| noam shazeer ||| zhenzhong lan ||| youlong cheng ||| nan ding ||| le hou ||| 
2020 ||| from zero to hero: on the limitations of zero-shot cross-lingual transfer with multilingual transformers. ||| anne lauscher ||| vinit ravishankar ||| ivan vulic ||| goran glavas ||| 
2021 ||| trackformer: multi-object tracking with transformers. ||| tim meinhardt ||| alexander kirillov ||| laura leal-taix ||| christoph feichtenhofer ||| 
2018 ||| learning context-sensitive time-decay attention for role-based dialogue modeling. ||| shang-yu su ||| pei-chieh yuan ||| yun-nung chen ||| 
2019 ||| the evolved transformer. ||| david r. so ||| chen liang ||| quoc v. le ||| 
2021 ||| point cloud segmentation using sparse temporal local attention. ||| joshua knights ||| peyman moghadam ||| clinton fookes ||| sridha sridharan ||| 
2021 ||| multimodal end-to-end group emotion recognition using cross-modal attention. ||| lev evtodienko ||| 
2019 ||| sequential recommendation with relation-aware kernelized self-attention. ||| mingi ji ||| weonyoung joo ||| kyungwoo song ||| yoon-yeong kim ||| il-chul moon ||| 
2020 ||| attention-based multi-modal fusion network for semantic scene completion. ||| siqi li ||| changqing zou ||| yipeng li ||| xibin zhao ||| yue gao ||| 
2021 ||| tent: tensorized encoder transformer for temperature forecasting. ||| onur bilgin ||| pawel maka ||| thomas vergutz ||| siamak mehrkanoon ||| 
2020 ||| crisisbert: a robust transformer for crisis classification and contextual crisis embedding. ||| junhua liu ||| trisha singhal ||| luci ||| nne t. m. blessing ||| kristin l. wood ||| kwan hui lim ||| 
2018 ||| self-attention equipped graph convolutions for disease prediction. ||| anees kazi ||| s. arvind krishna ||| shayan shekarforoush ||| karsten kortuem ||| shadi albarqouni ||| nassir navab ||| 
2021 ||| trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer. ||| xiaojie gao ||| yueming jin ||| yong-hao long ||| qi dou ||| pheng-ann heng ||| 
2022 ||| correcting diacritics and typos with byt5 transformer model. ||| lukas stankevicius ||| mantas lukosevicius ||| jurgita kapociute-dzikiene ||| monika briediene ||| tomas krilavicius ||| 
2019 ||| voice transformer network: sequence-to-sequence voice conversion using transformer with text-to-speech pretraining. ||| wen-chin huang ||| tomoki hayashi ||| yi-chiao wu ||| hirokazu kameoka ||| tomoki toda ||| 
2021 ||| cmsaone@dravidian-codemix-fire2020: a meta embedding and transformer model for code-mixed sentiment analysis on social media text. ||| suman dowlagar ||| radhika mamidi ||| 
2019 ||| attention enriched deep learning model for breast tumor segmentation in ultrasound images. ||| aleksandar vakanski ||| min xian ||| phoebe freer ||| 
2021 ||| mergebert: program merge conflict resolution via neural transformers. ||| alexey svyatkovskiy ||| todd mytkowicz ||| negar ghorbani ||| sarah fakhoury ||| elizabeth dinella ||| christian bird ||| neel sundaresan ||| shuvendu k. lahiri ||| 
2018 ||| more specificity, more attention to social context: reframing how we address "bad actors". ||| libby hemphill ||| 
2018 ||| convolutional spatial attention model for reading comprehension with multiple-choice questions. ||| zhipeng chen ||| yiming cui ||| wentao ma ||| shijin wang ||| guoping hu ||| 
2020 ||| sentence level human translation quality estimation with attention-based neural networks. ||| yu yuan ||| serge sharoff ||| 
2018 ||| factorized attention: self-attention with linear complexities. ||| zhuoran shen ||| mingyuan zhang ||| shuai yi ||| junjie yan ||| haiyu zhao ||| 
2019 ||| fingerspelling recognition in the wild with iterative visual attention. ||| bowen shi ||| aurora martinez del rio ||| jonathan keane ||| diane brentari ||| greg shakhnarovich ||| karen livescu ||| 
2022 ||| quantum expectation transformers for cost analysis. ||| martin avanzini ||| georg moser ||| romain p ||| choux ||| simon perdrix ||| vladimir zamdzhiev ||| 
2018 ||| end-to-end audio visual scene-aware dialog using multimodal attention-based video features. ||| chiori hori ||| huda alamri ||| jue wang ||| gordon wichern ||| takaaki hori ||| anoop cherian ||| tim k. marks ||| vincent cartillier ||| raphael gontijo lopes ||| abhishek das ||| irfan essa ||| dhruv batra ||| devi parikh ||| 
2021 ||| a new gastric histopathology subsize image database (gashissdb) for classification algorithm test: from linear regression to visual transformer. ||| weiming hu ||| chen li ||| xiaoyan li ||| md mamunur rahaman ||| jiquan ma ||| haoyuan chen ||| wanli liu ||| changhao sun ||| marcin grzegorzek ||| 
2022 ||| global2local: a joint-hierarchical attention for video captioning. ||| chengpeng dai ||| fuhai chen ||| xiaoshuai sun ||| rongrong ji ||| qixiang ye ||| yongjian wu ||| 
2021 ||| multi-modal perception attention network with self-supervised learning for audio-visual speaker tracking. ||| yidi li ||| hong liu ||| hao tang ||| 
2020 ||| purifying real images with an attention-guided style transfer network for gaze estimation. ||| yuxiao yan ||| yang yan ||| jinjia peng ||| huibing wang ||| xianping fu ||| 
2021 ||| ssan: separable self-attention network for video representation learning. ||| xudong guo ||| xun guo ||| yan lu ||| 
2021 ||| a multi-modal transformer-based code summarization approach for smart contracts. ||| zhen yang ||| jacky keung ||| xiao yu ||| xiaodong gu ||| zhengyuan wei ||| xiaoxue ma ||| miao zhang ||| 
2022 ||| multi-head temporal attention-augmented bilinear network for financial time series prediction. ||| mostafa shabani ||| dat thanh tran ||| martin magris ||| juho kanniainen ||| alexandros iosifidis ||| 
2021 ||| ow-detr: open-world detection transformer. ||| akshita gupta ||| sanath narayan ||| k. j. joseph ||| salman h. khan ||| fahad shahbaz khan ||| mubarak shah ||| 
2022 ||| dit: self-supervised pre-training for document image transformer. ||| junlong li ||| yiheng xu ||| tengchao lv ||| lei cui ||| cha zhang ||| furu wei ||| 
2021 ||| faketransformer: exposing face forgery from spatial-temporal representation modeled by facial pixel variations. ||| yuyang sun ||| zhiyong zhang ||| changzhen qiu ||| liang wang ||| zekai wang ||| 
2018 ||| modality attention for end-to-end audio-visual speech recognition. ||| pan zhou ||| wenwen yang ||| wei chen ||| yanfeng wang ||| jia jia ||| 
2021 ||| skeletal graph self-attention: embedding a skeleton inductive bias into sign language production. ||| ben saunders ||| necati cihan camg ||| z ||| richard bowden ||| 
2021 ||| topological attention for time series forecasting. ||| sebastian zeng ||| florian graf ||| christoph d. hofer ||| roland kwitt ||| 
2019 ||| image captioning with weakly-supervised attention penalty. ||| jiayun li ||| mohammad k. ebrahimpour ||| azadeh moghtaderi ||| yen-yun yu ||| 
2019 ||| promoting the knowledge of source syntax in transformer nmt is not needed. ||| thuong-hai pham ||| dominik mach ||| cek ||| ondrej bojar ||| 
2021 ||| the cat set on the mat: cross attention for set matching in bipartite hypergraphs. ||| govind sharma ||| swyam prakash singh ||| v. susheela devi ||| m. narasimha murty ||| 
2022 ||| dual-flattening transformers through decomposed row and column queries for semantic segmentation. ||| ying wang ||| chiuman ho ||| wenju xu ||| ziwei xuan ||| xudong liu ||| guo-jun qi ||| 
2022 ||| multi-dimensional model compression of vision transformer. ||| zejiang hou ||| sun-yuan kung ||| 
2021 ||| hformer: hybrid cnn-transformer for fringe order prediction in phase unwrapping of fringe projection. ||| xinjun zhu ||| zhiqiang han ||| mengkai yuan ||| qinghua guo ||| hongyi wang ||| 
2022 ||| cp-vit: cascade vision transformer pruning via progressive sparsity prediction. ||| zhuoran song ||| yihong xu ||| zhezhi he ||| li jiang ||| naifeng jing ||| xiaoyao liang ||| 
2021 ||| elsa: enhanced local self-attention for vision transformer. ||| jingkai zhou ||| pichao wang ||| fan wang ||| qiong liu ||| hao li ||| rong jin ||| 
2021 ||| dpt-fsnet: dual-path transformer based full-band and sub-band fusion network for speech enhancement. ||| feng dang ||| hangting chen ||| pengyuan zhang ||| 
2022 ||| swinunet3d - a hierarchical architecture for deep traffic prediction using shifted window transformers. ||| alabi bojesomo ||| hasan al-marzouqi ||| panos liatsis ||| 
2019 ||| 3dviewgraph: learning global features for 3d shapes from a graph of unordered views with attention. ||| zhizhong han ||| xiyang wang ||| chi-man vong ||| yu-shen liu ||| matthias zwicker ||| c. l. philip chen ||| 
2018 ||| hierarchical lstms with adaptive attention for visual captioning. ||| jingkuan song ||| xiangpeng li ||| lianli gao ||| heng tao shen ||| 
2021 ||| super-resolution-based change detection network with stacked attention module for images with different resolutions. ||| mengxi liu ||| qian shi ||| andrea marinoni ||| da he ||| xiaoping liu ||| liangpei zhang ||| 
2020 ||| insertion-deletion transformer. ||| laura ruis ||| mitchell stern ||| julia proskurnia ||| william chan ||| 
2020 ||| transmodality: an end2end fusion method with transformer for multimodal sentiment analysis. ||| zilong wang ||| zhaohong wan ||| xiaojun wan ||| 
2021 ||| systematic generalization with edge transformers. ||| leon bergen ||| timothy j. o'donnell ||| dzmitry bahdanau ||| 
2021 ||| pica: a pixel correlation-based attentional black-box adversarial attack. ||| jie wang ||| zhaoxia yin ||| jin tang ||| jing jiang ||| bin luo ||| 
2020 ||| tatl at w-nut 2020 task 2: a transformer-based baseline system for identification of informative covid-19 english tweets. ||| anh tuan nguyen ||| 
2021 ||| transformer in transformer. ||| kai han ||| an xiao ||| enhua wu ||| jianyuan guo ||| chunjing xu ||| yunhe wang ||| 
2021 ||| multi-modal trajectory prediction for autonomous driving with semantic map and dynamic graph attention network. ||| bo dong ||| hao liu ||| yu bai ||| jinbiao lin ||| zhuoran xu ||| xinyu xu ||| qi kong ||| 
2018 ||| attend and rectify: a gated attention mechanism for fine-grained recovery. ||| pau rodr ||| guez ||| josep m. gonfaus ||| guillem cucurull ||| f. xavier roca ||| jordi gonz ||| lez ||| 
2021 ||| abusive language detection in heterogeneous contexts: dataset collection and the role of supervised attention. ||| hongyu gong ||| alberto valido ||| katherine m. ingram ||| giulia fanti ||| suma bhat ||| dorothy l. espelage ||| 
2020 ||| unit test case generation with transformers. ||| michele tufano ||| dawn drain ||| alexey svyatkovskiy ||| shao kun deng ||| neel sundaresan ||| 
2021 ||| tiled squeeze-and-excite: channel attention with local spatial context. ||| niv vosco ||| alon shenkler ||| mark grobman ||| 
2021 ||| passive attention in artificial neural networks predicts human visual selectivity. ||| thomas a. langlois ||| h. charles zhao ||| erin grant ||| ishita dasgupta ||| thomas l. griffiths ||| nori jacoby ||| 
2020 ||| efficient image super-resolution using pixel attention. ||| hengyuan zhao ||| xiangtao kong ||| jingwen he ||| yu qiao ||| chao dong ||| 
2021 ||| raga: relation-aware graph attention networks for global entity alignment. ||| renbo zhu ||| meng ma ||| ping wang ||| 
2019 ||| muse: parallel multi-scale attention for sequence to sequence learning. ||| guangxiang zhao ||| xu sun ||| jingjing xu ||| zhiyuan zhang ||| liangchen luo ||| 
2019 ||| mrnn: a multi-resolution neural network with duplex attention for document retrieval in the context of question answering. ||| tolgahan cakaloglu ||| xiaowei xu ||| 
2018 ||| attentional road safety networks. ||| sonu gupta ||| deepak srivatsav ||| a. venkata subramanyam ||| ponnurangam kumaraguru ||| 
2021 ||| self-supervised video transformer. ||| kanchana ranasinghe ||| muzammal naseer ||| salman khan ||| fahad shahbaz khan ||| michael ryoo ||| 
2021 ||| information bottleneck approach to spatial attention learning. ||| qiuxia lai ||| yu li ||| ailing zeng ||| minhao liu ||| hanqiu sun ||| qiang xu ||| 
2020 ||| long range arena: a benchmark for efficient transformers. ||| yi tay ||| mostafa dehghani ||| samira abnar ||| yikang shen ||| dara bahri ||| philip pham ||| jinfeng rao ||| liu yang ||| sebastian ruder ||| donald metzler ||| 
2021 ||| learning hierarchical attention for weakly-supervised chest x-ray abnormality localization and diagnosis. ||| xi ouyang ||| srikrishna karanam ||| ziyan wu ||| terrence chen ||| jiayu huo ||| xiang sean zhou ||| qian wang ||| jie-zhi cheng ||| 
2022 ||| entropy-based attention regularization frees unintended bias mitigation from lists. ||| giuseppe attanasio ||| debora nozza ||| dirk hovy ||| elena baralis ||| 
2020 ||| attend to the beginning: a study on using bidirectional attention for extractive summarization. ||| ahmed magooda ||| cezary marcjan ||| 
2020 ||| unsupervised sparse-view backprojection via convolutional and spatial transformer networks. ||| xueqing liu ||| paul sajda ||| 
2021 ||| sequential attention module for natural language processing. ||| mengyuan zhou ||| jian ma ||| haiqin yang ||| lian-xin jiang ||| yang mo ||| 
2021 ||| masked-attention mask transformer for universal image segmentation. ||| bowen cheng ||| ishan misra ||| alexander g. schwing ||| alexander kirillov ||| rohit girdhar ||| 
2021 ||| delayed propagation transformer: a universal computation engine towards practical control in cyber-physical systems. ||| wenqing zheng ||| qiangqiang guo ||| hao yang ||| peihao wang ||| zhangyang wang ||| 
2020 ||| a survey on visual transformer. ||| kai han ||| yunhe wang ||| hanting chen ||| xinghao chen ||| jianyuan guo ||| zhenhua liu ||| yehui tang ||| an xiao ||| chunjing xu ||| yixing xu ||| zhaohui yang ||| yiman zhang ||| dacheng tao ||| 
2019 ||| attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems. ||| zihan liu ||| genta indra winata ||| zhaojiang lin ||| peng xu ||| pascale fung ||| 
2022 ||| denseunets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. ||| juan p. vigueras-guill ||| n ||| jeroen van rooij ||| bart t. h. van dooren ||| hans g. lemij ||| esma islamaj ||| lucas j. van vliet ||| koenraad a. vermeer ||| 
2021 ||| "killing me" is not a spoiler: spoiler detection model using graph neural networks with dependency relation-aware attention mechanism. ||| buru chang ||| inggeol lee ||| hyunjae kim ||| jaewoo kang ||| 
2021 ||| trans2seg: transparent object segmentation with transformer. ||| enze xie ||| wenjia wang ||| wenhai wang ||| peize sun ||| hang xu ||| ding liang ||| ping luo ||| 
2018 ||| long short-term attention. ||| guoqiang zhong ||| xin lin ||| kang chen ||| 
2021 ||| trtr: visual tracking with transformer. ||| moju zhao ||| kei okada ||| masayuki inaba ||| 
2018 ||| edge attention-based multi-relational graph convolutional networks. ||| 
2017 ||| video fill in the blank using lr/rl lstms with spatial-temporal attentions. ||| amir mazaheri ||| dong zhang ||| mubarak shah ||| 
2019 ||| deep bidirectional transformers for relation extraction without supervision. ||| yannis papanikolaou ||| ian roberts ||| andrea pierleoni ||| 
2020 ||| human versus machine attention in deep reinforcement learning tasks. ||| ruohan zhang ||| bo liu ||| yifeng zhu ||| sihang guo ||| mary m. hayhoe ||| dana h. ballard ||| peter stone ||| 
2021 ||| improving super-resolution performance using meta-attention layers. ||| matthew aquilina ||| christian galea ||| john abela ||| kenneth p. camilleri ||| reuben a. farrugia ||| 
2021 ||| m3detr: multi-representation, multi-scale, mutual-relation 3d object detection with transformers. ||| tianrui guan ||| jun wang ||| shiyi lan ||| rohan chandra ||| zuxuan wu ||| larry davis ||| dinesh manocha ||| 
2020 ||| attention-gating for improved radio galaxy classification. ||| micah bowles ||| anna m. m. scaife ||| fiona porter ||| hongming tang ||| david j. bastien ||| 
2018 ||| attention-based walking gait and direction recognition in wi-fi networks. ||| 
2021 ||| shaq: single headed attention with quasi-recurrence. ||| nashwin bharwani ||| warren kushner ||| sangeet dandona ||| ben schreiber ||| 
2021 ||| hard-attention for scalable image classification. ||| athanasios papadopoulos ||| pawel korus ||| nasir d. memon ||| 
2019 ||| can attention masks improve adversarial robustness? ||| pratik vaishnavi ||| tianji cong ||| kevin eykholt ||| atul prakash ||| amir rahmati ||| 
2019 ||| hierarchically-refined label attention network for sequence labeling. ||| leyang cui ||| yue zhang ||| 
2021 ||| efficient-capsnet: capsule network with self-attention routing. ||| vittorio mazzia ||| francesco salvetti ||| marcello chiaberge ||| 
2021 ||| cpt: a pre-trained unbalanced transformer for both chinese language understanding and generation. ||| yunfan shao ||| zhichao geng ||| yitao liu ||| junqi dai ||| fei yang ||| li zhe ||| hujun bao ||| xipeng qiu ||| 
2022 ||| convolutional self-attention-based multi-user mimo demapper. ||| athur michon ||| fay ||| al ait aoudia ||| k. pavan srinath ||| 
2019 ||| one-stage inpainting with bilateral attention and pyramid filling block. ||| hongyu liu ||| bin jiang ||| wei huang ||| chao yang ||| 
2021 ||| non-invasive self-attention for side information fusion in sequential recommendation. ||| chang liu ||| xiaoguang li ||| guohao cai ||| zhenhua dong ||| hong zhu ||| lifeng shang ||| 
2022 ||| boat: bilateral local attention vision transformer. ||| tan yu ||| gangming zhao ||| ping li ||| yizhou yu ||| 
2020 ||| uncertainty-aware attention graph neural network for defending adversarial attacks. ||| boyuan feng ||| yuke wang ||| zheng wang ||| yufei ding ||| 
2021 ||| listening to the city, attentively: a spatio-temporal attention boosted autoencoder for the short-term flow prediction problem. ||| stefano fiorini ||| michele ciavotta ||| andrea maurino ||| 
2018 ||| triplet network with attention for speaker diarization. ||| huan song ||| megan m. willi ||| jayaraman j. thiagarajan ||| visar berisha ||| andreas spanias ||| 
2018 ||| an analysis of attention mechanisms: the case of word sense disambiguation in neural machine translation. ||| gongbo tang ||| rico sennrich ||| joakim nivre ||| 
2021 ||| multi-modal transformers excel at class-agnostic object detection. ||| muhammad maaz ||| hanoona abdul rasheed ||| salman h. khan ||| fahad shahbaz khan ||| rao muhammad anwer ||| ming-hsuan yang ||| 
2021 ||| learning to shift attention for motion generation. ||| you zhou ||| jianfeng gao ||| tamim asfour ||| 
2021 ||| nlp-iis@ut at semeval-2021 task 4: machine reading comprehension using the long document transformer. ||| hossein basafa ||| sajad movahedi ||| ali ebrahimi ||| azadeh shakery ||| heshaam faili ||| 
2021 ||| streaming transformer transducer based speech recognition using non-causal convolution. ||| yangyang shi ||| chunyang wu ||| dilin wang ||| alex xiao ||| jay mahadeokar ||| xiaohui zhang ||| chunxi liu ||| ke li ||| yuan shangguan ||| varun nagaraja ||| ozlem kalinli ||| mike seltzer ||| 
2020 ||| attention model enhanced network for classification of breast cancer image. ||| xiao kang ||| xingbo liu ||| xiushan nie ||| xiaoming xi ||| yilong yin ||| 
2021 ||| class semantics-based attention for action detection. ||| deepak sridhar ||| niamul quader ||| srikanth muralidharan ||| yaoxin li ||| peng dai ||| juwei lu ||| 
2021 ||| scenes and surroundings: scene graph generation using relation transformer. ||| rajat koner ||| poulami sinhamahapatra ||| volker tresp ||| 
2020 ||| adaptive attention span in computer vision. ||| jerrod parker ||| shakti kumar ||| joe roussy ||| 
2021 ||| a volumetric transformer for accurate 3d tumor segmentation. ||| himashi peiris ||| munawar hayat ||| zhaolin chen ||| gary f. egan ||| mehrtash harandi ||| 
2021 ||| attend2pack: bin packing through deep reinforcement learning with attention. ||| jingwei zhang ||| bin zi ||| xiaoyu ge ||| 
2021 ||| feature pyramid network with multi-head attention for semantic segmentation of fine-resolution remotely sensed images. ||| rui li ||| shunyi zheng ||| chenxi duan ||| 
2021 ||| duality temporal-channel-frequency attention enhanced speaker representation learning. ||| li zhang ||| qing wang ||| lei xie ||| 
2021 ||| transmef: a transformer-based multi-exposure image fusion framework using self-supervised multi-task learning. ||| linhao qu ||| shaolei liu ||| manning wang ||| zhijian song ||| 
2017 ||| task-driven visual saliency and attention-based visual question answering. ||| yuetan lin ||| zhangyang pang ||| donghui wang ||| yueting zhuang ||| 
2021 ||| hift: hierarchical feature transformer for aerial tracking. ||| ziang cao ||| changhong fu ||| junjie ye ||| bowen li ||| yiming li ||| 
2019 ||| extracting multiple-relations in one-pass with pre-trained transformers. ||| haoyu wang ||| ming tan ||| mo yu ||| shiyu chang ||| dakuo wang ||| kun xu ||| xiaoxiao guo ||| saloni potdar ||| 
2019 ||| positional attention-based frame identification with bert: a deep learning approach to target disambiguation and semantic frame selection. ||| sang-sang tan ||| jin-cheon na ||| 
2021 ||| end-to-end prostate cancer detection in bpmri via 3d cnns: effect of attention mechanisms, clinical priori and decoupled false positive reduction. ||| anindo saha ||| matin hosseinzadeh ||| henkjan j. huisman ||| 
2021 ||| no-reference image quality assessment via transformers, relative ranking, and self-consistency. ||| s. alireza golestaneh ||| saba dadsetan ||| kris m. kitani ||| 
2021 ||| inducing meaningful units from character sequences with slot attention. ||| melika behjati ||| james henderson ||| 
2021 ||| swin-unet: unet-like pure transformer for medical image segmentation. ||| hu cao ||| yueyue wang ||| joy chen ||| dongsheng jiang ||| xiaopeng zhang ||| qi tian ||| manning wang ||| 
2020 ||| multi-scale transformer language models. ||| sandeep subramanian ||| ronan collobert ||| marc'aurelio ranzato ||| y-lan boureau ||| 
2018 ||| paccmann: prediction of anticancer compound sensitivity with multi-modal attention-based neural networks. ||| ali oskooei ||| jannis born ||| matteo manica ||| vigneshwari subramanian ||| julio s ||| ez-rodr ||| guez ||| mar ||| a rodr ||| guez mart ||| nez ||| 
2020 ||| hierarchical transformer for task oriented dialog systems. ||| bishal santra ||| potnuru anusha ||| pawan goyal ||| 
2017 ||| mining significant microblogs for misinformation identification: an attention-based approach. ||| qiang liu ||| feng yu ||| shu wu ||| liang wang ||| 
2021 ||| trear: transformer-based rgb-d egocentric action recognition. ||| xiangyu li ||| yonghong hou ||| pichao wang ||| zhimin gao ||| mingliang xu ||| wanqing li ||| 
2021 ||| using transformer based ensemble learning to classify scientific articles. ||| sohom ghosh ||| ankush chopra ||| 
2021 ||| attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton. ||| xi zhang ||| xiaolin wu ||| 
2021 ||| a latent transformer for disentangled and identity-preserving face editing. ||| xu yao ||| alasdair newson ||| yann gousseau ||| pierre hellier ||| 
2020 ||| neural arbitrary style transfer for portrait images using the attention mechanism. ||| s. a. berezin ||| v. m. volkova ||| 
2020 ||| transformer reasoning network for image-text matching and retrieval. ||| nicola messina ||| fabrizio falchi ||| andrea esuli ||| giuseppe amato ||| 
2021 ||| lightweight transformer in federated setting for human activity recognition. ||| ali raza ||| kim phuc tran ||| ludovic koehl ||| shujun li ||| xianyi zeng ||| khaled benzaidi ||| 
2019 ||| fan: focused attention networks. ||| chu wang ||| babak samari ||| vladimir g. kim ||| siddhartha chaudhuri ||| kaleem siddiqi ||| 
2021 ||| transmask: a compact and fast speech separation model based on transformer. ||| zining zhang ||| bingsheng he ||| zhenjie zhang ||| 
2021 ||| parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. ||| rabeeh karimi mahabadi ||| sebastian ruder ||| mostafa dehghani ||| james henderson ||| 
2021 ||| sscan: a spatial-spectral cross attention network for hyperspectral image denoising. ||| zhiqiang wang ||| zhenfeng shao ||| xiao huang ||| jiaming wang ||| tao lu ||| sihang zhang ||| 
2018 ||| residential transformer overloading risk assessment using clustering analysis. ||| ming dong ||| benzhe li ||| alex nassif ||| 
2020 ||| difference attention based error correction lstm model for time series prediction. ||| yuxuan liu ||| jiangyong duan ||| juan meng ||| 
2020 ||| higher order linear transformer. ||| jean mercat ||| 
2020 ||| multimodal spatial attention module for targeting multimodal pet-ct lung tumor segmentation. ||| xiaohang fu ||| lei bi ||| ashnil kumar ||| michael j. fulham ||| jinman kim ||| 
2022 ||| parallel spatio-temporal attention-based tcn for multivariate time series prediction. ||| jin fan ||| ke zhang ||| yipan huang ||| yifei zhu ||| baiping chen ||| 
2021 ||| pale transformer: a general vision transformer backbone with pale-shaped attention. ||| sitong wu ||| tianyi wu ||| haoru tan ||| guodong guo ||| 
2021 ||| cap-gan: towards adversarial robustness with cycle-consistent attentional purification. ||| mingu kang ||| trung quang tran ||| seung ju cho ||| daeyoung kim ||| 
2021 ||| disenkgat: knowledge graph embedding with disentangled graph attention network. ||| junkang wu ||| wentao shi ||| xuezhi cao ||| jiawei chen ||| wenqiang lei ||| fuzheng zhang ||| wei wu ||| xiangnan he ||| 
2019 ||| attention-based structural-plasticity. ||| soheil kolouri ||| nicholas ketz ||| xinyun zou ||| jeffrey l. krichmar ||| praveen k. pilly ||| 
2018 ||| to find where you talk: temporal sentence localization in video with attention based location regression. ||| yitian yuan ||| tao mei ||| wenwu zhu ||| 
2020 ||| human-to-robot attention transfer for robot execution failure avoidance using stacked neural networks. ||| boyi song ||| yuntao peng ||| ruijiao luo ||| rui liu ||| 
2019 ||| question classification with deep contextualized transformer. ||| haozheng luo ||| ningwei liu ||| charles feng ||| 
2020 ||| stacked debert: all attention in incomplete data for text classification. ||| gwenaelle cunha sergio ||| minho lee ||| 
2022 ||| indication as prior knowledge for multimodal disease classification in chest radiographs with transformers. ||| grzegorz jacenk ||| w ||| alison q. o'neil ||| sotirios a. tsaftaris ||| 
2020 ||| a transformer-based embedding model for personalized product search. ||| keping bi ||| qingyao ai ||| w. bruce croft ||| 
2020 ||| pedestrian tracking with gated recurrent units and attention mechanisms. ||| mahdi elhousni ||| xinming huang ||| 
2021 ||| dual causal/non-causal self-attention for streaming end-to-end speech recognition. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2021 ||| on the rate of convergence of a classifier based on a transformer encoder. ||| iryna gurevych ||| michael kohler ||| g ||| zde g ||| l sahin ||| 
2021 ||| mixed transformer u-net for medical image segmentation. ||| hongyi wang ||| shiao xie ||| lanfen lin ||| yutaro iwamoto ||| xian-hua han ||| yen-wei chen ||| ruofeng tong ||| 
2020 ||| towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters. ||| yujie he ||| changhong fu ||| fuling lin ||| yiming li ||| peng lu ||| 
2020 ||| meranet: facial micro-expression recognition using 3d residual attention network. ||| gajjala viswanatha reddy ||| sai prasanna teja reddy ||| snehasis mukherjee ||| shiv ram dubey ||| 
2021 ||| bevt: bert pretraining of video transformers. ||| rui wang ||| dongdong chen ||| zuxuan wu ||| yinpeng chen ||| xiyang dai ||| mengchen liu ||| yu-gang jiang ||| luowei zhou ||| lu yuan ||| 
2019 ||| generating natural language explanations for visual question answering using scene graphs and visual attention. ||| shalini ghosh ||| giedrius burachas ||| arijit ray ||| avi ziskind ||| 
2020 ||| unsupervised attention based instance discriminative learning for person re-identification. ||| kshitij nikhal ||| benjamin s. riggan ||| 
2022 ||| fullsubnet+: channel attention fullsubnet with complex spectrograms for speech enhancement. ||| jun chen ||| zilin wang ||| deyi tuo ||| zhiyong wu ||| shiyin kang ||| helen meng ||| 
2020 ||| adaptive compact attention for few-shot video-to-video translation. ||| risheng huang ||| li shen ||| xuan wang ||| cheng lin ||| hao-zhi huang ||| 
2021 ||| finding strong gravitational lenses through self-attention. ||| hareesh thuruthipilly ||| adam zadrozny ||| agnieszka pollo ||| 
2021 ||| end-to-end attention-based image captioning. ||| carola sundaramoorthy ||| lin ziwen kelvin ||| mahak sarin ||| shubham gupta ||| 
2020 ||| hierarchical attention learning of scene flow in 3d point clouds. ||| guangming wang ||| xinrui wu ||| zhe liu ||| hesheng wang ||| 
2017 ||| a gru-gated attention model for neural machine translation. ||| biao zhang ||| deyi xiong ||| jinsong su ||| 
2021 ||| channel-temporal attention for first-person video domain adaptation. ||| xianyuan liu ||| shuo zhou ||| tao lei ||| haiping lu ||| 
2021 ||| multilingual pre-trained transformers and convolutional nn classification models for technical domain identification. ||| suman dowlagar ||| radhika mamidi ||| 
2020 ||| one-shot text field labeling using attention and belief propagation for structure information extraction. ||| mengli cheng ||| minghui qiu ||| xing shi ||| jun huang ||| wei lin ||| 
2017 ||| look-ahead attention for generation in neural machine translation. ||| long zhou ||| jiajun zhang ||| chengqing zong ||| 
2018 ||| attention-based recurrent neural network for urban vehicle trajectory prediction. ||| seongjin choi ||| jiwon kim ||| hwasoo yeo ||| 
2020 ||| does presence of social media plugins in a journal website result in higher social media attention of its research publications. ||| mousumi karmakar ||| sumit kumar banshal ||| vivek kumar singh ||| 
2018 ||| attention branch network: learning of attention mechanism for visual explanation. ||| hiroshi fukui ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2022 ||| rice diseases detection and classification using attention based neural network and bayesian optimization. ||| yibin wang ||| haifeng wang ||| zhaohua peng ||| 
2021 ||| explainable health risk predictor with transformer-based medicare claim encoder. ||| chuhong lahlou ||| ancil crayton ||| caroline trier ||| evan willett ||| 
2017 ||| attngan: fine-grained text to image generation with attentional generative adversarial networks. ||| tao xu ||| pengchuan zhang ||| qiuyuan huang ||| han zhang ||| zhe gan ||| xiaolei huang ||| xiaodong he ||| 
2020 ||| social commonsense reasoning with multi-head knowledge attention. ||| debjit paul ||| anette frank ||| 
2019 ||| fully quantizing a simplified transformer for end-to-end speech recognition. ||| alex bie ||| bharat venkitesh ||| joao monteiro ||| md. akmal haidar ||| mehdi rezagholizadeh ||| 
2019 ||| character-based nmt with transformer. ||| rohit gupta ||| laurent besacier ||| marc dymetman ||| matthias gall ||| 
2018 ||| connecting gaze, scene, and attention: generalized attention estimation via joint modeling of gaze and scene saliency. ||| eunji chong ||| nataniel ruiz ||| yongxin wang ||| yun zhang ||| agata rozga ||| james m. rehg ||| 
2020 ||| utilizing bidirectional encoder representations from transformers for answer selection. ||| md. tahmid rahman laskar ||| enamul hoque ||| jimmy xiangji huang ||| 
2021 ||| low-latency auditory spatial attention detection based on spectro-spatial features from eeg. ||| siqi cai ||| pengcheng sun ||| tanja schultz ||| haizhou li ||| 
2021 ||| transfer: learning relation-aware facial expression representations with transformers. ||| fanglei xue ||| qiangchang wang ||| guodong guo ||| 
2019 ||| towards explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders. ||| matteo manica ||| ali oskooei ||| jannis born ||| vigneshwari subramanian ||| julio s ||| ez-rodr ||| guez ||| mar ||| a rodr ||| guez mart ||| nez ||| 
2020 ||| epipolar transformers. ||| yihui he ||| rui yan ||| katerina fragkiadaki ||| shoou-i yu ||| 
2018 ||| attention mechanism in speaker recognition: what does it learn in deep speaker embedding? ||| qiongqiong wang ||| koji okabe ||| kong aik lee ||| hitoshi yamamoto ||| takafumi koshinaka ||| 
2018 ||| toward extractive summarization of online forum discussions via hierarchical attention networks. ||| sansiri tarnpradab ||| fei liu ||| kien a. hua ||| 
2021 ||| wsgat: weighted and signed graph attention networks for link prediction. ||| marco grassia ||| giuseppe mangioni ||| 
2021 ||| civil rephrases of toxic texts with self-supervised transformers. ||| leo laugier ||| john pavlopoulos ||| jeffrey sorensen ||| lucas dixon ||| 
2019 ||| sequence to sequence with attention for influenza prevalence prediction using google trends. ||| kenjiro kondo ||| akihiko ishikawa ||| masashi kimura ||| 
2019 ||| contextualized sparse representation with rectified n-gram attention for open-domain question answering. ||| jinhyuk lee ||| min joon seo ||| hannaneh hajishirzi ||| jaewoo kang ||| 
2020 ||| blur-attention: a boosting mechanism for non-uniform blurred image restoration. ||| xiaoguang li ||| feifan yang ||| kin-man lam ||| li zhuo ||| jiafeng li ||| 
2020 ||| compressing transformer-based semantic parsing models using compositional code embeddings. ||| prafull prakash ||| saurabh kumar shashidhar ||| wenlong zhao ||| subendhu rongali ||| haidar khan ||| michael kayser ||| 
2020 ||| texture transform attention for realistic image inpainting. ||| yejin kim ||| manri cheon ||| junwoo lee ||| 
2021 ||| self-attention based anchor proposal for skeleton-based action recognition. ||| ruijie hou ||| zhao wang ||| 
2020 ||| gobo: quantizing attention-based nlp models for low latency and energy efficient inference. ||| ali hadi zadeh ||| andreas moshovos ||| 
2021 ||| sotr: segmenting objects with transformers. ||| ruohao guo ||| dantong niu ||| liao qu ||| zhenbo li ||| 
2021 ||| graph kernel attention transformers. ||| krzysztof choromanski ||| han lin ||| haoxian chen ||| jack parker-holder ||| 
2020 ||| context-transformer: tackling object confusion for few-shot detection. ||| ze yang ||| yali wang ||| xianyu chen ||| jianzhuang liu ||| yu qiao ||| 
2022 ||| ssha: video violence recognition and localization using a semi-supervised hard attention model. ||| hamid mohammadi ||| ehsan nazerfard ||| 
2018 ||| graph convolutional neural networks via motif-based attention. ||| hao peng ||| jianxin li ||| qiran gong ||| yuanxing ning ||| lihong wang ||| 
2018 ||| skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention. ||| guyue hu ||| bo cui ||| shan yu ||| 
2021 ||| exploring separable attention for multi-contrast mr image super-resolution. ||| chun-mei feng ||| yunlu yan ||| chengliang liu ||| huazhu fu ||| yong xu ||| ling shao ||| 
2021 ||| context-aware attentional pooling (cap) for fine-grained visual classification. ||| ardhendu behera ||| zachary wharton ||| pradeep hewage ||| asish bera ||| 
2022 ||| learning semantics for visual place recognition through multi-scale attention. ||| valerio paolicelli ||| antonio tavera ||| carlo masone ||| gabriele moreno berton ||| barbara caputo ||| 
2018 ||| pay more attention - neural architectures for question-answering. ||| zia hasan ||| sebastian fischer ||| 
2021 ||| radams: resilient and adaptive alert and attention management strategy against informational denial-of-service (idos) attacks. ||| linan huang ||| quanyan zhu ||| 
2021 ||| context transformer with stacked pointer networks for conversational question answering over knowledge graphs. ||| joan plepi ||| endri kacupaj ||| kuldeep singh ||| harsh thakkar ||| jens lehmann ||| 
2022 ||| scaling laws under the microscope: predicting transformer performance from small scale experiments. ||| maor ivgi ||| yair carmon ||| jonathan berant ||| 
2020 ||| contour transformer network for one-shot segmentation of anatomical structures. ||| yuhang lu ||| kang zheng ||| weijian li ||| yirui wang ||| adam p. harrison ||| chihung lin ||| song wang ||| jing xiao ||| le lu ||| chang-fu kuo ||| shun miao ||| 
2021 ||| cma-net: a cascaded mutual attention network for light field salient object detection. ||| yi zhang ||| lu zhang ||| wassim hamidouche ||| olivier d ||| forges ||| 
2021 ||| spatio-temporal attention mechanism and knowledge distillation for lip reading. ||| shahd elashmawy ||| marian ramsis ||| hesham m. eraqi ||| farah eldeshnawy ||| hadeel mabrouk ||| omar abugabal ||| nourhan sakr ||| 
2020 ||| da-transformer: distance-aware transformer. ||| chuhan wu ||| fangzhao wu ||| yongfeng huang ||| 
2021 ||| widecaps: a wide attention based capsule network for image classification. ||| pawan s. jogi ||| rishi sharma ||| hemanth sai ram reddy ||| m. vani ||| jeny rajan ||| 
2021 ||| oriented object detection with transformer. ||| teli ma ||| mingyuan mao ||| honghui zheng ||| peng gao ||| xiaodi wang ||| shumin han ||| errui ding ||| baochang zhang ||| david s. doermann ||| 
2021 ||| bayesian attention belief networks. ||| shujian zhang ||| xinjie fan ||| bo chen ||| mingyuan zhou ||| 
2021 ||| global-local attention for emotion recognition. ||| nhat le ||| khanh nguyen ||| anh nguyen ||| bac le ||| 
2020 ||| looking for change? roll the dice and demand attention. ||| foivos i. diakogiannis ||| fran ||| ois waldner ||| peter caccetta ||| 
2020 ||| on the computational power of transformers and its implications in sequence modeling. ||| satwik bhattamishra ||| arkil patel ||| navin goyal ||| 
2019 ||| improving generation quality of pointer networks via guided attention. ||| kushal chawla ||| kundan krishna ||| balaji vasan srinivasan ||| 
2019 ||| sid4vam: a benchmark dataset with synthetic images for visual attention modeling. ||| david berga ||| xos |||  ram ||| n fdez-vidal ||| xavier otazu ||| xos |||  m. pardo ||| 
2018 ||| aspect term extraction with history attention and selective transformation. ||| xin li ||| lidong bing ||| piji li ||| wai lam ||| zhimou yang ||| 
2021 ||| neural attention-aware hierarchical topic model. ||| yuan jin ||| he zhao ||| ming liu ||| lan du ||| wray l. buntine ||| 
2020 ||| pruning redundant mappings in transformer models via spectral-normalized identity prior. ||| zi lin ||| jeremiah zhe liu ||| zi yang ||| nan hua ||| dan roth ||| 
2019 ||| uan: unified attention network for convolutional neural networks. ||| tony joseph ||| konstantinos g. derpanis ||| faisal z. qureshi ||| 
2021 ||| rethinking spatial dimensions of vision transformers. ||| byeongho heo ||| sangdoo yun ||| dongyoon han ||| sanghyuk chun ||| junsuk choe ||| seong joon oh ||| 
2020 ||| transformers for modeling physical systems. ||| nicholas geneva ||| nicholas zabaras ||| 
2021 ||| analysis of graphsum's attention weights to improve the explainability of multi-document summarization. ||| m. lautaro hickmann ||| fabian wurzberger ||| megi hoxhalli ||| arne lochner ||| jessica t ||| llich ||| ansgar scherp ||| 
2021 ||| real-time instance segmentation of surgical instruments using attention and multi-scale feature fusion. ||| juan carlos angeles ceron ||| gilberto ochoa-ruiz ||| leonardo chang ||| sharib ali ||| 
2019 ||| a transformer-based approach to irony and sarcasm detection. ||| rolandos alexandros potamias ||| georgios siolas ||| andreas-georgios stafylopatis ||| 
2021 ||| vision transformer for fast and efficient scene text recognition. ||| rowel atienza ||| 
2021 ||| giid-net: generalizable image inpainting detection via neural architecture search and attention. ||| haiwei wu ||| jiantao zhou ||| 
2021 ||| demystifying the better performance of position encoding variants for transformer. ||| pu-chin chen ||| henry tsai ||| srinadh bhojanapalli ||| hyung won chung ||| yin-wen chang ||| chun-sung ferng ||| 
2021 ||| graph attention multi-layer perceptron. ||| wentao zhang ||| ziqi yin ||| zeang sheng ||| wen ouyang ||| xiaosen li ||| yangyu tao ||| zhi yang ||| bin cui ||| 
2020 ||| exploring global diverse attention via pairwise temporal relation for video summarization. ||| ping li ||| qinghao ye ||| luming zhang ||| li yuan ||| xianghua xu ||| ling shao ||| 
2022 ||| blue at memotion 2.0 2022: you have my image, my text and my transformer. ||| ana-maria bucur ||| adrian cosma ||| ioan-bogdan iordache ||| 
2018 ||| attention driven person re-identification. ||| fan yang ||| ke yan ||| shijian lu ||| huizhu jia ||| xiaodong xie ||| wen gao ||| 
2019 ||| neural-attention-based deep learning architectures for modeling traffic dynamics on lane graphs. ||| matthew a. wright ||| simon f. g. ehlers ||| roberto horowitz ||| 
2018 ||| on attention modules for audio-visual synchronization. ||| naji khosravan ||| shervin ardeshir ||| rohit puri ||| 
2017 ||| pre-training attention mechanisms. ||| jack lindsey ||| 
2018 ||| area attention. ||| yang li ||| lukasz kaiser ||| samy bengio ||| si si ||| 
2022 ||| septr: separable transformer for audio spectrogram processing. ||| nicolae-catalin ristea ||| radu tudor ionescu ||| fahad shahbaz khan ||| 
2022 ||| training-free transformer architecture search. ||| qinqin zhou ||| kekai sheng ||| xiawu zheng ||| ke li ||| xing sun ||| yonghong tian ||| jie chen ||| rongrong ji ||| 
2018 ||| visual attention and its intimate links to spatial cognition. ||| john k. tsotsos ||| iuliia kotseruba ||| amir rasouli ||| markus d. solbach ||| 
2020 ||| human-centric spatio-temporal video grounding with visual transformers. ||| zongheng tang ||| yue liao ||| si liu ||| guanbin li ||| xiaojie jin ||| hongxu jiang ||| qian yu ||| dong xu ||| 
2019 ||| comparison of neuronal attention models. ||| mohamed karim belaid ||| 
2021 ||| exploiting multi-scale fusion, spatial attention and patch interaction techniques for text-independent writer identification. ||| abhishek srivastava ||| sukalpa chanda ||| umapada pal ||| 
2020 ||| iscas at semeval-2020 task 5: pre-trained transformers for counterfactual statement modeling. ||| yaojie lu ||| annan li ||| hongyu lin ||| xianpei han ||| le sun ||| 
2019 ||| attention, please! a critical review of neural attention models in natural language processing. ||| andrea galassi ||| marco lippi ||| paolo torroni ||| 
2022 ||| a context-integrated transformer-based neural network for auction design. ||| zhijian duan ||| jingwu tang ||| yutong yin ||| zhe feng ||| xiang yan ||| manzil zaheer ||| xiaotie deng ||| 
2021 ||| episodic transformer for vision-and-language navigation. ||| alexander pashevich ||| cordelia schmid ||| chen sun ||| 
2018 ||| attention-aware generalized mean pooling for image retrieval. ||| yinzheng gu ||| chuanpeng li ||| jinbin xie ||| 
2020 ||| attention cube network for image restoration. ||| yucheng hang ||| qingmin liao ||| wenming yang ||| yupeng chen ||| jie zhou ||| 
2019 ||| adaptive attention span in transformers. ||| sainbayar sukhbaatar ||| edouard grave ||| piotr bojanowski ||| armand joulin ||| 
2018 ||| improving the transformer translation model with document-level context. ||| jiacheng zhang ||| huanbo luan ||| maosong sun ||| feifei zhai ||| jingfang xu ||| min zhang ||| yang liu ||| 
2018 ||| attention-guided answer distillation for machine reading comprehension. ||| minghao hu ||| yuxing peng ||| furu wei ||| zhen huang ||| dongsheng li ||| nan yang ||| ming zhou ||| 
2020 ||| automatic detection of cardiac chambers using an attention-based yolov4 framework from four-chamber view of fetal echocardiography. ||| sibo qiao ||| shanchen pang ||| gang luo ||| silin pan ||| xun wang ||| min wang ||| xue zhai ||| taotao chen ||| 
2020 ||| augmenting transformers with knn-based composite memory for dialogue. ||| angela fan ||| claire gardent ||| chlo |||  braud ||| antoine bordes ||| 
2021 ||| block-skim: efficient question answering for transformer. ||| yue guan ||| zhengyi li ||| jingwen leng ||| zhouhan lin ||| minyi guo ||| yuhao zhu ||| 
2021 ||| group-free 3d object detection via transformers. ||| ze liu ||| zheng zhang ||| yue cao ||| han hu ||| xin tong ||| 
2021 ||| continuous-time sequential recommendation with temporal graph collaborative transformer. ||| ziwei fan ||| zhiwei liu ||| jiawei zhang ||| yun xiong ||| lei zheng ||| philip s. yu ||| 
2017 ||| two-stream flow-guided convolutional attention networks for action recognition. ||| an tran ||| loong-fah cheong ||| 
2021 ||| baller2vec: a multi-entity transformer for multi-agent spatiotemporal modeling. ||| michael a. alcorn ||| anh nguyen ||| 
2019 ||| haxmlnet: hierarchical attention network for extreme multi-label text classification. ||| ronghui you ||| zihan zhang ||| suyang dai ||| shanfeng zhu ||| 
2019 ||| spa-gan: spatial attention gan for image-to-image translation. ||| hajar emami ||| majid moradi aliabadi ||| ming dong ||| ratna babu chinnam ||| 
2020 ||| cross-lingual zero- and few-shot hate speech detection utilising frozen transformer language models and axel. ||| lukas stappen ||| fabian brunn ||| bj ||| rn w. schuller ||| 
2019 ||| attention control with metric learning alignment for image set-based recognition. ||| xiaofeng liu ||| zhenhua guo ||| jane you ||| b. v. k. vijaya kumar ||| 
2017 ||| multi-task coupled attentions for category-specific aspect and opinion terms co-extraction. ||| wenya wang ||| sinno jialin pan ||| daniel dahlmeier ||| 
2021 ||| transformer based trajectory prediction. ||| aleksey postnikov ||| aleksander gamayunov ||| gonzalo ferrer ||| 
2020 ||| explicitly modeling adaptive depths for transformer. ||| yijin liu ||| fandong meng ||| jie zhou ||| yufeng chen ||| jinan xu ||| 
2022 ||| graph attention retrospective. ||| kimon fountoulakis ||| amit levi ||| shenghao yang ||| aseem baranwal ||| aukosh jagannath ||| 
2020 ||| retinotopicnet: an iterative attention mechanism using local descriptors with global context. ||| thomas kurbiel ||| shahrzad khaleghian ||| 
2020 ||| self-supervised attention learning for depth and ego-motion estimation. ||| assem sadek ||| boris chidlovskii ||| 
2020 ||| toward improving the evaluation of visual attention models: a crowdsourcing approach. ||| dario zanca ||| stefano melacci ||| marco gori ||| 
2021 ||| spatial attention-based non-reference perceptual quality prediction network for omnidirectional images. ||| li yang ||| mai xu ||| xin deng ||| bo feng ||| 
2018 ||| denssiam: end-to-end densely-siamese network with self-attention model for object tracking. ||| mohamed h. abdelpakey ||| mohamed s. shehata ||| mostafa m. mohamed ||| 
2021 ||| unsupervised out-of-domain detection via pre-trained transformers. ||| keyang xu ||| tongzheng ren ||| shikun zhang ||| yihao feng ||| caiming xiong ||| 
2021 ||| kvt: k-nn attention for boosting vision transformers. ||| pichao wang ||| xue wang ||| fan wang ||| ming lin ||| shuning chang ||| wen xie ||| hao li ||| rong jin ||| 
2021 ||| automated essay scoring using transformer models. ||| sabrina ludwig ||| christian mayer ||| christopher hansen ||| kerstin eilers ||| steffen brandt ||| 
2022 ||| incremental transformer structure enhanced image inpainting with masking positional encoding. ||| qiaole dong ||| chenjie cao ||| yanwei fu ||| 
2022 ||| vitranspad: video transformer using convolution and self-attention for face presentation attack detection. ||| zuheng ming ||| zitong yu ||| musab qassem al-ghadi ||| muriel visani ||| muhammad muzzamil luqman ||| jean-christophe burie ||| 
2022 ||| arbitrary shape text detection using transformers. ||| zobeir raisi ||| georges younes ||| john s. zelek ||| 
2019 ||| attention: a big surprise for cross-domain person re-identification. ||| haijun liu ||| jian cheng ||| shiguang wang ||| wen wang ||| 
2020 ||| moltrans: molecular interaction transformer for drug target interaction prediction. ||| kexin huang ||| cao xiao ||| lucas glass ||| jimeng sun ||| 
2021 ||| co-scale conv-attentional image transformers. ||| weijian xu ||| yifan xu ||| tyler a. chang ||| zhuowen tu ||| 
2020 ||| perceive, attend, and drive: learning spatial attention for safe self-driving. ||| bob wei ||| mengye ren ||| wenyuan zeng ||| ming liang ||| bin yang ||| raquel urtasun ||| 
2021 ||| spatial-temporal transformer for dynamic scene graph generation. ||| yuren cong ||| wentong liao ||| hanno ackermann ||| michael ying yang ||| bodo rosenhahn ||| 
2021 ||| a new state-of-the-art transformers-based load forecaster on the smart grid domain. ||| andr |||  luiz farias novaes ||| rui alexandre de matos ara ||| jo ||| jose figueiredo ||| lucas aguiar pavanelli ||| 
2020 ||| problemconquero at semeval-2020 task 12: transformer and soft label-based approaches. ||| karishma laud ||| jagriti singh ||| randeep kumar sahu ||| ashutosh modi ||| 
2022 ||| pretr: spatio-temporal non-autoregressive trajectory prediction transformer. ||| lina achaji ||| thierno barry ||| thibault fouqueray ||| julien moreau ||| fran ||| ois aioun ||| fran ||| ois charpillet ||| 
2020 ||| compressing large-scale transformer-based models: a case study on bert. ||| prakhar ganesh ||| yao chen ||| xin lou ||| mohammad ali khan ||| yin yang ||| deming chen ||| marianne winslett ||| hassan sajjad ||| preslav nakov ||| 
2020 ||| pruning and sparsemax methods for hierarchical attention networks. ||| jo ||| o g. ribeiro ||| frederico s. felisberto ||| isabel c. neto ||| 
2022 ||| uvcgan: unet vision transformer cycle-consistent gan for unpaired image-to-image translation. ||| dmitrii torbunov ||| yi huang ||| haiwang yu ||| jin huang ||| shinjae yoo ||| meifeng lin ||| brett viren ||| yihui ren ||| 
2019 ||| porous lattice-based transformer encoder for chinese ner. ||| mengge xue ||| bowen yu ||| tingwen liu ||| bin wang ||| erli meng ||| quangang li ||| 
2022 ||| your "attention" deserves attention: a self-diversified multi-channel attention for facial action analysis. ||| xiaotian li ||| zhihua li ||| huiyuan yang ||| geran zhao ||| lijun yin ||| 
2022 ||| mdan: multi-level dependent attention network for visual emotion analysis. ||| liwen xu ||| zhengtao wang ||| bin wu ||| simon lui ||| 
2021 ||| instance-level image retrieval using reranking transformers. ||| fuwen tan ||| jiangbo yuan ||| vicente ordonez ||| 
2019 ||| spatio-temporal crop classification of low-resolution satellite imagery with capsule layers and distributed attention. ||| john brandt ||| 
2021 ||| generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. ||| hila chefer ||| shir gur ||| lior wolf ||| 
2021 ||| racebert - a transformer-based model for predicting race and ethnicity from names. ||| prasanna parasurama ||| 
2020 ||| learning efficient representations of mouse movements to predict user attention. ||| ioannis arapakis ||| luis a. leiva ||| 
2022 ||| pear: personalized re-ranking with contextualized transformer for recommendation. ||| yi li ||| jieming zhu ||| weiwen liu ||| liangcai su ||| guohao cai ||| qi zhang ||| ruiming tang ||| xi xiao ||| xiuqiang he ||| 
2020 ||| applying the transformer to character-level transduction. ||| shijie wu ||| ryan cotterell ||| mans hulden ||| 
2021 ||| dispensed transformer network for unsupervised domain adaptation. ||| yunxiang li ||| jingxiong li ||| ruilong dan ||| shuai wang ||| kai jin ||| guodong zeng ||| jun wang ||| xiangji pan ||| qianni zhang ||| huiyu zhou ||| qun jin ||| li wang ||| yaqi wang ||| 
2021 ||| transzero++: cross attribute-guided transformer for zero-shot learning. ||| shiming chen ||| ziming hong ||| guo-sen xie ||| jian zhao ||| hao li ||| xinge you ||| shuicheng yan ||| ling shao ||| 
2021 ||| sparsity and sentence structure in encoder-decoder attention of summarization systems. ||| potsawee manakul ||| mark j. f. gales ||| 
2019 ||| knowledge-enriched transformer for emotion detection in textual conversations. ||| peixiang zhong ||| di wang ||| chunyan miao ||| 
2021 ||| what's hidden in a one-layer randomly weighted transformer? ||| sheng shen ||| zhewei yao ||| douwe kiela ||| kurt keutzer ||| michael w. mahoney ||| 
2019 ||| temporal transformer networks: joint learning of invariant and discriminative time warping. ||| suhas lohit ||| qiao wang ||| pavan k. turaga ||| 
2020 ||| a co-interactive transformer for joint slot filling and intent detection. ||| libo qin ||| tailu liu ||| wanxiang che ||| bingbing kang ||| sendong zhao ||| ting liu ||| 
2022 ||| when transformer meets robotic grasping: exploits context for efficient grasp detection. ||| shaochen wang ||| zhangli zhou ||| zhen kan ||| 
2021 ||| visualsparta: sparse transformer fragment-level matching for large-scale text-to-image search. ||| xiaopeng lu ||| tiancheng zhao ||| kyusong lee ||| 
2021 ||| automatic detection of rail components via a deep convolutional transformer network. ||| tiange wang ||| zijun zhang ||| fangfang yang ||| kwok-leung tsui ||| 
2020 ||| moving target defense for robust monitoring of electric grid transformers in adversarial environments. ||| sailik sengupta ||| kaustav basu ||| arunabha sen ||| subbarao kambhampati ||| 
2019 ||| hats: a hierarchical graph attention network for stock movement prediction. ||| raehyun kim ||| chan ho so ||| minbyul jeong ||| sanghoon lee ||| jinkyu kim ||| jaewoo kang ||| 
2018 ||| jointly multiple events extraction via attention-based graph information aggregation. ||| xiao liu ||| zhunchen luo ||| heyan huang ||| 
2021 ||| cross attentional audio-visual fusion for dimensional emotion recognition. ||| gnana praveen r ||| eric granger ||| patrick cardinal ||| 
2021 ||| can the transformer be used as a drop-in replacement for rnns in text-generating gans? ||| kevin blin ||| andrei kucharavy ||| 
2019 ||| attention-based deep reinforcement learning for multi-view environments. ||| elaheh barati ||| xuewen chen ||| zichun zhong ||| 
2018 ||| dual recurrent attention units for visual question answering. ||| ahmed osman ||| wojciech samek ||| 
2019 ||| deraincyclegan: an attention-guided unsupervised benchmark for single image deraining and rainmaking. ||| yanyan wei ||| zhao zhang ||| jicong fan ||| yang wang ||| shuicheng yan ||| meng wang ||| 
2021 ||| learning dynamic graph representation of brain connectome with spatio-temporal attention. ||| byung-hoon kim ||| jong chul ye ||| jae-jin kim ||| 
2018 ||| a question-focused multi-factor attention network for question answering. ||| souvik kundu ||| hwee tou ng ||| 
2021 ||| decoupled spatial-temporal transformer for video inpainting. ||| rui liu ||| hanming deng ||| yangyi huang ||| xiaoyu shi ||| lewei lu ||| wenxiu sun ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2021 ||| swin transformer v2: scaling up capacity and resolution. ||| ze liu ||| han hu ||| yutong lin ||| zhuliang yao ||| zhenda xie ||| yixuan wei ||| jia ning ||| yue cao ||| zheng zhang ||| li dong ||| furu wei ||| baining guo ||| 
2019 ||| weakly labelled audioset classification with attention neural networks. ||| qiuqiang kong ||| changsong yu ||| turab iqbal ||| yong xu ||| wenwu wang ||| mark d. plumbley ||| 
2020 ||| han-ecg: an interpretable atrial fibrillation detection model using hierarchical attention networks. ||| sajad mousavi ||| fatemeh afghah ||| u. rajendra acharya ||| 
2021 ||| attention mechanisms and deep learning for machine vision: a survey of the state of the art. ||| abdul mueed hafiz ||| shabir ahmad parah ||| rouf ul alam bhat ||| 
2018 ||| bert: pre-training of deep bidirectional transformers for language understanding. ||| jacob devlin ||| ming-wei chang ||| kenton lee ||| kristina toutanova ||| 
2020 ||| probabilistic transformers. ||| javier r. movellan ||| 
2021 ||| visual keyword spotting with attention. ||| k. r. prajwal ||| liliane momeni ||| triantafyllos afouras ||| andrew zisserman ||| 
2019 ||| an end-to-end approach for lexical stress detection based on transformer. ||| yong ruan ||| xiangdong wang ||| hong liu ||| zhigang ou ||| yun gao ||| jianfeng cheng ||| yueliang qian ||| 
2019 ||| learning to deceive with attention-based explanations. ||| danish pruthi ||| mansi gupta ||| bhuwan dhingra ||| graham neubig ||| zachary c. lipton ||| 
2020 ||| distilling knowledge from ensembles of acoustic models for joint ctc-attention end-to-end speech recognition. ||| yan gao ||| titouan parcollet ||| nicholas d. lane ||| 
2018 ||| adaptive edge features guided graph attention networks. ||| liyu gong ||| qiang cheng ||| 
2021 ||| tdan: top-down attention networks for enhanced feature selectivity in cnns. ||| shantanu jaiswal ||| basura fernando ||| cheston tan ||| 
2021 ||| single-shot motion completion with transformer. ||| yinglin duan ||| tianyang shi ||| zhengxia zou ||| yenan lin ||| zhehui qian ||| bohan zhang ||| yi yuan ||| 
2021 ||| cross-domain generalization and knowledge transfer in transformers trained on legal data. ||| jarom ||| r savelka ||| hannes westermann ||| karim benyekhlef ||| 
2018 ||| improved training of end-to-end attention models for speech recognition. ||| albert zeyer ||| kazuki irie ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| scarlet: explainable attention based graph neural network for fake news spreader prediction. ||| bhavtosh rath ||| xavier morales ||| jaideep srivastava ||| 
2019 ||| non-autoregressive transformer by position learning. ||| yu bao ||| hao zhou ||| jiangtao feng ||| mingxuan wang ||| shujian huang ||| jiajun chen ||| lei li ||| 
2019 ||| weakly-supervised completion moment detection using temporal attention. ||| farnoosh heidarivincheh ||| majid mirmehdi ||| dima damen ||| 
2018 ||| a discourse-aware attention model for abstractive summarization of long documents. ||| arman cohan ||| franck dernoncourt ||| doo soon kim ||| trung bui ||| seokhwan kim ||| walter chang ||| nazli goharian ||| 
2021 ||| decision transformer: reinforcement learning via sequence modeling. ||| lili chen ||| kevin lu ||| aravind rajeswaran ||| kimin lee ||| aditya grover ||| michael laskin ||| pieter abbeel ||| aravind srinivas ||| igor mordatch ||| 
2021 ||| handwriting transformers. ||| ankan kumar bhunia ||| salman h. khan ||| hisham cholakkal ||| rao muhammad anwer ||| fahad shahbaz khan ||| mubarak shah ||| 
2021 ||| unifying multimodal transformer for bi-directional image and text generation. ||| yupan huang ||| hongwei xue ||| bei liu ||| yutong lu ||| 
2020 ||| vstreamdrls: dynamic graph representation learning with self-attention for enterprise distributed video streaming solutions. ||| stefanos antaris ||| dimitrios rafailidis ||| 
2018 ||| discovery and usage of joint attention in images. ||| daniel harari ||| joshua b. tenenbaum ||| shimon ullman ||| 
2021 ||| distantly supervised transformers for e-commerce product qa. ||| happy mittal ||| aniket chakrabarti ||| belhassen bayar ||| animesh anant sharma ||| nikhil rasiwasia ||| 
2021 ||| co-training transformer with videos and images improves action recognition. ||| bowen zhang ||| jiahui yu ||| christopher fifty ||| wei han ||| andrew m. dai ||| ruoming pang ||| fei sha ||| 
2021 ||| turn-to-diarize: online speaker diarization constrained by transformer transducer speaker turn detection. ||| wei xia ||| han lu ||| quan wang ||| anshuman tripathi ||| ignacio lopez-moreno ||| hasim sak ||| 
2020 ||| linguistically-aware attention for reducing the semantic-gap in vision-language tasks. ||| gouthaman kv ||| athira m. nambiar ||| kancheti sai srinivas ||| anurag mittal ||| 
2021 ||| readnet: a hierarchical transformer framework for web article readability analysis. ||| changping meng ||| muhao chen ||| jie mao ||| jennifer neville ||| 
2021 ||| object based attention through internal gating. ||| jordan lei ||| ari s. benjamin ||| konrad p. kording ||| 
2020 ||| xd at semeval-2020 task 12: ensemble approach to offensive language identification in social media using transformer encoders. ||| xiangjue dong ||| jinho d. choi ||| 
2021 ||| lotr: face landmark localization using localization transformer. ||| ukrit watchareeruetai ||| benjaphan sommana ||| sanjana jain ||| pavit noinongyao ||| ankush ganguly ||| aubin samaco ||| ts ||| samuel w. f. earp ||| nakarin sritrakool ||| 
2020 ||| permutationless many-jet event reconstruction with symmetry preserving attention networks. ||| michael james fenton ||| alexander shmakov ||| ta-wei ho ||| shih-chieh hsu ||| daniel whiteson ||| pierre baldi ||| 
2020 ||| attention over parameters for dialogue systems. ||| andrea madotto ||| zhaojiang lin ||| chien-sheng wu ||| jamin shin ||| pascale fung ||| 
2022 ||| mhsnet: multi-head and spatial attention network with false-positive reduction for pulmonary nodules detection. ||| juanyun mai ||| minghao wang ||| jiayin zheng ||| yanbo shao ||| zhaoqi diao ||| xinliang fu ||| yulong chen ||| jianyu xiao ||| jian you ||| airu yin ||| yang yang ||| xiangcheng qiu ||| jingsheng tao ||| bo wang ||| hua ji ||| 
2022 ||| spatio-temporal tuples transformer for skeleton-based action recognition. ||| helei qiu ||| biao hou ||| bo ren ||| xiaohua zhang ||| 
2020 ||| attviz: online exploration of self-attention for transparent neural language modeling. ||| blaz skrlj ||| nika erzen ||| shane sheehan ||| saturnino luz ||| marko robnik-sikonja ||| senja pollak ||| 
2017 ||| delineation of skin strata in reflectance confocal microscopy images using recurrent convolutional networks with toeplitz attention. ||| alican bozkurt ||| kivan |||  k ||| se ||| jaume coll-font ||| christi alessi-fox ||| dana h. brooks ||| jennifer g. dy ||| milind rajadhyaksha ||| 
2021 ||| tsn-ca: a two-stage network with channel attention for low-light image enhancement. ||| xinxu wei ||| xian-shi zhang ||| shisen wang ||| yanlin huang ||| yongjie li ||| 
2018 ||| multi-pointer co-attention networks for recommendation. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2019 ||| improving the harmony of the composite image by spatial-separated attention module. ||| xiaodong cun ||| chi-man pun ||| 
2018 ||| convolutional self-attention network. ||| 
2021 ||| fast multi-resolution transformer fine-tuning for extreme multi-label text classification. ||| jiong zhang ||| wei-cheng chang ||| hsiang-fu yu ||| inderjit s. dhillon ||| 
2020 ||| sequential weakly labeled multi-activity recognition and location on wearable sensors using recurrent attention network. ||| kun wang ||| jun he ||| lei zhang ||| 
2019 ||| improved attention models for memory augmented neural network adaptive controllers. ||| deepan muthirayan ||| scott a. nivison ||| pramod p. khargonekar ||| 
2021 ||| progressively normalized self-attention network for video polyp segmentation. ||| ge-peng ji ||| yu-cheng chou ||| deng-ping fan ||| geng chen ||| huazhu fu ||| debesh jha ||| ling shao ||| 
2019 ||| assessing the ability of self-attention networks to learn word order. ||| baosong yang ||| longyue wang ||| derek f. wong ||| lidia s. chao ||| zhaopeng tu ||| 
2020 ||| cross-global attention graph kernel network prediction of drug prescription. ||| hao-ren yao ||| der-chen chang ||| ophir frieder ||| wendy huang ||| i-chia liang ||| chi-feng hung ||| 
2021 ||| multi-granularity network with modal attention for dense affective understanding. ||| baoming yan ||| lin wang ||| ke gao ||| bo gao ||| xiao liu ||| chao ban ||| jiang yang ||| xiaobo li ||| 
2020 ||| targeted attention attack on deep learning models in road sign recognition. ||| xinghao yang ||| weifeng liu ||| shengli zhang ||| wei liu ||| dacheng tao ||| 
2019 ||| resilient combination of complementary cnn and rnn features for text classification through attention and ensembling. ||| athanasios giannakopoulos ||| maxime coriou ||| andreea hossmann ||| michael baeriswyl ||| claudiu musat ||| 
2020 ||| attention, please: a spatio-temporal transformer for 3d human motion prediction. ||| emre aksan ||| peng cao ||| manuel kaufmann ||| otmar hilliges ||| 
2021 ||| transformer with a mixture of gaussian keys. ||| tam nguyen ||| tan m. nguyen ||| dung le ||| khuong nguyen ||| anh tran ||| richard g. baraniuk ||| nhat ho ||| stanley j. osher ||| 
2017 ||| video question answering via attribute-augmented attention network learning. ||| yunan ye ||| zhou zhao ||| yimeng li ||| long chen ||| jun xiao ||| yueting zhuang ||| 
2022 ||| solving dynamic graph problems with multi-attention deep reinforcement learning. ||| udesh gunarathna ||| renata borovica-gajic ||| shanika karunasekera ||| egemen tanin ||| 
2020 ||| syntactically look-ahead attention network for sentence compression. ||| hidetaka kamigaito ||| manabu okumura ||| 
2021 ||| transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence. ||| kelvin lo ||| yuan jin ||| weicong tan ||| ming liu ||| lan du ||| wray l. buntine ||| 
2019 ||| image inpainting with learnable bidirectional attention maps. ||| chaohao xie ||| shaohui liu ||| chao li ||| ming-ming cheng ||| wangmeng zuo ||| xiao liu ||| shilei wen ||| errui ding ||| 
2020 ||| point transformer. ||| nico engel ||| vasileios belagiannis ||| klaus dietmayer ||| 
2021 ||| heat: holistic edge attention transformer for structured reconstruction. ||| jiacheng chen ||| yiming qian ||| yasutaka furukawa ||| 
2022 ||| hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism. ||| yunhui zeng ||| zijun liao ||| yuanzhi dai ||| rong wang ||| xiu li ||| bo yuan ||| 
2019 ||| convolutional quantum-like language model with mutual-attention for product rating prediction. ||| qing ping ||| chaomei chen ||| 
2021 ||| unidrop: a simple yet effective technique to improve transformer without extra cost. ||| zhen wu ||| lijun wu ||| qi meng ||| yingce xia ||| shufang xie ||| tao qin ||| xinyu dai ||| tie-yan liu ||| 
2020 ||| structured attention graphs for understanding deep image classifications. ||| vivswan shitole ||| fuxin li ||| minsuk kahng ||| prasad tadepalli ||| alan fern ||| 
2021 ||| linear algebra with transformers. ||| fran ||| ois charton ||| 
2022 ||| lgt-net: indoor panoramic room layout estimation with geometry-aware transformer network. ||| zhigang jiang ||| zhongzheng xiang ||| jinhua xu ||| ming zhao ||| 
2021 ||| word2pix: word to pixel cross attention transformer in visual grounding. ||| heng zhao ||| joey tianyi zhou ||| yew-soon ong ||| 
2021 ||| long-range transformers for dynamic spatiotemporal forecasting. ||| jake grigsby ||| zhe wang ||| yanjun qi ||| 
2019 ||| improving textual network embedding with global attention via optimal transport. ||| liqun chen ||| guoyin wang ||| chenyang tao ||| dinghan shen ||| pengyu cheng ||| xinyuan zhang ||| wenlin wang ||| yizhe zhang ||| lawrence carin ||| 
2022 ||| give me your attention: dot-product attention considered harmful for adversarial patch robustness. ||| giulio lovisotto ||| nicole finnie ||| mauricio munoz ||| chaithanya kumar mummadi ||| jan hendrik metzen ||| 
2022 ||| meta-attention for vit-backed continual learning. ||| mengqi xue ||| haofei zhang ||| jie song ||| mingli song ||| 
2021 ||| describing and localizing multiple changes with transformers. ||| yue qiu ||| shintaro yamamoto ||| kodai nakashima ||| ryota suzuki ||| kenji iwata ||| hirokatsu kataoka ||| yutaka satoh ||| 
2021 ||| end-to-end video object detection with spatial-temporal transformers. ||| lu he ||| qianyu zhou ||| xiangtai li ||| li niu ||| guangliang cheng ||| xiao li ||| wenxuan liu ||| yunhai tong ||| lizhuang ma ||| liqing zhang ||| 
2018 ||| skeleton transformer networks: 3d human pose and skinned mesh from single rgb image. ||| yusuke yoshiyasu ||| ryusuke sagawa ||| ko ayusawa ||| akihiko murai ||| 
2021 ||| attention-based multi-scale gated recurrent encoder with novel correlation loss for covid-19 progression prediction. ||| aishik konwer ||| joseph bae ||| gagandeep singh ||| rishabh gattu ||| syed ali ||| jeremy green ||| tej phatak ||| prateek prasanna ||| 
2019 ||| age progression and regression with spatial attention modules. ||| qi li ||| yunfan liu ||| zhenan sun ||| 
2022 ||| multimodal depression classification using articulatory coordination features and hierarchical attention based text embeddings. ||| nadee seneviratne ||| carol y. espy-wilson ||| 
2020 ||| task-adaptive feature transformer for few-shot segmentation. ||| jun seo ||| young-hyun park ||| sung whan yoon ||| jaekyun moon ||| 
2021 ||| attentionlite: towards efficient self-attention models for vision. ||| souvik kundu ||| sairam sundaresan ||| 
2020 ||| language modelling for source code with transformer-xl. ||| thomas dowdell ||| hongyu zhang ||| 
2021 ||| joint aec and beamforming with double-talk detection using rnn-transformer. ||| vinay kothapally ||| yong xu ||| meng yu ||| shi-xiong zhang ||| dong yu ||| 
2021 ||| discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer. ||| shivani kumar ||| anubhav shrimal ||| md. shad akhtar ||| tanmoy chakraborty ||| 
2021 ||| siamtrans: zero-shot multi-frame image restoration with pre-trained siamese transformers. ||| lin liu ||| shanxin yuan ||| jianzhuang liu ||| xin guo ||| youliang yan ||| qi tian ||| 
2021 ||| targeted aspect based multimodal sentiment analysis: an attention capsule extraction and multi-head fusion network. ||| jiaqian wang ||| donghong gu ||| chi yang ||| yun xue ||| zhengxin song ||| haoliang zhao ||| luwei xiao ||| 
2020 ||| end-to-end lane shape prediction with transformers. ||| ruijin liu ||| zejian yuan ||| tie liu ||| zhiliang xiong ||| 
2018 ||| an attention-based word-level interaction model: relation detection for knowledge base question answering. ||| hongzhi zhang ||| guandong xu ||| xiao liang ||| tinglei huang ||| kun fu ||| 
2021 ||| muslcat: multi-scale multi-level convolutional attention transformer for discriminative music modeling on raw waveforms. ||| kai middlebrook ||| shyam sudhakaran ||| david guy brizan ||| 
2020 ||| afn: attentional feedback network based 3d terrain super-resolution. ||| ashish kubade ||| diptiben patel ||| avinash sharma ||| k. s. rajan ||| 
2017 ||| vain: attentional multi-agent predictive modeling. ||| yedid hoshen ||| 
2021 ||| don't sweep your learning rate under the rug: a closer look at cross-modal transfer of pretrained transformers. ||| danielle rothermel ||| margaret li ||| tim rockt ||| schel ||| jakob n. foerster ||| 
2021 ||| transformer guided geometry model for flow-based unsupervised visual odometry. ||| xiangyu li ||| yonghong hou ||| pichao wang ||| zhimin gao ||| mingliang xu ||| wanqing li ||| 
2017 ||| multichannel attention network for analyzing visual behavior in public speaking. ||| rahul sharma ||| tanaya guha ||| gaurav sharma ||| 
2019 ||| relation-aware graph attention network for visual question answering. ||| linjie li ||| zhe gan ||| yu cheng ||| jingjing liu ||| 
2022 ||| do transformers encode a foundational ontology? probing abstract classes in natural language. ||| mael jullien ||| marco valentino ||| andr |||  freitas ||| 
2019 ||| sample efficient text summarization using a single pre-trained transformer. ||| urvashi khandelwal ||| kevin clark ||| dan jurafsky ||| lukasz kaiser ||| 
2021 ||| dual attention suppression attack: generate adversarial camouflage in physical world. ||| jiakai wang ||| aishan liu ||| zixin yin ||| shunchang liu ||| shiyu tang ||| xianglong liu ||| 
2021 ||| handwritten mathematical expression recognition via attention aggregation based bi-directional mutual learning. ||| xiaohang bian ||| bo qin ||| xiaozhe xin ||| jianwu li ||| xuefeng su ||| yanfeng wang ||| 
2021 ||| hi-behrt: hierarchical transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. ||| yikuan li ||| mohammad mamouei ||| gholamreza salimi khorshidi ||| shishir rao ||| abdelaali hassa ||| ne ||| dexter canoy ||| thomas lukasiewicz ||| kazem rahimi ||| 
2021 ||| detection of winding axial deformation in power transformers by uwb radar imaging. ||| mohammad s. golsorkhi ||| m. a. hejazi ||| g. b. gharepetian ||| razieh mosayebi ||| 
2020 ||| application of transformer impedance correction tables in power flow studies. ||| pooria dehghanian ||| ju hee yeo ||| jessica wert ||| hanyue li ||| komal s. shetye ||| thomas j. overbye ||| 
2019 ||| topic sensitive attention on generic corpora corrects sense bias in pretrained embeddings. ||| vihari piratla ||| sunita sarawagi ||| soumen chakrabarti ||| 
2020 ||| fastidious attention network for navel orange segmentation. ||| xiaoye sun ||| gongyan li ||| shaoyun xu ||| 
2021 ||| can attention enable mlps to catch up with cnns? ||| meng-hao guo ||| zheng-ning liu ||| tai-jiang mu ||| dun liang ||| ralph r. martin ||| shi-min hu ||| 
2018 ||| improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. ||| duy-kien nguyen ||| takayuki okatani ||| 
2019 ||| deep built-structure counting in satellite imagery using attention based re-weighting. ||| anza shakeel ||| waqas sultani ||| mohsen ali ||| 
2019 ||| region tracking in an image sequence: preventing driver inattention. ||| matthew kowal ||| gillian sandison ||| len yabuki-soh ||| raner la bastide ||| 
2021 ||| on the regularity of attention. ||| james vuckovic ||| aristide baratin ||| remi tachet des combes ||| 
2021 ||| stransgan: an empirical study on transformer in gans. ||| rui xu ||| xiangyu xu ||| kai chen ||| bolei zhou ||| chen change loy ||| 
2018 ||| end-to-end models with auditory attention in multi-channel keyword spotting. ||| haitong zhang ||| junbo zhang ||| yujun wang ||| 
2022 ||| gcn-transformer for short-term passenger flow prediction on holidays in urban rail transit systems. ||| shuxin zhang ||| jinlei zhang ||| lixing yang ||| ziyou gao ||| 
2021 ||| fine-grained visual classification of plant species in the wild: object detection as a reinforced means of attention. ||| matthew r. keaton ||| ram j. zaveri ||| meghana kovur ||| cole henderson ||| donald a. adjeroh ||| gianfranco doretto ||| 
2021 ||| multi-attention multiple instance learning. ||| andrei v. konstantinov ||| lev v. utkin ||| 
2019 ||| multi-stream network with temporal attention for environmental sound classification. ||| xinyu li ||| venkata chebiyyam ||| katrin kirchhoff ||| 
2020 ||| attention2angiogan: synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks. ||| sharif amit kamran ||| khondker fariha hossain ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| 
2022 ||| a high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network. ||| xiuyuan li ||| fengchao li ||| jiangang yu ||| guowen an ||| 
2022 ||| et-bert: a contextualized datagram representation with pre-training transformers for encrypted traffic classification. ||| xinjie lin ||| gang xiong ||| gaopeng gou ||| zhen li ||| junzheng shi ||| jing yu ||| 
2020 ||| beyond chemical 1d knowledge using transformers. ||| ruud van deursen ||| igor v. tetko ||| guillaume godin ||| 
2021 ||| plate: visually-grounded planning with transformers in procedural tasks. ||| jiankai sun ||| de-an huang ||| bo lu ||| yun-hui liu ||| bolei zhou ||| animesh garg ||| 
2021 ||| a cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. ||| ziwang fu ||| feng liu ||| hanyang wang ||| jiayin qi ||| xiangling fu ||| aimin zhou ||| zhibin li ||| 
2022 ||| protecting celebrities with identity consistency transformer. ||| xiaoyi dong ||| jianmin bao ||| dongdong chen ||| ting zhang ||| weiming zhang ||| nenghai yu ||| dong chen ||| fang wen ||| baining guo ||| 
2018 ||| describe and attend to track: learning natural language guided structural representation and visual attention for object tracking. ||| xiao wang ||| chenglong li ||| rui yang ||| tianzhu zhang ||| jin tang ||| bin luo ||| 
2021 ||| mist-net: multi-domain integrative swin transformer network for sparse-view ct reconstruction. ||| jiayi pan ||| weiwen wu ||| zhifan gao ||| heye zhang ||| 
2020 ||| neural retrieval for question answering with cross-attention supervised data augmentation. ||| yinfei yang ||| ning jin ||| kuo lin ||| mandy guo ||| daniel cer ||| 
2021 ||| tuber: tube-transformer for action detection. ||| jiaojiao zhao ||| xinyu li ||| chunhui liu ||| bing shuai ||| hao chen ||| cees g. m. snoek ||| joseph tighe ||| 
2021 ||| transformer language models with lstm-based cross-utterance information representation. ||| guangzhi sun ||| chao zhang ||| philip c. woodland ||| 
2022 ||| active phase-encode selection for slice-specific fast mr scanning using a transformer-based deep reinforcement learning framework. ||| yiming liu ||| yanwei pang ||| ruiqi jin ||| zhenchang wang ||| 
2022 ||| tts-gan: a transformer-based time-series generative adversarial network. ||| xiaomin li ||| vangelis metsis ||| huangyingrui wang ||| anne hee hiong ngu ||| 
2020 ||| empirical study of transformers for source code. ||| nadezhda chirkova ||| sergey troshin ||| 
2021 ||| anticipative video transformer. ||| rohit girdhar ||| kristen grauman ||| 
2021 ||| pocformer: a lightweight transformer architecture for detection of covid-19 using point of care ultrasound. ||| shehan perera ||| srikar adhikari ||| alper yilmaz ||| 
2020 ||| spotnet: self-attention multi-task network for object detection. ||| hughes perreault ||| guillaume-alexandre bilodeau ||| nicolas saunier ||| maguelonne h ||| ritier ||| 
2021 ||| 6d-vit: category-level 6d object pose estimation via transformer-based instance representation learning. ||| lu zou ||| zhangjin huang ||| naijie gu ||| guoping wang ||| 
2019 ||| an attention-based recurrent convolutional network for vehicle taillight recognition. ||| kuan-hui lee ||| takaaki tagawa ||| jia-en m. pan ||| adrien gaidon ||| bertrand douillard ||| 
2018 ||| document-level neural machine translation with hierarchical attention networks. ||| lesly miculicich werlen ||| dhananjay ram ||| nikolaos pappas ||| james henderson ||| 
2020 ||| attention based writer independent handwriting verification. ||| mohammad abuzar shaikh ||| tiehang duan ||| mihir chauhan ||| sargur n. srihari ||| 
2021 ||| reinforced attention for few-shot learning and beyond. ||| jie hong ||| pengfei fang ||| weihao li ||| tong zhang ||| christian simon ||| mehrtash harandi ||| lars petersson ||| 
2021 ||| exploring low-cost transformer model compression for large-scale commercial reply suggestions. ||| vaishnavi shrivastava ||| radhika gaonkar ||| shashank gupta ||| abhishek jha ||| 
2021 ||| label-attention transformer with geometrically coherent objects for image captioning. ||| shikha dubey ||| farrukh olimov ||| muhammad aasim rafique ||| joonmo kim ||| moongu jeon ||| 
2021 ||| lerna: transformer architectures for configuring error correction tools for short- and long-read genome sequencing. ||| atul sharma ||| pranjal jain ||| ashraf mahgoub ||| zihan zhou ||| kanak mahadik ||| somali chaterji ||| 
2020 ||| multi-step joint-modality attention network for scene-aware dialogue system. ||| yun-wei chu ||| kuan-yen lin ||| chao-chun hsu ||| lun-wei ku ||| 
2021 ||| discrete-continuous action space policy gradient-based attention for image-text matching. ||| shiyang yan ||| li yu ||| yuan xie ||| 
2020 ||| universal vector neural machine translation with effective attention. ||| satish mylapore ||| ryan quincy paul ||| joshua yi ||| robert d. slater ||| 
2020 ||| very deep transformers for neural machine translation. ||| xiaodong liu ||| kevin duh ||| liyuan liu ||| jianfeng gao ||| 
2020 ||| pyramid attention networks for image restoration. ||| yiqun mei ||| yuchen fan ||| yulun zhang ||| jiahui yu ||| yuqian zhou ||| ding liu ||| yun fu ||| thomas s. huang ||| humphrey shi ||| 
2021 ||| isegformer: interactive image segmentation with transformers. ||| qin liu ||| 
2020 ||| disease state prediction from single-cell data using graph attention networks. ||| neal g. ravindra ||| arijit sehanobish ||| jenna l. pappalardo ||| david a. hafler ||| david van dijk ||| 
2020 ||| interpreting attention models with human visual attention in machine reading comprehension. ||| ekta sood ||| simon tannert ||| diego frassinelli ||| andreas bulling ||| ngoc thang vu ||| 
2022 ||| compact bidirectional transformer for image captioning. ||| yuanen zhou ||| zhenzhen hu ||| daqing liu ||| huixia ben ||| meng wang ||| 
2019 ||| autodiscern: rating the quality of online health information with hierarchical encoder attention-based neural networks. ||| laura kinkead ||| ahmed allam ||| michael krauthammer ||| 
2020 ||| prediction diversity and selective attention in the wisdom of crowds. ||| davi a. nobre ||| jos |||  f. fontanari ||| 
2020 ||| towards transparent and explainable attention models. ||| akash kumar mohankumar ||| preksha nema ||| sharan narasimhan ||| mitesh m. khapra ||| balaji vasan srinivasan ||| balaraman ravindran ||| 
2022 ||| learning operators with coupled attention. ||| georgios kissas ||| jacob h. seidman ||| leonardo ferreira guilhoto ||| victor m. preciado ||| george j. pappas ||| paris perdikaris ||| 
2021 ||| the nlp cookbook: modern recipes for transformer based deep learning architectures. ||| sushant singh ||| ausif mahmood ||| 
2020 ||| ma-unet: an improved version of unet based on multi-scale and attention mechanism for medical image segmentation. ||| yutong cai ||| yong wang ||| 
2021 ||| cs60075_team2 at semeval-2021 task 1 : lexical complexity prediction using transformer-based language models pre-trained on various text corpora. ||| abhilash nandy ||| sayantan adak ||| tanurima halder ||| sai mahesh pokala ||| 
2021 ||| embedding calibration for music semantic similarity using auto-regressive transformer. ||| xinran zhang ||| maosong sun ||| jiafeng liu ||| xiaobing li ||| 
2020 ||| improving audio anomalies recognition using temporal convolutional attention network. ||| qiang huang ||| thomas hain ||| 
2021 ||| robust gps-vision localization via integrity-driven landmark attention. ||| sriramya bhamidipati ||| grace xingxin gao ||| 
2021 ||| dynamics of cross-platform attention to retracted papers: pervasiveness, audience skepticism, and timing of retractions. ||| hao peng ||| daniel m. romero ||| emoke- ||| gnes horv ||| t ||| 
2017 ||| learning social image embedding with deep multimodal attention networks. ||| feiran huang ||| xiaoming zhang ||| zhoujun li ||| tao mei ||| yueying he ||| zhonghua zhao ||| 
2017 ||| multi-focus attention network for efficient deep reinforcement learning. ||| jinyoung choi ||| beom-jin lee ||| byoung-tak zhang ||| 
2021 ||| traisformer-a generative transformer for ais trajectory prediction. ||| duong nguyen ||| ronan fablet ||| 
2020 ||| fetal ecg extraction from maternal ecg using attention-based asymmetric cyclegan. ||| mohammadreza mohebbian ||| seyed shahim vedaei ||| khan a. wahid ||| anh dinh ||| hamid reza marateb ||| 
2022 ||| vinter: image narrative generation with emotion-arc-aware transformer. ||| kohei uehara ||| yusuke mori ||| yusuke mukuta ||| tatsuya harada ||| 
2021 ||| picaso: permutation-invariant cascaded attentional set operator. ||| samira zare ||| hien van nguyen ||| 
2018 ||| parameter-free spatial attention network for person re-identification. ||| haoran wang ||| yue fan ||| zexin wang ||| licheng jiao ||| bernt schiele ||| 
2022 ||| deformable vistr: spatio temporal deformable attention for video instance segmentation. ||| sudhir yarram ||| jialian wu ||| pan ji ||| yi xu ||| junsong yuan ||| 
2019 ||| understanding attention in graph neural networks. ||| boris knyazev ||| graham w. taylor ||| mohamed r. amer ||| 
2018 ||| larnn: linear attention recurrent neural network. ||| guillaume chevalier ||| 
2020 ||| spatio-temporal relation and attention learning for facial action unit detection. ||| zhiwen shao ||| lixin zou ||| jianfei cai ||| yunsheng wu ||| lizhuang ma ||| 
2020 ||| multi-label image recognition with multi-class attentional regions. ||| bin-bin gao ||| hong-yu zhou ||| 
2020 ||| planning on the fast lane: learning to interact using attention mechanisms in path integral inverse reinforcement learning. ||| sascha rosbach ||| xing li ||| simon gro ||| johann ||| silviu homoceanu ||| stefan roth ||| 
2022 ||| arm3d: attention-based relation module for indoor 3d object detection. ||| yuqing lan ||| yao duan ||| chenyi liu ||| chenyang zhu ||| yueshan xiong ||| hui huang ||| kai xu ||| 
2019 ||| component attention guided face super-resolution network: cagface. ||| ratheesh kalarot ||| tao li ||| fatih porikli ||| 
2021 ||| lambdanetworks: modeling long-range interactions without attention. ||| irwan bello ||| 
2022 ||| masked transformer for neighhourhood-aware click-through rate prediction. ||| erxue min ||| yu rong ||| tingyang xu ||| yatao bian ||| peilin zhao ||| junzhou huang ||| da luo ||| kangyi lin ||| sophia ananiadou ||| 
2021 ||| changing the mind of transformers for topically-controllable language generation. ||| haw-shiuan chang ||| jiaming yuan ||| mohit iyyer ||| andrew mccallum ||| 
2021 ||| nam: normalization-based attention module. ||| yichao liu ||| zongru shao ||| yueyang teng ||| nico hoffmann ||| 
2020 ||| computational efficient deep neural network with difference attention maps for facial action unit detection. ||| jing chen ||| chenhui wang ||| kejun wang ||| meichen liu ||| 
2021 ||| attdlnet: attention-based dl network for 3d lidar place recognition. ||| tiago barros ||| lu ||| s garrote ||| ricardo pereira ||| cristiano premebida ||| urbano j. nunes ||| 
2021 ||| iot: instance-wise layer reordering for transformer structures. ||| jinhua zhu ||| lijun wu ||| yingce xia ||| shufang xie ||| tao qin ||| wengang zhou ||| houqiang li ||| tie-yan liu ||| 
2021 ||| visual-semantic transformer for scene text recognition. ||| xin tang ||| yongquan lai ||| ying liu ||| yuanyuan fu ||| rui fang ||| 
2021 ||| sstvos: sparse spatiotemporal transformers for video object segmentation. ||| brendan duke ||| abdalla ahmed ||| christian wolf ||| parham aarabi ||| graham w. taylor ||| 
2019 ||| basn - learning steganography with binary attention mechanism. ||| yang yang ||| 
2022 ||| patch similarity aware data-free quantization for vision transformers. ||| zhikai li ||| liping ma ||| mengjuan chen ||| junrui xiao ||| qingyi gu ||| 
2021 ||| diagonal attention and style-based gan for content-style disentanglement in image generation and translation. ||| gihyun kwon ||| jong chul ye ||| 
2021 ||| anchor-free 3d single stage detector with mask-guided attention for point cloud. ||| jiale li ||| hang dai ||| ling shao ||| yong ding ||| 
2021 ||| medical code prediction from discharge summary: document to sequence bert using sequence attention. ||| tak-sung heo ||| yongmin yoo ||| yeongjoon park ||| byeong-cheol jo ||| 
2021 ||| transcmd: cross-modal decoder equipped with transformer for rgb-d salient object detection. ||| youwei pang ||| xiaoqi zhao ||| lihe zhang ||| huchuan lu ||| 
2020 ||| training transformers for information security tasks: a case study on malicious url prediction. ||| ethan m. rudd ||| ahmed abdallah ||| 
2019 ||| how does bert answer questions? a layer-wise analysis of transformer representations. ||| betty van aken ||| benjamin winter ||| alexander l ||| ser ||| felix a. gers ||| 
2021 ||| hotr: end-to-end human-object interaction detection with transformers. ||| bumsoo kim ||| junhyun lee ||| jaewoo kang ||| eun-sol kim ||| hyunwoo j. kim ||| 
2018 ||| hierarchical attention: what really counts in various nlp tasks. ||| zehao dou ||| zhihua zhang ||| 
2021 ||| calliope - a polyphonic music transformer. ||| andrea valenti ||| stefano berti ||| davide bacciu ||| 
2021 ||| shift-and-balance attention. ||| chunjie luo ||| jianfeng zhan ||| tianshu hao ||| lei wang ||| wanling gao ||| 
2020 ||| sequence-to-sequence learning via attention transfer for incremental speech recognition. ||| sashi novitasari ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| an automatic detection method of cerebral aneurysms in time-of-flight magnetic resonance angiography images based on attention 3d u-net. ||| chen geng ||| meng chen ||| ruoyu di ||| dongdong wang ||| liqin yang ||| wei xia ||| yuxin li ||| daoying geng ||| 
2021 ||| deep visual attention-based transfer clustering. ||| akshaykumar gunari ||| shashidhar veerappa kudari ||| sukanya nadagadalli ||| keerthi goudnaik ||| ramesh ashok tabib ||| uma mudenagudi ||| adarsh jamadandi ||| 
2021 ||| beamtransformer: microphone array-based overlapping speech detection. ||| siqi zheng ||| shiliang zhang ||| weilong huang ||| qian chen ||| hongbin suo ||| ming lei ||| jinwei feng ||| zhijie yan ||| 
2017 ||| attention-based extraction of structured information from street view imagery. ||| zbigniew wojna ||| alexander n. gorban ||| dar-shyang lee ||| kevin murphy ||| qian yu ||| yeqing li ||| julian ibarz ||| 
2019 ||| convolutional self-attention networks. ||| baosong yang ||| longyue wang ||| derek f. wong ||| lidia s. chao ||| zhaopeng tu ||| 
2018 ||| mri reconstruction via cascaded channel-wise attention network. ||| qiaoying huang ||| dong yang ||| pengxiang wu ||| hui qu ||| jingru yi ||| dimitris n. metaxas ||| 
2018 ||| attention, please! adversarial defense via attention rectification and preservation. ||| shangxi wu ||| jitao sang ||| kaiyuan xu ||| jiaming zhang ||| yanfeng sun ||| liping jing ||| jian yu ||| 
2020 ||| transformer-transducers for code-switched speech recognition. ||| siddharth dalmia ||| yuzong liu ||| srikanth ronanki ||| katrin kirchhoff ||| 
2019 ||| sharing attention weights for fast transformer. ||| tong xiao ||| yinqiao li ||| jingbo zhu ||| zhengtao yu ||| tongran liu ||| 
2021 ||| audiomer: a convolutional transformer for keyword spotting. ||| surya kant sahu ||| sai mitheran ||| juhi kamdar ||| meet gandhi ||| 
2020 ||| pretrained transformers improve out-of-distribution robustness. ||| dan hendrycks ||| xiaoyuan liu ||| eric wallace ||| adam dziedzic ||| rishabh krishnan ||| dawn song ||| 
2021 ||| oh-former: omni-relational high-order transformer for person re-identification. ||| xianing chen ||| jialang xu ||| jiale xu ||| shenghua gao ||| 
2021 ||| updet: universal multi-agent reinforcement learning via policy decoupling with transformers. ||| siyi hu ||| fengda zhu ||| xiaojun chang ||| xiaodan liang ||| 
2021 ||| transformer-encoder-gru (t-e-gru) for chinese sentiment analysis on chinese comment text. ||| binlong zhang ||| wei zhou ||| 
2020 ||| wavetransformer: a novel architecture for audio captioning based on learning temporal and time-frequency information. ||| an tran ||| konstantinos drossos ||| tuomas virtanen ||| 
2021 ||| tracer: extreme attention guided salient object tracing network. ||| min seok lee ||| wooseok shin ||| sung won han ||| 
2020 ||| robust speaker recognition using speech enhancement and attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2022 ||| predicting physics in mesh-reduced space with temporal attention. ||| xu han ||| han gao ||| tobias pffaf ||| jian-xun wang ||| li-ping liu ||| 
2020 ||| edinburghnlp at wnut-2020 task 2: leveraging transformers with generalized augmentation for identifying informativeness in covid-19 tweets. ||| nickil maveli ||| 
2020 ||| video super-resolution with temporal group attention. ||| takashi isobe ||| songjiang li ||| xu jia ||| shanxin yuan ||| gregory g. slabaugh ||| chunjing xu ||| ya-li li ||| shengjin wang ||| qi tian ||| 
2021 ||| explaining the attention mechanism of end-to-end speech recognition using decision trees. ||| yuanchao wang ||| wenji du ||| chenghao cai ||| yanyan xu ||| 
2021 ||| ab-mapper: attention and bicnet based multi-agent path finding for dynamic crowded environment. ||| huifeng guan ||| yuan gao ||| min zhao ||| yong yang ||| fuqin deng ||| tin lun lam ||| 
2022 ||| claret: pre-training a correlation-aware context-to-event transformer for event-centric generation and classification. ||| yucheng zhou ||| tao shen ||| xiubo geng ||| guodong long ||| daxin jiang ||| 
2018 ||| detail preserving depth estimation from a single image using attention guided networks. ||| zhixiang hao ||| yu li ||| shaodi you ||| feng lu ||| 
2021 ||| learned token pruning for transformers. ||| sehoon kim ||| sheng shen ||| david thorsley ||| amir gholami ||| joseph hassoun ||| kurt keutzer ||| 
2019 ||| attention deep model with multi-scale deep supervision for person re-identification. ||| di wu ||| chao wang ||| yong wu ||| de-shuang huang ||| 
2019 ||| stgrat: a spatio-temporal graph attention network for traffic forecasting. ||| cheonbok park ||| chunggi lee ||| hyojin bahng ||| taeyun won ||| kihwan kim ||| seungmin jin ||| sungahn ko ||| jaegul choo ||| 
2019 ||| attention convolutional binary neural tree for fine-grained visual categorization. ||| ruyi ji ||| longyin wen ||| libo zhang ||| dawei du ||| yanjun wu ||| chen zhao ||| xianglong liu ||| feiyue huang ||| 
2019 ||| mvp-net: multi-view fpn with position-aware attention for deep universal lesion detection. ||| zihao li ||| shu zhang ||| junge zhang ||| kaiqi huang ||| yizhou wang ||| yizhou yu ||| 
2020 ||| robustness verification for transformers. ||| zhouxing shi ||| huan zhang ||| kai-wei chang ||| minlie huang ||| cho-jui hsieh ||| 
2018 ||| attentional multi-reading sarcasm detection. ||| reza ghaeini ||| xiaoli z. fern ||| prasad tadepalli ||| 
2019 ||| not all attention is needed: gated attention network for sequence data. ||| lanqing xue ||| xiaopeng li ||| nevin l. zhang ||| 
2020 ||| adapterhub: a framework for adapting transformers. ||| jonas pfeiffer ||| andreas r ||| ckl ||| clifton poth ||| aishwarya kamath ||| ivan vulic ||| sebastian ruder ||| kyunghyun cho ||| iryna gurevych ||| 
2020 ||| history repeats itself: human motion prediction via motion attention. ||| wei mao ||| miaomiao liu ||| mathieu salzmann ||| 
2021 ||| tfpose: direct human pose estimation with transformers. ||| weian mao ||| yongtao ge ||| chunhua shen ||| zhi tian ||| xinlong wang ||| zhibin wang ||| 
2020 ||| academic performance estimation with attention-based graph convolutional networks. ||| qian hu ||| huzefa rangwala ||| 
2022 ||| design optimization of a three-phase transformer using finite element analysis. ||| ahmet furkan hacan ||| bilal kabas ||| samet oguten ||| 
2021 ||| spvit: enabling faster vision transformers via soft token pruning. ||| zhenglun kong ||| peiyan dong ||| xiaolong ma ||| xin meng ||| wei niu ||| mengshu sun ||| bin ren ||| minghai qin ||| hao tang ||| yanzhi wang ||| 
2020 ||| how the world's collective attention is being paid to a pandemic: covid-19 related 1-gram time series for 24 languages on twitter. ||| thayer alshaabi ||| joshua r. minot ||| michael v. arnold ||| jane lydia adams ||| david rushing dewhurst ||| andrew j. reagan ||| r. muhamad ||| christopher m. danforth ||| peter sheridan dodds ||| 
2021 ||| pre-trained transformer-based approach for arabic question answering : a comparative study. ||| kholoud alsubhi ||| amani t. jamal ||| areej alhothali ||| 
2022 ||| multi-modal multi-label facial action unit detection with transformer. ||| lingfeng wang ||| shisen wang ||| jin qi ||| 
2021 ||| t3-vis: a visual analytic framework for training and fine-tuning transformers in nlp. ||| raymond li ||| wen xiao ||| lanjun wang ||| hyeju jang ||| giuseppe carenini ||| 
2020 ||| energy transmission control for a grid-connected modern power system non-linear loads with a series multi-stage transformer voltage reinjection with controlled converters. ||| appalabathula venkatesh ||| shankar nalinakshan ||| s. s. kiran ||| pradeepa h ||| 
2020 ||| distributed control of charging for electric vehicle fleets under dynamic transformer ratings. ||| micah botkin-levy ||| alexander engelmann ||| tillmann m ||| hlpfordt ||| timm faulwasser ||| mads r. almassalkhi ||| 
2021 ||| iacn: influence-aware and attention-based co-evolutionary network for recommendation. ||| shalini pandey ||| george karypis ||| jaideep srivastava ||| 
2019 ||| an analysis of deep neural networks with attention for action recognition from a neurophysiological perspective. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2019 ||| conversion prediction using multi-task conditional attention networks to support the creation of effective ad creative. ||| shunsuke kitada ||| hitoshi iyatomi ||| yoshifumi seki ||| 
2020 ||| character-level translation with self-attention. ||| yingqiang gao ||| nikola i. nikolov ||| yuhuang hu ||| richard h. r. hahnloser ||| 
2020 ||| finding experts in transformer models. ||| xavier suau ||| luca zappella ||| nicholas apostoloff ||| 
2021 ||| on the interplay between fine-tuning and composition in transformers. ||| lang yu ||| allyson ettinger ||| 
2022 ||| auto-scaling vision transformers without training. ||| wuyang chen ||| wei huang ||| xianzhi du ||| xiaodan song ||| zhangyang wang ||| denny zhou ||| 
2021 ||| refiner: refining self-attention for vision transformers. ||| daquan zhou ||| yujun shi ||| bingyi kang ||| weihao yu ||| zihang jiang ||| yuan li ||| xiaojie jin ||| qibin hou ||| jiashi feng ||| 
2021 ||| the transformer network for the traveling salesman problem. ||| xavier bresson ||| thomas laurent ||| 
2021 ||| msht: multi-stage hybrid transformer for the rose image analysis of pancreatic cancer. ||| tianyi zhang ||| yunlu feng ||| yu zhao ||| guangda fan ||| aiming yang ||| shangqin lyu ||| peng zhang ||| fan song ||| chenbin ma ||| yangyang sun ||| youdan feng ||| guanglei zhang ||| 
2020 ||| spatial context-aware self-attention model for multi-organ segmentation. ||| hao tang ||| xingwei liu ||| kun han ||| shanlin sun ||| narisu bai ||| xuming chen ||| qian huang ||| yong liu ||| xiaohui xie ||| 
2022 ||| pedestrian detection: domain generalization, cnns, transformers and beyond. ||| irtiza hasan ||| shengcai liao ||| jinpeng li ||| saad ullah akram ||| ling shao ||| 
2021 ||| semantic reinforced attention learning for visual place recognition. ||| guohao peng ||| yufeng yue ||| jun zhang ||| zhenyu wu ||| xiaoyu tang ||| danwei wang ||| 
2018 ||| deep attention-guided fusion network for lesion segmentation. ||| hengliang zhu ||| yangyang hao ||| lizhuang ma ||| ruixing li ||| hua wang ||| 
2021 ||| methodology and feasibility of neurofeedback to improve visual attention to letters in mild alzheimer's disease. ||| deirdre mclaughlin ||| daniel klee ||| tab memmott ||| betts peters ||| jack wiedrick ||| melanie fried-oken ||| barry oken ||| 
2022 ||| disentangled latent transformer for interpretable monocular height estimation. ||| zhitong xiong ||| sining chen ||| yilei shi ||| xiao xiang zhu ||| 
2022 ||| equivariant graph attention networks for molecular property prediction. ||| tuan le ||| frank no ||| djork-arn |||  clevert ||| 
2020 ||| u2-onet: a two-level nested octave u-structure with multiscale attention mechanism for moving instances segmentation. ||| chenjie wang ||| chengyuan li ||| bin luo ||| 
2021 ||| stable, fast and accurate: kernelized attention with relative positional encoding. ||| shengjie luo ||| shanda li ||| tianle cai ||| di he ||| dinglan peng ||| shuxin zheng ||| guolin ke ||| liwei wang ||| tie-yan liu ||| 
2021 ||| trig: transformer-based text recognizer with initial embedding guidance. ||| yue tao ||| zhiwei jia ||| runze ma ||| shugong xu ||| 
2019 ||| neural related work summarization with a joint context-driven attention mechanism. ||| yongzhen wang ||| xiaozhong liu ||| zheng gao ||| 
2021 ||| -former: infinite memory transformer. ||| pedro henrique martins ||| zita marinho ||| andr |||  f. t. martins ||| 
2020 ||| question directed graph attention network for numerical reasoning over text. ||| kunlong chen ||| weidi xu ||| xingyi cheng ||| zou xiaochuan ||| yuyu zhang ||| le song ||| taifeng wang ||| yuan qi ||| wei chu ||| 
2021 ||| show why the answer is correct! towards explainable ai using compositional temporal attention. ||| nihar bendre ||| kevin desai ||| peyman najafirad ||| 
2021 ||| agmi: attention-guided multi-omics integration for drug response prediction with graph neural networks. ||| ruiwei feng ||| yufeng xie ||| minshan lai ||| danny z. chen ||| ji cao ||| jian wu ||| 
2021 ||| kdexplainer: a task-oriented attention model for explaining knowledge distillation. ||| mengqi xue ||| jie song ||| xinchao wang ||| ying chen ||| xingen wang ||| mingli song ||| 
2020 ||| evaluating the role of language typology in transformer-based multilingual text classification. ||| sophie groenwold ||| samhita honnavalli ||| lily ou ||| aesha parekh ||| sharon levy ||| diba mirza ||| william yang wang ||| 
2021 ||| transrppg: remote photoplethysmography transformer for 3d mask face presentation attack detection. ||| zitong yu ||| xiaobai li ||| pichao wang ||| guoying zhao ||| 
2017 ||| joint modeling of event sequence and time series with attentional twin recurrent neural networks. ||| shuai xiao ||| junchi yan ||| mehrdad farajtabar ||| le song ||| xiaokang yang ||| hongyuan zha ||| 
2020 ||| improving auto-encoder novelty detection using channel attention and entropy minimization. ||| dongyan guo ||| miao tian ||| ying cui ||| xiang pan ||| shengyong chen ||| 
2020 ||| xlm-t: scaling up multilingual machine translation with pretrained cross-lingual transformer encoders. ||| shuming ma ||| jian yang ||| haoyang huang ||| zewen chi ||| li dong ||| dongdong zhang ||| hany hassan awadalla ||| alexandre muzio ||| akiko eriguchi ||| saksham singhal ||| xia song ||| arul menezes ||| furu wei ||| 
2021 ||| enhancing transformer for video understanding using gated multi-level attention and temporal adversarial training. ||| saurabh sahu ||| palash goyal ||| 
2019 ||| satellite image time series classification with pixel-set encoders and temporal self-attention. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| s ||| bastien giordano ||| nesrine chehata ||| 
2018 ||| deep imbalanced attribute classification using visual attention aggregation. ||| nikolaos sarafianos ||| xiang xu ||| ioannis a. kakadiaris ||| 
2020 ||| streaming automatic speech recognition with the transformer model. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| sbat: video captioning with sparse boundary-aware transformer. ||| tao jin ||| siyu huang ||| ming chen ||| yingming li ||| zhongfei zhang ||| 
2018 ||| pay attention! - robustifying a deep visuomotor policy through task-focused attention. ||| pooya abolghasemi ||| amir mazaheri ||| mubarak shah ||| ladislau b ||| l ||| ni ||| 
2020 ||| mlnet: an adaptive multiple receptive-field attention neural network for voice activity detection. ||| zhenpeng zheng ||| jianzong wang ||| ning cheng ||| jian luo ||| jing xiao ||| 
2021 ||| fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. ||| sumon biswas ||| hridesh rajan ||| 
2021 ||| code structure guided transformer for source code summarization. ||| shuzheng gao ||| cuiyun gao ||| yulan he ||| jichuan zeng ||| lun yiu nie ||| xin xia ||| 
2018 ||| weakly supervised one-shot detection with attention siamese networks. ||| gil keren ||| maximilian schmitt ||| thomas kehrenberg ||| bj ||| rn w. schuller ||| 
2021 ||| on the adversarial robustness of visual transformers. ||| rulin shao ||| zhouxing shi ||| jinfeng yi ||| pin-yu chen ||| cho-jui hsieh ||| 
2020 ||| vocoder-free end-to-end voice conversion with transformer network. ||| june-woo kim ||| ho-young jung ||| minho lee ||| 
2022 ||| v2x-vit: vehicle-to-everything cooperative perception with vision transformer. ||| runsheng xu ||| hao xiang ||| zhengzhong tu ||| xin xia ||| ming-hsuan yang ||| jiaqi ma ||| 
2019 ||| scale-aware attention network for crowd counting. ||| rahul rama varior ||| bing shuai ||| joe tighe ||| davide modolo ||| 
2021 ||| key-sparse transformer with cascaded cross-attention block for multimodal speech emotion recognition. ||| weidong chen ||| xiaofeng xing ||| xiangmin xu ||| jichen yang ||| jianxin pang ||| 
2020 ||| sac: accelerating and structuring self-attention via sparse adaptive connection. ||| xiaoya li ||| yuxian meng ||| qinghong han ||| fei wu ||| jiwei li ||| 
2022 ||| ct-sat: contextual transformer for sequential audio tagging. ||| yuanbo hou ||| zhaoyi liu ||| bo kang ||| yun wang ||| dick botteldooren ||| 
2020 ||| microscopic fine-grained instance classification through deep attention. ||| mengran fan ||| tapabrata chakraborti ||| eric i-chao chang ||| yan xu ||| jens rittscher ||| 
2019 ||| self-attention network for skeleton-based human action recognition. ||| sangwoo cho ||| muhammad hasan maqbool ||| fei liu ||| hassan foroosh ||| 
2019 ||| crowd transformer network. ||| viresh ranjan ||| mubarak shah ||| minh hoai nguyen ||| 
2020 ||| residual channel attention generative adversarial network for image super-resolution and noise reduction. ||| jie cai ||| zibo meng ||| chiu man ho ||| 
2019 ||| auto-sizing the transformer network: improving speed, efficiency, and performance for low-resource machine translation. ||| kenton murray ||| jeffery kinnison ||| toan q. nguyen ||| walter j. scheirer ||| david chiang ||| 
2021 ||| a note on learning rare events in molecular dynamics using lstm and transformer. ||| wenqi zeng ||| siqin cao ||| xuhui huang ||| yuan yao ||| 
2021 ||| multi-view stereo with transformer. ||| jie zhu ||| bo peng ||| wanqing li ||| haifeng shen ||| zhe zhang ||| jianjun lei ||| 
2021 ||| next generation multitarget trackers: random finite set methods vs transformer-based deep learning. ||| juliano pinto ||| georg hess ||| william ljungbergh ||| yuxuan xia ||| lennart svensson ||| henk wymeersch ||| 
2021 ||| video instance segmentation using inter-frame communication transformers. ||| sukjun hwang ||| miran heo ||| seoung wug oh ||| seon joo kim ||| 
2020 ||| cafe-gan: arbitrary face attribute editing with complementary attention feature. ||| jeong-gi kwak ||| david k. han ||| hanseok ko ||| 
2020 ||| saadb: a self-attention guided adb network for person re-identification. ||| bo jiang ||| sheng wang ||| xiao wang ||| aihua zheng ||| 
2021 ||| frequency-temporal attention network for singing melody extraction. ||| shuai yu ||| xiaoheng sun ||| yi yu ||| wei li ||| 
2018 ||| i can see your aim: estimating user attention from gaze for handheld robot collaboration. ||| janis stolzenwald ||| walterio w. mayol-cuevas ||| 
2019 ||| small object detection using context and attention. ||| jeong-seon lim ||| marcella astrid ||| hyun-jin yoon ||| seung-ik lee ||| 
2022 ||| codedvtr: codebook-based sparse voxel transformer with geometric guidance. ||| tianchen zhao ||| niansong zhang ||| xuefei ning ||| he wang ||| li yi ||| yu wang ||| 
2020 ||| visual transformers: token-based image representation and processing for computer vision. ||| bichen wu ||| chenfeng xu ||| xiaoliang dai ||| alvin wan ||| peizhao zhang ||| masayoshi tomizuka ||| kurt keutzer ||| peter vajda ||| 
2022 ||| s3t: self-supervised pre-training with swin transformer for music classification. ||| hang zhao ||| chen zhang ||| belei zhu ||| zejun ma ||| kejun zhang ||| 
2018 ||| multi-scale attention with dense encoder for handwritten mathematical expression recognition. ||| jianshu zhang ||| jun du ||| lirong dai ||| 
2018 ||| amnet: memorability estimation with attention. ||| jiri fajtl ||| vasileios argyriou ||| dorothy monekosso ||| paolo remagnino ||| 
2021 ||| retrieval-augmented transformer-xl for close-domain dialog generation. ||| giovanni bonetta ||| rossella cancelliere ||| ding liu ||| paul vozila ||| 
2021 ||| efficient conformer with prob-sparse attention mechanism for end-to-endspeech recognition. ||| xiong wang ||| sining sun ||| lei xie ||| long ma ||| 
2022 ||| structure-aware transformer for graph representation learning. ||| dexiong chen ||| leslie o'bray ||| karsten m. borgwardt ||| 
2021 ||| vision transformer based video hashing retrieval for tracing the source of fake videos. ||| pengfei pei ||| xianfeng zhao ||| jinchuan li ||| yun cao ||| xiaowei yi ||| 
2021 ||| cae-transformer: transformer-based model to predict invasiveness of lung adenocarcinoma subsolid nodules from non-thin section 3d ct scans. ||| shahin heidarian ||| parnian afshar ||| anastasia oikonomou ||| konstantinos n. plataniotis ||| arash mohammadi ||| 
2021 ||| channel-based attention for lcc using sentinel-2 time series. ||| hermann courteille ||| a. beno ||| t ||| nicolas m ||| ger ||| abdourrahmane m. atto ||| dino ienco ||| 
2019 ||| spatiotemporal attention networks for wind power forecasting. ||| xingbo fu ||| feng gao ||| jiang wu ||| xinyu wei ||| fangwei duan ||| 
2021 ||| enriching transformers with structured tensor-product representations for abstractive summarization. ||| yichen jiang ||| asli celikyilmaz ||| paul smolensky ||| paul soulos ||| sudha rao ||| hamid palangi ||| roland fernandez ||| caitlin smith ||| mohit bansal ||| jianfeng gao ||| 
2020 ||| attention-based saliency hashing for ophthalmic image retrieval. ||| jiansheng fang ||| yanwu xu ||| xiaoqing zhang ||| yan hu ||| jiang liu ||| 
2020 ||| neural architecture search with reinforce and masked attention autoregressive density estimators. ||| chepuri shri krishna ||| ashish gupta ||| himanshu rai ||| swarnim narayan ||| 
2021 ||| cubetr: learning to solve the rubiks cube using transformers. ||| mustafa ebrahim chasmai ||| 
2021 ||| vision transformer for learning driving policies in complex multi-agent environments. ||| eshagh kargar ||| ville kyrki ||| 
2019 ||| attention guided metal artifact correction in mri using deep neural networks. ||| jee won kim ||| kinam kwon ||| byungjai kim ||| hyunwook park ||| 
2021 ||| fp-age: leveraging face parsing attention for facial age estimation in the wild. ||| yiming lin ||| jie shen ||| yujiang wang ||| maja pantic ||| 
2021 ||| eeg-based classification of drivers attention using convolutional neural network. ||| fred atilla ||| maryam alimardani ||| 
2019 ||| reinforcement learning with attention that works: a self-supervised approach. ||| anthony manchin ||| ehsan abbasnejad ||| anton van den hengel ||| 
2021 ||| pruning attention heads of transformer models using a* search: a novel approach to compress big nlp architectures. ||| archit parnami ||| rahul singh ||| tarun joshi ||| 
2020 ||| brain atlas guided attention u-net for white matter hyperintensity segmentation. ||| zicong zhang ||| kimerly a. powell ||| changchang yin ||| shilei cao ||| dani gonzalez ||| yousef hannawi ||| ping zhang ||| 
2020 ||| multi-task temporal shift attention networks for on-device contactless vitals measurement. ||| xin liu ||| josh fromm ||| shwetak n. patel ||| daniel j. mcduff ||| 
2020 ||| attention augmented convlstm forenvironment prediction. ||| bernard lange ||| masha itkina ||| mykel j. kochenderfer ||| 
2022 ||| a novel perspective to look at attention: bi-level attention-based explainable topic modeling for news classification. ||| dairui liu ||| derek greene ||| ruihai dong ||| 
2019 ||| multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation. ||| hao tang ||| dan xu ||| nicu sebe ||| yanzhi wang ||| jason j. corso ||| yan yan ||| 
2021 ||| ernie-tiny : a progressive distillation framework for pretrained transformer compression. ||| weiyue su ||| xuyi chen ||| shikun feng ||| jiaxiang liu ||| weixin liu ||| yu sun ||| hao tian ||| hua wu ||| haifeng wang ||| 
2020 ||| person image generation with semantic attention network for person re-identification. ||| meichen liu ||| kejun wang ||| juihang ji ||| shuzhi sam ge ||| 
2019 ||| dialogue act classification with context-aware self-attention. ||| vipul raheja ||| joel r. tetreault ||| 
2019 ||| big mood: relating transformers to explicit commonsense knowledge. ||| jeff da ||| 
2020 ||| toward transformer-based object detection. ||| josh beal ||| eric kim ||| eric tzeng ||| dong huk park ||| andrew zhai ||| dmitry kislyuk ||| 
2020 ||| deformable detr: deformable transformers for end-to-end object detection. ||| xizhou zhu ||| weijie su ||| lewei lu ||| bin li ||| xiaogang wang ||| jifeng dai ||| 
2020 ||| paying more attention to snapshots of iterative pruning: improving model compression via ensemble distillation. ||| duong h. le ||| vo trung nhan ||| nam thoai ||| 
2021 ||| exploring text-to-text transformers for english to hinglish machine translation with synthetic code-mixing. ||| ganesh jawahar ||| el moatez billah nagoudi ||| muhammad abdul-mageed ||| laks v. s. lakshmanan ||| 
2021 ||| going full-tilt boogie on document understanding with text-image-layout transformer. ||| rafal powalski ||| lukasz borchmann ||| dawid jurkiewicz ||| tomasz dwojak ||| michal pietruszka ||| gabriela palka ||| 
2021 ||| causal transformers perform below chance on recursive nested constructions, unlike humans. ||| yair lakretz ||| theo desbordes ||| dieuwke hupkes ||| stanislas dehaene ||| 
2020 ||| knowledge-aware attention network for protein-protein interaction extraction. ||| huiwei zhou ||| zhuang liu ||| shixian ning ||| chengkun lang ||| yingyu lin ||| lei du ||| 
2017 ||| attention based convolutional neural network for predicting rna-protein binding sites. ||| xiaoyong pan ||| junchi yan ||| 
2018 ||| mist: multiple instance spatial transformer network. ||| baptiste angles ||| shahram izadi ||| andrea tagliasacchi ||| kwang moo yi ||| 
2021 ||| is image size important? a robustness comparison of deep learning methods for multi-scale cell image classification tasks: from convolutional neural networks to visual transformers. ||| wanli liu ||| chen li |||  ||| hongzan sun ||| weiming hu ||| haoyuan chen ||| changhao sun ||| marcin grzegorzek ||| 
2021 ||| emotion classification in a resource constrained language using transformer-based approach. ||| avishek das ||| omar sharif ||| mohammed moshiul hoque ||| iqbal h. sarker ||| 
2021 ||| faceformer: speech-driven 3d facial animation with transformers. ||| yingruo fan ||| zhaojiang lin ||| jun saito ||| wenping wang ||| taku komura ||| 
2018 ||| latent alignment and variational attention. ||| yuntian deng ||| yoon kim ||| justin t. chiu ||| demi guo ||| alexander m. rush ||| 
2020 ||| multi-head attention: collaborate instead of concatenate. ||| jean-baptiste cordonnier ||| andreas loukas ||| martin jaggi ||| 
2021 ||| convolutional neural network (cnn) vs visual transformer (vit) for digital holography. ||| st ||| phane cuenat ||| rapha ||| l couturier ||| 
2020 ||| a universal representation transformer layer for few-shot image classification. ||| lu liu ||| william l. hamilton ||| guodong long ||| jing jiang ||| hugo larochelle ||| 
2018 ||| a novel focal tversky loss function with improved attention u-net for lesion segmentation. ||| nabila abraham ||| naimul mefraz khan ||| 
2021 ||| so-vit: mind visual tokens for vision transformer. ||| jiangtao xie ||| ruiren zeng ||| qilong wang ||| ziqi zhou ||| peihua li ||| 
2021 ||| knowledge distillation from bert transformer to speech transformer for intent classification. ||| yidi jiang ||| bidisha sharma ||| maulik c. madhavi ||| haizhou li ||| 
2020 ||| improving bert with syntax-aware local attention. ||| zhongli li ||| qingyu zhou ||| chao li ||| ke xu ||| yunbo cao ||| 
2019 ||| a sparse annotation strategy based on attention-guided active learning for 3d medical image segmentation. ||| zhenxi zhang ||| jie li ||| zhusi zhong ||| zhicheng jiao ||| xinbo gao ||| 
2019 ||| alphastock: a buying-winners-and-selling-losers investment strategy using interpretable deep reinforcement attention networks. ||| jingyuan wang ||| yang zhang ||| ke tang ||| junjie wu ||| zhang xiong ||| 
2020 ||| arabert: transformer-based model for arabic language understanding. ||| wissam antoun ||| fady baly ||| hazem m. hajj ||| 
2017 ||| attend and diagnose: clinical time series analysis using attention models. ||| huan song ||| deepta rajan ||| jayaraman j. thiagarajan ||| andreas spanias ||| 
2020 ||| rotate to attend: convolutional triplet attention module. ||| diganta misra ||| trikay nalamada ||| ajay uppili arasanipalai ||| qibin hou ||| 
2021 ||| homogeneous learning: self-attention decentralized deep learning. ||| yuwei sun ||| hideya ochiai ||| 
2019 ||| nat: neural architecture transformer for accurate and compact architectures. ||| yong guo ||| yin zheng ||| mingkui tan ||| qi chen ||| jian chen ||| peilin zhao ||| junzhou huang ||| 
2019 ||| attention network robustification for person reid. ||| hussam lawen ||| avi ben-cohen ||| matan protter ||| itamar friedman ||| lihi zelnik-manor ||| 
2020 ||| g-darts-a: groups of channel parallel sampling with attention. ||| zhaowen wang ||| wei zhang ||| zhiming wang ||| 
2018 ||| attention-based audio-visual fusion for robust automatic speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2018 ||| yuanfudao at semeval-2018 task 11: three-way attention and relational knowledge for commonsense machine comprehension. ||| liang wang ||| meng sun ||| wei zhao ||| kewei shen ||| jingming liu ||| 
2020 ||| comparing transformers and rnns on predicting human sentence processing data. ||| danny merkx ||| stefan l. frank ||| 
2019 ||| enhancing the transformer with explicit relational encoding for math problem solving. ||| imanol schlag ||| paul smolensky ||| roland fernandez ||| nebojsa jojic ||| j ||| rgen schmidhuber ||| jianfeng gao ||| 
2021 ||| convit: improving vision transformers with soft convolutional inductive biases. ||| st ||| phane d'ascoli ||| hugo touvron ||| matthew l. leavitt ||| ari s. morcos ||| giulio biroli ||| levent sagun ||| 
2021 ||| score transformer: generating musical score from note-level representation. ||| masahiro suzuki ||| 
2018 ||| reciprocal attention fusion for visual question answering. ||| moshiur r. farazi ||| salman h. khan ||| 
2021 ||| teasel: a transformer-based speech-prefixed language model. ||| mehdi arjmand ||| mohammad javad dousti ||| hadi moradi ||| 
2021 ||| multi-task prediction of clinical outcomes in the intensive care unit using flexible multimodal transformers. ||| benjamin shickel ||| patrick j. tighe ||| azra bihorac ||| parisa rashidi ||| 
2022 ||| how do vision transformers work? ||| namuk park ||| songkuk kim ||| 
2021 ||| mate: multi-view attention for table transformer efficiency. ||| julian martin eisenschlos ||| maharshi gor ||| thomas m ||| ller ||| william w. cohen ||| 
2021 ||| improving vision transformers for incremental learning. ||| pei yu ||| yinpeng chen ||| ying jin ||| zicheng liu ||| 
2021 ||| epsanet: an efficient pyramid split attention block on convolutional neural network. ||| hu zhang ||| keke zu ||| jian lu ||| yuru zou ||| deyu meng ||| 
2018 ||| an affect-rich neural conversational model with biased attention and weighted cross-entropy loss. ||| peixiang zhong ||| di wang ||| chunyan miao ||| 
2022 ||| multi-level attention for unsupervised person re-identification. ||| yi zheng ||| 
2021 ||| lstm-sakt: lstm-encoded sakt-like transformer for knowledge tracing. ||| takashi oya ||| shigeo morishima ||| 
2022 ||| transvod: end-to-end video object detection with spatial-temporal transformers. ||| qianyu zhou ||| xiangtai li ||| lu he ||| yibo yang ||| guangliang cheng ||| yunhai tong ||| lizhuang ma ||| dacheng tao ||| 
2019 ||| supervised multimodal bitransformers for classifying images and text. ||| douwe kiela ||| suvrat bhooshan ||| hamed firooz ||| davide testuggine ||| 
2019 ||| self-attention for raw optical satellite time series classification. ||| marc ru ||| wurm ||| marco k ||| rner ||| 
2020 ||| contextualised graph attention for improved relation extraction. ||| angrosh mandya ||| danushka bollegala ||| frans coenen ||| 
2020 ||| visbert: hidden-state visualizations for transformers. ||| betty van aken ||| benjamin winter ||| alexander l ||| ser ||| felix a. gers ||| 
2020 ||| polarization-driven semantic segmentation via efficient attention-bridged fusion. ||| kaite xiang ||| kailun yang ||| kaiwei wang ||| 
2021 ||| smac-seg: lidar panoptic segmentation via sparse multi-directional attention clustering. ||| enxu li ||| ryan razani ||| yixuan xu ||| bingbing liu ||| 
2018 ||| scene parsing via dense recurrent neural networks with attentional selection. ||| heng fan ||| peng chu ||| longin jan latecki ||| haibin ling ||| 
2021 ||| mvs2d: efficient multi-view stereo via attention-driven 2d convolutions. ||| zhenpei yang ||| zhile ren ||| qi shan ||| qixing huang ||| 
2021 ||| trading with the momentum transformer: an intelligent and interpretable architecture. ||| kieran wood ||| sven giegerich ||| stephen j. roberts ||| stefan zohren ||| 
2021 ||| a-esrgan: training real-world blind super-resolution with attention u-net discriminators. ||| zihao wei ||| yidong huang ||| yuang chen ||| chenhao zheng ||| jinnan gao ||| 
2020 ||| deriving contextualised semantic features from bert (and other transformer model) embeddings. ||| jacob turton ||| david p. vinson ||| robert elliott smith ||| 
2021 ||| musical speech: a transformer-based composition tool. ||| jason d'eon ||| sri harsha dumpala ||| chandramouli shama sastry ||| dani oore ||| sageev oore ||| 
2018 ||| crowd-robot interaction: crowd-aware robot navigation with attention-based deep reinforcement learning. ||| changan chen ||| yuejiang liu ||| sven kreiss ||| alexandre alahi ||| 
2021 ||| cloth interactive transformer for virtual try-on. ||| bin ren ||| hao tang ||| fanyang meng ||| runwei ding ||| ling shao ||| philip h. s. torr ||| nicu sebe ||| 
2021 ||| focal self-attention for local-global interactions in vision transformers. ||| jianwei yang ||| chunyuan li ||| pengchuan zhang ||| xiyang dai ||| bin xiao ||| lu yuan ||| jianfeng gao ||| 
2019 ||| cross-modal image fusion theory guided by subjective visual attention. ||| aiqing fang ||| xinbo zhao ||| yanning zhang ||| 
2020 ||| attention-based 3d object reconstruction from a single image. ||| andrey de aguiar salvi ||| nathan gavenski ||| eduardo h. p. pooch ||| felipe tasoniero ||| rodrigo c. barros ||| 
2021 ||| transformers can do bayesian inference. ||| samuel m ||| ller ||| noah hollmann ||| sebastian pineda-arango ||| josif grabocka ||| frank hutter ||| 
2020 ||| transquest: translation quality estimation with cross-lingual transformers. ||| tharindu ranasinghe ||| constantin orasan ||| ruslan mitkov ||| 
2021 ||| interpretable visual understanding with cognitive attention network. ||| xuejiao tang ||| wenbin zhang ||| yi yu ||| kea turner ||| tyler derr ||| mengyu wang ||| eirini ntoutsi ||| 
2019 ||| fcem: a novel fast correlation extract model for real time steganalysis of voip stream via multi-head attention. ||| hao yang ||| zhongliang yang ||| yongjian bao ||| sheng liu ||| yongfeng huang ||| 
2021 ||| adaadepth: adapting data augmentation and attention for self-supervised monocular depth estimation. ||| vinay kaushik ||| kartik jindgar ||| brejesh lall ||| 
2018 ||| image super-resolution using very deep residual channel attention networks. ||| yulun zhang ||| kunpeng li ||| kai li ||| lichen wang ||| bineng zhong ||| yun fu ||| 
2018 ||| multi-scale alignment and contextual history for attention mechanism in sequence-to-sequence model. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| visual transformer with statistical test for covid-19 classification. ||| chih-chung hsu ||| guan-lin chen ||| mei-hsuan wu ||| 
2018 ||| joint attention in driver-pedestrian interaction: from theory to practice. ||| amir rasouli ||| john k. tsotsos ||| 
2020 ||| how to train your robust human pose estimator: pay attention to the constraint cue. ||| junjie huang ||| zheng zhu ||| guan huang ||| dalong du ||| 
2021 ||| attention-based sensor fusion for human activity recognition using imu signals. ||| wenjin tao ||| haodong chen ||| md. moniruzzaman ||| ming c. leu ||| zhaozheng yi ||| ruwen qin ||| 
2021 ||| attention-guided temporal coherent video object matting. ||| yunke zhang ||| chi wang ||| miaomiao cui ||| peiran ren ||| xuansong xie ||| xian-sheng hua ||| hujun bao ||| qixing huang ||| weiwei xu ||| 
2021 ||| oscar-net: object-centric scene graph attention for image attribution. ||| eric nguyen ||| tu bui ||| vishy swaminathan ||| john p. collomosse ||| 
2021 ||| searching the search space of vision transformer. ||| minghao chen ||| kan wu ||| bolin ni ||| houwen peng ||| bei liu ||| jianlong fu ||| hongyang chao ||| haibin ling ||| 
2020 ||| gmat: global memory augmentation for transformers. ||| ankit gupta ||| jonathan berant ||| 
2021 ||| salypath: a deep-based architecture for visual attention prediction. ||| mohamed amine kerkouri ||| marouane tliba ||| aladine chetouani ||| rachid harba ||| 
2020 ||| volumetric attention for 3d medical image segmentation and detection. ||| xudong wang ||| shizhong han ||| yunqiang chen ||| dashan gao ||| nuno vasconcelos ||| 
2020 ||| coarse to fine: multi-label image classification with global/local attention. ||| fan lyu ||| fuyuan hu ||| victor s. sheng ||| zhengtian wu ||| qiming fu ||| baochuan fu ||| 
2020 ||| transformers are rnns: fast autoregressive transformers with linear attention. ||| angelos katharopoulos ||| apoorv vyas ||| nikolaos pappas ||| fran ||| ois fleuret ||| 
2020 ||| multi-scale receptive fields graph attention network for point cloud classification. ||| xi-an li ||| lei zhang ||| li-yan wang ||| jian lu ||| 
2021 ||| semi-supervised segmentation of radiation-induced pulmonary fibrosis from lung ct scans with multi-scale guided dense attention. ||| guotai wang ||| shuwei zhai ||| giovanni lasio ||| baoshe zhang ||| byong yi ||| shifeng chen ||| thomas j. macvittie ||| dimitris n. metaxas ||| jinghao zhou ||| shaoting zhang ||| 
2017 ||| attend and predict: understanding gene regulation by selective attention on chromatin. ||| ritambhara singh ||| jack lanchantin ||| arshdeep sekhon ||| yanjun qi ||| 
2018 ||| attention-based active visual search for mobile robots. ||| amir rasouli ||| pablo lanillos ||| gordon cheng ||| john k. tsotsos ||| 
2020 ||| single headed attention based sequence-to-sequence model for state-of-the-art results on switchboard-300. ||| zolt ||| n t ||| ske ||| george saon ||| kartik audhkhasi ||| brian kingsbury ||| 
2021 ||| ice hockey player identification via transformers. ||| kanav vats ||| william j. mcnally ||| pascale walters ||| david a. clausi ||| john s. zelek ||| 
2022 ||| boosting crowd counting via multifaceted attention. ||| hui lin ||| zhiheng ma ||| rongrong ji ||| yaowei wang ||| xiaopeng hong ||| 
2020 ||| transformer-encoder detector module: using context to improve robustness to adversarial attacks on object detection. ||| faisal alamri ||| sinan kalkan ||| nicolas pugeault ||| 
2020 ||| conv-transformer transducer: low latency, low frame rate, streamable end-to-end speech recognition. ||| wenyong huang ||| wenchao hu ||| yu ting yeung ||| xiao chen ||| 
2020 ||| conditional self-attention for query-based summarization. ||| yujia xie ||| tianyi zhou ||| yi mao ||| weizhu chen ||| 
2020 ||| sliceout: training transformers and cnns faster while using less memory. ||| pascal notin ||| aidan n. gomez ||| joanna yoo ||| yarin gal ||| 
2019 ||| the channel attention based context encoder network for inner limiting membrane detection. ||| hao qiu ||| zaiwang gu ||| lei mou ||| xiaoqian mao ||| liyang fang ||| yitian zhao ||| jiang liu ||| jun cheng ||| 
2022 ||| dsrrtracker: dynamic search region refinement for attention-based siamese multi-object tracking. ||| jiaxu wan ||| hong zhang ||| jin zhang ||| yuan ding ||| yifan yang ||| yan li ||| xuliang li ||| 
2017 ||| where to focus: deep attention-based spatially recurrent bilinear networks for fine-grained visual recognition. ||| lin wu ||| yang wang ||| 
2021 ||| unified questioner transformer for descriptive question generation in goal-oriented visual dialogue. ||| shoya matsumori ||| kosuke shingyouchi ||| yuki abe ||| yosuke fukuchi ||| komei sugiura ||| michita imai ||| 
2019 ||| multi-task attention-based semi-supervised learning for medical image segmentation. ||| shuai chen ||| gerda bortsova ||| antonio garcia-uceda juarez ||| gijs van tulder ||| marleen de bruijne ||| 
2019 ||| document-level neural machine translation with inter-sentence attention. ||| shu jiang ||| rui wang ||| zuchao li ||| masao utiyama ||| kehai chen ||| eiichiro sumita ||| hai zhao ||| bao-liang lu ||| 
2021 ||| deep neural network loses attention to adversarial images. ||| shashank kotyan ||| danilo vasconcellos vargas ||| 
2021 ||| is space-time attention all you need for video understanding? ||| gedas bertasius ||| heng wang ||| lorenzo torresani ||| 
2018 ||| attention fusion networks: combining behavior and e-mail content to improve customer support. ||| stephane fotso ||| philip spanoudes ||| benjamin c. ponedel ||| brian reynoso ||| janet ko ||| 
2022 ||| detail-preserving transformer for light field image super-resolution. ||| shunzhou wang ||| tianfei zhou ||| yao lu ||| huijun di ||| 
2021 ||| pre-training and fine-tuning transformers for fmri prediction tasks. ||| itzik malkiel ||| gony rosenman ||| lior wolf ||| talma hendler ||| 
2018 ||| iterative transformer network for 3d point cloud. ||| wentao yuan ||| david held ||| christoph mertz ||| martial hebert ||| 
2021 ||| bayesian transformer language models for speech recognition. ||| boyang xue ||| jianwei yu ||| junhao xu ||| shansong liu ||| shoukang hu ||| zi ye ||| mengzhe geng ||| xunying liu ||| helen meng ||| 
2020 ||| guiding attention in sequence-to-sequence models for dialogue act prediction. ||| pierre colombo ||| emile chapuis ||| matteo manica ||| emmanuel vignon ||| giovanna varni ||| chlo |||  clavel ||| 
2021 ||| quantifying and maximizing the benefits of back-end noise adaption on attention-based speech recognition models. ||| coleman hooper ||| thierry tambe ||| gu-yeon wei ||| 
2019 ||| cross attention network for semantic segmentation. ||| mengyu liu ||| hujun yin ||| 
2020 ||| lossless attention in convolutional networks for facial expression recognition in the wild. ||| chuang wang ||| ruimin hu ||| min hu ||| jiang liu ||| ting ren ||| shan he ||| ming jiang ||| jing miao ||| 
2022 ||| aggregating global features into local vision transformer. ||| krushi patel ||| andres m. bur ||| fengjun li ||| guanghui wang ||| 
2021 ||| capturing multi-resolution context by dilated self-attention. ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2021 ||| on the difficulty of segmenting words with attention. ||| ramon sanabria ||| hao tang ||| sharon goldwater ||| 
2021 ||| vidt: an efficient and effective fully transformer-based object detector. ||| hwanjun song ||| deqing sun ||| sanghyuk chun ||| varun jampani ||| dongyoon han ||| byeongho heo ||| wonjae kim ||| ming-hsuan yang ||| 
2019 ||| classification of hand movements from eeg using a deep attention-based lstm network. ||| guangyi zhang ||| vandad davoodnia ||| alireza sepas-moghaddam ||| yaoxue zhang ||| s. ali etemad ||| 
2017 ||| end-to-end flow correlation tracking with spatial-temporal attention. ||| zheng zhu ||| wei wu ||| wei zou ||| junjie yan ||| 
2021 ||| self-attention meta-learner for continual learning. ||| ghada sokar ||| decebal constantin mocanu ||| mykola pechenizkiy ||| 
2021 ||| adaptable gan encoders for image reconstruction via multi-type latent vectors with two-scale attentions. ||| cheng yu ||| wenmin wang ||| 
2021 ||| wake word detection with streaming transformers. ||| yiming wang ||| hang lv ||| daniel povey ||| lei xie ||| sanjeev khudanpur ||| 
2021 ||| exploring text-transformers in aaai 2021 shared task: covid-19 fake news detection in english. ||| xiangyang li ||| yu xia ||| xiang long ||| zheng li ||| sujian li ||| 
2018 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks. ||| timo hackel ||| mikhail usvyatsov ||| silvano galliani ||| jan dirk wegner ||| konrad schindler ||| 
2020 ||| information extraction from swedish medical prescriptions with sig-transformer encoder. ||| john pougue biyong ||| bo wang ||| terry j. lyons ||| alejo j. nevado-holgado ||| 
2022 ||| drtam: dual rank-1 tensor attention module. ||| hanxing chi ||| baihong lin ||| jun hu ||| liang wang ||| 
2021 ||| deep attention-guided graph clustering with dual self-supervision. ||| zhihao peng ||| hui liu ||| yuheng jia ||| junhui hou ||| 
2021 ||| infusing future information into monotonic attention through language models. ||| mohd abbas zaidi ||| sathish reddy indurthi ||| beomseok lee ||| nikhil kumar lakumarapu ||| sangha kim ||| 
2020 ||| exploring self-attention for image recognition. ||| hengshuang zhao ||| jiaya jia ||| vladlen koltun ||| 
2020 ||| improving image captioning by leveraging intra- and inter-layer global representation in transformer network. ||| jiayi ji ||| yunpeng luo ||| xiaoshuai sun ||| fuhai chen ||| gen luo ||| yongjian wu ||| yue gao ||| rongrong ji ||| 
2020 ||| a dc-autotransformer based multilevel inverter for automotive applications. ||| ferdinand grimm ||| john wood ||| mehdi baghdadi ||| 
2021 ||| deep reinforced attention regression for partial sketch based image retrieval. ||| dingrong wang ||| hitesh sapkota ||| xumin liu ||| qi yu ||| 
2017 ||| attention networks for image-to-text. ||| jason poulos ||| rafael valle ||| 
2018 ||| coarse-to-fine: a rnn-based hierarchical attention model for vehicle re-identification. ||| xiu-shen wei ||| chen-lin zhang ||| lingqiao liu ||| chunhua shen ||| jianxin wu ||| 
2019 ||| attention guided anomaly detection and localization in images. ||| shashanka venkataramanan ||| kuan-chuan peng ||| rajat vikram singh ||| abhijit mahalanobis ||| 
2021 ||| impact of attention on adversarial robustness of image classification models. ||| prachi agrawal ||| narinder singh punn ||| sanjay kumar sonbhadra ||| sonali agarwal ||| 
2019 ||| conditioning lstm decoder and bi-directional attention based question answering system. ||| heguang liu ||| 
2017 ||| data-driven approach to measuring the level of press freedom using media attention diversity from unfiltered news. ||| jisun an ||| haewoon kwak ||| 
2021 ||| attention-guided nir image colorization via adaptive fusion of semantic and texture clues. ||| xingxing yang ||| jie chen ||| zaifeng yang ||| zhenghua chen ||| 
2020 ||| glu variants improve transformer. ||| noam shazeer ||| 
2021 ||| vision transformer architecture search. ||| xiu su ||| shan you ||| jiyang xie ||| mingkai zheng ||| fei wang ||| chen qian ||| changshui zhang ||| xiaogang wang ||| chang xu ||| 
2021 ||| using large pre-trained models with cross-modal attention for multi-modal emotion recognition. ||| krishna d. n ||| 
2021 ||| speech summarization using restricted self-attention. ||| roshan sharma ||| shruti palaskar ||| alan w. black ||| florian metze ||| 
2020 ||| on task-level dialogue composition of generative transformer model. ||| prasanna parthasarathi ||| arvind neelakantan ||| sharan narang ||| 
2019 ||| learning target-oriented dual attention for robust rgb-t tracking. ||| rui yang ||| yabin zhu ||| xiao wang ||| chenglong li ||| jin tang ||| 
2021 ||| lymph node detection in t2 mri with transformers. ||| tejas sudharshan mathai ||| sungwon lee ||| daniel c. elton ||| thomas c. shen ||| yifan peng ||| zhiyong lu ||| ronald m. summers ||| 
2021 ||| feature combination meets attention: baidu soccer embeddings and transformer based temporal detection. ||| xin zhou ||| le kang ||| zhiyu cheng ||| bo he ||| jingyu xin ||| 
2017 ||| a nested attention neural hybrid model for grammatical error correction. ||| jianshu ji ||| qinlong wang ||| kristina toutanova ||| yongen gong ||| steven truong ||| jianfeng gao ||| 
2019 ||| a two-stream end-to-end deep learning network for recognizing atypical visual attention in autism spectrum disorder. ||| jin xie ||| longfei wang ||| paula webster ||| yang yao ||| jiayao sun ||| shuo wang ||| huihui zhou ||| 
2020 ||| code completion using neural attention and byte pair encoding. ||| youri arkesteijn ||| nikhil saldanha ||| bastijn kostense ||| 
2021 ||| acnet: mask-aware attention with dynamic context enhancement for robust acne detection. ||| kyungseo min ||| gun-hee lee ||| seong-whan lee ||| 
2022 ||| audio-visual generalised zero-shot learning with cross-modal attention and language. ||| otniel-bogdan mercea ||| lukas riesch ||| a. sophia koepke ||| zeynep akata ||| 
2019 ||| comet: commonsense transformers for automatic knowledge graph construction. ||| antoine bosselut ||| hannah rashkin ||| maarten sap ||| chaitanya malaviya ||| asli celikyilmaz ||| yejin choi ||| 
2022 ||| transformer for graphs: an overview from architecture perspective. ||| erxue min ||| runfa chen ||| yatao bian ||| tingyang xu ||| kangfei zhao ||| wenbing huang ||| peilin zhao ||| junzhou huang ||| sophia ananiadou ||| yu rong ||| 
2021 ||| edgeconv with attention module for monocular depth estimation. ||| minhyeok lee ||| sangwon hwang ||| chaewon park ||| sangyoun lee ||| 
2020 ||| exploiting typed syntactic dependencies for targeted sentiment classification using graph attention neural network. ||| xuefeng bai ||| pengbo liu ||| yue zhang ||| 
2020 ||| hamlet: a hierarchical multimodal attention-based human activity recognition algorithm. ||| md mofijul islam ||| tariq iqbal ||| 
2021 ||| predicting discourse trees from transformer-based neural summarizers. ||| wen xiao ||| patrick huber ||| giuseppe carenini ||| 
2021 ||| attention-enhanced cross-task network for analysing multiple attributes of lung nodules in ct. ||| xiaohang fu ||| lei bi ||| ashnil kumar ||| michael j. fulham ||| jinman kim ||| 
2021 ||| attention-based stylisation for exemplar image colourisation. ||| marc g ||| rriz blanch ||| issa khalifeh ||| alan f. smeaton ||| noel e. o'connor ||| marta mrak ||| 
2018 ||| convolutional attention networks for multimodal emotion recognition from speech and text data. ||| 
2018 ||| hierarchical attention networks for knowledge base completion via joint adversarial training. ||| chen li ||| xutan peng ||| shanghang zhang ||| jianxin li ||| lihong wang ||| 
2022 ||| shapeformer: transformer-based shape completion via sparse representation. ||| xingguang yan ||| liqiang lin ||| niloy j. mitra ||| dani lischinski ||| danny cohen-or ||| hui huang ||| 
2020 ||| multi-head linear attention generative adversarial network for thin cloud removal. ||| chenxi duan ||| rui li ||| 
2018 ||| comparing attention-based convolutional and recurrent neural networks: success and limitations in machine reading comprehension. ||| matthias blohm ||| glorianna jagfeld ||| ekta sood ||| xiang yu ||| ngoc thang vu ||| 
2021 ||| mixed precision of quantization of transformer language models for speech recognition. ||| junhao xu ||| shoukang hu ||| jianwei yu ||| xunying liu ||| helen meng ||| 
2022 ||| transformer compressed sensing via global image tokens. ||| marlon bran lorenzana ||| craig engstrom ||| shekhar s. chandra ||| 
2018 ||| improving robustness of attention models on graphs. ||| uday shankar shanthamallu ||| jayaraman j. thiagarajan ||| andreas spanias ||| 
2019 ||| graph representation learning via hard and channel-wise attention networks. ||| hongyang gao ||| shuiwang ji ||| 
2019 ||| audiovisual transformer architectures for large-scale classification and synchronization of weakly labeled audio events. ||| wim boes ||| hugo van hamme ||| 
2021 ||| consistent accelerated inference via confident adaptive transformers. ||| tal schuster ||| adam fisch ||| tommi s. jaakkola ||| regina barzilay ||| 
2021 ||| geometric transformers for protein interface contact prediction. ||| alex morehead ||| chen chen ||| jianlin cheng ||| 
2021 ||| real-time attention span tracking in online education. ||| rahul rk ||| shantha kumar s ||| vykunth p ||| sairamnath k ||| 
2020 ||| svga-net: sparse voxel-graph attention network for 3d object detection from point clouds. ||| qingdong he ||| zhengning wang ||| hao zeng ||| yi zeng ||| shuaicheng liu ||| bing zeng ||| 
2019 ||| attention optimization for abstractive document summarization. ||| min gui ||| junfeng tian ||| rui wang ||| zhenglu yang ||| 
2018 ||| generative model for material experiments based on prior knowledge and attention mechanism. ||| mincong luo ||| xinfu he ||| li liu ||| 
2021 ||| graph attention networks for anti-spoofing. ||| hemlata tak ||| jee-weon jung ||| jose patino ||| massimiliano todisco ||| nicholas w. d. evans ||| 
2021 ||| video joint modelling based on hierarchical transformer for co-summarization. ||| haopeng li ||| qiuhong ke ||| mingming gong ||| rui zhang ||| 
2019 ||| differentiating features for scene segmentation based on dedicated attention mechanisms. ||| zhiqiang xiong ||| zhicheng wang ||| zhaohui yu ||| xi gu ||| 
2020 ||| attention guided semantic relationship parsing for visual question answering. ||| moshiur r. farazi ||| salman h. khan ||| nick barnes ||| 
2021 ||| transformer-unet: raw image processing with unet. ||| youyang sha ||| yonghong zhang ||| xuquan ji ||| lei hu ||| 
2020 ||| track-assignment detailed routing using attention-based policy model with supervision. ||| haiguang liao ||| qingyi dong ||| weiyi qi ||| elias fallon ||| levent burak kara ||| 
2020 ||| luke: deep contextualized entity representations with entity-aware self-attention. ||| ikuya yamada ||| akari asai ||| hiroyuki shindo ||| hideaki takeda ||| yuji matsumoto ||| 
2021 ||| chasing sparsity in vision transformers: an end-to-end exploration. ||| tianlong chen ||| yu cheng ||| zhe gan ||| lu yuan ||| lei zhang ||| zhangyang wang ||| 
2021 ||| attentional graph neural network for parking-slot detection. ||| chen min ||| jiaolong xu ||| liang xiao ||| dawei zhao ||| yiming nie ||| bin dai ||| 
2019 ||| stnreid : deep convolutional networks with pairwise spatial transformer networks for partial person re-identification. ||| hao luo ||| xing fan ||| chi zhang ||| wei jiang ||| 
2021 ||| transfg: a transformer architecture for fine-grained recognition. ||| ju he ||| jieneng chen ||| shuai liu ||| adam kortylewski ||| cheng yang ||| yutong bai ||| changhu wang ||| alan l. yuille ||| 
2021 ||| language modeling using lmus: 10x better data efficiency or improved scaling compared to transformers. ||| narsimha chilkuri ||| eric hunsberger ||| aaro ||| n voelker ||| gurshaant malik ||| chris eliasmith ||| 
2022 ||| dual-decoder transformer for end-to-end mandarin chinese speech recognition with pinyin and character. ||| zhao yang ||| wei xi ||| rui wang ||| rui jiang ||| jizhong zhao ||| 
2020 ||| sparsifying transformer models with differentiable representation pooling. ||| michal pietruszka ||| lukasz borchmann ||| filip gralinski ||| 
2021 ||| translational equivariance in kernelizable attention. ||| max horn ||| kumar shridhar ||| elrich groenewald ||| philipp f. m. baumann ||| 
2019 ||| latent suicide risk detection on microblog via suicide-oriented word embeddings and layered attention. ||| lei cao ||| huijun zhang ||| ling feng ||| zihan wei ||| xin wang ||| ningyun li ||| xiaohao he ||| 
2020 ||| quantifying attention flow in transformers. ||| samira abnar ||| willem h. zuidema ||| 
2020 ||| emptransfo: a multi-head transformer architecture for creating empathetic dialog systems. ||| rohola zandie ||| mohammad h. mahoor ||| 
2021 ||| bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification. ||| am ||| lie gruel ||| jean martinet ||| 
2021 ||| variational transformer networks for layout generation. ||| diego mart ||| n arroyo ||| janis postels ||| federico tombari ||| 
2020 ||| graphspeech: syntax-aware graph attention network for neural speech synthesis. ||| rui liu ||| berrak sisman ||| haizhou li ||| 
2021 ||| robust lane detection via expanded self attention. ||| minhyeok lee ||| junhyeop lee ||| dogyoon lee ||| woo jin kim ||| sang-won hwang ||| sangyoun lee ||| 
2021 ||| twins: revisiting spatial attention design in vision transformers. ||| xiangxiang chu ||| zhi tian ||| yuqing wang ||| bo zhang ||| haibing ren ||| xiaolin wei ||| huaxia xia ||| chunhua shen ||| 
2022 ||| transformer-based approaches for legal text processing. ||| ha-thanh nguyen ||| minh phuong nguyen ||| thi-hai-yen vuong ||| minh-quan bui ||| chau minh nguyen ||| tran binh dang ||| vu tran ||| le-minh nguyen ||| ken satoh ||| 
2018 ||| deep ordinal hashing with spatial attention. ||| lu jin ||| xiangbo shu ||| kai li ||| zechao li ||| guo-jun qi ||| jinhui tang ||| 
2021 ||| attention-based feature decomposition-reconstruction network for scene text detection. ||| qi zhao ||| yufei wang ||| shuchang lyu ||| lijiang chen ||| 
2020 ||| multi-task network for noise-robust keyword spotting and speaker verification using ctc-based soft vad and global query attention. ||| myunghun jung ||| youngmoon jung ||| jahyun goo ||| hoirin kim ||| 
2021 ||| transvos: video object segmentation with transformers. ||| jianbiao mei ||| mengmeng wang ||| yeneng lin ||| yong liu ||| 
2022 ||| towards data-efficient detection transformers. ||| wen wang ||| jing zhang ||| yang cao ||| yongliang shen ||| dacheng tao ||| 
2021 ||| dodrio: exploring transformer models with interactive visualization. ||| zijie j. wang ||| robert turko ||| duen horng chau ||| 
2019 ||| xlsor: a robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. ||| youbao tang ||| yuxing tang ||| jing xiao ||| ronald m. summers ||| 
2021 ||| facial attribute transformers for precise and robust makeup transfer. ||| zhaoyi wan ||| haoran chen ||| jielei zhang ||| wentao jiang ||| cong yao ||| jiebo luo ||| 
2021 ||| stochastic layers in vision transformers. ||| nikola popovic ||| danda pani paudel ||| thomas probst ||| luc van gool ||| 
2021 ||| do transformer modifications transfer across implementations and applications? ||| sharan narang ||| hyung won chung ||| yi tay ||| william fedus ||| thibault f ||| vry ||| michael matena ||| karishma malkan ||| noah fiedel ||| noam shazeer ||| zhenzhong lan ||| yanqi zhou ||| wei li ||| nan ding ||| jake marcus ||| adam roberts ||| colin raffel ||| 
2021 ||| conditional generative data-free knowledge distillation based on attention transfer. ||| xinyi yu ||| ling yan ||| linlin ou ||| 
2022 ||| similarity and content-based phonetic self attention for speech recognition. ||| kyuhong shim ||| wonyong sung ||| 
2019 ||| rasnet: segmentation for tracking surgical instruments in surgical videos using refined attention segmentation network. ||| zhen-liang ni ||| gui-bin bian ||| xiaoliang xie ||| zeng-guang hou ||| xiao-hu zhou ||| yan-jie zhou ||| 
2022 ||| chitransformer: towards reliable stereo from cues. ||| qing su ||| shihao ji ||| 
2019 ||| multimodal unified attention networks for vision-and-language interactions. ||| zhou yu ||| yuhao cui ||| jun yu ||| dacheng tao ||| qi tian ||| 
2021 ||| development and testing of an image transformer for explainable autonomous driving systems. ||| jiqian dong ||| sikai chen ||| shuya zong ||| tiantian chen ||| mohammad miralinaghi ||| samuel labi ||| 
2020 ||| transformer vq-vae for unsupervised unit discovery and speech synthesis: zerospeech 2020 challenge. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2022 ||| translog: a unified transformer-based framework for log anomaly detection. ||| hongcheng guo ||| xingyu lin ||| jian yang ||| yi zhuang ||| jiaqi bai ||| tieqiao zheng ||| bo zhang ||| zhoujun li ||| 
2019 ||| self-attention networks for connectionist temporal classification in speech recognition. ||| julian salazar ||| katrin kirchhoff ||| zhiheng huang ||| 
2022 ||| keypoints tracking via transformer networks. ||| oleksii nasypanyi ||| fran ||| ois rameau ||| 
2020 ||| keyphrase generation with cross-document attention. ||| shizhe diao ||| yan song ||| tong zhang ||| 
2020 ||| augmented transformer achieves 97% and 85% for top5 prediction of direct and classical retro-synthesis. ||| igor v. tetko ||| pavel karpov ||| ruud van deursen ||| guillaume godin ||| 
2020 ||| transformer in action: a comparative study of transformer-based acoustic models for large scale speech recognition applications. ||| yongqiang wang ||| yangyang shi ||| frank zhang ||| chunyang wu ||| julian chan ||| ching-feng yeh ||| alex xiao ||| 
2021 ||| dualformer: local-global stratified transformer for efficient video recognition. ||| yuxuan liang ||| pan zhou ||| roger zimmermann ||| shuicheng yan ||| 
2017 ||| residual attention network for image classification. ||| fei wang ||| mengqing jiang ||| chen qian ||| shuo yang ||| cheng li ||| honggang zhang ||| xiaogang wang ||| xiaoou tang ||| 
2019 ||| improving attention mechanism in graph neural networks via cardinality preservation. ||| shuo zhang ||| lei xie ||| 
2019 ||| effective use of transformer networks for entity tracking. ||| aditya gupta ||| greg durrett ||| 
2017 ||| iterative multi-document neural attention for multiple answer prediction. ||| claudio greco ||| alessandro suglia ||| pierpaolo basile ||| gaetano rossiello ||| giovanni semeraro ||| 
2021 ||| spatial attention improves iterative 6d object pose estimation. ||| stefan stevsic ||| otmar hilliges ||| 
2022 ||| efficient classification of long documents using transformers. ||| hyunji hayley park ||| yogarshi vyas ||| kashif shah ||| 
2021 ||| incorporating transformer and lstm to kalman filter with em algorithm for state estimation. ||| zhuangwei shi ||| 
2022 ||| graph decipher: a transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification. ||| yan pang ||| chao liu ||| 
2021 ||| bolt-dumbo transformer: asynchronous consensus as fast as pipelined bft. ||| yuan lu ||| zhenliang lu ||| qiang tang ||| 
2020 ||| an image is worth 16x16 words: transformers for image recognition at scale. ||| alexey dosovitskiy ||| lucas beyer ||| alexander kolesnikov ||| dirk weissenborn ||| xiaohua zhai ||| thomas unterthiner ||| mostafa dehghani ||| matthias minderer ||| georg heigold ||| sylvain gelly ||| jakob uszkoreit ||| neil houlsby ||| 
2022 ||| swin-pose: swin transformer based human pose estimation. ||| zinan xiong ||| chenxi wang ||| ying li ||| yan luo ||| yu cao ||| 
2021 ||| han: higher-order attention network for spoken language understanding. ||| dongsheng chen ||| zhiqi huang ||| yuexian zou ||| 
2017 ||| saliency-based sequential image attention with multiset prediction. ||| sean welleck ||| jialin mao ||| kyunghyun cho ||| zheng zhang ||| 
2021 ||| g-transformer for document-level machine translation. ||| guangsheng bao ||| yue zhang ||| zhiyang teng ||| boxing chen ||| weihua luo ||| 
2021 ||| m2gan: a multi-stage self-attention network for image rain removal on autonomous vehicles. ||| duc manh nguyen ||| sang-woong lee ||| 
2020 ||| patenttransformer-2: controlling patent text generation by structural metadata. ||| jieh-sheng lee ||| jieh hsiang ||| 
2020 ||| weakly supervised attention pyramid convolutional neural network for fine-grained visual classification. ||| yifeng ding ||| shaoguo wen ||| jiyang xie ||| dongliang chang ||| zhanyu ma ||| zhongwei si ||| haibin ling ||| 
2018 ||| formal verification of spacecraft control programs using a metalanguage for state transformers. ||| andrey mokhov ||| georgy lukyanov ||| jakob lechner ||| 
2022 ||| attention mechanism meets with hybrid dense network for hyperspectral image classification. ||| muhammad ahmad ||| adil mehmood khan ||| manuel mazzara ||| salvatore distefano ||| swalpa kumar roy ||| xin wu ||| 
2019 ||| tf-attention-net: an end to end neural network for singing voice separation. ||| tingle li ||| jiawei chen ||| haowen hou ||| ming li ||| 
2021 ||| tunet: a block-online bandwidth extension model based on transformers and self-supervised pretraining. ||| viet-anh nguyen ||| anh h. t. nguyen ||| andy w. h. khong ||| 
2019 ||| modeling recurrence for transformer. ||| jie hao ||| xing wang ||| baosong yang ||| longyue wang ||| jinfeng zhang ||| zhaopeng tu ||| 
2018 ||| an initial attempt of combining visual selective attention with deep reinforcement learning. ||| liu yuezhang ||| ruohan zhang ||| dana h. ballard ||| 
2019 ||| whatcha lookin' at? deeplifting bert's attention in question answering. ||| ekaterina arkhangelskaia ||| sourav dutta ||| 
2021 ||| more identifiable yet equally performant transformers for text classification. ||| rishabh bhardwaj ||| navonil majumder ||| soujanya poria ||| eduard h. hovy ||| 
2021 ||| self-attention networks can process bounded hierarchical languages. ||| shunyu yao ||| binghui peng ||| christos h. papadimitriou ||| karthik narasimhan ||| 
2019 ||| residual attention graph convolutional network for geometric 3d scene classification. ||| albert mosella-montoro ||| javier ruiz hidalgo ||| 
2022 ||| efficient long-range attention network for image super-resolution. ||| xindong zhang ||| hui zeng ||| shi guo ||| lei zhang ||| 
2021 ||| bioie: biomedical information extraction with multi-head attention enhanced graph convolutional network. ||| jialun wu ||| yang liu ||| zeyu gao ||| tieliang gong ||| chunbao wang ||| chen li ||| 
2019 ||| multi-vision attention networks for on-line red jujube grading. ||| xiaoye sun ||| liyan ma ||| gongyan li ||| 
2021 ||| nast: non-autoregressive spatial-temporal transformer for time series forecasting. ||| kai chen ||| guang chen ||| dan xu ||| lijun zhang ||| yuyao huang ||| alois c. knoll ||| 
2020 ||| focus longer to see better: recursively refined attention for fine-grained image classification. ||| prateek shroff ||| tianlong chen ||| yunchao wei ||| zhangyang wang ||| 
2017 ||| mining fine-grained opinions on closed captions of youtube videos with an attention-rnn. ||| edison marrese-taylor ||| jorge a. balazs ||| yutaka matsuo ||| 
2021 ||| fully attentional network for semantic segmentation. ||| qi song ||| jie li ||| chenghong li ||| hao guo ||| rui huang ||| 
2022 ||| an attention-based convlstm autoencoder with dynamic thresholding for unsupervised anomaly detection in multivariate time series. ||| tareq tayeh ||| sulaiman aburakhia ||| ryan myers ||| abdallah shami ||| 
2021 ||| knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. ||| lianghao xia ||| chao huang ||| yong xu ||| peng dai ||| xiyue zhang ||| hongsheng yang ||| jian pei ||| liefeng bo ||| 
2020 ||| image captioning with attention for smart local tourism using efficientnet. ||| dhomas hatta fudholi ||| yurio windiatmoko ||| nurdi afrianto ||| prastyo eko susanto ||| magfirah suyuti ||| ahmad fathan hidayatullah ||| ridho rahmadi ||| 
2020 ||| generalisable cardiac structure segmentation via attentional and stacked image adaptation. ||| hongwei li ||| jianguo zhang ||| bjoern h. menze ||| 
2021 ||| attention to warp: deep metric learning for multivariate time series. ||| shinnosuke matsuo ||| xiaomeng wu ||| gantugs atarsaikhan ||| akisato kimura ||| kunio kashino ||| brian kenji iwana ||| seiichi uchida ||| 
2019 ||| an attention-guided deep regression model for landmark detection in cephalograms. ||| zhusi zhong ||| jie li ||| zhenxi zhang ||| zhicheng jiao ||| xinbo gao ||| 
2019 ||| transmission lines positive sequence parameters estimation and instrument transformers calibration based on pmu measurement error model. ||| chen wang ||| virgilio a. centeno ||| kevin d. jones ||| duotong yang ||| 
2021 ||| transformer-based models for question answering on covid19. ||| hillary ngai ||| yoona park ||| john chen ||| mahboobeh parsapoor ||| 
2022 ||| a prospective approach for human-to-human interaction recognition from wi-fi channel data using attention bidirectional gated recurrent neural network with gui application implementation. ||| md. mohi uddin khan ||| abdullah bin shams ||| md. mohsin sarker raihan ||| 
2018 ||| attention based natural language grounding by navigating virtual environment. ||| abhishek sinha ||| akilesh b ||| mausoom sarkar ||| balaji krishnamurthy ||| 
2022 ||| attention-based deep neural networks for battery discharge capacity forecasting. ||| yadong zhang ||| chenye zou ||| xin chen ||| 
2021 ||| learning fair face representation with progressive cross transformer. ||| yong li ||| yufei sun ||| zhen cui ||| shiguang shan ||| jian yang ||| 
2021 ||| counterfactual attention learning for fine-grained visual categorization and re-identification. ||| yongming rao ||| guangyi chen ||| jiwen lu ||| jie zhou ||| 
2019 ||| attention-based fusion for outfit recommendation. ||| katrien laenen ||| marie-francine moens ||| 
2021 ||| contrastive out-of-distribution detection for pretrained transformers. ||| wenxuan zhou ||| muhao chen ||| 
2019 ||| self-attention aligner: a latency-control end-to-end model for asr using self-attention network and chunk-hopping. ||| linhao dong ||| feng wang ||| bo xu ||| 
2020 ||| investigating the true performance of transformers in low-resource languages: a case study in automatic corpus creation. ||| jan christian blaise cruz ||| jose kristian resabal ||| james lin ||| dan john velasco ||| charibeth cheng ||| 
2022 ||| fedformer: frequency enhanced decomposed transformer for long-term series forecasting. ||| tian zhou ||| ziqing ma ||| qingsong wen ||| xue wang ||| liang sun ||| rong jin ||| 
2022 ||| spherical transformer. ||| sungmin cho ||| raehyuk jung ||| junseok kwon ||| 
2020 ||| longformer: the long-document transformer. ||| iz beltagy ||| matthew e. peters ||| arman cohan ||| 
2021 ||| relational self-attention: what's missing in attention for video understanding. ||| manjin kim ||| heeseung kwon ||| chunyu wang ||| suha kwak ||| minsu cho ||| 
2021 ||| on the expressive power of self-attention matrices. ||| valerii likhosherstov ||| krzysztof choromanski ||| adrian weller ||| 
2020 ||| translating natural language instructions for behavioral robot navigation with a multi-head attention mechanism. ||| patricio cerda-mardini ||| vladimir araujo ||| alvaro soto ||| 
2020 ||| end-to-end domain adaptive attention network for cross-domain person re-identification. ||| amena khatun ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2020 ||| stronger transformers for neural multi-hop question generation. ||| devendra singh sachan ||| lingfei wu ||| mrinmaya sachan ||| william l. hamilton ||| 
2019 ||| comic: towards a compact image captioning model with attention. ||| jia huei tan ||| chee seng chan ||| joon huang chuah ||| 
2020 ||| attention and misinformation sharing on social media. ||| zaid amin ||| nazlena mohamad ali ||| alan f. smeaton ||| 
2022 ||| stdan: deformable attention network for space-time video super-resolution. ||| hai wang ||| xiaoyu xiang ||| yapeng tian ||| wenming yang ||| qingmin liao ||| 
2021 ||| decoupling the role of data, attention, and losses in multimodal transformers. ||| lisa anne hendricks ||| john mellor ||| rosalia schneider ||| jean-baptiste alayrac ||| aida nematzadeh ||| 
2019 ||| neural review rating prediction with hierarchical attentions and latent factors. ||| xianchen wang ||| hongtao liu ||| peiyi wang ||| fangzhao wu ||| hongyan xu ||| wenjun wang ||| xing xie ||| 
2021 ||| d2a u-net: automatic segmentation of covid-19 lesions from ct slices with dilated convolution and dual attention mechanism. ||| xiangyu zhao ||| peng zhang ||| fan song ||| guangda fan ||| yangyang sun ||| yujia wang ||| zheyuan tian ||| luqi zhang ||| guanglei zhang ||| 
2021 ||| cpt: convolutional point transformer for 3d point cloud processing. ||| chaitanya kaul ||| joshua mitton ||| hang dai ||| roderick murray-smith ||| 
2021 ||| dla-net: learning dual local attention features for semantic segmentation of large-scale building facade point clouds. ||| yanfei su ||| weiquan liu ||| zhimin yuan ||| ming cheng ||| zhihong zhang ||| xuelun shen ||| cheng wang ||| 
2021 ||| iip-transformer: intra-inter-part transformer for skeleton-based action recognition. ||| qingtian wang ||| jianlin peng ||| shuze shi ||| tingxi liu ||| jiabin he ||| renliang weng ||| 
2022 ||| ai can evolve without labels: self-evolving vision transformer for chest x-ray diagnosis through knowledge distillation. ||| sangjoon park ||| gwanghyun kim ||| yujin oh ||| joon beom seo ||| sang min lee ||| jin hwan kim ||| sungjun moon ||| jae-kwang lim ||| chang min park ||| jong chul ye ||| 
2021 ||| multi-attention-based soft partition network for vehicle re-identification. ||| sangrok lee ||| taekang woo ||| sang hun lee ||| 
2017 ||| single shot text detector with regional attention. ||| pan he ||| weilin huang ||| tong he ||| qile zhu ||| yu qiao ||| xiaolin li ||| 
2021 ||| toward accurate and realistic outfits visualization with attention to details. ||| kedan li ||| min jin chong ||| jeffrey zhang ||| jingen liu ||| 
2019 ||| multi-modal simultaneous forecasting of vehicle position sequences using social attention. ||| jean mercat ||| thomas gilles ||| nicole el zoghby ||| guillaume sandou ||| dominique beauvois ||| guillermo pita gil ||| 
2020 ||| can transformers reason about effects of actions? ||| pratyay banerjee ||| chitta baral ||| man luo ||| arindam mitra ||| kuntal kumar pal ||| tran cao son ||| neeraj varshney ||| 
2022 ||| caft: clustering and filter on tokens of transformer for weakly supervised object localization. ||| ming li ||| 
2019 ||| improving relation extraction with knowledge-attention. ||| pengfei li ||| kezhi mao ||| xuefeng yang ||| qi li ||| 
2021 ||| early lane change prediction for automated driving systems using multi-task attention-based convolutional neural networks. ||| sajjad mozaffari ||| eduardo arnold ||| mehrdad dianati ||| saber fallah ||| 
2017 |||  attention-driven neural distant supervision. ||| tushar nagarajan ||| sharmistha jat ||| partha p. talukdar ||| 
2021 ||| ai-upv at iberlef-2021 detoxis task: toxicity detection in immigration-related web news comments using transformers and statistical models. ||| angel felipe magnoss ||| o de paula ||| ipek baris schlicht ||| 
2018 ||| next item recommendation with self-attention. ||| shuai zhang ||| yi tay ||| lina yao ||| aixin sun ||| 
2021 ||| task adaptive pretraining of transformers for hostility detection. ||| tathagata raha ||| sayar ghosh roy ||| ujwal narayan ||| zubair abid ||| vasudeva varma ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| peng gao ||| minghang zheng ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2021 ||| deep transformers for fast small intestine grounding in capsule endoscope video. ||| xinkai zhao ||| chaowei fang ||| feng gao ||| de-jun fan ||| xutao lin ||| guanbin li ||| 
2020 ||| iitk at semeval-2020 task 10: transformers for emphasis selection. ||| vipul singhal ||| sahil dhull ||| rishabh agarwal ||| ashutosh modi ||| 
2018 ||| parallel attention mechanisms in neural machine translation. ||| julian r. medina ||| jugal kalita ||| 
2017 ||| brain inspired cognitive model with attention for self-driving cars. ||| shi-tao chen ||| songyi zhang ||| jinghao shang ||| badong chen ||| nanning zheng ||| 
2021 ||| regionvit: regional-to-local attention for vision transformers. ||| chun-fu chen ||| rameswar panda ||| quanfu fan ||| 
2020 ||| ctc-synchronous training for monotonic attention model. ||| hirofumi inaguma ||| masato mimura ||| tatsuya kawahara ||| 
2020 ||| leugan: low-light image enhancement by unsupervised generative attentional networks. ||| yangyang qu ||| chao liu ||| yongsheng ou ||| 
2020 ||| cross-modal self-attention distillation for prostate cancer segmentation. ||| guokai zhang ||| xiaoang shen ||| ye luo ||| jihao luo ||| zeju wang ||| weigang wang ||| binghui zhao ||| jianwei lu ||| 
2021 ||| attention-based distributed speech enhancement for unconstrained microphone arrays with varying number of nodes. ||| nicolas furnon ||| romain serizel ||| slim essid ||| irina illina ||| 
2022 ||| training vision transformers with only 2040 images. ||| yun-hao cao ||| hao yu ||| jianxin wu ||| 
2021 ||| audio transformers: transformer architectures for large scale audio understanding. adieu convolutions. ||| prateek verma ||| jonathan berger ||| 
2021 ||| spatial-temporal transformer for 3d point cloud sequences. ||| yimin wei ||| hao liu ||| tingting xie ||| qiuhong ke ||| yulan guo ||| 
2022 ||| visual attention analysis of pathologists examining whole slide images of prostate cancer. ||| souradeep chakraborty ||| ke ma ||| rajarsi gupta ||| beatrice knudsen ||| gregory j. zelinsky ||| joel h. saltz ||| dimitris samaras ||| 
2021 ||| voice quality and pitch features in transformer-based speech recognition. ||| guillermo c ||| mbara ||| jordi luque ||| mireia farr ||| s ||| 
2021 ||| loglab: attention-based labeling of log data anomalies via weak supervision. ||| thorsten wittkopp ||| philipp wiesner ||| dominik scheinert ||| alexander acker ||| 
2020 ||| mmm : exploring conditional multi-track music generation with the transformer. ||| jeffrey ens ||| philippe pasquier ||| 
2021 ||| methodology to assess quality, presence, empathy, attitude, and attention in social vr: international experiences use case. ||| marta orduna ||| pablo p ||| rez ||| jes ||| s guti ||| rrez ||| narciso garc ||| a ||| 
2021 ||| svt-net: a super light-weight network for large scale place recognition using sparse voxel transformers. ||| zhaoxin fan ||| zhenbo song ||| hongyan liu ||| jun he ||| xiaoyong du ||| 
2020 ||| t-recs: a transformer-based recommender generating textual explanations and integrating unsupervised language-based critiquing. ||| diego antognini ||| claudiu musat ||| boi faltings ||| 
2019 ||| cloze-driven pretraining of self-attention networks. ||| alexei baevski ||| sergey edunov ||| yinhan liu ||| luke zettlemoyer ||| michael auli ||| 
2019 ||| multi-task bidirectional transformer representations for irony detection. ||| chiyu zhang ||| muhammad abdul-mageed ||| 
2021 ||| trans4trans: efficient transformer for transparent object and semantic scene segmentation in real-world navigation assistance. ||| jiaming zhang ||| kailun yang ||| angela constantinescu ||| kunyu peng ||| karin m ||| ller ||| rainer stiefelhagen ||| 
2019 ||| syntax-enhanced self-attention-based semantic role labeling. ||| yue zhang ||| rui wang ||| luo si ||| 
2021 ||| improved transformer for high-resolution gans. ||| long zhao ||| zizhao zhang ||| ting chen ||| dimitris n. metaxas ||| han zhang ||| 
2022 ||| the principle of diversity: training stronger vision transformers calls for reducing all levels of redundancy. ||| tianlong chen ||| zhenyu zhang ||| yu cheng ||| ahmed hassan awadallah ||| zhangyang wang ||| 
2020 ||| fcanet: frequency channel attention networks. ||| zequn qin ||| pengyi zhang ||| fei wu ||| xi li ||| 
2021 ||| graph attention network for microwave imaging of brain anomaly. ||| ahmed al-saffar ||| lei guo ||| amin m. abbosh ||| 
2022 ||| table structure recognition with conditional attention. ||| bin xiao ||| murat simsek ||| burak kantarci ||| ala abu alkheir ||| 
2017 ||| what does attention in neural machine translation pay attention to? ||| hamidreza ghader ||| christof monz ||| 
2021 ||| ppt fusion: pyramid patch transformerfor a case study in image fusion. ||| yu fu ||| tianyang xu ||| xiaojun wu ||| josef kittler ||| 
2020 ||| fat albert: finding answers in large texts using semantic similarity attention layer based on bert. ||| omar mossad ||| amgad ahmed ||| anandharaju raju ||| hari karthikeyan ||| zayed ahmed ||| 
2020 ||| audio-visual event localization via recursive fusion by joint co-attention. ||| bin duan ||| hao tang ||| wei wang ||| ziliang zong ||| guowei yang ||| yan yan ||| 
2021 ||| a probabilistic hard attention model for sequentially observed scenes. ||| samrudhdhi b. rangrej ||| james j. clark ||| 
2019 ||| squeeze-and-attention networks for semantic segmentation. ||| zilong zhong ||| zhong qiu lin ||| rene bidart ||| xiaodan hu ||| ibrahim ben daya ||| jonathan li ||| alexander wong ||| 
2020 ||| vmrfanet: view-specific multi-receptive field attention network for person re-identification. ||| honglong cai ||| yuedong fang ||| zhiguan wang ||| tingchun yeh ||| jinxing cheng ||| 
2022 ||| attention-based proposals refinement for 3d object detection. ||| minh-quan dao ||| elwan h ||| ry ||| vincent fr ||| mont ||| 
2018 ||| attention-gan for object transfiguration in wild images. ||| xinyuan chen ||| chang xu ||| xiaokang yang ||| dacheng tao ||| 
2021 ||| exploring multi-task multi-lingual learning of transformer models for hate speech and offensive speech identification in social media. ||| sudhanshu mishra ||| shivangi prasad ||| shubhanshu mishra ||| 
2019 ||| capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation. ||| guangyi zhang ||| s. ali etemad ||| 
2018 ||| interpretable visual question answering by visual grounding from attention supervision mining. ||| yundong zhang ||| juan carlos niebles ||| alvaro soto ||| 
2020 ||| scouter: slot attention-based classifier for explainable image recognition. ||| liangzhi li ||| bowen wang ||| manisha verma ||| yuta nakashima ||| ryo kawasaki ||| hajime nagahara ||| 
2019 ||| interpretation of feature space using multi-channel attentional sub-networks. ||| masanari kimura ||| masayuki tanaka ||| 
2021 ||| multistream graph attention networks for wind speed forecasting. ||| dogan aykas ||| siamak mehrkanoon ||| 
2019 ||| self-attention based bilstm-cnn classifier for the prediction of ischemic and non-ischemic cardiomyopathy. ||| kavita dubey ||| anant agarwal ||| astitwa sarthak lathe ||| ranjeet kumar ||| vishal srivastava ||| 
2019 ||| humor detection: a transformer gets the last laugh. ||| orion weller ||| kevin d. seppi ||| 
2020 ||| scalar coupling constant prediction using graph embedding local attention encoder. ||| caiqing jian ||| xinyu cheng ||| jian zhang ||| lihui wang ||| 
2020 ||| one model to pronounce them all: multilingual grapheme-to-phoneme conversion with a transformer ensemble. ||| kaili vesik ||| muhammad abdul-mageed ||| miikka silfverberg ||| 
2020 ||| how to teach dnns to pay attention to the visual modality in speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2020 ||| where is the model looking at?-concentrate and explain the network attention. ||| wenjia xu ||| jiuniu wang ||| yang wang ||| guangluan xu ||| wei dai ||| yirong wu ||| 
2019 ||| knowledge enhanced attention for robust natural language inference. ||| alexander hanbo li ||| abhinav sethy ||| 
2021 ||| attention-based neural re-ranking approach for next city in trip recommendations. ||| aleksandr petrov ||| yuriy makarov ||| 
2021 ||| probing image-language transformers for verb understanding. ||| lisa anne hendricks ||| aida nematzadeh ||| 
2021 ||| attention weights in transformer nmt fail aligning words between sequences but largely explain model predictions. ||| javier ferrando ||| marta r. costa-juss ||| 
2020 ||| communities of attention networks: introducing qualitative and conversational perspectives for altmetrics. ||| ronaldo ferreira araujo ||| 
2019 ||| a sensitivity analysis of attention-gated convolutional neural networks for sentence classification. ||| yang liu ||| jianpeng zhang ||| chao gao ||| jinghua qu ||| lixin ji ||| 
2021 ||| iris presentation attack detection by attention-based and deep pixel-wise binary supervision network. ||| meiling fang ||| naser damer ||| fadi boutros ||| florian kirchbuchner ||| arjan kuijper ||| 
2021 ||| easy and efficient transformer : scalable inference solution for large nlp mode. ||| gongzheng li ||| yadong xi ||| jingzhen ding ||| duan wang ||| bai liu ||| changjie fan ||| xiaoxi mao ||| zeng zhao ||| 
2021 ||| a dynamic residual self-attention network for lightweight single image super-resolution. ||| karam park ||| jae woong soh ||| nam ik cho ||| 
2021 ||| prostformer: pre-trained progressive space-time self-attention model for traffic flow forecasting. ||| xiao yan ||| xianghua gan ||| jingjing tang ||| rui wang ||| 
2018 ||| character-level language modeling with deeper self-attention. ||| rami al-rfou ||| dokook choe ||| noah constant ||| mandy guo ||| llion jones ||| 
2021 ||| crossatnet - a novel cross-attention based framework for sketch-based image retrieval. ||| ushasi chaudhuri ||| biplab banerjee ||| avik bhattacharya ||| mihai datcu ||| 
2019 ||| tanda: transfer and adapt pre-trained transformer models for answer sentence selection. ||| siddhant garg ||| thuy vu ||| alessandro moschitti ||| 
2019 ||| siamese attentional keypoint network for high performance visual tracking. ||| peng gao ||| yipeng ma ||| ruyue yuan ||| liyi xiao ||| fei wang ||| 
2022 ||| extreme precipitation forecasting using attention augmented convolutions. ||| weichen huang ||| 
2021 ||| efficient vision transformers via fine-grained manifold distillation. ||| ding jia ||| kai han ||| yunhe wang ||| yehui tang ||| jianyuan guo ||| chao zhang ||| dacheng tao ||| 
2021 ||| inpainting transformer for anomaly detection. ||| jonathan pirnay ||| keng chai ||| 
2021 ||| mdmmt: multidomain multimodal transformer for video retrieval. ||| maksim dzabraev ||| maksim kalashnikov ||| stepan komkov ||| aleksandr petiushko ||| 
2017 ||| modality-specific cross-modal similarity measurement with recurrent attention network. ||| yuxin peng ||| jinwei qi ||| yuxin yuan ||| 
2021 ||| incorporating convolution designs into visual transformers. ||| kun yuan ||| shaopeng guo ||| ziwei liu ||| aojun zhou ||| fengwei yu ||| wei wu ||| 
2020 ||| using a bi-directional lstm model with attention mechanism trained on midi data for generating unique music. ||| ashish ranjan ||| varun nagesh jolly behera ||| motahar reza ||| 
2022 ||| hts-at: a hierarchical token-semantic audio transformer for sound classification and detection. ||| ke chen ||| xingjian du ||| bilei zhu ||| zejun ma ||| taylor berg-kirkpatrick ||| shlomo dubnov ||| 
2022 ||| transdarc: transformer-based driver activity recognition with latent space feature calibration. ||| kunyu peng ||| alina roitberg ||| kailun yang ||| jiaming zhang ||| rainer stiefelhagen ||| 
2021 ||| an emd-based method for the detection of power transformer faults with a hierarchical ensemble classifier. ||| shoaib meraj sami ||| mohammed imamul hassan bhuiyan ||| 
2021 ||| each attribute matters: contrastive attention for sentence-based image editing. ||| liuqing zhao ||| fan lyu ||| fuyuan hu ||| kaizhu huang ||| fenglei xu ||| linyan li ||| 
2021 ||| vidtr: video transformer without convolutions. ||| xinyu li ||| yanyi zhang ||| chunhui liu ||| bing shuai ||| yi zhu ||| biagio brattoli ||| hao chen ||| ivan marsic ||| joseph tighe ||| 
2022 ||| cnns and transformers perceive hybrid images similar to humans. ||| ali borji ||| 
2020 ||| attention dynamics on the chinese social media sina weibo during the covid-19 pandemic. ||| hao cui ||| j ||| nos kert ||| sz ||| 
2019 ||| selective attention based graph convolutional networks for aspect-level sentiment classification. ||| xiaochen hou ||| jing huang ||| guangtao wang ||| kevin huang ||| xiaodong he ||| bowen zhou ||| 
2020 ||| entity linking via dual and cross-attention encoders. ||| oshin agarwal ||| daniel m. bikel ||| 
2018 ||| multi-level structured self-attentions for distantly supervised relation extraction. ||| jinhua du ||| jingguang han ||| andy way ||| dadong wan ||| 
2021 ||| hierarchical transformer-based large-context end-to-end asr with large-context knowledge distillation. ||| ryo masumura ||| naoki makishima ||| mana ihori ||| akihiko takashima ||| tomohiro tanaka ||| shota orihashi ||| 
2021 ||| where and when: space-time attention for audio-visual explanations. ||| yanbei chen ||| thomas hummel ||| a. sophia koepke ||| zeynep akata ||| 
2021 ||| tecanet: temporal-contextual attention network for environment-aware speech dereverberation. ||| helin wang ||| bo wu ||| lianwu chen ||| meng yu ||| jianwei yu ||| yong xu ||| shi-xiong zhang ||| chao weng ||| dan su ||| dong yu ||| 
2019 ||| transformer-xl: attentive language models beyond a fixed-length context. ||| zihang dai ||| zhilin yang ||| yiming yang ||| jaime g. carbonell ||| quoc v. le ||| ruslan salakhutdinov ||| 
2018 ||| uni-due student team: tackling fact checking through decomposable attention neural network. ||| jan kowollik ||| ahmet aker ||| 
2021 ||| pisltrc: position-informed sign language transformer with content-aware convolution. ||| pan xie ||| mengyi zhao ||| xiaohui hu ||| 
2020 ||| face hallucination using split-attention in split-attention network. ||| yuanzhi wang ||| tao lu ||| yu wang ||| yanduo zhang ||| 
2021 ||| medusa: multi-scale encoder-decoder self-attention deep neural network architecture for medical image analysis. ||| hossein aboutalebi ||| maya pavlova ||| hayden gunraj ||| mohammad javad shafiee ||| ali sabri ||| amer alaref ||| alexander wong ||| 
2020 ||| m3d-cam: a pytorch library to generate 3d data attention maps for medical deep learning. ||| karol gotkowski ||| camila gonz ||| lez ||| andreas bucher ||| anirban mukhopadhyay ||| 
2022 ||| look closer: bridging egocentric and third-person views with transformers for robotic manipulation. ||| rishabh jangir ||| nicklas hansen ||| sambaran ghosal ||| mohit jain ||| xiaolong wang ||| 
2021 ||| subdimensional expansion using attention-based learning for multi-agent path finding. ||| lakshay virmani ||| zhongqiang ren ||| sivakumar rathinam ||| howie choset ||| 
2020 ||| decoupled spatial-temporal attention network for skeleton-based action recognition. ||| lei shi ||| yifan zhang ||| jian cheng ||| hanqing lu ||| 
2021 ||| synthesizing abstract transformers. ||| pankaj kumar kalita ||| sujit kumar muduli ||| loris d'antoni ||| thomas w. reps ||| subhajit roy ||| 
2021 ||| distract your attention: multi-head cross attention network for facial expression recognition. ||| zhengyao wen ||| wenzhong lin ||| tao wang ||| ge xu ||| 
2019 ||| graph transformer for graph-to-sequence learning. ||| deng cai ||| wai lam ||| 
2022 ||| investigating expressiveness of transformer in spectral domain for graphs. ||| anson bastos ||| abhishek nadgeri ||| kuldeep singh ||| hiroki kanezashi ||| toyotaro suzumura ||| isaiah onando mulang' ||| 
2021 ||| attanet: attention-augmented network for fast and accurate scene parsing. ||| qi song ||| kangfu mei ||| rui huang ||| 
2019 ||| eaten: entity-aware attention for single shot visual text extraction. ||| he guo ||| xiameng qin ||| jiaming liu ||| junyu han ||| jingtuo liu ||| errui ding ||| 
2020 ||| patient cohort retrieval using transformer language models. ||| sarvesh soni ||| kirk roberts ||| 
2020 ||| multi-attention-network for semantic segmentation of high-resolution remote sensing images. ||| rui li ||| shunyi zheng ||| chenxi duan ||| jianlin su ||| 
2017 ||| why do men get more attention? exploring factors behind success in an online design community. ||| johannes wachs ||| anik |||  hann ||| k ||| andr ||| s v ||| r ||| s ||| b ||| lint dar ||| czy ||| 
2019 ||| the resale price prediction of secondhand jewelry items using a multi-modal deep model with iterative co-attention. ||| yusuke yamaura ||| nobuya kanemaki ||| yukihiro tsuboshita ||| 
2021 ||| hybrid local-global transformer for image dehazing. ||| dong zhao ||| jia li ||| hongyu li ||| long xu ||| 
2020 ||| modeling long-term and short-term interests with parallel attentions for session-based recommendation. ||| jing zhu ||| yanan xu ||| yanmin zhu ||| 
2022 ||| gradvit: gradient inversion of vision transformers. ||| ali hatamizadeh ||| hongxu yin ||| holger roth ||| wenqi li ||| jan kautz ||| daguang xu ||| pavlo molchanov ||| 
2021 ||| u2-former: a nested u-shaped transformer for image restoration. ||| haobo ji ||| xin feng ||| wenjie pei ||| jinxing li ||| guangming lu ||| 
2021 ||| multi-scale attention neural network for acoustic echo cancellation. ||| lu ma ||| song yang ||| yaguang gong ||| zhongqin wu ||| 
2021 ||| dancenet3d: music based dance generation with parametric motion transformer. ||| buyu li ||| yongchi zhao ||| lu sheng ||| 
2020 ||| economical visual attention test for elderly drivers. ||| akinari onishi ||| 
2018 ||| cross-relation cross-bag attention for distantly-supervised relation extraction. ||| yujin yuan ||| liyuan liu ||| siliang tang ||| zhongfei zhang ||| yueting zhuang ||| shiliang pu ||| fei wu ||| xiang ren ||| 
2020 ||| perm2vec: graph permutation selection for decoding of error correction codes using self-attention. ||| nir raviv ||| avi caciularu ||| tomer raviv ||| jacob goldberger ||| yair be'ery ||| 
2017 ||| cross-domain image retrieval with attention modeling. ||| xin ji ||| wei wang ||| meihui zhang ||| yang yang ||| 
2020 ||| up-detr: unsupervised pre-training for object detection with transformers. ||| zhigang dai ||| bolun cai ||| yugeng lin ||| junying chen ||| 
2020 ||| where to look and how to describe: fashion image retrieval with an attentional heterogeneous bilinear network. ||| haibo su ||| peng wang ||| lingqiao liu ||| hui li ||| zhen li ||| yanning zhang ||| 
2021 ||| structure-regularized attention for deformable object representation. ||| shenao zhang ||| li shen ||| zhifeng li ||| wei liu ||| 
2022 ||| ensemble transformer for efficient and accurate ranking tasks: an application to question answering systems. ||| yoshitomo matsubara ||| luca soldaini ||| eric lind ||| alessandro moschitti ||| 
2021 ||| boosting transformers for job expression extraction and classification in a low-resource setting. ||| lukas lange ||| heike adel ||| jannik str ||| tgen ||| 
2021 ||| a survey of visual transformers. ||| yang liu ||| yao zhang ||| yixin wang ||| feng hou ||| jin yuan ||| jiang tian ||| yang zhang ||| zhongchao shi ||| jianping fan ||| zhiqiang he ||| 
2021 ||| sleep staging based on serialized dual attention network. ||| huafeng wang ||| chonggang lu ||| qi zhang ||| zhimin hu ||| xiaodong yuan ||| pingshu zhang ||| wanquan liu ||| 
2021 ||| vitgan: training gans with vision transformers. ||| kwonjoon lee ||| huiwen chang ||| lu jiang ||| han zhang ||| zhuowen tu ||| ce liu ||| 
2020 ||| investigating african-american vernacular english in transformer-based text generation. ||| sophie groenwold ||| lily ou ||| aesha parekh ||| samhita honnavalli ||| sharon levy ||| diba mirza ||| william yang wang ||| 
2019 ||| interactive variance attention based online spoiler detection for time-sync comments. ||| wenmian yang ||| weijia jia ||| wenyuan gao ||| xiaojie zhou ||| yutao luo ||| 
2021 ||| description-based label attention classifier for explainable icd-9 classification. ||| malte feucht ||| zhiliang wu ||| sophia althammer ||| volker tresp ||| 
2020 ||| attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view. ||| shen wang ||| jibing gong ||| jinlong wang ||| wenzheng feng ||| hao peng ||| jie tang ||| philip s. yu ||| 
2021 ||| mcl@iitk at semeval-2021 task 2: multilingual and cross-lingual word-in-context disambiguation using augmented data, signals, and transformers. ||| rohan gupta ||| jay mundra ||| deepak mahajan ||| ashutosh modi ||| 
2021 ||| a graph attention learning approach to antenna tilt optimization. ||| yifei jin ||| filippo vannella ||| maxime bouton ||| jaeseong jeong ||| ezeddin al hakim ||| 
2020 ||| concatenated attention neural network for image restoration. ||| yingjie tian ||| yiqi wang ||| linrui yang ||| zhiquan qi ||| 
2021 ||| conditional attention networks for distilling knowledge graphs in recommendation. ||| ke tu ||| peng cui ||| daixin wang ||| zhiqiang zhang ||| jun zhou ||| yuan qi ||| wenwu zhu ||| 
2017 ||| skeleton based human action recognition with global context-aware attention lstm networks. ||| jun liu ||| gang wang ||| ling-yu duan ||| ping hu ||| alex c. kot ||| 
2017 ||| temporal attention augmented bilinear network for financial time-series data analysis. ||| dat thanh tran ||| alexandros iosifidis ||| juho kanniainen ||| moncef gabbouj ||| 
2021 ||| replica: enhanced feature pyramid network by local image translation and conjunct attention for high-resolution breast tumor detection. ||| yifan zhang ||| haoyu dong ||| nicholas konz ||| hanxue gu ||| maciej a. mazurowski ||| 
2020 ||| spike-triggered non-autoregressive transformer for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| ye bai ||| shuai zhang ||| zhengqi wen ||| 
2021 ||| estimating articulatory movements in speech production with transformer networks. ||| sathvik udupa ||| anwesha roy ||| abhayjeet singh ||| aravind illa ||| prasanta kumar ghosh ||| 
2021 ||| opinion extraction as a structured sentiment analysis using transformers. ||| yucheng liu ||| tian zhu ||| 
2021 ||| ptt: point-track-transformer module for 3d single object tracking in point clouds. ||| jiayao shan ||| sifan zhou ||| zheng fang ||| yubo cui ||| 
2019 ||| guided attention network for object detection and counting on drones. ||| yuanqiang cai ||| dawei du ||| libo zhang ||| longyin wen ||| weiqiang wang ||| yanjun wu ||| siwei lyu ||| 
2020 ||| bridging text and video: a universal multimodal transformer for video-audio scene-aware dialog. ||| zekang li ||| zongjia li ||| jinchao zhang ||| yang feng ||| cheng niu ||| jie zhou ||| 
2020 ||| inside: steering spatial attention with non-imaging information in cnns. ||| grzegorz jacenk ||| w ||| alison q. o'neil ||| brian mohr ||| sotirios a. tsaftaris ||| 
2020 ||| transformers are better than humans at identifying generated text. ||| antonis maronikolakis ||| mark stevenson ||| hinrich sch ||| tze ||| 
2020 ||| hyperspectral image classification with attention aided cnns. ||| renlong hang ||| zhu li ||| qingshan liu ||| pedram ghamisi ||| shuvra s. bhattacharyya ||| 
2020 ||| on the dynamics of training attention models. ||| haoye lu ||| yongyi mao ||| amiya nayak ||| 
2021 ||| visual composite set detection using part-and-sum transformers. ||| qi dong ||| zhuowen tu ||| haofu liao ||| yuting zhang ||| vijay mahadevan ||| stefano soatto ||| 
2020 ||| guided transformer: leveraging multiple external sources for representation learning in conversational search. ||| helia hashemi ||| hamed zamani ||| w. bruce croft ||| 
2019 ||| stack-vs: stacked visual-semantic attention for image caption generation. ||| wei wei ||| ling cheng ||| xianling mao ||| guangyou zhou ||| feida zhu ||| 
2020 ||| cognitive-driven convolutional beamforming using eeg-based auditory attention decoding. ||| ali aroudi ||| marc delcroix ||| tomohiro nakatani ||| keisuke kinoshita ||| shoko araki ||| simon doclo ||| 
2020 ||| end-to-end neural transformer based spoken language understanding. ||| martin radfar ||| athanasios mouchtaris ||| siegfried kunzmann ||| 
2021 ||| infrared small-dim target detection with transformer under complex backgrounds. ||| fangcen liu ||| chenqiang gao ||| fang chen ||| deyu meng ||| wangmeng zuo ||| xinbo gao ||| 
2020 ||| t-vse: transformer-based visual semantic embedding. ||| muhammet bastan ||| arnau ramisa ||| mehmet tek ||| 
2021 ||| mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction. ||| yuanhao cai ||| jing lin ||| xiaowan hu ||| haoqian wang ||| xin yuan ||| yulun zhang ||| radu timofte ||| luc van gool ||| 
2020 ||| co-saliency detection with co-attention fully convolutional network. ||| guangshuai gao ||| wenting zhao ||| qingjie liu ||| yunhong wang ||| 
2021 ||| case relation transformer: a crossmodal language generation model for fetching instructions. ||| motonari kambara ||| komei sugiura ||| 
2019 ||| attention based pruning for shift networks. ||| ghouthi boukli hacene ||| carlos eduardo rosar k ||| s lassance ||| vincent gripon ||| matthieu courbariaux ||| yoshua bengio ||| 
2021 ||| attention based video summaries of live online zoom classes. ||| hyowon lee ||| mingming liu ||| hamza riaz ||| navaneethan rajasekaren ||| michael scriney ||| alan f. smeaton ||| 
2019 ||| bertqa - attention on steroids. ||| ankit chadha ||| rewa sood ||| 
2017 ||| attention clusters: purely attention based local feature integration for video classification. ||| xiang long ||| chuang gan ||| gerard de melo ||| jiajun wu ||| xiao liu ||| shilei wen ||| 
2021 ||| indt5: a text-to-text transformer for 10 indigenous languages. ||| el moatez billah nagoudi ||| wei-rui chen ||| muhammad abdul-mageed ||| hasan cavusoglu ||| 
2021 ||| attention based semantic segmentation on uav dataset for natural disaster damage assessment. ||| tashnim chowdhury ||| maryam rahnemoonfar ||| 
2021 ||| transformer-based behavioral representation learning enables transfer learning for mobile sensing in small datasets. ||| mike a. merrill ||| tim althoff ||| 
2022 ||| raytran: 3d pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. ||| michal j. tyszkiewicz ||| kevis-kokitsi maninis ||| stefan popov ||| vittorio ferrari ||| 
2022 ||| persformer: 3d lane detection via perspective transformer and the openlane benchmark. ||| li chen ||| chonghao sima ||| yang li ||| zehan zheng ||| jiajie xu ||| xiangwei geng ||| hongyang li ||| conghui he ||| jianping shi ||| yu qiao ||| junchi yan ||| 
2020 ||| visual-semantic graph attention network for human-object interaction detection. ||| zhijun liang ||| yisheng guan ||| juan rojas ||| 
2021 ||| exercise? i thought you said 'extra fries': leveraging sentence demarcations and multi-hop attention for meme affect analysis. ||| shraman pramanick ||| md. shad akhtar ||| tanmoy chakraborty ||| 
2021 ||| attnmove: history enhanced trajectory recovery via attentional network. ||| tong xia ||| yunhan qi ||| jie feng ||| fengli xu ||| funing sun ||| diansheng guo ||| yong li ||| 
2021 ||| staf: a spatio-temporal attention fusion network for few-shot video classification. ||| rex liu ||| huanle zhang ||| hamed pirsiavash ||| xin liu ||| 
2021 ||| redesigning the transformer architecture with insights from multi-particle dynamical systems. ||| subhabrata dutta ||| tanya gautam ||| soumen chakrabarti ||| tanmoy chakraborty ||| 
2021 ||| multi-branch with attention network for hand-based person recognition. ||| nathanael l. baisa ||| bryan m. williams ||| hossein rahmani ||| plamen p. angelov ||| sue black ||| 
2021 ||| a study of social and behavioral determinants of health in lung cancer patients using transformers-based natural language processing models. ||| zehao yu ||| xi yang ||| chong dang ||| songzi wu ||| prakash adekkanattu ||| jyotishman pathak ||| thomas j. george ||| william r. hogan ||| yi guo ||| jiang bian ||| yonghui wu ||| 
2021 ||| packet routing with graph attention multi-agent reinforcement learning. ||| xuan mai ||| quanzhi fu ||| yi chen ||| 
2021 ||| parameter selection: why we should pay more attention to it. ||| jie-jyun liu ||| tsung-han yang ||| si-an chen ||| chih-jen lin ||| 
2017 ||| ruminating reader: reasoning with gated multi-hop attention. ||| yichen gong ||| samuel r. bowman ||| 
2018 ||| stacked cross attention for image-text matching. ||| kuang-huei lee ||| xi chen ||| gang hua ||| houdong hu ||| xiaodong he ||| 
2022 ||| dxm-transfuse u-net: dual cross-modal transformer fusion u-net for automated nerve identification. ||| baijun xie ||| gary milam ||| bo ning ||| jaepyeong cha ||| chung hyuk park ||| 
2021 ||| medical image segmentation using squeeze-and-expansion transformers. ||| shaohua li ||| xiuchao sui ||| xiangde luo ||| xinxing xu ||| yong liu ||| rick siow mong goh ||| 
2021 ||| detection of transformer winding axial displacement by kirchhoff and delay and sum radar imaging algorithms. ||| mohammad s. golsorkhi ||| raziyeh mosayebi ||| maryam a. hejazi ||| gevorg b. gharehpetian ||| hamid sheikhzadeh ||| 
2020 ||| attention-based graph resnet for motor intent detection from raw eeg signals. ||| shuyue jia ||| yimin hou ||| yan shi ||| yang li ||| 
2021 ||| bam: a lightweight and efficient balanced attention mechanism for single image super resolution. ||| fanyi wang ||| haotian hu ||| cheng shen ||| 
2020 ||| manipulated face detector: joint spatial and frequency domain attention network. ||| zehao chen ||| hua yang ||| 
2021 ||| on pursuit of designing multi-modal transformer for video grounding. ||| meng cao ||| long chen ||| mike zheng shou ||| can zhang ||| yuexian zou ||| 
2020 ||| segattngan: text to image generation with segmentation attention. ||| yuchuan gou ||| qiancheng wu ||| minghao li ||| bo gong ||| mei han ||| 
2018 ||| interpreting recurrent and attention-based neural models: a case study on natural language inference. ||| reza ghaeini ||| xiaoli z. fern ||| prasad tadepalli ||| 
2021 ||| towards reinforcement learning for pivot-based neural machine translation with non-autoregressive transformer. ||| evgeniia tokarchuk ||| jan rosendahl ||| weiyue wang ||| pavel petrushkov ||| tomer lancewicki ||| shahram khadivi ||| hermann ney ||| 
2021 ||| a comparative study of transformers on word sense disambiguation. ||| avi chawla ||| nidhi mulay ||| vikas bishnoi ||| gaurav dhama ||| anil kumar singh ||| 
2018 ||| combining pyramid pooling and attention mechanism for pelvic mr image semantic segmentaion. ||| ting-ting liang ||| satoshi tsutsui ||| liangcai gao ||| jing-jing lu ||| mengyan sun ||| 
2022 ||| cmkd: cnn/transformer-based cross-model knowledge distillation for audio classification. ||| yuan gong ||| sameer khurana ||| andrew rouditchenko ||| james r. glass ||| 
2021 ||| relaxed transformer decoders for direct action proposal generation. ||| jing tan ||| jiaqi tang ||| limin wang ||| gangshan wu ||| 
2019 ||| collaborative self-attention for recommender systems. ||| 
2022 ||| botnets breaking transformers: localization of power botnet attacks against the distribution grid. ||| lynn pepin ||| lizhi wang ||| jiangwei wang ||| songyang han ||| pranav pishawikar ||| amir herzberg ||| peng zhang ||| fei miao ||| 
2021 ||| exploring transformer based models to identify hate speech and offensive content in english and indo-aryan languages. ||| somnath banerjee ||| maulindu sarkar ||| nancy agrawal ||| punyajoy saha ||| mithun das ||| 
2021 ||| agkd-bml: defense against adversarial attack by attention guided knowledge distillation and bi-directional metric learning. ||| hong wang ||| yuefan deng ||| shinjae yoo ||| haibin ling ||| yuewei lin ||| 
2020 ||| end to end dialogue transformer. ||| ondrej mekota ||| memduh g ||| kirmak ||| petr laitoch ||| 
2020 ||| neutral theory for competing attention in social networks. ||| carlos a. plata ||| emanuele pigani ||| sandro azaele ||| violeta calleja-solanas ||| mar ||| a j. palazzi ||| albert sol ||| -ribalta ||| sandro meloni ||| javier borge-holthoefer ||| samir suweis ||| 
2021 ||| something old, something new: grammar-based ccg parsing with transformer models. ||| stephen clark ||| 
2017 ||| face attention network: an effective face detector for the occluded faces. ||| jianfeng wang ||| ye yuan ||| gang yu ||| 
2019 ||| region attention networks for pose and occlusion robust facial expression recognition. ||| kai wang ||| xiaojiang peng ||| jianfei yang ||| debin meng ||| yu qiao ||| 
2021 ||| haconvgnn: hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks. ||| xuye liu ||| dakuo wang ||| april yi wang ||| lingfei wu ||| 
2019 ||| attention-based method for categorizing different types of online harassment language. ||| christos karatsalos ||| yannis panagiotakis ||| 
2019 ||| drug-drug adverse effect prediction with graph co-attention. ||| andreea deac ||| yu-hsiang huang ||| petar velickovic ||| pietro li ||| jian tang ||| 
2022 ||| pmp-net++: point cloud completion by transformer-enhanced multi-step point moving paths. ||| xin wen ||| peng xiang ||| zhizhong han ||| yan-pei cao ||| pengfei wan ||| wen zheng ||| yu-shen liu ||| 
2021 ||| ufo-vit: high performance linear vision transformer without softmax. ||| jeong-geun song ||| 
2021 ||| multi-channel transformer transducer for speech recognition. ||| feng-ju chang ||| martin radfar ||| athanasios mouchtaris ||| maurizio omologo ||| 
2021 ||| complexity-based partitioning of csfi problem instances with transformers. ||| luca benedetto ||| paolo fantozzi ||| luigi laura ||| 
2020 ||| concentrated multi-grained multi-attention network for video based person re-identification. ||| panwen hu ||| jiazhen liu ||| rui huang ||| 
2020 ||| unsupervised foveal vision neural networks with top-down attention. ||| ryan burt ||| nina n. thigpen ||| andreas keil ||| jos |||  c. pr ||| ncipe ||| 
2021 ||| brain tumors classification for mr images based on attention guided deep learning model. ||| yuhao zhang ||| shuhang wang ||| haoxiang wu ||| kejia hu ||| shufan ji ||| 
2020 ||| multiresolution and multimodal speech recognition with transformers. ||| georgios paraskevopoulos ||| srinivas parthasarathy ||| aparna khare ||| shiva sundaram ||| 
2020 ||| conformer: convolution-augmented transformer for speech recognition. ||| anmol gulati ||| james qin ||| chung-cheng chiu ||| niki parmar ||| yu zhang ||| jiahui yu ||| wei han ||| shibo wang ||| zhengdong zhang ||| yonghui wu ||| ruoming pang ||| 
2017 ||| a regularized framework for sparse and structured neural attention. ||| vlad niculae ||| mathieu blondel ||| 
2021 ||| global-local transformer for brain age estimation. ||| sheng he ||| patricia ellen grant ||| yangming ou ||| 
2020 ||| diablo: dictionary-based attention block for deep metric learning. ||| pierre jacob ||| david picard ||| aymeric histace ||| edouard klein ||| 
2019 ||| inter and intra document attention for depression risk assessment. ||| diego maupom ||| marc queudot ||| marie-jean meurs ||| 
2020 ||| attention-guided generative adversarial network to address atypical anatomy in modality transfer. ||| hajar emami ||| ming dong ||| carri k. glide-hurst ||| 
2019 ||| denseattentionseg: segment hands from interacted objects using depth input. ||| zihao bo ||| hang zhang ||| jun-hai yong ||| feng xu ||| 
2021 ||| stochastic transformer networks with linear competing units: application to end-to-end sl translation. ||| andreas voskou ||| konstantinos p. panousis ||| dimitrios kosmopoulos ||| dimitris n. metaxas ||| sotirios chatzis ||| 
2020 ||| learning dual semantic relations with graph attention for image-text matching. ||| keyu wen ||| xiaodong gu ||| qingrong cheng ||| 
2021 ||| sam: a self-adaptive attention module for context-aware recommendation system. ||| jiabin liu ||| zheng wei ||| zhengpin li ||| xiaojun mao ||| jian wang ||| zhongyu wei ||| qi zhang ||| 
2019 ||| text steganalysis with attentional lstm-cnn. ||| yongjian bao ||| hao yang ||| zhongliang yang ||| sheng liu ||| yongfeng huang ||| 
2021 ||| gn-transformer: fusing sequence and graph representation for improved code summarization. ||| junyan cheng ||| iordanis fostiropoulos ||| barry w. boehm ||| 
2021 ||| grounding dialogue systems via knowledge graph aware decoding with pre-trained transformers. ||| debanjan chaudhuri ||| md. rashad al hasan rony ||| jens lehmann ||| 
2020 ||| non-autoregressive transformer asr with ctc-enhanced decoder input. ||| xingchen song ||| zhiyong wu ||| yiheng huang ||| chao weng ||| dan su ||| helen meng ||| 
2019 ||| hierarchical transformers for long document classification. ||| raghavendra pappagari ||| piotr zelasko ||| jes ||| s villalba ||| yishay carmiel ||| najim dehak ||| 
2019 ||| da-refinenet:a dual input whole slide image segmentation algorithm based on attention. ||| ziqiang li ||| rentuo tao ||| qianrun wu ||| bin li ||| 
2020 ||| self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. ||| yude wang ||| jie zhang ||| meina kan ||| shiguang shan ||| xilin chen ||| 
2020 ||| denoising pre-training and data augmentation strategies for enhanced rdf verbalization with transformers. ||| sebastien montella ||| betty fabre ||| tanguy urvoy ||| johannes heinecke ||| lina maria rojas-barahona ||| 
2020 ||| paying per-label attention for multi-label extraction from radiology reports. ||| patrick schrempf ||| hannah watson ||| shadia mikhael ||| maciej pajak ||| mat ||| s falis ||| aneta lisowska ||| keith w. muir ||| david harris-birtill ||| alison q. o'neil ||| 
2019 ||| how much research output from india gets social media attention? ||| sumit kumar banshal ||| vivek kumar singh ||| pranab k. muhuri ||| philipp mayr ||| 
2020 ||| multi-pass transformer for machine translation. ||| peng gao ||| chiori hori ||| shijie geng ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| tensorcoder: dimension-wise attention via tensor representation for natural language modeling. ||| shuai zhang ||| peng zhang ||| xindian ma ||| junqiu wei ||| ningning wang ||| qun liu ||| 
2017 ||| neobility at semeval-2017 task 1: an attention-based sentence similarity model. ||| wenli zhuang ||| ernie chang ||| 
2020 ||| a novel global spatial attention mechanism in convolutional neural network for medical image classification. ||| linchuan xu ||| jun huang ||| atsushi nitanda ||| ryo asaoka ||| kenji yamanishi ||| 
2019 ||| stabilizing transformers for reinforcement learning. ||| emilio parisotto ||| h. francis song ||| jack w. rae ||| razvan pascanu ||| aglar g ||| l ||| ehre ||| siddhant m. jayakumar ||| max jaderberg ||| raphael lopez kaufman ||| aidan clark ||| seb noury ||| matthew m. botvinick ||| nicolas heess ||| raia hadsell ||| 
2020 ||| detecting expressions with multimodal transformers. ||| srinivas parthasarathy ||| shiva sundaram ||| 
2020 ||| unpaired image enhancement with quality-attention generative adversarial network. ||| zhangkai ni ||| wenhan yang ||| shiqi wang ||| lin ma ||| sam kwong ||| 
2021 ||| speech emotion recognition with multiscale area attention and data augmentation. ||| mingke xu ||| fan zhang ||| xiaodong cui ||| wei zhang ||| 
2019 ||| class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment. ||| sagi eppel ||| 
2018 ||| self-attentional acoustic models. ||| matthias sperber ||| jan niehues ||| graham neubig ||| sebastian st ||| ker ||| alex waibel ||| 
2021 ||| combining efficientnet and vision transformers for video deepfake detection. ||| davide coccomini ||| nicola messina ||| claudio gennaro ||| fabrizio falchi ||| 
2022 ||| d^2etr: decoder-only detr with computationally efficient cross-scale attention. ||| junyu lin ||| xiaofeng mao ||| yuefeng chen ||| lei xu ||| yuan he ||| hui xue ||| 
2020 ||| flat: chinese ner using flat-lattice transformer. ||| xiaonan li ||| hang yan ||| xipeng qiu ||| xuanjing huang ||| 
2021 ||| mltr: multi-label classification with transformer. ||| xing cheng ||| hezheng lin ||| xiangyu wu ||| fan yang ||| dong shen ||| zhongyuan wang ||| nian shi ||| honglin liu ||| 
2021 ||| thg: transformer with hyperbolic geometry. ||| zhe liu ||| yibin xu ||| 
2022 ||| end-to-end human-gaze-target detection with transformers. ||| danyang tu ||| xiongkuo min ||| huiyu duan ||| guodong guo ||| guangtao zhai ||| wei shen ||| 
2020 ||| multivariate time-series anomaly detection via graph attention network. ||| hang zhao ||| yujing wang ||| juanyong duan ||| congrui huang ||| defu cao ||| yunhai tong ||| bixiong xu ||| jing bai ||| jie tong ||| qi zhang ||| 
2020 ||| human action performance using deep neuro-fuzzy recurrent attention model. ||| nihar bendre ||| nima ebadi ||| paul rad ||| 
2021 ||| relative molecule self-attention transformer. ||| lukasz maziarka ||| dawid majchrowski ||| tomasz danel ||| piotr gainski ||| jacek tabor ||| igor t. podolak ||| pawel morkisz ||| stanislaw jastrzebski ||| 
2021 ||| ecg-based heart arrhythmia diagnosis through attentional convolutional neural networks. ||| ziyu liu ||| xiang zhang ||| 
2021 ||| classification of breast cancer lesions in ultrasound images by using attention layer and loss ensembles in deep convolutional neural networks. ||| elham yousef kalafi ||| ata jodeiri ||| seyed kamaledin setarehdan ||| ng wei lin ||| kartini binti rahman ||| nur aishah taib ||| sarinder kaur dhillon ||| 
2018 ||| linguistically-informed self-attention for semantic role labeling. ||| emma strubell ||| patrick verga ||| daniel andor ||| david weiss ||| andrew mccallum ||| 
2020 ||| query-key normalization for transformers. ||| alex henry ||| prudhvi raj dachapally ||| shubham shantaram pawar ||| yuxuan chen ||| 
2022 ||| do transformers use variable binding? ||| tommi gr ||| ndahl ||| n. asokan ||| 
2018 ||| bi-directional block self-attention for fast and memory-efficient sequence modeling. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2021 ||| have attention heads in bert learned constituency grammar? ||| ziyang luo ||| 
2021 ||| an attention score based attacker for black-box nlp classifier. ||| yueyang liu ||| hunmin lee ||| zhipeng cai ||| 
2021 ||| gated transformer networks for multivariate time series classification. ||| minghao liu ||| shengqi ren ||| siyuan ma ||| jiahui jiao ||| yizhou chen ||| zhiguang wang ||| wei song ||| 
2019 ||| l2g auto-encoder: understanding point clouds by local-to-global reconstruction with hierarchical self-attention. ||| xinhai liu ||| zhizhong han ||| xin wen ||| yu-shen liu ||| matthias zwicker ||| 
2021 ||| investigating the limitations of the transformers with simple arithmetic tasks. ||| rodrigo nogueira ||| zhiying jiang ||| jimmy lin ||| 
2022 ||| aprnet: attention-based pixel-wise rendering network for photo-realistic text image generation. ||| yangming shi ||| haisong ding ||| kai chen ||| qiang huo ||| 
2018 ||| iterative attention mining for weakly supervised thoracic disease pattern localization in chest x-rays. ||| jinzheng cai ||| le lu ||| adam p. harrison ||| xiaoshuang shi ||| pingjun chen ||| lin yang ||| 
2022 ||| from unstructured text to causal knowledge graphs: a transformer-based approach. ||| scott e. friedman ||| ian h. magnusson ||| vasanth sarathy ||| sonja schmer-galunder ||| 
2019 ||| models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. ||| william havard ||| jean-pierre chevrot ||| laurent besacier ||| 
2021 ||| text compression-aided transformer encoding. ||| zuchao li ||| zhuosheng zhang ||| hai zhao ||| rui wang ||| kehai chen ||| masao utiyama ||| eiichiro sumita ||| 
2021 ||| semantic communication with adaptive universal transformer. ||| qingyang zhou ||| rongpeng li ||| zhifeng zhao ||| chenghui peng ||| honggang zhang ||| 
2018 ||| self-attention with relative position representations. ||| peter shaw ||| jakob uszkoreit ||| ashish vaswani ||| 
2021 ||| a generative model for raw audio using transformer architectures. ||| prateek verma ||| chris chafe ||| 
2022 ||| attention-based vandalism detection in openstreetmap. ||| nicolas tempelmeier ||| elena demidova ||| 
2022 ||| spatio-temporal vision transformer for super-resolution microscopy. ||| charles n. christensen ||| meng lu ||| edward n. ward ||| pietro li ||| clemens f. kaminski ||| 
2020 ||| anchor-based spatial-temporal attention convolutional networks for dynamic 3d point cloud sequences. ||| guangming wang ||| hanwen liu ||| muyao chen ||| yehui yang ||| zhe liu ||| hesheng wang ||| 
2021 ||| explainable student performance prediction with personalized attention for explaining why a student fails. ||| kun niu ||| xipeng cao ||| yicong yu ||| 
2021 ||| language-based video editing via multi-modal multi-level transformer. ||| tsu-jui fu ||| xin eric wang ||| scott t. grafton ||| miguel p. eckstein ||| william yang wang ||| 
2019 ||| analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be pruned. ||| elena voita ||| david talbot ||| fedor moiseev ||| rico sennrich ||| ivan titov ||| 
2017 ||| visual attention models for scene text recognition. ||| suman k. ghosh ||| ernest valveny ||| andrew d. bagdanov ||| 
2019 ||| dialogue transformers. ||| vladimir vlasov ||| johannes e. m. mosig ||| alan nichol ||| 
2021 ||| attention-guided generative models for extractive question answering. ||| peng xu ||| davis liang ||| zhiheng huang ||| bing xiang ||| 
2020 ||| augmented equivariant attention networks for electron microscopy image super-resolution. ||| yaochen xie ||| yu ding ||| shuiwang ji ||| 
2020 ||| spatial-angular attention network for light field reconstruction. ||| gaochang wu ||| yebin liu ||| lu fang ||| tianyou chai ||| 
2021 ||| tntc: two-stream network with transformer-based complementarity for gait-based emotion recognition. ||| chuanfei hu ||| weijie sheng ||| bo dong ||| xinde li ||| 
2020 ||| a simple yet effective method for video temporal grounding with cross-modality attention. ||| binjie zhang ||| yu li ||| chun yuan ||| dejing xu ||| pin jiang ||| ying shan ||| 
2020 ||| rotation averaging with attention graph neural networks. ||| joshua thorpe ||| ruwan b. tennakoon ||| alireza bab-hadiashar ||| 
2021 ||| a million tweets are worth a few points: tuning transformers for customer service tasks. ||| amir hadifar ||| sofie labat ||| v ||| ronique hoste ||| chris develder ||| thomas demeester ||| 
2021 ||| focal attention networks: optimising attention for biomedical image segmentation. ||| michael yeung ||| leonardo rundo ||| evis sala ||| carola-bibiane sch ||| nlieb ||| guang yang ||| 
2018 ||| r-vqa: learning visual relation facts with semantic attention for visual question answering. ||| pan lu ||| lei ji ||| wei zhang ||| nan duan ||| ming zhou ||| jianyong wang ||| 
2018 ||| knowing where to look? analysis on attention of visual question answering system. ||| wei li ||| zehuan yuan ||| xiangzhong fang ||| changhu wang ||| 
2022 ||| regression transformer: concurrent conditional generation and regression by blending numerical and textual tokens. ||| jannis born ||| matteo manica ||| 
2021 ||| attention-based convolutional autoencoders for 3d-variational data assimilation. ||| julian mack ||| rossella arcucci ||| miguel molina-solana ||| yi-ke guo ||| 
2021 ||| scalable transformers for neural machine translation. ||| peng gao ||| shijie geng ||| yu qiao ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2021 ||| transformer for polyp detection. ||| shijie liu ||| hongyu zhou ||| xiaozhou shi ||| junwen pan ||| 
2020 ||| tabtransformer: tabular data modeling using contextual embeddings. ||| xin huang ||| ashish khetan ||| milan cvitkovic ||| zohar s. karnin ||| 
2021 ||| a transformer-based cross-modal fusion model with adversarial training for vqa challenge 2021. ||| ke-han lu ||| bo-han fang ||| kuan-yu chen ||| 
2021 ||| towards transferable adversarial attacks on vision transformers. ||| zhipeng wei ||| jingjing chen ||| micah goldblum ||| zuxuan wu ||| tom goldstein ||| yu-gang jiang ||| 
2019 ||| attentional policies for cross-context multi-agent reinforcement learning. ||| matthew a. wright ||| roberto horowitz ||| 
2020 ||| advancing multiple instance learning with attention modeling for categorical speech emotion recognition. ||| shuiyang mao ||| p. c. ching ||| c.-c. jay kuo ||| tan lee ||| 
2019 ||| attention-wrapped hierarchical blstms for ddi extraction. ||| vahab mostafapour ||| oguz dikenelli ||| 
2022 ||| cosformer: rethinking softmax in attention. ||| zhen qin ||| weixuan sun ||| hui deng ||| dongxu li ||| yunshen wei ||| baohong lv ||| junjie yan ||| lingpeng kong ||| yiran zhong ||| 
2021 ||| attention bottlenecks for multimodal fusion. ||| arsha nagrani ||| shan yang ||| anurag arnab ||| aren jansen ||| cordelia schmid ||| chen sun ||| 
2021 ||| regional attention network (ran) for head pose and fine-grained gesture recognition. ||| ardhendu behera ||| zachary wharton ||| morteza ghahremani ||| swagat kumar ||| nik bessis ||| 
2020 ||| finnish language modeling with deep transformer models. ||| abhilash jain ||| aku rouhe ||| stig-arne gr ||| nroos ||| mikko kurimo ||| 
2020 ||| finding fast transformers: one-shot neural architecture search by component composition. ||| henry tsai ||| jayden ooi ||| chun-sung ferng ||| hyung won chung ||| jason riesa ||| 
2018 ||| exploring a unified attention-based pooling framework for speaker verification. ||| yi liu ||| liang he ||| weiwei liu ||| jia liu ||| 
2020 ||| ratt: recurrent attention to transient tasks for continual image captioning. ||| riccardo del chiaro ||| bartlomiej twardowski ||| andrew d. bagdanov ||| joost van de weijer ||| 
2021 ||| identifying ards using the hierarchical attention network with sentence objectives framework. ||| kevin lybarger ||| linzee mabrey ||| matthew thau ||| pavan k. bhatraju ||| mark m. wurfel ||| meliha yetisgen ||| 
2021 ||| context-aware transformer transducer for speech recognition. ||| feng-ju chang ||| jing liu ||| martin radfar ||| athanasios mouchtaris ||| maurizio omologo ||| ariya rastrow ||| siegfried kunzmann ||| 
2021 ||| rethinking lifelong sequential recommendation with incremental multi-interest attention. ||| yongji wu ||| lu yin ||| defu lian ||| mingyang yin ||| neil zhenqiang gong ||| jingren zhou ||| hongxia yang ||| 
2020 ||| rethinking the value of transformer components. ||| wenxuan wang ||| zhaopeng tu ||| 
2020 ||| looking glamorous: vehicle re-id in heterogeneous cameras networks with global and local attention. ||| abhijit suprem ||| calton pu ||| 
2021 ||| softermax: hardware/software co-design of an efficient softmax for transformers. ||| jacob r. stevens ||| rangharajan venkatesan ||| steve dai ||| brucek khailany ||| anand raghunathan ||| 
2019 ||| patent citation dynamics modeling via multi-attention recurrent networks. ||| taoran ji ||| zhiqian chen ||| nathan self ||| kaiqun fu ||| chang-tien lu ||| naren ramakrishnan ||| 
2021 ||| hierarchical attention fusion for geo-localization. ||| liqi yan ||| yiming cui ||| yingjie victor chen ||| dongfang liu ||| 
2021 ||| fine-tuning transformers: vocabulary transfer. ||| igor samenko ||| alexey tikhonov ||| borislav kozlovskii ||| ivan p. yamshchikov ||| 
2021 ||| multi-modal attention network for stock movements prediction. ||| shwai he ||| shi gu ||| 
2018 ||| volatility in the issue attention economy. ||| chico q. camargo ||| scott a. hale ||| peter john ||| helen z. margetts ||| 
2021 ||| adversarial robustness comparison of vision transformer and mlp-mixer to cnns. ||| philipp benz ||| soomin ham ||| chaoning zhang ||| adil karjauv ||| in so kweon ||| 
2021 ||| cdtrans: cross-domain transformer for unsupervised domain adaptation. ||| tongkun xu ||| weihua chen ||| pichao wang ||| fan wang ||| hao li ||| rong jin ||| 
2020 ||| camta: casual attention model for multi-touch attribution. ||| sachin kumar ||| garima gupta ||| ranjitha prasad ||| arnab chatterjee ||| lovekesh vig ||| gautam shroff ||| 
2018 ||| close to human quality tts with transformer. ||| naihan li ||| shujie liu ||| yanqing liu ||| sheng zhao ||| ming liu ||| ming zhou ||| 
2019 ||| attentional encoder network for targeted sentiment classification. ||| youwei song ||| jiahai wang ||| tao jiang ||| zhiyue liu ||| yanghui rao ||| 
2021 ||| geometry attention transformer with position-aware lstms for image captioning. ||| chi wang ||| yulin shen ||| luping ji ||| 
2020 ||| multi-head attention with joint agent-map representation for trajectory prediction in autonomous driving. ||| kaouther messaoud ||| nachiket deo ||| mohan m. trivedi ||| fawzi nashashibi ||| 
2019 ||| near-optimal glimpse sequences for improved hard attention neural network training. ||| william harvey ||| michael teng ||| frank wood ||| 
2017 ||| online and linear-time attention by enforcing monotonic alignments. ||| colin raffel ||| minh-thang luong ||| peter j. liu ||| ron j. weiss ||| douglas eck ||| 
2020 ||| attention aware cost volume pyramid based multi-view stereo network for 3d reconstruction. ||| anzhu yu ||| wenyue guo ||| bing liu ||| xin chen ||| xin wang ||| xuefeng cao ||| bingchuan jiang ||| 
2020 ||| know what you don't need: single-shot meta-pruning for attention heads. ||| zhengyan zhang ||| fanchao qi ||| zhiyuan liu ||| qun liu ||| maosong sun ||| 
2019 ||| co-attention based neural network for source-dependent essay scoring. ||| haoran zhang ||| diane j. litman ||| 
2021 ||| coarse temporal attention network (cta-net) for driver's activity recognition. ||| zachary wharton ||| ardhendu behera ||| yonghuai liu ||| nik bessis ||| 
2021 ||| transalnet: visual saliency prediction using transformers. ||| jianxun lou ||| hanhe lin ||| david marshall ||| dietmar saupe ||| hantao liu ||| 
2019 ||| effective attention modeling for neural relation extraction. ||| tapas nayak ||| hwee tou ng ||| 
2021 ||| high-resolution complex scene synthesis with transformers. ||| manuel jahn ||| robin rombach ||| bj ||| rn ommer ||| 
2021 ||| rethinking skip connection with layer normalization in transformers and resnets. ||| fenglin liu ||| xuancheng ren ||| zhiyuan zhang ||| xu sun ||| yuexian zou ||| 
2021 ||| lung cancer diagnosis using deep attention based on multiple instance learning and radiomics. ||| junhua chen ||| haiyan zeng ||| chong zhang ||| zhenwei shi ||| andre dekker ||| leonard wee ||| i ||| igo bermejo ||| 
2019 ||| proposal-free temporal moment localization of a natural-language query in video using guided attention. ||| cristian rodriguez opazo ||| edison marrese-taylor ||| fatemeh sadat saleh ||| hongdong li ||| stephen gould ||| 
2018 ||| modelling the dynamic joint policy of teammates with attention multi-agent ddpg. ||| hangyu mao ||| zhengchao zhang ||| zhen xiao ||| zhibo gong ||| 
2018 ||| integrating transformer and paraphrase rules for sentence simplification. ||| sanqiang zhao ||| rui meng ||| daqing he ||| andi saptono ||| bambang parmanto ||| 
2020 ||| whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. ||| jiawen yao ||| xinliang zhu ||| jitendra jonnagaddala ||| nicholas j. hawkins ||| junzhou huang ||| 
2021 ||| position information in transformers: an overview. ||| philipp dufter ||| martin schmitt ||| hinrich sch ||| tze ||| 
2020 ||| learning selective mutual attention and contrast for rgb-d saliency detection. ||| nian liu ||| ni zhang ||| ling shao ||| junwei han ||| 
2022 ||| numhtml: numeric-oriented hierarchical transformer model for multi-task financial forecasting. ||| linyi yang ||| jiazheng li ||| ruihai dong ||| yue zhang ||| barry smyth ||| 
2020 ||| unsupervised deep metric learning with transformed attention consistency and contrastive clustering loss. ||| yang li ||| shichao kan ||| zhihai he ||| 
2018 ||| facial action unit detection using attention and relation learning. ||| zhiwen shao ||| zhilei liu ||| jianfei cai ||| yunsheng wu ||| lizhuang ma ||| 
2021 ||| dga-net dynamic gaussian attention network for sentence semantic matching. ||| kun zhang ||| guangyi lv ||| meng wang ||| enhong chen ||| 
2021 ||| sequence-to-sequence piano transcription with transformers. ||| curtis hawthorne ||| ian simon ||| rigel swavely ||| ethan manilow ||| jesse h. engel ||| 
2021 ||| hit: a hierarchically fused deep attention network for robust code-mixed language representation. ||| ayan sengupta ||| sourabh kumar bhattacharjee ||| tanmoy chakraborty ||| md. shad akhtar ||| 
2020 ||| frequency-based multi task learning with attention mechanism for fault detection in power systems. ||| peyman tehrani ||| marco levorato ||| 
2021 ||| towards interpretable attention networks for cervical cancer analysis. ||| ruiqi wang ||| mohammad ali armin ||| simon denman ||| lars petersson ||| david ahmedt-aristizabal ||| 
2020 ||| sparse sinkhorn attention. ||| yi tay ||| dara bahri ||| liu yang ||| donald metzler ||| da-cheng juan ||| 
2021 ||| raw produce quality detection with shifted window self-attention. ||| oh joon kwon ||| byungsoo kim ||| youngduck choi ||| 
2021 ||| empirical evaluation of pre-trained transformers for human-level nlp: the role of sample size and dimensionality. ||| adithya v. ganesan ||| matthew matero ||| aravind reddy ravula ||| huy vu ||| h. andrew schwartz ||| 
2021 ||| random feature attention. ||| hao peng ||| nikolaos pappas ||| dani yogatama ||| roy schwartz ||| noah a. smith ||| lingpeng kong ||| 
2018 ||| improved hybrid ctc-attention model for speech recognition. ||| zhe yuan ||| zhuoran lyu ||| jiwei li ||| xi zhou ||| 
2021 ||| anatomy-guided parallel bottleneck transformer network for automated evaluation of root canal therapy. ||| yunxiang li ||| guodong zeng ||| yifan zhang ||| jun wang ||| qianni zhang ||| qun jin ||| lingling sun ||| qisi lian ||| neng xia ||| ruizi peng ||| kai tang ||| yaqi wang ||| shuai wang ||| 
2019 ||| attention model for articulatory features detection. ||| ievgen karaulov ||| dmytro tkanov ||| 
2019 ||| gram: scalable generative models for graphs with graph attention mechanism. ||| wataru kawai ||| yusuke mukuta ||| tatsuya harada ||| 
2020 ||| adversarial watermarking transformer: towards tracing text provenance with data hiding. ||| sahar abdelnabi ||| mario fritz ||| 
2020 ||| attention-based planning with active perception. ||| haoxiang ma ||| jie fu ||| 
2021 ||| effective attention sheds light on interpretability. ||| kaiser sun ||| ana marasovic ||| 
2020 ||| transformer based multilingual document embedding model. ||| wei li ||| brian mak ||| 
2020 ||| transformer-based online ctc/attention end-to-end speech recognition architecture. ||| haoran miao ||| gaofeng cheng ||| changfeng gao ||| pengyuan zhang ||| yonghong yan ||| 
2018 ||| self-attention generative adversarial networks. ||| han zhang ||| ian j. goodfellow ||| dimitris n. metaxas ||| augustus odena ||| 
2017 ||| paying more attention to saliency: image captioning with saliency and context attention. ||| marcella cornia ||| lorenzo baraldi ||| giuseppe serra ||| rita cucchiara ||| 
2020 ||| da2: deep attention adapter for memory-efficienton-device multi-domain learning. ||| li yang ||| adnan siraj rakin ||| deliang fan ||| 
2019 ||| joint source-target self attention with locality constraints. ||| jos |||  a. r. fonollosa ||| noe casas ||| marta r. costa-juss ||| 
2022 ||| transfollower: long-sequence car-following trajectory prediction through transformer. ||| meixin zhu ||| simon s. du ||| xuesong wang ||| hao yang ||| ziyuan pu ||| yinhai wang ||| 
2020 ||| gradient-based adversarial training on transformer networks for detecting check-worthy factual claims. ||| kevin meng ||| damian jimenez ||| fatma arslan ||| jacob daniel devasier ||| daniel obembe ||| chengkai li ||| 
2022 ||| representing long-range context for graph neural networks with global attention. ||| zhanghao wu ||| paras jain ||| matthew a. wright ||| azalia mirhoseini ||| joseph e. gonzalez ||| ion stoica ||| 
2021 ||| multi-factors aware dual-attentional knowledge tracing. ||| moyu zhang ||| xinning zhu ||| chunhong zhang ||| yang ji ||| feng pan ||| changchuan yin ||| 
2019 ||| utterance-level end-to-end language identification using attention-based cnn-blstm. ||| weicheng cai ||| danwei cai ||| shen huang ||| ming li ||| 
2021 ||| towards a unified foundation model: jointly pre-training transformers on unpaired images and text. ||| qing li ||| boqing gong ||| yin cui ||| dan kondratyuk ||| xianzhi du ||| ming-hsuan yang ||| matthew brown ||| 
2022 ||| rich cnn-transformer feature aggregation networks for super-resolution. ||| jinsu yoo ||| taehoon kim ||| sihaeng lee ||| seung hwan kim ||| honglak lee ||| tae hyun kim ||| 
2017 ||| tracking gaze and visual focus of attention of people involved in social interaction. ||| benoit mass ||| sil ||| ye o. ba ||| radu horaud ||| 
2021 ||| joint intent detection and slot filling with wheel-graph attention networks. ||| pengfei wei ||| bi zeng ||| wenxiong liao ||| 
2020 ||| self-supervised visual attention learning for vehicle re-identification. ||| ming li ||| yiming zhao ||| yecheng lyu ||| ziming zhang ||| 
2019 ||| synaptic partner assignment using attentional voxel association networks. ||| nicholas l. turner ||| kisuk lee ||| ran lu ||| jingpeng wu ||| dodam ih ||| h. sebastian seung ||| 
2021 ||| inadvert: an interactive and adaptive counterdeception platform for attention enhancement and phishing prevention. ||| linan huang ||| quanyan zhu ||| 
2021 ||| oodformer: out-of-distribution detection transformer. ||| rajat koner ||| poulami sinhamahapatra ||| karsten roscher ||| stephan g ||| nnemann ||| volker tresp ||| 
2020 ||| landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition. ||| darshan gera ||| s. balasubramanian ||| 
2021 ||| transformer uncertainty estimation with hierarchical stochastic attention. ||| jiahuan pei ||| cheng wang ||| gy ||| rgy szarvas ||| 
2020 ||| mebal: a multimodal database for eye blink detection and attention level estimation. ||| roberto daza ||| aythami morales ||| julian fi ||| rrez ||| rub ||| n tolosana ||| 
2021 ||| dr-tanet: dynamic receptive temporal attention network for street scene change detection. ||| shuo chen ||| kailun yang ||| rainer stiefelhagen ||| 
2021 ||| cisco at semeval-2021 task 5: what's toxic?: leveraging transformers for multiple toxic span extraction from online comments. ||| sreyan ghosh ||| sonal kumar ||| 
2021 ||| graph attention networks for channel estimation in ris-assisted satellite iot communications. ||| k ||| rsat tekbiyik ||| g ||| nes karabulut-kurt ||| ali riza ekti ||| halim yanikomeroglu ||| 
2018 ||| aspect level sentiment classification with attention-over-attention neural networks. ||| binxuan huang ||| yanglan ou ||| kathleen m. carley ||| 
2020 ||| temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition. ||| negar heidari ||| alexandros iosifidis ||| 
2021 ||| spagan: shortest path graph attention network. ||| yiding yang ||| xinchao wang ||| mingli song ||| junsong yuan ||| dacheng tao ||| 
2018 ||| actor conditioned attention maps for video action detection. ||| oytun ulutan ||| swati rallapalli ||| mudhakar srivatsa ||| b. s. manjunath ||| 
2020 ||| deep transformers with latent depth. ||| xian li ||| asa cooper stickland ||| yuqing tang ||| xiang kong ||| 
2021 ||| human parity on commonsenseqa: augmenting self-attention with external attention. ||| yichong xu ||| chenguang zhu ||| shuohang wang ||| siqi sun ||| hao cheng ||| xiaodong liu ||| jianfeng gao ||| pengcheng he ||| michael zeng ||| xuedong huang ||| 
2020 ||| subjective question answering: deciphering the inner workings of transformers in the realm of subjectivity. ||| lukas muttenthaler ||| 
2019 ||| speech emotion recognition using multi-hop attention mechanism. ||| seunghyun yoon ||| seokhyun byun ||| subhadeep dey ||| kyomin jung ||| 
2021 ||| rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos. ||| chinedu innocent nwoye ||| tong yu ||| cristians gonzalez ||| barbara seeliger ||| pietro mascagni ||| didier mutter ||| jacques marescaux ||| nicolas padoy ||| 
2021 ||| vision transformers for weeds and crops classification of high resolution uav images. ||| reenul reedha ||| eric dericquebourg ||| rapha ||| l canals ||| adel hafiane ||| 
2021 ||| transformers solve the limited receptive field for monocular depth prediction. ||| guanglei yang ||| hao tang ||| mingli ding ||| nicu sebe ||| elisa ricci ||| 
2020 ||| local self-attention over long text for efficient document retrieval. ||| sebastian hofst ||| tter ||| hamed zamani ||| bhaskar mitra ||| nick craswell ||| allan hanbury ||| 
2020 ||| text classification based on multi-granularity attention hybrid neural network. ||| zhenyu liu ||| chaohong lu ||| haiwei huang ||| shengfei lyu ||| zhenchao tao ||| 
2022 ||| surds: self-supervised attention-guided reconstruction and dual triplet loss for writer independent offline signature verification. ||| soumitri chattopadhyay ||| siladittya manna ||| saumik bhattacharya ||| umapada pal ||| 
2022 ||| tranad: deep transformer networks for anomaly detection in multivariate time series data. ||| shreshth tuli ||| giuliano casale ||| nicholas r. jennings ||| 
2019 ||| enhancing pre-trained chinese character representation with word-aligned attention. ||| yanzeng li ||| bowen yu ||| mengge xue ||| tingwen liu ||| 
2019 ||| savehr: self attention vector representations for ehr based personalized chronic disease onset prediction and interpretability. ||| sunil mallya ||| j. marc overhage ||| sravan bodapati ||| navneet srivastava ||| sahika genc ||| 
2021 ||| multi-level attention pooling for graph neural networks: unifying graph representations with multiple localities. ||| takeshi d. itoh ||| takatomi kubo ||| kazushi ikeda ||| 
2020 ||| recurrent attention model with log-polar mapping is robust against adversarial attacks. ||| taro kiritani ||| koji ono ||| 
2020 ||| autotrans: automating transformer design via reinforced architecture search. ||| wei zhu ||| xiaoling wang ||| xipeng qiu ||| yuan ni ||| guotong xie ||| 
2018 ||| transformer for emotion recognition. ||| jean-benoit delbrouck ||| 
2017 ||| multi-modal conditional attention fusion for dimensional emotion prediction. ||| shizhe chen ||| qin jin ||| 
2020 ||| studying attention models in sentiment attitude extraction task. ||| nicolay rusnachenko ||| natalia v. loukachevitch ||| 
2022 ||| an attention-based method for action unit detection at the 3rd abaw competition. ||| duy le hoai ||| eunchae lim ||| eunbin choi ||| sieun kim ||| sudarshan pant ||| guee-sang lee ||| soo-hyung kim ||| hyung-jeong yang ||| 
2019 ||| an attention enhanced graph convolutional lstm network for skeleton-based action recognition. ||| chenyang si ||| wentao chen ||| wei wang ||| liang wang ||| tieniu tan ||| 
2019 ||| toward interpretable music tagging with self-attention. ||| minz won ||| sanghyuk chun ||| xavier serra ||| 
2020 ||| probabilistic crowd gan: multimodal pedestrian trajectory prediction using a graph vehicle-pedestrian attention network. ||| stuart eiffert ||| kunming li ||| mao shan ||| stewart worrall ||| salah sukkarieh ||| eduardo m. nebot ||| 
2020 ||| automatic ischemic stroke lesion segmentation from computed tomography perfusion images by image synthesis and attention-based deep neural networks. ||| guotai wang ||| tao song ||| qiang dong ||| mei cui ||| ning huang ||| shaoting zhang ||| 
2017 ||| a hierarchical contextual attention-based gru network for sequential recommendation. ||| 
2018 ||| aaane: attention-based adversarial autoencoder for multi-scale network embedding. ||| lei sang ||| min xu ||| shengsheng qian ||| xindong wu ||| 
2020 ||| future-guided incremental transformer for simultaneous translation. ||| shaolei zhang ||| yang feng ||| liangyou li ||| 
2021 ||| car-net: unsupervised co-attention guided registration network for joint registration and structure learning. ||| xiang chen ||| yan xia ||| nishant ravikumar ||| alejandro f. frangi ||| 
2020 ||| a deep neural network for audio classification with a classifier attention mechanism. ||| haoye lu ||| haolong zhang ||| amit nayak ||| 
2020 ||| predicting rigid body dynamics using dual quaternion recurrent neural networks with quaternion attention. ||| johannes p ||| ppelbaum ||| andreas schwung ||| 
2020 ||| a price-per-attention auction scheme using mouse cursor information. ||| ioannis arapakis ||| antonio penta ||| hideo joho ||| luis a. leiva ||| 
2022 ||| a differential attention fusion model based on transformer for time series forecasting. ||| benhan li ||| shengdong du ||| tianrui li ||| 
2018 ||| uncertainty-aware attention for reliable interpretation and prediction. ||| jay heo ||| haebeom lee ||| saehoon kim ||| juho lee ||| kwang joon kim ||| eunho yang ||| sung ju hwang ||| 
2021 ||| diverse part discovery: occluded person re-identification with part-aware transformer. ||| yulin li ||| jianfeng he ||| tianzhu zhang ||| xiang liu ||| yongdong zhang ||| feng wu ||| 
2020 ||| lightweight temporal self-attention for classifying satellite image time series. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| 
2021 ||| zero-shot certified defense against adversarial patches with vision transformers. ||| yuheng huang ||| yuanchun li ||| 
2021 ||| exploiting attention-based sequence-to-sequence architectures for sound event localization. ||| christopher schymura ||| tsubasa ochiai ||| marc delcroix ||| keisuke kinoshita ||| tomohiro nakatani ||| shoko araki ||| dorothea kolossa ||| 
2020 ||| revisiting robust neural machine translation: a transformer case study. ||| peyman passban ||| puneeth s. m. saladi ||| qun liu ||| 
2021 ||| relation-aware hierarchical attention framework for video question answering. ||| fangtao li ||| ting bai ||| chenyu cao ||| zihe liu ||| chenghao yan ||| bin wu ||| 
2021 ||| visual grounding with transformers. ||| ye du ||| zehua fu ||| qingjie liu ||| yunhong wang ||| 
2021 ||| video crowd localization with multi-focus gaussian neighbor attention and a large-scale benchmark. ||| haopeng li ||| lingbo liu ||| kunlin yang ||| shinan liu ||| junyu gao ||| bin zhao ||| rui zhang ||| jun hou ||| 
2021 ||| discrete auto-regressive variational attention models for text modeling. ||| xianghong fang ||| haoli bai ||| jian li ||| zenglin xu ||| michael r. lyu ||| irwin king ||| 
2021 ||| pilot: introducing transformers for probabilistic sound event localization. ||| christopher schymura ||| benedikt t. b ||| nninghoff ||| tsubasa ochiai ||| marc delcroix ||| keisuke kinoshita ||| tomohiro nakatani ||| shoko araki ||| dorothea kolossa ||| 
2019 ||| iterative answer prediction with pointer-augmented multimodal transformers for textvqa. ||| ronghang hu ||| amanpreet singh ||| trevor darrell ||| marcus rohrbach ||| 
2020 ||| atss-net: target speaker separation via attention-based neural network. ||| tingle li ||| qingjian lin ||| yuanyuan bao ||| ming li ||| 
2020 ||| focus on the present: a regularization method for the asr source-target attention layer. ||| nanxin chen ||| piotr zelasko ||| jes ||| s villalba ||| najim dehak ||| 
2020 ||| pay attention to what you read: non-recurrent handwritten text-line recognition. ||| lei kang ||| pau riba ||| mar ||| al rusi ||| ol ||| alicia forn ||| s ||| mauricio villegas ||| 
2022 ||| can pre-trained transformers be used in detecting complex sensitive sentences? - a monsanto case study. ||| roelien c. timmer ||| david liebowitz ||| surya nepal ||| salil s. kanhere ||| 
2017 ||| high-order attention models for visual question answering. ||| idan schwartz ||| alexander g. schwing ||| tamir hazan ||| 
2021 ||| depth infused binaural audio generation using hierarchical cross-modal attention. ||| kranti kumar parida ||| siddharth srivastava ||| neeraj matiyali ||| gaurav sharma ||| 
2019 ||| personalizing graph neural networks with attention mechanism for session-based recommendation. ||| shu wu ||| mengqi zhang ||| xin jiang ||| ke xu ||| liang wang ||| 
2021 ||| building interpretable models for business process prediction using shared and specialised attention mechanisms. ||| bemali wickramanayake ||| zhipeng he ||| chun ouyang ||| catarina moreira ||| yue xu ||| renuka sindhgatta ||| 
2022 ||| uniformer: unified transformer for efficient spatiotemporal representation learning. ||| kunchang li ||| yali wang ||| peng gao ||| guanglu song ||| yu liu ||| hongsheng li ||| yu qiao ||| 
2021 ||| semi-supervised wide-angle portraits correction by multi-scale transformer. ||| fushun zhu ||| shan zhao ||| peng wang ||| hao wang ||| hua yan ||| shuaicheng liu ||| 
2019 ||| acnet: attention based network to exploit complementary features for rgbd semantic segmentation. ||| xinxin hu ||| kailun yang ||| lei fei ||| kaiwei wang ||| 
2019 ||| prime sample attention in object detection. ||| yuhang cao ||| kai chen ||| chen change loy ||| dahua lin ||| 
2018 ||| agil: learning attention from human for visuomotor tasks. ||| ruohan zhang ||| zhuode liu ||| luxin zhang ||| jake a. whritner ||| karl s. muller ||| mary m. hayhoe ||| dana h. ballard ||| 
2021 ||| smart bird: learnable sparse attention for efficient and effective transformer. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| binxing jiao ||| daxin jiang ||| yongfeng huang ||| 
2021 ||| segformer: simple and efficient design for semantic segmentation with transformers. ||| enze xie ||| wenhai wang ||| zhiding yu ||| anima anandkumar ||| jose m. alvarez ||| ping luo ||| 
2022 ||| bending reality: distortion-aware transformers for adapting to panoramic semantic segmentation. ||| jiaming zhang ||| kailun yang ||| chaoxiang ma ||| simon rei ||| kunyu peng ||| rainer stiefelhagen ||| 
2021 ||| mask attention networks: rethinking and strengthen transformer. ||| zhihao fan ||| yeyun gong ||| dayiheng liu ||| zhongyu wei ||| siyuan wang ||| jian jiao ||| nan duan ||| ruofei zhang ||| xuanjing huang ||| 
2022 ||| sats: self-attention transfer for continual semantic segmentation. ||| yiqiao qiu ||| yixing shen ||| zhuohao sun ||| yanchong zheng ||| xiaobin chang ||| weishi zheng ||| ruixuan wang ||| 
2021 ||| reconstruction student with attention for student-teacher pyramid matching. ||| shinji yamada ||| kazuhiro hotta ||| 
2018 ||| attend before you act: leveraging human visual attention for continual learning. ||| khimya khetarpal ||| doina precup ||| 
2019 ||| fast and accurate capitalization and punctuation for automatic speech recognition using transformer and chunk merging. ||| binh nguyen ||| vu bao hung nguyen ||| hien nguyen ||| pham ngoc phuong ||| the-loc nguyen ||| quoc truong do ||| luong chi mai ||| 
2019 ||| mapping social media attention in microbiology: identifying main topics and actors. ||| nicol ||| s robinson-garc ||| a ||| wenceslao arroyo-machado ||| daniel torres-salinas ||| 
2019 ||| semantic relation classification via bidirectional lstm networks with entity-aware attention using latent entity typing. ||| joohong lee ||| sangwoo seo ||| yong suk choi ||| 
2021 ||| spatio-temporal weather forecasting and attention mechanism on convolutional lstms. ||| selim furkan tekin ||| oguzhan karaahmetoglu ||| fatih ilhan ||| ismail balaban ||| suleyman serdar kozat ||| 
2021 ||| pointr: diverse point cloud completion with geometry-aware transformers. ||| xumin yu ||| yongming rao ||| ziyi wang ||| zuyan liu ||| jiwen lu ||| jie zhou ||| 
2021 ||| task transformer network for joint mri reconstruction and super-resolution. ||| chun-mei feng ||| yunlu yan ||| huazhu fu ||| li chen ||| yong xu ||| 
2022 ||| style transformer for image inversion and editing. ||| xueqi hu ||| qiusheng huang ||| zhengyi shi ||| siyuan li ||| changxin gao ||| li sun ||| qingli li ||| 
2020 ||| vd-bert: a unified vision and dialog transformer with bert. ||| yue wang ||| shafiq r. joty ||| michael r. lyu ||| irwin king ||| caiming xiong ||| steven c. h. hoi ||| 
2018 ||| deep co-attention based comparators for relative representation learning in person re-identification. ||| lin wu ||| yang wang ||| junbin gao ||| dacheng tao ||| 
2021 ||| psa-gan: progressive self attention gans for synthetic time series. ||| paul jeha ||| michael bohlke-schneider ||| pedro mercado ||| rajbir-singh nirwan ||| shubham kapoor ||| valentin flunkert ||| jan gasthaus ||| tim januschowski ||| 
2022 ||| learning confidence for transformer-based neural machine translation. ||| yu lu ||| jiali zeng ||| jiajun zhang ||| shuangzhi wu ||| mu li ||| 
2019 ||| a coarse-to-fine framework for learned color enhancement with non-local attention. ||| chaowei shan ||| zhizheng zhang ||| zhibo chen ||| 
2021 ||| mvit: mask vision transformer for facial expression recognition in the wild. ||| hanting li ||| mingzhe sui ||| feng zhao ||| zhengjun zha ||| feng wu ||| 
2021 ||| transformer-based lexically constrained headline generation. ||| kosuke yamada ||| yuta hitomi ||| hideaki tamori ||| ryohei sasano ||| naoaki okazaki ||| kentaro inui ||| koichi takeda ||| 
2021 ||| coordinate attention for efficient mobile network design. ||| qibin hou ||| daquan zhou ||| jiashi feng ||| 
2021 ||| structure information is the key: self-attention roi feature extractor in 3d object detection. ||| diankun zhang ||| zhijie zheng ||| xueting bi ||| xiaojun liu ||| 
2019 ||| : meshed-memory transformer for image captioning. ||| marcella cornia ||| matteo stefanini ||| lorenzo baraldi ||| rita cucchiara ||| 
2021 ||| multi-scale high-resolution vision transformer for semantic segmentation. ||| jiaqi gu ||| hyoukjun kwon ||| dilin wang ||| wei ye ||| meng li ||| yu-hsin chen ||| liangzhen lai ||| vikas chandra ||| david z. pan ||| 
2021 ||| tenet: temporal cnn with attention for anomaly detection in automotive cyber-physical systems. ||| sooryaa vignesh thiruloga ||| vipin kumar kukkala ||| sudeep pasricha ||| 
2019 ||| dada: a large-scale benchmark and model for driver attention prediction in accidental scenarios. ||| jianwu fang ||| dingxin yan ||| jiahuan qiao ||| jianru xue ||| 
2021 ||| attention-based reinforcement learning for real-time uav semantic communication. ||| won joon yun ||| byungju lim ||| soyi jung ||| young-chai ko ||| jihong park ||| joongheon kim ||| mehdi bennis ||| 
2021 ||| massformer: tandem mass spectrum prediction with graph transformers. ||| adamo young ||| bo wang ||| hannes r ||| st ||| 
2020 ||| orientation-aware vehicle re-identification with semantics-guided part attention network. ||| tsai-shien chen ||| chih-ting liu ||| chih-wei wu ||| shao-yi chien ||| 
2022 ||| fine- and coarse-granularity hybrid self-attention for efficient bert. ||| jing zhao ||| yifan wang ||| junwei bao ||| youzheng wu ||| xiaodong he ||| 
2019 ||| learning parallax attention for stereo image super-resolution. ||| longguang wang ||| yingqian wang ||| zhengfa liang ||| zaiping lin ||| jungang yang ||| wei an ||| yulan guo ||| 
2021 ||| heterogeneous graph attention network for multi-hop machine reading comprehension. ||| feng gao ||| jiancheng ni ||| peng gao ||| zili zhou ||| yan-yan li ||| hamido fujita ||| 
2021 ||| salient object ranking with position-preserved attention. ||| hao fang ||| daoxin zhang ||| yi zhang ||| minghao chen ||| jiawei li ||| yao hu ||| deng cai ||| xiaofei he ||| 
2021 ||| long short-term transformer for online action detection. ||| mingze xu ||| yuanjun xiong ||| hao chen ||| xinyu li ||| wei xia ||| zhuowen tu ||| stefano soatto ||| 
2022 ||| semantic-aligned fusion transformer for one-shot object detection. ||| yizhou zhao ||| xun guo ||| yan lu ||| 
2020 ||| pre-training transformers as energy-based cloze models. ||| kevin clark ||| minh-thang luong ||| quoc v. le ||| christopher d. manning ||| 
2022 ||| grounding commands for autonomous vehicles via layer fusion with region-specific dynamic layer attention. ||| hou pong chan ||| mingxi guo ||| cheng-zhong xu ||| 
2021 ||| colorization transformer. ||| manoj kumar ||| dirk weissenborn ||| nal kalchbrenner ||| 
2021 ||| cascaded head-colliding attention. ||| lin zheng ||| zhiyong wu ||| lingpeng kong ||| 
2021 ||| t6d-direct: transformers for multi-object 6d pose direct regression. ||| arash amini ||| arul selvam periyasamy ||| sven behnke ||| 
2019 ||| kernel graph attention network for fact verification. ||| zhenghao liu ||| chenyan xiong ||| maosong sun ||| 
2020 ||| locating cephalometric x-ray landmarks with foveated pyramid attention. ||| logan gilmour ||| nilanjan ray ||| 
2017 ||| improving visually grounded sentence representations with self-attention. ||| kang min yoo ||| youhyun shin ||| sang-goo lee ||| 
2018 ||| group-attention single-shot detector (ga-ssd): finding pulmonary nodules in large-scale ct images. ||| jiechao ma ||| xiang li ||| hongwei li ||| bjoern h. menze ||| sen liang ||| rongguo zhang ||| wei-shi zheng ||| 
2021 ||| transcamp: graph transformer for 6-dof camera pose estimation. ||| xinyi li ||| haibin ling ||| 
2021 ||| global interaction modelling in vision transformer via super tokens. ||| ammarah farooq ||| muhammad awais ||| sara atito ali ahmed ||| josef kittler ||| 
2022 ||| rgb-d slam using attention guided frame association. ||| ali caglayan ||| nevrez imamoglu ||| oguzhan guclu ||| ali osman serhatoglu ||| weimin wang ||| ahmet burak can ||| ryosuke nakamura ||| 
2020 ||| parallel rescoring with transformer for streaming on-device speech recognition. ||| wei li ||| james qin ||| chung-cheng chiu ||| ruoming pang ||| yanzhang he ||| 
2020 ||| forecasting photovoltaic power production using a deep learning sequence to sequence model with attention. ||| elizaveta kharlova ||| daniel may ||| petr mus ||| lek ||| 
2020 ||| alp-kd: attention-based layer projection for knowledge distillation. ||| peyman passban ||| yimeng wu ||| mehdi rezagholizadeh ||| qun liu ||| 
2021 ||| planetr: structure-guided transformers for 3d plane recovery. ||| bin tan ||| nan xue ||| song bai ||| tianfu wu ||| gui-song xia ||| 
2021 ||| do pedestrians pay attention? eye contact detection in the wild. ||| younes belkada ||| lorenzo bertoni ||| romain caristan ||| taylor mordan ||| alexandre alahi ||| 
2020 ||| gated recurrent context: softmax-free attention for online encoder-decoder speech recognition. ||| hyeonseung lee ||| woo hyun kang ||| sung jun cheon ||| hyeongju kim ||| nam soo kim ||| 
2020 ||| adaptive attentional network for few-shot knowledge graph completion. ||| jiawei sheng ||| shu guo ||| zhenyu chen ||| juwei yue ||| lihong wang ||| tingwen liu ||| hongbo xu ||| 
2021 ||| deep co-supervision and attention fusion strategy for automatic covid-19 lung infection segmentation on ct images. ||| haigen hu ||| leizhao shen ||| qiu guan ||| xiaoxin li ||| qianwei zhou ||| su ruan ||| 
2021 ||| transformer-based korean pretrained language models: a survey on three years of progress. ||| kichang yang ||| 
2019 ||| a short virtual reality mindfulness meditation training for regaining sustained attention. ||| minkesh asati ||| taizo miyachi ||| 
2021 ||| sequence-to-sequence lexical normalization with multilingual transformers. ||| ana-maria bucur ||| adrian cosma ||| liviu p. dinu ||| 
2021 ||| guiding query position and performing similar attention for transformer-based detection heads. ||| xiaohu jiang ||| ze chen ||| zhicheng wang ||| erjin zhou ||| chun yuan ||| 
2021 ||| learning transformer features for image quality assessment. ||| chao zeng ||| sam kwong ||| 
2020 ||| assessing phrasal representation and composition in transformers. ||| lang yu ||| allyson ettinger ||| 
2021 ||| transforming fake news: robust generalisable news classification using transformers. ||| ciara blackledge ||| amir atapour-abarghouei ||| 
2020 ||| baksa at semeval-2020 task 9: bolstering cnn with self-attention for sentiment analysis of code mixed text. ||| ayush kumar ||| harsh agarwal ||| keshav bansal ||| ashutosh modi ||| 
2019 ||| knee menisci segmentation and relaxometry of 3d ultrashort echo time (ute) cones mr imaging using attention u-net with transfer learning. ||| michal byra ||| mei wu ||| xiaodong zhang ||| hyungseok jang ||| yajun ma ||| eric y. chang ||| sameer shah ||| jiang du ||| 
2019 ||| attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| hao tang ||| dan xu ||| nicu sebe ||| yan yan ||| 
2017 ||| person re-identification using visual attention. ||| alireza rahimpour ||| liu liu ||| ali taalimi ||| yang song ||| hairong qi ||| 
2019 ||| neutron: an implementation of the transformer translation model and its variants. ||| hongfei xu ||| qiuhui liu ||| 
2020 ||| efficient transformers: a survey. ||| yi tay ||| mostafa dehghani ||| dara bahri ||| donald metzler ||| 
2021 ||| cotext: multi-task learning with code-text transformer. ||| long n. phan ||| hieu tran ||| daniel le ||| hieu nguyen ||| james t. anibal ||| alec peltekian ||| yanfang ye ||| 
2021 ||| high-order tensor pooling with attention for action recognition. ||| piotr koniusz ||| lei wang ||| ke sun ||| 
2019 ||| explanation vs attention: a two-player game to obtain attention for vqa. ||| badri n. patro ||| anupriy ||| vinay p. namboodiri ||| 
2019 ||| non-intrusive load monitoring with an attention-based deep neural network. ||| antonio maria sudoso ||| veronica piccialli ||| 
2020 ||| powertransformer: unsupervised controllable revision for biased language correction. ||| xinyao ma ||| maarten sap ||| hannah rashkin ||| yejin choi ||| 
2019 ||| multi-modal attention-based fusion model for semantic segmentation of rgb-depth images. ||| fahimeh fooladgar ||| shohreh kasaei ||| 
2020 ||| hierachical delta-attention method for multimodal fusion. ||| kunjal panchal ||| 
2021 ||| a state-of-the-art survey of artificial neural networks for whole-slide image analysis: from popular convolutional neural networks to potential visual transformers. ||| chen li ||| xintong li ||| xiaoyan li ||| md mamunur rahaman ||| xiaoqi li ||| jian wu ||| yudong yao ||| marcin grzegorzek ||| 
2019 ||| manipulation-skill assessment from videos with spatial attention network. ||| zhenqiang li ||| yifei huang ||| minjie cai ||| yoichi sato ||| 
2021 ||| covtanet: a hybrid tri-level attention based network for lesion segmentation, diagnosis, and severity prediction of covid-19 chest ct scans. ||| tanvir mahmud ||| md. jahin alam ||| sakib chowdhury ||| shams nafisa ali ||| md maisoon rahman ||| shaikh anowarul fattah ||| mohammad saquib ||| 
2021 ||| u-shape transformer for underwater image enhancement. ||| lintao peng ||| chunli zhu ||| liheng bian ||| 
2021 ||| an efficient transformer decoder with compressed sub-layers. ||| yanyang li ||| ye lin ||| tong xiao ||| jingbo zhu ||| 
2021 ||| melody structure transfer network: generating music with separable self-attention. ||| ning zhang ||| junchi yan ||| 
2021 ||| ear-u-net: efficientnet and attention-based residual u-net for automatic liver segmentation in ct. ||| jinke wang ||| xiangyang zhang ||| peiqing lv ||| lubiao zhou ||| haiying wang ||| 
2019 ||| self-supervised attention model for weakly labeled audio event classification. ||| bongjun kim ||| shabnam ghaffarzadegan ||| 
2021 ||| mhattnsurv: multi-head attention for survival prediction using whole-slide pathology images. ||| shuai jiang ||| arief a. suriawinata ||| saeed hassanpour ||| 
2020 ||| self-attention generative adversarial network for speech enhancement. ||| huy phan ||| huy le nguyen ||| oliver y. ch ||| n ||| philipp koch ||| ngoc q. k. duong ||| ian mcloughlin ||| alfred mertins ||| 
2021 ||| deep gaussian denoiser epistemic uncertainty and decoupled dual-attention fusion. ||| xiaoqi ma ||| xiaoyu lin ||| majed el helou ||| sabine s ||| sstrunk ||| 
2020 ||| liver segmentation in abdominal ct images via auto-context neural network and self-supervised contour attention. ||| minyoung chung ||| jingyu lee ||| jeongjin lee ||| yeong-gil shin ||| 
2021 ||| attribution mask: filtering out irrelevant features by recursively focusing attention on inputs of dnns. ||| jae-hong lee ||| joon-hyuk chang ||| 
2019 ||| logic and the 2-simplicial transformer. ||| james clift ||| dmitry doryn ||| daniel murfet ||| james wallbridge ||| 
2022 ||| gascn: graph attention shape completion network. ||| haojie huang ||| ziyi yang ||| robert platt ||| 
2020 ||| regularized forward-backward decoder for attention models. ||| tobias watzel ||| ludwig k ||| rzinger ||| lujun li ||| gerhard rigoll ||| 
2021 ||| rca-iunet: a residual cross-spatial attention guided inception u-net model for tumor segmentation in breast ultrasound imaging. ||| narinder singh punn ||| sonali agarwal ||| 
2022 ||| a lightweight and accurate spatial-temporal transformer for traffic forecasting. ||| guanyao li ||| shuhan zhong ||| letian xiang ||| s.-h. gary chan ||| ruiyuan li ||| chih-chieh hung ||| wen-chih peng ||| 
2021 ||| repairing human trust by promptly correcting robot mistakes with an attention transfer model. ||| ruijiao luo ||| chao huang ||| yuntao peng ||| boyi song ||| rui liu ||| 
2021 ||| 3d object tracking with transformer. ||| yubo cui ||| zheng fang ||| jiayao shan ||| zuoxu gu ||| sifan zhou ||| 
2021 ||| bossnas: exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. ||| changlin li ||| tao tang ||| guangrun wang ||| jiefeng peng ||| bing wang ||| xiaodan liang ||| xiaojun chang ||| 
2019 ||| improving referring expression grounding with cross-modal attention-guided erasing. ||| xihui liu ||| zihao wang ||| jing shao ||| xiaogang wang ||| hongsheng li ||| 
2021 ||| scaled-time-attention robust edge network. ||| richard lau ||| lihan yao ||| todd huster ||| william johnson ||| stephen arleth ||| justin wong ||| devin ridge ||| michael fletcher ||| william c. headley ||| 
2020 ||| attributes-guided and pure-visual attention alignment for few-shot recognition. ||| siteng huang ||| min zhang ||| yachen kang ||| donglin wang ||| 
2020 ||| hat: hardware-aware transformers for efficient natural language processing. ||| hanrui wang ||| zhanghao wu ||| zhijian liu ||| han cai ||| ligeng zhu ||| chuang gan ||| song han ||| 
2021 ||| invertible attention. ||| jiajun zha ||| yiran zhong ||| jing zhang ||| richard hartley ||| liang zheng ||| 
2018 ||| attending to mathematical language with transformers. ||| artit wangperawong ||| 
2020 ||| strokecoder: path-based image generation from single examples using transformers. ||| sabine wieluch ||| friedhelm schwenker ||| 
2018 ||| same representation, different attentions: shareable sentence representation learning from multiple tasks. ||| renjie zheng ||| junkun chen ||| xipeng qiu ||| 
2020 ||| mapping the space of chemical reactions using attention-based neural networks. ||| philippe schwaller ||| daniel probst ||| alain c. vaucher ||| vishnu h. nair ||| david kreutter ||| teodoro laino ||| jean-louis reymond ||| 
2021 ||| anatomical-guided attention enhances unsupervised pet image denoising performance. ||| yuya onishi ||| fumio hashimoto ||| kibo ote ||| hiroyuki ohba ||| ryosuke ota ||| etsuji yoshikawa ||| yasuomi ouchi ||| 
2018 ||| ram: residual attention module for single image super-resolution. ||| jun-hyuk kim ||| jun-ho choi ||| manri cheon ||| jong-seok lee ||| 
2018 ||| stacked dense u-nets with dual transformers for robust face alignment. ||| jia guo ||| jiankang deng ||| niannan xue ||| stefanos zafeiriou ||| 
2018 ||| learning how to listen: a temporal-frequential attention model for sound event detection. ||| yu-han shen ||| ke-xin he ||| wei-qiang zhang ||| 
2020 ||| adaptiveweighted attention network with camera spectral sensitivity prior for spectral reconstruction from rgb images. ||| jiaojiao li ||| chaoxiong wu ||| rui song ||| yunsong li ||| fei liu ||| 
2019 ||| mixed pooling multi-view attention autoencoder for representation learning in healthcare. ||| shaika chowdhury ||| chenwei zhang ||| philip s. yu ||| yuan luo ||| 
2019 ||| eyenet: attention based convolutional encoder-decoder network for eye region segmentation. ||| priya kansal ||| sabari nathan ||| 
2021 ||| trunet: transformer-recurrent-u network for multi-channel reverberant sound source separation. ||| ali aroudi ||| stefan uhlich ||| marc ferras font ||| 
2021 ||| implicit transformer network for screen content image continuous super-resolution. ||| jingyu yang ||| sheng shen ||| huanjing yue ||| kun li ||| 
2021 ||| trilateral attention network for real-time medical image segmentation. ||| ghada zamzmi ||| vandana sachdev ||| sameer k. antani ||| 
2022 ||| a unified framework for attention-based few-shot object detection. ||| pierre le jeune ||| anissa mokraoui ||| 
2021 ||| unsupervised mri reconstruction via zero-shot learned adversarial transformers. ||| yilmaz korkmaz ||| salman ul hassan dar ||| mahmut yurt ||| muzaffer  ||| zbey ||| tolga  ||| ukur ||| 
2019 ||| attention-guided low-light image enhancement. ||| feifan lv ||| feng lu ||| 
2022 ||| ctformer: convolution-free token2token dilated vision transformer for low-dose ct denoising. ||| dayang wang ||| fenglei fan ||| zhan wu ||| rui liu ||| fei wang ||| hengyong yu ||| 
2020 ||| exploiting temporal attention features for effective denoising in videos. ||| aryansh omray ||| samyak jain ||| utsav krishnan ||| pratik chattopadhyay ||| 
2018 ||| lstms with attention for aggression detection. ||| nishant nikhil ||| ramit pahwa ||| mehul kumar nirala ||| rohan khilnani ||| 
2021 ||| transcouplet: transformer based chinese couplet generation. ||| kuan-yu chiang ||| shihao lin ||| joe chen ||| qian yin ||| qizhen jin ||| 
2019 ||| coherent semantic attention for image inpainting. ||| hongyu liu ||| bin jiang ||| yi xiao ||| chao yang ||| 
2019 ||| great ape detection in challenging jungle camera trap footage via attention-based spatial and temporal feature blending. ||| xinyu yang ||| majid mirmehdi ||| tilo burghardt ||| 
2021 ||| learning vision transformer with squeeze and excitation for facial expression recognition. ||| mouath aouayeb ||| wassim hamidouche ||| catherine soladi ||| kidiyo kpalma ||| renaud s ||| guier ||| 
2019 ||| video-based convolutional attention for person re-identification. ||| marco zamprogno ||| marco passon ||| niki martinel ||| giuseppe serra ||| giuseppe lancioni ||| christian micheloni ||| carlo tasso ||| gian luca foresti ||| 
2019 ||| dual path multi-scale fusion networks with attention for crowd counting. ||| liang zhu ||| zhijian zhao ||| chao lu ||| yining lin ||| yao peng ||| tangren yao ||| 
2020 ||| automatic composition of guitar tabs by transformers and groove modeling. ||| yu-hua chen ||| yu-hsiang huang ||| wen-yi hsiao ||| yi-hsuan yang ||| 
2021 ||| attend and guide (ag-net): a keypoints-driven attention-based deep network for image recognition. ||| asish bera ||| zachary wharton ||| yonghuai liu ||| nik bessis ||| ardhendu behera ||| 
2021 ||| sea: graph shell attention in graph neural networks. ||| christian m. m. frey ||| yunpu ma ||| matthias schubert ||| 
2021 ||| dropout regularization for self-supervised learning of transformer encoder speech representation. ||| jian luo ||| jianzong wang ||| ning cheng ||| jing xiao ||| 
2021 ||| multi-modal land cover mapping of remote sensing images using pyramid attention and gated fusion networks. ||| qinghui liu ||| michael kampffmeyer ||| robert jenssen ||| arnt-b ||| rre salberg ||| 
2018 ||| attention-based neural text segmentation. ||| pinkesh badjatiya ||| litton j. kurisinkel ||| manish gupta ||| vasudeva varma ||| 
2021 ||| covid-net us: a tailored, highly efficient, self-attention deep convolutional neural network design for detection of covid-19 patient cases from point-of-care ultrasound imaging. ||| alexander maclean ||| saad abbasi ||| ashkan ebadi ||| andy zhao ||| maya pavlova ||| hayden gunraj ||| pengcheng xi ||| sonny kohli ||| alexander wong ||| 
2021 ||| eigen analysis of self-attention and its reconstruction from partial computation. ||| srinadh bhojanapalli ||| ayan chakrabarti ||| himanshu jain ||| sanjiv kumar ||| michal lukasik ||| andreas veit ||| 
2020 ||| feathertts: robust and efficient attention based neural tts. ||| qiao tian ||| zewang zhang ||| chao liu ||| heng lu ||| linghui chen ||| bin wei ||| pujiang he ||| shan liu ||| 
2022 ||| swin transformer for fast mri. ||| jiahao huang ||| yingying fang ||| yinzhe wu ||| huanjun wu ||| zhifan gao ||| yang li ||| javier del ser ||| jun xia ||| guang yang ||| 
2021 ||| mvt: multi-view vision transformer for 3d object recognition. ||| shuo chen ||| tan yu ||| ping li ||| 
2022 ||| geometric transformer for fast and robust point cloud registration. ||| zheng qin ||| hao yu ||| changjian wang ||| yulan guo ||| yuxing peng ||| kai xu ||| 
2018 ||| predicting blood pressure response to fluid bolus therapy using attention-based neural networks for clinical interpretability. ||| uma m. girkar ||| ryo uchimido ||| li-wei h. lehman ||| peter szolovits ||| leo a. celi ||| wei-hung weng ||| 
2020 ||| normalized attention without probability cage. ||| oliver richter ||| roger wattenhofer ||| 
2018 ||| compositional attention networks for machine reasoning. ||| drew a. hudson ||| christopher d. manning ||| 
2019 ||| medical time series classification with hierarchical attention-based temporal convolutional networks: a case study of myotonic dystrophy diagnosis. ||| lei lin ||| beilei xu ||| wencheng wu ||| trevor w. richardson ||| edgar a. bernal ||| 
2022 ||| transformers and the representation of biomedical background knowledge. ||| oskar wysocki ||| zili zhou ||| paul o'regan ||| deborah ferreira ||| magdalena wysocka ||| donal landers ||| andr |||  freitas ||| 
2019 ||| event-based attention and tracking on neuromorphic hardware. ||| alpha renner ||| matthew evanusa ||| yulia sandamirskaya ||| 
2019 ||| attentionrnn: a structured spatial attention mechanism. ||| siddhesh khandelwal ||| leonid sigal ||| 
2020 ||| towards fully 8-bit integer inference for the transformer model. ||| ye lin ||| yanyang li ||| tengbo liu ||| tong xiao ||| tongran liu ||| jingbo zhu ||| 
2020 ||| ma-dst: multi-attention based scalable dialog state tracking. ||| adarsh kumar ||| peter ku ||| anuj kumar goyal ||| angeliki metallinou ||| dilek hakkani-t ||| r ||| 
2021 ||| iiitt@lt-edi-eacl2021-hope speech detection: there is always hope in transformers. ||| karthik puranik ||| adeep hande ||| ruba priyadharshini ||| sajeetha thavareesan ||| bharathi raja chakravarthi ||| 
2021 ||| face transformer for recognition. ||| yaoyao zhong ||| weihong deng ||| 
2021 ||| spectnt: a time-frequency transformer for music audio. ||| wei tsung lu ||| ju-chiang wang ||| minz won ||| keunwoo choi ||| xuchen song ||| 
2018 ||| attention-based deep multiple instance learning. ||| maximilian ilse ||| jakub m. tomczak ||| max welling ||| 
2019 ||| self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction. ||| yang li ||| guodong long ||| tao shen ||| tianyi zhou ||| lina yao ||| huan huo ||| jing jiang ||| 
2021 ||| single-read reconstruction for dna data storage using transformers. ||| yotam nahum ||| eyar ben-tolila ||| leon anavy ||| 
2019 ||| one-pass multi-task networks with cross-task guided attention for brain tumor segmentation. ||| chenhong zhou ||| changxing ding ||| xinchao wang ||| zhentai lu ||| dacheng tao ||| 
2020 ||| upb at semeval-2020 task 9: identifying sentiment in code-mixed social media texts using transformers and multi-task learning. ||| george-eduard zaharia ||| george-alexandru vlad ||| dumitru-clementin cercel ||| traian rebedea ||| costin-gabriel chiru ||| 
2021 ||| pay attention with focus: a novel learning scheme for classification of whole slide images. ||| shivam kalra ||| mohammed adnan ||| sobhan hemati ||| taher dehkharghanian ||| shahryar rahnamayan ||| hamid r. tizhoosh ||| 
2021 ||| transvg: end-to-end visual grounding with transformers. ||| jiajun deng ||| zhengyuan yang ||| tianlang chen ||| wengang zhou ||| houqiang li ||| 
2020 ||| attention-guided context feature pyramid network for object detection. ||| junxu cao ||| qi chen ||| jun guo ||| ruichao shi ||| 
2020 ||| capsules with inverted dot-product attention routing. ||| yao-hung hubert tsai ||| nitish srivastava ||| hanlin goh ||| ruslan salakhutdinov ||| 
2018 ||| pixel-wise attentional gating for parsimonious pixel labeling. ||| shu kong ||| charless c. fowlkes ||| 
2021 ||| melons: generating melody with long-term structure using transformers and structure graph. ||| yi zou ||| pei zou ||| yi zhao ||| kaixiang zhang ||| ran zhang ||| xiaorui wang ||| 
2021 ||| bitr-unet: a cnn-transformer combined network for mri brain tumor segmentation. ||| qiran jia ||| hai shu ||| 
2021 ||| attention, please! a survey of neural attention models in deep learning. ||| alana de santana correia ||| esther luna colombini ||| 
2021 ||| survtrace: transformers for survival analysis with competing events. ||| zifeng wang ||| jimeng sun ||| 
2019 ||| enhancing salient object segmentation through attention. ||| anuj pahuja ||| avishek majumder ||| anirban chakraborty ||| r. venkatesh babu ||| 
2020 ||| multi-modal attention for speech emotion recognition. ||| zexu pan ||| zhaojie luo ||| jichen yang ||| haizhou li ||| 
2020 ||| a novel fusion of attention and sequence to sequence autoencoders to predict sleepiness from speech. ||| shahin amiriparian ||| pawel winokurow ||| vincent karas ||| sandra ottl ||| maurice gerczuk ||| bj ||| rn w. schuller ||| 
2019 ||| granular multimodal attention networks for visual dialog. ||| badri n. patro ||| shivansh patel ||| vinay p. namboodiri ||| 
2018 ||| kernel transformer networks for compact spherical convolution. ||| yu-chuan su ||| kristen grauman ||| 
2021 ||| proposal-free one-stage referring expression via grid-word cross-attention. ||| wei suo ||| mengyang sun ||| peng wang ||| qi wu ||| 
2021 ||| docformer: end-to-end transformer for document understanding. ||| srikar appalaraju ||| bhavan jasani ||| bhargava urala kota ||| yusheng xie ||| r. manmatha ||| 
2022 ||| webformer: the web-page transformer for structure information extraction. ||| qifan wang ||| yi fang ||| anirudh ravula ||| fuli feng ||| xiaojun quan ||| dongfang liu ||| 
2019 ||| coulgat: an experiment on interpretability of graph attention networks. ||| burc gokden ||| 
2021 ||| end-to-end speaker height and age estimation using attention mechanism with lstm-rnn. ||| manav kaushik ||| van tung pham ||| eng siong chng ||| 
2020 ||| hybrid attention-based transformer block model for distant supervision relation extraction. ||| yan xiao ||| yaochu jin ||| ran cheng ||| kuangrong hao ||| 
2019 ||| towards generation of visual attention map for source code. ||| takeshi d. itoh ||| takatomi kubo ||| kiyoka ikeda ||| yuki maruno ||| yoshiharu ikutani ||| hideaki hata ||| kenichi matsumoto ||| kazushi ikeda ||| 
2020 ||| multi-task learning from videos via efficient inter-frame attention. ||| donghyun kim ||| tian lan ||| chuhang zou ||| ning xu ||| bryan a. plummer ||| stan sclaroff ||| jayan eledath ||| g ||| rard g. medioni ||| 
2020 ||| etc: encoding long and structured data in transformers. ||| joshua ainslie ||| santiago onta ||| n ||| chris alberti ||| philip pham ||| anirudh ravula ||| sumit sanghai ||| 
2020 ||| modulated fusion using transformer for linguistic-acoustic emotion recognition. ||| jean-benoit delbrouck ||| no |||  tits ||| st ||| phane dupont ||| 
2021 ||| vivit: a video vision transformer. ||| anurag arnab ||| mostafa dehghani ||| georg heigold ||| chen sun ||| mario lucic ||| cordelia schmid ||| 
2022 ||| rethinking attention-model explainability through faithfulness violation test. ||| yibing liu ||| haoliang li ||| yangyang guo ||| chenqi kong ||| jing li ||| shiqi wang ||| 
2021 ||| an end-to-end trainable video panoptic segmentation method usingtransformers. ||| jeongwon ryu ||| kwangjin yoon ||| 
2021 ||| sit: self-supervised vision transformer. ||| sara atito ali ahmed ||| muhammad awais ||| josef kittler ||| 
2020 ||| minilmv2: multi-head self-attention relation distillation for compressing pretrained transformers. ||| wenhui wang ||| hangbo bao ||| shaohan huang ||| li dong ||| furu wei ||| 
2019 ||| discourse-aware semantic self-attention for narrative reading comprehension. ||| todor mihaylov ||| anette frank ||| 
2018 ||| actor-attention-critic for multi-agent reinforcement learning. ||| shariq iqbal ||| fei sha ||| 
2021 ||| vad-free streaming hybrid ctc/attention asr for unsegmented recording. ||| hirofumi inaguma ||| tatsuya kawahara ||| 
2020 ||| an implicit attention mechanism for deep learning pedestrian re-identification frameworks. ||| ehsan yaghoubi ||| diana borza ||| aruna kumar ||| hugo proen ||| a ||| 
2019 ||| lane attention: predicting vehicles' moving trajectories by learning their attention over lanes. ||| jiacheng pan ||| hongyi sun ||| kecheng xu ||| yifei jiang ||| xiangquan xiao ||| jiangtao hu ||| jinghao miao ||| 
2019 ||| adaptive embedding gate for attention-based scene text recognition. ||| xiaoxue chen ||| tianwei wang ||| yuanzhi zhu ||| lianwen jin ||| canjie luo ||| 
2019 ||| why adam beats sgd for attention models. ||| jingzhao zhang ||| sai praneeth karimireddy ||| andreas veit ||| seungyeon kim ||| sashank j. reddi ||| sanjiv kumar ||| suvrit sra ||| 
2020 ||| dual attention model for citation recommendation. ||| yang zhang ||| qiang ma ||| 
2020 ||| attention as activation. ||| yimian dai ||| stefan oehmcke ||| yiquan wu ||| kobus barnard ||| 
2021 ||| sleeptransformer: automatic sleep staging with interpretability and uncertainty quantification. ||| huy phan ||| kaare b. mikkelsen ||| oliver y. ch ||| n ||| philipp koch ||| alfred mertins ||| maarten de vos ||| 
2022 ||| transformer memory as a differentiable search index. ||| yi tay ||| vinh q. tran ||| mostafa dehghani ||| jianmo ni ||| dara bahri ||| harsh mehta ||| zhen qin ||| kai hui ||| zhe zhao ||| jai prakash gupta ||| tal schuster ||| william w. cohen ||| donald metzler ||| 
2020 ||| untangling tradeoffs between recurrence and self-attention in neural networks. ||| giancarlo kerg ||| bhargav kanuparthi ||| anirudh goyal ||| kyle goyette ||| yoshua bengio ||| guillaume lajoie ||| 
2017 ||| visual-textual attention driven fine-grained representation learning. ||| xiangteng he ||| yuxin peng ||| 
2021 ||| pq-transformer: jointly parsing 3d objects and layouts from point clouds. ||| xiaoxue chen ||| hao zhao ||| guyue zhou ||| ya-qin zhang ||| 
2018 ||| universal transformers. ||| mostafa dehghani ||| stephan gouws ||| oriol vinyals ||| jakob uszkoreit ||| lukasz kaiser ||| 
2021 ||| tapl: dynamic part-based visual tracking via attention-guided part localization. ||| wei han ||| hantao huang ||| xiaoxi yu ||| 
2020 ||| spatial-temporal dynamic graph attention networks for ride-hailing demand prediction. ||| weiguo pian ||| yingbo wu ||| 
2021 ||| transreid: transformer-based object re-identification. ||| shuting he ||| hao luo ||| pichao wang ||| fan wang ||| hao li ||| wei jiang ||| 
2021 ||| streamult: streaming multimodal transformer for heterogeneous and arbitrary long sequential data. ||| victor pellegrain ||| myriam tami ||| michel batteux ||| c ||| line hudelot ||| 
2022 ||| short range correlation transformer for occluded person re-identification. ||| yunbin zhao ||| songhao zhu ||| dongsheng wang ||| zhiwei liang ||| 
2021 ||| edge-augmented graph transformers: global self-attention is enough for graphs. ||| md. shamim hussain ||| mohammed j. zaki ||| dharmashankar subramanian ||| 
2019 ||| very deep self-attention networks for end-to-end speech recognition. ||| ngoc-quan pham ||| thai-son nguyen ||| jan niehues ||| markus m ||| ller ||| alex waibel ||| 
2020 ||| machine translation of novels in the age of transformer. ||| antonio toral ||| antoni oliver ||| pau ribas ballest ||| n ||| 
2020 ||| bilinear fusion of commonsense knowledge with attention-based nli models. ||| amit gajbhiye ||| thomas winterbottom ||| noura al moubayed ||| steven bradley ||| 
2020 ||| safe: self-attention based unsupervised road safety classification in hazardous environments. ||| divya kothandaraman ||| rohan chandra ||| dinesh manocha ||| 
2021 ||| a unified transformer-based framework for duplex text normalization. ||| tuan manh lai ||| yang zhang ||| evelina bakhturina ||| boris ginsburg ||| heng ji ||| 
2022 ||| on guiding visual attention with language specification. ||| suzanne petryk ||| lisa dunlap ||| keyan nasseri ||| joseph gonzalez ||| trevor darrell ||| anna rohrbach ||| 
2019 ||| pre-training of graph augmented transformers for medication recommendation. ||| junyuan shang ||| tengfei ma ||| cao xiao ||| jimeng sun ||| 
2018 ||| learning deep structured multi-scale features using attention-gated crfs for contour prediction. ||| dan xu ||| wanli ouyang ||| xavier alameda-pineda ||| elisa ricci ||| xiaogang wang ||| nicu sebe ||| 
2020 ||| multi-modal feature fusion with feature attention for vatex captioning challenge 2020. ||| ke lin ||| zhuoxin gan ||| liwei wang ||| 
2019 ||| crowd counting using scale-aware attention networks. ||| mohammad asiful hossain ||| mehrdad hosseinzadeh ||| omit chanda ||| yang wang ||| 
2018 ||| an introductory survey on attention mechanisms in nlp problems. ||| dichao hu ||| 
2019 ||| multi-scale body-part mask guided attention for person re-identification. ||| honglong cai ||| zhiguan wang ||| jinxing cheng ||| 
2022 ||| wasserstein adversarial transformer for cloud workload prediction. ||| shivani arbat ||| vinodh kumaran jayakumar ||| jaewoo lee ||| wei wang ||| in kee kim ||| 
2022 ||| xai for transformers: better explanations through conservative propagation. ||| ameen ali ||| thomas schnake ||| oliver eberle ||| gr ||| goire montavon ||| klaus-robert m ||| ller ||| lior wolf ||| 
2022 ||| compare learning: bi-attention network for few-shot learning. ||| li ke ||| meng pan ||| weigao wen ||| dong li ||| 
2021 ||| dynamic allocation of visual attention for vision-based autonomous navigation under data rate constraints. ||| ali reza pedram ||| riku funada ||| takashi tanaka ||| 
2021 ||| a single-target license plate detection with attention. ||| wenyun li ||| chi-man pun ||| 
2019 ||| polytransform: deep polygon transformer for instance segmentation. ||| justin liang ||| namdar homayounfar ||| wei-chiu ma ||| yuwen xiong ||| rui hu ||| raquel urtasun ||| 
2021 ||| generalizing rnn-transducer to out-domain audio via sparse self-attention layers. ||| juntae kim ||| jeehye lee ||| yoonhan lee ||| 
2022 ||| global tracking transformers. ||| xingyi zhou ||| tianwei yin ||| vladlen koltun ||| phillip kr ||| henb ||| hl ||| 
2017 ||| classification of radiology reports using neural attention models. ||| bonggun shin ||| falgun h. chokshi ||| timothy lee ||| jinho d. choi ||| 
2021 ||| ctrl-c: camera calibration transformer with line-classification. ||| jinwoo lee ||| hyunsung go ||| hyunjoon lee ||| sunghyun cho ||| min-hyuk sung ||| junho kim ||| 
2018 ||| lsta: long short-term attention for egocentric action recognition. ||| swathikiran sudhakaran ||| sergio escalera ||| oswald lanz ||| 
2019 ||| example-guided scene image synthesis using masked spatial-channel attention and patch-based self-supervision. ||| haitian zheng ||| haofu liao ||| lele chen ||| wei xiong ||| tianlang chen ||| jiebo luo ||| 
2022 ||| formula graph self-attention network for representation-domain independent materials discovery. ||| achintha ihalage ||| yang hao ||| 
2021 ||| motion guided attention fusion to recognize interactions from videos. ||| tae soo kim ||| jonathan d. jones ||| gregory d. hager ||| 
2022 ||| internal language model estimation through explicit context vector learning for attention-based encoder-decoder asr. ||| yufei liu ||| rao ma ||| haihua xu ||| yi he ||| zejun ma ||| weibin zhang ||| 
2021 ||| a unified generative adversarial network training via self-labeling and self-attention. ||| tomoki watanabe ||| paolo favaro ||| 
2021 ||| region attention and graph embedding network for occlusion objective class-based micro-expression recognition. ||| qirong mao ||| ling zhou ||| wenming zheng ||| xiuyan shao ||| xiaohua huang ||| 
2019 ||| state-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions. ||| kyu j. han ||| ramon prieto ||| kaixing wu ||| tao ma ||| 
2021 ||| musiq: multi-scale image quality transformer. ||| junjie ke ||| qifei wang ||| yilin wang ||| peyman milanfar ||| feng yang ||| 
2021 ||| omninet: omnidirectional representations from transformers. ||| yi tay ||| mostafa dehghani ||| vamsi aribandi ||| jai prakash gupta ||| philip pham ||| zhen qin ||| dara bahri ||| da-cheng juan ||| donald metzler ||| 
2021 ||| paint transformer: feed forward neural painting with stroke prediction. ||| songhua liu ||| tianwei lin ||| dongliang he ||| fu li ||| ruifeng deng ||| xin li ||| errui ding ||| hao wang ||| 
2020 ||| a survey on principles, models and methods for learning from irregularly sampled time series: from discretization to attention and invariance. ||| satya narayan shukla ||| benjamin m. marlin ||| 
2021 ||| extracting qualitative causal structure with transformer-based nlp. ||| scott e. friedman ||| ian h. magnusson ||| sonja m. schmer-galunder ||| 
2020 ||| tsam: temporal link prediction in directed networks based on self-attention mechanism. ||| jinsong li ||| jianhua peng ||| shuxin liu ||| lintianran weng ||| cong li ||| 
2022 ||| eeg to fmri synthesis benefits from attentional graphs of electrode relationships. ||| david calhas ||| rui henriques ||| 
2021 ||| you only sample (almost) once: linear cost self-attention via bernoulli sampling. ||| zhanpeng zeng ||| yunyang xiong ||| sathya n. ravi ||| shailesh acharya ||| glenn fung ||| vikas singh ||| 
2021 ||| a comparison for anti-noise robustness of deep learning classification methods on a tiny object image dataset: from convolutional neural network to visual transformer and performer. ||| ao chen ||| chen li ||| haoyuan chen ||| hechen yang ||| peng zhao ||| weiming hu ||| wanli liu ||| shuojia zou ||| marcin grzegorzek ||| 
2021 ||| swinbert: end-to-end transformers with sparse attention for video captioning. ||| kevin lin ||| linjie li ||| chung-ching lin ||| faisal ahmed ||| zhe gan ||| zicheng liu ||| yumao lu ||| lijuan wang ||| 
2020 ||| off-policy self-critical training for transformer in visual paragraph generation. ||| shiyang yan ||| yang hua ||| neil martin robertson ||| 
2019 ||| learning where to see: a novel attention model for automated immunohistochemical scoring. ||| talha qaiser ||| nasir m. rajpoot ||| 
2019 ||| transformer dissection: an unified understanding for transformer's attention via the lens of kernel. ||| yao-hung hubert tsai ||| shaojie bai ||| makoto yamada ||| louis-philippe morency ||| ruslan salakhutdinov ||| 
2021 ||| pedestrian attribute recognition in video surveillance scenarios based on view-attribute attention localization. ||| weichen chen ||| xinyi yu ||| linlin ou ||| 
2020 ||| transformer query-target knowledge discovery (tend): drug discovery from cord-19. ||| leo k. tam ||| xiaosong wang ||| daguang xu ||| 
2020 ||| la furca: iterative context-aware end-to-end monaural speech separation based on dual-path deep parallel inter-intra bi-lstm with attention. ||| ziqiang shi ||| rujie liu ||| jiqing han ||| 
2021 ||| exploring vision transformers for fine-grained classification. ||| marcos v. conde ||| kerem turgutlu ||| 
2019 ||| a bilingual generative transformer for semantic sentence embedding. ||| john wieting ||| graham neubig ||| taylor berg-kirkpatrick ||| 
2020 ||| attention u-net based adversarial architectures for chest x-ray lung segmentation. ||| guszt ||| v ga ||| l ||| bal ||| zs maga ||| andr ||| s luk ||| cs ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| chiara plizzari ||| marco cannici ||| matteo matteucci ||| 
2018 ||| a self-attention network for hierarchical data structures with an application to claims management. ||| leander l ||| w ||| martin spindler ||| eike c. brechmann ||| 
2022 ||| transformers in self-supervised monocular depth estimation with unknown camera intrinsics. ||| arnav varma ||| hemang chawla ||| bahram zonooz ||| elahe arani ||| 
2018 ||| attention models with random features for multi-layered graph embeddings. ||| uday shankar shanthamallu ||| jayaraman j. thiagarajan ||| huan song ||| andreas spanias ||| 
2021 ||| spherical transformer: adapting spherical signal to cnns. ||| haikuan du ||| hui cao ||| shen cai ||| junchi yan ||| siyu zhang ||| 
2021 ||| hierarchical self attention based autoencoder for open-set human activity recognition. ||| m. tanjid hasan tonmoy ||| saif mahmud ||| a. k. m. mahbubur rahman ||| m. ashraful amin ||| amin ahsan ali ||| 
2021 ||| soft-sensing conformer: a curriculum learning-based convolutional transformer. ||| jaswanth yella ||| chao zhang ||| sergei petrov ||| yu huang ||| xiaoye qian ||| ali a. minai ||| sthitie bom ||| 
2018 ||| senti-attend: image captioning using sentiment and attention. ||| omid mohamad nezami ||| mark dras ||| stephen wan ||| c ||| cile paris ||| 
2021 ||| attention on global-local embedding spaces in recommender systems. ||| munlika rattaphun ||| wen-chieh fang ||| chih-yi chiu ||| 
2021 ||| multi-stream attention learning for monocular vehicle velocity and inter-vehicle distance estimation. ||| kuan-chih huang ||| yu-kai huang ||| winston h. hsu ||| 
2018 ||| troy: give attention to saliency and for saliency. ||| 
2020 ||| on optimal transformer depth for low-resource language translation. ||| elan van biljon ||| arnu pretorius ||| julia kreutzer ||| 
2020 ||| confidence-aware non-repetitive multimodal transformers for textcaps. ||| zhaokai wang ||| renda bao ||| qi wu ||| si liu ||| 
2021 ||| power transformer fault diagnosis with intrinsic time-scale decomposition and xgboost classifier. ||| shoaib meraj sami ||| mohammed imamul hassan bhuiyan ||| 
2020 ||| focus of attention improves information transfer in visual features. ||| matteo tiezzi ||| stefano melacci ||| alessandro betti ||| marco maggini ||| marco gori ||| 
2019 ||| on the relationship between self-attention and convolutional layers. ||| jean-baptiste cordonnier ||| andreas loukas ||| martin jaggi ||| 
2020 ||| contextual pyramid attention network for building segmentation in aerial imagery. ||| clint sebastian ||| raffaele imbriaco ||| egor bondarev ||| peter h. n. de with ||| 
2020 ||| the perceptual boost of visual attention is task-dependent in naturalistic settings. ||| freddie bickford smith ||| xiaoliang luo ||| brett d. roads ||| bradley c. love ||| 
2021 ||| statistically meaningful approximation: a case study on approximating turing machines with transformers. ||| colin wei ||| yining chen ||| tengyu ma ||| 
2019 ||| learning where to look: semantic-guided multi-attention localization for zero-shot learning. ||| yizhe zhu ||| jianwen xie ||| zhiqiang tang ||| xi peng ||| ahmed elgammal ||| 
2018 ||| psychophysical evaluation of individual low-level feature influences on visual attention. ||| david berga ||| xos |||  ram ||| n fdez-vidal ||| xavier otazu ||| v ||| ctor lebor ||| n ||| xos |||  m. pardo ||| 
2021 ||| an attention free transformer. ||| shuangfei zhai ||| walter talbott ||| nitish srivastava ||| chen huang ||| hanlin goh ||| ruixiang zhang ||| josh m. susskind ||| 
2021 ||| dual attention-in-attention model for joint rain streak and raindrop removal. ||| kaihao zhang ||| dongxu li ||| wenhan luo ||| wenqi ren ||| lin ma ||| hongdong li ||| 
2017 ||| human trajectory prediction using spatially aware deep attention models. ||| daksh varshneya ||| g. srinivasaraghavan ||| 
2019 ||| link prediction with mutual attention for text-attributed networks. ||| robin brochier ||| adrien guille ||| julien velcin ||| 
2021 ||| cetransformer: casual effect estimation via transformer based representation learning. ||| zhenyu guo ||| shuai zheng ||| zhizhe liu ||| kun yan ||| zhenfeng zhu ||| 
2021 ||| is attention to bounding boxes all you need for pedestrian action prediction? ||| lina achaji ||| julien moreau ||| thibault fouqueray ||| fran ||| ois aioun ||| fran ||| ois charpillet ||| 
2021 ||| enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning. ||| jinsong su ||| jialong tang ||| hui jiang ||| ziyao lu ||| yubin ge ||| linfeng song ||| deyi xiong ||| le sun ||| jiebo luo ||| 
2021 ||| convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. ||| xinyang feng ||| dongjin song ||| yuncong chen ||| zhengzhang chen ||| jingchao ni ||| haifeng chen ||| 
2021 ||| siamese transformer pyramid networks for real-time uav tracking. ||| daitao xing ||| nikolaos evangeliou ||| athanasios tsoukalas ||| anthony tzes ||| 
2022 ||| aggregated pyramid vision transformer: split-transform-merge strategy for image recognition without convolutions. ||| rui-yang ju ||| ting-yu lin ||| jen-shiun chiang ||| jia-hao jian ||| yu-shian lin ||| liu-rui-yi huang ||| 
2021 ||| transmil: transformer based correlated multiple instance learning for whole slide image classication. ||| zhuchen shao ||| hao bian ||| yang chen ||| yifeng wang ||| jian zhang ||| xiangyang ji ||| yongbing zhang ||| 
2020 ||| probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles. ||| chenxu luo ||| lin sun ||| dariush dabiri ||| alan l. yuille ||| 
2019 ||| federated multi-task hierarchical attention model for sensor analytics. ||| yujing chen ||| yue ning ||| zheng chai ||| huzefa rangwala ||| 
2018 ||| hyperbolic attention networks. ||| aglar g ||| l ||| ehre ||| misha denil ||| mateusz malinowski ||| ali razavi ||| razvan pascanu ||| karl moritz hermann ||| peter w. battaglia ||| victor bapst ||| david raposo ||| adam santoro ||| nando de freitas ||| 
2020 ||| dual attention gans for semantic image synthesis. ||| hao tang ||| song bai ||| nicu sebe ||| 
2020 ||| e2eet: from pipeline to end-to-end entity typing via transformer-based embeddings. ||| michael stewart ||| wei liu ||| 
2022 ||| ensembles of vision transformers as a new paradigm for automated classification in ecology. ||| s. p. kyathanahally ||| t. hardeman ||| m. reyes ||| e. merz ||| t. bulas ||| f. pomati ||| marco baity-jesi ||| 
2022 ||| es-drnn with dynamic attention for short-term load forecasting. ||| slawek smyl ||| grzegorz dudek ||| pawel pelka ||| 
2021 ||| charformer: fast character transformers via gradient-based subword tokenization. ||| yi tay ||| vinh q. tran ||| sebastian ruder ||| jai prakash gupta ||| hyung won chung ||| dara bahri ||| zhen qin ||| simon baumgartner ||| cong yu ||| donald metzler ||| 
2021 ||| motion planning transformers: one model to plan them all. ||| jacob j. johnson ||| linjun li ||| ahmed hussain qureshi ||| michael c. yip ||| 
2019 ||| prediction of reaction time and vigilance variability from spatiospectral features of resting-state eeg in a long sustained attention task. ||| mastaneh torkamani-azar ||| sumeyra demir kanik ||| serap aydin ||| m ||| jdat  ||| etin ||| 
2022 ||| does entity abstraction help generative transformers reason? ||| nicolas gontier ||| siva reddy ||| christopher pal ||| 
2021 ||| how attentive are graph attention networks? ||| shaked brody ||| uri alon ||| eran yahav ||| 
2021 ||| decoding high-level imagined speech using attention-based deep neural networks. ||| dae-hyeok lee ||| sung-jin kim ||| keon-woo lee ||| 
2017 ||| predicting the driver's focus of attention: the dr(eye)ve project. ||| andrea palazzi ||| davide abati ||| simone calderara ||| francesco solera ||| rita cucchiara ||| 
2020 ||| code prediction by feeding trees to transformers. ||| seohyun kim ||| jinman zhao ||| yuchi tian ||| satish chandra ||| 
2021 ||| the brownian motion in the transformer model. ||| yingshi chen ||| 
2021 ||| lightxml: transformer with dynamic negative sampling for high-performance extreme multi-label text classification. ||| ting jiang ||| deqing wang ||| leilei sun ||| huayi yang ||| zhengyang zhao ||| fuzhen zhuang ||| 
2020 ||| vertical-horizontal structured attention for generating music with chords. ||| yizhou zhao ||| liang qiu ||| wensi ai ||| feng shi ||| song-chun zhu ||| 
2021 ||| multi-rate attention architecture for fast streamable text-to-speech spectrum modeling. ||| qing he ||| zhiping xiu ||| thilo k ||| hler ||| jilong wu ||| 
2019 ||| et-usb: transformer-based sequential behavior modeling for inbound customer service. ||| ta-chun su ||| guan-ying chen ||| 
2021 ||| transformer-based approach towards music emotion recognition from lyrics. ||| yudhik agrawal ||| ramaguru guru ravi shanker ||| vinoo alluri ||| 
2019 ||| bag: bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. ||| yu cao ||| meng fang ||| dacheng tao ||| 
2020 ||| pum at semeval-2020 task 12: aggregation of transformer-based models' features for offensive language recognition. ||| piotr janiszewski ||| mateusz skiba ||| urszula walinska ||| 
2021 ||| a unified efficient pyramid transformer for semantic segmentation. ||| fangrui zhu ||| yi zhu ||| li zhang ||| chongruo wu ||| yanwei fu ||| mu li ||| 
2020 ||| bangla text classification using transformers. ||| tanvirul alam ||| akib khan ||| firoj alam ||| 
2019 ||| tree-transformer: a transformer-based method for correction of tree-structured data. ||| jacob harer ||| christopher p. reale ||| peter chin ||| 
2019 ||| what would elsa do? freezing layers during transformer fine-tuning. ||| jaejun lee ||| raphael tang ||| jimmy lin ||| 
2019 ||| attention monitoring and hazard assessment with bio-sensing and vision: empirical analysis utilizing cnns on the kitti dataset. ||| siddharth ||| mohan m. trivedi ||| 
2017 ||| attention-based vocabulary selection for nmt decoding. ||| baskaran sankaran ||| markus freitag ||| yaser al-onaizan ||| 
2021 ||| aei: actors-environment interaction with adaptive attention for temporal action proposals generation. ||| khoa vo ||| hyekang joo ||| kashu yamazaki ||| sang truong ||| kris kitani ||| minh-triet tran ||| ngan le ||| 
2021 ||| sparse fuzzy attention for structured sentiment analysis. ||| letian peng ||| zuchao li ||| hai zhao ||| 
2019 ||| pcan: 3d attention map learning using contextual information for point cloud based retrieval. ||| wenxiao zhang ||| chunxia xiao ||| 
2021 ||| temgnet: deep transformer-based decoding of upperlimb semg for hand gestures recognition. ||| elahe rahimian ||| soheil zabihi ||| amir asif ||| dario farina ||| seyed farokh atashzar ||| arash mohammadi ||| 
2020 ||| superbloom: bloom filter meets transformer. ||| john r. anderson ||| qingqing huang ||| walid krichene ||| steffen rendle ||| li zhang ||| 
2019 ||| attention-based multi-instance neural network for medical diagnosis from incomplete and low quality data. ||| zeyuan wang ||| josiah poon ||| shiding sun ||| simon k. poon ||| 
2018 ||| attention based visual analysis for fast grasp planning with multi-fingered robotic hand. ||| zhen deng ||| ge gao ||| simone frintrop ||| jianwei zhang ||| 
2021 ||| transclaw u-net: claw u-net with transformers for medical image segmentation. ||| yao chang ||| menghan hu ||| guangtao zhai ||| xiao-ping zhang ||| 
2020 ||| autoregressive reasoning over chains of facts with transformers. ||| ruben cartuyvels ||| graham spinks ||| marie-francine moens ||| 
2020 ||| learning to solve vehicle routing problems with time windows through joint attention. ||| jonas k. falkner ||| lars schmidt-thieme ||| 
2018 ||| unsupervised word segmentation from speech with attention. ||| pierre godard ||| marcely zanon boito ||| lucas ondel ||| alexandre berard ||| fran ||| ois yvon ||| aline villavicencio ||| laurent besacier ||| 
2019 ||| singing synthesis: with a little help from my attention. ||| orazio angelini ||| alexis moinet ||| kayoko yanagisawa ||| thomas drugman ||| 
2022 ||| stacked hybrid-attention and group collaborative learning for unbiased scene graph generation. ||| xingning dong ||| tian gan ||| xuemeng song ||| jianlong wu ||| yuan cheng ||| liqiang nie ||| 
2021 ||| deep learning transformer architecture for named entity recognition on low resourced languages: state of the art results. ||| ridewaan hanslo ||| 
2019 ||| video question generation via cross-modal self-attention networks learning. ||| yu-siang wang ||| hung-ting su ||| chen-hsi chang ||| winston h. hsu ||| 
2019 ||| smart: training shallow memory-aware transformers for robotic explainability. ||| marcella cornia ||| lorenzo baraldi ||| rita cucchiara ||| 
2021 ||| fastformer: additive attention can be all you need. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| yongfeng huang ||| xing xie ||| 
2020 ||| spatial-spectral ffpnet: attention-based pyramid network for segmentation and classification of remote sensing images. ||| qingsong xu ||| xin yuan ||| chaojun ouyang ||| yue zeng ||| 
2021 ||| exploiting both domain-specific and invariant knowledge via a win-win transformer for unsupervised domain adaptation. ||| wenxuan ma ||| jinming zhang ||| shuang li ||| chi harold liu ||| yulin wang ||| wei li ||| 
2019 ||| cross attention network for few-shot classification. ||| ruibing hou ||| hong chang ||| bingpeng ma ||| shiguang shan ||| xilin chen ||| 
2019 ||| complex transformer: a framework for modeling complex-valued sequence. ||| muqiao yang ||| martin q. ma ||| dongyu li ||| yao-hung hubert tsai ||| ruslan salakhutdinov ||| 
2020 ||| fast graph attention networks using effective resistance based graph sparsification. ||| rakshith sharma srinivasa ||| cao xiao ||| lucas glass ||| justin romberg ||| jimeng sun ||| 
2021 ||| stjla: a multi-context aware spatio-temporal joint linear attention network for traffic forecasting. ||| yuchen fang ||| yanjun qin ||| haiyong luo ||| fang zhao ||| chenxing wang ||| 
2021 ||| fdgatii : fast dynamic graph attention with initial residual and identity mapping. ||| gayan k. kulatilleke ||| marius portmann ||| ryan ko ||| shekhar s. chandra ||| 
2021 ||| an attention-based weakly supervised framework for spitzoid melanocytic lesion diagnosis in wsi. ||| roc ||| o del amor ||| la ||| titia launet ||| adri ||| n colomer ||| ana ||| s moscard ||| andr ||| s mosquera-zamudio ||| carlos monteagudo ||| valery naranjo ||| 
2019 ||| attention-guided network for ghost-free high dynamic range imaging. ||| qingsen yan ||| dong gong ||| qinfeng shi ||| anton van den hengel ||| chunhua shen ||| ian d. reid ||| yanning zhang ||| 
2021 ||| physics driven domain specific transporter framework with attention mechanism for ultrasound imaging. ||| arpan tripathi ||| abhilash rakkunedeth ||| mahesh raveendranatha panicker ||| jack zhang ||| naveenjyote boora ||| jessica knight ||| jacob l. jaremko ||| yale tung chen ||| kiran vishnu narayan ||| kesavadas c ||| 
2017 ||| time series forecasting using rnns: an extended attention mechanism to model periods and handle missing values. ||| yagmur gizem cinar ||| hamid mirisaee ||| parantapa goswami ||| ric gaussier ||| ali a ||| t-bachir ||| vadim v. strijov ||| 
2021 ||| pare: part attention regressor for 3d human body estimation. ||| muhammed kocabas ||| chun-hao p. huang ||| otmar hilliges ||| michael j. black ||| 
2020 ||| adapterdrop: on the efficiency of adapters in transformers. ||| andreas r ||| ckl ||| gregor geigle ||| max glockner ||| tilman beck ||| jonas pfeiffer ||| nils reimers ||| iryna gurevych ||| 
2021 ||| diagnosing transformers in task-oriented semantic parsing. ||| shrey desai ||| ahmed aly ||| 
2019 ||| a neural topic-attention model for medical term abbreviation disambiguation. ||| irene li ||| michihiro yasunaga ||| muhammed yavuz nuzumlali ||| cesar caraballo ||| shiwani mahajan ||| harlan m. krumholz ||| dragomir r. radev ||| 
2022 ||| hierarchical point cloud encoding and decoding with lightweight self-attention based model. ||| en yen puang ||| hao zhang ||| hongyuan zhu ||| wei jing ||| 
2021 ||| self-supervised pre-training for transformer-based person re-identification. ||| hao luo ||| pichao wang ||| yi xu ||| feng ding ||| yanxin zhou ||| fan wang ||| hao li ||| rong jin ||| 
2020 ||| mast: multimodal abstractive summarization with trimodal hierarchical attention. ||| aman khullar ||| udit arora ||| 
2019 ||| distilling transformers into simple neural networks with unlabeled transfer data. ||| subhabrata mukherjee ||| ahmed hassan awadallah ||| 
2017 ||| syntax-directed attention for neural machine translation. ||| 
2019 ||| pag-net: progressive attention guided depth super-resolution network. ||| arpit bansal ||| sankaraganesh jonna ||| rajiv r. sahay ||| 
2020 ||| multiscale mesh deformation component analysis with attention-based autoencoders. ||| jie yang ||| lin gao ||| qingyang tan ||| yihua huang ||| shihong xia ||| yu-kun lai ||| 
2021 ||| nxmtransformer: semi-structured sparsification for natural language understanding via admm. ||| connor holmes ||| minjia zhang ||| yuxiong he ||| bo wu ||| 
2018 ||| focal visual-text attention for visual question answering. ||| junwei liang ||| lu jiang ||| liangliang cao ||| li-jia li ||| alexander g. hauptmann ||| 
2021 ||| generic event boundary detection challenge at cvpr 2021 technical report: cascaded temporal attention network (castanet). ||| dexiang hong ||| congcong li ||| longyin wen ||| xinyao wang ||| libo zhang ||| 
2018 ||| bilinear attention networks. ||| jin-hwa kim ||| jaehyun jun ||| byoung-tak zhang ||| 
2018 ||| attention gated networks: learning to leverage salient regions in medical images. ||| jo schlemper ||| ozan oktay ||| michiel schaap ||| mattias p. heinrich ||| bernhard kainz ||| ben glocker ||| daniel rueckert ||| 
2021 ||| deep neural networks evolve human-like attention distribution during reading comprehension. ||| jiajie zou ||| nai ding ||| 
2021 ||| attention w-net: improved skip connections for better representations. ||| shikhar mohan ||| saumik bhattacharya ||| sayantari ghosh ||| 
2020 ||| eeg based continuous speech recognition using transformers. ||| gautam krishna ||| co tran ||| mason carnahan ||| ahmed h. tewfik ||| 
2021 ||| cvt-assd: convolutional vision-transformer based attentive single shot multibox detector. ||| weiqiang jin ||| hang yu ||| hang yu ||| 
2017 ||| what gets media attention and how media attention evolves over time - large-scale empirical evidence from 196 countries. ||| jisun an ||| haewoon kwak ||| 
2021 ||| do multilingual neural machine translation models contain language pair specific attention heads? ||| zae myung kim ||| laurent besacier ||| vassilina nikoulina ||| didier schwab ||| 
2018 ||| a fully attention-based information retriever. ||| alvaro henrique chaim correia ||| jorge luiz moreira silva ||| thiago de castro martins ||| f ||| bio gagliardi cozman ||| 
2019 ||| transfer learning from transformers to fake news challenge stance detection (fnc-1) task. ||| valeriya slovikovskaya ||| 
2022 ||| team yao at factify 2022: utilizing pre-trained models and co-attention networks for multi-modal fact verification. ||| wei-yao wang ||| wen-chih peng ||| 
2021 ||| bch-nlp at biocreative vii track 3: medications detection in tweets using transformer networks and multi-task learning. ||| dongfang xu ||| shan chen ||| timothy miller ||| 
2021 ||| gashis-transformer: a multi-scale visual transformer approach for gastric histopathology image classification. ||| haoyuan chen ||| chen li ||| xiaoyan li ||| weiming hu ||| yixin li ||| wanli liu ||| changhao sun ||| yudong yao ||| marcin grzegorzek ||| 
2020 ||| deep generative model for image inpainting with local binary pattern learning and spatial attention. ||| haiwei wu ||| jiantao zhou ||| yuanman li ||| 
2021 ||| corgi: content-rich graph neural networks with attention. ||| jooyeon kim ||| angus lamb ||| simon woodhead ||| simon peyton jones ||| cheng zheng ||| miltiadis allamanis ||| 
2021 ||| predicting attention sparsity in transformers. ||| marcos v. treviso ||| ant ||| nio g ||| is ||| patrick fernandes ||| erick r. fonseca ||| andr |||  f. t. martins ||| 
2021 ||| prose2poem: the blessing of transformers in translating prose to persian poetry. ||| reza khanmohammadi ||| mitra sadat mirshafiee ||| yazdan rezaee jouryabi ||| seyed abolghasem mirroshandel ||| 
2019 ||| divided we stand: a novel residual group attention mechanism for medical image segmentation. ||| chaitanya kaul ||| nick e. pears ||| suresh manandhar ||| 
2021 ||| soft sensing transformer: hundreds of sensors are worth a single word. ||| chao zhang ||| jaswanth yella ||| yu huang ||| xiaoye qian ||| sergei petrov ||| andrey rzhetsky ||| sthitie bom ||| 
2020 ||| pranet: parallel reverse attention network for polyp segmentation. ||| deng-ping fan ||| ge-peng ji ||| tao zhou ||| geng chen ||| huazhu fu ||| jianbing shen ||| ling shao ||| 
2020 ||| editor: an edit-based transformer with repositioning for neural machine translation with soft lexical constraints. ||| weijia xu ||| marine carpuat ||| 
2020 ||| a hierarchical transformer for unsupervised parsing. ||| ashok thillaisundaram ||| 
2022 ||| panformer: a transformer based model for pan-sharpening. ||| huanyu zhou ||| qingjie liu ||| yunhong wang ||| 
2018 ||| abdominal multi-organ segmentation with organ-attention networks and statistical fusion. ||| yan wang ||| yuyin zhou ||| wei shen ||| seyoun park ||| elliot k. fishman ||| alan l. yuille ||| 
2022 ||| an audio-visual attention based multimodal network for fake talking face videos detection. ||| ganglai wang ||| peng zhang ||| lei xie ||| wei huang ||| yufei zha ||| yanning zhang ||| 
2021 ||| high-resolution pelvic mri reconstruction using a generative adversarial network with attention and cyclic loss. ||| guangyuan li ||| jun lv ||| xiangrong tong ||| chengyan wang ||| guang yang ||| 
2021 ||| transferring knowledge with attention distillation for multi-domain image-to-image translation. ||| runze li ||| tomaso fontanini ||| luca donati ||| andrea prati ||| bir bhanu ||| 
2021 ||| multi-query multi-head attention pooling and inter-topk penalty for speaker verification. ||| miao zhao ||| yufeng ma ||| yiwei ding ||| yu zheng ||| min liu ||| minqiang xu ||| 
2020 ||| sign language transformers: joint end-to-end sign language recognition and translation. ||| necati cihan camg ||| z ||| oscar koller ||| simon hadfield ||| richard bowden ||| 
2020 ||| the elephant in the interpretability room: why use attention as explanation when we have saliency methods? ||| jasmijn bastings ||| katja filippova ||| 
2021 ||| recurrent glimpse-based decoder for detection with transformer. ||| zhe chen ||| jing zhang ||| dacheng tao ||| 
2021 ||| on biasing transformer attention towards monotonicity. ||| annette rios ||| chantal amrhein ||| no ||| mi aepli ||| rico sennrich ||| 
2021 ||| frequency effects on syntactic rule learning in transformers. ||| jason wei ||| dan garrette ||| tal linzen ||| ellie pavlick ||| 
2021 ||| beyond mono to binaural: generating binaural audio from mono audio with depth and cross modal attention. ||| kranti kumar parida ||| siddharth srivastava ||| gaurav sharma ||| 
2021 ||| what helps transformers recognize conversational structure? importance of context, punctuation, and labels in dialog act recognition. ||| piotr zelasko ||| raghavendra pappagari ||| najim dehak ||| 
2021 ||| are vision transformers robust to patch perturbations? ||| jindong gu ||| volker tresp ||| yao qin ||| 
2020 ||| attention-guided chained context aggregation for semantic segmentation. ||| quan tang ||| fagui liu ||| jun jiang ||| yu zhang ||| 
2021 ||| transformers for eeg emotion recognition. ||| jiyao liu ||| li zhang ||| hao wu ||| huan zhao ||| 
2019 ||| response of selective attention in middle temporal area. ||| linda wang ||| 
2019 ||| a bi-directional transformer for musical chord recognition. ||| jonggwon park ||| kyoyun choi ||| sungwook jeon ||| dokyun kim ||| jonghun park ||| 
2020 ||| the jazz transformer on the front line: exploring the shortcomings of ai-composed music through quantitative measures. ||| shih-lun wu ||| yi-hsuan yang ||| 
2018 ||| multi-task learning with multi-view attention for answer selection and knowledge base question answering. ||| yang deng ||| yuexiang xie ||| yaliang li ||| min yang ||| nan du ||| wei fan ||| kai lei ||| ying shen ||| 
2021 ||| comparison of convexificated sqcqp and pso for the optimal transmission system operation based on incremental in-phase and quadrature voltage controlled transformers. ||| marcel sarstedt ||| thomas leveringhaus ||| leonard klu ||| lutz hofmann ||| 
2021 ||| heterogeneity-aware twitter bot detection with relational graph transformers. ||| shangbin feng ||| zhaoxuan tan ||| rui li ||| minnan luo ||| 
2020 ||| hierarchical bi-directional self-attention networks for paper review rating recommendation. ||| zhongfen deng ||| hao peng ||| congying xia ||| jianxin li ||| lifang he ||| philip s. yu ||| 
2022 ||| awsnet: an auto-weighted supervision attention network for myocardial scar and edema segmentation in multi-sequence cardiac magnetic resonance images. ||| kai-ni wang ||| xin yang ||| juzheng miao ||| lei li ||| jing yao ||| ping zhou ||| wufeng xue ||| guang-quan zhou ||| xiahai zhuang ||| dong ni ||| 
2020 ||| efficient document re-ranking for transformers by precomputing term representations. ||| sean macavaney ||| franco maria nardini ||| raffaele perego ||| nicola tonellotto ||| nazli goharian ||| ophir frieder ||| 
2020 ||| ventral-dorsal neural networks: object detection via selective attention. ||| mohammad k. ebrahimpour ||| jiayun li ||| yen-yun yu ||| jackson l. reese ||| azadeh moghtaderi ||| ming-hsuan yang ||| david c. noelle ||| 
2021 ||| quantum mechanics and machine learning synergies: graph attention neural networks to predict chemical reactivity. ||| mohammadamin tavakoli ||| aaron mood ||| david van vranken ||| pierre baldi ||| 
2021 ||| voxel transformer for 3d object detection. ||| jiageng mao ||| yujing xue ||| minzhe niu ||| haoyue bai ||| jiashi feng ||| xiaodan liang ||| hang xu ||| chunjing xu ||| 
2021 ||| simullr: simultaneous lip reading transducer with attention-guided adaptive memory. ||| zhijie lin ||| zhou zhao ||| haoyuan li ||| jinglin liu ||| meng zhang ||| xingshan zeng ||| xiaofei he ||| 
2019 ||| sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization. ||| qiuqiang kong ||| yong xu ||| wenwu wang ||| mark d. plumbley ||| 
2020 ||| modality attention and sampling enables deep learning with heterogeneous marker combinations in fluorescence microscopy. ||| alvaro gomariz ||| tiziano portenier ||| patrick m. helbling ||| stephan isringhausen ||| ute suessbier ||| c ||| sar nombela-arrieta ||| or ||| un g ||| ksel ||| 
2018 ||| speaking style adaptation in text-to-speech synthesis using sequence-to-sequence models with attention. ||| bajibabu bollepalli ||| lauri juvela ||| paavo alku ||| 
2020 ||| knowing what, where and when to look: efficient video action modeling with attention. ||| juan-manuel perez-rua ||| brais mart ||| nez ||| xiatian zhu ||| antoine toisoul ||| victor escorcia ||| tao xiang ||| 
2021 ||| transsc: transformer-based shape completion for grasp evaluation. ||| wenkai chen ||| hongzhuo liang ||| zhaopeng chen ||| fuchun sun ||| jianwei zhang ||| 
2020 ||| attention-aware noisy label learning for image classification. ||| zhenzhen wang ||| chunyan xu ||| yap-peng tan ||| junsong yuan ||| 
2019 ||| speaker adaptation for attention-based end-to-end speech recognition. ||| zhong meng ||| yashesh gaur ||| jinyu li ||| yifan gong ||| 
2022 ||| omvp: a transformer-based time and team reinforcement learning scheme for observation-constrained multi-vehicle pursuit in urban area. ||| zheng yuan ||| tianhao wu ||| qinwen wang ||| yiying yang ||| lei li ||| lin zhang ||| 
2021 ||| automated identification of cell populations in flow cytometry data with transformers. ||| matthias w ||| dlinger ||| michael reiter ||| lisa weijler ||| margarita maurer-granofszky ||| angela schumich ||| michael dworzak ||| 
2020 ||| method and dataset entity mining in scientific literature: a cnn + bi-lstm model with self-attention. ||| linlin hou ||| ji zhang ||| ou wu ||| ting yu ||| zhen wang ||| zhao li ||| jianliang gao ||| yingchun ye ||| rujing yao ||| 
2022 ||| multi-modal brain tumor segmentation via missing modality synthesis and modality-level attention fusion. ||| ziqi huang ||| li lin ||| pujin cheng ||| linkai peng ||| xiaoying tang ||| 
2021 ||| end-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection. ||| hemlata tak ||| jee-weon jung ||| jose patino ||| madhu r. kamble ||| massimiliano todisco ||| nicholas w. d. evans ||| 
2021 ||| do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet. ||| luke melas-kyriazi ||| 
2021 ||| dual hierarchical attention networks for bi-typed heterogeneous graph learning. ||| yu zhao ||| shaopeng wei ||| huaming du ||| xingyan chen ||| qing li ||| fuzhen zhuang ||| ji liu ||| gang kou ||| 
2020 ||| self-attention gazetteer embeddings for named-entity recognition. ||| stanislav peshterliev ||| christophe dupuy ||| imre kiss ||| 
2021 ||| multimodal motion prediction with stacked transformers. ||| yicheng liu ||| jinghuai zhang ||| liangji fang ||| qinhong jiang ||| bolei zhou ||| 
2021 ||| disenhan: disentangled heterogeneous graph attention network for recommendation. ||| yifan wang ||| suyao tang ||| yuntong lei ||| weiping song ||| sheng wang ||| ming zhang ||| 
2018 ||| fading of collective attention shapes the evolution of linguistic variants. ||| diego e. shalom ||| gabriel b. mindlin ||| marcos alberto trevisan ||| 
2021 ||| exploiting global and local attentions for heavy rain removal on single images. ||| dac tung vu ||| juan luis gonzalez ||| munchurl kim ||| 
2019 ||| skin lesion classification using cnns with patch-based attention and diagnosis-guided loss weighting. ||| nils gessert ||| thilo sentker ||| frederic madesta ||| r ||| diger schmitz ||| helge kniep ||| ivo m. baltruschat ||| ren |||  werner ||| alexander schlaefer ||| 
2021 ||| fibro-cosanet: pulmonary fibrosis prognosis prediction using a convolutional self attention network. ||| zabir al nazi ||| fazla rabbi mashrur ||| md. amirul islam ||| shumit saha ||| 
2020 ||| deep transformer based data augmentation with subword units for morphologically rich online asr. ||| bal ||| zs tarj ||| n ||| gy ||| rgy szasz ||| k ||| tibor fegy ||| p ||| ter mihajlik ||| 
2021 ||| on the strengths of cross-attention in pretrained transformers for machine translation. ||| mozhdeh gheini ||| xiang ren ||| jonathan may ||| 
2020 ||| chart-to-text: generating natural language descriptions for charts by adapting the transformer model. ||| jason obeid ||| enamul hoque ||| 
2021 ||| boosting the speed of entity alignment 10*: dual attention matching network with normalized hard sample mining. ||| xin mao ||| wenting wang ||| yuanbin wu ||| man lan ||| 
2020 ||| mmft-bert: multimodal fusion transformer with bert encodings for visual question answering. ||| aisha urooj khan ||| amir mazaheri ||| niels da vitoria lobo ||| mubarak shah ||| 
2020 ||| multimodality biomedical image registration using free point transformer networks. ||| zachary m. c. baum ||| yipeng hu ||| dean c. barratt ||| 
2021 ||| action-conditioned 3d human motion synthesis with transformer vae. ||| mathis petrovich ||| michael j. black ||| g ||| l varol ||| 
2017 ||| multi-label image recognition by recurrently discovering attentional regions. ||| zhouxia wang ||| tianshui chen ||| guanbin li ||| ruijia xu ||| liang lin ||| 
2020 ||| gaussian constrained attention network for scene text recognition. ||| zhi qiao ||| xugong qin ||| yu zhou ||| fei yang ||| weiping wang ||| 
2020 ||| channel attention networks for robust mr fingerprinting matching. ||| refik soyak ||| ebru navruz ||| eda ozgu ersoy ||| gast ||| o cruz ||| claudia prieto ||| andrew p. king ||| devrim  ||| nay ||| ilkay  ||| ks ||| z ||| 
2021 ||| can vision transformers learn without natural images? ||| kodai nakashima ||| hirokatsu kataoka ||| asato matsumoto ||| kenji iwata ||| nakamasa inoue ||| 
2022 ||| reltr: relation transformer for scene graph generation. ||| yuren cong ||| michael ying yang ||| bodo rosenhahn ||| 
2021 ||| graph attention collaborative similarity embedding for recommender system. ||| jinbo song ||| chao chang ||| fei sun ||| zhenyang chen ||| guoyong hu ||| peng jiang ||| 
2019 ||| learning lightweight lane detection cnns by self attention distillation. ||| yuenan hou ||| zheng ma ||| chunxiao liu ||| chen change loy ||| 
2019 ||| multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures. ||| yuyin zhou ||| david dreizin ||| yingwei li ||| zhishuai zhang ||| yan wang ||| alan l. yuille ||| 
2020 ||| deep reinforced self-attention masks for abstractive summarization (dr.sas). ||| ankit chadha ||| mohamed masoud ||| 
2018 ||| defactonlp: fact verification using entity recognition, tfidf vector comparison and decomposable attention. ||| aniketh janardhan reddy ||| gil rocha ||| diego esteves ||| 
2022 ||| multi-label transformer for action unit detection. ||| gauthier tallec ||| edouard yvinec ||| arnaud dapogny ||| kevin bailly ||| 
2020 ||| salience estimation with multi-attention learning for abstractive text summarization. ||| piji li ||| lidong bing ||| zhongyu wei ||| wai lam ||| 
2018 ||| medical code prediction with multi-view convolution and description-regularized label-dependent attention. ||| najmeh sadoughi ||| greg p. finley ||| james fone ||| vignesh murali ||| maxim korenevsky ||| slava baryshnikov ||| nico axtmann ||| mark miller ||| david suendermann-oeft ||| 
2019 ||| star-transformer. ||| qipeng guo ||| xipeng qiu ||| pengfei liu ||| yunfan shao ||| xiangyang xue ||| zheng zhang ||| 
2020 ||| distance-aware molecule graph attention network for drug-target binding affinity prediction. ||| jingbo zhou ||| shuangli li ||| liang huang ||| haoyi xiong ||| fan wang ||| tong xu ||| hui xiong ||| dejing dou ||| 
2021 ||| april: finding the achilles' heel on privacy for vision transformers. ||| jiahao lu ||| xi sheryl zhang ||| tianli zhao ||| xiangyu he ||| jian cheng ||| 
2022 ||| artemis: attention-based retrieval with text-explicit matching and implicit similarity. ||| ginger delmas ||| rafael sampaio de rezende ||| gabriela csurka ||| diane larlus ||| 
2021 ||| f3snet: a four-step strategy for qim steganalysis of compressed speech based on hierarchical attention network. ||| chuanpeng guo ||| wei yang ||| liusheng huang ||| 
2019 ||| predicting retrosynthetic reaction using self-corrected transformer neural networks. ||| shuangjia zheng ||| jiahua rao ||| zhongyue zhang ||| jun xu ||| yuedong yang ||| 
2020 ||| heads-up! unsupervised constituency parsing via self-attention heads. ||| bowen li ||| taeuk kim ||| reinald kim amplayo ||| frank keller ||| 
2019 ||| cdsa: cross-dimensional self-attention for multivariate, geo-tagged time series imputation. ||| jiawei ma ||| zheng shou ||| alireza zareian ||| hassan mansour ||| anthony vetro ||| shih-fu chang ||| 
2021 ||| large scale audio understanding without transformers/ convolutions/ berts/ mixers/ attention/ rnns or .... ||| prateek verma ||| 
2021 ||| normformer: improved transformer pretraining with extra normalization. ||| sam shleifer ||| jason weston ||| myle ott ||| 
2021 ||| levit: a vision transformer in convnet's clothing for faster inference. ||| benjamin graham ||| alaaeldin el-nouby ||| hugo touvron ||| pierre stock ||| armand joulin ||| herv |||  j ||| gou ||| matthijs douze ||| 
2020 ||| caa-net: conditional atrous cnns with attention for explainable device-robust acoustic scene classification. ||| zhao ren ||| qiuqiang kong ||| jing han ||| mark d. plumbley ||| bj ||| rn w. schuller ||| 
2022 ||| glassoformer: a query-sparse transformer for post-fault power grid voltage prediction. ||| yunling zheng ||| carson hu ||| guang lin ||| meng yue ||| bao wang ||| jack xin ||| 
2021 ||| glit: neural architecture search for global and local image transformer. ||| boyu chen ||| peixia li ||| chuming li ||| baopu li ||| lei bai ||| chen lin ||| ming sun ||| junjie yan ||| wanli ouyang ||| 
2020 ||| whaletrans: e2e whisper to natural speech conversion using modified transformer network. ||| abhishek niranjan ||| mukesh sharma ||| sai bharath chandra gutha ||| m. ali basha shaik ||| 
2020 ||| gcan: graph-aware co-attention networks for explainable fake news detection on social media. ||| yi-ju lu ||| cheng-te li ||| 
2019 ||| question generation by transformers. ||| kettip kriangchaivech ||| artit wangperawong ||| 
2020 ||| see, attend and brake: an attention-based saliency map prediction model for end-to-end driving. ||| ekrem aksoy ||| ahmet yazici ||| mahmut kasap ||| 
2019 ||| attention is all you need for videos: self-attention based video summarization using universal transformers. ||| manjot bilkhu ||| siyang wang ||| tushar dobhal ||| 
2020 ||| attention-based neural bag-of-features learning for sequence data. ||| dat thanh tran ||| nikolaos passalis ||| anastasios tefas ||| moncef gabbouj ||| alexandros iosifidis ||| 
2019 ||| dynamic convolution: attention over convolution kernels. ||| yinpeng chen ||| xiyang dai ||| mengchen liu ||| dongdong chen ||| lu yuan ||| zicheng liu ||| 
2018 ||| multimodal affective analysis using hierarchical attention strategy with word-level alignment. ||| yue gu ||| kangning yang ||| shiyu fu ||| shuhong chen ||| xinyu li ||| ivan marsic ||| 
2020 ||| attention routing: track-assignment detailed routing using attention-based reinforcement learning. ||| haiguang liao ||| qingyi dong ||| xuliang dong ||| wentai zhang ||| wangyang zhang ||| weiyi qi ||| elias fallon ||| levent burak kara ||| 
2020 ||| fastformers: highly efficient transformer models for natural language understanding. ||| young jin kim ||| hany hassan awadalla ||| 
2019 ||| caire_hkust at semeval-2019 task 3: hierarchical attention for dialogue emotion classification. ||| genta indra winata ||| andrea madotto ||| zhaojiang lin ||| jamin shin ||| yan xu ||| peng xu ||| pascale fung ||| 
2021 ||| dynamic attention-based communication-efficient federated learning. ||| zihan chen ||| kai fong ernest chong ||| tony q. s. quek ||| 
2021 ||| normal learning in videos with attention prototype network. ||| chao hu ||| fan wu ||| weijie wu ||| weibin qiu ||| shengxin lai ||| 
2017 ||| multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory. ||| marco martinolli ||| wulfram gerstner ||| aditya gilra ||| 
2018 ||| watch, listen, and describe: globally and locally aligned cross-modal attentions for video captioning. ||| xin wang ||| yuan-fang wang ||| william yang wang ||| 
2020 ||| end-to-end multi-speaker speech recognition with transformer. ||| xuankai chang ||| wangyou zhang ||| yanmin qian ||| jonathan le roux ||| shinji watanabe ||| 
2021 ||| hierarchical multimodal transformer to summarize videos. ||| bin zhao ||| maoguo gong ||| xuelong li ||| 
2021 ||| probabilistic attention for interactive segmentation. ||| prasad gabbur ||| manjot bilkhu ||| javier r. movellan ||| 
2021 ||| thermal image super-resolution using second-order channel attention with varying receptive fields. ||| nolan b. gutierrez ||| william j. beksi ||| 
2020 ||| user attention and behaviour in virtual reality art encounter. ||| mu mu ||| murtada dohan ||| alison goodyear ||| gary hill ||| cleyon johns ||| andreas mauthe ||| 
2021 ||| a battle of network structures: an empirical study of cnn, transformer, and mlp. ||| yucheng zhao ||| guangting wang ||| chuanxin tang ||| chong luo ||| wenjun zeng ||| zheng-jun zha ||| 
2021 ||| end-to-end speaker diarization with transformer. ||| yongquan lai ||| xin tang ||| yuanyuan fu ||| rui fang ||| 
2021 ||| self-attention generative adversarial network for iterative reconstruction of ct images. ||| ruiwen xing ||| thomas humphries ||| dong si ||| 
2019 ||| understanding spatial correlation in eye-fixation maps for visual attention in videos. ||| tariq alshawi ||| zhiling long ||| ghassan alregib ||| 
2019 ||| rap-net: recurrent attention pooling networks for dialogue response selection. ||| chao-wei huang ||| ting-rui chiang ||| shang-yu su ||| yun-nung chen ||| 
2021 ||| multi-time attention networks for irregularly sampled time series. ||| satya narayan shukla ||| benjamin m. marlin ||| 
2021 ||| group-based distinctive image captioning with memory attention. ||| jiuniu wang ||| wenjia xu ||| qingzhong wang ||| antoni b. chan ||| 
2022 ||| simulation-driven training of vision transformers enabling metal segmentation in x-ray images. ||| fuxin fan ||| ludwig ritschl ||| marcel beister ||| ramyar biniazan ||| bj ||| rn w. kreher ||| tristan m. gottschalk ||| steffen kappler ||| andreas k. maier ||| 
2018 ||| attention boosted sequential inference model. ||| guanyu li ||| pengfei zhang ||| caiyan jia ||| 
2019 ||| drone-based joint density map estimation, localization and tracking with space-time multi-scale attention network. ||| longyin wen ||| dawei du ||| pengfei zhu ||| qinghua hu ||| qilong wang ||| liefeng bo ||| siwei lyu ||| 
2021 ||| mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. ||| sachin mehta ||| mohammad rastegari ||| 
2019 ||| a tensorized transformer for language modeling. ||| xindian ma ||| peng zhang ||| shuai zhang ||| nan duan ||| yuexian hou ||| dawei song ||| ming zhou ||| 
2019 ||| integrating source-channel and attention-based sequence-to-sequence models for speech recognition. ||| qiujia li ||| chao zhang ||| philip c. woodland ||| 
2020 ||| few-shot action recognition via improved attention with self-supervision. ||| hongguang zhang ||| li zhang ||| xiaojuan qi ||| hongdong li ||| philip h. s. torr ||| piotr koniusz ||| 
2019 ||| sanvis: visual analytics for understanding self-attention networks. ||| cheonbok park ||| inyoup na ||| yongjang jo ||| sungbok shin ||| jaehyo yoo ||| bum chul kwon ||| jian zhao ||| hyungjong noh ||| yeonsoo lee ||| jaegul choo ||| 
2019 ||| robust invisible video watermarking with attention. ||| kevin alex zhang ||| lei xu ||| alfredo cuesta-infante ||| kalyan veeramachaneni ||| 
2020 ||| mt-bioner: multi-task learning for biomedical named entity recognition using deep bidirectional transformers. ||| muhammad raza khan ||| morteza ziyadi ||| mohamed abdelhady ||| 
2021 ||| tvt: transferable vision transformer for unsupervised domain adaptation. ||| jinyu yang ||| jingjing liu ||| ning xu ||| junzhou huang ||| 
2022 ||| measuring the mixing of contextual information in the transformer. ||| javier ferrando ||| gerard i. g ||| llego ||| marta r. costa-juss ||| 
2022 ||| attention aided csi wireless localization. ||| artan salihu ||| stefan schwarz ||| markus rupp ||| 
2021 ||| fusion of medical imaging and electronic health records with attention and multi-head machanisms. ||| cheng jiang ||| yihao chen ||| jianbo chang ||| ming feng ||| renzhi wang ||| jianhua yao ||| 
2021 ||| vision transformer hashing for image retrieval. ||| shiv ram dubey ||| satish kumar singh ||| wei-ta chu ||| 
2018 ||| sequence-based person attribute recognition with joint ctc-attention model. ||| hao liu ||| jingjing wu ||| jianguo jiang ||| meibin qi ||| bo ren ||| 
2019 ||| self-attention based end-to-end hindi-english neural machine translation. ||| siddhant srivastava ||| ritu tiwari ||| 
2019 ||| real image denoising with feature attention. ||| saeed anwar ||| nick barnes ||| 
2018 ||| multilingual nmt with a language-independent attention bridge. ||| ra ||| l v ||| zquez ||| alessandro raganato ||| j ||| rg tiedemann ||| mathias creutz ||| 
2021 ||| multimodal pet/ct tumour segmentation and prediction of progression-free survival using a full-scale unet with attention. ||| emmanuelle bourigault ||| daniel r. mcgowan ||| abolfazl mehranian ||| bartlomiej w. papiez ||| 
2020 ||| transformer based language models for similar text retrieval and ranking. ||| javed qadrud-din ||| ashraf bah rabiou ||| ryan walker ||| ravi soni ||| martin gajek ||| gabriel pack ||| akhil rangaraj ||| 
2020 ||| character-aware attention-based end-to-end speech recognition. ||| zhong meng ||| yashesh gaur ||| jinyu li ||| yifan gong ||| 
2021 ||| visual parser: representing part-whole hierarchies with transformers. ||| shuyang sun ||| xiaoyu yue ||| song bai ||| philip h. s. torr ||| 
2022 ||| acort: a compact object relation transformer for parameter efficient image captioning. ||| jia huei tan ||| ying hua tan ||| chee seng chan ||| joon huang chuah ||| 
2021 ||| heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning. ||| jingwen li ||| liang xin ||| zhiguang cao ||| andrew lim ||| wen song ||| jie zhang ||| 
2021 ||| predicting vehicles trajectories in urban scenarios with transformer networks and augmented information. ||| lvaro quintanar ||| david fern ||| ndez llorca ||| ignacio parra ||| rub ||| n izquierdo ||| miguel  ||| ngel sotelo ||| 
2018 ||| forward attention in sequence-to-sequence acoustic modelling for speech synthesis. ||| jing-xuan zhang ||| zhen-hua ling ||| li-rong dai ||| 
2018 ||| classification based grasp detection using spatial transformer network. ||| dongwon park ||| se young chun ||| 
2020 ||| transformer-based end-to-end speech recognition with local dense synthesizer attention. ||| menglong xu ||| shengqiang li ||| xiao-lei zhang ||| 
2020 ||| heart sound segmentation using bidirectional lstms with attention. ||| tharindu fernando ||| houman ghaemmaghami ||| simon denman ||| sridha sridharan ||| nayyar hussain ||| clinton fookes ||| 
2021 ||| content-augmented feature pyramid network with light linear transformers. ||| yongxiang gu ||| xiaolin qin ||| yuncong peng ||| lu li ||| 
2020 ||| lite transformer with long-short range attention. ||| zhanghao wu ||| zhijian liu ||| ji lin ||| yujun lin ||| song han ||| 
2020 ||| exploration of audio quality assessment and anomaly localisation using attention models. ||| qiang huang ||| thomas hain ||| 
2021 ||| augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. ||| zhiwei liu ||| ziwei fan ||| yu wang ||| philip s. yu ||| 
2020 ||| keep your eyes on the lane: attention-guided lane detection. ||| lucas tabelini torres ||| rodrigo ferreira berriel ||| thiago m. paix ||| o ||| claudine badue ||| alberto f. de souza ||| thiago oliveira-santos ||| 
2021 ||| offroadtranseg: semi-supervised segmentation using transformers on offroad environments. ||| anukriti singh ||| kartikeya singh ||| p. b. sujit ||| 
2021 ||| not all images are worth 16x16 words: dynamic vision transformers with adaptive sequence length. ||| yulin wang ||| rui huang ||| shiji song ||| zeyi huang ||| gao huang ||| 
2020 ||| contextualized graph attention network for recommendation with item knowledge graph. ||| susen yang ||| yong liu ||| yonghui xu ||| chunyan miao ||| min wu ||| juyong zhang ||| 
2020 ||| semi-supervised learning of galaxy morphology using equivariant transformer variational autoencoders. ||| mizu nishikawa-toomey ||| lewis smith ||| yarin gal ||| 
2018 ||| deep neural net with attention for multi-channel multi-touch attribution. ||| ning li ||| sai kumar arava ||| chen dong ||| zhenyu yan ||| abhishek pani ||| 
2021 ||| poshan: cardinal pos pattern guided attention for news headline incongruence. ||| rahul mishra ||| shuo zhang ||| 
2019 ||| deep learning investigation for chess player attention prediction using eye-tracking and game data. ||| justin le louedec ||| thomas guntz ||| james l. crowley ||| dominique vaufreydaz ||| 
2021 ||| revisiting linformer with a modified self-attention with linear complexity. ||| madhusudan verma ||| 
2021 ||| non-autoregressive transformer with unified bidirectional decoder for automatic speech recognition. ||| chuan-fei zhang ||| yan liu ||| tian-hao zhang ||| song-lu chen ||| feng chen ||| xu-cheng yin ||| 
2021 ||| unetr: transformers for 3d medical image segmentation. ||| ali hatamizadeh ||| dong yang ||| holger roth ||| daguang xu ||| 
2021 ||| where to look at the movies : analyzing visual attention to understand movie editing. ||| alexandre bruckert ||| marc christie ||| olivier le meur ||| 
2021 ||| local multi-head channel self-attention for facial expression recognition. ||| roberto pecoraro ||| valerio basile ||| viviana bono ||| sara gallo ||| 
2020 ||| attention based on-device streaming speech recognition with large speech corpus. ||| kwangyoun kim ||| kyungmin lee ||| dhananjaya gowda ||| junmo park ||| sungsoo kim ||| sichen jin ||| young-yoon lee ||| jinsu yeo ||| daehyun kim ||| seokyeong jung ||| jungin lee ||| myoungji han ||| chanwoo kim ||| 
2021 ||| lmr-cbt: learning modality-fused representations with cb-transformer for multimodal emotion recognition from unaligned multimodal sequences. ||| ziwang fu ||| feng liu ||| hanyang wang ||| siyuan shen ||| jiahao zhang ||| jiayin qi ||| xiangling fu ||| aimin zhou ||| 
2021 ||| realtrans: end-to-end simultaneous speech translation with convolutional weighted-shrinking transformer. ||| xingshan zeng ||| liangyou li ||| qun liu ||| 
2022 ||| using multi-scale swintransformer-htc with data augmentation in conic challenge. ||| chia-yen lee ||| hsiang-chin chien ||| ching-ping wang ||| hong yen ||| kai-wen zhen ||| hong-kun lin ||| 
2021 ||| leveraging human selective attention for medical image analysis with limited training data. ||| yifei huang ||| xiaoxiao li ||| lijin yang ||| lin gu ||| yingying zhu ||| hirofumi seo ||| qiuming meng ||| tatsuya harada ||| yoichi sato ||| 
2022 ||| under the hood of transformer networks for trajectory forecasting. ||| luca franco ||| leonardo placidi ||| francesco giuliari ||| irtiza hasan ||| marco cristani ||| fabio galasso ||| 
2020 ||| attention-aware inference for neural abstractive summarization. ||| ye ma ||| lu zong ||| 
2021 ||| eegdnet: fusing non-local and local self-similarity for 1-d eeg signal denoising with 2-d transformer. ||| peng yi ||| kecheng chen ||| zhaoqi ma ||| di zhao ||| xiaorong pu ||| yazhou ren ||| 
2021 ||| longt5: efficient text-to-text transformer for long sequences. ||| mandy guo ||| joshua ainslie ||| david c. uthus ||| santiago onta ||| n ||| jianmo ni ||| yun-hsuan sung ||| yinfei yang ||| 
2021 ||| geometric algebra attention networks for small point clouds. ||| matthew spellings ||| 
2021 ||| guided interactive video object segmentation using reliability-based attention maps. ||| yuk heo ||| yeong jun koh ||| chang-su kim ||| 
2021 ||| self-supervision and spatial-sequential attention based loss for multi-person pose estimation. ||| haiyang liu ||| dingli luo ||| songlin du ||| takeshi ikenaga ||| 
2021 ||| natural language inference with a human touch: using human explanations to guide model attention. ||| joe stacey ||| yonatan belinkov ||| marek rei ||| 
2021 ||| multi-mode transformer transducer with stochastic future context. ||| kwangyoun kim ||| felix wu ||| prashant sridhar ||| kyu j. han ||| shinji watanabe ||| 
2019 ||| modelling bahdanau attention using election methods aided by q-learning. ||| rakesh bal ||| sayan sinha ||| 
2019 ||| social attention for autonomous decision-making in dense traffic. ||| edouard leurent ||| jean mercat ||| 
2022 ||| linearizing transformer with key-value memory bank. ||| yizhe zhang ||| deng cai ||| 
2020 ||| message-aware graph attention networks for large-scale multi-robot path planning. ||| qingbiao li ||| weizhe lin ||| zhe liu ||| amanda prorok ||| 
2018 ||| phrase-based attentions. ||| phi xuan nguyen ||| shafiq r. joty ||| 
2018 ||| weakly supervised attention learning for textual phrases grounding. ||| zhiyuan fang ||| shu kong ||| tianshu yu ||| yezhou yang ||| 
2017 ||| dipole: diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. ||| fenglong ma ||| radha chitta ||| jing zhou ||| quanzeng you ||| tong sun ||| jing gao ||| 
2021 ||| tri-transformer hawkes process: three heads are better than one. ||| zhi-yan song ||| jianwei liu ||| lu-ning zhang ||| ya-nan han ||| 
2020 ||| automated topical component extraction using neural network attention scores from source-based essay scoring. ||| haoran zhang ||| diane j. litman ||| 
2019 ||| multisource region attention network for fine-grained object recognition in remote sensing imagery. ||| gencer sumbul ||| ramazan gokberk cinbis ||| selim aksoy ||| 
2022 ||| zero-shot action recognition with transformer-based video semantic embedding. ||| keval doshi ||| yasin yilmaz ||| 
2021 ||| oracle linguistic graphs complement a pretrained transformer language model: a cross-formalism comparison. ||| jakob prange ||| nathan schneider ||| lingpeng kong ||| 
2019 ||| enhancing neural sequence labeling with position-aware self-attention. ||| wei wei ||| zanbo wang ||| xianling mao ||| guangyou zhou ||| pan zhou ||| sheng jiang ||| 
2021 ||| did the cat drink the coffee? challenging transformers with generalized event knowledge. ||| paolo pedinotti ||| giulia rambelli ||| emmanuele chersoni ||| enrico santus ||| alessandro lenci ||| philippe blache ||| 
2021 ||| where to look: a unified attention model for visual recognition with reinforcement learning. ||| gang chen ||| 
2021 ||| on evolving attention towards domain adaptation. ||| kekai sheng ||| ke li ||| xiawu zheng ||| jian liang ||| weiming dong ||| feiyue huang ||| rongrong ji ||| xing sun ||| 
2021 ||| pretrained transformers as universal computation engines. ||| kevin lu ||| aditya grover ||| pieter abbeel ||| igor mordatch ||| 
2020 |||  net: augmented parallel-pyramid net for attention guided pose estimation. ||| luanxuan hou ||| jie cao ||| yuan zhao ||| haifeng shen ||| jian tang ||| ran he ||| 
2018 ||| deriving machine attention from human rationales. ||| yujia bao ||| shiyu chang ||| mo yu ||| regina barzilay ||| 
2018 ||| subjective annotations for vision-based attention level estimation. ||| andrea coifman ||| p ||| ter rohoska ||| miklas s. kristoffersen ||| sven ewan shepstone ||| zheng-hua tan ||| 
2022 ||| efficient-dyn: dynamic graph representation learning via event-based temporal sparse attention network. ||| yan pang ||| chao liu ||| 
2022 ||| interacting attention graph for single image two-hand reconstruction. ||| mengcheng li ||| liang an ||| hongwen zhang ||| lianpeng wu ||| feng chen ||| tao yu ||| yebin liu ||| 
2021 ||| local frequency domain transformer networks for video prediction. ||| hafez farazi ||| jan nogga ||| sven behnke ||| 
2019 ||| hyper-sagnn: a self-attention based graph neural network for hypergraphs. ||| ruochi zhang ||| yuesong zou ||| jian ma ||| 
2019 ||| signed graph attention networks. ||| junjie huang ||| huawei shen ||| liang hou ||| xueqi cheng ||| 
2022 ||| spatiotemporal transformer attention network for 3d voxel level joint segmentation and motion prediction in point cloud. ||| zhensong wei ||| xuewei qi ||| zhengwei bai ||| guoyuan wu ||| saswat priyadarshi nayak ||| peng hao ||| matthew j. barth ||| yongkang liu ||| kentaro oguchi ||| 
2022 ||| hindi/bengali sentiment analysis using transfer learning and joint dual input learning with self attention. ||| shahrukh khan ||| mahnoor shahid ||| 
2017 ||| structured attention networks. ||| yoon kim ||| carl denton ||| luong hoang ||| alexander m. rush ||| 
2022 ||| multi-tailed vision transformer for efficient inference. ||| yunke wang ||| bo du ||| chang xu ||| 
2021 ||| full transformer framework for robust point cloud registration with deep information interaction. ||| guangyan chen ||| meiling wang ||| yufeng yue ||| qingxiang zhang ||| li yuan ||| 
2021 ||| sub-word level lip reading with visual attention. ||| prajwal k. r ||| triantafyllos afouras ||| andrew zisserman ||| 
2021 ||| improving visual quality of image synthesis by a token-based generator with transformers. ||| yanhong zeng ||| huan yang ||| hongyang chao ||| jianbo wang ||| jianlong fu ||| 
2018 ||| deep attentional structured representation learning for visual recognition. ||| krishna kanth nakka ||| mathieu salzmann ||| 
2021 ||| irene: interpretable energy prediction for transformers. ||| qingqing cao ||| yash kumar lal ||| harsh trivedi ||| aruna balasubramanian ||| niranjan balasubramanian ||| 
2020 ||| bet: a backtranslation approach for easy data augmentation in transformer-based paraphrase identification context. ||| jean-philippe corbeil ||| hadi abdi ghadivel ||| 
2021 ||| attentionflow: visualising influence in networks of time series. ||| minjeong shin ||| alasdair tran ||| siqi wu ||| alexander patrick mathews ||| rong wang ||| georgiana lyall ||| lexing xie ||| 
2021 ||| uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer. ||| haonan wang ||| peng cao ||| jiaqi wang ||| osmar r. za ||| ane ||| 
2020 ||| channel attention with embedding gaussian process: a probabilistic methodology. ||| jiyang xie ||| dongliang chang ||| zhanyu ma ||| guoqiang zhang ||| jun guo ||| 
2021 ||| spatial-channel transformer network for trajectory prediction on the traffic scenes. ||| jingwen zhao ||| xuanpeng li ||| qifan xue ||| weigong zhang ||| 
2021 ||| pa-resseg: a phase attention residual network for liver tumor segmentation from multi-phase ct images. ||| yingying xu ||| ming cai ||| lanfen lin ||| yue zhang ||| hongjie hu ||| zhiyi peng ||| qiaowei zhang ||| qingqing chen ||| xiongwei mao ||| yutaro iwamoto ||| xian-hua han ||| yen-wei chen ||| ruofeng tong ||| 
2021 ||| improving customer service chatbots with attention-based transfer learning. ||| jordan j. bird ||| 
2019 ||| relation-aware global attention. ||| zhizheng zhang ||| cuiling lan ||| wenjun zeng ||| xin jin ||| zhibo chen ||| 
2018 ||| self attention grid for person re-identification. ||| jean-paul ainam ||| ke qin ||| guisong liu ||| 
2021 ||| a tri-attention fusion guided multi-modal segmentation network. ||| tongxue zhou ||| su ruan ||| pierre vera ||| st ||| phane canu ||| 
2020 ||| group equivariant stand-alone self-attention for vision. ||| david w. romero ||| jean-baptiste cordonnier ||| 
2022 ||| grapheye: a novel solution for detecting vulnerable functions based on graph attention network. ||| li zhou ||| minhuan huang ||| yujun li ||| yuanping nie ||| jin li ||| yiwei liu ||| 
2021 ||| convolution-free waveform transformers for multi-lead ecg classification. ||| annamalai natarajan ||| gregory boverman ||| yale chang ||| corneliu antonescu ||| jonathan rubin ||| 
2021 ||| understanding robustness of transformers for image classification. ||| srinadh bhojanapalli ||| ayan chakrabarti ||| daniel glasner ||| daliang li ||| thomas unterthiner ||| andreas veit ||| 
2021 ||| full attention bidirectional deep learning structure for single channel speech enhancement. ||| yuzi yan ||| wei-qiang zhang ||| michael t. johnson ||| 
2021 ||| a graph vae and graph transformer approach to generating molecular graphs. ||| joshua mitton ||| hans m. senn ||| klaas wynne ||| roderick murray-smith ||| 
2018 ||| attention-mechanism-based tracking method for intelligent internet of vehicles. ||| xu kang ||| bin song ||| jie guo ||| xiaojiang du ||| mohsen guizani ||| 
2022 ||| deep learning assisted end-to-end synthesis of mm-wave passive networks with 3d em structures: a study on a transformer-based matching network. ||| siawpeng er ||| edward liu ||| minshuo chen ||| yan li ||| yuqi liu ||| tuo zhao ||| hua wang ||| 
2019 ||| session-based social recommendation via dynamic graph attention networks. ||| weiping song ||| zhiping xiao ||| yifan wang ||| laurent charlin ||| ming zhang ||| jian tang ||| 
2021 ||| investigating transfer learning capabilities of vision transformers and cnns by fine-tuning a single trainable block. ||| durvesh malpure ||| onkar litake ||| rajesh ingle ||| 
2020 ||| super resolution using segmentation-prior self-attention generative adversarial network. ||| yuxin zhang ||| zuquan zheng ||| roland hu ||| 
2021 ||| nommer: nominate synergistic context in vision transformer for visual recognition. ||| hao liu ||| xinghua jiang ||| xin li ||| zhimin bao ||| deqiang jiang ||| bo ren ||| 
2018 ||| advancing connectionist temporal classification with attention modeling. ||| amit das ||| jinyu li ||| rui zhao ||| yifan gong ||| 
2020 ||| multi-scale attention u-net (msaunet): a modified u-net architecture for scene segmentation. ||| soham chattopadhyay ||| hritam basak ||| 
2021 ||| relating transformers to models and neural representations of the hippocampal formation. ||| james c. r. whittington ||| joseph warren ||| timothy edward john behrens ||| 
2021 ||| style transfer with target feature palette and attention coloring. ||| suhyeon ha ||| guisik kim ||| junseok kwon ||| 
2021 ||| transformer based automatic covid-19 fake news detection system. ||| sunil gundapu ||| radhika mamidi ||| 
2021 ||| adaptive fourier neural operators: efficient token mixers for transformers. ||| john guibas ||| morteza mardani ||| zongyi li ||| andrew tao ||| anima anandkumar ||| bryan catanzaro ||| 
2018 ||| attention-based temporal weighted convolutional neural network for action recognition. ||| jinliang zang ||| le wang ||| zi-yi liu ||| qilin zhang ||| zhenxing niu ||| gang hua ||| nanning zheng ||| 
2022 ||| overcoming a theoretical limitation of self-attention. ||| david chiang ||| peter cholak ||| 
2020 ||| step: sequence-to-sequence transformer pre-training for document summarization. ||| yanyan zou ||| xingxing zhang ||| wei lu ||| furu wei ||| ming zhou ||| 
2019 ||| transfer nas: knowledge transfer between search spaces with transformer agents. ||| zal ||| n borsos ||| andrey khorlin ||| andrea gesmundo ||| 
2021 ||| transformers and transfer learning for improving portuguese semantic role labeling. ||| sofia oliveira ||| daniel loureiro ||| al ||| pio jorge ||| 
2021 ||| learned queries for efficient local attention. ||| moab arar ||| ariel shamir ||| amit h. bermano ||| 
2021 ||| mrat-sql+gap: a portuguese text-to-sql transformer. ||| marcelo archanjo jos ||| f ||| bio gagliardi cozman ||| 
2019 ||| rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation. ||| christoph l ||| scher ||| eugen beck ||| kazuki irie ||| markus kitza ||| wilfried michel ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2019 ||| a radio signal modulation recognition algorithm based on residual networks and attention mechanisms. ||| ruisen luo ||| tao hu ||| zuodong tang ||| chen wang ||| xiaofeng gong ||| haiyan tu ||| 
2021 ||| boosting neural machine translation with dependency-scaled self-attention network. ||| ru peng ||| nankai lin ||| yi fang ||| shengyi jiang ||| junbo zhao ||| 
2018 ||| dynamic self-attention : computing attention over words dynamically for sentence embedding. ||| deunsol yoon ||| dongbok lee ||| sangkeun lee ||| 
2020 ||| improve transformer models with better relative position embeddings. ||| zhiheng huang ||| davis liang ||| peng xu ||| bing xiang ||| 
2017 ||| structured attentions for visual question answering. ||| chen zhu ||| yanpeng zhao ||| shuaiyi huang ||| kewei tu ||| yi ma ||| 
2020 ||| multi-unit transformers for neural machine translation. ||| jianhao yan ||| fandong meng ||| jie zhou ||| 
2021 ||| the center of attention: center-keypoint grouping via attention for multi-person pose estimation. ||| guillem bras ||| nikita kister ||| laura leal-taix ||| 
2020 ||| geometric scattering attention networks. ||| yimeng min ||| frederik wenkel ||| guy wolf ||| 
2021 ||| videogpt: video generation using vq-vae and transformers. ||| wilson yan ||| yunzhi zhang ||| pieter abbeel ||| aravind srinivas ||| 
2022 ||| input-specific attention subnetworks for adversarial detection. ||| emil biju ||| anirudh sriram ||| pratyush kumar ||| mitesh m. khapra ||| 
2020 ||| attention-guided network for iris presentation attack detection. ||| cunjian chen ||| arun ross ||| 
2020 ||| identify speakers in cocktail parties with end-to-end attention. ||| junzhe zhu ||| mark hasegawa-johnson ||| leda sari ||| 
2021 ||| thinking like transformers. ||| gail weiss ||| yoav goldberg ||| eran yahav ||| 
2019 ||| dependency-aware named entity recognition with relative and global attentions. ||| gustavo aguilar ||| thamar solorio ||| 
2018 ||| image denoising and restoration with cnn-lstm encoder decoder with direct attention. ||| kazi nazmul haque ||| mohammad abu yousuf ||| rajib rana ||| 
2020 ||| wat zei je? detecting out-of-distribution translations with variational transformers. ||| tim z. xiao ||| aidan n. gomez ||| yarin gal ||| 
2021 ||| nested multiple instance learning with attention mechanisms. ||| saul fuster ||| trygve eftest ||| l ||| kjersti engan ||| 
2021 ||| x-volution: on the unification of convolution and self-attention. ||| xuanhong chen ||| hang wang ||| bingbing ni ||| 
2021 ||| vqa-mhug: a gaze dataset to study multimodal neural attention in visual question answering. ||| ekta sood ||| fabian k ||| gel ||| florian strohm ||| prajit dhar ||| andreas bulling ||| 
2020 ||| audio adversarial examples for robust hybrid ctc/attention speech recognition. ||| ludwig k ||| rzinger ||| edgar ricardo chavez rosas ||| lujun li ||| tobias watzel ||| gerhard rigoll ||| 
2022 ||| deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation. ||| ahmad hammoudeh ||| bastien vanderplaetse ||| st ||| phane dupont ||| 
2021 ||| fully adaptive self-stabilizing transformer for lcl problems. ||| shimon bitton ||| yuval emek ||| taisuke izumi ||| shay kutten ||| 
2022 ||| viewformer: nerf-free neural rendering from few images using transformers. ||| jon ||| s kulh ||| nek ||| erik derner ||| torsten sattler ||| robert babuska ||| 
2021 ||| the next 700 program transformers. ||| geoffrey hamilton ||| 
2020 ||| telling bert's full story: from local attention to global aggregation. ||| damian pascual ||| gino brunner ||| roger wattenhofer ||| 
2022 ||| planet: dynamic content planning in autoregressive transformers for long-form text generation. ||| zhe hu ||| hou pong chan ||| jiachen liu ||| xinyan xiao ||| hua wu ||| lifu huang ||| 
2021 ||| domain transformer: predicting samples of unseen, future domains. ||| johannes schneider ||| 
2018 ||| multilingual end-to-end speech recognition with a single transformer on low-resource languages. ||| shiyu zhou ||| shuang xu ||| bo xu ||| 
2019 ||| taking a stance on fake news: towards automatic disinformation assessment via deep bidirectional transformer language models for stance detection. ||| chris dulhanty ||| jason l. deglint ||| ibrahim ben daya ||| alexander wong ||| 
2020 ||| learning light-weight translation models from deep transformer. ||| bei li ||| ziyang wang ||| hui liu ||| quan du ||| tong xiao ||| chunliang zhang ||| jingbo zhu ||| 
2017 ||| attention-based end-to-end speech recognition in mandarin. ||| changhao shan ||| junbo zhang ||| yujun wang ||| lei xie ||| 
2021 ||| global structure-aware drum transcription based on self-attention mechanisms. ||| ryoto ishizuka ||| ryo nishikimi ||| kazuyoshi yoshii ||| 
2022 ||| smoothing matters: momentum transformer for domain adaptive semantic segmentation. ||| runfa chen ||| yu rong ||| shangmin guo ||| jiaqi han ||| fuchun sun ||| tingyang xu ||| wenbing huang ||| 
2018 ||| diagnose like a radiologist: attention guided convolutional neural network for thorax disease classification. ||| qingji guan ||| yaping huang ||| zhun zhong ||| zhedong zheng ||| liang zheng ||| yi yang ||| 
2018 ||| social media attention increases article visits: an investigation on article-level referral data of peerj. ||| xianwen wang ||| yunxue cui ||| qingchun li ||| xinhui guo ||| 
2021 ||| eeg-transformer: self-attention from transformer architecture for decoding eeg of imagined speech. ||| young eun lee ||| seo-hyun lee ||| 
2017 ||| deep semantic role labeling with self-attention. ||| zhixing tan ||| mingxuan wang ||| jun xie ||| yidong chen ||| xiaodong shi ||| 
2021 ||| va-gcn: a vector attention graph convolution network for learning on point clouds. ||| haotian hu ||| fanyi wang ||| huixiao le ||| 
2021 ||| differentiable subset pruning of transformer heads. ||| jiaoda li ||| ryan cotterell ||| mrinmaya sachan ||| 
2021 ||| orthogonal attention: a cloze-style approach to negation scope resolution. ||| aditya khandelwal ||| vahida attar ||| 
2021 ||| generating abstractive summaries of lithuanian news articles using a transformer model. ||| lukas stankevicius ||| mantas lukosevicius ||| 
2021 ||| a text autoencoder from transformer for fast encoding language representation. ||| tan huang ||| 
2020 ||| incorporating effective global information via adaptive gate attention for text classification. ||| xianming li ||| zongxi li ||| yingbin zhao ||| haoran xie ||| qing li ||| 
2019 ||| maanet: multi-view aware attention networks for image super-resolution. ||| jingcai guo ||| shiheng ma ||| song guo ||| 
2021 ||| starnet: joint action-space prediction with star graphs and implicit global frame self-attention. ||| faris janjos ||| maxim dolgov ||| j. marius z ||| llner ||| 
2020 ||| fma-eta: estimating travel time entirely based on ffn with attention. ||| yiwen sun ||| yulu wang ||| kun fu ||| zheng wang ||| ziang yan ||| changshui zhang ||| jieping ye ||| 
2020 ||| berters: multimodal representation learning for expert recommendation system with transformer. ||| narjes nikzad-khasmakhi ||| mohammad ali balafar ||| mohammad-reza feizi-derakhshi ||| cina motamed ||| 
2021 ||| boxer: box-attention for 2d and 3d transformers. ||| duy-kien nguyen ||| jihong ju ||| olaf booji ||| martin r. oswald ||| cees g. m. snoek ||| 
2019 ||| less memory, faster speed: refining self-attention module for image reconstruction. ||| zheng wang ||| jianwu li ||| ge song ||| tieling li ||| 
2019 ||| temporal self-attention network for medical concept embedding. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| michael blumenstein ||| 
2020 ||| topological planning with transformers for vision-and-language navigation. ||| kevin chen ||| junshen k. chen ||| jo chuang ||| marynel v ||| zquez ||| silvio savarese ||| 
2018 ||| augmenting neural response generation with context-aware topical attention. ||| nouha dziri ||| ehsan kamalloo ||| kory w. mathewson ||| osmar r. za ||| ane ||| 
2021 ||| cswin transformer: a general vision transformer backbone with cross-shaped windows. ||| xiaoyi dong ||| jianmin bao ||| dongdong chen ||| weiming zhang ||| nenghai yu ||| lu yuan ||| dong chen ||| baining guo ||| 
2022 ||| deep bidirectional transformers for soc flow specification mining. ||| md rubel ahmed ||| hao zheng ||| 
2019 ||| aspect specific opinion expression extraction using attention based lstm-crf network. ||| abhishek laddha ||| arjun mukherjee ||| 
2019 ||| progressive self-supervised attention learning for aspect-level sentiment analysis. ||| jialong tang ||| ziyao lu ||| jinsong su ||| yubin ge ||| linfeng song ||| le sun ||| jiebo luo ||| 
2021 ||| few-shot temporal action localization with query adaptive transformer. ||| sauradip nag ||| xiatian zhu ||| tao xiang ||| 
2021 ||| inversemv: composing piano scores with a convolutional video-music transformer. ||| chin-tung lin ||| mu yang ||| 
2020 ||| weak supervision and referring attention for temporal-textual association learning. ||| zhiyuan fang ||| shu kong ||| zhe wang ||| charless c. fowlkes ||| yezhou yang ||| 
2022 ||| transfusion: robust lidar-camera fusion for 3d object detection with transformers. ||| xuyang bai ||| zeyu hu ||| xinge zhu ||| qingqiu huang ||| yilun chen ||| hongbo fu ||| chiew-lan tai ||| 
2019 ||| cross-modal self-attention network for referring image segmentation. ||| linwei ye ||| mrigank rochan ||| zhi liu ||| yang wang ||| 
2021 ||| are pre-trained convolutions better than pre-trained transformers? ||| yi tay ||| mostafa dehghani ||| jai prakash gupta ||| dara bahri ||| vamsi aribandi ||| zhen qin ||| donald metzler ||| 
2018 ||| sequential attention gan for interactive image editing via dialogue. ||| yu cheng ||| zhe gan ||| yitong li ||| jingjing liu ||| jianfeng gao ||| 
2021 ||| attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. ||| jefferson fontinele ||| gabriel lefundes ||| luciano oliveira ||| 
2018 ||| harmonious attention network for person re-identification. ||| wei li ||| xiatian zhu ||| shaogang gong ||| 
2020 ||| electricity theft detection with self-attention. ||| paulo finardi ||| israel campiotti ||| gustavo plensack ||| rafael derradi de souza ||| rodrigo nogueira ||| gustavo r. pinheiro ||| roberto de alencar lotufo ||| 
2017 ||| interpretable learning for self-driving cars by visualizing causal attention. ||| jinkyu kim ||| john f. canny ||| 
2020 ||| stress test evaluation of transformer-based models in natural language understanding tasks. ||| carlos aspillaga ||| andr ||| s carvallo ||| vladimir araujo ||| 
2020 ||| see more, know more: unsupervised video object segmentation with co-attention siamese networks. ||| xiankai lu ||| wenguan wang ||| chao ma ||| jianbing shen ||| ling shao ||| fatih porikli ||| 
2021 ||| m-based algorithm for approximating self-attention. ||| yunyang xiong ||| zhanpeng zeng ||| rudrasis chakraborty ||| mingxing tan ||| glenn fung ||| yin li ||| vikas singh ||| 
2021 ||| distantly supervised relation extraction via recursive hierarchy-interactive attention and entity-order perception. ||| ridong han ||| tao peng ||| jiayu han ||| lin yue ||| hai cui ||| lu liu ||| 
2019 ||| program classification using gated graph attention neural network for online programming service. ||| mingming lu ||| dingwu tan ||| naixue xiong ||| zailiang chen ||| haifeng li ||| 
2017 ||| a neural attention model for categorizing patient safety events. ||| arman cohan ||| allan fong ||| nazli goharian ||| raj m. ratwani ||| 
2018 ||| video object segmentation with joint re-identification and attention-aware mask propagation. ||| xiaoxiao li ||| chen change loy ||| 
2021 ||| transferring bert-like transformers' knowledge for authorship verification. ||| andrei manolache ||| florin brad ||| elena burceanu ||| antonio barbalau ||| radu tudor ionescu ||| marius popescu ||| 
2018 ||| exgate: externally controlled gating for feature-based attention in artificial neural networks. ||| jarryd son ||| amit kumar mishra ||| 
2019 ||| self-attention with functional time representation learning. ||| da xu ||| chuanwei ruan ||| sushant kumar ||| evren k ||| rpeoglu ||| kannan achan ||| 
2021 ||| efficient conformer-based speech recognition with linear attention. ||| shengqiang li ||| menglong xu ||| xiao-lei zhang ||| 
2018 ||| video-based person re-identification via 3d convolutional networks and non-local attention. ||| xingyu liao ||| lingxiao he ||| zhouwang yang ||| 
2021 ||| tsnat: two-step non-autoregressvie transformer models for speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| ye bai ||| shuai zhang ||| zhengqi wen ||| xuefei liu ||| 
2022 ||| transfuse: a unified transformer-based image fusion framework using self-supervised learning. ||| linhao qu ||| shaolei liu ||| manning wang ||| shiman li ||| siqi yin ||| qin qiao ||| zhijian song ||| 
2020 ||| age and gender prediction from face images using attentional convolutional network. ||| amirali abdolrashidi ||| mehdi minaei ||| elham azimi ||| shervin minaee ||| 
2020 ||| rethinking batch normalization in transformers. ||| sheng shen ||| zhewei yao ||| amir gholami ||| michael w. mahoney ||| kurt keutzer ||| 
2022 ||| improving fraud detection via hierarchical attention-based graph neural network. ||| yajing liu ||| zhengya sun ||| wensheng zhang ||| 
2019 ||| spatiotemporal tile-based attention-guided lstms for traffic video prediction. ||| tu nguyen ||| 
2019 ||| image super-resolution via attention based back projection networks. ||| zhi-song liu ||| li-wen wang ||| chu-tak li ||| wan-chi siu ||| yui-lam chan ||| 
2022 ||| x-trans2cap: cross-modal knowledge transfer using transformer for 3d dense captioning. ||| zhihao yuan ||| xu yan ||| yinghong liao ||| yao guo ||| guanbin li ||| zhen li ||| shuguang cui ||| 
2020 ||| sceneformer: indoor scene generation with transformers. ||| xinpeng wang ||| chandan yeshwanth ||| matthias nie ||| ner ||| 
2018 ||| learning when to concentrate or divert attention: self-adaptive attention temperature for neural machine translation. ||| junyang lin ||| xu sun ||| xuancheng ren ||| muyu li ||| qi su ||| 
2019 ||| forced spatial attention for driver foot activity classification. ||| akshay rangesh ||| mohan m. trivedi ||| 
2019 ||| image captioning using facial expression and attention. ||| omid mohamad nezami ||| mark dras ||| stephen wan ||| c ||| cile paris ||| 
2021 ||| study of positional encoding approaches for audio spectrogram transformers. ||| leonardo pepino ||| pablo riera ||| luciana ferrer ||| 
2022 ||| estimation of speaker age and height from speech signal using bi-encoder transformer mixture model. ||| tarun gupta ||| duc-tuan truong ||| tran the anh ||| chng eng siong ||| 
2019 ||| progressive face super-resolution via attention to facial landmark. ||| deokyun kim ||| minseon kim ||| gihyun kwon ||| daeshik kim ||| 
2020 ||| loss-analysis via attention-scale for physiologic time series. ||| jiawei yang ||| madalena costa ||| ary l. goldberger ||| jeffrey m. hausdorff ||| 
2022 ||| robustness verification for attention networks using mixed integer programming. ||| hsuan-cheng liao ||| chih-hong cheng ||| maximilian knei ||| l ||| alois c. knoll ||| 
2021 ||| federated learning with dynamic transformer for text to speech. ||| zhenhou hong ||| jianzong wang ||| xiaoyang qu ||| jie liu ||| chendong zhao ||| jing xiao ||| 
2022 ||| uofa-truth at factify 2022 : transformer and transfer learning based multi-modal fact-checking. ||| abhishek dhankar ||| osmar r. za ||| ane ||| fran ||| ois bolduc ||| 
2018 ||| mattnet: modular attention network for referring expression comprehension. ||| licheng yu ||| zhe lin ||| xiaohui shen ||| jimei yang ||| xin lu ||| mohit bansal ||| tamara l. berg ||| 
2020 ||| coral: code representation learning with weakly-supervised transformers for analyzing data analysis. ||| ge zhang ||| mike a. merrill ||| yang liu ||| jeffrey heer ||| tim althoff ||| 
2021 ||| learning graph structures with transformer for multivariate time series anomaly detection in iot. ||| zekai chen ||| dingshuo chen ||| zixuan yuan ||| xiuzhen cheng ||| xiao zhang ||| 
2020 ||| pinet: attention pooling for graph classification. ||| peter meltzer ||| marcelo daniel gutierrez mallea ||| peter j. bentley ||| 
2019 ||| multi-graph transformer for free-hand sketch recognition. ||| peng xu ||| chaitanya k. joshi ||| xavier bresson ||| 
2020 ||| why is attention not so attentive? ||| bing bai ||| jian liang ||| guanhua zhang ||| hao li ||| kun bai ||| fei wang ||| 
2020 ||| sequential neural rendering with transformer. ||| phong nguyen-ha ||| lam huynh ||| esa rahtu ||| janne heikkil ||| 
2022 ||| tableformer: table structure understanding with transformers. ||| ahmed nassar ||| nikolaos livathinos ||| maksym lysak ||| peter w. j. staar ||| 
2021 ||| class token and knowledge distillation for multi-head self-attention speaker verification systems. ||| victoria mingote ||| antonio miguel ||| alfonso ortega gim ||| nez ||| eduardo lleida ||| 
2021 ||| mutually-constrained monotonic multihead attention for online asr. ||| jaeyun song ||| hajin shim ||| eunho yang ||| 
2021 ||| gene transformer: transformers for the gene expression-based classification of cancer subtypes. ||| anwar khan ||| boreom lee ||| 
2021 ||| u-gat: multimodal graph attention network for covid-19 outcome prediction. ||| matthias keicher ||| hendrik burwinkel ||| david bani-harouni ||| magdalini paschali ||| tobias czempiel ||| egon burian ||| marcus r. makowski ||| rickmer braren ||| nassir navab ||| thomas wendler ||| 
2020 ||| cnrl at semeval-2020 task 5: modelling causal reasoning in language with multi-head self-attention weights based counterfactual detection. ||| rajaswa patil ||| veeky baths ||| 
2020 ||| airborne lidar point cloud classification with graph attention convolution neural network. ||| congcong wen ||| xiang li ||| xiaojing yao ||| ling peng ||| tianhe chi ||| 
2021 ||| hi-transformer: hierarchical interactive transformer for efficient and effective long document modeling. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| yongfeng huang ||| 
2020 ||| weakly supervised segmentation with multi-scale adversarial attention gates. ||| gabriele valvano ||| andrea leo ||| sotirios a. tsaftaris ||| 
2021 ||| attention based communication and control for multi-uav path planning. ||| hamid shiri ||| hyowoon seo ||| jihong park ||| mehdi bennis ||| 
2021 ||| anchor detr: query design for transformer-based detector. ||| yingming wang ||| xiangyu zhang ||| tong yang ||| jian sun ||| 
2017 ||| diversity driven attention model for query-based abstractive summarization. ||| preksha nema ||| mitesh m. khapra ||| anirban laha ||| balaraman ravindran ||| 
2018 ||| end-to-end neural relation extraction using deep biaffine attention. ||| dat quoc nguyen ||| karin verspoor ||| 
2020 ||| attention-based network for low-light image enhancement. ||| cheng zhang ||| qingsen yan ||| yu zhu ||| xianjun li ||| jinqiu sun ||| yanning zhang ||| 
2019 ||| learning dynamics of attention: human prior for interpretable machine reasoning. ||| wonjae kim ||| yoonho lee ||| 
2020 ||| fairs - soft focus generator and attention for robust object segmentation from extreme points. ||| ahmed h. shahin ||| prateek munjal ||| ling shao ||| shadab khan ||| 
2017 ||| bottom-up and top-down attention for image captioning and vqa. ||| peter anderson ||| xiaodong he ||| chris buehler ||| damien teney ||| mark johnson ||| stephen gould ||| lei zhang ||| 
2020 ||| convolutional neural network optimization via channel reassessment attention module. ||| yutao shen ||| ying wen ||| 
2020 ||| spatten: efficient sparse attention architecture with cascade token and head pruning. ||| hanrui wang ||| zhekai zhang ||| song han ||| 
2019 ||| point clouds learning with attention-based graph convolution networks. ||| zhuyang xie ||| junzhou chen ||| bo peng ||| 
2019 ||| improving object detection with inverted attention. ||| zeyi huang ||| wei ke ||| dong huang ||| 
2022 ||| signal-aware direction-of-arrival estimation using attention mechanisms. ||| wolfgang mack ||| julian wechsler ||| emanu ||| l a. p. habets ||| 
2021 ||| transformer-based dual relation graph for multi-label image recognition. ||| jiawei zhao ||| ke yan ||| yifan zhao ||| xiaowei guo ||| feiyue huang ||| jia li ||| 
2022 ||| 3d multi-object tracking using graph neural networks with cross-edge modality attention. ||| martin buchner ||| abhinav valada ||| 
2020 ||| graph-based universal dependency parsing in the age of the transformer: what works, and what doesn't. ||| stefan gr ||| newald ||| annemarie friedrich ||| jonas kuhn ||| 
2021 ||| revitalizing cnn attentions via transformers in self-supervised visual representation learning. ||| chongjian ge ||| youwei liang ||| yibing song ||| jianbo jiao ||| jue wang ||| ping luo ||| 
2020 ||| cars can't fly up in the sky: improving urban-scene segmentation via height-driven attention networks. ||| sungha choi ||| joanne taery kim ||| jaegul choo ||| 
2021 ||| query2label: a simple transformer way to multi-label classification. ||| shilong liu ||| lei zhang ||| xiao yang ||| hang su ||| jun zhu ||| 
2020 ||| spatial transformer point convolution. ||| yuan fang ||| chunyan xu ||| zhen cui ||| yuan zong ||| jian yang ||| 
2018 ||| statistical transformer networks: learning shape and appearance models via self supervision. ||| anil bas ||| william a. p. smith ||| 
2018 ||| automated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks. ||| volodymyr lyubinets ||| taras boiko ||| deon nicholas ||| 
2021 ||| investigating attention mechanism in 3d point cloud object detection. ||| shi qiu ||| yunfan wu ||| saeed anwar ||| chongyi li ||| 
2021 ||| lattegan: visually guided language attention for multi-turn text-conditioned image manipulation. ||| shoya matsumori ||| yuki abe ||| kosuke shingyouchi ||| komei sugiura ||| michita imai ||| 
2019 ||| boosted attention: leveraging human attention for image captioning. ||| shi chen ||| qi zhao ||| 
2022 ||| multiscale convolutional transformer with center mask pretraining for hyperspectral image classification. ||| sen jia ||| yifan wang ||| 
2022 ||| datr: domain-adaptive transformer for multi-domain landmark detection. ||| heqin zhu ||| qingsong yao ||| s. kevin zhou ||| 
2020 ||| alphanet: an attention guided deep network for automatic image matting. ||| rishab sharma ||| rahul deora ||| anirudha vishvakarma ||| 
2020 ||| semg gesture recognition with a simple model of attention. ||| david josephs ||| carson drake ||| andrew heroy ||| john santerre ||| 
2020 |||  english fasttext embeddings with the transformer. ||| tosin p. adewumi ||| foteini liwicki ||| marcus liwicki ||| 
2022 ||| crossformer: cross spatio-temporal transformer for 3d human pose estimation. ||| mohammed hassanin ||| abdelwahed khamiss ||| mohammed bennamoun ||| farid boussa ||| d ||| ibrahim radwan ||| 
2020 ||| deformer: decomposing pre-trained transformers for faster question answering. ||| qingqing cao ||| harsh trivedi ||| aruna balasubramanian ||| niranjan balasubramanian ||| 
2022 ||| dnnfuser: generative pre-trained transformer as a generalized mapper for layer fusion in dnn accelerators. ||| sheng-chun kao ||| xiaoyu huang ||| tushar krishna ||| 
2020 ||| s-vectors: speaker embeddings based on transformer's encoder for text-independent speaker verification. ||| metilda sagaya mary n. j ||| sandesh v. katta ||| srinivasan umesh ||| 
2021 ||| rethinking and improving relative position encoding for vision transformer. ||| kan wu ||| houwen peng ||| minghao chen ||| jianlong fu ||| hongyang chao ||| 
2021 ||| the entire network structure of crossmodal transformer. ||| meng li ||| changyan lin ||| lixia shu ||| xin pu ||| yi chen ||| heng wu ||| jiasong li ||| hongshuai cao ||| 
2019 ||| cross-attention end-to-end asr for two-party conversations. ||| suyoun kim ||| siddharth dalmia ||| florian metze ||| 
2019 ||| location attention for extrapolation to longer sequences. ||| yann dubois ||| gautier dagan ||| dieuwke hupkes ||| elia bruni ||| 
2021 ||| unifying global-local representations in salient object detection with transformer. ||| sucheng ren ||| qiang wen ||| nanxuan zhao ||| guoqiang han ||| shengfeng he ||| 
2020 ||| sea-net: squeeze-and-excitation attention net for diabetic retinopathy grading. ||| ziyuan zhao ||| kartik chopra ||| zeng zeng ||| xiaoli li ||| 
2020 ||| knowledge fusion transformers for video action recognition. ||| ganesh samarth ||| sheetal ojha ||| nikhil pareek ||| 
2020 ||| brums at semeval-2020 task 12 : transformer based multilingual offensive language identification in social media. ||| tharindu ranasinghe ||| hansi hettiarachchi ||| 
2020 ||| cost-effective interactive attention learning with neural attention processes. ||| jay heo ||| junhyeon park ||| hyewon jeong ||| kwang joon kim ||| juho lee ||| eunho yang ||| sung ju hwang ||| 
2021 ||| neural attention distillation: erasing backdoor triggers from deep neural networks. ||| yige li ||| xixiang lyu ||| nodens koren ||| lingjuan lyu ||| bo li ||| xingjun ma ||| 
2020 ||| guiding monocular depth estimation using depth-attention volume. ||| lam huynh ||| phong nguyen-ha ||| jiri matas ||| esa rahtu ||| janne heikkil ||| 
2021 ||| an ensemble of pre-trained transformer models for imbalanced multiclass malware classification. ||| ferhat demirkiran ||| aykut  ||| ayir ||| ugur  ||| nal ||| hasan dag ||| 
2020 ||| apan: asynchronous propagate attention network for real-time temporal graph embedding. ||| xuhong wang ||| ding lyu ||| mengjian li ||| yang xia ||| qi yang ||| xinwen wang ||| xinguang wang ||| ping cui ||| yupu yang ||| bowen sun ||| zhenyu guo ||| 
2021 ||| caltext: contextual attention localization for offline handwritten text. ||| tayaba anjum ||| nazar khan ||| 
2021 ||| ask2transformers: zero-shot domain labelling with pre-trained language models. ||| oscar sainz ||| german rigau ||| 
2020 ||| hierarchical multi-scale attention for semantic segmentation. ||| andrew tao ||| karan sapra ||| bryan catanzaro ||| 
2021 ||| new approaches to long document summarization: fourier transform based attention in a transformer model. ||| andrew kiruluta ||| andreas lemos ||| eric lundy ||| 
2021 ||| hierarchical rnns-based transformers maddpg for mixed cooperative-competitive environments. ||| xiaolong wei ||| lifang yang ||| xianglin huang ||| gang cao ||| zhulin tao ||| zhengyang du ||| jing an ||| 
2022 ||| multi-view subspace adaptive learning via autoencoder and attention. ||| jian-wei liu ||| hao-jie xie ||| runkun lu ||| xionglin luo ||| 
2022 ||| uniformer: unifying convolution and self-attention for visual recognition. ||| kunchang li ||| yali wang ||| junhao zhang ||| peng gao ||| guanglu song ||| yu liu ||| hongsheng li ||| yu qiao ||| 
2020 ||| supervised attention for speaker recognition. ||| seong min kye ||| joon son chung ||| hoirin kim ||| 
2019 ||| camal: context-aware multi-scale attention framework for lightweight visual place recognition. ||| ahmad khaliq ||| shoaib ehsan ||| michael milford ||| klaus d. mcdonald-maier ||| 
2020 ||| epgat: gene essentiality prediction with graph attention networks. ||| jo ||| o schapke ||| anderson tavares ||| mariana recamonde mendoza ||| 
2018 ||| attentioned convolutional lstm inpaintingnetwork for anomaly detection in videos. ||| itamar ben-ari ||| ravid shwartz-ziv ||| 
2019 ||| an attention mechanism for musical instrument recognition. ||| siddharth gururani ||| mohit sharma ||| alexander lerch ||| 
2021 ||| sgtr: end-to-end scene graph generation with transformer. ||| rongjie li ||| songyang zhang ||| xuming he ||| 
2019 ||| directing dnns attention for facial attribution classification using gradient-weighted class activation mapping. ||| xi yang ||| bojian wu ||| issei sato ||| takeo igarashi ||| 
2020 ||| attention-slam: a visual monocular slam learning from human gaze. ||| jinquan li ||| ling pei ||| danping zou ||| songpengcheng xia ||| qi wu ||| tao li ||| zhen sun ||| wenxian yu ||| 
2021 ||| human attention in fine-grained classification. ||| yao rong ||| wenjia xu ||| zeynep akata ||| enkelejda kasneci ||| 
2020 ||| global self-attention networks for image recognition. ||| zhuoran shen ||| irwan bello ||| raviteja vemulapalli ||| xuhui jia ||| ching-hui chen ||| 
2020 ||| cross-correlated attention networks for person re-identification. ||| jieming zhou ||| soumava kumar roy ||| pengfei fang ||| mehrtash harandi ||| lars petersson ||| 
2021 ||| multi-modal fusion transformer for end-to-end autonomous driving. ||| aditya prakash ||| kashyap chitta ||| andreas geiger ||| 
2021 ||| multi-task learning with cross attention for keyword spotting. ||| takuya higuchi ||| anmol gupta ||| chandra dhir ||| 
2020 ||| on the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition. ||| negar heidari ||| alexandros iosifidis ||| 
2019 ||| dynamic graph attention for referring expression comprehension. ||| sibei yang ||| guanbin li ||| yizhou yu ||| 
2018 ||| compositional attention networks for interpretability in natural language question answering. ||| selvakumar murugan ||| suriyadeepan ramamoorthy ||| vaidheeswaran archana ||| malaikannan sankarasubbu ||| 
2020 ||| radial deformation emplacement in power transformers using long short-term memory networks. ||| arash moradzadeh ||| kazem pourhossein ||| behnam mohammadi-ivatloo ||| tohid khalili ||| ali bidram ||| 
2019 ||| mina: multilevel knowledge-guided attention for modeling electrocardiography signals. ||| shenda hong ||| cao xiao ||| tengfei ma ||| hongyan li ||| jimeng sun ||| 
2021 ||| hopper: multi-hop transformer for spatiotemporal reasoning. ||| honglu zhou ||| asim kadav ||| farley lai ||| alexandru niculescu-mizil ||| martin renqiang min ||| mubbasir kapadia ||| hans peter graf ||| 
2019 ||| accelerating transformer decoding via a hybrid of self-attention and recurrent neural network. ||| chengyi wang ||| shuangzhi wu ||| shujie liu ||| 
2022 ||| mdmmt-2: multidomain multimodal transformer for video retrieval, one more step towards generalization. ||| alexander kunitsyn ||| maksim kalashnikov ||| maksim dzabraev ||| andrei ivaniuta ||| 
2019 ||| semantically conditioned dialog response generation via hierarchical disentangled self-attention. ||| wenhu chen ||| jianshu chen ||| pengda qin ||| xifeng yan ||| william yang wang ||| 
2017 ||| su-rug at the conll-sigmorphon 2017 shared task: morphological inflection with attentional sequence-to-sequence models. ||| robert  ||| stling ||| johannes bjerva ||| 
2021 ||| graph relation transformer: incorporating pairwise object features into the transformer architecture. ||| michael yang ||| aditya anantharaman ||| zachary kitowski ||| derik clive robert ||| 
2021 ||| dual attention-based federated learning for wireless traffic prediction. ||| chuanting zhang ||| shuping dang ||| basem shihada ||| mohamed-slim alouini ||| 
2021 ||| a weakly-supervised depth estimation network using attention mechanism. ||| fang gao ||| jiabao wang ||| jun yu ||| yaoxiong wang ||| feng shuang ||| 
2021 ||| attributing fair decisions with attention interventions. ||| ninareh mehrabi ||| umang gupta ||| fred morstatter ||| greg ver steeg ||| aram galstyan ||| 
2021 ||| on the robustness of vision transformers to adversarial examples. ||| kaleel mahmood ||| rigel mahmood ||| marten van dijk ||| 
2018 ||| attention-aware generative adversarial networks (ata-gans). ||| dimitris kastaniotis ||| ioanna ntinou ||| dimitrios tsourounis ||| george economou ||| spiros fotopoulos ||| 
2020 ||| multistage attention resu-net for semantic segmentation of fine-resolution remote sensing images. ||| rui li ||| jianlin su ||| chenxi duan ||| shunyi zheng ||| 
2019 ||| is attention all what you need? - an empirical investigation on convolution-based active memory and self-attention. ||| thomas dowdell ||| hongyu zhang ||| 
2021 ||| variational attention: propagating domain-specific knowledge for multi-domain learning in crowd counting. ||| binghui chen ||| zhaoyi yan ||| ke li ||| pengyu li ||| biao wang ||| wangmeng zuo ||| lei zhang ||| 
2019 ||| attentiongan: unpaired image-to-image translation using attention-guided generative adversarial networks. ||| hao tang ||| hong liu ||| dan xu ||| philip h. s. torr ||| nicu sebe ||| 
2021 ||| understanding top-down attention using task-oriented ablation design. ||| freddie bickford smith ||| brett d. roads ||| xiaoliang luo ||| bradley c. love ||| 
2021 ||| adversarially regularized graph attention networks for inductive learning on partially labeled graphs. ||| jiaren xiao ||| quanyu dai ||| xiaochen xie ||| james lam ||| ka-wai kwok ||| 
2018 ||| multi-cast attention networks for retrieval-based question answering and response prediction. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2020 ||| do syntax trees help pre-trained transformers extract information? ||| devendra singh sachan ||| yuhao zhang ||| peng qi ||| william l. hamilton ||| 
2020 ||| $o(n)$ connections are expressive enough: universal approximability of sparse transformers. ||| chulhee yun ||| yin-wen chang ||| srinadh bhojanapalli ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2021 ||| open set domain recognition via attention-based gcn and semantic matching optimization. ||| xinxing he ||| yuan yuan ||| zhiyu jiang ||| 
2022 ||| knowledge-enriched attention network with group-wise semantic for visual storytelling. ||| tengpeng li ||| hanli wang ||| bin he ||| chang wen chen ||| 
2019 ||| explainable authorship verification in social media via attention-based similarity learning. ||| benedikt t. boenninghoff ||| steffen hessler ||| dorothea kolossa ||| robert m. nickel ||| 
2021 ||| learning to recognize actions on objects in egocentric video with attention dictionaries. ||| swathikiran sudhakaran ||| sergio escalera ||| oswald lanz ||| 
2020 ||| detection of lexical stress errors in non-native (l2) english with data augmentation and attention. ||| daniel korzekwa ||| roberto barra-chicote ||| szymon zaporowski ||| grzegorz beringer ||| jaime lorenzo-trueba ||| alicja serafinowicz ||| jasha droppo ||| thomas drugman ||| bozena kostek ||| 
2021 ||| teach me how to label: labeling functions from natural language with text-to-text transformers. ||| yannis papanikolaou ||| 
2020 ||| geometric attention for prediction of differential properties in 3d point clouds. ||| albert matveev ||| alexey artemov ||| denis zorin ||| evgeny burnaev ||| 
2018 ||| pcas: pruning channels with attention statistics. ||| kohei yamamoto ||| kurato maeno ||| 
2021 ||| dual-camera super-resolution with aligned attention modules. ||| tengfei wang ||| jiaxin xie ||| wenxiu sun ||| qiong yan ||| qifeng chen ||| 
2021 ||| image inpainting with edge-guided learnable bidirectional attention maps. ||| dongsheng wang ||| chaohao xie ||| shaohui liu ||| zhenxing niu ||| wangmeng zuo ||| 
2021 ||| ast-transformer: encoding abstract syntax trees efficiently for code summarization. ||| ze tang ||| chuanyi li ||| jidong ge ||| xiaoyu shen ||| zheling zhu ||| bin luo ||| 
2022 ||| tactis: transformer-attentional copulas for time series. ||| alexandre drouin ||| tienne marcotte ||| nicolas chapados ||| 
2020 ||| multi-interactive attention network for fine-grained feature learning in ctr prediction. ||| kai zhang ||| hao qian ||| qing cui ||| qi liu ||| longfei li ||| jun zhou ||| jianhui ma ||| enhong chen ||| 
2020 ||| label enhanced event detection with heterogeneous graph attention networks. ||| shiyao cui ||| bowen yu ||| xin cong ||| tingwen liu ||| quangang li ||| jinqiao shi ||| 
2021 ||| an iterative contextualization algorithm with second-order attention. ||| diego maupom ||| marie-jean meurs ||| 
2019 ||| an attentional neural network architecture for folk song classification. ||| aitor arronte alvarez ||| francisco g ||| mez-martin ||| 
2021 ||| all the attention you need: global-local, spatial-channel attention for image retrieval. ||| chull hwan song ||| hye joo han ||| yannis avrithis ||| 
2021 ||| el-attention: memory efficient lossless attention for generation. ||| yu yan ||| jiusheng chen ||| weizhen qi ||| nikhil bhendawade ||| yeyun gong ||| nan duan ||| ruofei zhang ||| 
2021 ||| dnn quantization with attention. ||| ghouthi boukli hacene ||| lukas mauch ||| stefan uhlich ||| fabien cardinaux ||| 
2020 ||| selective particle attention: visual feature-based attention in deep reinforcement learning. ||| sam blakeman ||| denis mareschal ||| 
2021 ||| multi-task learning with attention for end-to-end autonomous driving. ||| keishi ishihara ||| anssi kanervisto ||| jun miura ||| ville hautam ||| ki ||| 
2019 ||| improving graph attention networks with large margin-based constraints. ||| guangtao wang ||| rex ying ||| jing huang ||| jure leskovec ||| 
2021 ||| detecting log anomalies with multi-head attention (lama). ||| yicheng guo ||| yujin wen ||| congwei jiang ||| yixin lian ||| yi wan ||| 
2020 ||| attention driven fusion for multi-modal emotion recognition. ||| darshana priyasad ||| tharindu fernando ||| simon denman ||| clinton fookes ||| sridha sridharan ||| 
2020 ||| la-hcn: label-based attention for hierarchical multi-label textclassification neural network. ||| xinyi zhang ||| jiahao xu ||| charlie soh ||| lihui chen ||| 
2022 ||| hyperbolic vision transformers: combining improvements in metric learning. ||| aleksandr ermolov ||| leyla mirvakhabova ||| valentin khrulkov ||| nicu sebe ||| ivan v. oseledets ||| 
2018 ||| exploring the use of attention within an neural machine translation decoder states to translate idioms. ||| giancarlo d. salton ||| robert j. ross ||| john d. kelleher ||| 
2017 ||| character-level intra attention network for natural language inference. ||| han yang ||| marta r. costa-juss ||| jos |||  a. r. fonollosa ||| 
2021 ||| multivariate and propagation graph attention network for spatial-temporal prediction with outdoor cellular traffic. ||| chung-yi lin ||| hung-ting su ||| shen-lung tung ||| winston h. hsu ||| 
2021 ||| date: detecting anomalies in text via self-supervision of transformers. ||| andrei manolache ||| florin brad ||| elena burceanu ||| 
2020 ||| news-driven stock prediction with attention-based noisy recurrent state transition. ||| xiao liu ||| heyan huang ||| yue zhang ||| changsen yuan ||| 
2020 ||| transformer-based context-aware sarcasm detection in conversation threads from social media. ||| xiangjue dong ||| changmao li ||| jinho d. choi ||| 
2021 ||| information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition. ||| hengshun zhou ||| jun du ||| yuanyuan zhang ||| qing wang ||| qing-feng liu ||| chin-hui lee ||| 
2020 ||| attendaffectnet: self-attention based networks for predicting affective responses from movies. ||| ha thi phuong thao ||| balamurali b. t. ||| dorien herremans ||| gemma roig ||| 
2019 ||| multi-granularity self-attention for neural machine translation. ||| jie hao ||| xing wang ||| shuming shi ||| jinfeng zhang ||| zhaopeng tu ||| 
2022 ||| external attention assisted multi-phase splenic vascular injury segmentation with limited data. ||| yuyin zhou ||| david dreizin ||| yan wang ||| fengze liu ||| wei shen ||| alan l. yuille ||| 
2019 ||| query-based interactive recommendation by meta-path and adapted attention-gru. ||| yu zhu ||| yu gong ||| qingwen liu ||| yingcai ma ||| wenwu ou ||| junxiong zhu ||| beidou wang ||| ziyu guan ||| deng cai ||| 
2022 ||| trasetr: track-to-segment transformer with contrastive query for instance-level instrument segmentation in robotic surgery. ||| zixu zhao ||| yueming jin ||| pheng-ann heng ||| 
2020 ||| feature pyramid transformer. ||| dong zhang ||| hanwang zhang ||| jinhui tang ||| meng wang ||| xiansheng hua ||| qianru sun ||| 
2021 ||| cca-mdd: a coupled cross-attention based framework for streaming mispronunciation detection and diagnosis. ||| nianzu zheng ||| liqun deng ||| wenyong huang ||| yu ting yeung ||| baohua xu ||| yuanyuan guo ||| yasheng wang ||| xin jiang ||| qun liu ||| 
2021 ||| dynamicvit: efficient vision transformers with dynamic token sparsification. ||| yongming rao ||| wenliang zhao ||| benlin liu ||| jiwen lu ||| jie zhou ||| cho-jui hsieh ||| 
2020 ||| transformer based multi-source domain adaptation. ||| dustin wright ||| isabelle augenstein ||| 
2021 ||| efficient training of visual transformers with small-size datasets. ||| yahui liu ||| enver sangineto ||| wei bi ||| nicu sebe ||| bruno lepri ||| marco de nadai ||| 
2020 ||| kronecker attention networks. ||| hongyang gao ||| zhengyang wang ||| shuiwang ji ||| 
2017 ||| reliability assessment of distribution system using fuzzy logic for modelling of transformer and line uncertainties. ||| ahmad shokrollahi ||| hossein sangrody ||| mahdi motalleb ||| mandana rezaeiahari ||| elham foruzan ||| fattah hassanzadeh ||| 
2021 ||| offensive language detection with bert-based models, by customizing attention probabilities. ||| peyman alavi ||| pouria nikvand ||| mehrnoush shamsfard ||| 
2021 ||| 3d-transformer: molecular representation with transformer in 3d space. ||| fang wu ||| qiang zhang ||| dragomir radev ||| jiyu cui ||| wen zhang ||| huabin xing ||| ningyu zhang ||| huajun chen ||| 
2020 ||| san-m: memory equipped self-attention for end-to-end speech recognition. ||| zhifu gao ||| shiliang zhang ||| ming lei ||| ian mcloughlin ||| 
2021 ||| evaluating transformer based semantic segmentation networks for pathological image segmentation. ||| cam nguyen ||| zuhayr asad ||| yuankai huo ||| 
2020 ||| digital twin of distribution power transformer for real-time monitoring of medium voltage from low voltage measurements. ||| panayiotis moutis ||| omid alizadeh mousavi ||| 
2021 ||| dual contrastive loss and attention for gans. ||| ning yu ||| guilin liu ||| aysegul dundar ||| andrew tao ||| bryan catanzaro ||| larry davis ||| mario fritz ||| 
2021 ||| main: multihead-attention imputation networks. ||| spyridon mouselinos ||| kyriakos polymenakos ||| antonis nikitakis ||| konstantinos kyriakopoulos ||| 
2020 ||| predicting chemical properties using self-attention multi-task learning based on smiles representation. ||| sangrak lim ||| yong oh lee ||| 
2021 ||| attention-based contextual language model adaptation for speech recognition. ||| richard diehl martinez ||| scott novotney ||| ivan bulyko ||| ariya rastrow ||| andreas stolcke ||| ankur gandhe ||| 
2018 ||| a mixed hierarchical attention based encoder-decoder approach for standard table summarization. ||| parag jain ||| anirban laha ||| karthik sankaranarayanan ||| preksha nema ||| mitesh m. khapra ||| shreyas shetty ||| 
2021 ||| improving hybrid ctc/attention end-to-end speech recognition with pretrained acoustic and language model. ||| keqi deng ||| songjun cao ||| yike zhang ||| long ma ||| 
2021 ||| low-rank temporal attention-augmented bilinear network for financial time-series forecasting. ||| mostafa shabani ||| alexandros iosifidis ||| 
2022 ||| can transformers be strong treatment effect estimators? ||| yi-fan zhang ||| hanlin zhang ||| zachary c. lipton ||| li erran li ||| eric p. xing ||| 
2021 ||| enhancing content preservation in text style transfer using reverse attention and conditional layer normalization. ||| dongkyu lee ||| zhiliang tian ||| lanqing xue ||| nevin l. zhang ||| 
2022 ||| end-to-end video text spotting with transformer. ||| weijia wu ||| debing zhang ||| ying fu ||| chunhua shen ||| hong zhou ||| yuanqiang cai ||| ping luo ||| 
2021 ||| efficient large-scale image retrieval with deep feature orthogonality and hybrid-swin-transformers. ||| christof henkel ||| 
2022 ||| bioformers: embedding transformers for ultra-low power semg-based gesture recognition. ||| alessio burrello ||| francesco bianco morghet ||| moritz scherer ||| simone benatti ||| luca benini ||| enrico macii ||| massimo poncino ||| daniele jahier pagliari ||| 
2018 ||| destnet: densely fused spatial transformer networks. ||| roberto annunziata ||| christos sagonas ||| jacques cal ||| 
2021 ||| meetsum: transforming meeting transcript summarization using transformers! ||| nima sadri ||| bohan zhang ||| bihan liu ||| 
2021 ||| dyadformer: a multi-modal transformer for long-range modeling of dyadic interactions. ||| david curto ||| albert clap ||| s ||| javier selva ||| sorina smeureanu ||| j ||| lio c. s. jacques j ||| nior ||| david gallardo-pujol ||| georgina guilera ||| david leiva ||| thomas b. moeslund ||| sergio escalera ||| cristina palmero ||| 
2022 ||| self-supervised and interpretable anomaly detection using network transformers. ||| daniel l. marino ||| chathurika s. wickramasinghe ||| craig rieger ||| milos manic ||| 
2018 ||| an unsupervised model with attention autoencoders for question retrieval. ||| minghua zhang ||| yunfang wu ||| 
2019 ||| reducing transformer depth on demand with structured dropout. ||| angela fan ||| edouard grave ||| armand joulin ||| 
2020 ||| ernie-doc: the retrospective long-document modeling transformer. ||| siyu ding ||| junyuan shang ||| shuohuan wang ||| yu sun ||| hao tian ||| hua wu ||| haifeng wang ||| 
2020 ||| egad: evolving graph representation learning with self-attention and knowledge distillation for live video streaming events. ||| stefanos antaris ||| dimitrios rafailidis ||| sarunas girdzijauskas ||| 
2021 ||| multi-view analysis of unregistered medical images using cross-view transformers. ||| gijs van tulder ||| yao tong ||| elena marchiori ||| 
2019 ||| who did they respond to? conversation structure modeling using masked hierarchical transformer. ||| henghui zhu ||| feng nan ||| zhiguo wang ||| ramesh nallapati ||| bing xiang ||| 
2022 ||| towards weakly-supervised text spotting using a multi-task transformer. ||| yair kittenplon ||| inbal lavi ||| sharon fogel ||| yarin bar ||| r. manmatha ||| pietro perona ||| 
2019 ||| controlling utterance length in nmt-based word segmentation with attention. ||| pierre godard ||| laurent besacier ||| fran ||| ois yvon ||| 
2019 ||| multi-agent attentional activity recognition. ||| kaixuan chen ||| lina yao ||| dalin zhang ||| bin guo ||| zhiwen yu ||| 
2020 ||| dr-spaam: a spatial-attention and auto-regressive model for person detection in 2d range data. ||| dan jia ||| alexander hermans ||| bastian leibe ||| 
2022 ||| factored attention and embedding for unstructured-view topic-related ultrasound report generation. ||| fuhai chen ||| rongrong ji ||| chengpeng dai ||| xuri ge ||| shengchuan zhang ||| xiaojing ma ||| yue gao ||| 
2021 ||| dct: dynamic compressive transformer for modeling unbounded sequence. ||| kai-po chang ||| wei-yun ma ||| 
2021 ||| gcn-se: attention as explainability for node classification in dynamic graphs. ||| yucai fan ||| yuhang yao ||| carlee joe-wong ||| 
2020 ||| meantime: mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. ||| sung min cho ||| eunhyeok park ||| sungjoo yoo ||| 
2020 ||| "when they say weed causes depression, but it's your fav antidepressant": knowledge-aware attention framework for relationship extraction. ||| shweta yadav ||| usha lokala ||| raminta daniulaityte ||| krishnaprasad thirunarayan ||| francois r. lamy ||| amit p. sheth ||| 
2020 ||| attention-based joint detection of object and semantic part. ||| keval morabia ||| jatin arora ||| tara vijaykumar ||| 
2021 ||| fusformer: a transformer-based fusion approach for hyperspectral image super-resolution. ||| jin-fan hu ||| ting-zhu huang ||| liang-jian deng ||| 
2020 ||| pneumoxttention: a cnn compensating for human fallibility when detecting pneumonia through cxr images with attention. ||| sanskriti singh ||| 
2018 ||| aggregated sparse attention for steering angle prediction. ||| sen he ||| dmitry kangin ||| yang mi ||| nicolas pugeault ||| 
2022 ||| direcformer: a directed attention in transformer approach to robust action recognition. ||| thanh-dat truong ||| quoc-huy bui ||| chi nhan duong ||| han-seok seo ||| son lam phung ||| xin li ||| khoa luu ||| 
2021 ||| transformer-based image compression. ||| ming lu ||| peiyao guo ||| huiqing shi ||| chuntong cao ||| zhan ma ||| 
2021 ||| automated news summarization using transformers. ||| anushka gupta ||| diksha chugh ||| anjum ||| rahul katarya ||| 
2022 ||| satr: slice attention with transformer for universal lesion detection. ||| han li ||| long chen ||| hu han ||| s. kevin zhou ||| 
2019 ||| text generation from knowledge graphs with graph transformers. ||| rik koncel-kedziorski ||| dhanush bekal ||| yi luan ||| mirella lapata ||| hannaneh hajishirzi ||| 
2020 ||| streaming attention-based models with augmented memory for end-to-end speech recognition. ||| ching-feng yeh ||| yongqiang wang ||| yangyang shi ||| chunyang wu ||| frank zhang ||| julian chan ||| michael l. seltzer ||| 
2022 ||| pathsage: spatial graph attention neural networks with random path sampling. ||| junhua ma ||| jiajun li ||| xueming li ||| xu li ||| 
2019 ||| bira-net: bilinear attention net for diabetic retinopathy grading. ||| ziyuan zhao ||| kerui zhang ||| xuejie hao ||| jing tian ||| matthew chin heng chua ||| li chen ||| xin xu ||| 
2021 ||| wassa@iitk at wassa 2021: multi-task learning and transformer finetuning for emotion classification and empathy prediction. ||| jay mundra ||| rohan gupta ||| sagnik mukherjee ||| 
2021 ||| zero-shot cross-lingual transfer in legal domain using transformer models. ||| zein shaheen ||| gerhard wohlgenannt ||| dmitry muromtsev ||| 
2017 ||| dense transformer networks. ||| jun li ||| yongjun chen ||| lei cai ||| ian davidson ||| shuiwang ji ||| 
2021 ||| learning to iteratively solve routing problems with dual-aspect collaborative transformer. ||| yining ma ||| jingwen li ||| zhiguang cao ||| wen song ||| le zhang ||| zhenghua chen ||| jing tang ||| 
2017 ||| refining raw sentence representations for textual entailment recognition via attention. ||| jorge a. balazs ||| edison marrese-taylor ||| pablo loyola ||| yutaka matsuo ||| 
2017 ||| code completion with neural attention and pointer networks. ||| jian li ||| yue wang ||| irwin king ||| michael r. lyu ||| 
2020 ||| attention-based residual speech portrait model for speech to face generation. ||| jianrong wang ||| xiaosheng hu ||| li liu ||| wei liu ||| mei yu ||| tianyi xu ||| 
2021 ||| single-layer vision transformers for more accurate early exits with less overhead. ||| arian bakhtiarnia ||| qi zhang ||| alexandros iosifidis ||| 
2020 ||| hybrid dynamic-static context-aware attention network for action assessment in long videos. ||| ling-an zeng ||| fa-ting hong ||| wei-shi zheng ||| qi-zhi yu ||| wei zeng ||| yao-wei wang ||| jian-huang lai ||| 
2020 ||| improved biomedical word embeddings in the transformer era. ||| jiho noh ||| ramakanth kavuluru ||| 
2022 ||| monocular robot navigation with self-supervised pretrained vision transformers. ||| miguel a. saavedra-ruiz ||| sacha morin ||| liam paull ||| 
2021 ||| transformers generalize deepsets and can be extended to graphs and hypergraphs. ||| jinwoo kim ||| saeyoon oh ||| seunghoon hong ||| 
2020 ||| how to track your dragon: a multi-attentional framework for real-time rgb-d 6-dof object pose tracking. ||| isidoros marougkas ||| petros koutras ||| nikolaos kardaris ||| georgios retsinas ||| georgia chalvatzaki ||| petros maragos ||| 
2019 ||| spatial attention for far-field speech recognition with deep beamforming neural networks. ||| weipeng he ||| lu lu ||| biqiao zhang ||| jay mahadeokar ||| kaustubh kalgaonkar ||| christian fuegen ||| 
2020 ||| stable style transformer: delete and generate approach with encoder-decoder for text style transfer. ||| joosung lee ||| 
2021 ||| gtnet: guided transformer network for detecting human-object interactions. ||| a. s. m. iftekhar ||| satish kumar ||| r. austin mcever ||| suya you ||| b. s. manjunath ||| 
2019 ||| pdanet: polarity-consistent deep attention network for fine-grained visual emotion regression. ||| sicheng zhao ||| zizhou jia ||| hui chen ||| leida li ||| guiguang ding ||| kurt keutzer ||| 
2018 ||| multimodal dual attention memory for video story question answering. ||| kyung-min kim ||| seong-ho choi ||| jin-hwa kim ||| byoung-tak zhang ||| 
2020 ||| free-form image inpainting via contrastive attention network. ||| xin ma ||| xiaoqiang zhou ||| huaibo huang ||| zhenhua chai ||| xiaolin wei ||| ran he ||| 
2021 ||| an attention-based unsupervised adversarial model for movie review spam detection. ||| yuan gao ||| maoguo gong ||| yu xie ||| a. kai qin ||| 
2020 ||| gatcluster: self-supervised gaussian-attention network for image clustering. ||| chuang niu ||| jun zhang ||| ge wang ||| jimin liang ||| 
2021 ||| regional attention with architecture-rebuilt 3d network for rgb-d gesture recognition. ||| benjia zhou ||| yunan li ||| jun wan ||| 
2018 ||| attention to refine through multi-scales for semantic segmentation. ||| shiqi yang ||| gang peng ||| 
2018 ||| attention-based graph neural network for semi-supervised learning. ||| kiran koshy thekumparampil ||| chong wang ||| sewoong oh ||| li-jia li ||| 
2020 ||| wessa at semeval-2020 task 9: code-mixed sentiment analysis using transformers. ||| ahmed sultan ||| mahmoud salim ||| amina gaber ||| islam el hosary ||| 
2022 ||| o-vit: orthogonal vision transformer. ||| yanhong fei ||| yingjie liu ||| xian wei ||| mingsong chen ||| 
2020 ||| unsupervised extractive summarization by pre-training hierarchical transformers. ||| shusheng xu ||| xingxing zhang ||| yi wu ||| furu wei ||| ming zhou ||| 
2019 ||| integrating temporal and spatial attentions for vatex video captioning challenge 2019. ||| shizhe chen ||| yida zhao ||| yuqing song ||| qin jin ||| qi wu ||| 
2021 ||| progressive attention on multi-level dense difference maps for generic event boundary detection. ||| jiaqi tang ||| zhaoyang liu ||| chen qian ||| wayne wu ||| limin wang ||| 
2018 ||| region proposal networks with contextual selective attention for real-time organ detection. ||| awais mansoor ||| antonio r. porras ||| marius george linguraru ||| 
2022 ||| the devil is in the details: window-based attention for image compression. ||| renjie zou ||| chunfeng song ||| zhaoxiang zhang ||| 
2019 ||| single headed attention rnn: stop thinking with your head. ||| stephen merity ||| 
2021 ||| transformers with competitive ensembles of independent mechanisms. ||| alex lamb ||| di he ||| anirudh goyal ||| guolin ke ||| chien-feng liao ||| mirco ravanelli ||| yoshua bengio ||| 
2019 ||| progress notes classification and keyword extraction using attention-based deep learning models with bert. ||| matthew tang ||| priyanka gandhi ||| md. ahsanul kabir ||| christopher zou ||| jordyn blakey ||| xiao luo ||| 
2019 ||| learning good representation via continuous attention. ||| liang zhao ||| wei xu ||| 
2022 ||| learning wave propagation with attention-based convolutional recurrent autoencoder net. ||| indu kant deo ||| rajeev jaiman ||| 
2021 ||| armour: generalizable compact self-attention for vision transformers. ||| lingchuan meng ||| 
2021 ||| an empirical study of training self-supervised vision transformers. ||| xinlei chen ||| saining xie ||| kaiming he ||| 
2020 ||| data mining in clinical trial text: transformers for classification and question answering tasks. ||| lena schmidt ||| julie weeds ||| julian p. t. higgins ||| 
2021 ||| spoiler in a textstack: how much can transformers help? ||| anna wr ||| blewska ||| pawel rzepinski ||| sylwia sysko-romanczuk ||| 
2021 ||| adela: automatic dense labeling with attention for viewpoint adaptation in semantic segmentation. ||| yanchao yang ||| hanxiang ren ||| he wang ||| bokui shen ||| qingnan fan ||| youyi zheng ||| c. karen liu ||| leonidas j. guibas ||| 
2021 ||| p2t: pyramid pooling transformer for scene understanding. ||| yu-huan wu ||| yun liu ||| xin zhan ||| ming-ming cheng ||| 
2019 ||| topic spotting using hierarchical networks with self attention. ||| pooja chitkara ||| ashutosh modi ||| pravalika avvaru ||| sepehr janghorbani ||| mubbasir kapadia ||| 
2021 ||| transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. ||| gabriel de souza pereira moreira ||| sara rabhi ||| ronay ak ||| md yasin kabir ||| even oldridge ||| 
2018 ||| -nets: double attention networks. ||| yunpeng chen ||| yannis kalantidis ||| jianshu li ||| shuicheng yan ||| jiashi feng ||| 
2021 ||| sega: semantic guided attention on visual prototype for few-shot learning. ||| fengyuan yang ||| ruiping wang ||| xilin chen ||| 
2021 ||| spatio-temporal self-attention network for video saliency prediction. ||| ziqiang wang ||| zhi liu ||| gongyang li ||| tianhong zhang ||| lihua xu ||| jijun wang ||| 
2020 ||| visual attention: deep rare features. ||| mancas matei ||| phutphalla kong ||| bernard gosselin ||| 
2018 ||| mars: memory attention-aware recommender system. ||| lei zheng ||| chun-ta lu ||| lifang he ||| sihong xie ||| vahid noroozi ||| he huang ||| philip s. yu ||| 
2021 ||| when vision transformers outperform resnets without pretraining or strong data augmentations. ||| xiangning chen ||| cho-jui hsieh ||| boqing gong ||| 
2019 ||| a deep patent landscaping model using transformer and graph convolutional network. ||| 
2018 ||| attention-based few-shot person re-identification using meta learning. ||| alireza rahimpour ||| hairong qi ||| 
2020 ||| hybrid attention for automatic segmentation of whole fetal head in prenatal ultrasound volumes. ||| xin yang ||| xu wang ||| yi wang ||| haoran dou ||| shengli li ||| huaxuan wen ||| yi lin ||| pheng-ann heng ||| dong ni ||| 
2018 ||| wider channel attention network for remote sensing image super-resolution. ||| 
2021 ||| full-attention based neural architecture search using context auto-regression. ||| yuan zhou ||| haiyang wang ||| shuwei huo ||| boyu wang ||| 
2021 ||| pynet-ca: enhanced pynet with channel attention for end-to-end mobile image signal processing. ||| byung-hoon kim ||| joonyoung song ||| jong chul ye ||| jaehyun baek ||| 
2021 ||| multi-modal fusion using fine-tuned self-attention and transfer learning for veracity analysis of web information. ||| priyanka meel ||| dinesh kumar vishwakarma ||| 
2021 ||| point-bert: pre-training 3d point cloud transformers with masked point modeling. ||| xumin yu ||| lulu tang ||| yongming rao ||| tiejun huang ||| jie zhou ||| jiwen lu ||| 
2020 ||| recursive non-autoregressive graph-to-graph transformer for dependency parsing with iterative refinement. ||| alireza mohammadshahi ||| james henderson ||| 
2020 ||| few-shot object detection with self-adaptive attention network for remote sensing images. ||| zixuan xiao ||| wei xue ||| ping zhong ||| 
2021 ||| predicting mood disorder symptoms with remotely collected videos using an interpretable multimodal dynamic attention fusion network. ||| tathagata banerjee ||| matthew kollada ||| pablo gersberg ||| oscar rodriguez ||| jane tiller ||| andrew e. jaffe ||| john reynders ||| 
2022 ||| a comprehensive study of vision transformers on dense prediction tasks. ||| kishaan jeeveswaran ||| senthilkumar kathiresan ||| arnav varma ||| omar magdy ||| bahram zonooz ||| elahe arani ||| 
2020 ||| heterogeneous graph attention networks for early detection of rumors on twitter. ||| qi huang ||| junshuai yu ||| jia wu ||| bin wang ||| 
2020 ||| research on modeling units of transformer transducer for mandarin speech recognition. ||| li fu ||| xiaoxiao li ||| libo zi ||| 
2022 ||| natural language to code using transformers. ||| uday kusupati ||| venkata ravi teja ailavarapu ||| 
2019 ||| emotion recognition with spatial attention and temporal softmax pooling. ||| masih aminbeidokhti ||| marco pedersoli ||| patrick cardinal ||| eric granger ||| 
2020 ||| facial uv map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual unets. ||| in seop na ||| chung tran ||| dung nguyen ||| sang dinh ||| 
2021 ||| self-adversarial training incorporating forgery attention for image forgery localization. ||| long zhuo ||| shunquan tan ||| bin li ||| jiwu huang ||| 
2021 ||| multi-head or single-head? an empirical comparison for transformer training. ||| liyuan liu ||| jialu liu ||| jiawei han ||| 
2022 ||| spectral compressive imaging reconstruction using convolution and spectral contextual transformer. ||| lishun wang ||| zongliang wu ||| yong zhong ||| xin yuan ||| 
2021 ||| beyond self-attention: external attention using two linear layers for visual tasks. ||| meng-hao guo ||| zheng-ning liu ||| tai-jiang mu ||| shi-min hu ||| 
2019 ||| multi-attention networks for temporal localization of video-level labels. ||| lijun zhang ||| srinath nizampatnam ||| ahana gangopadhyay ||| marcos v. conde ||| 
2018 ||| neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering. ||| rui zhang ||| c ||| cero nogueira dos santos ||| michihiro yasunaga ||| bing xiang ||| dragomir r. radev ||| 
2019 ||| what i see is what you see: joint attention learning for first and third person video co-analysis. ||| huangyue yu ||| minjie cai ||| yunfei liu ||| feng lu ||| 
2020 ||| classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. ||| adriana borowa ||| dawid rymarczyk ||| dorota ochonska ||| monika brzychczy-wloch ||| bartosz zielinski ||| 
2022 ||| multi-view fusion transformer for sensor-based human activity recognition. ||| yimu wang ||| kun yu ||| yan wang ||| hui xue ||| 
2020 ||| multi-channel attention selection gans for guided image-to-image translation. ||| hao tang ||| dan xu ||| yan yan ||| jason j. corso ||| philip h. s. torr ||| nicu sebe ||| 
2021 ||| exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. ||| g. jignesh chowdary ||| g. v. s. n. durga yathisha ||| suganya g ||| premalatha m ||| 
2021 ||| glance-and-gaze vision transformer. ||| qihang yu ||| yingda xia ||| yutong bai ||| yongyi lu ||| alan l. yuille ||| wei shen ||| 
2021 ||| 3d-anas v2: grafting transformer module on automatically designed convnet for hyperspectral image classification. ||| xizhe xue ||| haokui zhang ||| zongwen bai ||| ying li ||| 
2021 ||| investigating the vision transformer model for image retrieval tasks. ||| socratis gkelios ||| yiannis s. boutalis ||| savvas a. chatzichristofis ||| 
2017 ||| code attention: translating code to comments by exploiting domain features. ||| wenhao zheng ||| hong-yu zhou ||| ming li ||| jianxin wu ||| 
2021 ||| tracking by joint local and global search: a target-aware attention based approach. ||| xiao wang ||| jin tang ||| bin luo ||| yaowei wang ||| yonghong tian ||| feng wu ||| 
2022 ||| heat: hyperedge attention networks. ||| dobrik georgiev ||| marc brockschmidt ||| miltiadis allamanis ||| 
2020 ||| symmetric parallax attention for stereo image super-resolution. ||| yingqian wang ||| xinyi ying ||| longguang wang ||| jungang yang ||| wei an ||| yulan guo ||| 
2017 ||| a gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. ||| jianshu zhang ||| jun du ||| li-rong dai ||| 
2020 ||| graph transformer networks with syntactic and semantic structures for event argument extraction. ||| amir pouran ben veyseh ||| tuan ngo nguyen ||| thien huu nguyen ||| 
2022 ||| image search with text feedback by additive attention compositional learning. ||| yuxin tian ||| shawn d. newsam ||| kofi boakye ||| 
2018 ||| learning attentional communication for multi-agent cooperation. ||| jiechuan jiang ||| zongqing lu ||| 
2020 ||| multiple sclerosis lesion activity segmentation with attention-guided two-path cnns. ||| nils gessert ||| julia kr ||| ger ||| roland opfer ||| ann-christin ostwaldt ||| praveena manogaran ||| hagen h. kitzler ||| sven schippling ||| alexander schlaefer ||| 
2021 ||| mm-pyramid: multimodal pyramid attentional network for audio-visual event localization and video parsing. ||| jiashuo yu ||| ying cheng ||| rui-wei zhao ||| rui feng ||| yuejie zhang ||| 
2021 ||| cctrans: simplifying and improving crowd counting with transformer. ||| ye tian ||| xiangxiang chu ||| hongpeng wang ||| 
2022 ||| fast online video super-resolution with deformable attention pyramid. ||| dario fuoli ||| martin danelljan ||| radu timofte ||| luc van gool ||| 
2021 ||| attention-based aspect reasoning for knowledge base question answering on clinical notes. ||| ping wang ||| tian shi ||| khushbu agarwal ||| sutanay choudhury ||| chandan k. reddy ||| 
2021 ||| covid-19 tweets analysis through transformer language models. ||| abdul hameed azeemi ||| adeel waheed ||| 
2018 ||| image captioning based on a hierarchical attention mechanism and policy gradient optimization. ||| shiyang yan ||| fangyu wu ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2021 ||| multi-stream transformers. ||| mikhail s. burtsev ||| anna rumshisky ||| 
2021 ||| question-aware transformer models for consumer health question summarization. ||| shweta yadav ||| deepak gupta ||| asma ben abacha ||| dina demner-fushman ||| 
2019 ||| dada-2000: can driving accident be predicted by driver attention? analyzed by a benchmark. ||| jianwu fang ||| dingxin yan ||| jiahuan qiao ||| jianru xue ||| he wang ||| sen li ||| 
2022 ||| remaining useful life prediction using temporal deep degradation network for complex machinery with attention-based feature extraction. ||| yuwen qin ||| ningbo cai ||| chen gao ||| yadong zhang ||| yonghong cheng ||| xin chen ||| 
2021 ||| a transformer-based model to detect phishing urls. ||| pingfan xu ||| 
2021 ||| scaling vision transformers. ||| xiaohua zhai ||| alexander kolesnikov ||| neil houlsby ||| lucas beyer ||| 
2020 ||| an automatic covid-19 ct segmentation network using spatial and channel attention mechanism. ||| tongxue zhou ||| st ||| phane canu ||| su ruan ||| 
2020 ||| a knowledge-enhanced recommendation model with attribute-level co-attention. ||| deqing yang ||| zengchun song ||| lvxin xue ||| yanghua xiao ||| 
2022 ||| staged training for transformer language models. ||| sheng shen ||| pete walsh ||| kurt keutzer ||| jesse dodge ||| matthew e. peters ||| iz beltagy ||| 
2019 ||| generating diverse translation by manipulating multi-head attention. ||| zewei sun ||| shujian huang ||| hao-ran wei ||| xin-yu dai ||| jiajun chen ||| 
2021 ||| resvit: residual vision transformers for multi-modal medical image synthesis. ||| onat dalmaz ||| mahmut yurt ||| tolga  ||| ukur ||| 
2020 ||| superpixel image classification with graph attention networks. ||| pedro h. c. avelar ||| anderson r. tavares ||| thiago l. t. da silveira ||| cl ||| udio r. jung ||| lu ||| s c. lamb ||| 
2021 ||| a dual-questioning attention network for emotion-cause pair extraction with context awareness. ||| qixuan sun ||| yaqi yin ||| hong yu ||| 
2017 ||| japanese sentiment classification using a tree-structured long short-term memory with attention. ||| ryosuke miyazaki ||| mamoru komachi ||| 
2021 ||| transfuse: fusing transformers and cnns for medical image segmentation. ||| yundong zhang ||| huiye liu ||| qiang hu ||| 
2018 ||| attention-based hierarchical neural query suggestion. ||| wanyu chen ||| fei cai ||| honghui chen ||| maarten de rijke ||| 
2022 ||| dual-tasks siamese transformer framework for building damage assessment. ||| hongruixuan chen ||| edoardo nemni ||| sofia vallecorsa ||| xi li ||| chen wu ||| lars bromley ||| 
2019 ||| learning coupled spatial-temporal attention for skeleton-based action recognition. ||| jiayun wang ||| 
2021 ||| attentive contractive flow: improved contractive flows with lipschitz-constrained self-attention. ||| avideep mukherjee ||| badri narayana patro ||| sahil sidheekh ||| maneesh singh ||| vinay p. namboodiri ||| 
2021 ||| session-based recommendation with self-attention networks. ||| jun fang ||| 
2019 ||| multi-frame content integration with a spatio-temporal attention mechanism for person video motion transfer. ||| kun cheng ||| hao-zhi huang ||| chun yuan ||| lingyiqing zhou ||| wei liu ||| 
2019 ||| theme-matters: fashion compatibility learning via theme attention. ||| jui-hsin lai ||| bo wu ||| xin wang ||| dan zeng ||| tao mei ||| jingen liu ||| 
2020 ||| students need more attention: bert-based attentionmodel for small data with application to automaticpatient message triage. ||| shijing si ||| rui wang ||| jedrek wosik ||| hao zhang ||| david dov ||| guoyin wang ||| ricardo henao ||| lawrence carin ||| 
2021 ||| tune-in: training under negative environments with interference for attention networks simulating cocktail party effect. ||| jun wang ||| max w. y. lam ||| dan su ||| dong yu ||| 
2018 ||| da-gan: instance-level image translation by deep attention generative adversarial networks (with supplementary materials). ||| shuang ma ||| jianlong fu ||| chang wen chen ||| tao mei ||| 
2019 ||| agglomerative attention. ||| matthew spellings ||| 
2020 ||| attention module is not only a weight: analyzing transformers with vector norms. ||| goro kobayashi ||| tatsuki kuribayashi ||| sho yokoi ||| kentaro inui ||| 
2020 ||| blind deinterleaving of signals in time series with self-attention based soft min-cost flow learning. ||| ogul can ||| yeti ziya g ||| rb ||| z ||| berkin yildirim ||| a. aydin alatan ||| 
2021 ||| vision transformers are robust learners. ||| sayak paul ||| pin-yu chen ||| 
2021 ||| neural rule-execution tracking machine for transformer-based text generation. ||| yufei wang ||| can xu ||| huang hu ||| chongyang tao ||| stephen wan ||| mark dras ||| mark johnson ||| daxin jiang ||| 
2018 ||| adversarial tableqa: attention supervision for question answering on tables. ||| minseok cho ||| reinald kim amplayo ||| seung-won hwang ||| jonghyuck park ||| 
2020 ||| lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. ||| junbo yin ||| jianbing shen ||| chenye guan ||| dingfu zhou ||| ruigang yang ||| 
2021 ||| abnormal occupancy grid map recognition using attention network. ||| fuqin deng ||| hua feng ||| mingjian liang ||| qi feng ||| ningbo yi ||| yong yang ||| yuan gao ||| junfeng chen ||| tin lun lam ||| 
2020 ||| rose: real one-stage effort to detect the fingerprint singular point based on multi-scale spatial attention. ||| liaojun pang ||| jiong chen ||| fei guo ||| zhicheng x. cao ||| heng zhao ||| 
2022 ||| gtrans: spatiotemporal autoregressive transformer with graph embeddings for nowcasting extreme events. ||| bo feng ||| geoffrey c. fox ||| 
2019 ||| collaborative attention network for person re-identification. ||| wenpeng li ||| yongli sun ||| jinjun wang ||| han xu ||| xiangru yang ||| long cui ||| 
2021 ||| hierarchical transformer for multilingual machine translation. ||| albina khusainova ||| adil khan ||| ad ||| n ram ||| rez rivera ||| vitaly romanov ||| 
2021 ||| phraseformer: multimodal key-phrase extraction using transformer and graph embedding. ||| narjes nikzad-khasmakhi ||| mohammad-reza feizi-derakhshi ||| meysam asgari-chenaghlu ||| mohammad ali balafar ||| ali-reza feizi-derakhshi ||| taymaz rahkar-farshi ||| majid ramezani ||| zoleikha jahanbakhsh-nagadeh ||| elnaz zafarani-moattar ||| mehrdad ranjbar-khadivi ||| 
2021 ||| 3d reconstruction with transformer. ||| dan wang ||| xinrui cui ||| xun chen ||| zhengxia zou ||| tianyang shi ||| septimiu salcudean ||| z. jane wang ||| rabab ward ||| 
2022 ||| anti-oversmoothing in deep vision transformers via the fourier domain analysis: from theory to practice. ||| peihao wang ||| wenqing zheng ||| tianlong chen ||| zhangyang wang ||| 
2022 ||| pre-trained language transformers are universal image classifiers. ||| rahul goel ||| modar sulaiman ||| kimia noorbakhsh ||| mahdi sharifi ||| rajesh sharma ||| pooyan jamshidi ||| kallol roy ||| 
2021 ||| towards understanding the effectiveness of attention mechanism. ||| xiang ye ||| zihang he ||| heng wang ||| yong li ||| 
2021 ||| a transformer-based model for default prediction in mid-cap corporate markets. ||| kamesh korangi ||| christophe mues ||| cristi ||| n bravo ||| 
2021 ||| mounting video metadata on transformer-based language model for open-ended video question answering. ||| donggeon lee ||| seongho choi ||| youwon jang ||| byoung-tak zhang ||| 
2020 ||| agatha: automatic graph-mining and transformer based hypothesis generation approach. ||| justin sybrandt ||| ilya tyagin ||| michael shtutman ||| ilya safro ||| 
2018 ||| deep attention-based classification network for robust depth prediction. ||| ruibo li ||| ke xian ||| chunhua shen ||| zhiguo cao ||| hao lu ||| lingxiao hang ||| 
2021 ||| multi-person 3d motion prediction with multi-range transformers. ||| jiashun wang ||| huazhe xu ||| medhini narasimhan ||| xiaolong wang ||| 
2020 ||| manet: multimodal attention network based point- view fusion for 3d shape recognition. ||| yaxin zhao ||| jichao jiao ||| tangkun zhang ||| 
2020 ||| divergent modes of online collective attention to the covid-19 pandemic are associated with future caseload variance. ||| david rushing dewhurst ||| thayer alshaabi ||| michael v. arnold ||| joshua r. minot ||| christopher m. danforth ||| peter sheridan dodds ||| 
2022 ||| temporal relation extraction with a graph-based deep biaffine attention model. ||| bo-ying su ||| shang-ling hsu ||| kuan-yin lai ||| amarnath gupta ||| 
2021 ||| advanced long-context end-to-end speech recognition using context-expanded transformers. ||| takaaki hori ||| niko moritz ||| chiori hori ||| jonathan le roux ||| 
2020 ||| bidirectional encoder representations from transformers (bert): a sentiment analysis odyssey. ||| shivaji alaparthi ||| manit mishra ||| 
2022 ||| unleashing the power of transformer for graphs. ||| lingbing guo ||| qiang zhang ||| huajun chen ||| 
2020 ||| attention-driven dynamic graph convolutional network for multi-label image recognition. ||| jin ye ||| junjun he ||| xiaojiang peng ||| wenhao wu ||| yu qiao ||| 
2021 ||| assessing the syntactic capabilities of transformer-based multilingual language models. ||| laura p ||| rez-mayos ||| alba t ||| boas garc ||| a ||| simon mille ||| leo wanner ||| 
2018 ||| generative image inpainting with contextual attention. ||| jiahui yu ||| zhe lin ||| jimei yang ||| xiaohui shen ||| xin lu ||| thomas s. huang ||| 
2019 ||| delta: deep learning transfer using feature map with attention for convolutional networks. ||| xingjian li ||| haoyi xiong ||| hanchao wang ||| yuxuan rao ||| liping liu ||| jun huan ||| 
2021 ||| knowledge graph embedding using graph convolutional networks with relation-aware attention. ||| nasrullah sheikh ||| xiao qin ||| berthold reinwald ||| christoph miksovic ||| thomas gschwind ||| paolo scotton ||| 
2019 ||| encoding musical style with transformer autoencoders. ||| kristy choi ||| curtis hawthorne ||| ian simon ||| monica dinculescu ||| jesse h. engel ||| 
2020 ||| tdaf: top-down attention framework for vision tasks. ||| bo pang ||| yizhuo li ||| jiefeng li ||| muchen li ||| hanwen cao ||| cewu lu ||| 
2019 ||| inverse attention guided deep crowd counting network. ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2018 ||| detection of paroxysmal atrial fibrillation using attention-based bidirectional recurrent neural networks. ||| supreeth prajwal shashikumar ||| amit j. shah ||| gari d. clifford ||| shamim nemati ||| 
2020 ||| pre-trained and attention-based neural networks for building noetic task-oriented dialogue systems. ||| jia-chen gu ||| tianda li ||| quan liu ||| xiaodan zhu ||| zhen-hua ling ||| yu-ping ruan ||| 
2019 ||| style transformer: unpaired text style transfer without disentangled latent representation. ||| ning dai ||| jianze liang ||| xipeng qiu ||| xuanjing huang ||| 
2021 ||| avatr: one-shot speaker extraction with transformers. ||| shell xu hu ||| md rifat arefin ||| viet-nhat nguyen ||| alish dipani ||| xaq pitkow ||| andreas savas tolias ||| 
2019 ||| forecasting human object interaction: joint prediction of motor attention and egocentric activity. ||| miao liu ||| siyu tang ||| yin li ||| james m. rehg ||| 
2021 ||| rams-trans: recurrent attention multi-scale transformer forfine-grained image recognition. ||| yunqing hu ||| xuan jin ||| yin zhang ||| haiwen hong ||| jingfeng zhang ||| yuan he ||| hui xue ||| 
2017 ||| query-by-example spoken term detection using attention-based multi-hop networks. ||| chia-wei ao ||| hung-yi lee ||| 
2019 ||| coinnet: deep ancient roman republican coin classification via feature fusion and attention. ||| hafeez anwar ||| saeed anwar ||| sebastian zambanini ||| fatih porikli ||| 
2020 ||| neural abstractive summarization with structural attention. ||| tanya chowdhury ||| sachin kumar ||| tanmoy chakraborty ||| 
2020 ||| lexically constrained neural machine translation with levenshtein transformer. ||| raymond hendy susanto ||| shamil chollampatt ||| liling tan ||| 
2020 ||| i-bert: inductive generalization of transformer to arbitrary context lengths. ||| hyoungwook nam ||| seung byum seo ||| vikram sharma mailthody ||| noor michael ||| lan li ||| 
2019 ||| saliency learning: teaching the model where to pay attention. ||| reza ghaeini ||| xiaoli z. fern ||| hamed shahbazi ||| prasad tadepalli ||| 
2019 ||| sequence-to-sequence singing synthesis using the feed-forward transformer. ||| merlijn blaauw ||| jordi bonada ||| 
2019 ||| cascade attention guided residue learning gan for cross-modal translation. ||| bin duan ||| wei wang ||| hao tang ||| hugo latapie ||| yan yan ||| 
2020 ||| transformer based unsupervised pre-training for acoustic representation learning. ||| ruixiong zhang ||| haiwei wu ||| wubo li ||| dongwei jiang ||| wei zou ||| xiangang li ||| 
2020 ||| spatiotemporal attention for multivariate time series prediction and interpretation. ||| tryambak gangopadhyay ||| sin yong tan ||| zhanhong jiang ||| rui meng ||| soumik sarkar ||| 
2021 ||| towards a question answering assistant for software development using a transformer-based language model. ||| liliane do nascimento vale ||| marcelo de almeida maia ||| 
2021 ||| attention-guided generative adversarial network for whisper to normal speech conversion. ||| teng gao ||| jian zhou ||| huabin wang ||| liang tao ||| hon keung kwan ||| 
2019 ||| transformer asr with contextual block processing. ||| emiru tsunoo ||| yosuke kashiwagi ||| toshiyuki kumakura ||| shinji watanabe ||| 
2020 ||| funnel-transformer: filtering out sequential redundancy for efficient language processing. ||| zihang dai ||| guokun lai ||| yiming yang ||| quoc v. le ||| 
2021 ||| paanet: progressive alternating attention for automatic medical image segmentation. ||| abhishek srivastava ||| sukalpa chanda ||| debesh jha ||| michael a. riegler ||| p ||| l halvorsen ||| dag johansen ||| umapada pal ||| 
2020 ||| deep attention spatio-temporal point processes. ||| shixiang zhu ||| minghe zhang ||| ruyi ding ||| yao xie ||| 
2021 ||| shuffle transformer: rethinking spatial shuffle for vision transformer. ||| zilong huang ||| youcheng ben ||| guozhong luo ||| pei cheng ||| gang yu ||| bin fu ||| 
2019 ||| theoretical limitations of self-attention in neural sequence models. ||| michael hahn ||| 
2020 ||| parameter efficient multimodal transformers for video representation learning. ||| sangho lee ||| youngjae yu ||| gunhee kim ||| thomas m. breuel ||| jan kautz ||| yale song ||| 
2018 ||| deep snp: an end-to-end deep neural network with attention-based localization for break-point detection in snp array genomic data. ||| hamid eghbal-zadeh ||| lukas fischer ||| niko popitsch ||| florian kromp ||| sabine taschner-mandl ||| khaled koutini ||| teresa gerber ||| eva bozsaky ||| peter f. ambros ||| inge m. ambros ||| gerhard widmer ||| bernhard alois moser ||| 
2021 ||| graphit: encoding graph structure in transformers. ||| gr ||| goire mialon ||| dexiong chen ||| margot selosse ||| julien mairal ||| 
2019 ||| adaptation of deep bidirectional multilingual transformers for russian language. ||| yuri kuratov ||| mikhail y. arkhipov ||| 
2022 ||| multi-view and multi-modal event detection utilizing transformer-based multi-sensor fusion. ||| masahiro yasuda ||| yasunori ohishi ||| shoichiro saito ||| noboru harada ||| 
2021 ||| context matters: self-attention for sign language recognition. ||| fares ben slimane ||| mohamed bouguessa ||| 
2021 ||| looking twice for partial clues: weakly-supervised part-mentored attention network for vehicle re-identification. ||| lisha tang ||| yi wang ||| lap-pui chau ||| 
2022 ||| mfa: tdnn with multi-scale frequency-channel attention for text-independent speaker verification with short utterances. ||| tianchi liu ||| rohan kumar das ||| kong aik lee ||| haizhou li ||| 
2021 ||| lea-net: layer-wise external attention network for efficient color anomaly detection. ||| ryoya katafuchi ||| terumasa tokunaga ||| 
2020 ||| axial-deeplab: stand-alone axial-attention for panoptic segmentation. ||| huiyu wang ||| yukun zhu ||| bradley green ||| hartwig adam ||| alan l. yuille ||| liang-chieh chen ||| 
2021 ||| predicting pedestrian crossing intention with feature fusion and spatio-temporal attention. ||| dongfang yang ||| haolin zhang ||| ekim yurtsever ||| keith a. redmill ||| mit  ||| zg ||| ner ||| 
2021 ||| are pretrained transformers robust in intent classification? a missing ingredient in evaluation of out-of-scope intent detection. ||| jianguo zhang ||| kazuma hashimoto ||| yao wan ||| ye liu ||| caiming xiong ||| philip s. yu ||| 
2020 ||| turkeyes: a web-based toolbox for crowdsourcing attention data. ||| anelise newman ||| barry a. mcnamara ||| camilo fosco ||| yun bin zhang ||| pat sukhum ||| matthew tancik ||| nam wook kim ||| zoya bylinskii ||| 
2019 ||| e2-capsule neural networks for facial expression recognition using au-aware attention. ||| shan cao ||| yuqian yao ||| gaoyun an ||| 
2021 ||| detection of deepfake videos using long distance attention. ||| wei lu ||| lingyi liu ||| junwei luo ||| xianfeng zhao ||| yicong zhou ||| jiwu huang ||| 
2021 ||| domain attention consistency for multi-source domain adaptation. ||| zhongying deng ||| kaiyang zhou ||| yongxin yang ||| tao xiang ||| 
2019 ||| recosa: detecting the relevant contexts with self-attention for multi-turn dialogue generation. ||| hainan zhang ||| yanyan lan ||| liang pang ||| jiafeng guo ||| xueqi cheng ||| 
2021 ||| conam: confidence attention module for convolutional neural networks. ||| yu xue ||| ziming yuan ||| ferrante neri ||| 
2021 ||| fine-grained style control in transformer-based text-to-speech synthesis. ||| li-wei chen ||| alexander i. rudnicky ||| 
2019 ||| language models with transformers. ||| chenguang wang ||| mu li ||| alexander j. smola ||| 
2019 ||| insertion transformer: flexible sequence generation via insertion operations. ||| mitchell stern ||| william chan ||| jamie kiros ||| jakob uszkoreit ||| 
2020 ||| stereo endoscopic image super-resolution using disparity-constrained parallel attention. ||| tianyi zhang ||| yun gu ||| xiaolin huang ||| enmei tu ||| jie yang ||| 
2021 ||| probabilistic spatial distribution prior based attentional keypoints matching network. ||| xiaoming zhao ||| jingmeng liu ||| xingming wu ||| weihai chen ||| fanghong guo ||| zhengguo li ||| 
2017 ||| path-based attention neural model for fine-grained entity typing. ||| denghui zhang ||| pengshan cai ||| yantao jia ||| manling li ||| yuanzhuo wang ||| 
2021 ||| scale efficiently: insights from pre-training and fine-tuning transformers. ||| yi tay ||| mostafa dehghani ||| jinfeng rao ||| william fedus ||| samira abnar ||| hyung won chung ||| sharan narang ||| dani yogatama ||| ashish vaswani ||| donald metzler ||| 
2021 ||| restormer: efficient transformer for high-resolution image restoration. ||| syed waqas zamir ||| aditya arora ||| salman h. khan ||| munawar hayat ||| fahad shahbaz khan ||| ming-hsuan yang ||| 
2019 ||| coarse-grain fine-grain coattention network for multi-evidence question answering. ||| victor zhong ||| caiming xiong ||| nitish shirish keskar ||| richard socher ||| 
2022 ||| local-global context aware transformer for language-guided video segmentation. ||| chen liang ||| wenguan wang ||| tianfei zhou ||| jiaxu miao ||| yawei luo ||| yi yang ||| 
2019 ||| lsanet: feature learning on point sets by local spatial attention. ||| lin-zhuo chen ||| xuan-yi li ||| deng-ping fan ||| ming-ming cheng ||| kai wang ||| shao-ping lu ||| 
2021 ||| transformers are deep infinite-dimensional non-mercer binary kernel machines. ||| matthew a. wright ||| joseph e. gonzalez ||| 
2021 ||| towards end-to-end image compression and analysis with transformers. ||| yuanchao bai ||| xu yang ||| xianming liu ||| junjun jiang ||| yaowei wang ||| xiangyang ji ||| wen gao ||| 
2020 ||| upgraded w-net with attention gates and its application in unsupervised 3d liver segmentation. ||| dhanunjaya mitta ||| soumick chatterjee ||| oliver speck ||| andreas n ||| rnberger ||| 
2020 ||| regularizing attention networks for anomaly detection in visual question answering. ||| doyup lee ||| yeongjae cheon ||| wook-shin han ||| 
2022 ||| aa-transunet: attention augmented transunet for nowcasting tasks. ||| yimin yang ||| siamak mehrkanoon ||| 
2019 ||| question-agnostic attention for visual question answering. ||| moshiur r. farazi ||| salman h. khan ||| nick barnes ||| 
2018 ||| scan: self-and-collaborative attention network for video person re-identification. ||| ruimao zhang ||| hongbin sun ||| jingyu li ||| yuying ge ||| liang lin ||| ping luo ||| xiaogang wang ||| 
2019 ||| processing megapixel images with deep attention-sampling models. ||| angelos katharopoulos ||| fran ||| ois fleuret ||| 
2021 ||| can transformers jump around right in natural language? assessing performance transfer from scan. ||| rahma chaabouni ||| roberto dess ||| eugene kharitonov ||| 
2021 ||| neural transfer learning with transformers for social science text analysis. ||| sandra wankm ||| ller ||| 
2021 ||| stabilizing deep q-learning with convnets and vision transformers under data augmentation. ||| nicklas hansen ||| hao su ||| xiaolong wang ||| 
2019 ||| a self-attention joint model for spoken language understanding in situational dialog applications. ||| mengyang chen ||| jin zeng ||| jie lou ||| 
2021 ||| pttr: relational 3d point cloud object tracking with transformer. ||| changqing zhou ||| zhipeng luo ||| yueru luo ||| tianrui liu ||| liang pan ||| zhongang cai ||| haiyu zhao ||| shijian lu ||| 
2021 ||| loftr: detector-free local feature matching with transformers. ||| jiaming sun ||| zehong shen ||| yuang wang ||| hujun bao ||| xiaowei zhou ||| 
2019 ||| image captioning with integrated bottom-up and multi-level residual top-down attention for game scene understanding. ||| jian zheng ||| sudha krishnamurthy ||| ruxin chen ||| min-hung chen ||| zhenhao ge ||| xiaohua li ||| 
2021 ||| lctr: on awakening the local continuity of transformer for weakly supervised object localization. ||| zhiwei chen ||| changan wang ||| yabiao wang ||| guannan jiang ||| yunhang shen ||| ying tai ||| chengjie wang ||| wei zhang ||| liujuan cao ||| 
2020 ||| devising malware characterstics using transformers. ||| simra shahid ||| tanmay singh ||| yash sharma ||| kapil sharma ||| 
2020 ||| query twice: dual mixture attention meta learning for video summarization. ||| junyan wang ||| yang bai ||| yang long ||| bingzhang hu ||| zhenhua chai ||| yu guan ||| xiaolin wei ||| 
2018 ||| who is killed by police: introducing supervised attention for hierarchical lstms. ||| minh nguyen ||| thien huu nguyen ||| 
2022 ||| improving across-dataset brain tissue segmentation using transformer. ||| vishwanatha m. rao ||| zihan wan ||| david j. ma ||| pin-yu lee ||| ye tian ||| andrew f. laine ||| jia guo ||| 
2020 ||| improving attention mechanism with query-value interaction. ||| chuhan wu ||| fangzhao wu ||| tao qi ||| yongfeng huang ||| 
2021 ||| pyramid medical transformer for medical image segmentation. ||| zhuangzhuang zhang ||| baozhou sun ||| weixiong zhang ||| 
2021 ||| multi-attentional deepfake detection. ||| hanqing zhao ||| wenbo zhou ||| dongdong chen ||| tianyi wei ||| weiming zhang ||| nenghai yu ||| 
2018 ||| attention-based end-to-end models for small-footprint keyword spotting. ||| changhao shan ||| junbo zhang ||| yujun wang ||| lei xie ||| 
2021 ||| operationalizing a national digital library: the case for a norwegian transformer model. ||| per egil kummervold ||| javier de la rosa ||| freddy wetjen ||| svein arne brygfjeld ||| 
2021 ||| leveraging redundancy in attention with reuse transformers. ||| srinadh bhojanapalli ||| ayan chakrabarti ||| andreas veit ||| michal lukasik ||| himanshu jain ||| frederick liu ||| yin-wen chang ||| sanjiv kumar ||| 
2020 ||| self-attention networks for intent detection. ||| sevinj yolchuyeva ||| g ||| za n ||| meth ||| b ||| lint gyires-t ||| th ||| 
2020 ||| masked language modeling for proteins via linearly scalable long-context transformers. ||| krzysztof choromanski ||| valerii likhosherstov ||| david dohan ||| xingyou song ||| jared davis ||| tam ||| s sarl ||| s ||| david belanger ||| lucy j. colwell ||| adrian weller ||| 
2021 ||| explaining a neural attention model for aspect-based sentiment classification using diagnostic classification. ||| lisa meijer ||| flavius frasincar ||| maria mihaela trusca ||| 
2021 ||| starformer: transformer with state-action-reward representations. ||| jinghuan shang ||| michael s. ryoo ||| 
2022 ||| short-term passenger flow prediction for multi-traffic modes: a residual network and transformer based multi-task learning method. ||| yongjie yang ||| jinlei zhang ||| lixing yang ||| ziyou gao ||| 
2019 ||| transformer based reinforcement learning for games. ||| uddeshya upadhyay ||| nikunj shah ||| sucheta ravikanti ||| mayanka medhe ||| 
2018 ||| image-level attentional context modeling using nested-graph neural networks. ||| guillaume jaume ||| behzad bozorgtabar ||| hazim kemal ekenel ||| jean-philippe thiran ||| maria gabrani ||| 
2020 ||| dilated convolutional attention network for medical code assignment from clinical text. ||| shaoxiong ji ||| erik cambria ||| pekka marttinen ||| 
2021 ||| groupbert: enhanced transformer architecture with efficient grouped structures. ||| ivan chelombiev ||| daniel justus ||| douglas orr ||| anastasia dietrich ||| frithjof gressmann ||| alexandros koliousis ||| carlo luschi ||| 
2021 ||| dpt: deformable patch-based transformer for visual recognition. ||| zhiyang chen ||| yousong zhu ||| chaoyang zhao ||| guosheng hu ||| wei zeng ||| jinqiao wang ||| ming tang ||| 
2022 ||| algorithm selection for software verification using graph attention networks. ||| will leeson ||| matthew b. dwyer ||| 
2022 ||| automatic audio captioning using attention weighted event based embeddings. ||| swapnil bhosale ||| rupayan chakraborty ||| sunil kumar kopparapu ||| 
2021 ||| gigapixel histopathological image analysis using attention-based neural networks. ||| nadia brancati ||| giuseppe de pietro ||| daniel riccio ||| maria frucci ||| 
2018 ||| craft: complementary recommendations using adversarial feature transformer. ||| cong phuoc huynh ||| arri ciptadi ||| ambrish tyagi ||| amit agrawal ||| 
2021 ||| attention-based neural networks for chroma intra prediction in video coding. ||| marc g ||| rriz ||| saverio g. blasi ||| alan f. smeaton ||| noel e. o'connor ||| marta mrak ||| 
2020 ||| deep exposure fusion with deghosting via homography estimation and attention learning. ||| sheng-yeh chen ||| yung-yu chuang ||| 
2020 ||| origin-aware next destination recommendation with personalized preference attention. ||| nicholas lim ||| bryan hooi ||| see-kiong ng ||| xueou wang ||| yong liang goh ||| renrong weng ||| rui tan ||| 
2022 ||| overlaptransformer: an efficient and rotation-invariant transformer network for lidar-based place recognition. ||| junyi ma ||| jun zhang ||| jintao xu ||| rui ai ||| weihao gu ||| cyrill stachniss ||| xieyuanli chen ||| 
2021 ||| transformer-based network for rgb-d saliency detection. ||| yue wang ||| xu jia ||| lu zhang ||| yuke li ||| james h. elder ||| huchuan lu ||| 
2021 ||| grapheme-to-phoneme transformer model for transfer learning dialects. ||| eric engelhart ||| mahsa elyasi ||| gaurav bharaj ||| 
2021 ||| self-attentive ensemble transformer: representing ensemble interactions in neural networks for earth system models. ||| tobias sebastian finn ||| 
2021 ||| deep attention-based representation learning for heart sound classification. ||| zhao ren ||| kun qian ||| fengquan dong ||| zhenyu dai ||| yoshiharu yamamoto ||| bj ||| rn w. schuller ||| 
2021 ||| sctn: sparse convolution-transformer network for scene flow estimation. ||| bing li ||| cheng zheng ||| silvio giancola ||| bernard ghanem ||| 
2022 ||| deepnet: scaling transformers to 1, 000 layers. ||| hongyu wang ||| shuming ma ||| li dong ||| shaohan huang ||| dongdong zhang ||| furu wei ||| 
2020 ||| mara-net: single image deraining network with multi-level connection and adaptive regional attention. ||| yeachan park ||| myeongho jeon ||| junho lee ||| myungjoo kang ||| 
2017 ||| integrating three mechanisms of visual attention for active visual search. ||| amir rasouli ||| john k. tsotsos ||| 
2020 ||| an improved attention for visual question answering. ||| tanzila rahman ||| shih-han chou ||| leonid sigal ||| giuseppe carenini ||| 
2020 ||| object 6d pose estimation with non-local attention. ||| jianhan mei ||| henghui ding ||| xudong jiang ||| 
2020 ||| extracting angina symptoms from clinical notes using pre-trained transformer architectures. ||| aaron s. eisman ||| nishant r. shah ||| carsten eickhoff ||| george zerveas ||| elizabeth s. chen ||| wen-chih wu ||| indra neil sarkar ||| 
2021 ||| rrnet: relational reasoning network with parallel multi-scale attention for salient object detection in optical remote sensing images. ||| runmin cong ||| yumo zhang ||| leyuan fang ||| jun li ||| chunjie zhang ||| yao zhao ||| sam kwong ||| 
2019 ||| a self-attentional neural architecture for code completion with multi-task learning. ||| 
2020 ||| attention meets perturbations: robust and interpretable attention with adversarial training. ||| shunsuke kitada ||| hitoshi iyatomi ||| 
2018 ||| what we read, what we search: media attention and public attention among 193 countries. ||| haewoon kwak ||| jisun an ||| joni salminen ||| soon-gyo jung ||| bernard j. jansen ||| 
2019 ||| neural simile recognition with cyclic multitask learning and local attention. ||| jiali zeng ||| linfeng song ||| jinsong su ||| jun xie ||| wei song ||| jiebo luo ||| 
2021 ||| drug-target interaction prediction with graph attention networks. ||| haiyang wang ||| guangyu zhou ||| siqi liu ||| jyun-yu jiang ||| wei wang ||| 
2019 ||| graph attention memory for visual navigation. ||| dong li ||| dongbin zhao ||| qichao zhang ||| yuzheng zhuang ||| bin wang ||| 
2021 ||| mt-transunet: mediating multi-task tokens in transformers for skin lesion segmentation and classification. ||| jingye chen ||| jieneng chen ||| zongwei zhou ||| bin li ||| alan l. yuille ||| yongyi lu ||| 
2022 ||| spatio-temporal outdoor lighting aggregation on image sequences using transformer networks. ||| haebom lee ||| christian homeyer ||| robert herzog ||| jan rexilius ||| carsten rother ||| 
2019 ||| learning high-order structural and attribute information by knowledge graph attention networks for enhancing knowledge graph embedding. ||| wenqiang liu ||| hongyun cai ||| xu cheng ||| sifa xie ||| yipeng yu ||| hanyu zhang ||| 
2017 ||| an attention-based collaboration framework for multi-view network representation learning. ||| meng qu ||| jian tang ||| jingbo shang ||| xiang ren ||| ming zhang ||| jiawei han ||| 
2021 ||| are the multilingual models better? improving czech sentiment with transformers. ||| pavel prib ||| n ||| josef steinberger ||| 
2019 ||| improving n-gram language models with pre-trained deep transformer. ||| yiren wang ||| hongzhao huang ||| zhe liu ||| yutong pang ||| yongqiang wang ||| chengxiang zhai ||| fuchun peng ||| 
2021 ||| wifimod: transformer-based indoor human mobility modeling using passive sensing. ||| amee trivedi ||| kate silverstein ||| emma strubell ||| prashant j. shenoy ||| 
2021 ||| da-fdftnet: dual attention fake detection fine-tuning network to detect various ai-generated fake images. ||| youngoh bang ||| simon s. woo ||| 
2018 ||| reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| sen wang ||| chengqi zhang ||| 
2020 ||| jdi-t: jointly trained duration informed transformer for text-to-speech without explicit alignment. ||| dan lim ||| won jang ||| gyeonghwan o ||| hyeyeong park ||| bongwan kim ||| jaesam yoon ||| 
2020 ||| dual-path transformer network: direct context-aware modeling for end-to-end monaural speech separation. ||| jingjing chen ||| qirong mao ||| dong liu ||| 
2020 ||| dissecting lottery ticket transformers: structural and behavioral study of sparse neural machine translation. ||| rajiv movva ||| jason y. zhao ||| 
2020 ||| adaptive bi-directional attention: exploring multi-granularity representations for machine reading comprehension. ||| nuo chen ||| fenglin liu ||| chenyu you ||| peilin zhou ||| yuexian zou ||| 
2019 ||| spectral graph transformer networks for brain surface parcellation. ||| ran he ||| karthik gopinath ||| christian desrosiers ||| herve lombaert ||| 
2022 ||| problem-dependent attention and effort in neural networks with an application to image resolution. ||| chris rohlfs ||| 
2019 ||| training optimus prime, m.d.: generating medical certification items by fine-tuning openai's gpt2 transformer model. ||| matthias von davier ||| 
2021 ||| sparse fusion for multimodal transformers. ||| yi ding ||| alex rich ||| mason wang ||| noah stier ||| matthew a. turk ||| pradeep sen ||| tobias h ||| llerer ||| 
2021 ||| densepass: dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange. ||| chaoxiang ma ||| jiaming zhang ||| kailun yang ||| alina roitberg ||| rainer stiefelhagen ||| 
2021 ||| lazyformer: self attention with lazy update. ||| chengxuan ying ||| guolin ke ||| di he ||| tie-yan liu ||| 
2022 ||| octattention: octree-based large-scale contexts model for point cloud compression. ||| chunyang fu ||| ge li ||| rui song ||| wei gao ||| shan liu ||| 
2018 ||| differential attention for visual question answering. ||| badri n. patro ||| vinay p. namboodiri ||| 
2019 ||| improving generalization of transformer for speech recognition with parallel schedule sampling and relative positional embedding. ||| pan zhou ||| ruchao fan ||| wei chen ||| jia jia ||| 
2019 ||| progressive sparse local attention for video object detection. ||| chaoxu guo ||| bin fan ||| jie gu ||| qian zhang ||| shiming xiang ||| v ||| ronique prinet ||| chunhong pan ||| 
2021 ||| full-reference speech quality estimation with attentional siamese neural networks. ||| gabriel mittag ||| sebastian m ||| ller ||| 
2017 ||| supervising neural attention models for video captioning by human gaze data. ||| youngjae yu ||| jongwook choi ||| yeonhwa kim ||| kyung yoo ||| sang-hun lee ||| gunhee kim ||| 
2020 ||| explicitly modeled attention maps for image classification. ||| andong tan ||| duc tam nguyen ||| maximilian dax ||| matthias nie ||| ner ||| thomas brox ||| 
2020 ||| multi-view self-attention for interpretable drug-target interaction prediction. ||| brighter agyemang ||| wei-ping wu ||| michael yelpengne kpiebaareh ||| zhihua lei ||| ebenezer nanor ||| lei chen ||| 
2021 ||| temporal-wise attention spiking neural networks for event streams classification. ||| man yao ||| huanhuan gao ||| guangshe zhao ||| dingheng wang ||| yihan lin ||| zhao-xu yang ||| guoqi li ||| 
2019 ||| do attention heads in bert track syntactic dependencies? ||| phu mon htut ||| jason phang ||| shikha bordia ||| samuel r. bowman ||| 
2020 ||| graph-aware transformer: is attention all graphs need? ||| sanghyun yoo ||| young-seok kim ||| kang hyun lee ||| kuhwan jeong ||| junhwi choi ||| hoshik lee ||| young sang choi ||| 
2018 ||| scaling notifications beyond alerts: from subtly drawing attention up to forcing the user to take action. ||| denys j. c. matthies ||| laura milena daza parra ||| bodo urban ||| 
2018 ||| modeling attention flow on graphs. ||| xiaoran xu ||| songpeng zu ||| chengliang gao ||| yuan zhang ||| wei feng ||| 
2021 ||| asl to pet translation by a semi-supervised residual-based attention-guided convolutional neural network. ||| sahar yousefi ||| hessam sokooti ||| wouter m. teeuwisse ||| dennis f. r. heijtel ||| aart j. nederveen ||| marius staring ||| matthias j. p. van osch ||| 
2019 ||| spatial-temporal self-attention network for flow prediction. ||| haoxing lin ||| weijia jia ||| yiping sun ||| yongjian you ||| 
2021 ||| self-supervised learning with swin transformers. ||| zhenda xie ||| yutong lin ||| zhuliang yao ||| zheng zhang ||| qi dai ||| yue cao ||| han hu ||| 
2020 ||| se(3)-transformers: 3d roto-translation equivariant attention networks. ||| fabian b. fuchs ||| daniel e. worrall ||| volker fischer ||| max welling ||| 
2021 ||| adaptive sparse transformer for multilingual translation. ||| hongyu gong ||| xian li ||| dmitriy genzel ||| 
2020 ||| structured attention for unsupervised dialogue structure induction. ||| liang qiu ||| yizhou zhao ||| weiyan shi ||| yuan liang ||| feng shi ||| tao yuan ||| zhou yu ||| song-chun zhu ||| 
2020 ||| temporal-channel transformer for 3d lidar-based video object detection in autonomous driving. ||| zhenxun yuan ||| xiao song ||| lei bai ||| wengang zhou ||| zhe wang ||| wanli ouyang ||| 
2019 ||| how far are we from quantifying visual attention in mobile hci? ||| mihai b ||| ce ||| sander staal ||| andreas bulling ||| 
2021 ||| cyclegan-based non-parallel speech enhancement with an adaptive attention-in-attention mechanism. ||| guochen yu ||| yutian wang ||| chengshi zheng ||| hui wang ||| qin zhang ||| 
2018 ||| recurrent neural network attention mechanisms for interpretable system log anomaly detection. ||| andy brown ||| aaron tuor ||| brian hutchinson ||| nicole nichols ||| 
2020 ||| pixel-bert: aligning image pixels with text by deep multi-modal transformers. ||| zhicheng huang ||| zhaoyang zeng ||| bei liu ||| dongmei fu ||| jianlong fu ||| 
2019 ||| transformer-cnn: fast and reliable tool for qsar. ||| pavel karpov ||| guillaume godin ||| igor v. tetko ||| 
2019 ||| message passing attention networks for document understanding. ||| giannis nikolentzos ||| antoine j.-p. tixier ||| michalis vazirgiannis ||| 
2018 ||| a brand-level ranking system with the customized attention-gru model. ||| yu zhu ||| junxiong zhu ||| jie hou ||| yongliang li ||| beidou wang ||| ziyu guan ||| deng cai ||| 
2021 ||| fine-tuning of pre-trained transformers for hate, offensive, and profane content detection in english and marathi. ||| anna glazkova ||| michael kadantsev ||| maksim glazkov ||| 
2020 ||| on the global self-attention mechanism for graph convolutional networks. ||| chen wang ||| chengyuan deng ||| 
2021 ||| generalized decision transformer for offline hindsight information matching. ||| hiroki furuta ||| yutaka matsuo ||| shixiang shane gu ||| 
2021 ||| memory-augmented non-local attention for video super-resolution. ||| jiyang yu ||| jingen liu ||| liefeng bo ||| tao mei ||| 
2021 ||| learning bounded context-free-grammar via lstm and the transformer: difference and explanations. ||| hui shi ||| sicun gao ||| yuandong tian ||| xinyun chen ||| jishen zhao ||| 
2021 ||| dynstgat: dynamic spatial-temporal graph attention network for traffic signal control. ||| libing wu ||| min wang ||| dan wu ||| jia wu ||| 
2020 ||| multi-image super resolution of remotely sensed images using residual feature attention deep neural networks. ||| francesco salvetti ||| vittorio mazzia ||| aleem khaliq ||| marcello chiaberge ||| 
2019 ||| perceptual attention-based predictive control. ||| keuntaek lee ||| gabriel nakajima an ||| viacheslav zakharov ||| evangelos a. theodorou ||| 
2021 ||| bumblebee: a transformer for music. ||| lucas fenaux ||| maria juliana quintero ||| 
2020 ||| learning frame level attention for environmental sound classification. ||| zhichao zhang ||| shugong xu ||| shunqing zhang ||| tianhao qiao ||| shan cao ||| 
2021 ||| memory efficient adaptive attention for multiple domain learning. ||| himanshu pradeep aswani ||| abhiraj sunil kanse ||| shubhang bhatnagar ||| amit sethi ||| 
2019 ||| scene memory transformer for embodied agents in long-horizon tasks. ||| kuan fang ||| alexander toshev ||| li fei-fei ||| silvio savarese ||| 
2020 ||| human brain activity for machine attention. ||| lukas muttenthaler ||| nora hollenstein ||| maria barrett ||| 
2021 ||| wlv-rit at germeval 2021: multitask learning with transformers to detect toxic, engaging, and fact-claiming comments. ||| skye morgan ||| tharindu ranasinghe ||| marcos zampieri ||| 
2021 ||| investigating transformers in the decomposition of polygonal shapes as point collections. ||| andrea alfieri ||| yancong lin ||| jan c. van gemert ||| 
2021 ||| transmix: attend to mix for vision transformers. ||| jieneng chen ||| shuyang sun ||| ju he ||| philip h. s. torr ||| alan l. yuille ||| song bai ||| 
2021 ||| stereo waterdrop removal with row-wise dilated attention. ||| zifan shi ||| na fan ||| dit-yan yeung ||| qifeng chen ||| 
2022 ||| tempera: spatial transformer feature pyramid network for cardiac mri segmentation. ||| christoforos galazis ||| huiyi wu ||| zhuoyu li ||| camille petri ||| anil a. bharath ||| marta varela ||| 
2020 ||| global voxel transformer networks for augmented microscopy. ||| zhengyang wang ||| yaochen xie ||| shuiwang ji ||| 
2022 ||| actformer: a gan transformer framework towards general action-conditioned 3d human motion generation. ||| ziyang song ||| dongliang wang ||| nan jiang ||| zhicheng fang ||| chenjing ding ||| weihao gan ||| wei wu ||| 
2020 ||| rra-u-net: a residual encoder to attention decoder by residual connections framework for spine segmentation under noisy labels. ||| ziyang wang ||| zhengdong zhang ||| irina voiculescu ||| 
2021 ||| syntax-bert: improving pre-trained transformers with syntax trees. ||| jiangang bai ||| yujing wang ||| yiren chen ||| yaming yang ||| jing bai ||| jing yu ||| yunhai tong ||| 
2021 ||| ds-transunet: dual swin transformer u-net for medical image segmentation. ||| ailiang lin ||| bingzhi chen ||| jiayu xu ||| zheng zhang ||| guangming lu ||| 
2020 ||| spatio-temporal graph transformer networks for pedestrian trajectory prediction. ||| cunjun yu ||| xiao ma ||| jiawei ren ||| haiyu zhao ||| shuai yi ||| 
2021 ||| motr: end-to-end multiple-object tracking with transformer. ||| fangao zeng ||| bin dong ||| tiancai wang ||| cheng chen ||| xiangyu zhang ||| yichen wei ||| 
2021 ||| representation learning for neural population activity with neural data transformers. ||| joel ye ||| chethan pandarinath ||| 
2021 ||| noise classification aided attention-based neural network for monaural speech enhancement. ||| lu ma ||| song yang ||| yaguang gong ||| zhongqin wu ||| 
2020 ||| weak-attention suppression for transformer based speech recognition. ||| yangyang shi ||| yongqiang wang ||| chunyang wu ||| christian fuegen ||| frank zhang ||| duc le ||| ching-feng yeh ||| michael l. seltzer ||| 
2021 ||| transformer based bengali chatbot using general knowledge dataset. ||| abu kaisar mohammad masum ||| sheikh abujar ||| sharmin akter ||| nushrat jahan ria ||| syed akhter hossain ||| 
2020 ||| inner attention modeling for flexible teaming of heterogeneous multi robots using multi-agent reinforcement learning. ||| chao huang ||| rui liu ||| 
2020 ||| multi-level head-wise match and aggregation in transformer for textual sequence matching. ||| shuohang wang ||| yunshi lan ||| yi tay ||| jing jiang ||| jingjing liu ||| 
2021 ||| glimpse-attend-and-explore: self-attention for active visual exploration. ||| soroush seifi ||| abhishek jha ||| tinne tuytelaars ||| 
2020 ||| mono vs multilingual transformer-based models: a comparison across several language tasks. ||| diego de vargas feij ||| viviane pereira moreira ||| 
2020 ||| specter: document-level representation learning using citation-informed transformers. ||| arman cohan ||| sergey feldman ||| iz beltagy ||| doug downey ||| daniel s. weld ||| 
2020 ||| attentionddi: siamese attention-based deep learning method for drug-drug interaction predictions. ||| kyriakos schwarz ||| ahmed allam ||| nicolas andres perez gonzalez ||| michael krauthammer ||| 
2019 ||| attention-gated graph convolution for extracting drugs and their interactions from drug labels. ||| tung tran ||| ramakanth kavuluru ||| halil kilicoglu ||| 
2019 ||| hybrid-attention based decoupled metric learning for zero-shot image retrieval. ||| binghui chen ||| weihong deng ||| 
2022 ||| task-adaptive feature transformer with semantic enrichment for few-shot segmentation. ||| jun seo ||| young-hyun park ||| sung whan yoon ||| jaekyun moon ||| 
2021 ||| power law graph transformer for machine translation and representation learning. ||| burc gokden ||| 
2022 ||| vitbis: vision transformer for biomedical image segmentation. ||| abhinav sagar ||| 
2021 ||| transformer for image quality assessment. ||| junyong you ||| jari korhonen ||| 
2018 ||| can: constrained attention networks for multi-aspect sentiment analysis. ||| mengting hu ||| shiwan zhao ||| li zhang ||| keke cai ||| zhong su ||| renhong cheng ||| xiaowei shen ||| 
2022 ||| edgeformer: a parameter-efficient transformer for on-device seq2seq generation. ||| tao ge ||| furu wei ||| 
2019 ||| attention guided graph convolutional networks for relation extraction. ||| zhijiang guo ||| yan zhang ||| wei lu ||| 
2019 ||| english-czech systems in wmt19: document-level transformer. ||| martin popel ||| dominik mach ||| cek ||| michal auersperger ||| ondrej bojar ||| pavel pecina ||| 
2019 ||| blockwise self-attention for long document understanding. ||| jiezhong qiu ||| hao ma ||| omer levy ||| scott wen-tau yih ||| sinong wang ||| jie tang ||| 
2021 ||| aasist: audio anti-spoofing using integrated spectro-temporal graph attention networks. ||| jee-weon jung ||| hee-soo heo ||| hemlata tak ||| hye-jin shim ||| joon son chung ||| bong-jin lee ||| ha-jin yu ||| nicholas w. d. evans ||| 
2020 ||| unsupervised evaluation for question answering with transformers. ||| lukas muttenthaler ||| isabelle augenstein ||| johannes bjerva ||| 
2021 ||| multimodal graph-based transformer framework for biomedical relation extraction. ||| sriram pingali ||| shweta yadav ||| pratik dutta ||| sriparna saha ||| 
2019 ||| semantic adversarial network with multi-scale pyramid attention for video classification. ||| de xie ||| cheng deng ||| hao wang ||| chao li ||| dapeng tao ||| 
2017 ||| clothing retrieval with visual attention model. ||| zhonghao wang ||| yujun gu ||| ya zhang ||| jun zhou ||| xiao gu ||| 
2018 ||| attention-aware compositional network for person re-identification. ||| jing xu ||| rui zhao ||| feng zhu ||| huaming wang ||| wanli ouyang ||| 
2021 ||| energon: towards efficient acceleration of transformers using dynamic sparse attention. ||| zhe zhou ||| junlin liu ||| zhenyu gu ||| guangyu sun ||| 
2019 ||| attention routing between capsules. ||| jaewoong choi ||| hyun seo ||| suii im ||| myungju kang ||| 
2020 ||| guider l'attention dans les modeles de sequence a sequence pour la prediction des actes de dialogue. ||| pierre colombo ||| emile chapuis ||| matteo manica ||| emmanuel vignon ||| giovanna varni ||| chlo |||  clavel ||| 
2019 ||| david: dual-attentional video deblurring. ||| junru wu ||| xiang yu ||| ding liu ||| manmohan chandraker ||| zhangyang wang ||| 
2021 ||| attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams. ||| yuanbo hou ||| zhesong yu ||| xia liang ||| xingjian du ||| bilei zhu ||| zejun ma ||| dick botteldooren ||| 
2021 ||| automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. ||| giammarco la barbera ||| pietro gori ||| haithem boussaid ||| bruno belucci ||| alessandro delmonte ||| jeanne goulin ||| sabine sarnacki ||| laurence rouet ||| isabelle bloch ||| 
2021 ||| interactively generating explanations for transformer language models. ||| patrick schramowski ||| felix friedrich ||| christopher tauchmann ||| kristian kersting ||| 
2021 ||| transformer based deliberation for two-pass speech recognition. ||| ke hu ||| ruoming pang ||| tara n. sainath ||| trevor strohman ||| 
2021 ||| learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. ||| ruihan yang ||| minghao zhang ||| nicklas hansen ||| huazhe xu ||| xiaolong wang ||| 
2020 ||| da-hgt: domain adaptive heterogeneous graph transformer. ||| tiancheng huang ||| ke xu ||| donglin wang ||| 
2022 ||| stock movement prediction based on bi-typed hybrid-relational market knowledge graph via dual attention networks. ||| yu zhao ||| huaming du ||| ying liu ||| shaopeng wei ||| xingyan chen ||| fuzhen zhuang ||| qing li ||| ji liu ||| gang kou ||| 
2021 ||| bio-inspired audio-visual cues integration for visual attention prediction. ||| yuan yuan ||| hailong ning ||| bin zhao ||| 
2021 ||| hierarchical transformers are more efficient language models. ||| piotr nawrot ||| szymon tworkowski ||| michal tyrolski ||| lukasz kaiser ||| yuhuai wu ||| christian szegedy ||| henryk michalewski ||| 
2020 ||| few-shot classification via adaptive attention. ||| zihang jiang ||| bingyi kang ||| kuangqi zhou ||| jiashi feng ||| 
2021 ||| tiny transformers for environmental sound classification at the edge. ||| david elliott ||| carlos e. otero ||| steven wyatt ||| evan martino ||| 
2022 ||| hyperprompt: prompt-based task-conditioning of transformers. ||| yun he ||| huaixiu steven zheng ||| yi tay ||| jai prakash gupta ||| yu du ||| vamsi aribandi ||| zhe zhao ||| yaguang li ||| zhao chen ||| donald metzler ||| heng-tze cheng ||| ed h. chi ||| 
2020 ||| deepremaster: temporal source-reference attention networks for comprehensive video enhancement. ||| satoshi iizuka ||| edgar simo-serra ||| 
2020 ||| efficient attention network: accelerate attention by searching where to plug. ||| zhongzhan huang ||| senwei liang ||| mingfu liang ||| wei he ||| haizhao yang ||| 
2021 ||| heuristic search planning with deep neural networks using imitation, attention and curriculum learning. ||| leah chrestien ||| tom ||| s pevn ||| anton ||| n komenda ||| stefan edelkamp ||| 
2021 ||| fingat: financial graph attention networks for recommending top-k profitable stocks. ||| yi-ling hsu ||| yu-che tsai ||| cheng-te li ||| 
2022 ||| disentangling patterns and transformations from one sequence of images with shape-invariant lie group transformer. ||| takumi takada ||| wataru shimaya ||| yoshiyuki ohmura ||| yasuo kuniyoshi ||| 
2021 ||| fast point transformer. ||| chunghyun park ||| yoonwoo jeong ||| minsu cho ||| jaesik park ||| 
2022 ||| paying u-attention to textures: multi-stage hourglass vision transformer for universal texture synthesis. ||| shouchang guo ||| valentin deschaintre ||| douglas c. noll ||| arthur roullier ||| 
2018 ||| a comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese. ||| shiyu zhou ||| linhao dong ||| shuang xu ||| bo xu ||| 
2021 ||| wlv-rit at semeval-2021 task 5: a neural transformer framework for detecting toxic spans. ||| tharindu ranasinghe ||| diptanu sarkar ||| marcos zampieri ||| alexander g. ororbia ||| 
2019 ||| situation-aware pedestrian trajectory prediction with spatio-temporal attention model. ||| sirin haddad ||| meiqing wu ||| he wei ||| siew kei lam ||| 
2021 ||| transicd: transformer based code-wise attention model for explainable icd coding. ||| biplob biswas ||| thai-hoang pham ||| ping zhang ||| 
2020 ||| voice and accompaniment separation in music using self-attention convolutional neural network. ||| yuzhou liu ||| balaji thoshkahna ||| ali milani ||| trausti kristjansson ||| 
2021 ||| swin transformer: hierarchical vision transformer using shifted windows. ||| ze liu ||| yutong lin ||| yue cao ||| han hu ||| yixuan wei ||| zheng zhang ||| stephen lin ||| baining guo ||| 
2019 ||| efficient adaptation of pretrained transformers for abstractive summarization. ||| andrew hoang ||| antoine bosselut ||| asli celikyilmaz ||| yejin choi ||| 
2021 ||| adding quaternion representations to attention networks for classification. ||| nazmul shahadat ||| anthony s. maida ||| 
2020 ||| hgat: hierarchical graph attention network for fake news detection. ||| yuxiang ren ||| jiawei zhang ||| 
2018 ||| iterative recursive attention model for interpretable sequence classification. ||| martin tutek ||| jan snajder ||| 
2021 ||| chinese sentences similarity via cross-attention based siamese network. ||| zhen wang ||| xiangxie zhang ||| yicong tan ||| 
2021 ||| attention can reflect syntactic structure (if you let it). ||| vinit ravishankar ||| artur kulmizev ||| mostafa abdou ||| anders s ||| gaard ||| joakim nivre ||| 
2021 ||| caspianet++: a multidimensional channel-spatial asymmetric attention network with noisy student curriculum learning paradigm for brain tumor segmentation. ||| andrea liew ||| chun cheng lee ||| boon leong lan ||| maxine tan ||| 
2021 ||| affinity attention graph neural network for weakly supervised semantic segmentation. ||| bingfeng zhang ||| jimin xiao ||| jianbo jiao ||| yunchao wei ||| yao zhao ||| 
2019 ||| smart transformer modelling in optimal power flow analysis. ||| junru chen ||| ran li ||| alireza soroudi ||| andrew keane ||| damian flynn ||| terence o'donnell ||| 
2020 ||| retrofitting structure-aware transformer language model for end tasks. ||| hao fei ||| yafeng ren ||| donghong ji ||| 
2018 ||| self-attention linguistic-acoustic decoder. ||| santiago pascual ||| antonio bonafonte ||| joan serr ||| 
2021 ||| a channel attention based mlp-mixer network for motor imagery decoding with eeg. ||| yanbin he ||| zhiyang lu ||| jun wang ||| jun shi ||| 
2021 ||| finetuning pretrained transformers into variational autoencoders. ||| seongmin park ||| jihwa lee ||| 
2020 ||| ampa-net: optimization-inspired attention neural network for deep compressed sensing. ||| nanyu li ||| charles c. zhou ||| 
2019 ||| improving semantic segmentation of aerial images using patch-based attention. ||| lei ding ||| hao tang ||| lorenzo bruzzone ||| 
2019 ||| a cnn-rnn framework with a novel patch-based multi-attention mechanism for multi-label image classification in remote sensing. ||| gencer sumbul ||| beg ||| m demir ||| 
2018 ||| learning to remember, forget and ignore using attention control in memory. ||| t. s. jayram ||| younes bouhadjar ||| ryan l. mcavoy ||| tomasz kornuta ||| alexis asseman ||| kamil rocki ||| ahmet s. ozcan ||| 
2021 ||| sru++: pioneering fast recurrence with attention for speech recognition. ||| jing pan ||| tao lei ||| kwangyoun kim ||| kyu j. han ||| shinji watanabe ||| 
2021 ||| socialbert - transformers for online socialnetwork language modelling. ||| ilia karpov ||| nick kartashev ||| 
2018 ||| agile amulet: real-time salient object detection with contextual attention. ||| pingping zhang ||| luyao wang ||| dong wang ||| huchuan lu ||| chunhua shen ||| 
2022 ||| acvnet: attention concatenation volume for accurate and efficient stereo matching. ||| gangwei xu ||| junda cheng ||| peng guo ||| xin yang ||| 
2020 ||| deep interleaved network for image super-resolution with asymmetric co-attention. ||| feng li ||| runming cong ||| huihui bai ||| yifan he ||| 
2019 ||| robot navigation in crowds by graph convolutional networks with attention learned from human gaze. ||| yuying chen ||| congcong liu ||| ming liu ||| bertram e. shi ||| 
2021 ||| improving patent mining and relevance classification using transformers. ||| th ||| o ding ||| walter vermeiren ||| sylvie ranwez ||| binbin xu ||| 
2020 ||| mhsan: multi-head self-attention network for visual semantic embedding. ||| geondo park ||| chihye han ||| wonjun yoon ||| daeshik kim ||| 
2020 ||| alleviating the inequality of attention heads for neural machine translation. ||| zewei sun ||| shujian huang ||| xinyu dai ||| jiajun chen ||| 
2017 ||| multiple range-restricted bidirectional gated recurrent units with attention for relation classification. ||| jonggu kim ||| jong-hyeok lee ||| 
2021 ||| compound word transformer: learning to compose full-song music over dynamic directed hypergraphs. ||| wen-yi hsiao ||| jen-yu liu ||| yin-cheng yeh ||| yi-hsuan yang ||| 
2021 ||| asformer: transformer for action segmentation. ||| fangqiu yi ||| hongyu wen ||| tingting jiang ||| 
2018 ||| 3d feature pyramid attention module for robust visual speech recognition. ||| jingyun xiao ||| shuang yang ||| yuanhang zhang ||| shiguang shan ||| xilin chen ||| 
2019 ||| self-attention transducers for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| ye bai ||| zhengqi wen ||| 
2019 ||| a seq-to-seq transformer premised temporal convolutional network for chinese word segmentation. ||| wei jiang ||| yan tang ||| 
2020 ||| attngrounder: talking to cars with attention. ||| vivek mittal ||| 
2020 ||| multi-head monotonic chunkwise attention for online speech recognition. ||| baiji liu ||| songjun cao ||| sining sun ||| weibin zhang ||| long ma ||| 
2021 ||| xrjl-hkust at semeval-2021 task 4: wordnet-enhanced dual multi-head co-attention for reading comprehension of abstract meaning. ||| yuxin jiang ||| ziyi shou ||| qijun wang ||| hao wu ||| fangzhen lin ||| 
2021 ||| caranet: context axial reverse attention network for segmentation of small medical objects. ||| ange lou ||| shuyue guan ||| murray h. loew ||| 
2019 ||| decoupled attention network for text recognition. ||| tianwei wang ||| yuanzhi zhu ||| lianwen jin ||| canjie luo ||| xiaoxue chen ||| yaqiang wu ||| qianying wang ||| mingxiang cai ||| 
2021 |||  adaptive attention convolutional neural network. ||| abenezer girma ||| abdollah homaifar ||| m. nabil mahmoud ||| xuyang yan ||| mrinmoy sarkar ||| 
2021 ||| trajectory-constrained deep latent visual attention for improved local planning in presence of heterogeneous terrain. ||| stefan wapnick ||| travis manderson ||| david meger ||| gregory dudek ||| 
2020 ||| s2a: wasserstein gan with spatio-spectral laplacian attention for multi-spectral band synthesis. ||| litu rout ||| indranil misra ||| s. manthira moorthi ||| debajyoti dhar ||| 
2021 ||| tsdae: using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning. ||| kexin wang ||| nils reimers ||| iryna gurevych ||| 
2018 ||| attention on attention: architectures for visual question answering (vqa). ||| jasdeep singh ||| vincent ying ||| alex nutkiewicz ||| 
2021 ||| improving streaming transformer based asr under a framework of self-supervised learning. ||| songjun cao ||| yueteng kang ||| yanzhe fu ||| xiaoshuo xu ||| sining sun ||| yike zhang ||| long ma ||| 
2021 ||| embodied bert: a transformer model for embodied, language-guided visual task completion. ||| alessandro suglia ||| qiaozi gao ||| jesse thomason ||| govind thattai ||| gaurav sukhatme ||| 
2021 ||| clip2tv: an empirical study on transformer-based methods for video-text retrieval. ||| zijian gao ||| jingyu liu ||| sheng chen ||| dedan chang ||| hao zhang ||| jinwei yuan ||| 
2021 ||| video frame interpolation transformer. ||| zhihao shi ||| xiangyu xu ||| xiaohong liu ||| jun chen ||| ming-hsuan yang ||| 
2019 ||| rthn: a rnn-transformer hierarchical network for emotion cause extraction. ||| rui xia ||| mengran zhang ||| zixiang ding ||| 
2020 ||| object-based attention for spatio-temporal reasoning: outperforming neuro-symbolic models with flexible distributed architectures. ||| david ding ||| felix hill ||| adam santoro ||| matt m. botvinick ||| 
2021 ||| sum-product-attention networks: leveraging self-attention in probabilistic circuits. ||| zhongjie yu ||| devendra singh dhami ||| kristian kersting ||| 
2021 ||| exploring and improving mobile level vision transformers. ||| pengguang chen ||| yixin chen ||| shu liu ||| mingchang yang ||| jiaya jia ||| 
2019 ||| levenshtein transformer. ||| jiatao gu ||| changhan wang ||| jake zhao ||| 
2021 ||| structext: structured text understanding with multi-modal transformers. ||| yulin li ||| yuxi qian ||| yuchen yu ||| xiameng qin ||| chengquan zhang ||| yan liu ||| kun yao ||| junyu han ||| jingtuo liu ||| errui ding ||| 
2021 ||| attend and select: a segment attention based selection mechanism for microblog hashtag generation. ||| qianren mao ||| xi li ||| hao peng ||| bang liu ||| shu guo ||| jianxin li ||| lihong wang ||| philip s. yu ||| 
2022 ||| vision transformer with deformable attention. ||| zhuofan xia ||| xuran pan ||| shiji song ||| li erran li ||| gao huang ||| 
2019 ||| two-headed monster and crossed co-attention networks. ||| yaoyiran li ||| jing jiang ||| 
2021 ||| unidirectional memory-self-attention transducer for online speech recognition. ||| jian luo ||| jianzong wang ||| ning cheng ||| jing xiao ||| 
2020 ||| traffic agent trajectory prediction using social convolution and attention mechanism. ||| tao yang ||| zhixiong nan ||| he zhang ||| shitao chen ||| nanning zheng ||| 
2020 ||| on the sub-layer functionalities of transformer decoder. ||| yilin yang ||| longyue wang ||| shuming shi ||| prasad tadepalli ||| stefan lee ||| zhaopeng tu ||| 
2022 ||| temporal attention for language models. ||| guy d. rosin ||| kira radinsky ||| 
2021 ||| robust facial expression recognition with convolutional visual transformers. ||| fuyan ma ||| bin sun ||| shutao li ||| 
2019 ||| multi-modal attention network learning for semantic source code retrieval. ||| yao wan ||| jingdong shu ||| yulei sui ||| guandong xu ||| zhou zhao ||| jian wu ||| philip s. yu ||| 
2021 ||| separable temporal convolution plus temporally pooled attention for lightweight high-performance keyword spotting. ||| shenghua hu ||| jing wang ||| yujun wang ||| wenjing yang ||| 
2022 ||| decepticons: corrupted transformers breach privacy in federated learning for language models. ||| liam fowl ||| jonas geiping ||| steven reich ||| yuxin wen ||| wojtek czaja ||| micah goldblum ||| tom goldstein ||| 
2022 ||| transition relation aware self-attention for session-based recommendation. ||| guanghui zhu ||| haojun hou ||| jingfan chen ||| chunfeng yuan ||| yihua huang ||| 
2020 ||| experiments with lvt and fre for transformer model. ||| ilshat gibadullin ||| aidar valeev ||| 
2019 ||| wikipedia and digital currencies: interplay between collective attention and market performance. ||| abeer elbahrawy ||| laura alessandretti ||| andrea baronchelli ||| 
2021 ||| mindless attractor: a false-positive resistant intervention for drawing attention using auditory perturbation. ||| riku arakawa ||| hiromu yakura ||| 
2021 ||| attention gate in traffic forecasting. ||| anh lam ||| anh nguyen ||| bac le ||| 
2022 ||| laneformer: object-aware row-column transformers for lane detection. ||| jianhua han ||| xiajun deng ||| xinyue cai ||| zhen yang ||| hang xu ||| chunjing xu ||| xiaodan liang ||| 
2021 ||| leveraging transformers for hate speech detection in conversational code-mixed tweets. ||| zaki mustafa farooqi ||| sreyan ghosh ||| rajiv ratn shah ||| 
2019 ||| transcoding compositionally: using attention to find more generalizable solutions. ||| kris korrel ||| dieuwke hupkes ||| verna dankers ||| elia bruni ||| 
2020 ||| turngpt: a transformer-based language model for predicting turn-taking in spoken dialog. ||| erik ekstedt ||| gabriel skantze ||| 
2022 ||| joint rotational invariance and adversarial training of a dual-stream transformer yields state of the art brain-score for area v4. ||| william berrios ||| arturo deza ||| 
2017 ||| reading scene text with attention convolutional sequence modeling. ||| yunze gao ||| yingying chen ||| jinqiao wang ||| hanqing lu ||| 
2021 ||| transformer-based deep imitation learning for dual-arm robot manipulation. ||| heecheol kim ||| yoshiyuki ohmura ||| yasuo kuniyoshi ||| 
2021 ||| legoformer: transformers for block-by-block multi-view 3d reconstruction. ||| farid yagubbayli ||| alessio tonioni ||| federico tombari ||| 
2022 ||| facial expression recognition with swin transformer. ||| jun-hwa kim ||| namho kim ||| chee sun won ||| 
2021 ||| attention-based multi-hypothesis fusion for speech summarization. ||| takatomo kano ||| atsunori ogawa ||| marc delcroix ||| shinji watanabe ||| 
2021 ||| cross-view geo-localization with evolving transformer. ||| hongji yang ||| xiufan lu ||| yingying zhu ||| 
2022 ||| continuous-time audiovisual fusion with recurrence vs. attention for in-the-wild affect recognition. ||| vincent karas ||| mani kumar tellamekala ||| adria mallol-ragolta ||| michel f. valstar ||| bj ||| rn w. schuller ||| 
2018 ||| understanding visual ads by aligning symbols and objects using co-attention. ||| karuna ahuja ||| karan sikka ||| anirban roy ||| ajay divakaran ||| 
2021 ||| audio-visual scene-aware dialog and reasoning using audio-visual transformers with joint student-teacher learning. ||| ankit p. shah ||| shijie geng ||| peng gao ||| anoop cherian ||| takaaki hori ||| tim k. marks ||| jonathan le roux ||| chiori hori ||| 
2020 ||| sag-gan: semi-supervised attention-guided gans for data augmentation on medical images. ||| chang qi ||| junyang chen ||| guizhi xu ||| zhenghua xu ||| thomas lukasiewicz ||| yang liu ||| 
2020 ||| preserving dynamic attention for long-term spatial-temporal prediction. ||| haoxing lin ||| rufan bai ||| weijia jia ||| xinyu yang ||| yongjian you ||| 
2020 ||| a label attention model for icd coding from clinical text. ||| thanh vu ||| dat quoc nguyen ||| anthony nguyen ||| 
2019 ||| position focused attention network for image-text matching. ||| yaxiong wang ||| hao yang ||| xueming qian ||| lin ma ||| jing lu ||| biao li ||| xin fan ||| 
2020 ||| robust brain magnetic resonance image segmentation for hydrocephalus patients: hard and soft attention. ||| xuhua ren ||| jiayu huo ||| kai xuan ||| dongming wei ||| lichi zhang ||| qian wang ||| 
2020 ||| spatio-temporal attention model for tactile texture recognition. ||| guanqun cao ||| yi zhou ||| danushka bollegala ||| shan luo ||| 
2020 ||| net2: a graph attention network method customized for pre-placement net length estimation. ||| zhiyao xie ||| rongjian liang ||| xiaoqing xu ||| jiang hu ||| yixiao duan ||| yiran chen ||| 
2021 ||| hierarchical transformer networks for longitudinal clinical document classification. ||| yuqi si ||| kirk roberts ||| 
2021 ||| attention map-guided two-stage anomaly detection using hard augmentation. ||| jou won song ||| kyeongbo kong ||| ye in park ||| suk-ju kang ||| 
2021 ||| persformer: a transformer architecture for topological machine learning. ||| raphael reinauer ||| matteo caorsi ||| nicolas berkouk ||| 
2022 ||| a multi-scale transformer for medical image segmentation: architectures, model efficiency, and benchmarks. ||| yunhe gao ||| mu zhou ||| di liu ||| dimitris n. metaxas ||| 
2020 ||| aragpt2: pre-trained transformer for arabic language generation. ||| wissam antoun ||| fady baly ||| hazem m. hajj ||| 
2020 ||| seq2seq ai chatbot with attention mechanism. ||| abonia sojasingarayar ||| 
2021 ||| clvsa: a convolutional lstm based variational sequence-to-sequence model with attention for predicting trends of financial markets. ||| jia wang ||| tong sun ||| benyuan liu ||| yu cao ||| hongwei zhu ||| 
2021 ||| diverse image inpainting with bidirectional and autoregressive transformers. ||| yingchen yu ||| fangneng zhan ||| rongliang wu ||| jianxiong pan ||| kaiwen cui ||| shijian lu ||| feiying ma ||| xuansong xie ||| chunyan miao ||| 
2017 ||| efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. ||| hideyuki tachibana ||| katsuya uenoyama ||| shunsuke aihara ||| 
2021 ||| ormer with enhanced self-attention. ||| chenglin yang ||| yilin wang ||| jianming zhang ||| he zhang ||| zijun wei ||| zhe lin ||| alan l. yuille ||| 
2020 ||| attention mechanism for multivariate time series recurrent model interpretability applied to the ironmaking industry. ||| cedric schockaert ||| reinhard leperlier ||| assaad moawad ||| 
2021 ||| curriculum pre-training heterogeneous subgraph transformer for top-n recommendation. ||| hui wang ||| kun zhou ||| wayne xin zhao ||| jingyuan wang ||| ji-rong wen ||| 
2021 ||| dual graph convolutional networks with transformer and curriculum learning for image captioning. ||| xinzhi dong ||| chengjiang long ||| wenju xu ||| chunxia xiao ||| 
2022 ||| latentformer: multi-agent transformer-based interaction modeling and trajectory prediction. ||| elmira amirloo abolfathi ||| amir rasouli ||| peter lakner ||| mohsen rohani ||| jun luo ||| 
2022 ||| visualizing and understanding patch interactions in vision transformer. ||| jie ma ||| yalong bai ||| bineng zhong ||| wei zhang ||| ting yao ||| tao mei ||| 
2021 ||| improving the faithfulness of attention-based explanations with task-specific information for text classification. ||| george chrysostomou ||| nikolaos aletras ||| 
2022 ||| dpst: de novo peptide sequencing with amino-acid-aware transformers. ||| yan yang ||| md. zakir hossain ||| khandaker asif ||| liyuan pan ||| shafin rahman ||| eric a. stone ||| 
2022 ||| dan: a segmentation-free document attention network for handwritten document recognition. ||| denis coquenet ||| cl ||| ment chatelain ||| thierry paquet ||| 
2018 ||| end-to-end dense video captioning with masked transformer. ||| luowei zhou ||| yingbo zhou ||| jason j. corso ||| richard socher ||| caiming xiong ||| 
2021 ||| a comparative study of transformer-based language models on extractive question answering. ||| kate pearce ||| tiffany zhan ||| aneesh komanduri ||| justin zhan ||| 
2021 ||| contrastive document representation learning with graph attention networks. ||| peng xu ||| xinchi chen ||| xiaofei ma ||| zhiheng huang ||| bing xiang ||| 
2021 ||| action unit detection with joint adaptive attention and graph relation. ||| chenggong zhang ||| juan song ||| qingyang zhang ||| weilong dong ||| ruomeng ding ||| zhilei liu ||| 
2022 ||| sunet: swin transformer unet for image denoising. ||| chi-mao fan ||| tsung-jung liu ||| kuan-hsien liu ||| 
2022 ||| task specific attention is one more thing you need for object detection. ||| sang-yon lee ||| 
2018 ||| image transformer. ||| niki parmar ||| ashish vaswani ||| jakob uszkoreit ||| lukasz kaiser ||| noam shazeer ||| alexander ku ||| 
2018 ||| fine-grained attention mechanism for neural machine translation. ||| heeyoul choi ||| kyunghyun cho ||| yoshua bengio ||| 
2019 ||| quantifying the impact of user attention on fair group representation in ranked lists. ||| piotr sapiezynski ||| wesley zeng ||| ronald e. robertson ||| alan mislove ||| christo wilson ||| 
2022 ||| improving sample efficiency of value based models using attention and vision transformers. ||| amir ardalan kalantari ||| mohammad amini ||| sarath chandar ||| doina precup ||| 
2019 ||| dynamic evaluation of transformer language models. ||| ben krause ||| emmanuel kahembwe ||| iain murray ||| steve renals ||| 
2021 ||| gumbel-attention for multi-modal machine translation. ||| pengbo liu ||| hailong cao ||| tiejun zhao ||| 
2021 ||| improving automated visual fault detection by combining a biologically plausible model of visual attention with deep learning. ||| frederik beuth ||| tobias schlosser ||| michael friedrich ||| danny kowerko ||| 
2018 ||| dual attention matching network for context-aware feature sequence based person re-identification. ||| jianlou si ||| honggang zhang ||| chun-guang li ||| jason kuen ||| xiangfei kong ||| alex c. kot ||| gang wang ||| 
2021 ||| transformer-based end-to-end speech recognition with residual gaussian-based self-attention. ||| chengdong liang ||| menglong xu ||| xiao-lei zhang ||| 
2018 ||| attentional aggregation of deep feature sets for multi-view 3d reconstruction. ||| bo yang ||| sen wang ||| andrew markham ||| niki trigoni ||| 
2021 ||| deep learning based ofdm channel estimation using frequency-time division and attention mechanism. ||| ang yang ||| peng sun ||| tamrakar rakesh ||| bule sun ||| fei qin ||| 
2019 ||| recurrent attention walk for semi-supervised classification. ||| uchenna akujuobi ||| qiannan zhang ||| han yufei ||| xiangliang zhang ||| 
2021 ||| cotr: efficiently bridging cnn and transformer for 3d medical image segmentation. ||| yutong xie ||| jianpeng zhang ||| chunhua shen ||| yong xia ||| 
2022 ||| metric hypertransformers are universal adapted maps. ||| beatrice acciaio ||| anastasis kratsios ||| gudmund pammer ||| 
2020 ||| streaming transformer-based acoustic models using self-attention with augmented memory. ||| chunyang wu ||| yongqiang wang ||| yangyang shi ||| ching-feng yeh ||| frank zhang ||| 
2022 ||| partially fake audio detection by self-attention-based fake span discovery. ||| haibin wu ||| heng-cheng kuo ||| naijun zheng ||| kuo-hsuan hung ||| hung-yi lee ||| yu tsao ||| hsin-min wang ||| helen meng ||| 
2021 ||| blt: bidirectional layout transformer for controllable layout generation. ||| xiang kong ||| lu jiang ||| huiwen chang ||| han zhang ||| yuan hao ||| haifeng gong ||| irfan essa ||| 
2017 ||| data fusion and machine learning integration for transformer loss of life estimation. ||| mohsen mahoor ||| amin khodaei ||| 
2022 ||| rformer: transformer-based generative adversarial network for real fundus image restoration on a new clinical benchmark. ||| zhuo deng ||| yuanhao cai ||| lu chen ||| zheng gong ||| qiqi bao ||| xue yao ||| dong fang ||| shaochong zhang ||| lan ma ||| 
2020 ||| meta-embeddings based on self-attention. ||| qichen li ||| xiaoke jiang ||| jun xia ||| jian li ||| 
2019 ||| input-cell attention reduces vanishing saliency of recurrent neural networks. ||| aya abdelsalam ismail ||| mohamed k. gunady ||| luiz pessoa ||| h ||| ctor corrada bravo ||| soheil feizi ||| 
2021 ||| medical sansformers: training self-supervised transformers without attention for electronic medical records. ||| yogesh kumar ||| alexander ilin ||| henri salo ||| sangita kulathinal ||| maarit k. leinonen ||| pekka marttinen ||| 
2019 ||| multimodal transformer networks for end-to-end video-grounded dialogue systems. ||| hung le ||| doyen sahoo ||| nancy f. chen ||| steven c. h. hoi ||| 
2019 ||| explicit sparse transformer: concentrated attention through explicit selection. ||| guangxiang zhao ||| junyang lin ||| zhiyuan zhang ||| xuancheng ren ||| qi su ||| xu sun ||| 
2021 ||| hypertenet: hypergraph and transformer-based neural network for personalized list continuation. ||| vijaikumar m ||| deepesh v. hada ||| shirish k. shevade ||| 
2020 ||| learning to execute programs with instruction pointer attention graph neural networks. ||| david bieber ||| charles sutton ||| hugo larochelle ||| daniel tarlow ||| 
2021 ||| attention-based keyword localisation in speech using visual grounding. ||| kayode olaleye ||| herman kamper ||| 
2021 ||| transformer is all you need: multimodal multitask learning with a unified transformer. ||| ronghang hu ||| amanpreet singh ||| 
2021 ||| tea: program repair using neural network based on program information attention matrix. ||| wenshuo wang ||| chen wu ||| liang cheng ||| yang zhang ||| 
2020 ||| temporal embeddings and transformer models for narrative text understanding. ||| vani kanjirangat ||| simone mellace ||| alessandro antonucci ||| 
2021 ||| training vision transformers for image retrieval. ||| alaaeldin el-nouby ||| natalia neverova ||| ivan laptev ||| herv |||  j ||| gou ||| 
2019 ||| path ranking with attention to type hierarchies. ||| weiyu liu ||| angel andres daruna ||| zsolt kira ||| sonia chernova ||| 
2020 ||| calibration of pre-trained transformers. ||| shrey desai ||| greg durrett ||| 
2019 ||| explicit pairwise word interaction modeling improves pretrained transformers for english semantic similarity tasks. ||| yinan zhang ||| raphael tang ||| jimmy lin ||| 
2021 ||| towards training stronger video vision transformers for epic-kitchens-100 action recognition. ||| ziyuan huang ||| zhiwu qing ||| xiang wang ||| yutong feng ||| shiwei zhang ||| jianwen jiang ||| zhurong xia ||| mingqian tang ||| nong sang ||| marcelo h. ang jr. ||| 
2021 ||| pgganet: pose guided graph attention network for person re-identification. ||| zhijun he ||| hongbo zhao ||| wenquan feng ||| 
2022 ||| uncovering more shallow heuristics: probing the natural language inference capacities of transformer-based pre-trained language models using syllogistic patterns. ||| reto gubelmann ||| siegfried handschuh ||| 
2022 ||| tsa-net: tube self-attention network for action quality assessment. ||| shunli wang ||| dingkang yang ||| peng zhai ||| chixiao chen ||| lihua zhang ||| 
2019 ||| hirenet: a hierarchical attention model for the automatic analysis of asynchronous video job interviews. ||| l ||| o hemamou ||| ghazi felhi ||| vincent vandenbussche ||| jean-claude martin ||| chlo |||  clavel ||| 
2019 ||| context-aware graph attention networks. ||| bo jiang ||| leiling wang ||| jin tang ||| bin luo ||| 
2021 ||| auditory attention decoding from eeg using convolutional recurrent neural network. ||| zhen fu ||| bo wang ||| xihong wu ||| jing chen ||| 
2021 ||| escaping the gradient vanishing: periodic alternatives of softmax in attention mechanism. ||| shulun wang ||| bin liu ||| feng liu ||| 
2021 ||| exploring transformers in natural language generation: gpt, bert, and xlnet. ||| m. onat topal ||| anil bas ||| imke van heerden ||| 
2022 ||| a new generation of perspective api: efficient multilingual character-level transformers. ||| alyssa lees ||| vinh q. tran ||| yi tay ||| jeffrey sorensen ||| jai prakash gupta ||| donald metzler ||| lucy vasserman ||| 
2020 ||| a compare aggregate transformer for understanding document-grounded dialogue. ||| longxuan ma ||| weinan zhang ||| runxin sun ||| ting liu ||| 
2021 ||| feedback pyramid attention networks for single image super-resolution. ||| huapeng wu ||| jie gui ||| jun zhang ||| james t. kwok ||| zhihui wei ||| 
2019 ||| one-shot object detection with co-attention and co-excitation. ||| ting-i hsieh ||| yi-chen lo ||| hwann-tzong chen ||| tyng-luh liu ||| 
2020 ||| infinite attention: nngp and ntk for deep attention networks. ||| jiri hron ||| yasaman bahri ||| jascha sohl-dickstein ||| roman novak ||| 
2021 ||| cross-modality fusion transformer for multispectral object detection. ||| qingyun fang ||| dapeng han ||| zhaokui wang ||| 
2021 ||| i2c2w: image-to-character-to-word transformers for accurate scene text recognition. ||| chuhui xue ||| shijian lu ||| song bai ||| wenqing zhang ||| changhu wang ||| 
2021 ||| cross-level cross-scale cross-attention network for point cloud representation. ||| xian-feng han ||| zhang-yue he ||| jia chen ||| guo-qiang xiao ||| 
2022 ||| transformer module networks for systematic generalization in visual question answering. ||| moyuru yamada ||| vanessa d'amario ||| kentaro takemoto ||| xavier boix ||| tomotake sasaki ||| 
2019 ||| assigning medical codes at the encounter level by paying attention to documents. ||| han-chin shing ||| guoli wang ||| philip resnik ||| 
2019 ||| selective attention for context-aware neural machine translation. ||| sameen maruf ||| andr |||  f. t. martins ||| gholamreza haffari ||| 
2021 ||| improving transformer-kernel ranking model using conformer and query term independence. ||| bhaskar mitra ||| sebastian hofst ||| tter ||| hamed zamani ||| nick craswell ||| 
2021 ||| discrete representations strengthen vision transformer robustness. ||| chengzhi mao ||| lu jiang ||| mostafa dehghani ||| carl vondrick ||| rahul sukthankar ||| irfan essa ||| 
2017 ||| top-down flow transformer networks. ||| zhiwei jia ||| haoshen hong ||| siyang wang ||| zhuowen tu ||| 
2019 ||| adaptively sparse transformers. ||| gon ||| alo m. correia ||| vlad niculae ||| andr |||  f. t. martins ||| 
2020 ||| learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. ||| sarthak mittal ||| alex lamb ||| anirudh goyal ||| vikram voleti ||| murray shanahan ||| guillaume lajoie ||| michael mozer ||| yoshua bengio ||| 
2021 ||| paying attention to astronomical transients: photometric classification with the time-series transformer. ||| tarek allam jr. ||| jason d. mcewen ||| 
2020 ||| not all parameters are born equal: attention is mostly what you need. ||| nikolay bogoychev ||| 
2018 ||| a robust deep attention network to noisy labels in semi-supervised biomedical segmentation. ||| shaobo min ||| xuejin chen ||| 
2021 ||| are transformers more robust than cnns? ||| yutong bai ||| jieru mei ||| alan l. yuille ||| cihang xie ||| 
2021 ||| cotr: convolution in transformer network for end to end polyp detection. ||| zhiqiang shen ||| chaonan lin ||| shaohua zheng ||| 
2022 ||| efficacy of transformer networks for classification of raw eeg data. ||| gourav siddhad ||| anmol gupta ||| debi prosad dogra ||| partha pratim roy ||| 
2021 ||| air-nets: an attention-based framework for locally conditioned implicit representations. ||| simon giebenhain ||| bastian goldl ||| cke ||| 
2019 ||| polarimetric thermal to visible face verification via self-attention guided synthesis. ||| xing di ||| benjamin s. riggan ||| shuowen hu ||| nathaniel j. short ||| vishal m. patel ||| 
2021 ||| augmented shortcuts for vision transformers. ||| yehui tang ||| kai han ||| chang xu ||| an xiao ||| yiping deng ||| chao xu ||| yunhe wang ||| 
2019 ||| attention-passing models for robust and data-efficient end-to-end speech translation. ||| matthias sperber ||| graham neubig ||| jan niehues ||| alex waibel ||| 
2021 ||| livestock monitoring with transformer. ||| bhavesh tangirala ||| ishan bhandari ||| d ||| niel l ||| szl ||| deepak k. gupta ||| rajat mani thomas ||| devanshu arya ||| 
2021 ||| lightseq: accelerated training for transformer-based models on gpus. ||| xiaohui wang ||| ying xiong ||| xian qian ||| yang wei ||| lei li ||| mingxuan wang ||| 
2021 ||| modality fusion network and personalized attention in momentary stress detection in the wild. ||| han yu ||| thomas vaessen ||| inez myin-germeys ||| akane sano ||| 
2020 ||| implicit kernel attention. ||| kyungwoo song ||| yohan jung ||| dongjun kim ||| il-chul moon ||| 
2020 ||| learning to generate diverse dance motions with transformer. ||| jiaman li ||| yihang yin ||| hang chu ||| yi zhou ||| tingwu wang ||| sanja fidler ||| hao li ||| 
2020 ||| minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. ||| wenhui wang ||| furu wei ||| li dong ||| hangbo bao ||| nan yang ||| ming zhou ||| 
2021 ||| it's all in the heads: using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. ||| alexey tikhonov ||| max ryabinin ||| 
2018 ||| improving review representations with user attention and product attention for sentiment classification. ||| zhen wu ||| xin-yu dai ||| cunyan yin ||| shujian huang ||| jiajun chen ||| 
2022 ||| path-aware graph attention for hd maps in motion prediction. ||| fang da ||| yu zhang ||| 
2021 ||| shuffle transformer with feature alignment for video face parsing. ||| rui zhang ||| yang han ||| zilong huang ||| pei cheng ||| guozhong luo ||| gang yu ||| bin fu ||| 
2018 ||| an attention-gated convolutional neural network for sentence classification. ||| yang liu ||| lixin ji ||| ruiyang huang ||| tuosiyu ming ||| chao gao ||| 
2021 ||| fast-slow transformer for visually grounding speech. ||| puyuan peng ||| david harwath ||| 
2020 ||| cross-view image synthesis with deformable convolution and attention mechanism. ||| hao ding ||| songsong wu ||| hao tang ||| fei wu ||| guangwei gao ||| xiao-yuan jing ||| 
2020 ||| ganbert: generative adversarial networks with bidirectional encoder representations from transformers for mri to pet synthesis. ||| hoo-chang shin ||| alvin ihsani ||| swetha mandava ||| sharath turuvekere sreenivas ||| christopher forster ||| jiook cha ||| alzheimer's disease neuroimaging initiative ||| 
2022 ||| monodtr: monocular 3d object detection with depth-aware transformer. ||| kuan-chih huang ||| tsung-han wu ||| hung-ting su ||| winston h. hsu ||| 
2018 ||| a deep learning model with hierarchical lstms and supervised attention for anti-phishing. ||| minh nguyen ||| toan nguyen ||| thien huu nguyen ||| 
2020 ||| automatic brain tumor segmentation with scale attention network. ||| yading yuan ||| 
2020 ||| transformer with depth-wise lstm. ||| hongfei xu ||| qiuhui liu ||| deyi xiong ||| josef van genabith ||| 
2022 ||| the quarks of attention. ||| pierre baldi ||| roman vershynin ||| 
2021 ||| attention aware wavelet-based detection of morphed face images. ||| poorya aghdaie ||| baaria chaudhary ||| sobhan soleymani ||| jeremy m. dawson ||| nasser m. nasrabadi ||| 
2019 ||| can-ner: convolutional attention network for chinese named entity recognition. ||| yuying zhu ||| guoxin wang ||| b ||| rje f. karlsson ||| 
2021 ||| attention flows are shapley value explanations. ||| kawin ethayarajh ||| dan jurafsky ||| 
2021 ||| cerberus transformer: joint semantic, affordance and attribute parsing. ||| xiaoxue chen ||| tianyu liu ||| hao zhao ||| guyue zhou ||| ya-qin zhang ||| 
2022 ||| simcrosstrans: a simple cross-modality transfer learning for object detection with convnets or vision transformers. ||| xiaoke shen ||| ioannis stamos ||| 
2021 ||| universal transformer hawkes process with adaptive recursive iteration. ||| lu-ning zhang ||| jianwei liu ||| zhi-yan song ||| xin zuo ||| 
2017 ||| amc: attention guided multi-modal correlation learning for image search. ||| kan chen ||| trung bui ||| fang chen ||| zhaowen wang ||| ram nevatia ||| 
2021 ||| is it time to replace cnns with transformers for medical images? ||| christos matsoukas ||| johan fredin haslum ||| magnus s ||| derberg ||| kevin smith ||| 
2019 ||| dual attention networks for visual reference resolution in visual dialog. ||| gi-cheon kang ||| jaeseo lim ||| byoung-tak zhang ||| 
2020 ||| assemblenet++: assembling modality representations via attention connections. ||| michael s. ryoo ||| a. j. piergiovanni ||| juhana kangaspunta ||| anelia angelova ||| 
2021 ||| physformer: facial video-based physiological measurement with temporal difference transformer. ||| zitong yu ||| yuming shen ||| jingang shi ||| hengshuang zhao ||| philip h. s. torr ||| guoying zhao ||| 
2019 ||| pointwise attention-based atrous convolutional neural networks. ||| mobina mahdavi ||| fahimeh fooladgar ||| shohreh kasaei ||| 
2018 ||| a sequential guiding network with attention for image captioning. ||| daouda sow ||| zengchang qin ||| mouhamed niasse ||| tao wan ||| 
2021 ||| answer questions with right image regions: a visual attention regularization approach. ||| yibing liu ||| yangyang guo ||| jianhua yin ||| xuemeng song ||| weifeng liu ||| liqiang nie ||| 
2021 ||| panoptic segmentation of satellite image time series with convolutional temporal attention networks. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| 
2020 ||| multi^2oie: multilingual open information extraction based on multi-head attention with bert. ||| youngbin ro ||| yukyung lee ||| pilsung kang ||| 
2019 ||| realistic image generation using region-phrase attention. ||| wanming huang ||| yida xu ||| ian oppermann ||| 
2021 ||| lessons on parameter sharing across layers in transformers. ||| sho takase ||| shun kiyono ||| 
2019 ||| dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. ||| qitian wu ||| hengrui zhang ||| xiaofeng gao ||| peng he ||| paul weng ||| han gao ||| guihai chen ||| 
2022 ||| label dependent attention model for disease risk prediction using multimodal electronic health records. ||| shuai niu ||| qing yin ||| yunya song ||| yike guo ||| xian yang ||| 
2020 ||| automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study. ||| luyao shi ||| deepta rajan ||| shafiq abedin ||| manikanta srikar yellapragada ||| david beymer ||| ehsan dehghan ||| 
2022 ||| on using transformers for speech-separation. ||| cem subakan ||| mirco ravanelli ||| samuele cornell ||| fran ||| ois grondin ||| mirko bronzi ||| 
2020 ||| low-rank bottleneck in multi-head attention models. ||| srinadh bhojanapalli ||| chulhee yun ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2020 ||| cross-lingual relation extraction with transformers. ||| jian ni ||| taesun moon ||| parul awasthy ||| radu florian ||| 
2022 ||| block-sparse adversarial attack to fool transformer-based text classifiers. ||| sahar sadrizadeh ||| ljiljana dolamic ||| pascal frossard ||| 
2021 ||| coatnet: marrying convolution and attention for all data sizes. ||| zihang dai ||| hanxiao liu ||| quoc v. le ||| mingxing tan ||| 
2020 ||| learning spatial attention for face super-resolution. ||| chaofeng chen ||| dihong gong ||| hao wang ||| zhifeng li ||| kwan-yee k. wong ||| 
2020 ||| sparta: efficient open-domain question answering via sparse transformer matching retrieval. ||| tiancheng zhao ||| xiaopeng lu ||| kyusong lee ||| 
2019 ||| dynamic attention networks for task oriented grounding. ||| soumik dasgupta ||| badri n. patro ||| vinay p. namboodiri ||| 
2019 ||| skeleton based activity recognition by fusing part-wise spatio-temporal and attention driven residues. ||| chhavi dhiman ||| dinesh kumar vishwakarma ||| paras aggarwal ||| 
2022 ||| deepchorus: a hybrid model of multi-scale convolution and self-attention for chorus detection. ||| qiqi he ||| xiaoheng sun ||| yi yu ||| wei li ||| 
2020 ||| hierarchical transformer network for utterance-level emotion recognition. ||| qingbiao li ||| chunhua wu ||| kangfeng zheng ||| zhe wang ||| 
2021 ||| co-segmentation inspired attention module for video-based computer vision tasks. ||| arulkumar subramaniam ||| jayesh vaidya ||| muhammed abdul majeed ameen ||| athira m. nambiar ||| anurag mittal ||| 
2021 ||| comparison of czech transformers on text classification tasks. ||| jan lehecka ||| jan svec ||| 
2021 ||| moving towards centers: re-ranking with attention and memory for re-identification. ||| yunhao zhou ||| yi wang ||| lap-pui chau ||| 
2021 ||| kat: a knowledge augmented transformer for vision-and-language. ||| liangke gui ||| borui wang ||| qiuyuan huang ||| alex hauptmann ||| yonatan bisk ||| jianfeng gao ||| 
2021 ||| skeletor: skeletal transformers for robust body-pose estimation. ||| tao jiang ||| necati cihan camg ||| z ||| richard bowden ||| 
2019 ||| progressive attention memory network for movie story question answering. ||| junyeong kim ||| minuk ma ||| kyungsu kim ||| sungjin kim ||| chang d. yoo ||| 
2021 ||| flight demand forecasting with transformers. ||| liya wang ||| amy mykityshyn ||| craig johnson ||| jillian cheng ||| 
2019 ||| self-attention capsule networks for image classification. ||| assaf hoogi ||| brian wilcox ||| yachee gupta ||| daniel l. rubin ||| 
2020 ||| anatomy prior based u-net for pathology segmentation with attention. ||| yuncheng zhou ||| ke zhang ||| xinzhe luo ||| sihan wang ||| xiahai zhuang ||| 
2020 ||| multi-task learning with multi-head attention for multi-choice reading comprehension. ||| hui wan ||| 
2021 ||| mformer - approximation of self-attention by spectral shifting. ||| madhusudan verma ||| 
2021 ||| muse: multi-faceted attention for signed network embedding. ||| dengcheng yan ||| youwen zhang ||| wei li ||| yiwen zhang ||| 
2021 ||| transformer ensembles for sexism detection. ||| lily davies ||| marta baldracchi ||| carlo alessandro borella ||| konstantinos perifanos ||| 
2021 ||| mt6: multilingual pretrained text-to-text transformer with translation pairs. ||| zewen chi ||| li dong ||| shuming ma ||| shaohan huang ||| xian-ling mao ||| heyan huang ||| furu wei ||| 
2021 ||| deep personalized glucose level forecasting using attention-based recurrent neural networks. ||| mohammadreza armandpour ||| brian kidd ||| yu du ||| jianhua z. huang ||| 
2021 ||| pipetransformer: automated elastic pipelining for distributed training of transformers. ||| chaoyang he ||| shen li ||| mahdi soltanolkotabi ||| salman avestimehr ||| 
2019 ||| event recognition with automatic album detection based on sequential processing, neural attention and image captioning. ||| andrey v. savchenko ||| 
2021 ||| agsfcos: based on attention mechanism and scale-equalizing pyramid network of object detection. ||| li wang ||| wei xiang ||| ruhui xue ||| kaida zou ||| laili zhu ||| 
2022 ||| aspect-based api review classification: how far can pre-trained transformer model go? ||| chengran yang ||| bowen xu ||| junaed younus khan ||| gias uddin ||| donggyun han ||| zhou yang ||| david lo ||| 
2019 ||| scene-based factored attention for image captioning. ||| chen shen ||| rongrong ji ||| fuhai chen ||| xiaoshuai sun ||| xiangming li ||| 
2021 ||| sequence length is a domain: length-based overfitting in transformer models. ||| dusan varis ||| ondrej bojar ||| 
2020 ||| understanding the difficulty of training transformers. ||| liyuan liu ||| xiaodong liu ||| jianfeng gao ||| weizhu chen ||| jiawei han ||| 
2019 ||| fashion editing with multi-scale attention normalization. ||| haoye dong ||| xiaodan liang ||| yixuan zhang ||| xujie zhang ||| zhenyu xie ||| bowen wu ||| ziqi zhang ||| xiaohui shen ||| jian yin ||| 
2020 ||| understanding attention: in minds and machines. ||| shriraj p. sawant ||| shruti singh ||| 
2020 ||| trans-blstm: transformer with bidirectional lstm for language understanding. ||| zhiheng huang ||| peng xu ||| davis liang ||| ajay mishra ||| bing xiang ||| 
2019 ||| hypergraph convolution and hypergraph attention. ||| song bai ||| feihu zhang ||| philip h. s. torr ||| 
2020 ||| split then refine: stacked attention-guided resunets for blind single image visible watermark removal. ||| xiaodong cun ||| chi-man pun ||| 
2022 ||| flowformer: linearizing transformers with conservation flows. ||| haixu wu ||| jialong wu ||| jiehui xu ||| jianmin wang ||| mingsheng long ||| 
2020 ||| infominer at wnut-2020 task 2: transformer-based covid-19 informative tweet extraction. ||| hansi hettiarachchi ||| tharindu ranasinghe ||| 
2022 ||| cross-channel attention-based target speaker voice activity detection: experimental results for m2met challenge. ||| weiqing wang ||| xiaoyi qin ||| ming li ||| 
2021 ||| nlp-cuet@dravidianlangtech-eacl2021: offensive language detection from multilingual code-mixed text using transformers. ||| omar sharif ||| eftekhar hossain ||| mohammed moshiul hoque ||| 
2021 ||| 3d-man: 3d multi-frame attention network for object detection. ||| zetong yang ||| yin zhou ||| zhifeng chen ||| jiquan ngiam ||| 
2018 ||| an attention model for group-level emotion recognition. ||| aarush gupta ||| dakshit agrawal ||| hardik chauhan ||| jose dolz ||| marco pedersoli ||| 
2021 ||| the sensory neuron as a transformer: permutation-invariant neural networks for reinforcement learning. ||| yujin tang ||| david ha ||| 
2020 ||| earl: speedup transformer-based rankers with pre-computed representation. ||| luyu gao ||| zhuyun dai ||| jamie callan ||| 
2021 ||| unlocking pixels for reinforcement learning via implicit attention. ||| krzysztof choromanski ||| deepali jain ||| jack parker-holder ||| xingyou song ||| valerii likhosherstov ||| anirban santara ||| aldo pacchiano ||| yunhao tang ||| adrian weller ||| 
2021 ||| relaxed attention: a simple method to boost performance of end-to-end automatic speech recognition. ||| timo lohrenz ||| patrick schwarz ||| zhengyang li ||| tim fingscheidt ||| 
2017 ||| sequential attention. ||| sebastian brarda ||| philip yeres ||| samuel r. bowman ||| 
2021 ||| mfevit: a robust lightweight transformer-based network for multimodal 2d+3d facial expression recognition. ||| hanting li ||| mingzhe sui ||| zhaoqing zhu ||| feng zhao ||| 
2020 ||| efficient scene text detection with textual attention tower. ||| liang zhang ||| yufei liu ||| hang xiao ||| lu yang ||| guangming zhu ||| syed afaq ali shah ||| mohammed bennamoun ||| peiyi shen ||| 
2021 ||| mutformer: a context-dependent transformer-based model to predict pathogenic missense mutations. ||| theodore jiang ||| li fang ||| kai wang ||| 
2021 ||| gsa-forecaster: forecasting graph-based time-dependent data with graph sequence attention. ||| yang li ||| di wang ||| jos |||  m. f. moura ||| 
2021 ||| learning pruned structure and weights simultaneously from scratch: an attention based approach. ||| qisheng he ||| ming dong ||| loren schwiebert ||| weisong shi ||| 
2022 ||| integrating dependency tree into self-attention for sentence representation. ||| junhua ma ||| jiajun li ||| yuxuan liu ||| shangbo zhou ||| xue li ||| 
2022 ||| sparse cross-scale attention network for efficient lidar panoptic segmentation. ||| shuangjie xu ||| rui wan ||| maosheng ye ||| xiaoyi zou ||| tongyi cao ||| 
2021 ||| vision transformer based covid-19 detection using chest x-rays. ||| koushik sivarama krishnan ||| karthik sivarama krishnan ||| 
2021 ||| a multi-level attention model for evidence-based fact checking. ||| canasai kruengkrai ||| junichi yamagishi ||| xin wang ||| 
2021 ||| h-transformer-1d: fast one-dimensional hierarchical attention for sequences. ||| zhenhai zhu ||| radu soricut ||| 
2020 ||| attention neural network for trash detection on water channels. ||| mohbat tharani ||| abdul wahab amin ||| mohammad maaz ||| murtaza taj ||| 
2021 ||| sp-sedt: self-supervised pre-training for sound event detection transformer. ||| zhirong ye ||| xiangdong wang ||| hong liu ||| yueliang qian ||| rui tao ||| long yan ||| kazushige ouchi ||| 
2021 ||| structure-aware fine-tuning of sequence-to-sequence transformers for transition-based amr parsing. ||| jiawei zhou ||| tahira naseem ||| ram ||| n fernandez astudillo ||| young-suk lee ||| radu florian ||| salim roukos ||| 
2017 ||| minimum word error rate training for attention-based sequence-to-sequence models. ||| rohit prabhavalkar ||| tara n. sainath ||| yonghui wu ||| patrick nguyen ||| zhifeng chen ||| chung-cheng chiu ||| anjuli kannan ||| 
2020 |||  2020: identifying check-worthy tweets with transformer models. ||| alex nikolov ||| giovanni da san martino ||| ivan koychev ||| preslav nakov ||| 
2022 ||| block-recurrent transformers. ||| delesley hutchins ||| imanol schlag ||| yuhuai wu ||| ethan dyer ||| behnam neyshabur ||| 
2020 ||| attention-guided quality assessment for automated cryo-em grid screening. ||| hong xu ||| david e. timm ||| shireen y. elhabian ||| 
2022 ||| feat: face editing with attention. ||| xianxu hou ||| linlin shen ||| or patashnik ||| daniel cohen-or ||| hui huang ||| 
2018 ||| simple attention-based representation learning for ranking short social media posts. ||| peng shi ||| jinfeng rao ||| jimmy lin ||| 
2021 ||| using keypoint matching and interactive self attention network to verify retail posms. ||| harshita seth ||| sonaal kant ||| muktabh mayank srivastava ||| 
2018 ||| recurrently exploring class-wise attention in a hybrid convolutional and bidirectional lstm network for multi-label aerial image classification. ||| yuansheng hua ||| lichao mou ||| xiao xiang zhu ||| 
2021 ||| integrated training for sequence-to-sequence models using non-autoregressive transformer. ||| evgeniia tokarchuk ||| jan rosendahl ||| weiyue wang ||| pavel petrushkov ||| tomer lancewicki ||| shahram khadivi ||| hermann ney ||| 
2020 ||| dmd: a large-scale multi-modal driver monitoring dataset for attention and alertness analysis. ||| juan diego ortega ||| neslihan kose ||| paola ca ||| as ||| min-an chao ||| alexander unnervik ||| marcos nieto ||| oihana otaegui ||| luis salgado ||| 
2022 ||| lmn at semeval-2022 task 11: a transformer-based system for english named entity recognition. ||| ngoc minh lai ||| 
2021 ||| towards more effective prm-based crowd counting via a multi-resolution fusion and attention network. ||| usman sajid ||| guanghui wang ||| 
2020 ||| hypergrid: efficient multi-task transformers with grid-wise decomposable hyper projections. ||| yi tay ||| zhe zhao ||| dara bahri ||| donald metzler ||| da-cheng juan ||| 
2021 ||| attention meets geometry: geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation. ||| patrick ruhkamp ||| daoyi gao ||| hanzhi chen ||| nassir navab ||| benjamin busam ||| 
2022 ||| repre: improving self-supervised vision transformer with reconstructive pre-training. ||| luya wang ||| feng liang ||| yangguang li ||| honggang zhang ||| wanli ouyang ||| jing shao ||| 
2020 ||| lava nat: a non-autoregressive translation model with look-around decoding and vocabulary attention. ||| xiaoya li ||| yuxian meng ||| arianna yuan ||| fei wu ||| jiwei li ||| 
2021 ||| self-supervised transformer for multivariate clinical time-series with missing values. ||| sindhu tipirneni ||| chandan k. reddy ||| 
2020 ||| relational graph attention network for aspect-based sentiment analysis. ||| kai wang ||| weizhou shen ||| yunyi yang ||| xiaojun quan ||| rui wang ||| 
2021 ||| cagan: text-to-image generation with combined attention gans. ||| henning schulze ||| dogucan yaman ||| alexander waibel ||| 
2021 ||| deeppseudo: deep pseudo-code generation via transformer and code feature extraction. ||| guang yang ||| xiang chen ||| ke liu ||| chi yu ||| 
2021 ||| using transformers to provide teachers with personalized feedback on their classroom discourse: the talkmoves application. ||| abhijit suresh ||| jennifer jacobs ||| vivian lai ||| chenhao tan ||| wayne h. ward ||| james h. martin ||| tamara sumner ||| 
2022 ||| deep attention-based supernovae classification of multi-band light-curves. ||| scar pimentel ||| pablo a. est ||| vez ||| francisco f ||| rster ||| 
2021 ||| relation-aware graph attention model with adaptive self-adversarial training. ||| xiao qin ||| nasrullah sheikh ||| berthold reinwald ||| lingfei wu ||| 
2022 ||| towards exemplar-free continual learning in vision transformers: an account of attention, functional and weight regularization. ||| francesco pelosin ||| saurav jha ||| andrea torsello ||| bogdan c. raducanu ||| joost van de weijer ||| 
2021 ||| how to train your vit? data, augmentation, and regularization in vision transformers. ||| andreas steiner ||| alexander kolesnikov ||| xiaohua zhai ||| ross wightman ||| jakob uszkoreit ||| lucas beyer ||| 
2022 ||| technical report for iccv 2021 challenge sslad-track3b: transformers are better continual learners. ||| duo li ||| guimei cao ||| yunlu xu ||| zhanzhan cheng ||| yi niu ||| 
2019 ||| extreme low resolution activity recognition with spatial-temporal attention transfer. ||| yucai bai ||| giang dai ||| long chen ||| 
2020 ||| we learn better road pothole detection: from attention aggregation to adversarial domain adaptation. ||| rui fan ||| hengli wang ||| mohammud junaid bocus ||| ming liu ||| 
2019 ||| diversified co-attention towards informative live video commenting. ||| zhihan zhang ||| zhiyi yin ||| shuhuai ren ||| xinhang li ||| shicheng li ||| 
2020 ||| reservoir transformer. ||| sheng shen ||| alexei baevski ||| ari s. morcos ||| kurt keutzer ||| michael auli ||| douwe kiela ||| 
2021 ||| makeup216: logo recognition with adversarial attention representations. ||| junjun hu ||| yanhao zhu ||| bo zhao ||| jiexin zheng ||| chenxu zhao ||| xiangyu zhu ||| kangle wu ||| darun tang ||| 
2022 ||| etsformer: exponential smoothing transformers for time-series forecasting. ||| gerald woo ||| chenghao liu ||| doyen sahoo ||| akshat kumar ||| steven c. h. hoi ||| 
2022 ||| lap: an attention-based module for faithful interpretation and knowledge injection in convolutional neural networks. ||| rassa ghavami modegh ||| ahmad salimi ||| hamid r. rabiee ||| 
2021 ||| sml: a new semantic embedding alignment transformer for efficient cross-lingual natural language inference. ||| javier huertas-tato ||| alejandro mart ||| n ||| david camacho ||| 
2021 ||| a transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain mri images. ||| qing lyu ||| sanjeev v. namjoshi ||| emory mctyre ||| umit topaloglu ||| richard barcus ||| michael d. chan ||| christina k. cramer ||| waldemar debinski ||| metin n. gurcan ||| glenn j. lesser ||| hui-kuan lin ||| reginald f. munden ||| boris c. pasche ||| kiran kumar solingapuram sai ||| roy e. strowd ||| stephen b. tatter ||| kounosuke watabe ||| wei zhang ||| ge wang ||| christopher t. whitlow ||| 
2017 ||| object-part attention driven discriminative localization for fine-grained image classification. ||| yuxin peng ||| xiangteng he ||| junjie zhao ||| 
2021 ||| arat5: text-to-text transformers for arabic language understanding and generation. ||| el moatez billah nagoudi ||| abdelrahim a. elmadany ||| muhammad abdul-mageed ||| 
2021 ||| localizing objects with self-supervised transformers and no labels. ||| oriane sim ||| oni ||| gilles puy ||| huy v. vo ||| simon roburin ||| spyros gidaris ||| andrei bursuc ||| patrick p ||| rez ||| renaud marlet ||| jean ponce ||| 
2020 ||| hybrid attentional memory network for computational drug repositioning. ||| jieyue he ||| xinxing yang ||| zhuo gong ||| ibrahim zamit ||| 
2021 ||| bag of tricks for optimizing transformer efficiency. ||| ye lin ||| yanyang li ||| tong xiao ||| jingbo zhu ||| 
2021 ||| gaze estimation using transformer. ||| yihua cheng ||| feng lu ||| 
2019 ||| sanet: superpixel attention network for skin lesion attributes detection. ||| xinzi he ||| baiying lei ||| tianfu wang ||| 
2020 ||| guiding symbolic natural language grammar induction via transformer-based sequence probabilities. ||| ben goertzel ||| andres suarez madrigal ||| gino yu ||| 
2021 ||| ctal: pre-training cross-modal transformer for audio-and-language representations. ||| hang li ||| yu kang ||| tianqiao liu ||| wenbiao ding ||| zitao liu ||| 
2021 ||| observable and attention-directing bdi agents for human-autonomy teaming. ||| blair archibald ||| muffy calder ||| michele sevegnani ||| mengwei xu ||| 
2021 ||| referring segmentation in images and videos with cross-modal self-attention network. ||| linwei ye ||| mrigank rochan ||| zhi liu ||| xiaoqin zhang ||| yang wang ||| 
2022 ||| vision transformer slimming: multi-dimension searching in continuous optimization space. ||| arnav chavan ||| zhiqiang shen ||| zhuang liu ||| zechun liu ||| kwang-ting cheng ||| eric p. xing ||| 
2019 ||| follow the attention: combining partial pose and object motion for fine-grained action detection. ||| mohammad mahdi kazemi moghaddam ||| ehsan abbasnejad ||| javen shi ||| 
2021 ||| variational structured attention networks for deep visual representation learning. ||| guanglei yang ||| paolo rota ||| xavier alameda-pineda ||| dan xu ||| mingli ding ||| elisa ricci ||| 
2020 ||| a transformer-based audio captioning model with keyword estimation. ||| yuma koizumi ||| ryo masumura ||| kyosuke nishida ||| masahiro yasuda ||| shoichiro saito ||| 
2017 ||| incorporating global visual features into attention-based neural machine translation. ||| iacer calixto ||| qun liu ||| nick campbell ||| 
2021 ||| the case for translation-invariant self-attention in transformer-based language models. ||| ulme wennberg ||| gustav eje henter ||| 
2019 ||| fine-tuning pre-trained transformer language models to distantly supervised relation extraction. ||| christoph alt ||| marc h ||| bner ||| leonhard hennig ||| 
2019 ||| self multi-head attention for speaker recognition. ||| miquel india ||| pooyan safari ||| javier hernando ||| 
2021 ||| investigating methods to improve language model integration for attention-based encoder-decoder asr models. ||| mohammad zeineldeen ||| aleksandr glushko ||| wilfried michel ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2020 ||| undivided attention: are intermediate layers necessary for bert? ||| sharath nittur sridhar ||| anthony sarah ||| 
2019 ||| real-time inference in multi-sentence tasks with deep pretrained transformers. ||| samuel humeau ||| kurt shuster ||| marie-anne lachaux ||| jason weston ||| 
2020 ||| imram: iterative matching with recurrent attention memory for cross-modal image-text retrieval. ||| hui chen ||| guiguang ding ||| xudong liu ||| zijia lin ||| ji liu ||| jungong han ||| 
2022 ||| attention option-critic. ||| raviteja chunduru ||| doina precup ||| 
2021 ||| person re-identification via attention pyramid. ||| guangyi chen ||| tianpei gu ||| jiwen lu ||| jin-an bao ||| jie zhou ||| 
2020 ||| junk news bubbles: modelling the rise and fall of attention in online arenas. ||| maria castaldo ||| tommaso venturini ||| paolo frasca ||| 
2019 ||| augmenting self-attention with persistent memory. ||| sainbayar sukhbaatar ||| edouard grave ||| guillaume lample ||| herv |||  j ||| gou ||| armand joulin ||| 
2017 ||| interactive attention networks for aspect-level sentiment classification. ||| dehong ma ||| sujian li ||| xiaodong zhang ||| houfeng wang ||| 
2020 ||| generating accurate assert statements for unit test cases using pretrained transformers. ||| michele tufano ||| dawn drain ||| alexey svyatkovskiy ||| neel sundaresan ||| 
2021 ||| generative pre-trained transformer for design concept generation: an exploration. ||| qihao zhu ||| jianxi luo ||| 
2021 ||| multi-view trgru: transformer based spatiotemporal model for short-term metro origin-destination matrix prediction. ||| jiexia ye ||| furong zheng ||| juanjuan zhao ||| kejiang ye ||| chengzhong xu ||| 
2021 ||| accelerating covid-19 research with graph mining and transformer-based learning. ||| ilya tyagin ||| ankit kulshrestha ||| justin sybrandt ||| krish matta ||| michael shtutman ||| ilya safro ||| 
2021 ||| a unified pruning framework for vision transformers. ||| hao yu ||| jianxin wu ||| 
2021 ||| personalized transformer for explainable recommendation. ||| lei li ||| yongfeng zhang ||| li chen ||| 
2021 ||| a feature consistency driven attention erasing network for fine-grained image retrieval. ||| qi zhao ||| xu wang ||| shuchang lyu ||| binghao liu ||| yifan yang ||| 
2021 ||| interpretable multi-head self-attention model for sarcasm detection in social media. ||| ramya akula ||| ivan garibay ||| 
2022 ||| cats++: boosting cost aggregation with convolutions and transformers. ||| seokju cho ||| sunghwan hong ||| seungryong kim ||| 
2018 ||| gated hierarchical attention for image captioning. ||| qingzhong wang ||| antoni b. chan ||| 
2021 ||| learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction. ||| yan wang ||| peng tang ||| yuyin zhou ||| wei shen ||| elliot k. fishman ||| alan l. yuille ||| 
2019 ||| mixed high-order attention network for person re-identification. ||| binghui chen ||| weihong deng ||| jiani hu ||| 
2021 ||| high-fidelity pluralistic image completion with transformers. ||| ziyu wan ||| jingbo zhang ||| dongdong chen ||| jing liao ||| 
2022 ||| multiview transformers for video recognition. ||| shen yan ||| xuehan xiong ||| anurag arnab ||| zhichao lu ||| mi zhang ||| chen sun ||| cordelia schmid ||| 
2021 ||| gmair: unsupervised object detection based on spatial attention and gaussian mixture. ||| weijin zhu ||| yao shen ||| linfeng yu ||| lizeth patricia aguirre sanchez ||| 
2019 ||| single-modal and multi-modal false arrhythmia alarm reduction using attention-based convolutional and recurrent neural networks. ||| sajad mousavi ||| atiyeh fotoohinasab ||| fatemeh afghah ||| 
2021 ||| a daily tourism demand prediction framework based on multi-head attention cnn: the case of the foreign entrant in south korea. ||| dong-keon kim ||| sung kuk shyn ||| donghee kim ||| seungwoo jang ||| kwangsu kim ||| 
2019 ||| interactive attention for semantic text matching. ||| sendong zhao ||| yong huang ||| chang su ||| yuantong li ||| fei wang ||| 
2020 ||| graph attentional autoencoder for anticancer hyperfood prediction. ||| guadalupe gonzalez ||| shunwang gong ||| ivan laponogov ||| kirill a. veselkov ||| michael m. bronstein ||| 
2021 ||| complex spectral mapping with attention based convolution recurrent neural network for speech enhancement. ||| liming zhou ||| yongyu gao ||| ziluo wang ||| jiwei li ||| wenbin zhang ||| 
2022 ||| -scaled-attention: a novel fast attention mechanism for efficient modeling of protein sequences. ||| ashish ranjan ||| md. shah fahad ||| akshay deepak ||| 
2021 ||| cosformer: detecting co-salient object with transformers. ||| lv tang ||| 
2022 ||| supervised visual attention for simultaneous multimodal machine translation. ||| veneta haralampieva ||| ozan caglayan ||| lucia specia ||| 
2020 ||| dual-path self-attention rnn for real-time speech enhancement. ||| ashutosh pandey ||| deliang wang ||| 
2021 ||| agd-autoencoder: attention gated deep convolutional autoencoder for brain tumor segmentation. ||| tim cvetko ||| 
2021 ||| detecting dementia from speech and transcripts using transformers. ||| loukas ilias ||| dimitris askounis ||| john e. psarras ||| 
2021 ||| a hybrid attention mechanism for weakly-supervised temporal action localization. ||| ashraful islam ||| chengjiang long ||| richard j. radke ||| 
2020 ||| learning hard retrieval cross attention for transformer. ||| hongfei xu ||| qiuhui liu ||| 
2018 ||| exploring correlations in multiple facial attributes through graph attention network. ||| yan zhang ||| li sun ||| 
2020 ||| an experimental evaluation of transformer-based language models in the biomedical domain. ||| paul grouchy ||| shobhit jain ||| michael liu ||| kuhan wang ||| max tian ||| nidhi arora ||| hillary ngai ||| faiza khan khattak ||| elham dolatabadi ||| sedef akinli ko ||| ak ||| 
2017 ||| tweetit- analyzing topics for twitter users to garner maximum attention. ||| dhanasekar sundararaman ||| priya arora ||| vishwanath seshagiri ||| 
2021 ||| siamese network with interactive transformer for video object segmentation. ||| meng lan ||| jing zhang ||| fengxiang he ||| lefei zhang ||| 
2021 ||| evolving attention with residual convolutions. ||| yujing wang ||| yaming yang ||| jiangang bai ||| mingliang zhang ||| jing bai ||| jing yu ||| ce zhang ||| gao huang ||| yunhai tong ||| 
2021 ||| da-detr: domain adaptive detection transformer by hybrid attention. ||| jingyi zhang ||| jiaxing huang ||| zhipeng luo ||| gongjie zhang ||| shijian lu ||| 
2019 ||| characterizing attention cascades in whatsapp groups. ||| josemar alves caetano ||| gabriel magno ||| marcos andr |||  gon ||| alves ||| jussara m. almeida ||| humberto torres marques-neto ||| virg ||| lio a. f. almeida ||| 
2020 ||| nlnde: enhancing neural sequence taggers with attention and noisy channel for robust pharmacological entity detection. ||| lukas lange ||| heike adel ||| jannik str ||| tgen ||| 
2022 ||| transfusion: multi-view divergent fusion for medical image segmentation with transformers. ||| di liu ||| yunhe gao ||| qilong zhangli ||| zhennan yan ||| mu zhou ||| dimitris n. metaxas ||| 
2021 ||| kernel deformed exponential families for sparse continuous attention. ||| alexander moreno ||| supriya nagesh ||| zhenke wu ||| walter h. dempsey ||| james m. rehg ||| 
2020 ||| streaming simultaneous speech translation with augmented memory transformer. ||| xutai ma ||| yongqiang wang ||| mohammad javad dousti ||| philipp koehn ||| juan miguel pino ||| 
2021 ||| temporal-relational hypergraph tri-attention networks for stock trend prediction. ||| chaoran cui ||| xiaojie li ||| juan du ||| chunyun zhang ||| xiushan nie ||| meng wang ||| yilong yin ||| 
2021 ||| domain adaptation with category attention network for deep sentiment analysis. ||| dongbo xi ||| fuzhen zhuang ||| ganbin zhou ||| xiaohu cheng ||| fen lin ||| qing he ||| 
2021 ||| atiss: autoregressive transformers for indoor scene synthesis. ||| despoina paschalidou ||| amlan kar ||| maria shugrina ||| karsten kreis ||| andreas geiger ||| sanja fidler ||| 
2020 ||| rethinking transformer-based set prediction for object detection. ||| zhiqing sun ||| shengcao cao ||| yiming yang ||| kris kitani ||| 
2019 ||| self-attention with structural position representations. ||| xing wang ||| zhaopeng tu ||| longyue wang ||| shuming shi ||| 
2020 ||| reads: a rectified attentional double supervised network for scene text recognition. ||| qi song ||| qianyi jiang ||| nan li ||| rui zhang ||| xiaolin wei ||| 
2021 ||| multiplicative position-aware transformer models for language understanding. ||| zhiheng huang ||| davis liang ||| peng xu ||| bing xiang ||| 
2020 ||| transpose: towards explainable human pose estimation by transformer. ||| sen yang ||| zhibin quan ||| mu nie ||| wankou yang ||| 
2021 ||| early convolutions help transformers see better. ||| tete xiao ||| mannat singh ||| eric mintun ||| trevor darrell ||| piotr doll ||| r ||| ross b. girshick ||| 
2021 ||| empirical analysis of training strategies of transformer-based japanese chit-chat systems. ||| hiroaki sugiyama ||| masahiro mizukami ||| tsunehiro arimoto ||| hiromi narimatsu ||| yuya chiba ||| hideharu nakajima ||| toyomi meguro ||| 
2019 ||| transformer-based automatic post-editing with a context-aware encoding approach for multi-source inputs. ||| wonkee lee ||| junsu park ||| byung-hyun go ||| jong-hyeok lee ||| 
2020 ||| gast-net: graph attention spatio-temporal convolutional networks for 3d human pose estimation in video. ||| junfa liu ||| yisheng guang ||| juan rojas ||| 
2021 ||| fnr: a similarity and transformer-based approach to detect multi-modal fake news in social media. ||| faeze ghorbanpour ||| maryam ramezani ||| mohammad a. fazli ||| hamid r. rabiee ||| 
2020 ||| transformer with bidirectional decoder for speech recognition. ||| xi chen ||| songyang zhang ||| dandan song ||| peng ouyang ||| shouyi yin ||| 
2021 ||| dcf-asn: coarse-to-fine real-time visual tracking via discriminative correlation filter and attentional siamese network. ||| xizhe xue ||| ying li ||| xiaoyue yin ||| qiang shen ||| 
2019 ||| knowledge-grounded response generation with deep attentional latent-variable model. ||| hao-tong ye ||| kai-ling lo ||| shang-yu su ||| yun-nung chen ||| 
2022 ||| academic resource text level multi-label classification based on attention. ||| yue wang ||| yawen li ||| ang li ||| 
2020 ||| choppy: cut transformer for ranked list truncation. ||| dara bahri ||| yi tay ||| che zheng ||| donald metzler ||| andrew tomkins ||| 
2020 ||| a recursive network with dynamic attention for monaural speech enhancement. ||| andong li ||| chengshi zheng ||| cunhang fan ||| renhua peng ||| xiaodong li ||| 
2022 ||| learning affective meanings that derives the social behavior using bidirectional encoder representations from transformers. ||| moeen mostafavi ||| michael d. porter ||| dawn t. robinson ||| 
2020 ||| daf-net: a saliency based weakly supervised method of dual attention fusion for fine-grained image classification. ||| zichao dong ||| jilong wu ||| tingting ren ||| yue wang ||| mengying ge ||| 
2021 ||| exploring sequence feature alignment for domain adaptive detection transformers. ||| wen wang ||| yang cao ||| jing zhang ||| fengxiang he ||| zheng-jun zha ||| yonggang wen ||| dacheng tao ||| 
2021 ||| analyzing the nuances of transformers' polynomial simplification abilities. ||| vishesh agarwal ||| somak aditya ||| navin goyal ||| 
2018 ||| guided attention for large scale scene text verification. ||| dafang he ||| yeqing li ||| alexander n. gorban ||| derrall heath ||| julian ibarz ||| qian yu ||| daniel kifer ||| c. lee giles ||| 
2021 ||| enhancing transformers with gradient boosted decision trees for nli fine-tuning. ||| benjamin minixhofer ||| milan gritta ||| ignacio iacobacci ||| 
2021 ||| localization uncertainty-based attention for object detection. ||| sanghun park ||| kunhee kim ||| eunseop lee ||| daijin kim ||| 
2018 ||| forecasting user attention during everyday mobile interactions using device-integrated and wearable sensors. ||| julian steil ||| philipp m ||| ller ||| yusuke sugano ||| andreas bulling ||| 
2021 ||| neural attention models in deep learning: survey and taxonomy. ||| alana de santana correia ||| esther colombini ||| 
2021 ||| vatt: transformers for multimodal self-supervised learning from raw video, audio and text. ||| hassan akbari ||| liangzhe yuan ||| rui qian ||| wei-hong chuang ||| shih-fu chang ||| yin cui ||| boqing gong ||| 
2020 ||| multi-view attention networks for visual dialog. ||| sungjin park ||| taesun whang ||| yeochan yoon ||| heuiseok lim ||| 
2020 ||| trace: early detection of chronic kidney disease onset with transformer-enhanced feature embedding. ||| yu wang ||| ziqiao guan ||| wei hou ||| fusheng wang ||| 
2018 ||| video-based person re-identification using spatial-temporal attention networks. ||| shivansh rao ||| tanzila rahman ||| mrigank rochan ||| yang wang ||| 
2021 ||| enriched attention for robust relation extraction. ||| heike adel ||| jannik str ||| tgen ||| 
2021 ||| revisiting the onsets and frames model with additive attention. ||| kin wai cheuk ||| yin-jyun luo ||| emmanouil benetos ||| dorien herremans ||| 
2020 ||| streaming chunk-aware multihead attention for online end-to-end speech recognition. ||| shiliang zhang ||| zhifu gao ||| haoneng luo ||| ming lei ||| jie gao ||| zhijie yan ||| lei xie ||| 
2021 ||| covid-vit: classification of covid-19 from ct chest images based on vision transformer models. ||| xiaohong gao ||| yu qian ||| alice gao ||| 
2021 ||| attention-based contrastive learning for winograd schemas. ||| tassilo klein ||| moin nabi ||| 
2020 ||| air: attention with reasoning capability. ||| shi chen ||| ming jiang ||| jinhui yang ||| qi zhao ||| 
2020 ||| weakly-supervised action localization and action recognition using global-local attention of 3d cnn. ||| novanto yudistira ||| muthu subash kavitha ||| takio kurita ||| 
2020 ||| learning efficient gans using differentiable masks and co-attention distillation. ||| shaojie li ||| mingbao lin ||| yan wang ||| mingliang xu ||| feiyue huang ||| yongjian wu ||| ling shao ||| rongrong ji ||| 
2020 ||| improving target-driven visual navigation with attention on 3d spatial relationships. ||| yunlian lv ||| ning xie ||| yimin shi ||| zijiao wang ||| heng tao shen ||| 
2020 ||| high tissue contrast mri synthesis using multi-stage attention-gan for glioma segmentation. ||| mohammad hamghalam ||| baiying lei ||| tianfu wang ||| 
2021 ||| transfornn: capturing the sequential information in self-attention representations for language modeling. ||| tze yuang chong ||| xuyang wang ||| lin yang ||| junjie wang ||| 
2021 ||| attention-based model and deep reinforcement learning for distribution of event processing tasks. ||| andriy mazayev ||| faroq al-tam ||| n. correia ||| 
2021 ||| fmmformer: efficient and flexible transformer via decomposed near-field and far-field attention. ||| tan m. nguyen ||| vai suliafu ||| stanley j. osher ||| long chen ||| bao wang ||| 
2018 ||| being curious about the answers to questions: novelty search with learned attention. ||| nicholas guttenberg ||| martin biehl ||| nathaniel virgo ||| ryota kanai ||| 
2019 ||| automatic detection of ecg abnormalities by using an ensemble of deep residual networks with attention. ||| yang liu ||| runnan he ||| kuanquan wang ||| qince li ||| qiang sun ||| na zhao ||| henggui zhang ||| 
2021 ||| automatic sexism detection with multilingual transformer models. ||| mina sch ||| tz ||| jaqueline boeck ||| daria liakhovets ||| djordje slijepcevic ||| armin kirchknopf ||| manuel hecht ||| johannes bogensperger ||| sven schlarb ||| alexander schindler ||| matthias zeppelzauer ||| 
2021 ||| an empirical study on the usage of transformer models for code completion. ||| matteo ciniselli ||| nathan cooper ||| luca pascarella ||| antonio mastropaolo ||| emad aghajani ||| denys poshyvanyk ||| massimiliano di penta ||| gabriele bavota ||| 
2019 ||| an attention-based graph neural network for heterogeneous structural learning. ||| huiting hong ||| hantao guo ||| yucheng lin ||| xiaoqing yang ||| zang li ||| jieping ye ||| 
2021 ||| pre-training transformers for domain adaptation. ||| burhan ul tayyab ||| nicholas chua ||| 
2017 ||| video summarization with attention-based encoder-decoder networks. ||| zhong ji ||| kailin xiong ||| yanwei pang ||| xuelong li ||| 
2020 ||| ai-qmix: attention and imagination for dynamic multi-agent reinforcement learning. ||| shariq iqbal ||| christian a. schr ||| der de witt ||| bei peng ||| wendelin b ||| hmer ||| shimon whiteson ||| fei sha ||| 
2022 ||| not all patches are what you need: expediting vision transformers via token reorganizations. ||| youwei liang ||| chongjian ge ||| zhan tong ||| yibing song ||| jue wang ||| pengtao xie ||| 
2021 ||| are convolutional neural networks or transformers more like human vision? ||| shikhar tuli ||| ishita dasgupta ||| erin grant ||| thomas l. griffiths ||| 
2019 ||| reasoning about human-object interactions through dual attention networks. ||| tete xiao ||| quanfu fan ||| dan gutfreund ||| mathew monfort ||| aude oliva ||| bolei zhou ||| 
2020 ||| manifold-driven attention maps for weakly supervised segmentation. ||| sukesh adiga v ||| jose dolz ||| herve lombaert ||| 
2021 ||| transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. ||| zeyu yun ||| yubei chen ||| bruno a. olshausen ||| yann lecun ||| 
2020 ||| multi-label thoracic disease image classification with cross-attention networks. ||| congbo ma ||| hu wang ||| steven c. h. hoi ||| 
2020 ||| end-to-end handwritten paragraph text recognition using a vertical attention network. ||| denis coquenet ||| cl ||| ment chatelain ||| thierry paquet ||| 
2019 ||| contextual graph attention for answering logical queries over incomplete knowledge graphs. ||| gengchen mai ||| krzysztof janowicz ||| bo yan ||| rui zhu ||| ling cai ||| ni lao ||| 
2021 ||| precise learning of source code contextual semantics via hierarchical dependence structure and graph attention networks. ||| zhehao zhao ||| bo yang ||| ge li ||| huai liu ||| zhi jin ||| 
2021 ||| auto-tagging of short conversational sentences using transformer methods. ||| d. emre tasar ||| s ||| kr |||  ozan ||| umut  ||| zdil ||| m. fatih akca ||| oguzhan  ||| lmez ||| semih g ||| l ||| m ||| se ||| ilay kutal ||| ceren belhan ||| 
2022 ||| a lightweight dual-domain attention framework for sparse-view ct reconstruction. ||| chang sun ||| ken deng ||| yitong liu ||| hongwen yang ||| 
2021 ||| fine-tuning transformers for identifying self-reporting potential cases and symptoms of covid-19 in tweets. ||| max fleming ||| priyanka dondeti ||| caitlin n. dreisbach ||| adam poliak ||| 
2017 ||| structural attention neural networks for improved sentiment analysis. ||| filippos kokkinos ||| alexandros potamianos ||| 
2020 ||| one point is all you need: directional attention point for feature learning. ||| liqiang lin ||| pengdi huang ||| chi-wing fu ||| kai xu ||| hao zhang ||| hui huang ||| 
2021 ||| an attention-aided deep learning framework for massive mimo channel estimation. ||| jiabao gao ||| mu hu ||| caijun zhong ||| geoffrey ye li ||| zhaoyang zhang ||| 
2021 ||| sra-lstm: social relationship attention lstm for human trajectory prediction. ||| yusheng peng ||| gaofeng zhang ||| jun shi ||| benzhu xu ||| liping zheng ||| 
2019 ||| graph message passing with cross-location attentions for long-term ili prediction. ||| songgaojun deng ||| shusen wang ||| huzefa rangwala ||| lijing wang ||| yue ning ||| 
2021 ||| multi-head self-attention via vision transformer for zero-shot learning. ||| faisal alamri ||| anjan dutta ||| 
2019 ||| cnns, lstms, and attention networks for pathology detection in medical data. ||| nora vogt ||| 
2017 ||| multilingual hierarchical attention networks for document classification. ||| nikolaos pappas ||| andrei popescu-belis ||| 
2022 ||| qs-attn: query-selected attention for contrastive learning in i2i translation. ||| xueqi hu ||| xinyue zhou ||| qiusheng huang ||| zhengyi shi ||| li sun ||| qingli li ||| 
2019 ||| tdapnet: prototype network with recurrent top-down attention for robust object classification under partial occlusion. ||| mingqing xiao ||| adam kortylewski ||| ruihai wu ||| siyuan qiao ||| wei shen ||| alan l. yuille ||| 
2020 ||| a deep reinforcement learning algorithm using dynamic attention model for vehicle routing problems. ||| bo peng ||| jiahai wang ||| zizhen zhang ||| 
2021 ||| a reinforcement learning approach for sequential spatial transformer networks. ||| fatemeh azimi ||| federico raue ||| j ||| rn hees ||| andreas dengel ||| 
2021 ||| dynamic transformer for efficient machine translation on embedded devices. ||| hishan parry ||| lei xun ||| amin sabet ||| jia bi ||| jonathon hare ||| geoff v. merrett ||| 
2021 ||| intformer: predicting pedestrian intention with the aid of the transformer architecture. ||| javier lorenzo ||| ignacio parra ||| miguel  ||| ngel sotelo ||| 
2020 ||| a mathematical theory of attention. ||| james vuckovic ||| aristide baratin ||| remi tachet des combes ||| 
2019 ||| overt visual attention on rendered 3d objects. ||| oleksii sidorov ||| joshua s. harvey ||| hannah e. smithson ||| jon yngve hardeberg ||| 
2021 ||| a hierarchical conditional random field-based attention mechanism approach for gastric histopathology image classification. ||| yixin li ||| xinran wu ||| chen li ||| changhao sun ||| md mamunur rahaman ||| yudong yao ||| xiaoyan li ||| yong zhang ||| tao jiang ||| 
2022 ||| on the expressive power of message-passing neural networks as global feature map transformers. ||| floris geerts ||| jasper steegmans ||| jan van den bussche ||| 
2020 ||| ttpp: temporal transformer with progressive prediction for efficient action anticipation. ||| wen wang ||| xiaojiang peng ||| yanzhou su ||| yu qiao ||| jian cheng ||| 
2021 |||  - the transformer language model for bosnian, croatian, montenegrin and serbian. ||| nikola ljubesic ||| davor lauc ||| 
2021 ||| a simple single-scale vision transformer for object localization and instance segmentation. ||| wuyang chen ||| xianzhi du ||| fan yang ||| lucas beyer ||| xiaohua zhai ||| tsung-yi lin ||| huizhong chen ||| jing li ||| xiaodan song ||| zhangyang wang ||| denny zhou ||| 
2020 ||| spatial-temporal transformer networks for traffic flow forecasting. ||| mingxing xu ||| wenrui dai ||| chunmiao liu ||| xing gao ||| weiyao lin ||| guo-jun qi ||| hongkai xiong ||| 
2020 ||| testing pre-trained transformer models for lithuanian news clustering. ||| lukas stankevicius ||| mantas lukosevicius ||| 
2021 ||| serialized multi-layer multi-head attention for neural speaker embedding. ||| hongning zhu ||| kong aik lee ||| haizhou li ||| 
2020 ||| understanding self-attention of self-supervised audio transformers. ||| shu-wen yang ||| andy t. liu ||| hung-yi lee ||| 
2021 ||| context-aware heterogeneous graph attention network for user behavior prediction in local consumer service platform. ||| peiyuan zhu ||| xiaofeng wang ||| zisen sang ||| aiquan yuan ||| guodong cao ||| 
2022 ||| gransformer: transformer-based graph generation. ||| ahmad khajenezhad ||| seyed ali osia ||| mahmood karimian ||| hamid beigy ||| 
2020 ||| covid-19 detection using residual attention network an artificial intelligence approach. ||| vishal sharma ||| curtis e. dyreson ||| 
2018 ||| on attention models for human activity recognition. ||| vishvak s. murahari ||| thomas ploetz ||| 
2020 ||| sentence constituent-aware aspect-category sentiment analysis with graph attention networks. ||| yuncong li ||| cunxiang yin ||| sheng-hua zhong ||| 
2021 ||| shallow attention network for polyp segmentation. ||| jun wei ||| yiwen hu ||| ruimao zhang ||| zhen li ||| s. kevin zhou ||| shuguang cui ||| 
2022 ||| aspect-based sentiment analysis through edu-level attentions. ||| ting lin ||| aixin sun ||| yequan wang ||| 
2022 ||| adaptive transformers for robust few-shot cross-domain face anti-spoofing. ||| hsin-ping huang ||| deqing sun ||| yaojie liu ||| wen-sheng chu ||| taihong xiao ||| jinwei yuan ||| hartwig adam ||| ming-hsuan yang ||| 
2017 ||| exploring human-like attention supervision in visual question answering. ||| tingting qiao ||| jianfeng dong ||| duanqing xu ||| 
2021 ||| improving 3d object detection with channel-wise transformer. ||| hualian sheng ||| sijia cai ||| yuan liu ||| bing deng ||| jianqiang huang ||| xian-sheng hua ||| min-jian zhao ||| 
2020 ||| hurricanes and hashtags: characterizing online collective attention for natural disasters. ||| michael v. arnold ||| david rushing dewhurst ||| thayer alshaabi ||| joshua r. minot ||| jane lydia adams ||| christopher m. danforth ||| peter sheridan dodds ||| 
2021 ||| gcst: graph convolutional skeleton transformer for action recognition. ||| ruwen bai ||| min li ||| bo meng ||| fengfa li ||| junxing ren ||| miao jiang ||| degang sun ||| 
2018 ||| how to become instagram famous: post popularity prediction with dual-attention. ||| zhongping zhang ||| tianlang chen ||| zheng zhou ||| jiaxin li ||| jiebo luo ||| 
2019 ||| saliency-guided attention network for image-sentence matching. ||| zhong ji ||| haoran wang ||| jungong han ||| yanwei pang ||| 
2021 ||| bert got a date: introducing transformers to temporal tagging. ||| satya almasian ||| dennis aumiller ||| michael gertz ||| 
2021 ||| grad-cam guided channel-spatial attention module for fine-grained visual classification. ||| shuai xu ||| dongliang chang ||| jiyang xie ||| zhanyu ma ||| 
2020 ||| advances of transformer-based models for news headline generation. ||| alexey bukhtiyarov ||| ilya gusev ||| 
2021 ||| attentional multi-layer feature fusion convolution network for audio-visual speech enhancement. ||| xinmeng xu ||| yang wang ||| dongxiang xu ||| cong zhang ||| yiyuan peng ||| jie jia ||| binbin chen ||| 
2022 ||| unified fake news detection using transfer learning of bidirectional encoder representation from transformers model. ||| vijay srinivas tida ||| sonya hsu ||| xiali hei ||| 
2020 ||| contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems. ||| yanjie gou ||| yinjie lei ||| lingqiao liu ||| 
2020 ||| deeper or wider networks of point clouds with self-attention? ||| haoxi ran ||| li lu ||| 
2022 ||| efficient visual tracking via hierarchical cross-attention transformer. ||| xin chen ||| dong wang ||| dongdong li ||| huchuan lu ||| 
2021 ||| learnable compression network with transformer for approximate nearest neighbor search. ||| haokui zhang ||| wenze hu ||| xiaoyu wang ||| buzhou tang ||| 
2018 ||| attention-based pyramid aggregation network for visual place recognition. ||| yingying zhu ||| jiong wang ||| lingxi xie ||| liang zheng ||| 
2017 ||| gated-attention architectures for task-oriented language grounding. ||| devendra singh chaplot ||| kanthashree mysore sathyendra ||| rama kumar pasumarthi ||| dheeraj rajagopal ||| ruslan salakhutdinov ||| 
2017 ||| phase conductor on multi-layered attentions for machine comprehension. ||| rui liu ||| wei wei ||| weiguang mao ||| maria chikina ||| 
2021 ||| offline pre-trained multi-agent decision transformer: one big sequence model tackles all smac tasks. ||| linghui meng ||| muning wen ||| yaodong yang ||| chenyang le ||| xiyun li ||| weinan zhang ||| ying wen ||| haifeng zhang ||| jun wang ||| bo xu ||| 
2019 ||| temporal fusion transformers for interpretable multi-horizon time series forecasting. ||| bryan lim ||| sercan  ||| mer arik ||| nicolas loeff ||| tomas pfister ||| 
2022 ||| cell segmentation from telecentric bright-field transmitted light microscopic images using a residual attention u-net: a case study on hela line. ||| ali ghaznavi ||| renata rycht ||| rikov ||| mohammadmehdi saberioon ||| dalibor stys ||| 
2021 ||| embracing single stride 3d object detector with sparse transformer. ||| lue fan ||| ziqi pang ||| tianyuan zhang ||| yu-xiong wang ||| hang zhao ||| feng wang ||| naiyan wang ||| zhaoxiang zhang ||| 
2021 ||| monaural speech enhancement with complex convolutional block attention module and joint time frequency losses. ||| shengkui zhao ||| trung hieu nguyen ||| bin ma ||| 
2021 ||| visqa: x-raying vision and language reasoning in transformers. ||| theo jaunet ||| corentin kervadec ||| romain vuillemot ||| grigory antipov ||| moez baccouche ||| christian wolf ||| 
2021 ||| nn-lut: neural approximation of non-linear operations for efficient transformer inference. ||| joonsang yu ||| junki park ||| seongmin park ||| minsoo kim ||| sihwa lee ||| dong hyun lee ||| jungwook choi ||| 
2021 ||| 3d-retr: end-to-end single and multi-view 3d reconstruction with transformers. ||| zai shi ||| zhao meng ||| yiran xing ||| yunpu ma ||| roger wattenhofer ||| 
2021 ||| residual attention: a simple but effective method for multi-label recognition. ||| ke zhu ||| jianxin wu ||| 
2021 ||| transformers predicting the future. applying attention in next-frame and time series forecasting. ||| radostin cholakov ||| todor kolev ||| 
2021 ||| claws: contrastive learning with hard attention and weak supervision. ||| jansel herrera-gerena ||| ramakrishnan sundareswaran ||| john just ||| matthew j. darr ||| ali jannesari ||| 
2021 ||| dynamic token normalization improves vision transformer. ||| wenqi shao ||| yixiao ge ||| zhaoyang zhang ||| xuyuan xu ||| xiaogang wang ||| ying shan ||| ping luo ||| 
2018 ||| "factual" or "emotional": stylized image captioning with adaptive learning and attention. ||| tianlang chen ||| zhongping zhang ||| quanzeng you ||| chen fang ||| zhaowen wang ||| hailin jin ||| jiebo luo ||| 
2021 ||| demystify optimization challenges in multilingual transformers. ||| xian li ||| hongyu gong ||| 
2019 ||| span-based joint entity and relation extraction with transformer pre-training. ||| markus eberts ||| adrian ulges ||| 
2021 ||| deep rgb-d saliency detection with depth-sensitive attention and automatic multi-modal fusion. ||| peng sun ||| wenhu zhang ||| huanyu wang ||| songyuan li ||| xi li ||| 
2021 ||| tph-yolov5: improved yolov5 based on transformer prediction head for object detection on drone-captured scenarios. ||| xingkui zhu ||| shuchang lyu ||| xu wang ||| qi zhao ||| 
2021 ||| positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition. ||| jiyao liu ||| yanxi zhao ||| hao wu ||| dongmei jiang ||| 
2021 ||| vtgan: semi-supervised retinal image synthesis and disease prediction using vision transformers. ||| sharif amit kamran ||| khondker fariha hossain ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| kenton m. sanders ||| salah a. baker ||| 
2020 ||| flow-guided attention networks for video-based person re-identification. ||| madhu kiran ||| amran bhuiyan ||| louis-antoine blais-morin ||| mehrsan javan ||| ismail ben ayed ||| eric granger ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| yana agafonova ||| alexey tikhonov ||| ivan p. yamshchikov ||| 
2019 ||| to the attention of mobile software developers: guess what, test your app! ||| luis cruz |||  ||| rui abreu ||| david lo ||| 
2019 ||| connection sensitive attention u-net for accurate retinal vessel segmentation. ||| ruirui li ||| mingming li ||| jiacheng li ||| 
2018 ||| learning 6-dof grasping and pick-place using attention focus. ||| marcus gualtieri ||| robert platt jr. ||| 
2020 ||| modern hopfield networks and attention for immune repertoire classification. ||| michael widrich ||| bernhard sch ||| fl ||| hubert ramsauer ||| milena pavlovic ||| lukas gruber ||| markus holzleitner ||| johannes brandstetter ||| geir kjetil sandve ||| victor greiff ||| sepp hochreiter ||| g ||| nter klambauer ||| 
2021 ||| short and long range relation based spatio-temporal transformer for micro-expression recognition. ||| liangfei zhang ||| xiaopeng hong ||| ognjen arandjelovic ||| guoying zhao ||| 
2020 ||| optimizing transformers with approximate computing for faster, smaller and more accurate nlp models. ||| amrit nagarajan ||| sanchari sen ||| jacob r. stevens ||| anand raghunathan ||| 
2019 ||| attention augmented convolutional networks. ||| irwan bello ||| barret zoph ||| ashish vaswani ||| jonathon shlens ||| quoc v. le ||| 
2020 ||| reconsider: re-ranking using span-focused cross-attention for open domain question answering. ||| srinivasan iyer ||| sewon min ||| yashar mehdad ||| wen-tau yih ||| 
2020 ||| experts and authorities receive disproportionate attention on twitter during the covid-19 crisis. ||| kristina gligoric ||| manoel horta ribeiro ||| martin m ||| ller ||| olesia altunina ||| maxime peyrard ||| marcel salath ||| giovanni colavizza ||| robert west ||| 
2022 ||| hibrids: attention with hierarchical biases for structure-aware long document summarization. ||| shuyang cao ||| lu wang ||| 
2020 ||| deepsumm - deep code summaries using neural transformer architecture. ||| vivek gupta ||| 
2020 ||| convolutional hierarchical attention network for query-focused video summarization. ||| shuwen xiao ||| zhou zhao ||| zijian zhang ||| xiaohui yan ||| min yang ||| 
2020 ||| learning oracle attention for high-fidelity face completion. ||| tong zhou ||| changxing ding ||| shaowen lin ||| xinchao wang ||| dacheng tao ||| 
2022 ||| fourier disentangled space-time attention for aerial video recognition. ||| divya kothandaraman ||| tianrui guan ||| xijun wang ||| sean hu ||| ming lin ||| dinesh manocha ||| 
2020 ||| x-lxmert: paint, caption and answer questions with multi-modal transformers. ||| jaemin cho ||| jiasen lu ||| dustin schwenk ||| hannaneh hajishirzi ||| aniruddha kembhavi ||| 
2021 ||| few-shot segmentation via cycle-consistent transformer. ||| gengwei zhang ||| guoliang kang ||| yunchao wei ||| yi yang ||| 
2021 ||| the image local autoregressive transformer. ||| chenjie cao ||| yuxin hong ||| xiang li ||| chengrong wang ||| chengming xu ||| xiangyang xue ||| yanwei fu ||| 
2021 ||| covid-19 detection in chest x-ray images using swin-transformer and transformer in transformer. ||| juntao jiang ||| shuyi lin ||| 
2020 ||| sudden attention shifts on wikipedia following covid-19 mobility restrictions. ||| manoel horta ribeiro ||| kristina gligoric ||| maxime peyrard ||| florian lemmerich ||| markus strohmaier ||| robert west ||| 
2021 ||| cvt: introducing convolutions to vision transformers. ||| haiping wu ||| bin xiao ||| noel codella ||| mengchen liu ||| xiyang dai ||| lu yuan ||| lei zhang ||| 
2020 ||| agvnet: attention guided velocity learning for 3d human motion prediction. ||| xiaoli liu ||| jianqin yin ||| jun liu ||| 
2021 ||| vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers. ||| stella frank ||| emanuele bugliarello ||| desmond elliott ||| 
2018 ||| deep attention model for triage of emergency department patients. ||| djordje gligorijevic ||| jelena stojanovic ||| wayne satz ||| ivan stojkovic ||| kraftin schreyer ||| daniel del portal ||| zoran obradovic ||| 
2021 ||| a sample-based training method for distantly supervised relation extraction with pre-trained transformers. ||| mehrdad nasser ||| mohamad bagher sajadi ||| behrouz minaei-bidgoli ||| 
2021 ||| attention-based neural beamforming layers for multi-channel speech recognition. ||| bhargav pulugundla ||| yang gao ||| brian king ||| gokce keskin ||| sri harish mallidi ||| minhua wu ||| jasha droppo ||| roland maas ||| 
2021 ||| an empirical study of training end-to-end vision-and-language transformers. ||| zi-yi dou ||| yichong xu ||| zhe gan ||| jianfeng wang ||| shuohang wang ||| lijuan wang ||| chenguang zhu ||| pengchuan zhang ||| lu yuan ||| nanyun peng ||| zicheng liu ||| michael zeng ||| 
2021 ||| pu-transformer: point cloud upsampling transformer. ||| shi qiu ||| saeed anwar ||| nick barnes ||| 
2021 ||| instance-based vision transformer for subtyping of papillary renal cell carcinoma in histopathological image. ||| zeyu gao ||| bangyang hong ||| xianli zhang ||| yang li ||| chang jia ||| jialun wu ||| chunbao wang ||| deyu meng ||| chen li ||| 
2021 ||| cogview: mastering text-to-image generation via transformers. ||| ming ding ||| zhuoyi yang ||| wenyi hong ||| wendi zheng ||| chang zhou ||| da yin ||| junyang lin ||| xu zou ||| zhou shao ||| hongxia yang ||| jie tang ||| 
2017 ||| product function need recognition via semi-supervised attention network. ||| hu xu ||| sihong xie ||| lei shu ||| philip s. yu ||| 
2021 ||| cotr: correspondence transformer for matching across images. ||| wei jiang ||| eduard trulls ||| jan hosang ||| andrea tagliasacchi ||| kwang moo yi ||| 
2020 ||| end-to-end object detection with transformers. ||| nicolas carion ||| francisco massa ||| gabriel synnaeve ||| nicolas usunier ||| alexander kirillov ||| sergey zagoruyko ||| 
2020 ||| attention with multiple sources knowledges for covid-19 from ct images. ||| duy m. h. nguyen ||| huong vu ||| binh t. nguyen ||| fabrizio nunnari ||| daniel sonntag ||| 
2019 ||| can neural image captioning be controlled via forced attention? ||| philipp sadler ||| tatjana scheffler ||| david schlangen ||| 
2018 ||| st-gan: spatial transformer generative adversarial networks for image compositing. ||| chen-hsuan lin ||| ersin yumer ||| oliver wang ||| eli shechtman ||| simon lucey ||| 
2020 ||| transformer++. ||| prakhar thapak ||| prodip hore ||| 
2021 ||| alignment attention by matching key and query distributions. ||| shujian zhang ||| xinjie fan ||| huangjie zheng ||| korawat tanwisuth ||| mingyuan zhou ||| 
2021 ||| revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning. ||| amaia salvador ||| erhan gundogdu ||| loris bazzani ||| michael donoser ||| 
2022 ||| towards practical certifiable patch defense with vision transformer. ||| zhaoyu chen ||| bo li ||| jianghe xu ||| shuang wu ||| shouhong ding ||| wenqiang zhang ||| 
2018 ||| generating attention from classifier activations for fine-grained recognition. ||| wei shen ||| rujie liu ||| 
2020 ||| hhh: an online medical chatbot system based on knowledge graph and hierarchical bi-directional attention. ||| qiming bao ||| lin ni ||| jiamou liu ||| 
2020 ||| attentional speech recognition models misbehave on out-of-domain utterances. ||| phillip keung ||| wei niu ||| yichao lu ||| julian salazar ||| vikas bhardwaj ||| 
2020 ||| relationnet++: bridging visual representations for object detection via transformer decoder. ||| cheng chi ||| fangyun wei ||| han hu ||| 
2018 ||| multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. ||| wei wang ||| ming yan ||| chen wu ||| 
2021 ||| paenet: a progressive attention-enhanced network for 3d to 2d retinal vessel segmentation. ||| zhuojie wu ||| muyi sun ||| 
2020 ||| attention-based transducer for online speech recognition. ||| bin wang ||| yan yin ||| hui lin ||| 
2022 ||| local feature matching with transformers for low-end devices. ||| kyrylo kolodiazhnyi ||| 
2018 ||| facial landmarks detection by self-iterative regression based landmarks-attention network. ||| tao hu ||| honggang qi ||| jizheng xu ||| qingming huang ||| 
2021 ||| psvit: better vision transformer via token pooling and attention sharing. ||| boyu chen ||| peixia li ||| baopu li ||| chuming li ||| lei bai ||| chen lin ||| ming sun ||| junjie yan ||| wanli ouyang ||| 
2022 ||| contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection. ||| yu tian ||| guansong pang ||| fengbei liu ||| yuyuan liu ||| chong wang ||| yuanhong chen ||| johan w. verjans ||| gustavo carneiro ||| 
2018 ||| recurrent attention unit. ||| guoqiang zhong ||| guohua yue ||| xiao ling ||| 
2019 ||| ranet: ranking attention network for fast video object segmentation. ||| ziqin wang ||| jun xu ||| li liu ||| fan zhu ||| ling shao ||| 
2021 ||| scene-adaptive attention network for crowd counting. ||| xing wei ||| yuanrui kang ||| jihao yang ||| yunfeng qiu ||| dahu shi ||| wenming tan ||| yihong gong ||| 
2022 ||| transformers meet visual learning understanding: a comprehensive review. ||| yuting yang ||| licheng jiao ||| xu liu ||| fang liu ||| shuyuan yang ||| zhixi feng ||| xu tang ||| 
2022 ||| reshaping robot trajectories using natural language commands: a study of multi-modal data alignment using transformers. ||| arthur bucker ||| luis figueredo ||| sami haddadin ||| ashish kapoor ||| shuang ma ||| rogerio bonatti ||| 
2021 ||| point cloud learning with transformer. ||| xian-feng han ||| yu-jia kuang ||| guo-qiang xiao ||| 
2021 ||| hopeful_men@lt-edi-eacl2021: hope speech detection using indic transliteration and transformers. ||| ishan sanjeev upadhyay ||| nikhil e ||| anshul wadhawan ||| radhika mamidi ||| 
2019 ||| password-conditioned anonymization and deanonymization with face identity transformers. ||| xiuye gu ||| weixin luo ||| michael s. ryoo ||| yong jae lee ||| 
2018 ||| attentionxml: extreme multi-label text classification with multi-label attention based recurrent neural networks. ||| ronghui you ||| suyang dai ||| zihan zhang ||| hiroshi mamitsuka ||| shanfeng zhu ||| 
2019 ||| multi-level attention network using text, audio and video for depression prediction. ||| anupama ray ||| siddharth kumar ||| rutvik reddy ||| prerana mukherjee ||| ritu garg ||| 
2021 ||| grassmannian graph-attentional landmark selection for domain adaptation. ||| bin sun ||| shaofan wang ||| dehui kong ||| jinghua li ||| baocai yin ||| 
2022 ||| cmx: cross-modal fusion for rgb-x semantic segmentation with transformers. ||| huayao liu ||| jiaming zhang ||| kailun yang ||| xinxin hu ||| rainer stiefelhagen ||| 
2019 ||| retrosynthesis with attention-based nmt model and chemical analysis of the "wrong" predictions. ||| hongliang duan ||| ling wang ||| chengyun zhang ||| jianjun li ||| 
2018 ||| deepphys: video-based physiological measurement using convolutional attention networks. ||| weixuan chen ||| daniel j. mcduff ||| 
2021 ||| transforensics: image forgery localization with dense self-attention. ||| jing hao ||| zhixin zhang ||| shicai yang ||| di xie ||| shiliang pu ||| 
2021 ||| add: frequency attention and multi-view based knowledge distillation to detect low-quality compressed deepfake images. ||| binh m. le ||| simon s. woo ||| 
2019 ||| in conclusion not repetition: comprehensive abstractive summarization with diversified attention based on determinantal point processes. ||| lei li ||| wei liu ||| marina litvak ||| natalia vanetik ||| zuying huang ||| 
2019 ||| frequency and temporal convolutional attention for text-independent speaker recognition. ||| sarthak yadav ||| atul rai ||| 
2021 ||| staircase attention for recurrent processing of sequences. ||| da ju ||| stephen roller ||| sainbayar sukhbaatar ||| jason weston ||| 
2021 ||| transwic at semeval-2021 task 2: transformer-based multilingual and cross-lingual word-in-context disambiguation. ||| hansi hettiarachchi ||| tharindu ranasinghe ||| 
2022 ||| the paradox of choice: using attention in hierarchical reinforcement learning. ||| andrei cristian nica ||| khimya khetarpal ||| doina precup ||| 
2022 ||| wegformer: transformers for weakly supervised semantic segmentation. ||| chunmeng liu ||| enze xie ||| wenjia wang ||| wenhai wang ||| guangyao li ||| ping luo ||| 
2018 ||| interpretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling. ||| kaixuan chen ||| lina yao ||| xianzhi wang ||| dalin zhang ||| tao gu ||| zhiwen yu ||| zheng yang ||| 
2021 ||| agentformer: agent-aware transformers for socio-temporal multi-agent forecasting. ||| ye yuan ||| xinshuo weng ||| yanglan ou ||| kris kitani ||| 
2019 ||| gapnet: graph attention based point neural network for exploiting local feature of point cloud. ||| can chen ||| luca zanotti fragonara ||| antonios tsourdos ||| 
2020 ||| investigation of speaker-adaptation methods in transformer based asr. ||| vishwas m. shetty ||| metilda sagaya mary n. j ||| srinivasan umesh ||| 
2021 ||| contributions of transformer attention heads in multi- and cross-lingual tasks. ||| weicheng ma ||| kai zhang ||| renze lou ||| lili wang ||| soroush vosoughi ||| 
2021 ||| multi-view self-attention based transformer for speaker recognition. ||| rui wang ||| junyi ao ||| long zhou ||| shujie liu ||| zhihua wei ||| tom ko ||| qing li ||| yu zhang ||| 
2018 ||| a dual-attention hierarchical recurrent neural network for dialogue act classification. ||| ruizhe li ||| chenghua lin ||| matthew collinson ||| xiao li ||| guanyi chen ||| 
2020 ||| dual-sampling attention network for diagnosis of covid-19 from community acquired pneumonia. ||| xi ouyang ||| jiayu huo ||| liming xia ||| fei shan ||| jun liu ||| zhanhao mo ||| fuhua yan ||| zhongxiang ding ||| qi yang ||| bin song ||| feng shi ||| huan yuan ||| ying wei ||| xiaohuan cao ||| yaozong gao ||| dijia wu ||| qian wang ||| dinggang shen ||| 
2021 ||| cnn-based search model underestimates attention guidance by simple visual features. ||| endel p ||| der ||| 
2020 ||| differentiable window for dynamic local attention. ||| thanh-tung nguyen ||| xuan-phi nguyen ||| shafiq r. joty ||| xiaoli li ||| 
2020 ||| transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. ||| changmao li ||| jinho d. choi ||| 
2020 ||| the lipschitz constant of self-attention. ||| hyunjik kim ||| george papamakarios ||| andriy mnih ||| 
2020 ||| lip-reading with hierarchical pyramidal convolution and self-attention. ||| hang chen ||| jun du ||| yu hu ||| li-rong dai ||| chin-hui lee ||| bao-cai yin ||| 
2021 ||| video swin transformer. ||| ze liu ||| jia ning ||| yue cao ||| yixuan wei ||| zheng zhang ||| stephen lin ||| han hu ||| 
2021 ||| strategic mitigation of agent inattention in drivers with open-quantum cognition models. ||| qizi zhang ||| venkata sriram siddhardh nadendla ||| s. n. balakrishnan ||| jerome r. busemeyer ||| 
2021 ||| attention-driven graph clustering network. ||| zhihao peng ||| hui liu ||| yuheng jia ||| junhui hou ||| 
2020 ||| 3d axial-attention for lung nodule classification. ||| mundher al-shabi ||| kelvin shak ||| maxine tan ||| 
2021 ||| domain-adversarial training of self-attention based networks for land cover classification using multi-temporal sentinel-2 satellite imagery. ||| mauro martini ||| vittorio mazzia ||| aleem khaliq ||| marcello chiaberge ||| 
2021 ||| dynamic graph representation learning via graph transformer networks. ||| weilin cong ||| yanhong wu ||| yuandong tian ||| mengting gu ||| yinglong xia ||| mehrdad mahdavi ||| chun-cheng jason chen ||| 
2020 ||| arelu: attention-based rectified linear unit. ||| dengsheng chen ||| kai xu ||| 
2021 ||| tctn: a 3d-temporal convolutional transformer network for spatiotemporal predictive learning. ||| ziao yang ||| xiangrui yang ||| qifeng lin ||| 
2021 ||| spectr: spectral transformer for hyperspectral pathology image segmentation. ||| boxiang yun ||| yan wang ||| jieneng chen ||| huiyu wang ||| wei shen ||| qingli li ||| 
2022 ||| vit-p: rethinking data-efficient vision transformers from locality. ||| bin chen ||| ran wang ||| di ming ||| xin feng ||| 
2022 ||| optimal-er auctions through attention. ||| dmitry ivanov ||| iskander safiulin ||| ksenia balabaeva ||| igor filippov ||| 
2019 ||| fine-grained attention and feature-sharing generative adversarial networks for single image super-resolution. ||| yitong yan ||| chuangchuang liu ||| changyou chen ||| xianfang sun ||| longcun jin ||| xiang zhou ||| 
2019 ||| attention-based fusion for multi-source human image generation. ||| st ||| phane lathuili ||| re ||| enver sangineto ||| aliaksandr siarohin ||| nicu sebe ||| 
2022 ||| transbtsv2: wider instead of deeper transformer for medical image segmentation. ||| jiangyun li ||| wenxuan wang ||| chen chen ||| tianxiang zhang ||| sen zha ||| hong yu ||| jing wang ||| 
2021 ||| neuromorphic camera denoising using graph neural network-driven transformers. ||| yusra alkendi ||| rana azzam ||| abdulla ayyad ||| sajid javed ||| lakmal d. seneviratne ||| yahya h. zweiri ||| 
2019 ||| modeling graph structure in transformer for better amr-to-text generation. ||| jie zhu ||| junhui li ||| muhua zhu ||| longhua qian ||| min zhang ||| guodong zhou ||| 
2018 ||| a survey of attention management systems in ubiquitous computing environments. ||| christoph anderson ||| isabel fernanda h ||| bener ||| ann-kathrin seipp ||| sandra ohly ||| klaus david ||| veljko pejovic ||| 
2021 ||| patchformer: a versatile 3d transformer based on patch attention. ||| zhang cheng ||| haocheng wan ||| xinyi shen ||| zizhao wu ||| 
2018 ||| cross-topic argument mining from heterogeneous sources using attention-based neural networks. ||| christian stab ||| tristan miller ||| iryna gurevych ||| 
2021 ||| transformer meets tracker: exploiting temporal context for robust visual tracking. ||| ning wang ||| wengang zhou ||| jie wang ||| houqiang li ||| 
2019 ||| an actor-critic-attention mechanism for deep reinforcement learning in multi-view environments. ||| elaheh barati ||| xuewen chen ||| 
2021 ||| spatiotemporal transformer for video-based person re-identification. ||| tianyu zhang ||| longhui wei ||| lingxi xie ||| zijie zhuang ||| yongfei zhang ||| bo li ||| qi tian ||| 
2021 ||| transformer tracking. ||| xin chen ||| bin yan ||| jiawen zhu ||| dong wang ||| xiaoyun yang ||| huchuan lu ||| 
2021 ||| r2d2: recursive transformer based on differentiable tree for interpretable hierarchical language modeling. ||| xiang hu ||| haitao mi ||| zujie wen ||| yafang wang ||| yi su ||| jing zheng ||| gerard de melo ||| 
2021 ||| vtamiq: transformers for attention modulated image quality assessment. ||| andrei chubarau ||| james clark ||| 
2021 ||| learning defense transformers for counterattacking adversarial examples. ||| jincheng li ||| jiezhang cao ||| yifan zhang ||| jian chen ||| mingkui tan ||| 
2020 ||| tabular transformers for modeling multivariate time series. ||| inkit padhi ||| yair schiff ||| igor melnyk ||| mattia rigotti ||| youssef mroueh ||| pierre l. dognin ||| jerret ross ||| ravi nair ||| erik altman ||| 
2018 ||| deephttp: semantics-structure model with attention for anomalous http traffic detection and pattern mining. ||| yuqi yu ||| hanbing yan ||| hongchao guan ||| hao zhou ||| 
2020 ||| document modeling with graph attention networks for multi-grained machine reading comprehension. ||| bo zheng ||| haoyang wen ||| yaobo liang ||| nan duan ||| wanxiang che ||| daxin jiang ||| ming zhou ||| ting liu ||| 
2020 ||| residual network based direct synthesis of em structures: a study on one-to-one transformers. ||| david joseph munzer ||| siawpeng er ||| minshuo chen ||| yan li ||| naga sasikanth mannem ||| tuo zhao ||| hua wang ||| 
2021 ||| s2s-ft: fine-tuning pretrained transformer encoders for sequence-to-sequence learning. ||| hangbo bao ||| li dong ||| wenhui wang ||| nan yang ||| furu wei ||| 
2018 ||| reducing visual confusion with discriminative attention. ||| lezi wang ||| ziyan wu ||| srikrishna karanam ||| kuan-chuan peng ||| rajat vikram singh ||| 
2022 ||| isda: position-aware instance segmentation with deformable attention. ||| kaining ying ||| zhenhua wang ||| cong bai ||| pengfei zhou ||| 
2019 ||| unsupervised community detection with modularity-based attention model. ||| ivan lobov ||| sergey ivanov ||| 
2021 ||| csformer: bridging convolution and transformer for compressive sensing. ||| dongjie ye ||| zhangkai ni ||| hanli wang ||| jian zhang ||| shiqi wang ||| sam kwong ||| 
2020 ||| : accelerating attention mechanisms in neural networks with approximation. ||| tae jun ham ||| sungjun jung ||| seonghak kim ||| young h. oh ||| yeonhong park ||| yoonho song ||| jung-hun park ||| sanghee lee ||| kyoung park ||| jae w. lee ||| deog-kyoon jeong ||| 
2019 ||| music theme recognition using cnn and self-attention. ||| manoj sukhavasi ||| sainath adapa ||| 
2017 ||| attention-based multimodal fusion for video description. ||| chiori hori ||| takaaki hori ||| teng-yok lee ||| kazuhiro sumi ||| john r. hershey ||| tim k. marks ||| 
2019 ||| attention-based multi-input deep learning architecture for biological activity prediction: an application in egfr inhibitors. ||| huy ngoc pham ||| trung hoang le ||| 
2019 ||| outcome-driven clustering of acute coronary syndrome patients using multi-task neural network with attention. ||| eryu xia ||| xin du ||| jing mei ||| wen sun ||| suijun tong ||| zhiqing kang ||| jian sheng ||| jian li ||| changsheng ma ||| jianzeng dong ||| shaochun li ||| 
2021 ||| transformer-based map matching model with limited ground-truth data using transfer-learning approach. ||| zhixiong jin ||| seongjin choi ||| hwasoo yeo ||| 
2021 ||| transformers in vision: a survey. ||| salman h. khan ||| muzammal naseer ||| munawar hayat ||| syed waqas zamir ||| fahad shahbaz khan ||| mubarak shah ||| 
2021 ||| transformer-based language models for factoid question answering at bioasq9b. ||| urvashi khanna ||| diego moll ||| 
2021 ||| bio-inspired representation learning for visual attention prediction. ||| yuan yuan ||| hailong ning ||| xiaoqiang lu ||| 
2020 ||| cascaded semantic and positional self-attention network for document classification. ||| juyong jiang ||| jie zhang ||| kai zhang ||| 
2021 ||| encoder-decoder with multi-level attention for 3d human shape and pose estimation. ||| ziniu wan ||| zhengjia li ||| maoqing tian ||| jianbo liu ||| shuai yi ||| hongsheng li ||| 
2022 ||| inverted pyramid multi-task transformer for dense scene understanding. ||| hanrong ye ||| dan xu ||| 
2021 ||| multi-scale vision longformer: a new vision transformer for high-resolution image encoding. ||| pengchuan zhang ||| xiyang dai ||| jianwei yang ||| bin xiao ||| lu yuan ||| lei zhang ||| jianfeng gao ||| 
2021 ||| deeprare: generic unsupervised visual attention models. ||| phutphalla kong ||| matei mancas ||| bernard gosselin ||| kimtho po ||| 
2020 ||| an effective automatic image annotation model via attention model and data equilibrium. ||| amir vatani ||| milad taleby ahvanooey ||| mostafa rahimi ||| 
2022 ||| a transformer-based feature segmentation and region alignment method for uav-view geo-localization. ||| ming dai ||| jianhong hu ||| jiedong zhuang ||| enhui zheng ||| 
2022 ||| an end-to-end transformer model for crowd localization. ||| dingkang liang ||| wei xu ||| xiang bai ||| 
2018 ||| few-shot learning with attention-based sequence-to-sequence models. ||| bertrand higy ||| peter bell ||| 
2021 ||| ode transformer: an ordinary differential equation-inspired model for neural machine translation. ||| bei li ||| quan du ||| tao zhou ||| shuhan zhou ||| xin zeng ||| tong xiao ||| jingbo zhu ||| 
2017 ||| polar transformer networks. ||| carlos esteves ||| christine allen-blanchette ||| xiaowei zhou ||| kostas daniilidis ||| 
2018 ||| tri-axial self-attention for concurrent activity recognition. ||| yanyi zhang ||| xinyu li ||| kaixiang huang ||| yehan wang ||| shuhong chen ||| ivan marsic ||| 
2020 ||| sact: self-aware multi-space feature composition transformer for multinomial attention for video captioning. ||| chiranjib sur ||| 
2021 ||| connecting what to say with where to look by modeling human attention traces. ||| zihang meng ||| licheng yu ||| ning zhang ||| tamara l. berg ||| babak damavandi ||| vikas singh ||| amy bearman ||| 
2020 ||| the devil is in the details: evaluating limitations of transformer-based methods for granular tasks. ||| brihi joshi ||| neil shah ||| francesco barbieri ||| leonardo neves ||| 
2019 ||| bp-transformer: modelling long-range context via binary partitioning. ||| zihao ye ||| qipeng guo ||| quan gan ||| xipeng qiu ||| zheng zhang ||| 
2021 ||| small in-distribution changes in 3d perspective and lighting fool both cnns and transformers. ||| spandan madan ||| tomotake sasaki ||| tzu-mao li ||| xavier boix ||| hanspeter pfister ||| 
2022 ||| transformer based ensemble for emotion detection. ||| aditya kane ||| shantanu patankar ||| sahil khose ||| neeraja kirtane ||| 
2017 ||| from attention to participation: reviewing and modelling engagement with computers. ||| marc miquel-rib ||| 
2021 ||| audio-visual transformer based crowd counting. ||| usman sajid ||| xiangyu chen ||| hasan sajid ||| taejoon kim ||| guanghui wang ||| 
2019 ||| probing representations learned by multimodal recurrent and transformer models. ||| jindrich libovick ||| pranava madhyastha ||| 
2019 ||| smiles transformer: pre-trained molecular fingerprint for low data drug discovery. ||| shion honda ||| shoi shi ||| hiroki r. ueda ||| 
2021 ||| dual-branch attention-in-attention transformer for single-channel speech enhancement. ||| guochen yu ||| andong li ||| yutian wang ||| yinuo guo ||| hui wang ||| chengshi zheng ||| 
2021 ||| transformer models for text coherence assessment. ||| tushar abhishek ||| daksh rawat ||| manish gupta ||| vasudeva varma ||| 
2019 ||| a weakly-supervised attention-based visualization tool for assessing political affiliation. ||| srijith rajamohan ||| alana romanella ||| amit ramesh ||| 
2021 ||| towards accurate and compact architectures via neural architecture transformer. ||| yong guo ||| yin zheng ||| mingkui tan ||| qi chen ||| zhipeng li ||| jian chen ||| peilin zhao ||| junzhou huang ||| 
2017 ||| unwritten languages demand attention too! word discovery with encoder-decoder models. ||| marcely zanon boito ||| alexandre berard ||| aline villavicencio ||| laurent besacier ||| 
2020 ||| speech enhancement using self-adaptation and multi-head self-attention. ||| yuma koizumi ||| kohei yatabe ||| marc delcroix ||| yoshiki masuyama ||| daiki takeuchi ||| 
2021 ||| pose transformers (potr): human motion prediction with non-autoregressive transformers. ||| ngel mart ||| nez-gonz ||| lez ||| michael villamizar ||| jean-marc odobez ||| 
2021 ||| segmentation of lungs covid infected regions by attention mechanism and synthetic data. ||| parham yazdekhasty ||| ali zindari ||| zahra nabizadeh-shahrebabak ||| pejman khadivi ||| nader karimi ||| shadrokh samavi ||| 
2018 ||| cross-target stance classification with self-attention networks. ||| chang xu ||| c ||| cile paris ||| surya nepal ||| ross sparks ||| 
2018 ||| decoding-history-based adaptive control of attention for neural machine translation. ||| junyang lin ||| shuming ma ||| qi su ||| xu sun ||| 
2021 ||| blending anti-aliasing into vision transformer. ||| shengju qian ||| hao shao ||| yi zhu ||| mu li ||| jiaya jia ||| 
2021 ||| semask: semantically masked transformers for semantic segmentation. ||| jitesh jain ||| anukriti singh ||| nikita orlov ||| zilong huang ||| jiachen li ||| steven walton ||| humphrey shi ||| 
2021 ||| hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. ||| mingyu ding ||| xiaochen lian ||| linjie yang ||| peng wang ||| xiaojie jin ||| zhiwu lu ||| ping luo ||| 
2021 ||| pointer over attention: an improved bangla text summarization approach using hybrid pointer generator network. ||| nobel dhar ||| gaurob saha ||| prithwiraj bhattacharjee ||| avi mallick ||| md saiful islam ||| 
2020 ||| robust hierarchical graph classification with subgraph attention. ||| sambaran bandyopadhyay ||| manasvi aggarwal ||| m. narasimha murty ||| 
2021 ||| primer: searching for efficient transformers for language modeling. ||| david r. so ||| wojciech manke ||| hanxiao liu ||| zihang dai ||| noam shazeer ||| quoc v. le ||| 
2022 ||| singing-tacotron: global duration control attention and dynamic filter for end-to-end singing voice synthesis. ||| tao wang ||| ruibo fu ||| jiangyan yi ||| jianhua tao ||| zhengqi wen ||| 
2020 ||| molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. ||| mikolaj sacha ||| mikolaj blaz ||| piotr byrski ||| pawel wlodarczyk-pruszynski ||| stanislaw jastrzebski ||| 
2022 ||| transformers in medical image analysis: a review. ||| kelei he ||| chen gan ||| zhuoyuan li ||| islem rekik ||| zihao yin ||| wen ji ||| yang gao ||| qian wang ||| junfeng zhang ||| dinggang shen ||| 
2020 ||| evaluating german transformer language models with syntactic agreement tests. ||| karolina zaczynska ||| nils feldhus ||| robert schwarzenberg ||| aleksandra gabryszak ||| sebastian m ||| ller ||| 
2020 ||| ttvos: lightweight video object segmentation with adaptive template attention module and temporal consistency loss. ||| hyojin park ||| ganesh venkatesh ||| nojun kwak ||| 
2021 ||| efficient speech emotion recognition using multi-scale cnn and attention. ||| zixuan peng ||| yu lu ||| shengfeng pan ||| yunfeng liu ||| 
2022 ||| rxn hypergraph: a hypergraph attention model for chemical reaction representation. ||| mohammadamin tavakoli ||| alexander shmakov ||| francesco ceccarelli ||| pierre baldi ||| 
2017 ||| focusing attention: towards accurate text recognition in natural images. ||| zhanzhan cheng ||| fan bai ||| yunlu xu ||| gang zheng ||| shiliang pu ||| shuigeng zhou ||| 
2020 ||| hitter: hierarchical transformers for knowledge graph embeddings. ||| sanxing chen ||| xiaodong liu ||| jianfeng gao ||| jian jiao ||| ruofei zhang ||| yangfeng ji ||| 
2019 ||| dual attention mobdensenet(damdnet) for robust 3d face alignment. ||| lei jiang ||| xiao-jun wu ||| josef kittler ||| 
2019 ||| attention-aware answers of the crowd. ||| jingzheng tu ||| guoxian yu ||| jun wang ||| carlotta domeniconi ||| xiangliang zhang ||| 
2021 ||| multi-airport delay prediction with transformers. ||| liya wang ||| alex tien ||| jason chou ||| 
2021 ||| dependency learning for legal judgment prediction with a unified text-to-text transformer. ||| yunyun huang ||| xiaoyu shen ||| chuanyi li ||| jidong ge ||| bin luo ||| 
2020 ||| accenture at checkthat! 2020: if you say so: post-hoc fact-checking of claims using transformer-based models. ||| evan m. williams ||| paul rodrigues ||| valerie novak ||| 
2019 ||| self-attentional models for lattice inputs. ||| matthias sperber ||| graham neubig ||| ngoc-quan pham ||| alex waibel ||| 
2022 ||| uni4eye: unified 2d and 3d self-supervised pre-training via masked image modeling transformer for ophthalmic image classification. ||| zhiyuan cai ||| li lin ||| huaqing he ||| xiaoying tang ||| 
2020 ||| temporal event segmentation using attention-based perceptual prediction model for continual learning. ||| ramy mounir ||| roman gula ||| j ||| rn theuerkauf ||| sudeep sarkar ||| 
2019 ||| improving adversarial robustness via attention and adversarial logit pairing. ||| dou goodman ||| xingjian li ||| jun huan ||| tao wei ||| 
2018 ||| a3net: adversarial-and-attention network for machine reading comprehension. ||| jiuniu wang ||| xingyu fu ||| guangluan xu ||| yirong wu ||| ziyan chen ||| yang wei ||| li jin ||| 
2020 ||| the cascade transformer: an application for efficient answer sentence selection. ||| luca soldaini ||| alessandro moschitti ||| 
2021 ||| dynamic head: unifying object detection heads with attentions. ||| xiyang dai ||| yinpeng chen ||| bin xiao ||| dongdong chen ||| mengchen liu ||| lu yuan ||| lei zhang ||| 
2022 ||| transformer quality in linear time. ||| weizhe hua ||| zihang dai ||| hanxiao liu ||| quoc v. le ||| 
2020 ||| kalman filtering attention for user behavior modeling in ctr prediction. ||| hu liu ||| jing lu ||| xiwei zhao ||| sulong xu ||| hao peng ||| yutong liu ||| zehua zhang ||| jian li ||| junsheng jin ||| yongjun bao ||| weipeng yan ||| 
2017 ||| efficient attention using a fixed-size memory representation. ||| denny britz ||| melody y. guan ||| minh-thang luong ||| 
2020 ||| modality shifting attention network for multi-modal video question answering. ||| junyeong kim ||| minuk ma ||| trung x. pham ||| kyungsu kim ||| chang d. yoo ||| 
2021 ||| feanet: feature-enhanced attention network for rgb-thermal real-time semantic segmentation. ||| fuqin deng ||| hua feng ||| mingjian liang ||| hongmin wang ||| yong yang ||| yuan gao ||| junfeng chen ||| junjie hu ||| xiyue guo ||| tin lun lam ||| 
2019 ||| fully quantized transformer for improved translation. ||| gabriele prato ||| ella charlaix ||| mehdi rezagholizadeh ||| 
2019 ||| bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. ||| guan-lin chao ||| ian r. lane ||| 
2022 ||| swin unetr: swin transformers for semantic segmentation of brain tumors in mri images. ||| ali hatamizadeh ||| vishwesh nath ||| yucheng tang ||| dong yang ||| holger roth ||| daguang xu ||| 
2020 ||| text-to-image generation with attention based recurrent neural networks. ||| tehseen zia ||| shahan arif ||| shakeeb murtaza ||| mirza ahsan ullah ||| 
2020 ||| dapnet: a double self-attention convolutional network for segmentation of point clouds. ||| li chen ||| zewei xu ||| yongjian fu ||| haozhe huang ||| shaowen wang ||| haifeng li ||| 
2021 ||| classifying tweet sentiment using the hidden state and attention matrix of a fine-tuned bertweet model. ||| tommaso macr ||| freya murphy ||| yunfan zou ||| yves zumbach ||| 
2021 ||| video transformer for deepfake detection with incremental learning. ||| sohail ahmed khan ||| hang dai ||| 
2022 ||| plumeria at semeval-2022 task 6: robust approaches for sarcasm detection for english and arabic using transformers and data augmentation. ||| shubham kumar nigam ||| mosab shaheen ||| 
2019 ||| why attention? analyzing and remedying bilstm deficiency in modeling cross-context for ner. ||| 
2019 ||| linestofacephoto: face photo generation from lines with conditional self-attention generative adversarial network. ||| yuhang li ||| xuejin chen ||| feng wu ||| zheng-jun zha ||| 
2021 ||| semi-supervised medical image segmentation via cross teaching between cnn and transformer. ||| xiangde luo ||| minhao hu ||| tao song ||| guotai wang ||| shaoting zhang ||| 
2021 ||| geometric transformer for end-to-end molecule properties prediction. ||| yoni choukroun ||| lior wolf ||| 
2019 ||| multi-grained attention networks for single image super-resolution. ||| huapeng wu ||| zhengxia zou ||| jie gui ||| wen-jun zeng ||| jieping ye ||| jun zhang ||| hongyi liu ||| zhihui wei ||| 
2019 ||| the bottom-up evolution of representations in the transformer: a study with machine translation and language modeling objectives. ||| elena voita ||| rico sennrich ||| ivan titov ||| 
2020 ||| dilated-scale-aware attention convnet for multi-class object counting. ||| wei xu ||| dingkang liang ||| yixiao zheng ||| zhanyu ma ||| 
2022 ||| actionformer: localizing moments of actions with transformers. ||| chen-lin zhang ||| jianxin wu ||| yin li ||| 
2017 ||| "attention" for detecting unreliable news in the information age. ||| venkatesh duppada ||| 
2021 ||| learning a 3d-cnn and transformer prior for hyperspectral image super-resolution. ||| qing ma ||| junjun jiang ||| xianming liu ||| jiayi ma ||| 
2020 ||| sign language translation with transformers. ||| kayo yin ||| 
2017 ||| setting an attention region for convolutional neural networks using region selective features, for recognition of materials within glass vessels. ||| sagi eppel ||| 
2021 ||| demystifying local vision transformer: sparse connectivity, weight sharing, and dynamic weight. ||| qi han ||| zejia fan ||| qi dai ||| lei sun ||| ming-ming cheng ||| jiaying liu ||| jingdong wang ||| 
2018 ||| visual attention model for cross-sectional stock return prediction and end-to-end multimodal market representation learning. ||| ran zhao ||| yuntian deng ||| mark dredze ||| arun verma ||| david s. rosenberg ||| amanda stent ||| 
2020 ||| transformer-based models for automatic identification of argument relations: a cross-domain evaluation. ||| ramon ruiz-dolz ||| stella heras ||| jos |||  alemany ||| ana garc ||| a-fornes ||| 
2019 ||| stand-alone self-attention in vision models. ||| prajit ramachandran ||| niki parmar ||| ashish vaswani ||| irwan bello ||| anselm levskaya ||| jonathon shlens ||| 
2020 ||| segmentation with residual attention u-net and an edge-enhancement approach preserves cell shape features. ||| nanyan zhu ||| chen liu ||| zakary s. singer ||| tal danino ||| andrew f. laine ||| jia guo ||| 
2022 ||| self-supervised video-centralised transformer for video face clustering. ||| yujiang wang ||| mingzhi dong ||| jie shen ||| yiming luo ||| yiming lin ||| pingchuan ma ||| stavros petridis ||| maja pantic ||| 
2021 ||| robust and precise facial landmark detection by self-calibrated pose attention network. ||| jun wan ||| hui xi ||| jie zhou ||| zhihui lai ||| witold pedrycz ||| xu wang ||| hang sun ||| 
2020 ||| hierarchical residual attention network for single image super-resolution. ||| parichehr behjati ||| pau rodr ||| guez ||| armin mehri ||| isabelle hupont ||| carles fern ||| ndez tena ||| jordi gonz ||| lez ||| 
2019 ||| attention is not not explanation. ||| sarah wiegreffe ||| yuval pinter ||| 
2020 ||| exploring self-attention for visual odometry. ||| hamed damirchi ||| rooholla khorrambakht ||| hamid d. taghirad ||| 
2021 ||| u-shaped transformer with frequency-band aware attention for speech enhancement. ||| yi li ||| yang sun ||| syed mohsen naqvi ||| 
2021 ||| memory-efficient differentiable transformer architecture search. ||| yuekai zhao ||| li dong ||| yelong shen ||| zhihua zhang ||| furu wei ||| weizhu chen ||| 
2021 ||| transformer-s2a: robust and efficient speech-to-animation. ||| liyang chen ||| zhiyong wu ||| jun ling ||| runnan li ||| xu tan ||| sheng zhao ||| 
2019 ||| acfnet: attentional class feature network for semantic segmentation. ||| fan zhang ||| yanqin chen ||| zhihang li ||| zhibin hong ||| jingtuo liu ||| feifei ma ||| junyu han ||| errui ding ||| 
2017 ||| multi-modal factorized bilinear pooling with co-attention learning for visual question answering. ||| zhou yu ||| jun yu ||| jianping fan ||| dacheng tao ||| 
2020 ||| self-attention enhanced patient journey understanding in healthcare system. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| 
2019 ||| the unreasonable effectiveness of transformer language models in grammatical error correction. ||| dimitris alikaniotis ||| vipul raheja ||| 
2020 ||| simultaneous paraphrasing and translation by fine-tuning transformer models. ||| rakesh chada ||| 
2021 ||| cross attention-guided dense network for images fusion. ||| zhengwen shen ||| jun wang ||| zaiyu pan ||| yulian li ||| jiangyu wang ||| 
2022 ||| poseur: direct human pose regression with transformers. ||| weian mao ||| yongtao ge ||| chunhua shen ||| zhi tian ||| xinlong wang ||| zhibin wang ||| anton van den hengel ||| 
2021 ||| waveglove: transformer-based hand gesture recognition using multiple inertial sensors. ||| matej kr ||| lik ||| marek suppa ||| 
2021 ||| transfusion: cross-view fusion with transformer for 3d human pose estimation. ||| haoyu ma ||| liangjian chen ||| deying kong ||| zhe wang ||| xingwei liu ||| hao tang ||| xiangyi yan ||| yusheng xie ||| shih-yao lin ||| xiaohui xie ||| 
2021 ||| sparse is enough in scaling transformers. ||| sebastian jaszczur ||| aakanksha chowdhery ||| afroz mohiuddin ||| lukasz kaiser ||| wojciech gajewski ||| henryk michalewski ||| jonni kanerva ||| 
2019 ||| audio-attention discriminative language model for asr rescoring. ||| ankur gandhe ||| ariya rastrow ||| 
2021 ||| video transformer network. ||| daniel neimark ||| omri bar ||| maya zohar ||| dotan asselmann ||| 
2020 ||| a graph attention based approach for trajectory prediction in multi-agent sports games. ||| ding ding ||| h. howie huang ||| 
2020 ||| hard-coded gaussian attention for neural machine translation. ||| weiqiu you ||| simeng sun ||| mohit iyyer ||| 
2021 ||| pe-former: pose estimation transformer. ||| paschalis panteleris ||| antonis a. argyros ||| 
2017 ||| monotonic chunkwise attention. ||| chung-cheng chiu ||| colin raffel ||| 
2018 ||| modeling task effects in human reading with neural attention. ||| michael hahn ||| frank keller ||| 
2019 ||| synchronous transformers for end-to-end speech recognition. ||| zhengkun tian ||| jiangyan yi ||| ye bai ||| jianhua tao ||| shuai zhang ||| zhengqi wen ||| 
2022 ||| interpretable and generalizable graph learning via stochastic attention mechanism. ||| siqi miao ||| miaoyuan liu ||| pan li ||| 
2022 ||| minicons: enabling flexible behavioral and representational analyses of transformer language models. ||| kanishka misra ||| 
2017 ||| 3d morphable models as spatial transformer networks. ||| anil bas ||| patrik huber ||| william a. p. smith ||| muhammad awais ||| josef kittler ||| 
2020 ||| guiding attention for self-supervised learning with transformers. ||| ameet deshpande ||| karthik narasimhan ||| 
2021 ||| legal transformer models may not always help. ||| saibo geng ||| r ||| mi lebret ||| karl aberer ||| 
2021 ||| atp-net: an attention-based ternary projection network for compressed sensing. ||| guanxiong nie ||| yajian zhou ||| 
2020 ||| sg-net: syntax guided transformer for language representation. ||| zhuosheng zhang ||| yuwei wu ||| junru zhou ||| sufeng duan ||| hai zhao ||| rui wang ||| 
2021 ||| attention-guided progressive mapping for profile face recognition. ||| junyang huang ||| changxing ding ||| 
2021 ||| abd-net: attention based decomposition network for 3d point cloud decomposition. ||| siddharth katageri ||| shashidhar veerappa kudari ||| akshaykumar gunari ||| ramesh ashok tabib ||| uma mudenagudi ||| 
2021 ||| multiscale vision transformers. ||| haoqi fan ||| bo xiong ||| karttikeya mangalam ||| yanghao li ||| zhicheng yan ||| jitendra malik ||| christoph feichtenhofer ||| 
2021 ||| evo-vit: slow-fast token evolution for dynamic vision transformer. ||| yifan xu ||| zhijie zhang ||| mengdan zhang ||| kekai sheng ||| ke li ||| weiming dong ||| liqing zhang ||| changsheng xu ||| xing sun ||| 
2019 ||| kgat: knowledge graph attention network for recommendation. ||| xiang wang ||| xiangnan he ||| yixin cao ||| meng liu ||| tat-seng chua ||| 
2021 ||| classification of multivariate weakly-labelled time-series with attention. ||| surayez rahman ||| chang wei tan ||| 
2021 ||| multimodal personality recognition using cross-attention transformer and behaviour encoding. ||| tanay agrawal ||| dhruv agarwal ||| michal balazia ||| neelabh sinha ||| fran ||| ois br ||| mond ||| 
2021 ||| attention guided dialogue state tracking with sparse supervision. ||| shuailong liang ||| lahari poddar ||| gyuri szarvas ||| 
2020 ||| neural machine translation system of indic languages - an attention based approach. ||| parth shah ||| vishvajit bakrola ||| 
2018 ||| multilingual neural machine translation with task-specific attention. ||| graeme w. blackwood ||| miguel ballesteros ||| todd ward ||| 
2021 ||| bilateral cross-modality graph matching attention for feature fusion in visual question answering. ||| jianjian cao ||| xiameng qin ||| sanyuan zhao ||| jianbing shen ||| 
2020 ||| attention-based image upsampling. ||| souvik kundu ||| hesham mostafa ||| sharath nittur sridhar ||| sairam sundaresan ||| 
2021 ||| sparse attention with linear units. ||| biao zhang ||| ivan titov ||| rico sennrich ||| 
2021 ||| attention-based domain adaptation for time series forecasting. ||| xiaoyong jin ||| youngsuk park ||| danielle c. maddix ||| yuyang wang ||| xifeng yan ||| 
2021 ||| attention augmented convolutional transformer for tabular time-series. ||| sharath m. shankaranarayana ||| davor runje ||| 
2019 ||| attention-based lane change prediction. ||| oliver scheel ||| naveen shankar nagaraja ||| loren arthur schwarz ||| nassir navab ||| federico tombari ||| 
2021 ||| simpler is better: few-shot semantic segmentation with classifier weight transformer. ||| zhihe lu ||| sen he ||| xiatian zhu ||| li zhang ||| yi-zhe song ||| tao xiang ||| 
2022 ||| attention-based contextual multi-view graph convolutional networks for short-term population prediction. ||| yuki kubota ||| yuki ohira ||| tetsuo shimizu ||| 
2020 ||| inductive document network embedding with topic-word attention. ||| robin brochier ||| adrien guille ||| julien velcin ||| 
2021 ||| sequential recommendation with bidirectional chronological augmentation of transformer. ||| juyong jiang ||| yingtao luo ||| jae boum kim ||| kai zhang ||| sunghun kim ||| 
2021 ||| bottleneck transformers for visual recognition. ||| aravind srinivas ||| tsung-yi lin ||| niki parmar ||| jonathon shlens ||| pieter abbeel ||| ashish vaswani ||| 
2022 ||| attention enables zero approximation error. ||| zhiying fang ||| yidong ouyang ||| ding-xuan zhou ||| guang cheng ||| 
2018 ||| overcoming catastrophic forgetting with hard attention to the task. ||| joan serr ||| d ||| dac sur ||| s ||| marius miron ||| alexandros karatzoglou ||| 
2021 ||| a bilingual, openworld video text dataset and end-to-end video text spotter with transformer. ||| weijia wu ||| yuanqiang cai ||| debing zhang ||| sibo wang ||| zhuang li ||| jiahong li ||| yejun tang ||| hong zhou ||| 
2019 ||| attributed graph clustering: a deep attentional embedding approach. ||| chun wang ||| shirui pan ||| ruiqi hu ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2021 ||| structured co-reference graph attention for video-grounded dialogue. ||| junyeong kim ||| sunjae yoon ||| dahyun kim ||| chang d. yoo ||| 
2019 ||| solving math word problems with double-decoder transformer. ||| yuanliang meng ||| anna rumshisky ||| 
2020 ||| covid-transformer: detecting covid-19 trending topics on twitter using universal sentence encoder. ||| meysam asgari-chenaghlu ||| narjes nikzad-khasmakhi ||| shervin minaee ||| 
2021 ||| attention-driven read-aloud technology increases reading comprehension in children with reading disabilities. ||| gianluca schiavo ||| nadia mana ||| ornella mich ||| massimo zancanaro ||| remo job ||| 
2019 ||| graph-based knowledge distillation by multi-head attention network. ||| seung hyun lee ||| byung cheol song ||| 
2021 ||| reltransformer: balancing the visual relationship detection from local context, scene and memory. ||| jun chen ||| aniket agarwal ||| sherif abdelkarim ||| deyao zhu ||| mohamed elhoseiny ||| 
2019 ||| r-transformer: recurrent neural network enhanced transformer. ||| zhiwei wang ||| yao ma ||| zitao liu ||| jiliang tang ||| 
2021 ||| deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. ||| jinxian wang ||| xuejun liu ||| siyuan shen ||| lei deng ||| hui liu ||| 
2020 ||| do we really need that many parameters in transformer for extractive summarization? discourse can help ! ||| wen xiao ||| patrick huber ||| giuseppe carenini ||| 
2018 ||| leveraging financial news for stock trend prediction with attention-based recurrent neural network. ||| huicheng liu ||| 
2019 ||| enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. ||| shiyang li ||| xiaoyong jin ||| yao xuan ||| xiyou zhou ||| wenhu chen ||| yu-xiang wang ||| xifeng yan ||| 
2020 ||| $-i relation of single phase power transformer core by using on-line measurable parameters. ||| farhad gh. khodaei ||| 
2021 ||| few-shot fine-grained action recognition via bidirectional attention and contrastive meta-learning. ||| jiahao wang ||| yunhong wang ||| sheng liu ||| annan li ||| 
2017 ||| hierarchical multi-scale attention networks for action recognition. ||| shiyang yan ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2020 ||| pre-trained image processing transformer. ||| hanting chen ||| yunhe wang ||| tianyu guo ||| chang xu ||| yiping deng ||| zhenhua liu ||| siwei ma ||| chunjing xu ||| chao xu ||| wen gao ||| 
2022 ||| vision transformer compression with structured pruning and low rank approximation. ||| ankur kumar ||| 
2018 ||| learning visual question answering by bootstrapping hard attention. ||| mateusz malinowski ||| carl doersch ||| adam santoro ||| peter w. battaglia ||| 
2019 ||| faclstm: convlstm with focused attention for scene text recognition. ||| qingqing wang ||| wenjing jia ||| xiangjian he ||| yue lu ||| michael blumenstein ||| ye huang ||| 
2019 ||| hierarchical attentional hybrid neural networks for document classification. ||| jader abreu ||| luis fred ||| david mac ||| do ||| cleber zanchettin ||| 
2022 ||| sequential recommendation via stochastic self-attention. ||| ziwei fan ||| zhiwei liu ||| yu wang ||| alice wang ||| zahra nazari ||| lei zheng ||| hao peng ||| philip s. yu ||| 
2017 ||| recurrent neural network-based sentence encoder with gated attention for natural language inference. ||| qian chen ||| xiaodan zhu ||| zhen-hua ling ||| si wei ||| hui jiang ||| diana inkpen ||| 
2020 ||| attention! a lightweight 2d hand pose estimation approach. ||| nicholas santavas ||| ioannis kansizoglou ||| loukas bampis ||| evangelos g. karakasis ||| antonios gasteratos ||| 
2020 ||| tinyspeech: attention condensers for deep speech recognition neural networks on edge devices. ||| alexander wong ||| mahmoud famouri ||| maya pavlova ||| siddharth surana ||| 
2021 ||| from multimodal to unimodal attention in transformers using knowledge distillation. ||| dhruv agarwal ||| tanay agrawal ||| laura m. ferrari ||| fran ||| ois br ||| mond ||| 
2021 ||| latte: lstm self-attention based anomaly detection in embedded automotive platforms. ||| vipin kumar kukkala ||| sooryaa vignesh thiruloga ||| sudeep pasricha ||| 
2022 ||| locate: end-to-end localization of actions in 3d with transformers. ||| jiankai sun ||| bolei zhou ||| michael j. black ||| arjun chandrasekaran ||| 
2018 ||| dynamic graph representation learning via self-attention networks. ||| aravind sankar ||| yanhong wu ||| liang gou ||| wei zhang ||| hao yang ||| 
2019 ||| are transformers universal approximators of sequence-to-sequence functions? ||| chulhee yun ||| srinadh bhojanapalli ||| ankit singh rawat ||| sashank j. reddi ||| sanjiv kumar ||| 
2021 ||| the layout generation algorithm of graphic design based on transformer-cvae. ||| mengxi guo ||| dangqing huang ||| xiaodong xie ||| 
2021 ||| a dynamic spatial-temporal attention network for early anticipation of traffic accidents. ||| muhammad monjurul karim ||| yu li ||| ruwen qin ||| zhaozheng yin ||| 
2018 ||| supersaliency: predicting smooth pursuit-based attention with slicing cnns improves fixation prediction for naturalistic videos. ||| mikhail startsev ||| michael dorr ||| 
2021 ||| when attention meets fast recurrence: training language models with reduced compute. ||| tao lei ||| 
2021 ||| hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition. ||| nhat truong pham ||| duc ngoc minh dang ||| sy dzung nguyen ||| 
2019 ||| formula transformers and combinatorial test generators for propositional intuitionistic theorem provers. ||| paul tarau ||| 
2020 ||| multi-modal, multi-task, multi-attention (m3) deep learning detection of reticular pseudodrusen: towards automated and accessible classification of age-related macular degeneration. ||| qingyu chen ||| tiarnan d. l. keenan ||| alexis allot ||| yifan peng ||| elvira agr ||| n ||| amitha domalpally ||| caroline c. w. klaver ||| daniel t. luttikhuizen ||| marcus h. colyer ||| catherine a. cukras ||| henry e. wiley ||| m. teresa magone ||| chantal cousineau-krieger ||| wai t. wong ||| yingying zhu ||| emily y. chew ||| zhiyong lu ||| 
2017 ||| attention-based wav2text with feature transfer learning. ||| andros tjandra ||| sakriani sakti ||| satoshi nakamura ||| 
2021 ||| malware analysis with artificial intelligence and a particular attention on results interpretability. ||| benjamin marais ||| tony quertier ||| christophe chesneau ||| 
2021 ||| a multi-size neural network with attention mechanism for answer selection. ||| jie huang ||| 
2020 ||| self-supervised learning with cross-modal transformers for emotion recognition. ||| aparna khare ||| srinivas parthasarathy ||| shiva sundaram ||| 
2020 ||| sentiment analysis with contextual embeddings and self-attention. ||| katarzyna biesialska ||| magdalena biesialska ||| henryk rybinski ||| 
2021 ||| latr: layout-aware transformer for scene-text vqa. ||| ali furkan biten ||| ron litman ||| yusheng xie ||| srikar appalaraju ||| r. manmatha ||| 
2018 ||| pay attention to virality: understanding popularity of social media videos with the attention mechanism. ||| adam bielski ||| tomasz trzcinski ||| 
2021 ||| learning to ignore: rethinking attention in cnns. ||| firas laakom ||| kateryna chumachenko ||| jenni raitoharju ||| alexandros iosifidis ||| moncef gabbouj ||| 
2018 ||| fast directional self-attention mechanism. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| chengqi zhang ||| 
2021 ||| gtae: graph-transformer based auto-encoders for linguistic-constrained text style transfer. ||| yukai shi ||| sen zhang ||| chenxing zhou ||| xiaodan liang ||| xiaojun yang ||| liang lin ||| 
2021 ||| tedge-caching: transformer-based edge caching towards 6g networks. ||| zohreh hajiakhondi-meybodi ||| arash mohammadi ||| elahe rahimian ||| shahin heidarian ||| jamshid abouei ||| konstantinos n. plataniotis ||| 
2020 ||| exploring fluent query reformulations with text-to-text transformers and reinforcement learning. ||| jerry zikun chen ||| shi yu ||| haoran wang ||| 
2018 ||| structured attention guided convolutional neural fields for monocular depth estimation. ||| dan xu ||| wei wang ||| hao tang ||| hong liu ||| nicu sebe ||| elisa ricci ||| 
2021 ||| perceiver: general perception with iterative attention. ||| andrew jaegle ||| felix gimeno ||| andrew brock ||| andrew zisserman ||| oriol vinyals ||| jo ||| o carreira ||| 
2020 ||| single image super-resolution via a holistic attention network. ||| ben niu ||| weilei wen ||| wenqi ren ||| xiangde zhang ||| lianping yang ||| shuzhen wang ||| kaihao zhang ||| xiaochun cao ||| haifeng shen ||| 
2020 ||| extractive opinion summarization in quantized transformer spaces. ||| stefanos angelidis ||| reinald kim amplayo ||| yoshihiko suhara ||| xiaolan wang ||| mirella lapata ||| 
2021 ||| detectornet: transformer-enhanced spatial temporal graph neural network for traffic prediction. ||| he li ||| shiyu zhang ||| xuejiao li ||| liangcai su ||| hongjie huang ||| duo jin ||| linghao chen ||| jianbin huang ||| jaesoo yoo ||| 
2018 ||| investigation of enhanced tacotron text-to-speech synthesis systems with self-attention for pitch accent language. ||| yusuke yasuda ||| xin wang ||| shinji takaki ||| junichi yamagishi ||| 
2021 ||| realtime global attention network for semantic segmentation. ||| xi mo ||| xiangyu chen ||| 
2021 ||| point cloud transformers applied to collider physics. ||| vinicius mikuni ||| florencia canelli ||| 
2021 ||| cross modification attention based deliberation model for image captioning. ||| zheng lian ||| yanan zhang ||| haichang li ||| rui wang ||| xiaohui hu ||| 
2020 ||| red dragon ai at textgraphs 2020 shared task: lit : lstm-interleaved transformer for multi-hop explanation ranking. ||| yew ken chia ||| sam witteveen ||| martin andrews ||| 
2017 ||| efficiently applying attention to sequential data with the recurrent discounted attention unit. ||| brendan maginnis ||| pierre h. richemond ||| 
2021 ||| fanet: a feedback attention network for improved biomedical image segmentation. ||| nikhil kumar tomar ||| debesh jha ||| michael a. riegler ||| h ||| vard d. johansen ||| dag johansen ||| jens rittscher ||| p ||| l halvorsen ||| sharib ali ||| 
2020 ||| cloud transformers. ||| kirill mazur ||| victor lempitsky ||| 
2020 ||| affect expression behaviour analysis in the wild using spatio-channel attention and complementary context information. ||| darshan gera ||| s. balasubramanian ||| 
2021 ||| ats: adaptive token sampling for efficient vision transformers. ||| mohsen fayyaz ||| soroush abbasi koohpayegani ||| farnoush rezaei jafari ||| eric sommerlade ||| hamid reza vaezi joze ||| hamed pirsiavash ||| juergen gall ||| 
2021 ||| grounding spatio-temporal language with transformers. ||| tristan karch ||| laetitia teodorescu ||| katja hofmann ||| cl ||| ment moulin-frier ||| pierre-yves oudeyer ||| 
2021 ||| multi-modal temporal attention models for crop mapping from satellite time series. ||| vivien sainte fare garnot ||| lo ||| c landrieu ||| nesrine chehata ||| 
2021 ||| audio captioning transformer. ||| xinhao mei ||| xubo liu ||| qiushi huang ||| mark d. plumbley ||| wenwu wang ||| 
2019 ||| learning manipulation skills via hierarchical spatial attention. ||| marcus gualtieri ||| robert platt jr. ||| 
2021 ||| cross-modal self-attention with multi-task pre-training for medical visual question answering. ||| haifan gong ||| guanqi chen ||| sishuo liu ||| yizhou yu ||| guanbin li ||| 
2021 ||| turkish text classification: from lexicon analysis to bidirectional transformer. ||| deniz kavi ||| 
2020 ||| object-centric learning with slot attention. ||| francesco locatello ||| dirk weissenborn ||| thomas unterthiner ||| aravindh mahendran ||| georg heigold ||| jakob uszkoreit ||| alexey dosovitskiy ||| thomas kipf ||| 
2021 ||| ga-net: global attention network for point cloud semantic segmentation. ||| shuang deng ||| qiulei dong ||| 
2018 ||| fast efficient object detection using selective attention. ||| 
2018 ||| attention models in graphs: a survey. ||| john boaz lee ||| ryan a. rossi ||| sungchul kim ||| nesreen k. ahmed ||| eunyee koh ||| 
2019 ||| speech-xlnet: unsupervised acoustic model pretraining for self-attention networks. ||| xingcheng song ||| guangsen wang ||| zhiyong wu ||| yiheng huang ||| dan su ||| dong yu ||| helen meng ||| 
2018 ||| attention is all we need: nailing down object-centric attention for egocentric activity recognition. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2021 ||| graph attention networks with positional embeddings. ||| liheng ma ||| reihaneh rabbany ||| adriana romero-soriano ||| 
2021 ||| transmvsnet: global context-aware multi-view stereo network with transformers. ||| yikang ding ||| wentao yuan ||| qingtian zhu ||| haotian zhang ||| xiangyue liu ||| yuanjiang wang ||| xiao liu ||| 
2021 ||| generating bug-fixes using pretrained transformers. ||| dawn drain ||| chen wu ||| alexey svyatkovskiy ||| neel sundaresan ||| 
2021 ||| videolightformer: lightweight action recognition using transformers. ||| raivo e. koot ||| haiping lu ||| 
2019 ||| attention is (not) all you need for commonsense reasoning. ||| tassilo klein ||| moin nabi ||| 
2020 ||| how effective is task-agnostic data augmentation for pretrained transformers? ||| shayne longpre ||| yu wang ||| christopher dubois ||| 
2019 ||| attention-driven tree-structured convolutional lstm for high dimensional data understanding. ||| bin kong ||| xin wang ||| junjie bai ||| yi lu ||| feng gao ||| kunlin cao ||| qi song ||| shaoting zhang ||| siwei lyu ||| youbing yin ||| 
2020 ||| comparison of attention-based deep learning models for eeg classification. ||| giulia cisotto ||| alessio zanga ||| joanna chlebus ||| italo zoppis ||| sara manzoni ||| urszula markowska-kaczmar ||| 
2020 ||| semantic layout manipulation with high-resolution sparse attention. ||| haitian zheng ||| zhe lin ||| jingwan lu ||| scott cohen ||| jianming zhang ||| ning xu ||| jiebo luo ||| 
2018 ||| ntua-slp at semeval-2018 task 2: predicting emojis using rnns with context-aware attention. ||| christos baziotis ||| nikos athanasiou ||| georgios paraskevopoulos ||| nikolaos ellinas ||| athanasia kolovou ||| alexandros potamianos ||| 
2022 ||| self-attention fusion for audiovisual emotion recognition with incomplete data. ||| kateryna chumachenko ||| alexandros iosifidis ||| moncef gabbouj ||| 
2021 ||| miti-detr: object detection based on transformers with mitigatory self-attention convergence. ||| wenchi ma ||| tianxiao zhang ||| guanghui wang ||| 
2017 ||| attention-based information fusion using multi-encoder-decoder recurrent neural networks. ||| stephan baier ||| sigurd spieckermann ||| volker tresp ||| 
2021 ||| utnet: a hybrid transformer architecture for medical image segmentation. ||| yunhe gao ||| mu zhou ||| dimitris n. metaxas ||| 
2020 ||| persian ezafe recognition using transformers and its role in part-of-speech tagging. ||| ehsan doostmohammadi ||| minoo nassajian ||| adel rahimi ||| 
2017 ||| attention-based natural language person retrieval. ||| tao zhou ||| muhao chen ||| jie yu ||| demetri terzopoulos ||| 
2021 ||| transmot: spatial-temporal graph transformer for multiple object tracking. ||| peng chu ||| jiang wang ||| quanzeng you ||| haibin ling ||| zicheng liu ||| 
2020 ||| multi-modal transformer for video retrieval. ||| valentin gabeur ||| chen sun ||| karteek alahari ||| cordelia schmid ||| 
2022 ||| attention-based region of interest (roi) detection for speech emotion recognition. ||| jay desai ||| houwei cao ||| ravi shah ||| 
2022 ||| multi-direction and multi-scale pyramid in transformer for video-based pedestrian retrieval. ||| xianghao zang ||| ge li ||| wei gao ||| 
2020 ||| latent video transformer. ||| ruslan rakhimov ||| denis volkhonskiy ||| alexey artemov ||| denis zorin ||| evgeny burnaev ||| 
2019 ||| a hierarchical attention based seq2seq model for chinese lyrics generation. ||| haoshen fan ||| jie wang ||| bojin zhuang ||| shaojun wang ||| jing xiao ||| 
2021 ||| ts-cam: token semantic coupled attention map for weakly supervised object localization. ||| wei gao ||| fang wan ||| xingjia pan ||| zhiliang peng ||| qi tian ||| zhenjun han ||| bolei zhou ||| qixiang ye ||| 
2019 ||| two computational models for analyzing political attention in social media. ||| libby hemphill ||| angela m. sch ||| pke-gonzalez ||| 
2020 ||| solid-state inrush current limiter controller based on inrush prediction for large transformers. ||| nima tashakor ||| teymoor ghanbari ||| ebrahim farjah ||| ehsan bagheri ||| stefan m. goetz ||| 
2022 ||| characterizing renal structures with 3d block aggregate transformers. ||| xin yu ||| yucheng tang ||| yinchi zhou ||| riqiang gao ||| qi yang ||| ho hin lee ||| thomas li ||| shunxing bao ||| yuankai huo ||| zhoubing xu ||| thomas a. lasko ||| richard g. abramson ||| bennett a. landman ||| 
2021 ||| adaptation and attention for neural video coding. ||| nannan zou ||| honglei zhang ||| francesco cricri ||| ramin ghaznavi youvalari ||| hamed r. tavakoli ||| jani lainema ||| emre aksu ||| miska m. hannuksela ||| esa rahtu ||| 
2018 ||| deeply learning molecular structure-property relationships using graph attention neural network. ||| seongok ryu ||| jaechang lim ||| woo youn kim ||| 
2021 ||| unsupervised domain-specific deblurring using scale-specific attention. ||| praveen kandula ||| a. n. rajagopalan ||| 
2021 ||| efficient attention branch network with combined loss function for automatic speaker verification spoof detection. ||| amir mohammad rostami ||| mohammad mehdi homayounpour ||| ahmad nickabadi ||| 
2017 ||| dataset construction via attention for aspect term extraction with distant supervision. ||| athanasios giannakopoulos ||| diego antognini ||| claudiu musat ||| andreea hossmann ||| michael baeriswyl ||| 
2020 ||| an autoencoder wavelet based deep neural network with attention mechanism for multistep prediction of plant growth. ||| bashar alhnaity ||| stefanos d. kollias ||| georgios leontidis ||| shouyong jiang ||| bert schamp ||| simon pearson ||| 
2020 ||| covariance self-attention dual path unet for rectal tumor segmentation. ||| haijun gao ||| bochuan zheng ||| dazhi pan ||| xiangyin zeng ||| 
2021 ||| transcenter: transformers with dense queries for multiple-object tracking. ||| yihong xu ||| yutong ban ||| guillaume delorme ||| chuang gan ||| daniela rus ||| xavier alameda-pineda ||| 
2020 ||| spatially aware multimodal transformers for textvqa. ||| yash kant ||| dhruv batra ||| peter anderson ||| alexander g. schwing ||| devi parikh ||| jiasen lu ||| harsh agrawal ||| 
2019 ||| image-and-spatial transformer networks for structure-guided image registration. ||| matthew c. h. lee ||| ozan oktay ||| andreas schuh ||| michiel schaap ||| ben glocker ||| 
2021 ||| isomer: transfer enhanced dual-channel heterogeneous dependency attention network for aspect-based sentiment classification. ||| yukun cao ||| yijia tang ||| ziyue wei ||| chengkun jin ||| zeyu miao ||| yixin fang ||| haizhou du ||| feifei xu ||| 
2021 ||| visual navigation with spatial attention. ||| bar mayo ||| tamir hazan ||| ayellet tal ||| 
2021 ||| enhancing cross-sectional currency strategies by ranking refinement with transformer-based architectures. ||| daniel poh ||| bryan lim ||| stefan zohren ||| stephen j. roberts ||| 
2021 ||| deep co-attention network for multi-view subspace learning. ||| lecheng zheng ||| yu cheng ||| hongxia yang ||| nan cao ||| jingrui he ||| 
2019 ||| cawa: an attention-network for credit attribution. ||| saurav manchanda ||| george karypis ||| 
2018 ||| topical stance detection for twitter: a two-phase lstm model using attention. ||| kuntal dey ||| ritvik shrivastava ||| saroj kaushik ||| 
2021 ||| t-automl: automated machine learning for lesion segmentation using transformers in 3d medical imaging. ||| dong yang ||| andriy myronenko ||| xiaosong wang ||| ziyue xu ||| holger r. roth ||| daguang xu ||| 
2018 ||| self-erasing network for integral object attention. ||| qibin hou ||| peng-tao jiang ||| yunchao wei ||| ming-ming cheng ||| 
2021 ||| attention-free keyword spotting. ||| mashrur m. morshed ||| ahmad omar ahsan ||| 
2019 ||| lightweight and efficient end-to-end speech recognition using low-rank transformer. ||| genta indra winata ||| samuel cahyawijaya ||| zhaojiang lin ||| zihan liu ||| pascale fung ||| 
2021 ||| continuous speech separation with recurrent selective attention network. ||| yixuan zhang ||| zhuo chen ||| jian wu ||| takuya yoshioka ||| peidong wang ||| zhong meng ||| jinyu li ||| 
2018 ||| cbam: convolutional block attention module. ||| sanghyun woo ||| jongchan park ||| joon-young lee ||| in so kweon ||| 
2019 ||| multimodal attention branch network for perspective-free sentence generation. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2021 ||| end-to-end trainable multi-instance pose estimation with transformers. ||| lucas stoffl ||| maxime vidal ||| alexander mathis ||| 
2018 ||| incremental few-shot learning with attention attractor networks. ||| mengye ren ||| renjie liao ||| ethan fetaya ||| richard s. zemel ||| 
2021 ||| sinkformers: transformers with doubly stochastic attention. ||| michael e. sander ||| pierre ablin ||| mathieu blondel ||| gabriel peyr ||| 
2021 ||| an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech. ||| jianrong wang ||| nan gu ||| mei yu ||| xuewei li ||| qiang fang ||| li liu ||| 
2021 ||| augmenting convolutional networks with attention-based aggregation. ||| hugo touvron ||| matthieu cord ||| alaaeldin el-nouby ||| piotr bojanowski ||| armand joulin ||| gabriel synnaeve ||| herv |||  j ||| gou ||| 
2021 ||| improving on-screen sound separation for open domain videos with audio-visual self-attention. ||| efthymios tzinis ||| scott wisdom ||| tal remez ||| john r. hershey ||| 
2021 ||| msg-transformer: exchanging local spatial information by manipulating messenger tokens. ||| jiemin fang ||| lingxi xie ||| xinggang wang ||| xiaopeng zhang ||| wenyu liu ||| qi tian ||| 
2020 ||| ddanet: dual decoder attention network for automatic polyp segmentation. ||| nikhil kumar tomar ||| debesh jha ||| sharib ali ||| h ||| vard d. johansen ||| dag johansen ||| michael a. riegler ||| p ||| l halvorsen ||| 
2021 ||| tb-net: a tailored, self-attention deep convolutional neural network design for detection of tuberculosis cases from chest x-ray images. ||| alexander wong ||| james ren hou lee ||| hadi rahmat-khah ||| ali sabri ||| amer alaref ||| 
2021 ||| fine-tuning pretrained language models with label attention for explainable biomedical text classification. ||| bruce nguyen ||| shaoxiong ji ||| 
2020 ||| dynamically extracting outcome-specific problem lists from clinical notes with guided multi-headed attention. ||| justin r. lovelace ||| nathan c. hurley ||| adrian d. haimovich ||| bobak j. mortazavi ||| 
2020 ||| paragraph-level commonsense transformers with recurrent memory. ||| saadia gabriel ||| chandra bhagavatula ||| vered shwartz ||| ronan le bras ||| maxwell forbes ||| yejin choi ||| 
2022 ||| vitaev2: vision transformer advanced by exploring inductive bias for image recognition and beyond. ||| qiming zhang ||| yufei xu ||| jing zhang ||| dacheng tao ||| 
2019 ||| lattice-based transformer encoder for neural machine translation. ||| fengshun xiao ||| jiangtong li ||| hai zhao ||| rui wang ||| kehai chen ||| 
2021 ||| tanet: a new paradigm for global face super-resolution via transformer-cnn aggregation network. ||| yuanzhi wang ||| tao lu ||| yanduo zhang ||| junjun jiang ||| jiaming wang ||| zhongyuan wang ||| jiayi ma ||| 
2021 ||| bangla image caption generation through cnn-transformer based encoder-decoder network. ||| md aminul haque palash ||| m. d. abdullah al nasim ||| sourav saha ||| faria afrin ||| raisa mallik ||| sathishkumar samiappan ||| 
2021 ||| baanet: learning bi-directional adaptive attention gates for multispectral pedestrian detection. ||| xiaoxiao yang ||| yeqian qiang ||| huijie zhu ||| chunxiang wang ||| ming yang ||| 
2020 ||| common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. ||| anne lauscher ||| olga majewska ||| leonardo f. r. ribeiro ||| iryna gurevych ||| nikolai rozanov ||| goran glavas ||| 
2021 ||| mega-cda: memory guided attention for category-aware unsupervised domain adaptive object detection. ||| vibashan vs ||| vikram gupta ||| poojan oza ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2019 ||| predicting multiple demographic attributes with task specific embedding transformation and attention network. ||| raehyun kim ||| hyunjae kim ||| janghyuk lee ||| jaewoo kang ||| 
2021 ||| bloomnet: a robust transformer based model for bloom's learning outcome classification. ||| abdul waheed ||| muskan goyal ||| nimisha mittal ||| deepak gupta ||| ashish khanna ||| moolchand sharma ||| 
2022 ||| dawn of the transformer era in speech emotion recognition: closing the valence gap. ||| johannes wagner ||| andreas triantafyllopoulos ||| hage ||| n wierstorf ||| maximilian schmitt ||| felix burkhardt ||| florian eyben ||| bj ||| rn w. schuller ||| 
2019 ||| paying more attention to motion: attention distillation for learning video representations. ||| miao liu ||| xin chen ||| yun zhang ||| yin li ||| james m. rehg ||| 
2022 ||| automatic segmentation of head and neck tumor: how powerful transformers are? ||| ikboljon sobirov ||| otabek nazarov ||| hussain alasmawi ||| mohammad yaqub ||| 
2020 ||| focus-constrained attention mechanism for cvae-based response generation. ||| zhi cui ||| yanran li ||| jiayi zhang ||| jianwei cui ||| chen wei ||| bin wang ||| 
2020 ||| isia food-500: a dataset for large-scale food recognition via stacked global-local attention network. ||| weiqing min ||| linhu liu ||| zhiling wang ||| zhengdong luo ||| xiaoming wei ||| xiaolin wei ||| shuqiang jiang ||| 
2020 ||| qsan: a quantum-probability based signed attention network for explainable false information detection. ||| tian tian ||| yudong liu ||| xiaoyu yang ||| yuefei lyu ||| xi zhang ||| binxing fang ||| 
2018 ||| modeling localness for self-attention networks. ||| baosong yang ||| zhaopeng tu ||| derek f. wong ||| fandong meng ||| lidia s. chao ||| tong zhang ||| 
2021 ||| attention for image registration (air): an unsupervised transformer approach. ||| zihao wang ||| herv |||  delingette ||| 
2020 ||| augmented parallel-pyramid net for attention guided pose-estimation. ||| luanxuan hou ||| jie cao ||| yuan zhao ||| haifeng shen ||| yiping meng ||| ran he ||| jieping ye ||| 
2021 ||| hybrid encoder: towards efficient and precise native adsrecommendation via hybrid transformer encoding networks. ||| junhan yang ||| zheng liu ||| bowen jin ||| jianxun lian ||| defu lian ||| akshay soni ||| eun yong kang ||| yajun wang ||| guangzhong sun ||| xing xie ||| 
2020 ||| multiple structural priors guided self attention network for language understanding. ||| le qi ||| yu zhang ||| qingyu yin ||| ting liu ||| 
2021 ||| donut: document understanding transformer without ocr. ||| geewook kim ||| teakgyu hong ||| moonbin yim ||| jinyoung park ||| jinyeong yim ||| wonseok hwang ||| sangdoo yun ||| dongyoon han ||| seunghyun park ||| 
2021 ||| mixup training leads to reduced overfitting and improved calibration for the transformer architecture. ||| wancong zhang ||| ieshan vaidya ||| 
2017 ||| zoom out-and-in network with map attention decision for region proposal and object detection. ||| hongyang li ||| yu liu ||| wanli ouyang ||| xiaogang wang ||| 
2019 ||| behavior sequence transformer for e-commerce recommendation in alibaba. ||| qiwei chen ||| huan zhao ||| wei li ||| pipei huang ||| wenwu ou ||| 
2021 ||| attention-based neural load forecasting: a dynamic feature selection approach. ||| jing xiong ||| pengyang zhou ||| alan chen ||| yu zhang ||| 
2020 ||| extremely low bit transformer quantization for on-device neural machine translation. ||| insoo chung ||| byeongwook kim ||| yoonjung choi ||| se jung kwon ||| yongkweon jeon ||| baeseong park ||| sangha kim ||| dongsoo lee ||| 
2020 ||| sketchformer: transformer-based representation for sketched structure. ||| leonardo sampaio ferraz ribeiro ||| tu bui ||| john p. collomosse ||| moacir ponti ||| 
2021 ||| accounting for agreement phenomena in sentence comprehension with transformer language models: effects of similarity-based interference on surprisal and attention. ||| soo-hyun ryu ||| richard l. lewis ||| 
2022 ||| vovit: low latency graph-based audio-visual voice separation transformer. ||| juan f. montesinos ||| venkatesh s. kadandale ||| gloria haro ||| 
2022 ||| self-supervised vision transformers learn visual concepts in histopathology. ||| richard j. chen ||| rahul g. krishnan ||| 
2021 ||| the channel-spatial attention-based vision transformer network for automated, accurate prediction of crop nitrogen status from uav imagery. ||| xin zhang ||| liangxiu han ||| tam sobeih ||| lewis lappin ||| mark lee ||| andew howard ||| aron kisdi ||| 
2020 ||| weakly supervised few-shot object segmentation using co-attention with visual and semantic inputs. ||| mennatullah siam ||| naren doraiswamy ||| boris n. oreshkin ||| hengshuai yao ||| martin j ||| gersand ||| 
2019 ||| part-guided attention learning for vehicle re-identification. ||| xinyu zhang ||| rufeng zhang ||| jiewei cao ||| dong gong ||| mingyu you ||| chunhua shen ||| 
2020 ||| repulsive attention: rethinking multi-head attention as bayesian inference. ||| bang an ||| jie lyu ||| zhenyi wang ||| chunyuan li ||| changwei hu ||| fei tan ||| ruiyi zhang ||| yifan hu ||| changyou chen ||| 
2019 ||| jointly learning to align and translate with transformer models. ||| sarthak garg ||| stephan peitz ||| udhyakumar nallasamy ||| matthias paulik ||| 
2020 ||| grover: self-supervised message passing transformer on large-scale molecular data. ||| yu rong ||| yatao bian ||| tingyang xu ||| weiyang xie ||| ying wei ||| wenbing huang ||| junzhou huang ||| 
2020 ||| long document ranking with query-directed sparse transformer. ||| jyun-yu jiang ||| chenyan xiong ||| chia-jung lee ||| wei wang ||| 
2021 ||| efficient attentions for long document summarization. ||| luyang huang ||| shuyang cao ||| nikolaus nova parulian ||| heng ji ||| lu wang ||| 
2021 ||| transmed: transformers advance multi-modal medical image classification. ||| yin dai ||| yifan gao ||| 
2019 ||| attention-aware age-agnostic visual place recognition. ||| ziqi wang ||| jiahui li ||| seyran khademi ||| jan van gemert ||| 
2019 ||| attention based neural architecture for rumor detection with author context awareness. ||| sansiri tarnpradab ||| kien a. hua ||| 
2021 ||| end-to-end temporal action detection with transformer. ||| xiaolong liu ||| qimeng wang ||| yao hu ||| xu tang ||| song bai ||| xiang bai ||| 
2019 ||| distant supervision relation extraction with intra-bag and inter-bag attentions. ||| zhi-xiu ye ||| zhen-hua ling ||| 
2021 ||| transformer-f: a transformer network with effective methods for learning universal sentence representation. ||| yu shi ||| 
2019 ||| attention-based fault-tolerant approach for multi-agent reinforcement learning systems. ||| mingyang geng ||| kele xu ||| yiying li ||| shuqi liu ||| bo ding ||| huaimin wang ||| 
2022 ||| instantaneous physiological estimation using video transformers. ||| ambareesh revanur ||| ananyananda dasari ||| conrad s. tucker ||| l ||| szl |||  a. jeni ||| 
2020 ||| improving sentiment analysis over non-english tweets using multilingual transformers and automatic translation for data-augmentation. ||| valentin barri ||| re ||| alexandra balahur ||| 
2020 ||| an attention-based system for damage assessment using satellite imagery. ||| hanxiang hao ||| sriram baireddy ||| emily r. bartusiak ||| latisha konz ||| kevin j. latourette ||| michael gribbons ||| moses w. chan ||| mary l. comer ||| edward j. delp ||| 
2021 ||| fine-grained image generation from bangla text description using attentional generative adversarial network. ||| md aminul haque palash ||| md abdullah al nasim ||| aditi dhali ||| faria afrin ||| 
2020 ||| dualnet: locate then detect effective payload with deep attention network. ||| shiyi yang ||| peilun wu ||| hui guo ||| 
2021 ||| attend to who you are: supervising self-attention for keypoint detection and instance-aware association. ||| sen yang ||| zhicheng wang ||| ze chen ||| yanjie li ||| shoukui zhang ||| zhibin quan ||| shu-tao xia ||| yiping bao ||| erjin zhou ||| wankou yang ||| 
2021 ||| application of deep self-attention in knowledge tracing. ||| junhao zeng ||| qingchun zhang ||| ning xie ||| bochun yang ||| 
2020 ||| attention that does not explain away. ||| nan ding ||| xinjie fan ||| zhenzhong lan ||| dale schuurmans ||| radu soricut ||| 
2021 ||| timbre classification of musical instruments with a deep learning multi-head attention-based model. ||| carlos hernandez-olivan ||| jos |||  ram ||| n beltr ||| n ||| 
2019 ||| improving deep lesion detection using 3d contextual and spatial attention. ||| qingyi tao ||| zongyuan ge ||| jianfei cai ||| jianxiong yin ||| simon see ||| 
2021 ||| diverse single image generation with controllable global structure through self-attention. ||| sutharsan mahendren ||| chamira u. s. edussooriya ||| ranga rodrigo ||| 
2021 ||| visual selective attention system to intervene user attention in sharing covid-19 misinformation. ||| zaid amin ||| nazlena mohamad ali ||| alan f. smeaton ||| 
2021 ||| edge-featured graph attention network. ||| jun chen ||| haopeng chen ||| 
2021 ||| boosting few-shot semantic segmentation with transformers. ||| guolei sun ||| yun liu ||| jingyun liang ||| luc van gool ||| 
2020 |||  distillation through attention. ||| hugo touvron ||| matthieu cord ||| matthijs douze ||| francisco massa ||| alexandre sablayrolles ||| herv |||  j ||| gou ||| 
2018 ||| a visual attention grounding neural model for multimodal machine translation. ||| mingyang zhou ||| runxiang cheng ||| yong jae lee ||| zhou yu ||| 
2018 ||| double supervised network with attention mechanism for scene text recognition. ||| yuting gao ||| zheng huang ||| yuchen dai ||| 
2020 ||| pay attention to evolution: time series forecasting with deep graph-evolution learning. ||| gabriel spadon ||| shenda hong ||| bruno brandoli ||| stan matwin ||| jos |||  f. rodrigues jr. ||| jimeng sun ||| 
2020 ||| a self-attention network based node embedding model. ||| dai quoc nguyen ||| tu dinh nguyen ||| dinh phung ||| 
2020 ||| tnt-kid: transformer-based neural tagger for keyword identification. ||| matej martinc ||| blaz skrlj ||| senja pollak ||| 
2020 ||| neural encoding with visual attention. ||| meenakshi khosla ||| gia h. ngo ||| keith jamison ||| amy kuceyeski ||| mert r. sabuncu ||| 
2022 ||| vrt: a video restoration transformer. ||| jingyun liang ||| jiezhang cao ||| yuchen fan ||| kai zhang ||| rakesh ranjan ||| yawei li ||| radu timofte ||| luc van gool ||| 
2022 ||| rtnet: relation transformer network for diabetic retinopathy multi-lesion segmentation. ||| shiqi huang ||| jianan li ||| yuze xiao ||| ning shen ||| tingfa xu ||| 
2018 ||| paying attention to attention: highlighting influential samples in sequential analysis. ||| cynthia freeman ||| jonathan merriman ||| abhinav aggarwal ||| ian beaver ||| abdullah mueen ||| 
2022 ||| ernie-sparse: learning hierarchical efficient transformer through regularized self-attention. ||| yang liu ||| jiaxiang liu ||| li chen ||| yuxiang lu ||| shikun feng ||| zhida feng ||| yu sun ||| hao tian ||| hua wu ||| haifeng wang ||| 
2022 ||| multiple sclerosis lesions segmentation using attention-based cnns in flair images. ||| mehdi sadeghibakhi ||| hamidreza pourreza ||| hamidreza mahyar ||| 
2021 ||| regularizing transformers with deep probabilistic layers. ||| aurora cobo aguilera ||| pablo martinez-olmos ||| antonio art ||| s-rodr ||| guez ||| fernando p ||| rez-cruz ||| 
2021 ||| gradient-based adversarial attacks against text transformers. ||| chuan guo ||| alexandre sablayrolles ||| herv |||  j ||| gou ||| douwe kiela ||| 
2021 ||| attention! stay focus! ||| tu vo ||| 
2019 ||| a better way to attend: attention with trees for video question answering. ||| hongyang xue ||| wenqing chu ||| zhou zhao ||| deng cai ||| 
2018 ||| attention please: consider mockito when evaluating newly released automated program repair techniques. ||| shangwen wang ||| ming wen ||| deheng yang ||| xiaoguang mao ||| 
2021 ||| decoding 3d representation of visual imagery eeg using attention-based dual-stream convolutional neural network. ||| hyung-ju ahn ||| dae-hyeok lee ||| 
2021 ||| transformers for prompt-level ema non-response prediction. ||| supriya nagesh ||| alexander moreno ||| stephanie m. carpenter ||| jamie yap ||| soujanya chatterjee ||| steven lloyd lizotte ||| neng wan ||| santosh kumar ||| cho lam ||| david w. wetter ||| inbal nahum-shani ||| james m. rehg ||| 
2021 ||| generating fake cyber threat intelligence using transformer-based models. ||| priyanka ranade ||| aritran piplai ||| sudip mittal ||| anupam joshi ||| tim finin ||| 
2017 ||| audio set classification with attention model: a probabilistic perspective. ||| qiuqiang kong ||| yong xu ||| wenwu wang ||| mark d. plumbley ||| 
2021 ||| a simple approach to image tilt correction with self-attention mobilenet for smartphones. ||| siddhant garg ||| debi prasanna mohanty ||| siva prasad thota ||| sukumar moharana ||| 
2021 ||| ptq4vit: post-training quantization framework for vision transformers. ||| zhihang yuan ||| chenhao xue ||| yiqi chen ||| qiang wu ||| guangyu sun ||| 
2021 ||| deepprog: a transformer-based framework for predicting disease prognosis. ||| huy hoang nguyen ||| simo saarakkala ||| matthew b. blaschko ||| aleksei tiulpin ||| 
2021 ||| end-to-end speaker-attributed asr with transformer. ||| naoyuki kanda ||| guoli ye ||| yashesh gaur ||| xiaofei wang ||| zhong meng ||| zhuo chen ||| takuya yoshioka ||| 
2019 ||| dnanet: de-normalized attention based multi-resolution network for human pose estimation. ||| kun zhang ||| peng he ||| ping yao ||| ge chen ||| chuanguang yang ||| huimin li ||| li fu ||| tianyao zheng ||| 
2020 ||| a accuracy with transformer-based models on large complex documents. ||| chejui liao ||| tabish maniar ||| sravanajyothi n ||| anantha sharma ||| 
2018 ||| attention-based capsule networks with dynamic routing for relation extraction. ||| ningyu zhang ||| shumin deng ||| zhanlin sun ||| xi chen ||| wei zhang ||| huajun chen ||| 
2020 ||| inner attention supported adaptive cooperation for heterogeneous multi robots teaming based on multi-agent reinforcement learning. ||| chao huang ||| rui liu ||| 
2022 ||| segtransvae: hybrid cnn - transformer with regularization for medical image segmentation. ||| quan-dung pham ||| hai nguyen-truong ||| nam nguyen phuong ||| n. a. khoa nguyen ||| 
2021 ||| gattanet: global attention agreement for convolutional neural networks. ||| rufin vanrullen ||| andrea alamia ||| 
2020 ||| gret: global representation enhanced transformer. ||| rongxiang weng ||| hao-ran wei ||| shujian huang ||| heng yu ||| lidong bing ||| weihua luo ||| jiajun chen ||| 
2021 ||| cate: computation-aware neural architecture encoding with transformers. ||| shen yan ||| kaiqiang song ||| zhe feng ||| mi zhang ||| 
2021 ||| group-node attention for community evolution prediction. ||| matt revelle ||| carlotta domeniconi ||| ben u. gelman ||| 
2020 ||| hierarchical attention network for action segmentation. ||| harshala gammulle ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2022 ||| clustering text using attention. ||| lovedeep singh ||| 
2020 ||| learning to decouple relations: few-shot relation classification with entity-guided attention and confusion-aware training. ||| yingyao wang ||| junwei bao ||| guangyi liu ||| youzheng wu ||| xiaodong he ||| bowen zhou ||| tiejun zhao ||| 
2021 ||| spanet: generalized permutationless set assignment for particle physics using symmetry preserving attention. ||| alexander shmakov ||| michael james fenton ||| ta-wei ho ||| shih-chieh hsu ||| daniel whiteson ||| pierre baldi ||| 
2021 ||| augmented transformer with adaptive graph for temporal action proposal generation. ||| shuning chang ||| pichao wang ||| fan wang ||| hao li ||| jiashi feng ||| 
2018 ||| cascade attention network for person search: both image and text-image similarity selection. ||| ya jing ||| chenyang si ||| junbo wang ||| wei wang ||| liang wang ||| tieniu tan ||| 
2021 ||| dual transformer for point cloud analysis. ||| xian-feng han ||| yi-fei jin ||| hui-xian cheng ||| guo-qiang xiao ||| 
2020 ||| relational pretrained transformers towards democratizing data preparation [vision]. ||| nan tang ||| ju fan ||| fangyi li ||| jianhong tu ||| xiaoyong du ||| guoliang li ||| sam madden ||| mourad ouzzani ||| 
2020 ||| point cloud completion by skip-attention network with hierarchical folding. ||| xin wen ||| tianyang li ||| zhizhong han ||| yu-shen liu ||| 
2019 ||| hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification. ||| hao peng ||| jianxin li ||| qiran gong ||| senzhang wang ||| lifang he ||| bo li ||| lihong wang ||| philip s. yu ||| 
2021 ||| continuous 3d multi-channel sign language production via progressive transformers and mixture density networks. ||| ben saunders ||| necati cihan camg ||| z ||| richard bowden ||| 
2020 ||| character-level transformer-based neural machine translation. ||| nikolay banar ||| walter daelemans ||| mike kestemont ||| 
2021 ||| joint attention for multi-agent coordination and social learning. ||| dennis lee ||| natasha jaques ||| j. chase kew ||| douglas eck ||| dale schuurmans ||| aleksandra faust ||| 
2020 ||| feedback graph attention convolutional network for medical image enhancement. ||| xiaobin hu ||| yanyang yan ||| wenqi ren ||| hongwei li ||| yu zhao ||| amirhossein bayat ||| bjoern h. menze ||| 
2021 ||| attention-based partial face recognition. ||| stefan h ||| rmann ||| zeyuan zhang ||| martin knoche ||| torben teepe ||| gerhard rigoll ||| 
2021 ||| efficient hybrid transformer: learning global-local context for urban sence segmentation. ||| libo wang ||| shenghui fang ||| ce zhang ||| rui li ||| chenxi duan ||| 
2021 ||| graph attention network-based multi-agent reinforcement learning for slicing resource management in dense cellular network. ||| yan shao ||| rongpeng li ||| bing hu ||| yingxiao wu ||| zhifeng zhao ||| honggang zhang ||| 
2021 ||| triple m: a practical neural text-to-speech system with multi-guidance attention and multi-band multi-time lpcnet. ||| shilun lin ||| fenglong xie ||| xinhui li ||| li lu ||| 
2020 ||| transformer-based language modeling and decoding for conversational speech recognition. ||| kareem nassar ||| 
2021 ||| translation transformers rediscover inherent data domains. ||| maksym del ||| elizaveta korotkova ||| mark fishel ||| 
2019 ||| sequential attention-based network for noetic end-to-end response selection. ||| qian chen ||| wen wang ||| 
2018 ||| personalized attention-aware exposure control using reinforcement learning. ||| huan yang ||| baoyuan wang ||| noranart vesdapunt ||| minyi guo ||| sing bing kang ||| 
2021 ||| improved multiscale vision transformers for classification and detection. ||| yanghao li ||| chao-yuan wu ||| haoqi fan ||| karttikeya mangalam ||| bo xiong ||| jitendra malik ||| christoph feichtenhofer ||| 
2021 ||| learning slice-aware representations with mixture of attentions. ||| cheng wang ||| sungjin lee ||| sunghyun park ||| han li ||| young-bum kim ||| ruhi sarikaya ||| 
2020 ||| simultaneous left atrium anatomy and scar segmentations via deep learning in multiview information with attention. ||| guang yang ||| jun chen ||| zhifan gao ||| shuo li ||| hao ni ||| elsa d. angelini ||| tom wong ||| raad mohiaddin ||| eva nyktari ||| ricardo wage ||| lei xu ||| yanping zhang ||| xiuquan du ||| heye zhang ||| david n. firmin ||| jennifer keegan ||| 
2020 ||| ecapa-tdnn: emphasized channel attention, propagation and aggregation in tdnn based speaker verification. ||| brecht desplanques ||| jenthe thienpondt ||| kris demuynck ||| 
2021 ||| memory-efficient transformers via top-k attention. ||| ankit gupta ||| guy dar ||| shaya goodman ||| david ciprut ||| jonathan berant ||| 
2021 ||| h-net: unsupervised attention-based stereo depth estimation leveraging epipolar geometry. ||| baoru huang ||| jian-qing zheng ||| stamatia giannarou ||| daniel s. elson ||| 
2021 ||| hetformer: heterogeneous transformer with sparse attention for long-text extractive summarization. ||| ye liu ||| jian-guo zhang ||| yao wan ||| congying xia ||| lifang he ||| philip s. yu ||| 
2021 ||| heterogeneous transformer: a scale adaptable neural network architecture for device activity detection. ||| yang li ||| zhilin chen ||| yunqi wang ||| chenyang yang ||| yik-chung wu ||| 
2022 ||| joint cnn and transformer network via weakly supervised learning for efficient crowd counting. ||| fusen wang ||| kai liu ||| fei long ||| nong sang ||| xiaofeng xia ||| jun sang ||| 
2021 ||| attention-based bidirectional lstm for deceptive opinion spam classification. ||| ashish salunkhe ||| 
2021 ||| st-detr: spatio-temporal object traces attention detection transformer. ||| eslam mohamed ||| ahmad el sallab ||| 
2021 ||| prototypical cross-attention networks for multiple object tracking and segmentation. ||| lei ke ||| xia li ||| martin danelljan ||| yu-wing tai ||| chi-keung tang ||| fisher yu ||| 
2022 ||| effective urban region representation learning using heterogeneous urban graph attention network (hugat). ||| namwoo kim ||| yoonjin yoon ||| 
2021 ||| transzero: attribute-guided transformer for zero-shot learning. ||| shiming chen ||| ziming hong ||| yang liu ||| guo-sen xie ||| baigui sun ||| hao li ||| qinmu peng ||| ke lu ||| xinge you ||| 
2020 ||| large scale multimodal classification using an ensemble of transformer models and co-attention. ||| varnith chordia ||| vijay kumar b. g ||| 
2020 ||| self-and-mixed attention decoder with deep acoustic structure for transformer-based lvcsr. ||| xinyuan zhou ||| grandee lee ||| emre yilmaz ||| yanhua long ||| jiaen liang ||| haizhou li ||| 
2021 ||| segmenter: transformer for semantic segmentation. ||| robin strudel ||| ricardo garcia pinel ||| ivan laptev ||| cordelia schmid ||| 
2020 ||| decoupled self attention for accurate one stage object detection. ||| kehe wu ||| zuge chen ||| qi ma ||| xiaoliang zhang ||| wei li ||| 
2018 ||| audio-visual speech recognition with a hybrid ctc/attention architecture. ||| stavros petridis ||| themos stafylakis ||| pingchuan ma ||| georgios tzimiropoulos ||| maja pantic ||| 
2021 ||| transfer learning with causal counterfactual reasoning in decision transformers. ||| ayman boustati ||| hana chockler ||| daniel c. mcnamee ||| 
2020 ||| solar: second-order loss and attention for image retrieval. ||| tony ng ||| vassileios balntas ||| yurun tian ||| krystian mikolajczyk ||| 
2021 ||| mia-former: efficient and robust vision transformers via multi-grained input-adaptation. ||| zhongzhi yu ||| yonggan fu ||| sicheng li ||| chaojian li ||| yingyan lin ||| 
2019 ||| contrastive bidirectional transformer for temporal representation learning. ||| chen sun ||| fabien baradel ||| kevin murphy ||| cordelia schmid ||| 
2021 ||| pose recognition with cascade transformers. ||| ke li ||| shijie wang ||| xiang zhang ||| yifan xu ||| weijian xu ||| zhuowen tu ||| 
2021 ||| optimising knee injury detection with spatial attention and validating localisation ability. ||| niamh belton ||| ivan welaratne ||| adil dahlan ||| ronan t. hearne ||| misgina tsighe hagos ||| aonghus lawlor ||| kathleen m. curran ||| 
2017 ||| a recurrent neural model with attention for the recognition of chinese implicit discourse relations. ||| samuel r ||| nnqvist ||| niko schenk ||| christian chiarcos ||| 
2021 ||| attention-based vehicle self-localization with hd feature maps. ||| nico engel ||| vasileios belagiannis ||| klaus dietmayer ||| 
2020 ||| pct: point cloud transformer. ||| meng-hao guo ||| junxiong cai ||| zheng-ning liu ||| tai-jiang mu ||| ralph r. martin ||| shi-min hu ||| 
2017 ||| hierarchical lstm with adjusted temporal attention for video captioning. ||| jingkuan song ||| zhao guo ||| lianli gao ||| wu liu ||| dongxiang zhang ||| heng tao shen ||| 
2018 ||| bam: bottleneck attention module. ||| jongchan park ||| sanghyun woo ||| joon-young lee ||| in so kweon ||| 
2021 ||| self-attention for audio super-resolution. ||| nathana ||| l carraz rakotonirina ||| 
2019 ||| encoding database schemas with relation-aware self-attention for text-to-sql parsers. ||| richard shin ||| 
2018 ||| attentionmask: attentive, efficient object proposal generation focusing on small objects. ||| christian wilms ||| simone frintrop ||| 
2021 ||| deepra: predicting joint damage from radiographs using cnn with attention. ||| neelambuj chaturvedi ||| 
2020 ||| improving auditory attention decoding performance of linear and non-linear methods using state-space model. ||| ali aroudi ||| tobias de taillez ||| simon doclo ||| 
2018 ||| attention-based lstm for psychological stress detection from spoken language using distant supervision. ||| genta indra winata ||| onno pepijn kampman ||| pascale fung ||| 
2021 ||| mask guided attention for fine-grained patchy image classification. ||| jun wang ||| xiaohan yu ||| yongsheng gao ||| 
2021 ||| hierarchical transformer encoders for vietnamese spelling correction. ||| hieu tran ||| cuong v. dinh ||| long n. phan ||| son truong nguyen ||| 
2019 ||| detecting alzheimer's disease by estimating attention and elicitation path through the alignment of spoken picture descriptions with the picture prompt. ||| bahman mirheidari ||| yilin pan ||| traci walker ||| markus reuber ||| annalena venneri ||| daniel blackburn ||| heidi christensen ||| 
2021 ||| disentangling representations of text by masking transformers. ||| xiongyi zhang ||| jan-willem van de meent ||| byron c. wallace ||| 
2021 ||| r2d2: relational text decoding with transformers. ||| aryan arbabi ||| mingqiu wang ||| laurent el shafey ||| nan du ||| izhak shafran ||| 
2019 ||| multi-scale guided attention for medical image segmentation. ||| ashish sinha ||| jose dolz ||| 
2021 ||| keeping your eye on the ball: trajectory attention in video transformers. ||| mandela patrick ||| dylan campbell ||| yuki markus asano ||| ishan misra ||| florian metze ||| christoph feichtenhofer ||| andrea vedaldi ||| jo ||| o f. henriques ||| 
2021 ||| session-based recommendation with hypergraph attention networks. ||| jianling wang ||| kaize ding ||| ziwei zhu ||| james caverlee ||| 
2021 ||| sound event detection transformer: an event-based end-to-end model for sound event detection. ||| zhirong ye ||| xiangdong wang ||| hong liu ||| yueliang qian ||| rui tao ||| long yan ||| kazushige ouchi ||| 
2020 ||| adaptive transformers in rl. ||| shakti kumar ||| jerrod parker ||| panteha naderian ||| 
2020 ||| an explainable 3d residual self-attention deep neural network for joint atrophy localization and alzheimer's disease diagnosis using structural mri. ||| xin zhang ||| liangxiu han ||| wenyong zhu ||| liang sun ||| daoqiang zhang ||| 
2022 ||| sotitle: a transformer-based post title generation approach for stack overflow. ||| ke liu ||| guang yang ||| xiang chen ||| chi yu ||| 
2018 ||| neighbourhood watch: referring expression comprehension via language-guided graph attention networks. ||| peng wang ||| qi wu ||| jiewei cao ||| chunhua shen ||| lianli gao ||| anton van den hengel ||| 
2020 ||| gate: graph attention transformer encoder for cross-lingual relation and event extraction. ||| wasi uddin ahmad ||| nanyun peng ||| kai-wei chang ||| 
2022 ||| dp-kb: data programming with knowledge bases improves transformer fine tuning for answer sentence selection. ||| nic jedema ||| thuy vu ||| manish gupta ||| alessandro moschitti ||| 
2021 ||| on-device spatial attention based sequence learning approach for scene text script identification. ||| rutika moharir ||| arun d. prabhu ||| sukumar moharana ||| gopi ramena ||| rachit s. munjal ||| 
2020 ||| revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. ||| zhaoshuo li ||| xingtong liu ||| francis x. creighton ||| russell h. taylor ||| mathias unberath ||| 
2022 ||| guided generative protein design using regularized transformers. ||| egbert castro ||| abhinav godavarthi ||| julian rubinfien ||| kevin b. givechian ||| dhananjay bhaskar ||| smita krishnaswamy ||| 
2019 ||| convolutional temporal attention model for video-based person re-identification. ||| tanzila rahman ||| mrigank rochan ||| yang wang ||| 
2019 ||| a zero attention model for personalized product search. ||| qingyao ai ||| daniel n. hill ||| s. v. n. vishwanathan ||| w. bruce croft ||| 
2019 ||| speeding up reinforcement learning by combining attention and agency features. ||| berkay demirel ||| mart |||  s ||| nchez-fibla ||| 
2021 ||| df-conformer: integrated architecture of conv-tasnet and conformer using linear complexity self-attention for speech enhancement. ||| yuma koizumi ||| shigeki karita ||| scott wisdom ||| hakan erdogan ||| john r. hershey ||| llion jones ||| michiel bacchiani ||| 
2021 ||| somps-net : attention based social graph framework for early detection of fake health news. ||| prasannakumaran d ||| harish srinivasan ||| sowmiya sree s ||| sri gayathri devi i ||| saikrishnan s ||| vineeth vijayaraghavan ||| 
2019 ||| eyecar: modeling the visual attention allocation of drivers in semi-autonomous vehicles. ||| sonia baee ||| erfan pakdamanian ||| vicente ordonez ||| inki kim ||| lu feng ||| laura e. barnes ||| 
2021 ||| swintrack: a simple and strong baseline for transformer tracking. ||| liting lin ||| heng fan ||| yong xu ||| haibin ling ||| 
2021 ||| pyramid vision transformer: a versatile backbone for dense prediction without convolutions. ||| wenhai wang ||| enze xie ||| xiang li ||| deng-ping fan ||| kaitao song ||| ding liang ||| tong lu ||| ping luo ||| ling shao ||| 
2021 ||| attention-based transformation from latent features to point clouds. ||| kaiyi zhang ||| ximing yang ||| yuan wu ||| cheng jin ||| 
2021 ||| cuab: convolutional uncertainty attention block enhanced the chest x-ray image analysis. ||| chi-shiang wang ||| fang-yi su ||| tsung-lu michael lee ||| yi-shan tsai ||| jung-hsien chiang ||| 
2020 ||| chatbot interaction with artificial intelligence: human data augmentation with t5 and language transformer ensemble for text classification. ||| jordan j. bird ||| anik |||  ek ||| rt ||| diego r. faria ||| 
2020 ||| transtrack: multiple-object tracking with transformer. ||| peize sun ||| yi jiang ||| rufeng zhang ||| enze xie ||| jinkun cao ||| xinting hu ||| tao kong ||| zehuan yuan ||| changhu wang ||| ping luo ||| 
2020 ||| phyaat: physiology of auditory attention to speech dataset. ||| nikesh bajaj ||| jes ||| s requena-carri ||| n ||| francesco bellotti ||| 
2021 ||| development of deep transformer-based models for long-term prediction of transient production of oil wells. ||| ildar abdrakhmanov ||| evgenii kanin ||| sergei boronin ||| evgeny burnaev ||| andrei osiptsov ||| 
2021 ||| improving predictions of tail-end labels using concatenated biomed-transformers for long medical documents. ||| vithya yogarajan ||| bernhard pfahringer ||| tony smith ||| jacob montiel ||| 
2021 ||| multiplex behavioral relation learning for recommendation via memory augmented transformer network. ||| lianghao xia ||| chao huang ||| yong xu ||| peng dai ||| bo zhang ||| liefeng bo ||| 
2020 ||| a novel transferability attention neural network model for eeg emotion recognition. ||| yang li ||| boxun fu ||| fu li ||| guangming shi ||| wenming zheng ||| 
2019 ||| a multimodal target-source classifier with attention branches to understand ambiguous instructions for fetching daily objects. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2020 ||| transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis. ||| zhen wu ||| chengcan ying ||| xinyu dai ||| shujian huang ||| jiajun chen ||| 
2020 ||| brain tumor segmentation network using attention-based fusion and spatial relationship constraint. ||| chenyu liu ||| wangbin ding ||| lei li ||| zhen zhang ||| chenhao pei ||| liqin huang ||| xiahai zhuang ||| 
2021 ||| visual-aware attention dual-stream decoder for video captioning. ||| zhixin sun ||| xian zhong ||| shuqin chen ||| lin li ||| luo zhong ||| 
2021 ||| buildformer: automatic building extraction with vision transformer. ||| libo wang ||| yuechi yang ||| rui li ||| 
2017 ||| emoatt at emoint-2017: inner attention sentence embedding for emotion intensity. ||| edison marrese-taylor ||| yutaka matsuo ||| 
2017 ||| picanet: learning pixel-wise contextual attention in convnets and its application in saliency detection. ||| nian liu ||| junwei han ||| 
2019 ||| a time attention based fraud transaction detection framework. ||| longfei li ||| ziqi liu ||| chaochao chen ||| ya-lin zhang ||| jun zhou ||| xiaolong li ||| 
2022 ||| wind park power prediction: attention-based graph networks and deep learning to capture wake losses. ||| lars  ||| degaard bentsen ||| narada dilp warakagoda ||| roy stenbro ||| paal engelstad ||| 
2019 ||| mc-ista-net: adaptive measurement and initialization and channel attention optimization inspired neural network for compressive sensing. ||| 
2019 ||| eca-net: efficient channel attention for deep convolutional neural networks. ||| qilong wang ||| banggu wu ||| pengfei zhu ||| peihua li ||| wangmeng zuo ||| qinghua hu ||| 
2021 ||| self-supervised video retrieval transformer network. ||| xiangteng he ||| yulin pan ||| mingqian tang ||| yiliang lv ||| 
2017 ||| soft + hardwired attention: an lstm framework for human trajectory prediction and abnormal event detection. ||| tharindu fernando ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2017 ||| confidence through attention. ||| matiss rikters ||| mark fishel ||| 
2020 ||| fast transformers with clustered attention. ||| apoorv vyas ||| angelos katharopoulos ||| fran ||| ois fleuret ||| 
2019 ||| exploring the limits of transfer learning with a unified text-to-text transformer. ||| colin raffel ||| noam shazeer ||| adam roberts ||| katherine lee ||| sharan narang ||| michael matena ||| yanqi zhou ||| wei li ||| peter j. liu ||| 
2021 ||| can transformer models measure coherence in text? re-thinking the shuffle test. ||| philippe laban ||| luke dai ||| lucas bandarkar ||| marti a. hearst ||| 
2021 ||| locally shifted attention with early global integration. ||| shelly sheynin ||| sagie benaim ||| adam polyak ||| lior wolf ||| 
2022 ||| joint liver and hepatic lesion segmentation using a hybrid cnn with transformer layers. ||| georg hille ||| shubham agrawal ||| christian wybranski ||| maciej pech ||| alexey surov ||| sylvia saalfeld ||| 
2021 ||| pairconnect: a compute-efficient mlp alternative to attention. ||| zhaozhuo xu ||| minghao yan ||| junyan zhang ||| anshumali shrivastava ||| 
2017 ||| watch your step: learning graph embeddings through attention. ||| sami abu-el-haija ||| bryan perozzi ||| rami al-rfou ||| alex alemi ||| 
2019 ||| investigating self-attention network for chinese word segmentation. ||| leilei gan ||| yue zhang ||| 
2021 ||| what makes for hierarchical vision transformer? ||| yuxin fang ||| xinggang wang ||| rui wu ||| jianwei niu ||| wenyu liu ||| 
2019 ||| scar: spatial-/channel-wise attention regression networks for crowd counting. ||| junyu gao ||| qi wang ||| yuan yuan ||| 
2019 ||| perceive, transform, and act: multi-modal attention networks for vision-and-language navigation. ||| federico landi ||| lorenzo baraldi ||| marcella cornia ||| massimiliano corsini ||| rita cucchiara ||| 
2018 ||| atts2s-vc: sequence-to-sequence voice conversion with attention and context preservation mechanisms. ||| kou tanaka ||| hirokazu kameoka ||| takuhiro kaneko ||| nobukatsu hojo ||| 
2020 ||| msaf: multimodal split attention fusion. ||| lang su ||| chuqing hu ||| guofa li ||| dongpu cao ||| 
2021 ||| multimodal attention fusion for target speaker extraction. ||| hiroshi sato ||| tsubasa ochiai ||| keisuke kinoshita ||| marc delcroix ||| tomohiro nakatani ||| shoko araki ||| 
2021 ||| detecting gender bias in transformer-based models: a case study on bert. ||| bingbing li ||| hongwu peng ||| rajat sainju ||| junhuan yang ||| lei yang ||| yueying liang ||| weiwen jiang ||| binghui wang ||| hang liu ||| caiwen ding ||| 
2020 ||| don't shoot butterfly with rifles: multi-channel continuous speech separation with early exit transformer. ||| sanyuan chen ||| yu wu ||| zhuo chen ||| takuya yoshioka ||| shujie liu ||| jinyu li ||| 
2022 ||| truetype transformer: character and font style recognition in outline format. ||| yusuke nagata ||| jinki otao ||| daichi haraguchi ||| seiichi uchida ||| 
2017 ||| visual reference resolution using attention memory for visual dialog. ||| paul hongsuck seo ||| andreas m. lehrmann ||| bohyung han ||| leonid sigal ||| 
2019 ||| adversarial cross-domain action recognition with co-attention. ||| boxiao pan ||| zhangjie cao ||| ehsan adeli ||| juan carlos niebles ||| 
2020 ||| a contextual hierarchical attention network with adaptive objective for dialogue state tracking. ||| yong shan ||| zekang li ||| jinchao zhang ||| fandong meng ||| yang feng ||| cheng niu ||| jie zhou ||| 
2019 ||| joint language identification of code-switching speech using attention based e2e network. ||| sreeram ganji ||| kunal dhawan ||| kumar priyadarshi ||| rohit sinha ||| 
2017 ||| order-free rnn with visual attention for multi-label classification. ||| shang-fu chen ||| yi-chen chen ||| chih-kuan yeh ||| yu-chiang frank wang ||| 
2021 ||| tree decomposition attention for amr-to-text generation. ||| lisa jin ||| daniel gildea ||| 
2022 ||| continual transformers: redundancy-free attention for online inference. ||| lukas hedegaard ||| arian bakhtiarnia ||| alexandros iosifidis ||| 
2020 ||| classification of tactile perception and attention on natural textures from eeg signals. ||| myoung-ki kim ||| jeong-hyun cho ||| ji-hoon jeong ||| 
2018 ||| visual attention network for low dose ct. ||| wenchao du ||| hu chen ||| peixi liao ||| hongyu yang ||| ge wang ||| yi zhang ||| 
2021 ||| larger-scale transformers for multilingual masked language modeling. ||| naman goyal ||| jingfei du ||| myle ott ||| giri anantharaman ||| alexis conneau ||| 
2021 ||| on the control of attentional processes in vision. ||| john k. tsotsos ||| omar abid ||| iuliia kotseruba ||| markus d. solbach ||| 
2021 ||| multi-domain transformer-based counterfactual augmentation for earnings call analysis. ||| zixuan yuan ||| yada zhu ||| wei zhang ||| ziming huang ||| guangnan ye ||| hui xiong ||| 
2021 ||| boosting salient object detection with transformer-based asymmetric bilateral u-net. ||| yu qiu ||| yun liu ||| le zhang ||| jing xu ||| 
2021 ||| tbn-vit: temporal bilateral network with vision transformer for video scene parsing. ||| bo yan ||| leilei cao ||| hongbin wang ||| 
2022 ||| self-attention multi-view representation learning with diversity-promoting complementarity. ||| jian-wei liu ||| xi-hao ding ||| runkun lu ||| xionglin luo ||| 
2018 ||| attention-guided unified network for panoptic segmentation. ||| yanwei li ||| xinze chen ||| zheng zhu ||| lingxi xie ||| guan huang ||| dalong du ||| xingang wang ||| 
2021 ||| revisiting mahalanobis distance for transformer-based out-of-domain detection. ||| alexander podolskiy ||| dmitry lipin ||| andrey bout ||| ekaterina artemova ||| irina piontkovskaya ||| 
2022 ||| bert weaver: using weight averaging to enable lifelong learning for transformer-based models. ||| lisa langnickel ||| alexander schulz ||| barbara hammer ||| juliane fluck ||| 
2021 ||| target-dependent uniter: a transformer-based multimodal language comprehension model for domestic service robots. ||| shintaro ishikawa ||| komei sugiura ||| 
2022 ||| transformer embeddings of irregularly spaced events and their participants. ||| chenghao yang ||| hongyuan mei ||| jason eisner ||| 
2020 ||| attack on multi-node attention for object detection. ||| sizhe chen ||| fan he ||| xiaolin huang ||| kun zhang ||| 
2021 ||| attention-based model for predicting question relatedness on stack overflow. ||| jiayan pei ||| yimin wu ||| zishan qin ||| yao cong ||| jingtao guan ||| 
2022 ||| coarse-to-fine vision transformer. ||| mengzhao chen ||| mingbao lin ||| ke li ||| yunhang shen ||| yongjian wu ||| fei chao ||| rongrong ji ||| 
2019 ||| patch transformer for multi-tagging whole slide histopathology images. ||| weijian li ||| viet-duy nguyen ||| haofu liao ||| matt wilder ||| ke cheng ||| jiebo luo ||| 
2021 ||| vision transformers for femur fracture classification. ||| leonardo tanzi ||| andrea audisio ||| giansalvo cirrincione ||| alessandro aprato ||| enrico vezzetti ||| 
2021 ||| space-time mixing attention for video transformer. ||| adrian bulat ||| juan-manuel perez-rua ||| swathikiran sudhakaran ||| brais mart ||| nez ||| georgios tzimiropoulos ||| 
2019 ||| attention-based face antispoofing of rgb images, using a minimal end-2-end neural network. ||| ali ghofrani ||| rahil mahdian toroghi ||| seyed mojtaba tabatabaie ||| 
2021 ||| efficient two-stage detection of human-object interactions with a novel unary-pairwise transformer. ||| frederic z. zhang ||| dylan campbell ||| stephen gould ||| 
2020 ||| trailer: transformer-based time-wise long term relation modeling for citywide traffic flow prediction. ||| hao xue ||| flora d. salim ||| 
2020 ||| multi-scale adaptive task attention network for few-shot learning. ||| haoxing chen ||| huaxiong li ||| yaohui li ||| chunlin chen ||| 
2018 ||| anmm: ranking short answer texts with attention-based neural matching model. ||| liu yang ||| qingyao ai ||| jiafeng guo ||| w. bruce croft ||| 
2021 ||| muvam: a multi-view attention-based model for medical visual question answering. ||| haiwei pan ||| shuning he ||| kejia zhang ||| bo qu ||| chunling chen ||| kun shi ||| 
2021 ||| enconter: entity constrained progressive sequence generation via insertion-based transformer. ||| lee-hsun hsieh ||| yang-yin lee ||| ee-peng lim ||| 
2018 ||| multilingual constituency parsing with self-attention and pre-training. ||| nikita kitaev ||| dan klein ||| 
2021 ||| assistive tele-op: leveraging transformers to collect robotic task demonstrations. ||| henry m. clever ||| ankur handa ||| hammad mazhar ||| kevin parker ||| omer shapira ||| qian wan ||| yashraj s. narang ||| iretiayo akinola ||| maya cakmak ||| dieter fox ||| 
2021 ||| vis-top: visual transformer overlay processor. ||| wei hu ||| dian xu ||| zimeng fan ||| fang liu ||| yanxiang he ||| 
2020 ||| understanding image captioning models beyond visualizing attention. ||| jiamei sun ||| sebastian lapuschkin ||| wojciech samek ||| alexander binder ||| 
2020 ||| image super-resolution reconstruction based on attention mechanism and feature fusion. ||| jiawen lyn ||| sen yan ||| 
2020 ||| rsanet: recurrent slice-wise attention network for multiple sclerosis lesion segmentation. ||| hang zhang ||| jinwei zhang ||| qihao zhang ||| jeremy kim ||| shun zhang ||| susan a. gauthier ||| pascal spincemaille ||| thanh d. nguyen ||| mert r. sabuncu ||| yi wang ||| 
2017 ||| towards neural machine translation with latent tree attention. ||| james bradbury ||| richard socher ||| 
2020 ||| deep interest with hierarchical attention network for click-through rate prediction. ||| weinan xu ||| hengxu he ||| minshi tan ||| yunming li ||| jun lang ||| dongbai guo ||| 
2018 ||| pointgrow: autoregressively learned point cloud generation with self-attention. ||| yongbin sun ||| yue wang ||| ziwei liu ||| joshua e. siegel ||| sanjay e. sarma ||| 
2022 ||| no parameters left behind: sensitivity guided adaptive learning rate for training large transformer models. ||| chen liang ||| haoming jiang ||| simiao zuo ||| pengcheng he ||| xiaodong liu ||| jianfeng gao ||| weizhu chen ||| tuo zhao ||| 
2021 ||| dual-attention residual network for automatic diagnosis of covid-19. ||| jun shi ||| huite yi ||| xiaoyu hao ||| hong an ||| wei wei ||| 
2021 ||| spatial attention point network for deep-learning-based robust autonomous robot motion generation. ||| hideyuki ichiwara ||| hiroshi ito ||| kenjiro yamamoto ||| hiroki mori ||| tetsuya ogata ||| 
2021 ||| what all do audio transformer models hear? probing acoustic representations for language delivery and its structure. ||| jui shah ||| yaman kumar singla ||| changyou chen ||| rajiv ratn shah ||| 
2021 ||| couplformer: rethinking vision transformer with coupling attention map. ||| hai lan ||| xihao wang ||| xian wei ||| 
2022 ||| pay attention to relations: multi-embeddings for attributed multiplex networks. ||| joshua melton ||| michael ridenhour ||| siddharth krishnan ||| 
2021 ||| multi-modal motion prediction with transformer-based neural network for autonomous driving. ||| zhiyu huang ||| xiaoyu mo ||| chen lv ||| 
2018 ||| on the alignment problem in multi-head attention-based neural machine translation. ||| tamer alkhouli ||| gabriel bretschner ||| hermann ney ||| 
2020 ||| constructing a highlight classifier with an attention based lstm neural network. ||| michael kuehne ||| marius radu ||| 
2019 ||| fine-grained attention-based video face recognition. ||| zhaoxiang liu ||| huan hu ||| jinqiang bai ||| shaohua li ||| shiguo lian ||| 
2022 ||| cnn self-attention voice activity detector. ||| amit sofer ||| shlomo e. chazan ||| 
2021 ||| session-aware item-combination recommendation with transformer network. ||| tzu-heng lin ||| chen gao ||| 
2021 ||| a global-local attention framework for weakly labelled audio tagging. ||| helin wang ||| yuexian zou ||| wenwu wang ||| 
2021 ||| towards efficient cross-modal visual textual retrieval using transformer-encoder deep features. ||| nicola messina ||| giuseppe amato ||| fabrizio falchi ||| claudio gennaro ||| st ||| phane marchand-maillet ||| 
2020 ||| learning to fuse sentences with transformers for summarization. ||| logan lebanoff ||| franck dernoncourt ||| doo soon kim ||| lidan wang ||| walter chang ||| fei liu ||| 
2020 ||| multimodal transformer with pointer network for the dstc8 avsd challenge. ||| hung le ||| nancy f. chen ||| 
2021 ||| a nir-to-vis face recognition via part adaptive and relation attention module. ||| rushuang xu ||| myeongah cho ||| sangyoun lee ||| 
2021 ||| centeratt: fast 2-stage center attention network. ||| jianyun xu ||| xin tang ||| jian dou ||| xu shu ||| yushi zhu ||| 
2020 ||| bidirectional representation learning from transformers using multimodal electronic health record data for chronic to predict depression. ||| yiwen meng ||| william speier ||| michael k. ong ||| corey w. arnold ||| 
2018 ||| multistep speed prediction on traffic networks: a graph convolutional sequence-to-sequence learning approach with attention mechanism. ||| zhengchao zhang ||| meng li ||| xi lin ||| yinhai wang ||| fang he ||| 
2020 ||| sparse, dense, and attentional representations for text retrieval. ||| yi luan ||| jacob eisenstein ||| kristina toutanova ||| michael collins ||| 
2021 ||| weaving attention u-net: a novel hybrid cnn and attention-based method for organs-at-risk segmentation in head and neck ct images. ||| zhuangzhuang zhang ||| tianyu zhao ||| hiram gay ||| weixiong zhang ||| baozhou sun ||| 
2020 ||| context-aware cross-attention for non-autoregressive translation. ||| liang ding ||| longyue wang ||| di wu ||| dacheng tao ||| zhaopeng tu ||| 
2020 ||| on estimating gaze by self-attention augmented convolutions. ||| gabriel lefundes ||| luciano oliveira ||| 
2018 ||| logic attention based neighborhood aggregation for inductive knowledge graph embedding. ||| peifeng wang ||| jialong han ||| chenliang li ||| rong pan ||| 
2019 ||| gated group self-attention for answer selection. ||| dong xu ||| jianhui ji ||| haikuan huang ||| hongbo deng ||| wu-jun li ||| 
2018 ||| multi-level attention model for weakly supervised audio classification. ||| changsong yu ||| karim said barsim ||| qiuqiang kong ||| bin yang ||| 
2020 ||| attention beam: an image captioning approach. ||| anubhav shrimal ||| tanmoy chakraborty ||| 
2021 ||| on the power of saturated transformers: a view from circuit complexity. ||| william merrill ||| yoav goldberg ||| roy schwartz ||| noah a. smith ||| 
2020 ||| correction of faulty background knowledge based on condition aware and revise transformer for question answering. ||| xinyan zhao ||| xiao feng ||| haoming zhong ||| jun yao ||| huanhuan chen ||| 
2021 ||| solving arithmetic word problems with transformers and preprocessing of problem text. ||| kaden griffith ||| jugal kalita ||| 
2018 ||| placement delivery array design via attention-based deep neural network. ||| zhengming zhang ||| meng hua ||| chunguo li ||| yongming huang ||| luxi yang ||| 
2021 ||| discovering spatial relationships by transformers for domain generalization. ||| cuicui kang ||| karthik nandakumar ||| 
2019 ||| understanding and improving transformer from a multi-particle dynamic system point of view. ||| yiping lu ||| zhuohan li ||| di he ||| zhiqing sun ||| bin dong ||| tao qin ||| liwei wang ||| tie-yan liu ||| 
2020 ||| hand-crafted attention is all you need? a study of attention on self-supervised audio transformer. ||| tsung-han wu ||| chun-cheng hsieh ||| yen-hao chen ||| po-han chi ||| hung-yi lee ||| 
2019 ||| deja-vu: double feature presentation in deep transformer networks. ||| andros tjandra ||| chunxi liu ||| frank zhang ||| xiaohui zhang ||| yongqiang wang ||| gabriel synnaeve ||| satoshi nakamura ||| geoffrey zweig ||| 
2021 ||| person re-identification with a locally aware transformer. ||| charu sharma ||| siddhant raj kapil ||| david chapman ||| 
2021 ||| attentional prototype inference for few-shot semantic segmentation. ||| haoliang sun ||| xiankai lu ||| haochen wang ||| yilong yin ||| xiantong zhen ||| cees g. m. snoek ||| ling shao ||| 
2021 ||| what's in your head? emergent behaviour in multi-task transformer models. ||| mor geva ||| uri katz ||| aviv ben-arie ||| jonathan berant ||| 
2021 ||| multimodal continuous visual attention mechanisms. ||| ant ||| nio farinhas ||| andr |||  f. t. martins ||| pedro m. q. aguiar ||| 
2021 ||| causal attention for unbiased visual recognition. ||| tan wang ||| chang zhou ||| qianru sun ||| hanwang zhang ||| 
2020 ||| bertology meets biology: interpreting attention in protein language models. ||| jesse vig ||| ali madani ||| lav r. varshney ||| caiming xiong ||| richard socher ||| nazneen fatema rajani ||| 
2020 ||| cloud removal for remote sensing imagery via spatial attention generative adversarial network. ||| heng pan ||| 
2022 ||| dbt-net: dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement. ||| guochen yu ||| andong li ||| hui wang ||| yutian wang ||| yuxuan ke ||| chengshi zheng ||| 
2018 ||| show, attend and translate: unsupervised image translation with self-regularization and attention. ||| chao yang ||| taehwan kim ||| ruizhe wang ||| hao peng ||| c.-c. jay kuo ||| 
2017 ||| attention-aware face hallucination via deep reinforcement learning. ||| qingxing cao ||| liang lin ||| yukai shi ||| xiaodan liang ||| guanbin li ||| 
2020 ||| spatial attention pyramid network for unsupervised domain adaptation. ||| congcong li ||| dawei du ||| libo zhang ||| longyin wen ||| tiejian luo ||| yanjun wu ||| pengfei zhu ||| 
2021 ||| transformer transforms salient object detection and camouflaged object detection. ||| yuxin mao ||| jing zhang ||| zhexiong wan ||| yuchao dai ||| aixuan li ||| yunqiu lv ||| xinyu tian ||| deng-ping fan ||| nick barnes ||| 
2019 ||| pay attention to the activations: a modular attention mechanism for fine-grained image recognition. ||| pau rodr ||| guez l ||| pez ||| diego velazquez dorta ||| guillem cucurull ||| josep m. gonfaus ||| f. xavier roca ||| jordi gonz ||| lez sabat ||| 
2020 ||| natural image matting via guided contextual attention. ||| yaoyi li ||| hongtao lu ||| 
2022 ||| zero-shot long-form voice cloning with dynamic convolution attention. ||| artem gorodetskii ||| ivan ozhiganov ||| 
2018 ||| attention-gated networks for improving ultrasound scan plane detection. ||| jo schlemper ||| ozan oktay ||| liang chen ||| jacqueline matthew ||| caroline l. knight ||| bernhard kainz ||| ben glocker ||| daniel rueckert ||| 
2022 ||| switch trajectory transformer with distributional value approximation for multi-task reinforcement learning. ||| qinjie lin ||| han liu ||| biswa sengupta ||| 
2021 ||| hybrid attention network based on progressive embedding scale-context for crowd counting. ||| fusen wang ||| jun sang ||| zhongyuan wu ||| qi liu ||| nong sang ||| 
2020 ||| on the effectiveness of vision transformers for zero-shot face anti-spoofing. ||| anjith george ||| s ||| bastien marcel ||| 
2021 ||| efficient pre-training objectives for transformers. ||| luca di liello ||| matteo gabburo ||| alessandro moschitti ||| 
2021 ||| gtn-ed: event detection using graph transformer networks. ||| sanghamitra dutta ||| liang ma ||| tanay kumar saha ||| di lu ||| joel r. tetreault ||| alex jaimes ||| 
2021 ||| lavt: language-aware vision transformer for referring image segmentation. ||| zhao yang ||| jiaqi wang ||| yansong tang ||| kai chen ||| hengshuang zhao ||| philip h. s. torr ||| 
2019 ||| factor graph attention. ||| idan schwartz ||| seunghak yu ||| tamir hazan ||| alexander g. schwing ||| 
2021 ||| floorlevel-net: recognizing floor-level lines with height-attention-guided multi-task learning. ||| mengyang wu ||| wei zeng ||| chi-wing fu ||| 
2018 ||| variational self-attention model for sentence representation. ||| qiang zhang ||| shangsong liang ||| emine yilmaz ||| 
2022 ||| wavelet-attention cnn for image classification. ||| xiangyu zhao ||| 
2019 ||| characterizing collective attention via descriptor context in public discussions of crisis events. ||| ian stewart ||| diyi yang ||| jacob eisenstein ||| 
2021 ||| covid-19 fake news detection using bidirectional encoder representations from transformers based models. ||| yuxiang wang ||| yongheng zhang ||| xuebo li ||| xinyao yu ||| 
2021 ||| exploring corruption robustness: inductive biases in vision transformers and mlp-mixers. ||| katelyn morrison ||| benjamin gilby ||| colton lipchak ||| adam mattioli ||| adriana kovashka ||| 
2019 ||| your local gan: designing two dimensional local attention mechanisms for generative models. ||| giannis daras ||| augustus odena ||| han zhang ||| alexandros g. dimakis ||| 
2019 ||| attention-based convolutional neural network for weakly labeled human activities recognition with wearable sensors. ||| kun wang ||| jun he ||| lei zhang ||| 
2021 ||| discodvt: generating long text with discourse-aware discrete variational transformer. ||| haozhe ji ||| minlie huang ||| 
2021 ||| clickbait headline detection in indonesian news sites using multilingual bidirectional encoder representations from transformers (m-bert). ||| muhammad n. fakhruzzaman ||| saidah z. jannah ||| ratih a. ningrum ||| indah fahmiyah ||| 
2018 ||| action recognition with visual attention on skeleton images. ||| zhengyuan yang ||| yuncheng li ||| jianchao yang ||| jiebo luo ||| 
2021 ||| image denoising using attention-residual convolutional neural networks. ||| rafael goncalves pires ||| daniel f. s. santos ||| marcos c. s. santana ||| cl ||| udio f. g. santos ||| jo ||| o paulo papa ||| 
2022 ||| attention-based dual supervised decoder for rgbd semantic segmentation. ||| yang zhang ||| yang yang ||| chenyun xiong ||| guodong sun ||| yanwen guo ||| 
2020 ||| limits to depth efficiencies of self-attention. ||| yoav levine ||| noam wies ||| or sharir ||| hofit bata ||| amnon shashua ||| 
2017 ||| dynamic computational time for visual attention. ||| zhichao li ||| yi yang ||| xiao liu ||| shilei wen ||| wei xu ||| 
2021 ||| multimodal-boost: multimodal medical image super-resolution using multi-attention network with wavelet transform. ||| farah deeba ||| fayaz ali dharejo ||| muhammad zawish ||| yuanchun zhou ||| kapal dev ||| sunder ali khowaja ||| nawab muhammad faseeh qureshi ||| 
2020 ||| better distractions: transformer-based distractor generation and multiple choice question filtering. ||| jeroen offerijns ||| suzan verberne ||| tessa verhoef ||| 
2020 ||| multiscale attention guided network for covid-19 detection using chest x-ray images. ||| jingxiong li ||| yaqi wang ||| shuai wang ||| jun wang ||| jun liu ||| qun jin ||| lingling sun ||| 
2021 ||| cross-sean: a cross-stitch semi-supervised neural attention model for covid-19 fake news detection. ||| william scott paka ||| rachit bansal ||| abhay kaushik ||| shubhashis sengupta ||| tanmoy chakraborty ||| 
2021 ||| efficient transformers in reinforcement learning using actor-learner distillation. ||| emilio parisotto ||| ruslan salakhutdinov ||| 
2021 ||| semantic segmentation on vspw dataset through aggregation of transformer models. ||| zixuan chen ||| junhong zou ||| xiaotao wang ||| 
2021 ||| sa-matd3: self-attention-based multi-agent continuous control method in cooperative environments. ||| kai liu ||| yuyang zhao ||| gang wang ||| bei peng ||| 
2020 ||| attention-based quantum tomography. ||| peter cha ||| paul ginsparg ||| felix wu ||| juan carrasquilla ||| peter l. mcmahon ||| eun-ah kim ||| 
2019 ||| deep modular co-attention networks for visual question answering. ||| zhou yu ||| jun yu ||| yuhao cui ||| dacheng tao ||| qi tian ||| 
2021 ||| stableemit: selection probability discount for reducing emission latency of streaming monotonic attention asr. ||| hirofumi inaguma ||| tatsuya kawahara ||| 
2018 ||| sdnet: contextualized attention-based deep network for conversational question answering. ||| chenguang zhu ||| michael zeng ||| xuedong huang ||| 
2018 ||| learning to match transient sound events using attentional similarity for few-shot sound recognition. ||| szu-yu chou ||| kai-hsiang cheng ||| jyh-shing roger jang ||| yi-hsuan yang ||| 
2019 ||| transformer to cnn: label-scarce distillation for efficient text classification. ||| yew ken chia ||| sam witteveen ||| martin andrews ||| 
2022 ||| a squeeze-and-excitation and transformer based cross-task system for environmental sound recognition. ||| jisheng bai ||| jianfeng chen ||| mou wang ||| muhammad saad ayub ||| 
2018 ||| seq2graph: discovering dynamic dependencies from multivariate time series with multi-level attention. ||| xuan-hong dang ||| syed yousaf shah ||| petros zerfos ||| 
2021 ||| progressive transformer-based generation of radiology reports. ||| farhad nooralahzadeh ||| nicolas perez gonzalez ||| thomas frauenfelder ||| koji fujimoto ||| michael krauthammer ||| 
2021 ||| m2a: motion aware attention for accurate video action recognition. ||| brennan gebotys ||| alexander wong ||| david a. clausi ||| 
2022 ||| pami-ad: an activity detector exploiting part-attention and motion information in surveillance videos. ||| yunhao du ||| zhihang tong ||| junfeng wan ||| binyu zhang ||| yanyun zhao ||| 
2021 ||| (asna) an attention-based siamese-difference neural network with surrogate ranking loss function for perceptual image quality assessment. ||| seyed mehdi ayyoubzadeh ||| ali royat ||| 
2018 ||| attention u-net: learning where to look for the pancreas. ||| ozan oktay ||| jo schlemper ||| lo ||| c le folgoc ||| matthew c. h. lee ||| mattias p. heinrich ||| kazunari misawa ||| kensaku mori ||| steven g. mcdonagh ||| nils y. hammerla ||| bernhard kainz ||| ben glocker ||| daniel rueckert ||| 
2020 ||| attentional bottleneck: towards an interpretable deep driving network. ||| jinkyu kim ||| mayank bansal ||| 
2019 ||| structure tree-lstm: structure-aware attentional document encoders. ||| khalil mrini ||| claudiu musat ||| michael baeriswyl ||| martin jaggi ||| 
2021 ||| multi-view stereo network with attention thin volume. ||| zihang wan ||| 
2018 ||| deep attention-guided hashing. ||| zhan yang ||| osolo ian raymond ||| wuqing sun ||| jun long ||| 
2020 ||| on the importance of local information in transformer based models. ||| madhura pande ||| aakriti budhraja ||| preksha nema ||| pratyush kumar ||| mitesh m. khapra ||| 
2017 ||| predicting human interaction via relative attention model. ||| yichao yan ||| bingbing ni ||| xiaokang yang ||| 
2020 ||| safcar: structured attention fusion for compositional action recognition. ||| tae soo kim ||| gregory d. hager ||| 
2020 ||| hyperspectral image classification based on multi-scale residual network with attention mechanism. ||| xiangdong zhang ||| tengjun wang ||| yun yang ||| 
2019 ||| scram: spatially coherent randomized attention maps. ||| dan andrei calian ||| peter roelants ||| jacques cal ||| ben carr ||| krishna dubba ||| john e. reid ||| dell zhang ||| 
2021 ||| pay more attention to history: a context modeling strategy for conversational text-to-sql. ||| yuntao li ||| hanchu zhang ||| yutian li ||| sirui wang ||| wei wu ||| yan zhang ||| 
2021 ||| jointly learning truth-conditional denotations and groundings using parallel attention. ||| leon bergen ||| dzmitry bahdanau ||| timothy j. o'donnell ||| 
2019 ||| interpretable icd code embeddings with self- and mutual-attention mechanisms. ||| dixin luo ||| hongteng xu ||| lawrence carin ||| 
2021 ||| ia-gcn: interpretable attention based graph convolutional network for disease prediction. ||| anees kazi ||| soroush farghadani ||| nassir navab ||| 
2022 ||| learning patch-to-cluster attention in vision transformer. ||| ryan grainger ||| thomas paniagua ||| xi song ||| tianfu wu ||| 
2019 ||| from balustrades to pierre vinken: looking for syntax in transformer self-attentions. ||| david marecek ||| rudolf rosa ||| 
2021 ||| lesion segmentation and recist diameter prediction via click-driven attention and dual-path connection. ||| youbao tang ||| ke yan ||| jinzheng cai ||| lingyun huang ||| guotong xie ||| jing xiao ||| jingjing lu ||| gigin lin ||| le lu ||| 
2022 ||| dkma-uld: domain knowledge augmented multi-head attention based robust universal lesion detection. ||| manu sheoran ||| meghal dani ||| monika sharma ||| lovekesh vig ||| 
2020 ||| alanet: adaptive latent attention network forjoint video deblurring and interpolation. ||| akash gupta ||| abhishek aich ||| amit k. roy-chowdhury ||| 
2019 ||| attention for inference compilation. ||| william harvey ||| andreas munk ||| atilim g ||| nes baydin ||| alexander bergholm ||| frank wood ||| 
2021 ||| megan: memory enhanced graph attention network for space-time video super-resolution. ||| chenyu you ||| lianyi han ||| aosong feng ||| ruihan zhao ||| hui tang ||| wei fan ||| 
2020 ||| towards character-level transformer nmt by finetuning subword systems. ||| jindrich libovick ||| alexander fraser ||| 
2021 ||| efficient training of audio transformers with patchout. ||| khaled koutini ||| jan schl ||| ter ||| hamid eghbal-zadeh ||| gerhard widmer ||| 
2019 ||| intentional attention mask transformation for robust cnn classification. ||| masanari kimura ||| masayuki tanaka ||| 
2020 ||| glancing transformer for non-autoregressive neural machine translation. ||| lihua qian ||| hao zhou ||| yu bao ||| mingxuan wang ||| lin qiu ||| weinan zhang ||| yong yu ||| lei li ||| 
2022 ||| efficient non-local contrastive attention for image super-resolution. ||| bin xia ||| yucheng hang ||| yapeng tian ||| wenming yang ||| qingmin liao ||| jie zhou ||| 
2017 ||| attention-based mixture density recurrent networks for history-based recommendation. ||| tian wang ||| kyunghyun cho ||| 
2021 ||| dot: an efficient double transformer for nlp tasks with tables. ||| syrine krichene ||| thomas m ||| ller ||| julian martin eisenschlos ||| 
2019 ||| efficient attention mechanism for handling all the interactions between many inputs with application to visual dialog. ||| van-quang nguyen ||| masanori suganuma ||| takayuki okatani ||| 
2021 ||| a video is worth three views: trigeminal transformers for video-based person re-identification. ||| xuehu liu ||| pingping zhang ||| chenyang yu ||| huchuan lu ||| xuesheng qian ||| xiaoyun yang ||| 
2021 ||| semantics-aware attention improves neural machine translation. ||| aviv slobodkin ||| leshem choshen ||| omri abend ||| 
2021 ||| automated tabulation of clinical trial results: a joint entity and relation extraction approach with transformer-based language representations. ||| jetsun whitton ||| anthony hunter ||| 
2020 ||| peak detection on data independent acquisition mass spectrometry data with semisupervised convolutional transformers. ||| leon l. xu ||| hannes l. r ||| st ||| 
2020 ||| trade: transformers for density estimation. ||| rasool fakoor ||| pratik chaudhari ||| jonas mueller ||| alexander j. smola ||| 
2020 ||| multimodal matching transformer for live commenting. ||| chaoqun duan ||| lei cui ||| shuming ma ||| furu wei ||| conghui zhu ||| tiejun zhao ||| 
2018 ||| deep learning with attention to predict gestational age of the fetal brain. ||| liyue shen ||| katie s. shpanskaya ||| edward h. lee ||| emily mckenna ||| maryam maleki ||| quin lu ||| safwan halabi ||| john m. pauly ||| kristen w. yeom ||| 
2021 ||| a self-distillation embedded supervised affinity attention model for few-shot segmentation. ||| qi zhao ||| binghao liu ||| shuchang lyu ||| xu wang ||| yifan yang ||| 
2021 ||| a dual-attention neural network for pun location and using pun-gloss pairs for interpretation. ||| shen liu ||| meirong ma ||| hao yuan ||| jianchao zhu ||| yuanbin wu ||| man lan ||| 
2019 ||| transformers with convolutional context for asr. ||| abdelrahman mohamed ||| dmytro okhonko ||| luke zettlemoyer ||| 
2021 ||| hierarchical task learning from language instructions with unified transformers and self-monitoring. ||| yichi zhang ||| joyce chai ||| 
2020 ||| memory controlled sequential self attention for sound recognition. ||| arjun pankajakshan ||| helen l. bear ||| vinod subramanian ||| emmanouil benetos ||| 
2021 ||| sparse spatial transformers for few-shot learning. ||| haoxing chen ||| huaxiong li ||| yaohui li ||| chunlin chen ||| 
2021 ||| ndt-transformer: large-scale 3d point cloud localisation using the normal distribution transform representation. ||| zhicheng zhou ||| cheng zhao ||| daniel adolfsson ||| songzhi su ||| yang gao ||| tom duckett ||| li sun ||| 
2021 ||| an end-to-end transformer model for 3d object detection. ||| ishan misra ||| rohit girdhar ||| armand joulin ||| 
2021 ||| ssat: a symmetric semantic-aware transformer network for makeup transfer and removal. ||| zhaoyang sun ||| yaxiong chen ||| shengwu xiong ||| 
2021 ||| osteoporosis prescreening using panoramic radiographs through a deep convolutional neural network with attention mechanism. ||| heng fan ||| jiaxiang ren ||| jie yang ||| yi-xian qin ||| haibin ling ||| 
2022 ||| multi-modal learning for au detection based on multi-head fused transformers. ||| xiang zhang ||| lijun yin ||| 
2020 ||| deep feature mining via attention-based bilstm-gcn for human motor imagery recognition. ||| yimin hou ||| shuyue jia ||| shu zhang ||| xiangmin lun ||| yan shi ||| yang li ||| hanrui yang ||| rui zeng ||| jinglei lv ||| 
2020 ||| crosstransformers: spatially-aware few-shot transfer. ||| carl doersch ||| ankush gupta ||| andrew zisserman ||| 
2021 ||| iterative se(3)-transformers. ||| fabian b. fuchs ||| edward wagstaff ||| justas dauparas ||| ingmar posner ||| 
2021 ||| brain tumor segmentation and survival prediction using 3d attention unet. ||| mobarakol islam ||| vibashan vs ||| v. jeya maria jose ||| navodini wijethilake ||| uppal utkarsh ||| hongliang ren ||| 
2018 ||| arbitrary style transfer with style-attentional networks. ||| dae young park ||| kwang hee lee ||| 
2018 ||| improving distant supervision with maxpooled attention and sentence-level supervision. ||| iz beltagy ||| kyle lo ||| waleed ammar ||| 
2021 ||| scalable visual transformers with hierarchical pooling. ||| zizheng pan ||| bohan zhuang ||| jing liu ||| haoyu he ||| jianfei cai ||| 
2018 ||| self-attention recurrent network for saliency detection. ||| fengdong sun ||| wenhui li ||| yuanyuan guan ||| 
2020 ||| the costs and benefits of goal-directed attention in deep convolutional neural networks. ||| xiaoliang luo ||| brett d. roads ||| bradley c. love ||| 
2021 ||| multilingual speech translation with unified transformer: huawei noah's ark lab at iwslt 2021. ||| xingshan zeng ||| liangyou li ||| qun liu ||| 
2021 ||| transweather: transformer-based restoration of images degraded by adverse weather conditions. ||| jeya maria jose valanarasu ||| rajeev yasarla ||| vishal m. patel ||| 
2017 ||| an attention-based deep net for learning to rank. ||| baiyang wang ||| diego klabjan ||| 
2020 ||| contrastive triple extraction with generative transformer. ||| hongbin ye ||| ningyu zhang ||| shumin deng ||| mosha chen ||| chuanqi tan ||| fei huang ||| huajun chen ||| 
2021 ||| sparse mlp for image recognition: is self-attention really necessary? ||| chuanxin tang ||| yucheng zhao ||| guangting wang ||| chong luo ||| wenxuan xie ||| wenjun zeng ||| 
2020 ||| inno at semeval-2020 task 11: leveraging pure transformer for multi-class propaganda detection. ||| dmitry grigorev ||| vladimir ivanov ||| 
2021 ||| making attention mechanisms more robust and interpretable with virtual adversarial training for semi-supervised text classification. ||| shunsuke kitada ||| hitoshi iyatomi ||| 
2020 ||| transformer hawkes process. ||| simiao zuo ||| haoming jiang ||| zichong li ||| tuo zhao ||| hongyuan zha ||| 
2021 ||| learning synergistic attention for light field salient object detection. ||| yi zhang ||| geng chen ||| qian chen ||| yujia sun ||| olivier d ||| forges ||| wassim hamidouche ||| lu zhang ||| 
2022 ||| self-attention for incomplete utterance rewriting. ||| yong zhang ||| zhitao li ||| jianzong wang ||| ning cheng ||| jing xiao ||| 
2020 ||| improving natural language processing tasks with human gaze-guided neural attention. ||| ekta sood ||| simon tannert ||| philipp m ||| ller ||| andreas bulling ||| 
2020 ||| attention-based transformers for instance segmentation of cells in microstructures. ||| tim prangemeier ||| christoph reich ||| heinz koeppl ||| 
2022 ||| manner: multi-view attention network for noise erasure. ||| hyun joon park ||| byung ha kang ||| wooseok shin ||| jin sob kim ||| sung won han ||| 
2021 ||| spelling correction with denoising transformer. ||| alex kuznetsov ||| hector urdiales ||| 
2018 ||| set transformer. ||| juho lee ||| yoonho lee ||| jungtaek kim ||| adam r. kosiorek ||| seungjin choi ||| yee whye teh ||| 
2019 ||| ouroboros: on accelerating training of transformer-based language models. ||| qian yang ||| zhouyuan huo ||| wenlin wang ||| heng huang ||| lawrence carin ||| 
2021 ||| an attention model to analyse the risk of agitation and urinary tract infections in people with dementia. ||| honglin li ||| roonak rezvani ||| magdalena anita kolanko ||| david j. sharp ||| maitreyee wairagkar ||| ravi vaidyanathan ||| ramin nilforooshan ||| payam m. barnaghi ||| 
2019 ||| repurposing decoder-transformer language models for abstractive summarization. ||| luke de oliveira ||| alfredo l ||| inez rodrigo ||| 
2022 ||| hypertransformer: a textural and spectral feature fusion transformer for pansharpening. ||| wele gedara chaminda bandara ||| vishal m. patel ||| 
2021 ||| studying the effects of self-attention for medical image analysis. ||| adrit rao ||| jongchan park ||| sanghyun woo ||| joon-young lee ||| oliver aalami ||| 
2020 ||| dual-attention gan for large-pose face frontalization. ||| yu yin ||| songyao jiang ||| joseph p. robinson ||| yun fu ||| 
2021 ||| a lightweight graph transformer network for human mesh reconstruction from 2d human pose. ||| ce zheng ||| mat ||| as mendieta ||| pu wang ||| aidong lu ||| chen chen ||| 
2021 ||| e(2) equivariant self-attention for radio astronomy. ||| micah bowles ||| matthew bromley ||| max allen ||| anna scaife ||| 
2018 ||| deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization. ||| guoliang kang ||| liang zheng ||| yan yan ||| yi yang ||| 
2019 ||| griddehazenet: attention-based multi-scale network for image dehazing. ||| xiaohong liu ||| yongrui ma ||| zhihao shi ||| jun chen ||| 
2019 ||| context-aware self-attention networks. ||| baosong yang ||| jian li ||| derek f. wong ||| lidia s. chao ||| xing wang ||| zhaopeng tu ||| 
2021 ||| content-aware directed propagation network with pixel adaptive kernel attention. ||| min-cheol sagong ||| yoon-jae yeo ||| seung-won jung ||| sung-jea ko ||| 
2022 ||| attentional feature refinement and alignment network for aircraft detection in sar imagery. ||| yan zhao ||| lingjun zhao ||| zhong liu ||| dewen hu ||| gangyao kuang ||| li liu ||| 
2021 ||| certified patch robustness via smoothed vision transformers. ||| hadi salman ||| saachi jain ||| eric wong ||| aleksander madry ||| 
2020 ||| a further study of unsupervised pre-training for transformer based speech recognition. ||| dongwei jiang ||| wubo li ||| ruixiong zhang ||| miao cao ||| ne luo ||| yang han ||| wei zou ||| xiangang li ||| 
2022 ||| an intellectual property entity recognition method based on transformer and technological word information. ||| yuhui wang ||| junping du ||| yingxia shao ||| 
2017 ||| learning what to look in chest x-rays with a recurrent visual attention model. ||| petros-pavlos ypsilantis ||| giovanni montana ||| 
2018 ||| sequential image-based attention network for inferring force estimation without haptic sensor. ||| hochul shin ||| dongyi kim ||| daekwan ko ||| soo-chul lim ||| wonjun hwang ||| 
2018 ||| attention solves your tsp. ||| wouter kool ||| max welling ||| 
2021 ||| stepping back to smiles transformers for fast molecular representation inference. ||| wenhao zhu ||| ziyao li ||| lingsheng cai ||| guojie song ||| 
2020 ||| modeling document interactions for learning to rank with regularized self-attention. ||| shuo sun ||| kevin duh ||| 
2020 ||| modeling limited attention in opinion dynamics by topological interactions. ||| francesca ceragioli ||| paolo frasca ||| wilbert samuel rossi ||| 
2021 ||| festa: flow estimation via spatial-temporal attention for scene point clouds. ||| haiyan wang ||| jiahao pang ||| muhammad a. lodhi ||| yingli tian ||| dong tian ||| 
2018 ||| deep metric learning by online soft mining and class-aware attention. ||| xinshao wang ||| yang hua ||| elyor kodirov ||| guosheng hu ||| neil martin robertson ||| 
2019 ||| an attention-based multi-resolution model for prostate whole slide imageclassification and localization. ||| jiayun li ||| wenyuan li ||| arkadiusz gertych ||| beatrice s. knudsen ||| william speier ||| corey w. arnold ||| 
2021 ||| han: an efficient hierarchical self-attention network for skeleton-based gesture recognition. ||| jianbo liu ||| ying wang ||| shiming xiang ||| chunhong pan ||| 
2021 ||| tsformer: time series transformer for tourism demand forecasting. ||| siyuan yi ||| xing chen ||| chuanming tang ||| 
2019 ||| image super-resolution via residual blended attention generative adversarial network with dual discriminators. ||| 
2019 ||| har-net: joint learning of hybrid attention for single-stage object detection. ||| ya-li li ||| shengjin wang ||| 
2022 ||| handcrafted histological transformer (h2t): unsupervised representation of whole slide images. ||| quoc dang vu ||| kashif rajpoot ||| shan e ahmed raza ||| nasir m. rajpoot ||| 
2020 ||| self-attention attribution: interpreting information interactions inside transformer. ||| yaru hao ||| li dong ||| furu wei ||| ke xu ||| 
2020 ||| exploring transformers for large-scale speech recognition. ||| liang lu ||| changliang liu ||| jinyu li ||| yifan gong ||| 
2021 ||| how knowledge graph and attention help? a quantitative analysis into bag-level relation extraction. ||| zikun hu ||| yixin cao ||| lifu huang ||| tat-seng chua ||| 
2019 ||| aanet: attribute attention network for person re-identifications. ||| chiat-pin tay ||| sharmili roy ||| kim-hui yap ||| 
2020 ||| coot: cooperative hierarchical transformer for video-text representation learning. ||| simon ging ||| mohammadreza zolfaghari ||| hamed pirsiavash ||| thomas brox ||| 
2021 ||| progressive co-attention network for fine-grained visual classification. ||| tian zhang ||| dongliang chang ||| zhanyu ma ||| jun guo ||| 
2021 ||| attention-based multi-channel speaker verification with ad-hoc microphone arrays. ||| chengdong liang ||| junqi chen ||| shanzheng guan ||| xiao-lei zhang ||| 
2019 ||| co-attention hierarchical network: generating coherent long distractors for reading comprehension. ||| xiaorui zhou ||| senlin luo ||| yunfang wu ||| 
2020 ||| knowing what to listen to: early attention for deep speech representation learning. ||| amirhossein hajavi ||| ali etemad ||| 
2020 ||| attention transfer network for aspect-level sentiment classification. ||| fei zhao ||| zhen wu ||| xinyu dai ||| 
2022 ||| neuroplastic graph attention networks for nuclei segmentation in histopathology images. ||| yoav alon ||| huiyu zhou ||| 
2020 ||| s-transformer: segment-transformer for robust neural speech synthesis. ||| xi wang ||| huaiping ming ||| lei he ||| frank k. soong ||| 
2021 ||| what context features can transformer language models use? ||| joe o'connor ||| jacob andreas ||| 
2021 ||| mesa: a memory-saving training framework for transformers. ||| zizheng pan ||| peng chen ||| haoyu he ||| jing liu ||| jianfei cai ||| bohan zhuang ||| 
2020 ||| dual-stream maximum self-attention multi-instance learning. ||| bin li ||| kevin w. eliceiri ||| 
2021 ||| eformer: edge enhancement based transformer for medical image denoising. ||| achleshwar luthra ||| harsh sulakhe ||| tanish mittal ||| abhishek iyer ||| santosh yadav ||| 
2020 ||| structure-aware pre-training for table understanding with tree-based transformers. ||| zhiruo wang ||| haoyu dong ||| ran jia ||| jia li ||| zhiyi fu ||| shi han ||| dongmei zhang ||| 
2021 ||| accurate prediction of free solvation energy of organic molecules via graph attention network and message passing neural network from pairwise atomistic interactions. ||| ramin ansari ||| amirata ghorbani ||| 
2020 ||| motion segmentation using frequency domain transformer networks. ||| hafez farazi ||| sven behnke ||| 
2021 ||| ssast: self-supervised audio spectrogram transformer. ||| yuan gong ||| cheng-i jeff lai ||| yu-an chung ||| james r. glass ||| 
2021 ||| levi graph amr parser using heterogeneous attention. ||| han he ||| jinho d. choi ||| 
2018 ||| adcrowdnet: an attention-injective deformable convolutional network for crowd understanding. ||| ning liu ||| yongchao long ||| changqing zou ||| qun niu ||| li pan ||| hefeng wu ||| 
2020 ||| tourism demand forecasting with tourist attention: an ensemble deep learning approach. ||| shaolong sun ||| yanzhao li ||| shouyang wang ||| ju'e guo ||| 
2021 ||| gophormer: ego-graph transformer for node classification. ||| jianan zhao ||| chaozhuo li ||| qianlong wen ||| yiqi wang ||| yuming liu ||| hao sun ||| xing xie ||| yanfang ye ||| 
2020 ||| large scale legal text classification using transformer models. ||| zein shaheen ||| gerhard wohlgenannt ||| erwin filtz ||| 
2018 ||| question type guided attention in visual question answering. ||| yang shi ||| tommaso furlanello ||| sheng zha ||| animashree anandkumar ||| 
2019 ||| interrogating the explanatory power of attention in neural machine translation. ||| pooya moradi ||| nishan ||| t kambhatla ||| anoop sarkar ||| 
2022 ||| interpreting arabic transformer models. ||| ahmed abdelali ||| nadir durrani ||| fahim dalvi ||| hassan sajjad ||| 
2018 ||| an attention-based approach for single image super resolution. ||| yuan liu ||| yuancheng wang ||| nan li ||| xu cheng ||| yifeng zhang ||| yongming huang ||| guojun lu ||| 
2021 ||| learning delaunay triangulation using self-attention and domain knowledge. ||| jaeseung lee ||| woojin choi ||| jibum kim ||| 
2018 ||| cold-start aware user and product attention for sentiment classification. ||| reinald kim amplayo ||| jihyeok kim ||| sua sung ||| seung-won hwang ||| 
2019 ||| learning deep transformer models for machine translation. ||| qiang wang ||| bei li ||| tong xiao ||| jingbo zhu ||| changliang li ||| derek f. wong ||| lidia s. chao ||| 
2018 ||| glac net: glocal attention cascading networks for multi-image cued story generation. ||| taehyeong kim ||| min-oh heo ||| seonil son ||| kyoung-wha park ||| byoung-tak zhang ||| 
2019 ||| pseudo random number generators: attention for a newly proposed generator. ||| hiroshi haramoto ||| makoto matsumoto ||| mutsuo saito ||| 
2022 ||| contrastive embedding distribution refinement and entropy-aware attention for 3d point cloud classification. ||| feng yang ||| yichao cao ||| qifan xue ||| shuai jin ||| xuanpeng li ||| weigong zhang ||| 
2020 ||| context-based transformer models for answer sentence selection. ||| ivano lauriola ||| alessandro moschitti ||| 
2020 ||| predicting goal-directed human attention using inverse reinforcement learning. ||| zhibo yang ||| lihan huang ||| yupei chen ||| zijun wei ||| seoyoung ahn ||| gregory j. zelinsky ||| dimitris samaras ||| minh hoai ||| 
2021 ||| ted-net: convolution-free t2t vision transformer-based encoder-decoder dilation network for low-dose ct denoising. ||| dayang wang ||| zhan wu ||| hengyong yu ||| 
2021 ||| pushing the limits of rule reasoning in transformers through natural language satisfiability. ||| kyle richardson ||| ashish sabharwal ||| 
2021 ||| learning tracking representations via dual-branch fully transformer networks. ||| fei xie ||| chunyu wang ||| guangting wang ||| wankou yang ||| wenjun zeng ||| 
2017 ||| variational attention for sequence-to-sequence models. ||| hareesh bahuleyan ||| lili mou ||| olga vechtomova ||| pascal poupart ||| 
2020 ||| anchor attention for hybrid crowd forecasts aggregation. ||| yuzhong huang ||| andr ||| s abeliuk ||| fred morstatter ||| pavel atanasov ||| aram galstyan ||| 
2021 ||| 4d attention-based neural network for eeg emotion recognition. ||| guowen xiao ||| mengwen ye ||| bowen xu ||| zhendi chen ||| quansheng ren ||| 
2022 ||| game-on: graph attention network based multimodal fusion for fake news detection. ||| mudit dhawan ||| shakshi sharma ||| aditya kadam ||| rajesh sharma ||| ponnurangam kumaraguru ||| 
2021 ||| attention actor-critic algorithm for multi-agent constrained co-operative reinforcement learning. ||| p. parnika ||| raghuram bharadwaj diddigi ||| sai koti reddy danda ||| shalabh bhatnagar ||| 
2019 ||| orthogonality constrained multi-head attention for keyword spotting. ||| mingu lee ||| jinkyu lee ||| hye jin jang ||| byeonggeun kim ||| wonil chang ||| kyuwoong hwang ||| 
2022 ||| self-attention neural bag-of-features. ||| kateryna chumachenko ||| alexandros iosifidis ||| moncef gabbouj ||| 
2021 ||| mect: multi-metadata embedding based cross-transformer for chinese named entity recognition. ||| shuang wu ||| xiaoning song ||| zhen-hua feng ||| 
2020 ||| occluded prohibited items detection: an x-ray security inspection benchmark and de-occlusion attention module. ||| yanlu wei ||| renshuai tao ||| zhangjie wu ||| yuqing ma ||| libo zhang ||| xianglong liu ||| 
2022 ||| coarse-to-fine sparse transformer for hyperspectral image reconstruction. ||| jing lin ||| yuanhao cai ||| xiaowan hu ||| haoqian wang ||| xin yuan ||| yulun zhang ||| radu timofte ||| luc van gool ||| 
2019 ||| analysing coreference in transformer outputs. ||| ekaterina lapshinova-koltunski ||| cristina espa ||| a-bonet ||| josef van genabith ||| 
2021 ||| geometry-contrastive transformer for generalized 3d pose transfer. ||| haoyu chen ||| hao tang ||| zitong yu ||| nicu sebe ||| guoying zhao ||| 
2020 ||| should we hard-code the recurrence concept or learn it instead ? exploring the transformer architecture for audio-visual speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2022 ||| attention cannot be an explanation. ||| arjun r. akula ||| song-chun zhu ||| 
2022 ||| attribute surrogates learning and spectral tokens pooling in transformers for few-shot learning. ||| yangji he ||| weihan liang ||| dongyang zhao ||| hong-yu zhou ||| weifeng ge ||| yizhou yu ||| wenqiang zhang ||| 
2018 ||| fpan: fine-grained and progressive attention localization network for data retrieval. ||| sijia chen ||| bin song ||| jie guo ||| xiaojiang du ||| mohsen guizani ||| 
2021 ||| temporal convolutional networks and transformers for classifying the sleep stage in awake or asleep using pulse oximetry signals. ||| ramiro casal ||| leandro e. di persia ||| gast ||| n schlotthauer ||| 
2020 ||| defraudnet: end2end fingerprint spoof detection using patch level attention. ||| b. v. s. anusha ||| sayan banerjee ||| subhasis chaudhuri ||| 
2020 ||| on layer normalization in the transformer architecture. ||| ruibin xiong ||| yunchang yang ||| di he ||| kai zheng ||| shuxin zheng ||| chen xing ||| huishuai zhang ||| yanyan lan ||| liwei wang ||| tie-yan liu ||| 
2019 ||| u-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation. ||| junho kim ||| minjae kim ||| hyeonwoo kang ||| kwanghee lee ||| 
2018 ||| modeling and predicting popularity dynamics via deep learning attention mechanism. ||| sha yuan ||| yu zhang ||| jie tang ||| huawei shen ||| xingxing wei ||| 
2021 ||| interpreting deep learning based cerebral palsy prediction with channel attention. ||| manli zhu ||| qianhui men ||| edmond s. l. ho ||| howard leung ||| hubert p. h. shum ||| 
2018 ||| why self-attention? a targeted evaluation of neural machine translation architectures. ||| gongbo tang ||| mathias m ||| ller ||| annette rios ||| rico sennrich ||| 
2020 ||| enhancing the interpretability of deep models in heathcare through attention: application to glucose forecasting for diabetic people. ||| maxime de bois ||| mounim a. el-yacoubi ||| mehdi ammi ||| 
2020 ||| discrete variational attention models for language generation. ||| xianghong fang ||| haoli bai ||| zenglin xu ||| michael r. lyu ||| irwin king ||| 
2020 ||| an optimal transport kernel for feature aggregation and its relationship to attention. ||| gr ||| goire mialon ||| dexiong chen ||| alexandre d'aspremont ||| julien mairal ||| 
2021 ||| localvit: bringing locality to vision transformers. ||| yawei li ||| kai zhang ||| jiezhang cao ||| radu timofte ||| luc van gool ||| 
2020 ||| multi-encoder-decoder transformer for code-switching speech recognition. ||| xinyuan zhou ||| emre yilmaz ||| yanhua long ||| yijie li ||| haizhou li ||| 
2021 ||| resolution-invariant person reid based on feature transformation and self-weighted attention. ||| ziyue zhang ||| shuai jiang ||| congzhentao huang ||| richard yi da xu ||| 
2021 ||| long-term series forecasting with query selector - efficient model of sparse attention. ||| jacek kl ||| mek ||| jakub klimek ||| witold kraskiewicz ||| mateusz topolewski ||| 
2021 |||  beyond: transformer language models in information systems research. ||| ross gruetzemacher ||| david b. paradice ||| 
2020 ||| beyond 512 tokens: siamese multi-depth transformer-based hierarchical encoder for document matching. ||| liu yang ||| mingyang zhang ||| cheng li ||| michael bendersky ||| marc najork ||| 
2022 ||| hdam: heuristic difference attention module for convolutional neural networks. ||| yu xue ||| ziming yuan ||| 
2021 ||| ratchet: medical transformer for chest x-ray diagnosis and reporting. ||| benjamin hou ||| georgios kaissis ||| ronald m. summers ||| bernhard kainz ||| 
2017 ||| attentional network for visual object detection. ||| kota hara ||| ming-yu liu ||| oncel tuzel ||| amir-massoud farahmand ||| 
2021 ||| attention approximates sparse distributed memory. ||| trenton bricken ||| cengiz pehlevan ||| 
2021 ||| nisqa: a deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. ||| gabriel mittag ||| babak naderi ||| assmaa chehadi ||| sebastian m ||| ller ||| 
2018 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2021 ||| adaptively multi-view and temporal fusing transformer for 3d human pose estimation. ||| hui shuai ||| lele wu ||| qingshan liu ||| 
2020 ||| stepwise extractive summarization and planning with structured transformers. ||| shashi narayan ||| joshua maynez ||| jakub ad ||| mek ||| daniele pighin ||| blaz bratanic ||| ryan t. mcdonald ||| 
2020 ||| sa-unet: spatial attention u-net for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yugen yi ||| wenle wang ||| buer chen ||| changqi fan ||| 
2021 ||| point-voxel transformer: an efficient approach to 3d deep learning. ||| cheng zhang ||| haocheng wan ||| shengqiang liu ||| xinyi shen ||| zizhao wu ||| 
2022 ||| leveraging smooth attention prior for multi-agent trajectory prediction. ||| zhangjie cao ||| erdem biyik ||| guy rosman ||| dorsa sadigh ||| 
2021 ||| temporal-relational crosstransformers for few-shot action recognition. ||| toby perrett ||| alessandro masullo ||| tilo burghardt ||| majid mirmehdi ||| dima damen ||| 
2020 ||| sct: set constrained temporal transformer for set supervised action segmentation. ||| mohsen fayyaz ||| juergen gall ||| 
2021 ||| melt: message-level transformer with masked document representations as pre-training for stance detection. ||| matthew matero ||| nikita soni ||| niranjan balasubramanian ||| h. andrew schwartz ||| 
2020 ||| attention-driven body pose encoding for human activity recognition. ||| bappaditya debnath ||| mary o'brien ||| swagat kumar ||| ardhendu behera ||| 
2020 ||| analyzing word translation of transformer layers. ||| hongfei xu ||| josef van genabith ||| deyi xiong ||| qiuhui liu ||| 
2021 ||| towards interpreting zoonotic potential of betacoronavirus sequences with attention. ||| kahini wadhawan ||| payel das ||| barbara a. han ||| ilya r. fischhoff ||| adrian c. castellanos ||| arvind varsani ||| kush r. varshney ||| 
2019 ||| a gated self-attention memory network for answer selection. ||| tuan manh lai ||| quan hung tran ||| trung bui ||| daisuke kihara ||| 
2018 ||| fine-grained age estimation in the wild with attention lstm networks. ||| ke zhang ||| na liu ||| xingfang yuan ||| xinyao guo ||| ce gao ||| zhenbing zhao ||| 
2017 ||| attention strategies for multi-source sequence-to-sequence learning. ||| jindrich libovick ||| jindrich helcl ||| 
2019 ||| hibert: document level pre-training of hierarchical bidirectional transformers for document summarization. ||| xingxing zhang ||| furu wei ||| ming zhou ||| 
2021 ||| intriguing properties of vision transformers. ||| muzammal naseer ||| kanchana ranasinghe ||| salman h. khan ||| munawar hayat ||| fahad shahbaz khan ||| ming-hsuan yang ||| 
2017 ||| parallel attention: a unified framework for visual object discovery through dialogs and queries. ||| bohan zhuang ||| qi wu ||| chunhua shen ||| ian d. reid ||| anton van den hengel ||| 
2021 ||| spatio-temporal multi-task learning transformer for joint moving object detection and segmentation. ||| eslam mohamed ||| ahmad el sallab ||| 
2019 ||| attentional feature-pair relation networks for accurate face recognition. ||| bong-nam kang ||| yonghyun kim ||| bongjin jun ||| daijin kim ||| 
2020 ||| feedback attention for cell image segmentation. ||| hiroki tsuda ||| eisuke shibuya ||| kazuhiro hotta ||| 
2021 ||| phylotransformer: a discriminative model for mutation prediction based on a multi-head self-attention mechanism. ||| yingying wu ||| shusheng xu ||| shing-tung yau ||| yi wu ||| 
2021 ||| deep transformer networks for time series classification: the npp safety case. ||| bing zha ||| alessandro vanni ||| yassin hassan ||| tunc aldemir ||| alper yilmaz ||| 
2021 ||| vara-tts: non-autoregressive text-to-speech synthesis based on very deep vae with residual attention. ||| peng liu ||| yuewen cao ||| songxiang liu ||| na hu ||| guangzhi li ||| chao weng ||| dan su ||| 
2018 ||| pervasive attention: 2d convolutional neural networks for sequence-to-sequence prediction. ||| maha elbayad ||| laurent besacier ||| jakob verbeek ||| 
2021 ||| transformer in convolutional neural networks. ||| yun liu ||| guolei sun ||| yu qiu ||| le zhang ||| ajad chhatkuli ||| luc van gool ||| 
2021 ||| skim-attention: learning to focus via document layout. ||| laura nguyen ||| thomas scialom ||| jacopo staiano ||| benjamin piwowarski ||| 
2021 ||| learning to guide human attention on mobile telepresence robots with 360 degree vision. ||| kishan chandan ||| jack albertson ||| xiaohan zhang ||| xiaoyang zhang ||| yao liu ||| shiqi zhang ||| 
2021 ||| scale-aware network with regional and semantic attentions for crowd counting under cluttered background. ||| qiaosi yi ||| yunxing liu ||| aiwen jiang ||| juncheng li ||| kangfu mei ||| mingwen wang ||| 
2019 ||| graph-to-graph transformer for transition-based dependency parsing. ||| alireza mohammadshahi ||| james henderson ||| 
2021 ||| value-aware approximate attention. ||| ankit gupta ||| jonathan berant ||| 
2018 ||| attention-based guided structured sparsity of deep neural networks. ||| amirsina torfi ||| rouzbeh a. shirvani ||| 
2021 ||| missformer: (in-)attention-based handling of missing observations for trajectory filtering and prediction. ||| stefan becker ||| ronny hug ||| wolfgang h ||| bner ||| michael arens ||| brendan tran morris ||| 
2020 ||| attention patterns detection using brain computer interfaces. ||| felix g. hamza-lup ||| aditya suri ||| ionut emil iacob ||| ioana r. goldbach ||| lateef rasheed ||| paul nicolae borza ||| 
2021 ||| heterogeneous edge-enhanced graph attention network for multi-agent trajectory prediction. ||| xiaoyu mo ||| yang xing ||| chen lv ||| 
2017 ||| sentiment classification with word attention based on weakly supervised learning with a convolutional neural network. ||| gichang lee ||| jaeyun jeong ||| seungwan seo ||| czangyeob kim ||| pilsung kang ||| 
2017 ||| mindid: person identification from brain waves through attention-based recurrent neural network. ||| xiang zhang ||| lina yao ||| salil s. kanhere ||| yunhao liu ||| tao gu ||| kaixuan chen ||| 
2021 ||| make a long image short: adaptive token length for vision transformers. ||| yichen zhu ||| yuqin zhu ||| jie du ||| yi wang ||| zhicai ou ||| feifei feng ||| jian tang ||| 
2021 ||| n-best asr transformer: enhancing slu performance using multiple asr hypotheses. ||| karthik ganesan ||| pakhi bamdev ||| jaivarsan b ||| amresh venugopal ||| abhinav tushar ||| 
2020 ||| few-shot sequence learning with transformers. ||| lajanugen logeswaran ||| ann lee ||| myle ott ||| honglak lee ||| marc'aurelio ranzato ||| arthur szlam ||| 
2020 ||| parallax attention for unsupervised stereo correspondence learning. ||| longguang wang ||| yulan guo ||| yingqian wang ||| zhengfa liang ||| zaiping lin ||| jungang yang ||| wei an ||| 
2020 ||| attention prior for real image restoration. ||| saeed anwar ||| nick barnes ||| lars petersson ||| 
2021 ||| attention-like feature explanation for tabular data. ||| andrei v. konstantinov ||| lev v. utkin ||| 
2021 ||| a free lunch from vit: adaptive attention multi-scale fusion transformer for fine-grained visual recognition. ||| yuan zhang ||| jian cao ||| ling zhang ||| xiangcheng liu ||| zhiyi wang ||| feng ling ||| weiqian chen ||| 
2020 ||| attentive fusion enhanced audio-visual encoding for transformer based robust speech recognition. ||| liangfa wei ||| jie zhang ||| junfeng hou ||| lirong dai ||| 
2021 ||| pam: pose attention module for pose-invariant face recognition. ||| en-jung tsai ||| wei-chang yeh ||| 
2021 ||| semi-supervised vision transformers. ||| zejia weng ||| xitong yang ||| ang li ||| zuxuan wu ||| yu-gang jiang ||| 
2021 ||| lightner: a lightweight generative framework with prompt-guided attention for low-resource ner. ||| xiang chen ||| ningyu zhang ||| lei li ||| xin xie ||| shumin deng ||| chuanqi tan ||| fei huang ||| luo si ||| huajun chen ||| 
2020 ||| hopgat: hop-aware supervision graph attention networks for sparsely labeled graphs. ||| chaojie ji ||| ruxin wang ||| rongxiang zhu ||| yunpeng cai ||| hongyan wu ||| 
2019 ||| a self-attention based deep learning method for lesion attribute detection from ct reports. ||| yifan peng ||| ke yan ||| veit sandfort ||| ronald m. summers ||| zhiyong lu ||| 
2021 ||| multi-modal sarcasm detection based on contrastive attention mechanism. ||| xiaoqiang zhang ||| ying chen ||| guangyuan li ||| 
2021 ||| simvit: exploring a simple vision transformer with sliding windows. ||| gang li ||| di xu ||| xing cheng ||| lingyu si ||| changwen zheng ||| 
2020 ||| attendnets: tiny deep image recognition neural networks for the edge via visual attention condensers. ||| alexander wong ||| mahmoud famouri ||| mohammad javad shafiee ||| 
2019 ||| generating synthetic audio data for attention-based speech recognition systems. ||| nick rossenbach ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2018 ||| attentional multilabel learning over graphs: a message passing approach. ||| kien do ||| truyen tran ||| thin nguyen ||| svetha venkatesh ||| 
2021 ||| ishne: influence self-attention for heterogeneous network embedding. ||| yang yan ||| qiuyan wang ||| 
2019 ||| focusnet: an attention-based fully convolutional network for medical image segmentation. ||| chaitanya kaul ||| suresh manandhar ||| nick e. pears ||| 
2021 ||| attention is indeed all you need: semantically attention-guided decoding for data-to-text nlg. ||| juraj juraska ||| marilyn a. walker ||| 
2021 ||| on the analysis and design of high-frequency transformers for dual and triple active bridge converters in more electric aircraft. ||| babak rahrovi ||| ramin tafazzoli mehrjardi ||| mehrdad ehsani ||| 
2021 ||| vision-language transformer and query generation for referring segmentation. ||| henghui ding ||| chang liu ||| suchen wang ||| xudong jiang ||| 
2021 ||| bendr: using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data. ||| demetres kostas ||| stephane aroca-ouellette ||| frank rudzicz ||| 
2021 ||| opera: attention-regularized transformers for surgical phase recognition. ||| tobias czempiel ||| magdalini paschali ||| daniel ostler ||| seong-tae kim ||| benjamin busam ||| nassir navab ||| 
2017 ||| recurrent attentional reinforcement learning for multi-label image recognition. ||| tianshui chen ||| zhouxia wang ||| guanbin li ||| liang lin ||| 
2019 ||| time-guided high-order attention model of longitudinal heterogeneous healthcare data. ||| yi huang ||| xiaoshan yang ||| changsheng xu ||| 
2021 ||| grounded situation recognition with transformers. ||| junhyeong cho ||| youngseok yoon ||| hyeonjun lee ||| suha kwak ||| 
2020 ||| modrl/d-am: multiobjective deep reinforcement learning algorithm using decomposition and attention model for multiobjective optimization. ||| hong wu ||| jiahai wang ||| zizhen zhang ||| 
2020 ||| memformer: the memory-augmented transformer. ||| qingyang wu ||| zhenzhong lan ||| jing gu ||| zhou yu ||| 
2022 ||| self-promoted supervision for few-shot transformer. ||| bowen dong ||| pan zhou ||| shuicheng yan ||| wangmeng zuo ||| 
2020 ||| pretrained transformers for text ranking: bert and beyond. ||| jimmy lin ||| rodrigo nogueira ||| andrew yates ||| 
2021 ||| theme transformer: symbolic music generation with theme-conditioned transformer. ||| yi-jen shih ||| shih-lun wu ||| frank zalkow ||| meinard m ||| ller ||| yi-hsuan yang ||| 
2021 ||| portfolio optimization with 2d relative-attentional gated transformer. ||| tae wan kim ||| matloob khushi ||| 
2017 ||| satirical news detection and analysis using attention mechanism and linguistic features. ||| fan yang ||| arjun mukherjee ||| eduard constantin dragut ||| 
2019 ||| dropattention: a regularization method for fully-connected self-attention networks. ||| zehui lin ||| pengfei liu ||| luyao huang ||| junkun chen ||| xipeng qiu ||| xuanjing huang ||| 
2021 ||| mismatch: learning to change predictive confidences with attention for consistency-based, semi-supervised medical image segmentation. ||| mou-cheng xu ||| yukun zhou ||| chen jin ||| stefano b. blumberg ||| frederick j. wilson ||| marius de groot ||| neil p. oxtoby ||| daniel c. alexander ||| joseph jacob ||| 
2020 ||| actor-transformers for group activity recognition. ||| kirill gavrilyuk ||| ryan sanford ||| mehrsan javan ||| cees g. m. snoek ||| 
2021 ||| match-ignition: plugging pagerank into transformer for long-form text matching. ||| liang pang ||| yanyan lan ||| xueqi cheng ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| peng gao ||| minghang zheng ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2021 ||| cat: cross attention in vision transformer. ||| hezheng lin ||| xing cheng ||| xiangyu wu ||| fan yang ||| dong shen ||| zhongyuan wang ||| qing song ||| wei yuan ||| 
2021 ||| block pruning for faster transformers. ||| fran ||| ois lagunas ||| ella charlaix ||| victor sanh ||| alexander m. rush ||| 
2018 ||| hybrid self-attention network for machine translation. ||| kaitao song ||| tan xu ||| furong peng ||| jianfeng lu ||| 
2021 ||| operation-wise attention network for tampering localization fusion. ||| polychronis charitidis ||| giorgos kordopatis-zilos ||| symeon papadopoulos ||| ioannis kompatsiaris ||| 
2021 ||| vldeformer: learning visual-semantic embeddings by vision-language transformer decomposing. ||| lisai zhang ||| hongfa wu ||| qingcai chen ||| yimeng deng ||| zhonghua li ||| dejiang kong ||| zhao cao ||| joanna siebert ||| yunpeng han ||| 
2021 ||| pushing on text readability assessment: a transformer meets handcrafted linguistic features. ||| bruce w. lee ||| yoo sung jang ||| jason hyung-jong lee ||| 
2022 ||| unsupervised learning of temporal abstractions with slot-based transformers. ||| anand gopalakrishnan ||| kazuki irie ||| j ||| rgen schmidhuber ||| sjoerd van steenkiste ||| 
2019 ||| real-time emotion recognition via attention gated hierarchical memory network. ||| wenxiang jiao ||| michael r. lyu ||| irwin king ||| 
2019 ||| ga-gan: ct reconstruction from biplanar drrs using gan with guided attention. ||| ashish sinha ||| yohei sugawara ||| yuichiro hirano ||| 
2022 ||| crat-pred: vehicle trajectory prediction with crystal graph convolutional neural networks and multi-head self-attention. ||| julian schmidt ||| julian jordan ||| franz gritschneder ||| klaus dietmayer ||| 
2021 ||| sagan: adversarial spatial-asymmetric attention for noisy nona-bayer reconstruction. ||| s. m. a. sharif ||| rizwan ali naqvi ||| mithun biswas ||| 
2020 ||| ca-net: comprehensive attention convolutional neural networks for explainable medical image segmentation. ||| ran gu ||| guotai wang ||| tao song ||| rui huang ||| michael aertsen ||| jan deprest ||| s ||| bastien ourselin ||| tom vercauteren ||| shaoting zhang ||| 
2021 ||| an attention and prediction guided visual system for small target motion detection in complex natural environments. ||| hongxin wang ||| jiannan zhao ||| huatian wang ||| jigen peng ||| shigang yue ||| 
2018 ||| global-and-local attention networks for visual recognition. ||| drew linsley ||| dan scheibler ||| sven eberhardt ||| thomas serre ||| 
2020 ||| end-to-end human pose and mesh reconstruction with transformers. ||| kevin lin ||| lijuan wang ||| zicheng liu ||| 
2021 ||| a comparison for patch-level classification of deep learning methods on transparent images: from convolutional neural networks to visual transformers. ||| hechen yang ||| chen li ||| peng zhao ||| ao chen ||| xin zhao ||| marcin grzegorzek ||| 
2021 ||| hamilton-jacobi-bellman-isaacs equation for rational inattention in the long-run management of river environments under uncertainty. ||| hidekazu yoshioka ||| motoh tsujimura ||| 
2019 ||| conflict as an inverse of attention in sequence relationship. ||| rajarshee mitra ||| 
2020 ||| generating descriptions for sequential images with local-object attention and global semantic context modelling. ||| jing su ||| chenghua lin ||| mian zhou ||| qingyun dai ||| haoyu lv ||| 
2019 ||| generating long sequences with sparse transformers. ||| rewon child ||| scott gray ||| alec radford ||| ilya sutskever ||| 
2020 ||| on the usefulness of self-attention for automatic speech recognition with transformers. ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2020 ||| gcn for hin via implicit utilization of attention and meta-paths. ||| di jin ||| zhizhi yu ||| dongxiao he ||| carl yang ||| philip s. yu ||| jiawei han ||| 
2021 ||| morphmlp: a self-attention free, mlp-like backbone for image and video. ||| david junhao zhang ||| kunchang li ||| yunpeng chen ||| yali wang ||| shashwat chandra ||| yu qiao ||| luoqi liu ||| mike zheng shou ||| 
2018 ||| attention cropping: a novel data augmentation method for real-world plant species identification. ||| qingguo xiao ||| guangyao li ||| qiaochuan chen ||| li xie ||| enze xie ||| 
2022 ||| diffusion tensor estimation with transformer neural networks. ||| davood karimi ||| ali gholipour ||| 
2020 ||| adaptable multi-domain language model for transformer asr. ||| taewoo lee ||| min-joong lee ||| tae gyoon kang ||| seokyeong jung ||| minseok kwon ||| yeona hong ||| jungin lee ||| kyoung-gu woo ||| ho-gyeong kim ||| jiseung jeong ||| jihyun lee ||| hosik lee ||| young sang choi ||| 
2019 ||| beyond similarity: relation embedding with dual attentions for item-based recommendation. ||| liang zhang ||| guannan liu ||| junjie wu ||| 
2018 ||| semantic aware attention based deep object co-segmentation. ||| hong chen ||| yifei huang ||| hideki nakayama ||| 
2021 ||| optimizing latency for online video captioningusing audio-visual transformers. ||| chiori hori ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| stabilizing transformer-based action sequence generation for q-learning. ||| gideon stein ||| andrey filchenkov ||| arip asadulaev ||| 
2022 ||| fast mri reconstruction: how powerful transformers are? ||| jiahao huang ||| yinzhe wu ||| huanjun wu ||| guang yang ||| 
2021 ||| universal approximation under constraints is possible with transformers. ||| anastasis kratsios ||| behnoosh zamanlooy ||| tianlin liu ||| ivan dokmanic ||| 
2020 ||| inter-series attention model for covid-19 forecasting. ||| xiaoyong jin ||| yu-xiang wang ||| xifeng yan ||| 
2021 ||| towards incremental transformers: an empirical analysis of transformer models for incremental nlu. ||| patrick kahardipraja ||| brielen madureira ||| david schlangen ||| 
2021 ||| an explainable transformer-based deep learning model for the prediction of incident heart failure. ||| shishir rao ||| yikuan li ||| rema ramakrishnan ||| abdelaali hassa ||| ne ||| dexter canoy ||| john cleland ||| thomas lukasiewicz ||| gholamreza salimi khorshidi ||| kazem rahimi ||| 
2021 ||| using prior knowledge to guide bert's attention in semantic textual matching tasks. ||| tingyu xia ||| yue wang ||| yuan tian ||| yi chang ||| 
2021 ||| musemorphose: full-song and fine-grained music style transfer with just one transformer vae. ||| shih-lun wu ||| yi-hsuan yang ||| 
2018 ||| efficient super resolution for large-scale images using attentional gan. ||| harsh nilesh pathak ||| xinxin li ||| shervin minaee ||| brooke cowan ||| 
2017 ||| vqs: linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. ||| chuang gan ||| yandong li ||| haoxiang li ||| chen sun ||| boqing gong ||| 
2019 ||| huggingface's transformers: state-of-the-art natural language processing. ||| thomas wolf ||| lysandre debut ||| victor sanh ||| julien chaumond ||| clement delangue ||| anthony moi ||| pierric cistac ||| tim rault ||| r ||| mi louf ||| morgan funtowicz ||| jamie brew ||| 
2021 ||| transformer networks for data augmentation of human physical activity recognition. ||| sandeep ramachandra ||| alexander h ||| lzemann ||| kristof van laerhoven ||| 
2021 ||| visual transformer pruning. ||| mingjian zhu ||| kai han ||| yehui tang ||| yunhe wang ||| 
2019 ||| transformer-based cascaded multimodal speech translation. ||| zixiu wu ||| ozan caglayan ||| julia ive ||| josiah wang ||| lucia specia ||| 
2020 ||| self-attention encoding and pooling for speaker recognition. ||| pooyan safari ||| miquel india ||| javier hernando ||| 
2021 ||| self-attention between datapoints: going beyond individual input-output pairs in deep learning. ||| jannik kossen ||| neil band ||| clare lyle ||| aidan n. gomez ||| tom rainforth ||| yarin gal ||| 
2019 ||| attention is all you need for chinese word segmentation. ||| sufeng duan ||| hai zhao ||| 
2021 ||| scaling local self-attention for parameter efficient visual backbones. ||| ashish vaswani ||| prajit ramachandran ||| aravind srinivas ||| niki parmar ||| blake a. hechtman ||| jonathon shlens ||| 
2017 ||| fullie and wiselie: a dual-stream recurrent convolutional attention model for activity recognition. ||| kaixuan chen ||| lina yao ||| tao gu ||| zhiwen yu ||| xianzhi wang ||| dalin zhang ||| 
2021 ||| temporal action proposal generation with transformers. ||| lining wang ||| haosen yang ||| wenhao wu ||| hongxun yao ||| hujie huang ||| 
2020 ||| relevance transformer: generating concise code snippets with relevance feedback. ||| carlos gemmell ||| federico rossetto ||| jeffrey dalton ||| 
2021 ||| adaptive multi-resolution attention with linear complexity. ||| yao zhang ||| yunpu ma ||| thomas seidl ||| volker tresp ||| 
2020 ||| topicbert: a transformer transfer learning based memory-graph approach for multimodal streaming social media topic detection. ||| meysam asgari-chenaghlu ||| mohammad-reza feizi-derakhshi ||| leili farzinvash ||| mohammad ali balafar ||| cina motamed ||| 
2018 ||| recurrent transformer networks for semantic correspondence. ||| seungryong kim ||| stephen lin ||| sangryul jeon ||| dongbo min ||| kwanghoon sohn ||| 
2021 ||| inconsistent few-shot relation classification via cross-attentional prototype networks with contrastive learning. ||| hongru wang ||| zhijing jin ||| jiarun cao ||| gabriel pui cheong fung ||| kam-fai wong ||| 
2020 ||| aist: an interpretable attention-based deep learning model for crime prediction. ||| yeasir rayhan ||| tanzima hashem ||| 
2020 ||| multifaceted context representation using dual attention for ontology alignment. ||| vivek iyer ||| arvind agarwal ||| harshit kumar ||| 
2021 ||| understanding transformers for bot detection in twitter. ||| andr ||| s garc ||| a-silva ||| cristian berrio ||| jos |||  manu ||| l g ||| mez-p ||| rez ||| 
2022 ||| docentr: an end-to-end document image enhancement transformer. ||| mohamed ali souibgui ||| sanket biswas ||| sana khamekhem jemni ||| yousri kessentini ||| alicia forn ||| s ||| josep llad ||| s ||| umapada pal ||| 
2021 ||| learning disentangled representation implicitly via transformer for occluded person re-identification. ||| mengxi jia ||| xinhua cheng ||| shijian lu ||| jian zhang ||| 
2018 ||| persistence pays off: paying attention to what the lstm gating mechanism persists. ||| giancarlo d. salton ||| john d. kelleher ||| 
2021 ||| finetuning pretrained transformers into rnns. ||| jungo kasai ||| hao peng ||| yizhe zhang ||| dani yogatama ||| gabriel ilharco ||| nikolaos pappas ||| yi mao ||| weizhu chen ||| noah a. smith ||| 
2022 ||| transppg: two-stream transformer for remote heart rate estimate. ||| jiaqi kang ||| su yang ||| weishan zhang ||| 
2021 ||| automated question generation and question answering from turkish texts using text-to-text transformers. ||| fatih cagatay akyon ||| devrim cavusoglu ||| cemil cengiz ||| sinan onur altinuc ||| alptekin temizel ||| 
2020 ||| self-supervised nuclei segmentation in histopathological images using attention. ||| mihir sahasrabudhe ||| stergios christodoulidis ||| roberto salgado ||| stefan michiels ||| sherene loi ||| fabrice andr ||| nikos paragios ||| maria vakalopoulou ||| 
2021 ||| optimizing graph transformer networks with graph-based techniques. ||| loc hoang ||| udit agarwal ||| gurbinder gill ||| roshan dathathri ||| abhik seal ||| brian martin ||| keshav pingali ||| 
2020 ||| self-supervised gait encoding with locality-aware attention for person re-identification. ||| haocong rao ||| siqi wang ||| xiping hu ||| mingkui tan ||| huang da ||| jun cheng ||| bin hu ||| 
2021 ||| a practical survey on faster and lighter transformers. ||| quentin fournier ||| ga ||| tan marceau caron ||| daniel aloise ||| 
2021 ||| capturing row and column semantics in transformer based question answering over tables. ||| michael r. glass ||| mustafa canim ||| alfio gliozzo ||| saneem a. chemmengath ||| vishwajeet kumar ||| rishav chakravarti ||| avi sil ||| feifei pan ||| samarth bharadwaj ||| nicolas rodolfo fauceglia ||| 
2021 ||| anomaly detection in dynamic graphs via transformer. ||| yixin liu ||| shirui pan ||| yu guang wang ||| fei xiong ||| liang wang ||| vincent c. s. lee ||| 
2019 ||| attention is not explanation. ||| sarthak jain ||| byron c. wallace ||| 
2021 ||| uncertainty, edge, and reverse-attention guided generative adversarial network for automatic building detection in remotely sensed images. ||| somrita chattopadhyay ||| avinash c. kak ||| 
2021 ||| cola-net: collaborative attention network for image restoration. ||| chong mou ||| jian zhang ||| xiaopeng fan ||| hangfan liu ||| ronggang wang ||| 
2021 ||| distilling transformers for neural cross-domain search. ||| colin b. clement ||| chen wu ||| dawn drain ||| neel sundaresan ||| 
2020 ||| masked cross self-attention encoding for deep speaker embedding. ||| soonshin seo ||| daniel jun rim ||| junseok oh ||| ji-hwan kim ||| 
2018 ||| finefool: fine object contour attack via attention. ||| jinyin chen ||| haibin zheng ||| hui xiong ||| mengmeng su ||| 
2021 ||| crossformer: a versatile vision transformer based on cross-scale attention. ||| wenxiao wang ||| lu yao ||| long chen ||| deng cai ||| xiaofei he ||| wei liu ||| 
2019 ||| an attentive survey of attention models. ||| sneha chaudhari ||| gungor polatkan ||| rohan ramanath ||| varun mithal ||| 
2021 ||| predicting the factuality of reporting of news media using observations about user attention in their youtube channels. ||| krasimira bozhanova ||| yoan dinkov ||| ivan koychev ||| maria castaldo ||| tommaso venturini ||| preslav nakov ||| 
2020 ||| how does selective mechanism improve self-attention networks? ||| xinwei geng ||| longyue wang ||| xing wang ||| bing qin ||| ting liu ||| zhaopeng tu ||| 
2020 ||| fine-grained iterative attention network for temporallanguage localization in videos. ||| xiaoye qu ||| pengwei tang ||| zhikang zhou ||| yu cheng ||| jianfeng dong ||| pan zhou ||| 
2021 ||| cmt: convolutional neural networks meet vision transformers. ||| jianyuan guo ||| kai han ||| han wu ||| chang xu ||| yehui tang ||| chunjing xu ||| yunhe wang ||| 
2021 ||| are transformers a modern version of eliza? observations on french object verb agreement. ||| bingzhi li ||| guillaume wisniewski ||| beno ||| t crabb ||| 
2021 ||| 3d human texture estimation from a single image with transformers. ||| xiangyu xu ||| chen change loy ||| 
2020 ||| end-to-end spoken language understanding using transformer networks and self-supervised pre-trained features. ||| edmilson da silva morais ||| hong-kwang jeff kuo ||| samuel thomas ||| zolt ||| n t ||| ske ||| brian kingsbury ||| 
2021 ||| the deformer: an order-agnostic distribution estimating transformer. ||| michael a. alcorn ||| anh nguyen ||| 
2021 ||| semantic maps and metrics for science semantic maps and metrics for science using deep transformer encoders. ||| brendan chambers ||| james evans ||| 
2021 ||| jurassic mark: inattentional blindness for a datasaurus reveals that visualizations are explored, not seen. ||| tal boger ||| steven b. most ||| steven l. franconeri ||| 
2020 |||  lt2 at semeval-2020 task 12: fine-tuning of pre-trained transformer networks for offensive language detection. ||| gregor wiedemann ||| seid muhie yimam ||| chris biemann ||| 
2022 ||| transformer-based knowledge distillation for efficient semantic segmentation of road-driving scenes. ||| ruiping liu ||| kailun yang ||| huayao liu ||| jiaming zhang ||| kunyu peng ||| rainer stiefelhagen ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| chiara plizzari ||| marco cannici ||| matteo matteucci ||| 
2020 ||| multichannel cnn with attention for text classification. ||| zhenyu liu ||| haiwei huang ||| chaohong lu ||| shengfei lyu ||| 
2018 ||| hybrid ctc-attention based end-to-end speech recognition using subword units. ||| zhangyu xiao ||| zhijian ou ||| wei chu ||| hui lin ||| 
2020 ||| character-level japanese text generation with attention mechanism for chest radiography diagnosis. ||| kenya sakka ||| kotaro nakayama ||| nisei kimura ||| taiki inoue ||| yusuke iwasawa ||| ryohei yamaguchi ||| yoshimasa kawazoe ||| kazuhiko ohe ||| yutaka matsuo ||| 
2021 ||| a novel disaster image dataset and characteristics analysis using attention model. ||| fahim faisal niloy ||| arif ||| abu bakar siddik nayem ||| anis sarker ||| ovi paul ||| m. ashraful amin ||| amin ahsan ali ||| moinul islam zaber ||| a. k. m. mahbubur rahman ||| 
2018 ||| syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese. ||| shiyu zhou ||| linhao dong ||| shuang xu ||| bo xu ||| 
2021 ||| error detection in large-scale natural language understanding systems using transformer models. ||| rakesh chada ||| pradeep natarajan ||| darshan fofadiya ||| prathap ramachandra ||| 
2022 ||| a novel attention model for salient structure detection in seismic volumes. ||| muhammad amir shafiq ||| zhiling long ||| haibin di ||| ghassan alregib ||| 
2020 ||| universal adversarial attack on attention and the resulting dataset damagenet. ||| sizhe chen ||| zhengbao he ||| chengjin sun ||| xiaolin huang ||| 
2020 ||| pretrained transformers for simple question answering over knowledge graphs. ||| denis lukovnikov ||| asja fischer ||| jens lehmann ||| 
2022 ||| image-to-graph transformers for chemical structure recognition. ||| sanghyun yoo ||| ohyun kwon ||| hoshik lee ||| 
2019 ||| analyzing the structure of attention in a transformer language model. ||| jesse vig ||| yonatan belinkov ||| 
2021 ||| less is more: pay less attention in vision transformers. ||| zizheng pan ||| bohan zhuang ||| haoyu he ||| jing liu ||| jianfei cai ||| 
2019 ||| inattentional blindness for redirected walking using dynamic foveated rendering. ||| yashas joshi ||| charalambos poullis ||| 
2021 ||| on the validity of pre-trained transformers for natural language processing in the software engineering domain. ||| julian von der mosel ||| alexander trautsch ||| steffen herbold ||| 
2021 ||| neural attention for image captioning: review of outstanding methods. ||| zanyar zohourianshahzadi ||| jugal k. kalita ||| 
2021 ||| vision transformer for classification of breast ultrasound images. ||| behnaz gheflati ||| hassan rivaz ||| 
2021 ||| see, hear, read: leveraging multimodality with guided attention for abstractive text summarization. ||| yash kumar atri ||| shraman pramanick ||| vikram goyal ||| tanmoy chakraborty ||| 
2020 ||| spotfast networks with memory augmented lateral transformers for lipreading. ||| peratham wiriyathammabhum ||| 
2020 ||| e.t.: entity-transformers. coreference augmented neural language model for richer mention representations via entity-transformer blocks. ||| nikolaos stylianou ||| ioannis p. vlahavas ||| 
2019 ||| making asynchronous stochastic gradient descent work for transformers. ||| alham fikri aji ||| kenneth heafield ||| 
2021 ||| flying guide dog: walkable path discovery for the visually impaired utilizing drones and transformer-based semantic segmentation. ||| haobin tan ||| chang chen ||| xinyu luo ||| jiaming zhang ||| constantin seibold ||| kailun yang ||| rainer stiefelhagen ||| 
2019 ||| 2d attentional irregular scene text recognizer. ||| pengyuan lyu ||| zhicheng yang ||| xinhang leng ||| xiaojun wu ||| ruiyu li ||| xiaoyong shen ||| 
2021 ||| fbert: a neural transformer for identifying offensive content. ||| diptanu sarkar ||| marcos zampieri ||| tharindu ranasinghe ||| alexander g. ororbia ||| 
2019 ||| biomedical image segmentation by retina-like sequential attention mechanism using only a few training images. ||| shohei hayashi ||| bisser raytchev ||| toru tamaki ||| kazufumi kaneda ||| 
2019 ||| understanding the teaching styles by an attention based multi-task cross-media dimensional modelling. ||| suping zhou ||| jia jia ||| yufeng yin ||| xiang li ||| yang yao ||| ying zhang ||| zeyang ye ||| kehua lei ||| yan huang ||| jialie shen ||| 
2021 ||| memory and attention in deep learning. ||| hung le ||| 
2021 ||| construction material classification on imbalanced datasets for construction monitoring automation using vision transformer (vit) architecture. ||| maryam soleymani ||| mahdi bonyani ||| hadi mahami ||| farnad nasirzadeh ||| 
2021 ||| unsupervised-learning-based method for chest mri-ct transformation using structure constrained unsupervised generative attention networks. ||| hidetoshi matsuo ||| mizuho nishio ||| munenobu nogami ||| feibi zeng ||| takako kurimoto ||| sandeep kaushik ||| florian wiesinger ||| atsushi k. kono ||| takamichi murakami ||| 
2020 ||| barnet: bilinear attention network with adaptive receptive field for surgical instrument segmentation. ||| zhen-liang ni ||| gui-bin bian ||| guan'an wang ||| xiao-hu zhou ||| zeng-guang hou ||| xiao-liang xie ||| zhen li ||| yu-han wang ||| 
2018 ||| weakly supervised local attention network for fine-grained visual classification. ||| tao hu ||| honggang qi ||| cong huang ||| qingming huang ||| yan lu ||| jizheng xu ||| 
2021 ||| concad: contrastive learning-based cross attention for sleep apnea detection. ||| guanjie huang ||| fenglong ma ||| 
2021 ||| visual saliency transformer. ||| nian liu ||| ni zhang ||| kaiyuan wan ||| junwei han ||| ling shao ||| 
2020 ||| attention or memory? neurointerpretable agents in space and time. ||| lennart bramlage ||| aurelio cortese ||| 
2020 ||| attention-based neural networks for sentiment attitude extraction using distant supervision. ||| nicolay rusnachenko ||| natalia v. loukachevitch ||| 
2019 ||| deep neural network for fast and accurate single image super-resolution via channel-attention-based fusion of orientation-aware features. ||| du chen ||| zewei he ||| yanpeng cao ||| jiangxin yang ||| yanlong cao ||| michael ying yang ||| siliang tang ||| yueting zhuang ||| 
2021 ||| scout: socially-consistent and understandable graph attention network for trajectory prediction of vehicles and vrus. ||| sandra carrasco ||| david fern ||| ndez llorca ||| miguel  ||| ngel sotelo ||| 
2019 ||| regularized adversarial sampling and deep time-aware attention for click-through rate prediction. ||| yikai wang ||| liang zhang ||| quanyu dai ||| fuchun sun ||| bo zhang ||| yang he ||| weipeng yan ||| yongjun bao ||| 
2019 ||| single image super-resolution via dense blended attention generative adversarial network for clinical diagnosis. ||| 
2018 ||| parameter sharing methods for multilingual self-attentional translation models. ||| devendra singh sachan ||| graham neubig ||| 
2020 ||| ap-mtl: attention pruned multi-task learning model for real-time instrument detection and segmentation in robot-assisted surgery. ||| mobarakol islam ||| vibashan vs ||| hongliang ren ||| 
2021 ||| transgan: two transformers can make one strong gan. ||| yifan jiang ||| shiyu chang ||| zhangyang wang ||| 
2019 ||| temporal collaborative ranking via personalized transformer. ||| liwei wu ||| shuqing li ||| cho-jui hsieh ||| james sharpnack ||| 
2020 ||| pay attention to the cough: early diagnosis of covid-19 using interpretable symptoms embeddings with cough sound signal processing. ||| ankit pal ||| malaikannan sankarasubbu ||| 
2018 ||| ican: instance-centric attention network for human-object interaction detection. ||| chen gao ||| yuliang zou ||| jia-bin huang ||| 
2020 ||| staying true to your word: (how) can attention become explanation? ||| martin tutek ||| jan snajder ||| 
2020 ||| pona: pose-guided non-local attention for human pose transfer. ||| kun li ||| jinsong zhang ||| yebin liu ||| yu-kun lai ||| qionghai dai ||| 
2021 ||| instance-aware remote sensing image captioning with cross-hierarchy attention. ||| chengze wang ||| zhiyu jiang ||| yuan yuan ||| 
2020 |||  classification of transients in two-core symmetric phase angle regulating transformers. ||| pallav kumar bera ||| can isik ||| 
2021 ||| exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media. ||| shivam raval ||| hooman sedghamiz ||| enrico santus ||| tuka alhanai ||| mohammad m. ghassemi ||| emmanuele chersoni ||| 
2021 ||| rest: an efficient transformer for visual recognition. ||| qing-long zhang ||| yubin yang ||| 
2018 ||| improving speech emotion recognition via transformer-based predictive coding through transfer learning. ||| 
2021 ||| handwritten mathematical expression recognition with bidirectionally trained transformer. ||| wenqi zhao ||| liangcai gao ||| zuoyu yan ||| shuai peng ||| lin du ||| ziyin zhang ||| 
2020 ||| multi-head self-attention with role-guided masks. ||| dongsheng wang ||| casper hansen ||| lucas chaves lima ||| christian hansen ||| maria maistro ||| jakob grue simonsen ||| christina lioma ||| 
2021 ||| accelerating framework of transformer by hardware design and model compression co-optimization. ||| panjie qi ||| edwin hsing-mean sha ||| qingfeng zhuge ||| hongwu peng ||| shaoyi huang ||| zhenglun kong ||| yuhong song ||| bingbing li ||| 
2021 ||| zero-shot controlled generation with encoder-decoder transformers. ||| devamanyu hazarika ||| mahdi namazifar ||| dilek hakkani-t ||| r ||| 
2022 ||| segmenting medical instruments in minimally invasive surgeries using attentionmask. ||| christian wilms ||| alexander michael gerlach ||| r ||| diger schmitz ||| simone frintrop ||| 
2021 ||| target speaker verification with selective auditory attention for single and multi-talker speech. ||| chenglin xu ||| wei rao ||| jibin wu ||| haizhou li ||| 
2020 ||| single image deraining via scale-space invariant attention neural network. ||| bo pang ||| deming zhai ||| junjun jiang ||| xianming liu ||| 
2020 ||| visual attention for musical instrument recognition. ||| karn watcharasupat ||| siddharth gururani ||| alexander lerch ||| 
2021 ||| process outcome prediction: cnn vs. lstm (with attention). ||| hans weytjens ||| jochen de weerdt ||| 
2021 ||| attentive fine-tuning of transformers for translation of low-resourced languages @loresmt 2021. ||| karthik puranik ||| adeep hande ||| ruba priyadharshini ||| thenmozhi durairaj ||| anbukkarasi sampath ||| kingston pal thamburaj ||| bharathi raja chakravarthi ||| 
2019 ||| sequence-to-set semantic tagging: end-to-end multi-label prediction using neural attention for complex query reformulation and automated text categorization. ||| manirupa das ||| juanxi li ||| eric fosler-lussier ||| simon m. lin ||| soheil moosavinasab ||| steve rust ||| yungui huang ||| rajiv ramnath ||| 
2019 ||| heterogeneous graph attention network. ||| xiao wang ||| houye ji ||| chuan shi ||| bai wang ||| peng cui ||| philip s. yu ||| yanfang ye ||| 
2021 ||| transformer acceleration with dynamic sparse attention. ||| liu liu ||| zheng qu ||| zhaodong chen ||| yufei ding ||| yuan xie ||| 
2020 ||| detecting lane and road markings at a distance with perspective transformer layers. ||| zhuoping yu ||| xiaozhou ren ||| yuyao huang ||| wei tian ||| junqiao zhao ||| 
2020 ||| heterogeneous graph transformer. ||| ziniu hu ||| yuxiao dong ||| kuansan wang ||| yizhou sun ||| 
2020 ||| egocentric action recognition by video attention and temporal context. ||| juan-manuel perez-rua ||| antoine toisoul ||| brais mart ||| nez ||| victor escorcia ||| li zhang ||| xiatian zhu ||| tao xiang ||| 
2022 ||| the gatedtabtransformer. an enhanced deep learning architecture for tabular modeling. ||| radostin cholakov ||| todor kolev ||| 
2018 ||| learning roi transformer for detecting oriented objects in aerial images. ||| jian ding ||| nan xue ||| yang long ||| gui-song xia ||| qikai lu ||| 
2020 ||| gta: global temporal attention for video action understanding. ||| bo he ||| xitong yang ||| zuxuan wu ||| hao chen ||| ser-nam lim ||| abhinav shrivastava ||| 
2020 ||| pa-gan: progressive attention generative adversarial network for facial attribute editing. ||| zhenliang he ||| meina kan ||| jichao zhang ||| shiguang shan ||| 
2021 ||| tweac: transformer with extendable qa agent classifiers. ||| gregor geigle ||| nils reimers ||| andreas r ||| ckl ||| iryna gurevych ||| 
2021 ||| ssagcn: social soft attention graph convolution network for pedestrian trajectory prediction. ||| pei lv ||| wentong wang ||| yunxin wang ||| yuzhen zhang ||| mingliang xu ||| changsheng xu ||| 
2020 ||| multilevel text alignment with cross-document attention. ||| xuhui zhou ||| nikolaos pappas ||| noah a. smith ||| 
2022 ||| particle transformer for jet tagging. ||| huilin qu ||| congqiao li ||| sitian qian ||| 
2019 ||| structure-attentioned memory network for monocular depth estimation. ||| jing zhu ||| yunxiao shi ||| mengwei ren ||| yi fang ||| kuo-chin lien ||| junli gu ||| 
2020 ||| accelerating training of transformer-based language models with progressive layer dropping. ||| minjia zhang ||| yuxiong he ||| 
2018 ||| amobee at semeval-2018 task 1: gru neural network with a cnn attention mechanism for sentiment classification. ||| alon rozental ||| daniel fleischer ||| 
2021 ||| modeling concentrated cross-attention for neural machine translation with gaussian mixture model. ||| shaolei zhang ||| yang feng ||| 
2021 ||| approximating how single head attention learns. ||| charlie snell ||| ruiqi zhong ||| dan klein ||| jacob steinhardt ||| 
2021 ||| learning dynamic and hierarchical traffic spatiotemporal features with transformer. ||| haoyang yan ||| xiaolei ma ||| 
2019 ||| efficient multi-robot exploration via multi-head attention-based cooperation strategy. ||| shuqi liu ||| zhaoxia wu ||| 
2019 ||| periphery-fovea multi-resolution driving model guided by human attention. ||| ye xia ||| jinkyu kim ||| john f. canny ||| karl zipser ||| david whitney ||| 
2021 ||| towards coherent visual storytelling with ordered image attention. ||| tom braude ||| idan schwartz ||| alexander g. schwing ||| ariel shamir ||| 
2021 ||| artseg: employing attention for thermal images semantic segmentation. ||| farzeen munir ||| shoaib azam ||| unse fatima ||| moongu jeon ||| 
2019 ||| improving video compression with deep visual-attention models. ||| vitaliy lyudvichenko ||| mikhail erofeev ||| alexander ploshkin ||| dmitriy s. vatolin ||| 
2022 ||| half wavelet attention on m-net+ for low-light image enhancement. ||| chi-mao fan ||| tsung-jung liu ||| kuan-hsien liu ||| 
2021 ||| combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of covid-19 fake tweets. ||| rachit bansal ||| william scott paka ||| nidhi ||| shubhashis sengupta ||| tanmoy chakraborty ||| 
2017 ||| recurrent soft attention model for common object recognition. ||| liliang ren ||| 
2020 ||| delaying interaction layers in transformer-based encoders for efficient open domain question answering. ||| wissam siblini ||| mohamed challal ||| charlotte pasqual ||| 
2021 ||| time-frequency attention for monaural speech enhancement. ||| qiquan zhang ||| qi song ||| zhaoheng ni ||| aaron nicolson ||| haizhou li ||| 
2017 ||| hierarchical recurrent attention network for response generation. ||| chen xing ||| wei wu ||| yu wu ||| ming zhou ||| yalou huang ||| wei-ying ma ||| 
2019 ||| interactional and informational attention on twitter. ||| agathe baltzer ||| m ||| rton karsai ||| camille roth ||| 
2022 ||| multi-scale attention guided pose transfer. ||| prasun roy ||| saumik bhattacharya ||| subhankar ghosh ||| umapada pal ||| 
2021 ||| aggregating nested transformers. ||| zizhao zhang ||| han zhang ||| long zhao ||| ting chen ||| tomas pfister ||| 
2020 ||| real-time semantic segmentation with fast attention. ||| ping hu ||| federico perazzi ||| fabian caba heilbron ||| oliver wang ||| zhe lin ||| kate saenko ||| stan sclaroff ||| 
2020 ||| deep reinforcement learning with stacked hierarchical attention for text-based games. ||| yunqiu xu ||| meng fang ||| ling chen ||| yali du ||| joey tianyi zhou ||| chengqi zhang ||| 
2018 ||| attention mechanisms for object recognition with event-based cameras. ||| marco cannici ||| marco ciccone ||| andrea romanoni ||| matteo matteucci ||| 
2021 ||| cocatt: a cognitive-conditioned driver attention dataset. ||| yuan shen ||| niviru wijayaratne ||| pranav sriram ||| aamir hasan ||| peter du ||| katie driggs campbell ||| 
2018 ||| twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm. ||| yuxiao chen ||| jianbo yuan ||| quanzeng you ||| jiebo luo ||| 
2019 ||| equivariant transformer networks. ||| kai sheng tai ||| peter bailis ||| gregory valiant ||| 
2021 ||| boosting graph search with attention network for solving the general orienteering problem. ||| zongtao liu ||| jing xu ||| jintao su ||| tao xiao ||| yang yang ||| 
2020 ||| bengali abstractive news summarization(bans): a neural attention approach. ||| prithwiraj bhattacharjee ||| avi mallick ||| md saiful islam ||| marium-e. jannat ||| 
2021 ||| federated split vision transformer for covid-19 cxr diagnosis using task-agnostic training. ||| sangjoon park ||| gwanghyun kim ||| jeongsol kim ||| boah kim ||| jong chul ye ||| 
2022 ||| umt: unified multi-modal transformers for joint video moment retrieval and highlight detection. ||| ye liu ||| siyuan li ||| yang wu ||| chang wen chen ||| ying shan ||| xiaohu qie ||| 
2022 ||| social computational design method for generating product shapes with gan and transformer models. ||| maolin yang ||| pingyu jiang ||| 
2021 ||| pvg at wassa 2021: a multi-input, multi-task, transformer-based architecture for empathy and distress prediction. ||| atharva kulkarni ||| sunanda somwase ||| shivam rajput ||| manisha marathe ||| 
2019 ||| an augmented transformer architecture for natural language generation tasks. ||| hailiang li ||| adele y. c. wang ||| yang liu ||| du tang ||| zhibin lei ||| wenye li ||| 
2021 ||| contextual transformer networks for visual recognition. ||| yehao li ||| ting yao ||| yingwei pan ||| tao mei ||| 
2020 ||| confidence estimation for attention-based sequence-to-sequence models for speech recognition. ||| qiujia li ||| david qiu ||| yu zhang ||| bo li ||| yanzhang he ||| philip c. woodland ||| liangliang cao ||| trevor strohman ||| 
2021 ||| dual-attention enhanced bdense-unet for liver lesion segmentation. ||| wenming cao ||| philip l. h. yu ||| gilbert c. s. lui ||| keith w. h. chiu ||| ho-ming cheng ||| yanwen fang ||| man-fung yuen ||| wai-kay seto ||| 
2022 ||| matchformer: interleaving attention in transformers for feature matching. ||| qing wang ||| jiaming zhang ||| kailun yang ||| kunyu peng ||| rainer stiefelhagen ||| 
2022 ||| audio-visual speech separation based on joint feature representation with cross-modal attention. ||| junwen xiong ||| peng zhang ||| lei xie ||| wei huang ||| yufei zha ||| yanning zhang ||| 
2021 ||| image fusion transformer. ||| vibashan vs ||| jeya maria jose valanarasu ||| poojan oza ||| vishal m. patel ||| 
2019 ||| segmentation guided attention network for crowd counting via curriculum learning. ||| qian wang ||| toby p. breckon ||| 
2021 ||| searching for trionet: combining convolution with local and global self-attention. ||| huaijin pi ||| huiyu wang ||| yingwei li ||| zizhang li ||| alan l. yuille ||| 
2021 ||| foveater: foveated transformer for image classification. ||| aditya jonnalagadda ||| william wang ||| miguel p. eckstein ||| 
2021 ||| attention-augmented spatio-temporal segmentation for land cover mapping. ||| rahul ghosh ||| praveen ravirathinam ||| xiaowei jia ||| chenxi lin ||| zhenong jin ||| vipin kumar ||| 
2019 ||| personalizing search results using hierarchical rnn with query-aware attention. ||| songwei ge ||| zhicheng dou ||| zhengbao jiang ||| jian-yun nie ||| ji-rong wen ||| 
2019 ||| a data-driven dynamic rating forecast method and application for power transformer long-term planning. ||| ming dong ||| 
2021 ||| agpcnet: attention-guided pyramid context networks for infrared small target detection. ||| tianfang zhang ||| siying cao ||| tian pu ||| zhenming peng ||| 
2022 ||| associating objects with scalable transformers for video object segmentation. ||| zongxin yang ||| jiaxu miao ||| xiaohan wang ||| yunchao wei ||| yi yang ||| 
2019 ||| attention-based pairwise multi-perspective convolutional neural network for answer selection in question answering. ||| jamshid mozafari ||| mohammad ali nematbakhsh ||| afsaneh fatemi ||| 
2022 ||| rngdet: road network graph detection by transformer in aerial images. ||| zhenhua xu ||| yuxuan liu ||| lu gan ||| yuxiang sun ||| ming liu ||| lujia wang ||| 
2019 ||| multi-agent game abstraction via graph attention neural network. ||| yong liu ||| weixun wang ||| yujing hu ||| jianye hao ||| xingguo chen ||| yang gao ||| 
2021 ||| ba^2m: a batch aware attention module for image classification. ||| qishang cheng ||| hongliang li ||| qingbo wu ||| king ngi ngan ||| 
2022 ||| security evaluation of block-based image encryption for vision transformer against jigsaw puzzle solver attack. ||| tatsuya chuman ||| hitoshi kiya ||| 
2020 ||| capsule-transformer for neural machine translation. ||| sufeng duan ||| juncheng cao ||| hai zhao ||| 
2021 ||| transformesh: a transformer network for longitudinal modeling of anatomical meshes. ||| ignacio sarasua ||| sebastian p ||| lsterl ||| christian wachinger ||| 
2021 ||| geogat: graph model based on attention mechanism for geographic text classification. ||| weipeng jing ||| xianyang song ||| donglin di ||| houbing song ||| 
2019 ||| et-net: a generic edge-attention guidance network for medical image segmentation. ||| zhijie zhang ||| huazhu fu ||| hang dai ||| jianbing shen ||| yanwei pang ||| ling shao ||| 
2019 ||| language modeling with deep transformers. ||| kazuki irie ||| albert zeyer ||| ralf schl ||| ter ||| hermann ney ||| 
2017 ||| attentional pooling for action recognition. ||| rohit girdhar ||| deva ramanan ||| 
2019 ||| bert4rec: sequential recommendation with bidirectional encoder representations from transformer. ||| fei sun ||| jun liu ||| jian wu ||| changhua pei ||| xiao lin ||| wenwu ou ||| peng jiang ||| 
2021 ||| daema: denoising autoencoder with mask attention. ||| simon tihon ||| muhammad usama javaid ||| damien fourure ||| nicolas posocco ||| thomas peel ||| 
2021 ||| lightweight attentional feature fusion for video retrieval by text. ||| fan hu ||| aozhu chen ||| ziyue wang ||| fangming zhou ||| xirong li ||| 
2020 ||| tmt: a transformer-based modal translator for improving multimodal sequence representations in audio visual scene-aware dialog. ||| wubo li ||| dongwei jiang ||| wei zou ||| xiangang li ||| 
2019 ||| source dependency-aware transformer with supervised self-attention. ||| chengyi wang ||| shuangzhi wu ||| shujie liu ||| 
2020 ||| salient object detection combining a self-attention module and a feature pyramid network. ||| guangyu ren ||| tianhong dai ||| panagiotis barmpoutis ||| tania stathaki ||| 
2018 ||| identifying protein-protein interaction using tree lstm and structured attention. ||| mahtab ahmed ||| jumayel islam ||| muhammad rifayat samee ||| robert e. mercer ||| 
2022 ||| synthesizing tensor transformations for visual self-attention. ||| xian wei ||| xihao wang ||| hai lan ||| jiaming lei ||| yanhui huang ||| hui yu ||| jian yang ||| 
2022 ||| facial expression recognition based on multi-head cross attention network. ||| jae-yeop jeong ||| yeong-gi hong ||| daun kim ||| yuchul jung ||| jin-woo jeong ||| 
2020 ||| spatial and spectral deep attention fusion for multi-channel speech separation using deep embedding features. ||| cunhang fan ||| bin liu ||| jianhua tao ||| jiangyan yi ||| zhengqi wen ||| 
2021 ||| emerging properties in self-supervised vision transformers. ||| mathilde caron ||| hugo touvron ||| ishan misra ||| herv |||  j ||| gou ||| julien mairal ||| piotr bojanowski ||| armand joulin ||| 
2021 ||| signal transformer: complex-valued attention and meta-learning for signal recognition. ||| yihong dong ||| ying peng ||| muqiao yang ||| songtao lu ||| qingjiang shi ||| 
2021 ||| multi-encoder learning and stream fusion for transformer-based end-to-end automatic speech recognition. ||| timo lohrenz ||| zhengyang li ||| tim fingscheidt ||| 
2021 ||| unleashing transformers: parallel token prediction with discrete absorbing diffusion for fast high-resolution image generation from vector-quantized codes. ||| sam bond-taylor ||| peter hessey ||| hiroshi sasaki ||| toby p. breckon ||| chris g. willcocks ||| 
2021 ||| fine-tuning vision transformers for the prediction of state variables in ising models. ||| onur kara ||| arijit sehanobish ||| hector h. corzo ||| 
2019 ||| attention-based prototypical learning towards interpretable, confident and robust deep neural networks. ||| sercan  ||| mer arik ||| tomas pfister ||| 
2018 ||| learning universal sentence representations with mean-max attention autoencoder. ||| minghua zhang ||| yunfang wu ||| weikang li ||| wei li ||| 
2018 ||| stacked semantic-guided attention model for fine-grained zero-shot learning. ||| yunlong yu ||| zhong ji ||| yanwei fu ||| jichang guo ||| yanwei pang ||| zhongfei zhang ||| 
2021 ||| transformer-based source-free domain adaptation. ||| guanglei yang ||| hao tang ||| zhun zhong ||| mingli ding ||| ling shao ||| nicu sebe ||| elisa ricci ||| 
2019 ||| hyper vision net: kidney tumor segmentation using coordinate convolutional layer and attention unit. ||| d. sabari nathan ||| m. parisa beham ||| s. md. mansoor roomi ||| 
2022 ||| scalablevit: rethinking the context-oriented generalization of vision transformer. ||| rui yang ||| hailong ma ||| jie wu ||| yansong tang ||| xuefeng xiao ||| min zheng ||| xiu li ||| 
2021 ||| hierarchical attention-based age estimation and bias estimation. ||| shakediel hiba ||| yosi keller ||| 
2021 ||| transformers generalize linearly. ||| jackson petty ||| robert frank ||| 
2020 ||| spatial attention as an interface for image captioning models. ||| philipp sadler ||| 
2022 ||| radiotransformer: a cascaded global-focal transformer for visual attention-guided disease classification. ||| moinak bhattacharya ||| shubham jain ||| prateek prasanna ||| 
2020 ||| residual attention net for superior cross-domain time sequence modeling. ||| seth h. huang ||| lingjie xu ||| congwei jiang ||| 
2021 ||| line segment detection using transformers without edges. ||| yifan xu ||| weijian xu ||| david cheung ||| zhuowen tu ||| 
2022 ||| interactive attention ai to translate low light photos to captions for night scene understanding in women safety. ||| rajagopal a. ||| nirmala v. ||| arun muthuraj vedamanickam ||| 
2020 ||| transformer on a diet. ||| chenguang wang ||| zihao ye ||| aston zhang ||| zheng zhang ||| alexander j. smola ||| 
2019 ||| marginalized average attentional network for weakly-supervised learning. ||| yuan yuan ||| yueming lyu ||| xi shen ||| ivor w. tsang ||| dit-yan yeung ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| marius mosbach ||| anna khokhlova ||| michael a. hedderich ||| dietrich klakow ||| 
2019 ||| learning bodily and temporal attention in protective movement behavior detection. ||| chongyang wang ||| min peng ||| temitayo a. olugbade ||| nicholas d. lane ||| amanda c. de c. williams ||| nadia bianchi-berthouze ||| 
2020 ||| attention-guided version of 2d unet for automatic brain tumor segmentation. ||| mehrdad noori ||| ali bahri ||| karim mohammadi ||| 
2021 ||| attention forcing for machine translation. ||| qingyun dou ||| yiting lu ||| potsawee manakul ||| xixin wu ||| mark j. f. gales ||| 
2021 ||| generative residual attention network for disease detection. ||| euyoung kim ||| soochahn lee ||| kyoung mu lee ||| 
2017 ||| rra: recurrent residual attention for sequence learning. ||| cheng wang ||| 
2021 ||| efficient visual tracking with exemplar transformers. ||| philippe blatter ||| menelaos kanakis ||| martin danelljan ||| luc van gool ||| 
2021 ||| all you can embed: natural language based vehicle retrieval with spatio-temporal transformers. ||| carmelo scribano ||| davide sapienza ||| giorgia franchini ||| micaela verucchi ||| marko bertogna ||| 
2019 ||| adding interpretable attention to neural translation models improves word alignment. ||| thomas zenkel ||| joern wuebker ||| john denero ||| 
2022 ||| visual representation learning with self-supervised attention for low-label high-data regime. ||| prarthana bhattacharyya ||| chenge li ||| xiaonan zhao ||| istv ||| n feh ||| rv ||| ri ||| jason sun ||| 
2020 ||| enriching the transformer with linguistic and semantic factors for low-resource machine translation. ||| jordi armengol-estap ||| marta r. costa-juss ||| carlos escolano ||| 
2020 ||| channel-attention dense u-net for multichannel speech enhancement. ||| bahareh tolooshams ||| ritwik giri ||| andrew h. song ||| umut isik ||| arvindh krishnaswamy ||| 
2018 ||| recursive visual attention in visual dialog. ||| yulei niu ||| hanwang zhang ||| manli zhang ||| jianhong zhang ||| zhiwu lu ||| ji-rong wen ||| 
2020 ||| the chess transformer: mastering play using generative language models. ||| david noever ||| matthew ciolino ||| josh kalin ||| 
2021 ||| is sparse attention more interpretable? ||| clara meister ||| stefan lazov ||| isabelle augenstein ||| ryan cotterell ||| 
2021 ||| dan: decentralized attention-based neural network to solve the minmax multiple traveling salesman problem. ||| yuhong cao ||| zhanhong sun ||| guillaume sartoretti ||| 
2021 ||| routing with self-attention for multimodal capsule networks. ||| kevin duarte ||| brian chen ||| nina shvetsova ||| andrew rouditchenko ||| samuel thomas ||| alexander liu ||| david harwath ||| james r. glass ||| hilde kuehne ||| mubarak shah ||| 
2021 ||| end-to-end neural diarization: from transformer to conformer. ||| yi-chieh liu ||| eunjung han ||| chul lee ||| andreas stolcke ||| 
2018 ||| predicting gaze in egocentric video by learning task-dependent attention transition. ||| yifei huang ||| minjie cai ||| zhenqiang li ||| yoichi sato ||| 
2021 ||| the multimodal driver monitoring database: a naturalistic corpus to study driver attention. ||| sumit jha ||| mohamed f. marzban ||| tiancheng hu ||| mohamed h. mahmoud ||| naofal al-dhahir ||| carlos busso ||| 
2018 ||| phonetic-attention scoring for deep speaker features in speaker verification. ||| lantian li ||| zhiyuan tang ||| ying shi ||| dong wang ||| 
2021 ||| token labeling: training a 85.4% top-1 accuracy vision transformer with 56m parameters on imagenet. ||| zihang jiang ||| qibin hou ||| li yuan ||| daquan zhou ||| xiaojie jin ||| anran wang ||| jiashi feng ||| 
2021 ||| choose a transformer: fourier or galerkin. ||| shuhao cao ||| 
2019 ||| forecaster: a graph transformer for forecasting spatial and time-dependent data. ||| yang li ||| jos |||  m. f. moura ||| 
2021 ||| going deeper with image transformers. ||| hugo touvron ||| matthieu cord ||| alexandre sablayrolles ||| gabriel synnaeve ||| herv |||  j ||| gou ||| 
2021 ||| action forecasting with feature-wise self-attention. ||| yan bin ng ||| basura fernando ||| 
2022 ||| voxel set transformer: a set-to-set approach to 3d object detection from point clouds. ||| chenhang he ||| ruihuang li ||| shuai li ||| lei zhang ||| 
2019 ||| wildmix dataset and spectro-temporal transformer model for monoaural audio source separation. ||| amir zadeh ||| tianjun ma ||| soujanya poria ||| louis-philippe morency ||| 
2018 ||| allegation of scientific misconduct increases twitter attention. ||| lutz bornmann ||| robin haunschild ||| 
2021 ||| pixeltransformer: sample conditioned signal generation. ||| shubham tulsiani ||| abhinav gupta ||| 
2022 ||| memorizing transformers. ||| yuhuai wu ||| markus n. rabe ||| delesley hutchins ||| christian szegedy ||| 
2021 ||| (m)slae-net: multi-scale multi-level attention embedded network for retinal vessel segmentation. ||| shreshth saini ||| geetika agrawal ||| 
2021 ||| polyp-pvt: polyp segmentation with pyramid vision transformers. ||| bo dong ||| wenhai wang ||| deng-ping fan ||| jinpeng li ||| huazhu fu ||| ling shao ||| 
2022 ||| mask usage recognition using vision transformer with transfer learning and data augmentation. ||| hensel donato jahja ||| novanto yudistira ||| sutrisno ||| 
2022 ||| zero-shot sketch based image retrieval using graph transformer. ||| sumrit gupta ||| ushasi chaudhuri ||| biplab banerjee ||| 
2021 ||| upanets: learning from the universal pixel attention networks. ||| ching-hsun tseng ||| shin-jye lee ||| jia-nan feng ||| shengzhong mao ||| yu-ping wu ||| jia-yu shang ||| mou-chung tseng ||| xiao-jun zeng ||| 
2020 ||| transformer transducer: one model unifying streaming and non-streaming speech recognition. ||| anshuman tripathi ||| jaeyoung kim ||| qian zhang ||| han lu ||| hasim sak ||| 
2020 ||| accessing higher-level representations in sequential transformers with feedback memory. ||| angela fan ||| thibaut lavril ||| edouard grave ||| armand joulin ||| sainbayar sukhbaatar ||| 
2020 ||| liftformer: 3d human pose estimation using attention models. ||| adrian llopart ||| 
2019 ||| multi-head attention with diversity for learning grounded multilingual multimodal representations. ||| po-yao huang ||| xiaojun chang ||| alexander g. hauptmann ||| 
2022 ||| 3dctn: 3d convolution-transformer network for point cloud classification. ||| dening lu ||| qian xie ||| linlin xu ||| jonathan li ||| 
2019 ||| multi-criteria chinese word segmentation with transformer. ||| xipeng qiu ||| hengzhi pei ||| hang yan ||| xuanjing huang ||| 
2021 ||| levit-unet: make faster encoders with transformer for medical image segmentation. ||| guoping xu ||| xingrong wu ||| xuan zhang ||| xinwei he ||| 
2021 ||| soit: segmenting objects with instance-aware transformers. ||| xiaodong yu ||| dahu shi ||| xing wei ||| ye ren ||| tingqun ye ||| wenming tan ||| 
2020 ||| transformer-xl based music generation with multiple sequences of time-valued notes. ||| xianchao wu ||| chengyuan wang ||| qinying lei ||| 
2020 ||| deep attention fusion feature for speech separation with end-to-end post-filter method. ||| cunhang fan ||| jianhua tao ||| bin liu ||| jiangyan yi ||| zhengqi wen ||| xuefei liu ||| 
2021 ||| emoji-based co-attention network for microblog sentiment analysis. ||| xiaowei yuan ||| jingyuan hu ||| xiaodan zhang ||| honglei lv ||| hao liu ||| 
2019 ||| multimodal transformer with multi-view visual representation for image captioning. ||| jun yu ||| jing li ||| zhou yu ||| qingming huang ||| 
2019 ||| frame attention networks for facial expression recognition in videos. ||| debin meng ||| xiaojiang peng ||| kai wang ||| yu qiao ||| 
2021 ||| human interaction recognition framework based on interacting body part attention. ||| dong-gyu lee ||| seong-whan lee ||| 
2021 ||| tokens-to-token vit: training vision transformers from scratch on imagenet. ||| li yuan ||| yunpeng chen ||| tao wang ||| weihao yu ||| yujun shi ||| francis e. h. tay ||| jiashi feng ||| shuicheng yan ||| 
2021 ||| implicit and explicit attention for zero-shot learning. ||| faisal alamri ||| anjan dutta ||| 
2021 ||| attention-based adversarial appearance learning of augmented pedestrians. ||| kevin strauss ||| artem savkin ||| federico tombari ||| 
2019 ||| interpretable self-attention temporal reasoning for driving behavior understanding. ||| yi-chieh liu ||| yung-an hsieh ||| min-hung chen ||| chao-han huck yang ||| jesper tegn ||| r ||| yi-chang james tsai ||| 
2022 ||| vision transformer with convolutions architecture search. ||| haichao zhang ||| kuangrong hao ||| witold pedrycz ||| lei gao ||| xue-song tang ||| bing wei ||| 
2020 ||| investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets. ||| jinghui lu ||| brian macnamee ||| 
2021 ||| ft-tdr: frequency-guided transformer and top-down refinement network for blind face inpainting. ||| junke wang ||| shaoxiang chen ||| zuxuan wu ||| yu-gang jiang ||| 
2021 ||| stan: spatio-temporal attention network for next location recommendation. ||| yingtao luo ||| qiang liu ||| zhaocheng liu ||| 
2021 ||| had-net: hybrid attention-based diffusion network for glucose level forecast. ||| quentin blampey ||| mehdi rahim ||| 
2021 ||| aa3dnet: attention augmented real time 3d object detection. ||| abhinav sagar ||| 
2021 ||| a transformer-based math language model for handwritten math expression recognition. ||| quang huy ung ||| cuong tuan nguyen ||| hung tuan nguyen ||| thanh-nghia truong ||| masaki nakagawa ||| 
2018 ||| vmav-c: a deep attention-based reinforcement learning algorithm for model-based control. ||| xingxing liang ||| qi wang ||| yang-he feng ||| zhong liu ||| jincai huang ||| 
2019 ||| attention-aware multi-stroke style transfer. ||| yuan yao ||| jianqiang ren ||| xuansong xie ||| weidong liu ||| yong-jin liu ||| jun wang ||| 
2021 ||| dudotrans: dual-domain transformer provides more attention for sinogram restoration in sparse-view ct reconstruction. ||| ce wang ||| kun shang ||| haimiao zhang ||| qian li ||| yuan hui ||| s. kevin zhou ||| 
2020 ||| attentional feature fusion. ||| yimian dai ||| fabian gieseke ||| stefan oehmcke ||| yiquan wu ||| kobus barnard ||| 
2021 ||| interpretable attention guided network for fine-grained visual classification. ||| zhenhuan huang ||| xiaoyue duan ||| bo zhao ||| jinhu l ||| baochang zhang ||| 
2018 ||| cross-modal attentional context learning for rgb-d object detection. ||| guanbin li ||| yukang gan ||| hejun wu ||| nong xiao ||| liang lin ||| 
2019 ||| deep-emotion: facial expression recognition using attentional convolutional network. ||| shervin minaee ||| amirali abdolrashidi ||| 
2021 ||| stacked temporal attention: improving first-person action recognition by emphasizing discriminative clips. ||| lijin yang ||| yifei huang ||| yusuke sugano ||| yoichi sato ||| 
2020 ||| super-sam: using the supervision signal from a pose estimator to train a spatial attention module for personal protective equipment recognition. ||| adrian sandru ||| georgian-emilian duta ||| mariana-iuliana georgescu ||| radu tudor ionescu ||| 
2019 ||| a free lunch in generating datasets: building a vqg and vqa system with attention and humans in the loop. ||| jihyeon janel lee ||| sho arora ||| 
2020 ||| transformer interpretability beyond attention visualization. ||| hila chefer ||| shir gur ||| lior wolf ||| 
2019 ||| bionlp-ost 2019 rdoc tasks: multi-grain neural relevance ranking using topics and attention based query-document-sentence interactions. ||| yatin chaudhary ||| pankaj gupta ||| hinrich sch ||| tze ||| 
2021 ||| multi-compound transformer for accurate biomedical image segmentation. ||| yuanfeng ji ||| ruimao zhang ||| huijie wang ||| zhen li ||| lingyun wu ||| shaoting zhang ||| ping luo ||| 
2018 ||| learning multi-touch conversion attribution with dual-attention mechanisms for online advertising. ||| kan ren ||| yuchen fang ||| weinan zhang ||| shuhao liu ||| jiajun li ||| ya zhang ||| yong yu ||| jun wang ||| 
2017 ||| pay attention to those sets! learning quantification from images. ||| ionut sorodoc ||| sandro pezzelle ||| aur ||| lie herbelot ||| mariella dimiccoli ||| raffaella bernardi ||| 
2020 ||| dual-decoder transformer for joint automatic speech recognition and multilingual speech translation. ||| hang le ||| juan miguel pino ||| changhan wang ||| jiatao gu ||| didier schwab ||| laurent besacier ||| 
2020 ||| data movement is all you need: a case study on optimizing transformers. ||| andrei ivanov ||| nikoli dryden ||| tal ben-nun ||| shigang li ||| torsten hoefler ||| 
2021 ||| video super-resolution transformer. ||| jiezhang cao ||| yawei li ||| kai zhang ||| luc van gool ||| 
2021 ||| graph-based tri-attention network for answer ranking in cqa. ||| wei zhang ||| zeyuan chen ||| chao dong ||| wen wang ||| hongyuan zha ||| jianyong wang ||| 
2020 ||| transformers as soft reasoners over language. ||| peter clark ||| oyvind tafjord ||| kyle richardson ||| 
2021 ||| a transformer architecture for stress detection from ecg. ||| behnam behinaein ||| anubhav bhatti ||| dirk rodenburg ||| paul hungler ||| ali etemad ||| 
2021 ||| neat: neural attention fields for end-to-end autonomous driving. ||| kashyap chitta ||| aditya prakash ||| andreas geiger ||| 
2020 ||| tree-structured attention with hierarchical accumulation. ||| xuan-phi nguyen ||| shafiq r. joty ||| steven c. h. hoi ||| richard socher ||| 
2020 ||| support-bert: predicting quality of question-answer pairs in msdn using deep bidirectional transformer. ||| bhaskar sen ||| nikhil gopal ||| xinwei xue ||| 
2019 ||| axial attention in multidimensional transformers. ||| jonathan ho ||| nal kalchbrenner ||| dirk weissenborn ||| tim salimans ||| 
2019 ||| deep floor plan recognition using a multi-task network with room-boundary-guided attention. ||| zhiliang zeng ||| xianzhi li ||| ying kin yu ||| chi-wing fu ||| 
2020 ||| hybrid transformer/ctc networks for hardware efficient voice triggering. ||| saurabh adya ||| vineet garg ||| siddharth sigtia ||| pramod simha ||| chandra dhir ||| 
2021 ||| everything at once - multi-modal fusion transformer for video retrieval. ||| nina shvetsova ||| brian chen ||| andrew rouditchenko ||| samuel thomas ||| brian kingsbury ||| rog ||| rio feris ||| david harwath ||| james r. glass ||| hilde kuehne ||| 
2021 ||| soft-attention improves skin cancer classification performance. ||| soumyya kanti datta ||| mohammad abuzar shaikh ||| sargur n. srihari ||| mingchen gao ||| 
2018 ||| neural machine translation with key-value memory-augmented attention. ||| fandong meng ||| zhaopeng tu ||| yong cheng ||| haiyang wu ||| junjie zhai ||| yuekui yang ||| di wang ||| 
2021 ||| more than just attention: learning cross-modal attentions with contrastive constraints. ||| yuxiao chen ||| jianbo yuan ||| long zhao ||| rui luo ||| larry davis ||| dimitris n. metaxas ||| 
2020 ||| limited view tomographic reconstruction using a deep recurrent framework with residual dense spatial-channel attention network and sinogram consistency. ||| bo zhou ||| s. kevin zhou ||| james s. duncan ||| chi liu ||| 
2020 ||| taming transformers for high-resolution image synthesis. ||| patrick esser ||| robin rombach ||| bj ||| rn ommer ||| 
2022 ||| transformer-based htr for historical documents. ||| phillip benjamin str ||| bel ||| simon clematide ||| martin volk ||| tobias hodel ||| 
2022 ||| anovit: unsupervised anomaly detection and localization with vision transformer-based encoder-decoder. ||| yunseung lee ||| pilsung kang ||| 
2020 ||| self-segregating and coordinated-segregating transformer for focused deep multi-modular network for visual question answering. ||| chiranjib sur ||| 
2021 ||| efficient transformer for direct speech translation. ||| belen alastruey ||| gerard i. g ||| llego ||| marta r. costa-juss ||| 
2019 ||| earlier attention? aspect-aware lstm for aspect sentiment analysis. ||| bowen xing ||| lejian liao ||| dandan song ||| jingang wang ||| fuzheng zhang ||| zhongyuan wang ||| heyan huang ||| 
2021 ||| referring transformer: a one-step approach to multi-task visual grounding. ||| muchen li ||| leonid sigal ||| 
2019 ||| domain adaptive attention model for unsupervised cross-domain person re-identification. ||| yangru huang ||| peixi peng ||| yi jin ||| junliang xing ||| congyan lang ||| songhe feng ||| 
2022 ||| a topology-attention convlstm network and its application to em images. ||| jiaqi yang ||| xiaoling hu ||| chao chen ||| chialing tsai ||| 
2020 ||| jointly cross- and self-modal graph attention network for query-based moment localization. ||| daizong liu ||| xiaoye qu ||| xiao-yang liu ||| jianfeng dong ||| pan zhou ||| zichuan xu ||| 
2022 ||| dagam: a domain adversarial graph attention model for subject independent eeg-based emotion recognition. ||| tao xu ||| wang dang ||| jiabao wang ||| yun zhou ||| 
2018 ||| calibnet: self-supervised extrinsic calibration using 3d spatial transformer networks. ||| ganesh iyer ||| karnik ram r. ||| j. krishna murthy ||| k. madhava krishna ||| 
2021 ||| spatial transformer networks for curriculum learning. ||| fatemeh azimi ||| jean-francois jacques nicolas nies ||| sebastian palacio ||| federico raue ||| j ||| rn hees ||| andreas dengel ||| 
2021 ||| gradts: a gradient-based automatic auxiliary task selection method based on transformer networks. ||| weicheng ma ||| renze lou ||| kai zhang ||| lili wang ||| soroush vosoughi ||| 
2020 ||| bilateral attention network for rgb-d salient object detection. ||| zhao zhang ||| zheng lin ||| jun xu ||| wenda jin ||| shao-ping lu ||| deng-ping fan ||| 
2020 ||| reformer: the efficient transformer. ||| nikita kitaev ||| lukasz kaiser ||| anselm levskaya ||| 
2020 ||| domain adaptive transfer learning on visual attention aware data augmentation for fine-grained visual categorization. ||| ashiq imran ||| vassilis athitsos ||| 
2019 ||| towards interpretable reinforcement learning using attention augmented agents. ||| alex mott ||| daniel zoran ||| mike chrzanowski ||| daan wierstra ||| danilo j. rezende ||| 
2018 ||| attention to head locations for crowd counting. ||| youmei zhang ||| chunluan zhou ||| faliang chang ||| alex c. kot ||| 
2022 ||| self-supervised transformer for deepfake detection. ||| hanqing zhao ||| wenbo zhou ||| dongdong chen ||| weiming zhang ||| nenghai yu ||| 
2021 ||| bounded logit attention: learning to explain image classifiers. ||| thomas baumhauer ||| djordje slijepcevic ||| matthias zeppelzauer ||| 
2021 ||| graph attention networks with lstm-based path reweighting. ||| jianpeng chen ||| yujing wang ||| ming zeng ||| zongyi xiang ||| yazhou ren ||| 
2022 ||| enhancing local feature learning for 3d point cloud processing using unary-pairwise attention. ||| haoyi xiu ||| xin liu ||| weiming wang ||| kyoung-sook kim ||| takayuki shinohara ||| qiong chang ||| masashi matsuoka ||| 
2019 ||| time-weighted attentional session-aware recommender system. ||| mei wang ||| weizhi li ||| yan yan ||| 
2020 ||| fragmentvc: any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention. ||| yist y. lin ||| chung-ming chien ||| jheng-hao lin ||| hung-yi lee ||| lin-shan lee ||| 
2021 ||| moefication: conditional computation of transformer models for efficient inference. ||| zhengyan zhang ||| yankai lin ||| zhiyuan liu ||| peng li ||| maosong sun ||| jie zhou ||| 
2021 ||| dual-level collaborative transformer for image captioning. ||| yunpeng luo ||| jiayi ji ||| xiaoshuai sun ||| liujuan cao ||| yongjian wu ||| feiyue huang ||| chia-wen lin ||| rongrong ji ||| 
2021 ||| snowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer. ||| peng xiang ||| xin wen ||| yu-shen liu ||| yan-pei cao ||| pengfei wan ||| wen zheng ||| zhizhong han ||| 
2020 ||| atsal: an attention based architecture for saliency prediction in 360 videos. ||| yasser abdelaziz dahou djilali ||| marouane tliba ||| kevin mcguinness ||| noel e. o'connor ||| 
2021 ||| an empirical evaluation of attention-based multi-head models for improved turbofan engine remaining useful life prediction. ||| abiodun ayodeji ||| wenhai wang ||| jianzhong su ||| jianquan yuan ||| xinggao liu ||| 
2020 ||| hybrid-attention guided network with multiple resolution features for person re-identification. ||| guoqing zhang ||| junchuan yang ||| yuhui zheng ||| yi wu ||| shengyong chen ||| 
2021 ||| sliced recursive transformer. ||| zhiqiang shen ||| zechun liu ||| eric p. xing ||| 
2022 ||| clinical-longformer and clinical-bigbird: transformers for long clinical sequences. ||| yikuan li ||| ramsey m. wehbe ||| faraz s. ahmad ||| hanyin wang ||| yuan luo ||| 
2021 ||| contrastive attention network with dense field estimation for face completion. ||| xin ma ||| xiaoqiang zhou ||| huaibo huang ||| gengyun jia ||| zhenhua chai ||| xiaolin wei ||| 
2021 ||| self-attention based context-aware 3d object detection. ||| prarthana bhattacharyya ||| chengjie huang ||| krzysztof czarnecki ||| 
2022 ||| a pre-trained audio-visual transformer for emotion recognition. ||| minh tran ||| mohammad soleymani ||| 
2020 ||| trans^3: a transformer-based framework for unifying code summarization and code search. ||| wenhua wang ||| yuqun zhang ||| zhengran zeng ||| guandong xu ||| 
2020 ||| be more with less: hypergraph attention networks for inductive text classification. ||| kaize ding ||| jianling wang ||| jundong li ||| dingcheng li ||| huan liu ||| 
2021 ||| spectrum attention mechanism for time series classification. ||| shibo zhou ||| yu pan ||| 
2021 ||| pgt: pseudo relevance feedback using a graph-based transformer. ||| hongchien yu ||| zhuyun dai ||| jamie callan ||| 
2019 ||| scheduled sampling for transformers. ||| tsvetomila mihaylova ||| andr |||  f. t. martins ||| 
2020 ||| towards accurate pixel-wise object tracking by attention retrieval. ||| zhipeng zhang ||| bing li ||| weiming hu ||| houwen peng ||| 
2022 ||| semi-supervised new event type induction and description via contrastive loss-enforced batch attention. ||| carl edwards ||| heng ji ||| 
2019 ||| distraction-aware feature learning for human attribute recognition via coarse-to-fine attention mechanism. ||| mingda wu ||| di huang ||| yuanfang guo ||| yunhong wang ||| 
2021 ||| coarse-to-fine q-attention: efficient learning for visual robotic manipulation via discretisation. ||| stephen james ||| kentaro wada ||| tristan laidlow ||| andrew j. davison ||| 
2020 ||| liquid warping gan with attention: a unified framework for human image synthesis. ||| wen liu ||| zhixin piao ||| zhi tu ||| wenhan luo ||| lin ma ||| shenghua gao ||| 
2020 ||| isaaq - mastering textbook questions with pre-trained transformers and bottom-up and top-down attention. ||| jos |||  manu ||| l g ||| mez-p ||| rez ||| ra ||| l ortega ||| 
2020 ||| light weight residual dense attention net for spectral reconstruction from rgb images. ||| d. sabari nathan ||| k. uma ||| d. synthiya vinothini ||| b. sathya bama ||| s. m. md mansoor roomi ||| 
2021 ||| transformer-based methods for recognizing ultra fine-grained entities (rufes). ||| emanuela boros ||| antoine doucet ||| 
2021 ||| a transformer based approach for fighting covid-19 fake news. ||| s. m. sadiq-ur-rahman shifath ||| mohammad faiyaz khan ||| md. saiful islam ||| 
2022 ||| mirror-yolo: an attention-based instance segmentation and detection model for mirrors. ||| fengze li ||| jieming ma ||| zhongbei tian ||| ji ge ||| hai-ning liang ||| yungang zhang ||| tianxi wen ||| 
2021 ||| learning spatio-temporal transformer for visual tracking. ||| bin yan ||| houwen peng ||| jianlong fu ||| dong wang ||| huchuan lu ||| 
2019 ||| convert: efficient and accurate conversational representations from transformers. ||| matthew henderson ||| i ||| igo casanueva ||| nikola mrksic ||| pei-hao su ||| tsung-hsien wen ||| ivan vulic ||| 
2018 ||| graph2seq: graph to sequence learning with attention-based neural networks. ||| kun xu ||| lingfei wu ||| zhiguo wang ||| yansong feng ||| vadim sheinin ||| 
2020 ||| association rules enhanced knowledge graph attention network. ||| zhenghao zhang ||| jianbin huang ||| qinglin tan ||| 
2019 ||| stochastic region pooling: make attention more expressive. ||| mingnan luo ||| guihua wen ||| yang hu ||| dan dai ||| yingxue xu ||| 
2019 ||| automatic spelling correction with transformer for ctc-based end-to-end speech recognition. ||| shiliang zhang ||| ming lei ||| zhijie yan ||| 
2017 ||| convolutional attention-based seq2seq neural network for end-to-end asr. ||| dan lim ||| 
2019 ||| factorized multimodal transformer for multimodal sequential learning. ||| amir zadeh ||| chengfeng mao ||| kelly shi ||| yiwei zhang ||| paul pu liang ||| soujanya poria ||| louis-philippe morency ||| 
2018 ||| attention incorporate network: a network can adapt various data size. ||| liangbo he ||| hao sun ||| 
2021 ||| hand gesture recognition using temporal convolutions and attention mechanism. ||| elahe rahimian ||| soheil zabihi ||| amir asif ||| dario farina ||| seyed farokh atashzar ||| arash mohammadi ||| 
2021 ||| ammasurv: asymmetrical multi-modal attention for accurate survival analysis with whole slide images and gene expression data. ||| ruoqi wang ||| ziwang huang ||| haitao wang ||| hejun wu ||| 
2021 ||| towards robust vision transformer. ||| xiaofeng mao ||| gege qi ||| yuefeng chen ||| xiaodan li ||| ranjie duan ||| shaokai ye ||| yuan he ||| hui xue ||| 
2019 ||| treegen: a tree-based transformer architecture for code generation. ||| zeyu sun ||| qihao zhu ||| yingfei xiong ||| yican sun ||| lili mou ||| lu zhang ||| 
2021 ||| visual transformers with primal object queries for multi-label image classification. ||| vacit oguz yazici ||| joost van de weijer ||| longlong yu ||| 
2020 ||| multiresolution attention extractor for small object detection. ||| fan zhang ||| licheng jiao ||| lingling li ||| fang liu ||| xu liu ||| 
2020 ||| spectral graph attention network. ||| heng chang ||| yu rong ||| tingyang xu ||| wenbing huang ||| somayeh sojoudi ||| junzhou huang ||| wenwu zhu ||| 
2019 ||| depth-adaptive transformer. ||| maha elbayad ||| jiatao gu ||| edouard grave ||| michael auli ||| 
2021 ||| video relation detection via tracklet based visual transformer. ||| kaifeng gao ||| long chen ||| yifeng huang ||| jun xiao ||| 
2021 ||| visual attention in imaginative agents. ||| samrudhdhi b. rangrej ||| james j. clark ||| 
2021 ||| efficient transformer for single image super-resolution. ||| zhisheng lu ||| hong liu ||| juncheng li ||| linlin zhang ||| 
2019 ||| stage: spatio-temporal attention on graph entities for video action detection. ||| matteo tomei ||| lorenzo baraldi ||| simone calderara ||| simone bronzin ||| rita cucchiara ||| 
2021 ||| visual explanation using attention mechanism in actor-critic-based deep reinforcement learning. ||| hidenori itaya ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| komei sugiura ||| 
2021 ||| convolution-free medical image segmentation using transformers. ||| davood karimi ||| serge vasylechko ||| ali gholipour ||| 
2019 ||| computational attention system for children, adults and elderly. ||| onkar krishna ||| kiyoharu aizawa ||| go irie ||| 
2019 ||| what would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention. ||| antonino furnari ||| giovanni maria farinella ||| 
2020 ||| transformer-gcrf: recovering chinese dropped pronouns with general conditional random fields. ||| jingxuan yang ||| kerui xu ||| jun xu ||| si li ||| sheng gao ||| jun guo ||| ji-rong wen ||| nianwen xue ||| 
2020 ||| an attention-based deep learning model for multiple pedestrian attributes recognition. ||| ehsan yaghoubi ||| diana borza ||| jo ||| o c. neves ||| aruna kumar ||| hugo proen ||| a ||| 
2020 ||| attention-based query expansion learning. ||| albert gordo ||| filip radenovic ||| tamara berg ||| 
2021 ||| do context-aware translation models pay the right attention? ||| kayo yin ||| patrick fernandes ||| danish pruthi ||| aditi chaudhary ||| andr |||  f. t. martins ||| graham neubig ||| 
2020 ||| few-shot few-shot learning and the role of spatial attention. ||| yann lifchitz ||| yannis avrithis ||| sylvaine picard ||| 
2020 ||| a hierarchical transformer with speaker modeling for emotion recognition in conversation. ||| jiangnan li ||| zheng lin ||| peng fu ||| qingyi si ||| weiping wang ||| 
2018 ||| sam-gcnn: a gated convolutional neural network with segment-level attention mechanism for home activity monitoring. ||| yu-han shen ||| ke-xin he ||| wei-qiang zhang ||| 
2021 ||| polarized self-attention: towards high-quality pixel-wise regression. ||| huajun liu ||| fuqiang liu ||| xinyi fan ||| dong huang ||| 
2021 ||| activity graph transformer for temporal action localization. ||| megha nawhal ||| greg mori ||| 
2021 ||| a state-of-the-art survey of object detection techniques in microorganism image analysis: from traditional image processing and classical machine learning to current deep convolutional neural networks and potential visual transformers. ||| chen li ||| pingli ma ||| md mamunur rahaman ||| yudong yao ||| jiawei zhang ||| shuojia zou ||| xin zhao ||| marcin grzegorzek ||| 
2021 ||| matching with transformers in melt. ||| sven hertling ||| jan portisch ||| heiko paulheim ||| 
2017 ||| pose-conditioned spatio-temporal attention for human action recognition. ||| fabien baradel ||| christian wolf ||| julien mille ||| 
2019 ||| multimodal transformer for unaligned multimodal language sequences. ||| yao-hung hubert tsai ||| shaojie bai ||| paul pu liang ||| j. zico kolter ||| louis-philippe morency ||| ruslan salakhutdinov ||| 
2020 ||| wavelet channel attention module with a fusion network for single image deraining. ||| hao-hsiang yang ||| chao-han huck yang ||| yu-chiang frank wang ||| 
2018 ||| end-to-end segmentation with recurrent attention neural network. ||| qiaoying huang ||| xiao chen ||| mariappan s. nadar ||| 
2022 ||| towards efficient and elastic visual question answering with doubly slimmable transformer. ||| zhou yu ||| zitian jin ||| jun yu ||| mingliang xu ||| jianping fan ||| 
2021 ||| emoji-aware co-attention network with emograph2vec model for sentiment anaylsis. ||| xiaowei yuan ||| jingyuan hu ||| xiaodan zhang ||| honglei lv ||| hao liu ||| 
2022 ||| adaptively re-weighting multi-loss untrained transformer for sparse-view cone-beam ct reconstruction. ||| minghui wu ||| yangdi xu ||| yingying xu ||| guangwei wu ||| qingqing chen ||| hongxiang lin ||| 
2020 ||| channel estimation for full-duplex ris-assisted haps backhauling with graph attention networks. ||| k ||| rsat tekbiyik ||| g ||| nes karabulut-kurt ||| chongwen huang ||| ali riza ekti ||| halim yanikomeroglu ||| 
2021 ||| shatter: an efficient transformer encoder with single-headed self-attention and relative sequence partitioning. ||| ran tian ||| joshua maynez ||| ankur p. parikh ||| 
2021 ||| cascade network with guided loss and hybrid attention for finding good correspondences. ||| zhi chen ||| fan yang ||| wenbing tao ||| 
2021 ||| an ecologically valid examination of event-based and time-based prospective memory using immersive virtual reality: the influence of attention, memory, and executive function processes on real-world prospective memory. ||| panagiotis kourtesis ||| sarah e. macpherson ||| 
2018 ||| context, attention and audio feature explorations for audio visual scene-aware dialog. ||| shachi h. kumar ||| eda okur ||| saurav sahay ||| juan jose alvarado leanos ||| jonathan huang ||| lama nachman ||| 
2020 ||| temporal convolutional attention-based network for sequence modeling. ||| hongyan hao ||| yan wang ||| yudi xia ||| jian zhao ||| furao shen ||| 
2019 ||| feratt: facial expression recognition with attention net. ||| pedro d. marrero-fern ||| ndez ||| fidel a. guerrero-pe ||| a ||| tsang ing ren ||| alexandre cunha ||| 
2019 ||| syntax-infused transformer and bert models for machine translation and natural language understanding. ||| dhanasekar sundararaman ||| vivek subramanian ||| guoyin wang ||| shijing si ||| dinghan shen ||| dong wang ||| lawrence carin ||| 
2017 ||| sensor transformation attention networks. ||| stefan braun ||| daniel neil ||| enea ceolini ||| jithendar anumula ||| shih-chii liu ||| 
2021 ||| semantic attention and scale complementary network for instance segmentation in remote sensing images. ||| tianyang zhang ||| xiangrong zhang ||| peng zhu ||| xu tang ||| chen li ||| licheng jiao ||| huiyu zhou ||| 
2021 ||| rethinking graph transformers with spectral attention. ||| devin kreuzer ||| dominique beaini ||| william l. hamilton ||| vincent l ||| tourneau ||| prudencio tossou ||| 
2020 ||| attention: to better stand on the shoulders of giants. ||| sha yuan ||| zhou shao ||| yu zhang ||| xingxing wei ||| tong xiao ||| yifan wang ||| jie tang ||| 
2020 ||| n-ode transformer: a depth-adaptive variant of the transformer using neural ordinary differential equations. ||| aaron baier-reinio ||| hans de sterck ||| 
2020 ||| graph attention tracking. ||| dongyan guo ||| yanyan shao ||| ying cui ||| zhenhua wang ||| liyan zhang ||| chunhua shen ||| 
2019 ||| graph attention auto-encoders. ||| amin salehi ||| hasan davulcu ||| 
2019 ||| saccader: improving accuracy of hard attention models for vision. ||| gamaleldin f. elsayed ||| simon kornblith ||| quoc v. le ||| 
2019 ||| fine-grained sentiment analysis with faithful attention. ||| ruiqi zhong ||| steven shao ||| kathleen r. mckeown ||| 
2022 ||| motion-aware transformer for occluded person re-identification. ||| mi zhou ||| hongye liu ||| zhekun lv ||| wei hong ||| xiai chen ||| 
2018 ||| self-attention-based message-relevant response generation for neural conversation model. ||| jonggu kim ||| doyeon kong ||| jong-hyeok lee ||| 
2021 ||| tritransnet: rgb-d salient object detection with a triplet transformer embedding network. ||| zhengyi liu ||| yuan wang ||| zhengzheng tu ||| yun xiao ||| bin tang ||| 
2018 ||| attention-based sequence-to-sequence model for speech recognition: development of state-of-the-art system on librispeech and its application to non-native english. ||| yan yin ||| ramon prieto ||| bin wang ||| jianwei zhou ||| yiwei gu ||| yang liu ||| hui lin ||| 
2021 ||| reveal of vision transformers robustness against adversarial attacks. ||| ahmed aldahdooh ||| wassim hamidouche ||| olivier d ||| forges ||| 
2020 ||| rethinking attention with performers. ||| krzysztof choromanski ||| valerii likhosherstov ||| david dohan ||| xingyou song ||| andreea gane ||| tam ||| s sarl ||| s ||| peter hawkins ||| jared davis ||| afroz mohiuddin ||| lukasz kaiser ||| david belanger ||| lucy j. colwell ||| adrian weller ||| 
2019 ||| syntactically supervised transformers for faster neural machine translation. ||| nader akoury ||| kalpesh krishna ||| mohit iyyer ||| 
2020 ||| wavelet denoising and attention-based rnn-arima model to predict forex price. ||| zhiwen zeng ||| matloob khushi ||| 
2019 ||| t-gsa: transformer with gaussian-weighted self-attention for speech enhancement. ||| jaeyoung kim ||| mostafa el-khamy ||| jungwon lee ||| 
2021 ||| vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. ||| sangjoon park ||| gwanghyun kim ||| yujin oh ||| joon beom seo ||| sang min lee ||| jin hwan kim ||| sungjun moon ||| jae-kwang lim ||| jong chul ye ||| 
2022 ||| lilt: a simple yet effective language-independent layout transformer for structured document understanding. ||| jiapeng wang ||| lianwen jin ||| kai ding ||| 
2022 ||| source code summarization with structural relative position guided transformer. ||| zi gong ||| cuiyun gao ||| yasheng wang ||| wenchao gu ||| yun peng ||| zenglin xu ||| 
2020 ||| the go transformer: natural language modeling for game play. ||| matthew ciolino ||| david noever ||| josh kalin ||| 
2019 ||| sparse graph attention networks. ||| yang ye ||| shihao ji ||| 
2021 ||| multi-scale speaker embedding-based graph attention networks for speaker diarisation. ||| youngki kwon ||| hee-soo heo ||| jee-weon jung ||| you jin kim ||| bong-jin lee ||| joon son chung ||| 
2021 ||| mobile-former: bridging mobilenet and transformer. ||| yinpeng chen ||| xiyang dai ||| dongdong chen ||| mengchen liu ||| xiaoyi dong ||| lu yuan ||| zicheng liu ||| 
2020 ||| stp-udgat: spatial-temporal-preference user dimensional graph attention network for next poi recommendation. ||| nicholas lim ||| bryan hooi ||| see-kiong ng ||| xueou wang ||| yong liang goh ||| renrong weng ||| jagannadan varadarajan ||| 
2021 ||| on learning the transformer kernel. ||| sankalan pal chowdhury ||| adamos solomou ||| avinava dubey ||| mrinmaya sachan ||| 
2020 ||| meta-context transformers for domain-specific response generation. ||| debanjana kar ||| suranjana samanta ||| amar prakash azad ||| 
2020 ||| context-aware group captioning via self-attention and contrastive features. ||| zhuowan li ||| quan tran ||| long mai ||| zhe lin ||| alan l. yuille ||| 
2020 ||| permutohedral-gcn: graph convolutional networks with global attention. ||| hesham mostafa ||| marcel nassar ||| 
2021 ||| paying attention to multiscale feature maps in multimodal image matching. ||| aviad moreshet ||| yosi keller ||| 
2022 ||| gaussian multi-head attention for simultaneous machine translation. ||| shaolei zhang ||| yang feng ||| 
2019 ||| a hvs-inspired attention map to improve cnn-based perceptual losses for image restoration. ||| taimoor tariq ||| juan luis gonzalez ||| munchurl kim ||| 
2021 ||| a pseudo label-wise attention network for automatic icd coding. ||| yifan wu ||| min zeng ||| ying yu ||| min li ||| 
2021 ||| urltran: improving phishing url detection using transformers. ||| pranav maneriker ||| jack w. stokes ||| edir garcia lazo ||| diana carutasu ||| farid tajaddodianfar ||| arun gururajan ||| 
2019 ||| spatiotemporal co-attention recurrent neural networks for human-skeleton motion prediction. ||| xiangbo shu ||| liyan zhang ||| guo-jun qi ||| wei liu ||| jinhui tang ||| 
2019 ||| weakly supervised attention networks for fine-grained opinion mining and public health. ||| giannis karamanolakis ||| daniel hsu ||| luis gravano ||| 
2018 ||| prior attention for style-aware sequence-to-sequence models. ||| lucas sterckx ||| johannes deleu ||| chris develder ||| thomas demeester ||| 
2019 ||| transfer learning with edge attention for prostate mri segmentation. ||| xiangxiang qin ||| 
2020 ||| explaining autonomous driving by learning end-to-end visual attention. ||| luca cultrera ||| lorenzo seidenari ||| federico becattini ||| pietro pala ||| alberto del bimbo ||| 
2021 ||| domain-independent user simulation with transformers for task-oriented dialogue systems. ||| hsien-chin lin ||| nurul lubis ||| songbo hu ||| carel van niekerk ||| christian geishauser ||| michael heck ||| shutong feng ||| milica gasic ||| 
2021 ||| supervised video summarization via multiple feature sets with parallel attention. ||| junaid ahmed ghauri ||| sherzod hakimov ||| ralph ewerth ||| 
2021 ||| looking outside the window: wider-context transformer for the semantic segmentation of high-resolution remote sensing images. ||| lei ding ||| dong lin ||| shaofu lin ||| jing zhang ||| xiaojie cui ||| yuebin wang ||| hao tang ||| lorenzo bruzzone ||| 
2021 ||| weakly supervised attention-based models using activation maps for citrus mite and insect pest classification. ||| edson r. bollis ||| helena almeida maia ||| h ||| lio pedrini ||| sandra avila ||| 
2020 ||| few-shot relation learning with attention for eeg-based motor imagery classification. ||| sion an ||| soopil kim ||| philip chikontwe ||| sang hyun park ||| 
2021 ||| token shift transformer for video classification. ||| hao zhang ||| yanbin hao ||| chong-wah ngo ||| 
2021 ||| can a transformer pass the wug test? tuning copying bias in neural morphological inflection models. ||| ling liu ||| mans hulden ||| 
2021 ||| neurosymbolic transformers for multi-agent communication. ||| jeevana priya inala ||| yichen yang ||| james paulos ||| yewen pu ||| osbert bastani ||| vijay kumar ||| martin rinard ||| armando solar-lezama ||| 
2019 ||| neuro-dram: a 3d recurrent visual attention model for interpretable neuroimaging classification. ||| david wood ||| james h. cole ||| thomas c. booth ||| 
2019 ||| attention mechanism enhanced kernel prediction networks for denoising of burst images. ||| bin zhang ||| shenyao jin ||| yili xia ||| yongming huang ||| zixiang xiong ||| 
2021 ||| unsupervised training data generation of handwritten formulas using generative adversarial networks with self-attention. ||| matthias springstein ||| eric m ||| ller-budack ||| ralph ewerth ||| 
2019 ||| visualizing attention in transformer-based language representation models. ||| jesse vig ||| 
2021 ||| end-to-end multi-channel transformer for speech recognition. ||| feng-ju chang ||| martin radfar ||| athanasios mouchtaris ||| brian king ||| siegfried kunzmann ||| 
2020 ||| durian-sc: duration informed attention network based singing voice conversion system. ||| liqiang zhang ||| chengzhu yu ||| heng lu ||| chao weng ||| chunlei zhang ||| yusong wu ||| xiang xie ||| zijin li ||| dong yu ||| 
2022 ||| tableformer: robust transformer modeling for table-text encoding. ||| jingfeng yang ||| aditya gupta ||| shyam upadhyay ||| luheng he ||| rahul goel ||| shachi paul ||| 
2022 ||| beyond fixation: dynamic window visual transformer. ||| pengzhen ren ||| changlin li ||| guangrun wang ||| yun xiao ||| qing du ||| xiaodan liang ||| xiaojun chang ||| 
2020 ||| w-net: dual supervised medical image segmentation model with multi-dimensional attention and cascade multi-scale convolution. ||| bo wang ||| lei wang ||| junyang chen ||| zhenghua xu ||| thomas lukasiewicz ||| zhigang fu ||| 
2019 ||| patchwork: a patch-wise attention network for efficient object detection and segmentation in video streams. ||| yuning chai ||| 
2018 ||| picanet: pixel-wise contextual attention learning for accurate saliency detection. ||| nian liu ||| junwei han ||| ming-hsuan yang ||| 
2017 ||| attention-set based metric learning for video face recognition. ||| 
2021 ||| processtransformer: predictive business process monitoring with transformer network. ||| zaharah allah bukhsh ||| aaqib saeed ||| remco m. dijkman ||| 
2019 ||| photometric transformer networks and label adjustment for breast density prediction. ||| jaehwan lee ||| donggeon yoo ||| jung yin huh ||| hyo-eun kim ||| 
2021 ||| epistemic planning with attention as a bounded resource. ||| gaia belardinelli ||| rasmus k. rendsvig ||| 
2021 ||| inductive biases and variable creation in self-attention mechanisms. ||| benjamin l. edelman ||| surbhi goel ||| sham m. kakade ||| cyril zhang ||| 
2020 ||| multi-channel transformers for multi-articulatory sign language translation. ||| necati cihan camg ||| z ||| oscar koller ||| simon hadfield ||| richard bowden ||| 
2021 ||| textcnn with attention for text classification. ||| ibrahim alshubaily ||| 
2021 ||| s-at gcn: spatial-attention graph convolution network based feature enhancement for 3d object detection. ||| li wang ||| chenfei wang ||| xinyu zhang ||| tianwei lan ||| jun li ||| 
2021 ||| d-net: siamese based network with mutual attention for volume alignment. ||| jian-qing zheng ||| ngee han lim ||| bartlomiej w. papiez ||| 
2020 ||| scalable multi-agent inverse reinforcement learning via actor-attention-critic. ||| wonseok jeon ||| paul barde ||| derek nowrouzezahrai ||| joelle pineau ||| 
2020 ||| data-informed global sparseness in attention mechanisms for deep neural networks. ||| ileana rugina ||| rumen dangovski ||| li jing ||| preslav nakov ||| marin soljacic ||| 
2019 ||| human vs machine attention in neural networks: a comparative study. ||| qiuxia lai ||| wenguan wang ||| salman h. khan ||| jianbing shen ||| hanqiu sun ||| ling shao ||| 
2020 ||| single image super-resolution via residual neuron attention networks. ||| wenjie ai ||| xiaoguang tu ||| shilei cheng ||| mei xie ||| 
2021 ||| sentence bottleneck autoencoders from transformer language models. ||| ivan montero ||| nikolaos pappas ||| noah a. smith ||| 
2018 ||| an online attention-based model for speech recognition. ||| ruchao fan ||| pan zhou ||| wei chen ||| jia jia ||| gang liu ||| 
2022 ||| lightn: light-weight transformer network for performance-overhead tradeoff in point cloud downsampling. ||| xu wang ||| yi jin ||| yigang cen ||| tao wang ||| bowen tang ||| yidong li ||| 
2022 ||| sparse local patch transformer for robust face alignment and landmarks inherent relation learning. ||| jiahao xia ||| weiwei qu ||| wenjian huang ||| jianguo zhang ||| xi wang ||| min xu ||| 
2020 ||| cascaded text generation with markov transformers. ||| yuntian deng ||| alexander m. rush ||| 
2021 ||| classifying scientific publications with bert - is self-attention a feature selection method? ||| andr ||| s garc ||| a-silva ||| jos |||  manu ||| l g ||| mez-p ||| rez ||| 
2019 ||| tdam: a topic-dependent attention model for sentiment analysis. ||| gabriele pergola ||| lin gui ||| yulan he ||| 
2019 ||| decay-function-free time-aware attention to context and speaker indicator for spoken language understanding. ||| jonggu kim ||| jong-hyeok lee ||| 
2017 ||| a spatiotemporal model with visual attention for video classification. ||| mo shan ||| nikolay atanasov ||| 
2020 ||| a frequency and phase attention based deep learning framework for partial discharge detection on insulated overhead conductors. ||| mohammad zunaed rafi ||| ankur nath ||| md. saifur rahman ||| 
2020 ||| dynamic attention based generative adversarial network with phase post-processing for speech enhancement. ||| andong li ||| chengshi zheng ||| renhua peng ||| cunhang fan ||| xiaodong li ||| 
2019 ||| transformers without tears: improving the normalization of self-attention. ||| toan q. nguyen ||| julian salazar ||| 
2020 ||| co-attentional transformers for story-based video understanding. ||| bj ||| rn bebensee ||| byoung-tak zhang ||| 
2018 ||| weakly supervised domain-specific color naming based on attention. ||| lu yu ||| yongmei cheng ||| joost van de weijer ||| 
2021 ||| centroid transformers: learning to abstract with attention. ||| lemeng wu ||| xingchao liu ||| qiang liu ||| 
2020 ||| a response retrieval approach for dialogue using a multi-attentive transformer. ||| matteo antonio senese ||| alberto benincasa ||| barbara caputo ||| giuseppe rizzo ||| 
2021 ||| materialized knowledge bases from commonsense transformers. ||| tuan-phong nguyen ||| simon razniewski ||| 
2022 ||| transformer-based multimodal information fusion for facial expression analysis. ||| wei zhang ||| zhimeng zhang ||| feng qiu ||| suzhen wang ||| bowen ma ||| hao zeng ||| rudong an ||| yu ding ||| 
2018 ||| tssd: temporal single-shot object detection based on attention-aware lstm. ||| xingyu chen ||| zhengxing wu ||| junzhi yu ||| 
2019 ||| open-ended long-form video question answering via hierarchical convolutional self-attention networks. ||| zhu zhang ||| zhou zhao ||| zhijie lin ||| jingkuan song ||| xiaofei he ||| 
2021 ||| spectral transform forms scalable transformer. ||| bingxin zhou ||| xinliang liu ||| yuehua liu ||| yunying huang ||| pietro li ||| yuguang wang ||| 
2021 ||| a survey of transformers. ||| tianyang lin ||| yuxin wang ||| xiangyang liu ||| xipeng qiu ||| 
2021 ||| focused attention improves document-grounded generation. ||| shrimai prabhumoye ||| kazuma hashimoto ||| yingbo zhou ||| alan w. black ||| ruslan salakhutdinov ||| 
2020 ||| channel attention residual u-net for retinal vessel segmentation. ||| changlu guo ||| m ||| rton szemenyei ||| yugen yi ||| wei zhou ||| 
2021 ||| mention memory: incorporating textual knowledge into transformers through entity mention attention. ||| michiel de jong ||| yury zemlyanskiy ||| nicholas fitzgerald ||| fei sha ||| william cohen ||| 
2020 ||| generating radiology reports via memory-driven transformer. ||| zhihong chen ||| yan song ||| tsung-hui chang ||| xiang wan ||| 
2021 ||| streaming transformer for hardware efficient voice trigger detection and false trigger mitigation. ||| vineet garg ||| wonil chang ||| siddharth sigtia ||| saurabh adya ||| pramod simha ||| pranay dighe ||| chandra dhir ||| 
2020 ||| explainable rumor detection using inter and intra-feature attention networks. ||| mingxuan chen ||| ning wang ||| k. p. subbalakshmi ||| 
2021 ||| unsupervised brain anomaly detection and segmentation with transformers. ||| walter hugo lopez pinaya ||| petru-daniel tudosiu ||| robert gray ||| geraint rees ||| parashkev nachev ||| s ||| bastien ourselin ||| m. jorge cardoso ||| 
2021 ||| graphical models with attention for context-specific independence and an application to perceptual grouping. ||| guangyao zhou ||| wolfgang lehrach ||| antoine dedieu ||| miguel l ||| zaro-gredilla ||| dileep george ||| 
2020 ||| ultra lightweight image super-resolution with multi-attention layers. ||| abdul muqeet ||| jiwon hwang ||| subin yang ||| jung heum kang ||| yongwoo kim ||| sung-ho bae ||| 
2020 ||| adaptive graph convolutional network with attention graph clustering for co-saliency detection. ||| kaihua zhang ||| tengpeng li ||| shiwen shen ||| bo liu ||| jin chen ||| qingshan liu ||| 
2020 ||| developing real-time streaming transformer transducer for speech recognition on large-scale dataset. ||| xie chen ||| yu wu ||| zhenghao wang ||| shujie liu ||| jinyu li ||| 
2020 ||| a review-based transformer model for personalized product search. ||| keping bi ||| qingyao ai ||| w. bruce croft ||| 
2020 ||| attention sequence to sequence model for machine remaining useful life prediction. ||| mohamed ragab ||| zhenghua chen ||| min wu ||| chee-keong kwoh ||| ruqiang yan ||| xiaoli li ||| 
2021 ||| self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition. ||| rong gong ||| carl quillen ||| dushyant sharma ||| andrew goderre ||| jos |||  la ||| nez ||| ljubomir milanovic ||| 
2019 ||| h-vectors: utterance-level speaker embedding using a hierarchical attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2021 ||| attendseg: a tiny attention condenser neural network for semantic segmentation on the edge. ||| xiaoyu wen ||| mahmoud famouri ||| andrew hryniowski ||| alexander wong ||| 
2020 ||| x-linear attention networks for image captioning. ||| yingwei pan ||| ting yao ||| yehao li ||| tao mei ||| 
2021 ||| leveraging transformers for starcraft macromanagement prediction. ||| muhammad junaid khan ||| shah hassan ||| gita sukthankar ||| 
2018 ||| input combination strategies for multi-source transformer decoder. ||| jindrich libovick ||| jindrich helcl ||| david marecek ||| 
2020 ||| 3d attention mechanism for fine-grained classification of table tennis strokes using a twin spatio-temporal convolutional neural networks. ||| pierre-etienne martin ||| jenny benois-pineau ||| renaud p ||| teri ||| julien morlier ||| 
2020 ||| fine-tuning a transformer-based language model to avoid generating non-normative text. ||| xiangyu peng ||| siyan li ||| spencer frazier ||| mark riedl ||| 
2021 ||| classifying long clinical documents with pre-trained transformers. ||| xin su ||| timothy miller ||| xiyu ding ||| majid afshar ||| dmitriy dligach ||| 
2019 ||| a transformer with interleaved self-attention and convolution for hybrid acoustic models. ||| liang lu ||| 
2022 ||| q-vit: fully differentiable quantization for vision transformer. ||| zhexin li ||| tong yang ||| peisong wang ||| jian cheng ||| 
2019 ||| attention based glaucoma detection: a large-scale database and cnn model. ||| liu li ||| mai xu ||| xiaofei wang ||| lai jiang ||| hanruo liu ||| 
2022 ||| attention-based random forest and contamination model. ||| lev v. utkin ||| andrei v. konstantinov ||| 
2021 ||| cross-lingual hate speech detection using transformer models. ||| teodor tita ||| arkaitz zubiaga ||| 
2021 ||| vision transformer for covid-19 cxr diagnosis using chest x-ray feature corpus. ||| sangjoon park ||| gwanghyun kim ||| yujin oh ||| joon beom seo ||| sang min lee ||| jin hwan kim ||| sungjun moon ||| jae-kwang lim ||| jong chul ye ||| 
2019 ||| multi-layer depth and epipolar feature transformers for 3d scene reconstruction. ||| daeyun shin ||| zhile ren ||| erik b. sudderth ||| charless c. fowlkes ||| 
2019 ||| solving arithmetic word problems automatically using transformer and unambiguous representations. ||| kaden griffith ||| jugal kalita ||| 
2021 ||| automatic segmentation of gross target volume of nasopharynx cancer using ensemble of multiscale deep neural networks with spatial attention. ||| haochen mei ||| wenhui lei ||| ran gu ||| shan ye ||| zhengwentai sun ||| shichuan zhang ||| guotai wang ||| 
2021 ||| attention based broadly self-guided network for low light image enhancement. ||| zilong chen ||| yaling liang ||| minghui du ||| 
2019 ||| nasnet: a neuron attention stage-by-stage net for single image deraining. ||| 
2021 ||| weakly supervised instance attention for multisource fine-grained object recognition with an application to tree species classification. ||| bulut ayg ||| nes ||| ramazan g ||| kberk cinbis ||| selim aksoy ||| 
2020 ||| cass-nat: ctc alignment-based single step non-autoregressive transformer for speech recognition. ||| ruchao fan ||| wei chu ||| peng chang ||| jing xiao ||| 
2021 ||| behavioral research and practical models of drivers' attention. ||| iuliia kotseruba ||| john k. tsotsos ||| 
2019 ||| deeply supervised multimodal attentional translation embeddings for visual relationship detection. ||| nikolaos gkanatsios ||| vassilis pitsikalis ||| petros koutras ||| athanasia zlatintsi ||| petros maragos ||| 
2020 ||| leveraging affective bidirectional transformers for offensive language detection. ||| abdelrahim a. elmadany ||| chiyu zhang ||| muhammad abdul-mageed ||| azadeh hashemi ||| 
2021 ||| extracting temporal event relation with syntactic-guided temporal graph transformer. ||| shuaicheng zhang ||| lifu huang ||| qiang ning ||| 
2022 ||| proformer: learning data-efficient representations of body movement with prototype-based feature augmentation and visual transformers. ||| kunyu peng ||| alina roitberg ||| kailun yang ||| jiaming zhang ||| rainer stiefelhagen ||| 
2021 ||| cova: context-aware visual attention for webpage information extraction. ||| anurendra kumar ||| keval morabia ||| jingjin wang ||| kevin chen-chuan chang ||| alexander g. schwing ||| 
2021 ||| handsformer: keypoint transformer for monocular 3d pose estimation ofhands and object in interaction. ||| shreyas hampali ||| sayan deb sarkar ||| mahdi rad ||| vincent lepetit ||| 
2021 ||| multi-level motion attention for human motion prediction. ||| wei mao ||| miaomiao liu ||| mathieu salzmann ||| hongdong li ||| 
2022 ||| cascade transformers for end-to-end person search. ||| rui yu ||| dawei du ||| rodney lalonde ||| daniel davila ||| christopher funk ||| anthony hoogs ||| brian clipp ||| 
2020 ||| dynamically adjusting transformer batch size by monitoring gradient direction change. ||| hongfei xu ||| josef van genabith ||| deyi xiong ||| qiuhui liu ||| 
2018 ||| a deep learning approach with an attention mechanism for automatic sleep stage classification. ||| martin l ||| ngkvist ||| amy loutfi ||| 
2019 ||| video captioning with text-based dynamic attention and step-by-step learning. ||| huanhou xiao ||| jinglun shi ||| 
2021 ||| consistent depth prediction under various illuminations using dilated cross attention. ||| zitian zhang ||| chuhua xian ||| 
2022 ||| transformers in time series: a survey. ||| qingsong wen ||| tian zhou ||| chaoli zhang ||| weiqi chen ||| ziqing ma ||| junchi yan ||| liang sun ||| 
2020 ||| multi-decoder attention model with embedding glimpse for solving vehicle routing problems. ||| liang xin ||| wen song ||| zhiguang cao ||| jie zhang ||| 
2021 ||| learning regional attention over multi-resolution deep convolutional features for trademark retrieval. ||| osman tursun ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2021 ||| sensor-augmented egocentric-video captioning with dynamic modal attention. ||| katsuyuki nakamura ||| hiroki ohashi ||| mitsuhiro okada ||| 
2021 ||| autoformer: searching transformers for visual recognition. ||| minghao chen ||| houwen peng ||| jianlong fu ||| haibin ling ||| 
2021 ||| long-short temporal contrastive learning of video transformers. ||| jue wang ||| gedas bertasius ||| du tran ||| lorenzo torresani ||| 
2021 ||| ganav: group-wise attention network for classifying navigable regions in unstructured outdoor environments. ||| tianrui guan ||| divya kothandaraman ||| rohan chandra ||| dinesh manocha ||| 
2020 ||| hardware accelerator for multi-head attention and position-wise feed-forward in the transformer. ||| siyuan lu ||| meiqi wang ||| shuang liang ||| jun lin ||| zhongfeng wang ||| 
2018 ||| visual attention is beyond one single saliency map. ||| jian li ||| 
2022 ||| hyper attention recurrent neural network: tackling temporal covariate shift in time series analysis. ||| wenying duan ||| xiaoxi he ||| lu zhou ||| zimu zhou ||| lothar thiele ||| hong rao ||| 
2019 ||| point attention network for semantic segmentation of 3d point clouds. ||| mingtao feng ||| liang zhang ||| xuefei lin ||| syed zulqarnain gilani ||| ajmal mian ||| 
2020 ||| text information aggregation with centrality attention. ||| jingjing gong ||| hang yan ||| yining zheng ||| xipeng qiu ||| xuanjing huang ||| 
2020 ||| graph attention networks for speaker verification. ||| jee-weon jung ||| hee-soo heo ||| ha-jin yu ||| joon son chung ||| 
2021 ||| object propagation via inter-frame attentions for temporally stable video instance segmentation. ||| anirudh srinivasan chakravarthy ||| won-dong jang ||| zudi lin ||| donglai wei ||| song bai ||| hanspeter pfister ||| 
2021 ||| from known to unknown: knowledge-guided transformer for time-series sales forecasting in alibaba. ||| xinyuan qi ||| kai hou ||| tong liu ||| zhongzhong yu ||| sihao hu ||| wenwu ou ||| 
2021 ||| attention back-end for automatic speaker verification with multiple enrollment utterances. ||| chang zeng ||| xin wang ||| erica cooper ||| junichi yamagishi ||| 
2021 ||| end-to-end referring video object segmentation with multimodal transformers. ||| adam botach ||| evgenii zheltonozhskii ||| chaim baskin ||| 
2021 ||| generative flows with invertible attentions. ||| rhea sanjay sukthanker ||| zhiwu huang ||| suryansh kumar ||| radu timofte ||| luc van gool ||| 
2017 ||| faster than real-time facial alignment: a 3d spatial transformer network approach in unconstrained poses. ||| chandrasekhar bhagavatula ||| chenchen zhu ||| khoa luu ||| marios savvides ||| 
2019 ||| multiresolution graph attention networks for relevance matching. ||| ting zhang ||| bang liu ||| di niu ||| kunfeng lai ||| yu xu ||| 
2021 ||| dense nested attention network for infrared small target detection. ||| boyang li ||| chao xiao ||| longguang wang ||| yingqian wang ||| zaiping lin ||| miao li ||| wei an ||| yulan guo ||| 
2020 ||| pay attention when required. ||| swetha mandava ||| szymon migacz ||| alex fit florea ||| 
2020 ||| speaker-aware speech-transformer. ||| zhiyun fan ||| jie li ||| shiyu zhou ||| bo xu ||| 
2021 ||| analyzing covid-19 tweets with transformer-based language models. ||| philip feldman ||| sim tiwari ||| charissa s. l. cheah ||| james r. foulds ||| shimei pan ||| 
2019 ||| on the validity of self-attention as explanation in transformer models. ||| gino brunner ||| yang liu ||| dami ||| n pascual ||| oliver richter ||| roger wattenhofer ||| 
2021 ||| multimodal breast lesion classification using cross-attention deep networks. ||| hung q. vo ||| pengyu yuan ||| tiancheng he ||| stephen t. c. wong ||| hien van nguyen ||| 
2020 ||| pam: point-wise attention module for 6d object pose estimation. ||| myoungha song ||| jeongho lee ||| dong hwan kim ||| 
2018 ||| interaction-aware spatio-temporal pyramid attention networks for action classification. ||| yang du ||| chunfeng yuan ||| bing li ||| lili zhao ||| yangxi li ||| weiming hu ||| 
2017 ||| weighted transformer network for machine translation. ||| karim ahmed ||| nitish shirish keskar ||| richard socher ||| 
2021 ||| abc: attention with bounded-memory control. ||| hao peng ||| jungo kasai ||| nikolaos pappas ||| dani yogatama ||| zhaofeng wu ||| lingpeng kong ||| roy schwartz ||| noah a. smith ||| 
2021 ||| self-supervised learning with local attention-aware feature. ||| trung x. pham ||| rusty john lloyd mina ||| dias issa ||| chang d. yoo ||| 
2018 ||| noise-tolerant audio-visual online person verification using an attention-based neural network fusion. ||| suwon shon ||| tae-hyun oh ||| james r. glass ||| 
2018 ||| context-aware cascade attention-based rnn for video emotion recognition. ||| man-chin sun ||| shih-huan hsu ||| min-chun yang ||| jen-hsien chien ||| 
2020 ||| sparse and structured visual attention. ||| pedro henrique martins ||| vlad niculae ||| zita marinho ||| andr |||  f. t. martins ||| 
2019 ||| two-stream video classification with cross-modality attention. ||| lu chi ||| guiyu tian ||| yadong mu ||| qi tian ||| 
2020 ||| readonce transformers: reusable representations of text for transformers. ||| shih-ting lin ||| ashish sabharwal ||| tushar khot ||| 
2020 ||| attention-based fully gated cnn-bgru for russian handwritten text. ||| abdelrahman abdallah ||| mohamed hamada ||| daniyar b. nurseitov ||| 
2021 ||| transattunet: multi-level attention-guided u-net with transformer for medical image segmentation. ||| bingzhi chen ||| yishu liu ||| zheng zhang ||| guangming lu ||| david zhang ||| 
2019 ||| look, investigate, and classify: a deep hybrid attention method for breast cancer classification. ||| bolei xu ||| jingxin liu ||| xianxu hou ||| bozhi liu ||| jon garibaldi ||| ian o. ellis ||| andy green ||| linlin shen ||| guoping qiu ||| 
2020 ||| bi-directional attention for joint instance and semantic segmentation in point clouds. ||| guangnan wu ||| zhiyi pan ||| peng jiang ||| changhe tu ||| 
2021 ||| synchronized audio-visual frames with fractional positional encoding for transformers in video-to-text translation. ||| philipp harzig ||| moritz einfalt ||| rainer lienhart ||| 
2020 ||| transformers for one-shot visual imitation. ||| sudeep dasari ||| abhinav gupta ||| 
2021 ||| speech enhancement using separable polling attention and global layer normalization followed with prelu. ||| dengfeng ke ||| jinsong zhang ||| yanlu xie ||| yanyan xu ||| binghuai lin ||| 
2021 ||| mitigating the position bias of transformer models in passage re-ranking. ||| sebastian hofst ||| tter ||| aldo lipani ||| sophia althammer ||| markus zlabinger ||| allan hanbury ||| 
2022 ||| ethics, rules of engagement, and ai: neural narrative mapping using large transformer language models. ||| philip feldman ||| aaron dant ||| david rosenbluth ||| 
2021 ||| poolingformer: long document modeling with pooling attention. ||| hang zhang ||| yeyun gong ||| yelong shen ||| weisheng li ||| jiancheng lv ||| nan duan ||| weizhu chen ||| 
2019 ||| embedding human knowledge in deep neural network via attention map. ||| masahiro mitsuhara ||| hiroshi fukui ||| yusuke sakashita ||| takanori ogata ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| 
2021 ||| transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. ||| xianlong zeng ||| simon m. lin ||| chang liu ||| 
2020 ||| attention and encoder-decoder based models for transforming articulatory movements at different speaking rates. ||| abhayjeet singh ||| aravind illa ||| prasanta kumar ghosh ||| 
2020 ||| hatnet: an end-to-end holistic attention network for diagnosis of breast biopsy images. ||| sachin mehta ||| ximing lu ||| donald l. weaver ||| joann g. elmore ||| hannaneh hajishirzi ||| linda g. shapiro ||| 
2019 ||| deep progressive multi-scale attention for acoustic event classification. ||| xugang lu ||| peng shen ||| sheng li ||| yu tsao ||| hisashi kawai ||| 
2020 ||| stan: spatio-temporal attention network for pandemic prediction using real world evidence. ||| junyi gao ||| rakshith sharma srinivasa ||| cheng qian ||| lucas m. glass ||| jeffrey spaeder ||| justin romberg ||| jimeng sun ||| cao xiao ||| 
2017 ||| machine learning applications in estimating transformer loss of life. ||| alireza majzoobi ||| mohsen mahoor ||| amin khodaei ||| 
2019 ||| attention based image compression post-processing convolutional neural network. ||| yuyang xue ||| jiannan su ||| 
2017 ||| deep graph attention model. ||| john boaz lee ||| ryan a. rossi ||| xiangnan kong ||| 
2021 ||| vortx: volumetric 3d reconstruction with transformers for voxelwise view selection and fusion. ||| noah stier ||| alexander rich ||| pradeep sen ||| tobias h ||| llerer ||| 
2021 ||| an lstm-based plagiarism detection via attention mechanism and a population-based approach for pre-training parameters with imbalanced classes. ||| seyed vahid moravvej ||| seyed jalaleddin mousavirad ||| mahshid helali moghadam ||| mehrdad saadatmand ||| 
2022 ||| think global, act local: dual-scale graph transformer for vision-and-language navigation. ||| shizhe chen ||| pierre-louis guhur ||| makarand tapaswi ||| cordelia schmid ||| ivan laptev ||| 
2019 ||| medical image super-resolution method based on dense blended attention network. ||| kewen liu ||| yuan ma ||| hongxia xiong ||| zejun yan ||| zhijun zhou ||| panpan fang ||| chaoyang liu ||| 
2021 ||| scatterbrain: unifying sparse and low-rank attention approximation. ||| beidi chen ||| tri dao ||| eric winsor ||| zhao song ||| atri rudra ||| christopher r ||| 
2020 ||| attn-hybridnet: improving discriminability of hybrid features with attention fusion. ||| sunny verma ||| chen wang ||| liming zhu ||| wei liu ||| 
2021 ||| vt-adl: a vision transformer network for image anomaly detection and localization. ||| pankaj mishra ||| riccardo verk ||| daniele fornasier ||| claudio piciarelli ||| gian luca foresti ||| 
2021 ||| eeg-convtransformer for single-trial eeg based visual stimuli classification. ||| subhranil bagchi ||| deepti r. bathula ||| 
2021 ||| long-span dependencies in transformer-based summarization systems. ||| potsawee manakul ||| mark j. f. gales ||| 
2019 ||| label-aware document representation via hybrid attention for extreme multi-label text classification. ||| xin huang ||| boli chen ||| lin xiao ||| liping jing ||| 
2020 ||| classification by attention: scene graph classification with prior knowledge. ||| sahand sharifzadeh ||| sina moayed baharlou ||| volker tresp ||| 
2019 ||| instance-level microtubule segmentation using recurrent attention. ||| samira masoudi ||| afsaneh razi ||| cameron h. g. wright ||| jesse c. gatlin ||| ulas bagci ||| 
2018 ||| gaan: gated attention networks for learning on large and spatiotemporal graphs. ||| jiani zhang ||| xingjian shi ||| junyuan xie ||| hao ma ||| irwin king ||| dit-yan yeung ||| 
2020 ||| modifying memories in transformer models. ||| chen zhu ||| ankit singh rawat ||| manzil zaheer ||| srinadh bhojanapalli ||| daliang li ||| felix x. yu ||| sanjiv kumar ||| 
2021 ||| an attention module for convolutional neural networks. ||| baozhou zhu ||| h. peter hofstee ||| jinho lee ||| zaid al-ars ||| 
2020 ||| wave propagation of visual stimuli in focus of attention. ||| lapo faggi ||| alessandro betti ||| dario zanca ||| stefano melacci ||| marco gori ||| 
2021 ||| cross-modal attention for mri and ultrasound volume registration. ||| xinrui song ||| hengtao guo ||| xuanang xu ||| hanqing chao ||| sheng xu ||| baris turkbey ||| bradford j. wood ||| ge wang ||| pingkun yan ||| 
2019 ||| aspect and opinion terms extraction using double embeddings and attention mechanism for indonesian hotel reviews. ||| jordhy fernando ||| masayu leylia khodra ||| ali akbar septiandri ||| 
2020 ||| generating plausible counterfactual explanations for deep transformers in financial text classification. ||| linyi yang ||| eoin m. kenny ||| tin lok james ng ||| yi yang ||| barry smyth ||| ruihai dong ||| 
2022 ||| from discrimination to generation: knowledge graph completion with generative transformer. ||| xin xie ||| ningyu zhang ||| zhoubo li ||| shumin deng ||| hui chen ||| feiyu xiong ||| mosha chen ||| huajun chen ||| 
2019 ||| mockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders. ||| andy t. liu ||| shu-wen yang ||| po-han chi ||| po-chun hsu ||| hung-yi lee ||| 
2021 ||| peco: perceptual codebook for bert pre-training of vision transformers. ||| xiaoyi dong ||| jianmin bao ||| ting zhang ||| dongdong chen ||| weiming zhang ||| lu yuan ||| dong chen ||| fang wen ||| nenghai yu ||| 
2019 ||| scene graph parsing by attention graph. ||| martin andrews ||| yew ken chia ||| sam witteveen ||| 
2018 ||| context-aware attention for understanding twitter abuse. ||| tuhin chakrabarty ||| kilol gupta ||| 
2020 ||| cross-attention in coupled unmixing nets for unsupervised hyperspectral super-resolution. ||| jing yao ||| danfeng hong ||| jocelyn chanussot ||| deyu meng ||| xiaoxiang zhu ||| zongben xu ||| 
2018 ||| focus on what's important: self-attention model for human pose estimation. ||| 
2020 ||| unsupervised domain attention adaptation network for caricature attribute recognition. ||| wen ji ||| kelei he ||| jing huo ||| zheng gu ||| yang gao ||| 
2020 ||| hybrid generative-retrieval transformers for dialogue domain adaptation. ||| igor shalyminov ||| alessandro sordoni ||| adam atkinson ||| hannes schulz ||| 
2020 ||| the devil is in the details: self-supervised attention for vehicle re-identification. ||| pirazh khorramshahi ||| neehar peri ||| jun-cheng chen ||| rama chellappa ||| 
2020 ||| transition-based parsing with stack-transformers. ||| ram ||| n fernandez astudillo ||| miguel ballesteros ||| tahira naseem ||| austin blodgett ||| radu florian ||| 
2020 ||| end-to-end learning for video frame compression with self-attention. ||| nannan zou ||| honglei zhang ||| francesco cricri ||| hamed r. tavakoli ||| jani lainema ||| emre aksu ||| miska m. hannuksela ||| esa rahtu ||| 
2020 ||| iqiyi submission to activitynet challenge 2019 kinetics-700 challenge: hierarchical group-wise attention. ||| qian liu ||| dongyang cai ||| jie liu ||| nan ding ||| tao wang ||| 
2022 ||| human attention detection using am-fm representations. ||| wenjing shi ||| 
2019 ||| a multiscale visualization of attention in the transformer model. ||| jesse vig ||| 
2019 ||| bayesian optimized continual learning with attention mechanism. ||| ju xu ||| jin ma ||| zhanxing zhu ||| 
2021 ||| the spatial selective auditory attention of cochlear implant users in different conversational sound levels. ||| sara akbarzadeh ||| sungmin lee ||| chin-tuan tan ||| 
2021 ||| transformer-based language model fine-tuning methods for covid-19 fake news detection. ||| ben chen ||| bin chen ||| dehong gao ||| qijin chen ||| chengfu huo ||| xiaonan meng ||| weijun ren ||| yang zhou ||| 
2021 ||| mlpruning: a multilevel structured pruning framework for transformer-based models. ||| zhewei yao ||| linjian ma ||| sheng shen ||| kurt keutzer ||| michael w. mahoney ||| 
2021 ||| xcit: cross-covariance image transformers. ||| alaaeldin el-nouby ||| hugo touvron ||| mathilde caron ||| piotr bojanowski ||| matthijs douze ||| armand joulin ||| ivan laptev ||| natalia neverova ||| gabriel synnaeve ||| jakob verbeek ||| herv |||  j ||| gou ||| 
2018 ||| a novel neural sequence model with multiple attentions for word sense disambiguation. ||| mahtab ahmed ||| muhammad rifayat samee ||| robert e. mercer ||| 
2021 ||| knowledge neurons in pretrained transformers. ||| damai dai ||| li dong ||| yaru hao ||| zhifang sui ||| furu wei ||| 
2020 ||| inducing taxonomic knowledge from pretrained transformers. ||| catherine chen ||| kevin lin ||| dan klein ||| 
2021 ||| human interpretation and exploitation of self-attention patterns in transformers: a case study in extractive summarization. ||| raymond li ||| wen xiao ||| lanjun wang ||| giuseppe carenini ||| 
2019 ||| mask-guided attention network for occluded pedestrian detection. ||| yanwei pang ||| jin xie ||| muhammad haris khan ||| rao muhammad anwer ||| fahad shahbaz khan ||| ling shao ||| 
2021 ||| evaluating transformers for lightweight action recognition. ||| raivo e. koot ||| markus hennerbichler ||| haiping lu ||| 
2021 ||| domain adaptation with pre-trained transformers for query focused abstractive text summarization. ||| md. tahmid rahman laskar ||| enamul hoque ||| jimmy xiangji huang ||| 
2020 ||| rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. ||| sixiao zheng ||| jiachen lu ||| hengshuang zhao ||| xiatian zhu ||| zekun luo ||| yabiao wang ||| yanwei fu ||| jianfeng feng ||| tao xiang ||| philip h. s. torr ||| li zhang ||| 
2020 ||| transfer learning and distant supervision for multilingual transformer models: a study on african languages. ||| michael a. hedderich ||| david ifeoluwa adelani ||| dawei zhu ||| jesujoba o. alabi ||| udia markus ||| dietrich klakow ||| 
2018 ||| pyramid attention network for semantic segmentation. ||| hanchao li ||| pengfei xiong ||| jie an ||| lingxue wang ||| 
2018 ||| dynamic fusion with intra- and inter- modality attention flow for visual question answering. ||| peng gao ||| hongsheng li ||| haoxuan you ||| zhengkai jiang ||| pan lu ||| steven c. h. hoi ||| xiaogang wang ||| 
2022 ||| dynamic scene video deblurring using non-local attention. ||| maitreya suin ||| a. n. rajagopalan ||| 
2018 ||| a deep learning approach for multi-view engagement estimation of children in a child-robot joint attention task. ||| jack hadfield ||| georgia chalvatzaki ||| petros koutras ||| mehdi khamassi ||| costas s. tzafestas ||| petros maragos ||| 
2017 ||| multi-context attention for human pose estimation. ||| xiao chu ||| wei yang ||| wanli ouyang ||| cheng ma ||| alan l. yuille ||| xiaogang wang ||| 
2017 ||| enriched deep recurrent visual attention model for multiple object recognition. ||| artsiom ablavatski ||| shijian lu ||| jianfei cai ||| 
2019 ||| unsupervised universal self-attention network for graph classification. ||| dai quoc nguyen ||| tu dinh nguyen ||| dinh phung ||| 
2019 ||| auditory separation of a conversation from background via attentional gating. ||| shariq mobin ||| bruno a. olshausen ||| 
2021 ||| ufo: a unified transformer for vision-language representation learning. ||| jianfeng wang ||| xiaowei hu ||| zhe gan ||| zhengyuan yang ||| xiyang dai ||| zicheng liu ||| yumao lu ||| lijuan wang ||| 
2022 ||| class-aware generative adversarial transformers for medical image segmentation. ||| chenyu you ||| ruihan zhao ||| fenglin liu ||| sandeep chinchali ||| ufuk topcu ||| lawrence h. staib ||| james s. duncan ||| 
2022 ||| aligntransformer: hierarchical alignment of visual regions and disease tags for medical report generation. ||| di you ||| fenglin liu ||| shen ge ||| xiaoxia xie ||| jing zhang ||| xian wu ||| 
2022 ||| high-performance transformer tracking. ||| xin chen ||| bin yan ||| jiawen zhu ||| dong wang ||| huchuan lu ||| 
2018 ||| a comparison of lstms and attention mechanisms for forecasting financial time series. ||| thomas hollis ||| antoine viscardi ||| seung eun yi ||| 
2020 ||| resgcn: attention-based deep residual modeling for anomaly detection on attributed networks. ||| yulong pei ||| tianjin huang ||| werner van ipenburg ||| mykola pechenizkiy ||| 
2021 ||| attention module improves both performance and interpretability of 4d fmri decoding neural network. ||| zhoufan jiang ||| yanming wang ||| chenwei shi ||| yueyang wu ||| rongjie hu ||| shishuo chen ||| sheng hu ||| xiaoxiao wang ||| bensheng qiu ||| 
2018 ||| reproduction report on "learn to pay attention". ||| levan shugliashvili ||| davit soselia ||| shota amashukeli ||| irakli koberidze ||| 
2021 ||| between post-flaneur and smartphone zombie smartphone users altering visual attention and walking behavior in public space. ||| gorsev argin ||| burak pak ||| handan turkoglu ||| 
2018 ||| knowledge-enriched two-layered attention network for sentiment analysis. ||| abhishek kumar ||| daisuke kawahara ||| sadao kurohashi ||| 
2021 ||| bcfnet: a balanced collaborative filtering network with attention mechanism. ||| zi-yuan hu ||| jin huang ||| zhi-hong deng ||| chang-dong wang ||| ling huang ||| jian-huang lai ||| philip s. yu ||| 
2021 ||| pedestrian trajectory prediction via spatial interaction transformer network. ||| tong su ||| yu meng ||| yan xu ||| 
2018 ||| chinese herbal recognition based on competitive attentional fusion of multi-hierarchies pyramid features. ||| yingxue xu ||| guihua wen ||| yang hu ||| mingnan luo ||| dan dai ||| yishan zhuang ||| 
2018 ||| teaching machines to code: neural markup generation with visual attention. ||| sumeet s. singh ||| 
2020 ||| sketch-bert: learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. ||| hangyu lin ||| yanwei fu ||| yu-gang jiang ||| xiangyang xue ||| 
2020 ||| dense attention fluid network for salient object detection in optical remote sensing images. ||| qijian zhang ||| runmin cong ||| chongyi li ||| ming-ming cheng ||| yuming fang ||| xiaochun cao ||| yao zhao ||| sam kwong ||| 
2021 ||| physics-informed attention-based neural network for solving non-linear partial differential equations. ||| ruben rodriguez torrado ||| pablo ruiz ||| luis cueto-felgueroso ||| michael cerny green ||| tyler friesen ||| sebastien matringe ||| julian togelius ||| 
2020 ||| transformer based grapheme-to-phoneme conversion. ||| sevinj yolchuyeva ||| g ||| za n ||| meth ||| b ||| lint gyires-t ||| th ||| 
2021 ||| dbia: data-free backdoor injection attack against transformer networks. ||| peizhuo lv ||| hualong ma ||| jiachen zhou ||| ruigang liang ||| kai chen ||| shengzhi zhang ||| yunfei yang ||| 
2019 ||| fine-grained information status classification using discourse context-aware self-attention. ||| yufang hou ||| 
2017 ||| highrisk prediction from electronic medical records via deep attention networks. ||| you jin kim ||| yun-geun lee ||| jeong-whun kim ||| jin joo park ||| borim ryu ||| jung-woo ha ||| 
2022 ||| attention-based cross-layer domain alignment for unsupervised domain adaptation. ||| xu ma ||| junkun yuan ||| yen-wei chen ||| ruofeng tong ||| lanfen lin ||| 
2022 ||| hat5: hate language identification using text-to-text transfer transformer. ||| sana sabah sabry ||| tosin p. adewumi ||| nosheen abid ||| gy ||| rgy kov ||| cs ||| foteini liwicki ||| marcus liwicki ||| 
2022 ||| mhatc: autism spectrum disorder identification utilizing multi-head attention encoder along with temporal consolidation modules. ||| ranjeet ranjan jha ||| abhishek bhardwaj ||| devin garg ||| arnav bhavsar ||| aditya nigam ||| 
2021 ||| istr: end-to-end instance segmentation with transformers. ||| jie hu ||| liujuan cao ||| yao lu ||| shengchuan zhang ||| yan wang ||| ke li ||| feiyue huang ||| ling shao ||| rongrong ji ||| 
2019 ||| attention-based dropout layer for weakly supervised object localization. ||| junsuk choe ||| hyunjung shim ||| 
2021 ||| iterative decoding for compositional generalization in transformers. ||| luana ruiz ||| joshua ainslie ||| santiago onta ||| n ||| 
2021 ||| transformers for headline selection for russian news clusters. ||| pavel voropaev ||| olga sopilnyak ||| 
2020 ||| dtca: decision tree-based co-attention networks for explainable claim verification. ||| lianwei wu ||| yuan rao ||| yongqiang zhao ||| hao liang ||| ambreen nazir ||| 
2021 ||| image captioning using multiple transformers for self-attention mechanism. ||| farrukh olimov ||| shikha dubey ||| labina shrestha ||| tran trung tin ||| moongu jeon ||| 
2019 ||| incremental transformer with deliberation decoder for document grounded conversations. ||| zekang li ||| cheng niu ||| fandong meng ||| yang feng ||| qian li ||| jie zhou ||| 
2021 ||| q-attention: enabling efficient learning for vision-based robotic manipulation. ||| stephen james ||| andrew j. davison ||| 
2021 ||| ripple attention for visual perception with sub-quadratic complexity. ||| lin zheng ||| huijie pan ||| lingpeng kong ||| 
2020 ||| modeling dynamic heterogeneous network for link prediction using hierarchical attention with temporal rnn. ||| hansheng xue ||| luwei yang ||| wen jiang ||| yi wei ||| yi hu ||| yu lin ||| 
2020 ||| deep differentiable forest with sparse attention for the tabular data. ||| yingshi chen ||| 
2021 ||| pulmonary disease classification using globally correlated maximum likelihood: an auxiliary attention mechanism for convolutional neural networks. ||| edward verenich ||| tobias martin ||| alvaro velasquez ||| nazar khan ||| faraz hussain ||| 
2021 ||| mhformer: multi-hypothesis transformer for 3d human pose estimation. ||| wenhao li ||| hong liu ||| hao tang ||| pichao wang ||| luc van gool ||| 
2018 ||| ccnet: criss-cross attention for semantic segmentation. ||| zilong huang ||| xinggang wang ||| yunchao wei ||| lichao huang ||| humphrey shi ||| wenyu liu ||| thomas s. huang ||| 
2019 ||| do transformer attention heads provide transparency in abstractive summarization? ||| joris baan ||| maartje ter hoeve ||| marlies van der wees ||| anne schuth ||| maarten de rijke ||| 
2021 ||| geometry-aware transformer for molecular property prediction. ||| bumju kwak ||| jeonghee jo ||| byunghan lee ||| sungroh yoon ||| 
2019 ||| hyperbolic graph attention network. ||| yiding zhang ||| xiao wang ||| xunqiang jiang ||| chuan shi ||| yanfang ye ||| 
2019 ||| improving deep transformer with depth-scaled initialization and merged attention. ||| biao zhang ||| ivan titov ||| rico sennrich ||| 
2020 ||| image captioning through image transformer. ||| sen he ||| wentong liao ||| hamed r. tavakoli ||| michael ying yang ||| bodo rosenhahn ||| nicolas pugeault ||| 
2021 ||| uot-uwf-partai at semeval-2021 task 5: self attention based bi-gru with multi-embedding representation for toxicity highlighter. ||| hamed babaei giglou ||| taher rahgooy ||| mostafa rahgouy ||| jafar razmara ||| 
2021 ||| covid-19 pneumonia severity prediction using hybrid convolution-attention neural architectures. ||| nam nguyen ||| j. morris chang ||| 
2020 ||| jointly trained transformers models for spoken language translation. ||| hari krishna vydana ||| martin karafi ||| t ||| katerina zmol ||| kov ||| luk ||| s burget ||| honza cernock ||| 
2017 ||| multiplex media attention and disregard network among 129 countries. ||| haewoon kwak ||| jisun an ||| 
2019 ||| line drawings of natural scenes guide visual attention. ||| kai-fu yang ||| wen-wen jiang ||| teng-fei zhan ||| yong-jie li ||| 
2017 ||| the effect of collective attention on controversial debates on social media. ||| kiran garimella ||| gianmarco de francisci morales ||| aristides gionis ||| michael mathioudakis ||| 
2020 ||| dialogue relation extraction with document-level heterogeneous graph attention networks. ||| hui chen ||| pengfei hong ||| wei han ||| navonil majumder ||| soujanya poria ||| 
2019 ||| multi-scale self-attention for text classification. ||| qipeng guo ||| xipeng qiu ||| pengfei liu ||| xiangyang xue ||| zheng zhang ||| 
2021 ||| inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks. ||| yichen jiang ||| mohit bansal ||| 
2021 ||| bridging between cognitive processing signals and linguistic features via a unified attentional network. ||| yuqi ren ||| deyi xiong ||| 
2020 ||| bayesian attention modules. ||| xinjie fan ||| shujian zhang ||| bo chen ||| mingyuan zhou ||| 
2018 ||| scan: sliding convolutional attention network for scene text recognition. ||| yi-chao wu ||| fei yin ||| xu-yao zhang ||| li liu ||| cheng-lin liu ||| 
2021 ||| bidirectional lstm-crf attention-based model for chinese word segmentation. ||| chen jin ||| zhuangwei shi ||| weihua li ||| yanbu guo ||| 
2021 ||| decoupled transformer for scalable inference in open-domain question answering. ||| haytham elfadeel ||| stan peshterliev ||| 
2020 ||| shallow feature based dense attention network for crowd counting. ||| yunqi miao ||| zijia lin ||| guiguang ding ||| jungong han ||| 
2020 ||| dual multi-head co-attention for multi-choice reading comprehension. ||| pengfei zhu ||| hai zhao ||| xiaoguang li ||| 
2021 ||| trankit: a light-weight transformer-based toolkit for multilingual natural language processing. ||| minh van nguyen ||| viet dac lai ||| amir pouran ben veyseh ||| thien huu nguyen ||| 
2020 ||| a transformer-based joint-encoding for emotion recognition and sentiment analysis. ||| jean-benoit delbrouck ||| no |||  tits ||| mathilde brousmiche ||| st ||| phane dupont ||| 
2019 ||| local block-wise self attention for normal organ segmentation. ||| jue jiang ||| elguindi sharif ||| hyemin um ||| sean berry ||| harini veeraraghavan ||| 
2019 ||| canet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading. ||| xiaomeng li ||| xiaowei hu ||| lequan yu ||| lei zhu ||| chi-wing fu ||| pheng-ann heng ||| 
2020 ||| symbiotic attention with privileged information for egocentric action recognition. ||| xiaohan wang ||| yu wu ||| linchao zhu ||| yi yang ||| 
2021 ||| octree transformer: autoregressive 3d shape generation on hierarchically structured sequences. ||| moritz ibing ||| gregor kobsik ||| leif kobbelt ||| 
2018 ||| attention based fully convolutional network for speech emotion recognition. ||| yuanyuan zhang ||| jun du ||| zi-rui wang ||| jianshu zhang ||| 
2019 ||| end-to-end neural speaker diarization with self-attention. ||| yusuke fujita ||| naoyuki kanda ||| shota horiguchi ||| yawen xue ||| kenji nagamatsu ||| shinji watanabe ||| 
2020 ||| inducing alignment structure with gated graph attention networks for sentence matching. ||| peng cui ||| le hu ||| yuanchao liu ||| 
2022 ||| pyramidtnt: improved transformer-in-transformer baselines with pyramid architecture. ||| kai han ||| jianyuan guo ||| yehui tang ||| yunhe wang ||| 
2017 ||| semantic segmentation with reverse attention. ||| qin huang ||| chunyang xia ||| chi-hao wu ||| siyang li ||| ye wang ||| yuhang song ||| c.-c. jay kuo ||| 
2021 ||| transformer-based asr incorporating time-reduction layer and fine-tuning with self-knowledge distillation. ||| md. akmal haidar ||| chao xing ||| mehdi rezagholizadeh ||| 
2020 ||| tracking the twitter attention around the research efforts on the covid-19 pandemic. ||| zhichao fang ||| rodrigo costas ||| 
2020 ||| smaat-unet: precipitation nowcasting using a small attention-unet architecture. ||| kevin trebing ||| siamak mehrkanoon ||| 
2018 ||| memory attention networks for skeleton-based action recognition. ||| chunyu xie ||| ce li ||| baochang zhang ||| chen chen ||| jungong han ||| changqing zou ||| jianzhuang liu ||| 
2021 ||| extreme precipitation seasonal forecast using a transformer neural network. ||| daniel salles civitarese ||| daniela szwarcman ||| bianca zadrozny ||| campbell d. watson ||| 
2021 ||| neural hmms are all you need (for high-quality attention-free tts). ||| shivam mehta ||| va sz ||| kely ||| jonas beskow ||| gustav eje henter ||| 
2021 ||| feature importance-aware graph attention network and dueling double deep q-network combined approach for critical node detection problems. ||| xuwei tan ||| yangming zhou ||| zhang-hua fu ||| mengchu zhou ||| 
2021 ||| label-synchronous speech-to-text alignment for asr using forward and backward transformers. ||| yusuke kida ||| tatsuya komatsu ||| masahito togami ||| 
2021 ||| dpn-senet: a self-attention mechanism neural network for detection and diagnosis of covid-19 from chest x-ray images. ||| bo cheng ||| ruhui xue ||| hang yang ||| laili zhu ||| wei xiang ||| 
2022 ||| edgeformer: improving light-weight convnets by learning from vision transformers. ||| haokui zhang ||| wenze hu ||| xiaoyu wang ||| 
2018 ||| attention-based ensemble for deep metric learning. ||| wonsik kim ||| bhavya goyal ||| kunal chawla ||| jungmin lee ||| keunjoo kwon ||| 
2021 ||| c5t5: controllable generation of organic molecules with transformers. ||| daniel rothchild ||| alex tamkin ||| julie yu ||| ujval misra ||| joseph gonzalez ||| 
2018 ||| triple attention mixed link network for single image super resolution. ||| xi cheng ||| xiang li ||| jian yang ||| 
2019 ||| learning to caption images with two-stream attention and sentence auto-encoder. ||| arushi goel ||| basura fernando ||| thanh-son nguyen ||| hakan bilen ||| 
2019 ||| attention-based curiosity-driven exploration in deep reinforcement learning. ||| patrik reizinger ||| m ||| rton szemenyei ||| 
2021 ||| enhanced 3d human pose estimation from videos by using attention-based neural network with dilated convolutions. ||| ruixu liu ||| ju shen ||| he wang ||| chen chen ||| sen-ching s. cheung ||| vijayan k. asari ||| 
2018 ||| bottom-up attention, models of. ||| ali borji ||| hamed r. tavakoli ||| zoya bylinskii ||| 
2021 ||| gca-net : utilizing gated context attention for improving image forgery localization and detection. ||| sowmen das ||| md. saiful islam ||| md. ruhul amin ||| 
2021 ||| attention-based neural network for driving environment complexity perception. ||| ce zhang ||| azim eskandarian ||| xuelai du ||| 
2020 ||| transformer-based end-to-end question generation. ||| luis enrico lopez ||| diane kathryn cruz ||| jan christian blaise cruz ||| charibeth cheng ||| 
2021 ||| pyramidal dense attention networks for lightweight image super-resolution. ||| huapeng wu ||| jie gui ||| jun zhang ||| james t. kwok ||| zhihui wei ||| 
2020 ||| multiple attentional pyramid networks for chinese herbal recognition. ||| yingxue xu ||| guihua wen ||| yang hu ||| mingnan luo ||| dan dai ||| yishan zhuang ||| wendy hall ||| 
2021 ||| proto: program-guided transformer for program-guided tasks. ||| zelin zhao ||| karan samel ||| binghong chen ||| le song ||| 
2021 ||| shoulder implant x-ray manufacturer classification: exploring with vision transformer. ||| meng zhou ||| shanglin mo ||| 
2019 ||| non-local attention optimized deep image compression. ||| haojie liu ||| tong chen ||| peiyao guo ||| qiu shen ||| xun cao ||| yao wang ||| zhan ma ||| 
2019 ||| take an emotion walk: perceiving emotions from gaits using hierarchical attention pooling and affective mapping. ||| uttaran bhattacharya ||| christian roncal ||| trisha mittal ||| rohan chandra ||| aniket bera ||| dinesh manocha ||| 
2021 ||| hard attention control by mutual information maximization. ||| himanshu sahni ||| charles isbell ||| 
2018 ||| reverse attention for salient object detection. ||| shuhan chen ||| xiuli tan ||| ben wang ||| xuelong hu ||| 
2020 ||| program enhanced fact verification with verbalization and graph attention network. ||| xiaoyu yang ||| feng nie ||| yufei feng ||| quan liu ||| zhigang chen ||| xiaodan zhu ||| 
2019 ||| ultrafast video attention prediction with coupled knowledge distillation. ||| kui fu ||| peipei shi ||| yafei song ||| shiming ge ||| xiangju lu ||| jia li ||| 
2018 ||| co-stack residual affinity networks with multi-level attention refinement for matching text sequences. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| 
2021 ||| efficient self-supervised vision transformers for representation learning. ||| chunyuan li ||| jianwei yang ||| pengchuan zhang ||| mei gao ||| bin xiao ||| xiyang dai ||| lu yuan ||| jianfeng gao ||| 
2019 ||| attention on abstract visual reasoning. ||| lukas hahne ||| timo l ||| ddecke ||| florentin w ||| rg ||| tter ||| david kappel ||| 
2021 ||| presize: predicting size in e-commerce using transformers. ||| yotam eshel ||| or levi ||| haggai roitman ||| alexander nus ||| 
2021 ||| dcap: deep cross attentional product network for user response prediction. ||| zekai chen ||| fangtian zhong ||| zhumin chen ||| xiao zhang ||| robert pless ||| xiuzhen cheng ||| 
2021 ||| tlsan: time-aware long- and short-term attention network for next-item recommendation. ||| jianqing zhang ||| dongjing wang ||| dongjin yu ||| 
2022 ||| pyramid fusion transformer for semantic segmentation. ||| zipeng qin ||| jianbo liu ||| xiaolin zhang ||| maoqing tian ||| aojun zhou ||| shuai yi ||| hongsheng li ||| 
2020 ||| general multi-label image classification with transformers. ||| jack lanchantin ||| tianlu wang ||| vicente ordonez ||| yanjun qi ||| 
2021 ||| r-gat: relational graph attention network for multi-relational graphs. ||| meiqi chen ||| yuan zhang ||| xiaoyu kou ||| yuntao li ||| yan zhang ||| 
2021 ||| baller2vec++: a look-ahead multi-entity transformer for modeling coordinated agents. ||| michael a. alcorn ||| anh nguyen ||| 
2017 ||| leveraging sensory data in estimating transformer lifetime. ||| mohsen mahoor ||| alireza majzoobi ||| zohreh s. hosseini ||| amin khodaei ||| 
2021 ||| u-net with hierarchical bottleneck attention for landmark detection in fundus images of the degenerated retina. ||| shuyun tang ||| ziming qi ||| jacob granley ||| michael beyeler ||| 
2020 ||| many-to-many voice transformer network. ||| hirokazu kameoka ||| wen-chin huang ||| kou tanaka ||| takuhiro kaneko ||| nobukatsu hojo ||| tomoki toda ||| 
2021 ||| vision transformers for dense prediction. ||| ren |||  ranftl ||| alexey bochkovskiy ||| vladlen koltun ||| 
2020 ||| speaker-utterance dual attention for speaker and utterance verification. ||| tianchi liu ||| rohan kumar das ||| maulik c. madhavi ||| shengmei shen ||| haizhou li ||| 
2019 ||| exact hard monotonic attention for character-level transduction. ||| shijie wu ||| ryan cotterell ||| 
2019 ||| leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog. ||| shachi h. kumar ||| eda okur ||| saurav sahay ||| jonathan huang ||| lama nachman ||| 
2021 ||| tanet++: triple attention network with filtered pointcloud on 3d detection. ||| cong ma ||| 
2022 ||| dall-eval: probing the reasoning skills and social biases of text-to-image generative transformers. ||| jaemin cho ||| abhay zala ||| mohit bansal ||| 
2018 ||| efficient large-scale domain classification with personalized attention. ||| young-bum kim ||| dongchan kim ||| anjishnu kumar ||| ruhi sarikaya ||| 
2021 ||| which transformer architecture fits my data? a vocabulary bottleneck in self-attention. ||| noam wies ||| yoav levine ||| daniel jannai ||| amnon shashua ||| 
2021 ||| combating informational denial-of-service (idos) attacks: modeling and mitigation of attentional human vulnerability. ||| linan huang ||| quanyan zhu ||| 
2020 ||| graph-bert: only attention is needed for learning graph representations. ||| jiawei zhang ||| haopeng zhang ||| congying xia ||| li sun ||| 
2020 ||| attention flow: end-to-end joint attention estimation. ||| mer s ||| mer ||| peter gerjets ||| ulrich trautwein ||| enkelejda kasneci ||| 
2021 ||| oadtr: online action detection with transformers. ||| xiang wang ||| shiwei zhang ||| zhiwu qing ||| yuanjie shao ||| zhengrong zuo ||| changxin gao ||| nong sang ||| 
2018 ||| doubly attentive transformer machine translation. ||| hasan sait arslan ||| mark fishel ||| gholamreza anbarjafari ||| 
2021 ||| attacc the quadratic bottleneck of attention layers. ||| sheng-chun kao ||| suvinay subramanian ||| gaurav agrawal ||| tushar krishna ||| 
2020 ||| 3d facial geometry recovery from a depth view with attention guided generative adversarial network. ||| xiaoxu cai ||| hui yu ||| jianwen lou ||| xuguang zhang ||| gongfa li ||| junyu dong ||| 
2021 ||| context-aware attention-based data augmentation for poi recommendation. ||| yang li ||| yadan luo ||| zheng zhang ||| shazia w. sadiq ||| peng cui ||| 
2021 ||| ba-net: bridge attention for deep convolutional neural networks. ||| yue zhao ||| junzhou chen ||| zirui zhang ||| ronghui zhang ||| 
2021 ||| learning multi-scene absolute pose regression with transformers. ||| yoli shavit ||| ron ferens ||| yosi keller ||| 
2021 ||| kernel identification through transformers. ||| fergus simpson ||| ian davies ||| vidhi lalchand ||| alessandro vullo ||| nicolas durrande ||| carl e. rasmussen ||| 
2021 ||| fourier image transformer. ||| tim-oliver buchholz ||| florian jug ||| 
2020 ||| economic evaluation of transformer loss of life mitigation by energy storage and pv generation. ||| milad soleimani ||| mladen kezunovic ||| 
2019 ||| improving robustness in speaker identification using a two-stage attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2021 ||| attention vs non-attention for a shapley-based explanation method. ||| tom kersten ||| hugh mee wong ||| jaap jumelet ||| dieuwke hupkes ||| 
2019 ||| learning reinforced attentional representation for end-to-end visual tracking. ||| peng gao ||| qiquan zhang ||| liyi xiao ||| yan zhang ||| fei wang ||| 
2021 ||| improving online forums summarization via unifying hierarchical attention networks with convolutional neural networks. ||| sansiri tarnpradab ||| fereshteh jafariakinabad ||| kien a. hua ||| 
2021 ||| learnable multi-level frequency decomposition and hierarchical attention mechanism for generalized face presentation attack detection. ||| meiling fang ||| naser damer ||| florian kirchbuchner ||| arjan kuijper ||| 
2021 ||| atrm: attention-based task-level relation module for gnn-based few-shot learning. ||| yurong guo ||| zhanyu ma ||| xiaoxu li ||| yuan dong ||| 
2020 ||| rkt : relation-aware self-attention for knowledge tracing. ||| shalini pandey ||| jaideep srivastava ||| 
2018 ||| explainable social contextual image recommendation with hierarchical attention. ||| le wu ||| yong ge ||| qi liu ||| enhong chen ||| richang hong ||| meng wang ||| junping du ||| 
2020 ||| hierarchical gpt with congruent transformers for multi-sentence language models. ||| jihyeon roh ||| huiseong gim ||| soo-young lee ||| 
2021 ||| modetr: moving object detection with transformers. ||| eslam mohamed ||| ahmad el sallab ||| 
2019 ||| attention-augmented end-to-end multi-task learning for emotion prediction from speech. ||| zixing zhang ||| bingwen wu ||| bj ||| rn w. schuller ||| 
2019 ||| paying attention to function words. ||| shane steinert-threlkeld ||| 
2019 ||| but-fit at semeval-2019 task 7: determining the rumour stance with pre-trained deep bidirectional transformers. ||| martin fajcik ||| luk ||| s burget ||| pavel smrz ||| 
2019 ||| semantic feature attention network for liver tumor segmentation in large-scale ct database. ||| yao zhang ||| cheng zhong ||| yang zhang ||| zhongchao shi ||| zhiqiang he ||| 
2019 ||| exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed. ||| atta norouzian ||| bogdan mazoure ||| dermot connolly ||| daniel willett ||| 
2020 ||| soe-net: a self-attention and orientation encoding network for point cloud based place recognition. ||| yan xia ||| yusheng xu ||| shuang li ||| rui wang ||| juan du ||| daniel cremers ||| uwe stilla ||| 
2021 ||| high-accuracy rgb-d face recognition via segmentation-aware face depth estimation and mask-guided attention network. ||| meng-tzu chiu ||| hsun-ying cheng ||| chien-yi wang ||| shang-hong lai ||| 
2021 ||| how bpe affects memorization in transformers. ||| eugene kharitonov ||| marco baroni ||| dieuwke hupkes ||| 
2021 ||| bert based transformers lead the way in extraction of health information from social media. ||| sidharth r ||| abhiraj tiwari ||| parthivi choubey ||| saisha kashyap ||| sahil khose ||| kumud lakara ||| nishesh singh ||| ujjwal verma ||| 
2019 ||| cccnet: an attention based deep learning framework for categorized crowd counting. ||| sarkar snigdha sarathi das ||| syed md. mukit rashid ||| mohammed eunus ali ||| 
2021 ||| improving next-application prediction with deep personalized-attention neural network. ||| jun zhu ||| gautier viaud ||| c ||| line hudelot ||| 
2021 ||| tag: transformer attack from gradient. ||| jieren deng ||| yijue wang ||| ji li ||| chao shang ||| hang liu ||| sanguthevar rajasekaran ||| caiwen ding ||| 
2022 ||| preformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting. ||| dazhao du ||| bing su ||| zhewei wei ||| 
2021 ||| post-training quantization for vision transformer. ||| zhenhua liu ||| yunhe wang ||| kai han ||| siwei ma ||| wen gao ||| 
2017 ||| modeling attention in panoramic video: a deep reinforcement learning approach. ||| yuhang song ||| mai xu ||| minglang qiao ||| jianyi wang ||| liangyu huo ||| zulin wang ||| 
2021 ||| greedy layer pruning: decreasing inference time of transformer models. ||| david peer ||| sebastian stabinger ||| stefan engl ||| antonio jose rodr ||| guez-s ||| nchez ||| 
2019 ||| alcnn: attention-based model for fine-grained demand inference of dock-less shared bike in new cities. ||| chang liu ||| yanan xu ||| yanmin zhu ||| 
2021 ||| leveraging multilingual transformers for hate speech detection. ||| sayar ghosh roy ||| ujwal narayan ||| tathagata raha ||| zubair abid ||| vasudeva varma ||| 
2022 ||| learning stochastic dynamics and predicting emergent behavior using transformers. ||| corneel casert ||| isaac tamblyn ||| stephen whitelam ||| 
2021 ||| shape registration in the time of transformers. ||| giovanni trappolini ||| luca cosmo ||| luca moschella ||| riccardo marin ||| simone melzi ||| emanuele rodol ||| 
2021 ||| noisy text data: achilles' heel of popular transformer based nlp models. ||| kartikay bagla ||| ankit kumar ||| shivam gupta ||| anuj gupta ||| 
2020 |||  vu: a contextualized temporal attention mechanism for sequential recommendation. ||| jibang wu ||| renqin cai ||| hongning wang ||| 
2021 ||| transflower: probabilistic autoregressive dance generation with multimodal attention. ||| guillermo valle p ||| rez ||| gustav eje henter ||| jonas beskow ||| andre holzapfel ||| pierre-yves oudeyer ||| simon alexanderson ||| 
2021 ||| a^2-fpn: attention aggregation based feature pyramid network for instance segmentation. ||| miao hu ||| yali li ||| lu fang ||| shengjin wang ||| 
2017 ||| graph attention networks. ||| petar velickovic ||| guillem cucurull ||| arantxa casanova ||| adriana romero ||| pietro li ||| yoshua bengio ||| 
2020 ||| proformer: towards on-device lsh projection based transformers. ||| chinnadhurai sankar ||| sujith ravi ||| zornitsa kozareva ||| 
2019 ||| pay attention: leveraging sequence models to predict the useful life of batteries. ||| samuel paradis ||| michael whitmeyer ||| 
2019 ||| visualizing and understanding self-attention based music tagging. ||| minz won ||| sanghyuk chun ||| xavier serra ||| 
2020 ||| multi-agent trajectory prediction with fuzzy query attention. ||| nitin kamra ||| hao zhu ||| dweep trivedi ||| ming zhang ||| yan liu ||| 
2020 ||| dense cnn with self-attention for time-domain speech enhancement. ||| ashutosh pandey ||| deliang wang ||| 
2018 ||| position-aware self-attention with relative positional encodings for slot filling. ||| ivan bilan ||| benjamin roth ||| 
2019 ||| improving transformer models by reordering their sublayers. ||| ofir press ||| noah a. smith ||| omer levy ||| 
2020 ||| social explorative attention based recommendation for content distribution platforms. ||| wenyi xiao ||| huan zhao ||| haojie pan ||| yangqiu song ||| vincent w. zheng ||| qiang yang ||| 
2019 ||| deep angular embedding and feature correlation attention for breast mri cancer analysis. ||| luyang luo ||| hao chen ||| xi wang ||| qi dou ||| huangjing lin ||| juan zhou ||| gongjie li ||| pheng-ann heng ||| 
2020 ||| a transformer-based framework for multivariate time series representation learning. ||| george zerveas ||| srideepika jayaraman ||| dhaval patel ||| anuradha bhamidipaty ||| carsten eickhoff ||| 
2019 ||| afs: an attention-based mechanism for supervised feature selection. ||| ning gui ||| danni ge ||| ziyin hu ||| 
2021 ||| towards natural language question answering over earth observation linked data using attention-based neural machine translation. ||| abhishek v. potnis ||| rajat c. shinde ||| surya s. durbha ||| 
2021 ||| attention based occlusion removal for hybrid telepresence systems. ||| surabhi gupta ||| ashwath shetty ||| avinash sharma ||| 
2021 ||| transcrowd: weakly-supervised crowd counting with transformer. ||| dingkang liang ||| xiwu chen ||| wei xu ||| yu zhou ||| xiang bai ||| 
2021 ||| rain: reinforced hybrid attention inference network for motion forecasting. ||| jiachen li ||| fan yang ||| hengbo ma ||| srikanth malla ||| masayoshi tomizuka ||| chiho choi ||| 
2020 ||| attention word embedding. ||| shashank sonkar ||| andrew e. waters ||| richard g. baraniuk ||| 
2021 ||| aga-gan: attribute guided attention generative adversarial network with u-net for face hallucination. ||| abhishek srivastava ||| sukalpa chanda ||| umapada pal ||| 
2019 ||| esa: entity summarization with attention. ||| dongjun wei ||| yaxin liu ||| 
2021 ||| topic-driven and knowledge-aware transformer for dialogue emotion detection. ||| lixing zhu ||| gabriele pergola ||| lin gui ||| deyu zhou ||| yulan he ||| 
2020 ||| multi-head attention based probabilistic vehicle trajectory prediction. ||| hayoung kim ||| dongchan kim ||| gihoon kim ||| jeongmin cho ||| kunsoo huh ||| 
2020 ||| pre-training polish transformer-based language models at scale. ||| slawomir dadas ||| michal perelkiewicz ||| rafal poswiata ||| 
2021 ||| memx: an attention-aware smart eyewear system for personalized moment auto-capture. ||| yuhu chang ||| yingying zhao ||| mingzhi dong ||| yujiang wang ||| yutian lu ||| qin lv ||| robert p. dick ||| tun lu ||| ning gu ||| li shang ||| 
2020 ||| tensorized transformer for dynamical systems modeling. ||| anna shalova ||| ivan v. oseledets ||| 
2021 ||| domain composition and attention for unseen-domain generalizable medical image segmentation. ||| ran gu ||| jingyang zhang ||| rui huang ||| wenhui lei ||| guotai wang ||| shaoting zhang ||| 
2020 ||| identifying similar movie characters quickly but effectively using non-exhaustive pair-wise attention. ||| zhilin wang ||| weizhe lin ||| xiaodong wu ||| 
2021 ||| two heads are better than one: geometric-latent attention for point cloud classification and segmentation. ||| hanz cuevas-velasquez ||| antonio-javier gallego ||| robert b. fisher ||| 
2021 ||| perceptual image quality assessment with transformers. ||| manri cheon ||| sung-jun yoon ||| byungyeon kang ||| junwoo lee ||| 
2020 ||| cross-regional oil palm tree counting and detection via multi-level attention domain adaptation network. ||| juepeng zheng ||| haohuan fu ||| weijia li ||| wenzhao wu ||| yi zhao ||| runmin dong ||| le yu ||| 
2020 ||| pointiso: point cloud based deep learning model for detecting arbitrary-precision peptide features in lc-ms map through attention based segmentation. ||| fatema tuz zohora ||| m. ziaur rahman ||| ngoc hieu tran ||| lei xin ||| baozhen shan ||| ming li ||| 
2021 ||| roi tanh-polar transformer network for face parsing in the wild. ||| yiming lin ||| jie shen ||| yujiang wang ||| maja pantic ||| 
2020 ||| covid ct-net: predicting covid-19 from chest ct images using attentional convolutional network. ||| shakib yazdani ||| shervin minaee ||| rahele kafieh ||| narges saeedizadeh ||| milan sonka ||| 
2021 ||| attention head masking for inference time content selection in abstractive summarization. ||| shuyang cao ||| lu wang ||| 
2021 ||| linear transformers are secretly fast weight memory systems. ||| imanol schlag ||| kazuki irie ||| j ||| rgen schmidhuber ||| 
2021 ||| improving pneumonia localization via cross-attention on medical images and reports. ||| riddhish bhalodia ||| ali hatamizadeh ||| leo k. tam ||| ziyue xu ||| xiaosong wang ||| evrim turkbey ||| daguang xu ||| 
2019 ||| attention-based deep tropical cyclone rapid intensification prediction. ||| ching-yuan bai ||| buo-fu chen ||| hsuan-tien lin ||| 
2022 ||| group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition. ||| pengfei liu ||| kun li ||| helen meng ||| 
2021 ||| crossvit: cross-attention multi-scale vision transformer for image classification. ||| chun-fu chen ||| quanfu fan ||| rameswar panda ||| 
2021 ||| ctin: robust contextual transformer network for inertial navigation. ||| bingbing rao ||| ehsan kazemi ||| yifan ding ||| devu m. shila ||| frank m. tucker ||| liqiang wang ||| 
2020 ||| scene text recognition via transformer. ||| xinjie feng ||| hongxun yao ||| yuankai qi ||| jun zhang ||| shengping zhang ||| 
2020 ||| measuring systematic generalization in neural proof generation with transformers. ||| nicolas gontier ||| koustuv sinha ||| siva reddy ||| christopher pal ||| 
2020 ||| attentionnas: spatiotemporal attention cell search for video classification. ||| xiaofang wang ||| xuehan xiong ||| maxim neumann ||| a. j. piergiovanni ||| michael s. ryoo ||| anelia angelova ||| kris m. kitani ||| wei hua ||| 
2020 ||| learning deep interleaved networks with asymmetric co-attention for image restoration. ||| feng li ||| runmin cong ||| huihui bai ||| yifan he ||| yao zhao ||| ce zhu ||| 
2021 ||| banet: blur-aware attention networks for dynamic scene deblurring. ||| fu-jen tsai ||| yan-tsung peng ||| yen-yu lin ||| chung-chi tsai ||| chia-wen lin ||| 
2021 ||| survey: transformer based video-language pre-training. ||| ludan ruan ||| qin jin ||| 
2021 ||| train short, test long: attention with linear biases enables input length extrapolation. ||| ofir press ||| noah a. smith ||| mike lewis ||| 
2020 ||| probabilistic spatial transformers for bayesian data augmentation. ||| pola schw ||| bel ||| frederik warburg ||| martin j ||| rgensen ||| kristoffer h. madsen ||| s ||| ren hauberg ||| 
2020 ||| stroke constrained attention network for online handwritten mathematical expression recognition. ||| jia-ming wang ||| jun du ||| jianshu zhang ||| 
2020 ||| semantic segmentation with multi scale spatial attention for self driving cars. ||| abhinav sagar ||| rajkumar soundrapandiyan ||| 
2021 ||| causal attention for vision-language tasks. ||| xu yang ||| hanwang zhang ||| guojun qi ||| jianfei cai ||| 
2019 ||| iterative and adaptive sampling with spatial attention for black-box model explanations. ||| bhavan vasu ||| chengjiang long ||| 
2017 ||| hierarchical rnn with static sentence-level attention for text-based speaker change detection. ||| zhao meng ||| lili mou ||| zhi jin ||| 
2020 ||| self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. ||| adrian johnston ||| gustavo carneiro ||| 
2021 ||| medical transformer: gated axial-attention for medical image segmentation. ||| jeya maria jose valanarasu ||| poojan oza ||| ilker hacihaliloglu ||| vishal m. patel ||| 
2022 ||| where is my mind (looking at)? predicting visual attention from brain activity. ||| victor delvigne ||| no |||  tits ||| luca la fisca ||| nathan hubens ||| antoine maiorca ||| hazem wannous ||| thierry dutoit ||| jean-philippe vandeborre ||| 
2019 ||| spatially and temporally efficient non-local attention network for video-based person re-identification. ||| chih-ting liu ||| chih-wei wu ||| yu-chiang frank wang ||| shao-yi chien ||| 
2022 ||| explore and match: end-to-end video grounding with transformer. ||| sangmin woo ||| jinyoung park ||| inyong koo ||| sumin lee ||| minki jeong ||| changick kim ||| 
2021 ||| adnet: attention-guided deformable convolutional network for high dynamic range imaging. ||| zhen liu ||| wenjie lin ||| xinpeng li ||| qing rao ||| ting jiang ||| mingyan han ||| haoqiang fan ||| jian sun ||| shuaicheng liu ||| 
2020 ||| trat: tracking by attention using spatio-temporal features. ||| hasan saribas ||| hakan cevikalp ||| okan k ||| p ||| kl ||| bedirhan uzun ||| 
2022 ||| adversarial robustness of neural-statistical features in detection of generative transformers. ||| evan crothers ||| nathalie japkowicz ||| herna l. viktor ||| paula branco ||| 
2019 ||| efficient graph generation with graph recurrent attention networks. ||| renjie liao ||| yujia li ||| yang song ||| shenlong wang ||| charlie nash ||| william l. hamilton ||| david duvenaud ||| raquel urtasun ||| richard s. zemel ||| 
2019 ||| verifying asynchronous event-driven programs using partial abstract transformers (extended manuscript). ||| peizun liu ||| thomas wahl ||| akash lal ||| 
2021 ||| towards joint intent detection and slot filling via higher-order attention. ||| dongsheng chen ||| zhiqi huang ||| xian wu ||| shen ge ||| yuexian zou ||| 
2020 ||| co-gat: a co-interactive graph attention network for joint dialog act recognition and sentiment classification. ||| libo qin ||| zhouyang li ||| wanxiang che ||| minheng ni ||| ting liu ||| 
2021 ||| on efficient transformer and image pre-training for low-level vision. ||| wenbo li ||| xin lu ||| jiangbo lu ||| xiangyu zhang ||| jiaya jia ||| 
2021 ||| towards more efficient insertion transformer with fractional positional encoding. ||| zhisong zhang ||| yizhe zhang ||| bill dolan ||| 
2022 ||| guided visual attention model based on interactions between top-down and bottom-up information for robot pose prediction. ||| hyogo hiruma ||| hiroki mori ||| tetsuya ogata ||| 
2021 ||| multi-attention generative adversarial network for remote sensing image super-resolution. ||| meng xu ||| zhihao wang ||| jiasong zhu ||| xiuping jia ||| sen jia ||| 
2021 ||| dynamic attention guided multi-trajectory analysis for single object tracking. ||| xiao wang ||| zhe chen ||| jin tang ||| bin luo ||| yaowei wang ||| yonghong tian ||| feng wu ||| 
2021 ||| multiview detection with shadow transformer (and view-coherent data augmentation). ||| yunzhong hou ||| liang zheng ||| 
2019 ||| feature-attention graph convolutional networks for noise resilient learning. ||| min shi ||| yufei tang ||| xingquan zhu ||| jianxun liu ||| 
2020 ||| channel attention based iterative residual learning for depth map super-resolution. ||| xibin song ||| yuchao dai ||| dingfu zhou ||| liu liu ||| wei li ||| hongdong li ||| ruigang yang ||| 
2019 ||| attention-based transfer learning for brain-computer interface. ||| chuanqi tan ||| fuchun sun ||| tao kong ||| bin fang ||| wenchang zhang ||| 
2018 ||| semi-supervised confidence network aided gated attention based recurrent neural network for clickbait detection. ||| amrith rajagopal setlur ||| 
2020 ||| deep reinforced attention learning for quality-aware visual recognition. ||| duo li ||| qifeng chen ||| 
2021 ||| deep neural networks on eeg signals to predict auditory attention score using gramian angular difference field. ||| mahak kothari ||| shreyansh joshi ||| adarsh nandanwar ||| aadetya jaiswal ||| veeky baths ||| 
2022 ||| transformer-based streaming asr with cumulative attention. ||| mohan li ||| shucong zhang ||| catalin zorila ||| rama doddipatla ||| 
2021 ||| the animation transformer: visual correspondence via segment matching. ||| evan casey ||| v ||| ctor p ||| rez ||| zhuoru li ||| harry teitelman ||| nick boyajian ||| tim pulver ||| mike manh ||| william grisaitis ||| 
2018 ||| skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module. ||| chenyang li ||| xin zhang ||| lufan liao ||| lianwen jin ||| weixin yang ||| 
2021 ||| ast: audio spectrogram transformer. ||| yuan gong ||| yu-an chung ||| james r. glass ||| 
2022 ||| litetransformersearch: training-free on-device search for efficient autoregressive language models. ||| mojan javaheripi ||| shital shah ||| subhabrata mukherjee ||| tomasz l. religa ||| caio c. t. mendes ||| gustavo h. de rosa ||| s ||| bastien bubeck ||| farinaz koushanfar ||| debadeepta dey ||| 
2020 ||| informer: beyond efficient transformer for long sequence time-series forecasting. ||| haoyi zhou ||| shanghang zhang ||| jieqi peng ||| shuai zhang ||| jianxin li ||| hui xiong ||| wancai zhang ||| 
2021 ||| depth as attention for face representation learning. ||| hardik uppal ||| alireza sepas-moghaddam ||| michael a. greenspan ||| s. ali etemad ||| 
2020 ||| on the transformer growth for progressive bert training. ||| xiaotao gu ||| liyuan liu ||| hongkun yu ||| jing li ||| chen chen ||| jiawei han ||| 
2019 ||| acoustic scene analysis with multi-head attention networks. ||| weimin wang ||| weiran wang ||| ming sun ||| chao wang ||| 
2020 ||| efficient urdu caption generation using attention based lstms. ||| inaam ilahi ||| hafiz muhammad abdullah zia ||| ahtazaz ehsan ||| rauf tabassam ||| armaghan ahmed ||| 
2020 ||| attentional networks for music generation. ||| gullapalli keerti ||| a. n. vaishnavi ||| prerana mukherjee ||| a. sree vidya ||| gattineni sai sreenithya ||| deeksha nayab ||| 
2019 ||| on extractive and abstractive neural document summarization with transformer language models. ||| sandeep subramanian ||| raymond li ||| jonathan pilault ||| christopher j. pal ||| 
2021 ||| show, attend and distill: knowledge distillation via attention-based feature matching. ||| mingi ji ||| byeongho heo ||| sungrae park ||| 
2020 ||| dual-attention guided dropblock module for weakly supervised object localization. ||| junhui yin ||| siqing zhang ||| dongliang chang ||| zhanyu ma ||| jun guo ||| 
2021 ||| different kinds of cognitive plausibility: why are transformers better than rnns at predicting n400 amplitude? ||| james a. michaelov ||| megan d. bardolph ||| seana coulson ||| benjamin k. bergen ||| 
2022 ||| pali-nlp at semeval-2022 task 4: discriminative fine-tuning of deep transformers for patronizing and condescending language detection. ||| dou hu ||| mengyuan zhou ||| xiyang du ||| mengfei yuan ||| meizhi jin ||| lian-xin jiang ||| yang mo ||| xiaofeng shi ||| 
2020 ||| attentional local contrast networks for infrared small target detection. ||| yimian dai ||| yiquan wu ||| fei zhou ||| kobus barnard ||| 
2021 ||| cytran: cycle-consistent transformers for non-contrast to contrast ct translation. ||| nicolae-catalin ristea ||| andreea-iuliana miron ||| olivian savencu ||| mariana-iuliana georgescu ||| nicolae verga ||| fahad shahbaz khan ||| radu tudor ionescu ||| 
2021 ||| caegcn: cross-attention fusion based enhanced graph convolutional network for clustering. ||| guangyu huo ||| yong zhang ||| junbin gao ||| boyue wang ||| yongli hu ||| baocai yin ||| 
2019 ||| improving tree-lstm with tree attention. ||| mahtab ahmed ||| muhammad rifayat samee ||| robert e. mercer ||| 
2019 ||| pay less attention with lightweight and dynamic convolutions. ||| felix wu ||| angela fan ||| alexei baevski ||| yann n. dauphin ||| michael auli ||| 
2020 ||| lietransformer: equivariant self-attention for lie groups. ||| michael hutchinson ||| charline le lan ||| sheheryar zaidi ||| emilien dupont ||| yee whye teh ||| hyunjik kim ||| 
2020 ||| comprehensive attention self-distillation for weakly-supervised object detection. ||| zeyi huang ||| yang zou ||| vijayakumar bhagavatula ||| dong huang ||| 
2019 ||| listen and fill in the missing letters: non-autoregressive transformer for speech recognition. ||| nanxin chen ||| shinji watanabe ||| jes ||| s villalba ||| najim dehak ||| 
2021 ||| query-by-example keyword spotting system using multi-head attention and softtriple loss. ||| jinmiao huang ||| waseem gharbieh ||| han suk shim ||| eugene kim ||| 
2021 ||| pose-guided inter- and intra-part relational transformer for occluded person re-identification. ||| zhongxing ma ||| yifan zhao ||| jia li ||| 
2021 ||| scifive: a text-to-text transformer model for biomedical literature. ||| long n. phan ||| james t. anibal ||| hieu tran ||| shaurya chanana ||| erol bahadroglu ||| alec peltekian ||| gr ||| goire altan-bonnet ||| 
2020 ||| semi-supervised classification using attention-based regularization on coarse-resolution data. ||| guruprasad nayak ||| rahul ghosh ||| xiaowei jia ||| varun mithal ||| vipin kumar ||| 
2019 ||| improving multi-head attention with capsule networks. ||| shuhao gu ||| yang feng ||| 
2022 ||| fast monte-carlo approximation of the attention mechanism. ||| hyunjun kim ||| jeonggil ko ||| 
2021 ||| convolutions and self-attention: re-interpreting relative positions in pre-trained language models. ||| tyler a. chang ||| yifan xu ||| weijian xu ||| zhuowen tu ||| 
2022 ||| atek: augmenting transformers with expert knowledge for indoor layout synthesis. ||| kurt leimer ||| paul guerrero ||| tomer weiss ||| przemyslaw musialski ||| 
2018 ||| left-center-right separated neural network for aspect-based sentiment analysis with rotatory attention. ||| shiliang zheng ||| rui xia ||| 
2021 ||| aniformer: data-driven 3d animation with transformer. ||| haoyu chen ||| hao tang ||| nicu sebe ||| guoying zhao ||| 
2021 ||| star: sparse transformer-based action recognition. ||| feng shi ||| chonghan lee ||| liang qiu ||| yizhou zhao ||| tianyi shen ||| shivran muralidhar ||| tian han ||| song-chun zhu ||| vijaykrishnan narayanan ||| 
2019 ||| adaptively aligned image captioning via adaptive attention time. ||| lun huang ||| wenmin wang ||| yaxian xia ||| jie chen ||| 
2018 ||| where-and-when to look: deep siamese attention networks for video-based person re-identification. ||| lin wu ||| yang wang ||| junbin gao ||| xue li ||| 
2018 ||| salientdso: bringing attention to direct sparse odometry. ||| huai-jen liang ||| nitin j. sanket ||| cornelia ferm ||| ller ||| yiannis aloimonos ||| 
2019 ||| gla-net: an attention network with guided loss for mismatch removal. ||| zhi chen ||| fan yang ||| wenbing tao ||| 
2020 ||| molecule attention transformer. ||| lukasz maziarka ||| tomasz danel ||| slawomir mucha ||| krzysztof rataj ||| jacek tabor ||| stanislaw jastrzebski ||| 
2020 ||| mtgat: multimodal temporal graph attention networks for unaligned human multimodal language sequences. ||| jianing yang ||| yongxin wang ||| ruitao yi ||| yuying zhu ||| azaan rehman ||| amir zadeh ||| soujanya poria ||| louis-philippe morency ||| 
2019 ||| saliency guided self-attention network for weakly-supervised semantic segmentation. ||| qi yao ||| xiaojin gong ||| 
2020 ||| fine-grained human evaluation of transformer and recurrent approaches to neural machine translation for english-to-chinese. ||| yuying ye ||| antonio toral ||| 
2019 ||| lxmert: learning cross-modality encoder representations from transformers. ||| hao tan ||| mohit bansal ||| 
2021 ||| filming multimodal sarcasm detection with attention. ||| sundesh gupta ||| aditya shah ||| miten shah ||| laribok syiemlieh ||| chandresh maurya ||| 
2021 ||| soft attention: does it actually help to learn social interactions in pedestrian trajectory prediction? ||| laurent boucaud ||| daniel aloise ||| nicolas saunier ||| 
2022 ||| saits: self-attention-based imputation for time series. ||| wenjie du ||| david c ||| t ||| yan liu ||| 
2021 ||| siamapn++: siamese attentional aggregation network for real-time uav tracking. ||| ziang cao ||| changhong fu ||| junjie ye ||| bowen li ||| yiming li ||| 
2019 ||| real-time attention based look-alike model for recommender system. ||| yudan liu ||| kaikai ge ||| xu zhang ||| leyu lin ||| 
2020 ||| asap-net: attention and structure aware point cloud sequence segmentation. ||| hanwen cao ||| yongyi lu ||| cewu lu ||| bo pang ||| gongshen liu ||| alan l. yuille ||| 
2019 ||| self-attentional models application in task-oriented dialogue generation systems. ||| mansour saffar mehrjardi ||| amine trabelsi ||| osmar r. za ||| ane ||| 
2017 ||| deep joint entity disambiguation with local neural attention. ||| octavian-eugen ganea ||| thomas hofmann ||| 
2018 ||| qanet: combining local convolution with global self-attention for reading comprehension. ||| adams wei yu ||| david dohan ||| minh-thang luong ||| rui zhao ||| kai chen ||| mohammad norouzi ||| quoc v. le ||| 
2019 ||| dual-attention focused module for weakly supervised object localization. ||| yukun zhou ||| zailiang chen ||| hailan shen ||| qing liu ||| rongchang zhao ||| yixiong liang ||| 
2021 ||| let: linguistic knowledge enhanced graph transformer for chinese short text matching. ||| boer lyu ||| lu chen ||| su zhu ||| kai yu ||| 
2019 ||| progressive pose attention transfer for person image generation. ||| zhen zhu ||| tengteng huang ||| baoguang shi ||| miao yu ||| bofei wang ||| xiang bai ||| 
2021 ||| nested-block self-attention for robust radiotherapy planning segmentation. ||| harini veeraraghavan ||| jue jiang ||| elguindi sharif ||| sean l. berry ||| ifeanyirochukwu onochie ||| aditya p. apte ||| laura cervino ||| joseph o. deasy ||| 
2021 ||| shunted self-attention via multi-scale token aggregation. ||| sucheng ren ||| daquan zhou ||| shengfeng he ||| jiashi feng ||| xinchao wang ||| 
2021 ||| more than encoder: introducing transformer decoder to upsample. ||| yijiang li ||| wentian cai ||| ying gao ||| xiping hu ||| 
2021 ||| drug and disease interpretation learning with biomedical entity representation transformer. ||| zulfat miftahutdinov ||| artur kadurin ||| roman kudrin ||| elena tutubalina ||| 
2019 ||| social-bigat: multimodal trajectory forecasting using bicycle-gan and graph attention networks. ||| vineet kosaraju ||| amir sadeghian ||| roberto mart ||| n-mart ||| n ||| ian d. reid ||| seyed hamid rezatofighi ||| silvio savarese ||| 
2021 ||| on the integration of self-attention and convolution. ||| xuran pan ||| chunjiang ge ||| rui lu ||| shiji song ||| guanfu chen ||| zeyi huang ||| gao huang ||| 
2019 ||| assessing knee oa severity with cnn attention-based end-to-end architectures. ||| marc g ||| rriz ||| joseph antony ||| kevin mcguinness ||| xavier gir ||| -i-nieto ||| noel e. o'connor ||| 
2018 ||| could interaction with social robots facilitate joint attention of children with autism spectrum disorder? ||| wei cao ||| wenxu song ||| xinge li ||| sixiao zheng ||| ge zhang ||| yanting wu ||| sailing he ||| huilin zhu ||| jiajia chen ||| 
2021 ||| an integrated attribute guided dense attention model for fine-grained generalized zero-shot learning. ||| tasfia shermin ||| shyh wei teng ||| ferdous sohel ||| m. manzur murshed ||| guojun lu ||| 
2020 ||| direct multi-hop attention based graph neural network. ||| guangtao wang ||| rex ying ||| jing huang ||| jure leskovec ||| 
2021 ||| nnformer: interleaved transformer for volumetric segmentation. ||| hong-yu zhou ||| jiansen guo ||| yinghao zhang ||| lequan yu ||| liansheng wang ||| yizhou yu ||| 
2019 ||| predicate transformer semantics for hybrid systems: verification components for isabelle/hol. ||| jonathan juli ||| n huerta y munive ||| georg struth ||| 
2022 ||| transformers in action: weakly supervised action segmentation. ||| john ridley ||| huseyin coskun ||| david joseph tan ||| nassir navab ||| federico tombari ||| 
2021 ||| dpnet: dual-path network for efficient object detectioj with lightweight self-attention. ||| huimin shi ||| quan zhou ||| yinghao ni ||| xiaofu wu ||| longin jan latecki ||| 
2021 ||| hot-vae: learning high-order label correlation for multi-label classification via attention-based variational autoencoders. ||| wenting zhao ||| shufeng kong ||| junwen bai ||| daniel fink ||| carla p. gomes ||| 
2021 ||| how facial features convey attention in stationary environments. ||| janelle domantay ||| 
2021 ||| urban change detection by fully convolutional siamese concatenate network with attention. ||| farnoosh heidary ||| mehran yazdi ||| maryam dehghani ||| peyman setoodeh ||| 
2021 ||| efficient dialogue state tracking by masked hierarchical transformer. ||| min mao ||| jiasheng liu ||| jingyao zhou ||| haipang wu ||| 
2021 ||| graph transformer networks: learning meta-path graphs to improve gnns. ||| seongjun yun ||| minbyul jeong ||| sungdong yoo ||| seunghun lee ||| sean s. yi ||| raehyun kim ||| jaewoo kang ||| hyunwoo j. kim ||| 
2021 ||| tfill: image completion via a transformer-based architecture. ||| chuanxia zheng ||| tat-jen cham ||| jianfei cai ||| 
2022 ||| local information assisted attention-free decoder for audio captioning. ||| feiyang xiao ||| jian guan ||| qiaoxi zhu ||| haiyan lan ||| wenwu wang ||| 
2018 ||| where's your focus: personalized attention. ||| sikun lin ||| pan hui ||| 
2019 ||| how transformer revitalizes character-based neural machine translation: an investigation on japanese-vietnamese translation systems. ||| thi-vinh ngo ||| thanh-le ha ||| phuong-thai nguyen ||| le-minh nguyen ||| 
2019 ||| gasl: guided attention for sparsity learning in deep neural networks. ||| amirsina torfi ||| rouzbeh a. shirvani ||| sobhan soleymani ||| nasser m. nasrabadi ||| 
2020 ||| ftrans: energy-efficient acceleration of transformers using fpga. ||| bingbing li ||| santosh pandey ||| haowen fang ||| yanjun lyv ||| ji li ||| jieyang chen ||| mimi xie ||| lipeng wan ||| hang liu ||| caiwen ding ||| 
2022 ||| convolutional neural network with convolutional block attention module for finger vein recognition. ||| zhongxia zhang ||| mingwen wang ||| 
2021 ||| txt: crossmodal end-to-end learning with transformers. ||| jan-martin o. steitz ||| jonas pfeiffer ||| iryna gurevych ||| stefan roth ||| 
2021 ||| ac-covidnet: attention guided contrastive cnn for recognition of covid-19 in chest x-ray images. ||| anirudh ambati ||| shiv ram dubey ||| 
2021 ||| machine vision detection to daily facial fatigue with a nonlocal 3d attention network. ||| zeyu chen ||| xinhang zhang ||| juan li ||| jingxuan ni ||| gang chen ||| shaohua wang ||| fangfang fan ||| changfeng charles wang ||| xiaotao li ||| 
2021 ||| co-grounding networks with semantic attention for referring expression comprehension in videos. ||| sijie song ||| xudong lin ||| jiaying liu ||| zongming guo ||| shih-fu chang ||| 
2021 ||| sequential diagnosis prediction with transformer and ontological representation. ||| xueping peng ||| guodong long ||| tao shen ||| sen wang ||| jing jiang ||| 
2021 ||| do transformers really perform bad for graph representation? ||| chengxuan ying ||| tianle cai ||| shengjie luo ||| shuxin zheng ||| guolin ke ||| di he ||| yanming shen ||| tie-yan liu ||| 
2021 ||| a novel time-frequency transformer and its application in fault diagnosis of rolling bearings. ||| yifei ding ||| minping jia ||| qiuhua miao ||| yudong cao ||| 
2018 ||| styling with attention to details. ||| ayushi dalmia ||| sachindra joshi ||| raghavendra singh ||| vikas raykar ||| 
2021 ||| improving transformer-based sequential recommenders through preference editing. ||| muyang ma ||| pengjie ren ||| zhumin chen ||| zhaochun ren ||| huasheng liang ||| jun ma ||| maarten de rijke ||| 
2018 ||| learn to pay attention. ||| saumya jetley ||| nicholas a. lord ||| namhoon lee ||| philip h. s. torr ||| 
2021 ||| alebk: feasibility study of attention level estimation via blink detection applied to e-learning. ||| roberto daza ||| daniel dealcala ||| aythami morales ||| rub ||| n tolosana ||| ruth cobos ||| julian fi ||| rrez ||| 
2021 ||| turning transformer attention weights into zero-shot sequence labelers. ||| kamil bujel ||| helen yannakoudakis ||| marek rei ||| 
2018 ||| attention-based group recommendation. ||| tran dang quang vinh ||| tuan-anh nguyen pham ||| gao cong ||| xiao-li li ||| 
2020 ||| deep transformer models for time series forecasting: the influenza prevalence case. ||| neo wu ||| bradley green ||| xue ben ||| shawn o'banion ||| 
2018 ||| sequence-level knowledge distillation for model compression of attention-based sequence-to-sequence speech recognition. ||| raden mu'az mun'im ||| nakamasa inoue ||| koichi shinoda ||| 
2020 ||| translating similar languages: role of mutual intelligibility in multilingual transformers. ||| ife adebara ||| el moatez billah nagoudi ||| muhammad abdul-mageed ||| 
2021 ||| combining transformer generators with convolutional discriminators. ||| ricard durall ||| stanislav frolov ||| andreas dengel ||| janis keuper ||| 
2018 ||| lcanet: end-to-end lipreading with cascaded attention-ctc. ||| kai xu ||| dawei li ||| nick cassimatis ||| xiaolong wang ||| 
2021 ||| attention-based multi-reference learning for image super-resolution. ||| marco pesavento ||| marco volino ||| adrian hilton ||| 
2021 ||| assessment of self-attention on learned features for sound event localization and detection. ||| parthasaarathy sudarsanam ||| archontis politis ||| konstantinos drossos ||| 
2022 ||| hypermixer: an mlp-based green ai alternative to transformers. ||| florian mai ||| arnaud pannatier ||| fabio fehr ||| haolin chen ||| fran ||| ois marelli ||| fran ||| ois fleuret ||| james henderson ||| 
2021 ||| compositional transformers for scene generation. ||| drew a. hudson ||| c. lawrence zitnick ||| 
2018 ||| sta: spatial-temporal attention for large-scale video-based person re-identification. ||| yang fu ||| xiaoyang wang ||| yunchao wei ||| thomas s. huang ||| 
2021 ||| graph decoupling attention markov networks for semi-supervised graph node classification. ||| jie chen ||| shouzhen chen ||| mingyuan bai ||| jian pu ||| junping zhang ||| junbin gao ||| 
2017 ||| dcn+: mixed objective and deep residual coattention for question answering. ||| caiming xiong ||| victor zhong ||| richard socher ||| 
2021 ||| transformer assisted convolutional network for cell instance segmentation. ||| deepanshu pandey ||| pradyumna gupta ||| sumit bhattacharya ||| aman sinha ||| rohit agarwal ||| 
2022 ||| adaptive fine-tuning of transformer-based language models for named entity recognition. ||| felix stollenwerk ||| 
2022 ||| eventformer: au event transformer for facial action unit event detection. ||| yingjie chen ||| jiarui zhang ||| diqi chen ||| tao wang ||| yizhou wang ||| yun liang ||| 
2019 ||| durian: duration informed attention network for multimodal synthesis. ||| chengzhu yu ||| heng lu ||| na hu ||| meng yu ||| chao weng ||| kun xu ||| peng liu ||| deyi tuo ||| shiyin kang ||| guangzhi lei ||| dan su ||| dong yu ||| 
2020 ||| gated mechanism for attention based multimodal sentiment analysis. ||| ayush kumar ||| jithendra vepa ||| 
2020 ||| layer-stacked attention for heterogeneous network embedding. ||| nhat tran ||| jean gao ||| 
2019 ||| financial series prediction using attention lstm. ||| sangyeon kim ||| myungjoo kang ||| 
2019 ||| gman: a graph multi-attention network for traffic prediction. ||| chuanpan zheng ||| xiaoliang fan ||| cheng wang ||| jianzhong qi ||| 
2020 ||| extending equational monadic reasoning with monad transformers. ||| reynald affeldt ||| david nowak ||| 
2021 ||| learning to cluster faces via transformer. ||| jinxing ye ||| xiaojiang peng ||| baigui sun ||| kai wang ||| xiuyu sun ||| hao li ||| hanqing wu ||| 
2019 ||| deep short text classification with knowledge powered attention. ||| jindong chen ||| yizhou hu ||| jingping liu ||| yanghua xiao ||| haiyun jiang ||| 
2018 ||| equity of attention: amortizing individual fairness in rankings. ||| asia j. biega ||| krishna p. gummadi ||| gerhard weikum ||| 
2022 ||| headposr: end-to-end trainable head pose estimation using transformer encoders. ||| naina dhingra ||| 
2021 ||| sdtp: semantic-aware decoupled transformer pyramid for dense image prediction. ||| zekun li ||| yufan liu ||| bing li ||| weiming hu ||| kebin wu ||| pei wang ||| 
2020 ||| variational transformers for diverse response generation. ||| zhaojiang lin ||| genta indra winata ||| peng xu ||| zihan liu ||| pascale fung ||| 
2022 ||| gaze-guided class activation mapping: leveraging human attention for network attention in chest x-rays classification. ||| hongzhi zhu ||| septimiu salcudean ||| robert rohling ||| 
2019 ||| regularized context gates on transformer for machine translation. ||| xintong li ||| lemao liu ||| rui wang ||| guoping huang ||| max meng ||| 
2020 ||| global attention based graph convolutional neural networks for improved materials property prediction. ||| steph-yves m. louis ||| yong zhao ||| alireza nasiri ||| xiran wong ||| yuqi song ||| fei liu ||| jianjun hu ||| 
2018 ||| an improved relative self-attention mechanism for transformer with application to music generation. ||| cheng-zhi anna huang ||| ashish vaswani ||| jakob uszkoreit ||| noam shazeer ||| curtis hawthorne ||| andrew m. dai ||| matthew d. hoffman ||| douglas eck ||| 
2017 ||| deep cropping via attention box prediction and aesthetics assessment. ||| wenguan wang ||| jianbing shen ||| 
2021 ||| teds-net: enforcing diffeomorphisms in spatial transformers to guarantee topology preservation in segmentations. ||| madeleine k. wyburd ||| nicola k. dinsdale ||| ana i. l. namburete ||| mark jenkinson ||| 
2020 ||| pop music transformer: generating music with rhythm and harmony. ||| yu-siang huang ||| yi-hsuan yang ||| 
2021 ||| malbert: using transformers for cybersecurity and malicious software detection. ||| abir rahali ||| moulay a. akhloufi ||| 
2021 ||| boundary-aware transformers for skin lesion segmentation. ||| jiacheng wang ||| lan wei ||| liansheng wang ||| qichao zhou ||| lei zhu ||| jing qin ||| 
2020 ||| attention mesh: high-fidelity face mesh prediction in real-time. ||| ivan grishchenko ||| artsiom ablavatski ||| yury kartynnik ||| karthik raveendran ||| matthias grundmann ||| 
2019 ||| interlaced sparse self-attention for semantic segmentation. ||| lang huang ||| yuhui yuan ||| jianyuan guo ||| chao zhang ||| xilin chen ||| jingdong wang ||| 
2021 ||| rumor detection on twitter with claim-guided hierarchical graph attention networks. ||| hongzhan lin ||| jing ma ||| mingfei cheng ||| zhiwei yang ||| liangliang chen ||| guang chen ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| joan plepi ||| lucie flek ||| 
2021 ||| wangchanberta: pretraining transformer-based thai language models. ||| lalita lowphansirikul ||| charin polpanumas ||| nawat jantrakulchai ||| sarana nutanong ||| 
2021 ||| perspectives and prospects on transformer architecture for cross-modal tasks with language and vision. ||| andrew shin ||| masato ishii ||| takuya narihira ||| 
2021 ||| a review of bangla natural language processing tasks and the utility of transformer models. ||| firoj alam ||| md. arid hasan ||| tanvirul alam ||| akib khan ||| jannatul tajrin ||| naira khan ||| shammur absar chowdhury ||| 
2021 ||| subject independent emotion recognition using eeg signals employing attention driven neural networks. ||| arjun ||| aniket singh rajpoot ||| mahesh raveendranatha panicker ||| 
2021 ||| graph conditioned sparse-attention for improved source code understanding. ||| junyan cheng ||| iordanis fostiropoulos ||| barry w. boehm ||| 
2020 ||| improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images. ||| sharif amit kamran ||| alireza tavakkoli ||| stewart lee zuckerbrod ||| 
2021 ||| what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers. ||| boseop kim ||| hyoungseok kim ||| sang-woo lee ||| gichang lee ||| dong-hyun kwak ||| dong hyeon jeon ||| sunghyun park ||| sungju kim ||| seonhoon kim ||| dongpil seo ||| heungsub lee ||| minyoung jeong ||| sungjae lee ||| minsub kim ||| sukhyun ko ||| seokhun kim ||| taeyong park ||| jinuk kim ||| soyoung kang ||| na-hyeon ryu ||| kang min yoo ||| minsuk chang ||| soobin suh ||| sookyo in ||| jinseong park ||| kyungduk kim ||| hiun kim ||| jisu jeong ||| yong goo yeo ||| donghoon ham ||| dongju park ||| min young lee ||| jaewook kang ||| inho kang ||| jung-woo ha ||| woo-myoung park ||| nako sung ||| 
2021 ||| multi-scale context aggregation network with attention-guided for crowd counting. ||| xin wang ||| yang zhao ||| tangwen yang ||| qiuqi ruan ||| 
2021 ||| is attention always needed? a case study on language identification from speech. ||| atanu mandal ||| santanu pal ||| indranil dutta ||| mahidas bhattacharya ||| sudip kumar naskar ||| 
2019 ||| team papelo: transformer networks at fever. ||| christopher malon ||| 
2021 ||| a comparison of deep learning classification methods on small-scale image data set: from convolutional neural networks to visual transformers. ||| peng zhao ||| chen li ||| md mamunur rahaman ||| hechen yang ||| tao jiang ||| marcin grzegorzek ||| 
2020 ||| seeing both the forest and the trees: multi-head attention for joint classification on different compositional levels. ||| miruna pislar ||| marek rei ||| 
2021 ||| generative adversarial transformers. ||| drew a. hudson ||| c. lawrence zitnick ||| 
2018 ||| combining deep and depth: deep learning and face depth maps for driver attention monitoring. ||| guido borghi ||| 
2019 ||| attention filtering for multi-person spatiotemporal action detection on deep two-stream cnn architectures. ||| jo ||| o antunes ||| pedro abreu ||| alexandre bernardino ||| asim smailagic ||| daniel p. siewiorek ||| 
2021 ||| attention-guided progressive neural texture fusion for high dynamic range image restoration. ||| jie chen ||| zaifeng yang ||| tsz nam chan ||| hui li ||| junhui hou ||| lap-pui chau ||| 
2022 ||| visual attention network. ||| meng-hao guo ||| chengze lu ||| zheng-ning liu ||| ming-ming cheng ||| shimin hu ||| 
2019 ||| monotonic infinite lookback attention for simultaneous machine translation. ||| naveen arivazhagan ||| colin cherry ||| wolfgang macherey ||| chung-cheng chiu ||| semih yavuz ||| ruoming pang ||| wei li ||| colin raffel ||| 
2018 ||| advancing acoustic-to-word ctc model with attention and mixed-units. ||| amit das ||| jinyu li ||| guoli ye ||| rui zhao ||| yifan gong ||| 
2021 ||| sampling equivariant self-attention networks for object detection in aerial images. ||| guo-ye yang ||| xiang-li li ||| ralph r. martin ||| shi-min hu ||| 
2020 ||| linformer: self-attention with linear complexity. ||| sinong wang ||| belinda z. li ||| madian khabsa ||| han fang ||| hao ma ||| 
2017 ||| attention-based cnn matching net. ||| tzu-chien liu ||| yu-hsueh wu ||| hung-yi lee ||| 
2021 ||| polyvit: co-training vision transformers on images, videos and audio. ||| valerii likhosherstov ||| anurag arnab ||| krzysztof choromanski ||| mario lucic ||| yi tay ||| adrian weller ||| mostafa dehghani ||| 
2019 ||| deep fusion: an attention guided factorized bilinear pooling for audio-video emotion recognition. ||| yuanyuan zhang ||| zi-rui wang ||| jun du ||| 
2019 ||| a neural attention model for adaptive learning of social friends' preferences. ||| dimitrios rafailidis ||| gerhard weiss ||| 
2021 ||| action transformer: a self-attention model for short-time human action recognition. ||| vittorio mazzia ||| simone angarano ||| francesco salvetti ||| federico angelini ||| marcello chiaberge ||| 
2022 ||| audio visual scene-aware dialog generation with transformer-based video representations. ||| yoshihiro yamazaki ||| shota orihashi ||| ryo masumura ||| mihiro uchida ||| akihiko takashima ||| 
2021 ||| lipschitz normalization for self-attention layers with application to graph neural networks. ||| george dasoulas ||| kevin scaman ||| aladin virmaux ||| 
2019 ||| injecting hierarchy with u-net transformers. ||| david donahue ||| vladislav lialin ||| anna rumshisky ||| 
2020 ||| attend and decode: 4d fmri task state decoding using attention models. ||| sam nguyen ||| brenda ng ||| alan david kaplan ||| priyadip ray ||| 
2019 ||| disambiguating speech intention via audio-text co-attention framework: a case of prosody-semantics interface. ||| won-ik cho ||| jeonghwa cho ||| woo hyun kang ||| nam soo kim ||| 
2021 ||| dmsanet: dual multi scale attention network. ||| abhinav sagar ||| 
2021 ||| dq-gat: towards safe and efficient autonomous driving with deep q-learning and graph attention networks. ||| peide cai ||| hengli wang ||| yuxiang sun ||| ming liu ||| 
2020 ||| max-deeplab: end-to-end panoptic segmentation with mask transformers. ||| huiyu wang ||| yukun zhu ||| hartwig adam ||| alan l. yuille ||| liang-chieh chen ||| 
2020 ||| would you like sashimi even if it's sliced too thin? selective neural attention for aspect targeted sentiment analysis (snat). ||| zhe zhang ||| chung-wei hang ||| munindar p. singh ||| 
2022 ||| graph attention transformer network for multi-label image classification. ||| jin yuan ||| shikai chen ||| yao zhang ||| zhongchao shi ||| xin geng ||| jianping fan ||| yong rui ||| 
2021 ||| local citation recommendation with hierarchical-attention text encoder and scibert-based reranking. ||| nianlong gu ||| yingqiang gao ||| richard h. r. hahnloser ||| 
2021 ||| higher order recurrent space-time transformer. ||| tsung-ming tai ||| giuseppe fiameni ||| cheng-kuang lee ||| oswald lanz ||| 
2017 ||| frustratingly short attention spans in neural language modeling. ||| michal daniluk ||| tim rockt ||| schel ||| johannes welbl ||| sebastian riedel ||| 
2019 ||| show, price and negotiate: a hierarchical attention recurrent visual negotiator. ||| amin parvaneh ||| ehsan abbasnejad ||| qi wu ||| javen shi ||| 
2021 ||| temporal memory attention for video semantic segmentation. ||| hao wang ||| weining wang ||| jing liu ||| 
2021 ||| on-the-fly attention modularization for neural generation. ||| yue dong ||| chandra bhagavatula ||| ximing lu ||| jena d. hwang ||| antoine bosselut ||| jackie chi kit cheung ||| yejin choi ||| 
2018 ||| learning to exploit invariances in clinical time-series data using sequence transformer networks. ||| jeeheh oh ||| jiaxuan wang ||| jenna wiens ||| 
2020 ||| towards boosting the channel attention in real image denoising : sub-band pyramid attention. ||| huayu li ||| haiyu wu ||| xiwen chen ||| hanning zhang ||| abolfazl razi ||| 
2021 ||| yformer: u-net inspired transformer architecture for far horizon time series forecasting. ||| kiran madhusudhanan ||| johannes burchert ||| nghia duong-trung ||| stefan born ||| lars schmidt-thieme ||| 
2019 ||| pose-adaptive hierarchical attention network for facial expression recognition. ||| yuanyuan liu ||| jiyao peng ||| jiabei zeng ||| shiguang shan ||| 
2021 ||| after-unet: axial fusion transformer unet for medical image segmentation. ||| xiangyi yan ||| hao tang ||| shanlin sun ||| haoyu ma ||| deying kong ||| xiaohui xie ||| 
2021 ||| is attention better than matrix decomposition? ||| zhengyang geng ||| meng-hao guo ||| hongxu chen ||| xia li ||| ke wei ||| zhouchen lin ||| 
2021 ||| finetuning transformer models to build asag system. ||| mithun thakkar ||| 
2020 ||| transforming multi-concept attention into video summarization. ||| yen-ting liu ||| yu-jhe li ||| yu-chiang frank wang ||| 
2018 ||| the pros and cons: rank-aware temporal attention for skill determination in long videos. ||| hazel doughty ||| walterio w. mayol-cuevas ||| dima damen ||| 
2021 ||| thermal infrared image colorization for nighttime driving scenes with top-down guided attention. ||| fuya luo ||| yunhan li ||| guang zeng ||| peng peng ||| gang wang ||| yongjie li ||| 
2019 ||| is it worth the attention? a comparative evaluation of attention layers for argument unit segmentation. ||| maximilian splieth ||| ver ||| jonas klaff ||| hendrik heuer ||| 
2021 ||| from extreme multi-label to multi-class: a hierarchical approach for automated icd-10 coding using phrase-level attention. ||| cansu sen ||| bingyang ye ||| javed aslam ||| amir tahmasebi ||| 
2022 ||| mixformer: end-to-end tracking with iterative mixed attention. ||| yutao cui ||| cheng jiang ||| limin wang ||| gangshan wu ||| 
2019 ||| exbert: a visual analysis tool to explore learned representations in transformers models. ||| benjamin hoover ||| hendrik strobelt ||| sebastian gehrmann ||| 
2018 ||| deep adaptive attention for joint facial action unit detection and face alignment. ||| zhiwen shao ||| zhilei liu ||| jianfei cai ||| lizhuang ma ||| 
2018 ||| hierarchical attention-based recurrent highway networks for time series prediction. ||| yunzhe tao ||| lin ma ||| weizhong zhang ||| jian liu ||| wei liu ||| qiang du ||| 
2021 ||| the devil is in the detail: simple tricks improve systematic generalization of transformers. ||| r ||| bert csord ||| s ||| kazuki irie ||| j ||| rgen schmidhuber ||| 
2021 ||| asper: attention-based approach to extract syntactic patterns denoting semantic relations in sentential context. ||| md. ahsanul kabir ||| typer phillips ||| xiao luo ||| mohammad al hasan ||| 
2021 ||| pnp-detr: towards efficient visual analysis with transformers. ||| tao wang ||| li yuan ||| yunpeng chen ||| jiashi feng ||| shuicheng yan ||| 
2022 ||| a unified transformer framework for group-based segmentation: co-segmentation, co-saliency detection and video salient object detection. ||| yukun su ||| jingliang deng ||| ruizhou sun ||| guosheng lin ||| qingyao wu ||| 
2021 ||| the right to talk: an audio-visual transformer approach. ||| thanh-dat truong ||| chi nhan duong ||| the de vu ||| hoang anh pham ||| bhiksha raj ||| ngan le ||| khoa luu ||| 
2018 ||| component-based attention for large-scale trademark retrieval. ||| osman tursun ||| simon denman ||| sabesan sivapalan ||| sridha sridharan ||| clinton fookes ||| sandra mau ||| 
2021 ||| mlma-net: multi-level multi-attentional learning for multi-label object detection in textile defect images. ||| bing wei ||| kuangrong hao ||| lei gao ||| 
2019 ||| automatic segmentation of vestibular schwannoma from t2-weighted mri by deep spatial attention with hardness-weighted loss. ||| guotai wang ||| jonathan shapey ||| wenqi li ||| reuben dorent ||| alex demitriadis ||| sotirios bisdas ||| ian paddick ||| robert bradford ||| s ||| bastien ourselin ||| tom vercauteren ||| 
2022 ||| bvit: broad attention based vision transformer. ||| nannan li ||| yaran chen ||| weifan li ||| zixiang ding ||| dongbin zhao ||| 
2020 ||| understood in translation, transformers for domain understanding. ||| dimitrios christofidellis ||| matteo manica ||| leonidas georgopoulos ||| hans vandierendonck ||| 
2020 ||| contextualized attention-based knowledge transfer for spoken conversational question answering. ||| chenyu you ||| nuo chen ||| yuexian zou ||| 
2021 ||| improving the efficiency of transformers for resource-constrained devices. ||| hamid tabani ||| ajay balasubramaniam ||| shabbir marzban ||| elahe arani ||| bahram zonooz ||| 
2022 ||| transformer grammars: augmenting transformer language models with syntactic inductive biases at scale. ||| laurent sartran ||| samuel barrett ||| adhiguna kuncoro ||| milos stanojevic ||| phil blunsom ||| chris dyer ||| 
2020 ||| big bird: transformers for longer sequences. ||| manzil zaheer ||| guru guruganesh ||| avinava dubey ||| joshua ainslie ||| chris alberti ||| santiago onta ||| n ||| philip pham ||| anirudh ravula ||| qifan wang ||| li yang ||| amr ahmed ||| 
2021 ||| trans4trans: efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world. ||| jiaming zhang ||| kailun yang ||| angela constantinescu ||| kunyu peng ||| karin m ||| ller ||| rainer stiefelhagen ||| 
2020 ||| self-adaptive physics-informed neural networks using a soft attention mechanism. ||| levi d. mcclenny ||| ulisses m. braga-neto ||| 
2019 ||| ha-ccn: hierarchical attention-based crowd counting network. ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2021 ||| solve routing problems with a residual edge-graph attention neural network. ||| kun lei ||| peng guo ||| yi wang ||| xiao wu ||| wenchao zhao ||| 
2022 ||| video transformers: a survey. ||| javier selva ||| anders s. johansen ||| sergio escalera ||| kamal nasrollahi ||| thomas b. moeslund ||| albert clap ||| s ||| 
2022 ||| domain adaptation via bidirectional cross-attention transformer. ||| xiyu wang ||| pengxin guo ||| yu zhang ||| 
2022 ||| is cross-attention preferable to self-attention for multi-modal emotion recognition? ||| vandana rajan ||| alessio brutti ||| andrea cavallaro ||| 
2022 ||| spatial transformer k-means. ||| romain cosentino ||| randall balestriero ||| yanis bahroun ||| anirvan m. sengupta ||| richard g. baraniuk ||| behnaam aazhang ||| 
2021 ||| clinical relation extraction using transformer-based models. ||| xi yang ||| zehao yu ||| yi guo ||| jiang bian ||| yonghui wu ||| 
2022 ||| hypertransformer: model generation for supervised and semi-supervised few-shot learning. ||| andrey zhmoginov ||| mark sandler ||| max vladymyrov ||| 
2020 ||| a multi-channel temporal attention convolutional neural network model for environmental sound classification. ||| you wang ||| chuyao feng ||| david v. anderson ||| 
2019 ||| multi-scale time-frequency attention for rare sound event detection. ||| jingyang zhang ||| wenhao ding ||| jintao kang ||| liang he ||| 
2020 ||| a robust attentional framework for license plate recognition in the wild. ||| linjiang zhang ||| peng wang ||| hui li ||| zhen li ||| chunhua shen ||| yanning zhang ||| 
2020 ||| gravitational models explain shifts on human visual attention. ||| dario zanca ||| marco gori ||| stefano melacci ||| alessandra rufa ||| 
2019 ||| dual-attention graph convolutional network. ||| xueya zhang ||| tong zhang ||| wenting zhao ||| zhen cui ||| jian yang ||| 
2021 ||| contrastively learning visual attention as affordance cues from demonstrations for robotic grasping. ||| yantian zha ||| siddhant bhambri ||| lin guan ||| 
2018 ||| an end-to-end textspotter with explicit alignment and attention. ||| tong he ||| zhi tian ||| weilin huang ||| chunhua shen ||| yu qiao ||| changming sun ||| 
2020 ||| channel distillation: channel-wise attention for knowledge distillation. ||| zaida zhou ||| chaoran zhuge ||| xinwei guan ||| wen liu ||| 
2021 ||| ammus : a survey of transformer-based pretrained models in natural language processing. ||| katikapalli subramanyam kalyan ||| ajit rajasekharan ||| sivanesan sangeetha ||| 
2019 ||| semantic mask for transformer based end-to-end speech recognition. ||| chengyi wang ||| yu wu ||| yujiao du ||| jinyu li ||| shujie liu ||| liang lu ||| shuo ren ||| guoli ye ||| sheng zhao ||| ming zhou ||| 
2019 ||| hybrid residual attention network for single image super resolution. ||| abdul muqeet ||| md. tauhid bin iqbal ||| sung-ho bae ||| 
2021 ||| semantic correspondence with transformers. ||| seokju cho ||| sunghwan hong ||| sangryul jeon ||| yunsung lee ||| kwanghoon sohn ||| seungryong kim ||| 
2022 ||| vaqf: fully automatic software-hardware co-design framework for low-bit vision transformer. ||| mengshu sun ||| haoyu ma ||| guoliang kang ||| yifan jiang ||| tianlong chen ||| xiaolong ma ||| zhangyang wang ||| yanzhi wang ||| 
2020 ||| attendlight: universal attention-based reinforcement learning model for traffic signal control. ||| afshin oroojlooy ||| mohammadreza nazari ||| davood hajinezhad ||| jorge silva ||| 
2020 ||| the effect of top-down attention in occluded object recognition. ||| zahra sadeghi ||| 
2021 ||| self-slimmed vision transformer. ||| zhuofan zong ||| kunchang li ||| guanglu song ||| yali wang ||| yu qiao ||| biao leng ||| yu liu ||| 
2020 ||| da4ad: end-to-end deep attention aware features aided visual localization for autonomous driving. ||| yao zhou ||| guowei wan ||| shenhua hou ||| li yu ||| gang wang ||| xiaofei rui ||| shiyu song ||| 
2019 ||| tree transformer: integrating tree structures into self-attention. ||| yau-shian wang ||| hung-yi lee ||| yun-nung chen ||| 
2020 ||| highway transformer: self-gating enhanced self-attentive networks. ||| yekun chai ||| jin shuo ||| xinwen hou ||| 
2020 ||| musicoder: a universal music-acoustic encoder based on transformers. ||| yilun zhao ||| xinda wu ||| yuqing ye ||| jia guo ||| kejun zhang ||| 
2020 ||| do transformers need deep long-range memory. ||| jack w. rae ||| ali razavi ||| 
2019 ||| attention-guided lightweight network for real-time segmentation of robotic surgical instruments. ||| zhen-liang ni ||| gui-bin bian ||| zeng-guang hou ||| xiao-hu zhou ||| xiao-liang xie ||| zhen li ||| 
2022 ||| grouptransnet: group transformer network for rgb-d salient object detection. ||| xian fang ||| jinchao zhu ||| xiuli shao ||| hongpeng wang ||| 
2021 ||| attention in attention network for image super-resolution. ||| haoyu chen ||| jinjin gu ||| zhi zhang ||| 
2021 ||| caa : channelized axial attention for semantic segmentation. ||| ye huang ||| wenjing jia ||| xiangjian he ||| liu liu ||| yuxin li ||| dacheng tao ||| 
2021 ||| visformer: the vision-friendly transformer. ||| zhengsu chen ||| lingxi xie ||| jianwei niu ||| xuefeng liu ||| longhui wei ||| qi tian ||| 
2017 ||| call attention to rumors: deep attention based recurrent neural networks for early rumor detection. ||| tong chen ||| lin wu ||| xue li ||| jun zhang ||| hongzhi yin ||| yang wang ||| 
2020 ||| multi-domain dialogue state tracking - a purely transformer-based generative approach. ||| yan zeng ||| jian-yun nie ||| 
2018 ||| spatial transformer introspective neural network. ||| yunhan zhao ||| ye tian ||| wei shen ||| alan l. yuille ||| 
2022 ||| variational stacked local attention networks for diverse video captioning. ||| tonmoay deb ||| akib sadmanee ||| kishor kumar bhaumik ||| amin ahsan ali ||| m. ashraful amin ||| a. k. m. mahbubur rahman ||| 
2021 ||| heterogeneous graph attention networks for learning diverse communication. ||| esmaeil seraj ||| zheyuan wang ||| rohan r. paleja ||| matthew sklar ||| anirudh patel ||| matthew c. gombolay ||| 
2019 ||| attention-based supply-demand prediction for autonomous vehicles. ||| zikai zhang ||| yidong li ||| hairong dong ||| yizhe you ||| fengping zhao ||| 
2021 ||| locating faulty methods with a mixed rnn and attention model. ||| shouliang yang ||| junming cao ||| hushuang zeng ||| beijun shen ||| hao zhong ||| 
2020 ||| on the ability of self-attention networks to recognize counter languages. ||| satwik bhattamishra ||| kabir ahuja ||| navin goyal ||| 
2020 ||| style example-guided text generation using generative adversarial transformers. ||| kuo-hao zeng ||| mohammad shoeybi ||| ming-yu liu ||| 
2018 ||| understanding and improving recurrent networks for human activity recognition by continuous attention. ||| ming zeng ||| haoxiang gao ||| tong yu ||| ole j. mengshoel ||| helge langseth ||| ian r. lane ||| xiaobing liu ||| 
2017 ||| dynamic time-aware attention to speaker roles and contexts for spoken language understanding. ||| po-chun chen ||| ta-chung chi ||| shang-yu su ||| yun-nung chen ||| 
2020 ||| mango: a mask attention guided one-stage scene text spotter. ||| liang qiao ||| ying chen ||| zhanzhan cheng ||| yunlu xu ||| yi niu ||| shiliang pu ||| fei wu ||| 
2021 ||| assessing the impact of attention and self-attention mechanisms on the classification of skin lesions. ||| rafael pedro ||| arlindo l. oliveira ||| 
2021 ||| a novel attention-based network for fast salient object detection. ||| bin zhang ||| yang wu ||| xiaojing zhang ||| ming ma ||| 
2022 ||| gat-cadnet: graph attention network for panoptic symbol spotting in cad drawings. ||| zhaohua zheng ||| jianfang li ||| lingjie zhu ||| honghua li ||| frank petzold ||| ping tan ||| 
2021 ||| javabert: training a transformer-based model for the java programming language. ||| nelson tavares de sousa ||| wilhelm hasselbring ||| 
2022 ||| open set recognition using vision transformer with an additional detection head. ||| feiyang cai ||| zhenkai zhang ||| jie liu ||| xenofon d. koutsoukos ||| 
2021 ||| spatio-temporal graph dual-attention network for multi-agent prediction and tracking. ||| jiachen li ||| hengbo ma ||| zhihao zhang ||| jinning li ||| masayoshi tomizuka ||| 
2019 ||| semi-interactive attention network for answer understanding in reverse-qa. ||| qing yin ||| guan luo ||| xiaodong zhu ||| qinghua hu ||| ou wu ||| 
2020 ||| optimizing transformer for low-resource neural machine translation. ||| ali araabi ||| christof monz ||| 
2021 ||| demographic-guided attention in recurrent neural networks for modeling neuropathophysiological heterogeneity. ||| nicha c. dvornek ||| xiaoxiao li ||| juntang zhuang ||| pamela ventola ||| james s. duncan ||| 
2021 ||| lifting transformer for 3d human pose estimation in video. ||| wenhao li ||| hong liu ||| runwei ding ||| mengyuan liu ||| pichao wang ||| 
2017 ||| ssemnet: serial-section electron microscopy image registration using a spatial transformer network with learned features. ||| inwan yoo ||| david g. c. hildebrand ||| willie f. tobin ||| wei-chung allen lee ||| won-ki jeong ||| 
2019 ||| dual adversarial learning with attention mechanism for fine-grained medical image synthesis. ||| dong nie ||| lei xiang ||| qian wang ||| dinggang shen ||| 
2021 ||| mst: masked self-supervised transformer for visual representation. ||| zhaowen li ||| zhiyang chen ||| fan yang ||| wei li ||| yousong zhu ||| chaoyang zhao ||| rui deng ||| liwei wu ||| rui zhao ||| ming tang ||| jinqiao wang ||| 
2020 ||| point transformer. ||| hengshuang zhao ||| li jiang ||| jiaya jia ||| philip h. s. torr ||| vladlen koltun ||| 
2021 ||| bootstrapping vits: towards liberating vision transformers from pre-training. ||| haofei zhang ||| jiarui duan ||| mengqi xue ||| jie song ||| li sun ||| mingli song ||| 
2021 ||| understanding and overcoming the challenges of efficient transformer quantization. ||| yelysei bondarenko ||| markus nagel ||| tijmen blankevoort ||| 
2021 ||| efficient video transformers with spatial-temporal token selection. ||| junke wang ||| xitong yang ||| hengduo li ||| zuxuan wu ||| yu-gang jiang ||| 
2020 ||| granet: global relation-aware attentional network for als point cloud classification. ||| rong huang ||| yusheng xu ||| uwe stilla ||| 
2020 ||| automatic lyrics transcription using dilated convolutional neural networks with self-attention. ||| emir demirel ||| sven ahlb ||| ck ||| simon dixon ||| 
2020 ||| exploring recurrent, memory and attention based architectures for scoring interactional aspects of human-machine text dialog. ||| vikram ramanarayanan ||| matthew mulholland ||| debanjan ghosh ||| 
2021 ||| reason induced visual attention for explainable autonomous driving. ||| sikai chen ||| jiqian dong ||| runjia du ||| yujie li ||| samuel labi ||| 
2021 ||| the heads hypothesis: a unifying statistical approach towards understanding multi-headed attention in bert. ||| madhura pande ||| aakriti budhraja ||| preksha nema ||| pratyush kumar ||| mitesh m. khapra ||| 
2019 ||| permutohedral attention module for efficient non-local neural networks. ||| samuel joutard ||| reuben dorent ||| amanda isaac ||| s ||| bastien ourselin ||| tom vercauteren ||| marc modat ||| 
2018 ||| video action transformer network. ||| rohit girdhar ||| jo ||| o carreira ||| carl doersch ||| andrew zisserman ||| 
2020 ||| global attention for name tagging. ||| boliang zhang ||| spencer whitehead ||| lifu huang ||| heng ji ||| 
2021 ||| simple local attentions remain competitive for long-context tasks. ||| wenhan xiong ||| barlas oguz ||| anchit gupta ||| xilun chen ||| diana liskovich ||| omer levy ||| wen-tau yih ||| yashar mehdad ||| 
2020 ||| explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation. ||| hang dong ||| v ||| ctor su ||| rez-paniagua ||| william whiteley ||| honghan wu ||| 
2017 ||| paying attention to multi-word expressions in neural machine translation. ||| matiss rikters ||| ondrej bojar ||| 
2021 ||| fq-vit: fully quantized vision transformer without retraining. ||| yang lin ||| tianyu zhang ||| peiqin sun ||| zheng li ||| shuchang zhou ||| 
2021 ||| m2tr: multi-modal multi-scale transformers for deepfake detection. ||| junke wang ||| zuxuan wu ||| jingjing chen ||| yu-gang jiang ||| 
2022 ||| transsleep: transitioning-aware attention-based deep neural network for sleep staging. ||| jauen phyo ||| wonjun ko ||| eunjin jeon ||| heung-il suk ||| 
2020 ||| bidirectional attention network for monocular depth estimation. ||| shubhra aich ||| jean marie uwabeza vianney ||| md. amirul islam ||| mannat kaur ||| bingbing liu ||| 
2021 ||| mixed attention transformer for leveraging word-level knowledge to neural cross-lingual information retrieval. ||| zhiqi huang ||| hamed r. bonab ||| sheikh muhammad sarwar ||| razieh rahimi ||| james allan ||| 
2020 ||| local context attention for salient object segmentation. ||| jing tan ||| pengfei xiong ||| yuwen he ||| kuntao xiao ||| zhengyi lv ||| 
2021 ||| attestnet - an attention and subword tokenization based approach for code-switched hindi-english hate speech detection. ||| vedangi wagh ||| geet shingi ||| 
2019 ||| low rank factorization for compact multi-head self-attention. ||| sneha mehta ||| huzefa rangwala ||| naren ramakrishnan ||| 
2020 ||| t-vectors: weakly supervised speaker identification using hierarchical transformer model. ||| yanpei shi ||| mingjie chen ||| qiang huang ||| thomas hain ||| 
2020 ||| convtransformer: a convolutional transformer network for video frame synthesis. ||| zhouyong liu ||| shun luo ||| wubin li ||| jingben lu ||| yufan wu ||| chunguo li ||| luxi yang ||| 
2021 ||| evaluating pretrained transformer models for entity linking in task-oriented dialog. ||| sai muralidhar jayanthi ||| varsha embar ||| karthik raghunathan ||| 
2021 ||| generative pre-trained transformer for cardiac abnormality detection. ||| pierre louis gaudilliere ||| halla sigurthorsdottir ||| cl ||| mentine aguet ||| j ||| r ||| me van zaen ||| mathieu lemay ||| ricard delgado-gonzalo ||| 
2020 ||| dtgan: dual attention generative adversarial networks for text-to-image generation. ||| zhenxing zhang ||| lambert schomaker ||| 
2021 ||| sequential vessel segmentation via deep channel attention network. ||| dongdong hao ||| song ding ||| linwei qiu ||| yisong lv ||| baowei fei ||| yueqi zhu ||| binjie qin ||| 
2017 ||| a dual-stage attention-based recurrent neural network for time series prediction. ||| yao qin ||| dongjin song ||| haifeng chen ||| wei cheng ||| guofei jiang ||| garrison w. cottrell ||| 
2020 ||| attentional separation-and-aggregation network for self-supervised depth-pose learning in dynamic scenes. ||| feng gao ||| jincheng yu ||| hao shen ||| yu wang ||| huazhong yang ||| 
2021 ||| relative positional encoding for transformers with linear complexity. ||| antoine liutkus ||| ondrej c ||| fka ||| shih-lun wu ||| umut simsekli ||| yi-hsuan yang ||| ga ||| l richard ||| 
2021 ||| congested crowd instance localization with dilated convolutional swin transformer. ||| junyu gao ||| maoguo gong ||| xuelong li ||| 
2022 ||| recent advances in vision transformer: a survey and outlook of recent work. ||| khawar islam ||| 
2022 ||| transcam: transformer attention-based cam refinement for weakly supervised semantic segmentation. ||| ruiwen li ||| zheda mai ||| chiheb trabelsi ||| zhibo zhang ||| jongseong jang ||| scott sanner ||| 
2021 ||| aaseg: attention aware network for real time semantic segmentation. ||| abhinav sagar ||| 
2020 ||| a novel attention-based aggregation function to combine vision and language. ||| matteo stefanini ||| marcella cornia ||| lorenzo baraldi ||| rita cucchiara ||| 
2018 ||| attnconvnet at semeval-2018 task 1: attention-based convolutional neural networks for multi-label emotion classification. ||| yanghoon kim ||| hwanhee lee ||| kyomin jung ||| 
2018 ||| medical concept embedding with time-aware attention. ||| xiangrui cai ||| jinyang gao ||| kee yuan ngiam ||| beng chin ooi ||| ying zhang ||| xiaojie yuan ||| 
2020 ||| palomino-ochoa at semeval-2020 task 9: robust system based on transformer for code-mixed sentiment classification. ||| daniel palomino ||| jos |||  ochoa luna ||| 
2020 ||| understanding when spatial transformer networks do not support invariance, and what to do about it. ||| lukas finnveden ||| ylva jansson ||| tony lindeberg ||| 
2020 ||| attention based multiple instance learning for classification of blood cell disorders. ||| ario sadafi ||| asya makhro ||| anna bogdanova ||| nassir navab ||| tingying peng ||| shadi albarqouni ||| carsten marr ||| 
2022 ||| semi-supervised graph attention networks for event representation learning. ||| jo ||| o pedro rodrigues mattos ||| ricardo m. marcacini ||| 
2020 ||| weakly-supervised action localization by generative attention modeling. ||| baifeng shi ||| qi dai ||| yadong mu ||| jingdong wang ||| 
2021 ||| fast offline transformer-based end-to-end automatic speech recognition for real-world applications. ||| yoo rhee oh ||| kiyoung park ||| jeon gyu park ||| 
2021 ||| saccadecam: adaptive visual attention for monocular depth sensing. ||| brevin tilmon ||| sanjeev j. koppal ||| 
2021 ||| attention on classification for fire segmentation. ||| milad niknejad ||| alexandre bernardino ||| 
2020 ||| collaborative attention mechanism for multi-view action recognition. ||| yue bai ||| zhiqiang tao ||| lichen wang ||| sheng li ||| yu yin ||| yun fu ||| 
2021 ||| alignment knowledge distillation for online streaming attention-based speech recognition. ||| hirofumi inaguma ||| tatsuya kawahara ||| 
2020 ||| utilising visual attention cues for vehicle detection and tracking. ||| feiyan hu ||| g. m. venkatesh ||| noel e. o'connor ||| alan f. smeaton ||| suzanne little ||| 
2022 ||| unified visual transformer compression. ||| shixing yu ||| tianlong chen ||| jiayi shen ||| huan yuan ||| jianchao tan ||| sen yang ||| ji liu ||| zhangyang wang ||| 
2018 ||| point2sequence: learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. ||| xinhai liu ||| zhizhong han ||| yu-shen liu ||| matthias zwicker ||| 
2021 ||| sanmove: next location recommendation via self-attention network. ||| huifeng li ||| bin wang ||| sulei zhu ||| yanyan xu ||| 
2021 ||| dytox: transformers for continual learning with dynamic token expansion. ||| arthur douillard ||| alexandre ram ||| guillaume couairon ||| matthieu cord ||| 
2022 ||| learning affinity from attention: end-to-end weakly-supervised semantic segmentation with transformers. ||| lixiang ru ||| yibing zhan ||| baosheng yu ||| bo du ||| 
2020 ||| document-level event-based extraction using generative template-filling transformers. ||| xinya du ||| alexander m. rush ||| claire cardie ||| 
2020 ||| emformer: efficient memory transformer based acoustic model for low latency streaming speech recognition. ||| yangyang shi ||| yongqiang wang ||| chunyang wu ||| ching-feng yeh ||| julian chan ||| frank zhang ||| duc le ||| michael l. seltzer ||| 
2018 ||| cross-media multi-level alignment with relation attention network. ||| jinwei qi ||| yuxin peng ||| yuxin yuan ||| 
2020 ||| attention is all you need in speech separation. ||| cem subakan ||| mirco ravanelli ||| samuele cornell ||| mirko bronzi ||| jianyuan zhong ||| 
2022 ||| generalised image outpainting with u-transformer. ||| penglei gao ||| xi yang ||| rui zhang ||| kaizhu huang ||| yujie geng ||| 
2022 ||| simplicial attention networks. ||| l. giusti ||| claudio battiloro ||| p. di lorenzo ||| stefania sardellitti ||| sergio barbarossa ||| 
2021 ||| probabilistic graph attention network with conditional kernels for pixel-wise prediction. ||| dan xu ||| xavier alameda-pineda ||| wanli ouyang ||| elisa ricci ||| xiaogang wang ||| nicu sebe ||| 
2021 ||| combining gcn and transformer for chinese grammatical error detection. ||| jinhong zhang ||| 
2021 ||| towards low-latency energy-efficient deep snns via attention-guided compression. ||| souvik kundu ||| gourav datta ||| massoud pedram ||| peter a. beerel ||| 
2021 ||| on the distribution, sparsity, and inference-time quantization of attention values in transformers. ||| tianchu ji ||| shraddhan jain ||| michael ferdman ||| peter a. milder ||| h. andrew schwartz ||| niranjan balasubramanian ||| 
2022 ||| safl: a self-attention scene text recognizer with focal loss. ||| bao hieu tran ||| thanh le-cong ||| huu manh nguyen ||| duc anh le ||| thanh-hung nguyen ||| phi le nguyen ||| 
2021 ||| fine-grained attention for weakly supervised object localization. ||| junghyo sohn ||| eunjin jeon ||| wonsik jung ||| eunsong kang ||| heung-il suk ||| 
2019 ||| goal-directed behavior under variational predictive coding: dynamic organization of visual attention and working memory. ||| minju jung ||| takazumi matsumoto ||| jun tani ||| 
2020 ||| attention improves concentration when learning node embeddings. ||| matthew dippel ||| adam kiezun ||| tanay mehta ||| ravi sundaram ||| srikanth thirumalai ||| akshar varma ||| 
2019 ||| a comparative study on transformer vs rnn in speech applications. ||| shigeki karita ||| nanxin chen ||| tomoki hayashi ||| takaaki hori ||| hirofumi inaguma ||| ziyan jiang ||| masao someki ||| nelson enrique yalta soplin ||| ryuichi yamamoto ||| xiaofei wang ||| shinji watanabe ||| takenori yoshimura ||| wangyou zhang ||| 
2019 ||| attention guided network for retinal image segmentation. ||| shihao zhang ||| huazhu fu ||| yuguang yan ||| yubing zhang ||| qingyao wu ||| ming yang ||| mingkui tan ||| yanwu xu ||| 
2021 ||| bayesian attention networks for data compression. ||| michael tetelman ||| 
2017 ||| social attention: modeling attention in human crowds. ||| anirudh vemula ||| katharina m ||| lling ||| jean oh ||| 
2018 ||| accelerating neural transformer via an average attention network. ||| biao zhang ||| deyi xiong ||| jinsong su ||| 
2021 ||| video sentiment analysis with bimodal information-augmented multi-head attention. ||| ting wu ||| junjie peng ||| wenqiang zhang ||| huiran zhang ||| chuanshuai ma ||| yansong huang ||| 
2021 ||| salient positions based attention network for image classification. ||| sheng fang ||| kaiyu li ||| zhe li ||| 
2021 ||| cardiac segmentation on ct images through shape-aware contour attentions. ||| sanguk park ||| minyoung chung ||| 
2019 ||| modeling point clouds with self-attention and gumbel subset sampling. ||| jiancheng yang ||| qiang zhang ||| bingbing ni ||| linguo li ||| jinxian liu ||| mengdie zhou ||| qi tian ||| 
2020 ||| self-attention with cross-lingual position representation. ||| liang ding ||| longyue wang ||| dacheng tao ||| 
2020 ||| context-aware learning to rank with self-attention. ||| przemyslaw pobrotyn ||| tomasz bartczak ||| mikolaj synowiec ||| radoslaw bialobrzeski ||| jaroslaw bojar ||| 
2019 ||| scattnet: semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images. ||| haifeng li ||| kaijian qiu ||| li chen ||| xiaoming mei ||| liang hong ||| chao tao ||| 
2021 ||| pureformer: do we even need attention? ||| uladzislau yorsh ||| alexander kovalenko ||| 
2019 ||| deep features analysis with attention networks. ||| shipeng xie ||| da chen ||| rong zhang ||| hui xue ||| 
2020 ||| characterizing speech adversarial examples using self-attention u-net enhancement. ||| chao-han huck yang ||| jun qi ||| pin-yu chen ||| xiaoli ma ||| chin-hui lee ||| 
2021 ||| 4d attention: comprehensive framework for spatio-temporal gaze mapping. ||| shuji oishi ||| kenji koide ||| masashi yokozuka ||| atsuhiko banno ||| 
2021 ||| uninet: unified architecture search with convolution, transformer, and mlp. ||| jihao liu ||| hongsheng li ||| guanglu song ||| xin huang ||| yu liu ||| 
2020 ||| attention-oriented action recognition for real-time human-robot interaction. ||| ziyang song ||| ziyi yin ||| zejian yuan ||| chong zhang ||| wanchao chi ||| yonggen ling ||| shenghao zhang ||| 
2019 ||| multi-step reasoning via recurrent dual attention for visual dialog. ||| zhe gan ||| yu cheng ||| ahmed el kholy ||| linjie li ||| jingjing liu ||| jianfeng gao ||| 
2020 ||| ngat4rec: neighbor-aware graph attention network for recommendation. ||| jinbo song ||| chao chang ||| fei sun ||| xinbo song ||| peng jiang ||| 
2021 ||| amr parsing with action-pointer transformer. ||| jiawei zhou ||| tahira naseem ||| ram ||| n fernandez astudillo ||| radu florian ||| 
2022 ||| swin transformers make strong contextual encoders for vhr image road extraction. ||| tao chen ||| daguang jiang ||| ruirui li ||| 
2021 ||| locformer: enabling transformers to perform temporal moment localization on long untrimmed videos with a feature sampling approach. ||| cristian rodriguez opazo ||| edison marrese-taylor ||| basura fernando ||| hiroya takamura ||| qi wu ||| 
2021 ||| self-attention presents low-dimensional knowledge graph embeddings for link prediction. ||| peyman baghershahi ||| reshad hosseini ||| hadi moradi ||| 
2019 ||| collective link prediction oriented network embedding with hierarchical graph attention. ||| yizhu jiao ||| yun xiong ||| jiawei zhang ||| yangyong zhu ||| 
2020 ||| ulsam: ultra-lightweight subspace attention module for compact convolutional neural networks. ||| rajat saini ||| nandan kumar jha ||| bedanta das ||| sparsh mittal ||| c. krishna mohan ||| 
2021 ||| on the prunability of attention heads in multilingual bert. ||| aakriti budhraja ||| madhura pande ||| pratyush kumar ||| mitesh m. khapra ||| 
2020 ||| an exploratory study of argumentative writing by young students: a transformer-based approach. ||| debanjan ghosh ||| beata beigman klebanov ||| yi song ||| 
2020 ||| biomedical event extraction on graph edge-conditioned attention networks with hierarchical knowledge graphs. ||| kung-hsiang huang ||| mu yang ||| nanyun peng ||| 
2022 ||| dsformer: a dual-domain self-supervised transformer for accelerated multi-contrast mri reconstruction. ||| bo zhou ||| jo schlemper ||| neel dey ||| seyed sadegh mohseni salehi ||| chi liu ||| james s. duncan ||| michal sofka ||| 
2018 ||| a comparison of transformer and recurrent neural networks on multilingual neural machine translation. ||| surafel melaku lakew ||| mauro cettolo ||| marcello federico ||| 
2020 ||| dcanet: learning connected attentions for convolutional neural networks. ||| xu ma ||| jingda guo ||| sihai tang ||| zhinan qiao ||| qi chen ||| qing yang ||| song fu ||| 
2021 ||| transformer with peak suppression and knowledge guidance for fine-grained image recognition. ||| xinda liu ||| lili wang ||| xiaoguang han ||| 
2021 ||| artificial text detection via examining the topology of attention maps. ||| laida kushnareva ||| daniil cherniavskii ||| vladislav mikhailov ||| ekaterina artemova ||| serguei barannikov ||| alexander bernstein ||| irina piontkovskaya ||| dmitri piontkovski ||| evgeny burnaev ||| 
2018 ||| multi-attention recurrent network for human communication comprehension. ||| amir zadeh ||| paul pu liang ||| soujanya poria ||| prateek vij ||| erik cambria ||| louis-philippe morency ||| 
2021 ||| crack semantic segmentation using the u-net with full attention strategy. ||| fangzheng lin ||| jiesheng yang ||| jiangpeng shu ||| raimar j. scherer ||| 
2021 ||| aweu-net: an attention-aware weight excitation u-net for lung nodule segmentation. ||| syeda furruka banu ||| md. mostafa kamal sarker ||| mohamed abdel-nasser ||| domenec puig ||| hatem a. rashwan ||| 
2021 ||| improving face-based age estimation with attention-based dynamic patch fusion. ||| haoyi wang ||| victor sanchez ||| chang-tsun li ||| 
2020 ||| improving bert with self-supervised attention. ||| xiaoyu kou ||| yaming yang ||| yujing wang ||| ce zhang ||| yiren chen ||| yunhai tong ||| yan zhang ||| jing bai ||| 
2020 ||| weakly supervised training of hierarchical attention networks for speaker identification. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2020 ||| careful analysis of xrd patterns with attention. ||| koichi kano ||| takashi segi ||| hiroshi ozono ||| 
2018 ||| cram: clued recurrent attention model. ||| minki chung ||| sungzoon cho ||| 
2021 ||| generating symbolic reasoning problems with transformer gans. ||| jens u. kreber ||| christopher hahn ||| 
2021 ||| generating coherent and diverse slogans with sequence-to-sequence transformer. ||| yiping jin ||| akshay bhatia ||| dittaya wanvarie ||| phu t. v. le ||| 
2021 ||| video salient object detection via contrastive features and attention modules. ||| yi-wen chen ||| xiaojie jin ||| xiaohui shen ||| ming-hsuan yang ||| 
2022 ||| dualsc: automatic generation and summarization of shellcode via transformer and dual learning. ||| guang yang ||| xiang chen ||| yanlin zhou ||| chi yu ||| 
2021 ||| e images combining self-attention multiple instance learning with a recurrent neural network. ||| esther dietrich ||| patrick fuhlert ||| anne ernst ||| guido sauter ||| maximilian lennartz ||| h. siegfried stiehl ||| marina zimmermann ||| stefan bonn ||| 
2021 ||| improve the interpretability of attention: a fast, accurate, and interpretable high-resolution attention model. ||| tristan gomez ||| suiyi ling ||| thomas fr ||| our ||| harold mouch ||| re ||| 
2020 ||| multi-branch attentive transformer. ||| yang fan ||| shufang xie ||| yingce xia ||| lijun wu ||| tao qin ||| xiang-yang li ||| tie-yan liu ||| 
2022 ||| gateformer: speeding up news feed recommendation with input gated transformers. ||| peitian zhang ||| zheng liu ||| 
2021 ||| discriminative and generative transformer-based models for situation entity classification. ||| mehdi rezaee ||| kasra darvish ||| gaoussou youssouf kebe ||| francis ferraro ||| 
2021 ||| graph attention recurrent neural networks for correlated time series forecasting - full version. ||| razvan-gabriel cirstea ||| chenjuan guo ||| bin yang ||| 
2019 ||| unsupervised image-to-image translation with self-attention networks. ||| taewon kang ||| kwang hee lee ||| 
2020 ||| text segmentation by cross segment attention. ||| michal lukasik ||| boris dadachev ||| gon ||| alo sim ||| es ||| kishore papineni ||| 
2021 ||| non-autoregressive transformer-based end-to-end asr using bert. ||| fu-hao yu ||| kuan-yu chen ||| 
2021 ||| combiner: full attention transformer with sparse computation cost. ||| hongyu ren ||| hanjun dai ||| zihang dai ||| mengjiao yang ||| jure leskovec ||| dale schuurmans ||| bo dai ||| 
2019 ||| correction of automatic speech recognition with transformer sequence-to-sequence model. ||| oleksii hrinchuk ||| mariya popova ||| boris ginsburg ||| 
2017 ||| deep visual attention prediction. ||| wenguan wang ||| jianbing shen ||| 
2021 ||| accurate and clear precipitation nowcasting with consecutive attention and rain-map discrimination. ||| ashesh ||| buo-fu chen ||| treng-shi huang ||| boyo chen ||| chia-tung chang ||| hsuan-tien lin ||| 
2021 ||| vision transformer with progressive sampling. ||| xiaoyu yue ||| shuyang sun ||| zhanghui kuang ||| meng wei ||| philip h. s. torr ||| wayne zhang ||| dahua lin ||| 
2021 ||| indonesia's fake news detection using transformer network. ||| aisyah awalina ||| jibran fawaid ||| rifky yunus krisnabayu ||| novanto yudistira ||| 
2021 ||| boosting crowd counting with transformers. ||| guolei sun ||| yun liu ||| thomas probst ||| danda pani paudel ||| nikola popovic ||| luc van gool ||| 
2019 ||| analysis of various transformer structures for high frequency isolation applications. ||| mohammad saleh sanjarinia ||| sepehr saadatmand ||| pourya shamsi ||| mehdi ferdowsi ||| 
2021 ||| fda-gan: flow-based dual attention gan for human pose transfer. ||| liyuan ma ||| kejie huang ||| dongxu wei ||| zhaoyan ming ||| haibin shen ||| 
2021 ||| geometry-free view synthesis: transformers and no 3d priors. ||| robin rombach ||| patrick esser ||| bj ||| rn ommer ||| 
2021 ||| locally enhanced self-attention: rethinking self-attention as local and context terms. ||| chenglin yang ||| siyuan qiao ||| adam kortylewski ||| alan l. yuille ||| 
2017 ||| a guided spatial transformer network for histology cell differentiation. ||| marc aubreville ||| maximilian krappmann ||| christof bertram ||| robert klopfleisch ||| andreas k. maier ||| 
2021 ||| greenformers: improving computation and memory efficiency in transformer models via low-rank approximation. ||| samuel cahyawijaya ||| 
2018 ||| mask r-cnn with pyramid attention network for scene text detection. ||| zhida huang ||| zhuoyao zhong ||| lei sun ||| qiang huo ||| 
2022 ||| speech denoising in the waveform domain with self-attention. ||| zhifeng kong ||| wei ping ||| ambrish dantrey ||| bryan catanzaro ||| 
2020 ||| gsanet: semantic segmentation with global and selective attention. ||| qingfeng liu ||| mostafa el-khamy ||| dongwoon bai ||| jungwon lee ||| 
2018 ||| veram: view-enhanced recurrent attention model for 3d shape classification. ||| songle chen ||| lintao zheng ||| yan zhang ||| zhixin sun ||| kai xu ||| 
2020 ||| check_square at checkthat! claim detection in social media via fusion of transformer and syntactic features. ||| gullal s. cheema ||| sherzod hakimov ||| ralph ewerth ||| 
2019 ||| aspect category detection via topic-attention network. ||| sajad movahedi ||| erfan ghadery ||| heshaam faili ||| azadeh shakery ||| 
2021 ||| doctr: document image transformer for geometric unwarping and illumination correction. ||| hao feng ||| yuechen wang ||| wengang zhou ||| jiajun deng ||| houqiang li ||| 
2019 ||| graph transformer networks. ||| seongjun yun ||| minbyul jeong ||| raehyun kim ||| jaewoo kang ||| hyunwoo j. kim ||| 
2020 ||| attention augmented differentiable forest for tabular data. ||| yingshi chen ||| 
2021 ||| a spatio-temporal attention-based model for infant movement assessment from videos. ||| binh nguyen-thai ||| vuong le ||| catherine morgan ||| nadia badawi ||| truyen tran ||| svetha venkatesh ||| 
2020 ||| jaa-net: joint facial action unit detection and face alignment via adaptive attention. ||| zhiwen shao ||| zhilei liu ||| jianfei cai ||| lizhuang ma ||| 
2021 ||| multi-context attention fusion neural network for software vulnerability identification. ||| anshul tanwar ||| hariharan manikandan ||| krishna sundaresan ||| prasanna ganesan ||| sathish kumar chandrasekaran ||| sriram ravi ||| 
2020 ||| attention flows: analyzing and comparing attention mechanisms in language models. ||| joseph f. derose ||| jiayao wang ||| matthew berger ||| 
2022 ||| dftr: depth-supervised hierarchical feature fusion transformer for salient object detection. ||| heqin zhu ||| xu sun ||| yuexiang li ||| kai ma ||| s. kevin zhou ||| yefeng zheng ||| 
2019 ||| ctrl: a conditional transformer language model for controllable generation. ||| nitish shirish keskar ||| bryan mccann ||| lav r. varshney ||| caiming xiong ||| richard socher ||| 
2021 ||| uformer: a general u-shaped transformer for image restoration. ||| zhendong wang ||| xiaodong cun ||| jianmin bao ||| jianzhuang liu ||| 
2021 ||| a multi-input multi-output transformer-based hybrid neural network for multi-class privacy disclosure detection. ||| a. k. m. nuhil mehdy ||| hoda mehrpouyan ||| 
2019 ||| learning similarity attention. ||| meng zheng ||| srikrishna karanam ||| terrence chen ||| richard j. radke ||| ziyan wu ||| 
2021 ||| attention temperature matters in abstractive summarization distillation. ||| shengqiang zhang ||| xingxing zhang ||| hangbo bao ||| furu wei ||| 
2021 ||| associating objects with transformers for video object segmentation. ||| zongxin yang ||| yunchao wei ||| yi yang ||| 
2019 ||| low-rank hoca: efficient high-order cross-modal attention for video captioning. ||| tao jin ||| siyu huang ||| yingming li ||| zhongfei zhang ||| 
2020 ||| reproduction of lateral inhibition-inspired convolutional neural network for visual attention and saliency detection. ||| filip marcinek ||| 
2020 ||| dual attention on pyramid feature maps for image captioning. ||| litao yu ||| jian zhang ||| qiang wu ||| 
2021 ||| vision xformers: efficient attention for image classification. ||| pranav jeevan ||| amit sethi ||| 
2021 ||| cma-clip: cross-modality attention clip for image-text classification. ||| huidong liu ||| shaoyuan xu ||| jinmiao fu ||| yang liu ||| ning xie ||| chien-chih wang ||| bryan wang ||| yi sun ||| 
2018 ||| sharp attention network via adaptive sampling for person re-identification. ||| chen shen ||| guo-jun qi ||| rongxin jiang ||| zhongming jin ||| hongwei yong ||| yaowu chen ||| xian-sheng hua ||| 
2020 ||| adaptive transformers for learning multimodal representations. ||| prajjwal bhargava ||| 
2021 ||| mpvit: multi-path vision transformer for dense prediction. ||| youngwan lee ||| jonghee kim ||| jeffrey willette ||| sung ju hwang ||| 
2021 ||| reformer: the relational transformer for image captioning. ||| xuewen yang ||| yingru liu ||| xin wang ||| 
2021 ||| generative video transformer: can objects be the words? ||| yi-fu wu ||| jaesik yoon ||| sungjin ahn ||| 
2019 ||| lattice transformer for speech translation. ||| pei zhang ||| boxing chen ||| niyu ge ||| kai fan ||| 
2021 ||| adavit: adaptive tokens for efficient vision transformer. ||| hongxu yin ||| arash vahdat ||| jose m. alvarez ||| arun mallya ||| jan kautz ||| pavlo molchanov ||| 
2021 ||| transformer-based machine learning for fast sat solvers and logic synthesis. ||| feng shi ||| chonghan lee ||| mohammad khairul bashar ||| nikhil shukla ||| song-chun zhu ||| vijaykrishnan narayanan ||| 
2017 ||| online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. ||| qi chu ||| wanli ouyang ||| hongsheng li ||| xiaogang wang ||| bin liu ||| nenghai yu ||| 
2021 ||| the performance evaluation of attention-based neural asr under mixed speech input. ||| bradley he ||| martin radfar ||| 
2018 ||| pairwise body-part attention for recognizing human-object interactions. ||| haoshu fang ||| jinkun cao ||| yu-wing tai ||| cewu lu ||| 
2022 ||| reconformer: accelerated mri reconstruction using recurrent transformer. ||| pengfei guo ||| yiqun mei ||| jinyuan zhou ||| shanshan jiang ||| vishal m. patel ||| 
2020 ||| hierarchical attention transformer architecture for syntactic spell correction. ||| abhishek niranjan ||| m. ali basha shaik ||| kushal verma ||| 
2020 ||| pdanet: pyramid density-aware attention net for accurate crowd counting. ||| saeed amirgholipour ||| xiangjian he ||| wenjing jia ||| dadong wang ||| lei liu ||| 
2020 ||| pre-training text-to-text transformers for concept-centric common sense. ||| wangchunshu zhou ||| dong-ho lee ||| ravi kiran selvam ||| seyeon lee ||| bill yuchen lin ||| xiang ren ||| 
2020 ||| human activity recognition from wearable sensor data using self-attention. ||| saif mahmud ||| m. tanjid hasan tonmoy ||| kishor kumar bhaumik ||| a k m mahbubur rahman ||| m. ashraful amin ||| mohammad shoyaib ||| muhammad asif hossain khan ||| amin ahsan ali ||| 
2022 ||| vit-fod: a vision transformer based fine-grained object discriminator. ||| zi-chao zhang ||| zhen-duo chen ||| yongxin wang ||| xin luo ||| xin-shun xu ||| 
2019 ||| being the center of attention: a person-context cnn framework for personality recognition. ||| dario dotti ||| mirela popa ||| stylianos asteriadis ||| 
2019 ||| hierarchical attention networks for medical image segmentation. ||| fei ding ||| gang yang ||| jinlu liu ||| jun wu ||| dayong ding ||| jie xu ||| gangwei cheng ||| xirong li ||| 
2021 ||| do we really need explicit position encodings for vision transformers? ||| xiangxiang chu ||| bo zhang ||| zhi tian ||| xiaolin wei ||| huaxia xia ||| 
2019 ||| multi-agent actor-critic with hierarchical graph attention network. ||| heechang ryu ||| hayong shin ||| jinkyoo park ||| 
2020 ||| double multi-head attention for speaker verification. ||| miquel india ||| pooyan safari ||| javier hernando ||| 
2021 ||| an improved single step non-autoregressive transformer for automatic speech recognition. ||| ruchao fan ||| wei chu ||| peng chang ||| jing xiao ||| abeer alwan ||| 
2022 ||| snowflake point deconvolution for point cloud completion and generation with skip-transformer. ||| peng xiang ||| xin wen ||| yu-shen liu ||| yan-pei cao ||| pengfei wan ||| wen zheng ||| zhizhong han ||| 
2020 ||| attention-aware fusion rgb-d face recognition. ||| hardik uppal ||| alireza sepas-moghaddam ||| michael a. greenspan ||| s. ali etemad ||| 
2022 ||| transdreamer: reinforcement learning with transformer world models. ||| chang chen ||| yi-fu wu ||| jaesik yoon ||| sungjin ahn ||| 
2018 ||| training deeper neural machine translation models with transparent attention. ||| ankur bapna ||| mia xu chen ||| orhan firat ||| yuan cao ||| yonghui wu ||| 
2018 ||| molecular transformer for chemical reaction prediction and uncertainty estimation. ||| philippe schwaller ||| teodoro laino ||| th ||| ophile gaudin ||| peter bolgar ||| costas bekas ||| alpha a. lee ||| 
2020 ||| deliberate self-attention network with uncertainty estimation for multi-aspect review rating prediction. ||| tian shi ||| ping wang ||| chandan k. reddy ||| 
2017 ||| reinforcement learning with analogical similarity to guide schema induction and attention. ||| james m. foster ||| matt jones ||| 
2021 ||| sentimentarcs: a novel method for self-supervised sentiment analysis of time series shows sota transformers can struggle finding narrative arcs. ||| jon chun ||| 
2017 ||| end-to-end attention based text-dependent speaker verification. ||| shi-xiong zhang ||| zhuo chen ||| yong zhao ||| jinyu li ||| yifan gong ||| 
2021 ||| beit: bert pre-training of image transformers. ||| hangbo bao ||| li dong ||| furu wei ||| 
2022 ||| m2ts: multi-scale multi-modal approach based on transformer for source code summarization. ||| yuexiu gao ||| chen lyu ||| 
2020 ||| deeprhythm: exposing deepfakes with attentional visual heartbeat rhythms. ||| hua qi ||| qing guo ||| felix juefei-xu ||| xiaofei xie ||| lei ma ||| wei feng ||| yang liu ||| jianjun zhao ||| 
2018 ||| hierarchical spatial transformer network. ||| chang shu ||| xi chen ||| qiwei xie ||| hua han ||| 
2020 ||| data augmentation using pre-trained transformer models. ||| varun kumar ||| ashutosh choudhary ||| eunah cho ||| 
2021 ||| applications of artificial neural networks in microorganism image analysis: a comprehensive review from conventional multilayer perceptron to popular convolutional neural network and potential visual transformer. ||| jinghua zhang ||| chen li ||| marcin grzegorzek ||| 
2020 ||| bert-jam: boosting bert-enhanced neural machine translation with joint attention. ||| zhebin zhang ||| sai wu ||| dawei jiang ||| gang chen ||| 
2021 ||| ladra-net: locally-aware dynamic re-read attention net for sentence semantic matching. ||| kun zhang ||| guangyi lv ||| le wu ||| enhong chen ||| qi liu ||| meng wang ||| 
2021 ||| history aware multimodal transformer for vision-and-language navigation. ||| shizhe chen ||| pierre-louis guhur ||| cordelia schmid ||| ivan laptev ||| 
2021 ||| learning generalizable vision-tactile robotic grasping strategy for deformable objects via transformer. ||| yunhai han ||| rahul batra ||| nathan boyd ||| tuo zhao ||| yu she ||| seth hutchinson ||| ye zhao ||| 
2021 ||| tstnn: two-stage transformer based neural network for speech enhancement in the time domain. ||| kai wang ||| bengbeng he ||| wei-ping zhu ||| 
2021 ||| a trained humanoid robot can perform human-like crossmodal social attention conflict resolution. ||| di fu ||| fares abawi ||| hugo carneiro ||| matthias kerzel ||| ziwei chen ||| erik strahl ||| xun liu ||| stefan wermter ||| 
2021 ||| vut: versatile ui transformer for multi-modal multi-task user interface modeling. ||| yang li ||| gang li ||| xin zhou ||| mostafa dehghani ||| alexey a. gritsenko ||| 
2020 ||| improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. ||| zhe li ||| lianwen jin ||| songxuan lai ||| yecheng zhu ||| 
2017 ||| drug-drug interaction extraction via recurrent neural network with multiple attention layers. ||| zibo yi ||| shasha li ||| jie yu ||| qingbo wu ||| 
2020 ||| mu-gan: facial attribute editing based on multi-attention mechanism. ||| ke zhang ||| yukun su ||| xiwang guo ||| liang qi ||| zhenbing zhao ||| 
2021 ||| scene transformer: a unified multi-task model for behavior prediction and planning. ||| jiquan ngiam ||| benjamin caine ||| vijay vasudevan ||| zhengdong zhang ||| hao-tien lewis chiang ||| jeffrey ling ||| rebecca roelofs ||| alex bewley ||| chenxi liu ||| ashish venugopal ||| david weiss ||| benjamin sapp ||| zhifeng chen ||| jonathon shlens ||| 
2021 ||| patentminer: patent vacancy mining via context-enhanced and knowledge-guided graph attention. ||| gaochen wu ||| bin xu ||| yuxin qin ||| fei kong ||| bangchang liu ||| hongwen zhao ||| dejie chang ||| 
2021 ||| bitfit: simple parameter-efficient fine-tuning for transformer-based masked language-models. ||| elad ben zaken ||| shauli ravfogel ||| yoav goldberg ||| 
2019 ||| efficient 8-bit quantization of transformer neural machine language translation model. ||| aishwarya bhandare ||| vamsi sripathi ||| deepthi karkada ||| vivek menon ||| sun choi ||| kushal datta ||| vikram saletore ||| 
2022 ||| learning class prototypes from synthetic insar with vision transformers. ||| nikolaos-ioannis bountos ||| dimitrios michail ||| ioannis papoutsis ||| 
2021 ||| syntax-aware graph-to-graph transformer for semantic role labelling. ||| alireza mohammadshahi ||| james henderson ||| 
2017 ||| textual entailment with structured attentions and composition. ||| kai zhao ||| liang huang ||| mingbo ma ||| 
2020 ||| vsgnet: spatial attention network for detecting human object interactions using graph convolutions. ||| oytun ulutan ||| a. s. m. iftekhar ||| bangalore s. manjunath ||| 
2022 ||| dynamic n: m fine-grained structured sparse attention mechanism. ||| zhaodong chen ||| yuying quan ||| zheng qu ||| liu liu ||| yufei ding ||| yuan xie ||| 
2019 ||| ultrasound image representation learning by modeling sonographer visual attention. ||| richard droste ||| yifan cai ||| harshita sharma ||| pierre chatelain ||| lior drukker ||| aris t. papageorghiou ||| j. alison noble ||| 
2021 ||| attention mechanisms in computer vision: a survey. ||| meng-hao guo ||| tian-xing xu ||| jiangjiang liu ||| zheng-ning liu ||| peng-tao jiang ||| tai-jiang mu ||| song-hai zhang ||| ralph r. martin ||| ming-ming cheng ||| shi-min hu ||| 
2020 ||| automated labelling using an attention model for radiology reports of mri scans (alarm). ||| david a. wood ||| jeremy lynch ||| sina kafiabadi ||| emily guilhem ||| aisha al busaidi ||| antanas montvila ||| thomas varsavsky ||| juveria siddiqui ||| naveen gadapa ||| matthew townend ||| martin kiik ||| keena patel ||| gareth j. barker ||| s ||| bastien ourselin ||| james h. cole ||| thomas c. booth ||| 
2022 ||| abductionrules: training transformers to explain unexpected inputs. ||| nathan young ||| qiming bao ||| joshua bensemann ||| michael witbrock ||| 
2020 ||| simplified self-attention for transformer-based end-to-end speech recognition. ||| haoneng luo ||| shiliang zhang ||| ming lei ||| lei xie ||| 
2019 ||| attentionboost: learning what to attend by boosting fully convolutional networks. ||| gozde nur gunesli ||| cenk sokmensuer ||| cigdem gunduz demir ||| 
2022 ||| sguie-net: semantic attention guided underwater image enhancement with multi-scale perception. ||| qi qi ||| kunqian li ||| haiyong zheng ||| xiang gao ||| guojia hou ||| kun sun ||| 
2019 ||| spatial transformer for 3d points. ||| jiayun wang ||| rudrasis chakraborty ||| stella x. yu ||| 
2019 ||| synthesising expressiveness in peking opera via duration informed attention network. ||| yusong wu ||| shengchen li ||| chengzhu yu ||| heng lu ||| chao weng ||| liqiang zhang ||| dong yu ||| 
2021 ||| unsupervised visual attention and invariance for reinforcement learning. ||| xudong wang ||| long lian ||| stella x. yu ||| 
2021 ||| attention-guided supervised contrastive learning for semantic segmentation. ||| ho hin lee ||| yucheng tang ||| qi yang ||| xin yu ||| shunxing bao ||| bennett a. landman ||| yuankai huo ||| 
2021 ||| pay better attention to attention: head selection in multilingual and multi-domain sequence modeling. ||| hongyu gong ||| yun tang ||| juan miguel pino ||| xian li ||| 
2020 ||| faster transformer decoding: n-gram masked self-attention. ||| ciprian chelba ||| mia xu chen ||| ankur bapna ||| noam shazeer ||| 
2019 ||| deep attention based semi-supervised 2d-pose estimation for surgical instruments. ||| mert kayhan ||| okan k ||| p ||| kl ||| mhd hasan sarhan ||| mehmet yigitsoy ||| abouzar eslami ||| gerhard rigoll ||| 
2020 ||| linking social media posts to news with siamese transformers. ||| jacob danovitch ||| 
2022 ||| adaptive cross-layer attention for image restoration. ||| yancheng wang ||| ning xu ||| chong chen ||| yingzhen yang ||| 
2020 ||| low rank fusion based transformers for multimodal sequences. ||| saurav sahay ||| eda okur ||| shachi h. kumar ||| lama nachman ||| 
2021 ||| temporal attention augmented transformer hawkes process. ||| lu-ning zhang ||| jian-wei liu ||| zhi-yan song ||| xin zuo ||| 
2021 ||| dense dual-attention network for light field image super-resolution. ||| yu mo ||| yingqian wang ||| chao xiao ||| jungang yang ||| wei an ||| 
2021 ||| improving users' mental model with attention-directed counterfactual edits. ||| kamran alipour ||| arijit ray ||| xiao lin ||| michael cogswell ||| j ||| rgen p. schulze ||| yi yao ||| giedrius t. burachas ||| 
2020 ||| spatio-temporal point processes with attention for traffic congestion event modeling. ||| shixiang zhu ||| ruyi ding ||| minghe zhang ||| pascal van hentenryck ||| yao xie ||| 
2018 ||| supervised domain enablement attention for personalized domain classification. ||| joo-kyung kim ||| young-bum kim ||| 
2019 ||| inverse visual question answering with multi-level attentions. ||| yaser alwatter ||| yuhong guo ||| 
2019 ||| attention forcing for sequence-to-sequence model training. ||| qingyun dou ||| yiting lu ||| joshua efiong ||| mark j. f. gales ||| 
2019 ||| multi-head multi-layer attention to deep language representations for grammatical error detection. ||| masahiro kaneko ||| mamoru komachi ||| 
2021 ||| should i look at the head or the tail? dual-awareness attention for few-shot object detection. ||| tung-i chen ||| yueh-cheng liu ||| hung-ting su ||| yu-cheng chang ||| yu-hsiang lin ||| jia-fong yeh ||| winston h. hsu ||| 
2020 ||| enhance multimodal transformer with external label and in-domain pretrain: hateful meme challenge winning solution. ||| ron zhu ||| 
2021 ||| dsgpt: domain-specific generative pre-training of transformers for text generation in e-commerce title and review summarization. ||| xueying zhang ||| yunjiang jiang ||| yue shang ||| zhaomeng cheng ||| chi zhang ||| xiaochuan fan ||| yun xiao ||| bo long ||| 
2020 ||| end-to-end object detection with adaptive clustering transformer. ||| minghang zheng ||| peng gao ||| xiaogang wang ||| hongsheng li ||| hao dong ||| 
2021 ||| medical transformer: universal brain encoder for 3d mri analysis. ||| eunji jun ||| seungwoo jeong ||| da-woon heo ||| heung-il suk ||| 
2020 ||| sentibert: a transferable transformer-based architecture for compositional sentiment semantics. ||| da yin ||| tao meng ||| kai-wei chang ||| 
2019 ||| audio2face: generating speech/face animation from single audio with attention-based bidirectional lstm networks. ||| guanzhong tian ||| yi yuan ||| yong liu ||| 
2021 ||| shifted chunk transformer for spatio-temporal representational learning. ||| xuefan zha ||| wentao zhu ||| tingxun lv ||| sen yang ||| ji liu ||| 
2021 ||| surgical instruction generation with transformers. ||| jinglu zhang ||| yinyu nie ||| jian chang ||| jian-jun zhang ||| 
2021 ||| a cnn-bilstm model with attention mechanism for earthquake prediction. ||| parisa kavianpour ||| mohammadreza kavianpour ||| ehsan jahani ||| amin ramezani ||| 
2021 ||| multimodal transformer with variable-length memory for vision-and-language navigation. ||| chuang lin ||| yi jiang ||| jianfei cai ||| lizhen qu ||| gholamreza haffari ||| zehuan yuan ||| 
2021 ||| channel-wise attention-based network for self-supervised monocular depth estimation. ||| jiaxing yan ||| hong zhao ||| penghui bu ||| yusheng jin ||| 
2020 ||| deep attention aware feature learning for person re-identification. ||| yifan chen ||| han wang ||| xiaolu sun ||| bin fan ||| chu tang ||| 
2018 ||| ea-lstm: evolutionary attention-based lstm for time series prediction. ||| youru li ||| zhenfeng zhu ||| deqiang kong ||| hua han ||| yao zhao ||| 
2019 ||| infant brain mri segmentation with dilated convolution pyramid downsampling and self-attention. ||| zhihao lei ||| lin qi ||| ying wei ||| yunlong zhou ||| yunzhou zhang ||| 
2021 ||| symmetry-enhanced attention network for acute ischemic infarct segmentation with non-contrast ct images. ||| kongming liang ||| kai han ||| xiuli li ||| xiaoqing cheng ||| yiming li ||| yizhou wang ||| yizhou yu ||| 
2019 ||| estimating attention flow in online video networks. ||| siqi wu ||| marian-andrei rizoiu ||| lexing xie ||| 
2020 ||| avr: attention based salient visual relationship detection. ||| jianming lv ||| qinzhe xiao ||| jiajie zhong ||| 
2021 ||| bert transformer model for detecting arabic gpt2 auto-generated tweets. ||| fouzi harrag ||| maria debbah ||| kareem darwish ||| ahmed abdelali ||| 
2019 ||| path-augmented graph transformer network. ||| benson chen ||| regina barzilay ||| tommi s. jaakkola ||| 
2021 ||| moca: incorporating multi-stage domain pretraining and cross-guided multimodal attention for textbook question answering. ||| fangzhi xu ||| qika lin ||| jun liu ||| lingling zhang ||| tianzhe zhao ||| qi chai ||| yudai pan ||| 
2019 ||| giving attention to the unexpected: using prosody innovations in disfluency detection. ||| vicky zayats ||| mari ostendorf ||| 
2022 ||| attentionhtr: handwritten text recognition based on attention encoder-decoder networks. ||| dmitrijs kass ||| ekta vats ||| 
2018 ||| a collaborative computer aided diagnosis (c-cad) system with eye-tracking, sparse attentional model, and deep learning. ||| naji khosravan ||| haydar celik ||| baris turkbey ||| elizabeth jones ||| bradford j. wood ||| ulas bagci ||| 
2021 ||| long-short transformer: efficient transformers for language and vision. ||| chen zhu ||| wei ping ||| chaowei xiao ||| mohammad shoeybi ||| tom goldstein ||| anima anandkumar ||| bryan catanzaro ||| 
2021 ||| luna: linear unified nested attention. ||| xuezhe ma ||| xiang kong ||| sinong wang ||| chunting zhou ||| jonathan may ||| hao ma ||| luke zettlemoyer ||| 
2021 ||| topical language generation using transformers. ||| rohola zandie ||| mohammad h. mahoor ||| 
2021 ||| utnlp at semeval-2021 task 5: a comparative analysis of toxic span detection using attention-based, named entity recognition, and ensemble models. ||| alireza salemi ||| nazanin sabri ||| emad kebriaei ||| behnam bahrak ||| azadeh shakery ||| 
2019 ||| microsoft ai challenge india 2018: learning to rank passages for web question answering with deep attention networks. ||| chaitanya sai alaparthi ||| 
2017 ||| discrete choice and rational inattention: a general equivalence result. ||| mogens fosgerau ||| emerson melo ||| andr |||  de palma ||| matthew shum ||| 
2019 ||| modulated self-attention convolutional network for vqa. ||| jean-benoit delbrouck ||| antoine maiorca ||| nathan hubens ||| st ||| phane dupont ||| 
2021 ||| transformer meets convolution: a bilateral awareness net-work for semantic segmentation of very fine resolution ur-ban scene images. ||| libo wang ||| rui li ||| dongzhi wang ||| chenxi duan ||| teng wang ||| xiaoliang meng ||| 
2020 ||| sit3: code summarization with structure-induced transformer. ||| hongqiu wu ||| hai zhao ||| min zhang ||| 
2019 ||| towards online end-to-end transformer automatic speech recognition. ||| emiru tsunoo ||| yosuke kashiwagi ||| toshiyuki kumakura ||| shinji watanabe ||| 
2019 ||| i-mad: a novel interpretable malware detector using hierarchical transformer. ||| miles q. li ||| benjamin c. m. fung ||| philippe charland ||| steven h. h. ding ||| 
2018 ||| attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ||| yuxing tang ||| xiaosong wang ||| adam p. harrison ||| le lu ||| jing xiao ||| ronald m. summers ||| 
2021 ||| simple attention module based speaker verification with iterative noisy label detection. ||| xiaoyi qin ||| na li ||| chao weng ||| dan su ||| ming li ||| 
2019 ||| attention privileged reinforcement learning for domain transfer. ||| sasha salter ||| dushyant rao ||| markus wulfmeier ||| raia hadsell ||| ingmar posner ||| 
2021 ||| transformers to fight the covid-19 infodemic. ||| lasitha uyangodage ||| tharindu ranasinghe ||| hansi hettiarachchi ||| 
2019 ||| hierarchical transformers for multi-document summarization. ||| yang liu ||| mirella lapata ||| 
2019 ||| vssa-net: vertical spatial sequence attention network for traffic sign detection. ||| yuan yuan ||| zhitong xiong ||| qi wang ||| 
2021 ||| explainable identification of dementia from transcripts using transformer networks. ||| loukas ilias ||| dimitris askounis ||| 
2020 ||| two stage transformer model for covid-19 fake news detection and fact checking. ||| rutvik vijjali ||| prathyush potluri ||| siddharth kumar ||| sundeep teki ||| 
2018 ||| dual attention network for scene segmentation. ||| jun fu ||| jing liu ||| haijie tian ||| zhiwei fang ||| hanqing lu ||| 
2021 ||| a separable temporal convolution neural network with attention for small-footprint keyword spotting. ||| shenghua hu ||| jing wang ||| yujun wang ||| lidong yang ||| wenjing yang ||| 
2021 ||| patch slimming for efficient vision transformers. ||| yehui tang ||| kai han ||| yunhe wang ||| chang xu ||| jianyuan guo ||| chao xu ||| dacheng tao ||| 
2019 ||| ssa-cnn: semantic self-attention cnn for pedestrian detection. ||| 
2021 ||| scene representation transformer: geometry-free novel view synthesis through set-latent scene representations. ||| mehdi s. m. sajjadi ||| henning meyer ||| etienne pot ||| urs bergmann ||| klaus greff ||| noha radwan ||| suhani vora ||| mario lucic ||| daniel duckworth ||| alexey dosovitskiy ||| jakob uszkoreit ||| thomas a. funkhouser ||| andrea tagliasacchi ||| 
2019 ||| self-attention graph pooling. ||| junhyun lee ||| inyeop lee ||| jaewoo kang ||| 
2021 ||| linear-time self attention with codeword histogram for efficient recommendation. ||| yongji wu ||| defu lian ||| neil zhenqiang gong ||| lu yin ||| mingyang yin ||| jingren zhou ||| hongxia yang ||| 
2018 ||| micro-attention for micro-expression recognition. ||| chongyang wang ||| min peng ||| tao bi ||| tong chen ||| 
2019 ||| multimodal semantic attention network for video captioning. ||| liang sun ||| bing li ||| chunfeng yuan ||| zhengjun zha ||| weiming hu ||| 
2020 ||| transformer-based neural text generation with syntactic guidance. ||| yinghao li ||| rui feng ||| isaac rehg ||| chao zhang ||| 
2022 ||| lawin transformer: improving semantic segmentation transformer with multi-scale representations via large window attention. ||| haotian yan ||| chuang zhang ||| ming wu ||| 
2020 ||| mischief: a simple black-box attack against transformer architectures. ||| adrian de wynter ||| 
2020 ||| long-short term masking transformer: a simple but effective baseline for document-level neural machine translation. ||| pei zhang ||| boxing chen ||| niyu ge ||| kai fan ||| 
2021 ||| dsc-iitism at fincausal 2021: combining pos tagging with attention-based contextual representations for identifying causal relationships in financial documents. ||| gunjan haldar ||| aman mittal ||| pradyumna gupta ||| 
2019 ||| image super-resolution using attention based densenet with residual deconvolution. ||| zhuangzi li ||| 
2021 ||| pt-vton: an image-based virtual try-on network with progressive pose attention transfer. ||| hanhan zhou ||| tian lan ||| guru venkataramani ||| 
2020 ||| self-attention aggregation network for video face representation and recognition. ||| ihor protsenko ||| taras lehinevych ||| dmytro voitekh ||| ihor kroosh ||| nick hasty ||| anthony johnson ||| 
2021 ||| improved context-based offline meta-rl with attention and contrastive learning. ||| lanqing li ||| yuanhao huang ||| dijun luo ||| 
2021 ||| multi-density attention network for loop filtering in video compression. ||| zhao wang ||| changyue ma ||| yan ye ||| 
2019 ||| second-order non-local attention networks for person re-identification. ||| bryan (ning) xia ||| yuan gong ||| yizhe zhang ||| christian poellabauer ||| 
2019 ||| slices of attention in asynchronous video job interviews. ||| l ||| o hemamou ||| ghazi felhi ||| jean-claude martin ||| chlo |||  clavel ||| 
2019 ||| dependency-aware attention control for unconstrained face recognition with image sets. ||| xiaofeng liu ||| b. v. k. vijaya kumar ||| chao yang ||| qingming tang ||| jane you ||| 
2020 ||| unsupervised ct metal artifact learning using attention-guided beta-cyclegan. ||| junghyun lee ||| jawook gu ||| jong chul ye ||| 
2021 ||| xeroalign: zero-shot cross-lingual transformer alignment. ||| milan gritta ||| ignacio iacobacci ||| 
2021 ||| peta: photo albums event recognition using transformers attention. ||| tamar gl ||| aser ||| emanuel ben baruch ||| gilad sharir ||| nadav zamir ||| asaf noy ||| lihi zelnik-manor ||| 
2022 ||| knowledge amalgamation for object detection with transformers. ||| haofei zhang ||| feng mao ||| mengqi xue ||| gongfan fang ||| zunlei feng ||| jie song ||| mingli song ||| 
2021 ||| distance and hop-wise structures encoding enhanced graph attention networks. ||| zhiguo huang ||| xiaowei chen ||| bojuan wang ||| 
2021 ||| graformer: graph convolution transformer for 3d pose estimation. ||| weixi zhao ||| yunjie tian ||| qixiang ye ||| jianbin jiao ||| weiqiang wang ||| 
2020 ||| enriched pre-trained transformers for joint slot filling and intent detection. ||| momchil hardalov ||| ivan koychev ||| preslav nakov ||| 
2019 ||| multi-turn dialogue response generation with autoregressive transformer models. ||| oluwatobi olabiyi ||| erik t. mueller ||| 
2021 ||| atrous residual interconnected encoder to attention decoder framework for vertebrae segmentation via 3d volumetric ct images. ||| wenqiang li ||| yuk-ming tang ||| ziyang wang ||| kai ming yu ||| sandy to ||| 
2021 ||| cross-attention conformer for context modeling in speech enhancement for asr. ||| arun narayanan ||| chung-cheng chiu ||| tom o'malley ||| quan wang ||| yanzhang he ||| 
2021 ||| multimodal integration of human-like attention in visual question answering. ||| ekta sood ||| fabian k ||| gel ||| philipp m ||| ller ||| dominike thomas ||| mihai bace ||| andreas bulling ||| 
2020 ||| magnet: multi-region attention-assisted grounding of natural language queries at phrase level. ||| amar shrestha ||| krittaphat pugdeethosapol ||| haowen fang ||| qinru qiu ||| 
2019 ||| automatic prostate zonal segmentation using fully convolutional network with feature pyramid attention. ||| yongkai liu ||| guang yang ||| sohrab afshari mirak ||| melina hosseiny ||| afshin azadikhah ||| xinran zhong ||| robert e. reiter ||| yeejin lee ||| steven s. raman ||| kyung hyun sung ||| 
2017 ||| disan: directional self-attention network for rnn/cnn-free language understanding. ||| tao shen ||| tianyi zhou ||| guodong long ||| jing jiang ||| shirui pan ||| chengqi zhang ||| 
2021 ||| multimodal incremental transformer with visual grounding for visual dialogue generation. ||| feilong chen ||| fandong meng ||| xiuyi chen ||| peng li ||| jie zhou ||| 
2020 ||| pymt5: multi-mode translation of natural language and python code with transformers. ||| colin b. clement ||| dawn drain ||| jonathan timcheck ||| alexey svyatkovskiy ||| neel sundaresan ||| 
2021 ||| few-shot domain adaptation with polymorphic transformers. ||| shaohua li ||| xiuchao sui ||| jie fu ||| huazhu fu ||| xiangde luo ||| yangqin feng ||| xinxing xu ||| yong liu ||| daniel shu wei ting ||| rick siow mong goh ||| 
2021 ||| attentional meta-learners are polythetic classifiers. ||| ben day ||| ram ||| n vi ||| as ||| nikola simidjievski ||| pietro li ||| 
2020 ||| smyrf: efficient attention using asymmetric clustering. ||| giannis daras ||| nikita kitaev ||| augustus odena ||| alexandros g. dimakis ||| 
2022 ||| visual attention prediction improves performance of autonomous drone racing agents. ||| christian pfeiffer ||| simon wengeler ||| antonio loquercio ||| davide scaramuzza ||| 
2021 ||| pruning self-attentions into convolutional layers in single path. ||| haoyu he ||| jing liu ||| zizheng pan ||| jianfei cai ||| jing zhang ||| dacheng tao ||| bohan zhuang ||| 
2019 ||| controllable attention for structured layered video decomposition. ||| jean-baptiste alayrac ||| jo ||| o carreira ||| relja arandjelovic ||| andrew zisserman ||| 
2020 ||| structured self-attention weights encode semantics in sentiment analysis. ||| zhengxuan wu ||| thanh-son nguyen ||| desmond c. ong ||| 
2021 ||| sda-gan: unsupervised image translation using spectral domain attention-guided generative adversarial network. ||| qizhou wang ||| maksim makarenko ||| 
2021 ||| improved drug-target interaction prediction with intermolecular graph transformer. ||| siyuan liu ||| yusong wang ||| tong wang ||| yifan deng ||| liang he ||| bin shao ||| jian yin ||| nanning zheng ||| tie-yan liu ||| 
2019 ||| bsdar: beam search decoding with attention reward in neural keyphrase generation. ||| iftitahu ni'mah ||| vlado menkovski ||| mykola pechenizkiy ||| 
2020 ||| transformer feed-forward layers are key-value memories. ||| mor geva ||| roei schuster ||| jonathan berant ||| omer levy ||| 
2021 ||| multi-person extreme motion prediction with cross-interaction attention. ||| wen guo ||| xiaoyu bie ||| xavier alameda-pineda ||| francesc moreno-noguer ||| 
2021 ||| transformer network for significant stenosis detection in ccta of coronary arteries. ||| xinghua ma ||| gongning luo ||| wei wang ||| kuanquan wang ||| 
2021 ||| vilt: vision-and-language transformer without convolution or region supervision. ||| wonjae kim ||| bokyung son ||| ildoo kim ||| 
2021 ||| ds-net++: dynamic weight slicing for efficient inference in cnns and transformers. ||| changlin li ||| guangrun wang ||| bing wang ||| xiaodan liang ||| zhihui li ||| xiaojun chang ||| 
2022 ||| scala: accelerating adaptation of pre-trained transformer-based language models via efficient large-batch adversarial noise. ||| minjia zhang ||| uma-naresh niranjan ||| yuxiong he ||| 
2020 ||| when can self-attention be replaced by feed forward layers? ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2021 ||| local memory attention for fast video semantic segmentation. ||| matthieu paul ||| martin danelljan ||| luc van gool ||| radu timofte ||| 
2020 ||| efficient content-based sparse attention with routing transformers. ||| aurko roy ||| mohammad saffar ||| ashish vaswani ||| david grangier ||| 
2021 ||| delving deep into the generalization of vision transformers under distribution shifts. ||| chongzhi zhang ||| mingyuan zhang ||| shanghang zhang ||| daisheng jin ||| qiang zhou ||| zhongang cai ||| haiyu zhao ||| shuai yi ||| xianglong liu ||| ziwei liu ||| 
2019 ||| graph convolutional transformer: learning the graphical structure of electronic health records. ||| edward choi ||| zhen xu ||| yujia li ||| michael w. dusenberry ||| gerardo flores ||| yuan xue ||| andrew m. dai ||| 
2021 ||| ear-net: error attention refining network for retinal vessel segmentation. ||| jun wang ||| xiaohan yu ||| yongsheng gao ||| 
2021 ||| test-time personalization with a transformer for human pose estimation. ||| miao hao ||| yizhuo li ||| zonglin di ||| nitesh b. gundavarapu ||| xiaolong wang ||| 
2021 ||| extracting the locus of attention at a cocktail party from single-trial eeg using a joint cnn-lstm model. ||| ivine kuruvila ||| jan muncke ||| eghart fischer ||| ulrich hoppe ||| 
2020 ||| balancing cost and benefit with tied-multi transformers. ||| raj dabre ||| raphael rubino ||| atsushi fujita ||| 
2021 ||| recurrent attention models with object-centric capsule representation for multi-object recognition. ||| hossein adeli ||| seoyoung ahn ||| gregory j. zelinsky ||| 
2020 ||| tan-ntm: topic attention networks for neural topic modeling. ||| madhur panwar ||| shashank shailabh ||| milan aggarwal ||| balaji krishnamurthy ||| 
2020 ||| attention-based lstm network for covid-19 clinical trial parsing. ||| xiong liu ||| luca a. finelli ||| greg l. hersch ||| iya khalil ||| 
2021 ||| improved robustness of vision transformer via prelayernorm in patch embedding. ||| bum jun kim ||| hyeyeon choi ||| hyeonah jang ||| dong gu lee ||| wonseok jeong ||| sang woo kim ||| 
2019 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| zhichao zhang ||| shugong xu ||| tianhao qiao ||| shunqing zhang ||| shan cao ||| 
2021 ||| gacan: graph attention-convolution-attention networks for traffic forecasting based on multi-granularity time series. ||| sikai zhang ||| hong zheng ||| hongyi su ||| bo yan ||| jiamou liu ||| song yang ||| 
2021 ||| hybrid self-attention neat: a novel evolutionary approach to improve the neat algorithm. ||| saman khamesian ||| hamed malek ||| 
2017 ||| attention transfer from web images for video recognition. ||| junnan li ||| yongkang wong ||| qi zhao ||| mohan s. kankanhalli ||| 
2020 ||| interpretable crowd flow prediction with spatial-temporal self-attention. ||| haoxing lin ||| weijia jia ||| yongjian you ||| yiping sun ||| 
2022 ||| vit-hgr: vision transformer-based hand gesture recognition from high density surface emg signals. ||| mansooreh montazerin ||| soheil zabihi ||| elahe rahimian ||| arash mohammadi ||| farnoosh naderkhani ||| 
2021 ||| attention based sequence to sequence learning for machine translation of low resourced indic languages - a case of sanskrit to hindi. ||| vishvajit bakarola ||| jitendra nasriwala ||| 
2021 ||| transformer-based multi-task learning for disaster tweet categorisation. ||| congcong wang ||| paul nulty ||| david lillis ||| 
2020 ||| multi-source attention for unsupervised domain adaptation. ||| xia cui ||| danushka bollegala ||| 
2020 ||| the ioa system for deep noise suppression challenge using a framework combining dynamic attention and recursive learning. ||| andong li ||| chengshi zheng ||| renhua peng ||| linjuan cheng ||| xiaodong li ||| 
2018 ||| textually guided ranking network for attentional image retweet modeling. ||| zhou zhao ||| hanbing zhan ||| lingtao meng ||| jun xiao ||| jun yu ||| min yang ||| fei wu ||| deng cai ||| 
2020 ||| mba-raingan: multi-branch attention generative adversarial network for mixture of rain removal from single images. ||| yiyang shen ||| yidan feng ||| sen deng ||| dong liang ||| jing qin ||| haoran xie ||| mingqiang wei ||| 
2021 ||| crash report data analysis for creating scenario-wise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks. ||| yu li ||| muhammad monjurul karim ||| ruwen qin ||| 
2021 ||| contnet: why not use convolution and transformer at the same time? ||| haotian yan ||| zhe li ||| weijian li ||| changhu wang ||| ming wu ||| chuang zhang ||| 
2022 ||| metamorph: learning universal controllers with transformers. ||| agrim gupta ||| linxi fan ||| surya ganguli ||| li fei-fei ||| 
2021 ||| meningioma segmentation in t1-weighted mri leveraging global context and attention mechanisms. ||| david bouget ||| andr |||  pedersen ||| sayied abdol mohieb hosainey ||| ole solheim ||| ingerid reinertsen ||| 
2022 ||| a hybrid 2-stage vision transformer for ai-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies. ||| yujin oh ||| go eun bae ||| kyung-hee kim ||| min-kyung yeo ||| jong chul ye ||| 
2018 ||| sparse and constrained attention for neural machine translation. ||| chaitanya malaviya ||| pedro ferreira ||| andr |||  f. t. martins ||| 
2021 ||| grid partitioned attention: efficient transformerapproximation with inductive bias for high resolution detail generation. ||| nikolay jetchev ||| g ||| khan yildirim ||| christian bracher ||| roland vollgraf ||| 
2021 ||| a2p-mann: adaptive attention inference hops pruned memory-augmented neural networks. ||| mohsen ahmadzadeh ||| mehdi kamal ||| ali afzali-kusha ||| massoud pedram ||| 
2019 ||| dianet: bert and hierarchical attention multi-task learning of fine-grained dialect. ||| muhammad abdul-mageed ||| chiyu zhang ||| abdelrahim a. elmadany ||| arun rajendran ||| lyle ungar ||| 
2022 ||| monodetr: depth-aware transformer for monocular 3d object detection. ||| renrui zhang ||| han qiu ||| tai wang ||| xuanzhuo xu ||| ziyu guo ||| yu qiao ||| peng gao ||| hongsheng li ||| 
2021 ||| nlpbk at vlsp-2020 shared task: compose transformer pretrained models for reliable intelligence identification on social network. ||| thanh-chinh nguyen ||| van nha nguyen ||| 
2019 ||| residual attention based network for hand bone age assessment. ||| eric wu ||| bin kong ||| xin wang ||| junjie bai ||| yi lu ||| feng gao ||| shaoting zhang ||| kunlin cao ||| qi song ||| siwei lyu ||| youbing yin ||| 
2021 ||| the stem cell hypothesis: dilemma behind multi-task learning with transformer encoders. ||| han he ||| jinho d. choi ||| 
2021 ||| efficient softmax approximation for deep neural networks with attention mechanism. ||| ihor vasyltsov ||| wooseok chang ||| 
2020 ||| improving sequence tagging for vietnamese text using transformer-based neural models. ||| the viet bui ||| oanh thi tran ||| phuong le-hong ||| 
2021 ||| sea ice forecasting using attention-based ensemble lstm. ||| sahara ali ||| yiyi huang ||| xin huang ||| jianwu wang ||| 
2022 ||| what to hide from your students: attention-guided masked image modeling. ||| ioannis kakogeorgiou ||| spyros gidaris ||| bill psomas ||| yannis avrithis ||| andrei bursuc ||| konstantinos karantzalos ||| nikos komodakis ||| 
2021 ||| real-time prediction for mechanical ventilation in covid-19 patients using a multi-task gaussian process multi-objective self-attention network. ||| kai zhang ||| siddharth karanth ||| bela patel ||| robert murphy ||| xiaoqian jiang ||| 
2020 ||| mhsa-net: multi-head self-attention network for occluded person re-identification. ||| hongchen tan ||| xiuping liu ||| shengjing tian ||| baocai yin ||| xin li ||| 
2020 ||| text-to-image generation grounded by fine-grained user attention. ||| jing yu koh ||| jason baldridge ||| honglak lee ||| yinfei yang ||| 
2021 ||| cycletransgan-evc: a cyclegan-based emotional voice conversion model with transformer. ||| changzeng fu ||| chaoran liu ||| carlos toshinori ishi ||| hiroshi ishiguro ||| 
2020 ||| end-to-end contextual perception and prediction with interaction transformer. ||| lingyun luke li ||| bin yang ||| ming liang ||| wenyuan zeng ||| mengye ren ||| sean segal ||| raquel urtasun ||| 
2019 ||| what does bert look at? an analysis of bert's attention. ||| kevin clark ||| urvashi khandelwal ||| omer levy ||| christopher d. manning ||| 
2019 ||| bidirectional context-aware hierarchical attention network for document understanding. ||| jean-baptiste remy ||| antoine jean-pierre tixier ||| michalis vazirgiannis ||| 
2020 ||| reinforced medical report generation with x-linear attention and repetition penalty. ||| wenting xu ||| chang qi ||| zhenghua xu ||| thomas lukasiewicz ||| 
2020 ||| character region attention for text spotting. ||| youngmin baek ||| seung shin ||| jeonghun baek ||| sungrae park ||| junyeop lee ||| daehyun nam ||| hwalsuk lee ||| 
2020 ||| conditional set generation with transformers. ||| adam r. kosiorek ||| hyunjik kim ||| danilo j. rezende ||| 
2020 ||| image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. ||| yiqun mei ||| yuchen fan ||| yuqian zhou ||| lichao huang ||| thomas s. huang ||| humphrey shi ||| 
2021 ||| keyword transformer: a self-attention model for keyword spotting. ||| axel berg ||| mark o'connor ||| miguel tairum cruz ||| 
2020 ||| toward tag-free aspect based sentiment analysis: a multiple attention network approach. ||| yao qiang ||| xin li ||| dongxiao zhu ||| 
2022 ||| maskgit: masked generative image transformer. ||| huiwen chang ||| han zhang ||| lu jiang ||| ce liu ||| william t. freeman ||| 
2021 ||| gaussian kernelized self-attention for long sequence data and its application to ctc-based speech recognition. ||| yosuke kashiwagi ||| emiru tsunoo ||| shinji watanabe ||| 
2019 ||| on recognizing texts of arbitrary shapes with 2d self-attention. ||| junyeop lee ||| sungrae park ||| jeonghun baek ||| seong joon oh ||| seonghyeon kim ||| hwalsuk lee ||| 
2022 ||| mutual generative transformer learning for cross-view geo-localization. ||| jianwei zhao ||| qiang zhai ||| rui huang ||| hong cheng ||| 
2021 ||| open-domain conversational search assistant with transformers. ||| rafael ferreira ||| mariana leite ||| david semedo ||| jo ||| o magalh ||| es ||| 
2020 ||| feature importance estimation with self-attention networks. ||| blaz skrlj ||| saso dzeroski ||| nada lavrac ||| matej petkovic ||| 
2021 ||| improve vision transformers training by suppressing over-smoothing. ||| chengyue gong ||| dilin wang ||| meng li ||| vikas chandra ||| qiang liu ||| 
2020 ||| nestfuse: an infrared and visible image fusion architecture based on nest connection and spatial/channel attention models. ||| hui li ||| xiao-jun wu ||| tariq s. durrani ||| 
2021 ||| biomedical data-to-text generation via fine-tuning transformers. ||| ruslan yermakov ||| nicholas drago ||| angelo ziletti ||| 
2021 ||| thinking fast and slow: efficient text-to-visual retrieval with transformers. ||| antoine miech ||| jean-baptiste alayrac ||| ivan laptev ||| josef sivic ||| andrew zisserman ||| 
2020 ||| efficient folded attention for 3d medical image reconstruction and segmentation. ||| hang zhang ||| jinwei zhang ||| rongguang wang ||| qihao zhang ||| pascal spincemaille ||| thanh d. nguyen ||| yi wang ||| 
2018 ||| ca3net: contextual-attentional attribute-appearance network for person re-identification. ||| jiawei liu ||| zheng-jun zha ||| hongtao xie ||| zhiwei xiong ||| yongdong zhang ||| 
2021 ||| pose-guided feature disentangling for occluded person re-identification based on transformer. ||| tao wang ||| hong liu ||| pinhao song ||| tianyu guo ||| wei shi ||| 
2021 ||| focus attention: promoting faithfulness and diversity in summarization. ||| rahul aralikatte ||| shashi narayan ||| joshua maynez ||| sascha rothe ||| ryan t. mcdonald ||| 
2021 ||| local-to-global self-attention in vision transformers. ||| jinpeng li ||| yichao yan ||| shengcai liao ||| xiaokang yang ||| ling shao ||| 
2021 ||| clustering and attention model based for intelligent trading. ||| mimansa rana ||| nanxiang mao ||| ming ao ||| xiaohui wu ||| poning liang ||| matloob khushi ||| 
2022 ||| docsegtr: an instance-level end-to-end document image segmentation transformer. ||| sanket biswas ||| ayan banerjee ||| josep llad ||| s ||| umapada pal ||| 
2019 ||| improving neural machine translation with parent-scaled self-attention. ||| emanuele bugliarello ||| naoaki okazaki ||| 
2018 ||| on the importance of attention in meta-learning for few-shot text classification. ||| xiang jiang ||| mohammad havaei ||| gabriel chartrand ||| hassan chouaib ||| thomas vincent ||| andrew jesson ||| nicolas chapados ||| stan matwin ||| 
2019 ||| understanding multi-head attention in abstractive summarization. ||| joris baan ||| maartje ter hoeve ||| marlies van der wees ||| anne schuth ||| maarten de rijke ||| 
2021 ||| using knowledge-embedded attention to augment pre-trained language models for fine-grained emotion recognition. ||| varsha suresh ||| desmond c. ong ||| 
2020 ||| example-guided image synthesis across arbitrary scenes using masked spatial-channel attention and self-supervision. ||| haitian zheng ||| haofu liao ||| lele chen ||| wei xiong ||| tianlang chen ||| jiebo luo ||| 
2022 ||| exploiting spatial sparsity for event cameras with visual transformers. ||| zuowen wang ||| yuhuang hu ||| shih-chii liu ||| 
2021 ||| styleswin: transformer-based gan for high-resolution image generation. ||| bowen zhang ||| shuyang gu ||| bo zhang ||| jianmin bao ||| dong chen ||| fang wen ||| yong wang ||| baining guo ||| 
2021 ||| paying attention to activation maps in camera pose regression. ||| yoli shavit ||| ron ferens ||| yosi keller ||| 
2018 ||| looking beyond a clever narrative: visual context and attention are primary drivers of affect in video advertisements. ||| abhinav shukla ||| harish katti ||| mohan s. kankanhalli ||| ramanathan subramanian ||| 
2022 ||| torchmd-net: equivariant transformers for neural network based molecular potentials. ||| philipp th ||| lke ||| gianni de fabritiis ||| 
2020 ||| 365 dots in 2019: quantifying attention of news sources. ||| alexander c. nwala ||| michele c. weigle ||| michael l. nelson ||| 
2021 ||| layer-wise pruning of transformer attention heads for efficient language modeling. ||| kyuhong shim ||| iksoo choi ||| wonyong sung ||| jungwook choi ||| 
2019 ||| a self validation network for object-level human attention estimation. ||| zehua zhang ||| chen yu ||| david j. crandall ||| 
2019 ||| attention on attention for image captioning. ||| lun huang ||| wenmin wang ||| jie chen ||| xiaoyong wei ||| 
2020 ||| wireless image transmission using deep source channel coding with attention modules. ||| jialong xu ||| bo ai ||| wei chen ||| ang yang ||| peng sun ||| 
2021 ||| learning attributed graph representations with communicative message passing transformer. ||| jianwen chen ||| shuangjia zheng ||| ying song ||| jiahua rao ||| yuedong yang ||| 
2022 ||| towards automatic transcription of polyphonic electric guitar music: a new dataset and a multi-loss transformer model. ||| yu-hua chen ||| wen-yi hsiao ||| tsu-kuang hsieh ||| jyh-shing roger jang ||| yi-hsuan yang ||| 
2022 ||| unsupervised anomaly detection in medical images with a memory-augmented multi-level cross-attentional masked autoencoder. ||| yu tian ||| guansong pang ||| yuyuan liu ||| chong wang ||| yuanhong chen ||| fengbei liu ||| rajvinder singh ||| johan w. verjans ||| gustavo carneiro ||| 
2021 ||| on automatic text extractive summarization based on graph and pre-trained language model attention. ||| yuan-ching lin ||| jinwen ma ||| 
2020 ||| danhar: dual attention network for multimodal human activity recognition using wearable sensors. ||| wenbin gao ||| lei zhang ||| qi teng ||| hao wu ||| fuhong min ||| jun he ||| 
2021 ||| dual attention network for heart rate and respiratory rate estimation. ||| yuzhuo ren ||| braeden syrnyk ||| niranjan avadhanam ||| 
2021 ||| an attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery. ||| xuan yang ||| shanshan li ||| zhengchao chen ||| jocelyn chanussot ||| xiuping jia ||| bing zhang ||| baipeng li ||| pan chen ||| 
2021 ||| vehicle trajectory prediction in city-scale road networks using a direction-based sequence-to-sequence model with spatiotemporal attention mechanisms. ||| yuebing liang ||| zhan zhao ||| 
2020 ||| cross-modal food retrieval: learning a joint embedding of food images and recipes with semantic consistency and attention mechanism. ||| hao wang ||| doyen sahoo ||| chenghao liu ||| ke shu ||| palakorn achananuparp ||| ee-peng lim ||| steven c. h. hoi ||| 
2022 ||| reward modeling for mitigating toxicity in transformer-based language models. ||| farshid faal ||| ketra schmitt ||| jia yuan yu ||| 
2021 ||| contextual similarity aggregation with self-attention for visual re-ranking. ||| jianbo ouyang ||| hui wu ||| min wang ||| wengang zhou ||| houqiang li ||| 
2020 ||| streaming transformer asr with blockwise synchronous inference. ||| emiru tsunoo ||| yosuke kashiwagi ||| shinji watanabe ||| 
2021 ||| combining attention with flow for person image synthesis. ||| yurui ren ||| yubo wu ||| thomas h. li ||| shan liu ||| ge li ||| 
2020 ||| intellicode compose: code generation using transformer. ||| alexey svyatkovskiy ||| shao kun deng ||| shengyu fu ||| neel sundaresan ||| 
2018 ||| visual attention for behavioral cloning in autonomous driving. ||| sourav pal ||| tharun mohandoss ||| pabitra mitra ||| 
2021 ||| last query transformer rnn for knowledge tracing. ||| seungkee jeon ||| 
2021 ||| sa-net: shuffle attention for deep convolutional neural networks. ||| qing-long zhang ||| yu-bin yang ||| 
2020 ||| realformer: transformer likes residual attention. ||| ruining he ||| anirudh ravula ||| bhargav kanagal ||| joshua ainslie ||| 
2022 ||| detecting offensive language on social networks: an end-to-end detection method based on graph attention networks. ||| zhenxiong miao ||| xingshu chen ||| haizhou wang ||| rui tang ||| zhou yang ||| wenyi tang ||| 
2020 ||| scatter: selective context attentional scene text recognizer. ||| ron litman ||| oron anschel ||| shahar tsiper ||| roee litman ||| shai mazor ||| r. manmatha ||| 
2020 ||| toward understanding the conditions that promote higher attention in software developments - a first step on music and standups. ||| rozaliya amirova ||| sergey masyagin ||| anastasia reprintseva ||| giancarlo succi ||| herman tarasau ||| 
2019 ||| ffa-net: feature fusion attention network for single image dehazing. ||| xu qin ||| zhilin wang ||| yuanchao bai ||| xiaodong xie ||| huizhu jia ||| 
2022 ||| towards unsupervised domain adaptation via domain-transformer. ||| chuan-xian ren ||| yi-ming zhai ||| you-wei luo ||| meng-xue li ||| 
2021 ||| avgcn: trajectory prediction using graph convolutional networks guided by human attention. ||| congcong liu ||| yuying chen ||| ming liu ||| bertram e. shi ||| 
2018 ||| temporal pattern attention for multivariate time series forecasting. ||| shun-yao shih ||| fan-keng sun ||| hung-yi lee ||| 
2020 ||| unsupervised speaker adaptation using attention-based speaker memory for end-to-end asr. ||| leda sari ||| niko moritz ||| takaaki hori ||| jonathan le roux ||| 
2020 ||| a novel multimodal music genre classifier using hierarchical attention and convolutional neural network. ||| manish agrawal ||| abhilash nandy ||| 
2019 ||| pose guided attention for multi-label fashion image classification. ||| beatriz quintino ferreira ||| jo ||| o paulo costeira ||| ricardo gamelas sousa ||| liang-yan gui ||| jo ||| o pedro gomes ||| 
2017 ||| modeling image virality with pairwise spatial transformer networks. ||| abhimanyu dubey ||| sumeet agarwal ||| 
2020 ||| cascade network with guided loss and hybrid attention for two-view geometry. ||| zhi chen ||| fan yang ||| wenbing tao ||| 
2020 ||| urban crowdsensing using social media: an empirical study on transformer and recurrent neural networks. ||| jerome heng ||| junhua liu ||| kwan hui lim ||| 
2021 ||| bgt-net: bidirectional gru transformer network for scene graph generation. ||| naina dhingra ||| florian ritter ||| andreas m. kunz ||| 
2021 ||| a multi-branch hybrid transformer networkfor corneal endothelial cell segmentation. ||| yinglin zhang ||| risa higashita ||| huazhu fu ||| yanwu xu ||| yang zhang ||| haofeng liu ||| jian zhang ||| jiang liu ||| 
2019 ||| pyramnet: point cloud pyramid attention network and graph embedding module for classification and segmentation. ||| zhiheng kang ||| ning li ||| 
2020 ||| attentron: few-shot text-to-speech utilizing attention-based variable-length embedding. ||| seungwoo choi ||| seungju han ||| dongyoung kim ||| sungjoo ha ||| 
2021 ||| text2gestures: a transformer-based network for generating emotive body gestures for virtual agents. ||| uttaran bhattacharya ||| nicholas rewkowski ||| abhishek banerjee ||| pooja guhan ||| aniket bera ||| dinesh manocha ||| 
2021 ||| styleformer: transformer based generative adversarial networks with style vector. ||| jeeseung park ||| younggeun kim ||| 
2022 ||| hipa: hierarchical patch transformer for single image super resolution. ||| qing cai ||| yiming qian ||| jinxing li ||| jun lv ||| yee-hong yang ||| feng wu ||| david zhang ||| 
2022 ||| an ensemble approach to acronym extraction using transformers. ||| prashant sharma ||| hadeel saadany ||| leonardo zilio ||| diptesh kanojia ||| constantin orasan ||| 
2020 ||| iart: intent-aware response ranking with transformers in information-seeking conversation systems. ||| liu yang ||| minghui qiu ||| chen qu ||| cen chen ||| jiafeng guo ||| yongfeng zhang ||| w. bruce croft ||| haiqing chen ||| 
2020 ||| hybrid attention networks for flow and pressure forecasting in water distribution systems. ||| ziqing ma ||| shuming liu ||| guancheng guo ||| xipeng yu ||| 
2021 ||| escaping the big data paradigm with compact transformers. ||| ali hassani ||| steven walton ||| nikhil shah ||| abulikemu abuduweili ||| jiachen li ||| humphrey shi ||| 
2020 ||| channel pruning guided by spatial and channel attention for dnns in intelligent edge computing. ||| mengran liu ||| weiwei fang ||| xiaodong ma ||| wenyuan xu ||| naixue xiong ||| yi ding ||| 
2020 ||| relational learning between multiple pulmonary nodules via deep set attention transformers. ||| jiancheng yang ||| haoran deng ||| xiaoyang huang ||| bingbing ni ||| yi xu ||| 
2018 ||| topic-guided attention for image captioning. ||| zhihao zhu ||| zhan xue ||| zejian yuan ||| 
2021 ||| cptr: full transformer network for image captioning. ||| wei liu ||| sihan chen ||| longteng guo ||| xinxin zhu ||| jing liu ||| 
2020 ||| two-level transformer and auxiliary coherence modeling for improved text segmentation. ||| goran glavas ||| swapna somasundaran ||| 
2019 ||| global transformer u-nets for label-free prediction of fluorescence images. ||| yi liu ||| hao yuan ||| zhengyang wang ||| shuiwang ji ||| 
2019 ||| free-lunch saliency via attention in atari agents. ||| dmitry nikulin ||| anastasia ianina ||| vladimir aliev ||| sergey i. nikolenko ||| 
2021 ||| dancing along battery: enabling transformer with run-time reconfigurability on mobile devices. ||| yuhong song ||| weiwen jiang ||| bingbing li ||| panjie qi ||| qingfeng zhuge ||| edwin hsing-mean sha ||| sakyasingha dasgupta ||| yiyu shi ||| caiwen ding ||| 
2022 ||| attention-effective multiple instance learning on weakly stem cell colony segmentation. ||| novanto yudistira ||| muthu subash kavitha ||| jeny rajan ||| takio kurita ||| 
2021 ||| symbolicgpt: a generative transformer model for symbolic regression. ||| mojtaba valipour ||| bowen you ||| maysum panju ||| ali ghodsi ||| 
2018 ||| fine-grained video categorization with redundancy reduction attention. ||| chen zhu ||| xiao tan ||| feng zhou ||| xiao liu ||| kaiyu yue ||| errui ding ||| yi ma ||| 
2019 ||| why deep transformers are difficult to converge? from computation order to lipschitz restricted parameter initialization. ||| hongfei xu ||| qiuhui liu ||| josef van genabith ||| jingyi zhang ||| 
2019 ||| towards universal object detection by domain attention. ||| xudong wang ||| zhaowei cai ||| dashan gao ||| nuno vasconcelos ||| 
2021 ||| deep attentional guided image filtering. ||| zhiwei zhong ||| xianming liu ||| junjun jiang ||| debin zhao ||| xiangyang ji ||| 
2021 ||| compositional attention: disentangling search and retrieval. ||| sarthak mittal ||| sharath chandra raparthy ||| irina rish ||| yoshua bengio ||| guillaume lajoie ||| 
2019 ||| multi-layer attention mechanism for speech keyword recognition. ||| ruisen luo ||| tianran sun ||| chen wang ||| miao du ||| zuodong tang ||| kai zhou ||| xiaofeng gong ||| xiaomei yang ||| 
2021 ||| multi-pretext attention network for few-shot learning with self-supervision. ||| hainan li ||| renshuai tao ||| jun li ||| haotong qin ||| yifu ding ||| shuo wang ||| xianglong liu ||| 
2022 ||| on the efficacy of co-attention transformer layers in visual question answering. ||| ankur sikarwar ||| gabriel kreiman ||| 
2021 ||| improving ultrasound tongue image reconstruction from lip images using self-supervised learning and attention mechanism. ||| haiyang liu ||| jihan zhang ||| 
2019 ||| deep multi-kernel convolutional lstm networks and an attention-based mechanism for videos. ||| sebastian agethen ||| winston h. hsu ||| 
2021 ||| better pay attention whilst fuzzing. ||| shunkai zhu ||| jingyi wang ||| jun sun ||| jie yang ||| xingwei lin ||| liyi zhang ||| peng cheng ||| 
2022 ||| patch-based stochastic attention for image editing. ||| nicolas cherel ||| andr ||| s almansa ||| yann gousseau ||| alasdair newson ||| 
2021 ||| graph joint attention networks. ||| tiantian he ||| lu bai ||| yew-soon ong ||| 
2019 ||| construct dynamic graphs for hand gesture recognition via spatial-temporal attention. ||| yuxiao chen ||| long zhao ||| xi peng ||| jianbo yuan ||| dimitris n. metaxas ||| 
2019 ||| motion guided attention for video salient object detection. ||| haofeng li ||| guanqi chen ||| guanbin li ||| yizhou yu ||| 
2020 ||| coarse- and fine-grained attention network with background-aware loss for crowd density map estimation. ||| liangzi rong ||| chunping li ||| 
2019 ||| learning multi-level information for dialogue response selection by highway recurrent transformer. ||| ting-rui chiang ||| chao-wei huang ||| shang-yu su ||| yun-nung chen ||| 
2019 ||| a synchronized multi-modal attention-caption dataset and analysis. ||| sen he ||| hamed r. tavakoli ||| ali borji ||| nicolas pugeault ||| 
2020 ||| joint left atrial segmentation and scar quantification based on a dnn with spatial encoding and shape attention. ||| lei li ||| xin weng ||| julia a. schnabel ||| xiahai zhuang ||| 
2021 ||| exploring transformers in emotion recognition: a comparison of bert, distillbert, roberta, xlnet and electra. ||| diogo cortiz ||| 
2021 ||| power transformer faults diagnosis using undestructive methods (roger and iec) and artificial neural network for dissolved gas analysis applied on the functional transformer in the algerian north-eastern: a comparative study. ||| bouchaoui lahcene ||| kamel eddine hemsas ||| hacene mellah ||| saad eddine benlahneche ||| 
2021 ||| towards deep and efficient: a deep siamese self-attention fully efficient convolutional network for change detection in vhr images. ||| hongruixuan chen ||| chen wu ||| bo du ||| 
2022 ||| transvpr: transformer-based place recognition with multi-level attention aggregation. ||| ruotong wang ||| yanqing shen ||| weiliang zuo ||| sanping zhou ||| nanning zheng ||| 
2021 ||| ibert: idiom cloze-style reading comprehension with attention. ||| ruiyang qin ||| haozheng luo ||| zheheng fan ||| ziang ren ||| 
2020 ||| a two-stage cascade model with variational autoencoders and attention gates for mri brain tumor segmentation. ||| chenggang lyu ||| hai shu ||| 
2021 ||| wakavt: a sequential variational transformer for waka generation. ||| yuka takeishi ||| mingxuan niu ||| jing luo ||| zhong jin ||| xinyu yang ||| 
2020 ||| spectral pyramid graph attention network for hyperspectral image classification. ||| tinghuai wang ||| guangming wang ||| kuan eeik tan ||| donghui tan ||| 
2020 ||| cort: complementary rankings from transformers. ||| marco wrzalik ||| dirk krechel ||| 
2022 ||| iwin: human-object interaction detection via transformer with irregular windows. ||| danyang tu ||| xiongkuo min ||| huiyu duan ||| guodong guo ||| guangtao zhai ||| wei shen ||| 
2019 ||| fixed pattern noise reduction for infrared images based on cascade residual attention cnn. ||| juntao guan ||| rui lai ||| ai xiong ||| zesheng liu ||| lin gu ||| 
2021 ||| enjoy the salience: towards better transformer-based faithful explanations with word salience. ||| george chrysostomou ||| nikolaos aletras ||| 
2021 ||| looking beyond two frames: end-to-end multi-object tracking using spatial and temporal transformers. ||| tianyu zhu ||| markus hiller ||| mahsa ehsanpour ||| rongkai ma ||| tom drummond ||| hamid rezatofighi ||| 
2022 ||| contextual attention network: transformer meets u-net. ||| reza azad ||| moein heidari ||| yuli wu ||| dorit merhof ||| 
2021 ||| hat: hierarchical aggregation transformers for person re-identification. ||| guowen zhang ||| pingping zhang ||| jinqing qi ||| huchuan lu ||| 
2019 ||| relational graph attention networks. ||| dan busbridge ||| dane sherburn ||| pietro cavallo ||| nils y. hammerla ||| 
2020 ||| integrating human gaze into attention for egocentric activity recognition. ||| kyle min ||| jason j. corso ||| 
2018 ||| detecting visual relationships using box attention. ||| alexander kolesnikov ||| christoph h. lampert ||| vittorio ferrari ||| 
2021 ||| morph call: probing morphosyntactic content of multilingual transformers. ||| vladislav mikhailov ||| oleg serikov ||| ekaterina artemova ||| 
2019 ||| nrpa: neural recommendation with personalized attention. ||| hongtao liu ||| fangzhao wu ||| wenjun wang ||| xianchen wang ||| pengfei jiao ||| chuhan wu ||| xing xie ||| 
2020 ||| fine-grained 3d shape classification with hierarchical part-view attentions. ||| xinhai liu ||| zhizhong han ||| yu-shen liu ||| matthias zwicker ||| 
2022 ||| learning to merge tokens in vision transformers. ||| c ||| dric renggli ||| andr |||  susano pinto ||| neil houlsby ||| basil mustafa ||| joan puigcerver ||| carlos riquelme ||| 
2021 ||| pcam: product of cross-attention matrices for rigid registration of point clouds. ||| anh-quan cao ||| gilles puy ||| alexandre boulch ||| renaud marlet ||| 
2021 ||| xtremedistiltransformers: task transfer for task-agnostic distillation. ||| subhabrata mukherjee ||| ahmed hassan awadallah ||| jianfeng gao ||| 
2020 ||| transformer transducer: a streamable speech recognition model with transformer encoders and rnn-t loss. ||| qian zhang ||| han lu ||| hasim sak ||| anshuman tripathi ||| erik mcdermott ||| stephen koo ||| shankar kumar ||| 
2020 ||| multi-label text classification using attention-based graph neural network. ||| ankit pal ||| muru selvakumar ||| malaikannan sankarasubbu ||| 
2019 ||| indoor depth completion with boundary consistency and self-attention. ||| yu-kai huang ||| tsung-han wu ||| yueh-cheng liu ||| winston h. hsu ||| 
2020 ||| regression and learning with pixel-wise attention for retinal fundus glaucoma segmentation and detection. ||| peng liu ||| ruogu fang ||| 
2020 ||| a3t-gcn: attention temporal graph convolutional network for traffic forecasting. ||| jiawei zhu ||| yujiao song ||| ling zhao ||| haifeng li ||| 
2021 ||| poformer: a simple pooling transformer for speaker verification. ||| yufeng ma ||| yiwei ding ||| miao zhao ||| yu zheng ||| min liu ||| minqiang xu ||| 
2018 ||| multi-head attention with disagreement regularization. ||| jian li ||| zhaopeng tu ||| baosong yang ||| michael r. lyu ||| tong zhang ||| 
2021 ||| hate-alert@dravidianlangtech-eacl2021: ensembling strategies for transformer-based offensive language detection. ||| debjoy saha ||| naman paharia ||| debajit chakraborty ||| punyajoy saha ||| animesh mukherjee ||| 
2020 ||| peking opera synthesis via duration informed attention network. ||| yusong wu ||| shengchen li ||| chengzhu yu ||| heng lu ||| chao weng ||| liqiang zhang ||| dong yu ||| 
2021 ||| categorical difference and related brain regions of the attentional blink effect. ||| renzhou gui ||| xiaohong ji ||| 
2020 ||| a transformer-based approach for source code summarization. ||| wasi uddin ahmad ||| saikat chakraborty ||| baishakhi ray ||| kai-wei chang ||| 
2018 ||| attacks on state-of-the-art face recognition using attentional adversarial attack generative network. ||| qing song ||| yingqi wu ||| lu yang ||| 
2022 ||| bba-net: a bi-branch attention network for crowd counting. ||| yi hou ||| chengyang li ||| fan yang ||| cong ma ||| liping zhu ||| yuan li ||| huizhu jia ||| xiaodong xie ||| 
2021 ||| diformer: directional transformer for neural machine translation. ||| minghan wang ||| jiaxin guo ||| yuxia wang ||| daimeng wei ||| hengchao shang ||| chang su ||| yimeng chen ||| yinglu li ||| min zhang ||| shimin tao ||| hao yang ||| 
2018 ||| finding a needle in the haystack: attention-based classification of high resolution microscopy images. ||| naofumi tomita ||| behnaz abdollahi ||| jason wei ||| bing ren ||| arief a. suriawinata ||| saeed hassanpour ||| 
2018 ||| tell me where to look: guided attention inference network. ||| kunpeng li ||| ziyan wu ||| kuan-chuan peng ||| jan ernst ||| yun fu ||| 
2022 ||| holistic attention-fusion adversarial network for single image defogging. ||| wei liu ||| cheng chen ||| rui jiang ||| tao lu ||| zixiang xiong ||| 
2022 ||| tervit: an efficient ternary vision transformer. ||| sheng xu ||| yanjing li ||| teli ma ||| bohan zeng ||| baochang zhang ||| peng gao ||| jinhu lv ||| 
2022 ||| self-supervised transformers for unsupervised object discovery using normalized cut. ||| yangtao wang ||| xi shen ||| shell xu hu ||| yuan yuan ||| james l. crowley ||| dominique vaufreydaz ||| 
2020 ||| residual attention u-net for automated multi-class segmentation of covid-19 chest ct images. ||| xiaocong chen ||| lina yao ||| yu zhang ||| 
2021 ||| attention guided cosine margin for overcoming class-imbalance in few-shot road object detection. ||| ashutosh agarwal ||| anay majee ||| anbumani subramanian ||| chetan arora ||| 
2020 ||| an empirical investigation of pre-trained transformer language models for open-domain dialogue generation. ||| piji li ||| 
2022 ||| flow-guided sparse transformer for video deblurring. ||| jing lin ||| yuanhao cai ||| xiaowan hu ||| haoqian wang ||| youliang yan ||| xueyi zou ||| henghui ding ||| yulun zhang ||| radu timofte ||| luc van gool ||| 
2021 ||| global attention mechanism: retain information to enhance channel-spatial interactions. ||| yichao liu ||| zongru shao ||| nico hoffmann ||| 
2021 ||| mag-net: mutli-task attention guided network for brain tumor segmentation and classification. ||| sachin gupta ||| narinder singh punn ||| sanjay kumar sonbhadra ||| sonali agarwal ||| 
2021 ||| transformers in the loop: polarity in neural models of language. ||| lisa bylinina ||| alexey tikhonov ||| 
2021 ||| automated essay scoring using efficient transformer-based language models. ||| christopher m. ormerod ||| akanksha malhotra ||| amir jafari ||| 
2021 ||| graph attention network based single-pixel compressive direction of arrival estimation. ||| k ||| rsat tekbiyik ||| okan yurduseven ||| g ||| nes karabulut-kurt ||| 
2021 ||| fuseformer: fusing fine-grained information in transformers for video inpainting. ||| rui liu ||| hanming deng ||| yangyi huang ||| xiaoyu shi ||| lewei lu ||| wenxiu sun ||| xiaogang wang ||| jifeng dai ||| hongsheng li ||| 
2019 ||| an attention-based speaker naming method for online adaptation in non-fixed scenarios. ||| jungwoo pyo ||| joohyun lee ||| youngjune park ||| tien-cuong bui ||| sang-kyun cha ||| 
2019 ||| spatial-aware non-local attention for fashion landmark detection. ||| yixin li ||| shengqin tang ||| yun ye ||| jinwen ma ||| 
2021 ||| light field image super-resolution with transformers. ||| zhengyu liang ||| yingqian wang ||| longguang wang ||| jungang yang ||| shilin zhou ||| 
2021 ||| end-to-end information extraction by character-level embedding and multi-stage attentional u-net. ||| tuan anh nguyen dang ||| dat thanh nguyen ||| 
2021 ||| dam-al: dilated attention mechanism with attention loss for 3d infant brain image segmentation. ||| dinh-hieu hoang ||| gia-han diep ||| minh-triet tran ||| ngan t. h. le ||| 
2021 ||| multi-fold correlation attention network for predicting traffic speeds with heterogeneous frequency. ||| yidan sun ||| guiyuan jiang ||| siew-kei lam ||| peilan he ||| fangxin ning ||| 
2021 ||| graph pattern loss based diversified attention network for cross-modal retrieval. ||| xueying chen ||| rong zhang ||| yibing zhan ||| 
2019 ||| triplenet: triple attention network for multi-turn response selection in retrieval-based chatbots. ||| wentao ma ||| yiming cui ||| nan shao ||| su he ||| weinan zhang ||| ting liu ||| shijin wang ||| guoping hu ||| 
2019 ||| looking for the devil in the details: learning trilinear attention sampling network for fine-grained image recognition. ||| heliang zheng ||| jianlong fu ||| zheng-jun zha ||| jiebo luo ||| 
2021 ||| d-han: dynamic news recommendation with hierarchical attention network. ||| qinghua zhao ||| xu chen ||| hui zhang ||| shuai ma ||| 
2018 ||| ra-unet: a hybrid deep attention-aware network to extract liver and tumor in ct scans. ||| qiangguo jin ||| zhao-peng meng ||| changming sun ||| leyi wei ||| ran su ||| 
2017 ||| detection and attention: diagnosing pulmonary lung cancer from ct by imitating physicians. ||| ning li ||| haopeng liu ||| bin qiu ||| wei guo ||| shijun zhao ||| kungang li ||| jie he ||| 
2021 ||| transmorph: transformer for unsupervised medical image registration. ||| junyu chen ||| yong du ||| yufan he ||| william paul segars ||| ye li ||| eric c. frey ||| 
2022 ||| three things everyone should know about vision transformers. ||| hugo touvron ||| matthieu cord ||| alaaeldin el-nouby ||| jakob verbeek ||| herv |||  j ||| gou ||| 
2021 ||| novelty detection and analysis of traffic scenario infrastructures in the latent space of a vision transformer-based triplet autoencoder. ||| jonas wurst ||| lakshman balasubramanian ||| michael botsch ||| wolfgang utschick ||| 
2020 ||| graph attention network based pruning for reconstructing 3d liver vessel morphology from contrasted ct images. ||| donghao zhang ||| siqi liu ||| shikha chaganti ||| eli gibson ||| zhoubing xu ||| sasa grbic ||| weidong cai ||| dorin comaniciu ||| 
2021 ||| anomaly transformer: time series anomaly detection with association discrepancy. ||| jiehui xu ||| haixu wu ||| jianmin wang ||| mingsheng long ||| 
2019 ||| toward imitating visual attention of experts in software development tasks. ||| yoshiharu ikutani ||| nishanth koganti ||| hideaki hata ||| takatomi kubo ||| kenichi matsumoto ||| 
2019 ||| information aggregation for multi-head attention with routing-by-agreement. ||| jian li ||| baosong yang ||| zi-yi dou ||| xing wang ||| michael r. lyu ||| zhaopeng tu ||| 
2020 ||| auto completion of user interface layout design using transformer-based tree decoders. ||| yang li ||| julien amelot ||| xin zhou ||| samy bengio ||| si si ||| 
2021 ||| improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning. ||| ilwi yun ||| hyuk-jae lee ||| chae-eun rhee ||| 
2021 ||| mass: multi-attentional semantic segmentation of lidar data for dense top-view understanding. ||| kunyu peng ||| juncong fei ||| kailun yang ||| alina roitberg ||| jiaming zhang ||| frank bieder ||| philipp heidenreich ||| christoph stiller ||| rainer stiefelhagen ||| 
2021 ||| axm-net: cross-modal context sharing attention network for person re-id. ||| ammarah farooq ||| muhammad awais ||| josef kittler ||| syed safwan khalid ||| 
2017 ||| attention-based models for text-dependent speaker verification. ||| f. a. rezaur rahman chowdhury ||| quan wang ||| ignacio lopez-moreno ||| li wan ||| 
2020 ||| meta graph attention on heterogeneous graph with node-edge co-evolution. ||| yucheng lin ||| huiting hong ||| xiaoqing yang ||| xiaodi yang ||| pinghua gong ||| jieping ye ||| 
2019 ||| exploring reciprocal attention for salient object detection by cooperative learning. ||| changqun xia ||| jia li ||| jinming su ||| yonghong tian ||| 
2018 ||| collective attention towards scientists and research topics. ||| claudia wagner ||| olga zagovora ||| tatiana sennikova ||| fariba karimi ||| 
2021 ||| improved xception with dual attention mechanism and feature fusion for face forgery detection. ||| hao lin ||| weiqi luo ||| kangkang wei ||| minglin liu ||| 
2020 ||| image-level harmonization of multi-site data using image-and-spatial transformer networks. ||| robert robinson ||| qi dou ||| daniel coelho de castro ||| konstantinos kamnitsas ||| marius de groot ||| ronald m. summers ||| daniel rueckert ||| benjamin m. glocker ||| 
2022 ||| a deep neural framework for image caption generation using gru-based attention mechanism. ||| rashid khan ||| m. shujah islam ||| khadija kanwal ||| mansoor iqbal ||| md. imran hossain ||| zhongfu ye ||| 
2021 ||| contrastive attention for automatic chest x-ray report generation. ||| fenglin liu ||| changchang yin ||| xian wu ||| shen ge ||| ping zhang ||| xu sun ||| 
2021 ||| pose-driven attention-guided image generation for person re-identification. ||| amena khatun ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2017 ||| learning to detect chest radiographs containing lung nodules using visual attention networks. ||| emanuele pesce ||| petros-pavlos ypsilantis ||| samuel withey ||| robert bakewell ||| vicky goh ||| giovanni montana ||| 
2017 ||| learning an attention model in an artificial visual system. ||| alon hazan ||| yuval harel ||| ron meir ||| 
2019 ||| improving deep image clustering with spatial transformer layers. ||| thiago v. m. souza ||| cleber zanchettin ||| 
2021 ||| generalized wasserstein dice loss, test-time augmentation, and transformers for the brats 2021 challenge. ||| lucas fidon ||| suprosanna shit ||| ivan ezhov ||| johannes c. paetzold ||| s ||| bastien ourselin ||| tom vercauteren ||| 
2020 ||| text classification with lexicon from preattention mechanism. ||| qingbiao li ||| chunhua wu ||| kangfeng zheng ||| 
2019 ||| cross-modality attention with semantic graph embedding for multi-label classification. ||| renchun you ||| zhiyao guo ||| lei cui ||| xiang long ||| yingze bao ||| shilei wen ||| 
2019 ||| entropy-enhanced multimodal attention model for scene-aware dialogue generation. ||| kuan-yen lin ||| chao-chun hsu ||| yun-nung chen ||| lun-wei ku ||| 
2021 ||| parallel attention network with sequence matching for video grounding. ||| hao zhang ||| aixin sun ||| wei jing ||| liangli zhen ||| joey tianyi zhou ||| rick siow mong goh ||| 
2021 ||| video background music generation with controllable music transformer. ||| shangzhe di ||| zeren jiang ||| si liu ||| zhaokai wang ||| leyan zhu ||| zexin he ||| hongming liu ||| shuicheng yan ||| 
2020 ||| poor man's bert: smaller and faster transformer models. ||| hassan sajjad ||| fahim dalvi ||| nadir durrani ||| preslav nakov ||| 
2021 ||| understanding and improving robustness of vision transformers through patch-based negative augmentation. ||| yao qin ||| chiyuan zhang ||| ting chen ||| balaji lakshminarayanan ||| alex beutel ||| xuezhi wang ||| 
2021 ||| crossmap transformer: a crossmodal masked path transformer using double back-translation for vision-and-language navigation. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2019 ||| self-attention based molecule representation for predicting drug-target interaction. ||| bonggun shin ||| sungsoo park ||| keunsoo kang ||| joyce c. ho ||| 
2019 ||| transformer-based acoustic modeling for hybrid speech recognition. ||| yongqiang wang ||| abdelrahman mohamed ||| duc le ||| chunxi liu ||| alex xiao ||| jay mahadeokar ||| hongzhao huang ||| andros tjandra ||| xiaohui zhang ||| frank zhang ||| christian fuegen ||| geoffrey zweig ||| michael l. seltzer ||| 
2019 ||| a dual path modelwith adaptive attention for vehicle re-identification. ||| pirazh khorramshahi ||| amit kumar ||| neehar peri ||| sai saketh rambhatla ||| jun-cheng chen ||| rama chellappa ||| 
2020 ||| enhancing monotonic multihead attention for streaming asr. ||| hirofumi inaguma ||| masato mimura ||| tatsuya kawahara ||| 
2020 ||| res3atn - deep 3d residual attention network for hand gesture recognition in videos. ||| naina dhingra ||| andreas m. kunz ||| 
2021 ||| taming sparsely activated transformer with stochastic experts. ||| simiao zuo ||| xiaodong liu ||| jian jiao ||| young jin kim ||| hany hassan ||| ruofei zhang ||| tuo zhao ||| jianfeng gao ||| 
2018 ||| attention as a perspective for learning tempo-invariant audio queries. ||| matthias dorfer ||| jan hajic jr. ||| gerhard widmer ||| 
2021 ||| towards physically consistent data-driven weather forecasting: integrating data assimilation with equivariance-preserving deep spatial transformers. ||| ashesh chattopadhyay ||| mustafa mustafa ||| pedram hassanzadeh ||| eviatar bach ||| karthik kashinath ||| 
2021 ||| optimizing inference performance of transformers on cpus. ||| dave dice ||| alex kogan ||| 
2021 ||| convnets vs. transformers: whose visual representations are more transferable? ||| hong-yu zhou ||| chixiang lu ||| sibei yang ||| yizhou yu ||| 
2017 ||| two-stream collaborative learning with spatial-temporal attention for video classification. ||| yuxin peng ||| yunzhen zhao ||| junchao zhang ||| 
2020 ||| local contextual attention with hierarchical structure for dialogue act recognition. ||| zhigang dai ||| jinhua fu ||| qile zhu ||| hengbin cui ||| xiaolong li ||| yuan qi ||| 
2018 ||| coronary calcium detection using 3d attention identical dual deep network based on weakly supervised learning. ||| yuankai huo ||| james g. terry ||| jiachen wang ||| vishwesh nath ||| camilo bermudez ||| shunxing bao ||| prasanna parvathaneni ||| j. jeffrey carr ||| bennett a. landman ||| 
2017 ||| leveraging the flow of collective attention for computational communication research. ||| cheng-jun wang ||| zhicong chen ||| qiang qin ||| naipeng chao ||| 
2021 ||| gt u-net: a u-net like group transformer network for tooth root segmentation. ||| yunxiang li ||| shuai wang ||| jun wang ||| guodong zeng ||| wenjun liu ||| qianni zhang ||| qun jin ||| yaqi wang ||| 
2021 ||| knowing when to quit: selective cascaded regression with patch attention for real-time face alignment. ||| gil shapira ||| noga levy ||| ishay goldin ||| roy josef jevnisek ||| 
2019 ||| water supply prediction based on initialized attention residual network. ||| yuhao long ||| jingcheng wang ||| jingyi wang ||| 
2019 ||| an empirical study of efficient asr rescoring with transformers. ||| hongzhao huang ||| fuchun peng ||| 
2019 ||| learning soft-attention models for tempo-invariant audio-sheet music retrieval. ||| stefan balke ||| matthias dorfer ||| luis carvalho ||| andreas arzt ||| gerhard widmer ||| 
2020 ||| vision-based fall event detection in complex background using attention guided bi-directional lstm. ||| yong chen ||| lu wang ||| jiajia hu ||| mingbin ye ||| 
2020 ||| memory attentive fusion: external language model integration for transformer-based sequence-to-sequence model. ||| mana ihori ||| ryo masumura ||| naoki makishima ||| tomohiro tanaka ||| akihiko takashima ||| shota orihashi ||| 
2020 ||| generalizing spatial transformers to projective geometry with applications to 2d/3d registration. ||| cong gao ||| xingtong liu ||| wenhao gu ||| benjamin killeen ||| mehran armand ||| russell h. taylor ||| mathias unberath ||| 
2021 ||| dynamic memory based attention network for sequential recommendation. ||| qiaoyu tan ||| jianwei zhang ||| ninghao liu ||| xiao huang ||| hongxia yang ||| jingren zhou ||| xia hu ||| 
2019 ||| band attention convolutional networks for hyperspectral image classification. ||| hongwei dong ||| lamei zhang ||| bin zou ||| 
2019 ||| link prediction via graph attention network. ||| weiwei gu ||| fei gao ||| xiaodan lou ||| jiang zhang ||| 
2020 ||| social-wagdat: interaction-aware trajectory prediction via wasserstein graph double-attention network. ||| jiachen li ||| hengbo ma ||| zhihao zhang ||| masayoshi tomizuka ||| 
2017 ||| an attention mechanism for answer selection using a combined global and local view. ||| yoram bachrach ||| andrej zukov gregoric ||| sam coope ||| ed tovell ||| bogdan maksak ||| jos |||  rodr ||| guez ||| conan mcmurtie ||| 
2021 ||| pathologies in priors and inference for bayesian transformers. ||| tristan cinquin ||| alexander immer ||| max horn ||| vincent fortuin ||| 
2020 ||| layout generation and completion with self-attention. ||| kamal gupta ||| alessandro achille ||| justin lazarow ||| larry davis ||| vijay mahadevan ||| abhinav shrivastava ||| 
2017 ||| jointly trained sequential labeling and classification by sparse attention neural networks. ||| mingbo ma ||| kai zhao ||| liang huang ||| bing xiang ||| bowen zhou ||| 
2020 ||| adding recurrence to pretrained transformers for improved efficiency and context size. ||| davis yoshida ||| allyson ettinger ||| kevin gimpel ||| 
2021 ||| getam: gradient-weighted element-wise transformer attention map for weakly-supervised semantic segmentation. ||| weixuan sun ||| jing zhang ||| zheyuan liu ||| yiran zhong ||| nick barnes ||| 
2020 ||| attentional-gcnn: adaptive pedestrian trajectory prediction towards generic autonomous vehicle use cases. ||| kunming li ||| stuart eiffert ||| mao shan ||| francisco gomez-donoso ||| stewart worrall ||| eduardo m. nebot ||| 
2022 ||| attention-based lip audio-visual synthesis for talking face generation in the wild. ||| ganglai wang ||| peng zhang ||| lei xie ||| wei huang ||| yufei zha ||| 
2022 ||| transformers in medical imaging: a survey. ||| fahad shamshad ||| salman khan ||| syed waqas zamir ||| muhammad haris khan ||| munawar hayat ||| fahad shahbaz khan ||| huazhu fu ||| 
2022 ||| lile: look in-depth before looking elsewhere - a dual attention network using transformers for cross-modal information retrieval in histopathology archives. ||| danial maleki ||| hamid r. tizhoosh ||| 
2020 ||| multi-lead ecg classification via an information-based attention convolutional neural network. ||| hao tung ||| chao zheng ||| xinsheng mao ||| dahong qian ||| 
2022 ||| modelling the semantics of text in complex document layouts using graph transformer networks. ||| thomas roland barillot ||| jacob saks ||| polena lilyanova ||| edward torgas ||| yachen hu ||| yuanqing liu ||| varun balupuri ||| paul gaskell ||| 
2021 ||| learning inception attention for image synthesis and image recognition. ||| jianghao shen ||| tianfu wu ||| 
2021 ||| detect the interactions that matter in matter: geometric attention for many-body systems. ||| thorben frank ||| stefan chmiela ||| 
2021 ||| looking at ctr prediction again: is attention all you need? ||| yuan cheng ||| yanbo xue ||| 
2021 ||| stacked hourglass network with a multi-level attention mechanism: where to look for intervertebral disc labeling. ||| reza azad ||| lucas rouhier ||| julien cohen-adad ||| 
2019 ||| tener: adapting transformer encoder for named entity recognition. ||| hang yan ||| bocao deng ||| xiaonan li ||| xipeng qiu ||| 
2021 ||| a multi-view framework for bgp anomaly detection via graph attention network. ||| songtao peng ||| jiaqi nie ||| xincheng shu ||| zhongyuan ruan ||| lei wang ||| yunxuan sheng ||| qi xuan ||| 
2020 ||| parallel scheduling self-attention mechanism: generalization and optimization. ||| mingfei yu ||| masahiro fujita ||| 
2020 ||| inserting information bottlenecks for attribution in transformers. ||| zhiying jiang ||| raphael tang ||| ji xin ||| jimmy lin ||| 
2021 ||| marl: multimodal attentional representation learning for disease prediction. ||| ali hamdi ||| amr aboeleneen ||| khaled b. shaban ||| 
2021 ||| transformed cnns: recasting pre-trained convolutional layers with self-attention. ||| st ||| phane d'ascoli ||| levent sagun ||| giulio biroli ||| ari morcos ||| 
2021 ||| crisis domain adaptation using sequence-to-sequence transformers. ||| congcong wang ||| paul nulty ||| david lillis ||| 
2020 ||| span: spatial pyramid attention network forimage manipulation localization. ||| xuefeng hu ||| zhihan zhang ||| zhenye jiang ||| syomantak chaudhuri ||| zhenheng yang ||| ram nevatia ||| 
2020 ||| resolving the scope of speculation and negation using transformer-based architectures. ||| benita kathleen britto ||| aditya khandelwal ||| 
2020 ||| self-attention comparison module for boosting performance on retrieval-based open-domain dialog systems. ||| tian lan ||| xian-ling mao ||| zhipeng zhao ||| wei wei ||| heyan huang ||| 
2020 ||| attributed multi-relational attention network for fact-checking url recommendation. ||| di you ||| nguyen vo ||| kyumin lee ||| qiang liu ||| 
2019 ||| expectation-maximization attention networks for semantic segmentation. ||| xia li ||| zhisheng zhong ||| jianlong wu ||| yibo yang ||| zhouchen lin ||| hong liu ||| 
2020 ||| self-supervised transformers for activity classification using ambient sensors. ||| luke hicks ||| ariel ruiz-garcia ||| vasile palade ||| ibrahim almakky ||| 
2018 ||| plan-recognition-driven attention modeling for visual recognition. ||| yantian zha ||| yikang li ||| tianshu yu ||| subbarao kambhampati ||| baoxin li ||| 
2018 ||| global pose estimation with an attention-based recurrent network. ||| emilio parisotto ||| devendra singh chaplot ||| jian zhang ||| ruslan salakhutdinov ||| 
2019 ||| hierarchical attention generative adversarial networks for cross-domain sentiment classification. ||| yuebing zhang ||| duoqian miao ||| jiaqi wang ||| 
2020 ||| self-attention dense depth estimation network for unrectified video sequences. ||| alwyn mathew ||| aditya prakash patra ||| jimson mathew ||| 
2021 ||| shape: shifted absolute position embedding for transformers. ||| shun kiyono ||| sosuke kobayashi ||| jun suzuki ||| kentaro inui ||| 
2022 ||| tsam: a two-stream attention model for causal emotion entailment. ||| duzhen zhang ||| zhen yang ||| fandong meng ||| xiuyi chen ||| jie zhou ||| 
2019 ||| monotonic multihead attention. ||| xutai ma ||| juan miguel pino ||| james cross ||| liezl puzon ||| jiatao gu ||| 
2017 ||| decidenet: counting varying density crowds through attention guided detection and density estimation. ||| jiang liu ||| chenqiang gao ||| d ||| eyu meng ||| alexander g. hauptmann ||| 
2022 ||| dcsau-net: a deeper and more compact split-attention u-net for medical image segmentation. ||| qing xu ||| wenting duan ||| na he ||| 
2020 ||| alleviating the burden of labeling: sentence generation by attention branch encoder-decoder network. ||| tadashi ogura ||| aly magassouba ||| komei sugiura ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| hisashi kawai ||| 
2021 ||| efficient re-parameterization residual attention network for nonhomogeneous image dehazing. ||| tian ye ||| erkang chen ||| xinrui huang ||| peng chen ||| 
2021 ||| decision-based black-box attack against vision transformers via patch-wise adversarial removal. ||| yucheng shi ||| yahong han ||| 
2019 ||| transformer-transducer: end-to-end speech recognition with self-attention. ||| ching-feng yeh ||| jay mahadeokar ||| kaustubh kalgaonkar ||| yongqiang wang ||| duc le ||| mahaveer jain ||| kjell schubert ||| christian fuegen ||| michael l. seltzer ||| 
2021 ||| learning multi-attention context graph for group-based re-identification. ||| yichao yan ||| jie qin ||| bingbing ni ||| jiaxin chen ||| li liu ||| fan zhu ||| wei-shi zheng ||| xiaokang yang ||| ling shao ||| 
2021 ||| making transformers solve compositional tasks. ||| santiago onta ||| n ||| joshua ainslie ||| vaclav cvicek ||| zachary fisher ||| 
2018 ||| multi-modality sensor data classification with selective attention. ||| xiang zhang ||| lina yao ||| chaoran huang ||| sen wang ||| mingkui tan ||| guodong long ||| can wang ||| 
2021 ||| on improving adversarial transferability of vision transformers. ||| muzammal naseer ||| kanchana ranasinghe ||| salman h. khan ||| fahad shahbaz khan ||| fatih porikli ||| 
2021 ||| stytr^2: unbiased image style transfer with transformers. ||| yingying deng ||| fan tang ||| xingjia pan ||| weiming dong ||| chongyang ma ||| changsheng xu ||| 
2020 ||| accurate cell segmentation in digital pathology images via attention enforced networks. ||| muyi sun ||| zeyi yao ||| guanhong zhang ||| 
2021 ||| studying the usage of text-to-text transfer transformer to support code-related tasks. ||| antonio mastropaolo ||| simone scalabrino ||| nathan cooper ||| david nader-palacio ||| denys poshyvanyk ||| rocco oliveto ||| gabriele bavota ||| 
2021 ||| starvqa: space-time attention for video quality assessment. ||| fengchuang xing ||| yuan-gen wang ||| hanpin wang ||| leida li ||| guopu zhu ||| 
2021 ||| rethinking query, key, and value embedding in vision transformer under tiny model constraints. ||| jaesin ahn ||| jiuk hong ||| jeongwoo ju ||| heechul jung ||| 
2019 ||| unified attentional generative adversarial network for brain tumor segmentation from multimodal unpaired images. ||| wenguang yuan ||| jia wei ||| jiabing wang ||| qianli ma ||| tolga tasdizen ||| 
2021 ||| attention-based dual-stream vision transformer for radar gait recognition. ||| shiliang chen ||| wentao he ||| jianfeng ren ||| xudong jiang ||| 
2018 ||| identification of internal faults in indirect symmetrical phase shift transformers using ensemble learning. ||| pallav kumar bera ||| rajesh kumar ||| can isik ||| 
2021 ||| uvce-iiitt@dravidianlangtech-eacl2021: tamil troll meme classification: you need to pay more attention. ||| siddhanth u. hegde ||| adeep hande ||| ruba priyadharshini ||| sajeetha thavareesan ||| bharathi raja chakravarthi ||| 
2021 ||| efficient human pose estimation by maximizing fusion and high-level spatial attention. ||| zhiyuan ren ||| yaohai zhou ||| yizhe chen ||| ruisong zhou ||| yayu gao ||| 
2020 ||| turbotransformers: an efficient gpu serving system for transformer models. ||| jiarui fang ||| yang yu ||| chengduo zhao ||| jie zhou ||| 
2020 ||| look, listen, and attend: co-attention network for self-supervised audio-visual representation learning. ||| ying cheng ||| ruize wang ||| zhihao pan ||| rui feng ||| yuejie zhang ||| 
2020 ||| stochastic attention head removal: a simple and effective method for improving automatic speech recognition with transformers. ||| shucong zhang ||| erfan loweimi ||| peter bell ||| steve renals ||| 
2020 ||| achieving real-time execution of transformer-based large-scale models on mobile with compiler-aware neural architecture optimization. ||| wei niu ||| zhenglun kong ||| geng yuan ||| weiwen jiang ||| jiexiong guan ||| caiwen ding ||| pu zhao ||| sijia liu ||| bin ren ||| yanzhi wang ||| 
2019 ||| an analysis of attention over clinical notes for predictive tasks. ||| sarthak jain ||| ramin mohammadi ||| byron c. wallace ||| 
2020 ||| mt5: a massively multilingual pre-trained text-to-text transformer. ||| linting xue ||| noah constant ||| adam roberts ||| mihir kale ||| rami al-rfou ||| aditya siddhant ||| aditya barua ||| colin raffel ||| 
2019 ||| learning temporal attention in dynamic graphs with bilinear interactions. ||| boris knyazev ||| carolyn augusta ||| graham w. taylor ||| 
2019 ||| improved multi-stage training of online attention-based encoder-decoder models. ||| abhinav garg ||| dhananjaya gowda ||| ankur kumar ||| kwangyoun kim ||| mehul kumar ||| chanwoo kim ||| 
2020 ||| adapting pretrained transformer to lattices for spoken language understanding. ||| chao-wei huang ||| yun-nung chen ||| 
2021 ||| deepfake video detection using convolutional vision transformer. ||| deressa wodajo ||| solomon atnafu ||| 
2021 ||| learning robust scheduling with search and attention. ||| david sandberg ||| tor kvernvik ||| francesco davide calabrese ||| 
2017 ||| unconstrained fashion landmark detection via hierarchical recurrent transformer networks. ||| sijie yan ||| ziwei liu ||| ping luo ||| shi qiu ||| xiaogang wang ||| xiaoou tang ||| 
2022 ||| building robust spoken language understanding by cross attention between phoneme sequence and asr hypothesis. ||| zexun wang ||| yuquan le ||| yi zhu ||| yuming zhao ||| mingchao feng ||| meng chen ||| xiaodong he ||| 
2022 ||| dynamic group transformer: a general vision transformer backbone with dynamic group attention. ||| kai liu ||| tianyi wu ||| cong liu ||| guodong guo ||| 
2020 ||| attention-based assisted excitation for salient object segmentation. ||| saeed masoudnia ||| melika kheirieh ||| abdolhossein vahabie ||| babak nadjar araabi ||| 
2019 ||| look globally, age locally: face aging with an attention mechanism. ||| haiping zhu ||| zhizhong huang ||| hongming shan ||| junping zhang ||| 
2022 ||| edter: edge detection with transformer. ||| mengyang pu ||| yaping huang ||| yuming liu ||| qingji guan ||| haibin ling ||| 
2021 ||| saint: improved neural networks for tabular data via row attention and contrastive pre-training. ||| gowthami somepalli ||| micah goldblum ||| avi schwarzschild ||| c. bayan bruss ||| tom goldstein ||| 
2022 ||| end-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system. ||| zhengyi zhang ||| pan zhou ||| 
2021 ||| visual transformer for task-aware active learning. ||| razvan caramalau ||| binod bhattarai ||| tae-kyun kim ||| 
2020 ||| few-shot object detection with feature attention highlight module in remote sensing images. ||| zixuan xiao ||| ping zhong ||| yuan quan ||| xuping yin ||| wei xue ||| 
2020 ||| probing for multilingual numerical understanding in transformer-based language models. ||| devin johnson ||| denise mak ||| drew barker ||| lexi loessberg-zahl ||| 
2021 ||| spectralformer: rethinking hyperspectral image classification with transformers. ||| danfeng hong ||| zhu han ||| jing yao ||| lianru gao ||| bing zhang ||| antonio plaza ||| jocelyn chanussot ||| 
2021 ||| short text clustering with transformers. ||| leonid pugachev ||| mikhail s. burtsev ||| 
2020 ||| train large, then compress: rethinking model size for efficient training and inference of transformers. ||| zhuohan li ||| eric wallace ||| sheng shen ||| kevin lin ||| kurt keutzer ||| dan klein ||| joseph e. gonzalez ||| 
2022 ||| retroformer: pushing the limits of interpretable end-to-end retrosynthesis transformer. ||| yue wan ||| benben liao ||| chang-yu hsieh ||| shengyu zhang ||| 
2020 ||| pointtransformer for shape classification and retrieval of 3d and als roof pointclouds. ||| dimple a. shajahan ||| mukund varma t ||| ramanathan muthuganapathy ||| 
2019 ||| dstp-rnn: a dual-stage two-phase attention-based recurrent neural networks for long-term and multivariate time series prediction. ||| yeqi liu ||| chuanyang gong ||| ling yang ||| yingyi chen ||| 
2021 ||| exploring the promises of transformer-based lms for the representation of normative claims in the legal domain. ||| reto gubelmann ||| peter hongler ||| siegfried handschuh ||| 
2021 ||| multi-task time series forecasting with shared attention. ||| zekai chen ||| jiaze e ||| xiao zhang ||| hao sheng ||| xiuzheng cheng ||| 
2022 ||| patch-fool: are vision transformers always robust against adversarial perturbations? ||| yonggan fu ||| shunyao zhang ||| shang wu ||| cheng wan ||| yingyan lin ||| 
2018 ||| densely connected attention propagation for reading comprehension. ||| yi tay ||| luu anh tuan ||| siu cheung hui ||| jian su ||| 
2020 ||| epsnet: efficient panoptic segmentation network with cross-layer attention fusion. ||| chia-yuan chang ||| shuo-en chang ||| pei-yung hsiao ||| li-chen fu ||| 
2020 ||| discrimination of internal faults and other transients in an interconnected system with power transformers and phase angle regulators. ||| pallav kumar bera ||| can isik ||| vajendra kumar ||| 
2020 ||| suppressing mislabeled data via grouping and self-attention. ||| xiaojiang peng ||| kai wang ||| zhaoyang zeng ||| qing li ||| jianfei yang ||| yu qiao ||| 
2021 ||| learning generative vision transformer with energy-based latent space for saliency prediction. ||| jing zhang ||| jianwen xie ||| nick barnes ||| ping li ||| 
2021 ||| missformer: an effective medical image segmentation transformer. ||| xiaohong huang ||| zhifang deng ||| dandan li ||| xueguang yuan ||| 
2021 ||| hepatic vessel segmentation based on 3d swin-transformer with inductive biased multi-head self-attention. ||| mian wu ||| yinling qian ||| xiangyun liao ||| qiong wang ||| pheng-ann heng ||| 
2020 ||| parameter norm growth during training of transformers. ||| william merrill ||| vivek ramanujan ||| yoav goldberg ||| roy schwartz ||| noah a. smith ||| 
2021 ||| you only look at one sequence: rethinking transformer in vision through object detection. ||| yuxin fang ||| bencheng liao ||| xinggang wang ||| jiemin fang ||| jiyang qi ||| rui wu ||| jianwei niu ||| wenyu liu ||| 
2020 ||| antidote: attention-based dynamic optimization for neural network runtime efficiency. ||| fuxun yu ||| chenchen liu ||| di wang ||| yanzhi wang ||| xiang chen ||| 
2022 ||| transformer-based video front-ends for audio-visual speech recognition. ||| dmitriy serdyuk ||| otavio braga ||| olivier siohan ||| 
2020 ||| selective segmentation networks using top-down attention. ||| mahdi biparva ||| john k. tsotsos ||| 
2018 ||| multi-attention multi-class constraint for fine-grained image recognition. ||| ming sun ||| yuchen yuan ||| feng zhou ||| errui ding ||| 
2021 ||| cross-modal attention consistency for video-audio unsupervised learning. ||| shaobo min ||| qi dai ||| hongtao xie ||| chuang gan ||| yongdong zhang ||| jingdong wang ||| 
2022 ||| ode transformer: an ordinary differential equation-inspired model for sequence generation. ||| bei li ||| quan du ||| tao zhou ||| yi jing ||| shuhan zhou ||| xin zeng ||| tong xiao ||| jingbo zhu ||| xuebo liu ||| min zhang ||| 
2021 ||| high-resolution optical flow from 1d attention and correlation. ||| haofei xu ||| jiaolong yang ||| jianfei cai ||| juyong zhang ||| xin tong ||| 
2019 ||| locality-constrained spatial transformer network for video crowd counting. ||| yanyan fang ||| biyun zhan ||| wandi cai ||| shenghua gao ||| bo hu ||| 
2021 ||| herbert: efficiently pretrained transformer-based language model for polish. ||| robert mroczkowski ||| piotr rybak ||| alina wr ||| blewska ||| ireneusz gawlik ||| 
2019 ||| automatic short answer grading via multiway attention networks. ||| tianqiao liu ||| wenbiao ding ||| zhiwei wang ||| jiliang tang ||| gale yan huang ||| zitao liu ||| 
2021 ||| an interpretable framework for drug-target interaction with gated cross attention. ||| yeachan kim ||| bonggun shin ||| 
2021 ||| towards dynamic feature selection with attention to assist banking customers in establishing a new business. ||| mohammad amin edrisi ||| 
2020 ||| verbal focus-of-attention system for learning-from-demonstration. ||| naoki wake ||| iori yanokura ||| kazuhiro sasabuchi ||| katsushi ikeuchi ||| 
2021 ||| can't fool me: adversarially robust transformer for video understanding. ||| divya choudhary ||| palash goyal ||| saurabh sahu ||| 
2021 ||| transbts: multimodal brain tumor segmentation using transformer. ||| wenxuan wang ||| chen chen ||| meng ding ||| jiangyun li ||| hong yu ||| sen zha ||| 
2022 ||| aligning eyes between humans and deep neural network through interactive attention alignment. ||| yuyang gao ||| tong sun ||| liang zhao ||| sungsoo hong ||| 
2021 |||  marbert: deep bidirectional transformers for arabic. ||| muhammad abdul-mageed ||| abdelrahim a. elmadany ||| el moatez billah nagoudi ||| 
2020 ||| look here! a parametric learning based approach to redirect visual attention. ||| youssef alami mejjati ||| celso f. gomez ||| kwang in kim ||| eli shechtman ||| zoya bylinskii ||| 
2020 ||| svam: saliency-guided visual attention modeling by autonomous underwater robots. ||| md jahidul islam ||| ruobing wang ||| karin de langis ||| junaed sattar ||| 
2021 ||| expression snippet transformer for robust video-based facial expression recognition. ||| yuanyuan liu ||| wenbin wang ||| chuanxu feng ||| haoyu zhang ||| zhe chen ||| yibing zhan ||| 
2021 ||| multi-exit vision transformer for dynamic inference. ||| arian bakhtiarnia ||| qi zhang ||| alexandros iosifidis ||| 
2019 ||| fast transformer decoding: one write-head is all you need. ||| noam shazeer ||| 
2020 ||| bison: bm25-weighted self-attention framework for multi-fields document search. ||| xuan shan ||| chuanjie liu ||| yiqian xia ||| qi chen ||| yusi zhang ||| angen luo ||| yuxiang luo ||| 
2021 ||| adaptive channel encoding transformer for point cloud analysis. ||| guoquan xu ||| hezhi cao ||| jianwei wan ||| ke xu ||| yanxin ma ||| cong zhang ||| 
2021 ||| cross-modal transformer-based neural correction models for automatic speech recognition. ||| tomohiro tanaka ||| ryo masumura ||| mana ihori ||| akihiko takashima ||| takafumi moriya ||| takanori ashihara ||| shota orihashi ||| naoki makishima ||| 
2022 ||| entroformer: a transformer-based entropy model for learned image compression. ||| yichen qian ||| ming lin ||| xiuyu sun ||| zhiyu tan ||| rong jin ||| 
2021 ||| self-supervised pre-training of swin transformers for 3d medical image analysis. ||| yucheng tang ||| dong yang ||| wenqi li ||| holger roth ||| bennett a. landman ||| daguang xu ||| vishwesh nath ||| ali hatamizadeh ||| 
2020 ||| class: cross-level attention and supervision for salient objects detection. ||| lv tang ||| bo li ||| 
2019 ||| back attention knowledge transfer for low-resource named entity recognition. ||| linghao sun ||| huixiong yi ||| huanhuan chen ||| 
2021 ||| raanet: range-aware attention network for lidar-based 3d object detection with auxiliary density level estimation. ||| yantao lu ||| xuetao hao ||| shiqi sun ||| weiheng chai ||| muchenxuan tong ||| senem velipasalar ||| 
2021 ||| u-net transformer: self and cross attention for medical image segmentation. ||| olivier petit ||| nicolas thome ||| cl ||| ment rambour ||| luc soler ||| 
2021 ||| query-graph with cross-gating attention model for text-to-audio grounding. ||| haoyu tang ||| jihua zhu ||| qinghai zheng ||| zhiyong cheng ||| 
2021 ||| distributed attention for grounded image captioning. ||| nenglun chen ||| xingjia pan ||| runnan chen ||| lei yang ||| zhiwen lin ||| yuqiang ren ||| haolei yuan ||| xiaowei guo ||| feiyue huang ||| wenping wang ||| 
2021 ||| comformer: code comment generation via transformer and fusion method-based hybrid code representation. ||| guang yang ||| xiang chen ||| jinxin cao ||| shuyuan xu ||| zhanqi cui ||| chi yu ||| ke liu ||| 
2021 ||| transformer meets dcfam: a novel semantic segmentation scheme for fine-resolution remote sensing images. ||| libo wang ||| rui li ||| chenxi duan ||| shenghui fang ||| 
2019 ||| what can computational models learn from human selective attention? a review from an audiovisual crossmodal perspective. ||| di fu ||| cornelius weber ||| guochun yang ||| matthias kerzel ||| weizhi nan ||| pablo v. a. barros ||| haiyan wu ||| xun liu ||| stefan wermter ||| 
2019 ||| sesamebert: attention for anywhere. ||| ta-chun su ||| hsiang-chih cheng ||| 
2020 ||| tera: self-supervised learning of transformer encoder representation for speech. ||| andy t. liu ||| shang-wen li ||| hung-yi lee ||| 
2020 ||| weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos. ||| yijun song ||| jingwen wang ||| lin ma ||| zhou yu ||| jun yu ||| 
2021 ||| geometry-entangled visual semantic transformer for image captioning. ||| ling cheng ||| wei wei ||| feida zhu ||| yong liu ||| chunyan miao ||| 
2018 ||| r2cnn++: multi-dimensional attention based rotation invariant detector with robust anchor strategy. ||| xue yang ||| kun fu ||| hao sun ||| jirui yang ||| zhi guo ||| menglong yan ||| tengfei zhang ||| xian sun ||| 
2020 ||| monocular expressive body regression through body-driven attention. ||| vasileios choutas ||| georgios pavlakos ||| timo bolkart ||| dimitrios tzionas ||| michael j. black ||| 
2020 ||| sofa-net: second-order and first-order attention network for crowd counting. ||| haoran duan ||| shidong wang ||| yu guan ||| 
2019 ||| rethinking self-attention: an interpretable self-attentive encoder-decoder parser. ||| khalil mrini ||| franck dernoncourt ||| trung bui ||| walter chang ||| ndapa nakashole ||| 
2021 ||| attention is not all you need: pure attention loses rank doubly exponentially with depth. ||| yihe dong ||| jean-baptiste cordonnier ||| andreas loukas ||| 
2019 ||| online multi-object tracking with dual matching attention networks. ||| ji zhu ||| hua yang ||| nian liu ||| minyoung kim ||| wenjun zhang ||| ming-hsuan yang ||| 
2019 ||| vision-to-language tasks based on attributes and attention mechanism. ||| xuelong li ||| aihong yuan ||| xiaoqiang lu ||| 
2021 ||| mda-net: multi-dimensional attention-based neural network for 3d image segmentation. ||| rutu gandhi ||| yi hong ||| 
2021 ||| say their names: resurgence in the collective attention toward black victims of fatal police violence following the death of george floyd. ||| henry h. wu ||| ryan j. gallagher ||| thayer alshaabi ||| jane lydia adams ||| joshua r. minot ||| michael v. arnold ||| brooke foucault welles ||| randall harp ||| peter sheridan dodds ||| christopher m. danforth ||| 
2019 ||| dianet: dense-and-implicit attention network. ||| zhongzhan huang ||| senwei liang ||| mingfu liang ||| haizhao yang ||| 
2020 ||| robust watermarking using inverse gradient attention. ||| honglei zhang ||| hu wang ||| yidong li ||| yuanzhouhan cao ||| chunhua shen ||| 
2022 ||| online decision transformer. ||| qinqing zheng ||| amy zhang ||| aditya grover ||| 
2019 ||| attention-based conditioning methods for external knowledge integration. ||| katerina margatina ||| christos baziotis ||| alexandros potamianos ||| 
2021 ||| chunkformer: learning long time series with multi-stage chunked transformer. ||| yue ju ||| alka isac ||| yimin nie ||| 
2020 ||| attention-based scaling adaptation for target speech extraction. ||| jiangyu han ||| yanhua long ||| jiaen liang ||| 
2020 ||| breast mass segmentation based on ultrasonic entropy maps and attention gated u-net. ||| michal byra ||| piotr jarosik ||| katarzyna dobruch-sobczak ||| ziemowit klimonda ||| hanna piotrzkowska-wr ||| blewska ||| jerzy litniewski ||| andrzej nowicki ||| 
2017 ||| asap: automatic smoothing for attention prioritization in streaming time series visualization. ||| kexin rong ||| peter bailis ||| 
2021 ||| towards fine-grained visual representations by combining contrastive learning with image reconstruction and attention-weighted pooling. ||| jonas dippel ||| steffen vogler ||| johannes h ||| hne ||| 
2021 ||| business model canvas should pay more attention to the software startup team. ||| kai-kristian kemell ||| atte elonen ||| mari suoranta ||| anh nguyen-duc ||| juan garbajosa ||| rafael chanin ||| jorge melegati ||| usman rafiq ||| abdullah aldaeej ||| nana assyne ||| afonso sales ||| sami hyrynsalmi ||| juhani risku ||| henry edison ||| pekka abrahamsson ||| 
2021 ||| pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. ||| xianlong zeng ||| simon m. lin ||| chang liu ||| 
2021 ||| structural guidance for transformer language models. ||| peng qian ||| tahira naseem ||| roger levy ||| ram ||| n fernandez astudillo ||| 
2020 ||| spatio-temporal ranked-attention networks for video captioning. ||| anoop cherian ||| jue wang ||| chiori hori ||| tim k. marks ||| 
2021 ||| bidirectional multi-scale attention networks for semantic segmentation of oblique uav imagery. ||| ye lyu ||| george vosselman ||| guisong xia ||| michael ying yang ||| 
2018 ||| visual attention on the sun: what do existing models actually predict? ||| jia li ||| daowei li ||| kui fu ||| long xu ||| 
2021 ||| nvit: vision transformer compression and parameter redistribution. ||| huanrui yang ||| hongxu yin ||| pavlo molchanov ||| hai li ||| jan kautz ||| 
2018 ||| multiview two-task recursive attention model for left atrium and atrial scars segmentation. ||| jun chen ||| guang yang ||| zhifan gao ||| hao ni ||| elsa d. angelini ||| raad mohiaddin ||| tom wong ||| yanping zhang ||| xiuquan du ||| heye zhang ||| jennifer keegan ||| david n. firmin ||| 
2021 ||| vitae: vision transformer advanced by exploring intrinsic inductive bias. ||| yufei xu ||| qiming zhang ||| jing zhang ||| dacheng tao ||| 
2021 ||| trees in transformers: a theoretical analysis of the transformer's ability to represent trees. ||| qi he ||| jo ||| o sedoc ||| jordan rodu ||| 
2021 ||| feature fusion vision transformer for fine-grained visual categorization. ||| jun wang ||| xiaohan yu ||| yongsheng gao ||| 
2021 ||| efficient sequence training of attention models using approximative recombination. ||| nils-philipp wynands ||| wilfried michel ||| jan rosendahl ||| ralf schl ||| ter ||| hermann ney ||| 
2021 ||| the neural data router: adaptive control flow in transformers improves systematic generalization. ||| r ||| bert csord ||| s ||| kazuki irie ||| j ||| rgen schmidhuber ||| 
2021 ||| event camera simulator design for modeling attention-based inference architectures. ||| md jubaer hossain pantho ||| joel mandebi mbongue ||| pankaj bhowmik ||| christophe bobda ||| 
2021 ||| over-sampling de-occlusion attention network for prohibited items detection in noisy x-ray images. ||| renshuai tao ||| yanlu wei ||| hainan li ||| aishan liu ||| yifu ding ||| haotong qin ||| xianglong liu ||| 
2019 ||| towards robust image classification using sequential attention models. ||| daniel zoran ||| mike chrzanowski ||| po-sen huang ||| sven gowal ||| alex mott ||| pushmeet kohli ||| 
2020 ||| a transformer based pitch sequence autoencoder with midi augmentation. ||| mingshuo ding ||| yinghao ma ||| 
2020 ||| attention-based deep learning framework for human activity recognition with user adaptation. ||| davide buffelli ||| fabio vandin ||| 
2021 ||| topic scene graph generation by attention distillation from caption. ||| w. wang ||| r. wang ||| x. chen ||| 
2021 ||| cmtr: cross-modality transformer for visible-infrared person re-identification. ||| tengfei liang ||| yi jin ||| yajun gao ||| wu liu ||| songhe feng ||| tao wang ||| yidong li ||| 
2019 ||| raunet: residual attention u-net for semantic segmentation of cataract surgical instruments. ||| zhen-liang ni ||| gui-bin bian ||| xiao-hu zhou ||| zeng-guang hou ||| xiaoliang xie ||| chen wang ||| yan-jie zhou ||| rui-qi li ||| zhen li ||| 
2021 ||| progressive and aligned pose attention transfer for person image generation. ||| zhen zhu ||| tengteng huang ||| mengde xu ||| baoguang shi ||| wenqing cheng ||| xiang bai ||| 
2021 ||| attention-oriented brain storm optimization for multimodal optimization problems. ||| jian yang ||| yuhui shi ||| 
2021 ||| pay attention to mlps. ||| hanxiao liu ||| zihang dai ||| david r. so ||| quoc v. le ||| 
2018 ||| end-to-end multi-task learning with attention. ||| shikun liu ||| edward johns ||| andrew j. davison ||| 
2020 ||| atlas-istn: joint segmentation, registration and atlas construction with image-and-spatial transformer networks. ||| matthew sinclair ||| andreas schuh ||| karl hahn ||| kersten petersen ||| ying bai ||| james batten ||| michiel schaap ||| ben glocker ||| 
2020 ||| learning texture transformer network for image super-resolution. ||| fuzhi yang ||| huan yang ||| jianlong fu ||| hongtao lu ||| baining guo ||| 
2021 ||| multi-threshold attention u-net (mtau) based model for multimodal brain tumor segmentation in mri scans. ||| navchetan awasthi ||| rohit pardasani ||| swati gupta ||| 
2021 ||| vit-v-net: vision transformer for unsupervised volumetric medical image registration. ||| junyu chen ||| yufan he ||| eric c. frey ||| ye li ||| yong du ||| 
2021 ||| graph attention layer evolves semantic segmentation for road pothole detection: a benchmark and algorithms. ||| rui fan ||| hengli wang ||| yuan wang ||| ming liu ||| ioannis pitas ||| 
2021 ||| multi-level attention fusion network for audio-visual event recognition. ||| mathilde brousmiche ||| jean rouat ||| st ||| phane dupont ||| 
2019 ||| finetext: text classification via attention-based language model fine-tuning. ||| yunzhe tao ||| saurabh gupta ||| satyapriya krishna ||| xiong zhou ||| orchid majumder ||| vineet khare ||| 
2021 ||| uacanet: uncertainty augmented context attention for polyp segmentation. ||| taehun kim ||| hyemin lee ||| daijin kim ||| 
2021 ||| multi-scale graph convolutional networks with self-attention. ||| zhilong xiong ||| jia cai ||| 
2022 ||| multi-image super-resolution via quality map associated temporal attention network. ||| minji lee ||| inyong koo ||| kangwook ko ||| changick kim ||| 
2019 ||| npa: neural news recommendation with personalized attention. ||| chuhan wu ||| fangzhao wu ||| mingxiao an ||| jianqiang huang ||| yongfeng huang ||| xing xie ||| 
2020 ||| deformable siamese attention networks for visual object tracking. ||| yuechen yu ||| yilei xiong ||| weilin huang ||| matthew r. scott ||| 
2019 ||| attention interpretability across nlp tasks. ||| shikhar vashishth ||| shyam upadhyay ||| gaurav singh tomar ||| manaal faruqui ||| 
2019 ||| heterogeneous memory enhanced multimodal attention model for video question answering. ||| chenyou fan ||| xiaofan zhang ||| shu zhang ||| wensheng wang ||| chi zhang ||| heng huang ||| 
2020 ||| improved automatic summarization of subroutines via attention to file context. ||| sakib haque ||| alexander leclair ||| lingfei wu ||| collin mcmillan ||| 
2021 ||| viesum: how robust are transformer-based models on vietnamese summarization? ||| hieu nguyen ||| long phan ||| james t. anibal ||| alec peltekian ||| hieu tran ||| 
2020 ||| in-sample contrastive learning and consistent attention for weakly supervised object localization. ||| minsong ki ||| youngjung uh ||| wonyoung lee ||| hyeran byun ||| 
2021 ||| titan: t cell receptor specificity prediction with bimodal attention networks. ||| anna weber ||| jannis born ||| mar ||| a rodr ||| guez mart ||| nez ||| 
2022 ||| quadtree attention for vision transformers. ||| shitao tang ||| jiahui zhang ||| siyu zhu ||| ping tan ||| 
2020 ||| kernel self-attention in deep multiple instance learning. ||| dawid rymarczyk ||| jacek tabor ||| bartosz zielinski ||| 
2021 ||| multimodal fusion with bert and attention mechanism for fake news detection. ||| nguyen manh duc tuan ||| pham quang nhat minh ||| 
2017 ||| dual attention network for product compatibility and function satisfiability analysis. ||| hu xu ||| sihong xie ||| lei shu ||| philip s. yu ||| 
2021 ||| efficient transformer based method for remote sensing image change detection. ||| hao chen ||| zipeng qi ||| zhenwei shi ||| 
2018 ||| attention based sentence extraction from scientific articles using pseudo-labeled data. ||| parth mehta ||| gaurav arora ||| prasenjit majumder ||| 
2021 ||| glacier calving front segmentation using attention u-net. ||| michael holzmann ||| amirabbas davari ||| thorsten seehaus ||| matthias h. braun ||| andreas maier ||| vincent christlein ||| 
2022 ||| mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models. ||| ali hadi zadeh ||| mostafa mahmoud ||| ameer abdelhadi ||| andreas moshovos ||| 
2021 ||| transformer-based approach for joint handwriting and named entity recognition in historical documents. ||| ahmed cheikh rouhoua ||| marwa dhiaf ||| yousri kessentini ||| sinda ben salem ||| 
2020 ||| leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning. ||| zhiping zeng ||| van tung pham ||| haihua xu ||| yerbolat khassanov ||| eng siong chng ||| chongjia ni ||| bin ma ||| 
2018 ||| hard non-monotonic attention for character-level transduction. ||| shijie wu ||| pamela shapiro ||| ryan cotterell ||| 
2021 ||| towards accurate rgb-d saliency detection with complementary attention and adaptive integration. ||| hongbo bi ||| zi-qi liu ||| kang wang ||| bo dong ||| geng chen ||| jiquan ma ||| 
2021 ||| uncertainty-based query strategies for active learning with transformers. ||| christopher schr ||| der ||| andreas niekler ||| martin potthast ||| 
2017 ||| interacting attention-gated recurrent networks for recommendation. ||| wenjie pei ||| jie yang ||| zhu sun ||| jie zhang ||| alessandro bozzon ||| david m. j. tax ||| 
2019 ||| detecting robotic affordances on novel objects with regional attention and attributes. ||| fu-jen chu ||| ruinian xu ||| patricio a. vela ||| 
2019 ||| pay attention to convolution filters: towards fast and accurate fine-grained transfer learning. ||| xiangxi mo ||| ruizhe cheng ||| tianyi fang ||| 
2018 ||| the fine line between linguistic generalization and failure in seq2seq-attention models. ||| noah weber ||| leena shekhar ||| niranjan balasubramanian ||| 
2020 ||| deep convlstm with self-attention for human activity decoding using wearables. ||| satya prakash singh ||| aim |||  lay-ekuakille ||| deepak gangwar ||| madan kumar sharma ||| sukrit gupta ||| 
2018 ||| representation based and attention augmented meta learning. ||| yunxiao qin ||| chenxu zhao ||| zezheng wang ||| junliang xing ||| jun wan ||| zhen lei ||| 
2020 ||| dstc8-avsd: multimodal semantic transformer network with retrieval style word generator. ||| hwanhee lee ||| seunghyun yoon ||| franck dernoncourt ||| doo soon kim ||| trung bui ||| kyomin jung ||| 
2017 ||| show, attend and interact: perceivable human-robot social interaction through neural attention q-network. ||| ahmed hussain qureshi ||| yutaka nakamura ||| yuichiro yoshikawa ||| hiroshi ishiguro ||| 
2018 ||| script identification in natural scene image and video frame using attention based convolutional-lstm network. ||| ankan kumar bhunia ||| aishik konwer ||| abir bhowmick ||| ayan kumar bhunia ||| partha pratim roy ||| umapada pal ||| 
2019 ||| label aware graph convolutional network - not all edges deserve your attention. ||| hao chen ||| lu wang ||| senzhang wang ||| dijun luo ||| wenbing huang ||| zhoujun li ||| 
2021 ||| violet : end-to-end video-language transformers with masked visual-token modeling. ||| tsu-jui fu ||| linjie li ||| zhe gan ||| kevin lin ||| william yang wang ||| lijuan wang ||| zicheng liu ||| 
2020 ||| mart: memory-augmented recurrent transformer for coherent video paragraph captioning. ||| jie lei ||| liwei wang ||| yelong shen ||| dong yu ||| tamara l. berg ||| mohit bansal ||| 
2021 ||| robustness evaluation of transformer-based form field extractors via form attacks. ||| le xue ||| mingfei gao ||| zeyuan chen ||| caiming xiong ||| ran xu ||| 
2021 ||| how to help university students to manage their interruptions and improve their attention and time management. ||| aurora vizca ||| no ||| ignacio garc ||| a rodr ||| guez de guzm ||| n ||| antonio manjavacas ||| f ||| lix garc ||| a ||| jos |||  a. cruz-lemus ||| manuel  ||| ngel serrano ||| 
2020 ||| attention-based clustering: learning a kernel from context. ||| samuel coward ||| erik visse-martindale ||| chithrupa ramesh ||| 
2018 ||| a multi-sentiment-resource enhanced attention network for sentiment classification. ||| zeyang lei ||| yujiu yang ||| min yang ||| yi liu ||| 
2018 ||| patient risk assessment and warning symptom detection using deep attention-based neural networks. ||| ivan girardi ||| pengfei ji ||| an-phi nguyen ||| nora hollenstein ||| adam ivankay ||| lorenz kuhn ||| chiara marchiori ||| ce zhang ||| 
2021 ||| brain dynamics via cumulative auto-regressive self-attention. ||| usman mahmood ||| zening fu ||| vince d. calhoun ||| sergey m. plis ||| 
2019 ||| towards better modeling hierarchical structure for self-attention with ordered neurons. ||| jie hao ||| xing wang ||| shuming shi ||| jinfeng zhang ||| zhaopeng tu ||| 
2019 ||| location-relative attention mechanisms for robust long-form speech synthesis. ||| eric battenberg ||| r. j. skerry-ryan ||| soroosh mariooryad ||| daisy stanton ||| david kao ||| matt shannon ||| tom bagby ||| 
2021 ||| convolutional nets versus vision transformers for diabetic foot ulcer classification. ||| adrian galdran ||| gustavo carneiro ||| miguel  ||| ngel gonz ||| lez ballester ||| 
2018 ||| differentiable dynamic programming for structured prediction and attention. ||| arthur mensch ||| mathieu blondel ||| 
2020 ||| mcqa: multimodal co-attention based network for question answering. ||| abhishek kumar ||| trisha mittal ||| dinesh manocha ||| 
2020 ||| indic-transformers: an analysis of transformer language models for indian languages. ||| kushal jain ||| adwait deshpande ||| kumar shridhar ||| felix laumann ||| ayushman dash ||| 
2020 ||| open-domain frame semantic parsing using transformers. ||| aditya kalyanpur ||| or biran ||| tom breloff ||| jennifer chu-carroll ||| ariel diertani ||| owen rambow ||| mark sammons ||| 
2021 ||| soat: a scene- and object-aware transformer for vision-and-language navigation. ||| abhinav moudgil ||| arjun majumdar ||| harsh agrawal ||| stefan lee ||| dhruv batra ||| 
2017 ||| attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging. ||| yong xu ||| qiuqiang kong ||| qiang huang ||| wenwu wang ||| mark d. plumbley ||| 
2021 ||| all bark and no bite: rogue dimensions in transformer language models obscure representational quality. ||| william timkey ||| marten van schijndel ||| 
2018 ||| you may not need attention. ||| ofir press ||| noah a. smith ||| 
2020 ||| fixed encoder self-attention patterns in transformer-based machine translation. ||| alessandro raganato ||| yves scherrer ||| j ||| rg tiedemann ||| 
2021 ||| inferring prototypes for multi-label few-shot image classification with word vector guided attention. ||| kun yan ||| chenbin zhang ||| jun hou ||| ping wang ||| zied bouraoui ||| shoaib jameel ||| steven schockaert ||| 
2021 ||| can vision transformers perform convolution? ||| shanda li ||| xiangning chen ||| di he ||| cho-jui hsieh ||| 
2021 ||| an investigation of enhancing ctc model for triggered attention-based streaming asr. ||| huaibo zhao ||| yosuke higuchi ||| tetsuji ogawa ||| tetsunori kobayashi ||| 
2019 ||| few-shot object detection with attention-rpn and multi-relation detector. ||| qi fan ||| wei zhuo ||| yu-wing tai ||| 
2019 ||| is attention interpretable? ||| sofia serrano ||| noah a. smith ||| 
2019 ||| attention-based context aggregation network for monocular depth estimation. ||| yuru chen ||| haitao zhao ||| zhengwei hu ||| 
2021 ||| hyperspectral and lidar data classification based on linear self-attention. ||| min feng ||| feng gao ||| jian fang ||| junyu dong ||| 
2021 ||| attention based cnn-lstm network for pulmonary embolism prediction on chest computed tomography pulmonary angiograms. ||| sudhir suman ||| gagandeep singh ||| nicole sakla ||| rishabh gattu ||| jeremy green ||| tej phatak ||| dimitris samaras ||| prateek prasanna ||| 
2021 ||| auxiliary loss of transformer with residual connection for end-to-end speaker diarization. ||| yechan yu ||| dongkeon park ||| hong kook kim ||| 
2020 ||| hmanet: hybrid multiple attention network for semantic segmentation in aerial images. ||| ruigang niu ||| 
2020 ||| an end-to-end visual-audio attention network for emotion recognition in user-generated videos. ||| sicheng zhao ||| yunsheng ma ||| yang gu ||| jufeng yang ||| tengfei xing ||| pengfei xu ||| runbo hu ||| hua chai ||| kurt keutzer ||| 
2021 ||| probing inter-modality: visual parsing with self-attention for vision-language pre-training. ||| hongwei xue ||| yupan huang ||| bei liu ||| houwen peng ||| jianlong fu ||| houqiang li ||| jiebo luo ||| 
2020 ||| hierarchical recurrent attention networks for structured online maps. ||| namdar homayounfar ||| wei-chiu ma ||| shrinidhi kowshika lakshmikanth ||| raquel urtasun ||| 
2021 ||| efficient conformer: progressive downsampling and grouped attention for automatic speech recognition. ||| maxime burchi ||| valentin vielzeuf ||| 
2021 ||| multi-glimpse network: a robust and efficient classification architecture based on recurrent downsampled attention. ||| sia huat tan ||| runpei dong ||| kaisheng ma ||| 
2021 ||| when fasttext pays attention: efficient estimation of word representations using constrained positional weighting. ||| v ||| t novotn ||| michal stef ||| nik ||| eniafe festus ayetiran ||| petr sojka ||| 
2018 ||| where and when to look? spatio-temporal attention for action recognition in videos. ||| lili meng ||| bo zhao ||| bo chang ||| gao huang ||| frederick tung ||| leonid sigal ||| 
2020 ||| explainable cnn-attention networks (c-attention network) for automated detection of alzheimer's disease. ||| ning wang ||| mingxuan chen ||| k. p. subbalakshmi ||| 
2019 ||| dynamic fusion: attentional language model for neural machine translation. ||| michiki kurosawa ||| mamoru komachi ||| 
2020 ||| adrn: attention-based deep residual network for hyperspectral image denoising. ||| yongsen zhao ||| deming zhai ||| junjun jiang ||| xianming liu ||| 
2022 ||| d-former: a u-shaped dilated transformer for 3d medical image segmentation. ||| yixuan wu ||| kuanlun liao ||| jintai chen ||| jinhong wang ||| danny z. chen ||| honghao gao ||| jian wu ||| 
2020 ||| gated convolutional bidirectional attention-based model for off-topic spoken response detection. ||| yefei zha ||| ruobing li ||| hui lin ||| 
2021 ||| deep autoregressive models with spectral attention. ||| fernando moreno-pino ||| pablo m. olmos ||| antonio art ||| s-rodr ||| guez ||| 
2022 ||| are vision transformers robust to spurious correlations? ||| soumya suvra ghosal ||| yifei ming ||| yixuan li ||| 
2021 ||| object-region video transformers. ||| roei herzig ||| elad ben-avraham ||| karttikeya mangalam ||| amir bar ||| gal chechik ||| anna rohrbach ||| trevor darrell ||| amir globerson ||| 
2019 ||| multiresolution transformer networks: recurrence is not essential for modeling hierarchical structure. ||| vikas k. garg ||| inderjit s. dhillon ||| hsiang-fu yu ||| 
2021 ||| severity quantification and lesion localization of covid-19 on cxr using vision transformer. ||| gwanghyun kim ||| sangjoon park ||| yujin oh ||| joon beom seo ||| sang min lee ||| jin hwan kim ||| sungjun moon ||| jae-kwang lim ||| jong chul ye ||| 
2020 ||| attention-based learning on molecular ensembles. ||| kangway v. chuang ||| michael j. keiser ||| 
2019 ||| exploring context, attention and audio features for audio visual scene-aware dialog. ||| shachi h. kumar ||| eda okur ||| saurav sahay ||| jonathan huang ||| lama nachman ||| 
2020 ||| development of learning media based on android games for children with attention deficit hyperactivity disorder. ||| maria agustini ||| yufiarti yufiarti ||| wuryani wuryani ||| 
2017 ||| solitude or co-existence - or learning-together-apart with digital dialogic technologies for kids with developmental and attention difficulties. ||| elsebeth k. sorensen ||| hanne voldborg andersen ||| 
2017 ||| inducing omnipotence or powerlessness in learners with developmental and attention difficulties through structuring technologies. ||| hanne voldborg andersen ||| elsebeth k. sorensen ||| 
2021 ||| recipe recommendation with hierarchical graph attention network. ||| yijun tian ||| chuxu zhang ||| ronald a. metoyer ||| nitesh v. chawla ||| 
2021 ||| covid-19 as a research dynamic transformer: emerging cross-disciplinary and national characteristics. ||| ryosuke l. ohniwa ||| joji kijima ||| mizuho fukushige ||| osamu ohneda ||| 
2020 ||| dp3 signal as a neuro-indictor for attentional processing of stereoscopic contents in varied depths within the 'comfort zone'. ||| peng ye ||| xiang wu ||| dingguo gao ||| shaozhi deng ||| ningsheng xu ||| jun chen ||| 
2021 ||| a metric-based meta-learning approach combined attention mechanism and ensemble learning for few-shot learning. ||| nan guo ||| kexin di ||| hongyan liu ||| yifei wang ||| junfei qiao ||| 
2022 ||| cross attention redistribution with contrastive learning for few shot object detection. ||| jianing quan ||| baozhen ge ||| lei chen ||| 
2022 ||| class center attention network with spatial adaption for enhancing hepatic segments classification with low-visibility vascular. ||| yinli tian ||| peiwei sun ||| fei xue ||| ricardo lambo ||| meiyan yue ||| chao an ||| songhui diao ||| jianping lv ||| yaoqin xie ||| peng gong ||| hailin cao ||| wenjian qin ||| 
2021 ||| rafnet: rgb-d attention feature fusion network for indoor semantic segmentation. ||| xingchao yan ||| sujuan hou ||| karim awudu ||| weikuan jia ||| 
2021 ||| quadratic polynomial guided fuzzy c-means and dual attention mechanism for medical image segmentation. ||| weiwei cai ||| bo zhai ||| yun liu ||| runmin liu ||| xin ning ||| 
2021 ||| an improved image enhancement framework based on multiple attention mechanism. ||| qili chen ||| junfang fan ||| wenbai chen ||| 
2021 ||| t-gan: a deep learning framework for prediction of temporal complex networks with adaptive graph convolution and attention mechanism. ||| ru huang ||| lei ma ||| jianhua he ||| xiaoli chu ||| 
2022 ||| covid-19 ct image recognition algorithm based on transformer and cnn. ||| xiaole fan ||| xiufang feng ||| yunyun dong ||| huichao hou ||| 
2017 ||| backlight dimming based on saliency map acquired by visual attention analysis. ||| yong deok ahn ||| suk-ju kang ||| 
2020 ||| 3-d visual discomfort assessment considering optical and neural attention models. ||| jiachen yang ||| vanhung nguyen ||| kyohoon sim ||| yang zhao ||| wen lu ||| 
2020 ||| modeling the screen content image quality via multiscale edge attention similarity. ||| qi yang ||| zhan ma ||| yiling xu ||| le yang ||| wenjun zhang ||| jun sun ||| 
2020 |||  (clinical electronic medical record named entity recognition incorporating language model and attention mechanism). ||| guoqiang tang ||| daqi gao ||| tong ruan ||| qi ye ||| qi wang ||| 
2019 |||  (multi-view attentional approach to single-fact knowledge-based question answering). ||| da luo ||| jindian su ||| pengfei li ||| 
2019 |||  (sentiment classification towards question-answering based on bidirectional attention mechanism). ||| chenlin shen ||| lu zhang ||| liangqing wu ||| shoushan li ||| 
2019 |||  (event coreference resolution method based on attention mechanism). ||| haoyi cheng ||| peifeng li ||| qiaoming zhu ||| 
2017 |||  (attention of bilinear function based bi-lstm model for machine reading comprehension). ||| feilong liu ||| wenning hao ||| gang chen ||| dawei jin ||| jiaxing song ||| 
2019 |||  (asynchronous advantage actor-critic algorithm with visual attention mechanism). ||| jie li ||| xinghong ling ||| yuchen fu ||| quan liu ||| 
2019 |||  (employing multi-attention mechanism to resolve event coreference). ||| jie fang ||| peifeng li ||| qiaoming zhu ||| 
2019 |||  (distant supervision relation extraction model based on multi-level attention mechanism). ||| hao li ||| yongjian liu ||| qing xie ||| lingli tang ||| 
2019 |||  (deep neural network recommendation model based on user vectorization representation and attention mechanism). ||| xu guo ||| jinghua zhu ||| 
2019 |||  (image description model fusing word2vec and attention mechanism). ||| zhenrong deng ||| baojun zhang ||| zhouqin jiang ||| wenming huang ||| 
2019 |||  (video advertisement classification method based on shot segmentation and spatial attention model). ||| kai tan ||| qingbo wu ||| fanman meng ||| linfeng xu ||| 
2019 |||  (study on named entity recognition model based on attention mechanism - - taking military text as example). ||| yidong shan ||| hengjun wang ||| he huang ||| qian yan ||| 
2019 |||  (model of music theme recommendation based on attention lstm). ||| ning jia ||| chunjun zheng ||| 
2017 |||  (visual attention modeling based on multi-scale fusion of amplitude spectrum and phase spectrum). ||| xiaoyan yuan ||| anzhi wang ||| gang pan ||| minghui wang ||| 
2018 |||  (chinese part-of-speech tagging model using attention-based lstm). ||| nianwen si ||| hengjun wang ||| wei li ||| yidong shan ||| pengcheng xie ||| 
2019 |||  (attention mechanism based detection of malware call sequences). ||| lan zhang ||| yao lai ||| xiaojun ye ||| 
2019 |||  (prediction model of e-sports behavior pattern based on attention mechanism and lrua module). ||| cheng yu ||| wanning zhu ||| kun you ||| jinfu zhu ||| 
2019 |||  (attention based acoustics model combining bottleneck feature long xing-yan qu dan zhang wen-lin). ||| xingyan long ||| dan qu ||| wenlin zhang ||| 
2020 |||  (software requirements clustering algorithm based on self-attention mechanism and multi- channel pyramid convolution). ||| yan kang ||| guorong cui ||| hao li ||| qiyue yang ||| jinyuan li ||| peiyao wang ||| 
2019 |||  (bilstm-based implicit discourse relation classification combining self-attentionmechanism and syntactic information). ||| ziwei fan ||| min zhang ||| zhenghua li ||| 
2020 |||  (chinese short text keyphrase extraction model based on attention). ||| danhao yang ||| yuexin wu ||| chunxiao fan ||| 
2020 |||  (comment sentiment analysis and sentiment words detection based on attention mechanism). ||| yuan li ||| zhixing li ||| lei teng ||| huaming wang ||| guoyin wang ||| 
2018 |||  (neural machine translation based on attention convolution). ||| qi wang ||| xiangyu duan ||| 
2019 |||  (event temporal relation classification method based on self-attention mechanism). ||| yijie zhang ||| peifeng li ||| qiaoming zhu ||| 
2021 ||| hierarchical attention learning of scene flow in 3d point clouds. ||| guangming wang ||| xinrui wu ||| zhe liu ||| hesheng wang ||| 
2021 ||| part-guided relational transformers for fine-grained visual recognition. ||| yifan zhao ||| jia li ||| xiaowu chen ||| yonghong tian ||| 
2021 ||| interleaved deep artifacts-aware attention mechanism for concrete structural defect classification. ||| gaurab bhattacharya ||| bappaditya mandal ||| niladri b. puhan ||| 
2018 ||| spatio-temporal attention-based lstm networks for 3d action recognition and detection. ||| sijie song ||| cuiling lan ||| junliang xing ||| wenjun zeng ||| jiaying liu ||| 
2022 ||| dmra: depth-induced multi-scale recurrent attention network for rgb-d saliency detection. ||| wei ji ||| ge yan ||| jingjing li ||| yongri piao ||| shunyu yao ||| miao zhang ||| li cheng ||| huchuan lu ||| 
2021 ||| learning deep global multi-scale and local attention features for facial expression recognition in the wild. ||| zengqun zhao ||| qingshan liu ||| shanmin wang ||| 
2021 ||| discriminative cross-modality attention network for temporal inconsistent audio-visual event localization. ||| hanyu xuan ||| lei luo ||| zhenyu zhang ||| jian yang ||| yan yan ||| 
2020 ||| picanet: pixel-wise contextual attention learning for accurate saliency detection. ||| nian liu ||| junwei han ||| ming-hsuan yang ||| 
2021 ||| interpretable detail-fidelity attention network for single image super-resolution. ||| yuanfei huang ||| jie li ||| xinbo gao ||| yanting hu ||| wen lu ||| 
2019 ||| spatial-temporal attention-aware learning for video-based person re-identification. ||| guangyi chen ||| jiwen lu ||| ming yang ||| jie zhou ||| 
2020 ||| dual-path attention network for compressed sensing image reconstruction. ||| yubao sun ||| jiwei chen ||| qingshan liu ||| bo liu ||| guodong guo ||| 
2021 ||| spatial-aware texture transformer for high-fidelity garment transfer. ||| ting liu ||| jianfeng zhang ||| xuecheng nie ||| yunchao wei ||| shikui wei ||| yao zhao ||| jiashi feng ||| 
2019 ||| deep attention network for egocentric action recognition. ||| minlong lu ||| ze-nian li ||| yueming wang ||| gang pan ||| 
2020 ||| deep pyramidal pooling with attention for person re-identification. ||| niki martinel ||| gian luca foresti ||| christian micheloni ||| 
2021 ||| toward accurate pixelwise object tracking via attention retrieval. ||| zhipeng zhang ||| yufan liu ||| bing li ||| weiming hu ||| houwen peng ||| 
2019 ||| attention-based pedestrian attribute analysis. ||| zichang tan ||| yang yang ||| jun wan ||| hanyuan hang ||| guodong guo ||| stan z. li ||| 
2021 ||| attention-based multi-source domain adaptation. ||| yukun zuo ||| hantao yao ||| changsheng xu ||| 
2018 ||| recurrent spatial-temporal attention network for action recognition in videos. ||| wenbin du ||| yali wang ||| yu qiao ||| 
2021 ||| srgat: single image super-resolution with graph attention network. ||| yanyang yan ||| wenqi ren ||| xiaobin hu ||| kun li ||| haifeng shen ||| xiaochun cao ||| 
2021 ||| layer-output guided complementary attention learning for image defocus blur detection. ||| jinxing li ||| dandan fan ||| lingxiao yang ||| shuhang gu ||| guangming lu ||| yong xu ||| david zhang ||| 
2022 ||| cross-attentional spatio-temporal semantic graph networks for video question answering. ||| yun liu ||| xiaoming zhang ||| feiran huang ||| bo zhang ||| zhoujun li ||| 
2019 ||| saliency from growing neural gas: learning pre-attentional structures for a flexible attention system. ||| jan t ||| nnermann ||| christian born ||| b ||| rbel mertsching ||| 
2019 ||| deep ordinal hashing with spatial attention. ||| lu jin ||| xiangbo shu ||| kai li ||| zechao li ||| guo-jun qi ||| jinhui tang ||| 
2022 ||| personality assessment based on multimodal attention network learning with category-based mean square error. ||| xiao sun ||| jie huang ||| shixin zheng ||| xuanheng rao ||| meng wang ||| 
2022 ||| universal adversarial patch attack for automatic checkout using perceptual and attentional bias. ||| jiakai wang ||| aishan liu ||| xiao bai ||| xianglong liu ||| 
2020 ||| textual-visual reference-aware attention network for visual dialog. ||| dan guo ||| hui wang ||| shuhui wang ||| meng wang ||| 
2021 ||| loss-based attention for interpreting image-level prediction of convolutional neural networks. ||| xiaoshuang shi ||| fuyong xing ||| kaidi xu ||| pingjun chen ||| yun liang ||| zhiyong lu ||| zhenhua guo ||| 
2022 ||| deep stereo matching with hysteresis attention and supervised cost volume construction. ||| kai zeng ||| yaonan wang ||| jianxu mao ||| caiping liu ||| weixing peng ||| yin yang ||| 
2021 ||| multi-stream attention-aware graph convolution network for video salient object detection. ||| mingzhu xu ||| ping fu ||| bing liu ||| junbao li ||| 
2019 ||| visual attention prediction for stereoscopic video by multi-module fully convolutional network. ||| yuming fang ||| chi zhang ||| hanqin huang ||| jianjun lei ||| 
2021 ||| predicting task-driven attention via integrating bottom-up stimulus and top-down guidance. ||| zhixiong nan ||| jingjing jiang ||| xiaofeng gao ||| sanping zhou ||| weiliang zuo ||| ping wei ||| nanning zheng ||| 
2021 ||| dual attention-in-attention model for joint rain streak and raindrop removal. ||| kaihao zhang ||| dongxu li ||| wenhan luo ||| wenqi ren ||| 
2017 ||| end-to-end comparative attention networks for person re-identification. ||| hao liu ||| jiashi feng ||| meibin qi ||| jianguo jiang ||| shuicheng yan ||| 
2021 ||| learning spatial attention for face super-resolution. ||| chaofeng chen ||| dihong gong ||| hao wang ||| zhifeng li ||| kwan-yee k. wong ||| 
2021 ||| attention guided multiple source and target domain adaptation. ||| yuxi wang ||| zhaoxiang zhang ||| wangli hao ||| chunfeng song ||| 
2020 ||| mvsnet++: learning depth-based attention pyramid features for multi-view stereo. ||| po-heng chen ||| hsiao-chien yang ||| kuan-wen chen ||| yong-sheng chen ||| 
2019 ||| three-stream attention-aware network for rgb-d salient object detection. ||| hao chen ||| youfu li ||| 
2020 ||| har-net: joint learning of hybrid attention for single-stage object detection. ||| ya-li li ||| shengjin wang ||| 
2019 ||| bi-directional spatial-semantic attention networks for image-text matching. ||| feiran huang ||| xiaoming zhang ||| zhonghua zhao ||| zhoujun li ||| 
2017 ||| visual attention saccadic models learn to emulate gaze patterns from childhood to adulthood. ||| olivier le meur ||| antoine coutrot ||| zhi liu ||| pia rama ||| adrien le roch ||| andrea helo ||| 
2020 ||| bi-modal progressive mask attention for fine-grained recognition. ||| kaitao song ||| xiu-shen wei ||| xiangbo shu ||| ren-jie song ||| jianfeng lu ||| 
2021 ||| bilateral attention network for rgb-d salient object detection. ||| zhao zhang ||| zheng lin ||| jun xu ||| wenda jin ||| shao-ping lu ||| deng-ping fan ||| 
2019 ||| vssa-net: vertical spatial sequence attention network for traffic sign detection. ||| yuan yuan ||| zhitong xiong ||| qi wang ||| 
2021 ||| i understand you: blind 3d human attention inference from the perspective of third-person. ||| xiang shi ||| you yang ||| qiong liu ||| 
2021 ||| graph regularized flow attention network for video animal counting from drones. ||| pengfei zhu ||| tao peng ||| dawei du ||| hongtao yu ||| libo zhang ||| qinghua hu ||| 
2020 ||| learning rich part hierarchies with progressive attention networks for fine-grained image recognition. ||| heliang zheng ||| jianlong fu ||| zheng-jun zha ||| jiebo luo ||| tao mei ||| 
2020 ||| mava: multi-level adaptive visual-textual alignment by cross-media bi-attention mechanism. ||| yuxin peng ||| jinwei qi ||| yunkan zhuo ||| 
2018 ||| a better way to attend: attention with trees for video question answering. ||| hongyang xue ||| wenqing chu ||| zhou zhao ||| deng cai ||| 
2020 ||| sta-cnn: convolutional spatial-temporal attention learning for action recognition. ||| hao yang ||| chunfeng yuan ||| li zhang ||| yunda sun ||| weiming hu ||| stephen j. maybank ||| 
2021 ||| person re-identification via attention pyramid. ||| guangyi chen ||| tianpei gu ||| jiwen lu ||| jin-an bao ||| jie zhou ||| 
2020 ||| sequential dual attention network for rain streak removal in a single image. ||| chih-yang lin ||| zhuang tao ||| ai-sheng xu ||| li-wei kang ||| fityanul akhyar ||| 
2021 ||| sloan: scale-adaptive orientation attention network for scene text recognition. ||| pengwen dai ||| hua zhang ||| xiaochun cao ||| 
2021 ||| learning to match anchor-target video pairs with dual attentional holographic networks. ||| yanbin hao ||| chong-wah ngo ||| bin zhu ||| 
2019 ||| attention couplenet: fully convolutional attention coupling network for object detection. ||| yousong zhu ||| chaoyang zhao ||| haiyun guo ||| jinqiao wang ||| xu zhao ||| hanqing lu ||| 
2019 ||| scan: self-and-collaborative attention network for video person re-identification. ||| ruimao zhang ||| jingyu li ||| hongbin sun ||| yuying ge ||| ping luo ||| xiaogang wang ||| liang lin ||| 
2020 ||| image interpolation using multi-scale attention-aware inception network. ||| jiahuan ji ||| baojiang zhong ||| kai-kuang ma ||| 
2021 ||| graph attention layer evolves semantic segmentation for road pothole detection: a benchmark and algorithms. ||| rui fan ||| hengli wang ||| yuan wang ||| ming liu ||| ioannis pitas ||| 
2021 ||| fs-dsm: few-shot diagram-sentence matching via cross-modal attention graph model. ||| xin hu ||| lingling zhang ||| jun liu ||| qinghua zheng ||| jianlong zhou ||| 
2022 ||| improving face-based age estimation with attention-based dynamic patch fusion. ||| haoyi wang ||| victor sanchez ||| chang-tsun li ||| 
2020 ||| spatio-temporal memory attention for image captioning. ||| junzhong ji ||| cheng xu ||| xiaodan zhang ||| boyue wang ||| xinhang song ||| 
2021 ||| action anticipation using pairwise human-object interactions and transformers. ||| debaditya roy ||| basura fernando ||| 
2022 ||| sst: spatial and semantic transformers for multi-label image recognition. ||| zhao-min chen ||| quan cui ||| borui zhao ||| ren-jie song ||| xiaoqin zhang ||| osamu yoshie ||| 
2019 ||| two-level attention network with multi-grain ranking loss for vehicle re-identification. ||| haiyun guo ||| kuan zhu ||| ming tang ||| jinqiao wang ||| 
2020 ||| one-pass multi-task networks with cross-task guided attention for brain tumor segmentation. ||| chenhong zhou ||| changxing ding ||| xinchao wang ||| zhentai lu ||| dacheng tao ||| 
2021 ||| fine-grained 3d shape classification with hierarchical part-view attention. ||| xinhai liu ||| zhizhong han ||| yu-shen liu ||| matthias zwicker ||| 
2017 ||| unifying the video and question attentions for open-ended video question answering. ||| hongyang xue ||| zhou zhao ||| deng cai ||| 
2019 ||| learning semantics-preserving attention and contextual interaction for group activity recognition. ||| yansong tang ||| jiwen lu ||| zian wang ||| ming yang ||| jie zhou ||| 
2021 ||| ap-cnn: weakly supervised attention pyramid convolutional neural network for fine-grained visual classification. ||| yifeng ding ||| zhanyu ma ||| shaoguo wen ||| jiyang xie ||| dongliang chang ||| zhongwei si ||| ming wu ||| haibin ling ||| 
2018 ||| object-part attention model for fine-grained image classification. ||| yuxin peng ||| xiangteng he ||| junjie zhao ||| 
2019 ||| 3d2seqviews: aggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation. ||| zhizhong han ||| honglei lu ||| zhenbao liu ||| chi-man vong ||| yu-shen liu ||| matthias zwicker ||| junwei han ||| c. l. philip chen ||| 
2019 ||| show, attend, and translate: unsupervised image translation with self-regularization and attention. ||| chao yang ||| taehwan kim ||| ruizhe wang ||| hao peng ||| c.-c. jay kuo ||| 
2020 ||| multi-modal recurrent attention networks for facial expression recognition. ||| jiyoung lee ||| sunok kim ||| seungryong kim ||| kwanghoon sohn ||| 
2019 ||| seqviews2seqlabels: learning 3d global features via aggregating sequential views by rnn with attention. ||| zhizhong han ||| mingyang shang ||| zhenbao liu ||| chi-man vong ||| yu-shen liu ||| matthias zwicker ||| junwei han ||| c. l. philip chen ||| 
2018 ||| modality-specific cross-modal similarity measurement with recurrent attention network. ||| yuxin peng ||| jinwei qi ||| yuxin yuan ||| 
2022 ||| siamese implicit region proposal network with compound attention for visual tracking. ||| sixian chan ||| jian tao ||| xiao-long zhou ||| cong bai ||| xiaoqin zhang ||| 
2019 ||| cross-modal attentional context learning for rgb-d object detection. ||| guanbin li ||| yukang gan ||| hejun wu ||| nong xiao ||| liang lin ||| 
2020 ||| pona: pose-guided non-local attention for human pose transfer. ||| kun li ||| jinsong zhang ||| yebin liu ||| yu-kun lai ||| qionghai dai ||| 
2019 ||| multi-turn video question answering via hierarchical attention context reinforced networks. ||| zhou zhao ||| zhu zhang ||| xinghua jiang ||| deng cai ||| 
2021 ||| relational reasoning for group activity recognition via self-attention augmented conditional random field. ||| rizard renanda adhi pramono ||| wen-hsien fang ||| yie-tarng chen ||| 
2021 ||| re-attention for visual question answering. ||| wenya guo ||| ying zhang ||| jufeng yang ||| xiaojie yuan ||| 
2021 ||| floorlevel-net: recognizing floor-level lines with height-attention-guided multi-task learning. ||| mengyang wu ||| wei zeng ||| chi-wing fu ||| 
2021 ||| depth privileged scene recognition via dual attention hallucination. ||| junjie chen ||| li niu ||| liqing zhang ||| 
2021 ||| attend and guide (ag-net): a keypoints-driven attention-based deep network for image recognition. ||| asish bera ||| zachary wharton ||| yonghuai liu ||| nik bessis ||| ardhendu behera ||| 
2020 ||| an efficient fire detection method based on multiscale feature extraction, implicit deep supervision and channel attention mechanism. ||| songbin li ||| qiandong yan ||| peng liu ||| 
2021 ||| mlda-net: multi-level dual attention-based network for self-supervised monocular depth estimation. ||| xibin song ||| wei li ||| dingfu zhou ||| yuchao dai ||| jin fang ||| hongdong li ||| liangjun zhang ||| 
2021 ||| multi-scale spatial attention-guided monocular depth estimation with semantic enhancement. ||| xianfa xu ||| zhe chen ||| fuliang yin ||| 
2020 ||| forecasting future action sequences with attention: a new approach to weakly supervised action forecasting. ||| yan bin ng ||| basura fernando ||| 
2022 ||| structure-aware positional transformer for visible-infrared person re-identification. ||| cuiqun chen ||| mang ye ||| meibin qi ||| jingjing wu ||| jianguo jiang ||| chia-wen lin ||| 
2020 ||| learning recurrent 3d attention for video-based person re-identification. ||| guangyi chen ||| jiwen lu ||| ming yang ||| jie zhou ||| 
2021 ||| end-to-end learnt image compression via non-local attention optimization and improved context modeling. ||| tong chen ||| haojie liu ||| zhan ma ||| qiu shen ||| xun cao ||| yao wang ||| 
2021 ||| person re-identification with reinforced attribute attention selection. ||| jianfu zhang ||| li niu ||| liqing zhang ||| 
2021 ||| self-attention context network: addressing the threat of adversarial attacks for hyperspectral image classification. ||| yonghao xu ||| bo du ||| liangpei zhang ||| 
2020 ||| no-reference image quality assessment: an attention driven approach. ||| diqi chen ||| yizhou wang ||| wen gao ||| 
2022 ||| two-branch attention network via efficient semantic coupling for one-shot learning. ||| jun li ||| duorui wang ||| xianglong liu ||| zhi-ping shi ||| meng wang ||| 
2020 ||| a multi-scale spatial-temporal attention model for person re-identification in videos. ||| wei zhang ||| xuanyu he ||| xiaodong yu ||| weizhi lu ||| zhengjun zha ||| qi tian ||| 
2021 ||| multi-relation attention network for image patch matching. ||| dou quan ||| shuang wang ||| yi li ||| bowu yang ||| ning huyan ||| jocelyn chanussot ||| biao hou ||| licheng jiao ||| 
2021 ||| dense attention fluid network for salient object detection in optical remote sensing images. ||| qijian zhang ||| runmin cong ||| chongyi li ||| ming-ming cheng ||| yuming fang ||| xiaochun cao ||| yao zhao ||| sam kwong ||| 
2022 ||| meta pid attention network for flexible and efficient real-world noisy image denoising. ||| ruijun ma ||| shuyi li ||| bob zhang ||| haifeng hu ||| 
2020 ||| cascaded attention guidance network for single rainy image restoration. ||| guoqing wang ||| changming sun ||| arcot sowmya ||| 
2020 ||| learning a deep dual attention network for video super-resolution. ||| feng li ||| huihui bai ||| yao zhao ||| 
2020 ||| region attention networks for pose and occlusion robust facial expression recognition. ||| kai wang ||| xiaojiang peng ||| jianfei yang ||| debin meng ||| yu qiao ||| 
2022 ||| remote sensing scene classification via multi-branch local attention network. ||| sibao chen ||| qing-song wei ||| wenzhong wang ||| jin tang ||| bin luo ||| zu-yuan wang ||| 
2021 ||| mask-guided attention network and occlusion-sensitive hard example mining for occluded pedestrian detection. ||| jin xie ||| yanwei pang ||| muhammad haris khan ||| rao muhammad anwer ||| fahad shahbaz khan ||| ling shao ||| 
2021 ||| dpanet: depth potentiality-aware gated attention network for rgb-d salient object detection. ||| zuyao chen ||| runmin cong ||| qianqian xu ||| qingming huang ||| 
2020 ||| attention-aware multi-task convolutional neural networks. ||| kejie lyu ||| yingming li ||| zhongfei zhang ||| 
2018 ||| skeleton-based human action recognition with global context-aware attention lstm networks. ||| jun liu ||| gang wang ||| ling-yu duan ||| kamila abdiyeva ||| alex c. kot ||| 
2021 ||| dan: deep-attention network for 3d shape recognition. ||| weizhi nie ||| yue zhao ||| dan song ||| yue gao ||| 
2021 ||| spatial-angular attention network for light field reconstruction. ||| gaochang wu ||| yingqian wang ||| yebin liu ||| lu fang ||| tianyou chai ||| 
2019 ||| cam-rnn: co-attention model based rnn for video captioning. ||| bin zhao ||| xuelong li ||| xiaoqiang lu ||| 
2021 ||| coanet: connectivity attention network for road extraction from satellite imagery. ||| jie mei ||| roujing li ||| wang gao ||| ming-ming cheng ||| 
2020 ||| hierarchical u-shape attention network for salient object detection. ||| sanping zhou ||| jinjun wang ||| jimuyang zhang ||| le wang ||| dong huang ||| shaoyi du ||| nanning zheng ||| 
2021 ||| accurate and fast image denoising via attention guided scaling. ||| yulun zhang ||| kunpeng li ||| kai li ||| gan sun ||| yu kong ||| yun fu ||| 
2022 ||| weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification. ||| yanni dong ||| quanwei liu ||| bo du ||| liangpei zhang ||| 
2020 ||| ha-ccn: hierarchical attention-based crowd counting network. ||| vishwanath a. sindagi ||| vishal m. patel ||| 
2020 ||| improving the harmony of the composite image by spatial-separated attention module. ||| xiaodong cun ||| chi-man pun ||| 
2019 ||| discriminative feature learning with foreground attention for person re-identification. ||| sanping zhou ||| jinjun wang ||| deyu meng ||| yudong liang ||| yihong gong ||| nanning zheng ||| 
2017 ||| visual attention modeling for stereoscopic video: a benchmark and computational model. ||| yuming fang ||| chi zhang ||| jing li ||| jianjun lei ||| matthieu perreira da silva ||| patrick le callet ||| 
2018 ||| deep visual attention prediction. ||| wenguan wang ||| jianbing shen ||| 
2021 ||| learning to discover multi-class attentional regions for multi-label image recognition. ||| bin-bin gao ||| hong-yu zhou ||| 
2020 ||| compositional attention networks with two-stream fusion for video question answering. ||| ting yu ||| jun yu ||| zhou yu ||| dacheng tao ||| 
2020 ||| attribute-guided attention for referring expression generation and comprehension. ||| jingyu liu ||| wei wang ||| liang wang ||| ming-hsuan yang ||| 
2021 ||| aan-face: attention augmented networks for face recognition. ||| qiangchang wang ||| guodong guo ||| 
2021 ||| uncertainty guided multi-scale attention network for raindrop removal from a single image. ||| ming-wen shao ||| le li ||| deyu meng ||| wangmeng zuo ||| 
2022 ||| high-resolution depth maps imaging via attention-based hierarchical multi-modal fusion. ||| zhiwei zhong ||| xianming liu ||| junjun jiang ||| debin zhao ||| zhiwen chen ||| xiangyang ji ||| 
2020 ||| reverse attention-based residual network for salient object detection. ||| shuhan chen ||| xiuli tan ||| ben wang ||| huchuan lu ||| xuelong hu ||| yun fu ||| 
2019 ||| occlusion aware facial expression recognition using cnn with attention mechanism. ||| yong li ||| jiabei zeng ||| shiguang shan ||| xilin chen ||| 
2022 ||| health status diagnosis of distribution transformers based on big data mining. ||| yue wang ||| linfang hu ||| jianwei chen ||| yueqing ren ||| 
2021 ||| predicting the next location: a self-attention and recurrent neural network model with temporal context. ||| jun zeng ||| xin he ||| haoran tang ||| junhao wen ||| 
2021 ||| the three a's of wearable and ubiquitous computing: activity, affect, and attention. ||| kristof van laerhoven ||| 
2021 ||| eeg-based auditory attention detection and its possible future applications for passive bci. ||| joan belo ||| maureen clerc ||| daniele sch ||| n ||| 
2018 ||| data-driven charging strategy of pevs under transformer aging risk. ||| chaojie li ||| chen liu ||| ke deng ||| xinghuo yu ||| tingwen huang ||| 
2019 ||| port-controlled phasor hamiltonian modeling and ida-pbc control of solid-state transformer. ||| ragini v. meshram ||| monika bhagwat ||| shubhangi khade ||| sushama wagh ||| aleksandar m. stankovic ||| navdeep m. singh ||| 
2018 ||| short-term traffic speed forecasting based on attention convolutional neural network for arterials. ||| qingchao liu ||| bochen wang ||| yuquan zhu ||| 
2022 ||| spatiotemporal gated graph attention network for urban traffic flow prediction based on license plate recognition data. ||| jinjun tang ||| jie zeng ||| 
2018 ||| visual attention-based access: granting access based on users' joint attention on shared workspaces. ||| baris serim ||| ken pfeuffer ||| hans gellersen ||| giulio jacucci ||| 
2020 ||| giobalfusion: a global attentional deep learning framework for multisensor information fusion. ||| shengzhong liu ||| shuochao yao ||| jinyang li ||| dongxin liu ||| tianshi wang ||| huajie shao ||| tarek f. abdelzaher ||| 
2018 ||| mindid: person identification from brain waves through attention-based recurrent neural network. ||| xiang zhang ||| lina yao ||| salil s. kanhere ||| yunhao liu ||| tao gu ||| kaixuan chen ||| 
2021 ||| exploration of person-independent bcis for internal and external attention-detection in augmented reality. ||| lisa-marie vortmann ||| felix putze ||| 
2019 ||| classifying attention types with thermal imaging and eye tracking. ||| yomna abdelrahman ||| anam ahmad khan ||| joshua newn ||| eduardo velloso ||| sherine ashraf safwat ||| james bailey ||| andreas bulling ||| frank vetere ||| albrecht schmidt ||| 
2020 ||| mail: multi-scale attention-guided indoor localization using geomagnetic sequences. ||| qun niu ||| tao he ||| ning liu ||| suining he ||| xiaonan luo ||| fan zhou ||| 
2018 ||| a survey of attention management systems in ubiquitous computing environments. ||| christoph anderson ||| isabel fernanda h ||| bener ||| ann-kathrin seipp ||| sandra ohly ||| klaus david ||| veljko pejovic ||| 
2021 ||| opitrack: a wearable-based clinical opioid use tracker with temporal convolutional attention networks. ||| bhanu teja gullapalli ||| stephanie carreiro ||| brittany p. chapman ||| deepak ganesan ||| jan sjoquist ||| tauhidur rahman ||| 
2021 ||| to see or not to see: exploring inattentional blindness for the design of unobtrusive interfaces in shared public places. ||| linda hirsch ||| christina schneegass ||| robin welsch ||| andreas butz ||| 
2021 ||| memx: an attention-aware smart eyewear system for personalized moment auto-capture. ||| yuhu chang ||| yingying zhao ||| mingzhi dong ||| yujiang wang ||| yutian lu ||| qin lv ||| robert p. dick ||| tun lu ||| ning gu ||| li shang ||| 
2021 ||| multi-head spatio-temporal attention mechanism for urban anomaly event prediction. ||| huiqun huang ||| xi yang ||| suining he ||| 
2019 ||| decentralized attention-based personalized human mobility prediction. ||| zipei fan ||| xuan song ||| renhe jiang ||| quanjun chen ||| ryosuke shibasaki ||| 
2020 ||| segment boundary detection directed attention for online end-to-end speech recognition. ||| junfeng hou ||| wu guo ||| yan song ||| li-rong dai ||| 
2019 ||| a new joint ctc-attention-based speech recognition model with multi-level multi-head attention. ||| chu-xiong qin ||| wen-lin zhang ||| dan qu ||| 
2021 ||| neural network-based non-intrusive speech quality assessment using attention pooling function. ||| miao liu ||| jing wang ||| weiming yi ||| fang liu ||| 
2021 ||| adversarial joint training with self-attention mechanism for robust end-to-end speech recognition. ||| lujun li ||| yikai kang ||| yuchen shi ||| ludwig k ||| rzinger ||| tobias watzel ||| gerhard rigoll ||| 
2017 ||| the effects of attention monitoring with eeg biofeedback on university students' attention and self-efficacy: the case of anti-phishing instructional materials. ||| jerry chih-yuan sun ||| katherine pin-chen yeh ||| 
2017 ||| the indirect relationship of media multitasking self-efficacy on learning performance within the personal learning environment: implications from the mechanism of perceived attention problems and self-regulation strategies. ||| jiun-yu wu ||| 
2018 ||| exploring effects of discussion on visual attention, learning performance, and perceptions of students learning with str-support. ||| wu-yuin hwang ||| yung-hui li ||| rustam shadiev ||| 
2021 ||| the learning behaviours of dropouts in moocs: a collective attention network perspective. ||| jingjing zhang ||| ming gao ||| jiang zhang ||| 
2017 ||| seeing the instructor's face and gaze in demonstration video examples affects attention allocation but not learning. ||| margot van wermeskerken ||| tamara van gog ||| 
2019 ||| who is better adapted in learning online within the personal learning environment? relating gender differences in cognitive attention networks to digital distraction. ||| jiun-yu wu ||| tzuying cheng ||| 
2020 ||| does visual attention to the instructor in online video affect learning and learner perceptions? an eye-tracking analysis. ||| jiahui wang ||| pavlo d. antonenko ||| kara m. dawson ||| 
2020 ||| electronic storybook design, kindergartners' visual attention, and print awareness: an eye-tracking investigation. ||| chia-ning liao ||| kuo-en chang ||| yu-ching huang ||| yao-ting sung ||| 
2021 ||| generative chemical transformer: neural machine learning of molecular geometric structures from chemical language via attention. ||| hyunseung kim ||| jonggeol na ||| won bo lee ||| 
2020 ||| signal-3l 3.0: improving signal peptide prediction through combining attention deep learning with window-based scoring. ||| wei-xun zhang ||| xiaoyong pan ||| hong-bin shen ||| 
2021 ||| hidra: hierarchical network for drug response prediction with attention. ||| iljung jin ||| hojung nam ||| 
2019 ||| deepchemstable: chemical stability prediction with an attention-based graph convolution network. ||| xiuming li ||| xin yan ||| qiong gu ||| huihao zhou ||| di wu ||| jun xu ||| 
2019 ||| identifying structure-property relationships through smiles syntax analysis with self-attention mechanism. ||| shuangjia zheng ||| xin yan ||| yuedong yang ||| jun xu ||| 
2020 ||| predicting binding from screening assays with transformer network embeddings. ||| paul morris ||| rachel st. clair ||| william edward hahn ||| elan barenholtz ||| 
2020 ||| predicting retrosynthetic reactions using self-corrected transformer neural networks. ||| shuangjia zheng ||| jiahua rao ||| zhongyue zhang ||| jun xu ||| yuedong yang ||| 
2021 ||| msa-regularized protein sequence transformer toward predicting genome-wide chemical-protein interactions: application to gpcrome deorphanization. ||| tian cai ||| hansaim lim ||| kyra alyssa abbu ||| yue qiu ||| ruth nussinov ||| lei xie ||| 
2021 ||| valid, plausible, and diverse retrosynthesis using tied two-way transformers with latent variables. ||| eunji kim ||| dongseon lee ||| youngchun kwon ||| min sik park ||| youn-suk choi ||| 
2021 ||| molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. ||| mikolaj sacha ||| mikolaj blaz ||| piotr byrski ||| pawel dabrowski-tumanski ||| mikolaj chrominski ||| rafal loska ||| pawel wlodarczyk-pruszynski ||| stanislaw jastrzebski ||| 
2020 ||| predicting the feasibility of copper(i)-catalyzed alkyne-azide cycloaddition reactions using a recurrent neural network with a self-attention mechanism. ||| shimin su ||| yuyao yang ||| hanlin gan ||| shuangjia zheng ||| fenglong gu ||| chao zhao ||| jun xu ||| 
2022 ||| protein interaction network reconstruction with a structural gated attention deep model by incorporating network structure information. ||| fei zhu ||| feifei li ||| lei deng ||| fanwang meng ||| zhongjie liang ||| 
2021 ||| transferable multilevel attention neural network for accurate prediction of quantum chemistry properties via multitask learning. ||| ziteng liu ||| liqiang lin ||| qingqing jia ||| zheng cheng ||| yanyan jiang ||| yanwen guo ||| jing ma ||| 
2021 ||| applying transformer insulation using weibull extended distribution based on progressive censoring scheme. ||| hisham m. almongy ||| fatma y. alshenawy ||| ehab m. almetwally ||| doaa a. abdo ||| 
2017 ||| big data and the attention economy: big data (ubiquity symposium). ||| bernardo a. huberman ||| 
2021 ||| expression eeg multimodal emotion recognition method based on the bidirectional lstm and attention mechanism. ||| yifeng zhao ||| deyun chen ||| 
2020 ||| the eeg-based attention analysis in multimedia m-learning. ||| dan ni ||| shuo wang ||| guocheng liu ||| 
2020 ||| acnnt3: attention-cnn framework for prediction of sequence-based bacterial type iii secreted effectors. ||| jie li ||| zhong li ||| jiesi luo ||| yuhua yao ||| 
2018 ||| a new approach for advertising ctr prediction based on deep neural network via attention mechanism. ||| qianqian wang ||| fang'ai liu ||| shuning xing ||| xiaohui zhao ||| 
2020 ||| attention optimization method for eeg via the tgam. ||| yu wu ||| ning xie ||| 
2018 ||| fpan: fine-grained and progressive attention localization network for data retrieval. ||| sijia chen ||| bin song ||| jie guo ||| yanling zhang ||| xiaojiang du ||| mohsen guizani ||| 
2020 ||| guided soft attention network for classification of breast cancer histopathology images. ||| heechan yang ||| ji-ye kim ||| hyongsuk kim ||| shyam prasad adhikari ||| 
2020 ||| global pixel transformers for virtual staining of microscopy images. ||| yi liu ||| hao yuan ||| zhengyang wang ||| shuiwang ji ||| 
2021 ||| automated skin lesion segmentation via an adaptive dual attention module. ||| huisi wu ||| junquan pan ||| zhuoying li ||| zhenkun wen ||| jing qin ||| 
2020 ||| attention by selection: a deep selective attention approach to breast cancer classification. ||| bolei xu ||| jingxin liu ||| xianxu hou ||| bozhi liu ||| jon garibaldi ||| ian o. ellis ||| andy green ||| linlin shen ||| guoping qiu ||| 
2021 ||| ca-net: comprehensive attention convolutional neural networks for explainable medical image segmentation. ||| ran gu ||| guotai wang ||| tao song ||| rui huang ||| michael aertsen ||| jan deprest ||| s ||| bastien ourselin ||| tom vercauteren ||| shaoting zhang ||| 
2021 ||| lcanet: learnable connected attention network for human identification using dental images. ||| yancun lai ||| fei fan ||| qingsong wu ||| wenchi ke ||| peixi liao ||| zhenhua deng ||| hu chen ||| yi zhang ||| 
2019 ||| attention to lesion: lesion-aware convolutional neural network for retinal optical coherence tomography image classification. ||| leyuan fang ||| chong wang ||| shutao li ||| hossein rabbani ||| xiangdong chen ||| zhimin liu ||| 
2020 ||| learning an attention model for robust 2-d/3-d registration using point-to-plane correspondences. ||| roman schaffert ||| jian wang ||| peter fischer ||| anja borsdorf ||| andreas k. maier ||| 
2021 ||| cabnet: category attention block for imbalanced diabetic retinopathy grading. ||| along he ||| tao li ||| ning li ||| kai wang ||| huazhu fu ||| 
2021 ||| contour transformer network for one-shot segmentation of anatomical structures. ||| yuhang lu ||| kang zheng ||| weijian li ||| yirui wang ||| adam p. harrison ||| chihung lin ||| song wang ||| jing xiao ||| le lu ||| chang-fu kuo ||| shun miao ||| 
2021 ||| ala-net: adaptive lesion-aware attention network for 3d colorectal tumor segmentation. ||| yankai jiang ||| shufeng xu ||| hongjie fan ||| jiahong qian ||| weizhi luo ||| shihui zhen ||| yubo tao ||| jihong sun ||| hai lin ||| 
2021 ||| hierarchical temporal attention network for thyroid nodule recognition using dynamic ceus imaging. ||| peng wan ||| fang chen ||| chunrui liu ||| wentao kong ||| daoqiang zhang ||| 
2021 ||| 3d multi-attention guided multi-task learning network for automatic gastric tumor segmentation and lymph node classification. ||| yongtao zhang ||| haimei li ||| jie du ||| jing qin ||| tianfu wang ||| yue chen ||| bing liu ||| wenwen gao ||| guolin ma ||| baiying lei ||| 
2021 ||| deep relation transformer for diagnosing glaucoma with optical coherence tomography and visual field function. ||| diping song ||| bin fu ||| fei li ||| jian xiong ||| junjun he ||| xiulan zhang ||| yu qiao ||| 
2020 ||| canet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading. ||| xiaomeng li ||| xiaowei hu ||| lequan yu ||| lei zhu ||| chi-wing fu ||| pheng-ann heng ||| 
2020 ||| accurate screening of covid-19 using attention-based deep 3d multiple instance learning. ||| zhongyi han ||| benzheng wei ||| yanfei hong ||| tianyang li ||| jinyu cong ||| xue zhu ||| haifeng wei ||| wei zhang ||| 
2021 ||| self-supervised attention mechanism for pediatric bone age assessment with efficient weak annotation. ||| chuanbin liu ||| hongtao xie ||| yongdong zhang ||| 
2020 ||| prior-attention residual learning for more discriminative covid-19 screening in ct images. ||| jun wang ||| yiming bao ||| yaofeng wen ||| hongbing lu ||| hu luo ||| yunfei xiang ||| xiaoming li ||| chen liu ||| dahong qian ||| 
2020 ||| anatomical attention guided deep networks for roi segmentation of brain mr images. ||| liang sun ||| wei shao ||| daoqiang zhang ||| mingxia liu ||| 
2020 ||| dual-sampling attention network for diagnosis of covid-19 from community acquired pneumonia. ||| xi ouyang ||| jiayu huo ||| liming xia ||| fei shan ||| jun liu ||| zhanhao mo ||| fuhua yan ||| zhongxiang ding ||| qi yang ||| bin song ||| feng shi ||| huan yuan ||| ying wei ||| xiaohuan cao ||| yaozong gao ||| dijia wu ||| qian wang ||| dinggang shen ||| 
2021 ||| dual attention multi-instance deep learning for alzheimer's disease diagnosis with structural mri. ||| wenyong zhu ||| liang sun ||| jiashuang huang ||| liangxiu han ||| daoqiang zhang ||| 
2021 ||| mama net: multi-scale attention memory autoencoder network for anomaly detection. ||| yurong chen ||| hui zhang ||| yaonan wang ||| yimin yang ||| xianen zhou ||| q. m. jonathan wu ||| 
2020 ||| attention-diffusion-bilinear neural network for brain network analysis. ||| jiashuang huang ||| luping zhou ||| lei wang ||| daoqiang zhang ||| 
2020 ||| zoom in lesions for better diagnosis: attention guided deformation network for wce image classification. ||| xiaohan xing ||| yixuan yuan ||| max q.-h. meng ||| 
2019 ||| tetris: template transformer networks for image segmentation with shape priors. ||| matthew c. h. lee ||| kersten petersen ||| nick pawlowski ||| ben glocker ||| michiel schaap ||| 
2020 ||| attentionboost: learning what to attend for gland segmentation in histopathological images by boosting fully convolutional networks. ||| gozde nur gunesli ||| cenk sokmensuer ||| cigdem gunduz demir ||| 
2021 ||| learning to segment from scribbles using multi-scale adversarial attention gates. ||| gabriele valvano ||| andrea leo ||| sotirios a. tsaftaris ||| 
2021 ||| nhbs-net: a feature fusion attention network for ultrasound neonatal hip bone segmentation. ||| ruhan liu ||| mengyao liu ||| bin sheng ||| huating li ||| ping li ||| haitao song ||| ping zhang ||| lixin jiang ||| dinggang shen ||| 
2022 ||| semi-supervised segmentation of radiation-induced pulmonary fibrosis from lung ct scans with multi-scale guided dense attention. ||| guotai wang ||| shuwei zhai ||| giovanni lasio ||| baoshe zhang ||| byong yi ||| shifeng chen ||| thomas j. macvittie ||| dimitris n. metaxas ||| jinghao zhou ||| shaoting zhang ||| 
2022 ||| global-local transformer for brain age estimation. ||| sheng he ||| patricia ellen grant ||| yangming ou ||| 
2021 ||| limited view tomographic reconstruction using a cascaded residual dense spatial-channel attention network with projection data fidelity layer. ||| bo zhou ||| s. kevin zhou ||| james s. duncan ||| chi liu ||| 
2021 ||| diagnostic regions attention network (dra-net) for histopathology wsi recommendation and retrieval. ||| yushan zheng ||| zhiguo jiang ||| fengying xie ||| jun shi ||| haopeng zhang ||| jianguo huai ||| ming cao ||| xiaomiao yang ||| 
2021 ||| learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction. ||| yan wang ||| peng tang ||| yuyin zhou ||| wei shen ||| elliot k. fishman ||| alan l. yuille ||| 
2019 ||| attention residual learning for skin lesion classification. ||| jianpeng zhang ||| yutong xie ||| yong xia ||| chunhua shen ||| 
2019 ||| learning where to see: a novel attention model for automated immunohistochemical scoring. ||| talha qaiser ||| nasir m. rajpoot ||| 
2021 ||| multi-modal retinal image classification with modality-specific attention network. ||| xingxin he ||| ying deng ||| leyuan fang ||| qinghua peng ||| 
2020 ||| sacnn: self-attention convolutional neural network for low-dose ct denoising with self-supervised perceptual loss network. ||| meng li ||| william hsu ||| xiaodong xie ||| jason cong ||| wen gao ||| 
2021 ||| learning hierarchical attention for weakly-supervised chest x-ray abnormality localization and diagnosis. ||| xi ouyang ||| srikrishna karanam ||| ziyan wu ||| terrence chen ||| jiayu huo ||| xiang sean zhou ||| qian wang ||| jie-zhi cheng ||| 
2021 ||| artifact and detail attention generative adversarial networks for low-dose ct denoising. ||| xiong zhang ||| zefang han ||| hong shangguan ||| xinglong han ||| xueying cui ||| anhong wang ||| 
2020 ||| a large-scale database and a cnn model for attention-based glaucoma detection. ||| liu li ||| mai xu ||| hanruo liu ||| yang li ||| xiaofei wang ||| lai jiang ||| zulin wang ||| xiang fan ||| ningli wang ||| 
2020 ||| chordiogram image descriptor based on visual attention model for image retrieval. ||| s. sathiamoorthy ||| a. saravanan ||| r. ponnusamy ||| 
2021 ||| the principle of intelligent switch composition and algorithm of the built-in electronic voltage transformer. ||| xinhui cao ||| yuting dong ||| zhongzheng li ||| juan li ||| gang liang ||| honglian zhou ||| 
2021 ||| key n -gram extractions and analyses of different registers based on attention network. ||| haiyan wu ||| ying liu ||| shaoyun shi ||| qingfeng wu ||| yunlong huang ||| 
2022 ||| neural acoustic-phonetic approach for speaker verification with phonetic attention mask. ||| tianchi liu ||| rohan kumar das ||| kong aik lee ||| haizhou li ||| 
2020 ||| canet: concatenated attention neural network for image restoration. ||| yingjie tian ||| yiqi wang ||| linrui yang ||| zhiquan qi ||| 
2021 ||| macro: multi-attention convolutional recurrent model for subject-independent erp detection. ||| zhen lan ||| chao yan ||| zixing li ||| dengqing tang ||| xiaojia xiang ||| 
2018 ||| end-to-end feature integration for correlation filter tracking with channel attention. ||| dongdong li ||| gongjian wen ||| yangliu kuai ||| fatih porikli ||| 
2019 ||| multi-level dual-attention based cnn for macular optical coherence tomography classification. ||| sapna s. mishra ||| bappaditya mandal ||| niladri b. puhan ||| 
2019 ||| a skip attention mechanism for monaural singing voice separation. ||| weitao yuan ||| shengbei wang ||| xiangrui li ||| masashi unoki ||| wenwu wang ||| 
2021 ||| ata: attentional non-linear activation function approximation for vlsi-based neural networks. ||| linyu wei ||| jueping cai ||| wuzhuang wang ||| 
2021 ||| graphtte: travel time estimation based on attention-spatiotemporal graphs. ||| qiang wang ||| chen xu ||| wenqi zhang ||| jingjing li ||| 
2020 ||| memory attention: robust alignment using gating mechanism for end-to-end speech synthesis. ||| joun yeop lee ||| sung jun cheon ||| byoung jin choi ||| nam soo kim ||| 
2021 ||| interactive multimodal attention network for emotion recognition in conversation. ||| minjie ren ||| xiangdong huang ||| xiaoqi shi ||| weizhi nie ||| 
2020 ||| spectro-temporal attention-based voice activity detection. ||| younglo lee ||| jeongki min ||| david k. han ||| hanseok ko ||| 
2022 ||| a nested u-net with self-attention and dense connectivity for monaural speech enhancement. ||| xiaoxiao xiang ||| xiaojuan zhang ||| haozhe chen ||| 
2021 ||| wavelet multi-level attention capsule network for texture classification. ||| zhiyong tao ||| tong wei ||| jie li ||| 
2020 ||| interlayer selective attention network for robust personalized wake-up word detection. ||| hyungjun lim ||| younggwan kim ||| jahyun goo ||| hoirin kim ||| 
2022 ||| biattnnet: bilateral attention for improving real-time semantic segmentation. ||| genling li ||| liang li ||| jiawan zhang ||| 
2022 ||| cooperative light-field image super-resolution based on multi-modality embedding and fusion with frequency attention. ||| hui yao ||| jieji ren ||| xiangchao yan ||| mingjun ren ||| 
2022 ||| heterogeneous attention nested u-shaped network for blur detection. ||| wenliang guo ||| xiao xiao ||| yilong hui ||| wenming yang ||| amir sadovnik ||| 
2020 ||| a stereo attention module for stereo image super-resolution. ||| xinyi ying ||| yingqian wang ||| longguang wang ||| weidong sheng ||| wei an ||| yulan guo ||| 
2022 ||| hybrid autoregressive and non-autoregressive transformer models for speech recognition. ||| zhengkun tian ||| jiangyan yi ||| jianhua tao ||| shuai zhang ||| zhengqi wen ||| 
2021 ||| a deep feature fusion network based on multiple attention mechanisms for joint iris-periocular biometric recognition. ||| zhengding luo ||| junting li ||| yuesheng zhu ||| 
2021 ||| inheritance attention matrix-based universal adversarial perturbations on vision transformers. ||| haoqi hu ||| xiaofeng lu ||| xinpeng zhang ||| tianxing zhang ||| guangling sun ||| 
2021 ||| hanme: hierarchical attention network for singing melody extraction. ||| shuai yu ||| yi yu ||| xi chen ||| wei li ||| 
2022 ||| rgtransformer: region-graph transformer for image representation and few-shot classification. ||| bo jiang ||| kangkang zhao ||| jin tang ||| 
2018 ||| voice activity detection using an adaptive context attention model. ||| juntae kim ||| minsoo hahn ||| 
2022 ||| mtt: multi-scale temporal transformer for skeleton-based action recognition. ||| jun kong ||| yuhang bian ||| min jiang ||| 
2021 ||| non-autoregressive transformer for speech recognition. ||| nanxin chen ||| shinji watanabe ||| jes ||| s villalba ||| piotr zelasko ||| najim dehak ||| 
2019 ||| visual attention network for low-dose ct. ||| wenchao du ||| hu chen ||| peixi liao ||| hongyu yang ||| ge wang ||| yi zhang ||| 
2021 ||| pixel-attention cnn with color correlation loss for color image denoising. ||| fan jia ||| liyan ma ||| yijin yang ||| tieyong zeng ||| 
2021 ||| disentangling 3d/4d facial affect recognition with faster multi-view transformer. ||| muzammil behzad ||| xiaobai li ||| guoying zhao ||| 
2022 ||| apan: across-scale progressive attention network for single image deraining. ||| qiang wang ||| gan sun ||| huijie fan ||| wentao li ||| yandong tang ||| 
2018 ||| 3-d convolutional recurrent neural networks with attention model for speech emotion recognition. ||| mingyi chen ||| xuanji he ||| jing yang ||| han zhang ||| 
2021 ||| dilated-scale-aware category-attention convnet for multi-class object counting. ||| wei xu ||| dingkang liang ||| yixiao zheng ||| jiahao xie ||| zhanyu ma ||| 
2022 ||| light field image super-resolution with transformers. ||| zhengyu liang ||| yingqian wang ||| longguang wang ||| jun-gang yang ||| shilin zhou ||| 
2021 ||| improving super-resolution performance using meta-attention layers. ||| matthew aquilina ||| christian galea ||| john abela ||| kenneth p. camilleri ||| reuben a. farrugia ||| 
2020 ||| dually connected deraining net using pixel-wise attention. ||| weihong ren ||| jiandong tian ||| qiang wang ||| yandong tang ||| 
2019 ||| three-stream network with bidirectional self-attention for action recognition in extreme low resolution videos. ||| didik purwanto ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2021 ||| graph attention networks adjusted bi-lstm for video summarization. ||| rui zhong ||| rui wang ||| yang zou ||| zhiqiang hong ||| min hu ||| 
2022 ||| actionness-guided transformer for anchor-free temporal action localization. ||| peisen zhao ||| lingxi xie ||| ya zhang ||| qi tian ||| 
2022 ||| hashformer: vision transformer based deep hashing for image retrieval. ||| tao li ||| zheng zhang ||| lishen pei ||| yan gan ||| 
2020 ||| bi-directional seed attention network for interactive image segmentation. ||| gwangmo song ||| kyoung mu lee ||| 
2019 ||| a convolutional recurrent attention model for subject-independent eeg signal analysis. ||| dalin zhang ||| lina yao ||| kaixuan chen ||| jessica monaghan ||| 
2020 ||| curriculum enhanced supervised attention network for person re-identification. ||| xiaoguang zhu ||| jiuchao qian ||| haoyu wang ||| peilin liu ||| 
2021 ||| ga-net: global attention network for point cloud semantic segmentation. ||| shuang deng ||| qiulei dong ||| 
2021 ||| transrppg: remote photoplethysmography transformer for 3d mask face presentation attack detection. ||| zitong yu ||| xiaobai li ||| pichao wang ||| guoying zhao ||| 
2021 ||| a unimodal reinforced transformer with time squeeze fusion for multimodal sentiment analysis. ||| jiaxuan he ||| sijie mai ||| haifeng hu ||| 
2021 ||| multi-attention network for unsupervised video object segmentation. ||| guifang zhang ||| hon-cheng wong ||| sio-long lo ||| 
2020 ||| adaptively leverage unlabeled tracklets based on part attention model for few-example re-id. ||| jian han ||| yali li ||| shengjin wang ||| 
2022 ||| dropdim: a regularization method for transformer networks. ||| hao zhang ||| dan qu ||| keji shao ||| xu-kui yang ||| 
2021 ||| self-guided body part alignment with relation transformers for occluded person re-identification. ||| guanshuo wang ||| xiong chen ||| jialin gao ||| xi zhou ||| shiming ge ||| 
2021 ||| channel and space attention neural network for image denoising. ||| yi wang ||| xiao song ||| kai chen ||| 
2021 ||| complemental attention multi-feature fusion network for fine-grained classification. ||| zhuang miao ||| xun zhao ||| jiabao wang ||| yang li ||| hang li ||| 
2021 ||| manet: multi-scale attention network for correspondence learning. ||| yukai chen ||| linxin zheng ||| xin liu ||| guobao xiao ||| 
2021 ||| a convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement. ||| xiaoxiao xiang ||| xiaojuan zhang ||| haozhe chen ||| 
2018 ||| residual lstm attention network for object tracking. ||| hong-in kim ||| rae-hong park ||| 
2020 ||| corrections to "three-stream network with bidirectional self-attention for action recognition in extreme low resolution videos". ||| didik purwanto ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2020 ||| image editing via segmentation guided self-attention network. ||| jianfu zhang ||| peiming yang ||| wentao wang ||| yan hong ||| liqing zhang ||| 
2018 ||| rescoring of n-best hypotheses using top-down selective attention for automatic speech recognition. ||| ho-gyeong kim ||| hwaran lee ||| geon-min kim ||| sang-hoon oh ||| soo-young lee ||| 
2022 ||| acoustic word embedding based on multi-head attention quadruplet network. ||| shirong zhu ||| ying zhang ||| kai he ||| lasheng zhao ||| 
2021 ||| a two-stage spatiotemporal attention convolution network for continuous dimensional emotion recognition from facial video. ||| min hu ||| qian chu ||| xiaohua wang ||| lei he ||| fuji ren ||| 
2020 ||| attention aggregation encoder-decoder network framework for stereo matching. ||| yaru zhang ||| yaqian li ||| yating kong ||| bin liu ||| 
2017 ||| redesign of the attention process of patients with rheumatologic diseases: assessing the performance with analytic hierarchy process. ||| iouri gorbanev ||| ariel cortes ||| sandra agudelo-londo ||| o ||| 
2021 ||| simultaneous remote monitoring of transformers' ambient parameters by using iot. ||| m. hasir ||| serap cekli ||| cengiz polat uzunoglu ||| 
2021 ||| add: attention-based deepfake detection approach. ||| aminollah khormali ||| jiann-shiun yuan ||| 
2022 ||| the currency of the attentional economy: the uses and abuses of attention in our world. ||| jordan r. schoenherr ||| 
2022 ||| a deep learning approach for classifying vulnerability descriptions using self attention based neural network. ||| p. r. vishnu ||| p. vinod ||| suleiman y. yerima ||| 
2022 ||| nonlocal convolutional block attention module vnet for gliomas automatic segmentation. ||| ying fang ||| he huang ||| weiji yang ||| xiaomei xu ||| weiwei jiang ||| xiaobo lai ||| 
2021 ||| automatic covid-19 ct segmentation using u-net integrated spatial and channel attention mechanism. ||| tongxue zhou ||| st ||| phane canu ||| su ruan ||| 
2020 ||| spatio-temporal context based recurrent visual attention model for lymph node detection. ||| haixin peng ||| yinjun peng ||| 
2022 ||| breast cancer histopathological image classification using attention high-order deep network. ||| ying zou ||| jianxin zhang ||| shan huang ||| bin liu ||| 
2021 ||| deep learning super-resolution electron microscopy based on deep residual attention network. ||| jia wang ||| chuwen lan ||| caiyong wang ||| zehua gao ||| 
2022 ||| att2resnet: a deep attention-based approach for melanoma skin cancer classification. ||| said yacine boulahia ||| mohamed akrem benatia ||| abderrahmane bouzar ||| 
2022 ||| adopting attention-mechanisms for neural logic rule layers. ||| jan niclas reimann ||| andreas schwung ||| steven x. ding ||| 
2021 ||| exploring the distribution regularities of user attention and sentiment toward product aspects in online reviews. ||| chenglei qin ||| chengzhi zhang ||| yi bu ||| 
2019 ||| rumour verification through recurring information and an inner-attention mechanism. ||| ahmet aker ||| alfred sliwa ||| fahim dalvi ||| kalina bontcheva ||| 
2019 ||| collective attention patterns under controlled conditions. ||| marijn ten thij ||| andreas kaltenbrunner ||| david laniado ||| yana volkovich ||| 
2022 ||| predicate transformer semantics for hybrid systems. ||| jonathan juli ||| n huerta y munive ||| georg struth ||| 
2018 ||| a multi-criteria computer package for power transformer fault detection and diagnosis. ||| carlos roncero-clemente ||| eugenio roanes-lozano ||| 
2022 ||| eigat: incorporating global information in local attention for knowledge representation learning. ||| yu zhao ||| huali feng ||| han zhou ||| yanruo yang ||| xingyan chen ||| ruobing xie ||| fuzhen zhuang ||| qing li ||| 
2021 ||| coarse-to-fine: a dual-view attention network for click-through rate prediction. ||| kaitao song ||| qingkang huang ||| faen zhang ||| jianfeng lu ||| 
2022 ||| holistic graph neural networks based on a global-based attention mechanism. ||| asmaa rassil ||| hiba chougrad ||| hamid zouaki ||| 
2022 ||| ham-net: predictive business process monitoring with a hierarchical attention mechanism. ||| abdulrahman jalayer ||| mohsen kahani ||| asef pourmasoumi ||| amin beheshti ||| 
2022 ||| association rules enhanced knowledge graph attention network. ||| zhenghao zhang ||| jianbin huang ||| qinglin tan ||| 
2022 ||| self-attention neural architecture search for semantic image segmentation. ||| zhenkun fan ||| guosheng hu ||| xin sun ||| gaige wang ||| junyu dong ||| chi su ||| 
2021 ||| spatio-attention embedded recurrent neural network for air quality prediction. ||| yu huang ||| josh jia-ching ying ||| vincent s. tseng ||| 
2018 ||| domain attention model for multi-domain sentiment classification. ||| zhigang yuan ||| sixing wu ||| fangzhao wu ||| junxin liu ||| yongfeng huang ||| 
2021 ||| channel-spatial attention-based pan-sharpening of very high-resolution satellite images. ||| peijuan wang ||| elif sertel ||| 
2021 ||| attentional memory network with correlation-based embedding for time-aware poi recommendation. ||| meihui shi ||| derong shen ||| yue kou ||| tiezheng nie ||| ge yu ||| 
2021 ||| an end-to-end atrial fibrillation detection by a novel residual-based temporal attention convolutional neural network with exponential nonlinearity loss. ||| yibo gao ||| huan wang ||| zuhao liu ||| 
2022 ||| explainable attention guided adversarial deep network for 3d radiotherapy dose distribution prediction. ||| huidong li ||| xingchen peng ||| jie zeng ||| jianghong xiao ||| dong nie ||| chen zu ||| xi wu ||| jiliu zhou ||| yan wang ||| 
2021 ||| history-based attention in seq2seq model for multi-label text classification. ||| yaoqiang xiao ||| yi li ||| jin yuan ||| songrui guo ||| yi xiao ||| zhiyong li ||| 
2022 ||| phrase dependency relational graph attention network for aspect-based sentiment analysis. ||| haiyan wu ||| zhiqiang zhang ||| shaoyun shi ||| qingfeng wu ||| haiyu song ||| 
2021 ||| pose transfer generation with semantic parsing attention network for person re-identification. ||| meichen liu ||| kejun wang ||| ruihang ji ||| shuzhi sam ge ||| jing chen ||| 
2022 ||| gated attention fusion network for multimodal sentiment classification. ||| yongping du ||| yang liu ||| zhi peng ||| xingnan jin ||| 
2021 ||| dnnattention: a deep neural network and attention based architecture for cross project defect number prediction. ||| sushant kumar pandey ||| anil kumar tripathi ||| 
2019 ||| ea-lstm: evolutionary attention-based lstm for time series prediction. ||| youru li ||| zhenfeng zhu ||| deqiang kong ||| hua han ||| yao zhao ||| 
2020 ||| deep learning models and datasets for aspect term sentiment classification: implementing holistic recurrent attention on target-dependent memories. ||| hyunjung park ||| minchae song ||| kyung-shik shin ||| 
2021 ||| positionless aspect based sentiment analysis using attention mechanism. ||| rohan kumar yadav ||| lei jiao ||| morten goodwin ||| ole-christoffer granmo ||| 
2021 ||| iit-gat: instance-level image transformation via unsupervised generative attention networks with disentangled representations. ||| mingwen shao ||| youcai zhang ||| yuan fan ||| wangmeng zuo ||| deyu meng ||| 
2022 ||| mols-net: multi-organ and lesion segmentation network based on sequence feature pyramid and attention mechanism for aortic dissection diagnosis. ||| qingyang zhou ||| jiaohua qin ||| xuyu xiang ||| yun tan ||| yu ren ||| 
2022 ||| multi-view informed attention-based model for irony and satire detection in spanish variants. ||| reynier ortega-bueno ||| paolo rosso ||| jos |||  medina-pagola ||| 
2020 ||| multi-scale generative adversarial inpainting network based on cross-layer attention transfer mechanism. ||| mingwen shao ||| wentao zhang ||| wangmeng zuo ||| deyu meng ||| 
2020 ||| relation classification via knowledge graph enhanced transformer encoder. ||| wenti huang ||| yiyu mao ||| zhan yang ||| lei zhu ||| jun long ||| 
2021 ||| lgattnet: automatic micro-expression detection using dual-stream local and global attentions. ||| madhumita a. takalkar ||| selvarajah thuseethan ||| sutharshan rajasegarar ||| zenon chaczko ||| min xu ||| john yearwood ||| 
2020 ||| visual question answering via combining inferential attention and semantic space mapping. ||| yun liu ||| xiaoming zhang ||| feiran huang ||| zhibo zhou ||| zhonghua zhao ||| zhoujun li ||| 
2021 ||| st-lbagan: spatio-temporal learnable bidirectional attention generative adversarial networks for missing traffic data imputation. ||| bing yang ||| yan kang ||| yaoyao yuan ||| xin huang ||| hao li ||| 
2021 ||| dual-graph convolutional network based on band attention and sparse constraint for hyperspectral band selection. ||| jie feng ||| zhanwei ye ||| shuai liu ||| xiangrong zhang ||| jiantong chen ||| ronghua shang ||| licheng jiao ||| 
2021 ||| deep transformer modeling via grouping skip connection for neural machine translation. ||| yachao li ||| junhui li ||| min zhang ||| 
2021 ||| see, hear, read: leveraging multimodality with guided attention for abstractive text summarization. ||| yash kumar atri ||| shraman pramanick ||| vikram goyal ||| tanmoy chakraborty ||| 
2020 ||| an attention-guided and prior-embedded approach with multi-task learning for shadow detection. ||| shihui zhang ||| he li ||| weihang kong ||| xiaowei zhang ||| weidong ren ||| 
2021 ||| light-weight uav object tracking network based on strategy gradient and attention mechanism. ||| xia hua ||| xinqing wang ||| ting rui ||| faming shao ||| dong wang ||| 
2022 ||| multi-view self-attention networks. ||| mingzhou xu ||| baosong yang ||| derek f. wong ||| lidia s. chao ||| 
2022 ||| elementary discourse units with sparse attention for multi-label emotion classification. ||| yu zhu ||| ou wu ||| 
2020 ||| siamatt: siamese attention network for visual tracking. ||| kai yang ||| zhenyu he ||| zikun zhou ||| nana fan ||| 
2020 ||| effective person re-identification by self-attention model guided feature learning. ||| yang li ||| xiaoyan jiang ||| jenq-neng hwang ||| 
2022 ||| an explainable recommendation framework based on an improved knowledge graph attention network with massive volumes of side information. ||| ryotaro shimizu ||| megumi matsutani ||| masayuki goto ||| 
2021 ||| hackrl: reinforcement learning with hierarchical attention for cross-graph knowledge fusion and collaborative reasoning. ||| linyao yang ||| xiao wang ||| yuxin dai ||| kejun xin ||| xiaolong zheng ||| weiping ding ||| jun zhang ||| fei-yue wang ||| 
2020 ||| dam: transformer-based relation detection for question answering over knowledge base. ||| yongrui chen ||| huiying li ||| 
2022 ||| double-branch dehazing network based on self-calibrated attentional convolution. ||| chengyu zheng ||| juan zhang ||| jenq-neng hwang ||| bo huang ||| 
2021 ||| enhancing transformer-based language models with commonsense representations for knowledge-driven machine comprehension. ||| ronghan li ||| zejun jiang ||| lifang wang ||| xinyu lu ||| meng zhao ||| da-qing chen ||| 
2022 ||| video sentiment analysis with bimodal information-augmented multi-head attention. ||| ting wu ||| junjie peng ||| wenqiang zhang ||| huiran zhang ||| shuhua tan ||| fen yi ||| chuanshuai ma ||| yansong huang ||| 
2020 ||| hasvrec: a modularized hierarchical attention-based scholarly venue recommender system. ||| tribikram pradhan ||| abhinav gupta ||| sukomal pal ||| 
2021 ||| hierarchical attention link prediction neural network. ||| zhitao wang ||| wenjie li ||| hanjing su ||| 
2021 ||| attention uncovers task-relevant semantics in emotional narrative understanding. ||| thanh-son nguyen ||| zhengxuan wu ||| desmond c. ong ||| 
2018 ||| audio classification using attention-augmented convolutional neural network. ||| yu wu ||| hua mao ||| zhang yi ||| 
2021 ||| joint knowledge-powered topic level attention for a convolutional text summarization model. ||| shirin akther khanam ||| fei liu ||| yi-ping phoebe chen ||| 
2021 ||| hierarchical social similarity-guided model with dual-mode attention for session-based recommendation. ||| chaoqun feng ||| chongyang shi ||| shufeng hao ||| qi zhang ||| xinyu jiang ||| daohua yu ||| 
2022 ||| method and dataset entity mining in scientific literature: a cnn + bilstm model with self-attention. ||| linlin hou ||| ji zhang ||| ou wu ||| ting yu ||| zhen wang ||| zhao li ||| jianliang gao ||| yingchun ye ||| rujing yao ||| 
2021 ||| tagat: type-aware graph attention networks for reasoning over knowledge graphs. ||| yuzhuo wang ||| hongzhi wang ||| junwei he ||| wenbo lu ||| shuolin gao ||| 
2021 ||| dtdr-alstm: extracting dynamic time-delays to reconstruct multivariate data for improving attention-based lstm industrial time series prediction models. ||| jince li ||| bo yang ||| hongguang li ||| yongjian wang ||| chu qi ||| yi liu ||| 
2021 ||| soft-self and hard-cross graph attention network for knowledge graph entity alignment. ||| zhihuan yan ||| rong peng ||| yaqian wang ||| weidong li ||| 
2021 ||| tau: transferable attention u-net for optic disc and cup segmentation. ||| yuhao zhang ||| xiangrui cai ||| ying zhang ||| hong kang ||| xin ji ||| xiaojie yuan ||| 
2020 ||| tsasnet: tooth segmentation on dental panoramic x-ray images by two-stage attention segmentation network. ||| yue zhao ||| pengcheng li ||| chenqiang gao ||| yang liu ||| qiaoyi chen ||| feng yang ||| deyu meng ||| 
2020 ||| multi-view factorization machines for mobile app recommendation based on hierarchical attention. ||| tingting liang ||| lei zheng ||| liang chen ||| yao wan ||| philip s. yu ||| jian wu ||| 
2019 ||| topic-enhanced emotional conversation generation with attention mechanism. ||| yehong peng ||| yizhen fang ||| zhiwen xie ||| guangyou zhou ||| 
2021 ||| dual attention-based method for occluded person re-identification. ||| yunjie xu ||| liaoying zhao ||| feiwei qin ||| 
2021 ||| iterative graph attention memory network for cross-modal retrieval. ||| xinfeng dong ||| huaxiang zhang ||| xiao dong ||| xu lu ||| 
2020 ||| hagerec: hierarchical attention graph convolutional network incorporating knowledge graph for explainable recommendation. ||| zuoxi yang ||| shoubin dong ||| 
2022 ||| le-gan: unsupervised low-light image enhancement network using attention module and identity invariant loss. ||| ying fu ||| yang hong ||| linwei chen ||| shaodi you ||| 
2020 ||| hybrid time-aligned and context attention for time series prediction. ||| zhumei wang ||| liang zhang ||| zhiming ding ||| 
2022 ||| dynamic graph convolutional autoencoder with node-attribute-wise attention for kidney and tumor segmentation from ct volumes. ||| ping xuan ||| hui cui ||| hongda zhang ||| tiangang zhang ||| linlin wang ||| toshiya nakaguchi ||| henry b. l. duh ||| 
2020 ||| predicting taxi demands via an attention-based convolutional recurrent neural network. ||| tong liu ||| wenbin wu ||| yanmin zhu ||| weiqin tong ||| 
2020 ||| multi-domain modeling of atrial fibrillation detection with twin attentional convolutional long short-term memory neural networks. ||| yanrui jin ||| chengjin qin ||| yixiang huang ||| wenyi zhao ||| chengliang liu ||| 
2020 ||| sparse attention based separable dilated convolutional neural network for targeted sentiment analysis. ||| chenquan gan ||| lu wang ||| zufan zhang ||| zhangyi wang ||| 
2021 ||| hits-based attentional neural model for abstractive summarization. ||| xiaoyan cai ||| kaile shi ||| yuehan jiang ||| libin yang ||| sen liu ||| 
2020 ||| cross-modal recipe retrieval via parallel- and cross-attention networks learning. ||| da cao ||| jingjing chu ||| ningbo zhu ||| liqiang nie ||| 
2021 ||| a novel transfer diagnosis method under unbalanced sample based on discrete-peak joint attention enhancement mechanism. ||| kun xu ||| shunming li ||| xingxing jiang ||| jiantao lu ||| tianyi yu ||| ranran li ||| 
2021 ||| fgcan: filter-based gated contextual attention network for event detection. ||| shunyu yao ||| kai shuang ||| rui li ||| sen su ||| 
2022 ||| predrann: the spatiotemporal attention convolution recurrent neural network for precipitation nowcasting. ||| chuyao luo ||| xinyue zhao ||| yuxi sun ||| xutao li ||| yunming ye ||| 
2020 ||| lightweight multi-scale residual networks with attention for image super-resolution. ||| huan liu ||| feilong cao ||| chenglin wen ||| qinghua zhang ||| 
2022 ||| self-attention-based adaptive remaining useful life prediction for igbt with monte carlo dropout. ||| dengyu xiao ||| chengjin qin ||| jianwen ge ||| pengcheng xia ||| yixiang huang ||| chengliang liu ||| 
2021 ||| inphynet: leveraging attention-based multitask recurrent networks for multi-label physics text classification. ||| vishaal udandarao ||| abhishek agarwal ||| anubha gupta ||| tanmoy chakraborty ||| 
2021 ||| eaa-net: a novel edge assisted attention network for single image dehazing. ||| chao wang ||| hao-zhen shen ||| fan fan ||| ming-wen shao ||| chuan-sheng yang ||| jiancheng luo ||| liang-jian deng ||| 
2021 ||| reasoning like humans: on dynamic attention prior in image captioning. ||| yong wang ||| xian sun ||| xuan li ||| wenkai zhang ||| xin gao ||| 
2022 ||| joint-attention feature fusion network and dual-adaptive nms for object detection. ||| wentao ma ||| tongqing zhou ||| jiaohua qin ||| qingyang zhou ||| zhiping cai ||| 
2021 ||| knowledge-enhanced recommendation using item embedding and path attention. ||| yuan lin ||| bo xu ||| jiaojiao feng ||| hongfei lin ||| kan xu ||| 
2021 ||| amff: a new attention-based multi-feature fusion method for intention recognition. ||| cong liu ||| xiaolong xu ||| 
2021 ||| graph neural networks with multiple kernel ensemble attention. ||| haimin zhang ||| min xu ||| 
2022 ||| diagnosis of alzheimer's disease via an attention-based multi-scale convolutional neural network. ||| zhenbing liu ||| haoxiang lu ||| xipeng pan ||| mingchang xu ||| rushi lan ||| xiaonan luo ||| 
2021 ||| channel and spatial attention based deep object co-segmentation. ||| jia chen ||| yasong chen ||| weihao li ||| guoqin ning ||| mingwen tong ||| adrian hilton ||| 
2021 ||| jam: joint attention model for next event recommendation in event-based social networks. ||| guoqiong liao ||| lechuan yang ||| mingsong mao ||| changxuan wan ||| dexi liu ||| xiping liu ||| 
2022 ||| publication classification prediction via citation attention fusion based on dynamic relations. ||| caixia jing ||| liqing qiu ||| xiangbo tian ||| tingyu hao ||| 
2019 ||| acnn-fm: a novel recommender with attention-based convolutional neural network and factorization machines. ||| guangyao pang ||| xiaoming wang ||| fei hao ||| jiehang xie ||| xinyan wang ||| yaguang lin ||| xueyang qin ||| 
2022 ||| discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer. ||| shivani kumar ||| anubhav shrimal ||| md. shad akhtar ||| tanmoy chakraborty ||| 
2021 ||| personalized sentiment classification of customer reviews via an interactive attributes attention model. ||| you zhang ||| jin wang ||| xuejie zhang ||| 
2021 ||| retrieval-enhanced adversarial training with dynamic memory-augmented attention for image paragraph captioning. ||| chunpu xu ||| min yang ||| xiang ao ||| ying shen ||| ruifeng xu ||| jinwen tian ||| 
2020 ||| direction-sensitive relation extraction using bi-sdp attention model. ||| hailin wang ||| ke qin ||| guoming lu ||| guangchun luo ||| guisong liu ||| 
2020 ||| siamese attentional keypoint network for high performance visual tracking. ||| peng gao ||| ruyue yuan ||| fei wang ||| liyi xiao ||| hamido fujita ||| yan zhang ||| 
2021 ||| prior-knowledge and attention based meta-learning for few-shot learning. ||| yunxiao qin ||| weiguo zhang ||| chenxu zhao ||| zezheng wang ||| xiangyu zhu ||| jingping shi ||| guojun qi ||| zhen lei ||| 
2020 ||| attention-aware scoring learning for person re-identification. ||| miaohui zhang ||| ming xin ||| chengcheng gao ||| xile wang ||| sihan zhang ||| 
2021 ||| hierarchical accumulation network with grid attention for image super-resolution. ||| yue yang ||| yong qi ||| 
2019 ||| visual-textual sentiment classification with bi-directional multi-level attention networks. ||| jie xu ||| feiran huang ||| xiaoming zhang ||| senzhang wang ||| chaozhuo li ||| zhoujun li ||| yueying he ||| 
2021 ||| kernel multi-attention neural network for knowledge graph embedding. ||| dan jiang ||| ronggui wang ||| juan yang ||| lixia xue ||| 
2021 ||| multiscale fused network with additive channel-spatial attention for image segmentation. ||| chengling gao ||| hailiang ye ||| feilong cao ||| chenglin wen ||| qinghua zhang ||| feng zhang ||| 
2021 ||| gated multi-attention representation in reinforcement learning. ||| dayang liang ||| qihang chen ||| yunlong liu ||| 
2021 ||| asrnn: a recurrent neural network with an attention model for sequence labeling. ||| jerry chun-wei lin ||| yinan shao ||| youcef djenouri ||| unil yun ||| 
2021 ||| learning hyperbolic attention-based embeddings for link prediction in knowledge graphs. ||| adnan zeb ||| anwar ul haq ||| junde chen ||| zhenfeng lei ||| defu zhang ||| 
2022 ||| gar-net: a graph attention reasoning network for conversation understanding. ||| hua xu ||| ziqi yuan ||| kang zhao ||| yunfeng xu ||| jiyun zou ||| kai gao ||| 
2021 ||| co-attention networks based on aspect and context for aspect-level sentiment analysis. ||| meizhen liu ||| fengyu zhou ||| ke chen ||| yang zhao ||| 
2020 ||| adrl: an attention-based deep reinforcement learning framework for knowledge graph reasoning. ||| qi wang ||| yongsheng hao ||| jie cao ||| 
2020 ||| decab-lstm: deep contextualized attentional bidirectional lstm for cancer hallmark classification. ||| longquan jiang ||| xuan sun ||| francesco mercaldo ||| antonella santone ||| 
2021 ||| quantum probability-inspired graph attention network for modeling complex text interaction. ||| peng yan ||| linjing li ||| daniel zeng ||| 
2020 ||| recurrent neural network with pooling operation and attention mechanism for sentiment analysis: a multi-task learning approach. ||| yi cai ||| qingbao huang ||| zejun lin ||| jingyun xu ||| zhenhong chen ||| qing li ||| 
2021 ||| a novel network with multiple attention mechanisms for aspect-level sentiment analysis. ||| xiaodi wang ||| mingwei tang ||| tian yang ||| zhen wang ||| 
2020 ||| crga: homographic pun detection with a contextualized-representation: gated attention network. ||| yufeng diao ||| hongfei lin ||| liang yang ||| xiaochao fan ||| di wu ||| kan xu ||| 
2022 ||| multi-label modality enhanced attention based self-supervised deep cross-modal hashing. ||| xitao zou ||| song wu ||| nian zhang ||| erwin m. bakker ||| 
2022 ||| graph transformer network with temporal kernel attention for skeleton-based action recognition. ||| yanan liu ||| hao zhang ||| dan xu ||| kangjian he ||| 
2022 ||| 3d hierarchical dual-attention fully convolutional networks with hybrid losses for diverse glioma segmentation. ||| deting kong ||| xiyu liu ||| yan wang ||| dengwang li ||| jie xue ||| 
2020 ||| attention deep neural network for lane marking detection. ||| degui xiao ||| xuefeng yang ||| jianfang li ||| merabtene islam ||| 
2022 ||| semi-supervised npc segmentation with uncertainty and attention guided consistency. ||| lin hu ||| jiaxin li ||| xingchen peng ||| jianghong xiao ||| bo zhan ||| chen zu ||| xi wu ||| jiliu zhou ||| yan wang ||| 
2021 ||| accurate and explainable recommendation via hierarchical attention network oriented towards crowd intelligence. ||| chao yang ||| weixin zhou ||| zhiyu wang ||| bin jiang ||| dongsheng li ||| huawei shen ||| 
2019 ||| context-aware emotion cause analysis with multi-attention-based neural network. ||| xiangju li ||| shi feng ||| daling wang ||| yifei zhang ||| 
2022 ||| mixhead: breaking the low-rank bottleneck in multi-head attention language models. ||| zhong zhang ||| nian shao ||| chongming gao ||| rui miao ||| qinli yang ||| junming shao ||| 
2021 ||| usevis: visual analytics of attention-based neural embedding in information retrieval. ||| xiaonan ji ||| yamei tu ||| wenbin he ||| junpeng wang ||| han-wei shen ||| po-yin yen ||| 
2020 ||| a deep learning method based on an attention mechanism for wireless network traffic prediction. ||| ming li ||| yuewen wang ||| zhaowen wang ||| huiying zheng ||| 
2022 ||| small target recognition using dynamic time warping and visual attention. ||| xinpeng zhang ||| jigang wu ||| min meng ||| 
2022 ||| an attention enhanced cross-modal image-sound mutual generation model for birds. ||| wangli hao ||| meng han ||| shancang li ||| fuzhong li ||| 
2021 ||| predicting stance polarity and intensity in cyber argumentation with deep bidirectional transformers. ||| joseph winstead sirrianni ||| xiaoqing liu ||| douglas adams ||| 
2020 ||| bert-caps: a transformer-based capsule network for tweet act classification. ||| tulika saha ||| srivatsa ramesh jayashree ||| sriparna saha ||| pushpak bhattacharyya ||| 
2022 ||| the deep features and attention mechanism-based method to dish healthcare under social iot systems: an empirical study with a hand-deep local-global net. ||| honghao gao ||| kaili xu ||| min cao ||| junsheng xiao ||| qiang xu ||| yuyu yin ||| 
2020 ||| amnn: attention-based multimodal neural network model for hashtag recommendation. ||| qi yang ||| gaosheng wu ||| yuhua li ||| ruixuan li ||| xiwu gu ||| huicai deng ||| junzhuang wu ||| 
2021 ||| two-level attention model of representation learning for fraud detection. ||| ruihao cao ||| guanjun liu ||| yu xie ||| changjun jiang ||| 
2021 ||| two-phase multidocument summarization through content-attention-based subtopic detection. ||| luobing dong ||| meghana n. satpute ||| weili wu ||| ding-zhu du ||| 
2021 ||| memory augmented hierarchical attention network for next point-of-interest recommendation. ||| chenwang zheng ||| dan tao ||| jiangtao wang ||| lei cui ||| wenjie ruan ||| shui yu ||| 
2020 ||| robust and efficient wls-based dynamic state estimation considering transformer core saturation. ||| jonghoek kim ||| sungyun choi ||| 
2020 ||| hitanomaly: hierarchical transformers for anomaly detection in system log. ||| shaohan huang ||| yi liu ||| carol fung ||| rong he ||| yining zhao ||| hailong yang ||| zhongzhi luan ||| 
2021 ||| neural and attentional factorization machine-based web api recommendation for mashup development. ||| guosheng kang ||| jianxun liu ||| yong xiao ||| buqing cao ||| yu xu ||| manliang cao ||| 
2021 ||| deepcc: multi-agent deep reinforcement learning congestion control for multi-path tcp based on self-attention. ||| bo he ||| jing-yu wang ||| qi qi ||| haifeng sun ||| jianxin liao ||| chunning du ||| xiang-yang alex liu ||| zhu han ||| 
2021 ||| log sequence anomaly detection based on local information extraction and globally sparse transformer model. ||| chunkai zhang ||| xinyu wang ||| hongye zhang ||| hanyu zhang ||| peiyi han ||| 
2021 ||| one spatio-temporal sharpening attention mechanism for light-weight yolo models based on sharpening spatial attention. ||| mengfan xue ||| minghao chen ||| dongliang peng ||| yunfei guo ||| huajie chen ||| 
2021 ||| residual spatial and channel attention networks for single image dehazing. ||| xin jiang ||| chunlei zhao ||| ming zhu ||| zhicheng hao ||| wen gao ||| 
2021 ||| super-resolution network with information distillation and multi-scale attention for medical ct image. ||| tianliu zhao ||| lei hu ||| yongmei zhang ||| jianying fang ||| 
2020 ||| attention-deficit/hyperactivity disorder (adhd): integrating the moxo-dcpt with an eye tracker enhances diagnostic precision. ||| tomer elbaum ||| yoram braw ||| astar lev ||| yuri rassovsky ||| 
2020 ||| eeg-based emotion classification using long short-term memory network with attention mechanism. ||| youmin kim ||| ahyoung choi ||| 
2021 ||| fac-net: feedback attention network based on context encoder network for skin lesion segmentation. ||| yuying dong ||| liejun wang ||| shuli cheng ||| yongming li ||| 
2020 ||| study of the influence of winding and sensor design on ultra-high frequency partial discharge signals in power transformers. ||| chandra prakash beura ||| michael beltle ||| stefan tenbohlen ||| 
2020 ||| an online tea fixation state monitoring algorithm based on image energy attention mechanism and supervised clustering (ieamsc). ||| zhiyong yu ||| jin wang ||| tao zheng ||| guodong lu ||| 
2020 ||| end-to-end deep learning architecture for continuous blood pressure estimation using attention mechanism. ||| heesang eom ||| dongseok lee ||| seungwoo han ||| yuli sun hariyani ||| yonggyu lim ||| illsoo sohn ||| kwangsuk park ||| cheolsoo park ||| 
2020 ||| performance problems of non-toroidal shaped current transformers. ||| carlos a. platero ||| jos |||   ||| ngel s ||| nchez-fern ||| ndez ||| konstantinos n. gyftakis ||| francisco bl ||| zquez ||| ricardo granizo ||| 
2021 ||| ddostc: a transformer-based network attack detection hybrid mechanism in sdn. ||| haomin wang ||| wei li ||| 
2019 ||| design and performance test of transformer winding optical fibre composite wire based on raman scattering. ||| yunpeng liu ||| junyi yin ||| yuan tian ||| xiaozhou fan ||| 
2020 ||| extreme low-light image enhancement for surveillance cameras using attention u-net. ||| sophy ai ||| jangwoo kwon ||| 
2022 ||| attention autoencoder for generative latent representational learning in anomaly detection. ||| ariyo oluwasanmi ||| muhammad umar aftab ||| edward yellakuor baagyere ||| zhiguang qin ||| muhammad ahmad ||| manuel mazzara ||| 
2020 ||| detection of computer graphics using attention-based dual-branch convolutional neural network from fused color components. ||| peisong he ||| haoliang li ||| hongxia wang ||| ruimei zhang ||| 
2022 ||| a graph neural network with spatio-temporal attention for multi-sources time series data: an application to frost forecast. ||| hernan lira ||| luis mart ||| nayat s ||| nchez pi ||| 
2019 ||| a smart context-aware hazard attention system to help people with peripheral vision loss. ||| ola younis ||| waleed al-nuaimy ||| fiona rowe ||| mohammad h. alomari ||| 
2022 ||| wearable sensor-based human activity recognition with transformer model. ||| iveta dirgov |||  lupt ||| kov ||| martin kubovc ||| k ||| jir |||  posp ||| chal ||| 
2021 ||| how geometry affects sensitivity of a differential transformer for contactless characterization of liquids. ||| marc berger ||| anne zygmanowski ||| stefan zimmermann ||| 
2021 ||| the impact of attention mechanisms on speech emotion recognition. ||| shouyan chen ||| mingyan zhang ||| xiaofen yang ||| zhijia zhao ||| tao zou ||| xinqi sun ||| 
2021 ||| bendductor - transformer steel magnetomechanical force sensor. ||| przemyslaw grenda ||| monika kutyla ||| michal nowicki ||| tomasz charubin ||| 
2020 ||| pyramid inter-attention for high dynamic range imaging. ||| sungil choi ||| jaehoon cho ||| wonil song ||| jihwan choe ||| jisung yoo ||| kwanghoon sohn ||| 
2020 ||| spatial-spectral feature refinement for hyperspectral image classification based on attention-dense 3d-2d-cnn. ||| jin zhang ||| fengyuan wei ||| fan feng ||| chunyang wang ||| 
2020 ||| are inductive current transformers performance really affected by actual distorted network conditions? an experimental case study. ||| alessandro mingotti ||| lorenzo peretto ||| lorenzo bartolomei ||| diego cavaliere ||| roberto tinarelli ||| 
2020 ||| learning attention representation with a multi-scale cnn for gear fault diagnosis under different working conditions. ||| yong yao ||| sen zhang ||| suixian yang ||| gui gui ||| 
2021 ||| posture detection of individual pigs based on lightweight convolution neural networks and efficient channel-wise attention. ||| yizhi luo ||| zhixiong zeng ||| huazhong lu ||| enli lv ||| 
2021 ||| a simple method for compensating harmonic distortion in current transformers: experimental validation. ||| christian laurano ||| sergio toscani ||| michele zanoni ||| 
2022 ||| effect of auditory discrimination therapy on attentional processes of tinnitus patients. ||| ingrid g. rodr ||| guez-le ||| n ||| luz mar ||| a alonso-valerdi ||| ricardo a. salido-ruiz ||| israel rom ||| n-god ||| nez ||| david ibarra-zarate ||| sulema torres-ramos ||| 
2019 ||| static and dynamic evaluation of a winding deformation fbg sensor for power transformer applications. ||| aguinaldo goes de melo ||| daniel benetti ||| luiz alkimin de lacerda ||| rodrigo peres ||| claudio floridia ||| artur de araujo silva ||| jo ||| o batista rosolem ||| 
2022 ||| deep modular bilinear attention network for visual question answering. ||| feng yan ||| wushouer silamu ||| yanbing li ||| 
2021 ||| combining implicit and explicit feature extraction for eye tracking: attention classification using a heterogeneous input. ||| lisa-marie vortmann ||| felix putze ||| 
2020 ||| on the long-period accuracy behavior of inductive and low-power instrument transformers. ||| alessandro mingotti ||| lorenzo bartolomei ||| lorenzo peretto ||| roberto tinarelli ||| 
2020 ||| toward the standardization of limits to offset and noise in electronic instrument transformers. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2021 ||| joint soft-hard attention for self-supervised monocular depth estimation. ||| chao fan ||| zhenyu yin ||| fulong xu ||| anying chai ||| feiqing zhang ||| 
2021 ||| data-driven anomaly detection in high-voltage transformer bushings with lstm auto-encoder. ||| imene mitiche ||| tony mcgrail ||| philip boreham ||| alan nesbitt ||| gordon morison ||| 
2021 ||| deep-emotion: facial expression recognition using attentional convolutional network. ||| shervin minaee ||| mehdi minaei ||| amirali abdolrashidi ||| 
2021 ||| comparison of mathematical methods for compensating a current signal under current transformers saturation conditions. ||| ismoil odinaev ||| aminjon gulakhmadov ||| pavel murzin ||| alexander tavlintsev ||| sergey semenenko ||| evgenii kokorin ||| murodbek safaraliev ||| xi chen ||| 
2020 ||| validity evaluation method based on data driving for on-line monitoring data of transformer under dc-bias. ||| yuanda he ||| qi zhou ||| sheng lin ||| liping zhao ||| 
2020 ||| gps trajectory completion using end-to-end bidirectional convolutional recurrent encoder-decoder architecture with attention mechanism. ||| asif nawaz ||| zhiqiu huang ||| senzhang wang ||| muhammad azeem akbar ||| hussain alsalman ||| abdu gumaei ||| 
2021 ||| vehicle trajectory prediction with lane stream attention-based lstms and road geometry linearization. ||| dongyeon yu ||| honggyu lee ||| taehoon kim ||| sung-ho hwang ||| 
2018 ||| a lightweight convolutional neural network based on visual attention for sar image target classification. ||| jiaqi shao ||| changwen qu ||| jianwei li ||| shujuan peng ||| 
2021 ||| weed classification using explainable multi-resolution slot attention. ||| sadaf farkhani ||| s ||| ren kelstrup skovsen ||| mads dyrmann ||| rasmus nyholm j ||| rgensen ||| henrik karstoft ||| 
2022 ||| multi-task model for esophageal lesion analysis using endoscopic images: classification with image retrieval and segmentation with attention. ||| xiaoyuan yu ||| suigu tang ||| chak-fong cheang ||| hon ho yu ||| i. cheong choi ||| 
2020 ||| towards attention-based convolutional long short-term memory for travel time prediction of bus journeys. ||| jianqing wu ||| qiang wu ||| jun shen ||| chen cai ||| 
2019 ||| a convolution component-based method with attention mechanism for travel-time prediction. ||| xiangdong ran ||| zhiguang shan ||| yufei fang ||| chuang lin ||| 
2021 ||| mask attention-srgan for mobile sensing networks. ||| chi-en huang ||| ching-chun chang ||| yung-hui li ||| 
2021 ||| towards precise interpretation of oil transformers via novel combined techniques based on dga and partial discharge sensors. ||| sayed a. ward ||| adel el-faraskoury ||| mohamed badawi ||| shimaa a. ibrahim ||| karar mahmoud ||| matti lehtonen ||| mohamed m. f. darwish ||| 
2020 ||| fabrication of thermal conductivity detector based on mems for monitoring dissolved gases in power transformer. ||| tingliang tan ||| jianhai sun ||| tingting chen ||| xinxiao zhang ||| xiaofeng zhu ||| 
2022 ||| acceleration of magnetic resonance fingerprinting reconstruction using denoising and self-attention pyramidal convolutional neural network. ||| jia-sheng hong ||| ingo hermann ||| frank g. z ||| llner ||| lothar r. schad ||| shuu-jiun wang ||| wei-kai lee ||| yung-lin chen ||| yu chang ||| yu-te wu ||| 
2021 ||| attention-based multi-scale convolutional neural network (a+mcnn) for multi-class classification in road images. ||| elham eslami ||| hae-bum yun ||| 
2021 ||| transformers and generative adversarial networks for liveness detection in multitarget fingerprint sensors. ||| soha b. sandouka ||| yakoub bazi ||| naif alajlan ||| 
2021 ||| short-circuited turn fault diagnosis in transformers by using vibration signals, statistical time features, and support vector machines on fpga. ||| jose r. huerta-rosales ||| david granados-lieberman ||| arturo garcia-perez ||| david camarena-martinez ||| juan pablo amezquita-sanchez ||| martin valtierra-rodriguez ||| 
2022 ||| correlating personal resourcefulness and psychomotor skills: an analysis of stress, visual attention and technical metrics. ||| carmen guzm ||| n-garc ||| a ||| patricia s ||| nchez-gonz ||| lez ||| juan alberto s ||| nchez-margallo ||| nicola snoriguzzi ||| jos |||  castillo rabazo ||| francisco m. s ||| nchez-margallo ||| enrique j. g ||| mez ||| ignacio oropesa ||| 
2017 ||| modelling and optimization of four-segment shielding coils of current transformers. ||| yucheng gao ||| wei zhao ||| qing wang ||| kaifeng qu ||| he li ||| haiming shao ||| songling huang ||| 
2020 ||| attention based cnn-convlstm for pedestrian attribute recognition. ||| yang li ||| huahu xu ||| minjie bian ||| junsheng xiao ||| 
2020 ||| coastal land cover classification of high-resolution remote sensing images using attention-driven context encoding network. ||| jifa chen ||| gang chen ||| lizhe wang ||| bo fang ||| ping zhou ||| mingjie zhu ||| 
2020 ||| improving object tracking by added noise and channel attention. ||| mustansar fiaz ||| arif mahmood ||| ki yeol baek ||| sehar shahzad farooq ||| soon ki jung ||| 
2021 ||| luminance-degradation compensation based on multistream self-attention to address thin-film transistor-organic light emitting diode burn-in. ||| seong-chel park ||| kwan-ho park ||| joon-hyuk chang ||| 
2020 ||| global temperature sensing for an operating power transformer based on raman scattering. ||| yunpeng liu ||| xinye li ||| huan li ||| xiaozhou fan ||| 
2020 ||| global and local attention-based free-form image inpainting. ||| s. m. nadim uddin ||| yong ju jung ||| 
2021 ||| optical fiber sensors for structural monitoring in power transformers. ||| catarina s. monteiro ||| ant ||| nio v. rodrigues ||| duarte viveiros ||| cassiano c. linhares ||| h ||| lder mendes ||| susana o. silva ||| paulo v. s. marques ||| s ||| rgio m. o. tavares ||| orlando fraz ||| o ||| 
2021 ||| emotion detection for social robots based on nlp transformers and an emotion ontology. ||| wilfredo graterol ||| jos |||  a. diaz amado ||| yudith cardinale ||| irvin dongo ||| edmundo lopes-silva ||| cleia santos-libarino ||| 
2019 ||| a sensor system for detecting and localizing partial discharges in power transformers with improved immunity to interferences. ||| petr drexler ||| martin c ||| p ||| pavel fiala ||| miloslav steinbauer ||| radim kadlec ||| milos kaska ||| lubom ||| r kocis ||| 
2021 ||| graph attention feature fusion network for als point cloud classification. ||| jie yang ||| xinchang zhang ||| yun huang ||| 
2021 ||| social robots for evaluating attention state in older adults. ||| yi-chen chen ||| su-ling yeh ||| tsung-ren huang ||| yu-ling chang ||| joshua oon soo goh ||| li-chen fu ||| 
2021 ||| an attention-based multilayer gru model for multistep-ahead short-term load forecasting. ||| seungmin jung ||| jihoon moon ||| sungwoo park ||| eenjun hwang ||| 
2021 ||| instance sequence queries for video instance segmentation with transformers. ||| zhujun xu ||| damien vivet ||| 
2020 ||| bidirectional attention for text-dependent speaker verification. ||| xin fang ||| tian gao ||| liang zou ||| zhen-hua ling ||| 
2022 ||| efficient spatiotemporal attention network for remote heart rate variability analysis. ||| hailan kuang ||| fanbing lv ||| xiaolin ma ||| xinhua liu ||| 
2020 ||| hybrid attention network for language-based person search. ||| yang li ||| huahu xu ||| junsheng xiao ||| 
2020 ||| spatial-semantic and temporal attention mechanism-based online multi-object tracking. ||| fanjie meng ||| xinqing wang ||| dong wang ||| faming shao ||| lei fu ||| 
2021 ||| transformation of transient overvoltages by inductive voltage transformers. ||| michal kaczmarek ||| dariusz brodecki ||| 
2021 ||| multi-modal adaptive fusion transformer network for the estimation of depression level. ||| hao sun ||| jiaqing liu ||| shurong chai ||| zhaolin qiu ||| lanfen lin ||| xinyin huang ||| yen-wei chen ||| 
2019 ||| deep attention models for human tracking using rgbd. ||| maryamsadat rasoulidanesh ||| srishti yadav ||| sachini herath ||| yasaman vaghei ||| shahram payandeh ||| 
2020 ||| multi-modal explicit sparse attention networks for visual question answering. ||| zihan guo ||| dezhi han ||| 
2021 ||| a lightweight 1-d convolution augmented transformer with metric learning for hyperspectral image classification. ||| xiang hu ||| wenjing yang ||| hao wen ||| yu liu ||| yuanxi peng ||| 
2021 ||| improving ponzi scheme contract detection using multi-channel textcnn and transformer. ||| yizhou chen ||| heng dai ||| xiao yu ||| wenhua hu ||| zhiwen xie ||| cheng tan ||| 
2021 ||| korean grammatical error correction based on transformer with copying mechanisms and grammatical noise implantation methods. ||| myunghoon lee ||| hyeonho shin ||| dabin lee ||| sung-pil choi ||| 
2020 ||| robust building extraction for high spatial resolution remote sensing images with self-attention network. ||| dengji zhou ||| guizhou wang ||| guojin he ||| tengfei long ||| ranyu yin ||| zhaoming zhang ||| sibao chen ||| bin luo ||| 
2020 ||| radar emitter signal recognition based on one-dimensional convolutional neural network with attention mechanism. ||| bin wu ||| shibo yuan ||| peng li ||| zehuan jing ||| shao huang ||| yaodong zhao ||| 
2020 ||| an effective dense co-attention networks for visual question answering. ||| shirong he ||| dezhi han ||| 
2020 ||| hybrid network with attention mechanism for detection and location of myocardial infarction based on 12-lead electrocardiogram signals. ||| lidan fu ||| binchun lu ||| bo nie ||| zhiyun peng ||| hongying liu ||| xitian pi ||| 
2021 ||| dynamic hand gesture recognition in in-vehicle environment based on fmcw radar and transformer. ||| lianqing zheng ||| jie bai ||| xichan zhu ||| libo huang ||| chewu shan ||| qiong wu ||| lei zhang ||| 
2018 ||| novel simulation technique of electromagnetic wave propagation in the ultra high frequency range within power transformers. ||| takahiro umemoto ||| stefan tenbohlen ||| 
2019 ||| a dynamic part-attention model for person re-identification. ||| ziying yao ||| xinkai wu ||| zhongxia xiong ||| yalong ma ||| 
2021 ||| attention-guided network with densely connected convolution for skin lesion segmentation. ||| shengxin tao ||| yun jiang ||| simin cao ||| chao wu ||| zeqi ma ||| 
2022 ||| vehicle interaction behavior prediction with self-attention. ||| linhui li ||| xin sui ||| jing lian ||| fengning yu ||| yafu zhou ||| 
2019 ||| a classification method for select defects in power transformers based on the acoustic signals. ||| michal kunicki ||| daria wotzka ||| 
2021 ||| research on on-line detection method of transformer winding deformation based on vfto. ||| yanyun wang ||| guoqiong zhou ||| chunping zeng ||| wenbin zhang ||| yanan ren ||| yi ke ||| hequn chu ||| chunguang suo ||| 
2021 ||| nra-net - neg-region attention network for salient object detection with gaze tracking. ||| hoijun kim ||| soonchul kwon ||| seunghyun lee ||| 
2020 ||| distribution transformer parameters detection based on low-frequency noise, machine learning methods, and evolutionary algorithm. ||| daniel jancarczyk ||| marcin bernas ||| tomasz boczar ||| 
2021 ||| analysis of the influence of the breaking radiation magnetic field of a 10 kv intelligent circuit breaker on an electronic transformer. ||| wenchao lu ||| jiandong duan ||| lin cheng ||| jiangping lu ||| xiaotong du ||| 
2021 ||| msst-rt: multi-stream spatial-temporal relative transformer for skeleton-based action recognition. ||| yan sun ||| yixin shen ||| liyan ma ||| 
2021 ||| esophagus segmentation in ct images via spatial attention network and staple algorithm. ||| minh-trieu tran ||| soo-hyung kim ||| hyung-jeong yang ||| guee-sang lee ||| in-jae oh ||| sae-ryung kang ||| 
2021 ||| edge-enhanced with feedback attention network for image super-resolution. ||| chunmei fu ||| yong yin ||| 
2021 ||| improved action recognition with separable spatio-temporal attention using alternative skeletal and video pre-processing. ||| pau climent-p ||| rez ||| francisco fl ||| rez-revuelta ||| 
2022 ||| epileptic-net: an improved epileptic seizure detection system using dense convolutional block with attention network from eeg. ||| md shafiqul islam ||| keshav thapa ||| sung-hyun yang ||| 
2021 ||| moment-to-moment continuous attention fluctuation monitoring through consumer-grade eeg device. ||| shan zhang ||| zihan yan ||| shardul sapkota ||| shengdong zhao ||| wei tsang ooi ||| 
2021 ||| radar transformer: an object classification network based on 4d mmw imaging radar. ||| jie bai ||| lianqing zheng ||| sen li ||| bin tan ||| sihan chen ||| libo huang ||| 
2021 ||| implementation of an online auditory attention detection model with electroencephalography in a dichotomous listening experiment. ||| seung-cheol baek ||| jae ho chung ||| yoonseob lim ||| 
2018 ||| machine learning-based sensor data modeling methods for power transformer phm. ||| anyi li ||| xiaohui yang ||| huanyu dong ||| zihao xie ||| chunsheng yang ||| 
2022 ||| lstnet: a reference-based learning spectral transformer network for spectral super-resolution. ||| debao yuan ||| ling wu ||| huinan jiang ||| bingrui zhang ||| jian li ||| 
2021 ||| robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion. ||| baijun xie ||| mariia sidulova ||| chung hyuk park ||| 
2020 ||| sgdan - a spatio-temporal graph dual-attention neural network for quantified flight delay prediction. ||| ziyu guo ||| guangxu mei ||| shijun liu ||| li pan ||| lei bian ||| hongwu tang ||| diansheng wang ||| 
2021 ||| a novel transformers fault diagnosis method based on probabilistic neural network and bio-inspired optimizer. ||| lingyu tao ||| xiaohui yang ||| yichen zhou ||| li yang ||| 
2022 ||| dan-superpoint: self-supervised feature point detection algorithm with dual attention network. ||| zhaoyang li ||| jie cao ||| qun hao ||| xue zhao ||| yaqian ning ||| dongxing li ||| 
2020 ||| image deblurring using multi-stream bottom-top-bottom attention network and global information-based fusion and reconstruction network. ||| quan zhou ||| mingyue ding ||| xuming zhang ||| 
2021 ||| attention-based joint training of noise suppression and sound event detection for noise-robust classification. ||| jin-young son ||| joon-hyuk chang ||| 
2021 ||| super-resolution generative adversarial network based on the dual dimension attention mechanism for biometric image super-resolution. ||| chi-en huang ||| yung-hui li ||| muhammad saqlain aslam ||| ching-chun chang ||| 
2019 ||| dissolved gas analysis equipment for online monitoring of transformer oil: a review. ||| sergio bustamante ||| mario manana ||| alberto arroyo ||| pablo castro ||| alberto laso ||| raquel martinez ||| 
2020 ||| multi-branch convolutional neural network for automatic sleep stage classification with embedded stage refinement and residual attention channel fusion. ||| tianqi zhu ||| wei luo ||| feng yu ||| 
2021 ||| generalized deep learning eeg models for cross-participant and cross-task detection of the vigilance decrement in sustained attention tasks. ||| alexander kamrud ||| brett j. borghetti ||| christine m. schubert-kabban ||| michael miller ||| 
2021 ||| skeleton-based emotion recognition based on two-stream self-attention enhanced spatial-temporal graph convolutional network. ||| jiaqi shi ||| chaoran liu ||| carlos toshinori ishi ||| hiroshi ishiguro ||| 
2021 ||| vitt: vision transformer tracker. ||| xiaoning zhu ||| yannan jia ||| sun jian ||| lize gu ||| zhang pu ||| 
2018 ||| social image captioning: exploring visual attention and user attention. ||| leiquan wang ||| xiaoliang chu ||| weishan zhang ||| yiwei wei ||| weichen sun ||| chunlei wu ||| 
2020 ||| low-power voltage transformer smart frequency modeling and output prediction up to 2.5 khz, using sinc-response approach. ||| abbas ghaderi ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2022 ||| power transformer voltages classification with acoustic signal in various noisy environments. ||| mintai kim ||| sungju lee ||| 
2021 ||| ar3d: attention residual 3d network for human action recognition. ||| min dong ||| zhenglin fang ||| yongfa li ||| sheng bi ||| jiangcheng chen ||| 
2021 ||| high accurate environmental sound classification: sub-spectrogram segmentation versus temporal-frequency attention mechanism. ||| tianhao qiao ||| shunqing zhang ||| shan cao ||| shugong xu ||| 
2021 ||| single- and cross-modality near duplicate image pairs detection via spatial transformer comparing cnn. ||| yi zhang ||| shizhou zhang ||| ying li ||| yanning zhang ||| 
2020 ||| adst: forecasting metro flow using attention-based deep spatial-temporal networks with multi-task learning. ||| hongwei jia ||| haiyong luo ||| hao wang ||| fang zhao ||| qixue ke ||| mingyao wu ||| yunyun zhao ||| 
2021 ||| unsupervised trademark retrieval method based on attention mechanism. ||| jiangzhong cao ||| yunfei huang ||| qingyun dai ||| wing-kuen ling ||| 
2021 ||| gourmetnet: food segmentation using multi-scale waterfall features with spatial and channel attention. ||| udit sharma ||| bruno artacho ||| andreas e. savakis ||| 
2021 ||| panchromatic image super-resolution via self attention-augmented wasserstein generative adversarial network. ||| juan du ||| kuanhong cheng ||| yue yu ||| dabao wang ||| huixin zhou ||| 
2021 ||| adaptive attention memory graph convolutional networks for skeleton-based action recognition. ||| di liu ||| hui xu ||| jianzhong wang ||| yinghua lu ||| jun kong ||| miao qi ||| 
2021 ||| stac: spatial-temporal attention on compensation information for activity recognition in fpv. ||| yue zhang ||| shengli sun ||| linjian lei ||| huikai liu ||| hui xie ||| 
2022 ||| a comparative analysis applied to the partial discharges identification in dry-type transformers by hall and acoustic emission sensors. ||| bruno albuquerque de castro ||| vitor vecina dos santos ||| guilherme beraldi lucas ||| jorge alfredo ardila-rey ||| rudolf ribeiro riehl ||| andr |||  luiz andreoli ||| 
2022 ||| feature-based sentimental analysis on public attention towards covid-19 using cuda-sadbm classification model. ||| siva kumar pathuri ||| neelamegam anbazhagan ||| gyanendra prasad joshi ||| jinsang you ||| 
2022 ||| deterioration level estimation based on convolutional neural network using confidence-aware attention mechanism for infrastructure inspection. ||| naoki ogawa ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| color index of transformer oil: a low-cost measurement approach using ultraviolet-blue laser. ||| m. h. hasnul ||| pin jern ker ||| hui jing lee ||| yang sing leong ||| mahammad abdul hannan ||| md zaini jamaludin ||| mohd adzir mahdi ||| 
2022 ||| convolutional blur attention network for cell nuclei segmentation. ||| phuong thi le ||| tuan pham ||| yi-chiung hsu ||| jia-ching wang ||| 
2020 ||| wavelet-like transform to optimize the order of an autoregressive neural network model to predict the dissolved gas concentration in power transformer oil from sensor data. ||| francisco el ||| nio bezerra ||| fernando andr |||  zemuner garcia ||| s ||| lvio ikuyo nabeta ||| gilberto francisco martha de souza ||| ivan eduardo chabu ||| josemir coelho santos ||| shigueru nagao junior ||| f ||| bio henrique pereira ||| 
2019 ||| spatio-temporal attention model for foreground detection in cross-scene surveillance videos. ||| dong liang ||| jiaxing pan ||| han sun ||| huiyu zhou ||| 
2021 ||| design of insulation tape tension control system of transformer winding machine based on fuzzy pid. ||| liwei deng ||| hongfei suo ||| haonan ren ||| 
2020 ||| calibration procedure to test the effects of multiple influence quantities on low-power voltage transformers. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| 
2022 ||| deep learning post-filtering using multi-head attention and multiresolution feature fusion for image and intra-video quality enhancement. ||| ionut schiopu ||| adrian munteanu ||| 
2018 ||| transformerless ultrasonic ranging system with the feature of intrinsic safety for explosive environment. ||| yu wang ||| yuheng qiao ||| hongjuan zhang ||| yang gao ||| ming zhang ||| heng tan ||| dong wang ||| bao-quan jin ||| 
2021 ||| a bci based alerting system for attention recovery of uav operators. ||| jonghyuk park ||| jonghun park ||| dongmin shin ||| ye rim choi ||| 
2021 ||| attention networks for the quality enhancement of light field images. ||| ionut schiopu ||| adrian munteanu ||| 
2020 ||| real-time monitoring for hydraulic states based on convolutional bidirectional lstm with attention mechanism. ||| kyutae kim ||| jongpil jeong ||| 
2021 ||| vehicle destination prediction using bidirectional lstm with attention mechanism. ||| pietro casabianca ||| yu zhang ||| miguel mart ||| nez-garc ||| a ||| jiafu wan ||| 
2021 ||| remote sensing image dataset expansion based on generative adversarial networks with modified shuffle attention. ||| lu chen ||| hongjun wang ||| xianghao meng ||| 
2021 ||| hybrid attention cascade network for facial expression recognition. ||| xiaoliang zhu ||| shihao ye ||| liang zhao ||| zhicheng dai ||| 
2019 ||| bio-inspired phm model for diagnostics of faults in power transformers using dissolved gas-in-oil data. ||| huanyu dong ||| xiaohui yang ||| anyi li ||| zihao xie ||| yuanlong zuo ||| 
2020 ||| xfinger-net: pixel-wise segmentation method for partially defective fingerprint based on attention gates and u-net. ||| guo chun wan ||| meng meng li ||| he xu ||| wen hao kang ||| jin wen rui ||| mei song tong ||| 
2020 ||| time series multiple channel convolutional neural network with attention-based long short-term memory for predicting bearing remaining useful life. ||| jehn-ruey jiang ||| juei-en lee ||| yi-ming zeng ||| 
2021 ||| real-time semantic segmentation with dual encoder and self-attention mechanism for autonomous driving. ||| yu-bang chang ||| chieh tsai ||| chang hong lin ||| poki chen ||| 
2021 ||| automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze. ||| michael barz ||| daniel sonntag ||| 
2021 ||| a fast stereo matching network with multi-cross attention. ||| ming wei ||| ming zhu ||| yi wu ||| jiaqi sun ||| jiarong wang ||| changji liu ||| 
2020 ||| closed-loop attention restoration theory for virtual reality-based attentional engagement enhancement. ||| gang li ||| shihong zhou ||| zhen kong ||| mengyuan guo ||| 
2020 ||| principles of charge estimation methods using high-frequency current transformer sensors in partial discharge measurements. ||| armando rodrigo-mor ||| fabio andres mu ||| oz ||| luis carlos castro heredia ||| 
2021 ||| energy load forecasting using a dual-stage attention-based recurrent neural network. ||| alper ozcan ||| cagatay catal ||| ahmet kasif ||| 
2021 ||| attention fusion for one-stage multispectral pedestrian detection. ||| zhiwei cao ||| huihua yang ||| juan zhao ||| shuhong guo ||| lingqiao li ||| 
2021 ||| a driver's visual attention prediction using optical flow. ||| byeongkeun kang ||| yeejin lee ||| 
2019 ||| design and development of a bio-inspired uhf sensor for partial discharge detection in power transformers. ||| luiz augusto medeiros martins nobrega ||| george victor rocha xavier ||| marcus v. d. aquino ||| alexandre jean ren |||  serres ||| camila c. r. albuquerque ||| edson guedes da costa ||| 
2021 ||| adversarial learning with bidirectional attention for visual question answering. ||| qifeng li ||| xinyi tang ||| yi jian ||| 
2020 ||| tensor-based emotional category classification via visual attention-based heterogeneous cnn feature fusion. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2021 ||| automatic lung segmentation on chest x-rays using self-attention deep neural network. ||| minki kim ||| byoung-dai lee ||| 
2021 ||| learning region-based attention network for traffic sign recognition. ||| ke zhou ||| yufei zhan ||| dongmei fu ||| 
2021 ||| multi-scale squeeze u-segnet with multi global attention for brain mri segmentation. ||| chaitra dayananda ||| jae-young choi ||| bumshik lee ||| 
2022 ||| maff-net: multi-attention guided feature fusion network for change detection in remote sensing images. ||| jinming ma ||| gang shi ||| yanxiang li ||| ziyu zhao ||| 
2022 ||| a cascade attention based facial expression recognition network by fusing multi-scale spatio-temporal features. ||| xiaoliang zhu ||| zili he ||| liang zhao ||| zhicheng dai ||| qiaolai yang ||| 
2018 ||| attention-based recurrent temporal restricted boltzmann machine for radar high resolution range profile sequence recognition. ||| yifan zhang ||| xunzhang gao ||| xuan peng ||| jiaqi ye ||| xiang li ||| 
2021 ||| small object detection in traffic scenes based on attention feature fusion. ||| jing lian ||| yuhang yin ||| linhui li ||| zhenghao wang ||| yafu zhou ||| 
2019 ||| uhf partial discharge location in power transformers via solution of the maxwell equations in a computational environment. ||| luiz augusto medeiros martins nobrega ||| edson guedes da costa ||| alexandre jean ren |||  serres ||| george victor rocha xavier ||| marcus v. d. aquino ||| 
2021 ||| patch attention layer of embedding handcrafted features in cnn for facial expression recognition. ||| xingcan liang ||| linsen xu ||| jinfu liu ||| zhipeng liu ||| gaoxin cheng ||| jiajun xu ||| lei liu ||| 
2017 ||| design and experimental study of a current transformer with a stacked pcb based on b-dot. ||| jingang wang ||| diancheng si ||| tian tian ||| ran ren ||| 
2020 ||| time series forecasting and classification models based on recurrent with attention mechanism and generative adversarial networks. ||| kun zhou ||| wenyong wang ||| teng hu ||| kai deng ||| 
2021 ||| attention-based context aware network for semantic comprehension of aerial scenery. ||| weipeng shi ||| wenhu qin ||| zhonghua yun ||| peng ping ||| kaiyang wu ||| yuke qu ||| 
2020 ||| two-stream attention network for pain recognition from video sequences. ||| patrick thiam ||| hans a. kestler ||| friedhelm schwenker ||| 
2022 ||| heart rate measurement based on 3d central difference convolution with attention mechanism. ||| xinhua liu ||| wenqian wei ||| hailan kuang ||| xiaolin ma ||| 
2021 ||| erratum: rodrigo-mor et al. principles of charge estimation methods using high-frequency current transformer sensors in partial discharge measurements. sensors 2020, 20, 2520. ||| armando rodrigo-mor ||| fabio andres mu ||| oz ||| luis carlos castro heredia ||| 
2020 ||| multi-scale feature integrated attention-based rotation network for object detection in vhr aerial images. ||| feng yang ||| wentong li ||| haiwei hu ||| wanyi li ||| peng wang ||| 
2020 ||| unsupervised dark-channel attention-guided cyclegan for single-image dehazing. ||| jiahao chen ||| chong wu ||| hu chen ||| peng cheng ||| 
2020 ||| learning soft mask based feature fusion with channel and spatial attention for robust visual object tracking. ||| mustansar fiaz ||| arif mahmood ||| soon ki jung ||| 
2022 ||| application of thermography and adversarial reconstruction anomaly detection in power cast-resin transformer. ||| kuo-hao fanchiang ||| cheng-chien kuo ||| 
2022 ||| deep learning and transformer approaches for uav-based wildfire detection and segmentation. ||| rafik ghali ||| moulay a. akhloufi ||| wided souid ||| ne mseddi ||| 
2019 ||| traffic speed prediction: an attention-based method. ||| duanyang liu ||| longfeng tang ||| guojiang shen ||| xiao han ||| 
2022 ||| attention-based deep recurrent neural network to forecast the temperature behavior of an electric arc furnace side-wall. ||| diego f. godoy-rojas ||| jersson x. leon-medina ||| bernardo rueda ||| whilmar vargas ||| juan romero ||| c ||| sar pedraza ||| francesc pozo ||| diego a. tibaduiza ||| 
2018 ||| uv-vis spectroscopy: a new approach for assessing the color index of transformer insulating oil. ||| yang sing leong ||| pin jern ker ||| md zaini jamaludin ||| saifuddin m. nomanbhay ||| aiman bin ismail ||| fairuz abdullah ||| looe hui mun ||| chin kim lo ||| 
2018 ||| characterizing focused attention and working memory using eeg. ||| zainab mohamed ||| mohamed el halaby ||| tamer said ||| doaa shawky ||| ashraf h. badawi ||| 
2019 ||| classification of low frequency signals emitted by power transformers using sensors and machine learning methods. ||| daniel jancarczyk ||| marcin bernas ||| tomasz boczar ||| 
2021 ||| an approach to steady-state power transformer modeling considering direct current resistance test measurements. ||| henrique pires corr ||| a ||| fl ||| vio henrique teles vieira ||| 
2022 ||| lane mark detection with pre-aligned spatial-temporal attention. ||| yiman chen ||| zhiyu xiang ||| 
2021 ||| attention-guided image captioning through word information. ||| ziwei tang ||| yaohua yi ||| hao sheng ||| 
2022 ||| novel video surveillance-based fire and smoke classification using attentional feature map in capsule networks. ||| muksimova shakhnoza ||| umirzakova sabina ||| mardieva sevara ||| young-im cho ||| 
2021 ||| double ghost convolution attention mechanism network: a framework for hyperspectral reconstruction of a single rgb image. ||| wenju wang ||| jiangwei wang ||| 
2019 ||| fusionatt: deep fusional attention networks for multi-channel biomedical signals. ||| ye yuan ||| kebin jia ||| 
2021 ||| automatic pixel-level pavement crack recognition using a deep feature aggregation segmentation network with a scse attention mechanism module. ||| wenting qiao ||| qiangwei liu ||| xiaoguang wu ||| biao ma ||| gang li ||| 
2021 ||| dual branch attention network for person re-identification. ||| denghua fan ||| liejun wang ||| shuli cheng ||| yongming li ||| 
2021 ||| a two-stage multistep-ahead electricity load forecasting scheme based on lightgbm and attention-bilstm. ||| jinwoong park ||| eenjun hwang ||| 
2020 ||| partial discharge localization using time reversal: application to power transformers. ||| hamidreza karami ||| mohammad azadifar ||| amirhossein mostajabi ||| marcos rubinstein ||| hossein karami ||| gevork b. gharehpetian ||| farhad rachidi ||| 
2022 ||| cross encoder-decoder transformer with global-local visual extractor for medical image captioning. ||| hojun lee ||| hyunjun cho ||| jieun park ||| jinyeong chae ||| jihie kim ||| 
2019 ||| development of acoustic emission sensor optimized for partial discharge monitoring in power transformers. ||| wojciech sikorski ||| 
2022 ||| fast panoptic segmentation with soft attention embeddings. ||| andra petrovai ||| sergiu nedevschi ||| 
2018 ||| simultaneous ship detection and orientation estimation in sar images based on attention module and angle regression. ||| jizhou wang ||| changhua lu ||| weiwei jiang ||| 
2021 ||| a transformer-based neural machine translation model for arabic dialects that utilizes subword units. ||| laith h. baniata ||| isaac k. e. ampomah ||| seyoung park ||| 
2019 ||| anti-interference deep visual identification method for fault localization of transformer using a winding model. ||| jiajun duan ||| yigang he ||| xiaoxin wu ||| hui zhang ||| wenjie wu ||| 
2018 ||| action recognition by an attention-aware temporal weighted convolutional neural network. ||| le wang ||| jinliang zang ||| qilin zhang ||| zhenxing niu ||| gang hua ||| nanning zheng ||| 
2021 ||| optical voltage transformer based on fbg-pzt for power quality measurement. ||| marceli n. gon ||| alves ||| marcelo martins werneck ||| 
2020 ||| fusing visual attention cnn and bag of visual words for cross-corpus speech emotion recognition. ||| minji seo ||| myungho kim ||| 
2021 ||| harp: hierarchical attention oriented region-based processing for high-performance computation in vision sensor. ||| pankaj bhowmik ||| md jubaer hossain pantho ||| christophe bobda ||| 
2021 ||| dual memory lstm with dual attention neural network for spatiotemporal prediction. ||| teng li ||| yepeng guan ||| 
2021 ||| a clustering method of case-involved news by combining topic network and multi-head attention mechanism. ||| cunli mao ||| haoyuan liang ||| zhengtao yu ||| yuxin huang ||| junjun guo ||| 
2021 ||| monocular depth estimation with joint attention feature distillation and wavelet-based loss function. ||| peng liu ||| zonghua zhang ||| zhaozong meng ||| nan gao ||| 
2021 ||| computer-aided diagnosis of alzheimer's disease through weak supervision deep learning framework with attention mechanism. ||| shuang liang ||| yu gu ||| 
2021 ||| an efficient anomaly recognition framework using an attention residual lstm in surveillance videos. ||| waseem ullah ||| amin ullah ||| tanveer hussain ||| zulfiqar ahmad khan ||| sung wook baik ||| 
2019 ||| a novel differential high-frequency current transformer sensor for series arc fault detection. ||| guanghai bao ||| xiaoqing gao ||| run jiang ||| kai huang ||| 
2021 ||| blood pressure morphology assessment from photoplethysmogram and demographic information using deep learning with attention mechanism. ||| nicolas aguirre ||| edith grall-ma ||| s ||| leandro j. cymberknop ||| ricardo l. armentano ||| 
2021 ||| larnet: real-time detection of facial micro expression using lossless attention residual network. ||| mohammad farukh hashmi ||| b. kiran kumar ashish ||| vivek sharma ||| avinash g. keskar ||| neeraj dhanraj bokde ||| jin hee yoon ||| zong woo geem ||| 
2019 ||| uncertainty analysis of a test bed for calibrating voltage transformers vs. temperature. ||| alessandro mingotti ||| lorenzo peretto ||| roberto tinarelli ||| abbas ghaderi ||| 
2020 ||| a dual-attention recurrent neural network method for deep cone thickener underflow concentration prediction. ||| zhaolin yuan ||| jinlong hu ||| di wu ||| xiaojuan ban ||| 
2021 ||| cross-attention fusion based spatial-temporal multi-graph convolutional network for traffic flow prediction. ||| kun yu ||| xizhong qin ||| zhenhong jia ||| yan du ||| mengmeng lin ||| 
2021 ||| multi-scale capsule attention network and joint distributed optimal transport for bearing fault diagnosis under different working loads. ||| zihao sun ||| xianfeng yuan ||| xu fu ||| fengyu zhou ||| chengjin zhang ||| 
2021 ||| multi-u-net: residual module under multisensory field and attention mechanism based optimized u-net for vhr image semantic segmentation. ||| si ran ||| jianli ding ||| bohua liu ||| xiangyu ge ||| guolin ma ||| 
2021 ||| age and gender recognition using a convolutional neural network with a specially designed multi-attention module through speech spectrograms. ||| tursunov anvarjon ||| mustaqeem ||| joon yeon choeh ||| soonil kwon ||| 
2021 ||| a lightweight attention-based cnn model for efficient gait recognition with wearable imu sensors. ||| haohua huang ||| pan zhou ||| ye li ||| fangmin sun ||| 
2020 ||| end-to-end automatic pronunciation error detection based on improved hybrid ctc/attention architecture. ||| long zhang ||| ziping zhao ||| chunmei ma ||| linlin shan ||| huazhi sun ||| lifen jiang ||| shiwen deng ||| chang gao ||| 
2020 ||| object tracking in rgb-t videos using modal-aware attention network and competitive learning. ||| hui zhang ||| lei zhang ||| li zhuo ||| jing zhang ||| 
2020 ||| high-resolution neural network for driver visual attention prediction. ||| byeongkeun kang ||| yeejin lee ||| 
2018 ||| a feasibility study of transformer winding temperature and strain detection based on distributed optical fibre sensors. ||| yunpeng liu ||| yuan tian ||| xiaozhou fan ||| yanan bu ||| peng he ||| huan li ||| junyi yin ||| xiaojiang zheng ||| 
2021 ||| hadf-crowd: a hierarchical attention-based dense feature extraction network for single-image crowd counting. ||| naveed ilyas ||| boreom lee ||| kiseon kim ||| 
2020 ||| transformer winding deformation detection based on botdr and rotdr. ||| shuguo gao ||| yunpeng liu ||| huan li ||| lu sun ||| hongliang liu ||| qun rao ||| xiaozhou fan ||| 
2019 ||| real-time visual tracking with variational structure attention network. ||| yeongbin kim ||| joongchol shin ||| hasil park ||| joonki paik ||| 
2021 ||| numerical and experimental evaluation and heat transfer characteristics of a soft magnetic transformer built from laminated steel plates. ||| eduardo cano-pleite ||| andr ||| s barrado ||| n ||| stor garcia-hernando ||| emilio ol ||| as ||| antonio soria-verdugo ||| 
2021 ||| detecting attention levels in adhd children with a video game and the measurement of brain activity with a single-channel bci headset. ||| almudena serrano-barroso ||| roma siugzdaite ||| jaime guerrero-cubero ||| alberto j. molina-cantero ||| isabel m. g ||| mez-gonz ||| lez ||| juan carlos lopez ||| juan pedro vargas ||| 
2021 ||| relation-based deep attention network with hybrid memory for one-shot person re-identification. ||| runxuan si ||| jing zhao ||| yuhua tang ||| shaowu yang ||| 
2021 ||| quantitative analysis of metallographic image using attention-aware deep neural networks. ||| yifei xu ||| yuewan zhang ||| meizi zhang ||| mian wang ||| wujiang xu ||| chaoyong wang ||| yan sun ||| pingping wei ||| 
2021 ||| tunneling magnetoresistance dc current transformer for ion beam diagnostics. ||| eman azab ||| yasser g. hegazy ||| hansjoerg reeg ||| marcus schwickert ||| klaus hofmann ||| 
2020 ||| attpnet: attention-based deep neural network for 3d point set analysis. ||| yufeng yang ||| yixiao ma ||| jing zhang ||| xin gao ||| min xu ||| 
2019 ||| integrated on-chip transformers: recent progress in the design, layout, modeling and fabrication. ||| rayan bajwa ||| murat kaya yapici ||| 
2019 ||| an lstm-based method with attention mechanism for travel time prediction. ||| xiangdong ran ||| zhiguang shan ||| yufei fang ||| chuang lin ||| 
2021 ||| modeling capacitive low-power voltage transformer behavior over temperature and frequency. ||| alessandro mingotti ||| federica costa ||| gaetano pasini ||| lorenzo peretto ||| roberto tinarelli ||| 
2021 ||| prediction of head movement in 360-degree videos using attention model. ||| dongwon lee ||| minji choi ||| joohyun lee ||| 
2021 ||| cross attention squeeze excitation network (case-net) for whole body fetal mri segmentation. ||| justin lo ||| saiee nithiyanantham ||| jillian cardinell ||| dylan young ||| sherwin cho ||| abirami kirubarajan ||| matthias w. wagner ||| roxana azma ||| steven miller ||| mike seed ||| birgit ertl-wagner ||| dafna sussman ||| 
2021 ||| face manipulation detection based on supervised multi-feature fusion attention network. ||| lin cao ||| wenjun sheng ||| fan zhang ||| kangning du ||| chong fu ||| peiran song ||| 
2021 ||| dual crisscross attention module for road extraction from remote sensing images. ||| chuan chen ||| huilin zhao ||| wei cui ||| xin he ||| 
2020 ||| adversarial networks for scale feature-attention spectral image reconstruction from a single rgb. ||| pengfei liu ||| huaici zhao ||| 
2020 ||| skin lesion classification using densely connected convolutional networks with attention residual learning. ||| jing wu ||| wei hu ||| yuan wen ||| wenli tu ||| xiaoming liu ||| 
2022 ||| a two-stage approach to important area detection in gathering place using a novel multi-input attention network. ||| jianqiang xu ||| haoyu zhao ||| weidong min ||| 
2021 ||| visual attention and color cues for 6d pose estimation on occluded scenarios using rgb-d data. ||| joel vidal ||| chyi-yeu lin ||| robert mart ||| 
2020 ||| few-shot personalized saliency prediction based on adaptive image selection considering object and visual attention. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2020 ||| multi-modality emotion recognition model with gat-based multi-head inter-modality attention. ||| changzeng fu ||| chaoran liu ||| carlos toshinori ishi ||| hiroshi ishiguro ||| 
2021 ||| attention-based 3d human pose sequence refinement network. ||| do-yeop kim ||| ju yong chang ||| 
2020 ||| spatial attention fusion for obstacle detection using mmwave radar and vision sensor. ||| shuo chang ||| yifan zhang ||| fan zhang ||| xiaotong zhao ||| sai huang ||| zhiyong feng ||| zhiqing wei ||| 
2022 ||| spatial-temporal convolutional transformer network for multivariate time series forecasting. ||| lei huang ||| feng mao ||| kai zhang ||| zhiheng li ||| 
2021 ||| far-net: feature-wise attention-based relation network for multilabel jujube defect classification. ||| xiaohang xu ||| hong zheng ||| changhui you ||| zhongyuan guo ||| xiongbin wu ||| 
2019 ||| ship detection for optical remote sensing images based on visual attention enhanced network. ||| fukun bi ||| jinyuan hou ||| liang chen ||| zhihua yang ||| yanping wang ||| 
2021 ||| capformer: pedestrian crossing action prediction using transformer. ||| javier lorenzo ||| ignacio parra alonso ||| rub ||| n izquierdo ||| augusto luis ballardini ||| lvaro hern ||| ndez-saz ||| david fern ||| ndez llorca ||| miguel  ||| ngel sotelo ||| 
2021 ||| attendaffectnet-emotion prediction of movie viewers using multimodal fusion with self-attention. ||| ha thi phuong thao ||| balamurali b. t. ||| gemma roig ||| dorien herremans ||| 
2022 ||| eye-tracker study of influence of affective disruptive content on user's visual attention and emotional state. ||| anna lewandowska ||| izabela rejer ||| kamil bortko ||| jaroslaw jankowski ||| 
2021 ||| gated graph attention network for cancer prediction. ||| linling qiu ||| han li ||| meihong wang ||| xiaoli wang ||| 
2021 ||| computer vision-based bridge damage detection using deep convolutional networks with expectation maximum attention module. ||| wenting qiao ||| biao ma ||| qiangwei liu ||| xiaoguang wu ||| gang li ||| 
2020 ||| attention-based automated feature extraction for malware analysis. ||| sunoh choi ||| jangseong bae ||| changki lee ||| youngsoo kim ||| jonghyun kim ||| 
2021 ||| an attention-enhanced multi-scale and dual sign language recognition network based on a graph convolution network. ||| lu meng ||| ronghui li ||| 
2021 ||| attention based bidirectional convolutional lstm for high-resolution radio tomographic imaging. ||| hongzhuang wu ||| xiaoli ma ||| chao-han huck yang ||| songyong liu ||| 
2021 ||| synthesis design on wideband single-ended and differential dual-band filtering impedance transformer. ||| weijuan chen ||| yongle wu ||| weimin wang ||| kai xu ||| jin shi ||| 
2021 ||| all-pass network and transformer based sige bicmos phase shifter for multi-band arrays. ||| can  ||| aliskan ||| melik yazici ||| yasar gurbuz ||| 
2021 ||| a 28-ghz doherty power amplifier with a compact transformer-based quadrature hybrid in 65-nm cmos. ||| chongyu yu ||| jun feng ||| dixian zhao ||| 
2021 ||| doubly-tuned transformer networks: a tutorial. ||| andrea bevilacqua ||| andrea mazzanti ||| 
2021 ||| a 30-36 ghz passive hybrid phase shifter with a transformer-based high-resolution reflect-type phase shifting technique. ||| xuguang li ||| bing liu ||| haipeng fu ||| kaixue ma ||| 
2021 ||| a wideband cmos frequency quadrupler with transformer-based tail feedback loop. ||| yiming yu ||| pan tang ||| kai yi ||| chenxi zhao ||| huihua liu ||| yunqiu wu ||| wen-yan yin ||| kai kang ||| 
2022 ||| ensemble learning with attention-based multiple instance pooling for classification of spt. ||| qinghua zhou ||| xin zhang ||| yu-dong zhang ||| 
2021 ||| transformer secondary voltage based resonant frequency tracking for llc converter. ||| yuqi wei ||| quanming luo ||| zhiqing wang ||| h. alan mantooth ||| 
2021 ||| -order transformer-based injection-locked frequency divider with 87.1% locking range in 40-nm cmos. ||| junhua zhu ||| qiyao jiang ||| hamed mosalam ||| chenchang zhan ||| quan pan ||| 
2021 ||| a mmc-based multiport power electronic transformer with shared medium-frequency transformer. ||| dajun ma ||| wu chen ||| liangcai shu ||| xiaohui qu ||| kai hou ||| 
2017 ||| social sensors based online attention computing of public safety events. ||| zheng xu ||| neil y. yen ||| hui zhang ||| xiao wei ||| zhihan lv ||| kim-kwang raymond choo ||| lin mei ||| xiangfeng luo ||| 
2021 ||| is your document novel? let attention guide you. an attention-based model for document-level novelty detection. ||| tirthankar ghosal ||| vignesh edithal ||| asif ekbal ||| pushpak bhattacharyya ||| srinivasa satya sameer kumar chivukula ||| george tsatsaronis ||| 
2021 ||| an analysis of the impact of attentional momentum effect on driver's ability of awareness during night-time driving. ||| hirotoshi shirayanagi ||| toshio yoshii ||| shinya kurauchi ||| takahiro tsubota ||| 
2021 ||| attention-based multi-modal sentiment analysis and emotion detection in conversation using rnn. ||| mahesh g. huddar ||| sanjeev s. sannakki ||| vijay s. rajpurohit ||| 
2021 ||| self-attention implicit function networks for 3d dental data completion. ||| yuhan ping ||| guodong wei ||| lei yang ||| zhiming cui ||| wenping wang ||| 
2021 ||| attentional markov model for human mobility prediction. ||| huandong wang ||| yong li ||| depeng jin ||| zhu han ||| 
2021 ||| attention-weighted federated deep reinforcement learning for device-to-device assisted heterogeneous collaborative edge caching. ||| xiaofei wang ||| ruibin li ||| chenyang wang ||| xiuhua li ||| tarik taleb ||| victor c. m. leung ||| 
2022 ||| deep reinforcement learning with communication transformer for adaptive live streaming in wireless edge networks. ||| shuoyao wang ||| suzhi bi ||| ying-jun angela zhang ||| 
2021 ||| siabr: a structured intra-attention bidirectional recurrent deep learning method for ultra-accurate terahertz indoor localization. ||| shukai fan ||| yongzhi wu ||| chong han ||| xudong wang ||| 
2019 ||| single-phase common mode transformer-less soft-switching grid-connected inverter with eliminated leakage current. ||| mehdi tofigh azary ||| mehran sabahi ||| ebrahim babaei ||| 
2019 ||| a new structure of single-phase two-stage hybrid transformerless multilevel pv inverter. ||| sateesh kumar kuncham ||| kirubakaran annamalai ||| subrahmanyam nallamothu ||| 
2021 ||| a k-band high-gain power amplifier with slow-wave transmission-line transformer in 130-nm rf cmos. ||| haomin hou ||| jin he ||| junren pan ||| hao wang ||| sheng chang ||| qijun huang ||| yinxia zhu ||| 
2022 ||| a new design of transformerless, non-isolated, high step-up dc-dc converter with hybrid fuzzy logic mppt controller. ||| chakarajamula hussaian basha ||| matcha murali ||| 
2021 ||| non-isolated single-phase inverter based on an autotransformer for low-power applications. ||| douglas rosa corr ||| a ||| juliano de faria andrade ||| aniel silva de morais ||| leandro sousa vilefort ||| fernando lessa tofoli ||| 
2021 ||| multi-input transformer-less four-wire microinverter with distributed mppt for pv systems. ||| mahmoud a. gaafar ||| eltaib abdeen ibrahim ||| mohamed orabi ||| 
2019 ||| modified method for transformer magnetizing characteristic computation and point-on-wave control switching for inrush current mitigation. ||| abdelghani yahiou ||| abdelhafid bayadi ||| badreddine babes ||| 
2019 ||| analysis, modeling, and implementation of a new transformerless semi-quadratic buck-boost dc/dc converter. ||| sara hasanpour ||| ali mostaan ||| alfred baghramian ||| hamed mojallali ||| 
2018 ||| generalized analytical formulae to compute electrical characteristics of a homogenous ladder network of the transformer winding. ||| mithun mondal ||| ganesh balu kumbhar ||| 
2021 ||| design of high-isolation and low-loss single pole double throw switch based on the triple-coupled transformer for ultra-wideband phased array systems. ||| zeyuan wang ||| zhenrong li ||| yuxin wang ||| zhen li ||| yiqi zhuang ||| 
2017 ||| high-efficiency transformerless buck-boost dc-dc converter. ||| mohamad reza banaei ||| hossein ajdar faeghi bonab ||| 
2021 ||| multi-input multi-phase transformerless large voltage conversion ratio dc/dc converter. ||| zahra saadatizadeh ||| pedram chavoshipour heris ||| mehran sabahi ||| xiaodong liang ||| 
2021 ||| design and optimization of 3-kw inductive power transfer charging system with compact asymmetric loosely coupled transformer for special applications. ||| hongbo ma ||| xiaobin li ||| bin zhang ||| junhong yi ||| xiaoqiang wang ||| jianping xu ||| 
2018 ||| llc resonant converter utilizing a step-gap transformer structure for holdup time improvement. ||| jing-yuan lin ||| sih-yi lee ||| hsuan-yu yueh ||| 
2020 ||| modelling characteristics of the impulse transformer in a wide frequency range. ||| krzysztof g ||| recki ||| malgorzata godlewska ||| 
2018 ||| tuning the program transformers from cc to pdl. ||| pere pardo ||| enrique sari ||| n-morrillo ||| fernando soler-toscano ||| fernando r. vel ||| zquez-quesada ||| 
2021 ||| mobile app-based chatbot to deliver cognitive behavioral therapy and psychoeducation for adults with attention deficit: a development and feasibility/usability study. ||| sooah jang ||| jae-jin kim ||| soo-jeong kim ||| jieun hong ||| suji kim ||| eunjoo kim ||| 
2022 ||| automatic snomed ct coding of chinese clinical terms via attention-based semantic matching. ||| yani chen ||| danqing hu ||| mengyang li ||| huilong duan ||| xudong lu ||| 
2022 ||| explainable icd multi-label classification of ehrs in spanish with convolutional attention. ||| owen trigueros ||| alberto blanco ||| nuria lebe ||| a ||| arantza casillas ||| alicia p ||| rez ||| 
2020 ||| attention-deficit/ hyperactivity disorder mobile apps: a systematic review. ||| costina ruxandra pasarelu ||| gerhard andersson ||| anca dobrean ||| 
2017 ||| robotic autonomous behavior selection using episodic memory and attention system. ||| dong liu ||| ming cong ||| yu du ||| qiang zou ||| yingxue cui ||| 
2021 ||| improving novelty detection by self-supervised learning and channel attention mechanism. ||| miao tian ||| ying cui ||| haixia long ||| junxia li ||| 
2022 ||| an end-to-end deep context gate convolutional visual odometry system based on lightweight attention mechanism. ||| yan xu ||| hong qin ||| jiani huang ||| yanyun wang ||| 
2021 ||| a dual deep neural network with phrase structure and attention mechanism for sentiment analysis. ||| dongning rao ||| sihong huang ||| zhihua jiang ||| ganesh gopal deverajan ||| rizwan patan ||| 
2020 ||| prediction of attentional focus from respiration with simple feed-forward and time delay neural networks. ||| michael christopher melnychuk ||| peter r. murphy ||| ian h. robertson ||| joshua h. balsters ||| paul m. dockree ||| 
2022 ||| correction to: combining functional near-infrared spectroscopy and eeg measurements for the diagnosis of attention-deficit hyperactivity disorder. ||| ayseg ||| l g ||| ven ||| miray altinkaynak ||| nazan dolu ||| meltem izzetoglu ||| ferhat pektas ||| sevgi  ||| zmen ||| esra demirci ||| turgay batbat ||| 
2020 ||| efficient human motion recovery using bidirectional attention network. ||| qiongjie cui ||| huaijiang sun ||| yupeng li ||| yue kong ||| 
2022 ||| a topic-based multi-channel attention model under hybrid mode for image caption. ||| kui qian ||| lei tian ||| 
2021 ||| automatic identification of commodity label images using lightweight attention network. ||| junde chen ||| adnan zeb ||| shuangyuan yang ||| defu zhang ||| yaser ahangari nanehkaran ||| 
2021 ||| an attention-based cnn-lstm model for subjectivity detection in opinion-mining. ||| santwana sagnika ||| bhabani shankar prasad mishra ||| saroj k. meher ||| 
2022 ||| dual attention granularity network for vehicle re-identification. ||| jianhua zhang ||| jingbo chen ||| jiewei cao ||| ruyu liu ||| linjie bian ||| shengyong chen ||| 
2019 ||| context-aware attention network for image recognition. ||| jiaxu leng ||| ying liu ||| shang chen ||| 
2020 ||| crhasum: extractive text summarization with contextualized-representation hierarchical-attention summarization network. ||| yufeng diao ||| hongfei lin ||| liang yang ||| xiaochao fan ||| yonghe chu ||| di wu ||| dongyu zhang ||| kan xu ||| 
2020 ||| a self-attention-based destruction and construction learning fine-grained image classification method for retail product recognition. ||| wenyong wang ||| yongcheng cui ||| guangshun li ||| chuntao jiang ||| song deng ||| 
2022 ||| traditional chinese medicine entity relation extraction based on cnn with segment attention. ||| tian bai ||| haotian guan ||| shang wang ||| ye wang ||| lan huang ||| 
2021 ||| dilated causal convolution with multi-head self attention for sensor human activity recognition. ||| rebeen ali hamad ||| masashi kimura ||| longzhi yang ||| wai lok woo ||| bo wei ||| 
2020 ||| multi-scale attention vehicle re-identification. ||| aihua zheng ||| xianmin lin ||| jiacheng dong ||| wenzhong wang ||| jin tang ||| bin luo ||| 
2020 ||| combining functional near-infrared spectroscopy and eeg measurements for the diagnosis of attention-deficit hyperactivity disorder. ||| ayseg ||| l g ||| ven ||| miray altinkaynak ||| nazan dolu ||| meltem izzetoglu ||| ferhat pektas ||| sevgi  ||| zmen ||| esra demirci ||| turgay batbat ||| 
2021 ||| cda-lstm: an evolutionary convolution-based dual-attention lstm for univariate time series prediction. ||| xiaoquan chu ||| haibin jin ||| yue li ||| jianying feng ||| weisong mu ||| 
2022 ||| triple-layer attention mechanism-based network embedding approach for anchor link identification across social networks. ||| yao li ||| huiyuan cui ||| huilin liu ||| xiaoou li ||| 
2020 ||| a transformer-based approach to irony and sarcasm detection. ||| rolandos alexandros potamias ||| georgios siolas ||| andreas-georgios stafylopatis ||| 
2020 ||| deep joint two-stream wasserstein auto-encoder and selective attention alignment for unsupervised domain adaptation. ||| zhihong chen ||| chao chen ||| xinyu jin ||| yifu liu ||| zhaowei cheng ||| 
2022 ||| aenet: an attention-enabled neural architecture for fake news detection using contextual features. ||| vidit jain ||| rohit kumar kaliyar ||| anurag goswami ||| pratik narang ||| yashvardhan sharma ||| 
2020 ||| fabnet: feature attention-based network for simultaneous segmentation of microvessels and nerves in routine histology images of oral cancer. ||| m. m. fraz ||| syed ali khurram ||| simon graham ||| muhammad shaban ||| mariam hassan ||| asif loya ||| nasir m. rajpoot ||| 
2021 ||| a deep embedding model for knowledge graph completion based on attention mechanism. ||| jin huang ||| tinghua zhang ||| jia zhu ||| weihao yu ||| yong tang ||| yang he ||| 
2021 ||| transformer guided geometry model for flow-based unsupervised visual odometry. ||| xiangyu li ||| yonghong hou ||| pichao wang ||| zhimin gao ||| mingliang xu ||| wanqing li ||| 
2021 ||| han-regru: hierarchical attention network with residual gated recurrent unit for emotion recognition in conversation. ||| hui ma ||| jian wang ||| lingfei qian ||| hongfei lin ||| 
2021 ||| boosting attention fusion generative adversarial network for image denoising. ||| qiongshuai lyu ||| min guo ||| miao ma ||| 
2022 ||| deep semantic hashing with dual attention for cross-modal retrieval. ||| jiagao wu ||| weiwei weng ||| junxia fu ||| linfeng liu ||| bin hu ||| 
2021 ||| adaptive feature fusion with attention mechanism for multi-scale target detection. ||| moran ju ||| jiangning luo ||| zhongbo wang ||| haibo luo ||| 
2020 ||| electronic word-of-mouth effects on studio performance leveraging attention-based model. ||| yang liu ||| hao fei ||| qingguo zeng ||| bobo li ||| lili ma ||| donghong ji ||| joaqu ||| n b. ordieres mer ||| 
2020 ||| multi-granularity bidirectional attention stream machine comprehension method for emotion cause extraction. ||| yufeng diao ||| hongfei lin ||| liang yang ||| xiaochao fan ||| yonghe chu ||| di wu ||| kan xu ||| bo xu ||| 
2020 ||| spatiotemporal saliency-based multi-stream networks with attention-aware lstm for action recognition. ||| zhenbing liu ||| zeya li ||| ruili wang ||| ming zong ||| wanting ji ||| 
2020 ||| single-column cnn for crowd counting with pixel-wise attention mechanism. ||| bisheng wang ||| guo cao ||| yanfeng shang ||| licun zhou ||| youqiang zhang ||| xuesong li ||| 
2022 ||| discriminative attention-augmented feature learning for facial expression recognition in the wild. ||| linyi zhou ||| xijian fan ||| tardi tjahjadi ||| sruti das choudhury ||| 
2021 ||| character-based handwritten text transcription with attention networks. ||| jason poulos ||| rafael valle ||| 
2020 ||| single-image super-resolution with multilevel residual attention network. ||| ding qin ||| xiaodong gu ||| 
2022 ||| adaptive kernel selection network with attention constraint for surgical instrument classification. ||| yaqing hou ||| wenkai zhang ||| qian liu ||| hongwei ge ||| jun meng ||| qiang zhang ||| xiaopeng wei ||| 
2020 ||| path-based reasoning with constrained type attention for knowledge graph completion. ||| kai lei ||| jin zhang ||| yuexiang xie ||| desi wen ||| daoyuan chen ||| min yang ||| ying shen ||| 
2022 ||| hierarchical attention network for attributed community detection of joint representation. ||| qiqi zhao ||| huifang ma ||| lijun guo ||| zhixin li ||| 
2022 ||| multi-view dual attention network for 3d object recognition. ||| wenju wang ||| yu cai ||| tao wang ||| 
2022 ||| laanet: lightweight attention-guided asymmetric network for real-time semantic segmentation. ||| xiuling zhang ||| bingce du ||| ziyun wu ||| tingbo wan ||| 
2021 ||| local-aware spatio-temporal attention network with multi-stage feature fusion for human action recognition. ||| yaqing hou ||| hua yu ||| dongsheng zhou ||| pengfei wang ||| hongwei ge ||| jianxin zhang ||| qiang zhang ||| 
2022 ||| modeling and experimental validation of dry-type transformers with multiobjective swarm intelligence-based optimization algorithms for industrial application. ||| tugce demirdelen ||| burak esenboga ||| inayet ozge aksu ||| alican ozdogan ||| abdurrahman yavuzdeger ||| firat ekinci ||| mehmet t ||| may ||| 
2020 ||| application of improved genetic algorithm in ultrasonic location of transformer partial discharge. ||| youchan zhu ||| li zhou ||| haisheng xu ||| 
2020 ||| dkd-dad: a novel framework with discriminative kinematic descriptor and deep attention-pooled descriptor for action recognition. ||| ming tong ||| mingyang li ||| he bai ||| lei ma ||| mengao zhao ||| 
2020 ||| interactive knowledge-enhanced attention network for answer selection. ||| weiyi huang ||| qiang qu ||| min yang ||| 
2021 ||| improve relation extraction with dual attention-guided graph convolutional networks. ||| zhixin li ||| yaru sun ||| jianwei zhu ||| suqin tang ||| canlong zhang ||| huifang ma ||| 
2019 ||| in-air handwritten english word recognition using attention recurrent translator. ||| ji gan ||| weiqiang wang ||| 
2022 ||| temporal attention augmented transformer hawkes process. ||| lu-ning zhang ||| jian-wei liu ||| zhi-yan song ||| xin zuo ||| 
2021 ||| a structure distinguishable graph attention network for knowledge base completion. ||| xue zhou ||| bei hui ||| lizong zhang ||| kexi ji ||| 
2020 ||| recurrent neural network with attention mechanism for language model. ||| mu-yen chen ||| hsiu-sen chiang ||| arun kumar sangaiah ||| tsung-che hsieh ||| 
2021 ||| residual attention convolutional autoencoder for feature learning and fault detection in nonlinear industrial processes. ||| xing liu ||| jianbo yu ||| lyujiangnan ye ||| 
2020 ||| tilegan: category-oriented attention-based high-quality tiled clothes generation from dressed person. ||| wei zeng ||| mingbo zhao ||| yuan gao ||| zhao zhang ||| 
2021 ||| graph convolutional networks with attention for multi-label weather recognition. ||| kezhen xie ||| zhiqiang wei ||| lei huang ||| qibing qin ||| wenfeng zhang ||| 
2021 ||| frequency-amplitude coupling: a new approach for decoding of attended features in covert visual attention task. ||| saeideh davoudi ||| amirmasoud ahmadi ||| mohammad reza daliri ||| 
2019 ||| dilated residual attention network for load disaggregation. ||| min xia ||| wan'an liu ||| yiqing xu ||| ke wang ||| xu zhang ||| 
2021 ||| application of solely self-attention mechanism in csi-fingerprinting-based indoor localization. ||| kabo poloko nkabiti ||| yueyun chen ||| 
2021 ||| dgfau-net: global feature attention upsampling network for medical image segmentation. ||| dunlu peng ||| xi yu ||| wenjia peng ||| jianping lu ||| 
2021 ||| two-branch encoding and iterative attention decoding network for semantic segmentation. ||| hegui zhu ||| min zhang ||| xiangde zhang ||| libo zhang ||| 
2022 ||| gsta: gated spatial-temporal attention approach for travel time prediction. ||| alkilane khaled ||| alfateh m. tag elsir ||| yanming shen ||| 
2021 ||| improving neural machine translation using gated state network and focal adaptive attention networtk. ||| li huang ||| wenyu chen ||| yuguo liu ||| he zhang ||| hong qu ||| 
2020 ||| kganet: a knowledge graph attention network for enhancing natural language inference. ||| meina song ||| wen zhao ||| haihong e ||| 
2021 ||| automatic lumbar spinal mri image segmentation with a multi-scale attention network. ||| haixing li ||| haibo luo ||| huan wang ||| zelin shi ||| chongnan yan ||| lanbo wang ||| yueming mu ||| yunpeng liu ||| 
2021 ||| dual-attention network with multitask learning for multistep short-term speed prediction on expressways. ||| yanyun tao ||| guoqi yue ||| xiang wang ||| 
2021 ||| dm-ctsa: a discriminative multi-focused and complementary temporal/spatial attention framework for action recognition. ||| ming tong ||| kaibo yan ||| lei jin ||| xing yue ||| mingyang li ||| 
2020 ||| arbitrary-oriented object detection via dense feature fusion and attention model for remote sensing super-resolution image. ||| fuhao zou ||| wei xiao ||| wanting ji ||| kunkun he ||| zhixiang yang ||| jingkuan song ||| helen zhou ||| kai li ||| 
2020 ||| deep refinement: capsule network with attention mechanism-based system for text classification. ||| deepak kumar jain ||| rachna jain ||| yash upadhyay ||| abhishek kathuria ||| xiangyuan lan ||| 
2021 ||| a multimodal fake news detection model based on crossmodal attention residual and multichannel convolutional neural networks. ||| chenguang song ||| nianwen ning ||| yunlei zhang ||| bin wu ||| 
2021 ||| adversarial network integrating dual attention and sparse representation for semi-supervised semantic segmentation. ||| ge jin ||| chuancai liu ||| xu chen ||| 
2020 ||| aldonar: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalized domain ontology and a regularized neural attention model. ||| donatas meskele ||| flavius frasincar ||| 
2020 ||| a comparative study of outfit recommendation methods with a focus on attention-based fusion. ||| katrien laenen ||| marie-francine moens ||| 
2022 ||| a convolutional attention network for unifying general and sequential recommenders. ||| shahpar yakhchi ||| amin behehsti ||| seyed mohssen ghafari ||| imran razzak ||| mehmet a. orgun ||| mehdi elahi ||| 
2022 ||| fine-grained citation count prediction via a transformer-based model with among-attention mechanism. ||| shengzhi huang ||| yong huang ||| yi bu ||| wei lu ||| jiajia qian ||| dan wang ||| 
2019 ||| attention-based long short-term memory network using sentiment lexicon embedding for aspect-level sentiment analysis in korean. ||| minchae song ||| hyunjung park ||| kyung-shik shin ||| 
2019 ||| aspect-based sentiment analysis with alternating coattention networks. ||| chao yang ||| hefeng zhang ||| bin jiang ||| keqin li ||| 
2020 ||| mgat: multimodal graph attention network for recommendation. ||| zhulin tao ||| yinwei wei ||| xiang wang ||| xiangnan he ||| xianglin huang ||| tat-seng chua ||| 
2022 ||| multi-attribute adaptive aggregation transformer for vehicle re-identification. ||| zhi yu ||| jiaming pei ||| mingpeng zhu ||| jiwei zhang ||| jinhai li ||| 
2020 ||| multi-modal fusion with multi-level attention for visual dialog. ||| jingping zhang ||| qiang wang ||| yahong han ||| 
2022 ||| group event recommendation based on graph multi-head attention network combining explicit and implicit information. ||| guoqiong liao ||| xiaobin deng ||| changxuan wan ||| xiping liu ||| 
2022 ||| research of chinese intangible cultural heritage knowledge graph construction and attribute value extraction with graph attention network. ||| tao fan ||| hao wang ||| 
2020 ||| exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition. ||| chao li ||| zhongtian bao ||| linhao li ||| ziping zhao ||| 
2019 ||| tdam: a topic-dependent attention model for sentiment analysis. ||| gabriele pergola ||| lin gui ||| yulan he ||| 
2020 ||| dynamic attention-based explainable recommendation with textual and visual fusion. ||| peng liu ||| lemei zhang ||| jon atle gulla ||| 
2020 ||| image caption generation with dual attention mechanism. ||| maofu liu ||| lingjun li ||| huijun hu ||| weili guan ||| jing tian ||| 
2020 ||| hierarchical neural query suggestion with an attention mechanism. ||| wanyu chen ||| fei cai ||| honghui chen ||| maarten de rijke ||| 
2020 ||| video question answering via grounded cross-attention network learning. ||| yunan ye ||| shifeng zhang ||| yimeng li ||| xufeng qian ||| siliang tang ||| shiliang pu ||| jun xiao ||| 
2022 ||| bert-smap: paying attention to essential terms in passage ranking beyond bert. ||| dengwen lin ||| jintao tang ||| xinyi li ||| kunyuan pang ||| shasha li ||| ting wang ||| 
2020 ||| transformer based contextualization of pre-trained word embeddings for irony detection in twitter. ||| jos ||| - ||| ngel gonz ||| lez ||| llu ||| s-f. hurtado ||| ferran pla ||| 
2022 ||| combining non-sampling and self-attention for sequential recommendation. ||| guangjin chen ||| guoshuai zhao ||| li zhu ||| zhimin zhuo ||| xueming qian ||| 
2021 ||| deepgrp: engineering a software tool for predicting genomic repetitive elements using recurrent neural networks with attention. ||| fabian hausmann ||| stefan kurtz ||| 
2020 ||| mediation criteria for interactive serious games aimed at improving learning in children with attention deficit hyperactivity disorder (adhd). ||| teresa coma-rosello ||| ana cristina blasco-serrano ||| maria angeles garrido ||| antonio aguelo-arguis ||| 
2021 ||| joint attention behaviour in remote collaborative problem solving: exploring different attentional levels in dyadic interaction. ||| johanna p ||| ys ||| -tarhonen ||| nafisa awwal ||| p ||| ivi h ||| kkinen ||| suzanne otieno ||| 
2021 ||| correction to: mediation criteria for interactive serious games aimed at improving learning in children with attention deficit hyperactivity disorder (adhd). ||| teresa coma-rosello ||| ana cristina blasco-serrano ||| maria angeles garrido ||| antonio aguelo-arguis ||| 
2021 ||| attention: there is an inconsistency between android permissions and application metadata! ||| huseyin alecakir ||| burcu can ||| sevil sen ||| 
2021 ||| temporal pyramid attention-based spatiotemporal fusion model for parkinson's disease diagnosis from gait data. ||| xiaomin pei ||| huijie fan ||| yandong tang ||| 
2020 ||| a systematic study of inner-attention-based sentence representations in multilingual neural machine translation. ||| ra ||| l v ||| zquez ||| alessandro raganato ||| mathias creutz ||| j ||| rg tiedemann ||| 
2021 ||| technological troubleshooting based on sentence embedding with deep transformers. ||| antonio l. alfeo ||| mario g. c. a. cimino ||| gigliola vaglini ||| 
2021 ||| remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. ||| yu mo ||| qianhui wu ||| xiu li ||| biqing huang ||| 
2021 ||| implementation of a novel algorithm of wheelset and axle box concurrent fault identification based on an efficient neural network with the attention mechanism. ||| dechen yao ||| hengchang liu ||| jianwei yang ||| jiao zhang ||| 
2019 ||| guidelines to design tangible tabletop activities for children with attention deficit hyperactivity disorder. ||| eva cerezo ||| teresa coma ||| ana cristina blasco-serrano ||| clara bonillo ||| maria angeles garrido ||| sandra baldassarri ||| 
2020 ||| self-tracking while doing sport: comfort, motivation, attention and lifestyle of athletes using personal informatics tools. ||| amon rapp ||| lia tirabeni ||| 
2022 ||| the influence of audio effects and attention on the perceived duration of interaction. ||| pang suwanaposee ||| carl gutwin ||| andy cockburn ||| 
2019 ||| the mere presence of an attentive and emotionally responsive virtual character influences focus of attention and perceived stress. ||| anna felnhofer ||| marlene kaufmann ||| katharina atteneder ||| johanna xenia kafka ||| helmut hlavacs ||| leon beutl ||| kristina hennig-fast ||| oswald d. kothgassner ||| 
2018 ||| attention allocation for human multi-robot control: cognitive analysis based on behavior data and hidden states. ||| shih yi chien ||| yi-ling lin ||| pei-ju lee ||| shuguang han ||| michael lewis ||| katia p. sycara ||| 
2018 ||| collective attention and active consumer participation in community energy systems. ||| aikaterini bourazeri ||| jeremy pitt ||| 
2020 ||| attention-based activation pruning to reduce data movement in real-time ai: a case-study on local motion planning in autonomous vehicles. ||| kruttidipta samal ||| marilyn wolf ||| saibal mukhopadhyay ||| 
2021 ||| transformerless three-level flying-capacitor step-up pv micro-inverter without electrolytic capacitors. ||| wenlong qi ||| ming-hao wang ||| sinan li ||| 
2019 ||| visual attention-aware omnidirectional video streaming using optimal tiles for virtual reality. ||| cagri ozcinar ||| juli ||| n cabrera ||| aljosa smolic ||| 
2020 ||| anchor-free object detection with mask attention. ||| he yang ||| beibei fan ||| lingling guo ||| 
2017 ||| predicting students' attention in the classroom from kinect facial and body features. ||| janez zaletelj ||| andrej kosir ||| 
2021 ||| steganographic visual story with mutual-perceived joint attention. ||| yanyang guo ||| hanzhou wu ||| xinpeng zhang ||| 
2020 ||| learning attention for object tracking with adversarial learning network. ||| xu cheng ||| chen song ||| yongxiang gu ||| beijing chen ||| 
2021 ||| retinal vessel segmentation with constrained-based nonnegative matrix factorization and 3d modified attention u-net. ||| yang yu ||| hongqing zhu ||| 
2020 ||| collaboration or battle between minds? an attention training game through collaborative and competitive reinforcement. ||| yoones a. sekhavat ||| 
2019 ||| the influence of graphical elements on user's attention and control on a neurofeedback-based game. ||| gabriel alves mendes vasiljevic ||| leonardo cunha de miranda ||| 
2022 ||| multi-label classification of legislative contents with hierarchical label attention networks. ||| danielle caled ||| m ||| rio j. silva ||| bruno martins ||| miguel won ||| 
2021 ||| paying attention to video object pattern understanding. ||| wenguan wang ||| jianbing shen ||| xiankai lu ||| steven c. h. hoi ||| haibin ling ||| 
2020 ||| neural machine translation with deep attention. ||| biao zhang ||| deyi xiong ||| jinsong su ||| 
2022 ||| covariance attention for semantic segmentation. ||| yazhou liu ||| yuliang chen ||| pongsak lasang ||| qunsen sun ||| 
2018 ||| two-stream transformer networks for video-based face alignment. ||| hao liu ||| jiwen lu ||| jianjiang feng ||| jie zhou ||| 
2022 ||| visual grounding via accumulated attention. ||| chaorui deng ||| qi wu ||| qingyao wu ||| fuyuan hu ||| fan lyu ||| mingkui tan ||| 
2019 ||| focal visual-text attention for memex question answering. ||| junwei liang ||| lu jiang ||| liangliang cao ||| yannis kalantidis ||| li-jia li ||| alexander g. hauptmann ||| 
2017 ||| aligning where to see and what to tell: image captioning with region-based attention and scene-specific contexts. ||| kun fu ||| junqi jin ||| runpeng cui ||| fei sha ||| changshui zhang ||| 
2019 ||| predicting the driver's focus of attention: the dr(eye)ve project. ||| andrea palazzi ||| davide abati ||| simone calderara ||| francesco solera ||| rita cucchiara ||| 
2021 ||| attention-based dropout layer for weakly supervised single object localization and semantic segmentation. ||| junsuk choe ||| seungho lee ||| hyunjung shim ||| 
2022 ||| parallax attention for unsupervised stereo correspondence learning. ||| longguang wang ||| yulan guo ||| yingqian wang ||| zhengfa liang ||| zaiping lin ||| jungang yang ||| wei an ||| 
2022 ||| zero-shot video object segmentation with co-attention siamese networks. ||| xiankai lu ||| wenguan wang ||| jianbing shen ||| david j. crandall ||| jiebo luo ||| 
2022 ||| heterogeneous graph attention network for unsupervised multiple-target domain adaptation. ||| xu yang ||| cheng deng ||| tongliang liu ||| dacheng tao ||| 
2019 ||| aster: an attentional scene text recognizer with flexible rectification. ||| baoguang shi ||| mingkun yang ||| xinggang wang ||| pengyuan lyu ||| cong yao ||| xiang bai ||| 
2019 ||| a deep network solution for attention and aesthetics aware photo cropping. ||| wenguan wang ||| jianbing shen ||| haibin ling ||| 
2020 ||| guided attention inference network. ||| kunpeng li ||| ziyan wu ||| kuan-chuan peng ||| jan ernst ||| yun fu ||| 
2020 ||| hierarchical lstms with adaptive attention for visual captioning. ||| lianli gao ||| xiangpeng li ||| jingkuan song ||| heng tao shen ||| 
2022 ||| purely attention based local feature integration for video classification. ||| xiang long ||| gerard de melo ||| dongliang he ||| fu li ||| zhizhen chi ||| shilei wen ||| chuang gan ||| 
2018 ||| tracking gaze and visual focus of attention of people involved in social interaction. ||| benoit mass ||| sileye o. ba ||| radu horaud ||| 
2022 ||| mra-net: improving vqa via multi-modal relation attention network. ||| liang peng ||| yang yang ||| zheng wang ||| zi huang ||| heng tao shen ||| 
2020 ||| gravitational laws of focus of attention. ||| dario zanca ||| stefano melacci ||| marco gori ||| 
2022 ||| universal adversarial attack on attention and the resulting dataset damagenet. ||| sizhe chen ||| zhengbao he ||| chengjin sun ||| jie yang ||| xiaolin huang ||| 
2020 ||| leader-based multi-scale attention deep architecture for person re-identification. ||| xuelin qian ||| yanwei fu ||| tao xiang ||| yu-gang jiang ||| xiangyang xue ||| 
2017 ||| windowing-based threshold technique to play the simple breakout game at neutral attention level. ||| gauttam jangir ||| rakesh kumar ||| 
2021 ||| review helpfulness evaluation and recommendation based on an attention model of customer expectation. ||| xianshan qu ||| xiaopeng li ||| csilla farkas ||| john r. rose ||| 
2021 ||| unsupervised spatial-awareness attention-based and multi-scale domain adaption network for point cloud classification. ||| rui guo ||| yong zhou ||| jiaqi zhao ||| rui yao ||| bing liu ||| xunhui zhang ||| 
2019 ||| a compact transformer-combined polar/quadrature reconfigurable digital power amplifier in 28-nm logic lp cmos. ||| yun yin ||| yiting zhu ||| liang xiong ||| wei luo ||| bowen chen ||| tong li ||| na yan ||| hongtao xu ||| 
2020 ||| highly integrated zvs flyback converter ics with pulse transformer to optimize usb power delivery for fast-charging mobile devices. ||| wei-hsu chang ||| yen-ming chen ||| ching-jan chen ||| pin-ying wang ||| kun-yu lin ||| chun-ching lee ||| li-di lo ||| jenn-yu lin ||| ta-yung yang ||| 
2017 ||| on the design of wideband transformer-based fourth order matching networks for e-band receivers in 28-nm cmos. ||| marco vigilante ||| patrick reynaert ||| 
2022 ||| an 85-ghz power amplifier utilizing a transformer-based power combiner operating beyond the self-resonance frequency. ||| van-son trinh ||| jung-dong park ||| 
2019 ||| a compact dual-band digital polar doherty power amplifier using parallel-combining transformer. ||| yun yin ||| liang xiong ||| yiting zhu ||| bowen chen ||| hao min ||| hongtao xu ||| 
2017 ||| a 7.9-ghz transformer-feedback quadrature oscillator with a noise-shifting coupling network. ||| bingwei jiang ||| howard c. luong ||| 
2020 ||| a broadband switched-transformer digital power amplifier for deep back-off efficiency enhancement. ||| yun yin ||| tong li ||| liang xiong ||| yicheng li ||| hao min ||| na yan ||| hongtao xu ||| 
2018 ||| analysis and design of ultra-wideband mm-wave injection-locked frequency dividers using transformer-based high-order resonators. ||| jingzhi zhang ||| yixuan cheng ||| chenxi zhao ||| yunqiu wu ||| kai kang ||| 
2021 ||| s/dec, recurrent attention in-memory processor for keyword spotting. ||| hassan dbouk ||| sujan k. gonugondla ||| charbel sakr ||| naresh r. shanbhag ||| 
2021 ||| millimeter-wave sige radiometer front end with transformer-based dicke switch and on-chip calibration noise source. ||| milad frounchi ||| amirreza alizadeh ||| hanbin ying ||| christopher t. coen ||| albin j. gasiewski ||| john d. cressler ||| 
2019 |||  1-w isolated power transfer system using fully integrated transformer with magnetic core. ||| yue zhuo ||| shaoyu ma ||| tianting zhao ||| wenhui qin ||| yuanyuan zhao ||| yingjie guo ||| haiyang yan ||| baoxing chen ||| 
2021 ||| quadrature switched/floated capacitor power amplifier with reconfigurable self-coupling canceling transformer for deep back-off efficiency enhancement. ||| bingzheng yang ||| huizhen jenny qian ||| xun luo ||| 
2020 ||| highly linear high-power 802.11ac/ax wlan sige hbt power amplifiers with a compact 2nd-harmonic-shorted four-way transformer and a thermally compensating dynamic bias circuit. ||| inchan ju ||| yunyi gong ||| john d. cressler ||| 
2019 ||| an 82-107.6-ghz integer-n adpll employing a dco with split transformer and dual-path switched-capacitor ladder and a clock-skew-sampling delta-sigma tdc. ||| zhiqiang huang ||| howard c. luong ||| 
2021 ||| eeg-based emotion recognition via capsule network with channel-wise attention and lstm models. ||| lina deng ||| xiaoliang wang ||| frank jiang ||| robin doss ||| 
2019 ||| reinforcement adaptation of an attention-based neural natural language generator for spoken dialogue systems. ||| matthieu riou ||| bassam jabaian ||| st ||| phane huet ||| fabrice lef ||| vre ||| 
2021 ||| rational, emotional, and attentional models for recommender systems. ||| ameed almomani ||| cristina monreal ||| jorge sieira ||| juan gra ||| a ||| eduardo s ||| nchez ||| 
2020 ||| air pollution forecasting based on attention-based lstm neural network and ensemble learning. ||| duen-ren liu ||| shin-jye lee ||| yang huang ||| chien-ju chiu ||| 
2022 ||| weight attention layer-based document classification incorporating information gain. ||| min seok lee ||| seok woo yang ||| hong joo lee ||| 
2022 ||| attention deep learning-based large-scale learning classifier for cassava leaf disease classification. ||| vinayakumar ravi ||| vasundhara acharya ||| tuan d. pham ||| 
2018 ||| modelling a smart environment for nonintrusive analysis of attention in the workplace. ||| dalila dur ||| es ||| davide carneiro ||| javier bajo ||| paulo novais ||| 
2021 ||| cogcn: combining co-attention with graph convolutional network for entity linking with knowledge graphs. ||| ningning jia ||| xiang cheng ||| sen su ||| liyuan ding ||| 
2021 ||| an attention-driven videogame based on steady-state motion visual evoked potentials. ||| eduardo p ||| rez-valero ||| miguel angel lopez-gordo ||| miguel a. vaquero-blasco ||| 
2021 ||| resnet-attention model for human authentication using ecg signals. ||| mohamed hammad ||| pawel plawiak ||| kuanquan wang ||| udyavara rajendra acharya ||| 
2020 ||| a secure mhealth application for attention deficit and hyperactivity disorder. ||| nayra rodr ||| guez-p ||| rez ||| pino caballero-gil ||| alexandra rivero-garc ||| a ||| josu |||  toledo-castro ||| 
2019 ||| adaptive attention annotation model: optimizing the prediction path through dependency fusion. ||| fangxin wang ||| jie liu ||| shuwu zhang ||| guixuan zhang ||| yang zheng ||| xiaoqian li ||| wei liang ||| yuejun li ||| 
2021 ||| s2san: a sentence-to-sentence attention network for sentiment analysis of online reviews. ||| ping wang ||| jiangnan li ||| jingrui hou ||| 
2021 ||| improving fake news detection with domain-adversarial and graph-attention neural network. ||| hua yuan ||| jie zheng ||| qiongwei ye ||| yu qian ||| yan zhang ||| 
2021 ||| process data properties matter: introducing gated convolutional neural networks (gcnn) and key-value-predict attention networks (kvp) for next event prediction with deep learning. ||| kai heinrich ||| patrick zschech ||| christian janiesch ||| markus bonin ||| 
2018 ||| grounding by attention simulation in peripersonal space: pupils dilate to pinch grip but not big size nominal classifier. ||| marit lobben ||| agata bochynska ||| 
2021 ||| curious objects: how visual complexity guides attention and engagement. ||| zekun sun ||| chaz firestone ||| 
2022 ||| effect of repeated exposure to the visual environment on young children's attention. ||| karrie e. godwin ||| audrey j. leroux ||| howard j. seltman ||| peter scupelli ||| anna v. fisher ||| 
2018 ||| dividing attention between tasks: testing whether explicit payoff functions elicit optimal dual-task performance. ||| george farmer ||| christian p. janssen ||| anh t. nguyen ||| duncan p. brumby ||| 
2020 ||| the role of attention in word recognition: results from ob1-reader. ||| martijn meeter ||| yousri marzouki ||| arthur ervin avramiea ||| joshua snell ||| jonathan grainger ||| 
2021 ||| how reliably do eye parameters indicate internal versus external attentional focus? ||| sonja annerer-walcher ||| simon m. ceh ||| felix putze ||| marvin kampen ||| christof k ||| rner ||| mathias benedek ||| 
2018 ||| functional equivalence of sleep loss and time on task effects in sustained attention. ||| bella z. veksler ||| glenn gunzelmann ||| 
2019 ||| the cognitive architecture of perceived animacy: intention, attention, and memory. ||| tao gao ||| chris l. baker ||| ning tang ||| haokui xu ||| joshua b. tenenbaum ||| 
2018 ||| linguistic and perceptual mapping in spatial representations: an attentional account. ||| berenice vald ||| s ||| jos |||  antonio hinojosa ||| francisco j. rom ||| n ||| ver ||| nica romero-ferreiro ||| 
2019 ||| degree of language experience modulates visual attention to visible speech and iconic gestures during clear and degraded speech comprehension. ||| linda drijvers ||| julija vaitonyte ||| asli  ||| zy ||| rek ||| 
2021 ||| attention does not affect the speed of subjective time, but whether temporal information guides performance: a large-scale study of intrinsically motivated timers in a real-time strategy game. ||| robbert van der mijn ||| hedderik van rijn ||| 
2017 ||| automatic mechanisms for social attention are culturally penetrable. ||| adam s. cohen ||| joni y. sasaki ||| tamsin c. german ||| heejung s. kim ||| 
2021 ||| diverse dance synthesis via keyframes with transformer controllers. ||| junjun pan ||| siyuan wang ||| junxuan bai ||| ju dai ||| 
2018 ||| visual attention for rendered 3d shapes. ||| guillaume lavou ||| fr ||| d ||| ric cordier ||| hyewon seo ||| mohamed-chaker larabi ||| 
2020 ||| scga-net: skip connections global attention network for image restoration. ||| dongdong ren ||| jinbao li ||| meng han ||| minglei shu ||| 
2017 ||| flicker observer effect: guiding attention through high frequency flicker in images. ||| nicholas waldin ||| manuela waldner ||| ivan viola ||| 
2018 ||| visual attention feature (vaf) : a novel strategy for visual tracking based on cloud platform in intelligent surveillance systems. ||| zheng pan ||| shuai liu ||| arun kumar sangaiah ||| khan muhammad ||| 
2020 ||| detection of ship targets in photoelectric images based on an improved recurrent attention convolutional neural network. ||| zhijing xu ||| yuhao huo ||| kun liu ||| sidong liu ||| 
2018 ||| attention-mechanism-based tracking method for intelligent internet of vehicles. ||| xu kang ||| bin song ||| jie guo ||| xiaojiang du ||| mohsen guizani ||| 
2020 ||| network attack detection and visual payload labeling technology based on seq2seq architecture with attention mechanism. ||| fan shi ||| pengcheng zhu ||| xiangyu zhou ||| bintao yuan ||| yong fang ||| 
2021 ||| fusion of attentional and traditional convolutional networks for facial expression recognition. ||| tin trung nguyen ||| thai hoang le ||| 
2017 ||| self-rehabilitation of acquired brain injury patients including neglect and attention deficit disorder with a tablet game in a clinical setting. ||| hendrik knoche ||| kasper hald ||| dorte richter ||| helle rovsing m ||| ller j ||| rgensen ||| 
2020 ||| location of ambulance bases for the attention of traffic accidents in mexico city. ||| zaida e. alarc ||| n-bernal ||| ricardo aceves-garc ||| a ||| jorge l. rojas-arce ||| 
2021 ||| evaluation of in-service power transformer health condition for inspection, repair, and replacement (irr) maintenance planning in electric utilities. ||| raji murugan ||| ramasamy raju ||| 
2017 ||| differential relay reliability enhancement using fourth harmonic for a large power transformer. ||| rachid bouderbala ||| hamid bentarzi ||| 
2020 ||| parallel voltage sag compensator without an injection transformer. ||| woo-young choi ||| min-kwon yang ||| yu-jin kim ||| 
2021 ||| transformers for future medicinal chemists. ||| jonas bostr ||| m ||| 
2021 ||| multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. ||| jike wang ||| chang-yu hsieh ||| mingyang wang ||| xiaorui wang ||| zhenxing wu ||| dejun jiang ||| benben liao ||| xujun zhang ||| bo yang ||| qiaojun he ||| dongsheng cao ||| xi chen ||| tingjun hou ||| 
2021 ||| global voxel transformer networks for augmented microscopy. ||| zhengyang wang ||| yaochen xie ||| shuiwang ji ||| 
2021 ||| modality attention and sampling enables deep learning with heterogeneous marker combinations in fluorescence microscopy. ||| alvaro gomariz ||| tiziano portenier ||| patrick m. helbling ||| stephan isringhausen ||| ute suessbier ||| c ||| sar nombela-arrieta ||| orcun goksel ||| 
2021 ||| mapping the space of chemical reactions using attention-based neural networks. ||| philippe schwaller ||| daniel probst ||| alain c. vaucher ||| vishnu h. nair ||| david kreutter ||| teodoro laino ||| jean-louis reymond ||| 
2021 ||| 'attention, attention, exploring minds acknowledge digital structure!' the shift to digital humanities has happened, so what should information scientists do in response? ||| volkmar paul engerer ||| 
2021 ||| attention based multi-agent intrusion detection systems using reinforcement learning. ||| kamalakanta sethi ||| venu madhav yatam ||| rahul kumar ||| padmalochan bera ||| 
2019 ||| attention with structure regularization for action recognition. ||| yuhui quan ||| yixin chen ||| ruotao xu ||| hui ji ||| 
2019 ||| triple attention network for sentimental visual question answering. ||| nelson ruwa ||| qirong mao ||| heping song ||| hongjie jia ||| ming dong ||| 
2019 ||| visual skeleton and reparative attention for part-of-speech image captioning system. ||| liang yang ||| haifeng hu ||| 
2021 ||| weakly supervised action segmentation with effective use of attention and self-attention. ||| yan bin ng ||| basura fernando ||| 
2021 ||| multimodal attention networks for low-level vision-and-language navigation. ||| federico landi ||| lorenzo baraldi ||| marcella cornia ||| massimiliano corsini ||| rita cucchiara ||| 
2019 ||| drau: dual recurrent attention units for visual question answering. ||| ahmed osman ||| wojciech samek ||| 
2019 ||| residual attention unit for action recognition. ||| zhongke liao ||| haifeng hu ||| junxuan zhang ||| chang yin ||| 
2020 ||| the synergy of double attention: combine sentence-level and word-level attention for image captioning. ||| haiyang wei ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2021 ||| skeleton-based action recognition via spatial and temporal transformer networks. ||| chiara plizzari ||| marco cannici ||| matteo matteucci ||| 
2020 ||| cascade multi-head attention networks for action recognition. ||| jiaze wang ||| xiaojiang peng ||| yu qiao ||| 
2017 ||| human attention in visual question answering: do humans and deep networks look at the same regions? ||| abhishek das ||| harsh agrawal ||| larry zitnick ||| devi parikh ||| dhruv batra ||| 
2020 ||| an attention recurrent model for human cooperation detection. ||| david freire-obreg ||| n ||| modesto castrill ||| n santana ||| paola barra ||| carmen bisogni ||| michele nappi ||| 
2020 ||| pyramid channel-based feature attention network for image dehazing. ||| xiaoqin zhang ||| tao wang ||| jinxin wang ||| guiying tang ||| li zhao ||| 
2021 ||| main: multi-attention instance network for video segmentation. ||| juan le ||| n alc ||| zar ||| mar ||| a alejandra bravo ||| guillaume jeanneret ||| ali k. thabet ||| thomas brox ||| pablo arbel ||| ez ||| bernard ghanem ||| 
2021 ||| multi-scale attention network for image inpainting. ||| jia qin ||| huihui bai ||| yao zhao ||| 
2020 ||| ghost removal via channel attention in exposure fusion. ||| qingsen yan ||| bo wang ||| peipei li ||| xianjun li ||| ao zhang ||| qinfeng shi ||| zheng you ||| yu zhu ||| jinqiu sun ||| yanning zhang ||| 
2020 ||| multi-attention fusion modeling for sentiment analysis of educational big data. ||| guanlin zhai ||| yan yang ||| heng wang ||| shengdong du ||| 
2022 ||| magan: unsupervised low-light image enhancement guided by mixed-attention. ||| renjun wang ||| bin jiang ||| chao yang ||| qiao li ||| bolin zhang ||| 
2020 ||| a semi-supervised attention model for identifying authentic sneakers. ||| yang yang ||| nengjun zhu ||| yi-feng wu ||| jian cao ||| de-chuan zhan ||| hui xiong ||| 
2021 ||| attention-aware heterogeneous graph neural network. ||| jintao zhang ||| quan xu ||| 
2018 ||| relation classification via recurrent neural network with attention and tensor layers. ||| runyan zhang ||| fanrong meng ||| yong zhou ||| bing liu ||| 
2020 ||| ambiguous representations of semilattices, imperfect information, and predicate transformers. ||| oleh r. nykyforchyn ||| oksana mykytsey ||| 
2018 ||| external bridging and internal bonding: unlocking the generative resources of member time and attention spent in online communities. ||| yongsuk kim ||| sirkka l. jarvenpaa ||| bin gu ||| 
2019 ||| managerial attention alteration in integrated product-service development. ||| siri jagstedt ||| 
2019 ||| human nature is not a machine: on liberty, attention engineering, and learning analytics. ||| sarah hartman-caverly ||| 
2020 ||| group password strength meter based on attention mechanism. ||| daojing he ||| beibei zhou ||| xiao yang ||| sammy chan ||| yao cheng ||| nadra guizani ||| 
2019 ||| deepfocus: deep encoding brainwaves and emotions with multi-scenario behavior analytics for human attention enhancement. ||| min chen ||| yong cao ||| rui wang ||| yong li ||| di wu ||| zhongchun liu ||| 
2020 ||| spotlight on ai! - why everyone should pay attention now! ||| g. reza djavanshir ||| maria r. lee ||| jim kyung-soo liew ||| 
2019 ||| a survey of attention deficit hyperactivity disorder identification using psychophysiological data. ||| senuri de silva ||| sanuwani dayarathna ||| gangani ariyarathne ||| dulani meedeniya ||| sampath jayarathna ||| 
2021 ||| mitigating sentimental bias via a polar attention mechanism. ||| tao yang ||| rujing yao ||| qing yin ||| qiang tian ||| ou wu ||| 
2019 ||| hical self-attention networks. ||| shang gao ||| john x. qiu ||| mohammed m. alawad ||| jacob d. hinkle ||| noah schaefferkoetter ||| hong-jun yoon ||| james blair christian ||| paul a. fearn ||| lynne penberthy ||| xiao-cheng wu ||| linda coyle ||| georgia d. tourassi ||| arvind ramanathan ||| 
2020 ||| deep learning with attention supervision for automated motion artefact detection in quality control of cardiac t1-mapping. ||| qiang zhang ||| evan hann ||| konrad werys ||| cody wu ||| iulia a. popescu ||| elena lukaschuk ||| ahmet barutcu ||| vanessa m. ferreira ||| stefan k. piechnik ||| 
2021 ||| fully-channel regional attention network for disease-location recognition with tongue images. ||| yang hu ||| guihua wen ||| mingnan luo ||| pei yang ||| dan dai ||| zhiwen yu ||| changjun wang ||| wendy hall ||| 
2021 ||| multiple instance convolutional neural network with modality-based attention and contextual multi-instance learning pooling layer for effective differentiation between borderline and malignant epithelial ovarian tumors. ||| junming jian ||| wei xia ||| rui zhang ||| xingyu zhao ||| jiayi zhang ||| xiaodong wu ||| yong'ai li ||| jinwei qiang ||| xin gao ||| 
2021 ||| health issue identification in social media based on multi-task hierarchical neural networks with topic attention. ||| deyu zhou ||| jiale yuan ||| jiasheng si ||| 
2022 ||| resattengan: simultaneous segmentation of multiple spinal structures on axial lumbar mri image using residual attention and adversarial learning. ||| hao gong ||| jianhua liu ||| bo chen ||| shuo li ||| 
2020 ||| fully convolutional attention network for biomedical image segmentation. ||| junlong cheng ||| shengwei tian ||| long yu ||| hongchun lu ||| xiaoyi lv ||| 
2021 ||| an attention-based weakly supervised framework for spitzoid melanocytic lesion diagnosis in whole slide images. ||| roc ||| o del amor ||| la ||| titia launet ||| adri ||| n colomer ||| ana ||| s moscard ||| andr ||| s mosquera-zamudio ||| carlos monteagudo ||| valery naranjo ||| 
2021 ||| tp-ddi: transformer-based pipeline for the extraction of drug-drug interactions. ||| dimitrios zaikis ||| ioannis p. vlahavas ||| 
2019 ||| recurrent neural networks with segment attention and entity description for relation extraction from clinical texts. ||| zhi li ||| jinshan yang ||| xu gou ||| xiaorong qi ||| 
2021 ||| liver segmentation in abdominal ct images via auto-context neural network and self-supervised contour attention. ||| minyoung chung ||| jingyu lee ||| sanguk park ||| chae-eun lee ||| jeongjin lee ||| yeong-gil shin ||| 
2022 ||| enhancing dynamic ecg heartbeat classification with lightweight transformer model. ||| lingxiao meng ||| wenjun tan ||| jiangang ma ||| ruofei wang ||| xiaoxia yin ||| yanchun zhang ||| 
2020 ||| ecg-based multi-class arrhythmia detection using spatio-temporal attention-based convolutional recurrent neural network. ||| jing zhang ||| aiping liu ||| min gao ||| xiang chen ||| xu zhang ||| xun chen ||| 
2019 ||| identifying user profile by incorporating self-attention mechanism based on csdn data set. ||| junru lu ||| le chen ||| kongming meng ||| fengyi wang ||| jun xiang ||| nuo chen ||| xu han ||| binyang li ||| 
2021 ||| bidirectional guided attention network for 3-d semantic detection of remote sensing images. ||| zhibo rao ||| mingyi he ||| zhidong zhu ||| yuchao dai ||| renjie he ||| 
2022 ||| lightweight spectral-spatial attention network for hyperspectral image classification. ||| ying cui ||| jinbiao xia ||| zhiteng wang ||| shan gao ||| liguo wang ||| 
2022 ||| a relation-augmented embedded graph attention network for remote sensing object detection. ||| shu tian ||| lihong kang ||| xiangwei xing ||| jing tian ||| chunzhuo fan ||| ye zhang ||| 
2021 ||| hyperspectral image denoising using a 3-d attention denoising network. ||| qian shi ||| xiaopei tang ||| taoru yang ||| rong liu ||| liangpei zhang ||| 
2022 ||| multiattention network for semantic segmentation of fine-resolution remote sensing images. ||| rui li ||| shunyi zheng ||| ce zhang ||| chenxi duan ||| jianlin su ||| libo wang ||| peter m. atkinson ||| 
2022 ||| cloud detection method using cnn based on cascaded feature attention and channel attention. ||| jing zhang ||| jun wu ||| hui wang ||| yuchen wang ||| yunsong li ||| 
2020 ||| hyperspectral image super-resolution by band attention through adversarial learning. ||| jiaojiao li ||| ruxing cui ||| bo li ||| rui song ||| yunsong li ||| yuchao dai ||| qian du ||| 
2022 ||| semantic segmentation with attention mechanism for remote sensing images. ||| qi zhao ||| jiahui liu ||| yuewen li ||| hong zhang ||| 
2019 ||| visual attention-driven hyperspectral image classification. ||| juan mario haut ||| mercedes eugenia paoletti ||| javier plaza ||| antonio plaza ||| jun li ||| 
2022 ||| feedback attention-based dense cnn for hyperspectral image classification. ||| chunyan yu ||| rui han ||| meiping song ||| caiyu liu ||| chein-i chang ||| 
2021 ||| aru-net: reduction of atmospheric phase screen in sar interferometry using attention-based deep residual u-net. ||| yuxing chen ||| lorenzo bruzzone ||| liming jiang ||| qishi sun ||| 
2019 ||| scene classification with recurrent attention of vhr remote sensing images. ||| qi wang ||| shaoteng liu ||| jocelyn chanussot ||| xuelong li ||| 
2022 ||| attention-based 3-d seismic fault segmentation training by a few 2-d slice labels. ||| yimin dou ||| kewen li ||| jianbing zhu ||| xiao li ||| yingjie xi ||| 
2022 ||| attention-aware dynamic self-aggregation network for satellite image time series classification. ||| wei zhang ||| peijun du ||| pingjie fu ||| peng zhang ||| pengfei tang ||| hongrui zheng ||| yaping meng ||| erzhu li ||| 
2022 ||| transformer-based multistage enhancement for remote sensing image super-resolution. ||| sen lei ||| zhenwei shi ||| wenjing mo ||| 
2022 ||| db-blendmask: decomposed attention and balanced blendmask for instance segmentation of high-resolution remote sensing images. ||| zhenqian chen ||| yongheng shang ||| andre python ||| yuxiang cai ||| jianwei yin ||| 
2022 ||| bdanet: multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images. ||| yu shen ||| sijie zhu ||| taojiannan yang ||| chen chen ||| delu pan ||| jianyu chen ||| liang xiao ||| qian du ||| 
2020 ||| scene-adaptive remote sensing image super-resolution using a multiscale attention network. ||| shu zhang ||| qiangqiang yuan ||| jie li ||| jing sun ||| xuguo zhang ||| 
2021 ||| attention-based adaptive spectral-spatial kernel resnet for hyperspectral image classification. ||| swalpa kumar roy ||| suvojit manna ||| tiecheng song ||| lorenzo bruzzone ||| 
2022 ||| solo-to-collaborative dual-attention network for one-shot object detection in remote sensing images. ||| lingjun li ||| xiwen yao ||| gong cheng ||| mingliang xu ||| jungong han ||| junwei han ||| 
2020 ||| learning to pay attention on spectral domain: a spectral attention module-based convolutional network for hyperspectral image classification. ||| lichao mou ||| xiao xiang zhu ||| 
2021 ||| hyperspectral image classification based on 3-d octave convolution with spatial-spectral attention network. ||| xu tang ||| fanbo meng ||| xiangrong zhang ||| yiu-ming cheung ||| jingjing ma ||| fang liu ||| licheng jiao ||| 
2022 ||| feature-grouped network with spectral-spatial connected attention for hyperspectral image classification. ||| wenhui guo ||| hailiang ye ||| feilong cao ||| 
2022 ||| multilayer global spectral-spatial attention network for wetland hyperspectral image classification. ||| zhuojun xie ||| jianwen hu ||| xudong kang ||| puhong duan ||| shutao li ||| 
2022 ||| multiframe video satellite image super-resolution via attention-based residual learning. ||| zhi he ||| jun li ||| lin liu ||| dan he ||| man xiao ||| 
2022 ||| mixed loss graph attention network for few-shot sar target classification. ||| minjia yang ||| xueru bai ||| li wang ||| feng zhou ||| 
2022 ||| rrnet: relational reasoning network with parallel multiscale attention for salient object detection in optical remote sensing images. ||| runmin cong ||| yumo zhang ||| leyuan fang ||| jun li ||| yao zhao ||| sam kwong ||| 
2022 ||| a multilevel encoder-decoder attention network for change detection in hyperspectral images. ||| jiahui qu ||| shaoxiong hou ||| wenqian dong ||| yunsong li ||| weiying xie ||| 
2021 ||| scene-driven multitask parallel attention network for building extraction in high-resolution remote sensing images. ||| haonan guo ||| qian shi ||| bo du ||| liangpei zhang ||| dongzhi wang ||| huaxiang ding ||| 
2022 ||| deep multiscale siamese network with parallel convolutional structure and self-attention for change detection. ||| qingle guo ||| junping zhang ||| shengyu zhu ||| chongxiao zhong ||| ye zhang ||| 
2022 ||| effective pan-sharpening with transformer and invertible neural network. ||| man zhou ||| xueyang fu ||| jie huang ||| feng zhao ||| aiping liu ||| rujing wang ||| 
2022 ||| attention and hybrid loss guided deep learning for consecutively missing seismic data reconstruction. ||| jiaxu yu ||| bangyu wu ||| 
2021 ||| collaborative attention-based heterogeneous gated fusion network for land cover classification. ||| xiao li ||| lin lei ||| yuli sun ||| ming li ||| guangyao kuang ||| 
2021 ||| hyperspectral target detection with roi feature transformation and multiscale spectral attention. ||| yanzi shi ||| jiaojiao li ||| yuxuan zheng ||| bobo xi ||| yunsong li ||| 
2022 ||| cross-attention spectral-spatial network for hyperspectral image classification. ||| kai yang ||| hao sun ||| chunbo zou ||| xiaoqiang lu ||| 
2021 ||| attention multibranch convolutional neural network for hyperspectral image classification based on adaptive region search. ||| jie feng ||| xiande wu ||| ronghua shang ||| chenhong sui ||| jie li ||| licheng jiao ||| xiangrong zhang ||| 
2021 ||| optical remote sensing image change detection based on attention mechanism and image difference. ||| xueli peng ||| ruofei zhong ||| zhen li ||| qingyang li ||| 
2022 ||| multiple attention-guided capsule networks for hyperspectral image classification. ||| mercedes eugenia paoletti ||| sergio moreno- ||| lvarez ||| juan mario haut ||| 
2019 ||| remote sensing image superresolution using deep residual channel attention. ||| juan mario haut ||| rub ||| n fern ||| ndez-beltran ||| mercedes eugenia paoletti ||| javier plaza ||| antonio plaza ||| 
2022 ||| a multiscale self-attention deep clustering for change detection in sar images. ||| huihui dong ||| wenping ma ||| licheng jiao ||| fang liu ||| lingling li ||| 
2021 ||| lanet: local attention embedding to improve the semantic segmentation of remote sensing images. ||| lei ding ||| hao tang ||| lorenzo bruzzone ||| 
2022 ||| end-to-end method with transformer for 3-d detection of oil tank from single sar image. ||| chao ma ||| yueting zhang ||| jiayi guo ||| yuxin hu ||| xiurui geng ||| fangfang li ||| bin lei ||| chibiao ding ||| 
2021 ||| ship detection in large-scale sar images via spatial shuffle-group enhance attention. ||| zongyong cui ||| xiaoya wang ||| nengyuan liu ||| zongjie cao ||| jianyu yang ||| 
2022 ||| h2an: hierarchical homogeneity-attention network for hyperspectral image classification. ||| sen li ||| xiaoyan luo ||| qixiong wang ||| lei li ||| jihao yin ||| 
2022 ||| distance weight-graph attention model-based high-resolution remote sensing urban functional zone identification. ||| kui zhang ||| dongping ming ||| shigao du ||| lu xu ||| xiao ling ||| beichen zeng ||| xianwei lv ||| 
2022 ||| self-attention deep image prior network for unsupervised 3-d seismic data enhancement. ||| omar m. saad ||| yapo abol |||  serge innocent obou ||| min bai ||| lotfy samy ||| liuqing yang ||| yangkang chen ||| 
2020 ||| hyperspectral pansharpening using deep prior and dual attention residual network. ||| yuxuan zheng ||| jiaojiao li ||| yunsong li ||| jie guo ||| xianyun wu ||| jocelyn chanussot ||| 
2022 ||| net: spectral-spatial-semantic network for hyperspectral image classification with the multiway attention mechanism. ||| zhongqiang zhang ||| danhua liu ||| dahua gao ||| guangming shi ||| 
2022 ||| remote sensing image change detection with transformers. ||| hao chen ||| zipeng qi ||| zhenwei shi ||| 
2022 ||| remote sensing image super-resolution via dual-resolution network based on connected attention mechanism. ||| xiangrong zhang ||| zhenyu li ||| tianyang zhang ||| fengsheng liu ||| xu tang ||| puhua chen ||| licheng jiao ||| 
2022 ||| clustering feature constraint multiscale attention network for shadow extraction from remote sensing images. ||| yakun xie ||| dejun feng ||| xingyu shen ||| yangge liu ||| jun zhu ||| tanveer hussain ||| sung wook baik ||| 
2022 ||| single-image super-resolution for remote sensing images using a deep generative adversarial network with local and global attention mechanisms. ||| yadong li ||| s ||| bastien mavromatis ||| feng zhang ||| zhenhong du ||| jean sequeira ||| zhongyi wang ||| xianwei zhao ||| renyi liu ||| 
2021 ||| a spectral grouping and attention-driven residual dense network for hyperspectral image super-resolution. ||| denghong liu ||| jie li ||| qiangqiang yuan ||| 
2021 ||| attention-based second-order pooling network for hyperspectral image classification. ||| zhaohui xue ||| mengxue zhang ||| yifeng liu ||| peijun du ||| 
2022 ||| attention-based multiscale residual adaptation network for cross-scene classification. ||| sihan zhu ||| bo du ||| liangpei zhang ||| xue li ||| 
2022 ||| exploring vision transformers for polarimetric sar image classification. ||| hongwei dong ||| lamei zhang ||| bin zou ||| 
2021 ||| hyperspectral image classification with attention-aided cnns. ||| renlong hang ||| zhu li ||| qingshan liu ||| pedram ghamisi ||| shuvra s. bhattacharyya ||| 
2022 ||| high-resolution remote sensing image captioning based on structured attention. ||| rui zhao ||| zhenwei shi ||| zhengxia zou ||| 
2022 ||| ground-based remote sensing cloud classification via context graph attention network. ||| shuang liu ||| linlin duan ||| zhong zhang ||| xiaozhong cao ||| tariq s. durrani ||| 
2022 ||| a spectral and spatial attention network for change detection in hyperspectral images. ||| maoguo gong ||| fenlong jiang ||| alex kai qin ||| tongfei liu ||| tao zhan ||| di lu ||| hanhong zheng ||| mingyang zhang ||| 
2020 ||| hsi-bert: hyperspectral image classification using the bidirectional encoder representation from transformers. ||| ji he ||| lina zhao ||| hongwei yang ||| mengmeng zhang ||| wei li ||| 
2020 ||| sound active attention framework for remote sensing image captioning. ||| xiaoqiang lu ||| binqiang wang ||| xiangtao zheng ||| 
2022 ||| mrddanet: a multiscale residual dense dual attention network for sar image denoising. ||| shuaiqi liu ||| yu lei ||| luyao zhang ||| bing li ||| weiming hu ||| yudong zhang ||| 
2022 ||| a deformable attention network for high-resolution remote sensing images semantic segmentation. ||| renxiang zuo ||| guangyun zhang ||| rongting zhang ||| xiuping jia ||| 
2017 ||| robust infrared maritime target detection based on visual attention and spatiotemporal filtering. ||| lili dong ||| bin wang ||| ming zhao ||| wenhai xu ||| 
2021 ||| residual spectral-spatial attention network for hyperspectral image classification. ||| minghao zhu ||| licheng jiao ||| fang liu ||| shuyuan yang ||| jianing wang ||| 
2021 ||| nas-guided lightweight multiscale attention fusion network for hyperspectral image classification. ||| jianing wang ||| runhu huang ||| siying guo ||| linhao li ||| minghao zhu ||| shuyuan yang ||| licheng jiao ||| 
2022 ||| multiple attention siamese network for high-resolution image change detection. ||| jiru huang ||| qian shen ||| min wang ||| mengyuan yang ||| 
2022 ||| radar hrrp target recognition model based on a stacked cnn-bi-rnn with attention mechanism. ||| mian pan ||| ailin liu ||| yanzhen yu ||| penghui wang ||| jianjun li ||| yan liu ||| shuaishuai lv ||| he zhu ||| 
2021 ||| recurrent thrifty attention network for remote sensing scene recognition. ||| liyong fu ||| dong zhang ||| qiaolin ye ||| 
2022 ||| spectral super-resolution of multispectral images using spatial-spectral residual attention network. ||| xiangtao zheng ||| wenjing chen ||| xiaoqiang lu ||| 
2021 ||| multiscale cnn with autoencoder regularization joint contextual attention network for sar image classification. ||| zitong wu ||| biao hou ||| licheng jiao ||| 
2022 ||| spectralformer: rethinking hyperspectral image classification with transformers. ||| danfeng hong ||| zhu han ||| jing yao ||| lianru gao ||| bing zhang ||| antonio plaza ||| jocelyn chanussot ||| 
2021 ||| attention symbiotic neural network for hyperspectral image refined classification based on relative water content retrieval. ||| xuejian liang ||| ye zhang ||| junping zhang ||| 
2022 ||| super-resolution-based change detection network with stacked attention module for images with different resolutions. ||| mengxi liu ||| qian shi ||| andrea marinoni ||| da he ||| xiaoping liu ||| liangpei zhang ||| 
2021 ||| attention-aware pseudo-3-d convolutional neural network for hyperspectral image classification. ||| jianzhe lin ||| lichao mou ||| xiao xiang zhu ||| xiangyang ji ||| z. jane wang ||| 
2022 ||| multilabel aerial image classification with a concept attention graph neural network. ||| dan lin ||| jianzhe lin ||| liang zhao ||| z. jane wang ||| zhikui chen ||| 
2022 ||| spectral-spatial transformer network for hyperspectral image classification: a factorized architecture search framework. ||| zilong zhong ||| ying li ||| lingfei ma ||| jonathan li ||| wei-shi zheng ||| 
2022 ||| a deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection. ||| qian shi ||| mengxi liu ||| shengchen li ||| xiaoping liu ||| fei wang ||| liangpei zhang ||| 
2021 ||| hybrid 2-d-3-d deep residual attentional network with structure tensor constraints for spectral super-resolution of rgb images. ||| jiaojiao li ||| chaoxiong wu ||| rui song ||| weiying xie ||| chiru ge ||| bo li ||| yunsong li ||| 
2022 ||| msacon: mining spatial attention-based contextual information for road extraction. ||| yingxiao xu ||| hao chen ||| chun du ||| jun li ||| 
2022 ||| composite sequential network with poa attention for polsar image analysis. ||| rui yang ||| xin xu ||| rong gui ||| zhaozhuo xu ||| fangling pu ||| 
2020 ||| attention gans: unsupervised deep feature learning for aerial scene classification. ||| yunlong yu ||| xianzhi li ||| fuxian liu ||| 
2019 ||| multisource region attention network for fine-grained object recognition in remote sensing imagery. ||| gencer sumbul ||| ramazan gokberk cinbis ||| selim aksoy ||| 
2022 ||| hybrid multiple attention network for semantic segmentation in aerial images. ||| ruigang niu ||| xian sun ||| yu tian ||| wenhui diao ||| kaiqiang chen ||| kun fu ||| 
2022 ||| ssa-siamnet: spectral-spatial-wise attention-based siamese network for hyperspectral image change detection. ||| lifeng wang ||| liguo wang ||| qunming wang ||| peter m. atkinson ||| 
2022 ||| ccanet: class-constraint coarse-to-fine attentional deep network for subdecimeter aerial image semantic segmentation. ||| guohui deng ||| zhaocong wu ||| chengjun wang ||| miaozhong xu ||| yanfei zhong ||| 
2022 ||| gated recurrent multiattention network for vhr remote sensing image classification. ||| boyang li ||| yulan guo ||| jun-gang yang ||| longguang wang ||| yingqian wang ||| wei an ||| 
2021 ||| addcnn: an attention-based deep dilated convolutional neural network for seismic facies analysis with interpretable spatial-spectral maps. ||| fangyu li ||| huailai zhou ||| zengyan wang ||| xinming wu ||| 
2022 ||| dynamic-hierarchical attention distillation with synergetic instance selection for land cover classification using missing heterogeneity images. ||| xiao li ||| lin lei ||| yuli sun ||| gangyao kuang ||| 
2021 ||| unsupervised pansharpening based on self-attention mechanism. ||| ying qu ||| razieh kaviani baghbaderani ||| hairong qi ||| chiman kwan ||| 
2020 ||| roi extraction based on multiview learning and attention mechanism for unbalanced remote sensing data set. ||| jie ma ||| libao zhang ||| yang sun ||| 
2022 ||| attentional feature refinement and alignment network for aircraft detection in sar imagery. ||| yan zhao ||| lingjun zhao ||| zhong liu ||| dewen hu ||| gangyao kuang ||| li liu ||| 
2021 ||| correction to "scene-driven multitask parallel attention network for building extraction in high-resolution remote sensing images". ||| haonan guo ||| qian shi ||| bo du ||| liangpei zhang ||| dongzhi wang ||| huaxiang ding ||| 
2022 ||| end-to-end recognition of similar space cone-cylinder targets based on complex-valued coordinate attention networks. ||| yuan-peng zhang ||| qun zhang ||| le kang ||| ying luo ||| lei zhang ||| 
2022 ||| cnn cloud detection algorithm based on channel and spatial attention and probabilistic upsampling for remote sensing image. ||| jing zhang ||| yuchen wang ||| hui wang ||| jun wu ||| yunsong li ||| 
2022 ||| an attention-based hypocenter estimator for earthquake localization. ||| tai-lin chin ||| kuan-yu chen ||| da-yi chen ||| te-hsiu wang ||| 
2019 ||| dense attention pyramid networks for multi-scale ship detection in sar images. ||| zongyong cui ||| qi li ||| zongjie cao ||| nengyuan liu ||| 
2022 ||| spectral partitioning residual network with spatial attention mechanism for hyperspectral image classification. ||| xiangrong zhang ||| shouwang shang ||| xu tang ||| jie feng ||| licheng jiao ||| 
2022 ||| global visual feature and linguistic state guided attention for remote sensing image captioning. ||| zhengyuan zhang ||| wenkai zhang ||| menglong yan ||| xin gao ||| kun fu ||| xian sun ||| 
2022 ||| multistage dual-attention guided fusion network for hyperspectral pansharpening. ||| peiyan guan ||| edmund y. lam ||| 
2020 ||| spectral-spatial attention network for hyperspectral image classification. ||| hao sun ||| xiangtao zheng ||| xiaoqiang lu ||| siyuan wu ||| 
2022 ||| adaptive hash attention and lower triangular network for hyperspectral image classification. ||| zixian ge ||| guo cao ||| youqiang zhang ||| xuesong li ||| hao shi ||| peng fu ||| 
2022 ||| vision transformer: an excellent teacher for guiding small networks in remote sensing image scene classification. ||| kejie xu ||| peifang deng ||| hong huang ||| 
2018 ||| radar and rain gauge merging-based precipitation estimation via geographical-temporal attention continuous conditional random field. ||| yongqiang tang ||| xuebing yang ||| wensheng zhang ||| guoping zhang ||| 
2021 ||| compact band weighting module based on attention-driven for hyperspectral image classification. ||| lin zhao ||| jiawen yi ||| xi li ||| wenjing hu ||| jianhui wu ||| guoyun zhang ||| 
2018 ||| visual attention-based target detection and discrimination for high-resolution sar images in complex scenes. ||| zhaocheng wang ||| lan du ||| peng zhang ||| lu li ||| fei wang ||| shu-wen xu ||| hongtao su ||| 
2022 ||| hyperspectral image classification using attention-based bidirectional long short-term memory network. ||| shaohui mei ||| xingang li ||| xiao liu ||| huimin cai ||| qian du ||| 
2022 ||| high-order markov random field as attention network for high-resolution remote-sensing image compression. ||| yanwen chong ||| liang zhai ||| shaoming pan ||| 
2022 ||| multi-direction networks with attentional spectral prior for hyperspectral image classification. ||| bobo xi ||| jiaojiao li ||| yunsong li ||| rui song ||| yuchao xiao ||| yanzi shi ||| qian du ||| 
2022 ||| hyperspectral image classification based on deep attention graph convolutional network. ||| jing bai ||| bixiu ding ||| zhu xiao ||| licheng jiao ||| hongyang chen ||| amelia c. regan ||| 
2022 ||| nlrnet: an efficient nonlocal attention resnet for pansharpening. ||| dajiang lei ||| hao chen ||| liping zhang ||| weisheng li ||| 
2022 ||| spectral-spatial self-attention networks for hyperspectral image classification. ||| xuming zhang ||| genyun sun ||| xiuping jia ||| lixin wu ||| aizhu zhang ||| jinchang ren ||| hang fu ||| yanjuan yao ||| 
2022 ||| water retrieval embedded attention network with multiscale receptive fields for hyperspectral image refined classification. ||| xuejian liang ||| ye zhang ||| junping zhang ||| 
2021 ||| attentional local contrast networks for infrared small target detection. ||| yimian dai ||| yiquan wu ||| fei zhou ||| kobus barnard ||| 
2022 ||| map-net: sar and optical image matching via image-based convolutional network with attention mechanism and spatial pyramid aggregated pooling. ||| song cui ||| ailong ma ||| liangpei zhang ||| miaozhong xu ||| yanfei zhong ||| 
2022 ||| center-boundary dual attention for oriented object detection in remote sensing images. ||| shuai liu ||| lu zhang ||| huchuan lu ||| you he ||| 
2021 ||| scattering enhanced attention pyramid network for aircraft detection in sar images. ||| qian guo ||| haipeng wang ||| feng xu ||| 
2022 ||| mutual attention inception network for remote sensing visual question answering. ||| xiangtao zheng ||| binqiang wang ||| xingqian du ||| xiaoqiang lu ||| 
2022 ||| transformer-based regression network for pansharpening remote sensing images. ||| xunyang su ||| jinjiang li ||| zhen hua ||| 
2022 ||| recurrent attention and semantic gate for remote sensing image captioning. ||| yunpeng li ||| xiangrong zhang ||| jing gu ||| chen li ||| xin wang ||| xu tang ||| licheng jiao ||| 
2021 ||| remote sensing image super-resolution via mixed high-order attention network. ||| dongyang zhang ||| jie shao ||| xinyao li ||| heng tao shen ||| 
2022 ||| spectral feature fusion networks with dual attention for hyperspectral image classification. ||| xian li ||| mingli ding ||| aleksandra pizurica ||| 
2022 ||| unsupervised domain adaptation for semantic segmentation of high-resolution remote sensing imagery driven by category-certainty attention. ||| jie chen ||| jingru zhu ||| ya guo ||| geng sun ||| yi zhang ||| min deng ||| 
2022 ||| attention-based multistage fusion network for remote sensing image pansharpening. ||| wanwan zhang ||| jinjiang li ||| zhen hua ||| 
2021 ||| attention and working memory. ||| aggeliki maria zavitsanou ||| athanasios drigas ||| 
2021 ||| computer models of saliency alone fail to predict subjective visual attention to landmarks during observed navigation. ||| demet yesiltepe ||| ayse ozbil torun ||| antoine coutrot ||| michael hornberger ||| hugo j. spiers ||| ruth conroy dalton ||| 
2019 ||| classifying suspicious content in tor darknet through semantic attention keypoint filtering. ||| eduardo fidalgo ||| enrique alegre ||| laura fern ||| ndez-robles ||| v ||| ctor gonz ||| lez-castro ||| 
2021 ||| neutron: an attention-based neural decompiler. ||| ruigang liang ||| ying cao ||| peiwei hu ||| kai chen ||| 
2020 ||| a dga domain names detection modeling method based on integrating an attention mechanism and deep neural network. ||| fangli ren ||| zhengwei jiang ||| xuren wang ||| jian liu ||| 
2022 ||| application of the crow search algorithm to the problem of the parametric estimation in transformers considering voltage and current measures. ||| david gilberto gracia-vel ||| squez ||| andr ||| s steven morales-rodr ||| guez ||| oscar danilo montoya ||| 
2021 ||| black-hole optimization applied to the parametric estimation in distribution transformers considering voltage and current measures. ||| camilo andres arenas-acu ||| a ||| jonathan andres rodriguez-contreras ||| oscar danilo montoya ||| edwin rivas trujillo ||| 
2019 ||| the application of ant colony algorithms to improving the operation of traction rectifier transformers. ||| barbara kulesz ||| andrzej sikora ||| adam zielonka ||| 
2021 ||| fine-grained cross-modal retrieval for cultural items with focal attention and hierarchical encodings. ||| shurong sheng ||| katrien laenen ||| luc van gool ||| marie-francine moens ||| 
2021 ||| crowd counting with segmentation attention convolutional neural network. ||| jiwei chen ||| zengfu wang ||| 
2021 ||| towards accurate coronary artery calcium segmentation with multi-scale attention mechanism. ||| yang ning ||| yunfeng zhang ||| xuemei li ||| caiming zhang ||| 
2020 ||| multi-head mutual-attention cyclegan for unpaired image-to-image translation. ||| wei ji ||| jing guo ||| yun li ||| 
2019 ||| spatial non-local attention for thoracic disease diagnosis and visualisation in weakly supervised learning. ||| menglin yang ||| ding li ||| wensheng zhang ||| 
2021 ||| hard exudate segmentation in retinal image with attention mechanism. ||| ze si ||| dongmei fu ||| yang liu ||| zhicheng huang ||| 
2021 ||| ambcr: low-light image enhancement via attention guided multi-branch construction and retinex theory. ||| miao li ||| dongming zhou ||| rencan nie ||| shidong xie ||| yanyu liu ||| 
2022 ||| esa-cyclegan: edge feature and self-attention based cycle-consistent generative adversarial network for style transfer. ||| li wang ||| lidan wang ||| shubai chen ||| 
2020 ||| pavement crack detection network based on pyramid structure and attention mechanism. ||| xuezhi xiang ||| yuqi zhang ||| abdulmotaleb el-saddik ||| 
2022 ||| deep coordinate attention network for single image super-resolution. ||| chao xie ||| hongyu zhu ||| yeqi fei ||| 
2017 ||| ir small target detection based on human visual attention using pulsed discrete cosine transform. ||| mahdi nasiri ||| mohammad reza mosavi ||| sattar mirzakuchaki ||| 
2020 ||| dual attention convolutional network for action recognition. ||| xiaoqiang li ||| miao xie ||| yin zhang ||| guangtai ding ||| weiqin tong ||| 
2021 ||| a multi-class covid-19 segmentation network with pyramid attention and edge loss in ct images. ||| fuli yu ||| yu zhu ||| xiangxiang qin ||| ying xin ||| dawei yang ||| tao xu ||| 
2021 ||| alzheimer's disease diagnosis based on the visual attention model and equal-distance ring shape context features. ||| huan lao ||| xuejun zhang ||| yanyan tang ||| chan liang ||| 
2021 ||| part-level attention networks for cross-domain person re-identification. ||| qun zhao ||| nisuo du ||| zhi ouyang ||| ning kang ||| ziyan liu ||| xu wang ||| qing he ||| yiling xu ||| shichun ge ||| jingkuan song ||| 
2020 ||| object counting method based on dual attention network. ||| shihui zhang ||| he li ||| weihang kong ||| 
2021 ||| uda-net: densely attention network for underwater image enhancement. ||| yang li ||| rong chen ||| 
2021 ||| multi-dimensional weighted cross-attention network in crowded scenes. ||| yefan xie ||| jiangbin zheng ||| xuan hou ||| irfan raza naqvi ||| yue xi ||| nailiang kuang ||| 
2021 ||| efficient recurrent attention network for remote sensing scene classification. ||| le liang ||| guoli wang ||| 
2022 ||| learning spatial self-attention information for visual tracking. ||| shengwu li ||| xuande zhang ||| jing xiong ||| chenjing ning ||| mingke zhang ||| 
2020 ||| robust image hashing with visual attention model and invariant moments. ||| zhenjun tang ||| hanyun zhang ||| chi-man pun ||| mengzhu yu ||| chunqiang yu ||| xianquan zhang ||| 
2022 ||| ensemble cross-stage partial attention network for image classification. ||| hai lin ||| junjie yang ||| 
2021 ||| ca-pmg: channel attention and progressive multi-granularity training network for fine-grained visual classification. ||| peipei zhao ||| qiguang miao ||| hang yao ||| xiangzeng liu ||| ruyi liu ||| maoguo gong ||| 
2021 ||| a discriminative self-attention cycle gan for face super-resolution and recognition. ||| xiaoguang li ||| ning dong ||| jianglu huang ||| li zhuo ||| jiafeng li ||| 
2020 ||| multi-focus image fusion with siamese self-attention network. ||| xiaopeng guo ||| lingyu meng ||| liye mei ||| yueyun weng ||| hengqing tong ||| 
2021 ||| leveraging attention-based visual clue extraction for image classification. ||| yunbo cui ||| youtian du ||| xue wang ||| hang wang ||| chang su ||| 
2021 ||| a keypoint-based object detection method with wide dual-path backbone network and attention modules. ||| zhong qu ||| run zhang ||| kang-hua bao ||| 
2021 ||| multiple object tracking based on multi-task learning with strip attention. ||| yaoye song ||| peng zhang ||| wei huang ||| yufei zha ||| tao you ||| yanning zhang ||| 
2021 ||| salient target detection in hyperspectral image based on visual attention. ||| minghua zhao ||| liqin yue ||| jing hu ||| shuangli du ||| peng li ||| li wang ||| 
2021 ||| attention-based video object segmentation algorithm. ||| ying cao ||| lijuan sun ||| chong han ||| jian guo ||| 
2021 ||| bilateral attention network for semantic segmentation. ||| dongli wang ||| nanjun li ||| yan zhou ||| jinzhen mu ||| 
2022 ||| augmented global attention network for image super-resolution. ||| xiaobiao du ||| sai biao jiang ||| jie liu ||| 
2022 ||| image quality enhancement using hybrid attention networks. ||| jiachen wang ||| yingyun yang ||| yan hua ||| 
2020 ||| flow driven attention network for video salient object detection. ||| feng zhou ||| hui shuai ||| qingshan liu ||| guodong guo ||| 
2021 ||| paralleled attention modules and adaptive focal loss for siamese visual tracking. ||| yuyao zhao ||| min jiang ||| jun kong ||| sha li ||| 
2020 ||| mpa-net: multi-path attention stereo matching network. ||| haiwei sang ||| zu liu yang ||| xiaowei yang ||| yong zhao ||| 
2021 ||| detail texture detection based on yolov4-tiny combined with attention mechanism and bicubic interpolation. ||| tian hui ||| yuelei xu ||| rasol jarhinbek ||| 
2021 ||| visual-attention gan for interior sketch colourisation. ||| xinrong li ||| hong li ||| chiyu wang ||| xun hu ||| wei zhang ||| 
2020 ||| image dehazing with uneven illumination prior by dense residual channel attention network. ||| shibai yin ||| jin xin ||| yibin wang ||| anup basu ||| 
2021 ||| a dual-attention v-network for pulmonary lobe segmentation in ct scans. ||| shaohua zheng ||| weiyu nie ||| lin pan ||| bin zheng ||| zhiqiang shen ||| liqin huang ||| chenhao pei ||| yuhang she ||| liuqing chen ||| 
2022 ||| mam: a multipath attention mechanism for image recognition. ||| hao zhang ||| guoqin peng ||| zhichao wu ||| jian gong ||| dan xu ||| hongzhen shi ||| 
2020 ||| e2-capsule neural networks for facial expression recognition using au-aware attention. ||| shan cao ||| yuqian yao ||| gaoyun an ||| 
2022 ||| underwater image enhancement via lbp-based attention residual network. ||| zhixiong huang ||| jinjiang li ||| zhen hua ||| 
2021 ||| look into my "virtual" eyes: what dynamic virtual agents add to the realistic study of joint attention. ||| samantha e. a. gregory ||| cl ||| ona kelly ||| klaus kessler ||| 
2021 ||| computer- assessment of attention and memory utilizing ecologically valid distractions: a scoping review. ||| deanna pinnow ||| h. isabel hubbard ||| peter a. meulenbroek ||| 
2022 ||| measuring attentional distraction in children with adhd using virtual reality technology with eye-tracking. ||| jared d. stokes ||| albert rizzo ||| joy j. geng ||| julie b. schweitzer ||| 
2022 ||| automatic extraction of orchards from remote sensing image based on category attention mechanism. ||| hongyan liu ||| shun ren ||| dong ren ||| xuan liu ||| 
2022 ||| cultivated land segmentation of remote sensing image based on pspnet of attention mechanism. ||| shun ren ||| xuan liu ||| hongyan liu ||| lu wang ||| 
2017 ||| a video summarization approach based on the emulation of bottom-up mechanisms of visual attention. ||| hugo jacob ||| fl ||| vio l. c. p ||| dua ||| an ||| sio lacerda ||| adriano c. m. pereira ||| 
2021 ||| leveraging contextual embeddings and self-attention neural networks with bi-attention for sentiment analysis. ||| magdalena biesialska ||| katarzyna biesialska ||| henryk rybinski ||| 
2021 ||| caesar: context-aware explanation based on supervised attention for service recommendations. ||| lei li ||| li chen ||| ruihai dong ||| 
2021 ||| cgspn : cascading gated self-attention and phrase-attention network for sentence modeling. ||| yanping fu ||| yun liu ||| 
2017 ||| there is no agency without attention. ||| paul bello ||| will bridewell ||| 
2021 ||| social explorative attention based recommendation for content distribution platforms. ||| wenyi xiao ||| huan zhao ||| haojie pan ||| yangqiu song ||| vincent w. zheng ||| qiang yang ||| 
2021 ||| attention based adversarially regularized learning for network embedding. ||| jieyue he ||| jinmeng wang ||| zhizhou yu ||| 
2018 ||| eeg indices correlate with sustained attention performance in patients affected by diffuse axonal injury. ||| stefania coelli ||| riccardo barbieri ||| gianluigi reni ||| claudio zucca ||| anna maria bianchi ||| 
2022 ||| identifying risk factors of intracerebral hemorrhage stability using explainable attention model. ||| seshasayi rangaraj ||| mobarakol islam ||| vibashan vs ||| navodini wijethilake ||| utkarsh uppal ||| angela an qi see ||| jasmine chan ||| michael lucas james ||| nicolas kon kam king ||| hongliang ren ||| 
2019 ||| evaluation of divided attention using different stimulation models in event-related potentials. ||| turgay batbat ||| ayseg ||| l g ||| ven ||| nazan dolu ||| 
2021 ||| tumor type detection in brain mr images of the deep model developed using hypercolumn technique, attention modules, and residual blocks. ||| mesut toga ||| ar ||| burhan ergen ||| zafer c ||| mert ||| 
2021 ||| attention monitoring for synchronous distance learning. ||| andrea f. abate ||| lucia cascone ||| michele nappi ||| fabio narducci ||| ignazio passero ||| 
2020 ||| transformer fault diagnosis method using iot based monitoring system and ensemble machine learning. ||| chaolong zhang ||| yigang he ||| bolun du ||| lifen yuan ||| bing li ||| shanhe jiang ||| 
2020 ||| attention-based sentiment analysis using convolutional and recurrent neural network. ||| mohd usama ||| belal ahmad ||| enmin song ||| m. shamim hossain ||| mubarak alrashoud ||| ghulam muhammad ||| 
2020 ||| ransomware classification using patch-based cnn and self-attention network on embedded n-grams of opcodes. ||| bin zhang ||| wentao xiao ||| xi xiao ||| arun kumar sangaiah ||| weizhe zhang ||| jiajia zhang ||| 
2021 ||| automatic segmentation of bioabsorbable vascular stents in intravascular optical coherence images using weakly supervised attention network. ||| chenxi huang ||| guokai zhang ||| yiwen lu ||| yisha lan ||| sirui chen ||| siyuan guo ||| 
2020 ||| transformer based deep intelligent contextual embedding for twitter sentiment analysis. ||| usman naseem ||| imran razzak ||| katarzyna musial ||| muhammad imran ||| 
2018 ||| iot-based students interaction framework using attention-scoring assessment in elearning. ||| muhammad farhan ||| sohail jabbar ||| muhammad aslam ||| mohammad hammoudeh ||| mudassar ahmad ||| shehzad khalid ||| murad khan ||| kijun han ||| 
2019 ||| a general ai-defined attention network for predicting cdn performance. ||| junnan li ||| zhihui lu ||| yu tong ||| jie wu ||| shalin huang ||| meikang qiu ||| wei du ||| 
2021 ||| scalable multi-channel dilated cnn-bilstm model with attention mechanism for chinese textual sentiment analysis. ||| chenquan gan ||| qingdong feng ||| zufan zhang ||| 
2021 ||| multi-channel, convolutional attention based neural model for automated diagnostic coding of unstructured patient discharge summaries. ||| veena mayya ||| sowmya kamath s. ||| gokul s. krishnan ||| tushaar gangavarapu ||| 
2021 ||| human action recognition using attention based lstm network with dilated cnn features. ||| khan muhammad ||| mustaqeem ||| amin ullah ||| ali shariq imran ||| muhammad sajjad ||| mustafa servet kiran ||| giovanna sannino ||| victor hugo c. de albuquerque ||| 
2021 ||| abcdm: an attention-based bidirectional cnn-rnn deep model for sentiment analysis. ||| mohammad ehsan basiri ||| shahla nemati ||| moloud abdar ||| erik cambria ||| u. rajendra acharya ||| 
2020 ||| multi-entity sentiment analysis using self-attention based hierarchical dilated convolutional neural network. ||| chenquan gan ||| lu wang ||| zufan zhang ||| 
2022 ||| reliable customer analysis using federated learning and exploring deep-attention edge intelligence. ||| usman ahmed ||| gautam srivastava ||| jerry chun-wei lin ||| 
2021 ||| graph-cat: graph co-attention networks via local and global attribute augmentations. ||| liang yang ||| weixun li ||| yuanfang guo ||| junhua gu ||| 
2022 ||| acmf: an attention collaborative extended matrix factorization based model for mooc course service via a heterogeneous view. ||| deming sheng ||| jingling yuan ||| qing xie ||| lin li ||| 
2022 ||| eandc: an explainable attention network based deep adaptive clustering model for mental health treatment. ||| usman ahmed ||| gautam srivastava ||| unil yun ||| jerry chun-wei lin ||| 
2020 ||| simultaneous left atrium anatomy and scar segmentations via deep learning in multiview information with attention. ||| guang yang ||| jun chen ||| zhifan gao ||| shuo li ||| hao ni ||| elsa d. angelini ||| tom wong ||| raad mohiaddin ||| eva nyktari ||| ricardo wage ||| lei xu ||| yanping zhang ||| xiuquan du ||| heye zhang ||| david n. firmin ||| jennifer keegan ||| 
2022 ||| patch attention network with generative adversarial model for semi-supervised binocular disparity prediction. ||| zhibo rao ||| mingyi he ||| yuchao dai ||| zhelun shen ||| 
2018 ||| visual attention prediction for images with leading line structure. ||| issei mochizuki ||| masahiro toyoura ||| xiaoyang mao ||| 
2020 ||| fine-grained action recognition using multi-view attentions. ||| yisheng zhu ||| guangcan liu ||| 
2021 ||| (sarn)spatial-wise attention residual network for image super-resolution. ||| wenling shi ||| huiqian du ||| wenbo mei ||| zhifeng ma ||| 
2020 ||| 3d rans: 3d residual attention networks for action recognition. ||| jiahui cai ||| jianguo hu ||| 
2020 ||| sta-gcn: two-stream graph convolutional network with spatial-temporal attention for hand gesture recognition. ||| wei zhang ||| zeyi lin ||| jian cheng ||| cuixia ma ||| xiaoming deng ||| hongan wang ||| 
2022 ||| attention to fine-grained information: hierarchical multi-scale network for retinal vessel segmentation. ||| chengzhi lyu ||| guoqing hu ||| dan wang ||| 
2022 ||| contour-aware semantic segmentation network with spatial attention mechanism for medical image. ||| zhiming cheng ||| aiping qu ||| xiaofeng he ||| 
2020 ||| feature-attention module for context-aware image-to-image translation. ||| jing bai ||| ran chen ||| min liu ||| 
2021 ||| multi-level progressive parallel attention guided salient object detection for rgb-d images. ||| zhengyi liu ||| quntao duan ||| song shi ||| peng zhao ||| 
2020 ||| traffic transformer: capturing the continuity and periodicity of time series for traffic forecasting. ||| ling cai ||| krzysztof janowicz ||| gengchen mai ||| bo yan ||| rui zhu ||| 
2021 ||| do children with reading difficulties benefit from instructional game supports? exploring children's attention and understanding of feedback. ||| asimina vasalou ||| laura benton ||| seray b. ibrahim ||| emma sumner ||| nelly joye ||| elisabeth herbert ||| 
2017 ||| assessing the attention levels of students by using a novel attention aware system based on brainwave signals. ||| chih-ming chen ||| jung-ying wang ||| chih-ming yu ||| 
2021 ||| design of online monitoring system for distribution transformer based on cloud side end collaboration of internet of things. ||| guoqiang zu ||| wei si ||| ying yao ||| huifang liu ||| haishen liang ||| dalong ji ||| 
2022 ||| an sar target detector based on gradient harmonized mechanism and attention mechanism. ||| yuang du ||| lan du ||| lu li ||| 
2022 ||| remote sensing image generation based on attention mechanism and vae-msgan for roi extraction. ||| libao zhang ||| yanan liu ||| 
2022 ||| classfication of hyperspectral image with attention mechanism-based dual-path convolutional network. ||| chunyu pu ||| hong huang ||| liuyang luo ||| 
2022 ||| hybrid attention networks for flow and pressure forecasting in water distribution systems. ||| ziqing ma ||| shuming liu ||| guancheng guo ||| xipeng yu ||| 
2020 ||| synthetic aperture radar scene classification using multiview cross correlation attention network. ||| kang ni ||| yiquan wu ||| peng wang ||| 
2022 ||| multiscale building extraction with refined attention pyramid networks. ||| qinglin tian ||| yingjun zhao ||| yao li ||| jun chen ||| xuejiao chen ||| kai qin ||| 
2022 ||| erratum to "convolutional neural network with attention mechanism for sar automatic target recognition". ||| ming zhang ||| jubai an ||| dahua yu ||| li dong yang ||| liang wu ||| xiao qi lu ||| 
2022 ||| multidimensional attention learning for vhr remote sensing imagery recognition. ||| jie fang ||| xiaoqian cao ||| pengfei han ||| dianwei wang ||| 
2021 ||| pyramid attention dilated network for aircraft detection in sar images. ||| yan zhao ||| lingjun zhao ||| chuyin li ||| gangyao kuang ||| 
2022 ||| graph sample and aggregate-attention network for hyperspectral image classification. ||| yao ding ||| xiaofeng zhao ||| zhili zhang ||| wei cai ||| nengjun yang ||| 
2021 ||| attention-based convolutional neural network for earthquake event classification. ||| bonhwa ku ||| gwantae kim ||| jae-kwang ahn ||| jimin lee ||| hanseok ko ||| 
2022 ||| attention-driven graph convolution network for remote sensing image retrieval. ||| ushasi chaudhuri ||| biplab banerjee ||| avik bhattacharya ||| mihai datcu ||| 
2021 ||| joint spatial-spectral attention network for hyperspectral image classification. ||| lei li ||| jihao yin ||| xiuping jia ||| sen li ||| bingnan han ||| 
2022 ||| separable attention network in single- and mixed-precision floating point for land-cover classification of remote sensing images. ||| mercedes paoletti ||| juan mario haut ||| tayeb alipourfard ||| swalpa kumar roy ||| eligius m. t. hendrix ||| antonio plaza ||| 
2022 ||| markcapsnet: road marking extraction from aerial images using self-attention-guided capsule network. ||| yongtao yu ||| yinyin li ||| chao liu ||| jun wang ||| changhui yu ||| xiaoling jiang ||| lanfang wang ||| zuojun liu ||| yongjun zhang ||| 
2022 ||| context residual attention network for remote sensing scene classification. ||| yuhua wang ||| yaxin hu ||| yuezhu xu ||| peiyuan jiao ||| xiangrong zhang ||| huanyu cui ||| 
2022 ||| maenet: multiple attention encoder-decoder network for farmland segmentation of remote sensing images. ||| hai huan ||| yuan liu ||| yaqin xie ||| chao wang ||| dongdong xu ||| yi zhang ||| 
2022 ||| point transformer for shape classification and retrieval of urban roof point clouds. ||| dimple a. shajahan ||| mukund varma t ||| ramanathan muthuganapathy ||| 
2022 ||| the application of semisupervised attentional generative adversarial networks in desert seismic data denoising. ||| yue li ||| xinming luo ||| ning wu ||| xintong dong ||| 
2021 ||| multibranch spatial-channel attention for semantic labeling of very high-resolution remote sensing images. ||| bingnan han ||| jihao yin ||| xiaoyan luo ||| xiuping jia ||| 
2020 ||| an adaptive scale sea surface temperature predicting method based on deep learning with attention mechanism. ||| jiang xie ||| jiyuan zhang ||| jie yu ||| lingyu xu ||| 
2022 ||| dual-triple attention network for hyperspectral image classification using limited training samples. ||| ying cui ||| zikun yu ||| jiacheng han ||| shan gao ||| liguo wang ||| 
2022 ||| terrain segmentation in polarimetric sar images using dual-attention fusion network. ||| daifeng xiao ||| zhirui wang ||| youming wu ||| xin gao ||| xian sun ||| 
2022 ||| dbranet: road extraction by dual-branch encoder and regional attention decoder. ||| si-bao chen ||| yu-xin ji ||| jin tang ||| bin luo ||| wei-qiang wang ||| ke lv ||| 
2022 ||| ssa-net: spatial scale attention network for image-based geo-localization. ||| xiuwei zhang ||| xiangchuang meng ||| hanlin yin ||| yixin wang ||| yuanzeng yue ||| yinghui xing ||| yanning zhang ||| 
2022 ||| a mutual guidance attention-based multi-level fusion network for hyperspectral and lidar classification. ||| tongzhen zhang ||| song xiao ||| wenqian dong ||| jiahui qu ||| yufei yang ||| 
2022 ||| context-aware attentional graph u-net for hyperspectral image classification. ||| moule lin ||| weipeng jing ||| donglin di ||| guangsheng chen ||| houbing song ||| 
2022 ||| a global-local spectral weight network based on attention for hyperspectral band selection. ||| hongqi zhang ||| xudong sun ||| yuan zhu ||| fengqiang xu ||| xianping fu ||| 
2022 ||| hyperspectral image classification with multiattention fusion network. ||| zhaokui li ||| xiaodan zhao ||| yimin xu ||| wei li ||| lin zhai ||| zhuoqun fang ||| xiangbin shi ||| 
2022 ||| spectral-spatial residual graph attention network for hyperspectral image classification. ||| kejie xu ||| yue zhao ||| lingming zhang ||| chenqiang gao ||| hong huang ||| 
2022 ||| csafnet: channel similarity attention fusion network for multispectral pansharpening. ||| shuyue luo ||| shangbo zhou ||| ying qi ||| 
2021 ||| remote sensing image scene classification based on an enhanced attention module. ||| zhicheng zhao ||| jiaqi li ||| ze luo ||| jian li ||| can chen ||| 
2022 ||| improving deep learning-based cloud detection for satellite images with attention mechanism. ||| li zhang ||| jiahui sun ||| xubing yang ||| rui jiang ||| qiaolin ye ||| 
2020 ||| apdc-net: attention pooling-based convolutional network for aerial scene classification. ||| qi bi ||| kun qin ||| han zhang ||| jiafen xie ||| zhili li ||| kai xu ||| 
2022 ||| radar-based human activity recognition with 1-d dense attention network. ||| guoji lai ||| xin lou ||| wenbin ye ||| 
2022 ||| semantic segmentation of remote sensing image based on regional self-attention mechanism. ||| danpei zhao ||| chenxu wang ||| yue gao ||| zhenwei shi ||| fengying xie ||| 
2022 ||| contrasting yolov5, transformer, and efficientdet detectors for crop circle detection in desert. ||| mohamed lamine mekhalfi ||| carlo nicol ||| yakoub bazi ||| mohamad mahmoud al rahhal ||| norah a. alsharif ||| eslam al maghayreh ||| 
2022 ||| semantic segmentation of remote-sensing images based on multiscale feature fusion and attention refinement. ||| xin he ||| yong zhou ||| jiaqi zhao ||| man zhang ||| rui yao ||| bing liu ||| haichao li ||| 
2022 ||| remote-sensing image captioning based on multilayer aggregated transformer. ||| chenyang liu ||| rui zhao ||| zhenwei shi ||| 
2022 ||| development of a dual-attention u-net model for sea ice and open water classification on sar images. ||| yibin ren ||| xiaofeng li ||| xiaofeng yang ||| huan xu ||| 
2022 ||| attention-based polarimetric feature selection convolutional network for polsar image classification. ||| hongwei dong ||| lamei zhang ||| da lu ||| bin zou ||| 
2022 ||| consecutively missing seismic data interpolation based on coordinate attention unet. ||| xinze li ||| bangyu wu ||| xu zhu ||| hui yang ||| 
2022 ||| global context parallel attention for anchor-free instance segmentation in remote sensing images. ||| xinyu liu ||| xiaoguang di ||| 
2022 ||| land cover classification of multispectral lidar data with an efficient self-attention capsule network. ||| yongtao yu ||| chao liu ||| haiyan guan ||| lanfang wang ||| shangbing gao ||| haiyan zhang ||| yahong zhang ||| jonathan li ||| 
2021 ||| self-attention-based deep feature fusion for remote sensing scene classification. ||| ran cao ||| leyuan fang ||| ting lu ||| nanjun he ||| 
2021 ||| scattnet: semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images. ||| haifeng li ||| kaijian qiu ||| li chen ||| xiaoming mei ||| liang hong ||| chao tao ||| 
2022 ||| sea surface height prediction with deep learning based on attention mechanism. ||| jingjing liu ||| baogang jin ||| lei wang ||| lingyu xu ||| 
2021 ||| semisupervised classification for hyperspectral images using graph attention networks. ||| anshu sha ||| bin wang ||| xiaofeng wu ||| liming zhang ||| 
2022 ||| adaptive cross-attention-driven spatial-spectral graph convolutional network for hyperspectral image classification. ||| jin-yu yang ||| heng-chao li ||| wen-shuai hu ||| lei pan ||| qian du ||| 
2022 ||| end-to-end multilevel hybrid attention framework for hyperspectral image classification. ||| jianhong xiang ||| chen wei ||| minhui wang ||| long teng ||| 
2022 ||| synthetic data augmentation using multiscale attention cyclegan for aircraft detection in remote sensing images. ||| weixing liu ||| bin luo ||| jun liu ||| 
2022 ||| ground-based cloud detection using multiscale attention convolutional neural network. ||| zhong zhang ||| shuzhen yang ||| shuang liu ||| baihua xiao ||| xiaozhong cao ||| 
2021 ||| darecnet-bs: unsupervised dual-attention reconstruction network for hyperspectral band selection. ||| swalpa kumar roy ||| sayantan das ||| tiecheng song ||| bhabatosh chanda ||| 
2020 ||| combining multilevel features for remote sensing image scene classification with attention model. ||| jinsheng ji ||| tao zhang ||| linfeng jiang ||| weilin zhong ||| huilin xiong ||| 
2022 ||| local fusion attention network for semantic segmentation of building facade point clouds. ||| yanfei su ||| weiquan liu ||| ming cheng ||| zhimin yuan ||| cheng wang ||| 
2022 ||| a shadow detection algorithm based on multiscale spatial attention mechanism for aerial remote sensing images. ||| dongyang liu ||| junping zhang ||| yinhu wu ||| ye zhang ||| 
2022 ||| a spatiotemporal attention model for severe precipitation estimation. ||| cong wang ||| ping wang ||| pingping wang ||| bing xue ||| di wang ||| 
2022 ||| u-shaped attention connection network for remote-sensing image super-resolution. ||| wenzong jiang ||| lifei zhao ||| yanjiang wang ||| weifeng liu ||| bao-di liu ||| 
2020 ||| learning to match ground camera image and uav 3-d model-rendered image based on siamese network with attention mechanism. ||| weiquan liu ||| cheng wang ||| xuesheng bian ||| shuting chen ||| shangshu yu ||| xiuhong lin ||| shang-hong lai ||| dongdong weng ||| jonathan li ||| 
2022 ||| self-supervised convolutional neural network via spectral attention module for hyperspectral image classification. ||| hong huang ||| liuyang luo ||| chunyu pu ||| 
2022 ||| improving remote sensing image captioning by combining grid features and transformer. ||| shuo zhuang ||| ping wang ||| gang wang ||| di wang ||| jinyong chen ||| feng gao ||| 
2022 ||| remote sensing image classification based on a cross-attention mechanism and graph convolution. ||| weiwei cai ||| zhanguo wei ||| 
2022 ||| block multi-dimensional attention for road segmentation in remote sensing imagery. ||| sijun dong ||| zhengchao chen ||| 
2022 ||| ship segmentation via encoder-decoder network with global attention in high-resolution sar images. ||| jichao li ||| shuiping gou ||| ruimin li ||| jiawei chen ||| xiaolong sun ||| 
2021 ||| region-merging method with texture pattern attention for sar image segmentation. ||| shuchen fan ||| yuhe sun ||| penglang shui ||| 
2020 ||| multi-scale spatial and channel-wise attention for improving object detection in remote sensing imagery. ||| jie chen ||| li wan ||| jingru zhu ||| gang xu ||| min deng ||| 
2022 ||| multistage attention resu-net for semantic segmentation of fine-resolution remote sensing images. ||| rui li ||| shunyi zheng ||| chenxi duan ||| jianlin su ||| ce zhang ||| 
2020 ||| local attention networks for occluded airplane detection in remote sensing images. ||| min zhou ||| zhengxia zou ||| zhenwei shi ||| wen-jun zeng ||| jie gui ||| 
2022 ||| spectral-spatial graph attention network for semisupervised hyperspectral image classification. ||| zhengang zhao ||| hao wang ||| xianchuan yu ||| 
2022 ||| sscan: a spatial-spectral cross attention network for hyperspectral image denoising. ||| zhiqiang wang ||| zhenfeng shao ||| xiao huang ||| jiaming wang ||| tao lu ||| 
2020 ||| roof classification from 3-d lidar point clouds using multiview cnn with self-attention. ||| dimple a. shajahan ||| vaibhav nayel ||| ramanathan muthuganapathy ||| 
2022 ||| multiscale feature learning by transformer for building extraction from satellite images. ||| xin chen ||| chunping qiu ||| wenyue guo ||| anzhu yu ||| xiaochong tong ||| michael schmitt ||| 
2022 ||| hyperspectral image super-resolution based on multiscale mixed attention network fusion. ||| jianwen hu ||| yuan tang ||| yaoting liu ||| shaosheng fan ||| 
2021 ||| band selection of hyperspectral images using attention-based autoencoders. ||| zeyang dou ||| kun gao ||| xiaodian zhang ||| hong wang ||| lu han ||| 
2022 ||| automatic extraction of layover from insar imagery based on multilayer feature fusion attention mechanism. ||| xingmin cai ||| lifu chen ||| jin xing ||| xuemin xing ||| ru luo ||| siyu tan ||| jielan wang ||| 
2022 ||| da2net: distraction-attention-driven adversarial network for robust remote sensing image scene classification. ||| rui yang ||| fangling pu ||| zhaozhuo xu ||| chujiang ding ||| xin xu ||| 
2022 ||| visible-assisted infrared image super-resolution based on spatial attention residual network. ||| xiaodong yang ||| mengmeng zhang ||| wei li ||| ran tao ||| 
2022 ||| when cnns meet vision transformer: a joint framework for remote sensing scene classification. ||| peifang deng ||| kejie xu ||| hong huang ||| 
2022 ||| a novel transformer based semantic segmentation scheme for fine-resolution remote sensing images. ||| libo wang ||| rui li ||| chenxi duan ||| ce zhang ||| xiaoliang meng ||| shenghui fang ||| 
2021 ||| wide-context attention network for remote sensing image retrieval. ||| honghu wang ||| zhiqiang zhou ||| hua zong ||| lingjuan miao ||| 
2019 ||| optimized input for cnn-based hyperspectral image classification using spatial transformer network. ||| xin he ||| yushi chen ||| 
2022 ||| unsupervised hyperspectral pansharpening by ratio estimation and residual attention network. ||| jinyan nie ||| qizhi xu ||| junjun pan ||| 
2019 ||| attentional information fusion networks for cross-scene power line detection. ||| yan li ||| zehao xiao ||| xiantong zhen ||| xianbin cao ||| 
2019 ||| convolutional attention in ensemble with knowledge transferred for remote sensing image classification. ||| hainan wang ||| yunqi miao ||| hongren wang ||| baochang zhang ||| 
2022 ||| attention mask-based network with simple color annotation for uav vehicle re-identification. ||| aihuan yao ||| mengmeng huang ||| jiahao qi ||| ping zhong ||| 
2022 ||| scaf-net: scene context attention-based fusion network for vehicle detection in aerial imagery. ||| minghui wang ||| qingpeng li ||| yunchao gu ||| leyuan fang ||| xiao xiang zhu ||| 
2019 ||| multiscale visual attention networks for object detection in vhr remote sensing images. ||| chen wang ||| xiao bai ||| shuai wang ||| jun zhou ||| peng ren ||| 
2022 ||| sar target recognition based on efficient fully convolutional attention block cnn. ||| rui li ||| xiaodan wang ||| jian wang ||| yafei song ||| lei lei ||| 
2022 ||| convolutional neural network with attention mechanism for sar automatic target recognition. ||| ming zhang ||| jubai an ||| dahua yu ||| li dong yang ||| liang wu ||| xiaoqi lu ||| 
2022 ||| ed-drap: encoder-decoder deep residual attention prediction network for radar echoes. ||| hongshu che ||| dan niu ||| zengliang zang ||| yichao cao ||| xisong chen ||| 
2022 ||| channel attention-based temporal convolutional network for satellite image time series classification. ||| pengfei tang ||| peijun du ||| junshi xia ||| peng zhang ||| wei |||  zhang ||| 
2022 ||| target detection based on edge-aware and cross-coupling attention for sar images. ||| libao zhang ||| lan zhang ||| wanning zhu ||| 
2021 ||| cooperative spectral-spatial attention dense network for hyperspectral image classification. ||| zhimin dong ||| yaoming cai ||| zhihua cai ||| xiaobo liu ||| zhaoyu yang ||| mingchen zhuge ||| 
2022 ||| a lightweight detector based on attention mechanism for aluminum strip surface defect detection. ||| zhuxi ma ||| yibo li ||| minghui huang ||| qianbin huang ||| jie cheng ||| si tang ||| 
2018 ||| multi-view pedestrian captioning with an attention topic cnn model. ||| quan liu ||| yingying chen ||| jinqiao wang ||| sijiong zhang ||| 
2021 ||| a double-layer attention based adversarial network for partial transfer learning in machinery fault diagnosis. ||| yafei deng ||| delin huang ||| shichang du ||| guilong li ||| chen zhao ||| jun lv ||| 
2022 ||| a feature fusion enhanced multiscale cnn with attention mechanism for spot-welding surface appearance recognition. ||| meng xiao ||| bo yang ||| shilong wang ||| zhengping zhang ||| xiaoli tang ||| ling kang ||| 
2019 ||| coarse-to-fine document localization in natural scene image with regional attention and recursive corner refinement. ||| anna zhu ||| chen zhang ||| zhi li ||| shengwu xiong ||| 
2021 ||| eaml: ensemble self-attention-based mutual learning network for document image classification. ||| souhail bakkali ||| zuheng ming ||| micka ||| l coustaty ||| mar ||| al rusi ||| ol ||| 
2022 ||| $\hbox {tg}^2$: text-guided transformer gan for restoring document readability and perceived quality. ||| oldrich kodym ||| michal hradis ||| 
2020 ||| ma-crnn: a multi-scale attention crnn for chinese text line recognition in natural scenes. ||| guofeng tong ||| yong li ||| huashuai gao ||| huairong chen ||| hao wang ||| xiang yang ||| 
2020 ||| identify vulnerability fix commits automatically using hierarchical attention network. ||| mingxin sun ||| wenjie wang ||| hantao feng ||| hongu sun ||| yuqing zhang ||| 
2021 ||| leveraging attention-based deep neural networks for security vetting of android applications. ||| prabesh pathak ||| prabesh poudel ||| sankardas roy ||| doina caragea ||| 
2019 ||| triplet loss with channel attention for person re-identification. ||| daniel organisciak ||| chirine riachy ||| nauman aslam ||| hubert p. h. shum ||| 
2019 ||| robust target tracking algorithm based on superpixel visual attention mechanism: robust target tracking algorithm. ||| jia hu ||| xiaoping fan ||| shengzong liu ||| lirong huang ||| 
2021 ||| deep scaffold hopping with multimodal transformer neural networks. ||| shuangjia zheng ||| zengrong lei ||| haitao ai ||| hongming chen ||| daiguo deng ||| yuedong yang ||| 
2020 ||| a self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility. ||| bowen tang ||| skyler t. kramer ||| meijuan fang ||| yingkun qiu ||| zhen wu ||| dong xu ||| 
2020 ||| building attention and edge message passing neural networks for bioactivity and physical-chemical property prediction. ||| michael withnall ||| edvard lindel ||| f ||| ola engkvist ||| hongming chen ||| 
2020 ||| transformer-cnn: swiss knife for qsar modeling and interpretation. ||| pavel karpov ||| guillaume godin ||| igor v. tetko ||| 
2019 ||| biotransformer: a comprehensive computational tool for small molecule metabolism prediction and metabolite identification. ||| yannick djoumbou feunang ||| jarlei fiamoncini ||| alberto gil-de-la-fuente ||| russell greiner ||| claudine manach ||| david s. wishart ||| 
2021 ||| decimer 1.0: deep learning for chemical image recognition using transformers. ||| kohulan rajan ||| achim zielesny ||| christoph steinbeck ||| 
2021 ||| constrained versus unconstrained rational inattention. ||| yaron azrieli ||| 
2021 ||| champ versus chump: viewing an opponent's face engages attention but not reward systems. ||| ralph s. redden ||| greg a. gagliardi ||| chad c. williams ||| cameron d. hassall ||| olave e. krigolson ||| 
2022 ||| precise learning of source code contextual semantics via hierarchical dependence structure and graph attention networks. ||| zhehao zhao ||| bo yang ||| ge li ||| huai liu ||| zhi jin ||| 
2022 ||| prhan: automated pull request description generation based on hybrid attention network. ||| sen fang ||| tao zhang ||| youshuai tan ||| zhou xu ||| zhi-xin yuan ||| ling-ze meng ||| 
2020 ||| forecasting currency exchange rate time series with fireworks-algorithm-based higher order neural network with special attention to training data enrichment. ||| kishore kumar sahu ||| sarat chandra nayak ||| himansu sekhar behera ||| 
2020 ||| resistance and resolution: attentional dynamics in discourse. ||| justin bledin ||| kyle rawlins ||| 
2018 ||| effects of online synchronous instruction with an attention monitoring and alarm mechanism on sustained attention and learning performance. ||| chih-ming chen ||| jung-ying wang ||| 
2019 ||| improving effectiveness of learners' review of video lectures by using an attention-based video lecture review mechanism based on brainwave signals. ||| yong-teng lin ||| chih-ming chen ||| 
2018 ||| stop daydreaming, pay attention. ||| sue greener ||| 
2018 ||| effects of using a second-screen application on attention, learning, and user experience in an educational content. ||| seungyup lee ||| jongsoo baek ||| gunhee han ||| 
2017 ||| enhancing learning performance, attention, and meditation using a speech-to-text recognition application: evidence from multiple data sources. ||| rustam shadiev ||| ting-ting wu ||| yueh-min huang ||| 
2020 ||| attendance and attention. ||| sue greener ||| 
2017 ||| strategic pricing with rational inattention to quality. ||| daniel martin ||| 
2017 ||| texting and walking: a controlled field study of crossing behaviours and inattentional blindness in taiwan. ||| ping-ling chen ||| wafaa saleh ||| chih-wei pai ||| 
2018 ||| attention to esports advertisement: effects of ad animation and in-game dynamics on viewers' visual attention. ||| youngnam seo ||| minkyung kim ||| doohwang lee ||| younbo jung ||| 
2018 ||| improving periocular recognition by explicit attention to critical regions in deep neural network. ||| zijing zhao ||| ajay kumar ||| 
2020 ||| attention-based two-stream convolutional networks for face spoofing detection. ||| haonan chen ||| guosheng hu ||| zhen lei ||| yaowu chen ||| neil martin robertson ||| stan z. li ||| 
2020 ||| target-specific siamese attention network for real-time object tracking. ||| thanikasalam kokul ||| clinton fookes ||| sridha sridharan ||| amirthalingam ramanan ||| amalka pinidiyaarachchi ||| 
2022 ||| person re-identification by context-aware part attention and multi-head collaborative learning. ||| dongming wu ||| mang ye ||| gaojie lin ||| xin gao ||| jianbing shen ||| 
2019 ||| joint intensity transformer network for gait recognition robust against clothing and carrying status. ||| xiang li ||| yasushi makihara ||| chi xu ||| yasushi yagi ||| mingwu ren ||| 
2021 ||| depth as attention for face representation learning. ||| hardik uppal ||| alireza sepas-moghaddam ||| michael a. greenspan ||| ali etemad ||| 
2020 ||| person re-identification using spatial and layer-wise attention. ||| aske r. lejb ||| lle ||| kamal nasrollahi ||| benjamin krogh ||| thomas b. moeslund ||| 
2020 ||| towards complete and accurate iris segmentation using deep multi-task attention network for non-cooperative iris recognition. ||| caiyong wang ||| jawad muhammad ||| yunlong wang ||| zhaofeng he ||| zhenan sun ||| 
2021 ||| end-to-end domain adaptive attention network for cross-domain person re-identification. ||| amena khatun ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2021 ||| deepsbd: a deep neural network model with attention mechanism for socialbot detection. ||| mohd fazil ||| amit kumar sah ||| muhammad abulaish ||| 
2021 ||| multi-scale gradients self-attention residual learning for face photo-sketch transformation. ||| shuchao duan ||| zhenxue chen ||| q. m. jonathan wu ||| lei cai ||| dan lu ||| 
2021 ||| minutiae attention network with reciprocal distance loss for contactless to contact-based fingerprint identification. ||| hanzhuo tan ||| ajay kumar ||| 
2020 ||| dependency-aware attention control for image set-based face recognition. ||| xiaofeng liu ||| zhenhua guo ||| jane you ||| b. v. k. vijaya kumar ||| 
2021 ||| dsa-face: diverse and sparse attentions for face recognition robust to pose variation and occlusion. ||| qiangchang wang ||| guodong guo ||| 
2022 ||| self-adversarial training incorporating forgery attention for image forgery localization. ||| long zhuo ||| shunquan tan ||| bin li ||| jiwu huang ||| 
2020 ||| fused behavior recognition model based on attention mechanism. ||| lei chen ||| rui liu ||| dongsheng zhou ||| xin yang ||| qiang zhang ||| 
2017 ||| social media attention increases article visits: an investigation on article-level referral data of peerj. ||| xianwen wang ||| yunxue cui ||| qingchun li ||| xinhui guo ||| 
2018 ||| distracted reading: acts of attention in the age of the internet. ||| marion thain ||| 
2018 ||| from distracted to distributed attention: expanded learning through social media, augmented reality, remixing, and activist geocaching. ||| marina hassapopoulou ||| 
2018 ||| three-dimensional attention-based deep ranking model for video highlight detection. ||| yifan jiao ||| zhetao li ||| shucheng huang ||| xiaoshan yang ||| bin liu ||| tianzhu zhang ||| 
2022 ||| human action recognition by discriminative feature pooling and video segment attention model. ||| md. moniruzzaman ||| zhaozheng yin ||| zhihai he ||| ruwen qin ||| ming c. leu ||| 
2018 ||| gla: global-local attention for image description. ||| linghui li ||| sheng tang ||| yongdong zhang ||| lixi deng ||| qi tian ||| 
2021 ||| sal: selection and attention losses for weakly supervised semantic segmentation. ||| lei zhou ||| chen gong ||| zhi liu ||| keren fu ||| 
2019 ||| learning attentional recurrent neural network for visual tracking. ||| qiurui wang ||| chun yuan ||| jingdong wang ||| wenjun zeng ||| 
2020 ||| spatio-temporal attention networks for action recognition and detection. ||| jun li ||| xianglong liu ||| wenxuan zhang ||| mingyuan zhang ||| jingkuan song ||| nicu sebe ||| 
2019 ||| comic: toward a compact image captioning model with attention. ||| jia huei tan ||| chee seng chan ||| joon huang chuah ||| 
2021 ||| hybrid-attention enhanced two-stream fusion network for video venue prediction. ||| yanchao zhang ||| weiqing min ||| liqiang nie ||| shuqiang jiang ||| 
2021 ||| image-text multimodal emotion classification via multi-view attentional network. ||| xiaocui yang ||| shi feng ||| daling wang ||| yifei zhang ||| 
2021 ||| stacked u-shape network with channel-wise attention for salient object detection. ||| junxia li ||| zefeng pan ||| qingshan liu ||| ziyang wang ||| 
2022 ||| cola-net: collaborative attention network for image restoration. ||| chong mou ||| jian zhang ||| xiaopeng fan ||| hangfan liu ||| ronggang wang ||| 
2020 ||| convolutional networks with channel and stips attention model for action recognition in videos. ||| hanbo wu ||| xin ma ||| yibin li ||| 
2020 ||| corrections to "stat: spatial-temporal attention mechanism for video captioning". ||| chenggang yan ||| yunbin tu ||| xingzheng wang ||| yongbing zhang ||| xinhong hao ||| yongdong zhang ||| qionghai dai ||| 
2021 ||| beyond vision: a multimodal recurrent attention convolutional neural network for unified image aesthetic prediction tasks. ||| xiaodan zhang ||| xinbo gao ||| wen lu ||| lihuo he ||| jie li ||| 
2017 ||| videowhisper: toward discriminative unsupervised video feature learning with attention-based recurrent neural networks. ||| na zhao ||| hanwang zhang ||| richang hong ||| meng wang ||| tat-seng chua ||| 
2017 ||| diversified visual attention networks for fine-grained object classification. ||| bo zhao ||| xiao wu ||| jiashi feng ||| qiang peng ||| shuicheng yan ||| 
2022 ||| cross parallax attention network for stereo image super-resolution. ||| canqiang chen ||| chunmei qing ||| xiangmin xu ||| patrick dickinson ||| 
2021 ||| an attention-based unsupervised adversarial model for movie review spam detection. ||| yuan gao ||| maoguo gong ||| yu xie ||| alex kai qin ||| 
2019 ||| high-quality image captioning with fine-grained and semantic-guided visual attention. ||| zongjian zhang ||| qiang wu ||| yang wang ||| fang chen ||| 
2021 ||| a coarse-to-fine facial landmark detection method based on self-attention mechanism. ||| pengcheng gao ||| ke lu ||| jian xue ||| ling shao ||| jiayi lyu ||| 
2020 ||| a cuboid cnn model with an attention mechanism for skeleton-based action recognition. ||| kaijun zhu ||| ruxin wang ||| qingsong zhao ||| jun cheng ||| dapeng tao ||| 
2021 ||| br$^2$net: defocus blur detection via a bidirectional channel attention residual refining network. ||| chang tang ||| xinwang liu ||| shan an ||| pichao wang ||| 
2021 ||| transformer encoder with multi-modal multi-head attention for continuous affect recognition. ||| haifeng chen ||| dongmei jiang ||| hichem sahli ||| 
2021 ||| mvanet: multi-task guided multi-view attention network for chinese food recognition. ||| haozan liang ||| guihua wen ||| yang hu ||| mingnan luo ||| pei yang ||| yingxue xu ||| 
2019 ||| where-and-when to look: deep siamese attention networks for video-based person re-identification. ||| lin wu ||| yang wang ||| junbin gao ||| xue li ||| 
2022 ||| e-commerce storytelling recommendation using attentional domain-transfer network and adversarial pre-training. ||| xusong chen ||| chenyi lei ||| dong liu ||| guoxin wang ||| haihong tang ||| zheng-jun zha ||| houqiang li ||| 
2020 ||| stnreid: deep convolutional networks with pairwise spatial transformer networks for partial person re-identification. ||| hao luo ||| wei jiang ||| xing fan ||| chi zhang ||| 
2022 ||| multi-localized sensitive autoencoder-attention-lstm for skeleton-based action recognition. ||| wing w. y. ng ||| mingyang zhang ||| ting wang ||| 
2019 ||| attention-based multiview re-observation fusion network for skeletal action recognition. ||| zhaoxuan fan ||| xu zhao ||| tianwei lin ||| haisheng su ||| 
2021 ||| apse: attention-aware polarity-sensitive embedding for emotion-based image retrieval. ||| xingxu yao ||| sicheng zhao ||| yu-kun lai ||| dongyu she ||| jie liang ||| jufeng yang ||| 
2021 ||| pfan++: bi-directional image-text retrieval with position focused attention network. ||| yaxiong wang ||| hao yang ||| xiuxiu bai ||| xueming qian ||| lin ma ||| jing lu ||| biao li ||| xin fan ||| 
2021 ||| understanding more about human and machine attention in deep neural networks. ||| qiuxia lai ||| salman h. khan ||| yongwei nie ||| hanqiu sun ||| jianbing shen ||| ling shao ||| 
2020 ||| gaim: graph attention interaction model for collective activity recognition. ||| lihua lu ||| yao lu ||| ruizhe yu ||| huijun di ||| lin zhang ||| shunzhou wang ||| 
2020 ||| deep multi-kernel convolutional lstm networks and an attention-based mechanism for videos. ||| sebastian agethen ||| winston h. hsu ||| 
2020 ||| relation attention for temporal action localization. ||| peihao chen ||| chuang gan ||| guangyao shen ||| wenbing huang ||| runhao zeng ||| mingkui tan ||| 
2017 ||| video captioning with attention-based lstm and semantic consistency. ||| lianli gao ||| zhao guo ||| hanwang zhang ||| xing xu ||| heng tao shen ||| 
2020 ||| oriented spatial transformer network for pedestrian detection using fish-eye camera. ||| yeqiang qian ||| ming yang ||| xu zhao ||| chunxiang wang ||| bing wang ||| 
2022 ||| infrared action detection in the dark via cross-stream attention mechanism. ||| xu chen ||| chenqiang gao ||| chaoyu li ||| yi yang ||| deyu meng ||| 
2021 ||| spatial pyramid attention for deep convolutional neural networks. ||| xu ma ||| jingda guo ||| andrew sansom ||| mara mcguire ||| andrew kalaani ||| qi chen ||| sihai tang ||| qing yang ||| song fu ||| 
2021 ||| attentionfgan: infrared and visible image fusion using attention-based generative adversarial networks. ||| jing li ||| hongtao huo ||| chang li ||| renhua wang ||| qi feng ||| 
2020 ||| pay attention to the activations: a modular attention mechanism for fine-grained image recognition. ||| pau rodr ||| guez ||| diego velazquez dorta ||| guillem cucurull ||| josep m. gonfaus ||| f. xavier roca ||| jordi gonz ||| lez ||| 
2021 ||| character detection in animated movies using multi-style adaptation and visual attention. ||| hayeon kim ||| eun-cheol lee ||| yongseok seo ||| dong-hyuck im ||| in-kwon lee ||| 
2022 ||| sam: modeling scene, object and action with semantics attention modules for video recognition. ||| xing zhang ||| zuxuan wu ||| yu-gang jiang ||| 
2022 ||| focus your attention: a focal attention for multimodal learning. ||| chunxiao liu ||| zhendong mao ||| tianzhu zhang ||| an-an liu ||| bin wang ||| yongdong zhang ||| 
2021 ||| improving driver gaze prediction with reinforced attention. ||| kai lv ||| hao sheng ||| zhang xiong ||| wei li ||| liang zheng ||| 
2021 ||| attention-based deep reinforcement learning for virtual cinematography of 360$^{\circ}$ videos. ||| jianyi wang ||| mai xu ||| lai jiang ||| yuhang song ||| 
2020 ||| learning normal patterns via adversarial attention-based autoencoder for abnormal event detection in videos. ||| hao song ||| che sun ||| xinxiao wu ||| mei chen ||| yunde jia ||| 
2019 ||| attend and imagine: multi-label image classification with visual attention and recurrent neural networks. ||| fan lyu ||| qi wu ||| fuyuan hu ||| qingyao wu ||| mingkui tan ||| 
2020 ||| tamper-proofing video with hierarchical attention autoencoder hashing on blockchain. ||| tu bui ||| daniel cooper ||| john p. collomosse ||| mark bell ||| alex green ||| john sheridan ||| jez higgins ||| arindra das ||| jared robert keller ||| olivier thereaux ||| 
2021 ||| adversarial disentanglement spectrum variations and cross-modality attention networks for nir-vis face recognition. ||| weipeng hu ||| haifeng hu ||| 
2019 ||| unified spatio-temporal attention networks for action recognition in videos. ||| dong li ||| ting yao ||| ling-yu duan ||| tao mei ||| yong rui ||| 
2019 ||| decoupled spatial neural attention for weakly supervised semantic segmentation. ||| tianyi zhang ||| guosheng lin ||| jianfei cai ||| tong shen ||| chunhua shen ||| alex c. kot ||| 
2022 ||| lag-net: multi-granularity network for person re-identification via local attention system. ||| xun gong ||| zu yao ||| xin li ||| yueqiao fan ||| bin luo ||| jianfeng fan ||| boji lao ||| 
2022 ||| fine-grained attention and feature-sharing generative adversarial networks for single image super-resolution. ||| yitong yan ||| chuangchuang liu ||| changyou chen ||| xianfang sun ||| longcun jin ||| xinyi peng ||| xiang zhou ||| 
2021 ||| ld-man: layout-driven multimodal attention network for online news sentiment recognition. ||| wenya guo ||| ying zhang ||| xiangrui cai ||| lei meng ||| jufeng yang ||| xiaojie yuan ||| 
2021 ||| emotion attention-aware collaborative deep reinforcement learning for image cropping. ||| xiaoyan zhang ||| zhuopeng li ||| jianmin jiang ||| 
2020 ||| frame augmented alternating attention network for video question answering. ||| wenqiao zhang ||| siliang tang ||| yanpeng cao ||| shiliang pu ||| fei wu ||| yueting zhuang ||| 
2020 ||| hierarchical attention network for visually-aware food recommendation. ||| xiaoyan gao ||| fuli feng ||| xiangnan he ||| heyan huang ||| xinyu guan ||| chong feng ||| zhaoyan ming ||| tat-seng chua ||| 
2021 ||| self-adaptive neural module transformer for visual question answering. ||| huasong zhong ||| jingyuan chen ||| chen shen ||| hanwang zhang ||| jianqiang huang ||| xian-sheng hua ||| 
2022 ||| spatial-temporal action localization with hierarchical self-attention. ||| rizard renanda adhi pramono ||| yie-tarng chen ||| wen-hsien fang ||| 
2021 ||| heterogeneous community question answering via social-aware multi-modal co-attention convolutional matching. ||| jun hu ||| shengsheng qian ||| quan fang ||| changsheng xu ||| 
2020 ||| bidirectional attention-recognition model for fine-grained object classification. ||| chuanbin liu ||| hongtao xie ||| zhengjun zha ||| lingyun yu ||| zhineng chen ||| yongdong zhang ||| 
2022 ||| image co-saliency detection and instance co-segmentation using attention graph clustering based graph convolutional network. ||| tengpeng li ||| kaihua zhang ||| shiwen shen ||| bo liu ||| qingshan liu ||| zhu li ||| 
2021 ||| iptv channel zapping recommendation with attention mechanism. ||| guangyu li ||| lina qiu ||| chenguang yu ||| houwei cao ||| yong liu ||| can yang ||| 
2021 ||| caa-net: conditional atrous cnns with attention for explainable device-robust acoustic scene classification. ||| zhao ren ||| qiuqiang kong ||| jing han ||| mark d. plumbley ||| bj ||| rn w. schuller ||| 
2019 ||| pre-attention and spatial dependency driven no-reference image quality assessment. ||| lixiong liu ||| tianshu wang ||| hua huang ||| 
2018 ||| content-attention representation by factorized action-scene network for action recognition. ||| jingyi hou ||| xinxiao wu ||| yuchao sun ||| yunde jia ||| 
2020 ||| stat: spatial-temporal attention mechanism for video captioning. ||| chenggang yan ||| yunbin tu ||| xingzheng wang ||| yongbing zhang ||| xinhong hao ||| yongdong zhang ||| qionghai dai ||| 
2021 ||| spa-gan: spatial attention gan for image-to-image translation. ||| hajar emami ||| majid moradi aliabadi ||| ming dong ||| ratna babu chinnam ||| 
2020 ||| the effects of video games in memory and attention. ||| chrysovalantis kefalis ||| eirini-zoi kontostavlou ||| athanasios drigas ||| 
2018 ||| temporal reference, attentional modulation, and crossmodal assimilation. ||| yingqi wan ||| lihan chen ||| 
2019 ||| analysis of biased competition and cooperation for attention in the cerebral cortex. ||| tatyana s. turova ||| edmund t. rolls ||| 
2020 ||| biosignal-based attention monitoring to support nuclear operator safety-relevant tasks. ||| jung hwan kim ||| chul min kim ||| eun-soo jung ||| man-sung yim ||| 
2020 ||| attention in psychology, neuroscience, and machine learning. ||| grace w. lindsay ||| 
2021 ||| corrigendum: attention in psychology, neuroscience, and machine learning. ||| grace w. lindsay ||| 
2018 ||| multi-timescale memory dynamics extend task repertoire in a reinforcement learning network with attention-gated memory. ||| marco martinolli ||| wulfram gerstner ||| aditya gilra ||| 
2017 ||| a computational model for the automatic diagnosis of attention deficit hyperactivity disorder based on functional brain volume. ||| lirong tan ||| xinyu guo ||| sheng ren ||| jeff epstein ||| long j. lu ||| 
2018 ||| effect of stimulus contrast and visual attention on spike-gamma phase relationship in macaque primary visual cortex. ||| aritra das ||| supratim ray ||| 
2021 ||| toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. ||| katie spoon ||| hsinyu tsai ||| an chen ||| malte j. rasch ||| stefano ambrogio ||| charles mackin ||| andrea fasoli ||| alexander m. friz ||| pritish narayanan ||| milos stanisavljevic ||| geoffrey w. burr ||| 
2019 ||| a computational model of attention control in multi-attribute, context-dependent decision making. ||| kanghoon jung ||| jaeseung jeong ||| jerald d. kralik ||| 
2020 ||| integrating breakdown detection into dialogue systems to improve knowledge management: encoding temporal utterances with memory attention. ||| seolhwa lee ||| dongyub lee ||| danial hooshyar ||| jaechoon jo ||| heuiseok lim ||| 
2021 ||| ppanet: point-wise pyramid attention network for semantic segmentation. ||| mohammed a. m. elhassan ||| yuxuan chen ||| yunyi chen ||| chenxi huang ||| jane yang ||| xingcong yao ||| chenhui yang ||| yinuo cheng ||| 
2020 ||| multimodal fusion method based on self-attention mechanism. ||| hu zhu ||| ze wang ||| yu shi ||| yingying hua ||| guoxia xu ||| lizhen deng ||| 
2021 ||| joint generative image deblurring aided by edge attention prior and dynamic kernel selection. ||| zhichao zhang ||| hui chen ||| xiaoqing yin ||| jinsheng deng ||| 
2022 ||| remote sensing image fusion algorithm based on two-stream fusion network and residual channel attention mechanism. ||| mengxing huang ||| shi liu ||| zhenfeng li ||| siling feng ||| di wu ||| yuanyuan wu ||| feng shu ||| 
2020 ||| leveraging social relationship-based graph attention model for group event recommendation. ||| guoqiong liao ||| xiaobin deng ||| 
2021 ||| san-gal: spatial attention network guided by attribute label for person re-identification. ||| shaoqi hou ||| chunhui liu ||| kangning yin ||| yiyin ding ||| zhiguo wang ||| guangqiang yin ||| 
2021 ||| dual-level attention based on a heterogeneous graph convolution network for aspect-based sentiment classification. ||| peng yuan ||| lei jiang ||| jianxun liu ||| dong zhou ||| pei li ||| yang gao ||| 
2020 ||| automatic image captioning based on resnet50 and lstm with soft attention. ||| yan chu ||| xiao yue ||| lei yu ||| mikhailov sergei ||| zhengkui wang ||| 
2021 ||| attention mechanism-based cnn-lstm model for wind turbine fault prediction using ssn ontology annotation. ||| yuan xie ||| jisheng zhao ||| baohua qiang ||| luzhong mi ||| chenghua tang ||| longge li ||| 
2021 ||| eawnet: an edge attention-wise objector for real-time visual internet of things. ||| zhichao zhang ||| hui chen ||| xiaoqing yin ||| jinsheng deng ||| 
2017 ||| a semi-supervised inattention detection method using biological signal. ||| ye rim choi ||| jonghun park ||| dongmin shin ||| 
2021 ||| aspect-based capsule network with mutual attention for recommendations. ||| zhenyu yang ||| xuesong wang ||| yuhu cheng ||| guojing liu ||| 
2020 ||| recursive multi-signal temporal fusions with attention mechanism improves emg feature extraction. ||| rami n. khushaba ||| angkoon phinyomark ||| ali h. al-timemy ||| erik j. scheme ||| 
2021 ||| reverse graph self-attention for target-directed atomic importance estimation. ||| gyoung s. na ||| hyun woo kim ||| 
2020 ||| attention-guided cnn for image denoising. ||| chunwei tian ||| yong xu ||| zuoyong li ||| wangmeng zuo ||| lunke fei ||| hong liu ||| 
2021 ||| end-to-end keyword search system based on attention mechanism and energy scorer for low resource languages. ||| zeyu zhao ||| wei-qiang zhang ||| 
2019 ||| neural dynamics of spreading attentional labels in mental contour tracing. ||| mateja maric ||| drazen domijan ||| 
2020 ||| learning cascade attention for fine-grained image classification. ||| youxiang zhu ||| ruochen li ||| yin yang ||| ning ye ||| 
2020 ||| on the localness modeling for the self-attention based end-to-end speech synthesis. ||| shan yang ||| heng lu ||| shiyin kang ||| liumeng xue ||| jinba xiao ||| dan su ||| lei xie ||| dong yu ||| 
2022 ||| multi-level attention pooling for graph neural networks: unifying graph representations with multiple localities. ||| takeshi d. itoh ||| takatomi kubo ||| kazushi ikeda ||| 
2020 ||| sequential vessel segmentation via deep channel attention network. ||| dongdong hao ||| song ding ||| linwei qiu ||| yisong lv ||| baowei fei ||| yueqi zhu ||| binjie qin ||| 
2018 |||  attention: an lstm framework for human trajectory prediction and abnormal event detection. ||| tharindu fernando ||| simon denman ||| sridha sridharan ||| clinton fookes ||| 
2022 ||| transformers for modeling physical systems. ||| nicholas geneva ||| nicholas zabaras ||| 
2020 ||| neuromodulated attention and goal-driven perception in uncertain domains. ||| xinyun zou ||| soheil kolouri ||| praveen k. pilly ||| jeffrey l. krichmar ||| 
2021 ||| bilateral attention decoder: a lightweight decoder for real-time semantic segmentation. ||| chengli peng ||| tian tian ||| chen chen ||| xiaojie guo ||| jiayi ma ||| 
2022 ||| cross-attention-map-based regularization for adversarial domain adaptation. ||| jingwei li ||| huanjie wang ||| ke wu ||| chengbao liu ||| jie tan ||| 
2021 ||| unsupervised foveal vision neural architecture with top-down attention. ||| ryan burt ||| nina n. thigpen ||| andreas keil ||| jos |||  c. pr ||| ncipe ||| 
2021 ||| low-shot transfer with attention for highly imbalanced cursive character recognition. ||| amin jalali ||| swathi kavuri sri ||| minho lee ||| 
2021 ||| improved deep cnns based on nonlinear hybrid attention module for image classification. ||| nan guo ||| ke gu ||| junfei qiao ||| jing bi ||| 
2020 ||| cnn-mhsa: a convolutional neural network and multi-head self-attention combined approach for detecting phishing websites. ||| xi xiao ||| dianyan zhang ||| guangwu hu ||| yong jiang ||| shutao xia ||| 
2018 ||| distant supervision for neural relation extraction integrated with word attention and property features. ||| jianfeng qu ||| dantong ouyang ||| wen hua ||| yuxin ye ||| ximing li ||| 
2021 ||| optimal attention tuning in a neuro-computational model of the visual cortex-basal ganglia-prefrontal cortex loop. ||| oliver maith ||| alex schwarz ||| fred h. hamker ||| 
2021 ||| multi-scale attention convolutional neural network for time series classification. ||| wei chen ||| ke shi ||| 
2021 ||| tgan: a simple model update strategy for visual tracking via template-guidance attention network. ||| kai yang ||| haijun zhang ||| dongliang zhou ||| linlin liu ||| 
2021 ||| hiam: a hierarchical attention based model for knowledge graph multi-hop reasoning. ||| ting ma ||| shangwen lv ||| longtao huang ||| songlin hu ||| 
2021 ||| stacked debert: all attention in incomplete data for text classification. ||| gwenaelle cunha sergio ||| minho lee ||| 
2021 ||| graph embedding clustering: graph attention auto-encoder with cluster-specificity distribution. ||| huiling xu ||| wei xia ||| quanxue gao ||| jungong han ||| xinbo gao ||| 
2021 ||| combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition. ||| ziping zhao ||| qifei li ||| zixing zhang ||| nicholas cummins ||| haishuai wang ||| jianhua tao ||| bj ||| rn w. schuller ||| 
2018 ||| deep neural network for traffic sign recognition systems: an analysis of spatial transformers and stochastic optimisation methods. ||| lvaro arcos garc ||| a ||| juan antonio  ||| lvarez ||| luis miguel soria-morillo ||| 
2022 ||| utrad: anomaly detection and localization with u-transformer. ||| liyang chen ||| zhiyuan you ||| nian zhang ||| juntong xi ||| xinyi le ||| 
2021 ||| h-vectors: improving the robustness in utterance-level speaker embeddings using a hierarchical attention model. ||| yanpei shi ||| qiang huang ||| thomas hain ||| 
2020 ||| mgat: multi-view graph attention networks. ||| yu xie ||| yuanqiao zhang ||| maoguo gong ||| zedong tang ||| chao han ||| 
2021 ||| d-mona: a dilated mixed-order non-local attention network for speaker and language recognition. ||| xiaoxiao miao ||| ian mcloughlin ||| wenchao wang ||| pengyuan zhang ||| 
2022 ||| generalized attention-weighted reinforcement learning. ||| lennart bramlage ||| aurelio cortese ||| 
2018 ||| distant supervision for relation extraction with hierarchical selective attention. ||| peng zhou ||| jiaming xu ||| zhenyu qi ||| hongyun bao ||| zhineng chen ||| bo xu ||| 
2022 ||| dual global enhanced transformer for image captioning. ||| tiantao xian ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2020 ||| high tissue contrast image synthesis via multistage attention-gan: application to segmenting brain mr scans. ||| mohammad hamghalam ||| tianfu wang ||| baiying lei ||| 
2021 ||| self-selective attention using correlation between instances for distant supervision relation extraction. ||| yanru zhou ||| limin pan ||| chongyou bai ||| senlin luo ||| zhouting wu ||| 
2021 ||| igagcn: information geometry and attention-based spatiotemporal graph convolutional networks for traffic flow prediction. ||| jiyao an ||| liang guo ||| wei liu ||| zhiqiang fu ||| ping ren ||| xinzhi liu ||| tao li ||| 
2021 ||| sam-gan: self-attention supporting multi-stage generative adversarial networks for text-to-image synthesis. ||| dunlu peng ||| wuchen yang ||| cong liu ||| shuairui l ||| 
2021 ||| tumor attention networks: better feature selection, better tumor segmentation. ||| shuchao pang ||| anan du ||| mehmet a. orgun ||| yunyun wang ||| zhenmei yu ||| 
2019 ||| attention inspired network: steep learning curve in an invariant pattern recognition model. ||| luis sa-couto ||| andreas wichert ||| 
2020 ||| amd-gan: attention encoder and multi-branch structure based generative adversarial networks for fundus disease detection from scanning laser ophthalmoscopy images. ||| hai xie ||| haijun lei ||| xianlu zeng ||| yejun he ||| guozhen chen ||| ahmed elazab ||| guanghui yue ||| jiantao wang ||| guoming zhang ||| baiying lei ||| 
2021 ||| uncertainty-aware fusion of probabilistic classifiers for improved transformer diagnostics. ||| jose ignacio aizpurua ||| victoria m. catterson ||| brian g. stewart ||| stephen d. j. mcarthur ||| brandon lambert ||| james g. cross ||| 
2021 ||| self-attention-based temporary curiosity in reinforcement learning exploration. ||| hangkai hu ||| shiji song ||| gao huang ||| 
2018 ||| pricing when customers have limited attention. ||| tamer boyaci ||| yal ||| in ak ||| ay ||| 
2018 ||| maxing out globally: individualism, investor attention, and the cross section of expected stock returns. ||| yong-ho cheon ||| kuan-hui lee ||| 
2022 ||| inattention in contract markets: evidence from a consolidation of options in telecom. ||| bj ||| rn-atle reme ||| helene lie r ||| hr ||| morten s ||| thre ||| 
2021 ||| asset management and financial conglomerates: attention through stellar funds. ||| rafael zambrana ||| 
2020 ||| dynamic attention behavior under return predictability. ||| daniel andrei ||| michael hasler ||| 
2021 ||| intertemporal choices are causally influenced by fluctuations in visual attention. ||| geoffrey fisher ||| 
2017 ||| the value of nothing: asymmetric attention to opportunity costs drives intertemporal decision making. ||| daniel read ||| christopher y. olivola ||| david j. hardisty ||| 
2017 ||| the comovement of investor attention. ||| michael s. drake ||| jared jennings ||| darren t. roulstone ||| jacob r. thornock ||| 
2021 ||| cater to thy client: analyst responsiveness to institutional investor attention. ||| peng-chia chiu ||| ben lourie ||| alexander nekrasov ||| siew hong teoh ||| 
2021 ||| asymmetric attention and stock returns. ||| peter cziraki ||| jordi mondria ||| thomas wu ||| 
2021 ||| competition for attention in online social networks: implications for seeding strategies. ||| sarah gelper ||| ralf van der lans ||| gerrit h. van bruggen ||| 
2018 ||| promoting change from the outside: directing managerial attention in the implementation of environmental improvements. ||| suvrat s. dhanorkar ||| enno siemsen ||| kevin w. linderman ||| 
2018 ||| category encoders: a scikit-learn-contrib package of transformers for encoding categorical data. ||| william mcginnis ||| chapman siu ||| andre s. ||| hanyu huang ||| 
2021 ||| tx$^2$: transformer explainability and exploration. ||| nathan martindale ||| scott stewart ||| 
2021 ||| underwater image restoration and enhancement via residual two-fold attention networks. ||| bo fu ||| liyan wang ||| ruizi wang ||| shilin fu ||| fangfei liu ||| xin liu ||| 
2019 ||| attention pooling-based bidirectional gated recurrent units model for sentimental classification. ||| dejun zhang ||| mingbo hong ||| lu zou ||| fei han ||| fazhi he ||| zhigang tu ||| yafeng ren ||| 
2021 ||| deep learning-based short-term load forecasting for transformers in distribution grid. ||| renshu wang ||| jing zhao ||| 
2020 ||| sequential prediction of glycosylated hemoglobin based on long short-term memory with self-attention mechanism. ||| xiaojia wang ||| wenqing gong ||| keyu zhu ||| lushi yao ||| shanshan zhang ||| weiqun xu ||| yuxiang guan ||| 
2021 ||| a classification model of legal consulting questions based on multi-attention prototypical networks. ||| jianzhou feng ||| jinman cui ||| qikai wei ||| zhengji zhou ||| yuxiong wang ||| 
2021 ||| forecasting teleconsultation demand with an ensemble attention-based bidirectional long short-term memory model. ||| wenjia chen ||| lean yu ||| jinlin li ||| 
2021 ||| restoring attentional resources with nature: a replication study of berto's (2005) paradigm including commentary from dr. rita berto. ||| brittany n. neilson ||| curtis m. craig ||| raelyn y. curiel ||| martina i. klein ||| 
2019 ||| sustained attention to science: a tribute to the life and scholarship of joel warm. ||| peter a. hancock ||| james l. szalma ||| 
2018 ||| auditory task irrelevance: a basis for inattentional deafness. ||| menja scheer ||| heinrich h. b ||| lthoff ||| lewis l. chuang ||| 
2017 ||| task engagement and attentional resources. ||| gerald matthews ||| joel s. warm ||| andrew p. smith ||| 
2020 ||| classification of attentional tunneling through behavioral indices. ||| sean w. kortschot ||| greg a. jamieson ||| 
2020 ||| artificial optic flow guides visual attention in a driving scene. ||| yoko higuchi ||| satoshi inoue ||| hiroto hamada ||| takatsune kumada ||| 
2020 ||| attentional demand as a function of contextual factors in different traffic scenarios. ||| zhuofan liu ||| christer ahlstr ||| m ||| sa forsman ||| katja kircher ||| 
2017 ||| minimum required attention: a human-centered approach to driver inattention. ||| katja kircher ||| christer ahlstr ||| m ||| 
2018 ||| measuring and mitigating the costs of attentional switches in active network monitoring for cybersecurity. ||| sean w. kortschot ||| dusan sovilj ||| greg a. jamieson ||| scott sanner ||| chelsea carrasco ||| harold soh ||| 
2020 ||| shoulder muscular fatigue from static posture concurrently reduces cognitive attentional resources. ||| mitchell l. stephenson ||| alec g. ostrander ||| hamid norasi ||| michael c. dorneich ||| 
2018 ||| allocating attention to detect motorcycles: the role of inattentional blindness. ||| kristen pammer ||| stephanie sabadas ||| stephanie lentern ||| 
2019 ||| the abbreviated vigilance task and its attentional contributors. ||| curtis m. craig ||| martina i. klein ||| 
2017 ||| measuring memory and attention to preview in motion. ||| richard j. jagacinski ||| gordon m. hammond ||| emanuele rizzi ||| 
2017 ||| a closed-loop model of operator visual attention, situation awareness, and performance across automation mode transitions. ||| aaron w. johnson ||| kevin r. duda ||| thomas b. sheridan ||| charles m. oman ||| 
2017 ||| measuring sustained attention and perceived workload. ||| cynthia laurie-rose ||| lori m. curtindale ||| meredith frey ||| 
2019 ||| the effect of partial automation on driver attention: a naturalistic driving study. ||| john g. gaspar ||| cher carney ||| 
2020 ||| allocation of driver attention for varying in-vehicle system modalities. ||| ning li ||| linda ng boyle ||| 
2021 ||| drivers' spatio-temporal attentional distributions are influenced by vehicle dynamics and displayed point of view. ||| tyler n. morrison ||| emanuele rizzi ||| o. anil turkkan ||| richard j. jagacinski ||| haijun su ||| junmin wang ||| 
2019 ||| spatial attention model based target detection for aerial robotic systems. ||| meng zhang ||| shicheng wang ||| dongfang yang ||| yongfei li ||| hao he ||| 
2021 ||| transformers aftermath: current research and rising trends. ||| eduardo souza dos reis ||| cristiano andr |||  da costa ||| di ||| rgenes silveira ||| rodrigo bavaresco ||| rodrigo da rosa righi ||| jorge luis vict ||| ria barbosa ||| rodolfo stoffel antunes ||| m ||| rcio miguel gomes ||| gustavo federizzi ||| 
2018 ||| attention guided u-net for accurate iris segmentation. ||| sheng lian ||| zhiming luo ||| zhun zhong ||| xiang lin ||| songzhi su ||| shaozi li ||| 
2022 ||| attention integrated hierarchical networks for no-reference image quality assessment. ||| junyong you ||| jari korhonen ||| 
2021 ||| attention-based contextual interaction asymmetric network for rgb-d saliency prediction. ||| xinyue zhang ||| ting jin ||| wujie zhou ||| jingsheng lei ||| 
2021 ||| sign language recognition based on global-local attention. ||| shujun zhang ||| qun zhang ||| 
2020 ||| cross-level reinforced attention network for person re-identification. ||| min jiang ||| cong li ||| jun kong ||| zhende teng ||| danfeng zhuang ||| 
2019 ||| deep spatial attention hashing network for image retrieval. ||| lin-wei ge ||| jun zhang ||| yi xia ||| peng chen ||| bing wang ||| chun-hou zheng ||| 
2020 ||| ir saliency detection via a gcf-sb visual attention framework. ||| yufei zhao ||| yong song ||| xu li ||| muhammad sulaman ||| zhengkun guo ||| xin yang ||| fengning wang ||| qun hao ||| 
2019 ||| two-level attention with two-stage multi-task learning for facial emotion recognition. ||| xiaohua wang ||| muzi peng ||| lijuan pan ||| min hu ||| chunhua jin ||| fuji ren ||| 
2019 ||| weighted correlation filters guidance with spatial-temporal attention for online multi-object tracking. ||| sheng tian ||| lian zou ||| cian fan ||| liqiong chen ||| 
2020 ||| spatial-temporal saliency action mask attention network for action recognition. ||| min jiang ||| na pan ||| jun kong ||| 
2021 ||| spatial self-attention network with self-attention distillation for fine-grained image recognition. ||| adu asare baffour ||| zhen qin ||| yong wang ||| zhiguang qin ||| kim-kwang raymond choo ||| 
2021 ||| attention-guided image captioning with adaptive global and local feature fusion. ||| xian zhong ||| guozhang nie ||| wenxin huang ||| wenxuan liu ||| bo ma ||| chia-wen lin ||| 
2022 ||| dca-cyclegan: unsupervised single image dehazing using dark channel attention optimized cyclegan. ||| yaozong mo ||| chaofeng li ||| yuhui zheng ||| xiaojun wu ||| 
2021 ||| sequential alignment attention model for scene text recognition. ||| yan wu ||| jiaxin fan ||| renshuai tao ||| jiakai wang ||| haotong qin ||| aishan liu ||| xianglong liu ||| 
2021 ||| stable self-attention adversarial learning for semi-supervised semantic image segmentation. ||| jia zhang ||| zhixin li ||| canlong zhang ||| huifang ma ||| 
2020 ||| exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-identification. ||| yanbing geng ||| yongjian lian ||| mingliang zhou ||| yixue kong ||| yinong zhu ||| 
2019 ||| a multiscale dilated dense convolutional network for saliency prediction with instance-level attention competition. ||| hao li ||| fei qi ||| guangming shi ||| chunhuan lin ||| 
2021 ||| image super-resolution based on deep neural network of multiple attention mechanism. ||| xin yang ||| xiaochuan li ||| zhiqiang li ||| dake zhou ||| 
2021 ||| attention guided feature pyramid network for crowd counting. ||| huanpeng chu ||| jilin tang ||| haoji hu ||| 
2021 ||| a bottom-up and top-down human visual attention approach for hyperspectral anomaly detection. ||| ashkan taghipour ||| hassan ghassemian ||| 
2021 ||| residual attention-based tracking-by-detection network with attention-driven data augmentation. ||| zaifeng shi ||| cheng sun ||| qingjie cao ||| zhe wang ||| qiangqiang fan ||| 
2019 ||| multimodal activity recognition with local block cnn and attention-based spatial weighted cnn. ||| suguo zhu ||| zhenying fang ||| yi wang ||| jun yu ||| junping du ||| 
2021 ||| multi-view motion modelled deep attention networks (m2da-net) for video based sign language recognition. ||| suneetha m. ||| venkata durga prasad mareedu ||| p. v. v. kishore ||| 
2020 ||| local relation network with multilevel attention for visual question answering. ||| bo sun ||| zeng yao ||| yinghui zhang ||| lejun yu ||| 
2019 ||| an extensive evaluation of deep featuresof convolutional neural networks for saliency prediction of human visual attention. ||| ali mahdi ||| jun qin ||| 
2021 ||| gaze estimation via bilinear pooling-based attention networks. ||| dakai ren ||| jiazhong chen ||| jian zhong ||| zhaoming lu ||| tao jia ||| zongyi li ||| 
2021 ||| shape transformer nets: generating viewpoint-invariant 3d shapes from a single image. ||| jinglun yang ||| youhua li ||| lu yang ||| 
2021 ||| single image deblurring with cross-layer feature fusion and consecutive attention. ||| yaowei li ||| ye luo ||| guokai zhang ||| jianwei lu ||| 
2021 ||| multi-scale attention network for image super-resolution. ||| li wang ||| jie shen ||| e. tang ||| shengnan zheng ||| lizhong xu ||| 
2022 ||| cross-layer progressive attention bilinear fusion method for fine-grained visual classification. ||| chaoqing wang ||| yurong qian ||| weijun gong ||| junjong cheng ||| yongqiang wang ||| yuefei wang ||| 
2017 ||| exploring visual attention using random walks based eye tracking protocols. ||| xiu chen ||| zhenzhong chen ||| 
2021 ||| blind image quality assessment with channel attention based deep residual network and extended largevis dimensionality reduction. ||| han han ||| li zhuo ||| jiafeng li ||| jing zhang ||| meng wang ||| 
2021 ||| mranet: multi-atrous residual attention network for stereo image super-resolution. ||| luyao ning ||| anhong wang ||| lijun zhao ||| weimin xue ||| donghan bu ||| 
2022 ||| attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing. ||| yan liu ||| hassan al-shehari ||| hongying zhang ||| 
2021 ||| multiple attention encoded cascade r-cnn for scene text detection. ||| yirui wu ||| wenxiang liu ||| shaohua wan ||| 
2021 ||| role-based attention in deep reinforcement learning for games. ||| dong yang ||| wenjing yang ||| minglong li ||| qiong yang ||| 
2021 ||| aff-dehazing: attention-based feature fusion network for low-light image dehazing. ||| yu zhou ||| zhihua chen ||| bin sheng ||| ping li ||| jinman kim ||| enhua wu ||| 
2019 ||| classification and diagnosis of thyroid carcinoma using reinforcement residual network with visual attention mechanisms in ultrasound images. ||| yanming zhang ||| 
2019 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| swathikiran sudhakaran ||| oswald lanz ||| 
2019 ||| focusing attention in populations of semi-autonomously operating sensing nodes. ||| hanno hildmann ||| miguel almeida ||| abdel f. isakovic ||| fabrice saffre ||| 
2020 ||| at first sight: robots' subtle eye movement parameters affect human attentional engagement, spontaneous attunement and perceived human-likeness. ||| davide ghiglino ||| cesco willemse ||| davide de tommaso ||| francesco bossi ||| agnieszka wykowska ||| 
2017 ||| a customized attention-based long short-term memory network for distant supervised relation extraction. ||| dengchao he ||| hongjun zhang ||| wenning hao ||| rui zhang ||| kai cheng ||| 
2021 ||| real-time decoding of attentional states using closed-loop eeg neurofeedback. ||| greta tuckute ||| sofie therese hansen ||| troels wesenberg kjaer ||| lars kai hansen ||| 
2022 ||| spatial attention enhances crowded stimulus encoding across modeled receptive fields by increasing redundancy of feature representations. ||| justin d. theiss ||| joel d. bowen ||| michael a. silver ||| 
2021 ||| emergence of content-agnostic information processing by a robot using active inference, visual attention, working memory, and planning. ||| jeffrey frederic quei ||| er ||| minju jung ||| takazumi matsumoto ||| jun tani ||| 
2021 ||| flexible working memory through selective gating and attentional tagging. ||| wouter kruijne ||| sander m. boht ||| pieter r. roelfsema ||| christian n. l. olivers ||| 
2020 ||| fine-grained 3d-attention prototypes for few-shot learning. ||| xin hu ||| jun liu ||| jie ma ||| yudai pan ||| lingling zhang ||| 
2020 ||| semantic vector learning using pretrained transformers in natural language understanding. ||| sangkeun jung ||| 
2017 ||| computational visual attention models. ||| milind s. gide ||| lina j. karam ||| 
2021 ||| mutualrec: joint friend and item recommendations with mutualistic attentional graph neural networks. ||| yang xiao ||| qingqi pei ||| tingting xiao ||| lina yao ||| huan liu ||| 
2020 ||| a novel syntax-aware automatic graphics code generation with attention-based deep neural network. ||| xiong wen pang ||| yanqiang zhou ||| pengcheng li ||| weiwei lin ||| wentai wu ||| james z. wang ||| 
2022 ||| transformers for gui testing: a plausible solution to automated test case generation and flaky tests. ||| zubair khaliq ||| sheikh umar farooq ||| dawood ashraf khan ||| 
2017 ||| are they paying attention? a model-based method to identify individuals' mental states. ||| tongda zhang ||| renate fruchter ||| maria frank ||| 
2021 ||| ai ethics: a long history and a recent burst of attention. ||| jason borenstein ||| frances s. grodzinsky ||| ayanna m. howard ||| keith w. miller ||| marty j. wolf ||| 
2020 ||| design of dry-type transformer temperature controller based on internet of things. ||| yan leng ||| jian qi ||| yepeng liu ||| fujian zhu ||| 
2021 ||| multi-level spatial attention network for image data segmentation. ||| jun guo ||| zhixiong jiang ||| dingchao jiang ||| 
2021 ||| an alzheimer's disease identification and classification model based on the convolutional neural network with attention mechanisms. ||| yin chen ||| 
2021 ||| medical big data analysis with attention and large margin loss model for skin lesion application. ||| jing wu ||| hong guo ||| yuan wen ||| wei hu ||| yining li ||| tianyi liu ||| xiaoming liu ||| 
2021 ||| multimodal emotion recognition from art using sequential co-attention. ||| tsegaye misikir tashu ||| sakina hajiyeva ||| tom ||| s horv ||| th ||| 
2020 ||| attention-based fully gated cnn-bgru for russian handwritten text. ||| abdelrahman abdallah ||| mohamed hamada ||| daniyar b. nurseitov ||| 
2021 ||| skeleton-based attention mask for pedestrian attribute recognition network. ||| sorn sooksatra ||| sitapa rujikietgumjorn ||| 
2021 ||| to grasp the world at a glance: the role of attention in visual and semantic associative processing. ||| nurit gronau ||| 
2018 ||| macrobase: prioritizing attention in fast data. ||| firas abuzaid ||| peter bailis ||| jialin ding ||| edward gan ||| samuel madden ||| deepak narayanan ||| kexin rong ||| sahaana suri ||| 
2019 ||| multistep flow prediction on car-sharing systems: a multi-graph convolutional neural network with attention mechanism. ||| hongming zhu ||| yi luo ||| qin liu ||| hongfei fan ||| tianyou song ||| chang wu yu ||| bowen du ||| 
2020 ||| enabling reliability-driven optimization selection with gate graph attention neural network. ||| jiang wu ||| jianjun xu ||| xiankai meng ||| haoyu zhang ||| zhuo zhang ||| 
2020 ||| split attention pointer network for source code language modeling. ||| zhimin zhou ||| zhongwen chen ||| 
2021 ||| an effective method of evaluating pension service quality using multi-dimension attention convolutional neural networks. ||| chunshan li ||| yuanyuan wang ||| dongmei li ||| dianhui chu ||| mingxiao ma ||| 
2020 ||| opinion dynamics with topological gossiping: asynchronous updates under limited attention. ||| wilbert samuel rossi ||| paolo frasca ||| 
2018 ||| transformer semantics. ||| georg struth ||| 
2020 ||| a deep attention-based ensemble network for real-time face hallucination. ||| dongdong liu ||| jincai chen ||| zhenxing huang ||| ni zeng ||| ping lu ||| lin yang ||| haofeng wang ||| jinqiao kou ||| min wu ||| 
2022 ||| event camera simulator design for modeling attention-based inference architectures. ||| md jubaer hossain pantho ||| joel mandebi mbongue ||| pankaj bhowmik ||| christophe bobda ||| 
2021 ||| efficient unsupervised monocular depth estimation using attention guided generative adversarial network. ||| sumanta bhattacharyya ||| ju shen ||| stephen welch ||| chen chen ||| 
2021 ||| two-pathway attention network for real-time facial expression recognition. ||| lining wang ||| zheng he ||| bin meng ||| kai liu ||| qingyu dou ||| xiaomin yang ||| 
2021 ||| an improved one-stage pedestrian detection method based on multi-scale attention feature extraction. ||| jun ma ||| honglin wan ||| junxia wang ||| hao xia ||| chengjie bai ||| 
2021 ||| fast contour detection with supervised attention learning. ||| rufeng zhang ||| mingyu you ||| 
2019 ||| graph convolutional network with sequential attention for goal-oriented dialogue systems. ||| suman banerjee ||| mitesh m. khapra ||| 
2021 ||| augmenting transformers with knn-based composite memory for dialog. ||| angela fan ||| claire gardent ||| chlo |||  braud ||| antoine bordes ||| 
2021 ||| recursive non-autoregressive graph-to-graph transformer for dependency parsing with iterative refinement. ||| alireza mohammadshahi ||| james henderson ||| 
2021 ||| sparse, dense, and attentional representations for text retrieval. ||| yi luan ||| jacob eisenstein ||| kristina toutanova ||| michael collins ||| 
2020 ||| theoretical limitations of self-attention in neural sequence models. ||| michael hahn ||| 
2019 ||| attention-passing models for robust and data-efficient end-to-end speech translation. ||| matthias sperber ||| graham neubig ||| jan niehues ||| alex waibel ||| 
2018 ||| video captioning with multi-faceted attention. ||| xiang long ||| chuang gan ||| gerard de melo ||| 
2021 ||| efficient content-based sparse attention with routing transformers. ||| aurko roy ||| mohammad saffar ||| ashish vaswani ||| david grangier ||| 
2021 ||| extractive opinion summarization in quantized transformer spaces. ||| stefanos angelidis ||| reinald kim amplayo ||| yoshihiko suhara ||| xiaolan wang ||| mirella lapata ||| 
2020 ||| amr-to-text generation with graph transformer. ||| tianming wang ||| xiaojun wan ||| hanqi jin ||| 
2017 ||| overcoming language variation in sentiment analysis with social attention. ||| yi yang ||| jacob eisenstein ||| 
2018 ||| attentive convolution: equipping cnns with rnn-style attention mechanisms. ||| wenpeng yin ||| hinrich sch ||| tze ||| 
2021 ||| decoupling the role of data, attention, and losses in multimodal transformers. ||| lisa anne hendricks ||| john mellor ||| rosalia schneider ||| jean-baptiste alayrac ||| aida nematzadeh ||| 
2020 ||| target-guided structured attention network for target-dependent sentiment analysis. ||| ji zhang ||| chengyao chen ||| pengfei liu ||| chao he ||| cane wing-ki leung ||| 
2021 ||| editor: an edit-based transformer with repositioning for neural machine translation with soft lexical constraints. ||| weijia xu ||| marine carpuat ||| 
2018 ||| developing joint attention for children with autism in robot-enhanced therapy. ||| daniel o. david ||| cristina a. costescu ||| silviu-andrei matu ||| aurora szentagotai tatar ||| anca dobrean ||| 
2021 ||| does context matter? effects of robot appearance and reliability on social attention differs based on lifelikeness of gaze task. ||| abdulaziz abubshait ||| patrick p. weis ||| eva wiese ||| 
2018 ||| directing attention through gaze hints improves task solving in human-humanoid interaction. ||| eunice njeri mwangi ||| emilia i. barakova ||| marta d ||| az boladeras ||| andreu catal |||  mallofr ||| matthias rauterberg ||| 
2021 ||| personalized robot interventions for autistic children: an automated methodology for attention assessment. ||| fady alnajjar ||| massimiliano lorenzo cappuccio ||| abdulrahman majed renawi ||| omar mubin ||| chu kiong loo ||| 
2021 ||| analysis of attention in child-robot interaction among children diagnosed with cognitive impairment. ||| luthffi idzhar ismail ||| fazah akhtar hanapiah ||| tony belpaeme ||| joni dambre ||| francis wyffels ||| 
2017 ||| role of speaker cues in attention inference. ||| jin joo lee ||| cynthia breazeal ||| david desteno ||| 
2017 ||| the attention schema theory: a foundation for engineering artificial consciousness. ||| michael s. a. graziano ||| 
2020 ||| repetitive robot behavior impacts perception of intentionality and gaze-related attentional orienting. ||| abdulaziz abubshait ||| agnieszka wykowska ||| 
2017 ||| probabilistic mapping of human visual attention from head pose estimation. ||| andrea veronese ||| mattia racca ||| roel s. pieters ||| ville kyrki ||| 
2021 ||| attention enhancement for exoskeleton-assisted hand rehabilitation using fingertip haptic stimulation. ||| min li ||| jiazhou chen ||| guoying he ||| lei cui ||| chaoyang chen ||| emanuele lindo secco ||| wei yao ||| jun xie ||| guanghua xu ||| helge a. wurdemann ||| 
2020 ||| improving ct image tumor segmentation through deep supervision and attentional gates. ||| alzbeta tureckova ||| tom ||| s turecek ||| zuzana kom ||| nkov |||  oplatkov ||| antonio jose rodr ||| guez-s ||| nchez ||| 
2021 ||| mind the eyes: artificial agents' eye movements modulate attentional engagement and anthropomorphic attribution. ||| davide ghiglino ||| cesco willemse ||| davide de tommaso ||| agnieszka wykowska ||| 
2018 ||| debugging translations of transformer-based neural machine translation systems. ||| matiss rikters ||| marcis pinnis ||| 
2021 ||| power transmission line anomaly detection scheme based on cnn-transformer model. ||| ming gao ||| wenfei zhang ||| 
2021 ||| research on lstm+attention model of infant cry classification. ||| tianye jian ||| yizhun peng ||| wanlong peng ||| zhou yang ||| 
2020 ||| crowd counting network with self-attention distillation. ||| yaoyao li ||| li wang ||| huailin zhao ||| zhen nie ||| 
2020 ||| visual attention-based comparative study on disaster detection from social media images. ||| arif ||| m. ashraful amin ||| amin ahsan ali ||| a. k. m. mahhubur rahman ||| 
2021 ||| investigating and modeling the web elements' visual feature influence on free-viewing attention. ||| sandeep vidyapu ||| vijaya saradhi vedula ||| samit bhattacharya ||| 
2020 ||| herg-att: self-attention-based deep neural network for predicting herg blockers. ||| hyunho kim ||| hojung nam ||| 
2021 ||| able: attention based learning for enzyme classification. ||| mohan vamsi nallapareddy ||| rohit dwivedula ||| 
2020 ||| attention-based generative adversarial network for semi-supervised image classification. ||| xuezhi xiang ||| zeting yu ||| ning lv ||| xiangdong kong ||| abdulmotaleb el-saddik ||| 
2020 ||| hierarchical temporal fusion of multi-grained attention features for video question answering. ||| shaoning xiao ||| yimeng li ||| yunan ye ||| long chen ||| shiliang pu ||| zhou zhao ||| jian shao ||| jun xiao ||| 
2020 ||| semantic image segmentation with improved position attention and feature fusion. ||| hegui zhu ||| yan miao ||| xiangde zhang ||| 
2021 ||| multi-object tracking method based on efficient channel attention and switchable atrous convolution. ||| xuezhi xiang ||| wenkai ren ||| yujian qiu ||| kaixu zhang ||| ning lv ||| 
2019 ||| adaptive syncretic attention for constrained image captioning. ||| liang yang ||| haifeng hu ||| 
2021 ||| attention-based deep gated fully convolutional end-to-end architectures for time series classification. ||| mehak khan ||| hongzhi wang ||| alladoumbaye ngueilbaye ||| 
2019 ||| image captioning with text-based visual attention. ||| chen he ||| haifeng hu ||| 
2022 ||| scale-insensitive object detection via attention feature pyramid transformer network. ||| lingling li ||| changwen zheng ||| cunli mao ||| haibo deng ||| taisong jin ||| 
2019 ||| image caption with endogenous-exogenous attention. ||| teng wang ||| haifeng hu ||| chen he ||| 
2021 ||| refine for semantic segmentation based on parallel convolutional network with attention model. ||| gang peng ||| shiqi yang ||| hao wang ||| 
2020 ||| refocused attention: long short-term rewards guided video captioning. ||| jiarong dong ||| ke gao ||| xiaokai chen ||| juan cao ||| 
2021 ||| parsbert: transformer-based model for persian language understanding. ||| mehrdad farahani ||| mohammad gharachorloo ||| marzieh farahani ||| mohammad manthouri ||| 
2019 ||| image captioning with bidirectional semantic attention-based guiding of long short-term memory. ||| pengfei cao ||| zhongyi yang ||| liang sun ||| yanchun liang ||| mary qu yang ||| renchu guan ||| 
2022 ||| an improved attention and hybrid optimization technique for visual question answering. ||| himanshu sharma ||| anand singh jalal ||| 
2021 ||| self-supervised monocular trained depth estimation using triplet attention and funnel activation. ||| xuezhi xiang ||| xiangdong kong ||| yujian qiu ||| kaixu zhang ||| ning lv ||| 
2022 ||| proposal-based graph attention networks for workflow detection. ||| min zhang ||| haiyang hu ||| zhongjin li ||| jie chen ||| 
2020 ||| traffic sign recognition in harsh environment using attention based convolutional pooling neural network. ||| jun ho chung ||| dong won kim ||| tae koo kang ||| myo taeg lim ||| 
2021 ||| multilevel attention models for drug target binding affinity prediction. ||| nassima aleb ||| 
2020 ||| deep dual-stream network with scale context selection attention module for semantic segmentation. ||| yifu liu ||| chenfeng xu ||| zhihong chen ||| chao chen ||| han zhao ||| xinyu jin ||| 
2021 ||| exposing deepfake videos using attention based convolutional lstm network. ||| yishan su ||| huawei xia ||| qi liang ||| weizhi nie ||| 
2019 ||| image captioning using region-based attention joint with time-varying attention. ||| weixuan wang ||| haifeng hu ||| 
2021 ||| siamese pre-trained transformer encoder for knowledge base completion. ||| mengyao li ||| bo wang ||| jing jiang ||| 
2017 ||| capturing temporal structures for video captioning by spatio-temporal contexts and channel attention mechanism. ||| dashan guo ||| wei li ||| xiangzhong fang ||| 
2020 ||| bi-directional lstm model with symptoms-frequency position attention for question answering system in medical domain. ||| mingwen bi ||| qingchuan zhang ||| min zuo ||| zelong xu ||| qingyu jin ||| 
2020 ||| 3d model retrieval using bipartite graph matching based on attention. ||| shanlin sun ||| yun li ||| yunfeng xie ||| zhicheng tan ||| xing yao ||| rongyao zhang ||| 
2020 ||| visual sentiment prediction with attribute augmentation and multi-attention mechanism. ||| zhuanghui wu ||| min meng ||| jigang wu ||| 
2021 ||| attention refined network for human pose estimation. ||| xiangyang wang ||| jiangwei tong ||| rui wang ||| 
2021 ||| image captioning with dense fusion connection and improved stacked attention module. ||| hegui zhu ||| ru wang ||| xiangde zhang ||| 
2019 ||| deep captioning with attention-based visual concept transfer mechanism for enriching description. ||| junxuan zhang ||| haifeng hu ||| 
2019 ||| multi-task character-level attentional networks for medical concept normalization. ||| jinghao niu ||| yehui yang ||| siheng zhang ||| zhengya sun ||| wensheng zhang ||| 
2020 ||| multi-layer attention based cnn for target-dependent sentiment classification. ||| suqi zhang ||| xinyun xu ||| yanwei pang ||| jungong han ||| 
2020 ||| attentive semantic and perceptual faces completion using self-attention generative adversarial networks. ||| xiaowei liu ||| kenli li ||| keqin li ||| 
2021 ||| high gain and low noise wideband folded-switching mixer employing transformer and complimentary cs/cg hybrid topology. ||| qiuzhen wan ||| yixuan xie ||| wenkui ji ||| zidie yan ||| 
2021 ||| automatic mandible segmentation from ct image using 3d fully convolutional neural network based on denseaspp and attention gates. ||| jiangchang xu ||| jiannan liu ||| dingzhong zhang ||| zijie zhou ||| xiaoyi jiang ||| chenping zhang ||| xiaojun chen ||| 
2022 ||| gca-net: global context attention network for intestinal wall vascular segmentation. ||| sheng li ||| xueting kong ||| cheng lu ||| jinhui zhu ||| xiongxiong he ||| ruibiao fu ||| 
2021 ||| lightweight pyramid network with spatial attention mechanism for accurate retinal vessel segmentation. ||| tengfei tan ||| zhilun wang ||| hongwei du ||| jinzhang xu ||| bensheng qiu ||| 
2019 ||| memory-efficient 2.5d convolutional transformer networks for multi-modal deformable registration with weak label supervision applied to whole-heart ct and mri scans. ||| alessa hering ||| sven kuckertz ||| stefan heldmann ||| mattias p. heinrich ||| 
2021 ||| sha-mtl: soft and hard attention multi-task learning for automated breast cancer ultrasound image segmentation and classification. ||| guisheng zhang ||| kehui zhao ||| yanfei hong ||| xiaoyu qiu ||| kuixing zhang ||| benzheng wei ||| 
2021 ||| duda-net: a double u-shaped dilated attention network for automatic infection area segmentation in covid-19 lung ct images. ||| feng xie ||| zheng huang ||| zhengjin shi ||| tianyu wang ||| guoli song ||| bolun wang ||| zihong liu ||| 
2021 ||| 3d axial-attention for lung nodule classification. ||| mundher al-shabi ||| kelvin shak ||| maxine tan ||| 
2021 ||| neural linguistic steganalysis via multi-head self-attention. ||| saimei jiao ||| hai-feng wang ||| kun zhang ||| ya-qi hu ||| 
2019 ||| power transformer fault severity estimation based on dissolved gas analysis and energy of fault formation technique. ||| edwell t. mharakurwa ||| george nyauma nyakoe ||| aloys oriedi akumu ||| 
2021 ||| non-local attention based cnn model for aspect extraction. ||| dangguo shao ||| mingfang zhang ||| yan xiang rong ||| ting lu ||| 
2021 ||| residual network for deep reinforcement learning with attention mechanism. ||| hanhua zhu ||| tomoyuki kaneko ||| 
2022 ||| an explainable multi-modal hierarchical attention model for developing phishing threat intelligence. ||| yidong chai ||| yonghang zhou ||| weifeng li ||| yuanchun jiang ||| 
2021 ||| datingsec: detecting malicious accounts in dating apps using a content-based attention network. ||| xinlei he ||| qingyuan gong ||| yang chen ||| yang zhang ||| xin wang ||| xiaoming fu ||| 
2021 ||| monte carlo denoising via auxiliary feature guided self-attention. ||| jiaqi yu ||| yongwei nie ||| chengjiang long ||| wenjun xu ||| qing zhang ||| guiqing li ||| 
2019 ||| deepremaster: temporal source-reference attention networks for comprehensive video enhancement. ||| satoshi iizuka ||| edgar simo-serra ||| 
2021 ||| transflower: probabilistic autoregressive dance generation with multimodal attention. ||| guillermo valle p ||| rez ||| gustav eje henter ||| jonas beskow ||| andre holzapfel ||| pierre-yves oudeyer ||| simon alexanderson ||| 
2020 ||| a multimodal interlocutor-modulated attentional blstm for classifying autism subgroups during clinical interviews. ||| yun-shao lin ||| susan shur-fen gau ||| chi-chun lee ||| 
2020 ||| oidc-net: omnidirectional image distortion correction via coarse-to-fine region attention. ||| kang liao ||| chunyu lin ||| yao zhao ||| moncef gabbouj ||| yang zheng ||| 
2021 ||| lightweight tensor attention-driven convlstm neural network for hyperspectral image classification. ||| wen-shuai hu ||| heng-chao li ||| yang-jun deng ||| xian sun ||| qian du ||| antonio plaza ||| 
2021 ||| attention-based neural networks for chroma intra prediction in video coding. ||| marc g ||| rriz blanch ||| saverio g. blasi ||| alan f. smeaton ||| noel e. o'connor ||| marta mrak ||| 
2020 ||| gan-generated image detection with self-attention mechanism against gan generator defect. ||| zhongjie mi ||| xinghao jiang ||| tanfeng sun ||| ke xu ||| 
2017 ||| hybrid ctc/attention architecture for end-to-end speech recognition. ||| shinji watanabe ||| takaaki hori ||| suyoun kim ||| john r. hershey ||| tomoki hayashi ||| 
2020 ||| where is the model looking at? - concentrate and explain the network attention. ||| wenjia xu ||| jiuniu wang ||| yang wang ||| guangluan xu ||| daoyu lin ||| wei dai ||| yirong wu ||| 
2020 ||| automatic assessment of depression from speech via a hierarchical attention transfer network and attention autoencoders. ||| ziping zhao ||| zhongtian bao ||| zixing zhang ||| jun deng ||| nicholas cummins ||| haishuai wang ||| jianhua tao ||| bj ||| rn w. schuller ||| 
2020 ||| statistical modeling of visual attention of junior and senior anesthesiologists during the induction of general anesthesia in real and simulated cases. ||| tobias grundgeiger ||| thomas wurmb ||| oliver happel ||| 
2021 ||| sequential weakly labeled multiactivity localization and recognition on wearable sensors using recurrent attention networks. ||| kun wang ||| jun he ||| lei zhang ||| 
2019 ||| electroencephalographic phase-amplitude coupling in simulated driving with varying modality-specific attentional demand. ||| ernesto gonzalez-trejo ||| hannes m ||| gele ||| norbert pfleger ||| ronny hannemann ||| daniel j. strauss ||| 
2022 ||| eeg-based auditory attention detection via frequency and channel neural attention. ||| siqi cai ||| enze su ||| longhan xie ||| haizhou li ||| 
2022 ||| binocular feature fusion and spatial attention mechanism based gaze tracking. ||| lihong dai ||| jinguo liu ||| zhaojie ju ||| 
2020 ||| an ego-vision system for discovering human joint attention. ||| yifei huang ||| minjie cai ||| yoichi sato ||| 
2018 ||| design, development, and evaluation of a noninvasive autonomous robot-mediated joint attention intervention system for young children with asd. ||| zhi zheng ||| huan zhao ||| amy swanson ||| amy weitlauf ||| zachary e. warren ||| nilanjan sarkar ||| 
2019 ||| drivers' attentional instability on a winding roadway. ||| richard j. jagacinski ||| emanuele rizzi ||| benjamin j. bloom ||| o. anil turkkan ||| tyler n. morrison ||| haijun su ||| junmin wang ||| 
2018 ||| eye tracking the visual attention of nurses interpreting simulated vital signs scenarios: mining metrics to discriminate between performance level. ||| jonathan currie ||| raymond r. bond ||| paul j. mccullagh ||| pauline black ||| dewar d. finlay ||| aaron j. peace ||| 
2021 ||| risk assessment algorithm for power transformer fleets based on condition and strategic importance. ||| diego a. zaldivar ||| andr ||| s a. romero ||| sergio rivera-rodriguez ||| 
2021 ||| a domain adaptive person re-identification based on dual attention mechanism and camstyle transfer. ||| chengyan zhong ||| guanqiu qi ||| neal mazur ||| sarbani banerjee ||| devanshi malaviya ||| gang hu ||| 
2021 ||| sequential recommendation through graph neural networks and transformer encoder with degree encoding. ||| shuli wang ||| xuewen li ||| xiaomeng kou ||| jin zhang ||| shaojie zheng ||| jinlong wang ||| jibing gong ||| 
2020 ||| mdan-unet: multi-scale and dual attention enhanced nested u-net architecture for segmentation of optical coherence tomography images. ||| wen liu ||| yankui sun ||| qingge ji ||| 
2021 ||| closed-loop cognitive-driven gain control of competing sounds using auditory attention decoding. ||| ali aroudi ||| eghart fischer ||| maja serman ||| henning puder ||| simon doclo ||| 
2021 ||| a multinomial dga classifier for incipient fault detection in oil-impregnated power transformers. ||| george odongo ||| richard musabe ||| damien hanyurwimfura ||| 
2021 ||| pm2.5 concentration prediction based on cnn-bilstm and attention mechanism. ||| jinsong zhang ||| yongtao peng ||| bo ren ||| taoying li ||| 
2021 ||| an alternative explanation for attribute framing and spillover effects in multidimensional supplier evaluation and supplier termination: focusing on asymmetries in attention. ||| ricky s. wong ||| 
2018 ||| recommendation of site commissioning tests for rapid recovery transformers with an installation time less than 30 hours. ||| stefan riegler ||| ewald schweiger ||| christian ettl ||| martin st ||| ssl ||| sanjay bose ||| 
2022 ||| online dissolved gas analysis used for transformers - possibilities, experiences, and limitations. ||| christof riedmann ||| uwe schichler ||| wolfgang h ||| usler ||| wolfgang neuhold ||| 
2018 ||| measurement of thermal behavior of an ester-filled power transformer at ultra-low temperatures. ||| florian bachinger ||| peter hamberger ||| 
2020 ||| field experience of small quasi-dc bias on power transformers. ||| d. albert ||| p. schachinger ||| herwig renner ||| peter hamberger ||| f. klammer ||| g. achleitner ||| 
2018 ||| adhesives for bonding transformerboard: partial discharge and ageing behaviour. ||| christoph m ||| ller ||| z. zhang ||| r. schwarz ||| g. pukel ||| f. wiesbrock ||| b. bakija ||| michael muhr ||| 
2018 ||| geomagnetically induced currents modelling and monitoring transformer neutral currents in austria. ||| thomas halbedl ||| herwig renner ||| georg achleitner ||| 
2020 ||| verification of withstand capability for very fast transients of a 200 mva, 500 kv gsu transformer by modelling and testing. ||| a. rabel ||| j.-j. zhou ||| 
2020 ||| evaluation of dynamic loading capability for optimal loading strategies of power transformers. ||| irina lupandina ||| wolfgang gawlik ||| maria schrammel ||| a. ilgevicius ||| m. k ||| rten ||| k. viereck ||| 
2022 ||| mlfc-net: a multi-level feature combination attention model for remote sensing scene classification. ||| deyi wang ||| chengkun zhang ||| min han ||| 
2021 ||| dual-input attention network for automatic identification of detritus from river sands. ||| shiping ge ||| cong wang ||| zhiwei jiang ||| huizhen hao ||| qing gu ||| 
2022 ||| semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with deeplabv3+. ||| zhimin wang ||| jiasheng wang ||| kun yang ||| limeng wang ||| fanjie su ||| xinya chen ||| 
2021 ||| strip pooling channel spatial attention network for the segmentation of cloud and cloud shadow. ||| yi qu ||| min xia ||| yonghong zhang ||| 
2019 ||| improving bug detection via context-based code representation learning and attention-based neural networks. ||| yi li ||| shaohua wang ||| tien n. nguyen ||| son van nguyen ||| 
2019 ||| a predicate transformer semantics for effects (functional pearl). ||| wouter swierstra ||| tim baanen ||| 
2020 ||| bio-semantic relation extraction with attention-based external knowledge reinforcement. ||| zhijing li ||| yuchen lian ||| xiaoyong ma ||| xiangrong zhang ||| chen li ||| 
2022 ||| ggatlda: lncrna-disease association prediction based on graph-level graph attention network. ||| li wang ||| cheng zhong ||| 
2021 ||| binding affinity prediction for protein-ligand complex using deep attention mechanism based on intermolecular interactions. ||| sangmin seo ||| jonghwan choi ||| sanghyun park ||| jaegyoon ahn ||| 
2018 ||| biomedical event extraction based on gru integrating attention mechanism. ||| lishuang li ||| jia wan ||| jieqiong zheng ||| jian wang ||| 
2019 ||| chemical-induced disease relation extraction via attention-based distant supervision. ||| jinghang gu ||| fuqing sun ||| longhua qian ||| guodong zhou ||| 
2020 ||| incorporating representation learning and multihead attention to improve biomedical cross-sentence n-ary relation extraction. ||| di zhao ||| jian wang ||| yi-jia zhang ||| xin wang ||| hongfei lin ||| zhihao yang ||| 
2019 ||| biomedical word sense disambiguation with bidirectional long short-term memory and attention-based neural networks. ||| canlin zhang ||| daniel bis ||| xiuwen liu ||| zhe he ||| 
2022 ||| a-prot: protein structure modeling using msa transformer. ||| yiyu hong ||| juyong lee ||| junsu ko ||| 
2020 ||| hybrid attentional memory network for computational drug repositioning. ||| jieyue he ||| xinxing yang ||| zhuo gong ||| ibrahim zamit ||| 
2021 ||| identifying pirna targets on mrnas in c. elegans using a deep multi-head attention network. ||| tzu-hsien yang ||| sheng-cian shiue ||| kuan-yu chen ||| yan yuan tseng ||| wei-sheng wu ||| 
2021 ||| gat-li: a graph attention network based learning and interpreting method for functional brain network classification. ||| jinlong hu ||| lijie cao ||| tenghui li ||| shoubin dong ||| ping li ||| 
2021 ||| an efficient scrna-seq dropout imputation method using graph attention network. ||| chenyang xu ||| lei cai ||| jingyang gao ||| 
2021 ||| attentionddi: siamese attention-based deep learning method for drug-drug interaction predictions. ||| kyriakos schwarz ||| ahmed allam ||| nicolas andres perez gonzalez ||| michael krauthammer ||| 
2022 ||| lerna: transformer architectures for configuring error correction tools for short- and long-read genome sequencing. ||| atul sharma ||| pranjal jain ||| ashraf mahgoub ||| zihan zhou ||| kanak mahadik ||| somali chaterji ||| 
2021 ||| mathla: a robust framework for hla-peptide binding prediction integrating bidirectional lstm and multiple head attention mechanism. ||| yilin ye ||| jian wang ||| yunwan xu ||| yi wang ||| youdong pan ||| qi song ||| xing liu ||| ji wan ||| 
2020 ||| biomedical document triage using a hierarchical attention-based capsule network. ||| jian wang ||| mengying li ||| qishuai diao ||| hongfei lin ||| zhihao yang ||| yi-jia zhang ||| 
2017 ||| an attention-based effective neural model for drug-drug interactions extraction. ||| wei zheng ||| hongfei lin ||| ling luo ||| zhehuan zhao ||| zhengguang li ||| yijia zhang ||| zhihao yang ||| jian wang ||| 
2021 ||| jlan: medical code prediction via joint learning attention networks and denoising mechanism. ||| xingwang li ||| yijia zhang ||| faiz ul islam ||| deshi dong ||| hao wei ||| mingyu lu ||| 
2020 ||| assistant diagnosis with chinese electronic medical records based on cnn and bilstm with phrase-level and word-level attentions. ||| tong wang ||| ping xuan ||| zonglin liu ||| tiangang zhang ||| 
2019 ||| attention-based recurrent neural network for influenza epidemic prediction. ||| xianglei zhu ||| bofeng fu ||| yaodong yang ||| yu ma ||| jianye hao ||| siqi chen ||| shuang liu ||| tiegang li ||| sen liu ||| weiming guo ||| zhenyu liao ||| 
2019 ||| a hybrid self-attention deep learning framework for multivariate sleep stage classification. ||| ye yuan ||| kebin jia ||| fenglong ma ||| guangxu xun ||| yaqing wang ||| lu su ||| aidong zhang ||| 
2019 ||| adverse drug reaction detection via a multihop self-attention mechanism. ||| tongxuan zhang ||| hongfei lin ||| yuqi ren ||| liang yang ||| bo xu ||| zhihao yang ||| jian wang ||| yijia zhang ||| 
2019 ||| attention mechanism enhanced lstm with residual architecture and its application for protein-protein interaction residue pairs prediction. ||| jiale liu ||| xinqi gong ||| 
2019 ||| relation extraction between bacteria and biotopes from biomedical texts with attention mechanisms and domain-specific contextual representations. ||| amarin jettakul ||| duangdao wichadakul ||| peerapon vateekul ||| 
2021 ||| deepgrn: prediction of transcription factor binding site across cell-types using attention-based deep neural networks. ||| chen chen ||| jie hou ||| xiaowen shi ||| hua yang ||| james a. birchler ||| jianlin cheng ||| 
2022 ||| lmms reloaded: transformer-based sense embeddings for disambiguation and beyond. ||| daniel loureiro ||| al ||| pio m ||| rio jorge ||| jos |||  camacho-collados ||| 
2021 ||| enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning. ||| jinsong su ||| jialong tang ||| hui jiang ||| ziyao lu ||| yubin ge ||| linfeng song ||| deyi xiong ||| le sun ||| jiebo luo ||| 
2020 ||| protoattend: attention-based prototypical learning. ||| sercan  ||| mer arik ||| tomas pfister ||| 
2021 ||| attention is turing-complete. ||| jorge p ||| rez ||| pablo barcel ||| javier marinkovic ||| 
2020 ||| exploring the limits of transfer learning with a unified text-to-text transformer. ||| colin raffel ||| noam shazeer ||| adam roberts ||| katherine lee ||| sharan narang ||| michael matena ||| yanqi zhou ||| wei li ||| peter j. liu ||| 
2018 ||| attention to news and its dissemination on twitter: a survey. ||| claudia orellana-rodriguez ||| mark t. keane ||| 
2021 ||| an attention network based on feature sequences for cross-domain sentiment classification. ||| jiana meng ||| yu dong ||| yingchun long ||| dandan zhao ||| 
2021 ||| attention mechanism based lstm in classification of stressed speech under workload. ||| xiao yao ||| zhengyan sheng ||| min gu ||| haibin wang ||| ning xu ||| xiaofeng liu ||| 
2020 ||| knowledge-embodied attention for distantly supervised relation extraction. ||| kejun deng ||| xuemiao zhang ||| songtao ye ||| junfei liu ||| 
2022 ||| temporal link prediction in directed networks based on self-attention mechanism. ||| jinsong li ||| jianhua peng ||| shuxin liu ||| lintianran weng ||| cong li ||| 
2019 ||| an attention-gated convolutional neural network for sentence classification. ||| yang liu ||| lixin ji ||| ruiyang huang ||| tuosiyu ming ||| chao gao ||| jianpeng zhang ||| 
2019 ||| synergistic attention u-net for sublingual vein segmentation. ||| tingxiao yang ||| yuichiro yoshimura ||| akira morita ||| takao namiki ||| toshiya nakaguchi ||| 
2018 ||| word polarity attention in sentiment analysis. ||| yohei hiyama ||| hidekazu yanagimoto ||| 
2021 ||| choice modeling using dot-product attention mechanism. ||| mofei li ||| yutaka nakamura ||| hiroshi ishiguro ||| 
2021 ||| a hybrid ai approach for supporting clinical diagnosis of attention deficit hyperactivity disorder (adhd) in adults. ||| ilias tachmazidis ||| tianhua chen ||| marios adamou ||| grigoris antoniou ||| 
2019 ||| neural attention with character embeddings for hay fever detection from twitter. ||| jiahua du ||| sandra michalska ||| sudha subramani ||| hua wang ||| yanchun zhang ||| 
2019 ||| attention for web directory advertisements: a top-down or bottom-up process? ||| yaqin cao ||| qing-xing qu ||| vincent g. duffy ||| yi ding ||| 
2021 ||| mobile application user experience checklist: a tool to assess attention to core ux principles. ||| brianna richardson ||| marsha campbell-yeo ||| michael smit ||| 
2021 ||| effect of imperfect information and action automation on attentional allocation. ||| eug ||| nie avril ||| beno ||| t val ||| ry ||| jordan navarro ||| li ||| n wioland ||| julien cegarra ||| 
2021 ||| attention: theory, principles, models and applications. ||| christopher d. wickens ||| 
2021 ||| comparing youth engagement on the attentiontrip to the child attention network test. ||| swasti arora ||| colin r. mccormick ||| raymond m. klein ||| 
2017 ||| theory-based models of attention in visual workspaces. ||| kelly s. steelman ||| jason s. mccarley ||| christopher d. wickens ||| 
2021 ||| when preschoolers use tablets: the effect of educational serious games on children's attention development. ||| wen liu ||| liting tan ||| dan huang ||| nan chen ||| fang liu ||| 
2018 ||| using an eye-tracking approach to explore gender differences in visual attention and shopping attitudes in an online shopping environment. ||| yoonmin hwang ||| kun chang lee ||| 
2019 ||| a comparison of engagement between the attention network test and a videogame-like version, called the attentiontrip. ||| raymond m. klein ||| emily howes vallis ||| joseph d. chisholm ||| 
2019 ||| an sic-driven modular step-up converter with soft-switched module having 1: 1 turns ratio multiphase transformer for wind systems. ||| mehdi abbasi ||| john c. w. lam ||| 
2020 ||| a high step up sepic-based converter based on partly interleaved transformer. ||| shanshan gao ||| yijie wang ||| yueshi guan ||| dianguo xu ||| 
2018 ||| shielding technique for planar matrix transformers to suppress common-mode emi noise and improve efficiency. ||| chao fei ||| yuchen yang ||| qiang li ||| fred c. lee ||| 
2017 ||| a multilevel transformerless inverter employing ground connection between pv negative terminal and grid neutral point. ||| abhijit kadam ||| anshuman shukla ||| 
2018 ||| transformer-free, off-the-shelf electrical interface for low-voltage dc energy harvesting. ||| micka ||| l lallart ||| luong-vi ||| t phung ||| bertrand massot ||| 
2020 ||| scscn: a separated channel-spatial convolution net with attention for single-view reconstruction. ||| jiayi ma ||| hao zhang ||| peng yi ||| zhongyuan wang ||| 
2017 ||| a single-phase transformerless inverter with charge pump circuit concept for grid-tied pv applications. ||| jaber fallah ardashir ||| mehran sabahi ||| seyed hossein hosseini ||| frede blaabjerg ||| ebrahim babaei ||| gevork b. gharehpetian ||| 
2021 ||| novel virtual-ground single-stage single-inductor transformerless buck-boost inverter. ||| fazal akbar ||| honnyong cha ||| heung-geun kim ||| 
2018 ||| esi: a novel three-phase inverter with leakage current attenuation for transformerless pv systems. ||| xiaoqiang guo ||| yong yang ||| tieying zhu ||| 
2021 ||| reversible wideband hybrid model of two-winding transformer including the core nonlinearity and emtp implementation. ||| wenxia sima ||| daixiao peng ||| ming yang ||| potao sun ||| binyang zou ||| zhao xiong ||| 
2021 ||| multiscale convolutional attention network for predicting remaining useful life of machinery. ||| biao wang ||| yaguo lei ||| naipeng li ||| wenting wang ||| 
2017 ||| high-efficiency high-power-density llc converter with an integrated planar matrix transformer for high-output current applications. ||| chao fei ||| fred c. lee ||| qiang li ||| 
2022 ||| air-core-transformer-based solid-state fault-current limiter for bidirectional hvdc systems. ||| ming yang ||| xiaofeng wang ||| wenxia sima ||| tao yuan ||| potao sun ||| hui liu ||| 
2019 ||| adaptive power transformer lifetime predictions through machine learning and uncertainty modeling in nuclear power plants. ||| jose ignacio aizpurua ||| stephen d. j. mcarthur ||| brian g. stewart ||| brandon lambert ||| james g. cross ||| victoria m. catterson ||| 
2021 ||| front-end bidirectional symmetric bipolar outputs llc dc-transformer (dcx) for a half bridge class-d audio amplifier. ||| duo xu ||| guohua zhou ||| feiming liu ||| rui huang ||| hongbo ma ||| 
2020 ||| dc voltage control strategy of three-terminal medium-voltage power electronic transformer-based soft normally open points. ||| shaodi ouyang ||| jinjun liu ||| yue yang ||| xingxing chen ||| shuguang song ||| hongda wu ||| 
2020 ||| efficiency optimization of dc solid-state transformer for photovoltaic power systems. ||| haochen shi ||| huiqing wen ||| yihua hu ||| yong yang ||| yiwang wang ||| 
2018 ||| an improved dc solid state transformer based on switched capacitor and multiple-phase-shift shoot-through modulation for integration of lvdc energy storage system and mvdc distribution grid. ||| qiang song ||| biao zhao ||| jianguo li ||| wenhua liu ||| 
2022 ||| an advanced wideband model and a novel multitype insulation monitoring strategy for vsc-connected transformers based on common-mode impedance response. ||| geye lu ||| dayong zheng ||| pinjia zhang ||| 
2018 ||| concurrent voltage control and dispatch of active distribution networks by means of smart transformer and storage. ||| xiang gao ||| fabrizio sossan ||| konstantina christakou ||| mario paolone ||| marco liserre ||| 
2021 ||| attention recurrent neural network-based severity estimation method for interturn short-circuit fault in permanent magnet synchronous machines. ||| hojin lee ||| hyeyun jeong ||| gyogwon koo ||| jaepil ban ||| sang woo kim ||| 
2020 ||| three-step switching frequency selection criteria for the generalized cllc-type dc transformer in hybrid ac-dc microgrid. ||| jingjing huang ||| xin zhang ||| zhe zhang ||| 
2020 ||| improving current transformer-based energy extraction from ac power lines by manipulating magnetic field. ||| yuan zhuang ||| chen xu ||| chaoyun song ||| anqi chen ||| wei lee ||| yi huang ||| jiafeng zhou ||| 
2017 ||| current transformer saturation detection using morphological gradient and morphological decomposition and its hardware implementation. ||| tianyao ji ||| mengjie shi ||| mengshi li ||| lu-liang zhang ||| qinghua wu ||| 
2021 ||| a novel interleaved high step-up converter with built-in transformer voltage multiplier cell. ||| tohid nouri ||| naser vosoughi ||| mahdi shaneh ||| 
2017 ||| an improved zero-current-switching single-phase transformerless pv h6 inverter with switching loss-free. ||| hua f. xiao ||| li zhang ||| yanqing li ||| 
2017 ||| analysis and design of an electronic on-load tap changer distribution transformer for automatic voltage regulation. ||| josemar de oliveira quevedo ||| fabricio emmanuel cazakevicius ||| rafael concatto beltrame ||| tiago bandeira marchesan ||| leandro michels ||| cassiano rech ||| luciano schuch ||| 
2022 ||| an mvdc-based meshed hybrid microgrid enabled using smart transformers. ||| hrishikesan v. m. ||| chandan kumar ||| marco liserre ||| 
2020 ||| protection of single-phase fault at the transformer valve side of fb-mmc-based bipolar hvdc systems. ||| wei liu ||| gen li ||| jun liang ||| carlos e. ugalde-loo ||| chuanyue li ||| xavier guillaud ||| 
2017 ||| modulation technique for single-phase transformerless photovoltaic inverters with reactive power capability. ||| tan kheng suan freddy ||| june-hee lee ||| hyun-cheol moon ||| kyo-beum lee ||| nasrudin abd. rahim ||| 
2019 ||| modeling and hierarchical structure based model predictive control of cascaded flying capacitor bridge multilevel converter for active front-end rectifier in solid-state transformer. ||| si-hwan kim ||| yeong-hyeok jang ||| rae-young kim ||| 
2017 ||| uk-derived transformerless common-grounded pv microinverter in ccm. ||| vasav gautam ||| parthasarathi sensarma ||| 
2017 ||| highly efficient and reliable sic-based dc-dc converter for smart transformer. ||| levy ferreira costa ||| giampaolo buticchi ||| marco liserre ||| 
2017 ||| multilevel mvdc link strategy of high-frequency-link dc transformer based on switched capacitor for mvdc power distribution. ||| yu wang ||| qiang song ||| qianhao sun ||| biao zhao ||| jianguo li ||| wenhua liu ||| 
2017 ||| current-feed single-switch forward resonant dc transformer (dcx) with secondary diode-clamping. ||| wei qin ||| xinke wu ||| junming zhang ||| 
2022 ||| partial-power post-regulated llc resonant dc transformer. ||| jinpei duan ||| donglai zhang ||| runan gu ||| 
2019 ||| comparative analysis of multiple active bridge converters configurations in modular smart transformer. ||| levy ferreira costa ||| felix hoffmann ||| giampaolo buticchi ||| marco liserre ||| 
2020 ||| multitransformer primary-side regulated flyback converter for supplying isolated igbt and mosfet drivers. ||| maciej kolincio ||| piotr j. chrzan ||| piotr musznicki ||| 
2022 ||| input current step-doubling for autotransformer-based 12-pulse rectifier using two auxiliary diodes. ||| jingfang wang ||| anchen chen ||| xuliang yao ||| qi guan ||| qiming chen ||| 
2020 ||| a multiphysics design and optimization method for air-core planar transformers in high-frequency llc resonant converters. ||| godwin kwun yuan ho ||| yaoran fang ||| bryan m. h. pong ||| 
2021 ||| solid-state transformer based fast charging station for various categories of electric vehicles with batteries of vastly different ratings. ||| arun chandrasekharan nair ||| baylon g. fernandes ||| 
2018 ||| analysis and stabilization of a smart transformer-fed grid. ||| zhi-xiang zou ||| giampaolo buticchi ||| marco liserre ||| 
2022 ||| temporal attention convolutional neural network for estimation of icing probability on wind turbine blades. ||| xu cheng ||| fan shi ||| meng zhao ||| guoyuan li ||| houxiang zhang ||| shengyong chen ||| 
2017 ||| time-varying and constant switching frequency-based sliding-mode control methods for transformerless dvr employing half-bridge vsi. ||| hasan komurcugil ||| samet biricik ||| 
2018 ||| modular transformer-based regenerative-cascaded multicell converter for drives with multilevel voltage operation at both input and output sides. ||| shambhu sau ||| saikat karmakar ||| baylon g. fernandes ||| 
2020 ||| model predictive control for dual active bridge in naval dc microgrids supplying pulsed power loads featuring fast transition and online transformer current minimization. ||| qian xiao ||| linglin chen ||| hongjie jia ||| patrick w. wheeler ||| tomislav dragicevic ||| 
2020 ||| graph-theory-based modeling and control for system-level optimization of smart transformers. ||| marco liserre ||| vivek raveendran ||| markus andresen ||| 
2018 ||| intermediate voltage variation-based interconnecting transformer design for voltage and phase angle control with coupled field fea studies. ||| rajaram tukaram ugale ||| kedar deepak mejari ||| bhalchandra nemichand chaudhari ||| 
2020 ||| a new single-phase transformerless grid-connected inverter with boosting ability and common ground feature. ||| naser vosoughi ||| seyed hossein hosseini ||| mehran sabahi ||| 
2018 ||| improved fault-tolerant method and control strategy based on reverse charging for the power electronic traction transformer. ||| jianqiang liu ||| nan zhao ||| 
2019 ||| robust circuit parameters design for the cllc-type dc transformer in the hybrid ac-dc microgrid. ||| jingjing huang ||| xin zhang ||| zhikang shuai ||| xinan zhang ||| peng wang ||| leong hai koh ||| jianfang xiao ||| xiangqian tong ||| 
2017 ||| isolated ac-dc converter using medium frequency transformer for off-shore wind turbine dc collection grid. ||| harish s. krishnamoorthy ||| michael t. daniel ||| jorge ramos-ruiz ||| prasad enjeti ||| liming liu ||| eddy aeloiza ||| 
2019 ||| neutral current minimization control for solid state transformers under unbalanced loads in distribution systems. ||| junru chen ||| tao yang ||| cathal o'loughlin ||| terence o'donnell ||| 
2021 ||| noniterative design of litz-wire high-frequency gapped-transformer (lw-hfgt) for llc converters based on optimal core-geometry factor model (okgm). ||| daniyal ahmed ||| li wang ||| 
2019 ||| zero-voltage switching operation of transformer class-e inverter at any coupling coefficient. ||| agasthya ayachit ||| fabio corti ||| alberto reatti ||| marian k. kazimierczuk ||| 
2020 ||| cascaded transformer multilevel inverters with asymmetrical turns ratios based on npc. ||| filipe ant ||| nio da costa bahia ||| cursino brand ||| o jacobina ||| nady rocha ||| reuben palmer rezende de sousa ||| 
2020 ||| condition monitoring of transformer breather using a capacitive moisture sensor. ||| shailesh kumar ||| lokesh kumar ||| tarikul islam ||| kuldeep kumar raina ||| 
2018 ||| a bidirectional bridge modular switched-capacitor-based power electronics transformer. ||| liangzong he ||| jianhuan zhang ||| chen cheng ||| tong li ||| 
2019 ||| design and implementation of a new transformerless bidirectional dc-dc converter with wide conversion ratios. ||| noureldeen elsayad ||| hadi moradisizkoohi ||| osama a. mohammed ||| 
2018 ||| an llc-lc type bidirectional control strategy for an llc resonant converter in power electronic traction transformer. ||| jiepin zhang ||| jianqiang liu ||| jingxi yang ||| nan zhao ||| yang wang ||| trillion q. zheng ||| 
2017 ||| a single-phase buck matrix converter with high-frequency transformer isolation and reduced switch count. ||| hafiz furqan ahmed ||| honnyong cha ||| ashraf ali khan ||| 
2022 ||| health management of dry-type transformer based on broad learning system. ||| quanbo ge ||| mengmeng wang ||| haoyu jiang ||| zhenyu lu ||| gang yao ||| changyin sun ||| 
2018 ||| leakage current attenuation of a three-phase cascaded inverter for transformerless grid-connected pv systems. ||| xiaoqiang guo ||| jiale zhou ||| ran he ||| xiaoyu jia ||| christian a. rojas ||| 
2020 ||| a novel high-voltage dc transformer based on diode-clamped modular multilevel converters with voltage self-balancing capability. ||| tong zheng ||| congzhe gao ||| xiangdong liu ||| xiaozhong liao ||| zhen li ||| baiyan sun ||| jingliang lv ||| 
2020 ||| novel transformerless buck-boost inverters without leakage current. ||| ashraf ali khan ||| yun w. lu ||| usman ali khan ||| liwei wang ||| wilson eberle ||| mohammed agamy ||| 
2022 ||| cascaded transformers-based multilevel inverters with npc. ||| amanda pereira monteiro ||| filipe ant ||| nio da costa bahia ||| cursino brand ||| o jacobina ||| reuben palmer rezende de sousa ||| 
2021 ||| transformerless ups system based on the half-bridge hybrid switched-capacitor operating as ac-dc and dc-dc converter. ||| leonardo g ||| bel fernandes ||| alceu andr |||  badin ||| daniel flores cortez ||| roger gules ||| eduardo f ||| lix ribeiro romaneli ||| amauri amorin assef ||| 
2017 ||| active harmonic reduction using dc-side current injection applied in a novel large current rectifier based on fork-connected autotransformer. ||| fangang meng ||| xiaona xu ||| lei gao ||| chunwei cai ||| 
2018 ||| a new soft-switching configuration and its application in transformerless photovoltaic grid-connected inverters. ||| huafeng xiao ||| li zhang ||| zheng wang ||| ming cheng ||| 
2018 ||| combination analysis and switching method of a cascaded h-bridge multilevel inverter based on transformers with the different turns ratio for increasing the voltage level. ||| june-seok lee ||| hyun-woo sim ||| juyong kim ||| kyo-beum lee ||| 
2018 ||| a novel dc current injection suppression method for three-phase grid-connected inverter without the isolation transformer. ||| min chen ||| dehong xu ||| tao zhang ||| keyan shi ||| guofeng he ||| kaushik rajashekara ||| 
2022 ||| a single-phase five-level transformer-less pv inverter for leakage current reduction. ||| xiaonan zhu ||| hongliang wang ||| wenyuan zhang ||| hanzhe wang ||| xiaojun deng ||| xiumei yue ||| 
2019 ||| an improved partially interleaved transformer structure for high-voltage high-frequency multiple-output applications. ||| bin zhao ||| ziwei ouyang ||| maeve c. duffy ||| michael a. e. andersen ||| william gerard hurley ||| 
2021 ||| design and adaptive control of matrix transformer based indirect converter for large-capacity circuit breaker testing application. ||| liang shu ||| ziran wu ||| yingmin you ||| marcelo j. dapino ||| sheng zhao ||| 
2021 ||| a new harmonic mitigation system with double balanced impedance filtering power transformer for multistage distribution network. ||| ye tian ||| longfu luo ||| qianyi liu ||| yong li ||| zhao huang ||| 
2018 ||| measurement of the no-load characteristics of single-phase transformer using an improved low-frequency method. ||| xin liu ||| chenguo yao ||| shibin liang ||| junkai wang ||| tao liu ||| 
2019 ||| single-phase transformer-based hf-isolated impedance source inverters with voltage clamping techniques. ||| zeeshan aleem ||| simon lucas winberg ||| atif iqbal ||| mohammed abdulla e. al-hitmi ||| moin hanif ||| 
2020 ||| modular hybrid-full-bridge dc transformer with full-process matching switching strategy for mvdc power distribution application. ||| biao zhao ||| xiaoqian li ||| yingdong wei ||| feng an ||| jie sun ||| guowei liu ||| yuming zhao ||| 
2021 ||| component-level thermo-electromagnetic nonlinear transient finite element modeling of solid-state transformer for dc grid studies. ||| ning lin ||| peng liu ||| venkata dinavahi ||| 
2020 ||| a transformer integrated filtering system for power quality improvement of industrial dc supply system. ||| qianyi liu ||| yong li ||| sijia hu ||| longfu luo ||| 
2021 ||| machine remaining useful life prediction via an attention-based deep learning approach. ||| zhenghua chen ||| min wu ||| rui zhao ||| feri guretno ||| ruqiang yan ||| xiaoli li ||| 
2020 ||| macroscopic-microscopic attention in lstm networks based on fusion features for gear remaining life prediction. ||| yi qin ||| sheng xiang ||| yi chai ||| haizhou chen ||| 
2021 ||| deep learning with spatiotemporal attention-based lstm for industrial soft sensor model development. ||| xiaofeng yuan ||| lin li ||| yuri a. w. shardt ||| yalin wang ||| chunhua yang ||| 
2020 ||| impedance modeling and dc bus voltage stability assessment of a solid-state-transformer-enabled hybrid ac-dc grid considering bidirectional power flow. ||| qing ye ||| ran mo ||| hui li ||| 
2021 ||| a common grounded type dual-mode five-level transformerless inverter for photovoltaic applications. ||| md. noman habib khan ||| yam prasad siwakoti ||| mark j. scott ||| li li ||| shakil ahamed khan ||| dylan dah-chuan lu ||| reza barzegarkhoo ||| franciszek sidorski ||| frede blaabjerg ||| saad ul hasan ||| 
2022 ||| data-driven estimation of driver attention using calibration-free eye gaze and scene features. ||| zhongxu hu ||| chen lv ||| peng hang ||| chao huang ||| yang xing ||| 
2017 ||| integration of large photovoltaic and wind system by means of smart transformer. ||| rongwu zhu ||| giovanni de carne ||| fujin deng ||| marco liserre ||| 
2019 ||| visual-attention-based pixel dimming technique for oled displays of mobile devices. ||| chia-hung yeh ||| kyle shih-huang lo ||| weisi lin ||| 
2020 ||| experimental evaluation of transformer internal fault detection based on v-i characteristics. ||| xiaozhen zhao ||| chenguo yao ||| zehong zhou ||| chengxiang li ||| xianmin wang ||| tianyu zhu ||| ahmed abu-siada ||| 
2020 ||| a new testing method for the dielectric response of oil-immersed transformer. ||| dongyang wang ||| lijun zhou ||| zhi-xin yang ||| yi cui ||| lujia wang ||| junfei jiang ||| lei guo ||| 
2022 ||| transformer-less boost converter with reduced voltage stress for high voltage step-up applications. ||| shima sadaf ||| mahajan sagar bhaskar ||| mohammad meraj ||| atif iqbal ||| nasser al-emadi ||| 
2018 ||| lifetime-based power routing in parallel converters for smart transformer application. ||| markus andresen ||| vivek raveendran ||| giampaolo buticchi ||| marco liserre ||| 
2021 ||| a novel suppression method for grounding transformer against earth current from urban rail transit. ||| jinli zhu ||| chengxiong mao ||| zhaoyuan wang ||| jinfan chen ||| dan wang ||| le luan ||| yajun qiao ||| haijing wang ||| 
2018 ||| three-phase lines to single-phase coil planar contactless power transformer. ||| hirokazu matsumoto ||| yojiro shibako ||| yuta shiihara ||| ryoma nagata ||| yasuhiko neba ||| 
2021 ||| a novel leakage-current-based online insulation monitoring strategy for converter transformers using common-mode and differential-mode harmonics in vsc system. ||| geye lu ||| pinjia zhang ||| 
2017 ||| a new fxlms algorithm with offline and online secondary-path modeling scheme for active noise control of power transformers. ||| tong zhao ||| jiabi liang ||| liang zou ||| li zhang ||| 
2021 ||| a transformerless dc-dc modular multilevel converter for hybrid interconnections in hvdc. ||| saurav dey ||| tanmoy bhattacharya ||| 
2019 ||| high step-up transformerless inverter for ac module applications with active power decoupling. ||| jinia roy ||| yinglai xia ||| rajapandian ayyanar ||| 
2017 ||| series voltage regulator for a distribution transformer to compensate voltage sag/swell. ||| taeyong kang ||| sewan choi ||| ahmed s. morsy ||| prasad n. enjeti ||| 
2022 ||| fast grnn-based method for distinguishing inrush currents in power transformers. ||| shahabodin afrasiabi ||| mousa afrasiabi ||| benyamin parang ||| mohammad mohammadi ||| haidar samet ||| tomislav dragicevic ||| 
2017 ||| optimal design and implementation of high-voltage high-power silicon steel core medium-frequency transformer. ||| pei huang ||| chengxiong mao ||| dan wang ||| libing wang ||| yuping duan ||| jun qiu ||| guang xu ||| huihong cai ||| 
2017 ||| a zero-voltage-transition heric-type transformerless photovoltaic grid-connected inverter. ||| hua f. xiao ||| li zhang ||| yanqing li ||| 
2021 ||| a modular multilevel converter (mmc) based solid-state transformer (sst) topology with simplified energy conversion process and magnetic integration. ||| gengzhe zheng ||| yu chen ||| yong kang ||| 
2018 ||| a grid-connected single-phase transformerless inverter controlling two solar pv arrays operating under different atmospheric conditions. ||| subhendu dutta ||| dipankar debnath ||| kishore chatterjee ||| 
2018 ||| proposal of a photovoltaic ac-module with a single-stage transformerless grid-connected boost microinverter. ||| fernando c. melo ||| lucas s. garcia ||| luiz c. g. freitas ||| ernane ant ||| nio alves coelho ||| valdeir jos |||  farias ||| luiz carlos gomes de freitas ||| 
2021 ||| six-switch step-up common-grounded five-level inverter with switched-capacitor cell for transformerless grid-tied pv applications. ||| reza barzegarkhoo ||| yam prasad siwakoti ||| naser vosoughi ||| frede blaabjerg ||| 
2018 ||| sizing and soc management of a smart-transformer-based energy storage system. ||| chandan kumar ||| rongwu zhu ||| giampaolo buticchi ||| marco liserre ||| 
2021 ||| an ultrawide output range ${llc}$ resonant converter based on adjustable turns ratio transformer and reconfigurable bridge. ||| dongdong shu ||| haoyu wang ||| 
2021 ||| a doubly grounded transformerless pv grid-connected inverter without shoot-through problem. ||| zhilei yao ||| yubo zhang ||| 
2022 ||| extended-state-observer based model predictive control of a hybrid modular dc transformer. ||| hang zhang ||| yaohua li ||| zixin li ||| cong zhao ||| fanqiang gao ||| fei xu ||| ping wang ||| 
2017 ||| a novel structure for single-switch nonisolated transformerless buck-boost dc-dc converter. ||| mohamad reza banaei ||| hossein ajdar faeghi bonab ||| 
2018 ||| dual-transformer-based dab converter with wide zvs range for wide voltage conversion gain application. ||| guo xu ||| deshang sha ||| yaxiong xu ||| xiaozhong liao ||| 
2017 ||| active balancing of li-ion battery cells using transformer as energy carrier. ||| kyung-min lee ||| sang-won lee ||| yoon-geol choi ||| bongkoo kang ||| 
2019 ||| nsga-ii+fem based loss optimization of three-phase transformer. ||| mohammed sami mohammed ||| revna acar vural ||| 
2021 ||| structured protection measures for better use of nanocrystalline cores in air-cooled medium-frequency transformer for induction heating. ||| arun kumar paul ||| 
2019 ||| simultaneous common-mode resonance circulating current and leakage current suppression for transformerless three-level t-type pv inverter system. ||| xiaoyan li ||| xiangyang xing ||| chenghui zhang ||| alian chen ||| changwei qin ||| guangxian zhang ||| 
2020 ||| tunable high-power multilayer piezoelectric transformer. ||| xiaotian li ||| deepam maurya ||| alfredo v. carazo ||| mohan sanghadasa ||| shashank priya ||| 
2018 ||| analytical modeling and implementation of a coaxially wound transformer with integrated filter inductance for isolated soft-switching dc-dc converters. ||| seunghun baek ||| subhashish bhattacharya ||| 
2019 ||| a transformerless hybrid modular multilevel dc-dc converter with dc fault ride-through capability. ||| fei zhang ||| wei li ||| g ||| za jo ||| s ||| 
2018 ||| detection of power transformer winding deformation using improved fra based on binary morphology and extreme point variation. ||| zhongyong zhao ||| chenguo yao ||| chengxiang li ||| syed mofizul islam ||| 
2017 ||| 48-v voltage regulator module with pcb winding matrix transformer for future data centers. ||| mohamed h. ahmed ||| chao fei ||| fred c. lee ||| qiang li ||| 
2017 ||| performance and design analysis on round-shaped transformers applied in rectifier systems. ||| tiejun wang ||| fang fang ||| xiaoyi jiang ||| kaiyue wang ||| lv yang ||| 
2018 ||| transformerless line-interactive ups with low ground leakage current. ||| woo-young choi ||| min-kwon yang ||| 
2017 ||| design and implementation of an amorphous high-frequency transformer coupling multiple converters in a smart microgrid. ||| mohammad jafari ||| zahra malekjamshidi ||| gang lei ||| tianshi wang ||| glenn platt ||| jianguo zhu ||| 
2020 ||| quarter-turn transformer design and optimization for high power density 1-mhz llc resonant converter. ||| yu-chen liu ||| kai-de chen ||| chen chen ||| yong-long syu ||| guan-wei lin ||| katherine a. kim ||| huang-jen chiu ||| 
2019 ||| a push-pull modular-multilevel-converter-based low step-up ratio dc transformer. ||| xiaotian zhang ||| xin xiang ||| timothy c. green ||| xu yang ||| feng wang ||| 
2020 ||| voltage-adjustable capacitor isolated solid-state transformer. ||| baiyan sun ||| congzhe gao ||| xiangdong liu ||| zhen chen ||| tong zheng ||| 
2020 ||| a novel scalar pwm method to reduce leakage current in three-phase two-level transformerless grid-connected vsis. ||| junzhong xu ||| jingwen han ||| yong wang ||| salman habib ||| houjun tang ||| 
2017 ||| on the effect of disorder on stray capacitance of transformer winding in high-voltage power supplies. ||| morteza aghaei ||| shahriyar kaboli ||| 
2022 ||| design of symmetrical cllc-resonant dc transformer considering voltage transfer ratio and cascaded system stability. ||| fanfan lin ||| xin zhang ||| xinze li ||| hao ma ||| chunwei cai ||| 
2018 ||| input filter for a power electronics transformer in a railway traction application. ||| roberto aceit ||| n ||| jens weber ||| steffen bernet ||| 
2017 ||| corrections to "multilevel mvdc link strategy of high-frequency-link dc transformer based on switched capacitor for mvdc power distribution". ||| yu wang ||| qiang song ||| qianhao sun ||| biao zhao ||| jianguo li ||| wenhua liu ||| 
2018 ||| a modified phase disposition pulse width modulation to suppress the leakage current for the transformerless cascaded h-bridge inverters. ||| fusheng wang ||| zhen li ||| hieu thanh do ||| dehui zhang ||| 
2020 ||| double-ended active-clamp forward converter with low dc offset current of transformer. ||| myung-ho kim ||| seung-hoon lee ||| bom-seok lee ||| ji-yeon kim ||| jae-kuk kim ||| 
2018 ||| online condition monitoring of onboard traction transformer core based on core-loss calculation model. ||| qiang fu ||| jiaojiao zhu ||| zhi-hong mao ||| guixin zhang ||| tefang chen ||| 
2019 ||| current-fed isolated lcc-t resonant converter with zcs and improved transformer utilization. ||| venkata r. vakacharla ||| akshay kumar rathore ||| 
2017 ||| a modular multilevel dc-link front-to-front dc solid-state transformer based on high-frequency dual active phase shift for hvdc grid integration. ||| biao zhao ||| qiang song ||| jianguo li ||| wenhua liu ||| 
2019 ||| flicker-free single-switch quadratic boost led driver compatible with electronic transformers. ||| xueshan liu ||| xuewen li ||| qun zhou ||| jianping xu ||| 
2021 ||| the dual-mode combined control strategy for centralized photovoltaic grid-connected inverters based on double-split transformers. ||| ming li ||| xing zhang ||| zixuan guo ||| jilei wang ||| fei li ||| 
2022 ||| improved zvs range for three-phase dual-active-bridge converter with wye-extended-delta transformer. ||| lucas mondardo c ||| nico ||| andr |||  lu ||| s kirsten ||| 
2021 ||| leakage current suppression of single-phase five-level inverter for transformerless photovoltaic system. ||| xiaonan zhu ||| hongliang wang ||| renjie sun ||| hanzhe wang ||| wenyuan zhang ||| xiaojun deng ||| xiumei yue ||| 
2018 ||| a balance transformer-integrated rpfc for railway power system pq improvement with low-design capacity. ||| sijia hu ||| sheng li ||| yong li ||| olav krause ||| bin xie ||| bonan an ||| pengcheng wang ||| zhiwen zhang ||| longfu luo ||| 
2018 ||| maintaining continuous zvs operation of a dual active bridge by reduced coupling transformers. ||| jan riedel ||| donald grahame holmes ||| brendan p. mcgrath ||| carlos teixeira ||| 
2021 ||| exploration of a novel hts thin-film device combined with roles of transformer and overcurrent limiter. ||| le liang ||| yu wang ||| zhongming yan ||| weirong chen ||| 
2017 ||| a resonant zvzcs dc-dc converter with two uneven transformers for an mvdc collection system of offshore wind farms. ||| liangcai shu ||| wu chen ||| guangfu ning ||| wu cao ||| jun mei ||| jianfeng zhao ||| chun liu ||| guoqing he ||| 
2022 ||| active power backflow control strategy for cascaded photovoltaic solid-state transformer during low-voltage ride through. ||| tao zhao ||| daolian chen ||| 
2018 ||| common-ground-type transformerless inverters for single-phase solar photovoltaic systems. ||| yam prasad siwakoti ||| frede blaabjerg ||| 
2019 ||| a new design and analysis of round-shaped transformers supplying diode and controllable rectifiers. ||| seyed reza mousavi-aghdam ||| 
2021 ||| smart transformer-enabled meshed hybrid distribution grid. ||| dwijasish das ||| v. m. hrishikesan ||| chandan kumar ||| marco liserre ||| 
2021 ||| an estimation method for real-time thermal capacity of traction transformers under unbalanced loads. ||| lijun zhou ||| lujia wang ||| xiang zhang ||| qiang liu ||| lei guo ||| dongyang wang ||| 
2020 ||| magnetic-integrated multipulse rectifier transformer with a tight impedance equalizing strategy for power quality improvement of dc traction power supply system. ||| bonan an ||| yong li ||| hua xiao ||| bin xie ||| pengcheng wang ||| mengxia wang ||| longfu luo ||| 
2021 ||| quasi-clamped zsi with two transformers. ||| zeeshan aleem ||| hyoungkyu yang ||| hafiz furqan ahmed ||| jung-wook park ||| 
2020 ||| a new transformerless inverter with leakage current limiting and voltage boosting capabilities for grid-connected pv applications. ||| hossein khoun jahan ||| 
2018 ||| smart transformer-fed variable frequency distribution grid. ||| zhi-xiang zou ||| giovanni de carne ||| giampaolo buticchi ||| marco liserre ||| 
2022 ||| speed adaptability assessment of railway balise transmission module using a deep-adaptive-attention-based encoder-decoder network. ||| chong bian ||| shunkun yang ||| qingyang xu ||| jinghui meng ||| 
2019 ||| dc decoupling-based three-phase three-level transformerless pv inverter topology for minimization of leakage current. ||| venu sonti ||| sachin jain ||| 
2022 ||| divergence distance based index for discriminating inrush and internal fault currents in power transformers. ||| mohsen tajdinian ||| haidar samet ||| 
2020 ||| multifunction transformer with adjustable magnetic stage based on nanocomposite semihard magnets. ||| zhiwei chen ||| xiang li ||| hongmei li ||| 
2020 ||| a novel high step-up high efficiency interleaved dc-dc converter with coupled inductor and built-in transformer for renewable energy systems. ||| tohid nouri ||| nima nouri ||| naser vosoughi ||| 
2017 ||| integrating two stages as a common-mode transformerless photovoltaic converter. ||| nimrod v ||| zquez ||| jeziel v ||| zquez ||| joaquin vaquero ||| claudia hern ||| ndez ||| esl |||  v ||| zquez ||| ren |||  osorio sanchez ||| 
2021 ||| multitime scale frequency regulation of a general resonant dc transformer in a hybrid ac/dc microgrid. ||| jingjing huang ||| xin zhang ||| tian mao ||| 
2022 ||| hybrid high voltage gain transformerless dc-dc converter. ||| ant ||| nio manuel santos spencer andrade ||| tiago miguel klein faistel ||| ronaldo antonio guisso ||| ademir toebe ||| 
2021 ||| transformer current ringing in dual active bridge converters. ||| zian qin ||| zhan shen ||| frede blaabjerg ||| pavol bauer ||| 
2021 ||| analysis and implementation of a single-stage transformer-less converter with high step-down voltage gain for voltage regulator modules. ||| longyang yu ||| laili wang ||| chengzi yang ||| min wu ||| 
2020 ||| an efficient convolutional neural network model based on object-level attention mechanism for casting defect detection on radiography images. ||| chuanfei hu ||| yongxiong wang ||| 
2020 ||| a coupled inductor-based buck-boost type grid connected transformerless pv inverter having the ability to control two subarrays simultaneously. ||| subhendu dutta ||| kishore chatterjee ||| 
2019 ||| hybrid dc-ac zonal microgrid enabled by solid-state transformer and centralized esd integration. ||| alok agrawal ||| chandra sekhar nalamati ||| rajesh gupta ||| 
2022 ||| a novel double-stacked autoencoder for power transformers dga signals with an imbalanced data structure. ||| dongsheng yang ||| jia qin ||| yongheng pang ||| tingwen huang ||| 
2017 ||| dual-transformer-based asymmetrical triple-port active bridge (dt-atab) isolated dc-dc converter. ||| venkat nag someswar rao jakka ||| anshuman shukla ||| georgios d. demetriades ||| 
2019 ||| an interleaved high step-up converter with coupled inductor and built-in transformer voltage multiplier cell techniques. ||| tohid nouri ||| naser vosoughi ||| seyed hossein hosseini ||| ebrahim babaei ||| mehran sabahi ||| 
2022 ||| four-port, single-stage, multidirectional ac-ac converter for solid-state transformer applications. ||| cicero alisson dos santos ||| luan carlos dos santos mazza ||| fernando lessa tofoli ||| demercil de souza oliveira junior ||| paulo peixoto pra ||| a ||| fernando luiz marcelo antunes ||| 
2020 ||| emoji-based sentiment analysis using attention networks. ||| yinxia lou ||| yue zhang ||| fei li ||| tao qian ||| donghong ji ||| 
2022 ||| denigrate comment detection in low-resource hindi language using attention-based residual networks. ||| saurabh r. sangwan ||| m. p. s. bhatia ||| 
2019 ||| chinese-catalan: a neural machine translation approach based on pivoting and attention mechanisms. ||| marta r. costa-juss ||| noe casas ||| carlos escolano ||| jos |||  a. r. fonollosa ||| 
2021 ||| exploration of effective attention strategies for neural automatic post-editing with transformer. ||| jaehun shin ||| wonkee lee ||| byung-hyun go ||| baikjin jung ||| young kil kim ||| jong-hyeok lee ||| 
2021 ||| sentiment analysis using xlm-r transformer and zero-shot transfer learning on resource-poor indian language. ||| akshi kumar ||| victor hugo c. de albuquerque ||| 
2021 ||| geogat: graph model based on attention mechanism for geographic text classification. ||| weipeng jing ||| xianyang song ||| donglin di ||| houbing song ||| 
2020 ||| joint model of entity recognition and relation extraction with self-attention mechanism. ||| maofu liu ||| yukun zhang ||| wenjie li ||| donghong ji ||| 
2020 ||| chinese short text classification with mutual-attention convolutional neural networks. ||| ming hao ||| bo xu ||| jing-yi liang ||| bo-wen zhang ||| xu-cheng yin ||| 
2021 ||| a multi-classification sentiment analysis model of chinese short text based on gated linear units and attention mechanism. ||| lei liu ||| hao chen ||| yinghong sun ||| 
2021 ||| hybridization between neural computing and nature-inspired algorithms for a sentence similarity model based on the attention mechanism. ||| peiying zhang ||| xingzhe huang ||| maozhen li ||| yu xue ||| 
2021 ||| two-channel attention mechanism fusion model of stock price prediction based on cnn-lstm. ||| lin sun ||| wenzheng xu ||| jimin liu ||| 
2021 ||| bi-directional long short-term memory model with semantic positional attention for the question answering system. ||| mingwen bi ||| qingchuan zhang ||| min zuo ||| zelong xu ||| qingyu jin ||| 
2020 ||| attention mechanism for uyghur personal pronouns resolution. ||| qimeng yang ||| long yu ||| shengwei tian ||| jinmiao song ||| 
2022 ||| a transformer-based approach to multilingual fake news detection in low-resource languages. ||| arkadipta de ||| dibyanayan bandyopadhyay ||| baban gain ||| asif ekbal ||| 
2019 ||| pos tag-enhanced coarse-to-fine attention for neural machine translation. ||| yongjing yin ||| jinsong su ||| huating wen ||| jiali zeng ||| yang liu ||| yidong chen ||| 
2021 ||| emotion cause detection with enhanced-representation attention convolutional-context network. ||| yufeng diao ||| hongfei lin ||| liang yang ||| xiaochao fan ||| yonghe chu ||| di wu ||| kan xu ||| 
2021 ||| cross-modality co-attention networks for visual question answering. ||| dezhi han ||| shuli zhou ||| kuan-ching li ||| rodrigo fernandes de mello ||| 
2021 ||| abml: attention-based multi-task learning for jointly humor recognition and pun detection. ||| lu ren ||| bo xu ||| hongfei lin ||| liang yang ||| 
2019 ||| a soft-computing-based approach to artificial visual attention using human eye-fixation paradigm: toward a human-like skill in robot vision. ||| kurosh madani ||| viachaslau kachurka ||| christophe sabourin ||| vladimir a. golovko ||| 
2021 ||| pyramidal convolution attention generative adversarial network with data augmentation for image denoising. ||| qiongshuai lyu ||| dongliang xia ||| yaling liu ||| xiaojing yang ||| rui li ||| 
2022 ||| spatial-temporal attention fusion for traffic speed prediction. ||| anqin zhang ||| qizheng liu ||| ting zhang ||| 
2020 ||| public information, heterogeneous attention and market instability. ||| chengyao wu ||| huiyang chen ||| peng peng ||| yonghua cen ||| 
2018 ||| recognizing the human attention state using cardiac pulse from the noncontact and automatic-based measurements. ||| dazhi jiang ||| bo hu ||| yifei chen ||| yu xue ||| wei li ||| zhengping liang ||| 
2021 ||| multi-attention embedded network for salient object detection. ||| wei he ||| chen pan ||| wenlong xu ||| ning zhang ||| 
2017 ||| chaos-assisted multiobjective evolutionary algorithm to the design of transformer. ||| selvaraj tamilselvi ||| subramanian baskar ||| l. anandapadmanaban ||| mohaideen abdul kadhar kappuva ||| p. r. varshini ||| 
2021 ||| cascading residual-residual attention generative adversarial network for image super resolution. ||| jianqiang chen ||| yali zhang ||| xiang hu ||| calvin yu-chian chen ||| 
2022 ||| image super-resolution reconstruction based on generative adversarial network model with feedback and attention mechanisms. ||| yongqiang wang ||| xue li ||| fangzhe nan ||| feng liu ||| hua li ||| haitao wang ||| yurong qian ||| 
2020 ||| a model of co-saliency based audio attention. ||| xiaoming zhao ||| xinxin wang ||| de cheng ||| 
2017 ||| prediction of visual attention with deep cnn on artificially degraded videos for studies of attention of patients with dementia. ||| souad chaabouni ||| jenny benois-pineau ||| francois tison ||| chokri ben amar ||| akka zemmari ||| 
2018 ||| hybrid convolutional neural networks and optical flow for video visual attention prediction. ||| meijun sun ||| ziqi zhou ||| dong zhang ||| zheng wang ||| 
2020 ||| 3d model retrieval based on multi-view attentional convolutional neural network. ||| an-an liu ||| heyu zhou ||| meng-jie li ||| weizhi nie ||| 
2019 ||| tracking students' visual attention on manga-based interactive e-book while reading: an eye-movement approach. ||| chun-chia wang ||| jason c. hung ||| shih-nung chen ||| hsuan-pu chang ||| 
2022 ||| text multi-label learning method based on label-aware attention and semantic dependency. ||| baisong liu ||| xiaoling liu ||| hao ren ||| jiangbo qian ||| yangyang wang ||| 
2021 ||| medical image segmentation algorithm based on multilayer boundary perception-self attention deep learning model. ||| feng-ping an ||| jun-e liu ||| 
2021 ||| video multimodal emotion recognition based on bi-gru and attention fusion. ||| ruohong huan ||| jia shu ||| shenglin bao ||| ronghua liang ||| peng chen ||| kaikai chi ||| 
2022 ||| distracted driving detection based on the improved centernet with attention mechanism. ||| qingqing zhang ||| zhongjie zhu ||| yongqiang bai ||| guanglong liao ||| tingna liu ||| 
2020 ||| gatecap: gated spatial and semantic attention model for image captioning. ||| shiwei wang ||| long lan ||| xiang zhang ||| zhigang luo ||| 
2022 ||| fine-grained histopathological cell segmentation through residual attention with prior embedding. ||| tangqi shi ||| chaoqun li ||| dou xu ||| xiayue fan ||| 
2022 ||| multi-label image recognition with attentive transformer-localizer module. ||| lin nie ||| tianshui chen ||| zhouxia wang ||| wenxiong kang ||| liang lin ||| 
2021 ||| attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional lstm. ||| mahesh g. huddar ||| sanjeev s. sannakki ||| vijay s. rajpurohit ||| 
2018 ||| cross-modal recipe retrieval with stacked attention model. ||| jingjing chen ||| lei pang ||| chong-wah ngo ||| 
2020 ||| fine-grained facial image-to-image translation with an attention based pipeline generative adversarial framework. ||| yan zhao ||| ziqiang zheng ||| chao wang ||| zhaorui gu ||| min fu ||| zhibin yu ||| haiyong zheng ||| nan wang ||| bing zheng ||| 
2021 ||| using recurrent neural network structure with enhanced multi-head self-attention for sentiment analysis. ||| xue-liang leng ||| xiao-ai miao ||| tao liu ||| 
2022 ||| attentional networks for music generation. ||| gullapalli keerti ||| a. n. vaishnavi ||| prerana mukherjee ||| a. sree vidya ||| gattineni sai sreenithya ||| deeksha nayab ||| 
2022 ||| self-attention mechanism in person re-identification models. ||| wenbai chen ||| yue lu ||| hang ma ||| qili chen ||| xibao wu ||| peiliang wu ||| 
2021 ||| underwater target detection with an attention mechanism and improved scale. ||| xiangyu wei ||| long yu ||| shengwei tian ||| pengcheng feng ||| xin ning ||| 
2018 ||| saturation-aware human attention region of interest algorithm for efficient video compression. ||| sylvia o. n'guessan ||| nam ling ||| 
2020 ||| remote sensing image caption generation via transformer and reinforcement learning. ||| xiangqing shen ||| bing liu ||| yong zhou ||| jiaqi zhao ||| 
2018 ||| weakly supervised detection with decoupled attention-based deep representation. ||| wenhui jiang ||| zhicheng zhao ||| fei su ||| 
2020 ||| image attention retargeting using defocus map and bilateral filter. ||| maria kanwal ||| muhammad mohsin riaz ||| syed sohaib ali ||| abdul ghafoor ||| 
2021 ||| cascaded atrous dual attention u-net for tumor segmentation. ||| yu-cheng liu ||| mohammad shahid ||| wannaporn sarapugdi ||| yong-xiang lin ||| jyh-cheng chen ||| kai-lung hua ||| 
2021 ||| visual attention model based dual watermarking for simultaneous image copyright protection and authentication. ||| subhadeep koley ||| 
2019 ||| video attention prediction using gaze saliency. ||| yanxiang chen ||| gang tao ||| qiangqiang xie ||| minglong song ||| 
2021 ||| scale channel attention network for image segmentation. ||| jianjun chen ||| youliang tian ||| wei ma ||| zhengdong mao ||| yue hu ||| 
2020 ||| a novel attention-guided jnd model for improving robust image watermarking. ||| jun wang ||| wenbo wan ||| 
2022 ||| split-attention effects in multimedia learning environments: eye-tracking and eeg analysis. ||| duygu mutlu bayraktar ||| pinar  ||| zel ||| fatih altindis ||| b ||| lent yilmaz ||| 
2018 ||| video-based learners' observed attention estimates for lecture learning gain evaluation. ||| urban burnik ||| janez zaletelj ||| andrej kosir ||| 
2021 ||| asts: attention based spatio-temporal sequential framework for movie trailer genre classification. ||| yitong yu ||| ziyu lu ||| yang li ||| delong liu ||| 
2021 ||| tell and guess: cooperative learning for natural image caption generation with hierarchical refined attention. ||| wenqiao zhang ||| siliang tang ||| jiajie su ||| jun xiao ||| yueting zhuang ||| 
2020 ||| image captions: global-local and joint signals attention model (gl-jsam). ||| nuzhat naqvi ||| zhongfu ye ||| 
2021 ||| merta: micro-expression recognition with ternary attentions. ||| bing yang ||| jing cheng ||| yunxiang yang ||| bo zhang ||| jianxin li ||| 
2017 ||| anomaly detection based on spatio-temporal sparse representation and visual attention analysis. ||| chen wang ||| hongxun yao ||| xiaoshuai sun ||| 
2019 ||| an attention mechanism based convolutional lstm network for video action recognition. ||| hongwei ge ||| zehang yan ||| wenhao yu ||| liang sun ||| 
2020 ||| spatio-temporal sru with global context-aware attention for 3d human action recognition. ||| qingshan she ||| gaoyuan mu ||| haitao gan ||| yingle fan ||| 
2021 ||| attention-based dual context aggregation for image semantic segmentation. ||| dexin zhao ||| zhiyang qi ||| ruixue yang ||| zhaohui wang ||| 
2022 ||| two-stream adaptive-attentional subgraph convolution networks for skeleton-based action recognition. ||| xianshan li ||| fengchan meng ||| fengda zhao ||| dingding guo ||| fengwei lou ||| rong jing ||| 
2019 ||| combining sun-based visual attention model and saliency contour detection algorithm for apple image segmentation. ||| dandan wang ||| dongjian he ||| huaibo song ||| chang liu ||| hongting xiong ||| 
2022 ||| lightweight adaptive enhanced attention network for image super-resolution. ||| li wang ||| lizhong xu ||| jianqiang shi ||| jie shen ||| fengcheng huang ||| 
2021 ||| sketchformer: transformer-based approach for sketch recognition using vector images. ||| anil singh parihar ||| gaurav jain ||| shivang chopra ||| suransh chopra ||| 
2020 ||| regional bit allocation with visual attention and distortion sensitivity. ||| mesut pak ||| ulug bayazit ||| 
2020 ||| combining an information-maximization-based attention mechanism and illumination invariance theory for the recognition of green apples in natural scenes. ||| sashuang sun ||| mei jiang ||| ning liang ||| dongjian he ||| yan long ||| huaibo song ||| zhenjiang zhou ||| 
2019 ||| attention-based multi-modal fusion for improved real estate appraisal: a case study in los angeles. ||| junchi bin ||| bryan gardiner ||| zheng liu ||| eric li ||| 
2020 ||| bi-directional attention comparison for semantic sentence matching. ||| huiyuan lai ||| yizheng tao ||| chunliu wang ||| lunfan xu ||| dingyong tang ||| gongliang li ||| 
2020 ||| two stages double attention convolutional neural network for crowd counting. ||| zhao zou ||| chaofeng li ||| yuhui zheng ||| shoukun xu ||| 
2020 ||| human motion prediction based on attention mechanism. ||| haifeng sang ||| zi-zhen chen ||| dakuo he ||| 
2020 ||| local and global aligned spatiotemporal attention network for video-based person re-identification. ||| li cheng ||| xiao-yuan jing ||| xiaoke zhu ||| chang-hui hu ||| guangwei gao ||| songsong wu ||| 
2018 ||| looking deeper and transferring attention for image captioning. ||| fang fang ||| hanli wang ||| yihao chen ||| pengjie tang ||| 
2020 ||| object-aware semantics of attention for image captioning. ||| shiwei wang ||| long lan ||| xiang zhang ||| guohua dong ||| zhigang luo ||| 
2020 ||| deep attentional fine-grained similarity network with adversarial learning for cross-modal retrieval. ||| qingrong cheng ||| xiaodong gu ||| 
2020 ||| an attentional spatial temporal graph convolutional network with co-occurrence feature learning for action recognition. ||| dong tian ||| zhe-ming lu ||| xiao chen ||| longhua ma ||| 
2019 ||| self-attention recurrent network for saliency detection. ||| fengdong sun ||| wenhui li ||| yuanyuan guan ||| 
2020 ||| association between slides-format and major's contents: effects on perceived attention and significant learning. ||| antoni castell |||  tarrida ||| david chavez ||| ramon cladellas pros ||| 
2021 ||| tardb-net: triple-attention guided residual dense and bilstm networks for hyperspectral image classification. ||| weiwei cai ||| botao liu ||| zhanguo wei ||| meilin li ||| jiangming kan ||| 
2019 ||| word-to-region attention network for visual question answering. ||| liang peng ||| yang yang ||| yi bin ||| ning xie ||| fumin shen ||| yanli ji ||| xing xu ||| 
2021 ||| weakly supervised fine-grained recognition based on spatial-channel aware attention filters. ||| nannan yu ||| lei huang ||| zhiqiang wei ||| wenfeng zhang ||| bin wang ||| 
2020 ||| unsupervised densely attention network for infrared and visible image fusion. ||| yang li ||| jixiao wang ||| zhuang miao ||| jiabao wang ||| 
2020 ||| pain-attentive network: a deep spatio-temporal attention model for pain estimation. ||| dong huang ||| zhaoqiang xia ||| joshua mwesigye ||| xiaoyi feng ||| 
2022 ||| mask attention-guided graph convolution layer for weakly supervised temporal action detection. ||| mengyao zhao ||| zhengping hu ||| shufang li ||| shuai bi ||| zhe sun ||| 
2020 ||| a lane detection network based on ibn and attention. ||| wenhui li ||| feng qu ||| jialun liu ||| fengdong sun ||| ying wang ||| 
2022 ||| lightweight multi-scale aggregated residual attention networks for image super-resolution. ||| shurong pang ||| zhe chen ||| fuliang yin ||| 
2022 ||| assamese news image caption generation using attention mechanism. ||| ringki das ||| thoudam doren singh ||| 
2021 ||| an ensemble multi-scale residual attention network (emra-net) for image dehazing. ||| jixiao wang ||| chaofeng li ||| shoukun xu ||| 
2021 ||| enhanced ssd with interactive multi-scale attention features for object detection. ||| shuren zhou ||| jia qiu ||| 
2021 ||| attacks on state-of-the-art face recognition using attentional adversarial attack generative network. ||| lu yang ||| qing song ||| yingqi wu ||| 
2020 ||| improving person re-identification via attribute-identity representation and visual attention mechanism. ||| honglin quan ||| songhe feng ||| congyan lang ||| baifan chen ||| 
2021 ||| automated detection of retinopathy of prematurity by deep attention network. ||| baiying lei ||| xianlu zeng ||| shan huang ||| rugang zhang ||| guozhen chen ||| jinfeng zhao ||| tianfu wang ||| jiantao wang ||| guoming zhang ||| 
2022 ||| bit-wise attention deep complementary supervised hashing for image retrieval. ||| wing w. y. ng ||| jiayong li ||| xing tian ||| hui wang ||| 
2021 ||| semantic segmentation of brain tumor with nested residual attention networks. ||| jingchao sun ||| jianqiang li ||| lu liu ||| 
2020 ||| pedestrian object detection with fusion of visual attention mechanism and semantic computation. ||| feng xiao ||| baotong liu ||| runa li ||| 
2022 ||| continuous digital zoom with cross attention for dual camera system. ||| yifan yang ||| qi li ||| zhihai xu ||| huajun feng ||| yueting chen ||| 
2021 ||| self-attention-based neural networks for refining the overlength product titles. ||| yuming lin ||| yu fu ||| you li ||| guoyong cai ||| aoying zhou ||| 
2021 ||| mri enhancement based on visual-attention by adaptive contrast adjustment and image fusion. ||| rui zhu ||| xiongfei li ||| xiaoli zhang ||| xiaowei xu ||| 
2021 ||| multiple attention networks for stereo matching. ||| longyuan guo ||| houyu duan ||| wuwei zhou ||| 
2020 ||| person re-identification based on multi-level feature complementarity of cross-attention with part metric learning. ||| zeng lu ||| guoheng huang ||| chi-man pun ||| lianglun cheng ||| 
2020 ||| multi-level feature learning with attention for person re-identification. ||| suncheng xiang ||| yuzhuo fu ||| hao chen ||| wei ran ||| ting liu ||| 
2021 ||| adversarial erasing attention for fine-grained image classification. ||| jinsheng ji ||| linfeng jiang ||| tao zhang ||| weilin zhong ||| huilin xiong ||| 
2020 ||| low-sample size remote sensing image recognition based on a multihead attention integration network. ||| zesong wang ||| cui zou ||| xianping cui ||| 
2021 ||| correction to: attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional lstm. ||| mahesh g. huddar ||| sanjeev s. sannakki ||| vijay s. rajpurohit ||| 
2021 ||| mask-guided dual attention-aware network for visible-infrared person re-identification. ||| meibin qi ||| suzhi wang ||| guanghong huang ||| jianguo jiang ||| jingjing wu ||| cuiqun chen ||| 
2021 ||| action recognition in still images using a multi-attention guided network with weakly supervised saliency detection. ||| seyed sajad ashrafi ||| shahriar b. shokouhi ||| ahmad ayatollahi ||| 
2021 ||| attention-based encoder-decoder networks for workflow recognition. ||| min zhang ||| haiyang hu ||| zhongjin li ||| jie chen ||| 
2021 ||| computational attention model for children, adults and the elderly. ||| onkar krishna ||| kiyoharu aizawa ||| go irie ||| 
2018 ||| fine-grained attention for image caption generation. ||| yan-shuo chang ||| 
2019 ||| 3d convolution network and siamese-attention mechanism for expression recognition. ||| yi-feng zhang ||| tian xia ||| yuan liu ||| 
2020 ||| a part-based attention network for person re-identification. ||| weilin zhong ||| linfeng jiang ||| tao zhang ||| jinsheng ji ||| huilin xiong ||| 
2017 ||| a car-face region-based image retrieval method with attention of sift features. ||| changyou zhang ||| xiaoya wang ||| jun feng ||| yu cheng ||| cheng guo ||| 
2021 ||| attention cutting and padding learning for fine-grained image recognition. ||| zhuo cheng ||| hongjian li ||| xiaolin duan ||| xiangyan zeng ||| mingxuan he ||| hao luo ||| 
2020 ||| a target response adaptive correlation filter tracker with spatial attention. ||| yifei zhou ||| jing li ||| bo du ||| jun chang ||| yafu xiao ||| 
2019 ||| a novel hybrid image fusion method based on integer lifting wavelet and discrete cosine transformer for visual sensor networks. ||| boubakeur latreche ||| slami saadi ||| mecheri kious ||| ali benziane ||| 
2020 ||| dnn-based speech enhancement with self-attention on feature dimension. ||| jiaming cheng ||| ruiyu liang ||| li zhao ||| 
2021 ||| correction to: mri enhancement based on visual-attention by adaptive contrast adjustment and image fusion. ||| rui zhu ||| xiongfei li ||| xiaoli zhang ||| xiaowei xu ||| 
2018 ||| complex event detection via attention-based video representation and classification. ||| zhicheng zhao ||| rui xiang ||| fei su ||| 
2020 ||| attention-based convolutional neural network for deep face recognition. ||| hefei ling ||| jiyang wu ||| junrui huang ||| jiazhong chen ||| ping li ||| 
2021 ||| spam review detection using self attention based cnn and bi-directional lstm. ||| p. bhuvaneshwari ||| a. nagaraja rao ||| yesudhas harold robinson ||| 
2018 ||| a visual attention-based keyword extraction for document classification. ||| xing wu ||| zhikang du ||| yike guo ||| 
2017 ||| leveraging visual attention and neural activity for stereoscopic 3d visual comfort assessment. ||| qiuping jiang ||| feng shao ||| gangyi jiang ||| mei yu ||| zongju peng ||| 
2019 ||| multiple attention fully convolutional network for automated ventricle segmentation in cardiac magnetic resonance imaging. ||| tinghong zhang ||| ao li ||| minghui wang ||| xiaodong wu ||| bensheng qiu ||| 
2019 ||| brain function network analysis of children with attention-deficit/hyperactivity disorder based on adaptive sparse representation method. ||| xin pan ||| zhongyi jiang ||| hui bi ||| suhong wang ||| ling zou ||| 
2020 ||| analysis of electroencephalography signals on the contents of cognitive function game: attention and memory. ||| sung-wook shin ||| jung-hyun park ||| woo-jin lee ||| sungho kang ||| hyunggun kim ||| sung-taek chung ||| 
2021 ||| unsupervised deep learning network with self-attention mechanism for non-rigid registration of 3d brain mr images. ||| donggeon oh ||| bo hyoung kim ||| jeongjin lee ||| yeong-gil shin ||| 
2019 ||| application of deep convolutional neural networks in attention-deficit/hyperactivity disorder classification: data augmentation and convolutional neural network transfer learning. ||| li zhu ||| weike chang ||| 
2019 ||| contextual predictability and phonetic attention. ||| jonathan manker ||| 
2018 ||| perceptual attention as the locus of transfer to nonnative speech perception. ||| charles b. chang ||| 
2020 ||| time-compressed audio on attention, meditation, cognitive load, and learning. ||| xiaozhe yang ||| lin lin ||| yi wen ||| pei-yu cheng ||| xue yang ||| yun-jo an ||| 
2018 ||| is group polling better? an investigation of the effect of individual and group polling strategies on students' academic performance, anxiety, and attention. ||| jerry chih-yuan sun ||| ariel yu-zhen chen ||| katherine pin-chen yeh ||| yu-ting cheng ||| yu-yan lin ||| 
2018 ||| application of a gamified interactive response system to enhance the intrinsic and extrinsic motivation, student engagement, and attention of english learners. ||| jerry chih-yuan sun ||| pei-hsun hsieh ||| 
2017 ||| effects of attention cueing on learning speech organ operation through mobile phones. ||| hui-yu yang ||| 
2018 ||| a votable concept mapping approach to promoting students' attentional behavior: an analysis of sequential behavioral patterns and brainwave data. ||| jerry chih-yuan sun ||| gwo-jen hwang ||| yu-yan lin ||| shih-jou yu ||| liu-cheng pan ||| ariel yu-zhen chen ||| 
2021 ||| analysis and prediction of "ai + education" attention based on baidu index - taking guizhou province as an example. ||| yulin zhao ||| junke li ||| jiang-e. wang ||| 
2020 ||| basn - learning steganography with a binary attention mechanism. ||| pin wu ||| xuting chang ||| yang yang ||| xiaoqiang li ||| 
2018 ||| a bi-directional lstm-cnn model with attention for aspect-level text classification. ||| yonghua zhu ||| xun gao ||| weilin zhang ||| shenkai liu ||| yuanyuan zhang ||| 
2019 ||| a multi-attention network for aspect-level sentiment analysis. ||| qiuyue zhang ||| ran lu ||| 
2021 ||| the effects of the content elements of online banner ads on visual attention: evidence from an-eye-tracking study. ||| serhat peker ||| gonca gokce menekse dalveren ||| yavuz inal ||| 
2021 ||| person re-identification based on attention mechanism and context information fusion. ||| shengbo chen ||| hongchang zhang ||| zhou lei ||| 
2020 ||| hierarchical gated recurrent unit with semantic attention for event prediction. ||| zichun su ||| jialin jiang ||| 
2019 ||| an improved approach for text sentiment classification based on a deep neural network via a sentiment attention mechanism. ||| wenkuan li ||| peiyu liu ||| qiuyue zhang ||| wenfeng liu ||| 
2021 ||| knowledge-graph-based drug repositioning against covid-19 by graph convolutional network with attention mechanism. ||| mingxuan che ||| kui yao ||| chao che ||| zhangwei cao ||| fanchen kong ||| 
2020 ||| learning a hierarchical global attention for image classification. ||| kerang cao ||| jingyu gao ||| kwang-nam choi ||| lini duan ||| 
2021 ||| linking phubbing behavior to self-reported attentional failures and media multitasking. ||| kayla s. sansevere ||| nathan ward ||| 
2021 ||| video captioning based on channel soft attention and semantic reconstructor. ||| zhou lei ||| yiyong huang ||| 
2018 ||| chinese event extraction based on attention and semantic features: a bidirectional circular neural network. ||| yue wu ||| junyi zhang ||| 
2019 ||| dynamic group recommendation based on the attention mechanism. ||| haiyan xu ||| yanhui ding ||| jing sun ||| kun zhao ||| yuanjian chen ||| 
2021 ||| rumor detection based on attention cnn and time series of context information. ||| yun peng ||| jianmei wang ||| 
2019 ||| combined self-attention mechanism for chinese named entity recognition in military. ||| fei liao ||| liangli ma ||| jingjing pei ||| linshan tan ||| 
2021 ||| a classification method for academic resources based on a graph attention network. ||| jie yu ||| yaliu li ||| chenle pan ||| junwei wang ||| 
2019 ||| feature fusion text classification model combining cnn and bigru with multi-attention mechanism. ||| jingren zhang ||| fang'ai liu ||| weizhi xu ||| hui yu ||| 
2020 ||| unsteady multi-element time series analysis and prediction based on spatial-temporal attention and error forecast fusion. ||| xiaofan wang ||| lingyu xu ||| 
2022 ||| da-gan: dual attention generative adversarial network for cross-modal retrieval. ||| liewu cai ||| lei zhu ||| hongyan zhang ||| xinghui zhu ||| 
2019 ||| object detection network based on feature fusion and attention mechanism. ||| ying zhang ||| yimin chen ||| chen huang ||| ming-ke gao ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| yana agafonova ||| alexey tikhonov ||| ivan p. yamshchikov ||| 
2021 ||| dual-sampling attention pooling for graph neural networks on 3d mesh. ||| tingxi wen ||| jiafu zhuang ||| yu du ||| linjie yang ||| jianfei xu ||| 
2022 ||| dcacnet: dual context aggregation and attention-guided cross deconvolution network for medical image segmentation. ||| hongchun lu ||| shengwei tian ||| long yu ||| lu liu ||| junlong cheng ||| weidong wu ||| xiaojing kang ||| dezhi zhang ||| 
2019 ||| computer aided diagnosis system for multiple sclerosis disease based on phase to amplitude coupling in covert visual attention. ||| amirmasoud ahmadi ||| saeideh davoudi ||| mohammad reza daliri ||| 
2022 ||| multi-scale information with attention integration for classification of liver fibrosis in b-mode us image. ||| xiangfei feng ||| xin chen ||| changfeng dong ||| yingxia liu ||| zhong liu ||| ruixin ding ||| qinghua huang ||| 
2021 ||| xecgnet: fine-tuning attention map within convolutional neural network to improve detection and explainability of concurrent cardiac arrhythmias. ||| jungsun yoo ||| tae joon jun ||| young-hak kim ||| 
2021 ||| deep attention branch networks for skin lesion classification. ||| saisai ding ||| zhongyi wu ||| yanyan zheng ||| zhaobang liu ||| xiaodong yang ||| xiaokai yang ||| gang yuan ||| jing xie ||| 
2021 ||| gradual back-projection residual attention network for magnetic resonance image super-resolution. ||| defu qiu ||| yuhu cheng ||| xuesong wang ||| 
2018 ||| attentional bias in mdd: erp components analysis and classification using a dot-probe task. ||| xiaowei li ||| jianxiu li ||| bin hu ||| jing zhu ||| xuemin zhang ||| liuqing wei ||| ning zhong ||| mi li ||| zhijie ding ||| jing yang ||| lan zhang ||| 
2020 ||| hybrid attention for automatic segmentation of whole fetal head in prenatal ultrasound volumes. ||| xin yang ||| xu wang ||| yi wang ||| haoran dou ||| shengli li ||| huaxuan wen ||| yi lin ||| pheng-ann heng ||| dong ni ||| 
2021 ||| social media data analytics for outbreak risk communication: public attention on the "new normal" during the covid-19 pandemic in indonesia. ||| annisa ristya rahmanti ||| dina nur anggraini ningrum ||| lutfan lazuardi ||| hsuan-chia yang ||| yu-chuan (jack) li ||| 
2020 ||| avnet: a retinal artery/vein classification network with category-attention weighted fusion. ||| hong kang ||| yingqi gao ||| song guo ||| xia xu ||| tao li ||| kai wang ||| 
2021 ||| bseresu-net: an attention-based before-activation residual u-net for retinal vessel segmentation. ||| di li ||| susanto rahardja ||| 
2021 ||| automated ecg classification using a non-local convolutional block attention module. ||| jikuo wang ||| xu qiao ||| changchun liu ||| xinpei wang ||| yuanyuan liu ||| lianke yao ||| huan zhang ||| 
2020 ||| skeletal bone age prediction based on a deep residual network with spatial transformer. ||| yaxin han ||| guangbin wang ||| 
2021 ||| virus identification in electron microscopy images by residual mixed attention network. ||| chi xiao ||| xi chen ||| qiwei xie ||| guoqing li ||| hao xiao ||| jingdong song ||| hua han ||| 
2022 ||| classification of renal biopsy direct immunofluorescence image using multiple attention convolutional neural network. ||| liang zhang ||| ming li ||| yongfei wu ||| fang hao ||| chen wang ||| weixia han ||| dan niu ||| wen zheng ||| 
2021 ||| super-resolution of pneumocystis carinii pneumonia ct via self-attention gan. ||| hongqiang xie ||| tongtong zhang ||| weiwei song ||| shoujun wang ||| hongchang zhu ||| rumin zhang ||| weiping zhang ||| yong yu ||| yan zhao ||| 
2021 ||| time-series deep survival prediction for hemodialysis patients using an attention-based bi-gru network. ||| ziyue yang ||| yu tian ||| tianshu zhou ||| yilin zhu ||| ping zhang ||| jianghua chen ||| jingsong li ||| 
2022 ||| a cnn-transformer hybrid approach for decoding visual neural activity into text. ||| jiang zhang ||| chen li ||| ganwanming liu ||| min min ||| chong wang ||| jiyi li ||| yuting wang ||| hongmei yan ||| zhentao zuo ||| wei huang ||| huafu chen ||| 
2022 ||| automatic location scheme of anatomical landmarks in 3d head mri based on the scale attention hourglass network. ||| sai li ||| qiong gong ||| haojiang li ||| shuchao chen ||| yifei liu ||| guangying ruan ||| lin zhu ||| lizhi liu ||| hongbo chen ||| 
2018 ||| visuospatial working memory assessment using a digital tablet in adolescents with attention deficit hyperactivity disorder. ||| gi jung hyun ||| jin wan park ||| jin hee kim ||| kyoung joon min ||| young-sik lee ||| sun mi kim ||| doug hyun han ||| 
2017 ||| reliability and validity of ds-adhd: a decision support system on attention deficit hyperactivity disorders. ||| kuo-chung chu ||| yu-shu huang ||| chien-fu tseng ||| hsin-jou huang ||| chih-huan wang ||| hsin-yi tai ||| 
2021 ||| automated detection of conduct disorder and attention deficit hyperactivity disorder using decomposition and nonlinear techniques with eeg signals. ||| hui tian tor ||| ooi chui ping ||| nikki s. j. lim-ashworth ||| joel koh en wei ||| v. jahmunah ||| shu lih oh ||| u. rajendra acharya ||| daniel shuen sheng fung ||| 
2020 ||| classification of breast density categories based on se-attention neural networks. ||| jian deng ||| yanyun ma ||| deng-ao li ||| jumin zhao ||| yi liu ||| hui zhang ||| 
2019 ||| extracting chemical-protein interactions from biomedical literature via granular attention based recurrent neural networks. ||| hongbin lu ||| lishuang li ||| xinyu he ||| yang liu ||| anqiao zhou ||| 
2021 ||| multiscale attention guided u-net architecture for cardiac segmentation in short-axis mri images. ||| hengfei cui ||| chang yuwen ||| lei jiang ||| yong xia ||| yanning zhang ||| 
2020 ||| self-attention based recurrent convolutional neural network for disease prediction using healthcare data. ||| mohd usama ||| belal ahmad ||| wenjing xiao ||| m. shamim hossain ||| ghulam muhammad ||| 
2020 ||| semi-supervised segmentation of lesion from breast ultrasound images with attentional generative adversarial network. ||| luyi han ||| yunzhi huang ||| haoran dou ||| shuai wang ||| sahar ahamad ||| honghao luo ||| qi liu ||| jingfan fan ||| jiang zhang ||| 
2021 ||| clcu-net: cross-level connected u-shaped network with selective feature aggregation attention module for brain tumor segmentation. ||| y. l. wang ||| z. j. zhao ||| s. y. hu ||| f. l. chang ||| 
2018 ||| classification of auditory selective attention using spatial coherence and modular attention index. ||| ana paula de souza ||| quenaz bezerra soares ||| leonardo bonato felix ||| eduardo m. a. m. mendes ||| 
2017 ||| instructor presence in instructional video: effects on visual attention, recall, and perceived learning. ||| jiahui wang ||| pavlo d. antonenko ||| 
2018 ||| the effect of cellphones on attention and learning: the influences of time, distraction, and nomophobia. ||| jessica s. mendoza ||| benjamin c. pody ||| seungyeon lee ||| minsung kim ||| ian m. mcdonough ||| 
2018 ||| the female gaze: content composition and slot position in personalized banner ads, and how they influence visual attention in online shoppers. ||| yi-ting huang ||| 
2017 ||| effects of an integrated physiological signal-based attention-promoting and english listening system on students' learning performance and behavioral patterns. ||| yu-chen kuo ||| hui-chun chu ||| meng-chieh tsai ||| 
2017 ||| effectiveness, attention, and recall of human and artificial voices in an advertising story. prosody influence and functions of voices. ||| emma rodero ||| 
2019 ||| narcissistic adolescents' attention-seeking following social rejection: links with social media disclosure, problematic social media use, and smartphone stress. ||| skyler t. hawk ||| regina j. j. m. van den eijnden ||| caspar j. van lissa ||| tom f. m. ter bogt ||| 
2022 ||| share of attention: exploring the allocation of user attention to consumer applications. ||| lars rieser ||| brent furneaux ||| 
2019 ||| how attention level and cognitive style affect learning in a mooc environment? based on the perspective of brainwave analysis. ||| jung-jung chang ||| wen-shan lin ||| hong-ren chen ||| 
2021 ||| mobile use induces local attentional precedence and is associated with limited socio-cognitive skills in preschoolers. ||| veronika konok ||| krisztina liszkai-peres ||| n ||| ra bunford ||| bence ferdinandy ||| zsolt jur ||| nyi ||| dorottya j ||| lia ujfalussy ||| zs ||| fia r ||| ti ||| kos pog ||| ny ||| george kampis ||| d ||| m mikl ||| si ||| 
2018 ||| does game rules work as a game changer? analyzing the effect of rule orientation on brand attention and memory in advergames. ||| sreejesh s. ||| m. r. anusree ||| abhilash ponnam ||| 
2022 ||| predicting reposting latency of news content in social media: a focus on issue attention, temporal usage pattern, and information redundancy. ||| lu guan ||| hai liang ||| jonathan j. h. zhu ||| 
2020 ||| effects of spatial distance on the effectiveness of mental and physical integration strategies in learning from split-attention examples. ||| bj ||| rn b. de koning ||| gertjan rop ||| fred paas ||| 
2020 ||| enhanced memory-driven attentional capture in action video game players. ||| bao zhang ||| shuhui liu ||| cenlou hu ||| ziwen luo ||| sai huang ||| jie sui ||| 
2020 ||| the looking glass selfie: instagram use frequency predicts visual attention to high-anxiety body regions in young women. ||| amelia c. couture bue ||| 
2019 ||| comparative analysis of advertising attention to facebook social network: evidence from eye-movement data. ||| chun-chia wang ||| jason c. hung ||| 
2019 ||| accidentally attentive: comparing visual, close-ended, and open-ended measures of attention on social media. ||| emily k. vraga ||| leticia bode ||| anne-bennett smithson ||| sonya v. troller-renfree ||| 
2019 ||| electronic performance monitoring and sustained attention: social facilitation for modern applications. ||| victoria l. claypoole ||| james l. szalma ||| 
2020 ||| how can you persuade me online? the impact of goal-driven motivations on attention to online information. ||| sarah taylor ||| martin g. graff ||| rachel taylor ||| 
2020 ||| human-computer interaction based joint attention cues: implications on functional and physiological measures for children with autism spectrum disorder. ||| vishav jyoti ||| uttama lahiri ||| 
2018 ||| my friend likes this brand: do ads with social context attract more attention on social networking sites? ||| kasey windels ||| jun heo ||| yongick jeong ||| lance porter ||| a.-reum jung ||| rui wang ||| 
2018 ||| effects of instructor presence in video modeling examples on attention and learning. ||| margot van wermeskerken ||| susanna ravensbergen ||| tamara van gog ||| 
2018 ||| using time pressure and note-taking to prevent digital distraction behavior and enhance online search performance: perspectives from the load theory of attention and cognitive control. ||| jiun-yu wu ||| chen xie ||| 
2017 ||| mobile attachment: separation from the mobile phone induces physiological and behavioural stress and attentional bias to separation-related stimuli. ||| veronika konok ||| kos pog ||| ny ||| d ||| m mikl ||| si ||| 
2020 ||| testing the feasibility of a media multitasking self-regulation intervention for students: behaviour change, attention, and self-perception. ||| douglas a. parry ||| daniel b. le roux ||| jason r. bantjes ||| 
2020 ||| the influence of spatial distance and signaling on the split-attention effect. ||| sven cammeraat ||| gertjan rop ||| bj ||| rn b. de koning ||| 
2017 ||| switching on or switching off? everyday computer use as a predictor of sustained attention and cognitive reflection. ||| adam vujic ||| 
2017 ||| effects of cognition demand, mode of interactivity and brand anthropomorphism on gamers' brand attention and memory in advergames. ||| sreejesh s. ||| m. r. anusree ||| 
2018 ||| the influence of peer accountability on attention during gameplay. ||| yaqian zhang ||| wooi-boon goh ||| 
2020 ||| investigating the attentional bias and information processing mechanism of mobile phone addicts towards emotional information. ||| yixin hu ||| jiahui guo ||| min jou ||| shengqi zhou ||| dawei wang ||| phil maguire ||| jing wei ||| fangzheng qu ||| 
2017 ||| constructing perceptual common ground between human and robot through joint attention. ||| kang-woo lee ||| hyunseung choo ||| 
2020 ||| humanoid robots and autistic children: a review on technological tools to assess social attention and engagement. ||| fady alnajjar ||| massimiliano lorenzo cappuccio ||| omar mubin ||| rabiah arshad ||| suleman shahid ||| 
2019 ||| automatic carotid artery detection using attention layer region-based convolution neural network. ||| xiaoyan wang ||| xingyu zhong ||| ming xia ||| weiwei jiang ||| xiaojie huang ||| zheng gu ||| xiangsheng huang ||| 
2019 ||| fault diagnosis of a transformer based on polynomial neural networks. ||| ajin zou ||| rui deng ||| qixiang mei ||| lang zou ||| 
2019 ||| the impact of attention heterogeneity on stock market in the era of big data. ||| shiming deng ||| peipei liu ||| 
2019 ||| fault diagnosis for oil-filled transformers using voting based extreme learning machine. ||| liwei zhang ||| jian zhai ||| 
2021 ||| multimodal encoders and decoders with gate attention for visual question answering. ||| haiyan li ||| dezhi han ||| 
2020 ||| a recommendations model with multiaspect awareness and hierarchical user-product attention mechanisms. ||| zhongqin bi ||| shuming dou ||| zhe liu ||| yongbin li ||| 
2021 ||| a novel distant target region detection method using hybrid saliency-based attention model under complex textures. ||| jaepil ko ||| kyung joo cheoi ||| 
2020 ||| semi-supervised trajectory understanding with poi attention for end-to-end trip recommendation. ||| fan zhou ||| hantao wu ||| goce trajcevski ||| ashfaq a. khokhar ||| kunpeng zhang ||| 
2021 ||| deepran: attention-based bilstm and crf for ransomware early detection and classification. ||| krishna chandra roy ||| qian chen ||| 
2020 ||| how do small and medium-sized game companies use analytics? an attention-based view of game analytics. ||| matti m ||| ntym ||| ki ||| sami hyrynsalmi ||| antti koskenvoima ||| 
2021 ||| popular news are relevant news! how investor attention affects algorithmic decision-making and decision support in financial markets. ||| benjamin clapham ||| michael siering ||| peter gomber ||| 
2020 ||| attention-based recurrent neural network for multistep-ahead prediction of process performance. ||| majid moradi aliabadi ||| hajar emami ||| ming dong ||| yinlun huang ||| 
2021 ||| face-based attention recognition model for children with autism spectrum disorder. ||| bilikis banire ||| dena al-thani ||| marwa k. qaraqe ||| bilal mansoor ||| 
2019 ||| deepsnp: an end-to-end deep neural network with attention-based localization for breakpoint detection in single-nucleotide polymorphism array genomic data. ||| hamid eghbal-zadeh ||| lukas fischer ||| niko popitsch ||| florian kromp ||| sabine taschner-mandl ||| teresa gerber ||| eva bozsaky ||| peter f. ambros ||| inge m. ambros ||| gerhard widmer ||| bernhard alois moser ||| 
2018 ||| a computing model of selective attention for service robot based on spatial data fusion. ||| huanzhao chen ||| guohui tian ||| 
2021 ||| dncp: an attention-based deep learning approach enhanced with attractiveness and timeliness of news for online news click prediction. ||| jie xiong ||| li yu ||| dongsong zhang ||| youfang leng ||| 
2021 ||| solving multi-agent routing problems using deep attention mechanisms. ||| guillaume bono ||| jilles steeve dibangoye ||| olivier simonin ||| la ||| titia matignon ||| florian pereyron ||| 
2021 ||| an attention-based deep learning framework for trip destination prediction of sharing bike. ||| wei wang ||| xiaofeng zhao ||| zhiguo gong ||| zhikui chen ||| ning zhang ||| wei wei ||| 
2021 ||| detecting driver behavior using stacked long short term memory network with attention layer. ||| shokoufeh monjezi kouchak ||| ashraf gaffar ||| 
2022 ||| heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning. ||| jingwen li ||| liang xin ||| zhiguang cao ||| andrew lim ||| wen song ||| jie zhang ||| 
2021 ||| anomaly detection in automated vehicles using multistage attention-based convolutional neural network. ||| abdul rehman javed ||| muhammad usman ||| saif ur rehman ||| mohib ullah khan ||| mohammad sayad haghighi ||| 
2018 ||| multi-level contextual rnns with attention model for scene labeling. ||| heng fan ||| xue mei ||| danil v. prokhorov ||| haibin ling ||| 
2021 ||| detecting anomalies in intelligent vehicle charging and station power supply systems with multi-head attention models. ||| yidong li ||| li zhang ||| zhuo lv ||| wei wang ||| 
2021 ||| detecting text in scene and traffic guide panels with attention anchor mechanism. ||| jie-bo hou ||| xiaobin zhu ||| chang liu ||| chun yang ||| long-huang wu ||| hongfa wang ||| xu-cheng yin ||| 
2020 ||| fast pedestrian detection with attention-enhanced multi-scale rpn and soft-cascaded decision trees. ||| han wang ||| yali li ||| shengjin wang ||| 
2020 ||| driver inattention detection in the context of next-generation autonomous vehicles design: a survey. ||| alaa el khatib ||| chaojie ou ||| fakhri karray ||| 
2021 ||| a spatial-temporal attention approach for traffic prediction. ||| xiaoming shi ||| heng qi ||| yanming shen ||| genze wu ||| baocai yin ||| 
2019 ||| traffic sign detection using a multi-scale recurrent attention network. ||| yan tian ||| judith gelernter ||| xun wang ||| jianyuan li ||| yizhou yu ||| 
2022 ||| efficient resource allocation for multi-beam satellite-terrestrial vehicular networks: a multi-agent actor-critic method with attention mechanism. ||| ying he ||| yuhang wang ||| f. richard yu ||| qiuzhen lin ||| jianqiang li ||| victor c. m. leung ||| 
2022 ||| a multi-scale attributes attention model for transport mode identification. ||| guiyuan jiang ||| siew-kei lam ||| peilan he ||| changhai ou ||| dihao ai ||| 
2020 ||| how do drivers allocate their potential attention? driving fixation prediction via convolutional neural networks. ||| tao deng ||| hongmei yan ||| long qin ||| thuyen ngo ||| b. s. manjunath ||| 
2021 ||| using channel-wise attention for deep cnn based real-time semantic segmentation with class-aware edge information. ||| hsiang-yu han ||| yu-chi chen ||| pei-yung hsiao ||| li-chen fu ||| 
2021 ||| a hybrid deep learning model with attention-based conv-lstm networks for short-term traffic flow prediction. ||| haifeng zheng ||| feng lin ||| xinxin feng ||| youjia chen ||| 
2022 ||| subcycle waveform modeling of traffic intersections using recurrent attention networks. ||| yashaswi karnati ||| rahul sengupta ||| anand rangarajan ||| sanjay ranka ||| 
2019 ||| a reference model for driver attention in automation: glance behavior changes during lateral and longitudinal assistance. ||| alberto morando ||| trent victor ||| marco dozza ||| 
2021 ||| a robust attentional framework for license plate recognition in the wild. ||| linjiang zhang ||| peng wang ||| hui li ||| zhen li ||| chunhua shen ||| yanning zhang ||| 
2020 ||| attention-based deep ensemble net for large-scale online taxi-hailing demand prediction. ||| yang liu ||| zhiyuan liu ||| cheng lyu ||| jieping ye ||| 
2020 ||| simultaneous end-to-end vehicle and license plate detection with multi-branch attention neural network. ||| song-lu chen ||| chun yang ||| jia-wei ma ||| feng chen ||| xu-cheng yin ||| 
2018 ||| training tips for the transformer model. ||| martin popel ||| ondrej bojar ||| 
2017 ||| visualizing neural machine translation attention and confidence. ||| matiss rikters ||| mark fishel ||| ondrej bojar ||| 
2017 ||| generating alignments using target foresight in attention-based neural machine translation. ||| jan-thorsten peter ||| arne nix ||| hermann ney ||| 
2020 ||| every layer counts: multi-layer multi-head attention for neural machine translation. ||| isaac k. e. ampomah ||| sally i. mcclean ||| zhiwei lin ||| glenn i. hawe ||| 
2021 ||| spatiotemporal attention enhanced features fusion network for action recognition. ||| danfeng zhuang ||| min jiang ||| jun kong ||| tianshan liu ||| 
2022 ||| domain adaptive attention-based dropout for one-shot person re-identification. ||| xulin song ||| zhong jin ||| 
2021 ||| hierarchical multi-attention networks for document classification. ||| yingren huang ||| jiaojiao chen ||| shaomin zheng ||| yun xue ||| xiaohui hu ||| 
2021 ||| dlsa: dual-learning based on self-attention for rating prediction. ||| fulan qian ||| yafan huang ||| jianhong li ||| chengjun wang ||| shu zhao ||| yanping zhang ||| 
2021 ||| learning to capture contrast in sarcasm with contextual dual-view attention network. ||| lu ren ||| hongfei lin ||| bo xu ||| liang yang ||| dongyu zhang ||| 
2022 ||| mixed attention hourglass network for robust face alignment. ||| zou yang ||| xiongkai shao ||| jun wan ||| rong gao ||| zhihui lai ||| 
2021 ||| a hybrid of xgboost and aspect-based review mining with attention neural network for user preference prediction. ||| chin-hui lai ||| duen-ren liu ||| kun-sin lien ||| 
2019 ||| word-character attention model for chinese text classification. ||| xue qiao ||| chen peng ||| zhen liu ||| yanfeng hu ||| 
2021 ||| chinese medical relation extraction based on multi-hop self-attention mechanism. ||| tongxuan zhang ||| hongfei lin ||| michael m. tadesse ||| yuqi ren ||| xiaodong duan ||| bo xu ||| 
2021 ||| attention-based context aggregation network for monocular depth estimation. ||| yuru chen ||| haitao zhao ||| zhengwei hu ||| jingchao peng ||| 
2021 ||| a novel self-attention deep subspace clustering. ||| zhengfan chen ||| shifei ding ||| haiwei hou ||| 
2021 ||| sequence and graph structure co-awareness via gating mechanism and self-attention for session-based recommendation. ||| jingjing qiao ||| li wang ||| liguo duan ||| 
2019 ||| attention-based argumentation mining. ||| derwin suhartono ||| aryo pradipta gema ||| suhendro winton ||| theodorus david ||| mohamad ivan fanany ||| aniati murni arymurthy ||| 
2019 ||| bio-inspired visual attention process using spiking neural networks controlling a camera. ||| andr |||  cyr ||| fr ||| d ||| ric th ||| riault ||| 
2019 ||| an embodied agent learning affordances with intrinsic motivations and solving extrinsic tasks with attention and one-step planning. ||| gianluca baldassarre ||| william lord ||| giovanni granato ||| vieri giuliano santucci ||| 
2019 ||| attention based visual analysis for fast grasp planning with a multi-fingered robotic hand. ||| zhen deng ||| ge gao ||| simone frintrop ||| fuchun sun ||| changshui zhang ||| jianwei zhang ||| 
2021 ||| a biological inspired cognitive framework for memory-based multi-sensory joint attention in human-robot interactive tasks. ||| omar eldardeer ||| jonas gonzalez-billandon ||| lukas grasse ||| matthew s. tata ||| francesco rea ||| 
2021 ||| dual attention triplet hashing network for image retrieval. ||| zhukai jiang ||| zhichao lian ||| jinping wang ||| 
2022 ||| attention-guided multi-scale feature fusion network for low-light image enhancement. ||| hengshuai cui ||| jinjiang li ||| zhen hua ||| linwei fan ||| 
2021 ||| client-server approach for managing visual attention, integrated in a cognitive architecture for a social robot. ||| francisco mart ||| n ||| jonatan gin ||| s ||| francisco j. rodr ||| guez lera ||| ngel manuel guerrero higueras ||| vicente matell ||| n olivera ||| 
2021 ||| progressive multi-scale vision transformer for facial action unit detection. ||| chongwen wang ||| zicheng wang ||| 
2021 ||| multi-head attention-based long short-term memory for depression detection from speech. ||| yan zhao ||| zhenlin liang ||| jing du ||| li zhang ||| chengyu liu ||| li zhao ||| 
2021 ||| the acquisition of culturally patterned attention styles under active inference. ||| axel constant ||| alexander tschantz ||| beren millidge ||| felipe criado-boado ||| luis m. martinez ||| johannes m ||| ller ||| andy clark ||| 
2019 ||| an attention-controlled hand exoskeleton for the rehabilitation of finger extension and flexion using a rigid-soft combined mechanism. ||| min li ||| bo he ||| ziting liang ||| chen-guang zhao ||| jiazhou chen ||| yueyan zhuo ||| guanghua xu ||| jun xie ||| kaspar althoefer ||| 
2022 ||| query selector-efficient transformer with sparse attention. ||| jacek kl ||| mek ||| jakub klimek ||| witold kraskiewicz ||| mateusz topolewski ||| 
2017 ||| evolutionary multi-objective fault diagnosis of power transformers. ||| abdolrahman peimankar ||| stephen john weddell ||| thahirah jalal ||| andrew craig lapthorn ||| 
2017 ||| estimation of transformer parameters from nameplate data by imperialist competitive and gravitational search algorithms. ||| hazlee azil illias ||| k. j. mou ||| abd halim abu bakar ||| 
2018 ||| multi objective evolutionary algorithm for designing energy efficient distribution transformers. ||| selvaraj tamilselvi ||| subramanian baskar ||| l. anandapadmanaban ||| karthikeyan venkitusamy ||| rajasekar selvamuthukumaran ||| 
2022 ||| an anisotropic non-local attention network for image segmentation. ||| feiniu yuan ||| yaowen zhu ||| kang li ||| zhijun fang ||| jinting shi ||| 
2019 ||| a spatiotemporal attention-based resc3d model for large-scale gesture recognition. ||| yunan li ||| qiguang miao ||| xiangda qi ||| zhenxin ma ||| wanli ouyang ||| 
2018 ||| end-to-end temporal attention extraction and human action recognition. ||| hong zhang ||| miao xin ||| shuhang wang ||| yifan yang ||| lei zhang ||| helong wang ||| 
2021 ||| fae-gan: facial attribute editing with multi-scale attention normalization. ||| jiaqi zhu ||| pengxiang ouyang ||| ran tao ||| xin chen ||| jing wang ||| shu zhan ||| 
2021 ||| fpanet: feature-enhanced position attention network for semantic segmentation. ||| hai-xia xu ||| shuailong wang ||| yunjia huang ||| wei zhou ||| qi chen ||| dongbo zhang ||| 
2022 ||| rca-iunet: a residual cross-spatial attention-guided inception u-net model for tumor segmentation in breast ultrasound imaging. ||| narinder singh punn ||| sonali agarwal ||| 
2021 ||| sa-singan: self-attention for single-image generation adversarial networks. ||| xi chen ||| hongdong zhao ||| dongxu yang ||| yueyuan li ||| qing kang ||| haiyan lu ||| 
2021 ||| lesion-aware attention with neural support vector machine for retinopathy diagnosis. ||| shaik nagur shareef ||| teja krishna cherukuri ||| 
2021 ||| virtual animals as diegetic attention guidance mechanisms in 360-degree experiences. ||| nahal norouzi ||| gerd bruder ||| austin erickson ||| kangsoo kim ||| jeremy n. bailenson ||| pamela j. wisniewski ||| charles e. hughes ||| greg welch ||| 
2019 ||| veram: view-enhanced recurrent attention model for 3d shape classification. ||| songle chen ||| lintao zheng ||| yan zhang ||| zhixin sun ||| kai xu ||| 
2021 ||| attention flows: analyzing and comparing attention mechanisms in language models. ||| joseph f. derose ||| jiayao wang ||| matthew berger ||| 
2022 ||| visqa: x-raying vision and language reasoning in transformers. ||| theo jaunet ||| corentin kervadec ||| romain vuillemot ||| grigory antipov ||| moez baccouche ||| christian wolf ||| 
2020 ||| toward cognitive load inference for attention management in ubiquitous systems. ||| veljko pejovic ||| martin gjoreski ||| christoph anderson ||| klaus david ||| mitja lustrek ||| 
2020 ||| how far are we from quantifying visual attention in mobile hci? ||| mihai b ||| ce ||| sander staal ||| andreas bulling ||| 
2020 ||| attention paid versus paying attention in pervasive computing. ||| joseph a. paradiso ||| daniel p. siewiorek ||| 
2021 ||| robust finger vein recognition based on deep cnn with spatial attention and bias field correction. ||| zhe huang ||| chengan guo ||| 
2021 ||| cpman: change point detection approach in time series based on the prediction of multi-stage attention networks. ||| haizhou du ||| ziyi duan ||| yang zheng ||| 
2019 ||| modelling speaker-dependent auditory attention using a spiking neural network with temporal coding and supervised learning. ||| yating huang ||| jiaming xu ||| bo xu ||| 
2019 ||| safont: automatic font synthesis using self-attention mechanisms. ||| chunying ren ||| shujing lyu ||| hongjian zhan ||| yue lu ||| 
2019 ||| multi-lingual attention based multi-intent detection in dialogue system. ||| mauajama firdaus ||| ankit kumar ||| asif ekbal ||| pushpak bhattacharyya ||| 
2019 ||| a feedback attention network for classification and visualization of thyroid nodules on ultrasound images. ||| jianrong wang ||| ruixuan zhang ||| xi wei ||| xuewei li ||| mei yu ||| jian yu ||| jialin zhu ||| jie gao ||| hongqian shen ||| zhiqiang liu ||| ruiguo yu ||| 
2019 ||| a match-transformer framework for modeling diverse relevance patterns in ad-hoc retrieval. ||| yongyu jiang ||| peng zhang ||| hui gao ||| xindian ma ||| donghao zhao ||| zeting hu ||| junyan wang ||| meixian song ||| 
2019 ||| dissect sliced-rnn in multi-attention view. ||| zeping yu ||| linqing shi ||| gongshen liu ||| 
2019 ||| pyramnet: point cloud pyramid attention network and graph embedding module for classification and segmentation. ||| zhiheng kang ||| ning li ||| 
2021 ||| learning to construct nested polar codes: an attention-based set-to-element model. ||| yang li ||| zhitang chen ||| guochen liu ||| yik-chung wu ||| kai-kit wong ||| 
2021 ||| self-attention-based real-time signal detector for communication systems with unknown channel models. ||| lei chen ||| li sun ||| 
2022 ||| graph attention network-based single-pixel compressive direction of arrival estimation. ||| k ||| rsat tekbiyik ||| okan yurduseven ||| g ||| nes karabulut kurt ||| 
2021 ||| automatic modulation recognition based on adaptive attention mechanism and resnext wsl model. ||| zhi liang ||| mingliang tao ||| ling wang ||| jia su ||| xin yang ||| 
2021 ||| multi-head attention based popularity prediction caching in social content-centric networking with mobile edge computing. ||| jie liang ||| dali zhu ||| haitao liu ||| heng ping ||| ting li ||| hangsheng zhang ||| liru geng ||| yinlong liu ||| 
2020 ||| spatial-temporal attention-convolution network for citywide cellular traffic prediction. ||| nan zhao ||| zhiyang ye ||| yiyang pei ||| ying-chang liang ||| dusit niyato ||| 
2020 ||| radar emitter classification with attention-based multi-rnns. ||| xueqiong li ||| zhangmeng liu ||| zhi-tao huang ||| weisong liu ||| 
2021 ||| st-tran: spatial-temporal transformer for cellular traffic prediction. ||| qingyao liu ||| jianwu li ||| zhaoming lu ||| 
2021 ||| a self-attention-based i/q imbalance estimator for beyond 5g communication systems. ||| sangmi noh ||| dong jin ji ||| dong-ho cho ||| 
2021 ||| attention-based multilevel co-occurrence graph convolutional lstm for 3-d action recognition. ||| shihao xu ||| haocong rao ||| hong peng ||| xin jiang ||| yi guo ||| xiping hu ||| bin hu ||| 
2021 ||| hierarchical-attention-based defense method for load frequency control system against dos attack. ||| yuancheng li ||| rong huang ||| longqiang ma ||| 
2022 ||| environmental sound classification via time-frequency attention and framewise self-attention-based deep neural networks. ||| bo wu ||| xiao-ping zhang ||| 
2019 ||| deep model for store site recommendation with attentional spatial embeddings. ||| yan liu ||| bin guo ||| nuo li ||| jing zhang ||| jingmin chen ||| daqing zhang ||| yinxiao liu ||| zhiwen yu ||| sizhe zhang ||| lina yao ||| 
2021 ||| targeted attention attack on deep learning models in road sign recognition. ||| xinghao yang ||| weifeng liu ||| shengli zhang ||| wei liu ||| dacheng tao ||| 
2020 ||| multimodal alignment and attention-based person search via natural language description. ||| zhong ji ||| shengjia li ||| 
2018 ||| attention-in-attention networks for surveillance video understanding in internet of things. ||| ning xu ||| an-an liu ||| weizhi nie ||| yuting su ||| 
2022 ||| smart diagnosis: deep learning boosted driver inattention detection and abnormal driving prediction. ||| landu jiang ||| wen xie ||| dian zhang ||| tao gu ||| 
2021 ||| an adaptive neurofeedback method for attention regulation based on the internet of things. ||| hanshu cai ||| yi zhang ||| han xiao ||| jian zhang ||| bin hu ||| xiping hu ||| 
2019 ||| an attention mechanism inspired selective sensing framework for physical-cyber mapping in internet of things. ||| huansheng ning ||| xiaozhen ye ||| abdelkarim ben sada ||| lingfeng mao ||| mahmoud daneshmand ||| 
2021 ||| multimodality sentiment analysis in social internet of things based on hierarchical attentions and csat-tcn with mbm network. ||| guorong xiao ||| geng tu ||| lin zheng ||| teng zhou ||| xin li ||| syed hassan ahmed ||| dazhi jiang ||| 
2021 ||| temperature forecasting for stored grain: a deep spatiotemporal attention approach. ||| shanshan duan ||| weidong yang ||| xuyu wang ||| shiwen mao ||| yuan zhang ||| 
2021 ||| distributed attention-based temporal convolutional network for remaining useful life prediction. ||| yan song ||| shengyao gao ||| yibin li ||| lei jia ||| qiqiang li ||| fuzhen pang ||| 
2020 ||| man: mutual attention neural networks model for aspect-level sentiment classification in siot. ||| nan jiang ||| fang tian ||| jin li ||| xu yuan ||| jiaqi zheng ||| 
2020 ||| a novel iot-perceptive human activity recognition (har) approach using multihead convolutional attention. ||| haoxi zhang ||| zhiwen xiao ||| juan wang ||| fei li ||| edward szczerbicki ||| 
2021 ||| efficient covid-19 segmentation from ct slices exploiting semantic segmentation with integrated attention mechanism. ||| mit budak ||| musa  ||| ibuk ||| zafer c ||| mert ||| abdulkadir seng ||| r ||| 
2021 ||| fetal ultrasound image segmentation for automatic head circumference biometry using deeply supervised attention-gated v-net. ||| yan zeng ||| po-hsiang tsui ||| weiwei wu ||| zhuhuang zhou ||| shuicai wu ||| 
2020 ||| the invasiveness classification of ground-glass nodules using 3d attention network and hrct. ||| yangfan ni ||| yuanyuan yang ||| dezhong zheng ||| zhe xie ||| haozhe huang ||| weidong wang ||| 
2021 ||| evolutionary deep attention convolutional neural networks for 2d and 3d medical image segmentation. ||| tahereh hassanzadeh ||| daryl essam ||| ruhul a. sarker ||| 
2022 ||| high accuracy offering attention mechanisms based deep learning approach using cnn/bi-lstm for sentiment analysis. ||| venkateswara rao kota ||| shyamala devi munisamy ||| 
2021 ||| double attention u-net for brain tumor mr image segmentation. ||| na li ||| kai ren ||| 
2021 ||| research on clothing patterns generation based on multi-scales self-attention improved generative adversarial network. ||| zi-yan yu ||| tian-jian luo ||| 
2022 ||| auto-attentional mechanism in multi-domain convolutional neural networks for improving object tracking. ||| jinchao huang ||| 
2020 ||| loss analysis from capacitance between windings in multilayer transformer and loss improvement by winding layer layout considering working voltage. ||| toshiyuki watanabe ||| tetsuya oshikata ||| kimihiro nishijima ||| fujio kurokawa ||| 
2018 ||| the use of new technologies to improve attention in neurodevelopmental disabilities: new educational scenarios for the enhancement of differences. ||| anna maria murdaca ||| rosa angela fabio ||| tindara capr ||| 
2021 ||| focus within or on others: the impact of reviewers' attentional focus on review helpfulness. ||| zhanfei lei ||| dezhi yin ||| han zhang ||| 
2020 ||| content growth and attention contagion in information networks: addressing information poverty on wikipedia. ||| kai zhu ||| dylan walker ||| lev muchnik ||| 
2017 ||| cosearch attention and stock return predictability in supply chains. ||| ashish agarwal ||| alvin chung man leung ||| prabhudev konana ||| alok kumar ||| 
2021 ||| fake news, investor attention, and market reaction. ||| jonathan clarke ||| hailiang chen ||| ding du ||| yu (jeffrey) hu ||| 
2020 ||| 0.3 v 15-ghz band vco ics with novel transformer-based harmonic tuned tanks in 45-nm soi cmos. ||| xiao xu ||| tsuyoshi sugiura ||| toshihiko yoshimasu ||| 
2021 ||| cross-modal retrieval with dual multi-angle self-attention. ||| wenjie li ||| yi zheng ||| yuejie zhang ||| rui feng ||| tao zhang ||| weiguo fan ||| 
2021 ||| hierarchical attention model for personalized tag recommendation. ||| jianshan sun ||| mingyue zhu ||| yuanchun jiang ||| ye-zheng liu ||| le wu ||| 
2021 ||| impact of using bidirectional encoder representations from transformers (bert) models for arabic dialogue acts identification. ||| alaa joukhadar ||| nada ghneim ||| ghaida rebdawi ||| 
2020 ||| sclerasegnet: an attention assisted u-net model for accurate sclera segmentation. ||| caiyong wang ||| yunlong wang ||| yunfan liu ||| zhaofeng he ||| ran he ||| zhenan sun ||| 
2020 ||| detecting multi-scale faces using attention-based feature fusion and smoothed context enhancement. ||| lei shi ||| xiang xu ||| ioannis a. kakadiaris ||| 
2021 ||| attention-based spatial-temporal multi-scale network for face anti-spoofing. ||| wei zheng ||| mengyuan yue ||| shuhuan zhao ||| shuaiqi liu ||| 
2020 ||| evaluation and visualization of driver inattention rating from facial features. ||| isha dua ||| akshay uttama nambi ||| c. v. jawahar ||| venkata n. padmanabhan ||| 
2022 ||| arface: attention-aware and regularization for face recognition with reinforcement learning. ||| liping zhang ||| linjun sun ||| lina yu ||| xiaoli dong ||| jinchao chen ||| weiwei cai ||| chen wang ||| xin ning ||| 
2020 ||| interpretable spatio-temporal attention lstm model for flood forecasting. ||| yukai ding ||| yuelong zhu ||| jun feng ||| pengcheng zhang ||| zirun cheng ||| 
2020 ||| stock movement predictive network via incorporative attention mechanisms based on tweet and historical prices. ||| hongfeng xu ||| lei chai ||| zhiming luo ||| shaozi li ||| 
2021 ||| knowledge augmented transformer for adversarial multidomain multiclassification multimodal fake news detection. ||| chenguang song ||| nianwen ning ||| yunlei zhang ||| bin wu ||| 
2021 ||| nesting spatiotemporal attention networks for action recognition. ||| jiapeng li ||| ping wei ||| nanning zheng ||| 
2021 ||| gated graph neural attention networks for abstractive summarization. ||| zeyu liang ||| junping du ||| yingxia shao ||| houye ji ||| 
2021 ||| a cognitive brain model for multimodal sentiment analysis based on attention neural networks. ||| yuanqing li ||| ke zhang ||| jingyu wang ||| xinbo gao ||| 
2020 ||| bidirectional lstm with self-attention mechanism and multi-channel features for sentiment classification. ||| weijiang li ||| fang qi ||| ming tang ||| zhengtao yu ||| 
2020 ||| multi-level feature fusion based locality-constrained spatial transformer network for video crowd counting. ||| yanyan fang ||| shenghua gao ||| jing li ||| weixin luo ||| linfang he ||| bo hu ||| 
2021 ||| tlsan: time-aware long- and short-term attention network for next-item recommendation. ||| jianqing zhang ||| dongjing wang ||| dongjin yu ||| 
2021 ||| masg-gan: a multi-view attention superpixel-guided generative adversarial network for efficient and simultaneous histopathology image segmentation and classification. ||| huaqi zhang ||| jie liu ||| zekuan yu ||| pengyu wang ||| 
2021 ||| distant supervised relation extraction with position feature attention and selective bag attention. ||| jiasheng wang ||| qiongxin liu ||| 
2018 ||| hierarchical attention-based multimodal fusion for video captioning. ||| chunlei wu ||| yiwei wei ||| xiaoliang chu ||| weichen sun ||| fei su ||| leiquan wang ||| 
2021 ||| a time-frequency channel attention and vectorization network for automatic depression level prediction. ||| mingyue niu ||| bin liu ||| jianhua tao ||| qifei li ||| 
2019 ||| stacked u-shape networks with channel-wise attention for image super-resolution. ||| leilei zhu ||| shu zhan ||| haiyan zhang ||| 
2021 ||| image super-resolution based on residually dense distilled attention network. ||| yujie dun ||| zongyang da ||| shuai yang ||| xueming qian ||| 
2020 ||| knowledge attention sandwich neural network for text classification. ||| zhiqiang zhan ||| zifeng hou ||| qichuan yang ||| jianyu zhao ||| yang zhang ||| changjian hu ||| 
2021 ||| urca-gan: upsample residual channel-wise attention generative adversarial network for image-to-image translation. ||| xuan nie ||| haoxuan ding ||| manhua qi ||| yifei wang ||| edward k. wong ||| 
2021 ||| multi-scale stacking attention pooling for remote sensing scene classification. ||| qi bi ||| han zhang ||| kun qin ||| 
2021 ||| ransp: ranking attention network for saliency prediction on omnidirectional images. ||| dandan zhu ||| yongqing chen ||| xiongkuo min ||| yucheng zhu ||| guokai zhang ||| qiangqiang zhou ||| guangtao zhai ||| xiaokang yang ||| 
2020 ||| video salient object detection via spatiotemporal attention neural networks. ||| yi tang ||| wenbin zou ||| yang hua ||| zhi jin ||| xia li ||| 
2021 ||| a visual place recognition approach using learnable feature map filtering and graph attention networks. ||| cao qin ||| yunzhou zhang ||| yingda liu ||| sonya coleman ||| huijie du ||| dermot kerr ||| 
2021 ||| pdanet: pyramid density-aware attention based network for accurate crowd counting. ||| saeed amirgholipour ||| wenjing jia ||| lei liu ||| xiaochen fan ||| dadong wang ||| xiangjian he ||| 
2021 ||| multi-branch guided attention network for irregular text recognition. ||| cong wang ||| cheng-lin liu ||| 
2021 ||| group multi-scale attention pyramid network for traffic sign detection. ||| lili shen ||| liang you ||| bo peng ||| chuhe zhang ||| 
2022 ||| relaxnet: residual efficient learning and attention expected fusion network for real-time semantic segmentation. ||| jin liu ||| xiaoqing xu ||| yiqing shi ||| cheng deng ||| miaohua shi ||| 
2021 ||| sa-capsgan: using capsule networks with embedded self-attention for generative adversarial network. ||| guangcong sun ||| shifei ding ||| tongfeng sun ||| chenglong zhang ||| 
2021 ||| conciseness is better: recurrent attention lstm model for document-level sentiment analysis. ||| you zhang ||| jin wang ||| xuejie zhang ||| 
2021 ||| attention adjacency matrix based graph convolutional networks for skeleton-based action recognition. ||| jun xie ||| qiguang miao ||| ruyi liu ||| wentian xin ||| lei tang ||| sheng zhong ||| xuesong gao ||| 
2018 ||| a visual attention based roi detection method for facial expression recognition. ||| wenyun sun ||| haitao zhao ||| zhong jin ||| 
2021 ||| image super-resolution using multi-granularity perception and pyramid attention networks. ||| huan wang ||| chengdong wu ||| jianning chi ||| xiaosheng yu ||| qian hu ||| hao wu ||| 
2021 ||| multi-attention based deep neural network with hybrid features for dynamic sequential facial expression recognition. ||| xiao sun ||| pingping xia ||| fuji ren ||| 
2022 ||| semi-supervised classification via full-graph attention neural networks. ||| fei yang ||| huyin zhang ||| shiming tao ||| 
2021 ||| multi-stage attention spatial-temporal graph networks for traffic prediction. ||| xueyan yin ||| genze wu ||| jinze wei ||| yanming shen ||| heng qi ||| baocai yin ||| 
2021 ||| htda: hierarchical time-based directional attention network for sequential user behavior modeling. ||| zhenzhen sheng ||| tao zhang ||| yuejie zhang ||| 
2021 ||| fine-grained question-answer sentiment classification with hierarchical graph attention network. ||| jiandian zeng ||| tianyi liu ||| weijia jia ||| jiantao zhou ||| 
2021 ||| person image generation with attention-based injection network. ||| meichen liu ||| kejun wang ||| ruihang ji ||| shuzhi sam ge ||| jing chen ||| 
2018 ||| a two-level attention-based interaction model for multi-person activity recognition. ||| lihua lu ||| huijun di ||| yao lu ||| lin zhang ||| shunzhou wang ||| 
2021 ||| visual affordance detection using an efficient attention convolutional neural network. ||| qipeng gu ||| jianhua su ||| lei yuan ||| 
2022 ||| instance-level context attention network for instance segmentation. ||| chao shang ||| hongliang li ||| fanman meng ||| heqian qiu ||| qingbo wu ||| linfeng xu ||| king ngi ngan ||| 
2019 ||| daa: dual lstms with adaptive attention for image captioning. ||| fen xiao ||| xue gong ||| yiming zhang ||| yanqing shen ||| jun li ||| xieping gao ||| 
2021 ||| adversarial learning with collaborative attention for facial makeup removal. ||| xueling chen ||| yu zhu ||| yanning zhang ||| 
2020 ||| adaptive attention-aware network for unsupervised person re-identification. ||| wenfeng zhang ||| zhiqiang wei ||| lei huang ||| kezhen xie ||| qibing qin ||| 
2021 ||| tsrgan: real-world text image super-resolution based on adversarial learning and triplet attention. ||| chuantao fang ||| yu zhu ||| lei liao ||| xiaofeng ling ||| 
2021 ||| residual attention graph convolutional network for web services classification. ||| bing li ||| zhi li ||| yilong yang ||| 
2020 ||| polar coordinate sampling-based segmentation of overlapping cervical cells using attention u-net and random walk. ||| han zhang ||| hongqing zhu ||| xiaofeng ling ||| 
2021 ||| attention-based sequence to sequence model for machine remaining useful life prediction. ||| mohamed ragab ||| zhenghua chen ||| min wu ||| chee keong kwoh ||| ruqiang yan ||| xiaoli li ||| 
2021 ||| crowd counting based on attention-guided multi-scale fusion networks. ||| bo zhang ||| naiyao wang ||| zheng zhao ||| ajith abraham ||| hongbo liu ||| 
2022 ||| stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms. ||| hongfeng xu ||| lei chai ||| zhiming luo ||| shaozi li ||| 
2020 ||| deep multi-view residual attention network for crowd flows prediction. ||| hao yuan ||| xinning zhu ||| zheng hu ||| chunhong zhang ||| 
2021 ||| a review on the attention mechanism of deep learning. ||| zhaoyang niu ||| guoqiang zhong ||| hui yu ||| 
2021 ||| d-mmt: a concise decoder-only multi-modal transformer for abstractive summarization in videos. ||| nayu liu ||| xian sun ||| hongfeng yu ||| wenkai zhang ||| guangluan xu ||| 
2019 ||| hybrid attention for chinese character-level neural machine translation. ||| feng wang ||| wei chen ||| zhen yang ||| shuang xu ||| bo xu ||| 
2017 ||| the reward-attention circuit model: nicotine's influence on attentional focus and consequences on attention deficit hyperactivity disorder. ||| karine guimar ||| es ||| daniele q. m. madureira ||| alexandre l. madureira ||| 
2018 ||| period-aware content attention rnns for time series forecasting with missing values. ||| yagmur gizem cinar ||| hamid mirisaee ||| parantapa goswami ||| ric gaussier ||| ali a ||| t-bachir ||| 
2020 ||| deep attention user-based collaborative filtering for recommendation. ||| jie chen ||| xianshuang wang ||| shu zhao ||| fulan qian ||| yanping zhang ||| 
2020 ||| position-aware context attention for session-based recommendation. ||| yi cao ||| weifeng zhang ||| bo song ||| weike pan ||| congfu xu ||| 
2021 ||| automatic fluid segmentation in retinal optical coherence tomography images using attention based deep learning. ||| xiaoming liu ||| shaocheng wang ||| ying zhang ||| dong liu ||| wei hu ||| 
2021 ||| iae-clustergan: a new inverse autoencoder for generative adversarial attention clustering network. ||| chao ling ||| guitao cao ||| wenming cao ||| hong wang ||| he ren ||| 
2020 ||| att-moe: attention-based mixture of experts for nuclear and cytoplasmic segmentation. ||| jinhua liu ||| christian desrosiers ||| yuanfeng zhou ||| 
2021 ||| molecular graph enhanced transformer for retrosynthesis prediction. ||| kelong mao ||| xi xiao ||| tingyang xu ||| yu rong ||| junzhou huang ||| peilin zhao ||| 
2021 ||| exploiting vector attention and context prior for ultrasound image segmentation. ||| lu xu ||| shengbo gao ||| lijuan shi ||| boxuan wei ||| xiaowei liu ||| jicong zhang ||| yihua he ||| 
2021 ||| twilbert: pre-trained deep bidirectional transformers for spanish twitter. ||| jos ||| - ||| ngel gonz ||| lez ||| llu ||| s-f. hurtado ||| ferran pla ||| 
2021 ||| lsi-lstm: an attention-aware lstm for real-time driving destination prediction by considering location semantics and location importance of trajectory points. ||| zhipeng gui ||| yunzeng sun ||| le yang ||| dehua peng ||| fa li ||| huayi wu ||| chi guo ||| wenfei guo ||| jianya gong ||| 
2019 ||| aela-dlstms: attention-enabled and location-aware double lstms for aspect-level sentiment classification. ||| kai shuang ||| xintao ren ||| qianqian yang ||| rui li ||| jonathan loo ||| 
2021 ||| unpaired salient object translation via spatial attention prior. ||| xianfang zeng ||| yusu pan ||| hao zhang ||| mengmeng wang ||| guanzhong tian ||| yong liu ||| 
2021 ||| visual content-enhanced sequential recommendation with feature-level attention. ||| tong qu ||| wanggen wan ||| shoujin wang ||| 
2019 ||| ta-cnn: two-way attention models in deep convolutional neural network for plant recognition. ||| youxiang zhu ||| weiming sun ||| xiangying cao ||| chunyan wang ||| dongyang wu ||| yin yang ||| ning ye ||| 
2021 ||| a recurrent video quality enhancement framework with multi-granularity frame-fusion and frame difference based attention. ||| yongkai huo ||| qiyan lian ||| shaoshi yang ||| jianmin jiang ||| 
2020 ||| human action recognition using convolutional lstm and fully-connected lstm with different attentions. ||| zufan zhang ||| zongming lv ||| chenquan gan ||| qingyi zhu ||| 
2022 ||| hybrid attention-based transformer block model for distant supervision relation extraction. ||| yan xiao ||| yaochu jin ||| ran cheng ||| kuangrong hao ||| 
2021 ||| exploring multi-scale deformable context and channel-wise attention for salient object detection. ||| yi liu ||| mingxing duanmu ||| zhen huo ||| hang qi ||| zuntian chen ||| lei li ||| qiang zhang ||| 
2018 ||| textual sentiment analysis via three different attention convolutional neural networks and cross-modality consistent regression. ||| zu-fan zhang ||| yang zou ||| chenquan gan ||| 
2021 ||| learning graph attention-aware knowledge graph embedding. ||| chen li ||| xutan peng ||| yuhang niu ||| shanghang zhang ||| hao peng ||| chuan zhou ||| jianxin li ||| 
2020 ||| intelligent prognostics of machining tools based on adaptive variational mode decomposition and deep learning method with attention mechanism. ||| chongdang liu ||| linxuan zhang ||| jiahe niu ||| rong yao ||| cheng wu ||| 
2020 ||| dynamic attention network for semantic segmentation. ||| fei wu ||| feng chen ||| xiao-yuan jing ||| chang-hui hu ||| qi ge ||| yimu ji ||| 
2018 ||| feature-enhanced attention network for target-dependent sentiment classification. ||| min yang ||| qiang qu ||| xiaojun chen ||| chaoxue guo ||| ying shen ||| kai lei ||| 
2021 ||| graph transformer networks based text representation. ||| xin mei ||| xiaoyan cai ||| libin yang ||| nanxin wang ||| 
2021 ||| review-based hierarchical attention cooperative neural networks for recommendation. ||| yongping du ||| lulin wang ||| zhi peng ||| wenyang guo ||| 
2022 ||| combining dynamic local context focus and dependency cluster attention for aspect-level sentiment classification. ||| mayi xu ||| biqing zeng ||| heng yang ||| junlong chi ||| jiatao chen ||| hongye liu ||| 
2019 ||| electrode regulating system modeling in electrical smelting furnace using recurrent neural network with attention mechanism. ||| shenyi ding ||| zhijie wang ||| weijian kong ||| honghai yang ||| guangxiao song ||| 
2021 ||| efficient attention based deep fusion cnn for smoke detection in fog environment. ||| lijun he ||| xiaoli gong ||| sirou zhang ||| liejun wang ||| fan li ||| 
2021 ||| context-aware self-attention networks for natural language processing. ||| baosong yang ||| longyue wang ||| derek f. wong ||| shuming shi ||| zhaopeng tu ||| 
2020 ||| tean: timeliness enhanced attention network for session-based recommendation. ||| dongpei chen ||| xingming zhang ||| haoxiang wang ||| weina zhang ||| 
2019 ||| densely convolutional attention network for image super-resolution. ||| furui bai ||| wen lu ||| yuanfei huang ||| lin zha ||| jiachen yang ||| 
2020 ||| fine-grained vehicle type detection and recognition based on dense attention network. ||| xiao ke ||| yufeng zhang ||| 
2021 ||| enhancing two-view correspondence learning by local-global self-attention. ||| luanyuan dai ||| xin liu ||| yizhang liu ||| changcai yang ||| lifang wei ||| yaohai lin ||| riqing chen ||| 
2021 ||| csart: channel and spatial attention-guided residual learning for real-time object tracking. ||| dawei zhang ||| zhonglong zheng ||| minglu li ||| rixian liu ||| 
2020 ||| scale-aware spatial pyramid pooling with both encoder-mask and scale-attention for semantic segmentation. ||| feng zhou ||| yong hu ||| xukun shen ||| 
2020 ||| attention mechanism-based cnn for facial expression recognition. ||| jing li ||| kan jin ||| dalin zhou ||| naoyuki kubota ||| zhaojie ju ||| 
2020 ||| fixed pattern noise reduction for infrared images based on cascade residual attention cnn. ||| juntao guan ||| rui lai ||| ai xiong ||| zesheng liu ||| lin gu ||| 
2022 ||| tcct: tightly-coupled convolutional transformer on time series forecasting. ||| li shen ||| yangzhu wang ||| 
2021 ||| wrtre: weighted relative position transformer for joint entity and relation extraction. ||| wei zheng ||| zhen wang ||| quanming yao ||| xuelong li ||| 
2020 ||| aggregating diverse deep attention networks for large-scale plant species identification. ||| haixi zhang ||| zhenzhong kuang ||| xianlin peng ||| guiqing he ||| jinye peng ||| jianping fan ||| 
2020 ||| micro-attention for micro-expression recognition. ||| chongyang wang ||| min peng ||| tao bi ||| tong chen ||| 
2020 ||| a model with length-variable attention for spoken language understanding. ||| cong xu ||| qing li ||| dezheng zhang ||| jiarui cui ||| zhenqi sun ||| hao zhou ||| 
2020 ||| combining attention-based bidirectional gated recurrent neural network and two-dimensional convolutional neural network for document-level sentiment classification. ||| fagui liu ||| jingzhong zheng ||| lailei zheng ||| cheng chen ||| 
2018 ||| learning better discourse representation for implicit discourse relation recognition via attention networks. ||| biao zhang ||| deyi xiong ||| jinsong su ||| min zhang ||| 
2020 ||| robust visual tracking with channel attention and focal loss. ||| dongdong li ||| gongjian wen ||| yangliu kuai ||| lingxiao zhu ||| fatih porikli ||| 
2017 ||| toward an audiovisual attention model for multimodal video content. ||| naty ould sidaty ||| mohamed-chaker larabi ||| abdelhakim saadane ||| 
2022 ||| heterogeneous graph embedding by aggregating meta-path and meta-structure through attention mechanism. ||| guangxu mei ||| li pan ||| shijun liu ||| 
2019 ||| densely connected attentional pyramid residual network for human pose estimation. ||| yan tian ||| wei hu ||| hangsen jiang ||| jiachen wu ||| 
2019 ||| social recommendation based on users' attention and preference. ||| jiawei chen ||| can wang ||| qihao shi ||| yan feng ||| chun chen ||| 
2021 ||| aspect term extraction for opinion mining using a hierarchical self-attention network. ||| avinash kumar ||| veerubhotla aditya srikanth ||| vishnu teja narapareddy ||| vamshi aruru ||| lalita bhanu murthy neti ||| aruna malapati ||| 
2021 ||| gated pe-nl-ma: a multi-modal attention based network for video understanding. ||| chengyang xie ||| xiaoping wang ||| 
2021 ||| jwsaa: joint weak saliency and attention aware for person re-identification. ||| xin ning ||| ke gong ||| weijun li ||| liping zhang ||| 
2021 ||| summary-aware attention for social media short text abstractive summarization. ||| qianlong wang ||| jiangtao ren ||| 
2018 ||| salient object detection via multi-scale attention cnn. ||| yuzhu ji ||| haijun zhang ||| q. m. jonathan wu ||| 
2020 ||| occluded offline handwritten chinese character inpainting via generative adversarial network and self-attention mechanism. ||| ge song ||| jianwu li ||| zheng wang ||| 
2020 ||| infrared head pose estimation with multi-scales feature fusion on the irhp database for human attention recognition. ||| hai liu ||| xiang wang ||| wei zhang ||| zhaoli zhang ||| youfu li ||| 
2021 ||| multi-scale attention u-net for segmenting clinical target volume in graves' ophthalmopathy. ||| junjie hu ||| ying song ||| lei zhang ||| sen bai ||| zhang yi ||| 
2020 ||| a hierarchical temporal attention-based lstm encoder-decoder model for individual mobility prediction. ||| fa li ||| zhipeng gui ||| zhaoyu zhang ||| dehua peng ||| siyu tian ||| kunxiaojia yuan ||| yunzeng sun ||| huayi wu ||| jianya gong ||| yichen lei ||| 
2022 ||| multi-agent reinforcement learning by the actor-critic model with an attention interface. ||| lixiang zhang ||| jingchen li ||| yi'an zhu ||| haobin shi ||| kao-shing hwang ||| 
2017 ||| integration of touch attention mechanisms to improve the robotic haptic exploration of surfaces. ||| ricardo martins ||| jo ||| o filipe ferreira ||| miguel castelo-branco ||| jorge dias ||| 
2020 ||| face sketch-to-photo transformation with multi-scale self-attention gan. ||| yingtao lei ||| weiwei du ||| qinghua hu ||| 
2021 ||| soft sensor based on extreme gradient boosting and bidirectional converted gates long short-term memory self-attention network. ||| xiuli zhu ||| kuangrong hao ||| ruimin xie ||| biao huang ||| 
2020 ||| adaptive embedding gate for attention-based scene text recognition. ||| xiaoxue chen ||| tianwei wang ||| yuanzhi zhu ||| lianwen jin ||| canjie luo ||| 
2021 ||| attention-based full slice brain ct image diagnosis with explanations. ||| guanghui fu ||| jianqiang li ||| ruiqian wang ||| yue ma ||| yueda chen ||| 
2018 ||| fine-grained attention mechanism for neural machine translation. ||| heeyoul choi ||| kyunghyun cho ||| yoshua bengio ||| 
2022 ||| multi actor hierarchical attention critic with rnn-based feature extraction. ||| dianxi shi ||| chenran zhao ||| yajie wang ||| huanhuan yang ||| gongju wang ||| hao jiang ||| chao xue ||| shaowu yang ||| yongjun zhang ||| 
2020 ||| abstractive social media text summarization using selective reinforced seq2seq attention model. ||| zeyu liang ||| junping du ||| chaoyang li ||| 
2020 ||| a two-stage attention aware method for train bearing shed oil inspection based on convolutional neural networks. ||| xiao fu ||| kenli li ||| jing liu ||| keqin li ||| zeng zeng ||| cen chen ||| 
2020 ||| directional attention weaving for text-grounded conversational question answering. ||| ronghan li ||| zejun jiang ||| lifang wang ||| xinyu lu ||| meng zhao ||| 
2019 ||| scar: spatial-/channel-wise attention regression networks for crowd counting. ||| junyu gao ||| qi wang ||| yuan yuan ||| 
2017 ||| salient region detection via locally smoothed label propagation: with application to attention driven image abstraction. ||| hong li ||| enhua wu ||| wen wu ||| 
2020 ||| attention-guided dual spatial-temporal non-local network for video super-resolution. ||| wei sun ||| yanning zhang ||| 
2021 ||| a transfer approach with attention reptile method and long-term generation mechanism for few-shot traffic prediction. ||| chujie tian ||| xinning zhu ||| zheng hu ||| jian ma ||| 
2021 ||| exploiting dependency information to improve biomedical event detection via gated polar attention mechanism. ||| lishuang li ||| beibei zhang ||| 
2021 ||| filter gate network based on multi-head attention for aspect-level sentiment classification. ||| ziyu zhou ||| fang'ai liu ||| 
2022 ||| plam: a plug-in module for flexible graph attention learning. ||| xuran pan ||| shiji song ||| yiming chen ||| liejun wang ||| gao huang ||| 
2022 ||| joint usage of global and local attentions in hourglass network for human pose estimation. ||| xiena dong ||| jun yu ||| jian zhang ||| 
2019 ||| multi-resolution attention convolutional neural network for crowd counting. ||| youmei zhang ||| chunluan zhou ||| faliang chang ||| alex c. kot ||| 
2021 ||| automatic depression recognition using cnn with attention mechanism from videos. ||| lang he ||| jonathan cheung-wai chan ||| zhongmin wang ||| 
2021 ||| contextual similarity-based multi-level second-order attention network for semi-supervised few-shot learning. ||| wenjing li ||| tingting ren ||| fang li ||| jun zhang ||| zhongcheng wu ||| 
2020 ||| adcm: attention dropout convolutional module. ||| zhigang liu ||| juan du ||| mei wang ||| shuzhi sam ge ||| 
2020 ||| stochastic region pooling: make attention more expressive. ||| mingnan luo ||| guihua wen ||| yang hu ||| dan dai ||| yingxue xu ||| 
2021 ||| micro-expression action unit detection with spatial and channel attention. ||| yante li ||| xiaohua huang ||| guoying zhao ||| 
2021 ||| streamer action recognition in live video with spatial-temporal attention and deep dictionary learning. ||| chenhao li ||| jing zhang ||| jiacheng yao ||| 
2020 ||| question-led object attention for visual question answering. ||| lianli gao ||| liangfu cao ||| xing xu ||| jie shao ||| jingkuan song ||| 
2018 ||| boosting image sentiment analysis with visual attention. ||| kaikai song ||| ting yao ||| qiang ling ||| tao mei ||| 
2020 ||| xnet: task-specific attentional domain adaptation for satellite-to-aerial scene. ||| jianzhe lin ||| kaiwen yuan ||| rabab ward ||| zhen jane wang ||| 
2021 ||| target relational attention-oriented knowledge graph reasoning. ||| xiaojuan zhao ||| yan jia ||| aiping li ||| rong jiang ||| kai chen ||| ye wang ||| 
2020 ||| short-term traffic speed forecasting based on graph attention temporal convolutional networks. ||| ge guo ||| wei yuan ||| 
2021 ||| mask-guided contrastive attention and two-stream metric co-learning for person re-identification. ||| chunfeng song ||| caifeng shan ||| yan huang ||| liang wang ||| 
2022 ||| non-tumorous facial pigmentation classification based on multi-view convolutional neural network with attention mechanism. ||| yingjie tian ||| shiding sun ||| zhiquan qi ||| ying liu ||| zeyuan wang ||| 
2020 ||| unsupervised depth estimation from monocular videos with hybrid geometric-refined loss and contextual attention. ||| mingliang zhang ||| xinchen ye ||| xin fan ||| wei zhong ||| 
2020 ||| a non-parametric softmax for improving neural attention in time-series forecasting. ||| simone totaro ||| amir hussain ||| simone scardapane ||| 
2022 ||| remove and recover: deep end-to-end two-stage attention network for single-shot heavy rain removal. ||| woo-jin ahn ||| tae-koo kang ||| hyun duck choi ||| myo taeg lim ||| 
2021 ||| visual relationship detection with recurrent attention and negative sampling. ||| lei wang ||| peizhen lin ||| jun cheng ||| feng liu ||| xiaoliang ma ||| jian yin ||| 
2021 ||| deep saliency detection via spatial-wise dilated convolutional attention. ||| wenzhao cui ||| qing zhang ||| baochuan zuo ||| 
2021 ||| exploring attention mechanisms based on summary information for end-to-end automatic speech recognition. ||| jiabin xue ||| tieran zheng ||| jiqing han ||| 
2019 ||| optical flow estimation using channel attention mechanism and dilated convolutional neural networks. ||| mingliang zhai ||| xuezhi xiang ||| rongfang zhang ||| ning lv ||| abdulmotaleb el-saddik ||| 
2020 ||| crowd counting with crowd attention convolutional neural network. ||| jiwei chen ||| wen su ||| zengfu wang ||| 
2021 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| zhichao zhang ||| shugong xu ||| shunqing zhang ||| tianhao qiao ||| shan cao ||| 
2022 ||| ccaffmnet: dual-spectral semantic segmentation network with channel-coordinate attention feature fusion module. ||| shi yi ||| junjie li ||| xi liu ||| xuesong yuan ||| 
2022 ||| progressively real-time video salient object detection via cascaded fully convolutional networks with motion attention. ||| qingping zheng ||| ying li ||| ling zheng ||| qiang shen ||| 
2022 |||  attention network for video-based person re-identification. ||| chenrui zhang ||| ping chen ||| tao lei ||| yangxu wu ||| hongying meng ||| 
2022 ||| enhancing structure modeling for relation extraction with fine-grained gating and co-attention. ||| yubo chen ||| chuhan wu ||| yongfeng huang ||| 
2019 ||| vd-san: visual-densely semantic attention network for image caption generation. ||| xinwei he ||| yang yang ||| baoguang shi ||| xiang bai ||| 
2022 ||| mpan: multi-parallel attention network for session-based recommendation. ||| tianzi zang ||| yanmin zhu ||| jing zhu ||| yanan xu ||| haobing liu ||| 
2022 ||| segdq: segmentation assisted multi-object tracking with dynamic query-based transformers. ||| yating liu ||| tianxiang bai ||| yonglin tian ||| yutong wang ||| jiangong wang ||| xiao wang ||| fei-yue wang ||| 
2021 ||| attention-aware concentrated network for saliency prediction. ||| pengqian li ||| xiaofen xing ||| xiangmin xu ||| bolun cai ||| jun cheng ||| 
2021 ||| a novel transferability attention neural network model for eeg emotion recognition. ||| yang li ||| boxun fu ||| fu li ||| guangming shi ||| wenming zheng ||| 
2021 ||| bidirectional gated temporal convolution with attention for text classification. ||| jiansi ren ||| wei wu ||| gang liu ||| zhe chen ||| ruoxiang wang ||| 
2018 ||| image captioning with triple-attention and stack parallel lstm. ||| xinxin zhu ||| lixiang li ||| jing liu ||| ziyi li ||| haipeng peng ||| xinxin niu ||| 
2020 ||| scene image retrieval with siamese spatial attention pooling. ||| jinyu ma ||| xiaodong gu ||| 
2021 ||| detector-tracker integration framework and attention mechanism for multi-object tracking. ||| chunjiang li ||| guangzhu chen ||| rongsong gou ||| zaizuo tang ||| 
2018 ||| attention based collaborative filtering. ||| mingsheng fu ||| hong qu ||| dagmawi moges ||| li lu ||| 
2021 ||| da-dsunet: dual attention-based dense su-net for automatic head-and-neck tumor segmentation in mri images. ||| pin tang ||| chen zu ||| mei hong ||| rui yan ||| xingchen peng ||| jianghong xiao ||| xi wu ||| jiliu zhou ||| luping zhou ||| yan wang ||| 
2021 ||| on the diversity of multi-head attention. ||| jian li ||| xing wang ||| zhaopeng tu ||| michael r. lyu ||| 
2020 ||| portfolio trading system of digital currencies: a deep reinforcement learning with multidimensional attention gating mechanism. ||| liguo weng ||| xudong sun ||| min xia ||| jia liu ||| yiqing xu ||| 
2017 ||| emotion-modulated attention improves expression recognition: a deep learning model. ||| pablo v. a. barros ||| german ignacio parisi ||| cornelius weber ||| stefan wermter ||| 
2021 ||| incorporating sentimental trend into gated mechanism based transformer network for story ending generation. ||| linzhang mo ||| jielong wei ||| qingbao huang ||| yi cai ||| qingguang liu ||| xingmao zhang ||| qing li ||| 
2020 ||| attention-based face alignment: a solution to speed/accuracy trade-off. ||| teng wang ||| xinjie tong ||| wenzhe cai ||| 
2019 ||| single image super-resolution via multi-scale residual channel attention network. ||| feilong cao ||| huan liu ||| 
2022 ||| acort: a compact object relation transformer for parameter efficient image captioning. ||| jia huei tan ||| ying hua tan ||| chee seng chan ||| joon huang chuah ||| 
2021 ||| accelerated masked transformer for dense video captioning. ||| zhou yu ||| nanjia han ||| 
2020 ||| food det: detecting foods in refrigerator with supervised transformer network. ||| yousong zhu ||| xu zhao ||| chaoyang zhao ||| jinqiao wang ||| hanqing lu ||| 
2020 ||| the scale-invariant space for attention layer in neural network. ||| yue wang ||| yuting liu ||| zhi-ming ma ||| 
2021 ||| long- and short-term self-attention network for sequential recommendation. ||| chengfeng xu ||| jian feng ||| pengpeng zhao ||| fuzhen zhuang ||| deqing wang ||| yanchi liu ||| victor s. sheng ||| 
2020 ||| sarpnet: shape attention regional proposal network for lidar-based 3d object detection. ||| yangyang ye ||| houjin chen ||| chi zhang ||| xiaoli hao ||| zhaoxiang zhang ||| 
2021 ||| residual attention and other aspects module for aspect-based sentiment analysis. ||| chao wu ||| qingyu xiong ||| zhengyi yang ||| min gao ||| qiude li ||| yang yu ||| kaige wang ||| qiwu zhu ||| 
2022 ||| scan: a shared causal attention network for adverse drug reactions detection in tweets. ||| humayun kayesh ||| md. saiful islam ||| junhu wang ||| ryoma j. ohira ||| zhe wang ||| 
2020 ||| aspect-based sentiment classification with multi-attention network. ||| qiannan xu ||| li zhu ||| tao dai ||| chengbing yan ||| 
2020 ||| modeling bottom-up and top-down attention with a neurodynamic model of v1. ||| david berga ||| xavier otazu ||| 
2020 ||| fused gru with semantic-temporal attention for video captioning. ||| lianli gao ||| xuanhan wang ||| jingkuan song ||| yang liu ||| 
2019 ||| a hierarchical attention model for rating prediction by leveraging user and product reviews. ||| shuning xing ||| fang'ai liu ||| qianqian wang ||| xiaohui zhao ||| tianlai li ||| 
2019 ||| eeg model stability and online decoding of attentional demand during gait using gamma band features. ||| a. costa-garc ||| a ||| eduardo i ||| ez ||| antonio j. del ama ||| ngel gil-agudo ||| jos |||  mar ||| a azor ||| n ||| 
2019 ||| self-attention based speaker recognition using cluster-range loss. ||| tengyue bian ||| fangzhou chen ||| li xu ||| 
2021 ||| self-attention feature fusion network for semantic segmentation. ||| zhen zhou ||| yan zhou ||| dongli wang ||| jinzhen mu ||| haibin zhou ||| 
2020 ||| tag recommendation by text classification with attention-based capsule network. ||| kai lei ||| qiuai fu ||| min yang ||| yuzhi liang ||| 
2021 ||| self-supervised video object segmentation using integration-augmented attention. ||| wenjun zhu ||| jun meng ||| li xu ||| 
2020 ||| saanet: siamese action-units attention network for improving dynamic facial expression recognition. ||| daizong liu ||| xi ouyang ||| shuangjie xu ||| pan zhou ||| kun he ||| shiping wen ||| 
2021 ||| label-guided attention distillation for lane segmentation. ||| zhikang liu ||| lanyun zhu ||| 
2021 ||| adaptive multi-level feature fusion and attention-based network for arbitrary-oriented object detection in remote sensing imagery. ||| luchang chen ||| chunsheng liu ||| faliang chang ||| shuang li ||| zhaoying nie ||| 
2021 ||| enhancing social recommendation via two-level graph attentional networks. ||| yanbin jiang ||| huifang ma ||| yuhang liu ||| zhixin li ||| liang chang ||| 
2021 ||| single image super-resolution with attention-based densely connected module. ||| zijian wang ||| yao lu ||| weiqi li ||| shunzhou wang ||| xuebo wang ||| xiaozhen chen ||| 
2020 ||| learning from discrete gaussian label distribution and spatial channel-aware residual attention for head pose estimation. ||| yi zhang ||| keren fu ||| jiang wang ||| peng cheng ||| 
2022 ||| ecanet: explicit cyclic attention-based network for video saliency prediction. ||| hao xue ||| minghui sun ||| yanhua liang ||| 
2021 ||| context-aware positional representation for self-attention networks. ||| kehai chen ||| rui wang ||| masao utiyama ||| eiichiro sumita ||| 
2020 ||| traffic scene semantic segmentation using self-attention mechanism and bi-directional gru to correlate context. ||| min yan ||| junzheng wang ||| jing li ||| ke zhang ||| zimu yang ||| 
2022 ||| co-attention network with label embedding for text classification. ||| minqian liu ||| lizhao liu ||| junyi cao ||| qing du ||| 
2020 ||| gssa: pay attention to graph feature importance for gcn via statistical self-attention. ||| jin zheng ||| yang wang ||| wanjun xu ||| zilu gan ||| ping li ||| jiancheng lv ||| 
2018 ||| lattice-to-sequence attentional neural machine translation models. ||| zhixing tan ||| jinsong su ||| boli wang ||| yidong chen ||| xiaodong shi ||| 
2021 ||| local and correlation attention learning for subtle facial expression recognition. ||| shaocong wang ||| yuan yuan ||| xiangtao zheng ||| xiaoqiang lu ||| 
2021 ||| multi-level dictionary learning for fine-grained images categorization with attention model. ||| jinsheng ji ||| yiyou guo ||| zhen yang ||| tao zhang ||| xiankai lu ||| 
2021 ||| a method of traffic police detection based on attention mechanism in natural scene. ||| ying zheng ||| hong bao ||| chaochao meng ||| nan ma ||| 
2022 ||| multiscale face recognition in cluttered backgrounds based on visual attention. ||| peng guo ||| guoqing du ||| longsheng wei ||| huaiying lu ||| siwei chen ||| changxin gao ||| ying chen ||| jinsheng li ||| dapeng luo ||| 
2020 ||| two-branch fusion network with attention map for crowd counting. ||| yongjie wang ||| wei zhang ||| yanyan liu ||| jianghua zhu ||| 
2021 ||| gapointnet: graph attention based point neural network for exploiting local feature of point cloud. ||| can chen ||| luca zanotti fragonara ||| antonios tsourdos ||| 
2020 ||| bilstm with multi-polarity orthogonal attention for implicit sentiment analysis. ||| jiyao wei ||| jian liao ||| zhenfei yang ||| suge wang ||| qiang zhao ||| 
2022 ||| kaicd: a knowledge attention-based deep learning framework for automatic icd coding. ||| yifan wu ||| min zeng ||| zhihui fei ||| ying yu ||| fang-xiang wu ||| min li ||| 
2020 ||| attention-based bidirectional gated recurrent unit neural networks for well logs prediction and lithology identification. ||| lili zeng ||| weijian ren ||| liqun shan ||| 
2020 ||| deep attention based music genre classification. ||| yang yu ||| sen luo ||| shenglan liu ||| hong qiao ||| yang liu ||| lin feng ||| 
2020 ||| alstm: an attention-based long short-term memory framework for knowledge base reasoning. ||| qi wang ||| yongsheng hao ||| 
2020 ||| aplnet: attention-enhanced progressive learning network. ||| hui zhang ||| danqing kang ||| haibo he ||| fei-yue wang ||| 
2021 ||| residual attention-based multi-scale script identification in scene text images. ||| mengkai ma ||| qiu-feng wang ||| shan huang ||| shen huang ||| yannis goulermas ||| kaizhu huang ||| 
2021 ||| element graph-augmented abstractive summarization for legal public opinion news with graph transformer. ||| yuxin huang ||| zhengtao yu ||| junjun guo ||| yan xiang ||| yantuan xian ||| 
2021 ||| attention-aware conditional generative adversarial networks for facial age synthesis. ||| xiahui chen ||| yunlian sun ||| xiangbo shu ||| qi li ||| 
2021 ||| towards accurate rgb-d saliency detection with complementary attention and adaptive integration. ||| hongbo bi ||| zi-qi liu ||| kang wang ||| bo dong ||| geng chen ||| jiquan ma ||| 
2019 ||| end-to-end semantic-aware object retrieval based on region-wise attention. ||| xiu li ||| kun jin ||| rujiao long ||| 
2021 ||| automatic segmentation of gross target volume of nasopharynx cancer using ensemble of multiscale deep neural networks with spatial attention. ||| haochen mei ||| wenhui lei ||| ran gu ||| shan ye ||| zhengwentai sun ||| shichuan zhang ||| guotai wang ||| 
2021 ||| lrdnet: a lightweight and efficient network with refined dual attention decorder for real-time semantic segmentation. ||| mingxi zhuang ||| xunyu zhong ||| dongbing gu ||| liying feng ||| xungao zhong ||| huosheng hu ||| 
2021 ||| attm-cnn: attention and metric learning based cnn for pornography, age and child sexual abuse (csa) detection in images. ||| abhishek gangwar ||| v ||| ctor gonz ||| lez-castro ||| enrique alegre ||| eduardo fidalgo ||| 
2020 ||| radc-net: a residual attention based convolution network for aerial scene classification. ||| qi bi ||| kun qin ||| han zhang ||| zhili li ||| kai xu ||| 
2022 ||| skeleton-based abnormal gait recognition with spatio-temporal attention enhanced gait-structural graph convolutional networks. ||| haoyu tian ||| xin ma ||| hanbo wu ||| yi-bin li ||| 
2021 ||| daeanet: dual auto-encoder attention network for depth map super-resolution. ||| xiang cao ||| yihao luo ||| xianyi zhu ||| liangqi zhang ||| yan xu ||| haibo shen ||| tianjiang wang ||| qi feng ||| 
2022 ||| ganlda: graph attention network for lncrna-disease associations prediction. ||| wei lan ||| ximin wu ||| qingfeng chen ||| wei peng ||| jianxin wang ||| yi-ping phoebe chen ||| 
2018 ||| deep transformer: a framework for 2d text image rectification from planar transformations. ||| chengzhe yan ||| jie hu ||| changshui zhang ||| 
2020 ||| joint multi-level attentional model for emotion detection and emotion-cause pair extraction. ||| hao tang ||| donghong ji ||| qiji zhou ||| 
2021 ||| adaptive attention augmentor for weakly supervised object localization. ||| longhao zhang ||| huihua yang ||| 
2021 ||| ttpp: temporal transformer with progressive prediction for efficient action anticipation. ||| wen wang ||| xiaojiang peng ||| yanzhou su ||| yu qiao ||| jian cheng ||| 
2019 ||| dual residual attention module network for single image super resolution. ||| xiumei wang ||| yanan gu ||| xinbo gao ||| zheng hui ||| 
2019 ||| recurrent convolutional video captioning with global and local attention. ||| tao jin ||| yingming li ||| zhongfei zhang ||| 
2022 ||| point cloud recognition based on lightweight embeddable attention module. ||| guanyu zhu ||| yong zhou ||| jiaqi zhao ||| rui yao ||| man zhang ||| 
2020 ||| joint image deblurring and super-resolution with attention dual supervised network. ||| dongyang zhang ||| zhenwen liang ||| jie shao ||| 
2021 ||| mafnet: multi-style attention fusion network for salient object detection. ||| yanhua liang ||| guihe qin ||| minghui sun ||| jie yan ||| huiming jiang ||| 
2020 ||| unsupervised domain adaptation with self-attention for post-disaster building damage detection. ||| yundong li ||| chen lin ||| hongguang li ||| wei hu ||| han dong ||| yi liu ||| 
2020 ||| a holistic representation guided attention network for scene text recognition. ||| lu yang ||| peng wang ||| hui li ||| zhen li ||| yanning zhang ||| 
2021 ||| palmprint orientation field recovery via attention-based generative adversarial network. ||| bing liu ||| jufu feng ||| 
2022 ||| hierarchical multimodal transformer to summarize videos. ||| bin zhao ||| maoguo gong ||| xuelong li ||| 
2020 ||| dcgsa: a global self-attention network with dilated convolution for crowd density map generating. ||| liping zhu ||| chengyang li ||| bing wang ||| kun yuan ||| zhongguo yang ||| 
2021 ||| on combining acoustic and modulation spectrograms in an attention lstm-based system for speech intelligibility level classification. ||| ascensi ||| n gallardo-antol ||| n ||| juan manuel montero ||| 
2021 ||| multi-scale graph attention subspace clustering network. ||| tong wang ||| junhua wu ||| zhenquan zhang ||| wen zhou ||| guang chen ||| shasha liu ||| 
2021 ||| attention based multilayer feature fusion convolutional neural network for unsupervised monocular depth estimation. ||| zeyu lei ||| yan wang ||| zijian li ||| junyao yang ||| 
2021 ||| ast-gnn: an attention-based spatio-temporal graph neural network for interaction-aware pedestrian trajectory prediction. ||| hao zhou ||| dongchun ren ||| huaxia xia ||| mingyu fan ||| xu yang ||| hai huang ||| 
2022 ||| automatic and accurate segmentation of peripherally inserted central catheter (picc) from chest x-rays using multi-stage attention-guided learning. ||| xiaoyan wang ||| luyao wang ||| ye sheng ||| chenglu zhu ||| nan jiang ||| cong bai ||| ming xia ||| zhanpeng shao ||| zheng gu ||| xiaojie huang ||| ruiyi zhao ||| zhenjie liu ||| 
2021 ||| residual scale attention network for arbitrary scale image super-resolution. ||| ying fu ||| jian chen ||| tao zhang ||| yonggang lin ||| 
2020 ||| exploiting geographical-temporal awareness attention for next point-of-interest recommendation. ||| tongcun liu ||| jianxin liao ||| zhigen wu ||| yulong wang ||| jingyu wang ||| 
2021 ||| fusion layer attention for image-text matching. ||| depeng wang ||| liejun wang ||| shiji song ||| gao huang ||| yuchen guo ||| shuli cheng ||| naixiang ao ||| anyu du ||| 
2020 ||| tnam: a tag-aware neural attention model for top-n recommendation. ||| ruoran huang ||| nian wang ||| chuanqi han ||| fang yu ||| li cui ||| 
2017 ||| attention region detection based on closure prior in layered bit planes. ||| jiazhong chen ||| bingpeng ma ||| hua cao ||| jie chen ||| yebin fan ||| tao xia ||| rong li ||| 
2020 ||| cross-modal image fusion guided by subjective visual attention. ||| aiqing fang ||| xinbo zhao ||| yanning zhang ||| 
2021 ||| a comparative study of language transformers for video question answering. ||| zekun yang ||| noa garcia ||| chenhui chu ||| mayu otani ||| yuta nakashima ||| haruo takemura ||| 
2020 ||| multivariate time series forecasting via attention-based encoder-decoder framework. ||| shengdong du ||| tianrui li ||| yan yang ||| shi-jinn horng ||| 
2019 ||| topical co-attention networks for hashtag recommendation on microblogs. ||| yang li ||| ting liu ||| jingwen hu ||| jing jiang ||| 
2021 ||| anisotropic angle distribution learning for head pose estimation and attention understanding in human-computer interaction. ||| hai liu ||| hanwen nie ||| zhaoli zhang ||| youfu li ||| 
2020 ||| attention augmentation with multi-residual in bidirectional lstm. ||| ye wang ||| xinxiang zhang ||| mi lu ||| han wang ||| yoonsuck choe ||| 
2020 ||| multistage attention network for multivariate time series prediction. ||| jun hu ||| wendong zheng ||| 
2021 ||| scsa-net: presentation of two-view reliable correspondence learning via spatial-channel self-attention. ||| xin liu ||| guobao xiao ||| luanyuan dai ||| kun zeng ||| changcai yang ||| riqing chen ||| 
2021 ||| harmonious attention network for person re-identification via complementarity between groups and individuals. ||| lin chen ||| hua yang ||| qiling xu ||| zhiyong gao ||| 
2022 ||| towards more effective prm-based crowd counting via a multi-resolution fusion and attention network. ||| usman sajid ||| guanghui wang ||| 
2021 ||| a lightweight multi-scale channel attention network for image super-resolution. ||| wenbin li ||| juefei li ||| jinxin li ||| zhiyong huang ||| dengwen zhou ||| 
2021 ||| pairwise attention network for cross-domain image recognition. ||| zan gao ||| yanbo liu ||| guangping xu ||| xianbin wen ||| 
2020 ||| multi-attention guided feature fusion network for salient object detection. ||| anni li ||| jinqing qi ||| huchuan lu ||| 
2020 ||| spatial attention based visual semantic learning for action recognition in still images. ||| yunpeng zheng ||| xiangtao zheng ||| xiaoqiang lu ||| siyuan wu ||| 
2019 ||| extension of reward-attention circuit model: alcohol's influence on attentional focus and consequences on autism spectrum disorder. ||| karine guimar ||| es ||| 
2020 ||| multi-attention deep reinforcement learning and re-ranking for vehicle re-identification. ||| yu liu ||| jianbing shen ||| haibo he ||| 
2021 ||| automatic ultrasound image report generation with adaptive multimodal attention mechanism. ||| shaokang yang ||| jianwei niu ||| jiyan wu ||| yong wang ||| xuefeng liu ||| qingfeng li ||| 
2020 ||| epan: effective parts attention network for scene text recognition. ||| yunlong huang ||| zenghui sun ||| lianwen jin ||| canjie luo ||| 
2021 ||| facial image inpainting using attention-based multi-level generative network. ||| jie liu ||| cheolkon jung ||| 
2020 ||| wavelet-based residual attention network for image super-resolution. ||| shengke xue ||| wenyuan qiu ||| fan liu ||| xinyu jin ||| 
2021 ||| multi-view spectral graph convolution with consistent edge attention for molecular modeling. ||| chao shang ||| qinqing liu ||| qianqian tong ||| jiangwen sun ||| minghu song ||| jinbo bi ||| 
2020 ||| recurrent reverse attention guided residual learning for saliency object detection. ||| tengpeng li ||| huihui song ||| kaihua zhang ||| qingshan liu ||| 
2022 ||| adapted transformer network for news recommendation. ||| jinsheng huang ||| zhuobing han ||| hongyan xu ||| hongtao liu ||| 
2020 ||| a trajectory-based attention model for sequential impurity detection. ||| wenhao he ||| haitao song ||| yue guo ||| xiaonan wang ||| guibin bian ||| kui yuan ||| 
2021 ||| adaptive multi-scale dual attention network for semantic segmentation. ||| weizhen wang ||| suyu wang ||| yue li ||| yishu jin ||| 
2020 ||| sa-nli: a supervised attention based framework for natural language inference. ||| peiguang li ||| hongfeng yu ||| wenkai zhang ||| guangluan xu ||| xian sun ||| 
2022 ||| jac-net: joint learning with adaptive exploration and concise attention for unsupervised domain adaptive person re-identification. ||| yingchun guo ||| fang feng ||| xiaoke hao ||| xi chen ||| 
2021 ||| self-supervised attention flow for dialogue state tracking. ||| boyuan pan ||| yazheng yang ||| bo li ||| deng cai ||| 
2021 ||| environment sound classification using an attention-based residual neural network. ||| achyut mani tripathi ||| aakansha mishra ||| 
2021 ||| attention-based interpolation network for video deblurring. ||| xiaoqin zhang ||| runhua jiang ||| tao wang ||| pengcheng huang ||| li zhao ||| 
2021 ||| self-attention guided representation learning for image-text matching. ||| xuefei qi ||| ying zhang ||| jinqing qi ||| huchuan lu ||| 
2020 ||| point clouds learning with attention-based graph convolution networks. ||| zhuyang xie ||| junzhou chen ||| bo peng ||| 
2020 ||| a window-based self-attention approach for sentence encoding. ||| ting huang ||| zhi-hong deng ||| gehui shen ||| xi chen ||| 
2021 ||| single image haze removal via attention-based transmission estimation and classification fusion network. ||| shan wang ||| libao zhang ||| xiaohan wang ||| 
2021 ||| geometric attentional dynamic graph convolutional neural networks for point cloud analysis. ||| yiming cui ||| xin liu ||| hongmin liu ||| jiyong zhang ||| alina zare ||| bin fan ||| 
2021 ||| spatiotemporal and frequential cascaded attention networks for speech emotion recognition. ||| shuzhen li ||| xiaofen xing ||| weiquan fan ||| bolun cai ||| perry fordson ||| xiangmin xu ||| 
2021 ||| mscan: multimodal self-and-collaborative attention network for image aesthetic prediction tasks. ||| xiaodan zhang ||| xinbo gao ||| lihuo he ||| wen lu ||| 
2021 ||| predicting short-term next-active-object through visual attention and hand position. ||| jingjing jiang ||| zhixiong nan ||| hui chen ||| shitao chen ||| nanning zheng ||| 
2021 ||| automatic whole slide pathology image diagnosis framework via unit stochastic selection and attention fusion. ||| pingjun chen ||| yun liang ||| xiaoshuang shi ||| lin yang ||| paul d. gader ||| 
2022 ||| an accurate box localization method based on rotated-rpn with weighted edge attention for bin picking. ||| fengqin yao ||| shengke wang ||| rui li ||| long chen ||| feng gao ||| junyu dong ||| 
2022 ||| cephalometric landmark detection via global and local encoders and patch-wise attentions. ||| minkyung lee ||| minyoung chung ||| yeong-gil shin ||| 
2021 ||| attcry: attention-based neural network model for protein crystallization prediction. ||| chen jin ||| jianzhao gao ||| zhuangwei shi ||| han zhang ||| 
2020 ||| dilated residual networks with multi-level attention for speaker verification. ||| yanfeng wu ||| chenkai guo ||| hongcan gao ||| jing xu ||| guangdong bai ||| 
2020 ||| attention shake siamese network with auxiliary relocation branch for visual object tracking. ||| jun wang ||| weibin liu ||| weiwei xing ||| liqiang wang ||| shunli zhang ||| 
2021 ||| joint structured pruning and dense knowledge distillation for efficient transformer model compression. ||| baiyun cui ||| yingming li ||| zhongfei zhang ||| 
2022 ||| global-guided asymmetric attention network for image-text matching. ||| dongqing wu ||| huihui li ||| yinge tang ||| lei guo ||| hang liu ||| 
2022 ||| contrastive predictive coding with transformer for video representation learning. ||| yue liu ||| junqi ma ||| yufei xie ||| xuefeng yang ||| xingzhen tao ||| lin peng ||| wei gao ||| 
2021 ||| attention-based label consistency for semi-supervised deep learning based image classification. ||| jiaming chen ||| meng yang ||| jie ling ||| 
2021 ||| a joint object detection and semantic segmentation model with cross-attention and inner-attention mechanisms. ||| zhixiong nan ||| jizhi peng ||| jingjing jiang ||| hui chen ||| ben yang ||| jingmin xin ||| nanning zheng ||| 
2020 ||| a cross-modal multi-granularity attention network for rgb-ir person re-identification. ||| jianguo jiang ||| kaiyuan jin ||| meibin qi ||| qian wang ||| jingjing wu ||| cuiqun chen ||| 
2019 ||| structure-aware person search with self-attention and online instance aggregation matching. ||| cunyuan gao ||| rui yao ||| jiaqi zhao ||| yong zhou ||| fuyuan hu ||| leida li ||| 
2019 ||| bidirectional lstm with attention mechanism and convolutional layer for text classification. ||| gang liu ||| jiabao guo ||| 
2021 ||| video person re-identification with global statistic pooling and self-attention distillation. ||| gaojie lin ||| sanyuan zhao ||| jianbing shen ||| 
2021 ||| i2net: mining intra-video and inter-video attention for temporal action localization. ||| wei zhang ||| binglu wang ||| songhui ma ||| yani zhang ||| yongqiang zhao ||| 
2021 ||| multi-object tracking with hard-soft attention network and group-based cost minimization. ||| yating liu ||| xuesong li ||| tianxiang bai ||| kunfeng wang ||| fei-yue wang ||| 
2020 ||| artan: align reviews with topics in attention network for rating prediction. ||| yile liang ||| tieyun qian ||| huilin yu ||| 
2020 ||| triple attention network for video segmentation. ||| yan tian ||| yujie zhang ||| di zhou ||| guohua cheng ||| wei-gang chen ||| ruili wang ||| 
2021 ||| gsa-gan: global spatial attention generative adversarial networks. ||| lei an ||| jiajia zhao ||| bo ma ||| 
2021 ||| multiple perspective attention based on double bilstm for aspect and sentiment pair extract. ||| yujie fu ||| jian liao ||| yang li ||| suge wang ||| deyu li ||| xiaoli li ||| 
2020 ||| attentional coarse-and-fine generative adversarial networks for image inpainting. ||| minyu chen ||| zhi liu ||| linwei ye ||| yang wang ||| 
2019 ||| a hierarchical contextual attention-based network for sequential recommendation. ||| qiang cui ||| shu wu ||| yan huang ||| liang wang ||| 
2022 ||| a 2.5d semantic segmentation of the pancreas using attention guided dual context embedded u-net. ||| jingyuan li ||| guanqun liao ||| wenfang sun ||| ji sun ||| tai sheng ||| kaibin zhu ||| karen m. von deneen ||| yi zhang ||| 
2020 ||| self-constraining and attention-based hashing network for bit-scalable cross-modal retrieval. ||| xinzhi wang ||| xitao zou ||| erwin m. bakker ||| song wu ||| 
2021 ||| tagattention: mobile object tracing with zero appearance knowledge by vision-rfid fusion. ||| xiaofeng shi ||| haofan cai ||| minmei wang ||| ge wang ||| baiwen huang ||| junjie xie ||| chen qian ||| 
2022 ||| sasa: source-aware self-attention for ip hijack detection. ||| tal shapira ||| yuval shavitt ||| 
2017 ||| scalable verification of networks with packet transformers using atomic predicates. ||| hongkun yang ||| simon s. lam ||| 
2019 ||| wikipedia and cryptocurrencies: interplay between collective attention and market performance. ||| abeer elbahrawy ||| laura alessandretti ||| andrea baronchelli ||| 
2022 ||| cellvgae: an unsupervised scrna-seq analysis workflow with graph attention networks. ||| david buterez ||| ioana bica ||| ifrah tariq ||| helena andr ||| s-terr ||| pietro li ||| 
2022 ||| cpg transformer for imputation of single-cell methylomes. ||| gaetan de waele ||| jim clauwaert ||| gerben menschaert ||| willem waegeman ||| 
2019 ||| acme: pan-specific peptide-mhc class i binding prediction through attention-based deep neural networks. ||| yan hu ||| ziqiang wang ||| hailin hu ||| fangping wan ||| lin chen ||| yuanpeng xiong ||| xiaoxia wang ||| dan zhao ||| weiren huang ||| jianyang zeng ||| 
2021 ||| lbert: lexically aware transformer-based bidirectional encoder representation model for learning universal bio-entity relations. ||| neha warikoo ||| yung-chun chang ||| wen-lian hsu ||| 
2021 ||| crephan: cross-species prediction of enhancers by using hierarchical attention networks. ||| jianwei hong ||| ruitian gao ||| yang yang ||| 
2021 ||| on biases of attention in scientific discovery. ||| uriel singer ||| kira radinsky ||| eric horvitz ||| 
2020 ||| tempel: time-series mutation prediction of influenza a viruses via attention-based recurrent neural networks. ||| rui yin ||| emil luusua ||| jan dabrowski ||| yu zhang ||| chee keong kwoh ||| 
2021 ||| titan: t-cell receptor specificity prediction with bimodal attention networks. ||| anna weber ||| jannis born ||| mar ||| a rodr ||| guez mart ||| nez ||| 
2022 ||| mire2e: a full end-to-end deep model based on transformers for prediction of pre-mirnas. ||| jonathan raad ||| leandro a bugnon ||| diego h. milone ||| georgina stegmayer ||| 
2021 ||| bert4bitter: a bidirectional encoder representations from transformers (bert)-based model for improving the prediction of bitter peptides. ||| phasit charoenkwan ||| chanin nantasenamat ||| md. mehedi hasan ||| balachandran manavalan ||| watshara shoombuatong ||| 
2020 ||| identifying enhancer-promoter interactions with neural network based on pre-trained dna vectors and attention mechanism. ||| zengyan hong ||| xiangxiang zeng ||| leyi wei ||| xiangrong liu ||| 
2020 ||| cancer subtype classification and modeling by pathway attention and propagation. ||| sangseon lee ||| sangsoo lim ||| taeheon lee ||| inyoung sung ||| sun kim ||| 
2022 ||| hyperattentiondti: improving drug-protein interaction prediction by sequence-based deep learning with attention mechanism. ||| qichang zhao ||| haochen zhao ||| kai zheng ||| jianxin wang ||| 
2020 ||| saint: self-attention augmented inception-inside-inception network improves protein secondary structure prediction. ||| mostofa rafid uddin ||| sazan mahbub ||| mohammad saifur rahman ||| md. shamsuzzoha bayzid ||| 
2021 ||| fragat: a fragment-oriented multi-scale graph attention model for molecular property prediction. ||| ziqiao zhang ||| jihong guan ||| shuigeng zhou ||| 
2021 ||| graph contextualized attention network for predicting synthetic lethality in human cancers. ||| yahui long ||| min wu ||| yong liu ||| jie zheng ||| chee keong kwoh ||| jiawei luo ||| xiaoli li ||| 
2018 ||| an attention-based bilstm-crf approach to document-level chemical named entity recognition. ||| ling luo ||| zhihao yang ||| pei yang ||| yin zhang ||| lei wang ||| hongfei lin ||| jian wang ||| 
2021 ||| learning embedding features based on multisense-scaled attention architecture to improve the predictive performance of anticancer peptides. ||| wenjia he ||| yu wang ||| lizhen cui ||| ran su ||| leyi wei ||| 
2021 ||| moltrans: molecular interaction transformer for drug-target interaction prediction. ||| kexin huang ||| cao xiao ||| lucas m. glass ||| jimeng sun ||| 
2019 ||| deephint: understanding hiv-1 integration via deep learning with attention. ||| hailin hu ||| an xiao ||| sai zhang ||| yangyang li ||| xuanling shi ||| tao jiang ||| linqi zhang ||| lei zhang ||| jianyang zeng ||| 
2020 ||| transformercpi: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. ||| lifan chen ||| xiaoqin tan ||| dingyan wang ||| feisheng zhong ||| xiaohong liu ||| tianbiao yang ||| xiaomin luo ||| kaixian chen ||| hualiang jiang ||| mingyue zheng ||| arne elofsson ||| 
2021 ||| lbert: lexically-aware transformers based bidirectional encoder representation model for learning universal bio-entity relations. ||| neha warikoo ||| yung-chun chang ||| wen-lian hsu ||| 
2021 ||| dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. ||| yanrong ji ||| zhihan zhou ||| han liu ||| ramana v. davuluri ||| 
2021 ||| polypharmacy side-effect prediction with enhanced interpretability based on graph feature attention network. ||| sunjoo bang ||| jong ho jhee ||| hyunjung shin ||| 
2020 ||| mhcattnnet: predicting mhc-peptide bindings for mhc alleles classes i and ii using an attention-based deep neural model. ||| gopalakrishnan venkatesh ||| aayush grover ||| g. srinivasaraghavan ||| shrisha rao ||| 
2021 ||| tale: transformer-based protein function annotation with joint sequence-label embedding. ||| yue cao ||| yang shen ||| 
2022 ||| deepidp-2l: protein intrinsically disordered region prediction by combining convolutional attention network and hierarchical attention network. ||| yi-jun tang ||| yihe pang ||| bin liu ||| 
2021 ||| halcyon: an accurate basecaller exploiting an encoder-decoder model with monotonic attention. ||| hiroki konishi ||| rui yamaguchi ||| kiyoshi yamaguchi ||| yoichi furukawa ||| seiya imoto ||| 
2021 ||| bert-gt: cross-sentence n-ary relation extraction with bert and graph transformer. ||| po-ting lai ||| zhiyong lu ||| 
2022 ||| stonkgs: a sophisticated transformer trained on biomedical text and knowledge graphs. ||| helena balabin ||| charles tapley hoyt ||| colin birkenbihl ||| benjamin m. gyori ||| john a. bachman ||| alpha tom kodamullil ||| paul g. pl ||| ger ||| martin hofmann-apitius ||| daniel domingo-fern ||| ndez ||| 
2019 ||| annofly: annotating drosophila embryonic images based on an attention-enhanced rnn model. ||| yang yang ||| mingyu zhou ||| qingwei fang ||| hong-bin shen ||| 
2021 ||| in silico prediction of in vitro protein liquid-liquid phase separation experiments outcomes with multi-head neural attention. ||| daniele raimondi ||| gabriele orlando ||| emiel michiels ||| donya pakravan ||| anna bratek-skicki ||| ludo van den bosch ||| yves moreau ||| frederic rousseau ||| joost schymkowitz ||| 
2021 ||| ids-attention: an efficient algorithm for intrusion detection systems using attention mechanism. ||| fatimaezzahra laghrissi ||| samira douzi ||| khadija douzi ||| badr hssina ||| 
2022 ||| image captioning model using attention and object features to mimic human image understanding. ||| muhammad abdelhadie al-malla ||| assef jafar ||| nada ghneim ||| 
2020 ||| deep anomaly detection through visual attention in surveillance videos. ||| nasaruddin nasaruddin ||| kahlil muchtar ||| afdhal afdhal ||| alvin prayuda juniarta dwiyantoro ||| 
2021 ||| enhanced credit card fraud detection based on attention mechanism and lstm deep model. ||| ibtissam benchaji ||| samira douzi ||| bouabid el ouahidi ||| jaafar jaafari ||| 
2020 ||| argument annotation and analysis using deep learning with attention mechanism in bahasa indonesia. ||| derwin suhartono ||| aryo pradipta gema ||| suhendro winton ||| theodorus david ||| mohamad ivan fanany ||| aniati murni arymurthy ||| 
2021 ||| incsa-unet incsa-unet: spatial attention inception unet for aerial images segmentation. ||| ibrahim delibasoglu ||| 
2021 ||| pvrar: point-view relation neural network embedded with both attention mechanism and radon transform for 3d shape recognition. ||| jie zhou ||| ziping ma ||| jinlin ma ||| 
2019 ||| pay attention and you won't lose it: a deep learning approach to sequence imputation. ||| ilia sucholutsky ||| apurva narayan ||| matthias schonlau ||| sebastian fischmeister ||| 
2022 ||| attention enhanced capsule network for text classification by encoding syntactic dependency trees with graph convolutional neural network. ||| xudong jia ||| li wang ||| 
2021 ||| seedsortnet: a rapid and highly effificient lightweight cnn based on visual attention for seed sorting. ||| chunlei li ||| huanyu li ||| zhoufeng liu ||| bicao li ||| yun huang ||| 
2021 ||| video captioning with stacked attention and semantic hard pull. ||| md. mushfiqur rahman ||| thasin abedin ||| khondokar s. s. prottoy ||| ayana moshruba ||| fazlul hasan siddiqui ||| 
2021 ||| st-afn: a spatial-temporal attention based fusion network for lane-level traffic flow prediction. ||| guojiang shen ||| kaifeng yu ||| meiyu zhang ||| xiangjie kong ||| 
2022 ||| self-supervised recurrent depth estimation with attention mechanisms. ||| ilya makarov ||| maria bakhanova ||| sergey i. nikolenko ||| olga gerasimova ||| 
2021 ||| stochastic attentions and context learning for person re-identification. ||| nazia perwaiz ||| muhammad moazam fraz ||| muhammad shahzad ||| 
2021 ||| sva-ssd: saliency visual attention single shot detector for building detection in low contrast high-resolution satellite images. ||| ahmed i. shahin ||| sultan almotairi ||| 
2020 ||| online supervised attention-based recurrent depth estimation from monocular video. ||| dmitrii maslov ||| ilya makarov ||| 
2020 ||| being the center of attention: a person-context cnn framework for personality recognition. ||| dario dotti ||| mirela popa ||| stylianos asteriadis ||| 
2018 ||| estimating collective attention toward a public display. ||| wolfgang narzt ||| otto weichselbaum ||| gustav pomberger ||| markus hofmarcher ||| michael strauss ||| peter holzkorn ||| roland haring ||| monika sturm ||| 
2020 ||| learning multi-agent communication with double attentional deep reinforcement learning. ||| hangyu mao ||| zhengchao zhang ||| zhen xiao ||| zhibo gong ||| yan ni ||| 
2022 ||| embedded attention network for semantic segmentation. ||| qingxuan lv ||| mingzhe feng ||| xin sun ||| junyu dong ||| changrui chen ||| yu zhang ||| 
2021 ||| real-time semantic segmentation with fast attention. ||| ping hu ||| federico perazzi ||| fabian caba heilbron ||| oliver wang ||| zhe lin ||| kate saenko ||| stan sclaroff ||| 
2022 ||| look closer: bridging egocentric and third-person views with transformers for robotic manipulation. ||| rishabh jangir ||| nicklas hansen ||| sambaran ghosal ||| mohit jain ||| xiaolong wang ||| 
2020 ||| probabilistic crowd gan: multimodal pedestrian trajectory prediction using a graph vehicle-pedestrian attention network. ||| stuart eiffert ||| kunming li ||| mao shan ||| stewart worrall ||| salah sukkarieh ||| eduardo m. nebot ||| 
2022 ||| 3crossnet: cross-level cross-scale cross-attention network for point cloud representation. ||| xian-feng han ||| zhang-yue he ||| jia chen ||| guoqiang xiao ||| 
2020 ||| robot navigation in crowds by graph convolutional networks with attention learned from human gaze. ||| yuying chen ||| congcong liu ||| bertram e. shi ||| ming liu ||| 
2022 ||| g3doa: generalizable 3d descriptor with overlap attention for point cloud registration. ||| hengwang zhao ||| hanyang zhuang ||| chunxiang wang ||| ming yang ||| 
2022 ||| q-attention: enabling efficient learning for vision-based robotic manipulation. ||| stephen james ||| andrew j. davison ||| 
2020 ||| may i draw your attention? initial lessons from the large-scale generative mark maker. ||| aidan phillips ||| ashwin vinoo ||| naomi t. fitter ||| 
2022 ||| tracker meets night: a transformer enhancer for uav tracking. ||| junjie ye ||| changhong fu ||| ziang cao ||| shan an ||| guangze zheng ||| bowen li ||| 
2021 ||| adversarial inverse reinforcement learning with self-attention dynamics model. ||| jiankai sun ||| lantao yu ||| pinqian dong ||| bo lu ||| bolei zhou ||| 
2022 ||| hierarchical point cloud encoding and decoding with lightweight self-attention based model. ||| en yen puang ||| hao zhang ||| hongyuan zhu ||| wei jing ||| 
2020 ||| alleviating the burden of labeling: sentence generation by attention branch encoder-decoder network. ||| tadashi ogura ||| aly magassouba ||| komei sugiura ||| tsubasa hirakawa ||| takayoshi yamashita ||| hironobu fujiyoshi ||| hisashi kawai ||| 
2021 ||| 4d attention: comprehensive framework for spatio-temporal gaze mapping. ||| shuji oishi ||| kenji koide ||| masashi yokozuka ||| atsuhiko banno ||| 
2019 ||| using human attention to address human-robot motion. ||| remi paulin ||| thierry fraichard ||| patrick reignier ||| 
2021 ||| esa-vlad: a lightweight network based on second-order attention and netvlad for loop closure detection. ||| yan xu ||| jiani huang ||| jixiang wang ||| yanyun wang ||| hong qin ||| keqin nan ||| 
2022 ||| passive bimanual skills learning from demonstration with motion graph attention networks. ||| zhipeng dong ||| zhihao li ||| yunhui yan ||| sylvain calinon ||| fei chen ||| 
2021 ||| case relation transformer: a crossmodal language generation model for fetching instructions. ||| motonari kambara ||| komei sugiura ||| 
2021 ||| target-dependent uniter: a transformer-based multimodal language comprehension model for domestic service robots. ||| shintaro ishikawa ||| komei sugiura ||| 
2021 ||| fov-net: field-of-view extrapolation using self-attention and uncertainty. ||| liqian ma ||| stamatios georgoulis ||| xu jia ||| luc van gool ||| 
2020 ||| a multimodal target-source classifier with attention branches to understand ambiguous instructions for fetching daily objects. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2021 ||| multi-gat: a graphical attention-based hierarchical multimodal representation learning approach for human activity recognition. ||| md mofijul islam ||| tariq iqbal ||| 
2020 ||| self-attention based visual-tactile fusion learning for predicting grasp outcomes. ||| shaowei cui ||| rui wang ||| junhang wei ||| jingyi hu ||| shuo wang ||| 
2018 ||| learning context flexible attention model for long-term visual place recognition. ||| zetao chen ||| lingqiao liu ||| inkyu sa ||| zongyuan ge ||| margarita chli ||| 
2021 ||| message-aware graph attention networks for large-scale multi-robot path planning. ||| qingbiao li ||| weizhe lin ||| zhe liu ||| amanda prorok ||| 
2022 ||| realtime global attention network for semantic segmentation. ||| xi mo ||| xiangyu chen ||| 
2021 ||| crossmap transformer: a crossmodal masked path transformer using double back-translation for vision-and-language navigation. ||| aly magassouba ||| komei sugiura ||| hisashi kawai ||| 
2019 ||| sliding-window temporal attention based deep learning system for robust sensor modality fusion for ugv navigation. ||| halil utku unlu ||| naman patel ||| prashanth krishnamurthy ||| farshad khorrami ||| 
2022 ||| plate: visually-grounded planning with transformers in procedural tasks. ||| jiankai sun ||| de-an huang ||| bo lu ||| yun-hui liu ||| bolei zhou ||| animesh garg ||| 
2021 ||| attentional graph neural network for parking-slot detection. ||| chen min ||| jiaolong xu ||| liang xiao ||| dawei zhao ||| yiming nie ||| bin dai ||| 
2021 ||| towards an interpretable deep driving network by attentional bottleneck. ||| jinkyu kim ||| mayank bansal ||| 
2021 ||| keypoint matching for point cloud registration using multiplex dynamic graph attention networks. ||| chenghao shi ||| xieyuanli chen ||| kaihong huang ||| junhao xiao ||| huimin lu ||| cyrill stachniss ||| 
2020 ||| learning scheduling policies for multi-robot coordination with graph attention networks. ||| zheyuan wang ||| matthew c. gombolay ||| 
2022 ||| pq-transformer: jointly parsing 3d objects and layouts from point clouds. ||| xiaoxue chen ||| hao zhao ||| guyue zhou ||| ya-qin zhang ||| 
2020 ||| spatio-temporal deformable 3d convnets with attention for action recognition. ||| jun li ||| xianglong liu ||| mingyuan zhang ||| deqing wang ||| 
2022 ||| two-stage aware attentional siamese network for visual tracking. ||| xinglong sun ||| guangliang han ||| lihong guo ||| hang yang ||| xiaotian wu ||| qingqing li ||| 
2021 ||| exploring global diverse attention via pairwise temporal relation for video summarization. ||| ping li ||| qinghao ye ||| luming zhang ||| li yuan ||| xianghua xu ||| ling shao ||| 
2022 ||| defect attention template generation cyclegan for weakly supervised surface defect segmentation. ||| shuanlong niu ||| bin li ||| xinggang wang ||| songping he ||| yaru peng ||| 
2020 ||| metric learning-based kernel transformer with triplets and label constraints for feature fusion. ||| shichao kan ||| linna zhang ||| zhihai he ||| yigang cen ||| shiming chen ||| jikun zhou ||| 
2021 ||| hypergraph convolution and hypergraph attention. ||| song bai ||| feihu zhang ||| philip h. s. torr ||| 
2021 ||| divergent-convergent attention for image captioning. ||| junzhong ji ||| zhuoran du ||| xiaodan zhang ||| 
2022 ||| sparse attention block: aggregating contextual information for object detection. ||| chunlin chen ||| jun yu ||| qiang ling ||| 
2022 ||| deep co-supervision and attention fusion strategy for automatic covid-19 lung infection segmentation on ct images. ||| haigen hu ||| leizhao shen ||| qiu guan ||| xiaoxin li ||| qianwei zhou ||| su ruan ||| 
2022 ||| robust face alignment by dual-attentional spatial-aware capsule networks. ||| jinyan ma ||| jing li ||| bo du ||| jia wu ||| jun wan ||| yafu xiao ||| 
2021 ||| joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks. ||| jun-yu ye ||| yan-ming zhang ||| qing yang ||| cheng-lin liu ||| 
2019 ||| attention guided deep audio-face fusion for efficient speaker naming. ||| xin liu ||| jiajia geng ||| haibin ling ||| yiu-ming cheung ||| 
2020 ||| point attention network for semantic segmentation of 3d point clouds. ||| mingtao feng ||| liang zhang ||| xuefei lin ||| syed zulqarnain gilani ||| ajmal mian ||| 
2020 ||| video semantic segmentation via feature propagation with holistic attention. ||| junrong wu ||| zongzheng wen ||| sanyuan zhao ||| kele huang ||| 
2020 ||| semantic segmentation using stride spatial pyramid pooling and dual attention decoder. ||| chengli peng ||| jiayi ma ||| 
2019 ||| deep multi-path convolutional neural network joint with salient region attention for facial expression recognition. ||| siyue xie ||| haifeng hu ||| yongbo wu ||| 
2022 ||| multi-attention augmented network for single image super-resolution. ||| rui chen ||| heng zhang ||| jixin liu ||| 
2022 ||| action transformer: a self-attention model for short-time pose-based human action recognition. ||| vittorio mazzia ||| simone angarano ||| francesco salvetti ||| federico angelini ||| marcello chiaberge ||| 
2021 ||| oaenet: oriented attention ensemble for accurate facial expression recognition. ||| zhengning wang ||| fanwei zeng ||| shuaicheng liu ||| bing zeng ||| 
2021 ||| visual attention dehazing network with multi-level features refinement and fusion. ||| shibai yin ||| xiaolong yang ||| yibin wang ||| yee-hong yang ||| 
2019 ||| multi attention module for visual tracking. ||| boyu chen ||| peixia li ||| chong sun ||| dong wang ||| gang yang ||| huchuan lu ||| 
2022 ||| contrastive attention network with dense field estimation for face completion. ||| xin ma ||| xiaoqiang zhou ||| huaibo huang ||| gengyun jia ||| zhenhua chai ||| xiaolin wei ||| 
2021 ||| deep multi-task learning with relational attention for business success prediction. ||| jiejie zhao ||| bowen du ||| leilei sun ||| weifeng lv ||| yanchi liu ||| hui xiong ||| 
2020 ||| deep imitator: handwriting calligraphy imitation via deep attention networks. ||| bocheng zhao ||| jianhua tao ||| minghao yang ||| zhengkun tian ||| cunhang fan ||| ye bai ||| 
2019 ||| attention-based convolutional neural network and long short-term memory for short-term detection of mood disorders based on elicited speech responses. ||| kun-yi huang ||| chung-hsien wu ||| ming-hsiang su ||| 
2020 ||| multi-head enhanced self-attention network for novelty detection. ||| yingying zhang ||| yuxin gong ||| haogang zhu ||| xiao bai ||| wenzhong tang ||| 
2021 ||| generalized pyramid co-attention with learnable aggregation net for video question answering. ||| lianli gao ||| tangming chen ||| xiangpeng li ||| pengpeng zeng ||| lei zhao ||| yuan-fang li ||| 
2021 ||| dual self-attention with co-attention networks for visual question answering. ||| yun liu ||| xiaoming zhang ||| qianyun zhang ||| chaozhuo li ||| feiran huang ||| xianghong tang ||| zhoujun li ||| 
2020 ||| learning to infer human attention in daily activities. ||| zhixiong nan ||| tianmin shu ||| ran gong ||| shu wang ||| ping wei ||| song-chun zhu ||| nanning zheng ||| 
2020 ||| structured self-attention architecture for graph-level representation learning. ||| xiaolong fan ||| maoguo gong ||| yu xie ||| fenlong jiang ||| hao li ||| 
2021 ||| stroke constrained attention network for online handwritten mathematical expression recognition. ||| jiaming wang ||| jun du ||| jianshu zhang ||| bin wang ||| bo ren ||| 
2022 ||| dla-net: learning dual local attention features for semantic segmentation of large-scale building facade point clouds. ||| yanfei su ||| weiquan liu ||| zhimin yuan ||| ming cheng ||| zhihong zhang ||| xuelun shen ||| cheng wang ||| 
2022 ||| segmentation information with attention integration for classification of breast tumor in ultrasound image. ||| yaozhong luo ||| qinghua huang ||| xuelong li ||| 
2020 ||| graph convolutional network with structure pooling and joint-wise channel attention for action recognition. ||| yuxin chen ||| gaoqun ma ||| chunfeng yuan ||| bing li ||| hui zhang ||| fangshi wang ||| weiming hu ||| 
2022 ||| canet: co-attention network for rgb-d semantic segmentation. ||| hao zhou ||| lu qi ||| hai huang ||| xu yang ||| zhaoliang wan ||| xianglong wen ||| 
2021 ||| region-based dropout with attention prior for weakly supervised object localization. ||| junsuk choe ||| dongyoon han ||| sangdoo yun ||| jung-woo ha ||| seong joon oh ||| hyunjung shim ||| 
2022 ||| learning multiscale hierarchical attention for video summarization. ||| wencheng zhu ||| jiwen lu ||| yucheng han ||| jie zhou ||| 
2021 ||| multiple attentional pyramid networks for chinese herbal recognition. ||| yingxue xu ||| guihua wen ||| yang hu ||| mingnan luo ||| dan dai ||| yishan zhuang ||| wendy hall ||| 
2019 ||| script identification in natural scene image and video frames using an attention based convolutional-lstm network. ||| ankan kumar bhunia ||| aishik konwer ||| ayan kumar bhunia ||| abir bhowmick ||| partha pratim roy ||| umapada pal ||| 
2022 ||| contour-enhanced attention cnn for ct-based covid-19 segmentation. ||| r. karthik ||| r. menaka ||| m. hariharan ||| daehan won ||| 
2021 ||| deep ancient roman republican coin classification via feature fusion and attention. ||| hafeez anwar ||| saeed anwar ||| sebastian zambanini ||| fatih porikli ||| 
2019 ||| blind image quality assessment via learnable attention-based pooling. ||| jie gu ||| gaofeng meng ||| shiming xiang ||| chunhong pan ||| 
2021 ||| linguistically-aware attention for reducing the semantic gap in vision-language tasks. ||| gouthaman kv ||| athira m. nambiar ||| kancheti sai srinivas ||| anurag mittal ||| 
2021 ||| ggac: multi-relational image gated gcn with attention convolutional binary neural tree for identifying disease with chest x-rays. ||| bing yang ||| yan kang ||| lan zhang ||| hao li ||| 
2019 ||| aem: attentional ensemble model for personalized classifier weight learning. ||| hongzhi liu ||| yingpeng du ||| zhonghai wu ||| 
2022 ||| multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification. ||| lina yang ||| fengqi zhang ||| patrick shen-pei wang ||| xichun li ||| zuqiang meng ||| 
2022 ||| a tri-attention fusion guided multi-modal segmentation network. ||| tongxue zhou ||| su ruan ||| pierre vera ||| st ||| phane canu ||| 
2021 ||| a nested u-shape network with multi-scale upsample attention for robust retinal vascular segmentation. ||| ruohan zhao ||| qin li ||| jianrong wu ||| jane you ||| 
2022 ||| ae-net: fine-grained sketch-based image retrieval via attention-enhanced network. ||| yangdong chen ||| zhaolong zhang ||| yanfei wang ||| yuejie zhang ||| rui feng ||| tao zhang ||| weiguo fan ||| 
2019 ||| moran: a multi-object rectified attention network for scene text recognition. ||| canjie luo ||| lianwen jin ||| zenghui sun ||| 
2020 ||| a novel image-dehazing network with a parallel attention block. ||| shibai yin ||| yibin wang ||| yee-hong yang ||| 
2021 ||| view-invariant action recognition via unsupervised attention transfer (uant). ||| yanli ji ||| yang yang ||| heng tao shen ||| tatsuya harada ||| 
2018 ||| unsupervised image saliency detection with gestalt-laws guided optimization and visual attention based refinement. ||| yijun yan ||| jinchang ren ||| genyun sun ||| huimin zhao ||| junwei han ||| xuelong li ||| stephen marshall ||| jin zhan ||| 
2020 ||| attention and boundary guided salient object detection. ||| qing zhang ||| yanjiao shi ||| xueqin zhang ||| 
2021 ||| lightweight dynamic conditional gan with pyramid attention for text-to-image synthesis. ||| lianli gao ||| daiyuan chen ||| zhou zhao ||| jie shao ||| heng tao shen ||| 
2021 ||| mvdrnet: multi-view diabetic retinopathy detection by combining dcnns and attention mechanisms. ||| xiaoling luo ||| zuhui pu ||| yong xu ||| wai keung wong ||| jingyong su ||| xiaoyan dou ||| baikang ye ||| jiying hu ||| lisha mou ||| 
2020 ||| self-attention driven adversarial similarity learning network. ||| xinjian gao ||| zhao zhang ||| tingting mu ||| xudong zhang ||| chaoran cui ||| meng wang ||| 
2021 ||| covid-19 detection from x-ray images using multi-kernel-size spatial-channel attention network. ||| yuqi fan ||| jiahao liu ||| ruixuan yao ||| xiaohui yuan ||| 
2021 ||| weakly-supervised temporal attention 3d network for human action recognition. ||| jonghyun kim ||| gen li ||| inyong yun ||| cheolkon jung ||| joongkyu kim ||| 
2021 ||| channel attention in lidar-camera fusion for lane line segmentation. ||| xinyu zhang ||| zhiwei li ||| xin gao ||| dafeng jin ||| jun li ||| 
2019 ||| deep gated attention networks for large-scale street-level scene segmentation. ||| pingping zhang ||| wei liu ||| hongyu wang ||| yinjie lei ||| huchuan lu ||| 
2019 ||| attention driven person re-identification. ||| fan yang ||| ke yan ||| shijian lu ||| huizhu jia ||| xiaodong xie ||| wen gao ||| 
2022 ||| a unified deep sparse graph attention network for scene graph generation. ||| hao zhou ||| yazhou yang ||| tingjin luo ||| jun zhang ||| shuohao li ||| 
2020 ||| learning visual relationship and context-aware attention for image captioning. ||| junbo wang ||| wei wang ||| liang wang ||| zhiyong wang ||| david dagan feng ||| tieniu tan ||| 
2020 ||| long video question answering: a matching-guided attention model. ||| weining wang ||| yan huang ||| liang wang ||| 
2021 ||| position-aware self-attention based neural sequence labeling. ||| wei wei ||| zanbo wang ||| xianling mao ||| guangyou zhou ||| pan zhou ||| sheng jiang ||| 
2017 ||| age and gender recognition in the wild with deep attention. ||| pau rodr ||| guez ||| guillem cucurull ||| josep m. gonfaus ||| f. xavier roca ||| jordi gonz ||| lez ||| 
2021 ||| image super-resolution via channel attention and spatial graph convolutional network. ||| yue yang ||| yong qi ||| 
2021 ||| attention-shift based deep neural network for fine-grained visual categorization. ||| yi niu ||| yang jiao ||| guangming shi ||| 
2021 ||| acn: occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks. ||| hyunsung park ||| daijin kim ||| 
2022 ||| relation-aware dynamic attributed graph attention network for stocks recommendation. ||| shibo feng ||| chen xu ||| yu zuo ||| guo chen ||| fan lin ||| jianbing xiahou ||| 
2020 ||| generative attention adversarial classification network for unsupervised domain adaptation. ||| wendong chen ||| haifeng hu ||| 
2022 ||| balanced single-shot object detection using cross-context attention-guided network. ||| shuyu miao ||| shanshan du ||| rui feng ||| yuejie zhang ||| huayu li ||| tianbi liu ||| lin zheng ||| weiguo fan ||| 
2022 ||| a multimodal attention fusion network with a dynamic vocabulary for textvqa. ||| jiajia wu ||| jun du ||| fengren wang ||| chen yang ||| xinzhe jiang ||| jinshui hu ||| bing yin ||| jianshu zhang ||| lirong dai ||| 
2020 ||| multistage attention network for image inpainting. ||| ning wang ||| sihan ma ||| jingyuan li ||| yipeng zhang ||| lefei zhang ||| 
2022 ||| visual vs internal attention mechanisms in deep neural networks for image classification and object detection. ||| abraham montoya obeso ||| jenny benois-pineau ||| mireya sara |||  garc ||| a-v ||| zquez ||| alejandro alvaro ram ||| rez-acosta ||| 
2021 ||| stan: a sequential transformation attention-based network for scene text recognition. ||| qingxiang lin ||| canjie luo ||| lianwen jin ||| songxuan lai ||| 
2021 ||| memf: multi-level-attention embedding and multi-layer-feature fusion model for person re-identification. ||| jia sun ||| yanfeng li ||| houjin chen ||| bin zhang ||| jinlei zhu ||| 
2021 ||| multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network. ||| weijie sheng ||| xinde li ||| 
2022 ||| channel attention image steganography with generative adversarial networks. ||| jingxuan tan ||| xin liao ||| jiate liu ||| yun cao ||| hongbo jiang ||| 
2017 ||| information cascades in feed-based networks of users with limited attention. ||| sameet sreenivasan ||| kevin s. chan ||| ananthram swami ||| gy ||| rgy korniss ||| boleslaw k. szymanski ||| 
2021 ||| canintelliids: detecting in-vehicle intrusion attacks on a controller area network using cnn and attention-based gru. ||| abdul rehman javed ||| saif ur rehman ||| mohib ullah khan ||| mamoun alazab ||| thippa reddy gadekallu ||| 
2017 ||| visualization of brain activation during attention-demanding tasks using cognitive signal processing. ||| muthumeenakshi subramanian ||| b. geethanjali ||| n. p. guhan seshadri ||| bhavana venkat ||| r. vijayalakshmi ||| 
2021 ||| time-wise attention aided convolutional neural network for data-driven cellular traffic prediction. ||| wenxin shen ||| haixia zhang ||| shuaishuai guo ||| chuanting zhang ||| 
2019 ||| placement delivery array design via attention-based sequence-to-sequence model with deep neural network. ||| zhengming zhang ||| meng hua ||| chunguo li ||| yongming huang ||| luxi yang ||| 
2022 ||| semantic communication with adaptive universal transformer. ||| qingyang zhou ||| rongpeng li ||| zhifeng zhao ||| chenghui peng ||| honggang zhang ||| 
2021 ||| channelattention: utilizing attention layers for accurate massive mimo channel feedback. ||| dong jin ji ||| dong-ho cho ||| 
2021 ||| saldr: joint self-attention learning and dense refine for massive mimo csi feedback with multiple compression ratio. ||| xuan song ||| jun wang ||| jie wang ||| guan gui ||| tomoaki ohtsuki ||| haris gacanin ||| hikmet sari ||| 
2020 ||| chinese text classification based on attention mechanism and feature-enhanced fusion neural network. ||| jinbao xie ||| yongjin hou ||| yujing wang ||| qingyan wang ||| baiwei li ||| shiwei lv ||| yury vorotnitsky ||| 
2021 ||| air pollution prediction based on factory-aware attentional lstm neural network. ||| duen-ren liu ||| yi-kuan hsu ||| hsing-yu chen ||| huan-jian jau ||| 
2021 ||| inferbert: a transformer-based causal inference framework for enhancing pharmacovigilance. ||| xingqiao wang ||| xiaowei xu ||| weida tong ||| ruth roberts ||| zhichao liu ||| 
2021 ||| what does a language-and-vision transformer see: the impact of semantic information on visual representations. ||| nikolai ilinykh ||| simon dobnik ||| 
2021 ||| improving adversarial robustness via attention and adversarial logit pairing. ||| xingjian li ||| dou goodman ||| ji liu ||| tao wei ||| dejing dou ||| 
2021 ||| bi-gru model based on pooling and attention for text classification. ||| yu-lan hu ||| qing-shan zhao ||| 
2020 ||| sub-word attention mechanism and ensemble learning-based semantic annotation for heterogeneous networks. ||| liang zhang ||| zhaobin liu ||| jinxiang li ||| gang liu ||| yuanfeng yang ||| yi jin ||| xu zhang ||| 
2017 ||| survey of recent advances in 3d visual attention for robotics. ||| ekaterina potapova ||| michael zillich ||| markus vincze ||| 
2021 ||| predicting user visual attention in virtual reality with a deep learning model. ||| xiangdong li ||| yifei shan ||| wenqian chen ||| yue wu ||| praben hansen ||| simon t. perrault ||| 
2021 ||| sustained inattentional blindness in virtual reality and under conventional laboratory conditions. ||| benjamin sch ||| ne ||| rebecca sophia sylvester ||| elise leila radtke ||| thomas gruber ||| 
2022 ||| the predictive role of body image and anti-fat attitudes on attentional bias toward body area in haptic virtual reality environment. ||| line tremblay ||| brahim chebbi ||| st ||| phane bouchard ||| 
2021 ||| correction to: predicting user visual attention in virtual reality with a deep learning model. ||| xiangdong li ||| yifei shan ||| wenqian chen ||| yue wu ||| preben hansen ||| simon t. perrault ||| 
2022 ||| leveraging eye tracking to understand children's attention during game-based, tangible robotics activities. ||| jennifer k. olsen ||| arzu guneysu ozgur ||| kshitij sharma ||| wafa johal ||| 
2018 ||| image captioning with affective guiding and selective attention. ||| anqi wang ||| haifeng hu ||| liang yang ||| 
2021 ||| guessuneed: recommending courses via neural attention network and course prerequisite relation embeddings. ||| zhongying zhao ||| yonghao yang ||| chao li ||| liqiang nie ||| 
2020 ||| ab-lstm: attention-based bidirectional lstm model for scene text detection. ||| zhandong liu ||| wengang zhou ||| houqiang li ||| 
2020 ||| an end-to-end attention-based neural model for complementary clothing matching. ||| jinhuan liu ||| xuemeng song ||| liqiang nie ||| tian gan ||| jun ma ||| 
2020 ||| multichannel attention refinement for video question answering. ||| yueting zhuang ||| dejing xu ||| xin yan ||| wenzhuo cheng ||| zhou zhao ||| shiliang pu ||| jun xiao ||| 
2018 ||| paying more attention to saliency: image captioning with saliency and context attention. ||| marcella cornia ||| lorenzo baraldi ||| giuseppe serra ||| rita cucchiara ||| 
2019 ||| visual content recognition by exploiting semantic feature map with attention and multi-task learning. ||| rui-wei zhao ||| qi zhang ||| zuxuan wu ||| jianguo li ||| yu-gang jiang ||| 
2020 ||| enforcing affinity feature learning through self-attention for person re-identification. ||| jean-paul ainam ||| ke qin ||| guisong liu ||| guangchun luo ||| brighter agyemang ||| 
2020 ||| attention-based modality-gated networks for image-text sentiment analysis. ||| feiran huang ||| kaimin wei ||| jian weng ||| zhoujun li ||| 
2020 ||| recurrent attention network with reinforced generator for visual dialog. ||| hehe fan ||| linchao zhu ||| yi yang ||| fei wu ||| 
2020 ||| meta-path augmented sequential recommendation with contextual co-attention network. ||| xiaowen huang ||| shengsheng qian ||| quan fang ||| jitao sang ||| changsheng xu ||| 
2021 ||| bi-directional co-attention network for image captioning. ||| weitao jiang ||| weixuan wang ||| haifeng hu ||| 
2018 ||| image captioning via semantic guidance attention and consensus selection strategy. ||| jie wu ||| haifeng hu ||| yi wu ||| 
2022 ||| sadnet: semi-supervised single image dehazing method based on an attention mechanism. ||| ziyi sun ||| yunfeng zhang ||| fangxun bao ||| ping wang ||| xunxiang yao ||| caiming zhang ||| 
2020 ||| kernel attention network for single image super-resolution. ||| dongyang zhang ||| jie shao ||| heng tao shen ||| 
2020 ||| constrained lstm and residual attention for image captioning. ||| liang yang ||| haifeng hu ||| songlong xing ||| xinlong lu ||| 
2019 ||| learning click-based deep structure-preserving embeddings with visual attention. ||| yehao li ||| yingwei pan ||| ting yao ||| hongyang chao ||| yong rui ||| tao mei ||| 
2019 ||| video question answering via knowledge-based progressive spatial-temporal attention network. ||| weike jin ||| zhou zhao ||| yimeng li ||| jie li ||| jun xiao ||| yueting zhuang ||| 
2019 ||| convolutional attention networks for scene text recognition. ||| hongtao xie ||| shancheng fang ||| zheng-jun zha ||| yating yang ||| yan li ||| yongdong zhang ||| 
2019 ||| cmhne: attention-aware collaborative multimodal heterogeneous network embedding. ||| jun hu ||| shengsheng qian ||| quan fang ||| xueliang liu ||| changsheng xu ||| 
2020 ||| exploring disorder-aware attention for clinical event extraction. ||| shweta yadav ||| pralay ramteke ||| asif ekbal ||| sriparna saha ||| pushpak bhattacharyya ||| 
2019 ||| multi-source multi-level attention networks for visual question answering. ||| dongfei yu ||| jianlong fu ||| xinmei tian ||| tao mei ||| 
2019 ||| spatiotemporal-textual co-attention network for video question answering. ||| zheng-jun zha ||| jiawei liu ||| tianhao yang ||| yongdong zhang ||| 
2020 ||| joint stacked hourglass network and salient region attention refinement for robust face alignment. ||| junfeng zhang ||| haifeng hu ||| guobin shen ||| 
2020 ||| spatio-temporal deep residual network with hierarchical attentions for video event recognition. ||| yonggang li ||| chunping liu ||| yi ji ||| shengrong gong ||| haibao xu ||| 
2019 ||| pseudo-3d attention transfer network with content-aware strategy for image captioning. ||| jie wu ||| haifeng hu ||| liang yang ||| 
2022 ||| age-invariant face recognition by multi-feature fusionand decomposition with self-attention. ||| chenggang yan ||| lixuan meng ||| liang li ||| jiehua zhang ||| zhan wang ||| jian yin ||| jiyong zhang ||| yaoqi sun ||| bolun zheng ||| 
2021 ||| fine-grained visual textual alignment for cross-modal retrieval using transformer encoders. ||| nicola messina ||| giuseppe amato ||| andrea esuli ||| fabrizio falchi ||| claudio gennaro ||| st ||| phane marchand-maillet ||| 
2021 ||| adaptive attention-based high-level semantic introduction for image caption. ||| xiaoxiao liu ||| qingyang xu ||| 
2019 ||| image captioning with visual-semantic double attention. ||| chen he ||| haifeng hu ||| 
2019 ||| moving foreground-aware visual attention and key volume mining for human action recognition. ||| junxuan zhang ||| haifeng hu ||| xinlong lu ||| 
2020 ||| image captioning with a joint attention mechanism by visual concept samples. ||| jin yuan ||| lei zhang ||| songrui guo ||| yi xiao ||| zhiyong li ||| 
2021 ||| part-wise spatio-temporal attention driven cnn-based 3d human action recognition. ||| chhavi dhiman ||| dinesh kumar vishwakarma ||| paras agarwal ||| 
2022 ||| robust unsupervised gaze calibration using conversation and manipulation attention priors. ||| r ||| my siegfried ||| jean-marc odobez ||| 
2020 ||| tagging reading comprehension materials with document extraction attention networks. ||| bo sun ||| yunzong zhu ||| zeng yao ||| rong xiao ||| yongkang xiao ||| yungang wei ||| 
2020 ||| virtual reality based joint attention task platform for children with autism. ||| vishav jyoti ||| uttama lahiri ||| 
2020 ||| attention-based relation and context modeling for point cloud semantic segmentation. ||| zhiyu hu ||| dongbo zhang ||| shuai li ||| hong qin ||| 
2018 ||| exploring visual attention and saliency modeling for task-based visual analysis. ||| patrik polatsek ||| manuela waldner ||| ivan viola ||| peter kapec ||| wanda benesova ||| 
2022 ||| tg-net: reconstruct visual wood texture with semantic attention. ||| jiahao chen ||| yilin ge ||| quan wang ||| yizhuo zhang ||| 
2020 ||| anu-net: attention-based nested u-net to exploit full resolution features for medical image segmentation. ||| chen li ||| yusong tan ||| wei chen ||| xin luo ||| yulin he ||| yuanming gao ||| fei li ||| 
2021 ||| single image deraining via detail-guided efficient channel attention network. ||| xiao lin ||| qi huang ||| wei huang ||| xin tan ||| mei-e fang ||| lizhuang ma ||| 
2021 ||| cma: cross-modal attention for 6d object pose estimation. ||| lu zou ||| zhangjin huang ||| fangjun wang ||| zhouwang yang ||| guoping wang ||| 
2021 ||| text-conditioned transformer for automatic pronunciation error detection. ||| zhan zhang ||| yuehai wang ||| jianyi yang ||| 
2020 ||| masked multi-head self-attention for causal speech enhancement. ||| aaron nicolson ||| kuldip k. paliwal ||| 
2020 ||| anet: combining bidirectional lstm and self-attention for end-to-end learning of task-oriented dialogue system. ||| qun he ||| wenjing liu ||| cai zhangli ||| 
2021 ||| toward comprehensive user and item representations via three-tier attention network. ||| hongtao liu ||| wenjun wang ||| qiyao peng ||| nannan wu ||| fangzhao wu ||| pengfei jiao ||| 
2020 ||| a price-per-attention auction scheme using mouse cursor information. ||| ioannis arapakis ||| antonio penta ||| hideo joho ||| luis a. leiva ||| 
2021 ||| hgat: heterogeneous graph attention networks for semi-supervised short text classification. ||| tianchi yang ||| linmei hu ||| chuan shi ||| houye ji ||| xiaoli li ||| liqiang nie ||| 
2022 ||| cha: categorical hierarchy-based attention for next poi recommendation. ||| hongyu zang ||| dongcheng han ||| xin li ||| zhifeng wan ||| mingzhong wang ||| 
2020 ||| an attention-based deep relevance model for few-shot document filtering. ||| bulou liu ||| chenliang li ||| wei zhou ||| feng ji ||| yu duan ||| haiqing chen ||| 
2019 ||| from question to text: question-oriented feature attention for answer selection. ||| heyan huang ||| xiaochi wei ||| liqiang nie ||| xianling mao ||| xin-shun xu ||| 
2022 ||| on the study of transformers for query suggestion. ||| agn ||| s mustar ||| sylvain lamprier ||| benjamin piwowarski ||| 
2019 ||| emotion and attention: audiovisual models for group-level skin response recognition in short movies. ||| lvaro garc ||| a-faura ||| alejandro hern ||| ndez-garc ||| a ||| fernando fern ||| ndez mart ||| nez ||| fernando d ||| az-de-mar ||| a ||| rub ||| n san segundo ||| 
2020 ||| correlation alignment with attention mechanism for unsupervised domain adaptation. ||| rong chen ||| chongguang ren ||| 
2021 ||| transnet: shift invariant transformer network for power attack. ||| suvadeep hajra ||| sayandeep saha ||| manaar alam ||| debdeep mukhopadhyay ||| 
2022 ||| transformer encoder-based crypto-ransomware detection for low-power embedded processors. ||| hyunji kim ||| sejin lim ||| yeajun kang ||| won-woong kim ||| hwajeong seo ||| 
2018 ||| 5g-transformer: slicing and orchestrating transport networks for industry verticals. ||| antonio de la oliva ||| xi li ||| xavier p ||| rez costa ||| carlos jesus bernardos ||| philippe bertin ||| paola iovanna ||| thomas dei ||| josep mangues ||| alain mourad ||| claudio casetti ||| jose enrique gonzalez ||| arturo azcorra ||| 
2019 ||| causal discovery with attention-based convolutional neural networks. ||| meike nauta ||| doina bucur ||| christin seifert ||| 
2021 ||| a combined short time fourier transform and image classification transformer model for rolling element bearings fault diagnosis in electric motors. ||| christos t. alexakos ||| yannis l. karnavas ||| maria drakaki ||| ioannis a. tziafettas ||| 
2021 ||| attention deep model with multi-scale deep supervision for person re-identification. ||| di wu ||| chao wang ||| yong wu ||| qicong wang ||| de-shuang huang ||| 
2021 ||| graph attention network-based multi-agent reinforcement learning for slicing resource management in dense cellular network. ||| yan shao ||| rongpeng li ||| bing hu ||| yingxiao wu ||| zhifeng zhao ||| honggang zhang ||| 
2019 ||| a single-switch transformerless dc-dc converter with universal input voltage for fuel cell vehicles: analysis and design. ||| noureldeen elsayad ||| hadi moradisizkoohi ||| osama a. mohammed ||| 
2021 ||| hsta: a hierarchical spatio-temporal attention model for trajectory prediction. ||| ya wu ||| guang chen ||| zhijun li ||| lijun zhang ||| lu xiong ||| zhengfa liu ||| alois c. knoll ||| 
2021 ||| using appearance to predict pedestrian trajectories through disparity-guided attention and convolutional lstm. ||| yan feng ||| tingsheng zhang ||| abhishek pratap sah ||| lei han ||| zutao zhang ||| 
2017 ||| a modularization method for battery equalizers using multiwinding transformers. ||| yunlong shang ||| bing xia ||| chenghui zhang ||| naxin cui ||| jufeng yang ||| chris mi ||| 
2021 ||| environment-attention network for vehicle trajectory prediction. ||| yingfeng cai ||| zihao wang ||| hai wang ||| long chen ||| yicheng li ||| miguel  ||| ngel sotelo ||| zhixiong li ||| 
2021 ||| fii-centernet: an anchor-free detector with foreground attention for traffic object detection. ||| siqi fan ||| fenghua zhu ||| shichao chen ||| hui zhang ||| bin tian ||| yisheng lv ||| fei-yue wang ||| 
2019 ||| common semantic representation method based on object attention and adversarial learning for cross-modal data in iov. ||| feifei kou ||| junping du ||| wanqiu cui ||| lei shi ||| pengchao cheng ||| jiannan chen ||| jinxuan li ||| 
2019 ||| design considerations of efficiency enhanced llc pev charger using reconfigurable transformer. ||| haoyu wang ||| ming shang ||| dongdong shu ||| 
2021 ||| anomaly detection for in-vehicle network using cnn-lstm with attention mechanism. ||| heng sun ||| miaomiao chen ||| jian weng ||| zhiquan liu ||| guanggang geng ||| 
2021 ||| residual attention network-based confidence estimation algorithm for non-holonomic constraint in gnss/ins integrated navigation system. ||| yimin xiao ||| haiyong luo ||| fang zhao ||| fan wu ||| xile gao ||| qu wang ||| lizhen cui ||| 
2021 ||| hyperspectral image classification based on dual-branch spectral multiscale attention network. ||| cuiping shi ||| diling liao ||| yi xiong ||| tianyu zhang ||| liguo wang ||| 
2020 ||| hierarchical attention and bilinear fusion for remote sensing image scene classification. ||| donghang yu ||| haitao guo ||| qing xu ||| jun lu ||| chuan zhao ||| yuzhun lin ||| 
2021 ||| center attention network for hyperspectral image classification. ||| zhengang zhao ||| dan hu ||| hao wang ||| xianchuan yu ||| 
2021 ||| densely connected multiscale attention network for hyperspectral image classification. ||| hongmin gao ||| yawen miao ||| xueying cao ||| chenming li ||| 
2021 ||| attention_fpnet: two-branch remote sensing image pansharpening network based on attention feature fusion. ||| xiwu zhong ||| yurong qian ||| hui liu ||| long chen ||| yaling wan ||| liang gao ||| jing qian ||| jun liu ||| 
2022 ||| transformer-driven semantic relation inference for multilabel classification of high-resolution remote sensing images. ||| xiaowei tan ||| zhifeng xiao ||| jianjun zhu ||| qiao wan ||| kai wang ||| deren li ||| 
2021 ||| attention consistent network for remote sensing scene classification. ||| xu tang ||| qiushuo ma ||| xiangrong zhang ||| fang liu ||| jingjing ma ||| licheng jiao ||| 
2020 ||| multimodal bilinear fusion network with second-order attention-based channel selection for land cover classification. ||| xiao li ||| lin lei ||| yuli sun ||| ming li ||| gangyao kuang ||| 
2021 ||| multiview attention cnn-lstm network for sar automatic target recognition. ||| chenwei wang ||| xiaoyu liu ||| jifang pei ||| yulin huang ||| yin zhang ||| jianyu yang ||| 
2021 ||| cfcanet: a complete frequency channel attention network for sar image scene classification. ||| bo su ||| jun liu ||| xin su ||| bin luo ||| qing wang ||| 
2020 ||| attention-based domain adaptation using residual network for hyperspectral image classification. ||| robiulhossain mdrafi ||| qian du ||| ali cafer g ||| rb ||| z ||| bo tang ||| li ma ||| nicolas h. younan ||| 
2020 ||| deep collaborative attention network for hyperspectral image classification by combining 2-d cnn and 3-d cnn. ||| hao guo ||| jianjun liu ||| jinlong yang ||| zhiyong xiao ||| zebin wu ||| 
2021 ||| nonlocal band attention network for hyperspectral image band selection. ||| tiancong li ||| yaoming cai ||| zhihua cai ||| xiaobo liu ||| qiubo hu ||| 
2020 ||| using an attention-based lstm encoder-decoder network for near real-time disturbance detection. ||| yuan yuan ||| lei lin ||| lian-zhi huo ||| yun-long kong ||| zengguang zhou ||| bin wu ||| yan jia ||| 
2021 ||| mha-net: multipath hybrid attention network for building footprint extraction from high-resolution remote sensing imagery. ||| jihong cai ||| yimin chen ||| 
2022 ||| tr-misr: multiimage super-resolution based on feature fusion with transformers. ||| tai an ||| xin zhang ||| chunlei huo ||| bin xue ||| lingfeng wang ||| chunhong pan ||| 
2021 ||| integrating gate and attention modules for high-resolution image semantic segmentation. ||| zixian zheng ||| xueliang zhang ||| pengfeng xiao ||| zhenshi li ||| 
2021 ||| lightweight oriented object detection using multiscale context and enhanced channel attention in remote sensing images. ||| qiong ran ||| qing wang ||| boya zhao ||| yuanfeng wu ||| shengliang pu ||| zijin li ||| 
2021 ||| siamese spectral attention with channel consistency for hyperspectral image classification. ||| leiquan wang ||| yao lin ||| jinyun liu ||| zhongwei li ||| chunlei wu ||| 
2020 ||| deep prototypical networks with hybrid residual attention for hyperspectral image classification. ||| bobo xi ||| jiaojiao li ||| yunsong li ||| rui song ||| yanzi shi ||| songlin liu ||| qian du ||| 
2021 ||| attention-gate-based encoder-decoder network for automatical building extraction. ||| wenjing deng ||| qian shi ||| jun li ||| 
2021 ||| dspcanet: dual-channel scale-aware segmentation network with position and channel attentions for high-resolution aerial images. ||| yun-cheng li ||| heng-chao li ||| wen-shuai hu ||| hui-ling yu ||| 
2021 ||| stransfuse: fusing swin transformer and convolutional neural network for remote sensing image semantic segmentation. ||| liang gao ||| hui liu ||| minhang yang ||| long chen ||| yaling wan ||| zhengqing xiao ||| yurong qian ||| 
2021 ||| litescanet: an efficient lightweight network based on spectral and channel-wise attention for hyperspectral image classification. ||| su qiao ||| xue-mei dong ||| jiangtao peng ||| weiwei sun ||| 
2021 ||| agcdetnet: an attention-guided network for building change detection in high-resolution remote sensing images. ||| kaiqiang song ||| jie jiang ||| 
2021 ||| spectral-spatial attention feature extraction for hyperspectral image classification based on generative adversarial network. ||| hongbo liang ||| wenxing bao ||| xiangfei shen ||| xiaowu zhang ||| 
2021 ||| semsdnet: a multiscale dense network with attention for remote sensing scene classification. ||| tian tian ||| lingling li ||| weitao chen ||| huabing zhou ||| 
2021 ||| contextual sa-attention convolutional lstm for precipitation nowcasting: a spatiotemporal sequence forecasting view. ||| taisong xiong ||| jianxing he ||| hao wang ||| xiaowen tang ||| zhao shi ||| qiangyu zeng ||| 
2021 ||| csds: end-to-end aerial scenes classification with depthwise separable convolution and an attention mechanism. ||| xinyu wang ||| liming yuan ||| haixia xu ||| xianbin wen ||| 
2021 ||| road extraction using a dual attention dilated-linknet based on satellite images and floating vehicle trajectory data. ||| lipeng gao ||| jingyu wang ||| qixin wang ||| wenzhong shi ||| jiangbin zheng ||| hongping gan ||| zhiyong lv ||| honghai qiao ||| 
2019 ||| high-resolution aerial images semantic segmentation using deep fully convolutional network with channel attention mechanism. ||| haifeng luo ||| chongcheng chen ||| lina fang ||| xi zhu ||| lijing lu ||| 
2020 ||| an augmentation attention mechanism for high-spatial-resolution remote sensing image scene classification. ||| fengpeng li ||| ruyi feng ||| wei han ||| lizhe wang ||| 
2021 ||| a multiscale dual-branch feature fusion and attention network for hyperspectral images classification. ||| hongmin gao ||| yiyan zhang ||| zhonghao chen ||| chenming li ||| 
2022 ||| gcsanet: a global context spatial attention deep learning network for remote sensing scene classification. ||| weitao chen ||| shubing ouyang ||| wei tong ||| xianju li ||| xiongwei zheng ||| lizhe wang ||| 
2021 ||| few-shot object detection with self-adaptive attention network for remote sensing images. ||| zixuan xiao ||| jiahao qi ||| wei xue ||| ping zhong ||| 
2022 ||| attention-based octave network for hyperspectral image denoising. ||| ziwen kan ||| suhang li ||| mingzheng hou ||| leyuan fang ||| yi zhang ||| 
2021 ||| attention multisource fusion-based deep few-shot learning for hyperspectral image classification. ||| xuejian liang ||| ye zhang ||| junping zhang ||| 
2021 ||| sentinel-3 super-resolution based on dense multireceptive channel attention. ||| rafael fernandez ||| rub ||| n fern ||| ndez-beltran ||| jian kang ||| filiberto pla ||| 
2021 ||| dapnet: a double self-attention convolutional network for point cloud semantic labeling. ||| li chen ||| weiye chen ||| zewei xu ||| haozhe huang ||| shaowen wang ||| qing zhu ||| haifeng li ||| 
2020 ||| attention receptive pyramid network for ship detection in sar images. ||| yan zhao ||| lingjun zhao ||| boli xiong ||| gangyao kuang ||| 
2021 ||| multimodal representation learning and set attention for lwir in-scene atmospheric compensation. ||| nicholas westing ||| kevin c. gross ||| brett j. borghetti ||| christine m. schubert-kabban ||| jacob a. martin ||| joseph meola ||| 
2021 ||| multilabel remote sensing image annotation with multiscale attention and label correlation. ||| rui huang ||| fengcai zheng ||| wei huang ||| 
2022 ||| relation-attention networks for remote sensing scene classification. ||| xin wang ||| lin duan ||| chen ning ||| huiyu zhou ||| 
2021 ||| selective kernel res-attention unet: deep learning for generating decorrelation mask with applications to tandem-x interferograms. ||| qi zhang ||| teng wang ||| yuanyuan pei ||| xuguo shi ||| 
2022 ||| homo-heterogenous transformer learning framework for rs scene classification. ||| jingjing ma ||| mingteng li ||| xu tang ||| xiangrong zhang ||| fang liu ||| licheng jiao ||| 
2021 ||| attention-guided label refinement network for semantic segmentation of very high resolution aerial orthoimages. ||| jianfeng huang ||| xinchang zhang ||| ying sun ||| qinchuan xin ||| 
2021 ||| cs-hsnet: a cross-siamese change detection network based on hierarchical-split attention. ||| qingtian ke ||| peng zhang ||| 
2021 ||| attention-based tri-unet for remote sensing image pan-sharpening. ||| wanwan zhang ||| jinjiang li ||| zhen hua ||| 
2021 ||| sa-jstn: self-attention joint spatiotemporal network for temperature forecasting. ||| lukui shi ||| nanying liang ||| xia xu ||| tao li ||| zhou zhang ||| 
2021 ||| self-supervised pretraining of transformers for satellite image time series classification. ||| yuan yuan ||| lei lin ||| 
2020 ||| channel-attention-based densenet network for remote sensing image scene classification. ||| wei tong ||| weitao chen ||| wei han ||| xianju li ||| lizhe wang ||| 
2021 ||| cross-layer attention network for small object detection in remote sensing imagery. ||| yangyang li ||| qin huang ||| xuan pei ||| yanqiao chen ||| licheng jiao ||| ronghua shang ||| 
2020 ||| a cnn-transformer hybrid approach for crop classification using multitemporal multisensor images. ||| zhengtao li ||| guokun chen ||| tianxu zhang ||| 
2020 ||| 3-d channel and spatial attention based multiscale spatial-spectral residual network for hyperspectral image classification. ||| zhenyu lu ||| bin xu ||| le sun ||| tianming zhan ||| songze tang ||| 
2022 ||| sar image despeckling using continuous attention module. ||| jaekyun ko ||| sanghwan lee ||| 
2021 ||| hresnetam: hierarchical residual network with attention mechanism for hyperspectral image classification. ||| zhixiang xue ||| xuchu yu ||| bing liu ||| xiong tan ||| xiangpo wei ||| 
2021 ||| visual attention and background subtraction with adaptive weight for hyperspectral anomaly detection. ||| pei xiang ||| jiangluqi song ||| hanlin qin ||| wei tan ||| huan li ||| huixin zhou ||| 
2021 ||| da-roadnet: a dual-attention network for road extraction from high resolution satellite imagery. ||| jie wan ||| zhong xie ||| yongyang xu ||| siqiong chen ||| qinjun qiu ||| 
2022 ||| multiscale densely connected attention network for hyperspectral image classification. ||| xin wang ||| yanguo fan ||| 
2021 ||| umag-net: a new unsupervised multiattention-guided network for hyperspectral and multispectral image fusion. ||| shuaiqi liu ||| siyu miao ||| jian su ||| bing li ||| weiming hu ||| yu-dong zhang ||| 
2021 ||| remote sensing image super-resolution via residual aggregation and split attentional fusion network. ||| long chen ||| hui liu ||| minhang yang ||| yurong qian ||| zhengqing xiao ||| xiwu zhong ||| 
2020 ||| a novel attention fully convolutional network method for synthetic aperture radar image segmentation. ||| zhenyu yue ||| fei gao ||| qingxu xiong ||| jun wang ||| amir hussain ||| huiyu zhou ||| 
2021 ||| a multiscale attention network for remote sensing scene images classification. ||| guokai zhang ||| weizhe xu ||| wei zhao ||| chenxi huang ||| eddie y. k. ng ||| yongyong chen ||| jian su ||| 
2021 ||| attention-based object detection with saliency loss in remote sensing images. ||| qin wu ||| xingxing yuan ||| zikang yao ||| zhilei chai ||| 
2020 ||| video-based person re-identification with parallel spatial-temporal attention module. ||| jun kong ||| zhende teng ||| min jiang ||| hongtao huo ||| 
2021 ||| a-darts: attention-guided differentiable architecture search for lung nodule classification. ||| liangxiao hu ||| qinglin liu ||| jun zhang ||| feng jiang ||| yang liu ||| shengping zhang ||| 
2020 ||| aligned attention for common multimodal embeddings. ||| shagan sah ||| sabarish gopalakrishnan ||| raymond w. ptucha ||| 
2018 ||| dual-level attention-aware network for temporal emotion segmentation. ||| bo sun ||| meng guo ||| siming cao ||| jun he ||| lejun yu ||| 
2021 ||| action recognition for sports video analysis using part-attention spatio-temporal graph convolutional network. ||| jiatong liu ||| yanli che ||| 
2019 ||| carf-net: cnn attention and rnn fusion network for video-based person reidentification. ||| kajal kansal ||| a. venkata subramanyam ||| dilip k. prasad ||| mohan s. kankanhalli ||| 
2021 ||| deep progressive attention for person re-identification. ||| changhao wang ||| guanwen zhang ||| wei zhou ||| 
2021 ||| weighted feature fusion and attention mechanism for object detection. ||| yanhao cheng ||| weibin liu ||| weiwei xing ||| 
2020 ||| attn-eh aln: complex text-to-image generation with attention-enhancing adversarial learning networks. ||| cunyi lin ||| xianwei rong ||| ming liu ||| xiaoyan yu ||| 
2019 ||| widget detection network: widget detection in mobile screenshot with region-based attention networks. ||| lin qi ||| tiezhu wang ||| 
2021 ||| facial expression recognition based on facial part attention mechanism. ||| qiubo zhong ||| baofu fang ||| shenbin wei ||| zaijun wang ||| haoxiang zhang ||| 
2021 ||| tamnet: two attention modules-based network on facial expression recognition under uncertainty. ||| jie shao ||| yan luo ||| 
2021 ||| boundary-enhanced attention-aware network for detecting salient objects in rgb-depth images. ||| junwei wu ||| wujie zhou ||| 
2019 ||| spatiotemporal information deep fusion network with frame attention mechanism for video action recognition. ||| hongshi ou ||| jifeng sun ||| 
2021 ||| person re-identification based on attention clustering and long short-term memory network. ||| jun wang ||| jiahui zhu ||| zhimin yu ||| 
2019 ||| attention module-based spatial-temporal graph convolutional networks for skeleton-based action recognition. ||| yinghui kong ||| li li ||| ke zhang ||| qiang ni ||| jungong han ||| 
2021 ||| siampat: siamese point attention networks for robust visual tracking. ||| hang chen ||| weiguo zhang ||| danghui yan ||| 
2020 ||| spatial-temporal graph attention networks for skeleton-based action recognition. ||| qingqing huang ||| fengyu zhou ||| jiakai he ||| yang zhao ||| runze qin ||| 
2021 ||| progressive multi-scale attention network for compression artifact reduction. ||| xinyan zhang ||| peng gao ||| guitao li ||| liuguo yin ||| 
2021 ||| graph attention mechanism with global contextual information for multi-label image recognition. ||| xiaoxiao ban ||| peihua li ||| qilong wang ||| shoujun zhou ||| shijie guo ||| yuanquan wang ||| 
2020 ||| dpsa: dense pixelwise spatial attention network for hatching egg fertility detection. ||| lei geng ||| yunyun xu ||| zhitao xiao ||| jun tong ||| 
2021 ||| channel and spatial attention-based siamese network for visual object tracking. ||| shishun tian ||| zixi chen ||| bolin chen ||| wenbin zou ||| xia li ||| 
2020 ||| img-net: inner-cross-modal attentional multigranular network for description-based person re-identification. ||| zijie wang ||| aichun zhu ||| zhe zheng ||| jing jin ||| zhouxin xue ||| gang hua ||| 
2021 ||| dual attention and part drop network for person reidentification. ||| guang han ||| yuechuan ai ||| jixin liu ||| ning sun ||| guangwei gao ||| 
2021 ||| lightweight facial expression recognition method based on attention mechanism and key region fusion. ||| yinghui kong ||| zhaohan ren ||| ke zhang ||| shuaitong zhang ||| qiang ni ||| jungong han ||| 
2021 ||| adaptive scene-aware deep attention network for remote sensing image compression. ||| guowei zhai ||| gang liu ||| xiaohai he ||| zhengyong wang ||| chao ren ||| zhengxin chen ||| 
2020 ||| crowd counting via an inverse attention residual network. ||| yan-bo liu ||| ruisheng jia ||| qingming liu ||| zhi-feng xu ||| hong-mei sun ||| 
2021 ||| monodepthplus: self-supervised monocular depth estimation using soft-attention and learnable outlier-masking. ||| jun zhang ||| lu yang ||| 
2021 ||| abnormal event detection algorithm based on dual attention future frame prediction and gap fusion discrimination. ||| dongliang wang ||| suyu wang ||| 
2021 ||| violence behavior recognition of two-cascade temporal shift module with attention mechanism. ||| qiming liang ||| yong li ||| bowei chen ||| kaikai yang ||| 
2021 ||| super-resolution of compressed images using enhanced attention network. ||| xinhuan wang ||| zhengyong wang ||| xiaohai he ||| chao ren ||| pradeep karn ||| 
2021 ||| thermal imaging pedestrian detection algorithm based on attention guidance and local cross-level network. ||| lixian yu ||| yanni wang ||| xuesong sun ||| shipeng han ||| 
2019 ||| an attention-based spiking neural network for unsupervised spike-sorting. ||| marie bernert ||| blaise yvert ||| 
2019 ||| lateral inhibition organizes beta attentional modulation in the primary visual cortex. ||| elzbieta gajewska-dendek ||| andrzej wr ||| bel ||| marek bekisz ||| piotr suffczynski ||| 
2021 ||| the influence of visual attention on the performance of a novel tactile p300 brain-computer interface with cheeks-stim paradigm. ||| ying mao ||| jing jin ||| ren xu ||| shurui li ||| yangyang miao ||| andrzej cichocki ||| 
2021 ||| graph attention network with focal loss for seizure detection on electroencephalography signals. ||| yanna zhao ||| gaobo zhang ||| changxu dong ||| qi yuan ||| fangzhou xu ||| yuanjie zheng ||| 
2021 ||| multi-task learning with multi-view weighted fusion attention for artery-specific calcification analysis. ||| weiwei zhang ||| guang yang ||| nan zhang ||| lei xu ||| xiaoqing wang ||| yanping zhang ||| heye zhang ||| javier del ser ||| victor hugo c. de albuquerque ||| 
2019 ||| hierarchical multi-modal fusion fcn with attention model for rgb-d tracking. ||| ming-xin jiang ||| chao deng ||| jing-song shan ||| yuanyuan wang ||| yinjie jia ||| xing sun ||| 
2021 ||| a defense method based on attention mechanism against traffic sign adversarial samples. ||| hailiang li ||| bin zhang ||| yu zhang ||| xilin dang ||| yuwei han ||| linfeng wei ||| yijun mao ||| jian weng ||| 
2020 ||| a dual-branch attention fusion deep network for multiresolution remote-sensing image classification. ||| hao zhu ||| wenping ma ||| lingling li ||| licheng jiao ||| shuyuan yang ||| biao hou ||| 
2021 ||| pay attention to doctor-patient dialogues: multi-modal knowledge graph attention image-text embedding for covid-19 diagnosis. ||| wenbo zheng ||| lan yan ||| chao gou ||| zhi-cheng zhang ||| jun jason zhang ||| ming hu ||| fei-yue wang ||| 
2020 ||| granger causality-based information fusion applied to electrical measurements from power transformers. ||| jacob rodriguez-rivero ||| javier ram ||| rez ||| francisco jes ||| s mart ||| nez-murcia ||| ferm ||| n segovia ||| andr ||| s ortiz ||| diego salas-gonzalez ||| diego castillo-barnes ||| ignacio  ||| lvarez ill ||| n ||| carlos garc ||| a puntonet ||| carmen jimenez-mesa ||| f. j. leiva ||| s. carillo ||| john suckling ||| juan manuel g ||| rriz ||| 
2019 ||| cross-modality interactive attention network for multispectral pedestrian detection. ||| lu zhang ||| zhiyong liu ||| shifeng zhang ||| xu yang ||| hong qiao ||| kaizhu huang ||| amir hussain ||| 
2022 ||| a fusion spatial attention approach for few-shot learning. ||| heda song ||| bowen deng ||| michael p. pound ||| ender  ||| zcan ||| isaac triguero ||| 
2020 ||| multi-class arrhythmia detection from 12-lead varied-length ecg using attention-based time-incremental convolutional neural network. ||| qihang yao ||| ruxin wang ||| xiaomao fan ||| jikui liu ||| ye li ||| 
2020 ||| multimodal feature fusion by relational reasoning and attention for visual question answering. ||| weifeng zhang ||| jing yu ||| hua hu ||| haiyang hu ||| zengchang qin ||| 
2022 ||| pesa-net: permutation-equivariant split attention network for correspondence learning. ||| zhen zhong ||| guobao xiao ||| shiping wang ||| leyi wei ||| xiaoqin zhang ||| 
2021 ||| multimodal feature-wise co-attention method for visual question answering. ||| sheng zhang ||| min chen ||| jincai chen ||| fuhao zou ||| yuan-fang li ||| ping lu ||| 
2021 ||| transient finite element method for computing and analyzing the effect of harmonics on hysteresis and eddy current loss of distribution transformer. ||| vibhuti ||| deepika bhalla ||| genius walia ||| 
2022 ||| coupled field magnetostatic analysis for free buckling in double layer helical winding of a distribution transformer. ||| vibhuti ||| deepika bhalla ||| genius walia ||| 
2017 ||| an attentional model for autonomous mobile robots. ||| esther luna colombini ||| alexandre da silva sim ||| es ||| carlos henrique costa ribeiro ||| 
2020 ||| a hierarchical attention model for ctr prediction based on user interest. ||| qianqian wang ||| fangai liu ||| pu huang ||| shuning xing ||| xiaohui zhao ||| 
2018 ||| adequate planning of shunt power capacitors involving transformer capacity release benefit. ||| abdullah mohammed shaheen ||| ragab a. el-sehiemy ||| sobhy m. farrag ||| 
2017 ||| visual focus of attention estimation using eye center localization. ||| haibin cai ||| bangli liu ||| jianhua zhang ||| shengyong chen ||| honghai liu ||| 
2021 ||| discrimination of internal faults and other transients in an interconnected system with power transformers and phase angle regulators. ||| pallav kumar bera ||| can isik ||| vajendra kumar ||| 
2020 ||| mind: mind networked device architecture for attention-gated ambient assisted living systems. ||| anandarup mukherjee ||| sudip misra ||| abhay atrish ||| 
2020 ||| control of parallel ultc transformers in active distribution systems. ||| alireza pouladi ||| alimorad khajeh zadeh ||| alireza nouri ||| 
2017 ||| conaim: a conscious attention-based integrated model for human-like robots. ||| alexandre da silva sim ||| es ||| esther luna colombini ||| carlos henrique costa ribeiro ||| 
2020 ||| neural attention model with keyword memory for abstractive document summarization. ||| yunseok choi ||| dahae kim ||| jee-hyong lee ||| 
2021 ||| influence of promotion mode on purchase decision based on multilevel psychological distance dimension of visual attention model and data mining. ||| tiantian tang ||| pei hu ||| ge wu ||| 
2021 ||| a gastric cancer recognition algorithm on gastric pathological sections based on multistage attention-densenet. ||| bo liu ||| yelong zhao ||| bin yang ||| shuangtao zhao ||| rentao gu ||| mark gahegan ||| 
2021 ||| conditional pre-trained attention based chinese question generation. ||| liang zhang ||| ligang fang ||| zheng fan ||| wei li ||| jing an ||| 
2021 ||| image super-resolution with parallel convolution attention network. ||| qiao zhang ||| xiaomin yang ||| long xiao ||| feng yang ||| farhan hussain ||| pyoung won kim ||| 
2020 ||| various syncretic co-attention network for multimodal sentiment analysis. ||| meng cao ||| yonghua zhu ||| wenjing gao ||| mengyao li ||| shaoxiu wang ||| 
2021 ||| tatt-bilstm: web service classification with topical attention-based bilstm. ||| guosheng kang ||| yong xiao ||| jianxun liu ||| yingcheng cao ||| buqing cao ||| xiangping zhang ||| linghang ding ||| 
2021 ||| social rumor detection based on multilayer transformer encoding blocks. ||| lijun lin ||| zhiyun chen ||| 
2021 ||| sev-net: residual network embedded with attention mechanism for plant disease severity detection. ||| yun zhao ||| jiagui chen ||| xing xu ||| jingsheng lei ||| wujie zhou ||| 
2021 ||| multiscale channel attention network for infrared and visible image fusion. ||| jiahui zhu ||| qingyu dou ||| lihua jian ||| kai liu ||| farhan hussain ||| xiaomin yang ||| 
2021 ||| flexible scene text recognition based on dual attention mechanism. ||| zhiqiang tian ||| chunhui wang ||| youzi xiao ||| yuping lin ||| 
2021 ||| web service classification based on information gain theory and bidirectional long short-term memory with attention mechanism. ||| xiangping zhang ||| jianxun liu ||| buqing cao ||| min shi ||| 
2021 ||| calculation and analysis of dc magnetic bias current of urban main transformer under the action of stray current. ||| weili wu ||| wenmei chen ||| lei li ||| 
2021 ||| remote sensing data detection based on multiscale fusion and attention mechanism. ||| min huang ||| cong cheng ||| gennaro de luca ||| 
2021 ||| rdmmfet: representation of dense multimodality fusion encoder based on transformer. ||| xu zhang ||| dezhi han ||| chin-chen chang ||| 
2021 ||| facial expression recognition method combined with attention mechanism. ||| ming chen ||| junqiang cheng ||| zhifeng zhang ||| yuhua li ||| yi zhang ||| 
2021 ||| air quality prediction based on a spatiotemporal attention mechanism. ||| xiangyu zou ||| jinjin zhao ||| duan zhao ||| bin sun ||| yong-xin he ||| stelios fuentes ||| 
2021 ||| research on uyghur-chinese neural machine translation based on the transformer at multistrategy segmentation granularity. ||| zhiwang xu ||| huibin qin ||| yongzhu hua ||| 
2021 ||| multiscale dense cross-attention mechanism with covariance pooling for hyperspectral image scene classification. ||| runmin liu ||| xin ning ||| weiwei cai ||| guangjun li ||| 
2021 ||| music feature classification based on recurrent neural networks with channel attention mechanism. ||| jie gan ||| 
2021 ||| joint source-target encoding with pervasive attention. ||| maha elbayad ||| laurent besacier ||| jakob verbeek ||| 
2021 ||| joint extraction of entities and relations based on character graph convolutional network and multi-head self-attention mechanism. ||| zhao meng ||| shengwei tian ||| long yu ||| yalong lv ||| 
2020 ||| visualization of user's attention on objects in 3d environment using only eye tracking glasses. ||| ting-hao li ||| hiromasa suzuki ||| yutaka ohtake ||| 
2022 ||| attention-based spatial-temporal neural network for accurate phase recognition in minimally invasive surgery: feasibility and efficiency verification. ||| pan shi ||| zijian zhao ||| kaidi liu ||| feng li ||| 
2019 ||| attention and anticipation in fast visual-inertial navigation. ||| luca carlone ||| sertac karaman ||| 
2020 ||| learning manipulation skills via hierarchical spatial attention. ||| marcus gualtieri ||| robert platt jr. ||| 
2019 ||| pressure characteristics of a novel double rotor hydraulic transformer. ||| jihai jiang ||| zhongxun liu ||| 
2020 ||| variable speed digital hydraulic transformer-based servo drive. ||| matti linjama ||| 
2019 ||| the state monitoring method of electronic voltage transformer based on l-m algorithm. ||| han lian ||| 
2021 ||| mashup tag completion with attention-based topic model. ||| min shi ||| yufei tang ||| yu huang ||| maohua lin ||| 
2019 ||| 28-ghz cmos vco with capacitive splitting and transformer feedback techniques for 5g communication. ||| yupeng fu ||| lianming li ||| dongming wang ||| xuan wang ||| long he ||| 
2020 ||| a 3.15-mw +16.0-dbm iip3 22-db cg inductively source degenerated balun-lna mixer with integrated transformer-based gate inductor and im2 injection technique. ||| nandini vitee ||| harikrishnan ramiah ||| pui-in mak ||| jun yin ||| rui paulo martins ||| 
2017 ||| a 0.9-5.8-ghz software-defined receiver rf front-end with transformer-based current-gain boosting and harmonic rejection calibration. ||| liang wu ||| alan w. l. ng ||| shiyuan zheng ||| hiu fai leung ||| yue chao ||| alvin li ||| howard c. luong ||| 
2020 |||  full-span differential vector modulator phase rotator with transformer-based poly-phase quadrature network. ||| tso-wei li ||| jong seok park ||| hua wang ||| 
2021 ||| han-bsvd: a hierarchical attention network for binary software vulnerability detection. ||| han yan ||| senlin luo ||| limin pan ||| yifei zhang ||| 
2019 ||| attention-based convolutional approach for misinformation identification from massive and noisy microblog posts. ||| feng yu ||| qiang liu ||| shu wu ||| liang wang ||| tieniu tan ||| 
2020 ||| adsad: an unsupervised attention-based discrete sequence anomaly detection framework for network security analysis. ||| zhi-quan qin ||| xing-kong ma ||| yong-jun wang ||| 
2021 ||| finefool: a novel dnn object contour attack on image recognition based on the attention perturbation adversarial technique. ||| jinyin chen ||| haibin zheng ||| hui xiong ||| ruoxi chen ||| tianyu du ||| zhen hong ||| shouling ji ||| 
2021 ||| automatically predicting cyber attack preference with attributed heterogeneous attention networks and transductive learning. ||| jun zhao ||| xudong liu ||| qiben yan ||| bo li ||| minglai shao ||| hao peng ||| lichao sun ||| 
2021 ||| doubigru-a: software defect detection algorithm based on attention mechanism and double bigru. ||| jinxiong zhao ||| sensen guo ||| dejun mu ||| 
2021 ||| phishing websites detection via cnn and multi-head self-attention on imbalanced datasets. ||| xi xiao ||| wentao xiao ||| dianyan zhang ||| bin zhang ||| guangwu hu ||| qing li ||| shutao xia ||| 
2019 ||| neural malware analysis with attention mechanism. ||| hiromu yakura ||| shinnosuke shinozaki ||| reon nishimura ||| yoshihiro oyama ||| jun sakuma ||| 
2021 ||| : interpretable malware detector using galaxy transformer. ||| miles q. li ||| benjamin c. m. fung ||| philippe charland ||| steven h. h. ding ||| 
2021 ||| low pass filter design with improved stop-band suppression and synthesis with transformer-free ladders. ||| serkan yildiz ||| ahmet aksen ||| sedat kilinc ||| b. siddik yarman ||| 
2018 ||| low-power design for dc current transformer using class-d compensating amplifier. ||| slavko veinovic ||| milan ponjavic ||| sasa d. milic ||| radivoje djuric ||| 
2017 ||| serious games in k-12 education: benefits and impacts on students with attention, memory and developmental disabilities. ||| george papanastasiou ||| athanasios drigas ||| charalabos skianis ||| miltiadis d. lytras ||| 
2019 ||| multisensory integration and exogenous spatial attention: a time-window-of-integration analysis. ||| adele diederich ||| hans colonius ||| 
2020 ||| short-term smartphone app-based focused attention meditation diminishes cognitive flexibility. ||| nicole wolff ||| christian beste ||| 
2019 ||| modulation of event-related potentials of visual discrimination by meditation training and sustained attention. ||| anthony p. zanesco ||| brandon g. king ||| chivon powers ||| rosanna de meo ||| kezia wineberg ||| katherine a. maclean ||| clifford d. saron ||| 
2017 ||| distinct frontoparietal networks underlying attentional effort and cognitive control. ||| anne s. berry ||| martin sarter ||| cindy lustig ||| 
2021 ||| attentional templates are sharpened through differential signal enhancement, not differential allocation of attention. ||| dirk kerzel ||| stanislas huynh cong ||| 
2021 ||| steady-state visually evoked potentials and feature-based attention: preregistered null results and a focused review of methodological considerations. ||| kirsten c. s. adam ||| lillian chang ||| nicole rangan ||| john t. serences ||| 
2022 ||| diversion of attention leads to conflict between concurrently attended stimuli, not delayed orienting to the object of interest. ||| jennifer-ashley hoffmeister ||| andrea n. smit ||| ashley c. livingstone ||| john j. mcdonald ||| 
2020 ||| attentional access to multiple target objects in visual search. ||| nick berggren ||| martin eimer ||| 
2020 ||| deconstructing reorienting of attention: cue predictiveness modulates the inhibition of the no-target side and the hemispheric distribution of the p1 response to invalid targets. ||| fabrizio doricchi ||| michele pellegrino ||| fabio marson ||| mario pinto ||| ludovica caratelli ||| vincenzo cestari ||| clelia rossi-arnaud ||| stefano lasaponara ||| 
2022 ||| no evidence of attentional modulation of the neural response to the temporal fine structure of continuous musical pieces. ||| octave etard ||| remy ben messaoud ||| gabriel gaugain ||| tobias reichenbach ||| 
2022 ||| learning at variable attentional load requires cooperation of working memory, meta-learning, and attention-augmented reinforcement learning. ||| thilo womelsdorf ||| marcus r. watson ||| paul h. e. tiesinga ||| 
2019 ||| attentional facilitation of constituent features of an object does not spread automatically along object-defining cortical boundaries. ||| berit brummerloh ||| christopher gundlach ||| matthias m. m ||| ller ||| 
2019 ||| neural dynamics of cognitive control over working memory capture of attention. ||| peter s. whitehead ||| mathilde m. ooi ||| tobias egner ||| marty g. woldorff ||| 
2017 ||| "nonspatial" attentional deficits interact with spatial position in neglect. ||| dongyun li ||| christopher rorden ||| hans-otto karnath ||| 
2018 ||| prefrontal modulation of visual processing and sustained attention in aging, a tdcs-eeg coregistration approach. ||| m ||| adhbh b. brosnan ||| mahnaz arvaneh ||| siobh ||| n harty ||| tara maguire ||| redmond g. o'connell ||| ian h. robertson ||| paul m. dockree ||| 
2021 ||| stimulus-induced alpha suppression tracks the difficulty of attentional selection, not visual working memory storage. ||| sisi wang ||| emma e. megla ||| geoffrey f. woodman ||| 
2021 ||| no effect of transcranial direct current stimulation over left dorsolateral prefrontal cortex on temporal attention. ||| raquel e. london ||| heleen a. slagter ||| 
2020 ||| alpha-band activity tracks the zoom lens of attention. ||| tobias feldmann-w ||| stefeld ||| edward awh ||| 
2019 ||| neural correlates of enhanced visual attentional control in action video game players: an event-related potential study. ||| julia f ||| cker ||| matin mortazavi ||| wayne khoe ||| steven a. hillyard ||| daphne bavelier ||| 
2017 ||| power and phase of alpha oscillations reveal an interaction between spatial and temporal visual attention. ||| sayeed a. d. kizuk ||| kyle e. mathewson ||| 
2021 ||| shifting attention in feature space: fast facilitation of the to-be-attended feature is followed by slow inhibition of the to-be-ignored feature. ||| paula vieweg ||| matthias m. m ||| ller ||| 
2021 ||| sustained attention and spatial attention distinctly influence long-term memory encoding. ||| megan t. debettencourt ||| stephanie d. williams ||| edward k. vogel ||| edward awh ||| 
2017 ||| enhancing spatial attention and working memory in younger and older adults. ||| camarin e. rolle ||| joaquin a. anguera ||| sasha n. skinner ||| bradley voytek ||| adam gazzaley ||| 
2017 ||| feature-selective attention in frontoparietal cortex: multivoxel codes adjust to prioritize task-relevant information. ||| jade b. jackson ||| anina n. rich ||| mark a. williams ||| alexandra woolgar ||| 
2018 ||| pupillary correlates of fluctuations in sustained attention. ||| nash unsworth ||| matthew k. robison ||| ashley l. miller ||| 
2017 ||| the pivotal role of the right parietal lobe in temporal attention. ||| sara agosta ||| denise magnago ||| sarah c. tyler ||| emily d. grossman ||| emanuela galante ||| francesco ferraro ||| nunzia mazzini ||| gabriele miceli ||| lorella battelli ||| 
2017 ||| fluctuations of attentional networks and default mode network during the resting state reflect variations in cognitive states: evidence from a novel resting-state experience sampling method. ||| laurens van calster ||| arnaud d'argembeau ||| eric salmon ||| fr ||| d ||| ric peters ||| steve majerus ||| 
2021 ||| overlapping neuronal population responses in the human parietal cortex during visuospatial attention and arithmetic processing. ||| nan liu ||| pedro pinheiro chagas ||| clara sava-segal ||| sabine kastner ||| qi chen ||| josef parvizi ||| 
2019 ||| taking attention out of context: frontopolar transcranial magnetic stimulation abolishes the formation of new context memories in visual search. ||| artyom zinchenko ||| markus conci ||| paul c. j. taylor ||| hermann j. m ||| ller ||| thomas geyer ||| 
2018 ||| attending to what and where: background connectivity integrates categorical and spatial attention. ||| alexa tompary ||| naseem al-aidroos ||| nicholas b. turk-browne ||| 
2020 ||| unraveling the relation between eeg correlates of attentional orienting and sound localization performance: a diffusion model approach. ||| laura-isabelle klatt ||| daniel schneider ||| anna-lena schubert ||| christina hanenberg ||| j ||| rg lewald ||| edmund wascher ||| stephan getzmann ||| 
2021 ||| self-reported mind wandering and response time variability differentiate prestimulus electroencephalogram microstate dynamics during a sustained attention task. ||| anthony p. zanesco ||| ekaterina denkova ||| amishi p. jha ||| 
2018 ||| connectome-based models predict separable components of attention in novel individuals. ||| monica d. rosenberg ||| wei-ting hsu ||| dustin scheinost ||| r. todd constable ||| marvin m. chun ||| 
2020 ||| neural mechanisms of strategic adaptation in attentional flexibility. ||| anthony w. sali ||| jiefeng jiang ||| tobias egner ||| 
2017 ||| dopamine alters the fidelity of working memory representations according to attentional demands. ||| sean james fallon ||| nahid zokaei ||| agnes norbury ||| sanjay g. manohar ||| masud husain ||| 
2017 ||| global enhancement but local suppression in feature-based attention. ||| norman forschack ||| s ||| ren k. andersen ||| matthias m. m ||| ller ||| 
2017 ||| hierarchies of attention and experimental designs: effects of spatial and intermodal attention revisited. ||| talia shrem ||| leon y. deouell ||| 
2020 ||| resting-state functional connectivity of the right temporoparietal junction relates to belief updating and reorienting during spatial attention. ||| anne-sophie k ||| sbauer ||| paola mengotti ||| gereon r. fink ||| simone vossel ||| 
2022 ||| spatial and feature-selective attention have distinct, interacting effects on population-level tuning. ||| erin goddard ||| thomas a. carlson ||| alexandra woolgar ||| 
2019 ||| the functional consequences of social attention for memory-guided attention orienting and anticipatory neural dynamics. ||| brianna ruth doherty ||| freek van ede ||| alexander fraser ||| eva zita patai ||| anna christina nobre ||| gaia scerif ||| 
2019 ||| tuning attention to object categories: spatially global effects of attention to faces in visual processing. ||| viola s. st ||| rmer ||| michael a. cohen ||| george a. alvarez ||| 
2018 ||| dynamics of feature-based attentional selection during color-shape conjunction search. ||| jeongmi lee ||| carly j. leonard ||| steven j. luck ||| joy j. geng ||| 
2017 ||| comparing the effects of 10-hz repetitive tms on tasks of visual stm and attention. ||| stephen m. emrich ||| jeffrey s. johnson ||| david w. sutterer ||| bradley r. postle ||| 
2019 ||| attentional modulation of visual spatial integration: psychophysical evidence supported by population coding modeling. ||| alessandro grillini ||| remco j. renken ||| frans w. cornelissen ||| 
2019 ||| spatially specific attention mechanisms are sensitive to competition during visual search. ||| lu-chun yeh ||| yei-yu yeh ||| bo-cheng kuo ||| 
2018 ||| polarity-dependent effects of biparietal transcranial direct current stimulation on the interplay between target location and distractor saliency in visual attention. ||| magdalena chechlacz ||| peter c. hansen ||| joy j. geng ||| dario cazzoli ||| 
2017 ||| soap opera: self as object and agent in prioritizing attention. ||| grace truong ||| rebecca m. todd ||| 
2018 ||| auditory attention causes gain enhancement and frequency sharpening at successive stages of cortical processing - evidence from human electroencephalography. ||| jessica de boer ||| katrin krumbholz ||| 
2019 ||| dissociating reward- and attention-driven biasing of global feature-based selection in human visual cortex. ||| haydee g. garcia-lazaro ||| mandy v. bartsch ||| carsten nicolas boehler ||| ruth m. krebs ||| sarah e. donohue ||| joseph a. harris ||| mircea ariel schoenfeld ||| jens-max hopf ||| 
2019 ||| brain and cognitive mechanisms of top-down attentional control in a multisensory world: benefits of electrical neuroimaging. ||| pawel j. matusz ||| nora turoman ||| ruxandra i. tivadar ||| chrysa retsa ||| micah m. murray ||| 
2017 ||| attention to distinct goal-relevant features differentially guides semantic knowledge retrieval. ||| gavin k. hanson ||| evangelia g. chrysikou ||| 
2020 ||| slow endogenous fluctuations in cortical fmri signals correlate with reduced performance in a visual detection task and are suppressed by spatial attention. ||| david w. bressler ||| ariel rokem ||| michael a. silver ||| 
2017 ||| rebalancing spatial attention: endogenous orienting may partially overcome the left visual field bias in rapid serial visual presentation. ||| kamila smigasiewicz ||| gabriel sami hasan ||| rolf verleger ||| 
2020 ||| spatial attention and temporal expectation exert differential effects on visual and auditory discrimination. ||| anna wilsch ||| manuel r. mercier ||| jonas obleser ||| charles e. schroeder ||| saskia haegens ||| 
2019 ||| testing the possibility of model-based pavlovian control of attention to threat. ||| deborah talmi ||| martina slapkova ||| matthias j. wieser ||| 
2020 ||| contextual modulation of emotional distraction: attentional capture and motivational significance. ||| antonia micucci ||| vera ferrari ||| andrea de cesarei ||| maurizio codispoti ||| 
2017 ||| magnocellular bias in exogenous attention to biologically salient stimuli as revealed by manipulating their luminosity and color. ||| luis carreti ||| dominique kessel ||| mar ||| a j. garc ||| a-rubio ||| tamara gim ||| nez-fern ||| ndez ||| sandra hoyos ||| mar ||| a hern ||| ndez-lorca ||| 
2020 ||| tracking the effects of top-down attention on word discrimination using frequency-tagged neuromagnetic responses. ||| maxime niesen ||| marc vander ghinst ||| mathieu bourguignon ||| vincent wens ||| julie bertels ||| serge goldman ||| georges choufani ||| sergio hassid ||| xavier de ti ||| ge ||| 
2022 ||| leveraging spiking deep neural networks to understand the neural mechanisms underlying selective attention. ||| lynn k. a. s ||| rensen ||| davide zambrano ||| heleen a. slagter ||| sander m. boht ||| h. steven scholte ||| 
2018 ||| selective attention to faces in a rapid visual stream: hemispheric differences in enhancement and suppression of category-selective neural activity. ||| genevieve quek ||| dan nemrodov ||| bruno rossion ||| joan liu-shuang ||| 
2020 ||| attention for speaking: prestimulus motor-cortical alpha power predicts picture naming latencies. ||| suzanne r. jongman ||| ardi roelofs ||| ashley glen lewis ||| 
2021 ||| hemisphere-specific parietal contributions to the interplay between working memory and attention. ||| anastasia kiyonaga ||| john p. powers ||| yu-chin chiu ||| tobias egner ||| 
2018 ||| visual working memory load disrupts template-guided attentional selection during visual search. ||| nick berggren ||| martin eimer ||| 
2018 ||| contralateral delay activity indexes working memory storage, not the current focus of spatial attention. ||| tobias feldmann-w ||| stefeld ||| edward k. vogel ||| edward awh ||| 
2021 ||| motivational salience guides attention to valuable and threatening stimuli: evidence from behavior and functional magnetic resonance imaging. ||| haena kim ||| namrata nanavaty ||| humza ahmed ||| vani a. mathur ||| brian a. anderson ||| 
2018 ||| attentional fluctuations influence the neural fidelity and connectivity of stimulus representations. ||| david rothlein ||| joe degutis ||| michael esterman ||| 
2021 ||| caffeine boosts preparatory attention for reward-related stimulus information. ||| berry van den berg ||| marlon de jong ||| marty g. woldorff ||| monicque m. lorist ||| 
2018 ||| rapid improvement on a temporal attention task within a single session of high-frequency transcranial random noise stimulation. ||| sarah c. tyler ||| federica cont ||| lorella battelli ||| 
2020 ||| distinct neural mechanisms meet challenges in dynamic visual attention due to either load or object spacing. ||| veronica m ||| ki-marttunen ||| thomas hagen ||| bruno laeng ||| thomas espeseth ||| 
2021 ||| the microstructure of attentional control in the dorsal attention network. ||| abhijit rajan ||| sreenivasan meyyappan ||| yuelu liu ||| immanuel babu henry samuel ||| bijurika nandi ||| george r. mangun ||| mingzhou ding ||| 
2018 ||| the dorsal attention network reflects both encoding load and top-down control during working memory. ||| steve majerus ||| fr ||| d ||| ric peters ||| marion bouffier ||| nelson cowan ||| christophe phillips ||| 
2021 ||| attention biases competition for visual representation via dissociable influences from frontal and parietal cortex. ||| andrew d. sheldon ||| elyana saad ||| muhammet i. sahan ||| emma e. meyering ||| michael j. starrett ||| joshua j. larocque ||| nathan s. rose ||| bradley r. postle ||| 
2018 ||| independent attention mechanisms control the activation of tactile and visual working memory representations. ||| tobias katus ||| martin eimer ||| 
2018 ||| spatial attention enhances the neural representation of invisible signals embedded in noise. ||| cooper a. smout ||| jason b. mattingley ||| 
2017 ||| preparatory encoding of the fine scale of human spatial attention. ||| bradley voytek ||| jason samaha ||| camarin e. rolle ||| zachery greenberg ||| navdeep gill ||| shai porat ||| tahim kader ||| sabahat rahman ||| rick malzyner ||| adam gazzaley ||| 
2017 ||| moving beyond attentional biases: shifting the interhemispheric balance between left and right posterior parietal cortex modulates attentional control processes. ||| felix duecker ||| teresa schuhmann ||| nina bien ||| christianne jacobs ||| alexander thomas sack ||| 
2019 ||| hemifield-specific correlations between cue-related blood oxygen level dependent activity in bilateral nodes of the dorsal attention network and attentional benefits in a spatial orienting paradigm. ||| helen c. mayrhofer ||| felix duecker ||| vincent van de ven ||| heidi i. l. jacobs ||| alexander thomas sack ||| 
2020 ||| enhanced attention using head-mounted virtual reality. ||| gang li ||| joaquin a. anguera ||| samirah v. javed ||| muhammad adeel khan ||| guoxing wang ||| adam gazzaley ||| 
2020 ||| voluntary control of task selection does not eliminate the impact of selection history on attention. ||| dion t. henare ||| hanna kadel ||| anna schub ||| 
2020 ||| the neural consequences of attentional prioritization of internal representations in visual working memory. ||| muhammet i. sahan ||| andrew d. sheldon ||| bradley r. postle ||| 
2021 ||| dynamic interplay between reward and voluntary attention determines stimulus processing in visual cortex. ||| ivan grahek ||| antonio schettino ||| ernst h. w. koster ||| s ||| ren k. andersen ||| 
2018 ||| spatially selective alpha oscillations reveal moment-by-moment trade-offs between working memory and attention. ||| dirk van moorselaar ||| joshua j. foster ||| david w. sutterer ||| jan theeuwes ||| christian n. l. olivers ||| edward awh ||| 
2019 ||| top-down attention is limited within but not between feature dimensions. ||| nika adamian ||| elena slaustaite ||| s ||| ren k. andersen ||| 
2019 ||| causal evidence for the role of neuronal oscillations in top-down and bottom-up attention. ||| justin riddle ||| kai hwang ||| dillan cellier ||| sofia dhanani ||| mark d'esposito ||| 
2019 ||| attentional weighting in the face processing network: a magnetic response image-guided magnetoencephalography study using multiple cyclic entrainments. ||| eelke de vries ||| daniel baldauf ||| 
2020 ||| involuntary orienting and conflict resolution during auditory attention: the role of ventral and dorsal streams. ||| hannah j. stewart ||| dawei shen ||| nasim sham ||| claude alain ||| 
2021 ||| probing the neural systems underlying flexible dimensional attention. ||| aaron t. buss ||| vincent magnotta ||| eliot hazeltine ||| kaleb kinder ||| john p. spencer ||| 
2018 ||| cross-frequency phase-amplitude coupling as a mechanism for temporal orienting of attention in childhood. ||| giovanni mento ||| duncan e. astle ||| gaia scerif ||| 
2017 ||| intermodal attention shifts in multimodal working memory. ||| tobias katus ||| anna grubert ||| martin eimer ||| 
2021 ||| a direct comparison of spatial attention and stimulus-response compatibility between mice and humans. ||| ulf h. schnabel ||| tobias van der bijl ||| pieter r. roelfsema ||| jeannette a. m. lorteije ||| 
2017 ||| neural representation of working memory content is modulated by visual attentional demand. ||| anastasia kiyonaga ||| emma wu dowd ||| tobias egner ||| 
2020 ||| does closing the eyes enhance auditory attention? eye closure increases attentional alpha-power modulation but not listening performance. ||| malte w ||| stmann ||| lea-maria schmitt ||| jonas obleser ||| 
2021 ||| mosaic convolution-attention network for demosaicing multispectral filter array images. ||| kai feng ||| yongqiang zhao ||| jonathan cheung-wai chan ||| seong g. kong ||| xun zhang ||| binglu wang ||| 
2020 ||| cagan: a cycle-consistent generative adversarial network with attention for low-dose ct imaging. ||| zhiyuan huang ||| zixiang chen ||| qiyang zhang ||| guotao quan ||| min ji ||| chengjin zhang ||| yongfeng yang ||| xin liu ||| dong liang ||| hairong zheng ||| zhanli hu ||| 
2022 ||| rational inattention and public signals. ||| daniel susskind ||| 
2021 ||| dynamic choice under familiarity-based attention. ||| guy barokas ||| 
2018 ||| salience and limited attention. ||| yukinori iwata ||| 
2021 ||| automatic semicircular canal segmentation of ct volumes using improved 3d u-net with attention mechanism. ||| hongcheng wu ||| juanxiu liu ||| gui chen ||| weixing liu ||| ruqian hao ||| lin liu ||| guangming ni ||| yong liu ||| xiaowen zhang ||| jing zhang ||| 
2021 ||| an attention mechanism oriented hybrid cnn-rnn deep learning architecture of container terminal liner handling conditions prediction. ||| bin li ||| yuqing he ||| 
2021 ||| fraudulent news headline detection with attention mechanism. ||| hankun liu ||| daojing he ||| sammy chan ||| 
2020 ||| change detection of remote sensing images based on attention mechanism. ||| long chen ||| dezheng zhang ||| peng li ||| peng lv ||| 
2020 ||| inherent importance of early visual features in attraction of human attention. ||| reza eghdam ||| reza ebrahimpour ||| iman zabbah ||| sajjad zabbah ||| 
2020 ||| extracting parallel sentences from nonparallel corpora using parallel hierarchical attention network. ||| shaolin zhu ||| yong yang ||| chun xu ||| 
2021 ||| deep learning based on hierarchical self-attention for finance distress prediction incorporating text. ||| sumei ruan ||| xusheng sun ||| ruanxingchen yao ||| wei li ||| 
2021 ||| social recommendation system based on hypergraph attention network. ||| zhongxiu xia ||| weiyu zhang ||| ziqiang weng ||| 
2021 ||| remaining useful life estimation of aircraft engines using a joint deep learning model based on tcnn and transformer. ||| hai-kun wang ||| yi cheng ||| ke song ||| 
2021 ||| simultaneous pickup and delivery traveling salesman problem considering the express lockers using attention route planning network. ||| yu du ||| shaochuan fu ||| changxiang lu ||| qiang zhou ||| chunfang li ||| 
2021 ||| analysis of volleyball video intelligent description technology based on computer memory network and attention mechanism. ||| zhongzi zhang ||| 
2021 ||| multiscale convolutional neural networks with attention for plant species recognition. ||| xianfeng wang ||| chuanlei zhang ||| shanwen zhang ||| 
2021 ||| egat: extended graph attention network for pedestrian trajectory prediction. ||| wei kong ||| yun liu ||| hui li ||| chuanxu wang ||| 
2019 ||| dual cnn for relation extraction with knowledge-based attention and word embeddings. ||| jun li ||| guimin huang ||| jianheng chen ||| yabing wang ||| 
2021 ||| afi-net: attention-guided feature integration network for rgbd saliency detection. ||| liming li ||| shuguang zhao ||| rui sun ||| xiao-dong chai ||| shu-bin zheng ||| xingjie chen ||| zhaomin lv ||| 
2017 ||| fuzzy classification of high resolution remote sensing scenes using visual attention features. ||| linyi li ||| tingbao xu ||| yun chen ||| 
2021 ||| utilizing entity-based gated convolution and multilevel sentence attention to improve distantly supervised relation extraction. ||| qian yi ||| guixuan zhang ||| shuwu zhang ||| 
2021 ||| intelligent malaysian sign language translation system using convolutional-based attention module with residual network. ||| rehman ullah khan ||| hizbullah khattak ||| woei sheng wong ||| hussain alsalman ||| mogeeb a. a. mosleh ||| sk. md. mizanur rahman ||| 
2021 ||| attention-based temporal encoding network with background-independent motion mask for action recognition. ||| zhengkui weng ||| zhipeng jin ||| shuangxi chen ||| quanquan shen ||| xiangyang ren ||| wuzhao li ||| 
2019 ||| leveraging contextual sentences for text classification by using a neural attention model. ||| danfeng yan ||| shiyao guo ||| 
2021 ||| a novel time-incremental end-to-end shared neural network with attention-based feature fusion for multiclass motor imagery recognition. ||| shidong lian ||| jialin xu ||| guokun zuo ||| xia wei ||| huilin zhou ||| 
2022 ||| a pavement crack detection method based on multiscale attention and hfs. ||| chun li ||| yu wen ||| qingxuan shi ||| fang yang ||| hongyan ma ||| xuedong tian ||| 
2021 ||| hybrid lstm self-attention mechanism model for forecasting the reform of scientific research in morocco. ||| asmaa fahim ||| tan qingmei ||| mouna mazzi ||| md sahabuddin ||| bushra naz ||| sibghat ullah bazai ||| 
2019 ||| eeg alpha power is modulated by attentional changes during cognitive tasks and virtual reality immersion. ||| elisa magosso ||| francesca de crescenzio ||| giulia ricci ||| sergio piastra ||| mauro ursino ||| 
2019 ||| a stacked bilstm neural network based on coattention mechanism for question answering. ||| linqin cai ||| sitong zhou ||| xun yan ||| rongdi yuan ||| 
2019 ||| optimized complex network method (ocnm) for improving accuracy of measuring human attention in single-electrode neurofeedback system. ||| zhengping wu ||| wei zhang ||| jing zhao ||| chun chen ||| peng ji ||| 
2019 ||| attention-based personalized encoder-decoder model for local citation recommendation. ||| libin yang ||| zeqing zhang ||| xiaoyan cai ||| tao dai ||| 
2021 ||| research on volleyball video intelligent description technology combining the long-term and short-term memory network and attention mechanism. ||| yuhua gao ||| yong mo ||| heng zhang ||| ruiyin huang ||| zilong chen ||| 
2019 ||| context attention heterogeneous network embedding. ||| wei zhuo ||| qianyi zhan ||| yuan liu ||| zhenping xie ||| jing lu ||| 
2021 ||| dar-net: dense attentional residual network for vehicle detection in aerial images. ||| kaifeng li ||| bin wang ||| 
2021 ||| medical text classification using hybrid deep learning models with multihead attention. ||| sunil kumar prabhakar ||| dong-ok won ||| 
2021 ||| a generative adversarial network fused with dual-attention mechanism and its application in multitarget image fine segmentation. ||| jian yin ||| zhibo zhou ||| shaohua xu ||| ruiping yang ||| kun liu ||| 
2021 ||| identification of navel orange diseases and pests based on the fusion of densenet and self-attention mechanism. ||| yin'e zhang ||| yong ping liu ||| 
2021 ||| hierarchical attention-based multimodal fusion network for video emotion recognition. ||| xiaodong liu ||| songyang li ||| miao wang ||| 
2020 ||| effects of visual attention on tactile p300 bci. ||| zongmei chen ||| jing jin ||| ian daly ||| cili zuo ||| xingyu wang ||| andrzej cichocki ||| 
2017 ||| complexity analysis of resting-state fmri in adult patients with attention deficit hyperactivity disorder: brain entropy. ||| g ||| ls ||| m akdeniz ||| 
2021 ||| a multi-rnn research topic prediction model based on spatial attention and semantic consistency-based scientific influence modeling. ||| mingying xu ||| junping du ||| zeli guan ||| zhe xue ||| feifei kou ||| lei shi ||| xin xu ||| ang li ||| 
2020 ||| interactive dual attention network for text sentiment classification. ||| yinglin zhu ||| wenbin zheng ||| hong tang ||| 
2021 ||| dual-path attention compensation u-net for stroke lesion segmentation. ||| haisheng hui ||| xueying zhang ||| zelin wu ||| fenlian li ||| 
2021 ||| a multitask learning model with multiperspective attention and its application in recommendation. ||| yingshuai wang ||| dezheng zhang ||| aziguli wulamu ||| 
2022 ||| research on real-time face key point detection algorithm based on attention mechanism. ||| jiangjin gao ||| tao yang ||| 
2020 ||| a knowledge-fusion ranking system with an attention network for making assignment recommendations. ||| canghong jin ||| yuli zhou ||| shengyu ying ||| chi zhang ||| weisong wang ||| minghui wu ||| 
2021 ||| a study of two-way short- and long-term memory network intelligent computing iot model-assisted home education attention mechanism. ||| suling ma ||| 
2021 ||| a multiattention-based supervised feature selection method for multivariate time series. ||| li cao ||| yanting chen ||| zhiyang zhang ||| ning gui ||| 
2021 ||| multicomponent spatial-temporal graph attention convolution networks for traffic prediction with spatially sparse data. ||| shaohua liu ||| shijun dai ||| jingkai sun ||| tianlu mao ||| junsuo zhao ||| heng zhang ||| 
2020 ||| using cnn and channel attention mechanism to identify driver's distracted behavior. ||| lu ye ||| cheng chen ||| mingwei wu ||| samuel nwobodo ||| annor arnold antwi ||| chido natasha muponda ||| koi david ernest ||| rugamba sadam vedaste ||| 
2018 ||| attention decrease detection based on video analysis in e-learning. ||| liying wang ||| 
2020 ||| detecting aging substation transformers by audio signal with deep neural network. ||| wei ye ||| jiasai sun ||| min xu ||| xuemeng yang ||| hongliang li ||| yong liu ||| 
2021 ||| partial discharge pattern recognition of transformers based on the gray-level co-occurrence matrix of optimal parameters. ||| shengya sun ||| yuanyuan sun ||| gongde xu ||| lina zhang ||| yiru hu ||| ping liu ||| 
2020 ||| research and application of generator protection based on fiber optical current transformer. ||| jun chen ||| qingshan xu ||| kai wang ||| 
2020 ||| hybrid attention distribution and factorized embedding matrix in image captioning. ||| jian wang ||| jie feng ||| 
2020 ||| multi-scale attention generative adversarial networks for video frame interpolation. ||| jian xiao ||| xiaojun bi ||| 
2019 ||| r-trans: rnn transformer network for chinese machine reading comprehension. ||| shanshan liu ||| sheng zhang ||| xin zhang ||| hui wang ||| 
2019 ||| person re-identification based on two-stream network with attention and pose features. ||| xiaowei gong ||| suguo zhu ||| 
2022 ||| heterogeneous attention concentration link prediction algorithm for attracting customer flow in online brand community. ||| shugang li ||| boyi zhu ||| he zhu ||| fang liu ||| yuqi zhang ||| ru wang ||| hanyu lu ||| 
2020 ||| image inpainting based on inside-outside attention and wavelet decomposition. ||| xingchen he ||| xudong cui ||| qilong li ||| 
2019 ||| road marking segmentation based on siamese attention module and maximum stable external region. ||| weiwei zhang ||| zeyang mi ||| yaocheng zheng ||| qiaoming gao ||| wenjing li ||| 
2019 ||| captionnet: automatic end-to-end siamese difference captioning model with attention. ||| oluwasanmi ariyo ||| muhammad umar aftab ||| eatedal alabdulkreem ||| bulbula kumeda ||| edward yellakuor baagyere ||| zhiquang qin ||| 
2021 ||| real-time semantic segmentation of remote sensing images based on bilateral attention refined network. ||| jiali cai ||| chunjuan liu ||| haowen yan ||| xiaosuo wu ||| wanzhen lu ||| xiaoyu wang ||| changlin sang ||| 
2020 ||| capacity enhancement of a radial distribution grid using smart transformer. ||| v. m. hrishikesan ||| anup kumar deka ||| chandan kumar ||| 
2021 ||| csi-ianet: an inception attention network for human-human interaction recognition based on csi signal. ||| m. humayun kabir ||| md. hafizur rahman ||| wonjae shin ||| 
2020 ||| short-term load forecasting using recurrent neural networks with input attention mechanism and hidden connection mechanism. ||| mingfei zhang ||| zhoutao yu ||| zhenghua xu ||| 
2019 ||| attention-based character-word hybrid neural networks with semantic and structural information for identifying of urgent posts in mooc discussion forums. ||| shou xi guo ||| xia sun ||| shixiong wang ||| yi gao ||| jun feng ||| 
2019 ||| boosting arabic named-entity recognition with multi-attention layer. ||| mohammed nadher abdo ali ||| guanzheng tan ||| aamir hussain ||| 
2020 ||| hybrid attention densely connected ensemble framework for lesion segmentation from magnetic resonance images. ||| beibei hou ||| xin xu ||| guixia kang ||| yuan tang ||| chuan hu ||| 
2019 ||| lstm-crf neural network with gated self attention for chinese ner. ||| yanliang jin ||| jinfei xie ||| weisi guo ||| can luo ||| dijia wu ||| rui wang ||| 
2020 ||| separable attention capsule network for signal classification. ||| shaoqing liu ||| huiling liu ||| chen yang ||| shuyuan yang ||| min wang ||| 
2019 ||| an indoor temperature prediction framework based on hierarchical attention gated recurrent unit model for energy efficient buildings. ||| jiancai song ||| guixiang xue ||| yunpeng ma ||| han li ||| yu pan ||| zhangxiao hao ||| 
2018 ||| a 62-90 ghz high linearity and low noise cmos mixer using transformer-coupling cascode topology. ||| zhiqing liu ||| jiayu dong ||| zhilin chen ||| zhengdong jiang ||| pengxue liu ||| yunqiu wu ||| chenxi zhao ||| kai kang ||| 
2020 ||| an intrusion detection model with hierarchical attention mechanism. ||| chang liu ||| yang liu ||| yu yan ||| ji wang ||| 
2019 ||| aspect based sentiment analysis with feature enhanced attention cnn-bilstm. ||| wei meng ||| yongqing wei ||| peiyu liu ||| zhenfang zhu ||| hongxia yin ||| 
2022 ||| cmsea: compound model scaling with efficient attention for fine-grained image classification. ||| jinzheng guang ||| jianru liang ||| 
2021 ||| etdnet: an efficient transformer deraining model. ||| qin qin ||| jingke yan ||| xin wang ||| qin wang ||| minyao li ||| yuqing wang ||| 
2021 ||| a duality based quasi-steady-state model of three-phase five-limb sen transformer. ||| chao zhou ||| song han ||| na rong ||| min liu ||| 
2020 ||| fast trajectory prediction method with attention enhanced sru. ||| yadong li ||| bailong liu ||| lei zhang ||| susong yang ||| changxing shao ||| dan son ||| 
2019 ||| cascaded conditional generative adversarial networks with multi-scale attention fusion for automated bi-ventricle segmentation in cardiac mri. ||| lin qi ||| haoran zhang ||| wenjun tan ||| shouliang qi ||| lisheng xu ||| yudong yao ||| wei qian ||| 
2020 ||| a novel clothing attribute representation network-based self-attention mechanism. ||| yutong chun ||| chuansheng wang ||| mingke he ||| 
2022 ||| reliable estimation for health index of transformer oil based on novel combined predictive maintenance techniques. ||| mohamed badawi ||| shimaa a. ibrahim ||| diaa-eldin a. mansour ||| adel el-faraskoury ||| sayed a. ward ||| karar mahmoud ||| matti lehtonen ||| mohamed m. f. darwish ||| 
2019 ||| mscoa: multi-step co-attention model for multi-label classification. ||| haoyang ma ||| yujun li ||| xianpeng ji ||| junlei han ||| zeqiang li ||| 
2020 ||| aspect-context interactive attention representation for aspect-level sentiment classification. ||| zhuojia wu ||| yang li ||| jian liao ||| deyu li ||| xiaoli li ||| suge wang ||| 
2019 ||| hran: hybrid residual attention network for single image super-resolution. ||| abdul muqeet ||| md. tauhid bin iqbal ||| sung-ho bae ||| 
2022 ||| spatio-temporal self-attention network for fire detection and segmentation in video surveillance. ||| mohammad shahid ||| john jethro virtusio ||| yu-hsien wu ||| yung-yao chen ||| muhammad tanveer ||| khan muhammad ||| kai-lung hua ||| 
2019 ||| webshell detection based on the word attention mechanism. ||| tingting li ||| chunhui ren ||| yusheng fu ||| jie xu ||| jinhong guo ||| xinyu chen ||| 
2019 ||| inter-basket and intra-basket adaptive attention network for next basket recommendation. ||| binbin che ||| pengpeng zhao ||| junhua fang ||| lei zhao ||| victor s. sheng ||| zhiming cui ||| 
2021 ||| bayesian deep neural network to compensate for current transformer saturation. ||| sopheap key ||| sang-hee kang ||| namho lee ||| soon-ryul nam ||| 
2021 ||| attention-modulated triplet network for face sketch recognition. ||| liang fan ||| xianfang sun ||| paul l. rosin ||| 
2021 ||| understanding the influence of power transformer faults on the frequency response signature using simulation analysis and statistical indicators. ||| salem mgammal al-ameri ||| muhammad saufi kamarudin ||| mohd fairouz mohd yousof ||| ali ahmed salem ||| fahd a. banakhr ||| mohamed i. mosaad ||| ahmed abu-siada ||| 
2019 ||| high gain transformer-less double-duty-triple-mode dc/dc converter for dc microgrid. ||| mahajan sagar bhaskar ||| mohammad meraj ||| atif iqbal ||| sanjeevikumar padmanaban ||| pandav kiran maroti ||| rashid alammari ||| 
2021 ||| recognizing pests in field-based images by combining spatial and channel attention mechanism. ||| xinting yang ||| yongchen luo ||| ming li ||| zhankui yang ||| chuanheng sun ||| wenyong li ||| 
2020 ||| spatial attention based real-time object detection network for internet of things devices. ||| yongxin zhang ||| peng zhao ||| deguang li ||| kostromitin konstantin ||| 
2020 ||| multi-frame blind restoration for image of space target with frc and branch-attention. ||| peijian zhu ||| chunzhi xie ||| zhisheng gao ||| 
2019 ||| multi-dimensional residual dense attention network for stereo matching. ||| guanghui zhang ||| dongchen zhu ||| wenjun shi ||| xiaoqing ye ||| jiamao li ||| xiaolin zhang ||| 
2020 ||| face super-resolution reconstruction based on self-attention residual network. ||| qing-ming liu ||| ruisheng jia ||| chao-yue zhao ||| xiao-ying liu ||| hong-mei sun ||| xing-li zhang ||| 
2022 ||| respiratory sound classification: from fluid-solid coupling analysis to feature-band attention. ||| fuchuan tong ||| lingling liu ||| xingjia xie ||| qingyang hong ||| lin li ||| 
2022 ||| an object detection algorithm for rotary-wing uav based on awin transformer. ||| yunlong fan ||| ou li ||| guangyi liu ||| 
2019 ||| rapid transformer health state recognition through canopy cluster-merging of dissolved gas data in high-dimensional space. ||| bo qi ||| peng zhang ||| zhihai rong ||| jianyi wang ||| chengrong li ||| jinxiang chen ||| 
2021 ||| artificial intelligence-based power transformer health index for handling data uncertainty. ||| dhanu rediansyah ||| rahman azis prasojo ||| suwarno ||| ahmed abu-siada ||| 
2019 ||| human action recognition in unconstrained trimmed videos using residual attention network and joints path signature. ||| tasweer ahmad ||| lianwen jin ||| jialuo feng ||| guozhi tang ||| 
2021 ||| a residual-attention offline handwritten chinese text recognition based on fully convolutional neural networks. ||| yintong wang ||| yingjie yang ||| weiping ding ||| shuo li ||| 
2018 ||| attention-based memory network for text sentiment classification. ||| hu han ||| jin liu ||| guoli liu ||| 
2022 ||| an attention-based spatiotemporal ggnn for next poi recommendation. ||| quan li ||| xinhua xu ||| xinghong liu ||| qi chen ||| 
2021 ||| breast cancer histopathology image super-resolution using wide-attention gan with improved wasserstein gradient penalty and perceptual loss. ||| faezehsadat shahidi ||| 
2019 ||| two-stage short-term load forecasting for power transformers under different substation operating conditions. ||| hang liu ||| youyuan wang ||| chao wei ||| jiansheng li ||| yuandi lin ||| 
2019 ||| a hybrid bidirectional recurrent convolutional neural network attention-based model for text classification. ||| jin zheng ||| limin zheng ||| 
2021 ||| an object detection method combining multi-level feature fusion and region channel attention. ||| ge zhu ||| zizun wei ||| feng lin ||| 
2021 ||| triple-stage attention-based multiple parallel connection hybrid neural network model for conditional time series forecasting. ||| yepeng cheng ||| yasuhiko morimoto ||| 
2021 ||| a skill-based visual attention model for cloud gaming. ||| hamed ahmadi ||| saman zadtootaghaj ||| farhad pakdaman ||| mahmoud reza hashemi ||| shervin shirmohammadi ||| 
2020 ||| a dual-transformer-based bidirectional dc-dc converter of using blocking capacitor for wide zvs range. ||| yanke liang ||| xu liu ||| guokai xu ||| shuaichao yue ||| 
2021 ||| deep learning model for house price prediction using heterogeneous data analysis along with joint self-attention mechanism. ||| pei-ying wang ||| chiao-ting chen ||| jain-wun su ||| ting-yun wang ||| szu-hao huang ||| 
2018 ||| exploring new backbone and attention module for semantic segmentation in street scenes. ||| lei fan ||| wei-chien wang ||| fuyuan zha ||| jiapeng yan ||| 
2021 ||| from local to global: efficient dual attention mechanism for single image super-resolution. ||| pei zhang ||| edmund y. lam ||| 
2019 ||| vibro-acoustic methods in the condition assessment of power transformers: a survey. ||| adnan secic ||| matej krpan ||| igor kuzle ||| 
2021 ||| electroencephalogram-based attention level classification using convolution attention memory neural network. ||| chean khim toa ||| kok-swee sim ||| shing chiang tan ||| 
2018 ||| attention alignment multimodal lstm for fine-gained common space learning. ||| sijia chen ||| bin song ||| jie guo ||| 
2021 ||| siamese attentional cascade keypoints network for visual object tracking. ||| ershen wang ||| donglei wang ||| yufeng huang ||| gang tong ||| song xu ||| tao pang ||| 
2020 ||| an accurate ensemble forecasting approach for highly dynamic cloud workload with vmd and r-transformer. ||| shaojing zhou ||| jinguo li ||| kai zhang ||| mi wen ||| qijie guan ||| 
2021 ||| a human-robot interaction system calculating visual focus of human's attention level. ||| partha chakraborty ||| sabbir ahmed ||| mohammad abu yousuf ||| a. k. m. azad ||| salem a. alyami ||| mohammad ali moni ||| 
2022 ||| analysis and comparison of series resonant converter with embedded filters for high power density dcx of solid-state transformer. ||| shota okutani ||| pin-yu huang ||| ryo nishiyama ||| yuichi kado ||| 
2020 ||| attention-based convolutional lstm for describing video. ||| zhongyu liu ||| tian chen ||| enjie ding ||| yafeng liu ||| wanli yu ||| 
2020 ||| multiscale residual attention network for distinguishing stationary humans and common animals under through-wall condition using ultra-wideband radar. ||| yangyang ma ||| fugui qi ||| pengfei wang ||| fulai liang ||| hao lv ||| xiao yu ||| zhao li ||| huijun xue ||| jianqi wang ||| yang zhang ||| 
2019 ||| harsam: a hybrid model for recommendation supported by self-attention mechanism. ||| dunlu peng ||| weiwei yuan ||| cong liu ||| 
2022 ||| an attention-based predictive agent for static and dynamic environments. ||| murchana baruah ||| bonny banerjee ||| atulya k. nagar ||| 
2021 ||| enhancing local dependencies for transformer-based text-to-speech via hybrid lightweight convolution. ||| wei zhao ||| ting he ||| li xu ||| 
2022 ||| feature attention parallel aggregation network for single image haze removal. ||| cuili li ||| yufeng he ||| xu li ||| 
2021 ||| stackda: a stacked dual attention neural network for multivariate time-series forecasting. ||| jungsoo hong ||| jinuk park ||| sanghyun park ||| 
2020 ||| nonlinearity of magnetic core in evaluation of current and phase errors of transformation of higher harmonics of distorted current by inductive current transformers. ||| michal kaczmarek ||| ernest stano ||| 
2019 ||| sentiment analysis of text based on bidirectional lstm with multi-head attention. ||| fei long ||| kai zhou ||| weihua ou ||| 
2020 ||| arbitrary shape natural scene text detection method based on soft attention mechanism and dilated convolution. ||| xiao qin ||| jianhui jiang ||| chang-an yuan ||| shaojie qiao ||| wei fan ||| 
2020 ||| geospatial contextual attention mechanism for automatic and fast airport detection in sar imagery. ||| siyu tan ||| lifu chen ||| zhouhao pan ||| jin xing ||| zhenhong li ||| zhihui yuan ||| 
2021 ||| hierarchical graph attention based multi-view convolutional neural network for 3d object recognition. ||| hui zeng ||| tianmeng zhao ||| ruting cheng ||| fuzhou wang ||| jiwei liu ||| 
2021 ||| a deep learning based light-weight face mask detector with residual context attention and gaussian heatmap to fight against covid-19. ||| xinqi fan ||| mingjie jiang ||| hong yan ||| 
2021 ||| image inpainting with learnable edge-attention maps. ||| liujie sun ||| qinghan zhang ||| wenju wang ||| mingxi zhang ||| 
2019 ||| analysis of electrodermal activity signal collected during visual attention oriented tasks. ||| abdul momin ||| sudip sanyal ||| 
2020 ||| hilbert id considering multi-window feature extraction for transformer deep vision fault positioning. ||| xiaoxin wu ||| yigang he ||| chenyuan wang ||| wenjie wu ||| chuankun wang ||| jiajun duan ||| 
2019 ||| global-local attention network for aerial scene classification. ||| yiyou guo ||| jinsheng ji ||| xiankai lu ||| hong huo ||| tao fang ||| deren li ||| 
2020 ||| state of the art of solid-state transformers: advanced topologies, implementation issues, recent progress and improvements. ||| mahammad abdul hannan ||| pin jern ker ||| molla s. hossain lipu ||| zhen hang choi ||| mohamed abdur rahman ||| kashem m. muttaqi ||| frede blaabjerg ||| 
2022 ||| an interleaved high step-up dc-dc converter based on integration of coupled inductor and built-in-transformer with switched-capacitor cells for renewable energy applications. ||| ramin rahimi ||| saeed habibi ||| mehdi ferdowsi ||| pourya shamsi ||| 
2022 ||| lianet: layer interactive attention network for rgb-d salient object detection. ||| yibo han ||| liejun wang ||| anyu du ||| shaochen jiang ||| 
2020 ||| electrophysiological evidence of attentional avoidance in sub-clinical individuals with obsessive-compulsive symptoms. ||| meng-yun wang ||| zhong-ming zhang ||| xiaocui miao ||| xiaohong lin ||| zhen yuan ||| 
2019 ||| numerical study on natural convective heat transfer of nanofluids in disc-type transformer windings. ||| yunpeng zhang ||| siu lau ho ||| weinong fu ||| xinsheng yang ||| huihuan wu ||| 
2020 ||| a stochastic attention cnn model for rumor stance classification. ||| na bai ||| zhixiao wang ||| fanrong meng ||| 
2022 ||| medical image segmentation using transformer networks. ||| davood karimi ||| haoran dou ||| ali gholipour ||| 
2021 ||| inversion detection of transformer transient hot spot temperature. ||| jiangjun ruan ||| yongqing deng ||| yu quan ||| ruohan gong ||| 
2022 ||| transformer fault diagnosis based on multi-class adaboost algorithm. ||| jifang li ||| genxu li ||| chen hai ||| mengbo guo ||| 
2020 ||| a hybrid method with adaptive sub-series clustering and attention-based stacked residual lstms for multivariate time series forecasting. ||| fagui liu ||| yunsheng lu ||| muqing cai ||| 
2020 ||| attention-based sign language recognition network utilizing keyframe sampling and skeletal features. ||| wei pan ||| xiongquan zhang ||| zhongfu ye ||| 
2020 ||| non-local attention and densely-connected convolutional neural networks for malignancy suspiciousness classification of gastric ulcer. ||| muyi sun ||| kaiyi liang ||| wenbao zhang ||| qing chang ||| xiaoguang zhou ||| 
2020 ||| learning attention-enhanced spatiotemporal representation for action recognition. ||| zhensheng shi ||| liangjie cao ||| cheng guan ||| haiyong zheng ||| zhaorui gu ||| zhibin yu ||| bing zheng ||| 
2022 ||| masked face recognition with mask transfer and self-attention under the covid-19 pandemic. ||| meng zhang ||| rujie liu ||| daisuke deguchi ||| hiroshi murase ||| 
2019 ||| self-attention network for session-based recommendation with streaming data input. ||| shiming sun ||| yuanhe tang ||| zemei dai ||| fu zhou ||| 
2020 ||| an attention-based user preference matching network for recommender system. ||| yuchen liu ||| tan yang ||| tao qi ||| 
2019 ||| a novel unbalance compensation method for distribution solid-state transformer based on reduced order generalized integrator. ||| zhengwei qu ||| yunxiao yao ||| yunjing wang ||| chunjiang zhang ||| zhenxiao chong ||| ahmed abu-siada ||| 
2020 ||| sdban: salient object detection using bilateral attention network with dice coefficient loss. ||| donggoo kang ||| sangwoo park ||| joonki paik ||| 
2018 ||| a fine-grained spatial-temporal attention model for video captioning. ||| an-an liu ||| yurui qiu ||| yongkang wong ||| yuting su ||| mohan s. kankanhalli ||| 
2021 ||| bi-level attention model with topic information for classification. ||| hongtao liu ||| qimin qian ||| 
2020 ||| fault prediction for power transformer using optical spectrum of transformer oil and data mining analysis. ||| nur afini fauzi ||| n. h. nik ali ||| pin jern ker ||| vimal angela thiviyanathan ||| yang sing leong ||| ahmed haroun sabry ||| md zaini jamaludin ||| chin kim lo ||| looe hui mun ||| 
2021 ||| local structural aware heterogeneous information network embedding based on relational self-attention graph neural network. ||| meng cao ||| jinliang yuan ||| ming xu ||| hualei yu ||| chongjun wang ||| 
2019 ||| a novel transformerless current source inverter for leakage current reduction. ||| xiaoqiang guo ||| na wang ||| jianhua zhang ||| baocheng wang ||| minh-khai nguyen ||| 
2019 ||| entity disambiguation leveraging multi-perspective attention. ||| chengzhi wang ||| xian sun ||| hongfeng yu ||| wenkai zhang ||| 
2019 ||| attention-based convolutional and recurrent neural networks for driving behavior recognition using smartphone sensor data. ||| jun zhang ||| zhongcheng wu ||| fang li ||| jianfei luo ||| tingting ren ||| song hu ||| wenjing li ||| wei li ||| 
2021 ||| automated artifact retouching in morphed images with attention maps. ||| guido borghi ||| annalisa franco ||| gabriele graffieti ||| davide maltoni ||| 
2021 ||| a hybrid isolated bidirectional dc/dc solid-state transformer for dc distribution network. ||| yu wang ||| ling yang ||| quanli wu ||| si-zhe chen ||| zhibin yu ||| yuanpeng guan ||| yun zhang ||| 
2019 ||| entity alignment algorithm based on dual-attention and incremental learning mechanism. ||| yang yang ||| maojie hao ||| yonghua huo ||| liandong chen ||| zhipeng gao ||| 
2021 ||| a lightweight multiscale attention semantic segmentation algorithm for detecting laser welding defects on safety vent of power battery. ||| yishuang zhu ||| runze yang ||| yuqing he ||| junxian ma ||| haolin guo ||| yatao yang ||| li zhang ||| 
2020 ||| short text embedding autoencoders with attention-based neighborhood preservation. ||| chao wei ||| lijun zhu ||| jiaoxiang shi ||| 
2020 ||| an improved attention-based spatiotemporal-stream model for action recognition in videos. ||| dan liu ||| yunfeng ji ||| mao ye ||| yan gan ||| jianwei zhang ||| 
2021 ||| semantic-sca: semantic structure image inpainting with the spatial-channel attention. ||| jingjun qiu ||| yan gao ||| meisheng shen ||| 
2019 ||| alternative dielectric fluids for transformer insulation system: progress, challenges, and future prospects. ||| mohan rao ungarala ||| issouf fofana ||| t. jaya ||| esperanza mariela rodriguez-celis ||| jocelyn jalbert ||| patrick picher ||| 
2020 ||| the n270 in facial s1-s2 paradigm as a biomarker for children with attention-deficit/hyperactivity disorder. ||| fujun zhao ||| chao yang ||| yi zheng ||| 
2021 ||| a super resolution algorithm based on attention mechanism and srgan network. ||| baozhong liu ||| ji chen ||| 
2019 ||| characteristics of tin oxide chromatographic detector for dissolved gases analysis of transformer oil. ||| jingmin fan ||| zhe liu ||| anbo meng ||| hao yin ||| qiuqin sun ||| feng bin ||| qinji jiang ||| 
2019 ||| a method for hot spot temperature prediction of a 10 kv oil-immersed transformer. ||| yongqing deng ||| jiangjun ruan ||| yu quan ||| ruohan gong ||| daochun huang ||| cihan duan ||| yiming xie ||| 
2020 ||| a novel attention cooperative framework for automatic modulation recognition. ||| shiyao chen ||| yan zhang ||| zunwen he ||| jinbo nie ||| wancheng zhang ||| 
2020 ||| speech emotion recognition using 3d convolutions and attention-based sliding recurrent networks with auditory front-ends. ||| zhichao peng ||| xingfeng li ||| zhi zhu ||| masashi unoki ||| jianwu dang ||| masato akagi ||| 
2020 ||| a natural interaction method of multi-sensory channels for virtual assembly system of power transformer control cabinet. ||| xuqiang shao ||| xiaohua feng ||| yelu yu ||| zhaohui wu ||| peng mei ||| 
2020 ||| attention-based radar pri modulation recognition with recurrent neural networks. ||| xueqiong li ||| zhangmeng liu ||| zhitao huang ||| 
2021 ||| hierarchical self-attention embedded neural network with dense connection for remote-sensing image semantic segmentation. ||| chunhua li ||| xin li ||| runliang xia ||| tao li ||| xin lyu ||| yao tong ||| liancheng zhao ||| xinyuan wang ||| 
2022 ||| medt: using multimodal encoding-decoding network as in transformer for multimodal sentiment analysis. ||| qingfu qi ||| liyuan lin ||| rui zhang ||| chengrong xue ||| 
2019 ||| whisper to normal speech conversion using sequence-to-sequence mapping model with auditory attention. ||| hailun lian ||| yuting hu ||| weiwei yu ||| jian zhou ||| wenming zheng ||| 
2020 ||| ultra-short-term photovoltaic power prediction based on self-attention mechanism and multi-task learning. ||| yun ju ||| jing li ||| guangyu sun ||| 
2021 ||| method for internal fault testing of instrument transformers with sectioned active parts. ||| dora gazivoda ||| igor ziger ||| ivan novko ||| tomislav zupan ||| 
2020 ||| contextual attention refinement network for real-time semantic segmentation. ||| shijie hao ||| yuan zhou ||| youming zhang ||| yanrong guo ||| 
2019 ||| a dual-attention-based stock price trend prediction model with dual features. ||| yingxuan chen ||| weiwei lin ||| james z. wang ||| 
2021 ||| transformer based language identification for malayalam-english code-mixed text. ||| s. thara ||| prabaharan poornachandran ||| 
2019 ||| scientific literature summarization using document structure and hierarchical attention model. ||| huiyan xu ||| zhijian wang ||| xiaolan weng ||| 
2020 ||| comparison of ageing characteristics of superior insulating fluids with mineral oil for power transformer application. ||| raymon antony raj ||| ravi samikannu ||| abid yahya began ||| modisa mosalaosi ||| 
2020 ||| just another attention network for remaining useful life prediction of rolling element bearings. ||| gangjin huang ||| shungang hua ||| qiang zhou ||| hongkun li ||| yuanliang zhang ||| 
2021 ||| deeplpc-mhanet: multi-head self-attention for augmented kalman filter-based speech enhancement. ||| sujan kumar roy ||| aaron nicolson ||| kuldip k. paliwal ||| 
2020 ||| transformer condition monitoring based on load-varied vibration response and gru neural networks. ||| kaixing hong ||| jie pan ||| ming jin ||| 
2021 ||| 800 kv converter transformer bushing employing nano-hexagonal boron nitride paper using fem. ||| suman yadav ||| harold r. chamorro ||| wilfredo c. flores ||| ram krishna mehta ||| 
2020 ||| integrating hierarchical attentions for future subevent prediction. ||| linmei hu ||| 
2019 ||| self-attention-based bilstm model for short text fine-grained sentiment classification. ||| jun xie ||| bo chen ||| xinglong gu ||| fengmei liang ||| xinying xu ||| 
2019 ||| hpgat: high-order proximity informed graph attention network. ||| zhining liu ||| weiyi liu ||| pin-yu chen ||| chenyi zhuang ||| chengyun song ||| 
2017 ||| transient electromagnetic disturbance induced on the ports of intelligent component of electronic instrument transformer due to switching operations in 500 kv gis substations. ||| heng-tian wu ||| chong-qing jiao ||| xiang cui ||| xiao-fan liu ||| jian-fei ji ||| 
2020 ||| hybrid attention-based prototypical network for unfamiliar restaurant food image few-shot recognition. ||| gege song ||| zhulin tao ||| xianglin huang ||| gang cao ||| wei liu ||| lifang yang ||| 
2019 ||| recognizing food places in egocentric photo-streams using multi-scale atrous convolutional networks and self-attention mechanism. ||| md. mostafa kamal sarker ||| hatem a. rashwan ||| farhan akram ||| estefan ||| a talavera ||| syeda furruka banu ||| petia radeva ||| domenec puig ||| 
2019 ||| strawberry verticillium wilt detection network based on multi-task learning and attention. ||| xuan nie ||| luyao wang ||| haoxuan ding ||| min xu ||| 
2020 ||| enhanced network representation learning with community aware and relational attention. ||| mingqiang zhou ||| dan liu ||| yihan kong ||| haijiang jin ||| 
2020 ||| double attention for multi-label image classification. ||| haiying zhao ||| wei zhou ||| xiaogang hou ||| hui zhu ||| 
2022 ||| automatic requirements classification based on graph attention network. ||| gang li ||| chengpeng zheng ||| min li ||| haosen wang ||| 
2020 ||| a convolutional attention residual network for stereo matching. ||| guangyi huang ||| yongyi gong ||| qingzhen xu ||| kanoksak wattanachote ||| kun zeng ||| xiaonan luo ||| 
2020 ||| attention-based dense decoding network for monocular depth estimation. ||| jianrong wang ||| ge zhang ||| mei yu ||| tianyi xu ||| tao luo ||| 
2019 ||| a numerical-based attention method for stock market prediction with dual information. ||| guang liu ||| xiaojie wang ||| 
2020 ||| the image super-resolution algorithm based on the dense space attention network. ||| chunjiang duanmu ||| junjie zhu ||| 
2021 ||| improving bert with self-supervised attention. ||| yiren chen ||| xiaoyu kou ||| jiangang bai ||| yunhai tong ||| 
2019 ||| a hierarchical attention fused descriptor for 3d point matching. ||| wenjun shi ||| dongchen zhu ||| liang du ||| guanghui zhang ||| jiamao li ||| xiaolin zhang ||| 
2021 ||| diversified semantic attention model for fine-grained entity typing. ||| yanfeng hu ||| xue qiao ||| luo xing ||| chen peng ||| 
2020 ||| flexible design scheme for a simple dual-band ultra-high impedance transformer and its application in a balun. ||| rahul gupta ||| mohammad s. hashmi ||| muhammad akmal chaudhary ||| 
2020 ||| on some imperative ieee standards for usage of natural ester liquids in transformers. ||| mohan rao ungarala ||| issouf fofana ||| janvier sylvestre n'cho ||| 
2019 ||| transient current similarity-based protection for interconnecting transformers in wind farms. ||| jianquan liao ||| xiaonan zhu ||| qianggang wang ||| 
2022 ||| route-based proactive content caching using self-attention in hierarchical federated learning. ||| subina khanal ||| kyi thar ||| eui-nam huh ||| 
2020 ||| colorectal tumor segmentation of ct scans based on a convolutional neural network with an attention mechanism. ||| yun pei ||| lin mu ||| yu fu ||| kan he ||| hong li ||| shuxu guo ||| xiaoming liu ||| mingyang li ||| huimao zhang ||| xueyan li ||| 
2019 ||| interactive multi-head attention networks for aspect-level sentiment classification. ||| qiuyue zhang ||| ran lu ||| qicai wang ||| zhenfang zhu ||| peiyu liu ||| 
2021 ||| student program classification using gated graph attention neural network. ||| mingming lu ||| yulian wang ||| dingwu tan ||| ling zhao ||| 
2020 ||| efficient visual tracking with stacked channel-spatial attention learning. ||| md. maklachur rahman ||| mustansar fiaz ||| soon ki jung ||| 
2019 ||| spatial transformer generative adversarial network for robust image super-resolution. ||| hossam m. kasem ||| kwok-wai hung ||| jianmin jiang ||| 
2021 ||| dealing with data uncertainty for transformer insulation system health index. ||| rahman azis prasojo ||| suwarno ||| ahmed abu-siada ||| 
2021 ||| multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. ||| guangli li ||| chuanxiu li ||| guangting wu ||| donghong ji ||| hongbin zhang ||| 
2022 ||| improving clustering-based forecasting of aggregated distribution transformer loadings with gradient boosting and feature selection. ||| george rouwhorst ||| edgar mauricio salazar duque ||| phuong h. nguyen ||| han slootweg ||| 
2021 ||| combining context-aware embeddings and an attentional deep learning model for arabic affect analysis on twitter. ||| hanane elfaik ||| el habib nfaoui ||| 
2020 ||| aidan: an attention-guided dual-path network for pediatric echocardiography segmentation. ||| yujin hu ||| bei xia ||| muyi mao ||| zelong jin ||| jie du ||| libao guo ||| alejandro f. frangi ||| baiying lei ||| tianfu wang ||| 
2020 ||| attention network for non-uniform deblurring. ||| qing qi ||| jichang guo ||| weipei jin ||| 
2019 ||| joint attention mechanism for person re-identification. ||| shanshan jiao ||| jiabao wang ||| guyu hu ||| zhisong pan ||| lin du ||| jin zhang ||| 
2019 ||| a new online temperature compensation technique for electronic instrument transformers. ||| zhenhua li ||| yawei du ||| ahmed abu-siada ||| zhenxing li ||| tao zhang ||| 
2019 ||| an interpretable disease onset predictive model using crossover attention mechanism from electronic health records. ||| wei guo ||| wei ge ||| lizhen cui ||| hui li ||| lanju kong ||| 
2019 ||| automated heartbeat classification exploiting convolutional neural network with channel-wise attention. ||| feiteng li ||| jiaquan wu ||| menghan jia ||| zhijian chen ||| yu pu ||| 
2020 ||| agricultural pest super-resolution and identification with attention enhanced residual and dense fusion generative and adversarial network. ||| qiang dai ||| xi cheng ||| yan qiao ||| youhua zhang ||| 
2020 ||| using lumped element equivalent network model to derive analytical equations for interpretation of transformer frequency responses. ||| bozhi cheng ||| zhongdong wang ||| peter crossley ||| 
2019 ||| fault diagnosis of power transformers with membership degree. ||| enwen li ||| linong wang ||| bin song ||| 
2022 ||| hann: hybrid attention neural network for detecting covid-19 related rumors. ||| abdulqader m. almars ||| malik almaliki ||| talal h. noor ||| majed alwateer ||| el-sayed atlam ||| 
2020 ||| no load simulation and downscaled experiment of uhv single-phase autotransformer under dc bias. ||| bing li ||| zezhong wang ||| suxin guo ||| mingyang li ||| 
2021 ||| directed eeg functional connectivity features to reveal different attention indexes using hierarchical clustering. ||| hadriana iddas ||| keiji iramina ||| 
2019 ||| spatial-temporal graph attention networks: a deep learning approach for traffic forecasting. ||| chenhan zhang ||| james jian qiao yu ||| yi liu ||| 
2021 ||| synthetic aperture radar sar image target recognition algorithm based on attention mechanism. ||| baodai shi ||| qin zhang ||| dayan wang ||| yao li ||| 
2019 ||| intelligent localization of transformer internal degradations combining deep convolutional neural networks and image segmentation. ||| jiajun duan ||| yigang he ||| bolun du ||| ruaa m. rashad ghandour ||| wenjie wu ||| hui zhang ||| 
2021 ||| a partial discharge localization method in transformers based on linear conversion and density peak clustering. ||| shudong wang ||| yigang he ||| baiqiang yin ||| wenbo zeng ||| ying deng ||| zengchao hu ||| 
2021 ||| wind power forecasting using attention-based recurrent neural networks: a comparative study. ||| bin huang ||| yuying liang ||| xiaolin qiu ||| 
2019 ||| automatic prostate zonal segmentation using fully convolutional network with feature pyramid attention. ||| yongkai liu ||| kyung hyun sung ||| guang yang ||| sohrab afshari mirak ||| melina hosseiny ||| afshin azadikhah ||| xinran zhong ||| robert e. reiter ||| yeejin lee ||| steven s. raman ||| 
2020 ||| attention based multi-layer fusion of multispectral images for pedestrian detection. ||| yongtao zhang ||| zhishuai yin ||| linzhen nie ||| song huang ||| 
2021 ||| dpit: detecting defects of photovoltaic solar cells with image transformers. ||| xiangying xie ||| hu liu ||| zhixiong na ||| xin luo ||| dong wang ||| biao leng ||| 
2019 ||| attention-based deep learning model for predicting collaborations between different research affiliations. ||| hui zhou ||| jinqing sun ||| zhongying zhao ||| yong yang ||| ailei xie ||| francisco chiclana ||| 
2020 ||| a data-driven approach for collision risk early warning in vessel encounter situations using attention-bilstm. ||| jie ma ||| chengfeng jia ||| xin yang ||| xiaochun cheng ||| wenkai li ||| chunwei zhang ||| 
2022 ||| a deep neural network using double self-attention mechanism for als point cloud segmentation. ||| lili yu ||| haiyang yu ||| shuai yang ||| 
2019 ||| accf: learning attentional conformity for collaborative filtering. ||| bin liang ||| chaofeng sha ||| dong wu ||| bo xu ||| yanghua xiao ||| wei wang ||| 
2019 ||| remote sensing image change detection based on information transmission and attention mechanism. ||| ruochen liu ||| zhihong cheng ||| langlang zhang ||| jianxia li ||| 
2021 ||| incorporating relative position information in transformer-based sign language recognition and translation. ||| neena aloysius ||| m. geetha ||| prema nedungadi ||| 
2020 ||| multimodal data processing framework for smart city: a positional-attention based deep learning approach. ||| qianxia ma ||| yongfang nie ||| jingyan song ||| tao zhang ||| 
2019 ||| a deep learning approach for credit scoring of peer-to-peer lending using attention mechanism lstm. ||| chongren wang ||| dongmei han ||| qigang liu ||| suyuan luo ||| 
2020 ||| comparative study of full and reduced feature scenarios for health index computation of power transformers. ||| sherif s. m. ghoneim ||| ibrahim b. m. taha ||| 
2022 ||| neural network with hierarchical attention mechanism for contextual topic dialogue generation. ||| xiao sun ||| bingbing ding ||| 
2020 ||| hierarchical attention-based astronaut gesture recognition: a dataset and cnn model. ||| gu lingyun ||| zhang lin ||| zhaokui wang ||| 
2020 ||| crop leaf disease image super-resolution and identification with dual attention and topology fusion generative adversarial network. ||| qiang dai ||| xi cheng ||| yan qiao ||| youhua zhang ||| 
2020 ||| capsule networks with word-attention dynamic routing for cultural relics relation extraction. ||| min zhang ||| guohua geng ||| 
2021 ||| amsaseg: an attention-based multi-scale atrous convolutional neural network for real-time object segmentation from 3d point cloud. ||| moogab kim ||| naveed ilyas ||| kiseon kim ||| 
2020 ||| a modular multiple dc transformer based dc transmission system for pmsg based offshore wind farm integration. ||| pengfei hu ||| rui yin ||| zhengxu he ||| congling wang ||| 
2019 ||| exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition. ||| ziping zhao ||| zhongtian bao ||| yiqin zhao ||| zixing zhang ||| nicholas cummins ||| zhao ren ||| bj ||| rn w. schuller ||| 
2020 ||| a general traffic flow prediction approach based on spatial-temporal graph attention. ||| cong tang ||| jingru sun ||| yichuang sun ||| mu peng ||| nianfei gan ||| 
2021 ||| stereo feature learning based on attention and geometry for absolute hand pose estimation in egocentric stereo views. ||| kyeongeun seo ||| hyeonjoong cho ||| daewoong choi ||| taewook heo ||| 
2018 ||| matching descriptions to spatial entities using a siamese hierarchical attention network. ||| kai ma ||| liang wu ||| liufeng tao ||| wenjia li ||| zhong xie ||| 
2020 ||| a pornographic images recognition model based on deep one-class classification with visual attention mechanism. ||| junren chen ||| gang liang ||| wenbo he ||| chun xu ||| jin yang ||| ruihang liu ||| 
2019 ||| integrating an attention mechanism and convolution collaborative filtering for document context-aware rating prediction. ||| bangzuo zhang ||| haobo zhang ||| xiaoxin sun ||| guozhong feng ||| chunguang he ||| 
2020 ||| effect of temperature on space charge distribution in two layers of transformer oil and impregnated pressboard under dc voltage. ||| jinfeng zhang ||| minghe chi ||| qingguo chen ||| wenxin sun ||| jinming cao ||| 
2022 ||| vehicle re-identification based on global relational attention and multi-granularity feature learning. ||| xin tian ||| xiyu pang ||| gangwu jiang ||| qinglan meng ||| yanli zheng ||| 
2017 ||| towards automatic real-time estimation of observed learner's attention using psychophysiological and affective signals: the touch-typing study case. ||| marko meza ||| janja kosir ||| gregor strle ||| andrej kosir ||| 
2018 ||| full-bridge llc resonant converter with series-parallel connected transformers for electric vehicle on-board charger. ||| yanxia shen ||| wenhui zhao ||| zhe chen ||| chengchao cai ||| 
2021 ||| using lstm neural network based on improved pso and attention mechanism for predicting the effluent cod in a wastewater treatment plant. ||| xin liu ||| qiming shi ||| zhen liu ||| jia yuan ||| 
2019 ||| visual attention guided pixel-wise just noticeable difference model. ||| zhipeng zeng ||| huanqiang zeng ||| jing chen ||| jianqing zhu ||| yun zhang ||| kai-kuang ma ||| 
2022 ||| pulmonary nodule detection using 3-d residual u-net oriented context-guided attention and multi-branch classification network. ||| haiying yuan ||| yanrui wu ||| junpeng cheng ||| zhongwei fan ||| zhiyong zeng ||| 
2020 ||| a new modular multilevel ac/ac converter using hf transformer. ||| yongqing meng ||| ying kong ||| ziyue duan ||| xiuli wang ||| xifan wang ||| ruanming huang ||| rong ye ||| 
2021 ||| a modified generative adversarial network using spatial and channel-wise attention for cs-mri reconstruction. ||| guangyuan li ||| jun lv ||| chengyan wang ||| 
2019 ||| ahcnet: an application of attention mechanism and hybrid connection for liver tumor segmentation in ct volumes. ||| huiyan jiang ||| tianyu shi ||| zhiqi bai ||| liangliang huang ||| 
2019 ||| natural answer generation with attention over instances. ||| mengxi wei ||| yang zhang ||| 
2018 ||| enhancing pv penetration in lv networks using reactive power control and on load tap changer with existing transformers. ||| tariq aziz ||| nipon ketjoy ||| 
2021 ||| mixed high-order non-local attention network for single image super-resolution. ||| xiaobiao du ||| sai biao jiang ||| yujuan si ||| lina xu ||| chongjin liu ||| 
2019 ||| complex impedance transformers based on allowed and forbidden regions. ||| hee-ran ahn ||| manos m. tentzeris ||| 
2019 ||| numerical and experimental investigation of temperature distribution for oil-immersed transformer winding based on dimensionless least-squares and upwind finite element method. ||| gang liu ||| zhi zheng ||| xun ma ||| shichang rong ||| weige wu ||| lin li ||| 
2021 ||| deep neural networks using residual fast-slow refined highway and global atomic spatial attention for action recognition and detection. ||| manh-hung ha ||| oscal tzyh-chiang chen ||| 
2021 ||| classification of remote sensing images using efficientnet-b3 cnn model with attention. ||| haikel salem alhichri ||| asma s. alswayed ||| yakoub bazi ||| nassim ammour ||| naif al ajlan ||| 
2019 ||| design and analysis of pwm inverter for 100kva solid state transformer in a distribution system. ||| abdur rehman ||| muhammad ashraf ||| 
2020 ||| stability assessment of voltage control strategies for smart transformer-fed distribution grid. ||| zhi-xiang zou ||| marco liserre ||| zheng wang ||| ming cheng ||| 
2019 ||| diagnosing transformer winding deformation faults based on the analysis of binary image obtained from fra signature. ||| zhongyong zhao ||| chenguo yao ||| chao tang ||| chengxiang li ||| fayou yan ||| syed mofizul islam ||| 
2019 ||| isolation transformer for 3-port 3-phase dual-active bridge converters in medium voltage level. ||| seunghun baek ||| subhashish bhattacharya ||| 
2020 ||| a single-phase line-interactive ups system for transformer-coupled loading conditions. ||| syed sabir hussain bukhari ||| jong-suk ro ||| 
2019 ||| energy harvesting-based smart transportation mode detection system via attention-based lstm. ||| weitao xu ||| xingyu feng ||| jia wang ||| chengwen luo ||| jianqiang li ||| zhong ming ||| 
2020 ||| attention-based pose sequence machine for 3d hand pose estimation. ||| fangtai guo ||| zaixing he ||| shuyou zhang ||| xinyue zhao ||| jianrong tan ||| 
2021 ||| wide-range digital-analog mixed calibration technology of a dc instrument transformer test set. ||| haoliang hu ||| qi nie ||| xiong wu ||| fuchang lin ||| feng zhou ||| xiaofei li ||| jicheng yu ||| 
2020 ||| a novel curve database for moisture evaluation of transformer oil-immersed cellulose insulation using fds and exponential decay model. ||| jiefeng liu ||| zixiao wang ||| xianhao fan ||| yiyi zhang ||| jiaqi wang ||| 
2019 ||| unsupervised object-level image-to-image translation using positional attention bi-flow generative network. ||| liuchun yuan ||| dihu chen ||| haifeng hu ||| 
2018 ||| dc-transformer modelling, analysis and comparison of the experimental investigation of a non-inverting and non-isolated nx multilevel boost converter (nx mbc) for low to high dc voltage applications. ||| atif iqbal ||| mahajan sagar bhaskar ||| mohammad meraj ||| sanjeevikumar padmanaban ||| 
2021 ||| a new transformerless ultra high gain dc-dc converter for dc microgrid application. ||| shahrukh khan ||| mohammad zaid ||| arshad mahmood ||| abbas syed nooruddin ||| javed ahmad ||| mamdouh l. alghaythi ||| basem alamri ||| mohd tariq ||| adil sarwar ||| chang-hua lin ||| 
2021 ||| document-level neural tts using curriculum learning and attention masking. ||| sung-woong hwang ||| joon-hyuk chang ||| 
2021 ||| ladnet: an ultra-lightweight and efficient dilated residual network with light-attention module. ||| junyan yang ||| jie jiang ||| yujie fang ||| jiahao sun ||| 
2019 ||| a session-based customer preference learning method by using the gated recurrent units with attention function. ||| jenhui chen ||| ashu abdul ||| 
2020 ||| gate: graph-attention augmented temporal neural network for medication recommendation. ||| chenhao su ||| sheng gao ||| si li ||| 
2021 ||| atpgnn: reconstruction of neighborhood in graph neural networks with attention-based topological patterns. ||| kehao wang ||| hantao qian ||| xuming zeng ||| mozi chen ||| kezhong liu ||| kai zheng ||| pan zhou ||| dapeng wu ||| 
2019 ||| weighted optical flow prediction and attention model for object tracking. ||| wenming cao ||| yuhong li ||| zhiquan he ||| 
2020 ||| a partitioning-stacking prediction fusion network based on an improved attention u-net for stroke lesion segmentation. ||| haisheng hui ||| xueying zhang ||| fenglian li ||| xiaobi mei ||| yuling guo ||| 
2020 ||| web application attack detection based on attention and gated convolution networks. ||| jiancong li ||| yusheng fu ||| jie xu ||| chunhui ren ||| xin xiang ||| jinhong guo ||| 
2020 ||| design of high efficiency controller for wide input range dc-dc piezoelectric transformer converter. ||| seok-teak yun ||| seung-hyun kong ||| 
2019 ||| attentional pattern classification for automatic dementia detection. ||| maria teresa angelillo ||| fabrizio balducci ||| donato impedovo ||| giuseppe pirlo ||| gennaro vessio ||| 
2020 ||| multiple-aspect attentional graph neural networks for online social network user localization. ||| ting zhong ||| tianliang wang ||| jiahao wang ||| jin wu ||| fan zhou ||| 
2021 ||| multi-attention ghost residual fusion network for image classification. ||| xiaofen jia ||| shengjie du ||| yongcun guo ||| yourui huang ||| baiting zhao ||| 
2020 ||| a text normalization method for speech synthesis based on local attention mechanism. ||| lan huang ||| shunan zhuang ||| kangping wang ||| 
2021 ||| dielectric response of the oil-paper insulation system in nanofluid-based transformers. ||| daniel p ||| rez-rosa ||| bel ||| n garc ||| a ||| juan carlos burgos ||| 
2020 ||| a calculation method to adjust the short-circuit impedance of a transformer. ||| zhijun ye ||| wang yu ||| julong gou ||| kaijia tan ||| wenhui zeng ||| bonan an ||| yong li ||| 
2022 ||| attention retrieval model for entity relation extraction from biological literature. ||| prashant srivastava ||| saptarshi bej ||| kristian schultz ||| kristina y. yordanova ||| olaf wolkenhauer ||| 
2020 ||| sarcasm detection using multi-head attention based bidirectional lstm. ||| avinash kumar ||| vishnu teja narapareddy ||| veerubhotla aditya srikanth ||| aruna malapati ||| lalita bhanu murthy neti ||| 
2019 ||| interpretability analysis of heartbeat classification based on heartbeat activity's global sequence features and bilstm-attention neural network. ||| runchuan li ||| xingjin zhang ||| honghua dai ||| bing zhou ||| zongmin wang ||| 
2020 ||| adecnn: an improved model for aspect-level sentiment analysis based on deformable cnn and attention. ||| jie zhou ||| siqi jin ||| xinli huang ||| 
2021 ||| an alternating training method of attention-based adapters for visual explanation of multi-domain satellite images. ||| heejae kim ||| kyung-chae lee ||| changha lee ||| sanghyun hwang ||| chan-hyun youn ||| 
2021 ||| marn: multi-scale attention retinex network for low-light image enhancement. ||| xin zhang ||| xia wang ||| 
2020 ||| hagn: hierarchical attention guided network for crowd counting. ||| zuodong duan ||| yujun xie ||| jiahao deng ||| 
2020 ||| common-mode stability test and design guidelines for a transformer-based push-pull power amplifier. ||| van-son trinh ||| jung-dong park ||| 
2021 ||| transformer oil diagnosis based on a capacitive sensor frequency response analysis. ||| jos |||  manuel guerrero ||| alejandro e. castilla ||| jos |||   ||| ngel s ||| nchez-fern ||| ndez ||| carlos a. platero ||| 
2018 ||| lexicon-enhanced lstm with attention for general sentiment analysis. ||| xianghua fu ||| jingying yang ||| jianqiang li ||| min fang ||| huihui wang ||| 
2021 ||| three-phase transformer inrush current reduction strategy based on prefluxing and controlled switching. ||| yuanlin pan ||| xianggen yin ||| zhe zhang ||| binyan liu ||| maolin wang ||| xin yin ||| 
2021 ||| a two-stage multiscale residual attention network for light guide plate defect detection. ||| zhaopan li ||| junfeng li ||| wenzhan dai ||| 
2020 ||| a location-velocity-temporal attention lstm model for pedestrian trajectory prediction. ||| hao xue ||| du q. huynh ||| mark reynolds ||| 
2019 ||| ms-pointer network: abstractive text summary based on multi-head self-attention. ||| qian guo ||| jifeng huang ||| naixue xiong ||| pan wang ||| 
2021 ||| pamsgan: pyramid attention mechanism-oriented symmetry generative adversarial network for motion image deblurring. ||| zhenfeng zhang ||| 
2021 ||| multi-gate attention network for image captioning. ||| weitao jiang ||| xiying li ||| haifeng hu ||| qiang lu ||| bohong liu ||| 
2020 ||| attention-based siamese region proposals network for visual tracking. ||| fan wang ||| bo yang ||| jingting li ||| xiaopeng hu ||| zhihang ji ||| 
2021 ||| lattegan: visually guided language attention for multi-turn text-conditioned image manipulation. ||| shoya matsumori ||| yuki abe ||| kosuke shingyouchi ||| komei sugiura ||| michita imai ||| 
2020 ||| hybrid grey wolf optimizer for transformer fault diagnosis using dissolved gases considering uncertainty in measurements. ||| ayman hoballah ||| diaa-eldin a. mansour ||| ibrahim b. m. taha ||| 
2020 ||| the dynamical interplay of collective attention, awareness and epidemics spreading in the multiplex social networks during covid-19. ||| marialisa scat ||| barbara attanasio ||| grazia veronica aiosa ||| aurelio la corte ||| 
2019 ||| modeling and analyzing the influence of multi-information coexistence on attention. ||| nan zhao ||| siwei fang ||| nan chen ||| changxing pei ||| 
2019 ||| attention-based dense point cloud reconstruction from a single image. ||| qiang lu ||| mingjie xiao ||| yiyang lu ||| xiaohui yuan ||| ye yu ||| 
2021 ||| naem: noisy attention exploration module for deep reinforcement learning. ||| zhenwen cai ||| feifei lee ||| chunyan hu ||| koji kotani ||| qiu chen ||| 
2019 ||| second-order response transform attention network for image classification. ||| jianxin zhang ||| jiahua wang ||| qiule sun ||| cunhua li ||| bin liu ||| qiang zhang ||| xiaopeng wei ||| 
2021 ||| transformerless high step-up dc-dc converter with low voltage stress for fuel cells. ||| jiawei zhao ||| daolian chen ||| jiahui jiang ||| 
2022 ||| agnet: attention guided sparse depth completion using convolutional neural networks. ||| xiaolong liang ||| cheolkon jung ||| 
2020 ||| improved self-organizing map clustering of power transformer dissolved gas analysis using inputs pre-processing. ||| syahiduddin misbahulmunir ||| vigna k. ramachandaramurthy ||| yasmin hanum md. thayoob ||| 
2019 ||| assessment of hydraulic network models in predicting reverse flows in od cooled disc type transformer windings. ||| xiang zhang ||| zhongdong wang ||| 
2019 ||| real-time crop recognition in transplanted fields with prominent weed growth: a visual-attention-based approach. ||| nan li ||| xiaoguang zhang ||| chunlong zhang ||| huiwen guo ||| zhe sun ||| xinyu wu ||| 
2020 ||| attention-based lstm network for rotatory machine remaining useful life prediction. ||| hao zhang ||| qiang zhang ||| siyu shao ||| tianlin niu ||| xinyu yang ||| 
2019 ||| a semi-supervised synthetic aperture radar (sar) image recognition algorithm based on an attention mechanism and bias-variance decomposition. ||| fei gao ||| wei shi ||| jun wang ||| amir hussain ||| huiyu zhou ||| 
2021 ||| improved attention mechanism and residual network for remote sensing image scene classification. ||| jiayuan kong ||| yurong gao ||| yanjun zhang ||| huimin lei ||| yao wang ||| hesheng zhang ||| 
2020 ||| tser: a two-stage character segmentation network with two-stream attention and edge refinement. ||| jinyingming zhang ||| jin liu ||| xiongwei xu ||| peizhu gong ||| mingyang duan ||| 
2022 ||| an abnormal traffic detection model combined biindrnn with global attention. ||| huang li ||| hongjuan ge ||| haoqi yang ||| jie yan ||| yiqin sang ||| 
2021 ||| remaining useful life estimation combining two-step maximal information coefficient and temporal convolutional network with attention mechanism. ||| yalan jiang ||| chaoshun li ||| zhixin yang ||| yujie zhao ||| xianbo wang ||| 
2020 ||| the effects of visual stimuli on attention in children with autism spectrum disorder: an eye-tracking study. ||| bilikis banire ||| dena al-thani ||| marwa k. qaraqe ||| kamran khowaja ||| bilal mansoor ||| 
2020 ||| supersaliency: a novel pipeline for predicting smooth pursuit-based attention improves generalisability of video saliency. ||| mikhail startsev ||| michael dorr ||| 
2021 ||| a method of steel bar image segmentation based on multi-attention u-net. ||| jie shi ||| kunpeng wu ||| chaolin yang ||| nenghui deng ||| 
2020 ||| automatic modulation classification scheme based on lstm with random erasing and attention mechanism. ||| yufan chen ||| wei shao ||| jin liu ||| lu yu ||| zuping qian ||| 
2020 ||| multi-scale feature channel attention generative adversarial network for face sketch synthesis. ||| jieying zheng ||| yahong wu ||| wanru song ||| ran xu ||| feng liu ||| 
2020 ||| low light image enhancement network with attention mechanism and retinex model. ||| wei huang ||| yifeng zhu ||| rui huang ||| 
2021 ||| an efficient approach with application of linear and nonlinear models for evaluation of power transformer health index. ||| hamed zeinoddini meymand ||| salah kamel ||| baseem khan ||| 
2020 ||| real-time ultrasound image despeckling using mixed-attention mechanism based residual unet. ||| yancheng lan ||| xuming zhang ||| 
2020 ||| col-gan: plausible and collision-less trajectory prediction by attention-based gan. ||| shaohua liu ||| haibo liu ||| huikun bi ||| tianlu mao ||| 
2021 ||| abnormal detection of electricity consumption of user based on particle swarm optimization and long short term memory with the attention mechanism. ||| jiahao bian ||| lei wang ||| rafal scherer ||| marcin wozniak ||| pengchao zhang ||| wei wei ||| 
2019 ||| i-vals: visual attention localization for mobile service computing. ||| zhiping jiang ||| kun zhao ||| rui li ||| jizhong zhao ||| 
2020 ||| convolutional-neural-network-based partial discharge diagnosis for power transformer using uhf sensor. ||| the-duong do ||| vo-nguyen tuyet-doan ||| yong-sung cho ||| jong-ho sun ||| yong-hwa kim ||| 
2020 ||| towards a deep attention-based sequential recommender system. ||| shahpar yakhchi ||| amin beheshti ||| seyed mohssen ghafari ||| mehmet a. orgun ||| guanfeng liu ||| 
2019 ||| dielectric and thermal performance up-gradation of transformer oil using valuable nano-particles. ||| m. a. abid ||| ilyas khan ||| zahid ullah ||| kaleem ullah ||| a. haider ||| sahibzada muhammad ali ||| 
2019 ||| a multi-layer dual attention deep learning model with refined word embeddings for aspect-based sentiment analysis. ||| syeda rida-e.-fatima ||| ali javed ||| ameen banjar ||| aun irtaza ||| hassan dawood ||| hussain dawood ||| abdullah alamri ||| 
2019 ||| attention dense-u-net for automatic breast mass segmentation in digital mammogram. ||| shuyi li ||| min dong ||| guangming du ||| xiaomin mu ||| 
2022 ||| a dual-staged attention based conversion-gated long short term memory for multivariable time series prediction. ||| shufang feng ||| yong feng ||| 
2020 ||| single image reflection removal via attention model and sn-gan. ||| kuanhong cheng ||| jiangluqi song ||| juan du ||| shenghui rong ||| huixin zhou ||| 
2018 ||| development of power electronic distribution transformer based on adaptive pi controller. ||| khalid y. ahmed ||| nor zaihar yahaya ||| vijanth sagayan asirvadam ||| nordin bin saad ||| ramani kannan ||| oladimeji ibrahim ||| 
2021 ||| an efficient video coding system with an adaptive overfitted multi-scale attention network. ||| gang he ||| chang wu ||| li xu ||| lei li ||| ziyao xu ||| weiying xie ||| yunsong li ||| 
2019 ||| wide or narrow? a visual attention inspired model for view-type classification. ||| song tong ||| yuen peng loh ||| xuefeng liang ||| takatsune kumada ||| 
2020 ||| influence of the load on the impulse frequency response approach based diagnosis of transformer's inter-turn short-circuit. ||| natarajan shanmugam ||| balasubramanian madanmohan ||| rajesh rajamani ||| 
2020 ||| deep learning for load forecasting: sequence to sequence recurrent neural networks with attention. ||| ljubisa sehovac ||| katarina grolinger ||| 
2022 ||| efficient lightweight attention network for face recognition. ||| peng zhang ||| feng zhao ||| peng liu ||| mengwei li ||| 
2019 ||| a novel deep recurrent belief network model for trend prediction of transformer dga data. ||| bo qi ||| yiming wang ||| peng zhang ||| chengrong li ||| hongbin wang ||| 
2021 ||| multi-attention generative adversarial network for multivariate time series prediction. ||| xiang yin ||| yanni han ||| hongyu sun ||| zhen xu ||| haibo yu ||| xiaoyu duan ||| 
2021 ||| study on how expert and novice pilots can distribute their visual attention to improve flight performance. ||| huibin jin ||| zhanyao hu ||| kun li ||| mingjian chu ||| guoliang zou ||| guihua yu ||| jianlei zhang ||| 
2020 ||| a novel channel and temporal-wise attention in convolutional networks for multivariate time series classification. ||| xu cheng ||| peihua han ||| guoyuan li ||| shengyong chen ||| houxiang zhang ||| 
2021 ||| crowd density estimation by using attention based capsule network and multi-column cnn. ||| merve ayyuce kizrak ||| b ||| lent bolat ||| 
2020 ||| next basket recommendation model based on attribute-aware multi-level attention. ||| tong liu ||| xianrui yin ||| weijian ni ||| 
2021 ||| scep - a new image dimensional emotion recognition model based on spatial and channel-wise attention mechanisms. ||| bo li ||| hui ren ||| xuekun jiang ||| fang miao ||| feng feng ||| libiao jin ||| 
2021 ||| attention-based design and user decisions on information sharing: a thematic literature review. ||| zaid amin ||| nazlena mohamad ali ||| alan f. smeaton ||| 
2020 ||| attention-based adaptive memory network for recommendation with review and rating. ||| wei liu ||| zhiping lin ||| huaijie zhu ||| jing wang ||| arun kumar sangaiah ||| 
2021 ||| a serial-parallel self-attention network joint with multi-scale dilated convolution. ||| gaihua wang ||| tianlun zhang ||| dai yingying ||| lin jinheng ||| cheng lei ||| 
2021 ||| a method for fans' potential malfunction detection of onaf transformer using top-oil temperature monitoring. ||| lujia wang ||| wanwan zuo ||| zhi-xin yang ||| jianwen zhang ||| zhenlu cai ||| 
2022 ||| transformer network for remaining useful life prediction of lithium-ion batteries. ||| daoquan chen ||| weicong hong ||| xiuze zhou ||| 
2020 ||| saliency guided self-attention network for weakly and semi-supervised semantic segmentation. ||| qi yao ||| xiaojin gong ||| 
2020 ||| classification of selective attention within steady-state somatosensory evoked potentials from dry electrodes using mutual information-based spatio-spectral feature selection. ||| keun-tae kim ||| jaehyung lee ||| hyungmin kim ||| choong hyun kim ||| song joo lee ||| 
2020 ||| a novel analysis approach for dual-frequency parallel transmission-line transformer with complex terminal loads. ||| taiyang xie ||| xiaolong wang ||| zhewang ma ||| chun-ping chen ||| geyu lu ||| 
2019 ||| sfa: small faces attention face detector. ||| shi luo ||| xiongfei li ||| rui zhu ||| xiaoli zhang ||| 
2020 ||| an efficient procedure for temperature calculation of high current leads in large power transformers. ||| wilerson v. calil ||| pablo daniel paz salazar ||| andr |||  souza de melo ||| eduardo coelho marques costa ||| 
2019 ||| impact of load ramping on power transformer dissolved gas analysis. ||| huize cui ||| liuqing yang ||| shengtao li ||| guanghao qu ||| hao wang ||| ahmed abu-siada ||| syed mofizul islam ||| 
2018 ||| transformer fault condition prognosis using vibration signals over cloud environment. ||| mehdi bagheri ||| amin zollanvari ||| svyatoslav nezhivenko ||| 
2020 ||| rrgccan: re-ranking via graph convolution channel attention network for person re-identification. ||| xiaoqiang chen ||| ling zheng ||| chong zhao ||| qicong wang ||| maozhen li ||| 
2019 ||| k-reciprocal harmonious attention network for video-based person re-identification. ||| xinxing su ||| xiaoye qu ||| zhikang zou ||| pan zhou ||| wei wei ||| shiping wen ||| menglan hu ||| 
2021 |||  arrangement transformer under single-line-to-ground fault. ||| xiaohe wang ||| xu cai ||| qing chen ||| bin lin ||| rui xie ||| 
2021 ||| parameter-free attention in fmri decoding. ||| yong qi ||| huawei lin ||| yanping li ||| jiashu chen ||| 
2019 ||| multi-task learning for authorship attribution via topic approximation and competitive attention. ||| wei song ||| chen zhao ||| lizhen liu ||| 
2020 ||| image restoration via deep memory-based latent attention network. ||| xinyan zhang ||| peng gao ||| kongya zhao ||| sunxiangyu liu ||| guitao li ||| liuguo yin ||| 
2022 ||| improved yolov4 based on attention mechanism for ship detection in sar images. ||| yunlong gao ||| zhiyong wu ||| ming ren ||| chuan wu ||| 
2019 ||| hierarchical attention and knowledge matching networks with information enhancement for end-to-end task-oriented dialog systems. ||| junqing he ||| bing wang ||| mingming fu ||| tianqi yang ||| xuemin zhao ||| 
2019 ||| short text understanding combining text conceptualization and transformer embedding. ||| jun li ||| guimin huang ||| jianheng chen ||| yabing wang ||| 
2020 ||| fret: functional reinforced transformer with bert for code summarization. ||| ruyun wang ||| hanwen zhang ||| guoliang lu ||| lei lyu ||| chen lyu ||| 
2020 ||| compilation optimization pass selection using gate graph attention neural network for reliability improvement. ||| jiang wu ||| jianjun xu ||| xiankai meng ||| haoyu zhang ||| zhuo zhang ||| long li ||| 
2021 ||| chord conditioned melody generation with transformer based decoders. ||| kyoyun choi ||| jonggwon park ||| wan heo ||| sungwook jeon ||| jonghun park ||| 
2021 ||| biomedical text similarity evaluation using attention mechanism and siamese neural network. ||| zhengguang li ||| heng chen ||| huayue chen ||| 
2020 ||| pran: progressive residual attention network for super resolution. ||| jupeng shi ||| jing li ||| yan chen ||| zhengjia lu ||| 
2021 ||| attention to wi-fi diversity: resource management in wlans with heterogeneous aps. ||| jose saldana ||| jos |||  ru ||| z-mas ||| juli ||| n fern ||| ndez-navajas ||| jos |||  luis salazar ||| jean-philippe javaudin ||| jean-michel bonnamy ||| ma ||| l le dizes ||| 
2022 ||| transformerless quadruple high step-up dc/dc converter using coupled inductors. ||| sang-wha seo ||| joonhyoung ryu ||| yong kim ||| han ho choi ||| 
2019 ||| top-n-targets-balanced recommendation based on attentional sequence-to-sequence learning. ||| xingkai wang ||| yiqiang sheng ||| haojiang deng ||| zhenyu zhao ||| 
2019 ||| sarcasm detection using soft attention-based bidirectional long short-term memory model with convolution network. ||| le hoang son ||| akshi kumar ||| saurabh raj sangwan ||| anshika arora ||| anand nayyar ||| mohamed abdel-basset ||| 
2022 ||| linear arrhenius-weibull model for power transformer thermal stress assessment. ||| milad soleimani ||| mladen kezunovic ||| sergiy butenko ||| 
2021 ||| attention meets perturbations: robust and interpretable attention with adversarial training. ||| shunsuke kitada ||| hitoshi iyatomi ||| 
2020 ||| hovering control of submersible transformer inspection robot based on asmbc method. ||| yingbin feng ||| yanju liu ||| hongwei gao ||| zhaojie ju ||| 
2019 ||| syntax-directed hybrid attention network for aspect-level sentiment analysis. ||| xinyi wang ||| guangluan xu ||| jingyuan zhang ||| xian sun ||| lei wang ||| tinglei huang ||| 
2022 ||| simple and effective multimodal learning based on pre-trained transformer models. ||| kazuki miyazawa ||| yuta kyuragi ||| takayuki nagai ||| 
2020 ||| real-time speech enhancement algorithm based on attention lstm. ||| ruiyu liang ||| fanliu kong ||| yue xie ||| guichen tang ||| jiaming cheng ||| 
2022 ||| attention-based applications in extended reality to support autistic users: a systematic review. ||| katherine wang ||| simon j. julier ||| youngjun cho ||| 
2021 ||| multi-perspective attention network for fast temporal moment localization. ||| jungkyoo shin ||| jinyoung moon ||| 
2018 ||| variation of discharge characteristics with temperature in moving transformer oil contaminated by metallic particles. ||| cheng pan ||| ju tang ||| yongze zhang ||| xinyu luo ||| xingxing li ||| 
2020 ||| lightweight single image super-resolution with multi-scale spatial attention networks. ||| jae woong soh ||| nam ik cho ||| 
2019 ||| electric field distribution characteristics and space charge motion process in transformer oil under impulse voltage. ||| bo qi ||| chunjia gao ||| hao han ||| xiaolin zhao ||| qing yuan ||| shuqi zhang ||| chengrong li ||| 
2020 ||| exploring multi-level attention and semantic relationship for remote sensing image captioning. ||| zhenghang yuan ||| xuelong li ||| qi wang ||| 
2020 ||| robot-assisted joint attention: a comparative study between children with autism spectrum disorder and typically developing children in interaction with nao. ||| hoang-long cao ||| ramona simut ||| naomi desmet ||| albert de beir ||| greet van de perre ||| bram vanderborght ||| johan vanderfaeillie ||| 
2018 ||| improved method to obtain the online impulse frequency response signature of a power transformer by multi scale complex cwt. ||| zhongyong zhao ||| chao tang ||| chenguo yao ||| qu zhou ||| lingna xu ||| yingang gui ||| syed mofizul islam ||| 
2021 ||| a 25.1 dbm 25.9-db gain 25.4% pae x-band power amplifier utilizing voltage combining transformer in 65-nm cmos. ||| van-son trinh ||| jung-dong park ||| 
2020 ||| forecasting stock prices using a hybrid deep learning model integrating attention mechanism, multi-layer perceptron, and bidirectional long-short term memory neural network. ||| qian chen ||| wenyu zhang ||| yu lou ||| 
2018 ||| semi-empirical model for precise analysis of copper losses in high-frequency transformers. ||| bin chen ||| lin li ||| 
2019 ||| an attention enhanced bidirectional lstm for early forest fire smoke recognition. ||| yichao cao ||| feng yang ||| qingfei tang ||| xiaobo lu ||| 
2020 ||| zero-shot visual recognition via semantic attention-based compare network. ||| fudong nian ||| yikun sheng ||| junfeng wang ||| teng li ||| 
2019 ||| an attention-based bilstm-crf model for chinese clinic named entity recognition. ||| guohua wu ||| guangen tang ||| zhongru wang ||| zhen zhang ||| zhen wang ||| 
2019 ||| a memory term reduction approach for digital pre-distortion using the attention mechanism. ||| huihui yu ||| gaoming xu ||| taijun liu ||| jifu huang ||| xiupu zhang ||| 
2020 ||| aafm: adaptive attention fusion mechanism for crowd counting. ||| zuodong duan ||| huimin chen ||| jiahao deng ||| 
2020 ||| hot-spot temperature forecasting of the instrument transformer using an artificial neural network. ||| edgar alfredo juarez-balderas ||| joselito medina mar ||| n ||| juan carlos olivares-galvan ||| norberto hernandez-romero ||| juan carlos seck tuoh mora ||| alejandro rodriguez-aguilar ||| 
2020 ||| an efficient and accurate ddpg-based recurrent attention model for object localization. ||| fengkai ke ||| 
2018 ||| determination of core losses in open-core power voltage transformers. ||| igor ziger ||| bojan trkulja ||| zeljko stih ||| 
2021 ||| esophageal tumor segmentation in ct images using a dilated dense attention unet (ddaunet). ||| sahar yousefi ||| hessam sokooti ||| mohamed s. elmahdy ||| irene m. lips ||| mohammad t. manzuri shalmani ||| roel t. zinkstok ||| frank j. w. m. dankers ||| marius staring ||| 
2019 ||| deep attention neural network for multi-label classification in unmanned aerial vehicle imagery. ||| aaliyah alshehri ||| yakoub bazi ||| nassim ammour ||| haidar almubarak ||| naif alajlan ||| 
2022 ||| satsal: a multi-level self-attention based architecture for visual saliency prediction. ||| marouane tliba ||| mohamed amine kerkouri ||| bashir ghariba ||| aladine chetouani ||| arzu  ||| ltekin ||| mohamed s. shehata ||| alessandro bruno ||| 
2019 ||| distant supervision for relation extraction via piecewise attention and bag-level contextual inference. ||| van-thuy phi ||| joan santoso ||| van-hien tran ||| hiroyuki shindo ||| masashi shimbo ||| yuji matsumoto ||| 
2021 ||| appearance guidance attention for multi-object tracking. ||| yong chen ||| junjie huang ||| huanlin liu ||| meiyong huang ||| zhibo zou ||| 
2021 ||| semi-attentionae: an integrated model for graph representation learning. ||| lining yuan ||| yang wang ||| xianggui cheng ||| zhao liu ||| 
2020 ||| a concrete dam deformation prediction method based on lstm with attention mechanism. ||| dashan yang ||| chongshi gu ||| yantao zhu ||| bo dai ||| kang zhang ||| zhiduan zhang ||| bo li ||| 
2020 ||| graph attention networks with local structure awareness for knowledge graph completion. ||| kexi ji ||| bei hui ||| guangchun luo ||| 
2020 ||| text summarization method based on double attention pointer network. ||| zhixin li ||| zhi peng ||| suqin tang ||| canlong zhang ||| huifang ma ||| 
2019 ||| attention-guided coarse-to-fine network for 2d face alignment in the wild. ||| xin liu ||| huabin wang ||| jian zhou ||| liang tao ||| 
2021 ||| multi-horizon electricity load and price forecasting using an interpretable multi-head self-attention and eemd-based framework. ||| muhammad furqan azam ||| muhammad shahzad younis ||| 
2019 ||| multi-attention and incorporating background information model for chest x-ray image report generation. ||| xin huang ||| fengqi yan ||| wei xu ||| maozhen li ||| 
2020 ||| human action performance using deep neuro-fuzzy recurrent attention model. ||| nihar bendre ||| nima ebadi ||| john j. prevost ||| peyman najafirad ||| 
2021 ||| polysemy needs attention: short-text topic discovery with global and multi-sense information. ||| heng-yang lu ||| jun yang ||| yi zhang ||| zuoyong li ||| 
2021 ||| constructing bi-order-transformer-crf with neural cosine similarity function for power metering entity recognition. ||| kaihong zheng ||| jingfeng yang ||| lukun zeng ||| qihang gong ||| sheng li ||| shangli zhou ||| 
2020 ||| hierarchical transformer encoder with structured representation for abstract reasoning. ||| jinwon an ||| sungzoon cho ||| 
2020 ||| channel transformer network. ||| fuping zhang ||| pengcheng zhao ||| jianming wei ||| 
2020 ||| novel fault location method for power systems based on attention mechanism and double structure gru neural network. ||| fan zhang ||| qunying liu ||| yilu liu ||| ning tong ||| shuheng chen ||| chang-hua zhang ||| 
2019 ||| multiple object tracking with attention to appearance, structure, motion and size. ||| hasith karunasekera ||| han wang ||| handuo zhang ||| 
2019 ||| a fault diagnosis model of power transformers based on dissolved gas analysis features selection and improved krill herd algorithm optimized support vector machine. ||| yiyi zhang ||| xin li ||| hanbo zheng ||| huilu yao ||| jiefeng liu ||| chaohai zhang ||| hongbo peng ||| jian jiao ||| 
2020 ||| group activity recognition by using effective multiple modality relation representation with temporal-spatial attention. ||| dezhong xu ||| heng fu ||| lifang wu ||| meng jian ||| dong wang ||| xu liu ||| 
2021 ||| named entity recognition in electric power metering domain based on attention mechanism. ||| kaihong zheng ||| lingyun sun ||| xin wang ||| shangli zhou ||| hanbin li ||| sheng li ||| lukun zeng ||| qihang gong ||| 
2020 ||| a hybrid bert model that incorporates label semantics via adjustive attention for multi-label text classification. ||| linkun cai ||| yu song ||| tao liu ||| kunli zhang ||| 
2020 ||| detection of 2fal furanic compound in transformer oil using optical spectroscopy method and verification using morse oscillation theory. ||| vimal angela thiviyanathan ||| pin jern ker ||| yang sing leong ||| md. zaini bin jamaluddin ||| looe hui mun ||| 
2021 ||| single-trial decoding of motion direction during visual attention from local field potential signals. ||| mohammad reza nazari ||| ali motie nasrabadi ||| mohammad reza daliri ||| 
2019 ||| a bi-attention adversarial network for prostate cancer segmentation. ||| guokai zhang ||| weigang wang ||| dinghao yang ||| jihao luo ||| pengcheng he ||| yongtong wang ||| ye luo ||| binghui zhao ||| jianwei lu ||| 
2020 ||| a discriminative dual-stream model with a novel sustained attention mechanism for skeleton-based human action recognition. ||| zhihong liang ||| xiaoshan shi ||| yanxin zhang ||| bo liu ||| bo liu ||| 
2019 ||| scale pyramid attention for single shot multibox detector. ||| jie hao ||| feng jiang ||| rufei zhang ||| xipeng lin ||| biao leng ||| guanglu song ||| 
2020 ||| probabilistic health index-based apparent age estimation for power transformers. ||| shuaibing li ||| guangning wu ||| haiying dong ||| lei yang ||| xiaofei zhen ||| 
2020 ||| tblc-rattention: a deep neural network model for recognizing the emotional tendency of chinese medical comment. ||| qibing jin ||| xingrong xue ||| wenjuan peng ||| wu cai ||| yuming zhang ||| ling zhang ||| 
2020 ||| visual attention, mental stress and gender: a study using physiological signals. ||| abdul momin ||| saheli bhattacharya ||| sudip sanyal ||| pavan chakraborty ||| 
2020 ||| attentional generative adversarial networks with representativeness and diversity for generating text to realistic image. ||| anjie tian ||| lu lu ||| 
2019 ||| transmission lines positive sequence parameters estimation and instrument transformers calibration based on pmu measurement error model. ||| chen wang ||| virgilio a. centeno ||| kevin d. jones ||| duotong yang ||| 
2021 ||| a transformer fault diagnosis method based on parameters optimization of hybrid kernel extreme learning machine. ||| jifang li ||| chen hai ||| zhen feng ||| genxu li ||| 
2020 ||| two-level progressive attention convolutional network for fine-grained image recognition. ||| hua wei ||| ming zhu ||| bo wang ||| jiarong wang ||| deyao sun ||| 
2021 ||| a dynamic spatial-temporal attention-based gru model with healthy features for state-of-health estimation of lithium-ion batteries. ||| shengmin cui ||| inwhee joe ||| 
2021 ||| fraud detection in online product review systems via heterogeneous graph transformer. ||| songkai tang ||| luhua jin ||| fan cheng ||| 
2020 ||| sentiment classification based on part-of-speech and self-attention mechanism. ||| kefei cheng ||| yanan yue ||| zhiwen song ||| 
2019 ||| influential factors and correction method of furfural content in transformer oil. ||| dawei feng ||| lijun yang ||| luowei zhou ||| ruijin liao ||| 
2021 ||| light field image super-resolution via mutual attention guidance. ||| zijian wang ||| yao lu ||| 
2019 ||| a deep neural network based on an attention mechanism for sar ship detection in multiscale and complex scenarios. ||| chen chen ||| chuan he ||| changhua hu ||| hong pei ||| licheng jiao ||| 
2020 ||| high accuracy individual identification model of crested ibis (nipponia nippon) based on autoencoder with self-attention. ||| jiang-jian xie ||| jun yang ||| chang-qing ding ||| wenbin li ||| 
2019 ||| the experimental and cfd research on the pressure reduction process of the double rotor hydraulic transformer. ||| jihai jiang ||| zhongxun liu ||| 
2021 ||| unpaired stain style transfer using invertible neural networks based on channel attention and long-range residual. ||| junlin lan ||| shaojin cai ||| yuyang xue ||| qinquan gao ||| min du ||| hejun zhang ||| zhida wu ||| yanglin deng ||| yuxiu huang ||| tong tong ||| gang chen ||| 
2019 ||| classifying transformer winding deformation fault types and degrees using fra based on support vector machine. ||| jiangnan liu ||| zhongyong zhao ||| chao tang ||| chenguo yao ||| chengxiang li ||| syed mofizul islam ||| 
2020 ||| experimental research on lightning disturbance characteristics of 10-kv fusion voltage transformer intelligent component acquisition port. ||| qi wang ||| zihan teng ||| fanwu chu ||| yue tong ||| junjun xiong ||| guoxiong ye ||| xiong wu ||| 
2020 ||| measuring time-sensitive and topic-specific influence in social networks with lstm and self-attention. ||| cheng zheng ||| qin zhang ||| guodong long ||| chengqi zhang ||| sean d. young ||| wei wang ||| 
2021 ||| generative adversarial networks for abnormal event detection in videos based on self-attention mechanism. ||| weichao zhang ||| guanjun wang ||| mengxing huang ||| hongyu wang ||| shaoping wen ||| 
2019 ||| stream-flow forecasting based on dynamic spatio-temporal attention. ||| jun feng ||| le yan ||| tingting hang ||| 
2019 ||| ultra-compact and wideband v(u)hf 3-db power dividers consisting of novel asymmetric impedance transformers. ||| hee-ran ahn ||| manos m. tentzeris ||| 
2019 ||| short-term load forecasting based on deep learning for end-user transformer subject to volatile electric heating loads. ||| qifang chen ||| mingchao xia ||| teng lu ||| xichen jiang ||| wenxia liu ||| qinfei sun ||| 
2021 ||| the impact of geomagnetically produced negative-sequence harmonics on power transformers. ||| ahmed abuhussein ||| mohammad ashraf hossain sadi ||| 
2021 ||| lightweight channel attention and multiscale feature fusion discrimination for remote sensing scene classification. ||| huiyao wan ||| jie chen ||| zhixiang huang ||| yun feng ||| zheng zhou ||| xiaoping liu ||| baidong yao ||| tao xu ||| 
2020 ||| a three-phase constant common-mode voltage inverter with triple voltage boost for transformerless photovoltaic system. ||| tan-tai tran ||| minh-khai nguyen ||| van-quang-binh ngo ||| huu-nhan nguyen ||| truong-duy duong ||| young-cheol lim ||| joon-ho choi ||| 
2020 ||| action recognition using attention-joints graph convolutional neural networks. ||| tasweer ahmad ||| huiyun mao ||| luojun lin ||| guozhi tang ||| 
2020 ||| transfer2depth: dual attention network with transfer learning for monocular depth estimation. ||| chia-hung yeh ||| yao-pao huang ||| chih-yang lin ||| chuan-yu chang ||| 
2021 ||| chicken image segmentation via multi-scale attention-based deep convolutional neural network. ||| wei li ||| yang xiao ||| xibin song ||| na lv ||| xinbo jiang ||| yan huang ||| jingliang peng ||| 
2019 ||| improved distant supervised model in tibetan relation extraction using elmo and attention. ||| yuan sun ||| like wang ||| chaofan chen ||| tianci xia ||| xiaobing zhao ||| 
2021 ||| few-shot scene classification with multi-attention deepemd network in remote sensing. ||| zhengwu yuan ||| wendong huang ||| lin li ||| xiaobo luo ||| 
2021 ||| spatial-temporal genetic-based attention networks for short-term photovoltaic power forecasting. ||| tao fan ||| tao sun ||| hu liu ||| xiangying xie ||| zhixiong na ||| 
2020 ||| an empirical study on the influence of internet attention on the performance of individual stocks in the securities market under the environment of big data. ||| youwei chen ||| pengwei zhao ||| ying liu ||| 
2021 ||| relatable clothing: soft-attention mechanism for detecting worn/unworn objects. ||| thomas truong ||| svetlana n. yanushkevich ||| 
2021 ||| accent for visible and infrared registration (avir): attention block for increasing patch matching rate through edge emphasis. ||| inho park ||| jongmin jeong ||| sungho kim ||| 
2020 ||| color transfer with salient features mapping via attention maps between images. ||| zong-sheng wu ||| ru xue ||| 
2019 ||| lstm-attention-embedding model-based day-ahead prediction of photovoltaic power output using bayesian optimization. ||| tongguang yang ||| bin li ||| qian xun ||| 
2019 ||| a novel power transformer condition monitoring system based on wide-band measurement of core earth signals and correlation analysis with multi-source data. ||| xuezhi ke ||| lisheng pang ||| xiang dong ||| zhaohui li ||| 
2018 ||| transformerless common-mode current-source inverter grid-connected for pv applications. ||| hector l ||| pez ||| juvenal rodr ||| guez-res ||| ndiz ||| xiaoqiang guo ||| nimrod v ||| zquez ||| roberto v. carrillo-serrano ||| 
2020 ||| attention guided encoder-decoder network with multi-scale context aggregation for land cover segmentation. ||| shuyang wang ||| xiaodong mu ||| dongfang yang ||| hao he ||| peng zhao ||| 
2019 ||| attention-based dual-source spatiotemporal neural network for lightning forecast. ||| tianyang lin ||| qingyong li ||| yangli-ao geng ||| lei jiang ||| liangtao xu ||| dong zheng ||| wen yao ||| weitao lyu ||| yijun zhang ||| 
2020 ||| bi-modal learning with channel-wise attention for multi-label image classification. ||| peng li ||| peng chen ||| yonghong xie ||| dezheng zhang ||| 
2019 ||| improving human pose estimation with self-attention generative adversarial networks. ||| xiangyang wang ||| zhongzheng cao ||| rui wang ||| zhi liu ||| xiaoqiang zhu ||| 
2020 ||| a novel hybrid spatial-temporal attention-lstm model for heat load prediction. ||| tao lin ||| yu pan ||| guixiang xue ||| jiancai song ||| chengying qi ||| 
2022 ||| multi-size object detection in large scene remote sensing images under dual attention mechanism. ||| jinkang wang ||| xiaohui he ||| faming shao ||| guanlin lu ||| qunyan jiang ||| ruizhe hu ||| 
2020 ||| fdta: fully convolutional scene text detection with text attention. ||| yongcun cao ||| shuaisen ma ||| haichuan pan ||| 
2019 ||| aesgru: an attention-based temporal correlation approach for end-to-end machine health perception. ||| weiting zhang ||| dong yang ||| hongchao wang ||| jun zhang ||| mikael gidlund ||| 
2021 ||| a novel dynamic attack on classical ciphers using an attention-based lstm encoder-decoder model. ||| ezat ahmadzadeh ||| hyunil kim ||| ongee jeong ||| inkyu moon ||| 
2020 ||| ma-net: a multi-scale attention network for liver and tumor segmentation. ||| tongle fan ||| guanglei wang ||| yan li ||| hongrui wang ||| 
2020 ||| the research on additional errors of voltage transformer connected in series. ||| huanghui zhang ||| haiming shao ||| jiafu wang ||| 
2021 ||| impact of optimal control of distributed generation converters in smart transformer based meshed hybrid distribution network. ||| chandan kumar ||| rampelli manojkumar ||| sanjib ganguly ||| marco liserre ||| 
2019 ||| phishing email detection using improved rcnn model with multilevel vectors and attention mechanism. ||| yong fang ||| cheng zhang ||| cheng huang ||| liang liu ||| yue yang ||| 
2019 ||| a packet-length-adjustable attention model based on bytes embedding using flow-wgan for smart cybersecurity. ||| luchao han ||| yiqiang sheng ||| xuewen zeng ||| 
2021 ||| optimal sizing of energy storage system to reduce impacts of transportation electrification on power distribution transformers integrated with photovoltaic. ||| pravakar pradhan ||| iftekhar ahmad ||| daryoush habibi ||| asma aziz ||| bassam al-hanahi ||| mohammad a. s. masoum ||| 
2020 ||| adversarial erasing attention for person re-identification in camera networks under complex environments. ||| shuang liu ||| xiaolong hao ||| ronghua zhang ||| zhong zhang ||| tariq s. durrani ||| 
2020 ||| text sentiment orientation analysis based on multi-channel cnn and bidirectional gru with attention mechanism. ||| yan cheng ||| leibo yao ||| guoxiong xiang ||| guanghe zhang ||| tianwei tang ||| linhui zhong ||| 
2020 ||| locally adaptive channel attention-based network for denoising images. ||| haeyun lee ||| sunghyun cho ||| 
2020 ||| adaptive modulation strategy for modular multilevel high-frequency dc transformer in dc distribution networks. ||| yu wang ||| zongyao wang ||| jingmin fan ||| yuanpeng guan ||| yunxiang xie ||| si-zhe chen ||| guidong zhang ||| yun zhang ||| 
2020 ||| real-time surgical tool detection in minimally invasive surgery based on attention-guided convolutional neural network. ||| pan shi ||| zijian zhao ||| sanyuan hu ||| faliang chang ||| 
2021 ||| transformers for clinical coding in spanish. ||| guillermo l ||| pez-garc ||| a ||| jos |||  m. jerez ||| nuria ribelles ||| emilio alba ||| francisco j. veredas ||| 
2020 ||| a graph attention model for dictionary-guided named entity recognition. ||| yinxia lou ||| tao qian ||| fei li ||| donghong ji ||| 
2019 ||| an emotion-embedded visual attention model for dimensional emotion context learning. ||| yuhao tang ||| qirong mao ||| hongjie jia ||| heping song ||| yongzhao zhan ||| 
2019 ||| motion based inference of social circles via self-attention and contextualized embedding. ||| ting zhong ||| fang liu ||| fan zhou ||| goce trajcevski ||| kunpeng zhang ||| 
2019 ||| identifying implicit polarity of events by using an attention-based neural network model. ||| chunli xiang ||| yafeng ren ||| donghong ji ||| 
2021 ||| efficient audio-visual speech enhancement using deep u-net with early fusion of audio and video information and rnn attention blocks. ||| jung-wook hwang ||| rae-hong park ||| hyung-min park ||| 
2022 ||| homogeneous learning: self-attention decentralized deep learning. ||| yuwei sun ||| hideya ochiai ||| 
2021 ||| optimal data selection rule mining for transformer condition assessment. ||| peng zhang ||| bo qi ||| mengyu shao ||| chengrong li ||| zhihai rong ||| jinxiang chen ||| hongbin wang ||| 
2020 ||| an efficient adaptive attention neural network for social recommendation. ||| munan li ||| kenji tei ||| yoshiaki fukazawa ||| 
2021 ||| temperature rise test and thermal-fluid coupling simulation of an oil-immersed autotransformer under dc bias. ||| mingyang li ||| zezhong wang ||| junshuang zhang ||| zhengze ni ||| ruijuan tan ||| 
2019 ||| single-phase common-ground-type transformerless pv grid-connected inverters. ||| 
2019 ||| temporal attention networks for multitemporal multisensor crop classification. ||| zhengtao li ||| guokun chen ||| tianxu zhang ||| 
2019 ||| quantitative formula of blink rates-pupillometry for attention level detection in supervised machine learning. ||| fadilla zennifa ||| keiji iramina ||| 
2019 ||| multilayer dense attention model for image caption. ||| eric ke wang ||| xun zhang ||| fan wang ||| tsu-yang wu ||| chien-ming chen ||| 
2019 |||  nanoparticles and water on transformer oil electrical performance. ||| chenran zhang ||| yu wang ||| zhongming yan ||| zhengyou he ||| 
2022 ||| household energy consumption prediction using the stationary wavelet transform and transformers. ||| lyes saad saoud ||| hasan al-marzouqi ||| ramy hussein ||| 
2020 ||| joint attention mechanisms for monocular depth estimation with multi-scale convolutions and adaptive weight adjustment. ||| peng liu ||| zonghua zhang ||| zhaozong meng ||| nan gao ||| 
2020 ||| multimodal emotion recognition with transformer-based self supervised feature fusion. ||| shamane siriwardhana ||| tharindu kaluarachchi ||| mark billinghurst ||| suranga nanayakkara ||| 
2021 ||| scale-aware transformers for diagnosing melanocytic lesions. ||| wenjun wu ||| sachin mehta ||| shima nofallah ||| stevan knezevich ||| caitlin j. may ||| oliver chang ||| joann g. elmore ||| linda g. shapiro ||| 
2019 ||| an economic operation analysis method of transformer based on clustering. ||| junde chen ||| defu zhang ||| yaser ahangari nanehkaran ||| 
2020 ||| hyperspectral band selection using attention-based convolutional neural networks. ||| pablo ribalta lorenzo ||| lukasz tulczyjew ||| michal marcinkiewicz ||| jakub nalepa ||| 
2021 ||| progressive guided fusion network with multi-modal and multi-scale attention for rgb-d salient object detection. ||| jiajia wu ||| guangliang han ||| haining wang ||| hang yang ||| qingqing li ||| dongxu liu ||| fangjian ye ||| peixun liu ||| 
2021 ||| domain adaptation deep attention network for automatic logo detection and recognition in google street view. ||| ervin yohannes ||| chih-yang lin ||| timothy k. shih ||| chen-ya hong ||| avirmed enkhbat ||| fitri utaminingrum ||| 
2021 ||| modeling and optimization of semantic segmentation for track bed foreign object based on attention mechanism. ||| haoran song ||| shengchun wang ||| zichen gu ||| peng dai ||| xinyu du ||| yu cheng ||| 
2020 ||| one-dimensional deep attention convolution network (odacn) for signals classification. ||| shuyuan yang ||| chen yang ||| dongzhu feng ||| xiaoyang hao ||| min wang ||| 
2019 ||| r-stan: residual spatial-temporal attention network for action recognition. ||| quanle liu ||| xiangjiu che ||| mei bie ||| 
2020 ||| combining multi-perspective attention mechanism with convolutional networks for monaural speech enhancement. ||| tian lan ||| yilan lyu ||| wenzheng ye ||| guoqiang hui ||| zenglin xu ||| qiao liu ||| 
2021 ||| escaping the gradient vanishing: periodic alternatives of softmax in attention mechanism. ||| shulun wang ||| feng liu ||| bin liu ||| 
2021 ||| overview and partial discharge analysis of power transformers: a literature review. ||| md rashid hussain ||| shady s. refaat ||| haitham abu-rub ||| 
2020 ||| investigating a new approach for moisture assessment of transformer insulation system. ||| tao zhang ||| shuo wang ||| chen zhang ||| ahmed abu-siada ||| linduo li ||| jianwei han ||| zhengbo du ||| 
2020 ||| single-phase grid-tied transformerless inverter of zero leakage current for pv system. ||| ahmed haroun sabry ||| zeina mueen mohammed ||| farah hani nordin ||| n. h. nik ali ||| ali saadon al-ogaili ||| 
2021 ||| poat-net: parallel offset-attention assisted transformer for 3d object detection for autonomous driving. ||| jinyang wang ||| xiao lin ||| hongying yu ||| 
2021 ||| analysis and design of an integrated magnetics planar transformer for high power density llc resonant converter. ||| chul wan park ||| sang-kyoo han ||| 
2022 ||| design and emulation of physics-centric cyberattacks on an electrical power transformer. ||| john olijnyk ||| benjamin bond ||| julian l. rrushi ||| 
2020 ||| using features specifically: an efficient network for scene segmentation based on dedicated attention mechanisms. ||| zhiqiang xiong ||| zhicheng wang ||| jie li ||| zhaohui yu ||| xi gu ||| 
2021 ||| attention mechanism cloud detection with modified fcn for infrared remote sensing images. ||| liyuan li ||| xiaoyan li ||| xin liu ||| wenwen huang ||| zhuoyue hu ||| fansheng chen ||| 
2022 ||| temperature segment compensation method of dissipation factor for insulation diagnosis in converter transformer bushing. ||| quanmin dai ||| yanxia liu ||| xin yu ||| yinoon zhang ||| huidong chen ||| zhiming huang ||| 
2020 ||| easa: entity alignment algorithm based on semantic aggregation and attribute attention. ||| li-an huang ||| xiangfeng luo ||| 
2019 ||| attention-based dual-scale cnn in-loop filter for versatile video coding. ||| ming-ze wang ||| shuai wan ||| hao gong ||| ming-yang ma ||| 
2022 ||| energy management strategy of ac/dc hybrid microgrid based on solid-state transformer. ||| zhengwei qu ||| zhe shi ||| yunjing wang ||| ahmed abu-siada ||| zhenxiao chong ||| haiyan dong ||| 
2020 ||| improved relativistic cycle-consistent gan with dilated residual network and multi-attention for speech enhancement. ||| yutian wang ||| guochen yu ||| jingling wang ||| hui wang ||| qin zhang ||| 
2021 ||| an accurate method for leakage inductance calculation of shell-type multi core-segment transformers with circular windings. ||| morteza eslamian ||| mohammad kharezy ||| torbj ||| rn thiringer ||| 
2019 ||| water hazard detection using conditional generative adversarial network with mixture reflection attention units. ||| li wang ||| huan wang ||| 
2019 ||| transformer based memory network for sentiment analysis of web comments. ||| ming jiang ||| junlei wu ||| xiangrong shi ||| min zhang ||| 
2020 ||| disease-pertinent knowledge extraction in online health communities using gru based on a double attention mechanism. ||| yanli zhang ||| xinmiao li ||| zhe zhang ||| 
2021 ||| comprehensive analysis of winding electromagnetic force and deformation during no-load closing and short-circuiting of power transformers. ||| chenchen zhang ||| wenqi ge ||| yi xie ||| yingying li ||| 
2019 ||| adding prior knowledge in hierarchical attention neural network for cross domain sentiment classification. ||| manshu tu ||| bing wang ||| 
2021 ||| point transformer. ||| nico engel ||| vasileios belagiannis ||| klaus dietmayer ||| 
2018 ||| a cascade coupled convolutional neural network guided visual attention method for ship detection from sar images. ||| juanping zhao ||| zenghui zhang ||| wenxian yu ||| trieu-kien truong ||| 
2019 ||| an adaptive multi-robot therapy for improving joint attention and imitation of asd children. ||| sara ali ||| faisal mehmood ||| darren dancey ||| yasar ayaz ||| muhammad jawad khan ||| noman naseer ||| rita de cassia amadeu ||| haleema sadia ||| raheel nawaz ||| 
2021 ||| pyramid co-attention compare network for few-shot segmentation. ||| defu zhang ||| ronghua luo ||| xuebin chen ||| lingwei chen ||| 
2021 ||| inter-dimensional correlations aggregated attention network for action recognition. ||| xiaochao li ||| jianhao zhan ||| man yang ||| 
2019 ||| multi-channel cnn based inner-attention for compound sentence relation classification. ||| kaili sun ||| yuan li ||| dunhua deng ||| yang li ||| 
2020 ||| multimodal attention network for continuous-time emotion recognition using video and eeg signals. ||| dong-yoon choi ||| deok-hwan kim ||| byung cheol song ||| 
2021 ||| hydro-dynamic model and low-speed stability analysis of hydraulic transformer. ||| qianqian bao ||| junjie zhou ||| chongbo jing ||| tianrui li ||| miaomiao wang ||| 
2020 ||| scene classification of remote sensing images based on saliency dual attention residual network. ||| dongen guo ||| ying xia ||| xiaobo luo ||| 
2021 ||| mvan: multi-view attention networks for fake news detection on social media. ||| shiwen ni ||| jiawen li ||| hung-yu kao ||| 
2021 ||| image-based scam detection method using an attention capsule network. ||| lingyu bian ||| linlin zhang ||| kai zhao ||| hao wang ||| shengjia gong ||| 
2020 ||| cross-lingual image caption generation based on visual attention model. ||| bin wang ||| cungang wang ||| qian zhang ||| ying su ||| yang wang ||| yanyan xu ||| 
2020 ||| normalization for fds of transformer insulation considering the synergistic effect generated by temperature and moisture. ||| xianhao fan ||| shichang yang ||| lai benhui ||| jiefeng liu ||| yiyi zhang ||| zixiao wang ||| 
2020 ||| parameter estimation of electric power transformers using coyote optimization algorithm with experimental verification. ||| mohamed i. abdelwanis ||| amlak abaza ||| ragab a. el-sehiemy ||| mohamed n. ibrahim ||| hegazy rezk ||| 
2021 ||| dgattgan: cooperative up-sampling based dual generator attentional gan on text-to-image synthesis. ||| han zhang ||| hongqing zhu ||| suyi yang ||| wenhao li ||| 
2019 ||| spatial-temporal attention-based human dynamics retrospection. ||| minjing dong ||| chang xu ||| 
2021 ||| traffic sign detection and recognition using multi-scale fusion and prime sample attention. ||| jinghao cao ||| junju zhang ||| wei huang ||| 
2022 ||| fuzzy based condition monitoring tool for real-time analysis of synthetic ester fluid as transformer insulant. ||| mridula ||| shufali ashraf wani ||| a. j. amalanathan ||| ramanujam sarathi ||| 
2020 ||| da-capnet: dual attention deep learning based on u-net for nailfold capillary segmentation. ||| yuli sun hariyani ||| heesang eom ||| cheolsoo park ||| 
2019 ||| transformer-based neural network for answer selection in question answering. ||| taihua shao ||| yupu guo ||| honghui chen ||| zepeng hao ||| 
2019 ||| bandwidth enhancement of gan mmic doherty power amplifiers using broadband transformer-based load modulation network. ||| gholamreza nikandish ||| robert bogdan staszewski ||| anding zhu ||| 
2021 ||| exploring category attention for open set domain adaptation. ||| jinghua wang ||| 
2020 ||| the evolution of trapping parameters on three-layer oil-paper of partial discharge degradation for on-board traction transformers. ||| quanfu li ||| xiaonan li ||| yan yang ||| yixuan li ||| guangning wu ||| 
2021 ||| multi-scale attention network for diabetic retinopathy classification. ||| mohammad t. al-antary ||| yasmine arafa ||| 
2020 ||| an efficient voxel-based segmentation algorithm based on hierarchical clustering to extract lidar power equipment data in transformer substations. ||| jianlong guo ||| weixia feng ||| jiang xue ||| shan xiong ||| tengfei hao ||| ruiheng li ||| huben mao ||| 
2021 ||| attention-based bi-directional long-short term memory network for earthquake prediction. ||| md. hasan al banna ||| tapotosh ghosh ||| m. jaber al nahian ||| kazi abu taher ||| m. shamim kaiser ||| mufti mahmud ||| mohammad shahadat hossain ||| karl andersson ||| 
2020 ||| use of alternative fluids in very high-power transformers: experimental and numerical thermal studies. ||| ramazan altay ||| agustin santisteban ||| cristian olmo ||| carlos j. renedo ||| alfredo ortiz fern ||| ndez ||| f ||| lix ortiz ||| fernando delgado ||| 
2019 ||| a single attention-based combination of cnn and rnn for relation classification. ||| xiaoyu guo ||| hui zhang ||| haijun yang ||| lianyuan xu ||| zhiwen ye ||| 
2020 ||| recurrent attention dense network for single image de-raining. ||| guoqiang chai ||| zhao-ba wang ||| guodong guo ||| youxing chen ||| yong jin ||| wei wang ||| xia zhao ||| 
2020 ||| convolutional neural network-based pavement crack segmentation using pyramid attention network. ||| wenjun wang ||| chao su ||| 
2019 ||| inferring drivers' visual focus attention through head-mounted inertial sensors. ||| jos |||  m. ram ||| rez ||| marcela d. rodr ||| guez ||| ngel g. andrade ||| lu ||| s a. castro ||| jessica beltr ||| n ||| josu |||  s. armenta ||| 
2022 ||| channel attention gan trained with enhanced dataset for single-image shadow removal. ||| ryo abiko ||| masaaki ikehara ||| 
2020 ||| fine-grained age estimation with multi-attention network. ||| chunlong hu ||| junbin gao ||| jianjun chen ||| dengbiao jiang ||| yucheng shu ||| 
2019 ||| group recommendation via self-attention and collaborative metric learning model. ||| haiyan wang ||| yuliang li ||| felix frimpong ||| 
2021 ||| mtsan: multi-task semantic attention network for adas applications. ||| chun-yu lai ||| bo-xun wu ||| vinay malligere shivanna ||| jiun-in guo ||| 
2020 ||| temporal-frequency attention-based human activity recognition using commercial wifi devices. ||| xiaolong yang ||| ruoyu cao ||| mu zhou ||| liangbo xie ||| 
2020 ||| video synopsis based on attention mechanism and local transparent processing. ||| shengbo chen ||| xianrui liu ||| yiyong huang ||| congcong zhou ||| huaikou miao ||| 
2020 ||| context-aware cross-attention for skeleton-based human action recognition. ||| yanbo fan ||| shuchen weng ||| yong zhang ||| boxin shi ||| yi zhang ||| 
2019 ||| deep feature fusion by competitive attention for pedestrian detection. ||| zhichang chen ||| li zhang ||| abdul mateen khattak ||| wanlin gao ||| minjuan wang ||| 
2019 ||| combining the attention network and semantic representation for chinese verb metaphor identification. ||| dongyu zhang ||| hongfei lin ||| xikai liu ||| heting zhang ||| shaowu zhang ||| 
2020 ||| video alignment using bi-directional attention flow in a multi-stage learning model. ||| reham abobeah ||| amin a. shoukry ||| jiro katto ||| 
2021 ||| analytical calculation of magnetic field in fractional-slot windings linear phase-shifting transformer based on exact subdomain model. ||| guoqiang guo ||| jinghong zhao ||| mei wu ||| yiyong xiong ||| 
2019 ||| how to make attention mechanisms more practical in malware classification. ||| xin ma ||| shize guo ||| haiying li ||| zhisong pan ||| junyang qiu ||| yu ding ||| feiqiong chen ||| 
2021 ||| an average voltage approach to control energy storage device and tap changing transformers under high distributed generation. ||| ndamulelo tshivhase ||| ali n. hasan ||| thokozani shongwe ||| 
2021 ||| haze relevant feature attention network for single image dehazing. ||| xin jiang ||| lu lu ||| ming zhu ||| zhicheng hao ||| wen gao ||| 
2019 ||| attention gan-based method for designing intelligent making system. ||| lei shao ||| congcong liang ||| ke wang ||| wengang cao ||| wei zhang ||| guan gui ||| hikmet sari ||| 
2019 ||| correction to "a novel unbalance compensation method for distribution solid-state transformer based on reduced order generalized integrator". ||| zhengwei qu ||| yunxiao yao ||| yunjing wang ||| chunjiang zhang ||| zhenxiao chong ||| ahmed abu-siada ||| 
2019 ||| residual attention convolutional network for online visual tracking. ||| long gao ||| yunsong li ||| jifeng ning ||| 
2021 ||| investigation on design of novel step-up 18-pulse auto-transformer rectifier. ||| xiaoqiang chen ||| tao chen ||| ying wang ||| 
2019 ||| multi-scale visual attention deep convolutional neural network for multi-focus image fusion. ||| rui lai ||| yongxue li ||| juntao guan ||| ai xiong ||| 
2020 ||| collaborative filtering recommendation algorithm based on attention gru and adversarial learning. ||| hongbin xia ||| jing jing li ||| yuan liu ||| 
2020 ||| adversarial attention-based variational graph autoencoder. ||| ziqiang weng ||| weiyu zhang ||| wei dou ||| 
2020 ||| da-net: pedestrian detection using dense connected block and attention modules. ||| ruihong yin ||| rufei zhang ||| wei zhao ||| feng jiang ||| 
2020 ||| split-attention multiframe alignment network for image restoration. ||| yongyi yu ||| mingzhe liu ||| huajun feng ||| zhihai xu ||| qi li ||| 
2020 ||| multimodal encoder-decoder attention networks for visual question answering. ||| chongqing chen ||| dezhi han ||| jun wang ||| 
2020 ||| skin lesion segmentation based on multi-scale attention convolutional neural network. ||| yun jiang ||| simin cao ||| shengxin tao ||| hai zhang ||| 
2022 ||| a novel attention-based multi-modal modeling technique on mixed type data for improving tft-lcd repair process. ||| yi liu ||| hsueh-ping lu ||| ching-hao lai ||| 
2019 ||| sentiment-aware deep recommender system with neural attention networks. ||| aminu da'u ||| naomie salim ||| 
2021 ||| multi-domain aspect extraction using bidirectional encoder representations from transformers. ||| brucce neves dos santos ||| ricardo marcondes marcacini ||| solange oliveira rezende ||| 
2020 ||| stgat: spatial-temporal graph attention networks for traffic flow forecasting. ||| xiangyuan kong ||| weiwei xing ||| xiang wei ||| peng bao ||| jian zhang ||| wei lu ||| 
2019 ||| facial landmark detection via attention-adaptive deep network. ||| muhammad sadiq ||| daming shi ||| meiqin guo ||| xiaochun cheng ||| 
2021 ||| tactical decision-making for autonomous driving using dueling double deep q network with double attention. ||| shuwei zhang ||| yutian wu ||| harutoshi ogai ||| hiroshi inujima ||| shigeyuki tateno ||| 
2020 ||| target tracking method based on adaptive structured sparse representation with attention. ||| jie wang ||| shibin xuan ||| hao zhang ||| xuyang qin ||| 
2020 ||| improving whole-heart ct image segmentation by attention mechanism. ||| wei wang ||| chengqin ye ||| shanzhuo zhang ||| yong xu ||| kuanquan wang ||| 
2020 ||| wavenet with cross-attention for audiovisual speech recognition. ||| hui wang ||| fei gao ||| yue zhao ||| licheng wu ||| 
2020 ||| fpgan: an fpga accelerator for graph attention networks with software and hardware co-optimization. ||| weian yan ||| weiqin tong ||| xiaoli zhi ||| 
2021 ||| diagnosis of inter-turn shorts of loaded transformer under various load currents and power factors; impulse voltage-based frequency response approach. ||| natarajan shanmugam ||| srinivasan gopal ||| balasubramanian madanmohan ||| s. p. balaji ||| rajesh rajamani ||| 
2019 ||| convolution-based neural attention with applications to sentiment classification. ||| jiachen du ||| lin gui ||| yulan he ||| ruifeng xu ||| xuan wang ||| 
2020 ||| study on the operators' attention of different areas in university laboratories based on eye movement tracking technology. ||| zijun li ||| shuqi zhao ||| long su ||| chanjuan liao ||| 
2019 ||| improving distantly supervised relation classification with attention and semantic weight. ||| zhangdong zhu ||| jindian su ||| yang zhou ||| 
2020 ||| aedmts: an attention-based encoder-decoder framework for multi-sensory time series analytic. ||| jin fan ||| hongkun wang ||| yipan huang ||| ke zhang ||| bei zhao ||| 
2020 ||| a dimension reduction method used in detecting errors of distribution transformer connectivity. ||| zeyang tang ||| yu shen ||| kunpeng zhou ||| kan cao ||| fan yang ||| defu cai ||| chengke zhou ||| 
2021 ||| ultrasonic logging image denoising based on cnn and feature attention. ||| su li ||| bowen fu ||| jiangdong wei ||| yunfei lv ||| qingnan wang ||| jihui tu ||| 
2021 ||| ddanet: dual-path depth-aware attention network for fingerspelling recognition using rgb-d images. ||| shih-hung yang ||| wei-ren chen ||| wun-jhu huang ||| yon-ping chen ||| 
2020 ||| modeling and stability analysis of a smart transformer-fed grid. ||| zhi-xiang zou ||| marco liserre ||| zheng wang ||| ming cheng ||| 
2020 ||| salient object detection using recurrent guidance network with hierarchical attention features. ||| shanmei lu ||| qiang guo ||| yongxia zhang ||| 
2019 ||| a novel measuring method of interfacial tension of transformer oil combined pso optimized svm and multi frequency ultrasonic technology. ||| zhuang yang ||| qu zhou ||| xiaodong wu ||| zhongyong zhao ||| 
2020 ||| investigation on the parasitic capacitance of high frequency and high voltage transformers of multi-section windings. ||| le deng ||| pengbo wang ||| xiaofeng li ||| houxiu xiao ||| tao peng ||| 
2022 ||| a power transformer fault diagnosis method-based hybrid improved seagull optimization algorithm and support vector machine. ||| yuhan wu ||| xianbo sun ||| yi zhang ||| xianjing zhong ||| lei cheng ||| 
2021 ||| multi-head self-attention transformation networks for aspect-based sentiment analysis. ||| yuming lin ||| chaoqiang wang ||| hao song ||| you li ||| 
2021 ||| attention span prediction using head-pose estimation with deep neural networks. ||| tripti singh ||| mohan mohadikar ||| shilpa gite ||| shruti patil ||| biswajeet pradhan ||| abdullah m. alamri ||| 
2019 ||| no-reference stereoscopic image quality assessment based on visual attention and perception. ||| yafei li ||| feng yang ||| wenbo wan ||| jun wang ||| min gao ||| jia zhang ||| jiande sun ||| 
2020 ||| ca-gan: class-condition attention gan for underwater image enhancement. ||| jing wang ||| ping li ||| jianhua deng ||| yongzhao du ||| jiafu zhuang ||| peidong liang ||| peizhong liu ||| 
2019 ||| a single-phase transformer-based cascaded asymmetric multilevel inverter with balanced power distribution. ||| oswaldo lopez-santos ||| carlos a. jacanamejoy-jamioy ||| diego f. salazar-d'antonio ||| julian r. corredor-ram ||| rez ||| germain garcia ||| luis mart ||| nez-salamero ||| 
2019 ||| msnet: multi-head self-attention network for distantly supervised relation extraction. ||| tingting sun ||| chunhong zhang ||| yang ji ||| zheng hu ||| 
2020 ||| lightweight prediction and boundary attention-based semantic segmentation for road scene understanding. ||| jee-young sun ||| seung-won jung ||| sung-jea ko ||| 
2020 ||| capacitor voltage ripple minimization of a modular three-phase ac/dc power electronics transformer with four-winding power channel. ||| xiaohui li ||| linqian cheng ||| liqun he ||| zhongkui zhu ||| yong yang ||| cheng wang ||| 
2020 ||| hierarchical graph transformer-based deep learning model for large-scale multi-label text classification. ||| jibing gong ||| hongyuan ma ||| zhiyong teng ||| qi teng ||| hekai zhang ||| linfeng du ||| shuai chen ||| md. zakirul alam bhuiyan ||| jianhua li ||| mingsheng liu ||| 
2020 ||| s2n2: an interpretive semantic structure attention neural network for trajectory classification. ||| canghong jin ||| ting tao ||| xianzhe luo ||| zemin liu ||| minghui wu ||| 
2020 ||| multi-attention network for stereo matching. ||| xiaowei yang ||| lin he ||| yong zhao ||| haiwei sang ||| zu liu yang ||| xian jing cheng ||| 
2020 ||| classification and biomarker exploration of autism spectrum disorders based on recurrent attention model. ||| fengkai ke ||| rui yang ||| 
2020 ||| speech emotion recognition by combining a unified first-order attention network with data balance. ||| gang chen ||| shiqing zhang ||| xin tao ||| xiaoming zhao ||| 
2020 ||| msba: multiple scales, branches and attention network with bag of tricks for person re-identification. ||| hanlin tan ||| huaxin xiao ||| xiaoyu zhang ||| bin dai ||| shiming lai ||| yu liu ||| maojun zhang ||| 
2021 ||| soft thresholding attention network for adaptive feature denoising in sar ship detection. ||| rui wang ||| sihan shao ||| mengyu an ||| jiayi li ||| shifeng wang ||| xiping xu ||| 
2020 ||| influence of phase-shifted square wave modulation on medium frequency transformer in a mmc based sst. ||| rachit agarwal ||| sandro martin ||| hui li ||| 
2019 ||| single image rain removal via cascading attention aggregation network on challenging weather conditions. ||| junsheng wang ||| xiang huang ||| shan gai ||| 
2020 ||| effective exploitation of posterior information for attention-based speech recognition. ||| jian tang ||| junfeng hou ||| yan song ||| li-rong dai ||| ian mcloughlin ||| 
2022 ||| qos prediction of web services based on a two-level heterogeneous graph attention network. ||| shengkai lv ||| fangzhou yi ||| peng he ||| cheng zeng ||| 
2020 ||| a discriminative deep model with feature fusion and temporal attention for human action recognition. ||| jiahui yu ||| hongwei gao ||| wei yang ||| yueqiu jiang ||| wei hong chin ||| naoyuki kubota ||| zhaojie ju ||| 
2020 ||| multi-modal memory enhancement attention network for image-text matching. ||| zhong ji ||| zhigang lin ||| haoran wang ||| yuqing he ||| 
2020 ||| image retrieval using a deep attention-based hash. ||| xinlu li ||| mengfei xu ||| jiabo xu ||| thomas weise ||| le zou ||| fei sun ||| zhize wu ||| 
2021 ||| upper airway segmentation based on the attention mechanism of weak feature regions. ||| wei wu ||| yang yu ||| qing wang ||| dongxu liu ||| xiao yuan ||| 
2018 ||| an input-series-output-series modular multilevel dc transformer with inter-module arithmetic phase interleaving control to reduce dc ripples. ||| ran ding ||| jun mei ||| zhou guan ||| jianfeng zhao ||| 
2019 ||| a hierarchical bidirectional gru model with attention for eeg-based emotion classification. ||| jingxia chen ||| d. m. jiang ||| y. n. zhang ||| 
2019 ||| line impedance measurement to improve power systems protection of the gautrain 25 kv autotransformer traction power supply system. ||| ndaedzo m. moyo ||| ramesh c. bansal ||| raj naidoo ||| willem sprong ||| 
2020 ||| a novel csi feedback approach for massive mimo using lstm-attention cnn. ||| qi li ||| aihua zhang ||| pengcheng liu ||| jianjun li ||| chunlei li ||| 
2021 ||| health analysis of transformer winding insulation through thermal monitoring and fast fourier transform (fft) power spectrum. ||| muhammad aslam ||| inzamam ul haq ||| muhammad saad rehan ||| faheem ali ||| abdul basit ||| muhammad iftikhar khan ||| muhammad naeem ||| 
2020 ||| evaluation of sentiment analysis in finance: from lexicons to transformers. ||| kostadin mishev ||| ana gjorgjevikj ||| irena vodenska ||| lubomir t. chitkushev ||| dimitar trajanov ||| 
2020 ||| experimental and numerical study on stray loss in laminated magnetic shielding under 3-d ac-dc hybrid excitations for hvdc transformers. ||| xiaojun zhao ||| yuezhi cao ||| zhiguang cheng ||| behzad forghani ||| lanrong liu ||| jiawen wang ||| 
2020 ||| echobert: a transformer-based approach for behavior detection in echograms. ||| h ||| kon m ||| l ||| y ||| 
2020 ||| weakly supervised learning for land cover mapping of satellite image time series via attention-based cnn. ||| dino ienco ||| yawogan jean eudes gbodjo ||| raffaele gaetano ||| roberto interdonato ||| 
2019 ||| combining part-of-speech tags and self-attention mechanism for simile recognition. ||| pengfei zhang ||| yi cai ||| junying chen ||| wenhao chen ||| hengjie song ||| 
2018 ||| attention-based relation extraction with bidirectional gated recurrent unit and highway network in the analysis of geological data. ||| xiong luo ||| wenwen zhou ||| weiping wang ||| yueqin zhu ||| jing deng ||| 
2020 ||| interactive rule attention network for aspect-level sentiment analysis. ||| qiang lu ||| zhenfang zhu ||| dianyuan zhang ||| wenqing wu ||| qiangqiang guo ||| 
2020 ||| recognition of teachers' facial expression intensity based on convolutional neural network and attention mechanism. ||| kun zheng ||| dong yang ||| junhua liu ||| jinling cui ||| 
2019 ||| deep cnns with self-attention for speaker identification. ||| nguyen nang an ||| nguyen quang thanh ||| yanbing liu ||| 
2018 ||| comprehensive power losses model for electronic power transformer. ||| quanyou yue ||| canbing li ||| yijia cao ||| yuqing he ||| binglin cai ||| qiuwei wu ||| bin zhou ||| 
2021 ||| high-resolution pelvic mri reconstruction using a generative adversarial network with attention and cyclic loss. ||| guangyuan li ||| jun lv ||| xiangrong tong ||| chengyan wang ||| guang yang ||| 
2021 ||| adaptive dynamic meta-heuristics for feature selection and classification in diagnostic accuracy of transformer faults. ||| sherif s. m. ghoneim ||| tamer ahmed farrag ||| a. ali rashed ||| el-sayed m. el-kenawy ||| abdelhameed ibrahim ||| 
2020 ||| reducing the impacts of electric vehicle charging on power distribution transformers. ||| pravakar pradhan ||| iftekhar ahmad ||| daryoush habibi ||| ganesh kothapalli ||| mohammad a. s. masoum ||| 
2020 ||| stacked residual recurrent neural networks with cross-layer attention for text classification. ||| yangyang lan ||| yazhou hao ||| kui xia ||| buyue qian ||| chen li ||| 
2020 ||| solid-state transformer for power distribution grid based on a hybrid switched-capacitor llc-src converter: analysis, design, and experimentation. ||| rogerio luiz da silva ||| victor luiz flor borges ||| carlos eduardo possamai ||| ivo barbi ||| 
2021 ||| effect of iron/titania-based nanomaterials on the dielectric properties of mineral oil, natural and synthetic esters as transformers insulating fluid. ||| suhaib ahmad khan ||| mohd tariq ||| asfar ali khan ||| basem alamri ||| 
2021 ||| stack attention-pruning aggregates multiscale graph convolution networks for hyperspectral remote sensing image classification. ||| na liu ||| bin zhang ||| qiuhuan ma ||| qingqing zhu ||| xiaoling liu ||| 
2021 ||| hierarchically structured network with attention convolution and embedding propagation for imbalanced few-shot learning. ||| wei xiong ||| yu gong ||| weihua niu ||| ruonan wang ||| 
2020 ||| scalar coupling constant prediction using graph embedding local attention encoder. ||| caiqing jian ||| xinyu cheng ||| jian zhang ||| lihui wang ||| 
2019 ||| dominance in visual space of asd children using multi-robot joint attention integrated distributed imitation system. ||| faisal mehmood ||| yasar ayaz ||| sara ali ||| rita de cassia amadeu ||| haleema sadia ||| 
2019 ||| inversion of oil-immersed paper resistivity in transformer based on dielectric loss factor. ||| jiangjun ruan ||| yiming xie ||| shuo jin ||| yu shi ||| yu tian ||| yongqing deng ||| 
2019 ||| fully convolutional captionnet: siamese difference captioning attention model. ||| oluwasanmi ariyo ||| enoch frimpong ||| muhammad umar aftab ||| edward yellakuor baagyere ||| zhiguang qin ||| kifayat ullah ||| 
2021 ||| gigapixel histopathological image analysis using attention-based neural networks. ||| nadia brancati ||| giuseppe de pietro ||| daniel riccio ||| maria frucci ||| 
2019 ||| densely connected convolutional networks with attention lstm for crowd flows prediction. ||| wei li ||| wei tao ||| junyang qiu ||| xin liu ||| xingyu zhou ||| zhisong pan ||| 
2019 ||| image super-resolution using aggregated residual transformation networks with spatial attention. ||| xingrun xing ||| dawei zhang ||| 
2020 ||| a hierarchical structured multi-head attention network for multi-turn response generation. ||| fei lin ||| cong zhang ||| shengqiang liu ||| hong ma ||| 
2020 ||| semi-supervised breast histological image classification by node-attention graph transfer network. ||| liheng gong ||| jingjing yang ||| xiao zhang ||| 
2019 ||| time+user dual attention based sentiment prediction for multiple social network texts with time series. ||| lei li ||| yabin wu ||| yuwei zhang ||| tianyuan zhao ||| 
2020 ||| hybrid power quality compensation system for electric railway supplied by the hypotenuse of a scott transformer. ||| leilei zhao ||| mingli wu ||| qiujiang liu ||| peng peng ||| jing li ||| 
2020 ||| probabilistic matrix factorization recommendation of self-attention mechanism convolutional neural networks with item auxiliary information. ||| chenkun zhang ||| cheng wang ||| 
2019 ||| vaa: visual aligning attention model for remote sensing image captioning. ||| zhengyuan zhang ||| wenkai zhang ||| wenhui diao ||| menglong yan ||| xin gao ||| xian sun ||| 
2020 ||| transformer based multi-grained attention network for aspect-based sentiment analysis. ||| jiahui sun ||| ping han ||| zheng cheng ||| enming wu ||| wenqing wang ||| 
2018 ||| automatic generation of news comments based on gated attention neural networks. ||| hai-tao zheng ||| wei wang ||| wang chen ||| arun kumar sangaiah ||| 
2022 ||| attention mechanism-based light-field view synthesis. ||| muhammad shahzeb khan gul ||| m. umair mukati ||| michel b ||| tz ||| s ||| ren forchhammer ||| joachim keinert ||| 
2020 ||| hierarchical attentional factorization machines for expert recommendation in community question answering. ||| weizhao tang ||| tun lu ||| dongsheng li ||| hansu gu ||| ning gu ||| 
2020 ||| knowledge graph embedding via graph attenuated attention networks. ||| rui wang ||| bicheng li ||| shengwei hu ||| wenqian du ||| min zhang ||| 
2020 ||| human-centric emotion estimation based on correlation maximization considering changes with time in visual attention and brain activity. ||| yuya moroto ||| keisuke maeda ||| takahiro ogawa ||| miki haseyama ||| 
2019 ||| prediction of natural gas consumption for city-level dhs based on attention gru: a case study for a northern chinese city. ||| guixiang xue ||| jiancai song ||| xiangfei kong ||| yu pan ||| chengying qi ||| han li ||| 
2022 ||| roberta-lstm: a hybrid model for sentiment analysis with transformer and recurrent neural network. ||| kian long tan ||| chin-poo lee ||| kalaiarasi sonai muthu anbananthen ||| kian-ming lim ||| 
2020 ||| precious metal price prediction based on deep regularization self-attention regression. ||| junhao zhou ||| zhanhong he ||| ya nan song ||| hao wang ||| xiaoping yang ||| wenjuan lian ||| hong-ning dai ||| 
2019 ||| query is gan: scene retrieval with attentional text-to-image generative adversarial network. ||| rintaro yanagi ||| ren togo ||| takahiro ogawa ||| miki haseyama ||| 
2019 ||| a visuo-haptic attention training game with dynamic adjustment of difficulty. ||| cong peng ||| dangxiao wang ||| yuru zhang ||| jing xiao ||| 
2021 ||| multi-grained attention representation with albert for aspect-level sentiment classification. ||| yuezhe chen ||| lingyun kong ||| yang wang ||| dezhi kong ||| 
2020 ||| mscnn-am: a multi-scale convolutional neural network with attention mechanisms for retinal vessel segmentation. ||| qilong fu ||| shuqiu li ||| xin wang ||| 
2021 ||| state identification of transformer under dc bias based on wavelet singular entropy. ||| jun liu ||| huarong zeng ||| wei niu ||| peilong chen ||| kui xu ||| peng zeng ||| lu zhao ||| sheng lin ||| 
2020 ||| generative text summary based on enhanced semantic attention and gain-benefit gate. ||| jianli ding ||| yang li ||| huiyu ni ||| zhengquan yang ||| 
2019 ||| a very deep spatial transformer towards robust single image super-resolution. ||| jianmin jiang ||| hossam m. kasem ||| kwok-wai hung ||| 
2019 ||| pre-alignment guided attention for improving training efficiency and model stability in end-to-end speech synthesis. ||| xiaolian zhu ||| yuchao zhang ||| shan yang ||| liumeng xue ||| lei xie ||| 
2021 ||| unified transformer multi-task learning for intent classification with entity recognition. ||| alberto jos |||  benayas  ||| lamos ||| reyhaneh hashempour ||| damian rumble ||| shoaib jameel ||| renato cordeiro de amorim ||| 
2019 ||| parallax-based spatial and channel attention for stereo image super-resolution. ||| chenyang duan ||| nanfeng xiao ||| 
2020 ||| vision-based fall event detection in complex background using attention guided bi-directional lstm. ||| yong chen ||| weitong li ||| lu wang ||| jiajia hu ||| mingbin ye ||| 
2020 ||| ensemble learning with attention-integrated convolutional recurrent neural network for imbalanced speech emotion recognition. ||| xusheng ai ||| victor s. sheng ||| wei fang ||| charles x. ling ||| chunhua li ||| 
2021 ||| deep learning based mineral image classification combined with visual attention mechanism. ||| yang liu ||| zelin zhang ||| xiang liu ||| lei wang ||| xuhui xia ||| 
2022 ||| structured fusion attention network for image super-resolution reconstruction. ||| yaonan dai ||| jiuyang yu ||| tianhao hu ||| yang lu ||| xiaotao zheng ||| 
2021 ||| dynamic thermal model for power transformers. ||| muhammad aslam ||| inzamam ul haq ||| muhammad saad rehan ||| abdul basit ||| muhammad arif ||| muhammad iftikhar khan ||| muhammad sadiq ||| muhammad naeem ||| 
2019 ||| study on characteristic parameters of short-circuit impedance for a four-winding inductive filtering transformer in power system supplying nonlinear loads. ||| zhao huang ||| yuehui chen ||| saimei shi ||| longfu luo ||| 
2021 ||| generative adversarial networks with attention mechanisms at every scale. ||| farkhod makhmudkhujaev ||| in kyu park ||| 
2020 ||| robust qrs detection using high-resolution wavelet packet decomposition and time-attention convolutional neural network. ||| menghan jia ||| feiteng li ||| jiaquan wu ||| zhijian chen ||| yu pu ||| 
2021 ||| breakdown voltage of transformer oil containing cellulose particle contamination with and without bridge formation under lightning impulse stress. ||| sarizan bin saaidon ||| mohd aizam talib ||| muhammad nur khairul hafizi rohani ||| nor asiah muhamad ||| mohamad kamarol mohd jamil ||| 
2020 ||| adaptive inattentional framework for video object detection with reward-conditional training. ||| alejandro rodriguez-ramos ||| javier rodriguez-vazquez ||| carlos sampedro ||| pascual campoy ||| 
2020 ||| dairy goat image generation based on improved-self-attention generative adversarial networks. ||| huan li ||| jinglei tang ||| 
2019 ||| gisca: gradient-inductive segmentation network with contextual attention for scene text detection. ||| meng cao ||| yuexian zou ||| dongming yang ||| chao liu ||| 
2021 ||| an air target tactical intention recognition model based on bidirectional gru with attention mechanism. ||| fei teng ||| xinpeng guo ||| yafei song ||| gang wang ||| 
2018 ||| cross-modal multistep fusion network with co-attention for visual question answering. ||| mingrui lao ||| yanming guo ||| hui wang ||| xin zhang ||| 
2019 ||| coronary arteries segmentation based on 3d fcn with attention gate and level set function. ||| ye shen ||| zhijun fang ||| yongbin gao ||| naixue xiong ||| cengsi zhong ||| xianhua tang ||| 
2018 ||| estimation of perceptual surface property using deep networks with attention models. ||| hyunjoong cho ||| ye seul baek ||| youngshin kwak ||| seungjoon yang ||| 
2020 ||| prior attention enhanced convolutional neural network based automatic segmentation of organs at risk for head and neck cancer radiotherapy. ||| haibin chen ||| dongyun huang ||| li lin ||| zhenyu qi ||| peiliang xie ||| jun wei ||| lin chang ||| ying sun ||| dongmei wu ||| yao lu ||| 
2019 ||| an automatic scale-adaptive approach with attention mechanism-based crowd spatial information for crowd counting. ||| weihang kong ||| he li ||| guanglong xing ||| fengda zhao ||| 
2019 ||| short-term photovoltaic power forecasting based on long short term memory neural network and attention mechanism. ||| hangxia zhou ||| yujin zhang ||| lingfan yang ||| qian liu ||| ke yan ||| yang du ||| 
2018 ||| influence of high voltage dc transmission on measuring accuracy of current transformers. ||| shihai yang ||| gan zhou ||| zhinong wei ||| 
2020 ||| an attention guided semi-supervised learning mechanism to detect electricity frauds in the distribution systems. ||| zeeshan aslam ||| fahad ahmed ||| ahmad almogren ||| muhammad shafiq ||| mansour zuair ||| nadeem javaid ||| 
2020 ||| a semi-supervised autoencoder with an auxiliary task (saat) for power transformer fault diagnosis using dissolved gas analysis. ||| sunuwe kim ||| soo-ho jo ||| wongon kim ||| jongmin park ||| jingyo jeong ||| yeongmin han ||| daeil kim ||| byeng dong youn ||| 
2020 ||| validity evaluation of transformer dga online monitoring data in grid edge systems. ||| jun jia ||| fengbo tao ||| guojiang zhang ||| jin shao ||| xinghui zhang ||| bo wang ||| 
2019 ||| multi-gram cnn-based self-attention model for relation classification. ||| chunyun zhang ||| chaoran cui ||| sheng gao ||| xiushan nie ||| weiran xu ||| lu yang ||| xiaoming xi ||| yilong yin ||| 
2020 ||| modeling and characterization of series connected hybrid transformers for low-profile power converters. ||| muhammad abu bakar ||| muhammad farhan alam ||| kent bertilsson ||| 
2021 ||| impact of geomagnetically induced currents on high voltage transformers in malaysian power network and its mitigation. ||| zmnako mohammed khurshid abda ||| nur fadilah ab aziz ||| zeti akma rhazali ||| mohd zainal abidin ab kadir ||| 
2020 ||| an end to end framework with adaptive spatio-temporal attention module for human action recognition. ||| shaocan liu ||| xin ma ||| hanbo wu ||| yibin li ||| 
2020 ||| deep high-resolution network with double attention residual blocks for human pose estimation. ||| zhanqiang huo ||| han jin ||| yingxu qiao ||| fen luo ||| 
2020 ||| a novel approach for analyzing entity linking between words and entities for a knowledge base using an attention-based bilinear joint learning and weighted summation model. ||| shuanghu luo ||| penglong wang ||| min cao ||| 
2020 ||| enhanced switching pattern to improve cell balancing performance in active cell balancing circuit using multi-winding transformer. ||| sangjung lee ||| myoungho kim ||| juwon baek ||| daewook kang ||| jee-hoon jung ||| 
2020 ||| a cascaded r-cnn with multiscale attention and imbalanced samples for traffic sign detection. ||| jianming zhang ||| zhipeng xie ||| juan sun ||| xin zou ||| jin wang ||| 
2020 ||| multi-head self-attention-based deep clustering for single-channel speech separation. ||| yanliang jin ||| chenjun tang ||| qianhong liu ||| yan wang ||| 
2020 ||| mpan: multi-part attention network for point cloud based 3d shape retrieval. ||| zirui li ||| junyu xu ||| yue zhao ||| wenhui li ||| weizhi nie ||| 
2019 ||| attention-guided network for semantic video segmentation. ||| jiangyun li ||| yikai zhao ||| jun fu ||| jiajia wu ||| jing liu ||| 
2019 ||| long document classification from local word glimpses via recurrent attention learning. ||| jun he ||| liqun wang ||| liu liu ||| jiao feng ||| hao wu ||| 
2022 ||| on the effects of lamination artificial faults in a 15 kva three-phase transformer core. ||| ehsan altayef ||| fatih anayi ||| michael s. packianather ||| omar kherif ||| 
2020 ||| remaining useful life estimation of bldc motor considering voltage degradation and attention-based neural network. ||| tanvir alam shifat ||| jangwook hur ||| 
2019 ||| investigation of attention deficit/hyperactivity disorder assessment using electro interstitial scan based on chronoamperometry technique. ||| ree nah chua ||| yuan wen hau ||| chew ming tiew ||| wan leong hau ||| 
2019 ||| dualattn-gan: text to image synthesis with dual attentional generative adversarial network. ||| yali cai ||| xiaoru wang ||| zhihong yu ||| fu li ||| peirong xu ||| yueli li ||| lixian li ||| 
2021 ||| modulation and control of a dc-ac converter with high-frequency link transformer for grid-connected applications. ||| mahmoud a. sayed ||| takaharu takeshita ||| atif iqbal ||| zuhair muhammed alaas ||| m. m. r. ahmed ||| sherif m. dabour ||| 
2021 ||| sss-ae: anomaly detection using self-attention based sequence-to-sequence auto-encoder in smd assembly machine sound. ||| ki hyun nam ||| young jong song ||| il dong yun ||| 
2020 ||| attention-based sequence learning model for travel time estimation. ||| zhong wang ||| hao fu ||| guiquan liu ||| xianwei meng ||| 
2019 ||| hyperspectral image classification with pre-activation residual attention network. ||| hongmin gao ||| yao yang ||| dan yao ||| chenming li ||| 
2020 ||| an adaptive multiscale fusion network based on regional attention for remote sensing images. ||| wanzhen lu ||| longxue liang ||| xiaosuo wu ||| xiaoyu wang ||| jiali cai ||| 
2021 ||| sal-hmax: an enhanced hmax model in conjunction with a visual attention mechanism to improve object recognition task. ||| zahra sadat shariatmadar ||| karim faez ||| akbar siami namin ||| 
2020 ||| research on current limiting method used for short circuit fault current of resonant dc transformer based on inverted displacement phase control. ||| chaoran zhuo ||| xu yang ||| xiaotian zhang ||| xiong zhang ||| 
2021 ||| the impact of rotor torques for the pressure ratio characteristics of the double rotor hydraulic transformer. ||| zhongxun liu ||| qiaoyan liu ||| zhipeng xi ||| jihai jiang ||| 
2021 ||| emotion wheel attention-based emotion distribution learning. ||| xue-qiang zeng ||| qifan chen ||| xuefeng fu ||| jiali zuo ||| 
2020 ||| non-locally up-down convolutional attention network for remote sensing image super-resolution. ||| huan wang ||| qian hu ||| chengdong wu ||| jianning chi ||| xiaosheng yu ||| 
2019 ||| multi-label image classification by feature attention network. ||| zheng yan ||| weiwei liu ||| shiping wen ||| yin yang ||| 
2019 ||| prostate mr image segmentation with self-attention adversarial training based on wasserstein distance. ||| chengwei su ||| renxiang huang ||| chang liu ||| tailang yin ||| bo du ||| 
2018 ||| air core transformer winding disk deformation: a precise study on mutual inductance variation and its influence on frequency response spectrum. ||| mehdi bagheri ||| svyatoslav nezhivenko ||| b. t. phung ||| trevor blackburn ||| 
2020 ||| application of duality-based equivalent circuits for modeling multilimb transformers using alternative input parameters. ||| mohammad shafieipour ||| waldemar ziomek ||| rohitha p. jayasinghe ||| juan carlos garcia alonso ||| aniruddha m. gole ||| 
2021 ||| attention-based convolution skip bidirectional long short-term memory network for speech emotion recognition. ||| huiyun zhang ||| heming huang ||| henry han ||| 
2021 ||| foreground feature attention module based on unsupervised saliency detector for few-shot learning. ||| zhengmin kong ||| zhuolin fu ||| feng xiong ||| chenggang zhang ||| 
2021 ||| modeling global spatial-temporal graph attention network for traffic prediction. ||| bin sun ||| duan zhao ||| xinguo shi ||| yong-xin he ||| 
2019 ||| routine test analysis in power transformers by using firefly algorithm and computer program. ||| mehmet zile ||| 
2020 ||| flagging implausible inspection reports of distribution transformers via anomaly detection. ||| bin xiang ||| zhixiong liu ||| kunyi zhang ||| 
2021 ||| a transformer-less voltage equalizer for energy storage cells based on double-tiered multi-stacked converters. ||| yuanliang fan ||| luebin fang ||| mengran xu ||| han wu ||| zheng zhou ||| bingqian liu ||| shuang lin ||| yu wang ||| 
2021 ||| mobile service traffic classification based on joint deep learning with attention mechanism. ||| changbing li ||| chao dong ||| kai niu ||| zhengzhen zhang ||| 
2020 ||| weakly supervised local-global attention network for facial expression recognition. ||| haifeng zhang ||| wen su ||| zengfu wang ||| 
2020 ||| residual flux density measurement method of single-phase transformer core based on time constant. ||| cailing huo ||| shipu wu ||| yiming yang ||| chengcheng liu ||| youhua wang ||| 
2020 ||| dynamic phasor modeling of various multipulse rectifiers and a vsi fed by 18-pulse asymmetrical autotransformer rectifier unit for fast transient analysis. ||| dongsheng yuan ||| shuhong wang ||| yilu liu ||| 
2019 ||| r-transformer network based on position and self-attention mechanism for aspect-level sentiment classification. ||| ziyu zhou ||| fang'ai liu ||| qianqian wang ||| 
2019 ||| named entity recognition from biomedical texts using a fusion attention-based bilstm-crf. ||| hao wei ||| mingyuan gao ||| ai zhou ||| fei chen ||| wen qu ||| chunli wang ||| mingyu lu ||| 
2019 ||| multi-attention object detection model in remote sensing images based on multi-scale. ||| xiang ying ||| qiang wang ||| xuewei li ||| mei yu ||| han jiang ||| jie gao ||| zhiqiang liu ||| ruiguo yu ||| 
2020 ||| a zero-sequence steerable cbpwm strategy for eliminating zero-sequence current of dual-inverter fed open-end winding transformer based pv grid-tied system with common dc bus. ||| baoji wang ||| xing zhang ||| renxian cao ||| 
2021 ||| spacetransformers: language modeling for space systems. ||| audrey berquand ||| paul darm ||| annalisa riccardi ||| 
2021 ||| deep neural networks using capsule networks and skeleton-based attentions for action recognition. ||| manh-hung ha ||| oscal tzyh-chiang chen ||| 
2020 ||| recognizing the hrrp by combining cnn and birnn with attention mechanism. ||| jinwei wan ||| bo chen ||| yingqi liu ||| yijun yuan ||| hongwei liu ||| lin jin ||| 
2019 ||| tree-structured neural networks with topic attention for social emotion classification. ||| chang wang ||| bang wang ||| minghua xu ||| 
2022 ||| translating melody to chord: structured and flexible harmonization of melody with transformer. ||| seungyeon rhyu ||| hyeonseok choi ||| sarah kim ||| kyogu lee ||| 
2021 ||| enhanced dense space attention network for super-resolution construction from single input image. ||| yoong khang ooi ||| haidi ibrahim ||| muhammad nasiruddin mahyuddin ||| 
2020 ||| multi-layer transformer aggregation encoder for answer generation. ||| shengjie shang ||| jin liu ||| yihe yang ||| 
2019 ||| prediction of transformers conditions and lifetime using furan compounds analysis. ||| r. a. abd el-aal ||| khaled a. helal ||| a. m. m. hassan ||| s. s. dessouky ||| 
2020 ||| embedding encoder-decoder with attention mechanism for monaural speech enhancement. ||| tian lan ||| wenzheng ye ||| yilan lyu ||| junyi zhang ||| qiao liu ||| 
2019 ||| cross-domain sentiment classification with bidirectional contextualized transformer language models. ||| batsergelen myagmar ||| jie li ||| shigetomo kimura ||| 
2020 ||| pa-gan: a patch-attention based aggregation network for face recognition in surveillance. ||| ming liu ||| jinjin liu ||| ping zhang ||| qingbao li ||| 
2019 ||| individual dc voltage balance control for cascaded h-bridge electronic power transformer with separated dc-link topology. ||| jie tian ||| dewang hu ||| chunxiao zhou ||| yun yang ||| weihan wu ||| chengxiong mao ||| dan wang ||| 
2019 ||| a decentralized optimal operation of ac/dc hybrid microgrids equipped with power electronic transformer. ||| lei dong ||| tao zhang ||| tianjiao pu ||| naishi chen ||| yingyun sun ||| 
2021 ||| cross-domain fault diagnosis of rotating machinery using discriminative feature attention network. ||| gye-bong jang ||| jin-young kim ||| sung-bae cho ||| 
2019 ||| pam: pyramid attention mechanism based on contextual reasoning. ||| bohua chen ||| hanzhi ma ||| junjie he ||| yinzhang ding ||| lianghao wang ||| dongxiao li ||| ming zhang ||| 
2020 ||| sr-hgat: symmetric relations based heterogeneous graph attention network. ||| zhenghao zhang ||| jianbin huang ||| qinglin tan ||| 
2019 ||| a novel maintenance decision making model of power transformers based on reliability and economy assessment. ||| manling dong ||| hanbo zheng ||| yiyi zhang ||| kuikui shi ||| shuai yao ||| xiaokuo kou ||| guojun ding ||| lei guo ||| 
2021 ||| residual vector capsule: improving capsule by pose attention. ||| ning xie ||| xiaoxia wan ||| 
2020 ||| syntactic edge-enhanced graph convolutional networks for aspect-level sentiment classification with interactive attention. ||| yao xiao ||| guangyou zhou ||| 
2019 ||| abne: an attention-based network embedding for user alignment across social networks. ||| li liu ||| youmin zhang ||| shun fu ||| fujin zhong ||| jun hu ||| pu zhang ||| 
2020 ||| hierarchical multi-granularity attention- based hybrid neural network for text classification. ||| zhenyu liu ||| chaohong lu ||| haiwei huang ||| shengfei lyu ||| zhenchao tao ||| 
2020 ||| power control of a modular three-port solid-state transformer with three-phase unbalance regulation capabilities. ||| yuyang li ||| qiuye sun ||| dehao qin ||| ke cheng ||| zhibo li ||| 
2021 ||| ast-mtl: an attention-based multi-task learning strategy for traffic forecasting. ||| giovanni buroni ||| bertrand lebichot ||| gianluca bontempi ||| 
2021 ||| intelligent classifiers in distinguishing transformer faults using frequency response analysis. ||| mehdi bigdeli ||| pierluigi siano ||| hassan haes alhelou ||| 
2020 ||| a model of text-enhanced knowledge graph representation learning with mutual attention. ||| yashen wang ||| huanhuan zhang ||| ge shi ||| zhirun liu ||| qiang zhou ||| 
2020 ||| exploring the correlation between attention and cognitive load through association rule mining by using a brainwave sensing headband. ||| yueh-min huang ||| yu-ping cheng ||| shu-chen cheng ||| you-yi chen ||| 
2020 ||| a sparse transformer-based approach for image captioning. ||| zhou lei ||| congcong zhou ||| shengbo chen ||| yiyong huang ||| xianrui liu ||| 
2020 ||| transient oscillation suppression method of modular multilevel dc transformer. ||| dan lyu ||| yichao sun ||| yufan li ||| jianfeng zhao ||| zhendong ji ||| dongye li ||| 
2019 ||| ce-heat: an aspect-level sentiment classification approach with collaborative extraction hierarchical attention network. ||| yang gao ||| jianxun liu ||| pei li ||| dong zhou ||| 
2019 ||| comparison of pd and breakdown characteristics induced by metal particles and bubbles in flowing transformer oil. ||| yongze zhang ||| ju tang ||| cheng pan ||| xinyu luo ||| 
2018 ||| novel approach for optimizing the transformer's critical power limit. ||| claude ziad el-bayeh ||| imad mougharbel ||| dalal asber ||| maarouf saad ||| ambrish chandra ||| serge lefebvre ||| 
2021 ||| an accurate analytical method for leakage inductance calculation of shell-type transformers with rectangular windings. ||| morteza eslamian ||| mohammad kharezy ||| torbj ||| rn thiringer ||| 
2020 ||| an interpretable visual attention plug-in for convolutions. ||| chih-yang lin ||| chia-lin wu ||| hui-fuang ng ||| timothy k. shih ||| 
2021 ||| multi-level multi-modal cross-attention network for fake news detection. ||| long ying ||| hui yu ||| jinguang wang ||| yongze ji ||| shengsheng qian ||| 
2020 ||| aspect-based fashion recommendation with attention mechanism. ||| weiqian li ||| bugao xu ||| 
2020 ||| attention mask r-cnn for ship detection and segmentation from remote sensing images. ||| xuan nie ||| mengyang duan ||| haoxuan ding ||| bingliang hu ||| edward k. wong ||| 
2021 ||| power transformer fault diagnosis based on dga using a convolutional neural network with noise in measurements. ||| ibrahim b. m. taha ||| saleh ibrahim ||| diaa-eldin a. mansour ||| 
2019 ||| cross-media body-part attention network for image-to-video person re-identification. ||| benzhi yu ||| ning xu ||| jian zhou ||| 
2020 ||| stack-vs: stacked visual-semantic attention for image caption generation. ||| ling cheng ||| wei wei ||| xianling mao ||| yong liu ||| chunyan miao ||| 
2021 ||| adversarial networks with circular attention mechanism for fine-grained domain adaptation. ||| ningyu he ||| jie zhu ||| 
2020 ||| a novel end-to-end corporate credit rating model based on self-attention mechanism. ||| binbin chen ||| shengjie long ||| 
2021 ||| attentional behavior of children with asd in response to robotic agents. ||| faisal mehmood ||| hamed mahzoon ||| yuichiro yoshikawa ||| hiroshi ishiguro ||| haleema sadia ||| sara ali ||| yasar ayaz ||| 
2019 ||| two-level attention model based video action recognition network. ||| haifeng sang ||| ziyu zhao ||| dakuo he ||| 
2021 ||| targeted aspect-based multimodal sentiment analysis: an attention capsule extraction and multi-head fusion network. ||| donghong gu ||| jiaqian wang ||| shaohua cai ||| chi yang ||| zhengxin song ||| haoliang zhao ||| luwei xiao ||| hua wang ||| 
2019 ||| detecting hypernymy relations between medical compound entities using a hybrid-attention based bi-gru-capsnet model. ||| chenming xu ||| yangming zhou ||| qi wang ||| zhiyuan ma ||| yan zhu ||| 
2020 ||| conditionally learn to pay attention for sequential visual task. ||| jun he ||| quan-jie cao ||| lei zhang ||| hui tao ||| 
2020 ||| csanet: channel and spatial mixed attention cnn for pedestrian detection. ||| yunbo zhang ||| pengfei yi ||| dongsheng zhou ||| xin yang ||| deyun yang ||| qiang zhang ||| xiaopeng wei ||| 
2019 ||| a lightweight moving vehicle classification system through attention-based method and deep learning. ||| nasaruddin nasaruddin ||| kahlil muchtar ||| afdhal afdhal ||| 
2019 ||| enhancing attention-based lstm with position context for aspect-level sentiment classification. ||| jiangfeng zeng ||| xiao ma ||| ke zhou ||| 
2021 ||| discrete-state event-driven numerical prototyping of megawatt solid-state transformers and ac/dc hybrid microgrids. ||| bochen shi ||| shiqi ji ||| zhengming zhao ||| zhujun yu ||| 
2020 ||| attention-aware joint location constraint hashing for multi-label image retrieval. ||| yingqi zhang ||| yong feng ||| jiaxing shang ||| mingliang zhou ||| baohua qiang ||| 
2020 ||| metro passenger flow prediction model using attention-based neural network. ||| jun yang ||| xuchen dong ||| shangtai jin ||| 
2020 ||| object tracking based on channel attention. ||| zhiquan he ||| xuejun chen ||| 
2021 ||| study of the impregnation of power-transformer cellulosic materials with dielectric ester oils. ||| jaime sanz ||| oasis sancibri ||| n ||| cristian olmo ||| cristina m ||| ndez ||| alfredo ortiz ||| carlos j. renedo ||| 
2021 ||| fault diagnosis of oil-immersed power transformer based on difference-mutation brain storm optimized catboost model. ||| mei zhang ||| wanli chen ||| yu zhang ||| fei liu ||| dongshun yu ||| chaoyin zhang ||| li gao ||| 
2020 ||| self-attention-masking semantic decomposition and segmentation for facial attribute manipulation. ||| xuan xia ||| fengqi yu ||| nan li ||| yansong qu ||| jiajia zhang ||| chengguang zhu ||| 
2019 ||| attention-based denseunet network with adversarial training for skin lesion segmentation. ||| zenghui wei ||| hong song ||| lei chen ||| qiang li ||| guanghui han ||| 
2020 ||| multi-adversarial partial transfer learning with object-level attention mechanism for unsupervised remote sensing scene classification. ||| peng li ||| dezheng zhang ||| peng chen ||| xin liu ||| aziguli wulamu ||| 
2021 ||| regenerative active electronic load with current, voltage and frequency control for power transformer testing. ||| guilherme monteiro de rezende ||| matheus vieira de almeida ||| tiago de s |||  ferreira ||| clodualdo venicio de sousa ||| victor flores mendes ||| 
2020 ||| design and analysis of tension control system for transformer insulation layer winding. ||| haixiang huang ||| jiazhong xu ||| kewei sun ||| liwei deng ||| cheng huang ||| 
2021 ||| sca-net: a spatial and channel attention network for medical image segmentation. ||| tong shan ||| jiayong yan ||| 
2019 ||| improvements to the construction of bubble inception formulae for use with transformer insulation. ||| james p. hill ||| zhongdong wang ||| qiang liu ||| shanika matharage ||| andree hilker ||| david walker ||| 
2021 ||| efficient attention pyramid network for semantic segmentation. ||| qirui yang ||| tao ku ||| kunyuan hu ||| 
2018 ||| novel upper-limb rehabilitation system based on attention technology for post-stroke patients: a preliminary study. ||| bor-shing lin ||| jean-lon chen ||| hsiu-chi hsu ||| 
2020 ||| parameters identification and application of equivalent circuit at low frequency of oil-paper insulation in transformer. ||| yiming xie ||| jiangjun ruan ||| 
2019 ||| a new technique to estimate the degree of polymerization of insulation paper using multiple aging parameters of transformer oil. ||| shijun li ||| zhao ge ||| ahmed abu-siada ||| liuqing yang ||| shengtao li ||| kiyoshi wakimoto ||| 
2019 ||| aagan: enhanced single image dehazing with attention-to-attention generative adversarial network. ||| wenhui wang ||| anna wang ||| qing ai ||| chen liu ||| jinglu liu ||| 
2021 ||| moon impact crater detection using nested attention mechanism based unet++. ||| yutong jia ||| lei liu ||| chenyang zhang ||| 
2021 ||| transanomaly: video anomaly detection using video vision transformer. ||| hongchun yuan ||| zhenyu cai ||| hui zhou ||| yue wang ||| xiangzhi chen ||| 
2020 ||| load noise prediction of high-voltage transformers by equation applying 3-d emcn. ||| dae-kee kim ||| jun-yeol ryu ||| dong-min kim ||| do jin kim ||| myung-seop lim ||| 
2021 ||| millimeter wave path loss modeling for 5g communications using deep learning with dilated convolution and attention. ||| hong cheng ||| shengjie ma ||| hyukjoon lee ||| minsung cho ||| 
2022 ||| social media popularity prediction based on multi-modal self-attention mechanisms. ||| hung-hsiang lin ||| jiun-da lin ||| jose jaena mari ople ||| jun-cheng chen ||| kai-lung hua ||| 
2021 ||| a prediction model of hot spot temperature for split-windings traction transformer considering the load characteristics. ||| yiyi zhang ||| xingxiao wei ||| xianhao fan ||| ke wang ||| ran zhuo ||| wei zhang ||| shuo liang ||| jian hao ||| jiefeng liu ||| 
2020 ||| eye-contact game using mixed reality for the treatment of children with attention deficit hyperactivity disorder. ||| seongki kim ||| jinho ryu ||| youngchyul choi ||| yooseok kang ||| hongle li ||| kibum kim ||| 
2020 ||| three-phase flexible transformer based on bipolar direct ac/ac chopper and its control strategy. ||| yibo wang ||| guowei cai ||| chuang liu ||| bingda zhu ||| dongbo guo ||| hanwen zhang ||| 
2020 ||| a gated recurrent unit network model for predicting open channel flow in coal mines based on attention mechanisms. ||| zhanli li ||| tianyu gao ||| cheng guo ||| hong-an li ||| 
2019 ||| transformer incipient hybrid fault diagnosis based on solar-powered rfid sensor and optimized dbn approach. ||| tao wang ||| yigang he ||| tiancheng shi ||| bing li ||| 
2021 ||| a deep learning model based on bert and sentence transformer for semantic keyphrase extraction on big social data. ||| r. devika ||| subramaniyaswamy vairavasundaram ||| c. sakthi jay mahenthar ||| varadarajan vijayakumar ||| ketan kotecha ||| 
2022 ||| spatial attention guided residual attention network for hyperspectral image classification. ||| ningyang li ||| zhaohui wang ||| 
2020 ||| cephann: a multi-head attention network for cephalometric landmark detection. ||| jiahong qian ||| weizhi luo ||| ming cheng ||| yubo tao ||| jun lin ||| hai lin ||| 
2019 ||| the location of partial discharge sources inside power transformers based on tdoa database with uhf sensors. ||| naifan xue ||| junjie yang ||| daoyi shen ||| peng xu ||| kaixu yang ||| zhuhang zhuo ||| linnan zhang ||| jinshui zhang ||| 
2020 ||| global spatio-temporal attention for action recognition based on 3d human skeleton data. ||| yun han ||| sheng-luen chung ||| qiang xiao ||| wei-you lin ||| shun-feng su ||| 
2022 ||| super-resolution reconstruction of 3t-like images from 0.35t mri using a hybrid attention residual network. ||| jialiang jiang ||| fulang qi ||| huiyu du ||| jianan xu ||| yufu zhou ||| dayong gao ||| bensheng qiu ||| 
2019 ||| fca-net: adversarial learning for skin lesion segmentation based on multi-scale features and factorized channel attention. ||| vivek kumar singh ||| mohamed abdel-nasser ||| hatem a. rashwan ||| farhan akram ||| nidhi pandey ||| alain lalande ||| benoit presles ||| santiago romani ||| domenec puig ||| 
2020 ||| hierarchical attention-based fusion for image caption with multi-grained rewards. ||| chunlei wu ||| shaozu yuan ||| haiwen cao ||| yiwei wei ||| leiquan wang ||| 
2019 ||| co-attention network with question type for visual question answering. ||| chao yang ||| mengqi jiang ||| bin jiang ||| weixin zhou ||| keqin li ||| 
2021 ||| recipebowl: a cooking recommender for ingredients and recipes using set transformer. ||| keonwoo kim ||| donghyeon park ||| michael spranger ||| kana maruyama ||| jaewoo kang ||| 
2020 ||| multi-attention-based capsule network for uyghur personal pronouns resolution. ||| qimeng yang ||| long yu ||| shengwei tian ||| jinmiao song ||| 
2021 ||| experimental validation of a method of drying cellulose insulation in distribution transformers using circulating synthetic ester. ||| piotr przybylek ||| krzysztof walczak ||| wojciech sikorski ||| hanna moscicka-grzesiak ||| hubert moranda ||| mateusz cybulski ||| 
2020 ||| frequency domain spectroscopy for non-uniformly distributed moisture detection in transformer bushings. ||| quanmin dai ||| yanxia liu ||| guang cheng ||| 
2021 ||| tracking attention of social media event by hidden markov model-cases from sina weibo. ||| yinghong ma ||| hui jiao ||| le song ||| 
2019 ||| thermal modelling of a power transformer disc type winding immersed in mineral and ester-based oils using network models and cfd. ||| agustin santisteban ||| alejandro piquero ||| f ||| lix ortiz ||| fernando delgado ||| alfredo ortiz ||| 
2021 ||| research on variable frequency transformer: a smart power transmission technology. ||| mohd mohsin khan ||| asghar imdadullah ||| jamel nebhen ||| hafizur rahman ||| 
2021 ||| boosting inertial-based human activity recognition with transformers. ||| yoli shavit ||| itzik klein ||| 
2020 ||| a lexicon-enhanced attention network for aspect-level sentiment analysis. ||| zhiying ren ||| guangping zeng ||| liu chen ||| qingchuan zhang ||| chunguang zhang ||| dingqi pan ||| 
2021 ||| running status diagnosis of onboard traction transformers based on kernel principal component analysis and fuzzy clustering. ||| junmin zhu ||| shuaibing li ||| haiying dong ||| 
2021 ||| efficient attention fusion network in wavelet domain for demoireing. ||| chunyun sun ||| huicheng lai ||| liejun wang ||| zhenghong jia ||| 
2020 ||| inattentional blindness for redirected walking using dynamic foveated rendering. ||| yashas joshi ||| charalambos poullis ||| 
2020 ||| siamese cascaded region proposal networks with channel-interconnection-spatial attention for visual tracking. ||| zhoujuan cui ||| junshe an ||| qing ye ||| tianshu cui ||| 
2021 ||| an integrated transformer design with a center-core air-gap for dab converters. ||| eun s. lee ||| jin h. park ||| myung y. kim ||| seung h. han ||| 
2020 ||| enhanced visual attention-guided deep neural networks for image classification. ||| chia-hung yeh ||| min-hui lin ||| po-chao chang ||| li-wei kang ||| 
2022 ||| unrestricted attention may not be all you need-masked attention mechanism focuses better on relevant parts in aspect-based sentiment analysis. ||| ao feng ||| xuelei zhang ||| xinyu song ||| 
2021 ||| ragat: relation aware graph attention network for knowledge graph completion. ||| xiyang liu ||| huobin tan ||| qinghong chen ||| guangyan lin ||| 
2022 ||| lotr: face landmark localization using localization transformer. ||| ukrit watchareeruetai ||| benjaphan sommana ||| sanjana jain ||| pavit noinongyao ||| ankush ganguly ||| aubin samaco ||| ts ||| samuel w. f. earp ||| nakarin sritrakool ||| 
2019 ||| pedestrian heading estimation based on spatial transformer networks and hierarchical lstm. ||| qu wang ||| haiyong luo ||| langlang ye ||| aidong men ||| fang zhao ||| yan huang ||| changhai ou ||| 
2020 ||| channel-attention u-net: channel attention mechanism for semantic segmentation of esophagus and esophageal cancer. ||| guoheng huang ||| junwen zhu ||| jiajian li ||| zhuowei wang ||| lianglun cheng ||| lizhi liu ||| haojiang li ||| jian zhou ||| 
2021 ||| effects of spike voltages coupling with high dv/dt square wave on dielectric loss and electric-thermal field of high-frequency transformer. ||| weiwang wang ||| jiefeng he ||| ying liu ||| xin wang ||| shengtao li ||| 
2020 ||| an improved loss-separation method for transformer core loss calculation and its experimental verification. ||| huiqi li ||| lin wang ||| jun li ||| junjie zhang ||| 
2021 ||| study on operation parameter characteristics of induction filter distribution transformer in low-voltage distribution network. ||| xiao zhang ||| changyi li ||| delu li ||| su jiang ||| 
2019 ||| hybrid-frequency cascaded full-bridge solid-state transformer. ||| xueyin zhang ||| yonghai xu ||| yunbo long ||| shaobo xu ||| abubakar siddique ||| 
2020 ||| towards understanding attention-based speech recognition models. ||| chu-xiong qin ||| dan qu ||| 
2019 ||| infrared and visible image fusion using detail enhanced channel attention network. ||| yinghan cui ||| huiqian du ||| wenbo mei ||| 
2020 ||| feature pyramid attention model and multi-label focal loss for pedestrian attribute recognition. ||| ye li ||| fangyan shi ||| shaoqi hou ||| jipeng li ||| chao li ||| guangqiang yin ||| 
2020 ||| a data mining approach for transformer failure rate modeling based on daily oil chromatographic data. ||| wei huang ||| xuan li ||| bo hu ||| jiahao yan ||| lvbing peng ||| yue sun ||| xin cheng ||| jinfeng ding ||| kaigui xie ||| qinglong liao ||| lingyun wan ||| 
2019 ||| digital image steganalysis based on visual attention and deep reinforcement learning. ||| donghui hu ||| shengnan zhou ||| qiang shen ||| shuli zheng ||| zhongqiu zhao ||| yuqi fan ||| 
2021 ||| amr-net: arbitrary-oriented ship detection using attention module, multi-scale feature fusion and rotation pseudo-label. ||| yifan wu ||| wei zhao ||| rufei zhang ||| feng jiang ||| 
2019 ||| sequential image-based attention network for inferring force estimation without haptic sensor. ||| hochul shin ||| hyeon cho ||| dongyi kim ||| daekwan ko ||| soo-chul lim ||| wonjun hwang ||| 
2020 ||| sequence generation network based on hierarchical attention for multi-charge prediction. ||| kongfan zhu ||| baosen ma ||| tianhuan huang ||| zeqiang li ||| haoyang ma ||| yujun li ||| 
2020 ||| dielectric performance of magneto-nanofluids for advancing oil-immersed power transformer. ||| md rashid hussain ||| qasim khan ||| asfar ali khan ||| shady s. refaat ||| haitham abu-rub ||| 
2020 ||| a remote-sensing image pan-sharpening method based on multi-scale channel attention residual network. ||| xin li ||| feng xu ||| xin lyu ||| yao tong ||| ziqi chen ||| shengyang li ||| daofang liu ||| 
2022 ||| joint learning with bert-gcn and multi-attention for event text classification and event assignment. ||| xiangrong she ||| jianpeng chen ||| gang chen ||| 
2020 ||| a deep multi-attention driven approach for multi-label remote sensing image classification. ||| gencer sumbul ||| beg ||| m demir ||| 
2017 ||| 3d cnn based automatic diagnosis of attention deficit hyperactivity disorder using functional and structural mri. ||| liang zou ||| jiannan zheng ||| chunyan miao ||| martin j. mckeown ||| z. jane wang ||| 
2019 ||| temperature analysis in power transformer windings using created artificial bee algorithm and computer program. ||| mehmet zile ||| 
2020 ||| attention guided u-net with atrous convolution for accurate retinal vessels segmentation. ||| yan lv ||| hui ma ||| jianian li ||| shuangcai liu ||| 
2021 ||| estimation of life cycle of distribution transformer in context to furan content formation, pollution index, and dielectric strength. ||| rajkumar soni ||| prasun chakrabarti ||| zbigniew leonowicz ||| michal jasi ||| ski ||| krzysztof wieczorek ||| vadim bolshev ||| 
2020 ||| improving graph convolutional networks based on relation-aware attention for end-to-end relation extraction. ||| yin hong ||| yanxia liu ||| suizhu yang ||| kaiwen zhang ||| aiqing wen ||| jianjun hu ||| 
2021 ||| the nlp cookbook: modern recipes for transformer based deep learning architectures. ||| sushant singh ||| ausif mahmood ||| 
2020 ||| multiple attention network for facial expression recognition. ||| yanling gan ||| jingying chen ||| zongkai yang ||| luhui xu ||| 
2020 ||| bdars_capsnet: bi-directional attention routing sausage capsule network. ||| xin ning ||| weijuan tian ||| weijun li ||| yueyue lu ||| shuai nie ||| linjun sun ||| ziheng chen ||| 
2021 ||| multi-head attentional point cloud classification and segmentation using strictly rotation-invariant representations. ||| zhiyong tao ||| yixin zhu ||| tong wei ||| sen lin ||| 
2021 ||| a deep learning approach for robust detection of bots in twitter using transformers. ||| david mart ||| n-guti ||| rrez ||| gustavo hern ||| ndez-pe ||| aloza ||| alberto belmonte-hern ||| ndez ||| alicia lozano-diez ||| federico  ||| lvarez ||| 
2021 ||| sentiment analysis of review text based on bigru-attention and hybrid cnn. ||| qiannan zhu ||| xiaofan jiang ||| renzhen ye ||| 
2021 ||| hunt for unseen intrusion: multi-head self-attention neural detector. ||| seongyun seo ||| sungmin han ||| janghyeon park ||| shin-woo shim ||| han-eul ryu ||| byoungmo cho ||| sangkyun lee ||| 
2020 ||| long short-term memory with attention mechanism for state of charge estimation of lithium-ion batteries. ||| tadele mamo ||| fu-kwun wang ||| 
2019 ||| video captioning with adaptive attention and mixed loss optimization. ||| huanhou xiao ||| jinglun shi ||| 
2021 ||| pa-mvsnet: sparse-to-dense multi-view stereo with pyramid attention. ||| ke zhang ||| mengyu liu ||| jinlai zhang ||| zhenbiao dong ||| 
2022 ||| multi-modality reconstruction attention and difference enhancement network for brain mri image segmentation. ||| xiangfen zhang ||| yan liu ||| qingyi zhang ||| feiniu yuan ||| 
2020 ||| coupled rain streak and background estimation via separable element-wise attention. ||| yinjie tan ||| qiang wen ||| jing qin ||| jianbo jiao ||| guoqiang han ||| shengfeng he ||| 
2021 ||| forecasting copper electrorefining cathode rejection by means of recurrent neural networks with attention mechanism. ||| pedro pablo correa ||| aldo cipriano ||| felipe n ||| ez ||| juan carlos salas ||| hans lobel ||| 
2021 ||| multi-head self-attention for 3d point cloud classification. ||| xue-yao gao ||| yan-zhao wang ||| chun-xiang zhang ||| jia-qi lu ||| 
2018 ||| a modified single-phase transformerless y-source pv grid-connected inverter. ||| hongpeng liu ||| yan ran ||| kuan liu ||| wei wang ||| dianguo xu ||| 
2021 ||| enhancements of attention-based bidirectional lstm for hybrid automatic text summarization. ||| jiawen jiang ||| haiyang zhang ||| chenxu dai ||| qingjuan zhao ||| hao feng ||| zhanlin ji ||| ivan ganchev ||| 
2020 ||| pgcn-tca: pseudo graph convolutional network with temporal and channel-wise attention for skeleton-based action recognition. ||| hongye yang ||| yuzhang gu ||| jianchao  ||| zhu ||| keli hu ||| xiaolin zhang ||| 
2020 ||| improving the performance of convolutional neural network for the segmentation of optic disc in fundus images using attention gates and conditional random fields. ||| bhargav j. bhatkalkar ||| dheeraj r. reddy ||| srikanth prabhu ||| sulatha v. bhandary ||| 
2021 ||| sentiment analysis using multi-head attention capsules with multi-channel cnn and bidirectional gru. ||| yan cheng ||| huan sun ||| haomai chen ||| meng li ||| yingying cai ||| zhuang cai ||| jing huang ||| 
2020 ||| the performance analysis of signal recognition using attention based cnn method. ||| zan yin ||| bo chen ||| weimin zhen ||| chaojie wang ||| ting zhang ||| 
2021 ||| enhancing diagnostic accuracy of transformer faults using teaching-learning-based optimization. ||| sherif s. m. ghoneim ||| karar mahmoud ||| matti lehtonen ||| mohamed m. f. darwish ||| 
2021 ||| controllable and editable neural story plot generation via control-and-edit transformer. ||| jin chen ||| guangyi xiao ||| xu han ||| hao chen ||| 
2020 ||| self-attention network with joint loss for remote sensing image scene classification. ||| honglin wu ||| shuzhen zhao ||| liang li ||| chaoquan lu ||| wen chen ||| 
2019 ||| calculation of hot spot temperature of transformer bushing considering current fluctuation. ||| zhifei yang ||| jiangjun ruan ||| daochun huang ||| zhiye du ||| liezheng tang ||| taotao zhou ||| 
2019 ||| bi-level attention model for sentiment analysis of short texts. ||| wei liu ||| guoxi cao ||| jianqin yin ||| 
2019 ||| topological transient models of three-phase, three-legged transformer. ||| jianhui zhao ||| sergey e. zirka ||| yuriy i. moroz ||| cesare m. arturi ||| reigh a. walling ||| nasser tleis ||| olexandr l. tarchutkin ||| 
2020 ||| extended application for the impulse-based frequency response analysis: preliminary diagnosis of partial discharges in transformer. ||| kamalaselvan arunachalam ||| balasubramanian madanmohan ||| rajesh rajamani ||| 
2018 ||| understanding multimodal popularity prediction of social media videos with self-attention. ||| adam bielski ||| tomasz trzcinski ||| 
2021 ||| highly efficient hbt power amplifier using high-q single- and two-winding transformer with imd3 cancellation. ||| hyunjin ahn ||| kyutaek oh ||| ilku nam ||| ockgoo lee ||| 
2020 ||| deep attention and multi-scale networks for accurate remote sensing image segmentation. ||| xingqun qi ||| kaiqi li ||| pengkun liu ||| xiaoguang zhou ||| muyi sun ||| 
2020 ||| a fusion model-based label embedding and self-interaction attention for text classification. ||| yanru dong ||| peiyu liu ||| zhenfang zhu ||| qicai wang ||| qiuyue zhang ||| 
2020 ||| optimal area-product model (oapm) based non-iterative analytical design methodology for litz-wired high-frequency gapped- transformer (lhfgt) in llc converters. ||| daniyal ahmed ||| li wang ||| 
2020 ||| rating prediction based on merge-cnn and concise attention review mining. ||| yun-cheng chou ||| hsing-yu chen ||| duen-ren liu ||| der-shiuan chang ||| 
2021 ||| short text sentiment analysis based on multi-channel cnn with multi-head attention mechanism. ||| yue feng ||| yan cheng ||| 
2019 ||| theory and design of impedance matching network utilizing a lossy on-chip transformer. ||| van-son trinh ||| jung-dong park ||| 
2019 ||| attention embedded spatio-temporal network for video salient object detection. ||| lili huang ||| pengxiang yan ||| guanbin li ||| qing wang ||| liang lin ||| 
2021 ||| accuracy improvement of power transformer faults diagnostic using knn classifier with decision tree principle. ||| omar kherif ||| youcef benmahamed ||| madjid teguar ||| ahmed boubakeur ||| sherif s. m. ghoneim ||| 
2020 ||| a methodology for diagnosing faults in oil-immersed power transformers based on minimizing the maintenance cost. ||| ming-jong lin ||| liang-bi chen ||| chao-tang yu ||| 
2020 ||| multi-level context aggregation network with channel-wise attention for salient object detection. ||| zihui jia ||| zhenyu weng ||| fang wan ||| yuesheng zhu ||| 
2019 ||| exploiting transfer learning with attention for in-domain top-n recommendation. ||| ke-jia chen ||| hui zhang ||| 
2021 ||| single-shot detection based on cyclic attention. ||| kebai hu ||| daochun xu ||| jiangming kan ||| 
2020 ||| multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network. ||| ngoc-huynh ho ||| hyung-jeong yang ||| soo-hyung kim ||| gueesang lee ||| 
2019 ||| electro-thermal fault diagnosis method of rapo vegetable oil transformer based on characteristic gas and ratio criterion. ||| haoxi cong ||| shiyue du ||| qingmin li ||| jianben liu ||| 
2019 ||| recurrent models of visual co-attention for person re-identification. ||| lan lin ||| huan luo ||| renjie huang ||| mao ye ||| 
2019 ||| recent progress and challenges in transformer oil nanofluid development: a review on thermal and electrical properties. ||| danial amin ||| rashmi walvekar ||| mohammad khalid ||| mahesh vaka ||| nabisab mubarak mujawar ||| t. c. s. m. gupta ||| 
2021 ||| metric-based attention feature learning for video action recognition. ||| dae ha kim ||| fazliddin anvarov ||| jun min lee ||| byung cheol song ||| 
2019 ||| identification and location of pd defects in medium voltage underground power cables using high frequency current transformer. ||| muhammad shafiq ||| ivar kiitam ||| paul taklaja ||| lauri k ||| tt ||| kimmo kauhaniemi ||| ivo palu ||| 
2021 ||| trilateral attention network for real-time cardiac region segmentation. ||| ghada zamzmi ||| sivaramakrishnan rajaraman ||| vandana sachdev ||| sameer k. antani ||| 
2021 ||| person gender classification on rgb-d data with self-joint attention. ||| xiaoxiong zhang ||| sajid javed ||| jorge dias ||| naoufel werghi ||| 
2020 ||| thai spelling correction and word normalization on social text using a two-stage pipeline with neural contextual attention. ||| anuruth lertpiya ||| tawunrat chalothorn ||| ekapol chuangsuwanich ||| 
2021 ||| evaluating the inter-resonance characteristics of various power transformer winding designs. ||| sriyono ||| umar khayam ||| suwarno ||| 
2021 ||| single-image snow removal based on an attention mechanism and a generative adversarial network. ||| aiwen jia ||| zhenhong jia ||| jie yang ||| nikola k. kasabov ||| 
2019 ||| generating emotional controllable response based on multi-task and dual attention framework. ||| weiran xu ||| xiusen gu ||| guang chen ||| 
2019 ||| performance analysis and threshold quantization of transformer differential protection under sampled value packets loss/delay. ||| ruiwen he ||| hao peng ||| qingyun jiang ||| lei zhou ||| jianxin zhu ||| 
2020 ||| first-principles calculations of gas-sensing properties of pd clusters decorated alnnts to dissolved gases in transformer oil. ||| lingna xu ||| hanshan zhu ||| yingang gui ||| yingkai long ||| qian wang ||| pingan yang ||| 
2019 ||| multi-scale context attention network for stereo matching. ||| haiwei sang ||| quanhong wang ||| yong zhao ||| 
2020 ||| multi-element hierarchical attention capsule network for stock prediction. ||| jintao liu ||| hongfei lin ||| liang yang ||| bo xu ||| dongzhen wen ||| 
2020 ||| semantic segmentation of marine remote sensing based on a cross direction attention mechanism. ||| hao gao ||| lin cao ||| dingfeng yu ||| xuejun xiong ||| maoyong cao ||| 
2021 ||| complex-valued channel attention and application in ego-velocity estimation with automotive radar. ||| hyun-woong cho ||| sungdo choi ||| young-rae cho ||| jongseok kim ||| 
2020 ||| constrained image splicing detection and localization with attention-aware encoder-decoder and atrous convolution. ||| yaqi liu ||| xianfeng zhao ||| 
2019 ||| system-level efficiency evaluation of isolated dc/dc converters in power electronics transformers for medium-voltage dc systems. ||| you wang ||| kui wang ||| chi li ||| zedong zheng ||| yongdong li ||| 
2021 ||| two-stage attention over lstm with bayesian optimization for day-ahead solar power forecasting. ||| muhammad aslam ||| seung-jae lee ||| sang-hee khang ||| sugwon hong ||| 
2020 ||| a discriminative person re-identification model with global-local attention and adaptive weighted rank list loss. ||| yongchang gong ||| liejun wang ||| yongming li ||| anyu du ||| 
2019 ||| channel attention networks for image translation. ||| sun song ||| bo zhao ||| xin chen ||| muhammad mateen ||| junhao wen ||| 
2021 ||| active vision-based attention monitoring system for non-distracted driving. ||| lamia alam ||| mohammed moshiul hoque ||| m. ali akber dewan ||| nazmul h. siddique ||| i ||| aki ra ||| iqbal h. sarker ||| 
2021 ||| study on the sound radiation efficiency of a typical distribution transformer. ||| yuxing wang ||| huihui jin ||| xuan cai ||| peijun gong ||| xishan jiang ||| 
2019 ||| automatic pavement crack detection and classification using multiscale feature attention network. ||| weidong song ||| guohui jia ||| di jia ||| hong zhu ||| 
2020 ||| roi-attention vectorized cnn model for static facial expression recognition. ||| xiao sun ||| shixin zheng ||| hongshuai fu ||| 
2022 ||| classification of diabetic retinopathy severity based on gca attention mechanism. ||| binhua yang ||| tongyan li ||| haidi xie ||| yulin liao ||| yi-ping phoebe chen ||| 
2020 ||| enhanced multi-channel feature synthesis for hand gesture recognition based on cnn with a channel and spatial attention mechanism. ||| chuan du ||| lei zhang ||| xiping sun ||| junxu wang ||| jialian sheng ||| 
2020 ||| sentiment analysis model based on self-attention and character-level embedding. ||| hongbin xia ||| chenhui ding ||| yuan liu ||| 
2021 ||| joint image dehazing and super-resolution: closed shared source residual attention fusion network. ||| zhuoyuan yang ||| da pan ||| ping shi ||| 
2019 ||| self residual attention network for deep face recognition. ||| hefei ling ||| jiyang wu ||| lei wu ||| junrui huang ||| jiazhong chen ||| ping li ||| 
2021 ||| boundary-adaptive encoder with attention method for chinese sign language recognition. ||| shiliang huang ||| zhongfu ye ||| 
2020 ||| multiple features fusion attention mechanism enhanced deep knowledge tracing for student performance prediction. ||| dong liu ||| yunping zhang ||| jun zhang ||| qinpeng li ||| congpin zhang ||| yu yin ||| 
2022 ||| frequency characteristics of power transformer for isolated dc-dc converter. ||| takayuki nakamura ||| toshiyuki murakami ||| 
2019 ||| deep attention-guided hashing. ||| zhan yang ||| osolo ian raymond ||| wuqing sun ||| jun long ||| 
2021 ||| a novel deep learning-based multilevel parallel attention neural (mpan) model for multidomain arabic sentiment analysis. ||| mohammed ahmed el-affendi ||| khawla alrajhi ||| amir hussain ||| 
2020 ||| attention gate resu-net for automatic mri brain tumor segmentation. ||| jianxin zhang ||| zongkang jiang ||| jing dong ||| yaqing hou ||| bin liu ||| 
2019 ||| marking key segment of program input via attention mechanism. ||| xing zhang ||| chao feng ||| runhao li ||| jing lei ||| chaojing tang ||| 
2019 ||| to identify tree species with highly similar leaves based on a novel attention mechanism for cnn. ||| yupeng song ||| fazhi he ||| xiying zhang ||| 
2021 ||| a spam transformer model for sms spam detection. ||| xiaoxu liu ||| haoye lu ||| amiya nayak ||| 
2020 ||| power of attention in mooc dropout prediction. ||| shengjun yin ||| leqi lei ||| hongzhi wang ||| wentao chen ||| 
2019 ||| a magnetic integration half-turn planar transformer and its analysis for llc resonant dc-dc converters. ||| siqi li ||| qingyun min ||| enguo rong ||| rui zhang ||| xiao du ||| sizhao lu ||| 
2019 ||| unsupervised region attention network for person re-identification. ||| chenrui zhang ||| yangxu wu ||| tao lei ||| 
2020 ||| attention-block deep learning based features fusion in wearable social sensor for mental wellbeing evaluations. ||| jikun jin ||| bin gao ||| sihao yang ||| bingmei zhao ||| lizhu luo ||| wai lok woo ||| 
2018 ||| an attention-based word-level interaction model for knowledge base relation detection. ||| hongzhi zhang ||| guandong xu ||| xiao liang ||| guangluan xu ||| feng li ||| kun fu ||| lei wang ||| tinglei huang ||| 
2020 ||| monitoring method on loosened state and deformational fault of transformer winding based on vibration and reactance information. ||| chen cao ||| bowen xu ||| xuebin li ||| 
2021 ||| deep spatiotemporal attention network for fine particle matter 2.5 concentration prediction with causality analysis. ||| nohyoon seong ||| 
2018 ||| optimal wind farm cable routing: modeling branches and offshore transformer modules. ||| martina fischetti ||| david pisinger ||| 
2021 ||| power transformer fault diagnosis system based on internet of things. ||| guoshi wang ||| ying liu ||| xiaowen chen ||| qing yan ||| haibin sui ||| chao ma ||| junfei zhang ||| 
2021 ||| network security situation prediction based on feature separation and dual attention mechanism. ||| zhijian li ||| dongmei zhao ||| xinghua li ||| hongbin zhang ||| 
2020 ||| few-shot relation classification by context attention-based prototypical networks with bert. ||| bei hui ||| liang liu ||| jia chen ||| xue zhou ||| yuhui nian ||| 
2021 ||| three-phase distribution transformer connections modeling based on matrix operation method by phase-coordinates. ||| zhigang zhang ||| mingrui mo ||| caizhu wu ||| 
2019 ||| a novel approach to workload prediction using attention-based lstm encoder-decoder network in cloud environment. ||| yonghua zhu ||| weilin zhang ||| yihai chen ||| honghao gao ||| 
2021 ||| spatio-temporal self-attention weighted vlad neural network for action recognition. ||| shilei cheng ||| mei xie ||| zheng ma ||| siqi li ||| song gu ||| feng yang ||| 
2021 ||| neural architecture search for convolutional neural networks with attention. ||| kohei nakai ||| takashi matsubara ||| kuniaki uehara ||| 
2021 ||| haif: a hierarchical attention-based model of filtering invalid webpage. ||| chaoran zhou ||| jianping zhao ||| tai ma ||| xin zhou ||| 
2020 ||| sentence-embedding and similarity via hybrid bidirectional-lstm and cnn utilizing weighted-pooling attention. ||| degen huang ||| anil ahmed ||| syed yasser arafat ||| khawaja iftekhar rashid ||| qasim abbas ||| fuji ren ||| 
2020 ||| an attention-based gru network for anomaly detection from system logs. ||| yixi xie ||| lixin ji ||| xiaotao cheng ||| 
2018 ||| saliency priority of individual bottom-up attributes in designing visual attention models. ||| jila hosseinkhani ||| chris joslin ||| 
2022 ||| ahrnn: attention-based hybrid robust neural network for emotion recognition. ||| ke xu ||| bin liu ||| jianhua tao ||| zhao lv ||| cunhang fan ||| leichao song ||| 
2017 ||| a role for attentional reorienting during approximate multiplication and division. ||| curren katz ||| hannes hoesterey ||| andr |||  knops ||| 
2017 ||| dividing attention increases operational momentum. ||| koleen mccrink ||| timothy hubbard ||| 
2022 ||| graph attention spatial-temporal network with collaborative global-local learning for citywide mobile traffic prediction. ||| kaiwen he ||| xu chen ||| qiong wu ||| shuai yu ||| zhi zhou ||| 
2019 ||| wifi csi based passive human activity recognition using attention based blstm. ||| zhenghua chen ||| le zhang ||| chaoyang jiang ||| zhiguang cao ||| wei cui ||| 
2022 ||| attention-based gait recognition and walking direction estimation in wi-fi networks. ||| yang xu ||| wei yang ||| min chen ||| sheng chen ||| liusheng huang ||| 
2020 ||| a transformer-based model for multi-track music generation. ||| cong jin ||| tao wang ||| shouxun liu ||| yun tie ||| jianguang li ||| xiaobing li ||| simon lui ||| 
2020 ||| attention-based multimodal neural network for automatic evaluation of press conferences. ||| shengzhou yi ||| koshiro mochitomi ||| isao suzuki ||| xueting wang ||| toshihiko yamasaki ||| 
2020 ||| evolutionary large-scale multiobjective optimization for ratio error estimation of voltage transformers. ||| cheng he ||| ran cheng ||| chuanji zhang ||| ye tian ||| qin chen ||| xin yao ||| 
2021 ||| efficient evolutionary search of attention convolutional networks via sampled training and node inheritance. ||| haoyu zhang ||| yaochu jin ||| ran cheng ||| kuangrong hao ||| 
2021 ||| robust multimodal representation learning with evolutionary adversarial attention networks. ||| feiran huang ||| alireza jolfaei ||| ali kashif bashir ||| 
2021 ||| the impact of increase in minimum wages on consumer perceptions of service: a transformer model of online restaurant reviews. ||| dinesh puranam ||| vrinda kadiyali ||| vishal narayan ||| 
2017 ||| prominent attributes under limited attention. ||| yi zhu ||| anthony j. dukes ||| 
2021 ||| measuring competition for attention in social media: national women's soccer league players on twitter. ||| federico rossi ||| gaia rubera ||| 
2020 ||| a new time-frequency attention tensor network for language identification. ||| xiaoxiao miao ||| ian mcloughlin ||| yonghong yan ||| 
2019 ||| a 190 ghz vco with transformer-based push-push frequency doubler in 40 nm cmos. ||| yibo liu ||| luhong mao ||| sheng xie ||| baoyong chi ||| 
2020 ||| synthetic transformer design using commercially available active components. ||| mehmet dogan ||| erkan y ||| ce ||| shahram minaei ||| mehmet sagbas ||| 
2020 ||| attention and feature selection for automatic speech emotion recognition using utterance and syllable-level prosodic features. ||| starlet ben alex ||| leena mary ||| ben p. babu ||| 
2021 ||| a dual-band transformer-coupled notch filter mixer for 2.45-/5.2-ghz wlan application. ||| yoke theng wong ||| harikrishnan ramiah ||| nandini vitee ||| 
2021 ||| classification of flower image based on attention mechanism and multi-loss attention network. ||| mei zhang ||| huihui su ||| jinghua wen ||| 
2019 ||| removal notice to "equipping recurrent neural network with cnn-style attention mechanisms for sentiment analysis of network reviews" [comput. commun. (2019) 98-106]. ||| mohd usama ||| belal ahmad ||| jun yang ||| saqib qamar ||| parvez ahmad ||| yu zhang ||| jing lv ||| joze guna ||| 
2019 ||| removed: equipping recurrent neural network with cnn-style attention mechanisms for sentiment analysis of network reviews. ||| 
2022 ||| attention-based federated incremental learning for traffic classification in the internet of things. ||| meng-yuan zhu ||| zhuo chen ||| kefan chen ||| na lv ||| yun zhong ||| 
2021 ||| mask-rcnn with spatial attention for pedestrian segmentation in cyber-physical systems. ||| lin yuan ||| zhao qiu ||| 
2022 ||| face detection algorithm based on improved tinyyolov3 and attention mechanism. ||| jiangjin gao ||| tao yang ||| 
2021 ||| transformer text recognition with deep learning algorithm. ||| ye chen ||| hongchun shu ||| wenjiao xu ||| zhengyu yang ||| zhihu hong ||| mingshuai dong ||| 
2021 ||| fine-grained semantic image synthesis with object-attention generative adversarial network. ||| min wang ||| congyan lang ||| liqian liang ||| songhe feng ||| tao wang ||| yutong gao ||| 
2021 ||| temporal hierarchical graph attention network for traffic prediction. ||| ling huang ||| xing-xing liu ||| shu-qiang huang ||| chang-dong wang ||| wei tu ||| jia-meng xie ||| shuai tang ||| wendi xie ||| 
2018 ||| concept and attention-based cnn for question retrieval in multi-view learning. ||| pengwei wang ||| lei ji ||| jun yan ||| dejing dou ||| nisansa de silva ||| yong zhang ||| lianwen jin ||| 
2018 ||| characterizing user skills from application usage traces with hierarchical attention recurrent networks. ||| longqi yang ||| chen fang ||| hailin jin ||| matthew d. hoffman ||| deborah estrin ||| 
2020 ||| domain-attention conditional wasserstein distance for multi-source domain adaptation. ||| hanrui wu ||| yuguang yan ||| michael k. ng ||| qingyao wu ||| 
2020 ||| an attention-based rumor detection model with tree-structured recursive neural networks. ||| jing ma ||| wei gao ||| shafiq r. joty ||| kam-fai wong ||| 
2021 ||| gtae: graph transformer-based auto-encoders for linguistic-constrained text style transfer. ||| yukai shi ||| sen zhang ||| chenxing zhou ||| xiaodan liang ||| xiaojun yang ||| liang lin ||| 
2021 ||| mvgan: multi-view graph attention network for social event detection. ||| wanqiu cui ||| junping du ||| dawei wang ||| feifei kou ||| zhe xue ||| 
2021 ||| an attentive survey of attention models. ||| sneha chaudhari ||| varun mithal ||| gungor polatkan ||| rohan ramanath ||| 
2021 ||| multi-stage fusion and multi-source attention network for multi-modal remote sensing image segmentation. ||| jiaqi zhao ||| yong zhou ||| boyu shi ||| jingsong yang ||| di zhang ||| rui yao ||| 
2018 ||| mining significant microblogs for misinformation identification: an attention-based approach. ||| qiang liu ||| feng yu ||| shu wu ||| liang wang ||| 
2020 ||| a discriminative convolutional neural network with context-aware attention. ||| yuxiang zhou ||| lejian liao ||| yang gao ||| heyan huang ||| xiaochi wei ||| 
2018 ||| opposing effects of memory-driven and stimulus-driven attention on distractor perception. ||| suk won han ||| 
2020 ||| examining the roles of working memory and visual attention in multiple object tracking expertise. ||| david j. harris ||| mark r. wilson ||| emily m. crowe ||| samuel j. vine ||| 
2017 ||| the role of stimulus predictability in the allocation of attentional resources: an eye-tracking study. ||| magdalena kr ||| l ||| magdalena kilan-banach ||| renata strzelecka ||| 
2017 ||| the effects of multiphasic prepulses on automatic and attention-modulated prepulse inhibition. ||| albert b. poje ||| diane l. filion ||| 
2017 ||| erratum to: the effects of multiphasic prepulses on automatic and attention-modulated prepulse inhibition. ||| albert b. poje ||| diane l. filion ||| 
2019 ||| effects of attentional behaviours on infant visual preferences and object choice. ||| mitsuhiko ishikawa ||| mina yoshimura ||| hiroki sato ||| shoji itakura ||| 
2022 ||| do attentional focus instructions affect real-time reinvestment during level-ground walking in older adults? ||| toby c. t. mak ||| thomson w. l. wong ||| 
2017 ||| attention to body-parts varies with visual preference and verb-effector associations. ||| ty w. boyer ||| josita maouene ||| nitya sethuraman ||| 
2017 ||| a computational framework for attentional object discovery in rgb-d videos. ||| germ ||| n mart ||| n garc ||| a ||| mircea pavel ||| simone frintrop ||| 
2020 ||| cue-target onset asynchrony modulates interaction between exogenous attention and audiovisual integration. ||| zhihan xu ||| weiping yang ||| zhenhua zhou ||| yanna ren ||| 
2017 ||| dysfunctional personality traits in adolescence: effects on alerting, orienting and executive control of attention. ||| maria casagrande ||| andrea marotta ||| valeria canepone ||| alfredo spagna ||| caterina rosa ||| giancarlo dimaggio ||| augusto pasini ||| 
2019 ||| climate change images produce an attentional bias associated with pro-environmental disposition. ||| joshua m. carlson ||| betsy r. lehman ||| jessica leigh thompson ||| 
2020 ||| assessing orienting of attention to understand the time course of mental calculation. ||| stefania d'ascenzo ||| luisa lugli ||| roberto nicoletti ||| martin h. fischer ||| 
2017 ||| flow and quiet eye: the role of attentional control in flow experience. ||| david j. harris ||| samuel j. vine ||| mark r. wilson ||| 
2021 ||| hungry for colours? attentional bias for food crucially depends on perceptual information. ||| claudia del gatto ||| allegra indraccolo ||| claudio imperatori ||| riccardo brunetti ||| 
2017 ||| the appeal of the devil's eye: social evaluation affects social attention. ||| luciana carraro ||| mario dalmaso ||| luigi castelli ||| giovanni galfano ||| andrea bobbio ||| gabriele mantovani ||| 
2020 ||| prediction diversity and selective attention in the wisdom of crowds. ||| davi a. nobre ||| jos |||  f. fontanari ||| 
2021 ||| the online attention game for digital identity education: an exploratory study. ||| tadao obana ||| miha takubo ||| yohko orito ||| kiyoshi murata ||| hidenobu sai ||| tadayuki okamoto ||| 
2017 ||| competition for attention in the digital age: the case of single releases in the recorded music industry. ||| christian essling ||| johannes koenen ||| christian peukert ||| 
2021 ||| multi-field synergy manipulating soft polymeric hydrogel transformers. ||| dachuan zhang ||| jiawei zhang ||| yukun jian ||| baoyi wu ||| huizhen yan ||| huanhuan lu ||| shuxin wei ||| si wu ||| qunji xue ||| tao chen ||| 
2021 ||| an end-to-end framework for remaining useful life prediction of rolling bearing based on feature pre-extraction mechanism and deep adaptive transformer model. ||| xuanyuan su ||| hongmei liu ||| laifa tao ||| chen lu ||| mingliang suo ||| 
2021 ||| biomedical ontology matching through attention-based bidirectional long short-term memory network. ||| xingsi xue ||| chao jiang ||| jie zhang ||| cong hu ||| 
2021 ||| an improved steganography without embedding based on attention gan. ||| cong yu ||| donghui hu ||| shuli zheng ||| wenjie jiang ||| meng li ||| zhong-qiu zhao ||| 
2021 ||| from edge data to recommendation: a double attention-based deformable convolutional network. ||| zhe li ||| honglong chen ||| kai lin ||| vladimir v. shakhov ||| leyi shi ||| jiguo yu ||| 
2020 ||| a hybrid recommender method based on multiple dimension attention analysis. ||| minghu wu ||| songnan lv ||| chunyan zeng ||| zhifeng wang ||| nan zhao ||| li zhu ||| juan wang ||| ming wu ||| 
2022 ||| detection of spam reviews through a hierarchical attention architecture with n-gram cnn and bi-lstm. ||| yuxin liu ||| li wang ||| tengfei shi ||| jinyan li ||| 
2022 ||| multi-label legal document classification: a deep learning-based approach with label-attention and domain-specific pre-training. ||| dezhao song ||| andrew vold ||| kanika madan ||| frank schilder ||| 
2019 ||| an attention-augmented deep architecture for hard drive status monitoring in large-scale storage systems. ||| ji wang ||| weidong bao ||| lei zheng ||| xiaomin zhu ||| philip s. yu ||| 
2019 ||| anthropomorphisms in multimedia learning: attract attention but do not enhance learning? ||| tereza st ||| rkov ||| jir |||  lukavsk ||| ondrej javora ||| cyril brom ||| 
2021 ||| learning immunology in a game: learning outcomes, the use of player characters, immersion experiences and visual attention distributions. ||| yuan-yu teng ||| wen-chi chou ||| meng-tzu cheng ||| 
2020 ||| effects of prior knowledge and joint attention on learning from eye movement modelling examples. ||| lucia b. chisari ||| akvile mockeviciute ||| sterre k. ruitenburg ||| lian van vemde ||| ellen m. kok ||| tamara van gog ||| 
2021 ||| using eye movement modelling examples to guide visual attention and foster cognitive performance: a meta-analysis. ||| heping xie ||| tingting zhao ||| sue deng ||| ji peng ||| fuxing wang ||| zongkui zhou ||| 
2017 ||| the effects of cognitive capacity and gaming expertise on attention and comprehension. ||| yu-hao lee ||| carrie heeter ||| 
2021 ||| attention-driven read-aloud technology increases reading comprehension in children with reading disabilities. ||| gianluca schiavo ||| nadia mana ||| ornella mich ||| massimo zancanaro ||| remo job ||| 
2021 ||| unpacking the black-box of students' visual attention in mathematics and english classrooms: empirical evidence using mini-video recording gadgets. ||| danyal farsani ||| farzad radmehr ||| mohadaseh alizadeh ||| yusuf feyisara zakariya ||| 
2018 ||| attention to the model's face when learning from video modeling examples in adolescents with and without autism spectrum disorder. ||| margot van wermeskerken ||| bianca grimmius ||| tamara van gog ||| 
2018 ||| high frequency transformer design and optimization using bio-inspired algorithms. ||| jeyapradha ravichandran banumathy ||| rajini veeraraghavalu ||| 
2021 ||| automatic diagnosis of attention deficit hyperactivity disorder using machine learning. ||| tianhua chen ||| grigoris antoniou ||| marios adamou ||| ilias tachmazidis ||| pan su ||| 
2018 ||| real-world plant species identification based on deep convolutional neural networks and visual attention. ||| qingguo xiao ||| guangyao li ||| li xie ||| qiaochuan chen ||| 
2021 ||| electric signal synchronization as a behavioural strategy to generate social attention in small groups of mormyrid weakly electric fish and a mobile fish robot. ||| martin worm ||| tim landgraf ||| gerhard von der emde ||| 
2017 ||| an insect-inspired model for visual binding ii: functional analysis and visual attention. ||| brandon d. northcutt ||| charles m. higgins ||| 
2021 ||| multi-lingual character handwriting framework based on an integrated deep learning based sequence-to-sequence attention model. ||| besma rabhi ||| abdelkarim elbaati ||| houcine boubaker ||| yahia hamdi ||| amir hussain ||| adel m. alimi ||| 
2021 ||| a new model of transformer operation state evaluation based on analytic hierarchy process and association rule mining. ||| zhenyu zhou ||| huigang xu ||| haifeng ye ||| 
2021 ||| self-attention based sentiment analysis with effective embedding techniques. ||| soubraylu sivakumar ||| ratnavel rajalakshmi ||| 
2020 ||| the dilemma of user engagement in privacy notices: effects of interaction modes and habituation on user attention. ||| farzaneh karegar ||| john s ||| ren pettersson ||| simone fischer-h ||| bner ||| 
2019 ||| multiparameter-based fuzzy logic health index assessment for oil-immersed power transformers. ||| edwell t. mharakurwa ||| rutendo goboza ||| 
2022 ||| rose: real one-stage effort to detect the fingerprint singular point based on multi-scale spatial attention. ||| liaojun pang ||| jiong chen ||| fei guo ||| zhicheng x. cao ||| eryun liu ||| heng zhao ||| 
2021 ||| image super-resolution network based on a multi-branch attention mechanism. ||| xin yang ||| yingqing guo ||| zhiqiang li ||| dake zhou ||| 
2021 ||| lightweight attention convolutional neural network through network slimming for robust facial expression recognition. ||| hui ma ||| turgay  ||| elik ||| heng-chao li ||| 
2022 ||| dadan: dual-path attention with distribution analysis network for text-image matching. ||| wenhao li ||| hongqing zhu ||| suyi yang ||| han zhang ||| 
2021 ||| symmetric pyramid attention convolutional neural network for moving object detection. ||| shaocheng qu ||| hongrui zhang ||| wenhui wu ||| wenjun xu ||| yifei li ||| 
2021 ||| weighted least square design technique for hilbert transformer using fractional derivative. ||| nikhil agrawal ||| anil kumar ||| b. kuldeep ||| seungchan lee ||| heung-no lee ||| 
2022 ||| global attention-assisted representation learning for vehicle re-identification. ||| liping song ||| xin zhou ||| yuanyuan chen ||| 
2022 ||| multi-level attention network: application to brain tumor classification. ||| shaik nagur shareef ||| teja krishna cherukuri ||| 
2021 ||| view transform graph attention recurrent networks for skeleton-based action recognition. ||| qingqing huang ||| fengyu zhou ||| runze qin ||| yang zhao ||| 
2020 ||| correction to: high-speed tracking based on multi-cf filters and attention mechanism. ||| songyuan fan ||| rui wang ||| zhihao wu ||| seungmin rho ||| shaohui liu ||| jiaxin xiong ||| sen fu ||| feng jiang ||| 
2021 ||| fully convolutional network with attention modules for semantic segmentation. ||| yunjia huang ||| haixia xu ||| 
2021 ||| temporal attention learning for action quality assessment in sports video. ||| qing lei ||| hongbo zhang ||| jixiang du ||| 
2021 ||| pose estimation at night in infrared images using a lightweight multi-stage attention network. ||| ying zang ||| chunpeng fan ||| zeyu zheng ||| dongsheng yang ||| 
2021 ||| cascade-guided multi-scale attention network for crowd counting. ||| shufang li ||| zhengping hu ||| mengyao zhao ||| zhe sun ||| 
2022 ||| attention-guided multi-path cross-cnn for underwater image super-resolution. ||| yan zhang ||| shangxue yang ||| yemei sun ||| shudong liu ||| xianguo li ||| 
2022 ||| stratified attention dense network for image super-resolution. ||| zhiwei liu ||| xiaofeng mao ||| ji huang ||| menghan gan ||| yueyuan zhang ||| 
2021 ||| high-speed tracking based on multi-cf filters and attention mechanism. ||| songyuan fan ||| rui wang ||| zhihao wu ||| seungmin rho ||| shaohui liu ||| jiaxin xiong ||| sen fu ||| feng jiang ||| 
2021 ||| application of an attention u-net incorporating transfer learning for optic disc and cup segmentation. ||| xiaoye zhao ||| shengying wang ||| jing zhao ||| hai-cheng wei ||| ming-xia xiao ||| na ta ||| 
2021 ||| compound-attention network with original feature injection for visual question and answering. ||| chunlei wu ||| jing lu ||| haisheng li ||| jie wu ||| hailong duan ||| shaozu yuan ||| 
2022 ||| derainattentiongan: unsupervised single-image deraining using attention-guided generative adversarial networks. ||| zhaokang guo ||| mingzheng hou ||| mingjun sima ||| ziliang feng ||| 
2021 ||| regression loss in transformer-based supervised neural machine translation. ||| dongxing li ||| zuying luo ||| 
2018 ||| text classification research with attention-based recurrent neural networks. ||| changshun du ||| lei huang ||| 
2021 ||| efficient building extraction for high spatial resolution images based on dual attention network. ||| dandong zhao ||| haishi zhao ||| renchu guan ||| chen yang ||| 
2020 ||| research in the attention economy. ||| oliver hinz ||| wil m. p. van der aalst ||| christof weinhardt ||| 
2021 ||| enhancing sustained attention. ||| th ||| ophile demazure ||| alexander j. karran ||| pierre-majorique l ||| ger ||| lise labont ||| -lemoyne ||| sylvain s ||| n ||| cal ||| marc fredette ||| gilbert babin ||| 
2021 ||| data fusion analysis for attention-deficit hyperactivity disorder emotion recognition with thermal image and internet of things devices. ||| ying-hsun lai ||| yao-chung chang ||| chia-wei tsai ||| chih hsun lin ||| mu-yen chen ||| 
2021 ||| cross-group or within-group attention flow? exploring the amplification process among elite users and social media publics in sina weibo. ||| pianpian wang ||| wensen huang ||| 
2021 ||| sound design inducing attention in the context of audiovisual immersive environments. ||| in ||| s salselas ||| rui penha ||| gilberto bernardes ||| 
2021 ||| serious games for basic learning mechanisms: reinforcing mexican children's gross motor skills and attention. ||| raymundo cornejo ||| fernando mart ||| nez-reyes ||| vania c.  ||| lvarez ||| claudia barraza ||| franceli l. cibrian ||| ana i. mart ||| nez-garc ||| a ||| monica tentori ||| 
2022 ||| cagat: centrality-adjusted graph attention network for active scientific talent discovery. ||| chong li ||| jinjie zhang ||| yuchen wang ||| xuemin liu ||| 
2017 ||| web page attentional priority model. ||| jeremiah d. still ||| 
2020 ||| effects of working memory, attention, and expertise on pilots' situation awareness. ||| serkan cak ||| bilge say ||| mine misirlisoy ||| 
2017 ||| predictive aids can lead to sustained attention decrements in the detection of non-routine critical events in event monitoring. ||| dev minotra ||| michael d. mcneese ||| 
2021 ||| exploring the crossing behaviours and visual attention allocation of children in primary school in an outdoor road environment. ||| kang jiang ||| yulong wang ||| zhongxiang feng ||| tony n. n. sze ||| zhenhua yu ||| jianqiang cui ||| 
2019 ||| a new abstraction framework for affine transformers. ||| tushar sharma ||| thomas w. reps ||| 
2020 ||| extremely low-resource text simplification with pre-trained transformer language model. ||| takumi maruyama ||| kazuhide yamamoto ||| 
2017 ||| collaborative game-based learning with motion-sensing technology: analyzing students' motivation, attention, and relaxation levels. ||| cheng-yu hung ||| yu-ren lin ||| kai-yi huang ||| pao-ta yu ||| jerry chih-yuan sun ||| 
2021 ||| a weakly supervised learning method based on attention fusion for covid-19 segmentation in ct images. ||| hongyu chen ||| shengsheng wang ||| 
2022 ||| small object detection combining attention mechanism and a novel fpn. ||| junying chen ||| shipeng liu ||| liang zhao ||| dengfeng chen ||| weihua zhang ||| 
2022 ||| equivalent circuit modelling of a three-phase to seven-phase transformer using pso and ga. ||| md. tabrez ||| pradip kumar sadhu ||| atif iqbal ||| mohammed aslam husain ||| farhad ilahi bakhsh ||| s. p. singh ||| 
2019 ||| siamese hierarchical attention networks for extractive summarization. ||| jos ||| - ||| ngel gonz ||| lez ||| encarna segarra ||| fernando garc ||| a-granada ||| emilio sanchis ||| llu ||| s-f. hurtado ||| 
2021 ||| neighborhood aggregation based graph attention networks for open-world knowledge graph reasoning. ||| xiaojun chen ||| ling ding ||| yang xiang ||| 
2022 ||| research on named entity recognition of chinese electronic medical records based on multi-head attention mechanism and character-word information fusion. ||| qinghui zhang ||| meng wu ||| pengtao lv ||| mengya zhang ||| hongwei yang ||| 
2018 ||| transformer fault diagnosis method based on graph theory and rough set. ||| peng lu ||| wenhui li ||| dongmei huang ||| 
2022 ||| camgan: combining attention mechanism generative adversarial networks for cartoon face style transfer. ||| tao zhang ||| long yu ||| shengwei tian ||| 
2022 ||| lexical attention and aspect-oriented graph convolutional networks for aspect-based sentiment analysis. ||| wenwen li ||| shiqun yin ||| ting pu ||| 
2020 ||| multi-head attention model for aspect level sentiment analysis. ||| xinsheng zhang ||| teng gao ||| 
2021 ||| nighttime vehicle detection based on direction attention network and bayes corner localization. ||| danyang huang ||| zhiheng zhou ||| ming deng ||| zhihao li ||| 
2021 ||| towards corpus and model: hierarchical structured-attention-based features for indonesian named entity recognition. ||| yingwen fu ||| nankai lin ||| xiaotian lin ||| shengyi jiang ||| 
2022 ||| joint intent detection and slot filling with wheel-graph attention networks. ||| pengfei wei ||| bi zeng ||| wenxiong liao ||| 
2019 ||| power transformers internal fault diagnosis based on deep convolutional neural networks. ||| mousa afrasiabi ||| shahabodin afrasiabi ||| benyamin parang ||| mohammad mohammadi ||| 
2021 ||| deep context interaction network based on attention mechanism for click-through rate prediction. ||| ling yuan ||| zhuwen pan ||| ping sun ||| yinzhen wei ||| haiping yu ||| 
2020 ||| a fuzzy based system for target search using top-down visual attention. ||| j. amudha ||| k. v. divya ||| r. aarthi ||| 
2022 ||| optimization of apt attack detection based on a model combining attention and deep learning. ||| cho do xuan ||| duc duong ||| 
2021 ||| the enhanced deep plug-and-play super-resolution algorithm with residual channel attention networks. ||| hongguang pan ||| fan wen ||| xiangdong huang ||| xinyu lei ||| xiaoling yang ||| 
2021 ||| fusion of part-of-speech vectors and attention mechanisms for cross-domain sentiment analysis. ||| ting lu ||| yan xiang ||| junge liang ||| li zhang ||| mingfang zhang ||| 
2021 ||| non-diacritized arabic speech recognition based on cnn-lstm and attention-based models. ||| hamzah a. alsayadi ||| abdelaziz a. abdelhamid ||| islam hegazy ||| zaki taha fayed ||| 
2022 ||| aspect-level sentiment analysis for based on joint aspect and position hierarchy attention mechanism network. ||| dangguo shao ||| qing an ||| kun huang ||| yan xiang ||| lei ma ||| junjun guo ||| runda yin ||| 
2021 ||| sentiment classification based on dependency-relationship embedding and attention mechanism. ||| zhan yang ||| chengliang li ||| zhongying zhao ||| chao li ||| 
2020 ||| summarization for online reviews based on hierarchical attention network. ||| shibo zhang ||| yuanlan yu ||| boyuan zhang ||| yun sha ||| 
2022 ||| fmagan: fusing multiple attention and generative adversarial network to enhance underwater image. ||| long hou ||| long yu ||| shengwei tian ||| yanhan zhang ||| 
2020 ||| attention-based lstm, gru and cnn for short text classification. ||| shujuan yu ||| danlei liu ||| wenfeng zhu ||| yun zhang ||| shengmei zhao ||| 
2022 ||| end-to-end driving model based on deep learning and attention mechanism. ||| wuqiang zhu ||| yang lu ||| yongliang zhang ||| xing wei ||| zhen wei ||| 
2021 ||| normalization and deep learning based attention deficit hyperactivity disorder classification. ||| p. preetha ||| r. mallika ||| 
2021 ||| truncated attention mechanism and cascade loss for cross-modal person re-identification. ||| shuo shi ||| changwei huo ||| yingchun guo ||| stephen lean ||| gang yan ||| ming yu ||| 
2019 ||| feature attention based detection model for medical text. ||| qubo xie ||| ke zhou ||| xiao fu ||| xiaohu fan ||| 
2021 ||| composite load modeling by spatial-temporal deep attention network based on wide-area monitoring systems. ||| omid izadi ghafarokhi ||| mazda moattari ||| ahmad forouzantabar ||| 
2020 ||| bidirectional indrnn malicious webpages detection algorithm based on convolutional neural network and attention mechanism. ||| huan-huan wang ||| shengwei tian ||| long yu ||| xia ||| n-xian wang ||| qing-shan qi ||| ji-hong chen ||| 
2019 ||| bi-direction hierarchical lstm with spatial-temporal attention for action recognition. ||| haodong yang ||| jun zhang ||| shuohao li ||| tingjin luo ||| 
2018 ||| attention based english to punjabi neural machine translation. ||| shivkaran singh ||| m. anand kumar ||| k. p. soman ||| 
2018 ||| priority assessment model of on-line monitoring devices investment for power transformers. ||| yongliang liang ||| zhuoran lin ||| ke-jun li ||| lin niu ||| jianguo zhao ||| wei-jen lee ||| 
2022 ||| hsrec: hierarchical self-attention incorporating knowledge graph for sequential recommendation. ||| zuoxi yang ||| shoubin dong ||| 
2020 ||| an intelligent approach for the evaluation of transformers in a power distribution project. ||| funda samanlioglu ||| zeki ayag ||| 
2021 ||| correlation analysis of law-related news combining bidirectional attention flow of news title and body. ||| yu zhang ||| zhengtao yu ||| cunli mao ||| yuxin huang ||| shengxiang gao ||| 
2022 ||| an object detection network based on yolov4 and improved spatial attention mechanism. ||| zhixiong chen ||| shengwei tian ||| long yu ||| liqiang zhang ||| xinyu zhang ||| 
2021 ||| interval fuzzy probability method for power transformer multiple fault diagnosis. ||| lintao zhou ||| qinge wu ||| hu chen ||| tao hu ||| 
2022 ||| attention deeplabv3 model and its application into gear pitting measurement. ||| dejun xi ||| yi qin ||| zhiwen wang ||| 
2020 ||| guided attention mechanism: training network more efficiently. ||| chiyu wang ||| hong li ||| xinrong li ||| feifei hou ||| xun hu ||| 
2019 ||| dual attention based fine-grained leukocyte recognition for imbalanced microscopic images. ||| qinghao ye ||| daijian tu ||| feiwei qin ||| zizhao wu ||| yong peng ||| shuying shen ||| 
2021 ||| long-distance contextual attention network for skin disease segmentation. ||| yanhan zhang ||| shengwei tian ||| long yu ||| yuan ren ||| zhongyu gao ||| long hou ||| 
2020 ||| extractive summarization using siamese hierarchical transformer encoders. ||| jos ||| - ||| ngel gonz ||| lez ||| encarna segarra ||| fernando garc ||| a-granada ||| emilio sanchis ||| llu ||| s-f. hurtado ||| 
2020 ||| self-attention for twitter sentiment analysis in spanish. ||| jos ||| - ||| ngel gonz ||| lez ||| llu ||| s-f. hurtado ||| ferran pla ||| 
2021 ||| extracting relational facts based on hybrid syntax-guided transformer and pointer network. ||| luping liu ||| meiling wang ||| xiaohai he ||| linbo qing ||| jin zhang ||| 
2018 ||| eeg lecture on recommended activities for the induction of attention and concentration mental states on e-learning students. ||| julia y. arana-llanes ||| gabriel gonz ||| lez serna ||| rodrigo pineda-tapia ||| v ||| ctor olivares-peregrino ||| jorge j. ricarte-trives ||| jos |||  m. latorre-postigo ||| 
2019 ||| android malware analysis and detection based on attention-cnn-lstm. ||| shiqi luo ||| zhiyuan liu ||| bo li ||| huanhuan wang ||| hua sun ||| yong yuan ||| 
2019 ||| inattentional blindness factors that make people vulnerable to security threats in social networking sites. ||| abdullah algarni ||| 
2021 ||| an attention-based spatiotemporal lstm network for next poi recommendation. ||| liwei huang ||| yutao ma ||| shibo wang ||| yanbo liu ||| 
2019 ||| advancing acoustic-to-word ctc model with attention and mixed-units. ||| amit das ||| jinyu li ||| guoli ye ||| rui zhao ||| yifan gong ||| 
2021 ||| detection of multiple steganography methods in compressed speech based on code element embedding, bi-lstm and cnn with attention mechanisms. ||| songbin li ||| jingang wang ||| peng liu ||| miao wei ||| qiandong yan ||| 
2020 ||| improving self-attention networks with sequential relations. ||| zaixiang zheng ||| shujian huang ||| rongxiang weng ||| xin-yu dai ||| jiajun chen ||| 
2021 ||| preordering encoding on transformer for translation. ||| yuki kawara ||| chenhui chu ||| yuki arase ||| 
2020 ||| towards better word alignment in transformer. ||| kai song ||| xiaoqing zhou ||| heng yu ||| zhongqiang huang ||| yue zhang ||| weihua luo ||| xiangyu duan ||| min zhang ||| 
2019 ||| weakly labelled audioset tagging with attention neural networks. ||| qiuqiang kong ||| changsong yu ||| yong xu ||| turab iqbal ||| wenwu wang ||| mark d. plumbley ||| 
2019 ||| attention with sparsity regularization for neural machine translation and summarization. ||| jiajun zhang ||| yang zhao ||| haoran li ||| chengqing zong ||| 
2018 ||| a hierarchy-to-sequence attentional neural machine translation model. ||| jinsong su ||| jiali zeng ||| deyi xiong ||| yang liu ||| mingxuan wang ||| jun xie ||| 
2020 ||| audio object classification using distributed beliefs and attention. ||| ashwin bellur ||| mounya elhilali ||| 
2020 ||| multi-instrument automatic music transcription with self-attention-based instance segmentation. ||| yu-te wu ||| berlin chen ||| li su ||| 
2021 ||| deformable self-attention for text classification. ||| qianli ma ||| jiangyue yan ||| zhenxi lin ||| liuhong yu ||| zipeng chen ||| 
2018 ||| cross-language neural dialog state tracker for large ontologies using hierarchical attention. ||| youngsoo jang ||| jiyeon ham ||| byung-jun lee ||| kee-eung kim ||| 
2020 ||| end-to-end post-filter for speech separation with deep attention fusion features. ||| cunhang fan ||| jianhua tao ||| bin liu ||| jiangyan yi ||| zhengqi wen ||| xuefei liu ||| 
2019 ||| global-local mutual attention model for text classification. ||| qianli ma ||| liuhong yu ||| shuai tian ||| enhuan chen ||| wing w. y. ng ||| 
2020 ||| monaural speech dereverberation using temporal convolutional networks with self attention. ||| yan zhao ||| deliang wang ||| buye xu ||| tao zhang ||| 
2021 ||| audio-based piano performance evaluation for beginners with convolutional neural network and attention mechanism. ||| weiqing wang ||| jin pan ||| hua yi ||| zhanmei song ||| ming li ||| 
2021 ||| many-to-many voice transformer network. ||| hirokazu kameoka ||| wen-chin huang ||| kou tanaka ||| takuhiro kaneko ||| nobukatsu hojo ||| tomoki toda ||| 
2020 ||| online hybrid ctc/attention end-to-end automatic speech recognition architecture. ||| haoran miao ||| gaofeng cheng ||| pengyuan zhang ||| yonghong yan ||| 
2021 ||| investigating typed syntactic dependencies for targeted sentiment classification using graph attention neural network. ||| xuefeng bai ||| pengbo liu ||| yue zhang ||| 
2020 ||| swings and roundabouts: attention-structure interaction effect in deep semantic matching. ||| amulya gupta ||| zhu zhang ||| 
2020 ||| fast query-by-example speech search using attention-based deep binary embeddings. ||| yougen yuan ||| lei xie ||| cheung-chi leung ||| hongjie chen ||| bin ma ||| 
2021 ||| contrastive information extraction with generative transformer. ||| ningyu zhang ||| hongbin ye ||| shumin deng ||| chuanqi tan ||| mosha chen ||| songfang huang ||| fei huang ||| huajun chen ||| 
2020 ||| sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization. ||| qiuqiang kong ||| yong xu ||| wenwu wang ||| mark d. plumbley ||| 
2021 ||| grtr: generative-retrieval transformers for data-efficient dialogue domain adaptation. ||| igor shalyminov ||| alessandro sordoni ||| adam atkinson ||| hannes schulz ||| 
2021 ||| end-to-end recurrent cross-modality attention for video dialogue. ||| yun-wei chu ||| kuan-yen lin ||| chao-chun hsu ||| lun-wei ku ||| 
2021 ||| keyword search using attention-based end-to-end asr and frame-synchronous phoneme alignments. ||| runyan yang ||| gaofeng cheng ||| haoran miao ||| ta li ||| pengyuan zhang ||| yonghong yan ||| 
2020 ||| attention-based response generation using parallel double q-learning for dialog policy decision in a conversational system. ||| ming-hsiang su ||| chung-hsien wu ||| liang-yu chen ||| 
2021 ||| bridging text and video: a universal multimodal transformer for audio-visual scene-aware dialog. ||| zekang li ||| zongjia li ||| jinchao zhang ||| yang feng ||| jie zhou ||| 
2021 ||| a joint model for named entity recognition with sentence-level entity type attentions. ||| tao qian ||| meishan zhang ||| yinxia lou ||| daiwen hua ||| 
2020 ||| knowledge guided capsule attention network for aspect-based sentiment analysis. ||| bowen zhang ||| xutao li ||| xiaofei xu ||| ka-cheong leung ||| zhiyao chen ||| yunming ye ||| 
2020 ||| adversarial learning for multi-task sequence labeling with attention mechanism. ||| yu wang ||| yun li ||| ziye zhu ||| hanghang tong ||| yue huang ||| 
2020 ||| cognitive-driven binaural beamforming using eeg-based auditory attention decoding. ||| ali aroudi ||| simon doclo ||| 
2019 ||| adversarial regularization for attention based end-to-end robust speech recognition. ||| sining sun ||| pengcheng guo ||| lei xie ||| mei-yuh hwang ||| 
2020 ||| a two-stage transformer-based approach for variable-length abstractive summarization. ||| ming-hsiang su ||| chung-hsien wu ||| hao-tse cheng ||| 
2020 ||| entity-sensitive attention and fusion network for entity-level multimodal sentiment classification. ||| jianfei yu ||| jing jiang ||| rui xia ||| 
2021 ||| deep selective memory network with selective attention and inter-aspect modeling for aspect level sentiment classification. ||| peiqin lin ||| meng yang ||| jianhuang lai ||| 
2022 ||| automatic math word problem generation with topic-expression co-attention mechanism and reinforcement learning. ||| qinzhuo wu ||| qi zhang ||| xuanjing huang ||| 
2019 ||| speech emotion classification using attention-based lstm. ||| yue xie ||| ruiyu liang ||| zhenlin liang ||| chengwei huang ||| cairong zou ||| bj ||| rn w. schuller ||| 
2021 ||| tera: self-supervised learning of transformer encoder representation for speech. ||| andy t. liu ||| shang-wen li ||| hung-yi lee ||| 
2021 ||| hierarchical neighbor propagation with bidirectional graph attention network for relation prediction. ||| zhiwen xie ||| runjie zhu ||| jin liu ||| guangyou zhou ||| jimmy xiangji huang ||| 
2020 ||| audio replay spoof attack detection by joint segment-based linear filter bank feature extraction and attention-enhanced densenet-bilstm network. ||| lian huang ||| chi-man pun ||| 
2021 ||| medical term and status generation from chinese clinical dialogue with multi-granularity transformer. ||| mei li ||| lu xiang ||| xiaomian kang ||| yang zhao ||| yu zhou ||| chengqing zong ||| 
2021 ||| dense cnn with self-attention for time-domain speech enhancement. ||| ashutosh pandey ||| deliang wang ||| 
2021 ||| information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition. ||| hengshun zhou ||| jun du ||| yuanyuan zhang ||| qing wang ||| qing-feng liu ||| chin-hui lee ||| 
2021 ||| nt context: softmax-free attention for online encoder-decoder speech recognition. ||| hyeonseung lee ||| woo hyun kang ||| sung jun cheon ||| hyeongju kim ||| nam soo kim ||| 
2020 ||| how to teach dnns to pay attention to the visual modality in speech recognition. ||| george sterpu ||| christian saam ||| naomi harte ||| 
2021 ||| randomly wired network based on roberta and dialog history attention for response selection. ||| byoungjae kim ||| jungyun seo ||| myoung-wan koo ||| 
2021 ||| attending from foresight: a novel attention mechanism for neural machine translation. ||| xintong li ||| lemao liu ||| zhaopeng tu ||| guanlin li ||| shuming shi ||| max q.-h. meng ||| 
2021 ||| ctnet: conversational transformer network for emotion recognition. ||| zheng lian ||| bin liu ||| jianhua tao ||| 
2019 ||| low-rank and locality constrained self-attention for sequence modeling. ||| qipeng guo ||| xipeng qiu ||| xiangyang xue ||| zheng zhang ||| 
2020 ||| investigating self-attention network for chinese word segmentation. ||| leilei gan ||| yue zhang ||| 
2019 ||| sparse self-attention lstm for sentiment lexicon construction. ||| dong deng ||| liping jing ||| jian yu ||| shaolong sun ||| 
2022 ||| s-vectors and tesa: speaker embeddings and a speaker authenticator based on transformer encoder. ||| narla john metilda sagaya mary ||| srinivasan umesh ||| sandesh varadaraju katta ||| 
2021 ||| speech enhancement via attention masking network (seamnet): an end-to-end system for joint suppression of noise and reverberation. ||| bengt j. borgstr ||| m ||| michael s. brandstein ||| 
2022 ||| aggregating frame-level information in the spectral domain with self-attention for speaker embedding. ||| youzhi tu ||| man-wai mak ||| 
2021 ||| target speaker verification with selective auditory attention for single and multi-talker speech. ||| chenglin xu ||| wei rao ||| jibin wu ||| haizhou li ||| 
2019 ||| interdisciplinary scholarly communication: an exploratory study for the field of joint attention. ||| jian xu ||| ying ding ||| yi bu ||| shuqing deng ||| chen yu ||| yimin zou ||| andrew madden ||| 
2021 ||| production profiles in brazilian science, with special attention to social sciences and humanities. ||| concepta mcmanus ||| abilio afonso baeta neves ||| 
2018 ||| and now for something completely different: the congruence of the altmetric attention score's structure between different article groups. ||| bhaskar mukherjee ||| sinisa subotic ||| ajay kumar chaubey ||| 
2018 ||| competition between academic journals for scholars' attention: the 'nature effect' in scholarly communication. ||| jose a. garc ||| a ||| rosa rodr ||| guez-s ||| nchez ||| j. fdez-valdivia ||| 
2020 ||| communities of attention networks: introducing qualitative and conversational perspectives for altmetrics. ||| ronaldo ferreira araujo ||| 
2020 ||| an altmetric attention advantage for open access books in the humanities and social sciences. ||| michael taylor ||| 
2018 ||| the market of academic attention. ||| matteo migheli ||| giovanni b. ramello ||| 
2018 ||| allegation of scientific misconduct increases twitter attention. ||| lutz bornmann ||| robin haunschild ||| 
2018 ||| why do some research articles receive more online attention and higher altmetrics? reasons for online success according to the authors. ||| kim holmberg ||| julia vainio ||| 
2020 ||| modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models. ||| tong zeng ||| daniel e. acuna ||| 
2020 ||| does presence of social media plugins in a journal website result in higher social media attention of its research publications? ||| mousumi karmakar ||| sumit kumar banshal ||| vivek kumar singh ||| 
2021 ||| news media attention in climate action: latent topics and open access. ||| tahereh dehdarirad ||| kalle karlsson ||| 
2018 ||| understanding the formation of interdisciplinary research from the perspective of keyword evolution: a case study on joint attention. ||| jian xu ||| yi bu ||| ying ding ||| sinan yang ||| hongli zhang ||| chen yu ||| lin sun ||| 
2020 ||| evaluating the relationship between the academic and social impact of open access books based on citation behaviors and social media attention. ||| mingkun wei ||| abdolreza noroozi chakoli ||| 
2020 ||| the impact of preprints in library and information science: an analysis of citations, usage and social attention indicators. ||| zhiqi wang ||| wolfgang gl ||| nzel ||| yue chen ||| 
2021 ||| do research articles with more readable abstracts receive higher online attention? evidence from science. ||| tan jin ||| huiqiong duan ||| xiaofei lu ||| jing ni ||| kai guo ||| 
2021 ||| design of a novel topology of transformer-less cuk inverter for isolated load applications. ||| a. p. narmadha ||| h. habeebullah sait ||| 
2021 ||| optimized active power management in solar pv-fed transformerless grid-connected system for rural electrified microgrid. ||| bekkam krishna ||| t. s. bheemraj ||| v. karthikeyan ||| 
2021 ||| low-mismatch and low-loss transformer-based spdt switch with switching load technique for 5g applications. ||| xing quan ||| jinsong zhan ||| jiang luo ||| guodong su ||| xiang wang ||| 
2018 ||| a voltage multiplier based high voltage gain transformerless buck-boost dc-dc converter. ||| hossein ajdar faeghi bonab ||| mohamad reza banaei ||| navid taghizadegan kalantari ||| 
2020 ||| a low-loss and high-isolation transformer-based mm-wave spdt with integrated fan-out wafer level packaging. ||| xing quan ||| jiang luo ||| guodong su ||| kai jing ||| jinsong zhan ||| 
2020 ||| content-based attention network for person image generation. ||| xiongfei liu ||| bengao li ||| xin chen ||| haiyan zhang ||| shu zhan ||| 
2019 ||| design of 85 ghz high power gain sige buffer with transformer matching network. ||| guiheng zhang ||| wei zhang ||| jun fu ||| 
2017 ||| modified gorski-popiel technique and synthetic floating transformer circuit using minimum components. ||| mehmet sagbas ||| umut engin ayten ||| herman sedef ||| shahram minaei ||| 
2017 ||| spreading one's tweets: how can journalists gain attention for their tweeted news? ||| claudia orellana-rodriguez ||| derek greene ||| mark t. keane ||| 
2017 ||| why pay attention to paths in the practice of environmental modelling? ||| tuomas j. lahtinen ||| joseph h. a. guillaume ||| raimo p. h ||| m ||| l ||| inen ||| 
2021 ||| an attention u-net model for detection of fine-scale hydrologic streamlines. ||| zewei xu ||| shaowen wang ||| lawrence v. stanislawski ||| zhe jiang ||| nattapon jaroenchai ||| arpan man sainju ||| ethan shavers ||| e. lynn usery ||| li chen ||| zhiyu li ||| bin su ||| 
2021 ||| a serious-gamification blueprint towards a normalized attention. ||| saad alqithami ||| 
2022 ||| trg-datt: the target relational graph and double attention network based sentiment analysis and prediction for supporting decision making. ||| fan chen ||| jiaoxiong xia ||| honghao gao ||| huahu xu ||| wei wei ||| 
2018 ||| price shock detection with an influence-based model of social attention. ||| keli xiao ||| qi liu ||| chuanren liu ||| hui xiong ||| 
2019 ||| the time-course of component processes of selective attention. ||| tanya wen ||| john duncan ||| daniel j. mitchell ||| 
2021 ||| how does the brain navigate knowledge of social relations? testing for shared neural mechanisms for shifting attention in space and social knowledge. ||| meng du ||| ruby basyouni ||| carolyn parkinson ||| 
2017 ||| interactions between the default network and dorsal attention network vary across default subsystems, time, and cognitive states. ||| matthew l. dixon ||| jessica r. andrews-hanna ||| r. nathan spreng ||| zachary c. irving ||| caitlin mills ||| manesh girn ||| kalina christoff ||| 
2021 ||| organization of directed functional connectivity among nodes of ventral attention network reveals the common network mechanisms underlying saliency processing across distinct spatial and spatio-temporal scales. ||| priyanka ghosh ||| dipanjan roy ||| arpan banerjee ||| 
2018 ||| decoding selective attention to context memory: an aging study. ||| patrick s. powell ||| jonathan strunk ||| taylor james ||| sean m. polyn ||| audrey duarte ||| 
2018 ||| cortical summation and attentional modulation of combined chromatic and luminance signals. ||| jasna martinovic ||| s ||| ren k. andersen ||| 
2017 ||| dissociated roles of the parietal and frontal cortices in the scope and control of attention during visual working memory. ||| siyao li ||| ying cai ||| jing liu ||| dawei li ||| zifang feng ||| chuansheng chen ||| gui xue ||| 
2020 ||| theta and alpha oscillations as signatures of internal and external attention to delayed intentions: a magnetoencephalography (meg) study. ||| giorgia cona ||| francesco chiossi ||| silvia di tomasso ||| giovanni pellegrino ||| francesco piccione ||| patrizia silvia bisiacchi ||| giorgio arcara ||| 
2021 ||| the "when" and "where" of the interplay between attentional capture and response inhibition during a go/nogo variant. ||| joseph p. happer ||| laura c. wagner ||| lauren e. beaton ||| burke q. rosen ||| ksenija marinkovic ||| 
2019 ||| attention modulates event-related spectral power in multisensory self-motion perception. ||| ben townsend ||| joey k. legere ||| shannon o'malley ||| martin v. mohrenschildt ||| judith m. shedden ||| 
2018 ||| connectome-based predictive modeling of attention: comparing different functional connectivity features and prediction methods across datasets. ||| kwangsun yoo ||| monica d. rosenberg ||| wei-ting hsu ||| sheng zhang ||| chiang-shan ray li ||| dustin scheinost ||| r. todd constable ||| marvin m. chun ||| 
2019 ||| dynamic functional connectivity during task performance and rest predicts individual differences in attention across studies. ||| angus ho ching fong ||| kwangsun yoo ||| monica d. rosenberg ||| sheng zhang ||| chiang-shan ray li ||| dustin scheinost ||| r. todd constable ||| marvin m. chun ||| 
2021 ||| dimension-selective attention and dimensional salience modulate cortical tracking of acoustic dimensions. ||| ashley e. symons ||| fred dick ||| adam t. tierney ||| 
2022 ||| selective attention involves a feature-specific sequential release from inhibitory gating. ||| mattia f. pagnotta ||| david pascucci ||| gijs plomp ||| 
2018 ||| anticipating the good and the bad: a study on the neural correlates of bivalent emotion anticipation and their malleability via attentional deployment. ||| johann daniel kruschwitz ||| lea waller ||| david list ||| david wisniewski ||| vera u. ludwig ||| franziska korb ||| uta wolfensteller ||| thomas goschke ||| henrik walter ||| 
2021 ||| towards understanding how we pay attention in naturalistic visual search settings. ||| nora turoman ||| ruxandra i. tivadar ||| chrysa retsa ||| micah m. murray ||| pawel j. matusz ||| 
2021 ||| anatomical and functional coupling between the dorsal and ventral attention networks. ||| xinjun suo ||| hao ding ||| xi li ||| yaodan zhang ||| meng liang ||| yongqiang zhang ||| chunshui yu ||| wen qin ||| 
2017 ||| spatio-temporal dynamics of attentional selection stages during multiple object tracking. ||| christian merkel ||| jens-max hopf ||| mircea ariel schoenfeld ||| 
2019 ||| spatial attention underpins social word learning in the right fronto-parietal network. ||| laura verga ||| sonja a. kotz ||| 
2018 ||| changes in alpha activity reveal that social opinion modulates attention allocation during face processing. ||| evelien heyselaar ||| ali mazaheri ||| peter hagoort ||| katrien segaert ||| 
2020 ||| too little, too late, and in the wrong place: alpha band activity does not reflect an active mechanism of selective attention. ||| plamen a. antonov ||| ramakrishna chakravarthi ||| s ||| ren k. andersen ||| 
2019 ||| decoding of selective attention to continuous speech from the human auditory brainstem response. ||| octave etard ||| mikolaj kegler ||| chananel braiman ||| antonio elia forte ||| tobias reichenbach ||| 
2017 ||| attentional selection of multiple objects in the human visual system. ||| xilin zhang ||| nicole mlynaryk ||| shruti japee ||| leslie g. ungerleider ||| 
2019 ||| large-scale network interactions involved in dividing attention between the external environment and internal thoughts to pursue two distinct goals. ||| david maillet ||| roger e. beaty ||| aaron kucyi ||| daniel l. schacter ||| 
2018 ||| spatial attention modulates visual gamma oscillations across the human ventral stream. ||| lorenzo magazzini ||| krish d. singh ||| 
2021 ||| lateralized alpha activity and slow potential shifts over visual cortex track the time course of both endogenous and exogenous orienting of attention. ||| jonathan m. keefe ||| viola s. st ||| rmer ||| 
2019 ||| rapid sensory gain with emotional distracters precedes attentional deployment from a foreground task. ||| valeria bekhtereva ||| matt craddock ||| christopher gundlach ||| matthias m. m ||| ller ||| 
2018 ||| adaptive flexibility of the within-hand attentional gradient in touch: an meg study. ||| tetsuo kida ||| emi tanaka ||| ryusuke kakigi ||| 
2021 ||| callosal anisotropy predicts attentional network changes after parietal inhibitory stimulation. ||| selene schintu ||| catherine a. cunningham ||| michael freedberg ||| paul a. taylor ||| stephen j. gotts ||| sarah shomstein ||| eric m. wassermann ||| 
2022 ||| divided attention at retrieval does not influence neural correlates of recollection in young or older adults. ||| mingzhu hou ||| erin d. horne ||| marianne de chastelaine ||| michael d. rugg ||| 
2018 ||| neural mechanisms of divided feature-selective attention to colour. ||| jasna martinovic ||| sophie m. wuerger ||| steven a. hillyard ||| matthias m. m ||| ller ||| s ||| ren k. andersen ||| 
2020 ||| effects of auditory selective attention on neural phase: individual differences and short-term training. ||| aeron laffere ||| fred dick ||| adam tierney ||| 
2019 ||| impoverished auditory cues limit engagement of brain networks controlling spatial selective attention. ||| yuqi deng ||| inyong choi ||| barbara g. shinn-cunningham ||| robert baumgartner ||| 
2021 ||| can expectation suppression be explained by reduced attention to predictable stimuli? ||| arjen alink ||| helen blank ||| 
2020 ||| psilocybin acutely alters the functional connectivity of the claustrum with brain networks that support perception, memory, and attention. ||| frederick streeter barrett ||| samuel r. krimmel ||| roland r. griffiths ||| david a. seminowicz ||| brian n. mathur ||| 
2020 ||| evidence accumulation during perceptual decision-making is sensitive to the dynamics of attentional selection. ||| dragan rangelov ||| jason b. mattingley ||| 
2019 ||| focus of attention modulates the heartbeat evoked potential. ||| frederike petzschner ||| lilian a. e. weber ||| katharina v. wellstein ||| gina paolini ||| cao-tri do ||| klaas e. stephan ||| 
2017 ||| deciding where to attend: large-scale network mechanisms underlying attention and intention revealed by graph-theoretic analysis. ||| yuelu liu ||| xiangfei hong ||| jesse j. bengson ||| todd a. kelley ||| mingzhou ding ||| george r. mangun ||| 
2018 ||| tracking behavioral and neural fluctuations during sustained attention: a robust replication and extension. ||| francesca c. fortenbaugh ||| david rothlein ||| regina e. mcglinchey ||| joe degutis ||| michael esterman ||| 
2017 ||| rapid top-down control over template-guided attention shifts to multiple objects. ||| anna grubert ||| johannes j. fahrenfort ||| christian n. l. olivers ||| martin eimer ||| 
2021 ||| orienting auditory attention in time: lateralized alpha power reflects spatio-temporal filtering. ||| malte w ||| stmann ||| burkhard maess ||| jonas obleser ||| 
2021 ||| the pulse: transient fmri signal increases in subcortical arousal systems during transitions in attention. ||| rong li ||| jun hwan ryu ||| peter vincent ||| max springer ||| dan kluger ||| erik a. levinsohn ||| yu chen ||| huafu chen ||| hal blumenfeld ||| 
2018 ||| the involvement of alpha oscillations in voluntary attention directed towards encoding episodic memories. ||| tamas minarik ||| barbara berger ||| paul sauseng ||| 
2022 ||| reward-driven modulation of spatial attention in the human frontal eye-field. ||| alexia bourgeois ||| virginie sterpenich ||| giannina rita iannotti ||| patrik vuilleumier ||| 
2021 ||| brain state-based detection of attentional fluctuations and their modulation. ||| ayumu yamashita ||| david rothlein ||| aaron kucyi ||| eve m. valera ||| michael esterman ||| 
2017 ||| loss of lateral prefrontal cortex control in food-directed attention and goal-directed food choice in obesity. ||| lieneke k. janssen ||| iris duif ||| ilke van loon ||| joost wegman ||| jeanne h. m. de vries ||| roshan cools ||| esther aarts ||| 
2019 ||| selective effects of acute low-grade inflammation on human visual attention. ||| leonie jt. balter ||| jos a. bosch ||| sarah aldred ||| mark t. drayson ||| jet jcs. veldhuijzen van zanten ||| suzanne higgs ||| jane e. raymond ||| ali mazaheri ||| 
2020 ||| cholinergic white matter pathways make a stronger contribution to attention and memory in normal aging than cerebrovascular health and nucleus basalis of meynert. ||| milan nemy ||| nira cedres ||| michel j. grothe ||| j.-sebastian muehlboeck ||| olof lindberg ||| zuzana nedelska ||| olga step ||| nkov ||| lenka vyslouzilova ||| maria eriksdotter ||| jos |||  barroso ||| stefan teipel ||| eric westman ||| daniel ferreira ||| 
2018 ||| how acute stress may enhance subsequent memory for threat stimuli outside the focus of attention: dlpfc-amygdala decoupling. ||| yu luo ||| guill ||| n fern ||| ndez ||| erno j. hermans ||| susanne vogel ||| yu zhang ||| hong li ||| floris klumpers ||| 
2018 ||| a functional connectivity-based neuromarker of sustained attention generalizes to predict recall in a reading task. ||| david c. jangraw ||| javier gonzalez-castillo ||| daniel a. handwerker ||| merage ghane ||| monica d. rosenberg ||| puja panwar ||| peter a. bandettini ||| 
2018 ||| cortical tracking of multiple streams outside the focus of attention in naturalistic auditory scenes. ||| lars hausfeld ||| lars riecke ||| giancarlo valente ||| elia formisano ||| 
2019 ||| desynchronizing to be faster? perceptual- and attentional-modulation of brain rhythms at the sub-millisecond scale. ||| yasuki noguchi ||| yi xia ||| ryusuke kakigi ||| 
2017 ||| functional modular architecture underlying attentional control in aging. ||| zachary a. monge ||| benjamin r. geib ||| rachel e. siciliano ||| lauren e. packard ||| catherine w. tallman ||| david j. madden ||| 
2021 ||| auditory steady-state responses during and after a stimulus: cortical sources, and the influence of attention and musicality. ||| cassia low manting ||| bal ||| zs guly ||| s ||| fredrik ull ||| n ||| daniel lundqvist ||| 
2017 ||| network-targeted cerebellar transcranial magnetic stimulation improves attentional control. ||| michael esterman ||| michelle thai ||| hidefusa okabe ||| joe degutis ||| elyana saad ||| simon e. laganiere ||| mark a. halko ||| 
2019 ||| the ebb and flow of attention: between-subject variation in intrinsic connectivity and cognition associated with the dynamics of ongoing experience. ||| adam turnbull ||| hao-ting wang ||| jonathan w. schooler ||| elizabeth jefferies ||| daniel s. margulies ||| jonathan smallwood ||| 
2021 ||| spectral signature of attentional reorienting in the human brain. ||| sara spadone ||| viviana betti ||| carlo sestieri ||| vittorio pizzella ||| maurizio corbetta ||| stefania della penna ||| 
2017 ||| audio-visual synchrony and spatial attention enhance processing of dynamic visual stimulation independently and in parallel: a frequency-tagging study. ||| amra covic ||| christian keitel ||| emanuele porcu ||| erich schr ||| ger ||| matthias m. m ||| ller ||| 
2020 ||| spatiotemporal dynamics of attentional orienting and reorienting revealed by fast optical imaging in occipital and parietal cortices. ||| giorgia parisi ||| chiara mazzi ||| elisabetta colombari ||| antonio maria chiarelli ||| brian a. metzger ||| carlo a. marzi ||| silvia savazzi ||| 
2017 ||| a temporal dependency account of attentional inhibition in oculomotor control. ||| matthew d. weaver ||| wieske van zoest ||| clayton hickey ||| 
2018 ||| anticipatory neural dynamics of spatial-temporal orienting of attention in younger and older adults. ||| simone g. heideman ||| gustavo rohenkohl ||| joshua j. chauvin ||| clare e. palmer ||| freek van ede ||| anna christina nobre ||| 
2018 ||| quenching of spontaneous fluctuations by attention in human visual cortex. ||| rotem broday-dvir ||| shany grossman ||| edna furman-haran ||| rafael malach ||| 
2021 ||| the neural substrates of subliminal attentional bias and reduced inhibition in individuals with a higher bmi: a vbm and resting state connectivity study. ||| sofia adelaide osimo ||| luca piretti ||| silvio ionta ||| raffaella ida rumiati ||| marilena aiello ||| 
2019 ||| alpha and alpha-beta phase synchronization mediate the recruitment of the visuospatial attention network through the superior longitudinal fasciculus. ||| antea d'andrea ||| federico chella ||| tom r. marshall ||| vittorio pizzella ||| gian luca romani ||| ole jensen ||| laura marzetti ||| 
2018 ||| real-time decoding of covert attention in higher-order visual areas. ||| jinendra ekanayake ||| chloe hutton ||| gerard r. ridgway ||| frank scharnowski ||| nikolaus weiskopf ||| geraint rees ||| 
2019 ||| connectome-based models predict attentional control in aging adults. ||| stephanie fountain-zaragoza ||| shaadee samimy ||| monica d. rosenberg ||| ruchika shaurya prakash ||| 
2020 ||| connectome-based neurofeedback: a pilot study to improve sustained attention. ||| dustin scheinost ||| tiffany w. hsu ||| emily w. avery ||| michelle hampson ||| r. todd constable ||| marvin m. chun ||| monica d. rosenberg ||| 
2022 ||| distinct neural-behavioral correspondence within face processing and attention networks for the composite face effect. ||| changming chen ||| yixue lou ||| hong li ||| jiajin yuan ||| jiemin yang ||| heather winskel ||| shaozheng qin ||| 
2017 ||| attentional capture in visual search: capture and post-capture dynamics revealed by eeg. ||| heinrich r. liesefeld ||| anna marie liesefeld ||| thomas t ||| llner ||| hermann j. m ||| ller ||| 
2021 ||| children with attention-deficit/hyperactivity disorder spend more time in hyperconnected network states and less time in segregated network states as revealed by dynamic connectivity analysis. ||| heather shappell ||| kelly a. duffy ||| keri rosch ||| james j. pekar ||| stewart h. mostofsky ||| martin a. lindquist ||| jessica r. cohen ||| 
2019 ||| decoding attentional states for neurofeedback: mindfulness vs. wandering thoughts. ||| alexander y. zhigalov ||| erkka heinil ||| tiina parviainen ||| lauri parkkonen ||| aapo hyv ||| rinen ||| 
2017 ||| gamma-band activity reflects attentional guidance by facial expression. ||| kathrin m ||| sch ||| markus siegel ||| andreas k. engel ||| till r. schneider ||| 
2017 ||| linking dopaminergic reward signals to the development of attentional bias: a positron emission tomographic study. ||| brian a. anderson ||| hiroto kuwabara ||| dean f. wong ||| joshua roberts ||| arman rahmim ||| james r. brasic ||| susan m. courtney ||| 
2021 ||| neural correlates of visual attention during risky decision evidence integration. ||| john r. purcell ||| andrew jahn ||| justin m. fine ||| joshua w. brown ||| 
2020 ||| combining fmri during resting state and an attention bias task in children. ||| anita harrewijn ||| rany abend ||| julia linke ||| melissa a. brotman ||| nathan a. fox ||| ellen leibenluft ||| anderson m. winkler ||| daniel s. pine ||| 
2017 ||| metacognition of attention during tactile discrimination. ||| stephen whitmarsh ||| robert oostenveld ||| rita almeida ||| daniel lundqvist ||| 
2020 ||| a real-time marker of object-based attention in the human brain. a possible component of a "gate-keeping mechanism" performing late attentional selection in the ventro-lateral prefrontal cortex. ||| marcela perrone-bertolotti ||| samuel el bouza ||| di tiali ||| juan r. vidal ||| mathilde petton ||| anne claire croize ||| pierre deman ||| sylvain rheims ||| lorella minotti ||| manik bhattacharjee ||| monica baciu ||| philippe kahane ||| jean-philippe lachaux ||| 
2018 ||| neural circuitry underlying sustained attention in healthy adolescents and in adhd symptomatology. ||| laura o'halloran ||| zhipeng cao ||| kathy ruddy ||| lee jollans ||| matthew d. albaugh ||| andrea aleni ||| alexandra s. potter ||| nigel vahey ||| tobias banaschewski ||| sarah hohmann ||| arun l. w. bokde ||| uli bromberg ||| christian b ||| chel ||| erin burke quinlan ||| sylvane desrivi ||| res ||| herta flor ||| vincent frouin ||| penny a. gowland ||| robert whelan ||| 
2019 ||| self-directed down-regulation of auditory cortex activity mediated by real-time fmri neurofeedback augments attentional processes, resting cerebral perfusion, and auditory activation. ||| matthew s. sherwood ||| jason g. parker ||| emily e. diller ||| subhashini ganapathy ||| kevin b. bennett ||| carlos r. esquivel ||| jeremy t. nelson ||| 
2021 ||| common functional brain networks between attention deficit and disruptive behaviors in youth. ||| ting yat wong ||| han zhang ||| tonya white ||| liyuan xu ||| anqi qiu ||| 
2018 ||| distinct phase-amplitude couplings distinguish cognitive processes in human attention. ||| ravi v. chacko ||| byungchan kim ||| suh woo jung ||| amy daitch ||| jarod roland ||| nicholas v. metcalf ||| maurizio corbetta ||| gordon l. shulman ||| eric c. leuthardt ||| 
2017 ||| attentional processes, not implicit mentalizing, mediate performance in a perspective-taking task: evidence from stimulation of the temporoparietal junction. ||| idalmis santiesteban ||| simran kaur ||| geoffrey bird ||| caroline catmur ||| 
2017 ||| sensory-biased attention networks in human lateral frontal cortex revealed by intrinsic functional connectivity. ||| sean m. tobyne ||| david e. osher ||| samantha michalka ||| david somers ||| 
2018 ||| influence of talker discontinuity on cortical dynamics of auditory spatial attention. ||| golbarg mehraei ||| barbara g. shinn-cunningham ||| torsten dau ||| 
2020 ||| nested oscillations and brain connectivity during sequential stages of feature-based attention. ||| mattia f. pagnotta ||| david pascucci ||| gijs plomp ||| 
2019 ||| selective spatial attention involves two alpha-band components associated with distinct spatiotemporal and functional characteristics. ||| jianrong jia ||| fang fang ||| huan luo ||| 
2017 ||| temporal orienting precedes intersensory attention and has opposing effects on early evoked brain activity. ||| julian keil ||| ulrich pomper ||| nele feuerbach ||| daniel senkowski ||| 
2019 ||| auditory cortical generators of the frequency following response are modulated by intermodal attention. ||| thomas hartmann ||| nathan weisz ||| 
2017 ||| measuring the effects of attention to individual fingertips in somatosensory cortex using ultra-high field (7t) fmri. ||| alexander m. puckett ||| saskia bollmann ||| markus barth ||| ross cunnington ||| 
2019 ||| eye movements explain decodability during perception and cued attention in meg. ||| silvan c. quax ||| nadine dijkstra ||| mariel j. van staveren ||| sander e. bosch ||| marcel a. j. van gerven ||| 
2021 ||| fronto-temporal brain activity and connectivity track implicit attention to positive and negative social words in a novel socio-emotional stroop task. ||| maria arioli ||| gianpaolo basso ||| paolo poggi ||| nicola canessa ||| 
2020 ||| gradients of functional organization in posterior parietal cortex revealed by visual attention, visual short-term memory, and intrinsic functional connectivity. ||| ray w. lefco ||| james a. brissenden ||| abigail l. noyce ||| sean m. tobyne ||| david somers ||| 
2020 ||| multi-spectral oscillatory dynamics serving directed and divided attention. ||| marie c. mccusker ||| alex i. wiesman ||| mikki d. schantell ||| jacob a. eastman ||| tony w. wilson ||| 
2022 ||| attention enhances category representations across the brain with strengthened residual correlations to ventral temporal cortex. ||| arielle s. keller ||| akshay v. jagadeesh ||| lior bugatus ||| leanne m. williams ||| kalanit grill-spector ||| 
2019 ||| rapid adaptive adjustments of selective attention following errors revealed by the time course of steady-state visual evoked potentials. ||| marco steinhauser ||| s ||| ren k. andersen ||| 
2020 ||| fetal brain age estimation and anomaly detection using attention-based deep ensembles with uncertainty. ||| wen shi ||| guohui yan ||| yamin li ||| haotian li ||| tingting liu ||| cong sun ||| guangbin wang ||| yi zhang ||| yu zou ||| dan wu ||| 
2019 ||| theory of visual attention thalamic model for visual short-term memory capacity and top-down control: evidence from a thalamo-cortical structural connectivity analysis. ||| aurore menegaux ||| natan napi ||| rkowski ||| julia neitzel ||| adriana l. ruiz-rizzo ||| anders petersen ||| hermann j. m ||| ller ||| christian sorg ||| kathrin finke ||| 
2021 ||| frontal cortical regions associated with attention connect more strongly to central than peripheral v1. ||| sara a. sims ||| pinar demirayak ||| simone cedotal ||| kristina m. visscher ||| 
2019 ||| bidirectional signal exchanges and their mechanisms during joint attention interaction - a hyperscanning fmri study. ||| gadi goelman ||| rotem dan ||| gabriela st ||| el ||| heike tost ||| andreas meyer-lindenberg ||| edda bilek ||| 
2020 ||| attentional modulation of the auditory steady-state response across the cortex. ||| cassia low manting ||| lau m. andersen ||| bal ||| zs guly ||| s ||| fredrik ull ||| n ||| daniel lundqvist ||| 
2020 ||| spatial attention enhances cortical tracking of quasi-rhythmic visual stimuli. ||| davide tabarelli ||| christian keitel ||| joachim gross ||| daniel baldauf ||| 
2018 ||| cortical depth dependent population receptive field attraction by spatial attention in human v1. ||| barrie p. klein ||| alessio fracasso ||| jelle a. van dijk ||| chris l. e. paffen ||| susan f. te pas ||| serge o. dumoulin ||| 
2021 ||| multivariate analysis of eeg activity indexes contingent attentional capture. ||| jaap munneke ||| johannes jacobus fahrenfort ||| david w. sutterer ||| jan theeuwes ||| edward awh ||| 
2021 ||| breaking down the cocktail party: attentional modulation of cerebral audiovisual speech processing. ||| patrik wikman ||| elisa sahari ||| viljami r. salmela ||| alina leminen ||| miika leminen ||| matti laine ||| kimmo alho ||| 
2017 ||| expectation violation and attention to pain jointly modulate neural gain in somatosensory cortex. ||| francesca fardo ||| ryszard auksztulewicz ||| micah allen ||| martin jensen dietz ||| andreas roepstorff ||| karl j. friston ||| 
2020 ||| topographic specificity of alpha power during auditory spatial attention. ||| yuqi deng ||| inyong choi ||| barbara g. shinn-cunningham ||| 
2021 ||| attention reinforces human corticofugal system to aid speech perception in noise. ||| caitlin n. price ||| gavin m. bidelman ||| 
2021 ||| parallel cortical-brainstem pathways to attentional analgesia. ||| valeria oliva ||| rob gregory ||| wendy-elizabeth davies ||| lee harrison ||| rosalyn j. moran ||| anthony e. pickering ||| jonathan c. w. brooks ||| 
2021 ||| decreased emotional reactivity after 3-month socio-affective but not attention- or meta-cognitive-based mental training: a randomized, controlled, longitudinal fmri study. ||| pauline favre ||| philipp kanske ||| haakon g. engen ||| tania singer ||| 
2017 ||| spatiotemporal oscillatory dynamics of visual selective attention during a flanker task. ||| timothy j. mcdermott ||| alex i. wiesman ||| amy l. proskovec ||| elizabeth heinrichs-graham ||| tony w. wilson ||| 
2021 ||| attentional modulation of neural entrainment to sound streams in children with and without adhd. ||| aeron laffere ||| fred dick ||| lori l. holt ||| adam tierney ||| 
2019 ||| a division of labor between power and phase coherence in encoding attention to stimulus streams. ||| alessandro tavano ||| david poeppel ||| 
2019 ||| lateral prefrontal anodal transcranial direct current stimulation augments resolution of auditory perceptual-attentional conflicts. ||| nico adelh ||| fer ||| krutika gohil ||| susanne passow ||| christian beste ||| shu-chen li ||| 
2019 ||| age-related changes in attention control and their relationship with gait performance in older adults with high risk of falls. ||| natalia b. fernandez ||| m ||| lany hars ||| andrea trombetti ||| patrik vuilleumier ||| 
2018 ||| frequency-specific attentional modulation in human primary auditory cortex and midbrain. ||| lars riecke ||| judith c. peters ||| giancarlo valente ||| benedikt a. poser ||| valentin g. kemper ||| elia formisano ||| bettina sorger ||| 
2019 ||| prism adaptation enhances decoupling between the default mode network and the attentional networks. ||| meytal wilf ||| andrea serino ||| stephanie clarke ||| sonia crottaz-herbette ||| 
2021 ||| multi-band meg signatures of bold connectivity reorganization during visuospatial attention. ||| chiara favaretto ||| sara spadone ||| carlo sestieri ||| viviana betti ||| angelo cenedese ||| stefania della penna ||| maurizio corbetta ||| 
2018 ||| lateral prefrontal cortex lesion impairs regulation of internally and externally directed attention. ||| julia w. y. kam ||| anne-kristin solbakk ||| tor endestad ||| torstein meling ||| robert t. knight ||| 
2019 ||| attention differentially modulates the amplitude of resonance frequencies in the visual cortex. ||| rasa gulbinaite ||| diane h. m. roozendaal ||| rufin vanrullen ||| 
2021 ||| dika-nets: domain-invariant knowledge-guided attention networks for brain skull stripping of early developing macaques. ||| tao zhong ||| fenqiang zhao ||| yuchen pei ||| zhenyuan ning ||| lufan liao ||| zhengwang wu ||| yuyu niu ||| li wang ||| dinggang shen ||| yu zhang ||| gang li ||| 
2021 ||| integration and segregation across large-scale intrinsic brain networks as a marker of sustained attention and task-unrelated thought. ||| agnieszka zuberer ||| aaron kucyi ||| ayumu yamashita ||| charley m. wu ||| martin walter ||| eve m. valera ||| michael esterman ||| 
2020 ||| processing speed and attention training modifies autonomic flexibility: a mechanistic intervention study. ||| feng v. lin ||| ye tao ||| quanjing chen ||| mia anthony ||| zhengwu zhang ||| duje tadin ||| kathi l. heffner ||| 
2020 ||| overlapping attentional networks yield divergent behavioral predictions across tasks: neuromarkers for diffuse and focused attention? ||| esther x. w. wu ||| gwenisha j. liaw ||| rui zhe goh ||| tiffany t. y. chia ||| alisia m. j. chee ||| takashi obana ||| monica d. rosenberg ||| b. t. thomas yeo ||| christopher l. asplund ||| 
2017 ||| distinct roles of theta and alpha oscillations in the involuntary capture of goal-directed attention. ||| anthony m. harris ||| paul e. dux ||| caelyn n. jones ||| jason b. mattingley ||| 
2020 ||| network-based fmri-neurofeedback training of sustained attention. ||| gustavo s. p. pamplona ||| jennifer heldner ||| robert langner ||| yury koush ||| lars michels ||| silvio ionta ||| frank scharnowski ||| carlos ernesto garrido salmon ||| 
2020 ||| global effects of feature-based attention depend on surprise. ||| cooper a. smout ||| marta i. garrido ||| jason b. mattingley ||| 
2019 ||| using machine learning-based lesion behavior mapping to identify anatomical networks of cognitive dysfunction: spatial neglect and attention. ||| daniel wiesen ||| christoph sperber ||| grigori yourganov ||| christopher rorden ||| hans-otto karnath ||| 
2018 ||| high-alpha band synchronization across frontal, parietal and visual cortex mediates behavioral and neuronal effects of visuospatial attention. ||| muriel lobier ||| j. matias palva ||| satu palva ||| 
2020 ||| attention modulates the gating of primary somatosensory oscillations. ||| alex i. wiesman ||| tony w. wilson ||| 
2020 ||| relating alpha power modulations to competing visuospatial attention theories. ||| stefano gallotto ||| felix duecker ||| sanne ten oever ||| teresa schuhmann ||| tom a. de graaf ||| alexander thomas sack ||| 
2019 ||| homozygous lamc3 mutation links to structural and functional changes in visual attention networks. ||| buse m. urgen ||| yasemin topac ||| f. seyhun ustun ||| pinar demirayak ||| kader karli oguz ||| tulay kansu ||| serap saygi ||| tayfun ozcelik ||| huseyin boyaci ||| katja doerschner ||| 
2020 ||| parasympathetic arousal-related cortical activity is associated with attention during cognitive task performance. ||| anita d. barber ||| majnu john ||| pamela derosse ||| michael l. birnbaum ||| todd lencz ||| anil k. malhotra ||| 
2020 ||| isometric exercise facilitates attention to salient events in women via the noradrenergic system. ||| mara mather ||| ringo huang ||| david v. clewett ||| shawn e. nielsen ||| ricardo velasco ||| kristie tu ||| sophia han ||| briana l. kennedy ||| 
2017 ||| attention reorganizes connectivity across networks in a frequency specific manner. ||| soyoung kwon ||| masataka watanabe ||| elvira fischer ||| andreas m. bartels ||| 
2019 ||| diminished pre-stimulus alpha-lateralization suggests compromised self-initiated attentional control of auditory processing in old age. ||| martin j. dahl ||| liesa ilg ||| shu-chen li ||| susanne passow ||| markus werkle-bergner ||| 
2017 ||| age-related reduction of hemispheric lateralisation for spatial attention: an eeg study. ||| gemma learmonth ||| christopher s. y. benwell ||| gregor thut ||| monika harvey ||| 
2017 ||| internal and external attention and the default mode network. ||| hannah j. scheibner ||| carsten bogler ||| tobias gleich ||| john-dylan haynes ||| felix bermpohl ||| 
2017 ||| involuntary orienting of attention to a sound desynchronizes the occipital alpha rhythm and improves visual perception. ||| wenfeng feng ||| viola s. st ||| rmer ||| ant ||| gona mart ||| nez ||| john j. mcdonald ||| steven a. hillyard ||| 
2019 ||| default mode and visual network activity in an attention task: direct measurement with intracranial eeg. ||| jiajia li ||| sharif i. kronemer ||| wendy x. herman ||| hunki kwon ||| jun hwan ryu ||| christopher micek ||| ying wu ||| jason gerrard ||| dennis d. spencer ||| hal blumenfeld ||| 
2021 ||| learning with self-attention for rental market spatial dynamics in the atlanta metropolitan area. ||| xiaolu zhou ||| weitian tong ||| 
2021 ||| a stream prediction model based on attention-lstm. ||| le yan ||| changwei chen ||| tingting hang ||| youchuan hu ||| 
2021 ||| integrated image defogging network based on improved atmospheric scattering model and attention feature fusion. ||| shengmin he ||| zhixiang chen ||| fengli wang ||| meiya wang ||| 
2021 ||| latte: lstm self-attention based anomaly detection in e mbedded automotive platforms. ||| vipin kumar kukkala ||| sooryaa vignesh thiruloga ||| sudeep pasricha ||| 
2021 ||| algorithm-hardware co-design of attention mechanism on fpga devices. ||| xinyi zhang ||| yawen wu ||| peipei zhou ||| xulong tang ||| jingtong hu ||| 
2020 ||| meta-supervision for attention using counterfactual estimation. ||| seungtaek choi ||| haeju park ||| seung-won hwang ||| 
2019 ||| semantic based autoencoder-attention 3d reconstruction network. ||| fei hu ||| long ye ||| wei zhong ||| li fang ||| yun tie ||| qin zhang ||| 
2022 ||| edvam: a 3d eye-tracking dataset for visual attention modeling in a virtual museum. ||| yunzhan zhou ||| tian feng ||| shihui shuai ||| xiangdong li ||| lingyun sun ||| henry been-lirn duh ||| 
2017 ||| attention-based encoder-decoder model for answer selection in question answering. ||| yuanping nie ||| yi han ||| jiuming huang ||| bo jiao ||| aiping li ||| 
2021 ||| video summarization with a graph convolutional attention network. ||| ping li ||| chao tang ||| xianghua xu ||| 
2019 ||| attention shifting during child - robot interaction: a preliminary clinical study for children with autism spectrum disorder. ||| guo-bin wan ||| fu-hao deng ||| zijian jiang ||| sheng-zhao lin ||| cheng-lian zhao ||| boxun li ||| gong chen ||| shen-hong chen ||| xiao-hong cai ||| hao-bo wang ||| li-ping li ||| ting yan ||| jia-ming zhang ||| 
2020 ||| analysis and design of transformer-based cmos ultra-wideband millimeter-wave circuits for wireless applications: a review. ||| yiming yu ||| kai kang ||| 
2022 ||| attention-based graph resnet with focal loss for epileptic seizure detection. ||| changxu dong ||| yanna zhao ||| gaobo zhang ||| mingrui xue ||| dengyu chu ||| jiatong he ||| xinting ge ||| 
2021 ||| applying attention-based models for detecting cognitive processes and mental health conditions. ||| esa |||  villatoro-tello ||| shantipriya parida ||| sajit kumar ||| petr motl ||| cek ||| 
2019 ||| improving user attribute classification with text and social network attention. ||| yumeng li ||| liang yang ||| bo xu ||| jian wang ||| hongfei lin ||| 
2019 ||| unsupervised object transfiguration with attention. ||| zihan ye ||| fan lyu ||| linyan li ||| yu sun ||| qiming fu ||| fuyuan hu ||| 
2019 ||| multi-region risk-sensitive cognitive ensembler for accurate detection of attention-deficit/hyperactivity disorder. ||| vasily sachnev ||| suresh sundaram ||| narasimhan sundararajan ||| belathur suresh mahanand ||| muhammad w. azeem ||| saras saraswathi ||| 
2022 ||| sentic computing for aspect-based opinion summarization using multi-head attention with feature pooled pointer generator network. ||| akshi kumar ||| simran seth ||| shivam gupta ||| shivam maini ||| 
2022 ||| design and deployment of an image polarity detector with visual attention. ||| edoardo ragusa ||| tommaso apicella ||| christian gianoglio ||| rodolfo zunino ||| paolo gastaldo ||| 
2018 ||| attentional bias pattern recognition in spiking neural networks from spatio-temporal eeg data. ||| zohreh gholami doborjeh ||| maryam gholami doborjeh ||| nikola k. kasabov ||| 
2021 ||| modeling articulatory rehearsal in an attention-based model of working memory. ||| beno ||| t lemaire ||| charlotte heuer ||| sophie portrat ||| 
2021 ||| real-time lane detection by using biologically inspired attention mechanism to learn contextual information. ||| lu zhang ||| fengling jiang ||| bin kong ||| jing yang ||| can wang ||| 
2021 ||| hana: hierarchical attention network assembling for semantic segmentation. ||| wei liu ||| ding li ||| hongqi su ||| 
2022 ||| cat-bigru: convolution and attention with bi-directional gated recurrent unit for self-deprecating sarcasm detection. ||| ashraf kamal ||| muhammad abulaish ||| 
2022 ||| to ban or not to ban: bayesian attention networks for reliable hate speech detection. ||| kristian miok ||| blaz skrlj ||| daniela zaharie ||| marko robnik-sikonja ||| 
2021 ||| attention-augmented machine memory. ||| xin lin ||| guoqiang zhong ||| kang chen ||| qingyang li ||| kaizhu huang ||| 
2021 ||| a convolutional stacked bidirectional lstm with a multiplicative attention mechanism for aspect category and sentiment detection. ||| ashok kumar j ||| tina esther trueman ||| erik cambria ||| 
2021 ||| electroencephalography-based auditory attention decoding: toward neurosteered hearing devices. ||| simon geirnaert ||| servaas vandecappelle ||| emina alickovic ||| alain de cheveign ||| edmund c. lalor ||| bernd t. meyer ||| sina miran ||| tom francart ||| alexander bertrand ||| 
2021 ||| word and graph attention networks for semi-supervised classification. ||| jing zhang ||| mengxi li ||| kaisheng gao ||| shunmei meng ||| cangqi zhou ||| 
2021 ||| proaid: path-based reasoning for self-attentional disease prediction. ||| xudong lu ||| lizhen cui ||| zhenchao sun ||| yuening zhu ||| 
2022 ||| e2eet: from pipeline to end-to-end entity typing via transformer-based embeddings. ||| michael stewart ||| wei liu ||| 
2020 ||| exploiting review embedding and user attention for item recommendation. ||| yatong sun ||| guibing guo ||| xu chen ||| penghai zhang ||| xingwei wang ||| 
2021 ||| a relative position attention network for aspect-based sentiment analysis. ||| chao wu ||| qingyu xiong ||| min gao ||| qiude li ||| yang yu ||| kaige wang ||| 
2021 ||| convolutional recurrent neural network with attention for vietnamese speech to text problem in the operating room. ||| trinh tan dat ||| le tran anh dang ||| vu ngoc thanh sang ||| nhi lam thuy le ||| pham the bao ||| 
2017 ||| adequacy assessment of power distribution network with large fleets of phevs considering condition-dependent transformer faults. ||| jun tan ||| lingfeng wang ||| 
2019 ||| implementation of bidirectional resonant dc transformer in hybrid ac/dc micro-grid. ||| jingjing huang ||| jianfang xiao ||| changyun wen ||| peng wang ||| aimin zhang ||| 
2017 ||| a novel wireless multifunctional electronic current transformer based on zigbee-based communication. ||| kun-long chen ||| yan-ru chen ||| yuan-pin tsai ||| nanming chen ||| 
2021 ||| wide-band current transformers for traveling-waves-based protection applications. ||| amir ameli ||| khaled a. saleh ||| ehab f. el-saadany ||| magdy m. a. salama ||| hatem h. zeineldin ||| 
2018 ||| parallel operation of transformers with on load tap changer and photovoltaic systems with reactive power control. ||| markus kraiczy ||| thomas stetz ||| martin braun ||| 
2017 ||| optimized reactive power supports using transformer tap stagger in distribution networks. ||| linwei chen ||| haiyu li ||| 
2018 ||| a smart iec 61850 merging unit for impending fault detection in transformers. ||| ahmed m. gaouda ||| atef abdrabou ||| khaled bashir shaban ||| mutaz khairalla ||| ahmed m. abdrabou ||| ramadan el shatshat ||| m. m. a. salama ||| 
2018 ||| multiple resonances mitigation of paralleled inverters in a solid-state transformer (sst) enabled ac microgrid. ||| qing ye ||| ran mo ||| hui li ||| 
2019 ||| technical and economic impact of pv-bess charging station on transformer life: a case study. ||| carolina m. affonso ||| mladen kezunovic ||| 
2018 ||| a novel association rule mining method of big data for power transformers state parameters based on probabilistic graph model. ||| gehao sheng ||| huijuan hou ||| xiuchen jiang ||| yufeng chen ||| 
2019 ||| applicability of solid-state transformers in today's and future distribution grids. ||| jonas e. huber ||| johann w. kolar ||| 
2021 ||| optimal dispatch with transformer dynamic thermal rating in adns incorporating high pv penetration. ||| yinxiao li ||| yi wang ||| qixin chen ||| 
2018 ||| tracking transformer tap position in real-time distribution network power flow applications. ||| rabih a. jabr ||| izudin dzafic ||| sami h. karaki ||| 
2021 ||| online detection of inter-turn winding faults in single-phase distribution transformers using smart meter data. ||| kavya ashok ||| dan li ||| nagi gebraeel ||| deepak divan ||| 
2018 ||| a reconstruction of the wams-detected transformer sympathetic inrush phenomenon. ||| urban rudez ||| rafael mihalic ||| 
2020 ||| detection of hidden transformer tap change command attacks in transmission networks. ||| shantanu chakrabarty ||| biplab sikdar ||| 
2020 ||| distribution network marginal costs: enhanced ac opf including transformer degradation. ||| panagiotis andrianesis ||| michael caramanis ||| 
2019 ||| real-time primary frequency regulation using load power control by smart transformers. ||| giovanni de carne ||| giampaolo buticchi ||| marco liserre ||| constantine vournas ||| 
2020 ||| synchrophasor-based condition monitoring of instrument transformers using clustering approach. ||| bo cui ||| anurag k. srivastava ||| paramarshi banerjee ||| 
2018 ||| optimal penetration of home energy management systems in distribution networks considering transformer aging. ||| daniel julius olsen ||| mushfiqur r. sarker ||| miguel a. ortega-vazquez ||| 
2018 ||| load control using sensitivity identification by means of smart transformer. ||| giovanni de carne ||| giampaolo buticchi ||| marco liserre ||| constantine vournas ||| 
2021 ||| synchronization of low voltage grids fed by smart and conventional transformers. ||| stefano giacomuzzi ||| giovanni de carne ||| sante pugliese ||| giuseppe buja ||| marco liserre ||| ali kazerooni ||| 
2017 ||| co-optimization of distribution transformer aging and energy arbitrage using electric vehicles. ||| mushfiqur r. sarker ||| daniel julius olsen ||| miguel a. ortega-vazquez ||| 
2017 ||| a single-phase on-line ups system for multiple load transformers. ||| syed sabir hussain bukhari ||| byung-il kwon ||| 
2017 ||| a flexible two-section transmission-line transformer design approach for complex source and real load impedances. ||| xiaolong wang ||| masataka ohira ||| zhewang ma ||| 
2017 ||| design and testing of a novel rotary transformer for rotary ultrasonic machining. ||| jiyue duan ||| bin lin ||| qiang yang ||| yujia luan ||| 
2018 ||| a ka band cmos lo distribution buffer using transformer-based three-way power divider. ||| bowen ding ||| shengyue yuan ||| chen zhao ||| li tao ||| xiaoyun li ||| tong tian ||| 
2017 ||| a tunable transformer-based cmos directional coupler for uhf rfid readers. ||| yongan zheng ||| le ye ||| xiucheng hao ||| ying guo ||| runhua wang ||| huailin liao ||| 
2018 ||| fully-integrated linear cmos power amplifier with proportional series combining transformer for s-band applications. ||| haigang wu ||| bin li ||| zhaohui wu ||| yunfeng hu ||| kun wang ||| zhen liang ||| yang liu ||| 
2017 ||| characteristic analysis of relatively high speed, loosely coupled rotating excitation transformers in hev and ev drive motor excitation systems. ||| yong yu ||| xudong wang ||| 
2020 ||| equivalent circuit of interleaved air-core toroidal transformer derived from analogy with coupled transmission lines. ||| kazuki hashimoto ||| takashi hikihara ||| 
2021 ||| a 21.6dbm cmos power amplifier using a compact high-k output transformer for x-band application. ||| bingfei dou ||| siwei huang ||| jiajin song ||| xiao li ||| zongming duan ||| xiaojiang yao ||| dongping xiao ||| yongjie li ||| liguo sun ||| 
2017 ||| a three-phase off-line ups system for transformer coupled loads. ||| syed sabir hussain bukhari ||| muhammad ayub ||| shahid atiq ||| byung-il kwon ||| 
2019 ||| an equivalent lumped circuit model for on-chip helical transformers [ieice electronics express vol. 15 (2018) no. 3 pp. 20170818]. ||| wanghui zou ||| jin hu ||| diping chen ||| yun zeng ||| 
2018 ||| an equivalent lumped circuit model for on-chip helical transformers. ||| wanghui zou ||| jin hu ||| diping chen ||| yun zeng ||| 
2018 ||| a 1-13 ghz cmos low-noise amplifier using compact transformer-based inter-stage networks. ||| hyohyun nam ||| junsik park ||| jung-dong park ||| 
2019 ||| to the attention of mobile software developers: guess what, test your app! ||| luis cruz ||| rui abreu ||| david lo ||| 
2019 ||| racmf: robust attention convolutional matrix factorization for rating prediction. ||| biqing zeng ||| qi shang ||| xuli han ||| feng zeng ||| min zhang ||| 
2021 ||| a new method of hybrid time window embedding with transformer-based traffic data classification in iot-networked environment. ||| rafal kozik ||| marek pawlicki ||| michal choras ||| 
2022 ||| travel order quantity prediction via attention-based bidirectional lstm networks. ||| fei yang ||| huyin zhang ||| shiming tao ||| 
2020 ||| a parallel computing-based deep attention model for named entity recognition. ||| xiaojun liu ||| ning yang ||| yu jiang ||| lichuan gu ||| xianzhang shi ||| 
2021 ||| a new method of abnormal behavior detection using lstm network with temporal attention mechanism. ||| limin xia ||| zhenmin li ||| 
2020 ||| pulmonary nodule image super-resolution using multi-scale deep residual channel attention network with joint optimization. ||| yongjun qi ||| junhua gu ||| weixun li ||| zepei tian ||| yajuan zhang ||| juanping geng ||| 
2018 ||| a camera-based attention level assessment tool designed for classroom usage. ||| chun-hsiung tseng ||| yung-hui chen ||| 
2021 ||| a novel attention fusion network-based framework to ensemble the predictions of cnns for lymph node metastasis detection. ||| chinmay rane ||| raj mehrotra ||| shubham bhattacharyya ||| mukta sharma ||| mahua bhattacharya ||| 
2022 ||| improving the readability and saliency of abstractive text summarization using combination of deep neural networks equipped with auxiliary attention mechanism. ||| hassan aliakbarpour ||| mohammad taghi manzuri ||| amir masoud rahmani ||| 
2017 ||| deep neural network with attention model for scene text recognition. ||| shuohao li ||| min tang ||| qiang guo ||| jun lei ||| jun zhang ||| 
2021 ||| hybrid attention mechanism for few-shot relational learning of knowledge graphs. ||| ruixin ma ||| zeyang li ||| fangqing guo ||| liang zhao ||| 
2020 ||| skeleton-based attention-aware spatial-temporal model for action detection and recognition. ||| ran cui ||| aichun zhu ||| jingran wu ||| gang hua ||| 
2022 ||| parallax-based second-order mixed attention for stereo image super-resolution. ||| chenyang duan ||| nanfeng xiao ||| 
2020 ||| dual attention module and multi-label based fully convolutional network for crowd counting. ||| suyu wang ||| bin yang ||| bo liu ||| guanghui zheng ||| 
2019 ||| crowd counting using a self-attention multi-scale cascaded network. ||| he li ||| shihui zhang ||| weihang kong ||| 
2019 ||| attention-based spatial-temporal hierarchical convlstm network for action recognition in videos. ||| fei xue ||| hongbing ji ||| wenbo zhang ||| yi cao ||| 
2021 ||| l4net: an anchor-free generic object detector with attention mechanism for autonomous driving. ||| yanan wu ||| songhe feng ||| xiankai huang ||| zizhang wu ||| 
2021 ||| a tri-attention enhanced graph convolutional network for skeleton-based action recognition. ||| xingming li ||| wei zhai ||| yang cao ||| 
2022 ||| dpanet: dual pooling-aggregated attention network for fish segmentation. ||| wenbo zhang ||| chaoyi wu ||| zhenshan bao ||| 
2022 ||| multi-stream adaptive spatial-temporal attention graph convolutional network for skeleton-based action recognition. ||| yu lubin ||| lianfang tian ||| qiliang du ||| jameel ahmed bhutto ||| 
2021 ||| standing out in a networked communication context: toward a network contingency model of public attention. ||| aimei yang ||| adam j. saffer ||| 
2019 ||| selective attention in the news feed: an eye-tracking study on the perception and selection of political news posts on facebook. ||| michael s ||| lflow ||| svenja sch ||| fer ||| stephan winter ||| 
2018 ||| attention and amplification in the hybrid media system: the composition and activity of donald trump's twitter following during the 2016 presidential election. ||| yini zhang ||| chris wells ||| song wang ||| karl rohe ||| 
2021 ||| going with the flow: nudging attention online. ||| angela xiao wu ||| harsh taneja ||| james g. webster ||| 
2021 ||| book review: subprime attention crisis: advertising and the time bomb at the heart of the internet. ||| mario haim ||| 
2021 ||| identification of three-dimensional defect topology in concrete structures based on self-attention network using hammering response data. ||| masaya shimada ||| takahiko kurahashi ||| yuki murakami ||| fujio ikeda ||| ikuo ihara ||| 
2018 ||| a new deep spatial transformer convolutional neural network for image saliency detection. ||| xinsheng zhang ||| teng gao ||| dongdong gao ||| 
2021 ||| few-shot text classification by leveraging bi-directional attention and cross-class knowledge. ||| ning pang ||| xiang zhao ||| wei wang ||| weidong xiao ||| deke guo ||| 
2020 ||| faclstm: convlstm with focused attention for scene text recognition. ||| qingqing wang ||| ye huang ||| wenjing jia ||| xiangjian he ||| michael blumenstein ||| shujing lyu ||| yue lu ||| 
2021 ||| task-wise attention guided part complementary learning for few-shot image classification. ||| gong cheng ||| ruimin li ||| chunbo lang ||| junwei han ||| 
2017 ||| 45-ghz and 60-ghz 90 nm cmos power amplifiers with a fully symmetrical 8-way transformer power combiner. ||| zhengdong jiang ||| kaizhe guo ||| peng huang ||| yiming fan ||| chenxi zhao ||| yong-ling ban ||| jun liu ||| kai kang ||| 
2020 ||| hybrid first and second order attention unet for building segmentation in remote sensing images. ||| nanjun he ||| leyuan fang ||| antonio plaza ||| 
2021 ||| text information aggregation with centrality attention. ||| jingjing gong ||| hang yan ||| yining zheng ||| qipeng guo ||| xipeng qiu ||| xuanjing huang ||| 
2019 ||| haptics-mediated approaches for enhancing sustained attention: framework and challenges. ||| dangxiao wang ||| teng li ||| naqash afzal ||| jicong zhang ||| yuru zhang ||| 
2019 ||| irregular scene text detection via attention guided border labeling. ||| jie chen ||| zhouhui lian ||| yizhi wang ||| yingmin tang ||| jianguo xiao ||| 
2020 ||| multi-attention based cross-domain beauty product image retrieval. ||| zhihui wang ||| xing liu ||| jiawen lin ||| caifei yang ||| haojie li ||| 
2021 ||| dual-axial self-attention network for text classification. ||| xiaochuan zhang ||| xipeng qiu ||| jianmin pang ||| fudong liu ||| xingwei li ||| 
2021 ||| dual attention autoencoder for all-weather outdoor lighting estimation. ||| piaopiao yu ||| jie guo ||| longhai wu ||| cheng zhou ||| mengtian li ||| chenchen wang ||| yanwen guo ||| 
2019 ||| arpnet: attention region proposal network for 3d object detection. ||| yangyang ye ||| chi zhang ||| xiaoli hao ||| 
2017 ||| common patterns of online collective attention flow. ||| yong li ||| xiaofeng meng ||| qiang zhang ||| jiang zhang ||| changqing wang ||| 
2021 ||| pay attention to raw traces: a deep learning architecture for end-to-end profiling attacks. ||| xiangjun lu ||| chi zhang ||| pei cao ||| dawu gu ||| haining lu ||| 
2019 ||| channel-wise attention model-based fire and rating level detection in video. ||| yirui wu ||| yuechao he ||| palaiahnakote shivakumara ||| ziming li ||| hongxin guo ||| tong lu ||| 
2018 ||| temporal enhanced sentence-level attention model for hashtag recommendation. ||| jun ma ||| chong feng ||| ge shi ||| xuewen shi ||| heyang huang ||| 
2020 ||| affective computing study of attention recognition for the 3d guide system. ||| li-hong juang ||| ming-ni wu ||| cian-huei lin ||| 
2021 ||| agg: a novel intelligent network traffic prediction method based on joint attention and gcn-gru. ||| huaifeng shi ||| chengsheng pan ||| li yang ||| xiangxiang gu ||| 
2021 ||| spatial-channel attention-based class activation mapping for interpreting cnn-based image classification models. ||| nianwen si ||| wenlin zhang ||| dan qu ||| xiangyang luo ||| heyu chang ||| tong niu ||| 
2021 ||| f3snet: a four-step strategy for qim steganalysis of compressed speech based on hierarchical attention network. ||| chuanpeng guo ||| wei yang ||| mengxia shuai ||| liusheng huang ||| 
2021 ||| r2au-net: attention recurrent residual convolutional neural network for multimodal medical image segmentation. ||| qiang zuo ||| songyu chen ||| zhifang wang ||| 
2021 ||| attention-guided digital adversarial patches on visual detection. ||| dapeng lang ||| deyun chen ||| ran shi ||| yongjun he ||| 
2021 ||| a hierarchical approach for advanced persistent threat detection with attention-based graph neural networks. ||| zitong li ||| xiang cheng ||| lixiao sun ||| ji zhang ||| bing chen ||| 
2021 ||| hierarchical attention graph embedding networks for binary code similarity against compilation diversity. ||| yan wang ||| peng jia ||| cheng huang ||| jiayong liu ||| peisong he ||| 
2020 ||| slam: a malware detection method based on sliding local attention mechanism. ||| jun chen ||| shize guo ||| xin ma ||| haiying li ||| jinhong guo ||| ming chen ||| zhisong pan ||| 
2022 ||| rumor detection with bidirectional graph attention networks. ||| xiaohui yang ||| hailong ma ||| miao wang ||| 
2021 ||| social network spam detection based on albert and combination of bi-lstm with self-attention. ||| guangxia xu ||| daiqi zhou ||| jun liu ||| 
2021 ||| compressed wavelet tensor attention capsule network. ||| xiushan liu ||| chun shan ||| qin zhang ||| jun cheng ||| peng xu ||| 
2021 ||| an enhanced visual attention siamese network that updates template features online. ||| wenqiu zhu ||| guang zou ||| qiang liu ||| zhigao zeng ||| 
2021 ||| eeg correlates of sustained attention variability during discrete multi-finger force control tasks. ||| cong peng ||| weiwei peng ||| wanwei feng ||| yuru zhang ||| jing xiao ||| dangxiao wang ||| 
2021 ||| high generalization performance structured self-attention model for knapsack problem. ||| man ding ||| congying han ||| tiande guo ||| 
2021 ||| an unsupervised approach for fault diagnosis of power transformers. ||| luis dias ||| miguel ribeiro ||| armando leit ||| o ||| lu ||| s guimar ||| es ||| leonel m. carvalho ||| manuel a. matos ||| ricardo j. bessa ||| 
2019 ||| attentional multilabel learning over graphs: a message passing approach. ||| kien do ||| truyen tran ||| thin nguyen ||| svetha venkatesh ||| 
2019 ||| dynamic attention-integrated neural network for session-based news recommendation. ||| lemei zhang ||| peng liu ||| jon atle gulla ||| 
2019 ||| temporal pattern attention for multivariate time series forecasting. ||| shun-yao shih ||| fan-keng sun ||| hung-yi lee ||| 
2020 ||| convolutional multi-head self-attention on memory for aspect sentiment classification. ||| yaojie zhang ||| bing xu ||| tiejun zhao ||| 
2022 ||| highway lane change decision-making via attention-based deep reinforcement learning. ||| junjie wang ||| qichao zhang ||| dongbin zhao ||| 
2021 ||| global-attention-based neural networks for vision language intelligence. ||| pei liu ||| yingjie zhou ||| dezhong peng ||| dapeng wu ||| 
2019 ||| hierarchical visual attention model for saliency detection inspired by avian visual pathways. ||| xiaohua wang ||| haibin duan ||| 
2021 ||| mu-gan: facial attribute editing based on multi-attention mechanism. ||| ke zhang ||| yukun su ||| xiwang guo ||| liang qi ||| zhenbing zhao ||| 
2020 ||| a recurrent attention and interaction model for pedestrian trajectory prediction. ||| xuesong li ||| yating liu ||| kunfeng wang ||| fei-yue wang ||| 
2020 ||| a spatial-temporal attention model for human trajectory prediction. ||| xiaodong zhao ||| yaran chen ||| jin guo ||| dongbin zhao ||| 
2021 ||| a transformer-based architecture for fake news classification. ||| divyam mehta ||| aniket dwivedi ||| arunabha patra ||| m. anand kumar ||| 
2022 ||| abusive bangla comments detection on facebook using transformer-based deep learning models. ||| tanjim taharat aurpa ||| rifat sadik ||| md. shoaib ahmed ||| 
2022 ||| caviar-ws-based han: conditional autoregressive value at risk-water sailfish-based hierarchical attention network for emotion classification in covid-19 text review data. ||| b. venkateswarlu ||| v. viswanath shenoi ||| praveen tumuluru ||| 
2020 ||| a mathematical model of local and global attention in natural scene viewing. ||| noa malem-shinitski ||| manfred opper ||| sebastian reich ||| lisa schwetlick ||| stefan a. seelig ||| ralf engbert ||| 
2021 ||| learning, visualizing and exploring 16s rrna structure using an attention-based deep neural network. ||| zhengqiao zhao ||| stephen woloszynek ||| felix agbavor ||| joshua chang mell ||| bahrad a. sokhansanj ||| gail l. rosen ||| 
2020 ||| the impact of news exposure on collective attention in the united states during the 2016 zika epidemic. ||| michele tizzoni ||| andr |||  panisson ||| daniela paolotti ||| ciro cattuto ||| 
2020 ||| flocking in complex environments - attention trade-offs in collective information processing. ||| parisa rahmani ||| fernando peruani ||| pawel romanczuk ||| 
2020 ||| insight into the protein solubility driving forces with neural attention. ||| daniele raimondi ||| gabriele orlando ||| piero fariselli ||| yves moreau ||| 
2021 ||| rational inattention and tonic dopamine. ||| john g. mikhael ||| lucy lai ||| samuel j. gershman ||| 
2019 ||| deep attention networks reveal the rules of collective motion in zebrafish. ||| francisco j. h. heras ||| francisco romero-ferrero ||| robert hinz ||| gonzalo g. de polavieja ||| 
2019 ||| prediction of off-target specificity and cell-specific fitness of crispr-cas system using attention boosted deep learning and network-based gene feature. ||| qiao liu ||| di he ||| lei xie ||| 
2021 ||| aim: a network model of attention in auditory cortex. ||| kenny f. chou ||| kamal sen ||| 
2021 ||| analysis of spiking synchrony in visual cortex reveals distinct types of top-down modulation signals for spatial and object-based attention. ||| nobuhiko wagatsuma ||| brian hu ||| r ||| diger von der heydt ||| ernst niebur ||| 
2019 ||| a computational account of threat-related attentional bias. ||| toby wise ||| jochen michely ||| peter dayan ||| raymond j. dolan ||| 
2021 ||| attention-enhanced gradual machine learning for entity resolution. ||| ping zhong ||| zhanhuai li ||| qun chen ||| boyi hou ||| 
2021 ||| transformer-based models for automatic identification of argument relations: a cross-domain evaluation. ||| ramon ruiz-dolz ||| jos |||  alemany ||| stella heras barber ||| ana garc ||| a-fornes ||| 
2021 ||| tsnet: three-stream self-attention network for rgb-d indoor semantic segmentation. ||| wujie zhou ||| jianzhong yuan ||| jingsheng lei ||| ting luo ||| 
2018 ||| a defense of ad blocking and consumer inattention. ||| alexander zambrano ||| caleb pickard ||| 
2021 ||| the ethics of inattention: revitalising civil inattention as a privacy-protecting mechanism in public spaces. ||| tamar sharon ||| bert-jaap koops ||| 
2020 ||| the effects of long-term child-robot interaction on the attention and the engagement of children with autism. ||| maria van otterdijk ||| manon de korte ||| iris van den berk-smeekens ||| jorien hendrix ||| martine van dongen-boomsma ||| jenny den boer ||| jan k. buitelaar ||| tino lourens ||| jeffrey glennon ||| wouter staal ||| emilia i. barakova ||| 
2018 ||| deep learning systems for estimating visual attention in robot-assisted therapy of children with autism and intellectual disability. ||| alessandro g. di nuovo ||| daniela conti ||| grazia trubia ||| serafino buono ||| santo di nuovo ||| 
2021 ||| topic classification of electric vehicle consumer experiences with transformer-based deep learning. ||| sooji ha ||| daniel j. marchetto ||| sameer dharur ||| omar isaac asensio ||| 
2020 ||| two is better than one. improved attention guiding in ar by combining techniques. ||| philipp hein ||| max bernhagen ||| angelika c. bullinger ||| 
2021 ||| rsanet: towards real-time object detection with residual semantic-guided attention feature pyramid network. ||| quan zhou ||| jie wang ||| jia liu ||| shenghua li ||| weihua ou ||| xin jin ||| 
2020 ||| a topic attention mechanism and factorization machines based mobile application recommendation method. ||| buqing cao ||| junjie chen ||| jianxun liu ||| yiping wen ||| 
2021 ||| tprune: efficient transformer pruning for mobile devices. ||| jiachen mao ||| huanrui yang ||| ang li ||| hai li ||| yiran chen ||| 
2020 ||| the plausibility of using unmanned aerial vehicles as a serious game for dealing with attention deficit-hyperactivity disorder. ||| sonia l ||| pez ||| jos ||| -antonio cervantes ||| salvador cervantes ||| jahaziel molina ||| francisco cervantes ||| 
2021 ||| complementary interactions between classical and top-down driven inhibitory mechanisms of attention. ||| sock ching low ||| vasiliki vouloutsi ||| paul f. m. j. verschure ||| 
2018 ||| interactions between astrocytes and the reward-attention circuit: a model for attention focusing in the presence of nicotine. ||| karine guimar ||| es ||| daniele q. m. madureira ||| alexandre l. madureira ||| 
2019 ||| a mathematical model of the interaction between bottom-up and top-down attention controllers in response to a target and a distractor in human beings. ||| golnaz baghdadi ||| farzad towhidkhah ||| reza rostami ||| 
2020 ||| a bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes. ||| raymundo ramirez-pedraza ||| natividad vargas ||| carlos johnnatan sandoval ||| juan luis del valle-padilla ||| f ||| lix ramos ||| 
2019 ||| an eye-tracking attention based model for abstractive text headline. ||| jiehang xie ||| xiaoming wang ||| xinyan wang ||| guangyao pang ||| xueyang qin ||| 
2017 ||| public attention, social media, and the edward snowden saga. ||| yong jin park ||| s. mo jang ||| 
2019 ||| 'coherent clusters' or 'fuzzy zones' - understanding attention and structure in online political participation. ||| anders olof larsson ||| 
2021 ||| ava: a financial service chatbot based on deep bidirectional transformers. ||| shi yu ||| yuxin chen ||| hussain zaidi ||| 
2021 ||| the role of working memory and attention in older workers' learning. ||| effrosyni angelopoulou ||| zoi karabatzaki ||| athanasios drigas ||| 
2019 ||| entity recognition in chinese clinical text using attention-based cnn-lstm-crf. ||| buzhou tang ||| xiaolong wang ||| jun yan ||| qingcai chen ||| 
2021 ||| transformers-sklearn: a toolkit for medical language understanding with transformer-based models. ||| feihong yang ||| xuwen wang ||| hetong ma ||| jiao li ||| 
2019 ||| a deep learning model incorporating part of speech and self-matching attention for named entity recognition of chinese electronic medical records. ||| xiaoling cai ||| shoubin dong ||| jinlong hu ||| 
2020 ||| autodiscern: rating the quality of online health information with hierarchical encoder attention-based neural networks. ||| laura kinkead ||| ahmed allam ||| michael krauthammer ||| 
2020 ||| an interpretable risk prediction model for healthcare with pattern attention. ||| sundreen asad kamal ||| changchang yin ||| buyue qian ||| ping zhang ||| 
2021 ||| u-net combined with multi-scale attention mechanism for liver segmentation in ct images. ||| jiawei wu ||| shengqiang zhou ||| songlin zuo ||| yiyin chen ||| weiqin sun ||| jiang luo ||| jiantuan duan ||| hui wang ||| deguang wang ||| 
2020 ||| effective attention-based network for syndrome differentiation of aids. ||| huaxin pang ||| shikui wei ||| yufeng zhao ||| liyun he ||| jian wang ||| baoyan liu ||| yao zhao ||| 
2021 ||| automatic detection of actionable radiology reports using bidirectional encoder representations from transformers. ||| yuta nakamura ||| shouhei hanaoka ||| yukihiro nomura ||| takahiro nakao ||| soichiro miki ||| takeyuki watadani ||| takeharu yoshikawa ||| naoto hayashi ||| osamu abe ||| 
2019 ||| an attention-based deep learning model for clinical named entity recognition of chinese electronic medical records. ||| luqi li ||| jie zhao ||| li hou ||| yunkai zhai ||| jinming shi ||| fangfang cui ||| 
2019 ||| attention-based deep residual learning network for entity relation extraction in chinese emrs. ||| zhichang zhang ||| tong zhou ||| yu zhang ||| yali pang ||| 
2020 ||| multi-modality self-attention aware deep network for 3d biomedical segmentation. ||| xibin jia ||| yunfeng liu ||| zhenghan yang ||| dawei yang ||| 
2020 ||| interpretable clinical prediction via attention-based neural network. ||| peipei chen ||| wei dong ||| jinliang wang ||| xudong lu ||| uzay kaymak ||| zhengxing huang ||| 
2021 ||| constrained transformer network for ecg signal processing and arrhythmia classification. ||| chao che ||| peiliang zhang ||| min zhu ||| yue qu ||| bo jin ||| 
2021 ||| transformer-based deep neural network language models for alzheimer's disease risk assessment from targeted speech. ||| alireza roshanzamir ||| hamid aghajan ||| mahdieh soleymani baghshah ||| 
2022 ||| predicting mirna-disease associations via layer attention graph convolutional network model. ||| han han ||| rong zhu ||| jin-xing liu ||| ling-yun dai ||| 
2021 ||| a bci video game using neurofeedback improves the attention of children with autism. ||| jose mercado ||| lizbeth escobedo ||| monica tentori ||| 
2021 ||| empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum. ||| matias volonte ||| reza ghaiumy anaraky ||| rohith venkatakrishnan ||| roshan venkatakrishnan ||| bart p. knijnenburg ||| andrew t. duchowski ||| sabarish v. babu ||| 
2018 ||| leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups. ||| bertrand schneider ||| kshitij sharma ||| s ||| bastien cuendet ||| guillaume zufferey ||| pierre dillenbourg ||| roy pea ||| 
2019 ||| unsupervised learning of depth estimation based on attention model and global pose optimization. ||| renyue dai ||| yongbin gao ||| zhijun fang ||| xiaoyan jiang ||| anjie wang ||| juan zhang ||| cengsi zhong ||| 
2019 ||| vehicle joint make and model recognition with multiscale attention windows. ||| sina ghassemi ||| attilio fiandrotti ||| emanuele caimotti ||| gianluca francini ||| enrico magli ||| 
2020 ||| comprehensive feature fusion mechanism for video-based person re-identification via significance-aware attention. ||| lin chen ||| hua yang ||| zhiyong gao ||| 
2018 ||| modeling visual and word-conditional semantic attention for image captioning. ||| chunlei wu ||| yiwei wei ||| xiaoliang chu ||| fei su ||| leiquan wang ||| 
2020 ||| feature refinement for image-based driver action recognition via multi-scale attention convolutional neural network. ||| yaocong hu ||| mingqi lu ||| xiaobo lu ||| 
2019 ||| spatio-temporal attention mechanisms based model for collective activity recognition. ||| lihua lu ||| huijun di ||| yao lu ||| lin zhang ||| shunzhou wang ||| 
2021 ||| saliency4asd: challenge, dataset and tools for visual attention modeling for autism spectrum disorder. ||| jes ||| s guti ||| rrez ||| zhaohui che ||| guangtao zhai ||| patrick le callet ||| 
2018 ||| multiple rotation symmetry group detection via saliency-based visual attention and frieze expansion pattern. ||| ronggang huang ||| yiguang liu ||| zhenyu xu ||| pengfei wu ||| yongtao shi ||| 
2020 ||| image captioning using densenet network and adaptive attention. ||| zhenrong deng ||| zhouqin jiang ||| rushi lan ||| wenming huang ||| xiaonan luo ||| 
2021 ||| cdadnet: context-guided dense attentional dilated network for crowd counting. ||| aichun zhu ||| guoxiu duan ||| xiaomei zhu ||| lu zhao ||| yaoying huang ||| gang hua ||| hichem snoussi ||| 
2022 ||| perceiving informative key-points: a self-attention approach for person search. ||| guangyu gao ||| cen han ||| zhen liu ||| 
2021 ||| visual attention prediction for autism spectrum disorder with hierarchical semantic fusion. ||| yuming fang ||| haiyan zhang ||| yifan zuo ||| wenhui jiang ||| hanqin huang ||| jiebin yan ||| 
2020 ||| saliency detection in human crowd images of different density levels using attention mechanism. ||| minh tri nguyen ||| prarinya siritanawan ||| kazunori kotani ||| 
2019 ||| fusion global and local deep representations with neural attention for aesthetic quality assessment. ||| xiaodan zhang ||| xinbo gao ||| wen lu ||| ying yu ||| lihuo he ||| 
2021 ||| siamda: dual attention siamese network for real-time visual tracking. ||| lei pu ||| xinxi feng ||| zhiqiang hou ||| wangsheng yu ||| yufei zha ||| 
2022 ||| multi-scale visual attention for attribute disambiguation in zero-shot learning. ||| long tian ||| bo chen ||| jie ren ||| hao zhang ||| zhenhua wu ||| ning han ||| yuanwei chen ||| hongwei liu ||| 
2018 ||| hierarchical multi-scale attention networks for action recognition. ||| shiyang yan ||| jeremy s. smith ||| wenjin lu ||| bailing zhang ||| 
2021 ||| from semantic to spatial awareness: vehicle reidentification with multiple attention mechanisms. ||| wenqian zhu ||| zhongyuan wang ||| ruimin hu ||| dengshi li ||| 
2021 ||| feature-guided spatial attention upsampling for real-time stereo matching network. ||| yun xie ||| shaowu zheng ||| weihua li ||| 
2022 ||| transformer models in the home improvement domain. ||| macedo maia ||| markus endres ||| 
2021 ||| attention-based context boosted cyberbullying detection in social media. ||| nabi rezvani ||| amin beheshti ||| 
2021 ||| attention, consciousness, and linguistic cooperation with ai. ||| carlos montemayor ||| 
2022 ||| a design of global workspace model with attention: simulations of attentional blink and lag-1 sparing. ||| wenjie huang ||| antonio chella ||| angelo cangelosi ||| 
2020 ||| attention and consciousness in intentional action: steps toward rich artificial agency. ||| paul bello ||| will bridewell ||| 
2019 ||| activism via attention: interpretable spatiotemporal learning to forecast protest activities. ||| ali mert ertugrul ||| yu-ru lin ||| wen-ting chung ||| muheng yan ||| ang li ||| 
2021 ||| attention dynamics on the chinese social media sina weibo during the covid-19 pandemic. ||| hao cui ||| j ||| nos kert ||| sz ||| 
2017 ||| measuring and monitoring collective attention during shocking events. ||| xingsheng he ||| yu-ru lin ||| 
2021 ||| industrial process monitoring and fault diagnosis based on temporal attention augmented deep network. ||| ke mu ||| lin luo ||| qiao wang ||| fushun mao ||| 
2021 ||| audio and video bimodal emotion recognition in social networks based on improved alexnet network and attention mechanism. ||| min liu ||| jun tang ||| 
2019 ||| two-dimensional attention-based lstm model for stock index prediction. ||| yeonguk yu ||| yoon-joong kim ||| 
2021 ||| priming effect of colour on aiding the attentional reorientation in sequential presentations of temporal data visualization: evidence from eye-tracking. ||| ningyue peng ||| chengqi xue ||| haiyan wang ||| yafeng niu ||| lei wu ||| 
2020 ||| massive mimo csi reconstruction using cnn-lstm and attention mechanism. ||| zufan zhang ||| yue zheng ||| chenquan gan ||| qingyi zhu ||| 
2021 ||| siamese tracking combing frequency channel attention with adaptive template. ||| haibo pang ||| meiqin xie ||| chengming liu ||| rongqi ma ||| linxuan han ||| 
2021 ||| glacier classification from sentinel-2 imagery using spatial-spectral attention convolutional model. ||| shuai yan ||| linlin xu ||| guojiang yu ||| longshan yang ||| wenju yun ||| dehai zhu ||| sijing ye ||| xiaochuang yao ||| 
2021 ||| ads-net: an attention-based deeply supervised network for remote sensing image change detection. ||| decheng wang ||| xiangning chen ||| mingyong jiang ||| shuhan du ||| bijie xu ||| junda wang ||| 
2021 ||| image super-resolution with dense-sampling residual channel-spatial attention networks for multi-temporal remote sensing image classification. ||| yue zhu ||| christian gei ||| emily so ||| 
2022 ||| an attention-based u-net for detecting deforestation within satellite sensor imagery. ||| david john ||| ce zhang ||| 
2021 ||| a deep learning framework under attention mechanism for wheat yield estimation using remotely sensed indices in the guanzhong plain, pr china. ||| huiren tian ||| pengxin wang ||| kevin tansey ||| dong han ||| jingqi zhang ||| shuyu zhang ||| hongmei li ||| 
2021 ||| suacdnet: attentional change detection network based on siamese u-shaped structure. ||| lei song ||| min xia ||| junlan jin ||| ming qian ||| yonghong zhang ||| 
2021 ||| hierarchical semantic segmentation of urban scene point clouds via group proposal and graph attention network. ||| tengping jiang ||| jian sun ||| shan liu ||| xu zhang ||| qi wu ||| yongjun wang ||| 
2021 ||| dsa-net: a novel deeply supervised attention-guided network for building change detection in high-resolution remote sensing images. ||| qing ding ||| zhenfeng shao ||| xiao huang ||| orhan altan ||| 
2021 ||| deep color calibration for uav imagery in crop monitoring using semantic style transfer with local to global attention. ||| huasheng huang ||| aqing yang ||| yu tang ||| jiajun zhuang ||| chaojun hou ||| zhiping tan ||| sathian dananjayan ||| yong he ||| qiwei guo ||| shaoming luo ||| 
2022 ||| intrusion detection model using temporal convolutional network blend into attention mechanism. ||| ping zhao ||| zhijie fan ||| zhiwei cao ||| xin li ||| 
2019 ||| how academia and society pay attention to climate changes: a bibliometric and altmetric analysis. ||| forough rahimi ||| nosrat riahinia ||| hamzehali nourmohammadi ||| hajar sotudeh ||| mohammad tavakolizadeh-ravari ||| 
2021 ||| abstractive review summarization based on improved attention mechanism with pointer generator network model. ||| j. shobana ||| m. murali ||| 
2020 ||| preliminary study of a separative shared control scheme focusing on control-authority and attention allocation for multi-limb disaster response robots. ||| mitsuhiro kamezaki ||| takahiro katano ||| kui chen ||| tatsuzo ishida ||| shigeki sugano ||| 
2020 ||| can a humanoid robot continue to draw attention in an office environment? ||| yuki okafuji ||| jun baba ||| junya nakanishi ||| itaru kuramoto ||| kohei ogawa ||| yuichiro yoshikawa ||| hiroshi ishiguro ||| 
2019 ||| a human behavior model of multi-agent attention based on actor-observer switching for asynchronous motion tasks with limited field of view. ||| tingting zhang-xu ||| kolja k ||| hnlenz ||| 
2021 ||| adaptive neuro-fuzzy-based attention deficit/hyperactivity disorder diagnostic system. ||| anoop kumar singh ||| deepti kakkar ||| tanu wadhera ||| rajneesh rani ||| 
2020 ||| identifying depression in tweets using cnn-deep and bilstm with attention model. ||| fatima boumahdi ||| amina madani ||| ibrahim cheurfa ||| hamza hentabli ||| 
2019 ||| the effect of task on visual attention in interactive virtual environments. ||| jacob hadnett-hunter ||| george nicolaou ||| eamonn o'neill ||| michael j. proulx ||| 
2017 ||| gaze data for the analysis of attention in feature films. ||| katherine breeden ||| pat hanrahan ||| 
2021 ||| does what we see shape history? examining workload history as a function of performance and ambient/focal visual attention. ||| shannon patricia devlin ||| jennifer k. byham ||| sara lu riggs ||| 
2021 ||| lapformer: surgical tool detection in laparoscopic surgical video using transformer architecture. ||| satoshi kondo ||| 
2022 ||| temporal convolutional networks and transformers for classifying the sleep stage in awake or asleep using pulse oximetry signals. ||| ramiro casal ||| leandro e. di persia ||| gast ||| n schlotthauer ||| 
2021 ||| stan: spatio-temporal attention network for pandemic prediction using real-world evidence. ||| junyi gao ||| rakshith sharma srinivasa ||| cheng qian ||| lucas m. glass ||| jeffrey spaeder ||| justin romberg ||| jimeng sun ||| cao xiao ||| 
2018 ||| hierarchical attention networks for information extraction from cancer pathology reports. ||| shang gao ||| michael t. young ||| john x. qiu ||| hong-jun yoon ||| james blair christian ||| paul a. fearn ||| georgia d. tourassi ||| arvind ramanathan ||| 
2021 ||| multimodal, multitask, multiattention (m3) deep learning detection of reticular pseudodrusen: toward automated and accessible classification of age-related macular degeneration. ||| qingyu chen ||| tiarnan d. l. keenan ||| alexis allot ||| yifan peng ||| elvira agr ||| n ||| amitha domalpally ||| caroline c. w. klaver ||| daniel t. luttikhuizen ||| marcus h. colyer ||| catherine a. cukras ||| henry e. wiley ||| m. teresa magone ||| chantal cousineau-krieger ||| wai t. wong ||| yingying zhu ||| emily y. chew ||| zhiyong lu ||| 
2020 ||| clinical concept extraction using transformers. ||| xi yang ||| jiang bian ||| william r. hogan ||| yonghui wu ||| 
2019 ||| algorithm to detect pediatric provider attention to high bmi and associated medical risk. ||| christy b. turer ||| celette sugg skinner ||| sarah e. barlow ||| 
2020 ||| unified medical language system resources improve sieve-based generation and bidirectional encoder representations from transformers (bert)-based ranking for concept normalization. ||| dongfang xu ||| manoj gopale ||| jiacheng zhang ||| kris brown ||| edmon begoli ||| steven bethard ||| 
2020 ||| global channel attention networks for intracranial vessel segmentation. ||| jiajia ni ||| jianhuang wu ||| haoyu wang ||| jing tong ||| zhengming chen ||| kelvin kian loong wong ||| derek abbott ||| 
2019 ||| rianet: recurrent interleaved attention network for cardiac mri segmentation. ||| qianqian tong ||| caizi li ||| weixin si ||| xiangyun liao ||| yaliang tong ||| zhiyong yuan ||| pheng-ann heng ||| 
2022 ||| emotion recognition from eeg based on multi-task learning with capsule network and attention mechanism. ||| chang li ||| bin wang ||| silin zhang ||| yu liu ||| rencheng song ||| juan cheng ||| xun chen ||| 
2022 ||| safe medicine recommendation via star interactive enhanced-based transformer model. ||| nanxin wang ||| xiaoyan cai ||| libin yang ||| xin mei ||| 
2021 ||| paired-unpaired unsupervised attention guided gan with transfer learning for bidirectional brain mr-ct synthesis. ||| alaa abu-srhan ||| israa almallahi ||| mohammad a. m. abushariah ||| waleed mahafza ||| omar sultan al-kadi ||| 
2020 ||| han-ecg: an interpretable atrial fibrillation detection model using hierarchical attention networks. ||| sajad mousavi ||| fatemeh afghah ||| u. rajendra acharya ||| 
2021 ||| a novel m-segnet with global attention cnn architecture for automatic segmentation of brain mri. ||| nagaraj yamanakkanavar ||| bumshik lee ||| 
2022 ||| anatomically constrained squeeze-and-excitation graph attention network for cortical surface parcellation. ||| xinwei li ||| jia tan ||| panyu wang ||| hong liu ||| zhangyong li ||| wei wang ||| 
2021 ||| mca-dn: multi-path convolution leveraged attention deep network for salvageable tissue detection in ischemic stroke from multi-parametric mri. ||| anusha vupputuri ||| akshat gupta ||| nirmalya ghosh ||| 
2020 ||| massd: multi-scale attention single shot detector for surgical instruments. ||| lingtao yu ||| pengcheng wang ||| yusheng yan ||| yongqiang xia ||| wei cao ||| 
2021 ||| autism spectrum disorder diagnosis using graph attention network based on spatial-constrained sparse functional brain networks. ||| chunde yang ||| panyu wang ||| jia tan ||| qingshui liu ||| xinwei li ||| 
2021 ||| ta-net: triple attention network for medical image segmentation. ||| yang li ||| jun yang ||| jiajia ni ||| ahmed elazab ||| jianhuang wu ||| 
2021 ||| d2a u-net: automatic segmentation of covid-19 ct slices based on dual attention and hybrid dilated convolution. ||| xiangyu zhao ||| peng zhang ||| fan song ||| guangda fan ||| yangyang sun ||| yujia wang ||| zheyuan tian ||| luqi zhang ||| guanglei zhang ||| 
2017 ||| diagnosis of attention deficit hyperactivity disorder using imaging and signal processing techniques. ||| chaitra sridhar ||| shreya bhat ||| u. rajendra acharya ||| hojjat adeli ||| g. muralidhar bairy ||| 
2021 ||| a depthwise separable dense convolutional network with convolution block attention module for covid-19 diagnosis on ct scans. ||| qian li ||| jiangbo ning ||| jianping yuan ||| ling xiao ||| 
2021 ||| densely connected attention network for diagnosing covid-19 based on chest ct. ||| yu fu ||| peng xue ||| enqing dong ||| 
2022 ||| a multiscale double-branch residual attention network for anatomical-functional medical image fusion. ||| weisheng li ||| xiuxiu peng ||| jun fu ||| guofen wang ||| yuping huang ||| feifei chao ||| 
2022 ||| automated classification of attention deficit hyperactivity disorder and conduct disorder using entropy features with ecg signals. ||| joel e. w. koh ||| ooi chui ping ||| nikki s. j. lim-ashworth ||| jahmunah vicnesh ||| hui tian tor ||| shu lih oh ||| ru san tan ||| u. rajendra acharya ||| daniel shuen sheng fung ||| 
2021 ||| trp-bert: discrimination of transient receptor potential (trp) channels using contextual representations from deep bidirectional transformer based on bert. ||| syed muazzam ali shah ||| yu-yen ou ||| 
2022 ||| deepmgt-dti: transformer network incorporating multilayer graph information for drug-target interaction prediction. ||| peiliang zhang ||| ziqi wei ||| chao che ||| bo jin ||| 
2022 ||| is the aspect ratio of cells important in deep learning? a robust comparison of deep learning methods for multi-scale cytopathology cell image classification: from convolutional neural networks to visual transformers. ||| wanli liu ||| chen li ||| md mamunur rahaman ||| tao jiang ||| hongzan sun ||| xiangchen wu ||| weiming hu ||| haoyuan chen ||| changhao sun ||| yudong yao ||| marcin grzegorzek ||| 
2022 ||| attention-based 3d cnn with residual connections for efficient ecg-based covid-19 detection. ||| nebras sobahi ||| abdulkadir seng ||| r ||| ru san tan ||| u. rajendra acharya ||| 
2021 ||| caspianet++: a multidimensional channel-spatial asymmetric attention network with noisy student curriculum learning paradigm for brain tumor segmentation. ||| andrea liew ||| chun cheng lee ||| boon leong lan ||| maxine tan ||| 
2021 ||| automatic consecutive context perceived transformer gan for serial sectioning image blind inpainting. ||| lei wang ||| siqi zhang ||| ling gu ||| jie zhang ||| xiaoyue zhai ||| xianzheng sha ||| shijie chang ||| 
2021 ||| caagp: rethinking channel attention with adaptive global pooling for liver tumor segmentation. ||| chi zhang ||| jingben lu ||| luxi yang ||| chunguo li ||| 
2022 ||| il-mcam: an interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach. ||| haoyuan chen ||| chen li ||| xiaoyan li ||| md mamunur rahaman ||| weiming hu ||| yixin li ||| wanli liu ||| changhao sun ||| hongzan sun ||| xinyu huang ||| marcin grzegorzek ||| 
2019 ||| binary tree-like network with two-path fusion attention feature for cervical cell nucleus segmentation. ||| jianwei zhang ||| zhenmei liu ||| bohai du ||| junting he ||| guanzhao li ||| danni chen ||| 
2021 ||| attention-embedded complementary-stream cnn for false positive reduction in pulmonary nodule detection. ||| lingma sun ||| zhuoran wang ||| hong pu ||| guohui yuan ||| lu guo ||| tian pu ||| zhenming peng ||| 
2021 ||| hybrid dilation and attention residual u-net for medical image segmentation. ||| zekun wang ||| yanni zou ||| peter x. liu ||| 
2021 ||| fad-bert: improved prediction of fad binding sites using pre-training of deep bidirectional transformers. ||| quang-thai ho ||| trinh-trung-duong nguyen ||| nguyen-quoc-khanh le ||| yu-yen ou ||| 
2019 ||| document-level attention-based bilstm-crf incorporating disease dictionary for disease named entity recognition. ||| kai xu ||| zhenguo yang ||| peipei kang ||| qi wang ||| wenyin liu ||| 
2021 ||| focus u-net: a novel dual attention-gated cnn for polyp segmentation during colonoscopy. ||| michael yeung ||| evis sala ||| carola-bibiane sch ||| nlieb ||| leonardo rundo ||| 
2022 ||| development of adaptive human-computer interaction games to evaluate attention. ||| hasan kandemir ||| hatice kose ||| 
2020 ||| game-based promotion of motivation and attention for socio-emotional training in autism. ||| sven strickroth ||| dietmar zoerner ||| tobias moebert ||| anna morgiel ||| ulrike lucke ||| 
2021 ||| an end-to-end heterogeneous graph attention network for mycobacterium tuberculosis drug-resistance prediction. ||| yang yang ||| timothy m. walker ||| samaneh kouchaki ||| chenyang wang ||| timothy e. a peto ||| derrick w. crook ||| david a. clifton ||| 
2021 ||| saresnet: self-attention residual network for predicting dna-protein binding. ||| long-chen shen ||| yan liu ||| jiangning song ||| dong-jun yu ||| 
2021 ||| deep drug-target binding affinity prediction with multiple attention blocks. ||| yuni zeng ||| xiangru chen ||| yujie luo ||| xuedong li ||| dezhong peng ||| 
2021 ||| recognizing binding sites of poorly characterized rna-binding proteins on circular rnas using attention siamese network. ||| hehe wu ||| xiaoyong pan ||| yang yang ||| hong-bin shen ||| 
2022 ||| fusiondta: attention-based feature polymerizer and knowledge distillation for drug-target binding affinity prediction. ||| weining yuan ||| guanxing chen ||| calvin yu-chian chen ||| 
2021 ||| deepatt: a hybrid category attention neural network for identifying functional effects of dna sequences. ||| jiawei li ||| yuqian pu ||| jijun tang ||| quan zou ||| fei guo ||| 
2022 ||| deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. ||| jinxian wang ||| xuejun liu ||| siyuan shen ||| lei deng ||| hui liu ||| 
2021 ||| a transformer architecture based on bert and 2d convolutional neural network to identify dna enhancers from sequence information. ||| nguyen-quoc-khanh le ||| quang-thai ho ||| trinh-trung-duong nguyen ||| yu-yen ou ||| 
2021 ||| attentional multi-level representation encoding based on convolutional and variance autoencoders for lncrna-disease association prediction. ||| nan sheng ||| hui cui ||| tiangang zhang ||| ping xuan ||| 
2022 ||| gvdti: graph convolutional and variational autoencoders with attribute-level attention for drug-protein interaction prediction. ||| ping xuan ||| mengsi fan ||| hui cui ||| tiangang zhang ||| toshiya nakaguchi ||| 
2021 ||| a novel graph attention model for predicting frequencies of drug-side effects from multi-view data. ||| haochen zhao ||| kai zheng ||| yaohang li ||| jianxin wang ||| 
2022 ||| accurate protein function prediction via graph attention networks with predicted structure information. ||| boqiao lai ||| jinbo xu ||| 
2022 ||| multi-channel graph attention autoencoders for disease-related lncrnas prediction. ||| nan sheng ||| lan huang ||| yan wang ||| jing zhao ||| ping xuan ||| ling gao ||| yangkun cao ||| 
2022 ||| kgancda: predicting circrna-disease associations based on knowledge graph attention network. ||| wei lan ||| yi dong ||| qingfeng chen ||| ruiqing zheng ||| jin liu ||| yi pan ||| yi-ping phoebe chen ||| 
2021 ||| leveraging the attention mechanism to improve the identification of dna n6-methyladenine sites. ||| ying zhang ||| yan liu ||| jian xu ||| xiaoyu wang ||| xinxin peng ||| jiangning song ||| dong-jun yu ||| 
2022 ||| identifying drug-target interactions via heterogeneous graph attention networks combined with cross-modal similarities. ||| lu jiang ||| jiahao sun ||| yue wang ||| qiao ning ||| na luo ||| minghao yin ||| 
2021 ||| erratum to: evotuning protocols for transformer-based variant effect prediction on multi-domain proteins. ||| hideki yamaguchi ||| yutaka saito ||| 
2022 ||| egret: edge aggregated graph attention networks and transfer learning improve protein-protein interaction site prediction. ||| sazan mahbub ||| md. shamsuzzoha bayzid ||| 
2021 ||| explainability in transformer models for functional genomics. ||| jim clauwaert ||| gerben menschaert ||| willem waegeman ||| 
2021 ||| predicting human microbe-disease associations via graph attention networks with inductive matrix completion. ||| yahui long ||| jiawei luo ||| yu zhang ||| yan xia ||| 
2021 ||| multi-view multichannel attention graph convolutional network for mirna-disease association prediction. ||| xinru tang ||| jiawei luo ||| cong shen ||| zihan lai ||| 
2022 ||| a novel convolution attention model for predicting transcription factor binding sites by combination of sequence and shape. ||| yongqing zhang ||| zixuan wang ||| yuanqi zeng ||| yuhang liu ||| shuwen xiong ||| maocheng wang ||| jiliu zhou ||| quan zou ||| 
2021 ||| a spatial-temporal gated attention module for molecular property prediction based on molecular geometry. ||| chunyan li ||| jianmin wang ||| zhangming niu ||| junfeng yao ||| xiangxiang zeng ||| 
2022 ||| predicting mirna-disease associations based on graph random propagation network and attention network. ||| tangbo zhong ||| zhengwei li ||| zhu-hong you ||| ru nie ||| huan zhao ||| 
2022 ||| adapt-kcr: a novel deep learning framework for accurate prediction of lysine crotonylation sites based on learning embedding features and attention architecture. ||| zutan li ||| jingya fang ||| shining wang ||| liangyun zhang ||| yuanyuan chen ||| cong pian ||| 
2022 ||| maresnet: predicting transcription factor binding sites by combining multi-scale bottom-up and top-down attention and residual network. ||| ke han ||| long-chen shen ||| yi-heng zhu ||| jian xu ||| jiangning song ||| dong-jun yu ||| 
2021 ||| atse: a peptide toxicity predictor by exploiting structural and evolutionary information based on graph neural network and attention mechanism. ||| lesong wei ||| xiucai ye ||| yuyang xue ||| tetsuya sakurai ||| leyi wei ||| 
2022 ||| mdf-sa-ddi: predicting drug-drug interaction events based on multi-source drug fusion, multi-source feature fusion and transformer self-attention mechanism. ||| shenggeng lin ||| yanjing wang ||| lingfeng zhang ||| yanyi chu ||| yatong liu ||| yitian fang ||| mingming jiang ||| qiankun wang ||| bowen zhao ||| yi xiong ||| dong-qing wei ||| 
2022 ||| stable-abppred: a stacked ensemble predictor based on bilstm and attention mechanism for accelerated discovery of antibacterial peptides. ||| vishakha singh ||| sameer shrivastava ||| sanjay kumar singh ||| abhinav kumar ||| sonal saxena ||| 
2022 ||| amde: a novel attention-mechanism-based multidimensional feature encoder for drug-drug interaction prediction. ||| shanchen pang ||| ying zhang ||| tao song ||| xudong zhang ||| xun wang ||| alfonso rodr ||| guez-pat ||| n ||| 
2021 ||| evotuning protocols for transformer-based variant effect prediction on multi-domain proteins. ||| hideki yamaguchi ||| yutaka saito ||| 
2021 ||| predicting drug-disease associations through layer attention graph convolutional network. ||| zhouxin yu ||| feng huang ||| xiaohan zhao ||| wenjie xiao ||| wen zhang ||| 
2022 ||| heterogeneous graph attention network based on meta-paths for lncrna-disease association prediction. ||| xiaosa zhao ||| xiaowei zhao ||| minghao yin ||| 
2022 ||| dsgat: predicting frequencies of drug side effects by graph attention networks. ||| xianyu xu ||| ling yue ||| bingchun li ||| ying liu ||| yuan wang ||| wenjuan zhang ||| lin wang ||| 
2022 ||| alphafold2-aware protein-dna binding site prediction using graph transformer. ||| qianmu yuan ||| sheng chen ||| jiahua rao ||| shuangjia zheng ||| huiying zhao ||| yuedong yang ||| 
2019 ||| parsing heterogeneity in autism spectrum disorder and attention-deficit/hyperactivity disorder with individual connectome mapping. ||| dina r. dajani ||| catherine a. burrows ||| mary beth nebel ||| stewart h. mostofsky ||| kathleen m. gates ||| lucina q. uddin ||| 
2017 ||| structural and functional abnormalities in children with attention-deficit/hyperactivity disorder: a focus on subgenual anterior cingulate cortex. ||| chenyang zhan ||| yuhong liu ||| kai wu ||| yu gao ||| xiaobo li ||| 
2019 ||| functional connectivity of attention, visual, and language networks during audio, illustrated, and animated stories in preschool-age children. ||| john s. hutton ||| jonathan dudley ||| tzipi horowitz-kraus ||| tom dewitt ||| scott k. holland ||| 
2021 ||| directed flow of beta band communication during reorienting of attention within the dorsal attention network. ||| sara spadone ||| miroslaw wyczesany ||| stefania della penna ||| maurizio corbetta ||| paolo capotosto ||| 
2018 ||| resting-state network functional connectivity patterns associated with the mindful attention awareness scale. ||| elena bilevicius ||| stephen d. smith ||| jennifer kornelsen ||| 
2019 ||| integration and segregation of the brain relate to stability of performance in children and adolescents with varied levels of inattention and impulsivity. ||| keitaro machida ||| katherine a. johnson ||| 
2021 ||| topological aberrance of structural brain network provides quantitative substrates of post-traumatic brain injury attention deficits in children. ||| meng cao ||| yuyang luo ||| ziyan wu ||| catherine a. mazzola ||| lori catania ||| tara l. alvarez ||| jeffrey m. halperin ||| bharat b. biswal ||| xiaobo li ||| 
2021 ||| a review of the default mode network in autism spectrum disorders and attention deficit hyperactivity disorder. ||| amritha harikumar ||| david w. evans ||| chase c. dougherty ||| kimberly l. h. carpenter ||| andrew m. michael ||| 
2019 ||| electroencephalography functional networks reveal global effects of methylphenidate in youth with attention deficit/hyperactivity disorder. ||| mica rubinson ||| itai horowitz ||| jodie naim-feil ||| doron gothelf ||| elisha moses ||| nava levit-binnun ||| 
2021 ||| aggregation-and-attention network for brain tumor segmentation. ||| chih-wei lin ||| yu hong ||| jinfu liu ||| 
2021 ||| ve adults with attention-deficit hyperactivity disorder during fmri tasks of motor inhibition and cognitive switching. ||| jatta berberat ||| ruth huggenberger ||| margherita montali ||| philipp gruber ||| achmed pircher ||| karl-olof l ||| vblad ||| hanspeter e. killer ||| luca remonda ||| 
2020 ||| context-aware attention network for human emotion recognition in video. ||| xiaodong liu ||| miao wang ||| 
2018 ||| anomaly detection in moving crowds through spatiotemporal autoencoding and additional attention. ||| biao yang ||| jinmeng cao ||| rongrong ni ||| ling zou ||| 
2020 ||| stock index prices prediction via temporal pattern attention and long-short-term memory. ||| xiaolu wei ||| binbin lei ||| hongbing ouyang ||| qiufeng wu ||| 
2021 ||| improving power losses and thermal management in switch mode power converters using multiple transformers. ||| nagesh vangala ||| srinivasa rao gorantla ||| rayudu mannam ||| 
2021 ||| local-constraint transformer network for stock movement prediction. ||| jincheng hu ||| 
2020 ||| a short text conversation generation model combining bert and context attention mechanism. ||| huan zhao ||| jian lu ||| jie cao ||| 
2021 ||| rpt: relational pre-trained transformer is almost all you need towards democratizing data preparation. ||| nan tang ||| ju fan ||| fangyi li ||| jianhong tu ||| xiaoyong du ||| guoliang li ||| samuel madden ||| mourad ouzzani ||| 
2017 ||| asap: prioritizing attention via time series smoothing. ||| kexin rong ||| peter bailis ||| 
2021 ||| fear of missing out (fomo) among undergraduate students in relation to attention distraction and learning disengagement in lectures. ||| suad abdul aziz al-furaih ||| hamed m. al-awidi ||| 
2017 ||| design optimization of distribution transformers with nature-inspired metaheuristics: a comparative analysis. ||| levent alhan ||| nejat yumusak ||| 
2020 ||| analysis of acoustic sensor placement for pd location in power transformer. ||| khairul nadiah khalid ||| muhammad nur khairul hafizi rohani ||| baharuddin ismail ||| muzamir isa ||| chai chang yii ||| wan nurul auni wan muhammad ||| 
2019 ||| real-time implementation of electronic power transformer based on intelligent controller. ||| hakan a ||| ikg ||| z ||| kkes fatih ke ||| ecioglu ||| mustafa sekkeli ||| 
2017 ||| modeling and simulation of 2.5 mva sf6-gas-insulated transformer. ||| okan  ||| zg ||| nenel ||| dave w. p. thomas ||| 
2018 ||| application of acf-wavelet feature extraction for classification of some artificial pd models of power transformer. ||| vahid parvin darabad ||| 
2017 ||| calculation of creepage discharge safety factors against the tangential component of electric fields in the insulation structure of power transformers. ||| arsalan hekmati ||| 
2021 ||| exploring the attention process differentiation of attention deficit hyperactivity disorder (adhd) symptomatic adults using artificial intelligence on electroencephalography (eeg) signals. ||| gokhan guney ||| esra kisacik ||| canan kalaycioglu ||| gorkem saygili ||| 
2019 ||| investigation of control of power flow by using phase shifting transformers: turkey case study. ||| erdi dogan ||| nuran y ||| r ||| keren ||| 
2018 ||| transformer incipient fault diagnosis on the basis of energy-weighted dga using an artificial neural network. ||| md. danish equbal ||| shakeb ahmad khan ||| tarikul islam ||| 
2019 ||| a coordinated dc voltage control strategy for cascaded solid state transformer with star configuration. ||| zhendong ji ||| yichao sun ||| cheng jin ||| jianhua wang ||| jianfeng zhao ||| 
2018 ||| sf6 gas-insulated 50-kva distribution transformer design. ||| okan  ||| zg ||| nenel ||| david w. p. thomas ||| unal kurt ||| 
2018 ||| average modeling and evaluation of 18-pulse autotransformer rectifier unit without interphase transformers. ||| shahbaz khan ||| xiaobin zhang ||| husan ali ||| haider zaman ||| muhammad saad ||| bakht muhammad khan ||| 
2017 ||| discrete design optimization of distribution transformers with guaranteed optimum convergence using the cuckoo search algorithm. ||| levent alhan ||| nejat yumusak ||| 
2020 ||| attention mechanism based semi-supervised multi-gain image fusion. ||| ming fang ||| xu liang ||| feiran fu ||| yansong song ||| zhen shao ||| 
2021 ||| attention-inspired artificial neural networks for speech processing: a systematic review. ||| noel zacarias-morales ||| pablo pancardo ||| jos |||  ad ||| n hern ||| ndez-nolasco ||| matias garcia-constantino ||| 
2020 ||| unsupervised hashing with gradient attention. ||| shaochen jiang ||| liejun wang ||| shuli cheng ||| anyu du ||| yongming li ||| 
2020 ||| dual attention-guided multiscale dynamic aggregate graph convolutional networks for skeleton-based human action recognition. ||| zeyuan hu ||| eung-joo lee ||| 
2020 ||| automatic classification system of arrhythmias using 12-lead ecgs with a deep neural network based on an attention mechanism. ||| dengao li ||| hang wu ||| jumin zhao ||| ye tao ||| jian fu ||| 
2022 ||| attention optimized deep generative adversarial network for removing uneven dense haze. ||| wenxuan zhao ||| yaqin zhao ||| liqi feng ||| jiaxi tang ||| 
2020 ||| better understanding: stylized image captioning with style attention and adversarial training. ||| zhenyu yang ||| qiao liu ||| guojing liu ||| 
2021 ||| bidirectional symmetry network with dual-field cyclic attention for multi-temporal aerial remote sensing image registration. ||| ying chen ||| qi zhang ||| wencheng zhang ||| lei chen ||| 
2021 ||| attention to a moment in time impairs episodic distinctiveness during rapid serial visual presentation. ||| pierpaolo zivi ||| fabio ferlazzo ||| stefano sdoia ||| 
2021 ||| transfer detection of yolo to focus cnn's attention on nude regions for adult content detection. ||| nouar aldahoul ||| hezerul abdul karim ||| mohd haris lye abdullah ||| mohammad faizal ahmad fauzi ||| abdulaziz saleh ba wazir ||| sarina mansor ||| john see ||| 
2021 ||| attention modulated multiple object tracking with motion enhancement and dual correlation. ||| yifeng wang ||| zhijiang zhang ||| ning zhang ||| dan zeng ||| 
2019 ||| entity linking via symmetrical attention-based neural network and entity structural features. ||| shengze hu ||| zhen tan ||| weixin zeng ||| bin ge ||| weidong xiao ||| 
2021 ||| deep convolutional symmetric encoder - decoder neural networks to predict students' visual attention. ||| tomasz hachaj ||| anna stolinska ||| magdalena andrzejewska ||| piotr czerski ||| 
2021 ||| hotspot temperature prediction of dry-type transformers based on particle filter optimization with support vector regression. ||| yuanyuan sun ||| gongde xu ||| na li ||| kejun li ||| yongliang liang ||| hui zhong ||| lina zhang ||| ping liu ||| 
2019 ||| semantic relation classification via bidirectional lstm networks with entity-aware attention using latent entity typing. ||| joohong lee ||| sangwoo seo ||| yong suk choi ||| 
2018 ||| sequential dual attention: coarse-to-fine-grained hierarchical generation for image captioning. ||| zhi-bin guan ||| kang liu ||| yan ma ||| xu qian ||| tongkai ji ||| 
2021 ||| deep deterministic policy gradient algorithm based on convolutional block attention for autonomous driving. ||| yanliang jin ||| qianhong liu ||| liquan shen ||| leiji zhu ||| 
2021 ||| an ensemble of global and local-attention based convolutional neural networks for covid-19 diagnosis on chest x-ray images. ||| ahmed afifi ||| noor e. hafsa ||| mona a. s. ali ||| abdulaziz alhumam ||| safa alsalman ||| 
2021 ||| a multi-scale residual attention network for retinal vessel segmentation. ||| yun jiang ||| huixia yao ||| chao wu ||| wenhuan liu ||| 
2021 ||| automatic lung segmentation algorithm on chest x-ray images based on fusion variational auto-encoder and three-terminal attention mechanism. ||| feidao cao ||| huaici zhao ||| 
2019 ||| attention bilinear pooling for fine-grained classification. ||| wenqian wang ||| jun zhang ||| fenglei wang ||| 
2022 ||| tcan-ids: intrusion detection system for internet of vehicle using temporal convolutional attention network. ||| pengzhou cheng ||| kai xu ||| simin li ||| mu han ||| 
2020 ||| convolutional attention network with maximizing mutual information for fine-grained image classification. ||| fenglei wang ||| hao zhou ||| shuohao li ||| jun lei ||| jun zhang ||| 
2020 ||| an improved deep mutual-attention learning model for person re-identification. ||| miftah bedru jamal ||| jiang zhengang ||| fang ming ||| 
2018 ||| research on electronic voltage transformer for big data background. ||| zhenhua li ||| yao wang ||| zhengtian wu ||| zhenxing li ||| 
2020 ||| joint entity-relation extraction via improved graph attention networks. ||| qinghan lai ||| zihan zhou ||| song liu ||| 
2022 ||| adaptive memory-controlled self-attention for polyphonic sound event detection. ||| mei wang ||| yu yao ||| hongbing qiu ||| xiyu song ||| 
2021 ||| bi-sanet - bilateral network with scale attention for retinal vessel segmentation. ||| yun jiang ||| huixia yao ||| zeqi ma ||| jingyao zhang ||| 
2020 ||| attention-based lstm with filter mechanism for entity relation classification. ||| yanliang jin ||| dijia wu ||| weisi guo ||| 
2019 ||| symmetry encoder-decoder network with attention mechanism for fast video object segmentation. ||| mingyue guo ||| dejun zhang ||| jun sun ||| yiqi wu ||| 
2020 ||| aresu-net: attention residual u-net for brain tumor segmentation. ||| jianxin zhang ||| xiaogang lv ||| hengbo zhang ||| bin liu ||| 
2021 ||| mre: a military relation extraction model based on bigru and multi-head attention. ||| yiwei lu ||| ruopeng yang ||| xuping jiang ||| dan zhou ||| changsheng yin ||| zizhuo li ||| 
2021 ||| dual attention network for pitch estimation of monophonic music. ||| wenfang ma ||| ying hu ||| hao huang ||| 
2019 ||| a crucial role of attention in lateralisation of sound processing? ||| martine hausberger ||| hugo cousillas ||| ana ||| ke meter ||| genta karino ||| isabelle george ||| alban lemasson ||| catherine blois-heulin ||| 
2021 ||| contributions of the right prefrontal and parietal cortices to the attentional blink: a tdcs study. ||| anna pecchinenda ||| francesca de luca ||| bianca monachesi ||| manuel petrucci ||| mariella pazzaglia ||| fabrizio doricchi ||| michal lavidor ||| 
2018 ||| unsupervised multi-object detection for video surveillance using memory-based recurrent attention networks. ||| zhen he ||| hangen he ||| 
2020 ||| anticipatory defocusing of attention and contextual response priming but no role of aesthetic appreciation in simple symmetry judgments when switching between tasks. ||| svantje t. k ||| hler ||| thomas jacobsen ||| stina klein ||| mike wendt ||| 
2021 ||| neurofunctional symmetries and asymmetries during voluntary out-of- and within-body vivid imagery concurrent with orienting attention and visuospatial detection. ||| amedeo d'angiulli ||| darren kenney ||| dao anh thu pham ||| etienne lefebvre ||| justin bellavance ||| derrick matthew buchanan ||| 
2018 ||| neural relation classification using selective attention and symmetrical directional instances. ||| zhen tan ||| bo li ||| peixin huang ||| bin ge ||| weidong xiao ||| 
2020 ||| cloud detection for satellite imagery using attention-based u-net convolutional neural network. ||| yanan guo ||| xiao-qun cao ||| bainian liu ||| mei gao ||| 
2021 ||| improving sentiment classification of restaurant reviews with attention-based bi-gru neural network. ||| liangqiang li ||| liang yang ||| yuyang zeng ||| 
2022 ||| a vibration similarity model of converter transformers and its verification method. ||| hao wang ||| li zhang ||| youliang sun ||| guan wang ||| liang zou ||| 
2021 ||| mrda-mgfsnet: network based on a multi-rate dilated attention mechanism and multi-granularity feature sharer for image-based butterflies fine-grained classification. ||| maopeng li ||| guoxiong zhou ||| weiwei cai ||| jiayong li ||| mingxuan li ||| mingfang he ||| yahui hu ||| liujun li ||| 
2018 ||| practical feedback method for mobile cpr support systems considering noise and user's attention. ||| ren ohmura ||| kodai yamamoto ||| 
2021 ||| epitope prediction of antigen protein using attention-based lstm network. ||| toshiaki noumi ||| seiichi inoue ||| haruka fujita ||| kugatsu sadamitsu ||| makoto sakaguchi ||| akiko tenma ||| hironori nakagami ||| 
2018 ||| investors' attention and overpricing of ipo: an empirical study on china's growth enterprise market. ||| hailiang huang ||| yanhong li ||| yingying zhang ||| 
2020 ||| temporal continuity of visual attention for future gaze prediction in immersive virtual reality. ||| zhiming hu ||| sheng li ||| meng gai ||| 
2021 ||| self-attention transfer networks for speech emotion recognition. ||| ziping zhao ||| zhongtian bao ||| zixing zhang ||| nicholas cummins ||| shihuang sun ||| haishuai wang ||| jianhua tao ||| bj ||| rn w. schuller ||| 
2021 ||| fcca: hybrid code representation for functional clone detection using attention networks. ||| wei hua ||| yulei sui ||| yao wan ||| guangzhong liu ||| guandong xu ||| 
2020 ||| spare assessment of distribution power transformers considering the issues of redundancy and mus capability. ||| gomaa ahmed hamoud ||| sherif o. faried ||| 
2020 ||| hierarchical analysis of loops with relaxed abstract transformers. ||| banghu yin ||| liqian chen ||| jiangchao liu ||| ji wang ||| 
2020 ||| attention-based word-level contextual feature extraction and cross-modality fusion for sentiment analysis and emotion classification. ||| mahesh g. huddar ||| sanjeev s. sannakki ||| vijay s. rajpurohit ||| 
2017 ||| do researchers pay attention to publication subsidies? ||| tolga yuret ||| 
2020 ||| the power of social cues in the battle for attention: evidence from an online platform for scholarly commentary. ||| ho fai chan ||| sohel md bodiuzzman ||| benno torgler ||| 
2020 ||| comparison of citations and attention of cover and non-cover papers. ||| ling kong ||| dongbo wang ||| 
2018 ||| supervised search result diversification via subtopic attention. ||| zhengbao jiang ||| zhicheng dou ||| wayne xin zhao ||| jian-yun nie ||| ming yue ||| ji-rong wen ||| 
2021 ||| co-attention memory network for multimodal microblog's hashtag recommendation. ||| renfeng ma ||| xipeng qiu ||| qi zhang ||| xiangkun hu ||| yu-gang jiang ||| xuanjing huang ||| 
2022 ||| g-inspector: recurrent attention model on graph. ||| zhiling luo ||| yinghua cui ||| sha zhao ||| jianwei yin ||| 
2021 ||| hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification. ||| hao peng ||| jianxin li ||| senzhang wang ||| lihong wang ||| qiran gong ||| renyu yang ||| bo li ||| philip s. yu ||| lifang he ||| 
2020 ||| a hierarchical attention model for social contextual image recommendation. ||| le wu ||| lei chen ||| richang hong ||| yanjie fu ||| xing xie ||| meng wang ||| 
2017 ||| hierarchical contextual attention recurrent neural network for map query suggestion. ||| jun song ||| jun xiao ||| fei wu ||| haishan wu ||| tong zhang ||| zhongfei (mark) zhang ||| wenwu zhu ||| 
2021 ||| neural attention frameworks for explainable recommendation. ||| omer tal ||| yang liu ||| jimmy huang ||| xiaohui yu ||| bushra aljbawi ||| 
2021 ||| multi-level attention networks for multi-step citywide passenger demands prediction. ||| xian zhou ||| yanyan shen ||| linpeng huang ||| tianzi zang ||| yanmin zhu ||| 
2021 ||| fall detection with wearable sensors: a hierarchical attention-based convolutional neural network approach. ||| shuo yu ||| yidong chai ||| hsinchun chen ||| randall a. brown ||| scott j. sherman ||| jay f. nunamaker jr. ||| 
2018 ||| attention adjustment, renewal, and equilibrium seeking in online search: an eye-tracking approach. ||| jae-hyeon ahn ||| yoon-soo bae ||| jaehyeon ju ||| wonseok oh ||| 
2019 ||| a possible framework for attention-based politics: a field for research. ||| norbert merkovity ||| 
2021 ||| sparse deep lstms with convolutional attention for human action recognition. ||| atefe aghaei ||| ali nazari ||| mohsen ebrahimi moghaddam ||| 
2020 ||| mobile interface attentional priority model. ||| jeremiah d. still ||| john m. hicks ||| 
2020 ||| contextual stroke classification in online handwritten documents with edge graph attention networks. ||| jun-yu ye ||| yan-ming zhang ||| qing yang ||| cheng-lin liu ||| 
2022 ||| macularnet: towards fully automated attention-based deep cnn for macular disease classification. ||| sapna s. mishra ||| bappaditya mandal ||| niladri b. puhan ||| 
2021 ||| exploring multi-task multi-lingual learning of transformer models for hate speech and offensive speech identification in social media. ||| sudhanshu mishra ||| shivangi prasad ||| shubhanshu mishra ||| 
2021 ||| learning accurate integer transformer machine-translation models. ||| ephrem wu ||| 
2021 ||| deep learning-based image retrieval system with clustering on attention-based representations. ||| sumanth s. rao ||| shahid ikram ||| parashara ramesh ||| 
2020 ||| extracting opinion targets using attention-based neural model. ||| saja al-dabet ||| sara tedmori ||| mohammad al-smadi ||| 
2022 ||| bornon: bengali image captioning with transformer-based deep learning approach. ||| faisal muhammad shah ||| mayeesha humaira ||| md abidur rahman khan jim ||| amit saha ami ||| shimul paul ||| 
2021 ||| vision-to-language tasks based on attributes and attention mechanism. ||| xuelong li ||| aihong yuan ||| xiaoqiang lu ||| 
2022 ||| chinese image caption generation via visual attention and topic modeling. ||| maofu liu ||| huijun hu ||| lingjun li ||| yan yu ||| weili guan ||| 
2021 ||| asif-net: attention steered interweave fusion network for rgb-d salient object detection. ||| chongyi li ||| runmin cong ||| sam kwong ||| junhui hou ||| huazhu fu ||| guopu zhu ||| dingwen zhang ||| qingming huang ||| 
2020 ||| embedding attention and residual network for accurate salient object detection. ||| shuhan chen ||| ben wang ||| xiuli tan ||| xuelong hu ||| 
2020 ||| temporally identity-aware ssd with attentional lstm. ||| xingyu chen ||| junzhi yu ||| zhengxing wu ||| 
2022 ||| complementarity-aware attention network for salient object detection. ||| junxia li ||| zefeng pan ||| qingshan liu ||| ying cui ||| yubao sun ||| 
2021 ||| bio-inspired representation learning for visual attention prediction. ||| yuan yuan ||| hailong ning ||| xiaoqiang lu ||| 
2022 ||| sman: stacked multimodal attention network for cross-modal image-text retrieval. ||| zhong ji ||| haoran wang ||| jungong han ||| yanwei pang ||| 
2019 ||| deep attention-based spatially recursive networks for fine-grained visual recognition. ||| lin wu ||| yang wang ||| xue li ||| junbin gao ||| 
2020 ||| alignment-supervised bidimensional attention-based recursive autoencoders for bilingual phrase representation. ||| biao zhang ||| deyi xiong ||| jinsong su ||| yue qin ||| 
2020 ||| visual object tracking by hierarchical attention siamese network. ||| jianbing shen ||| xin tang ||| xingping dong ||| ling shao ||| 
2019 ||| describing video with attention-based bidirectional lstm. ||| yi bin ||| yang yang ||| fumin shen ||| ning xie ||| heng tao shen ||| xuelong li ||| 
2021 ||| attention-based convolutional neural network for bangla sentiment analysis. ||| sadia sharmin ||| danial chakma ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond ||| jimmy lin ||| rodrigo nogueira ||| andrew yates ||| 
2019 ||| a deep architecture for chinese semantic matching with pairwise comparisons and attention-pooling. ||| huiyuan lai ||| yizheng tao ||| chunliu wang ||| lunfan xu ||| dingyong tang ||| gongliang li ||| 
2020 ||| exploring the relationship between attention and awareness. neurophenomenology of the centroencephalic space of functional integration. ||| mauro n. maldonato ||| raffaele sperandeo ||| anna esposito ||| ciro punzo ||| silvia dell'orco ||| 
2017 ||| an ecog-based bci based on auditory attention to natural speech. ||| peter brunner ||| karen dijkstra ||| william g. coon ||| j ||| rgen mellinger ||| anthony l. ritaccio ||| gerwin schalk ||| 
2019 ||| online adaptive synchronous bci system with attention variations. ||| susan aliakbaryhosseinabadi ||| ernest nlandu kamavuako ||| ning jiang ||| dario farina ||| natalie mrachacz-kersting ||| 
2021 ||| few-shot knowledge reasoning: an attention mechanism based method. ||| haocheng xie ||| aiping li ||| yan jia ||| 
2020 ||| subtle visual attention guidance in vr. ||| steve grogorick ||| marcus a. magnor ||| 
