2021 ||| the attention-hesitation model. a non-intrusive intervention strategy for incremental smart home dialogue management. ||| 0 ||| 
2020 ||| visual attention in primates and for machines - neuronal mechanisms. ||| 1 ||| 
2017 ||| deployment of processing resources to concurrent stimulation by sustained spatial attention in touch. ||| 2 ||| 
2018 ||| improvement of the correlation between alse and bci by adjusting the feeding and positioning conditions based on indirect measurement of termination impedances using current transformers. ||| 3 ||| 
2021 ||| periodicity, surprisal, attention: skip conditions for recurrent neural networks. ||| 4 ||| 
2019 ||| breakdown mechanisms of the coreless transformer in data couplers at high voltages. ||| 5 ||| 
2018 ||| effective influences in neuronal networks: attentional modulation of effective influences underlying flexible processing and how to measure them. ||| 6 ||| 
2018 ||| object completion effects in attention and memory. ||| 7 ||| 
2021 ||| transformer-based nmt: modeling, training and implementation. ||| 8 ||| 
2017 ||| a computational model of visual attention. ||| 9 ||| 
2019 ||| sensing physiological arousal and visual attention during user interaction. ||| 10 ||| 
2017 ||| visual attention for high-fidelity imaging. ||| 11 ||| 
2017 ||| trade-offs between focused and distributed temporal attention. ||| 12 ||| 
2018 ||| visual attention mechanism in deep learning and its applications. ||| 13 ||| 
2019 ||| perceptual issues of visual attention and depth perception in augmented reality. ||| 14 ||| 
2019 ||| topology and attention in computational pathology. ||| 15 ||| 
2017 ||| ratives. (study and prediction of visual attention with deep learning networks in view of assessment of patients with neurodegenerative diseases). ||| 16 ||| 
2018 ||| e sur l'attention). ||| 17 ||| 
2019 ||| les d'attention). ||| 18 ||| 
2021 ||| e sur l'attention). ||| 19 ||| 
2017 ||| computational methods in the study of individuals' attention online. ||| 20 ||| 
2018 ||| limited attention, the use of accounting information and its impacts on individual investment decision making. ||| 21 ||| 
2021 ||| computational methods for measurement of visual attention from videos towards large-scale behavioral analysis. ||| 22 ||| 
2018 ||| uncertainty estimation of visual attention models using spatiotemporal analysis. ||| 23 ||| 
2018 ||| attentional shapecontextnet for point cloud recognition. ||| 24 ||| 
2017 ||| visual search does not fully characterize feature-based selective attention: evidence from the centroid paradigm. ||| 25 ||| 
2017 ||| using the centroid method to study feature based selective attention. ||| 26 ||| 
2020 ||| computational modeling for visual attention analysis. ||| 27 ||| 
2020 ||| adapting interaction based on users' visual attention. ||| 28 ||| 
2019 ||| memory for problem solving: comparative studies in attention, working and long-term memory. ||| 29 ||| 
2017 ||| active attention for target detection and recognition in robot vision. ||| 30 ||| 
2020 ||| assisting students with attention deficit disorder through technology. ||| 31 ||| 
2018 ||| a web application for reading and attentional assessments. ||| 32 ||| 33 ||| 34 ||| 35 ||| 36 ||| 
2019 ||| investigating the variation of mental fatigue and attention control of obstructive sleep apnea patients. ||| 37 ||| 38 ||| 39 ||| 40 ||| 41 ||| 42 ||| 43 ||| 
2020 ||| feature extraction with bidirectional encoder representations from transformers in hyperspectral images. ||| 44 ||| 45 ||| 46 ||| 
2019 ||| using morpheme-level attention mechanism for turkish sequence labelling. ||| 47 ||| 48 ||| 
2020 ||| using adaptive locally connected layer in attention based deep neural network for speech command recognition. ||| 49 ||| 50 ||| 
2017 ||| bit allocation with visual attention and visual distortion sensitivity. ||| 51 ||| 52 ||| 
2017 ||| maximum overlap discrete wavelet based transformer differential protection. ||| 53 ||| 54 ||| 
2021 ||| twitter dataset and evaluation of transformers for turkish sentiment analysis. ||| 55 ||| 56 ||| 57 ||| 58 ||| 59 ||| 
2021 ||| surgical activity recognition with transformer networks. ||| 60 ||| 61 ||| 
2018 ||| a passive brain-computer interface for monitoring mental attention state. ||| 62 ||| 63 ||| 64 ||| 
2020 ||| attention model for extracting saliency map in driving videos. ||| 65 ||| 66 ||| 
2021 ||| horizontal attention convolution layer for stereo matching. ||| 67 ||| 68 ||| 69 ||| 
2020 ||| transformer protection algorithm based on s-transform. ||| 70 ||| 53 ||| 71 ||| 
2018 ||| hierarchical neural model with attention mechanisms for the classification of social media text related to mental health. ||| 72 ||| 73 ||| 74 ||| 75 ||| 76 ||| 
2019 ||| fast domain adaptation of semantic parsers via paraphrase attention. ||| 77 ||| 78 ||| 79 ||| 
2019 ||| weakly supervised attentional model for low resource ad-hoc cross-lingual information retrieval. ||| 80 ||| 81 ||| 82 ||| 83 ||| 84 ||| 
2019 ||| deep bidirectional transformers for relation extraction without supervision. ||| 85 ||| 86 ||| 87 ||| 
2017 ||| what lies above: alternative user experiences produced through focussing attention on gnss infrastructure. ||| 88 ||| 89 ||| 90 ||| 91 ||| 
2017 ||| attention from afar: simulating the gazes of remote participants in hybrid meetings. ||| 92 ||| 93 ||| 94 ||| 
2018 ||| attending to breath: exploring how the cues in a virtual environment guide the attention to breath and shape the quality of experience to support mindfulness. ||| 95 ||| 96 ||| 97 ||| 98 ||| 99 ||| 100 ||| 101 ||| 102 ||| 
2021 ||| nomen est omen - the role of signatures in ascribing email author identity with transformer neural networks. ||| 103 ||| 104 ||| 105 ||| 106 ||| 
2021 ||| adversarial watermarking transformer: towards tracing text provenance with data hiding. ||| 107 ||| 108 ||| 
2021 ||| automatic modulation classification based on improved r-transformer. ||| 109 ||| 
2020 ||| a novel image classification model jointing attention and resnet for scratch. ||| 110 ||| 111 ||| 112 ||| 113 ||| 
2019 ||| an attention-mechanism-based traffic flow prediction scheme for smart city. ||| 114 ||| 115 ||| 116 ||| 117 ||| 118 ||| 119 ||| 
2021 ||| network intrusion detection based on dense dilated convolutions and attention mechanism. ||| 120 ||| 121 ||| 122 ||| 123 ||| 124 ||| 125 ||| 
2021 ||| a graph attention mechanism based multi-agent reinforcement learning method for efficient traffic light control. ||| 126 ||| 127 ||| 128 ||| 129 ||| 130 ||| 
2020 ||| ksf-st: video captioning based on key semantic frames extraction and spatio-temporal attention mechanism. ||| 131 ||| 132 ||| 133 ||| 134 ||| 135 ||| 136 ||| 
2021 ||| residual transformer network for 3d objects classification. ||| 137 ||| 138 ||| 139 ||| 
2021 ||| topic enhanced multi-head co-attention: generating distractors for reading comprehension. ||| 140 ||| 141 ||| 142 ||| 143 ||| 144 ||| 
2020 ||| cascading top-down attention for visual question answering. ||| 145 ||| 146 ||| 147 ||| 
2021 ||| learning dynamic coherence with graph attention network for biomedical entity linking. ||| 148 ||| 149 ||| 
2019 ||| urban area vehicle re-identification with self-attention stair feature fusion and temporal bayesian re-ranking. ||| 150 ||| 151 ||| 152 ||| 
2021 ||| attention-guided progressive partition network for human parsing. ||| 153 ||| 154 ||| 155 ||| 
2021 ||| annealing attention networks for user feature-based rumor early detection on weibo. ||| 156 ||| 157 ||| 158 ||| 159 ||| 
2019 ||| ta-blstm: tag attention-based bidirectional long short-term memory for service recommendation in mashup creation. ||| 160 ||| 161 ||| 162 ||| 
2021 ||| multi-aspect controlled response generation in a multimodal dialogue system using hierarchical transformer network. ||| 163 ||| 164 ||| 165 ||| 
2020 ||| a cascaded step-temporal attention network for ecg arrhythmia classification. ||| 166 ||| 167 ||| 168 ||| 169 ||| 170 ||| 
2019 ||| a novel two-factor attention encoder-decoder network through combining temporal and prior knowledge for weather forecasting. ||| 171 ||| 172 ||| 173 ||| 174 ||| 175 ||| 
2021 ||| pay attention: improving classification of pe malware using attention mechanisms based on system call analysis. ||| 176 ||| 177 ||| 178 ||| 179 ||| 180 ||| 
2020 ||| sasrm: a semantic and attention spatio-temporal recurrent model for next location prediction. ||| 181 ||| 182 ||| 183 ||| 184 ||| 185 ||| 
2019 ||| condensed convolution neural network by attention over self-attention for stance detection in twitter. ||| 186 ||| 187 ||| 188 ||| 189 ||| 
2020 ||| penalty-based sequence generative adversarial networks with enhanced transformer for text generation. ||| 190 ||| 191 ||| 
2020 ||| an hardware-aware image polarity detector enhanced with visual attention. ||| 192 ||| 193 ||| 194 ||| 195 ||| 196 ||| 
2020 ||| poem generation using transformers and doc2vec embeddings. ||| 197 ||| 198 ||| 
2021 ||| mask region-oriented diabetic retinopathies detection in ophthalmic medical images via non-local attention. ||| 199 ||| 200 ||| 201 ||| 202 ||| 
2021 ||| sequential recommendation with context-aware collaborative graph attention networks. ||| 203 ||| 204 ||| 205 ||| 206 ||| 207 ||| 
2021 ||| a relation-guided attention mechanism for relational triple extraction. ||| 208 ||| 209 ||| 210 ||| 
2021 ||| a spatial-temporal graph attention network for multi-intersection traffic light control. ||| 211 ||| 212 ||| 213 ||| 214 ||| 215 ||| 216 ||| 
2021 ||| ccgl-gan: criss-cross attention and global-local discriminator generative adversarial networks for text-to-image synthesis. ||| 217 ||| 218 ||| 
2021 ||| macro discourse relation recogniztion based on micro discourse structure and self-interactive attention network. ||| 219 ||| 220 ||| 221 ||| 222 ||| 
2021 ||| a structural transformer with relative positions in trees for code-to-sequence tasks. ||| 223 ||| 224 ||| 225 ||| 
2019 ||| attention-based adversarial training for seamless nudity censorship. ||| 226 ||| 227 ||| 228 ||| 229 ||| 
2019 ||| ensemble attention for text recognition in natural images. ||| 230 ||| 231 ||| 232 ||| 233 ||| 234 ||| 
2020 ||| using self-attention lstms to enhance observations in goal recognition. ||| 235 ||| 236 ||| 237 ||| 238 ||| 239 ||| 
2019 ||| multi-perspective feature generation based on attention mechanism. ||| 240 ||| 241 ||| 
2018 ||| a novel document classification algorithm based on statistical features and attention mechanism. ||| 242 ||| 243 ||| 244 ||| 
2020 ||| object detection with extended attention and spatial information. ||| 245 ||| 246 ||| 247 ||| 
2021 ||| region attention network for single image super-resolution. ||| 248 ||| 249 ||| 250 ||| 
2019 ||| hierarchical recurrent attention networks for context-aware education chatbots. ||| 251 ||| 252 ||| 
2021 ||| deep recurrent neural networks with attention mechanisms for respiratory anomaly classification. ||| 253 ||| 254 ||| 255 ||| 256 ||| 
2021 ||| a novel joint model with second-order features and matching attention for aspect-based sentiment analysis. ||| 257 ||| 258 ||| 259 ||| 260 ||| 254 ||| 
2021 ||| attentional social recommendation system with graph convolutional network. ||| 261 ||| 262 ||| 263 ||| 264 ||| 
2020 ||| attention-based 3d object reconstruction from a single image. ||| 265 ||| 266 ||| 267 ||| 268 ||| 229 ||| 
2019 ||| bci and multimodal feedback based attention regulation for lower limb rehabilitation. ||| 269 ||| 270 ||| 271 ||| 272 ||| 273 ||| 274 ||| 275 ||| 276 ||| 
2019 ||| improving universal language model fine-tuning using attention mechanism. ||| 277 ||| 278 ||| 279 ||| 280 ||| 281 ||| 282 ||| 
2020 ||| unleashing the potential of attention model for news headline generation. ||| 283 ||| 284 ||| 285 ||| 286 ||| 
2019 ||| a gan model with self-attention mechanism to generate multi-instruments symbolic music. ||| 287 ||| 288 ||| 289 ||| 
2020 ||| dynamic graph attention-aware networks for session-based recommendation. ||| 290 ||| 291 ||| 292 ||| 
2021 ||| attention based double layer lstm for chinese image captioning. ||| 293 ||| 294 ||| 
2019 ||| an attention-based hybrid lstm-cnn model for arrhythmias classification. ||| 295 ||| 296 ||| 297 ||| 298 ||| 299 ||| 300 ||| 301 ||| 
2021 ||| a dual-questioning attention network for emotion-cause pair extraction with context awareness. ||| 302 ||| 303 ||| 304 ||| 
2019 ||| attention-driven driving maneuver detection system. ||| 305 ||| 306 ||| 307 ||| 308 ||| 309 ||| 
2019 ||| convolutional lstm network with hierarchical attention for relation classification in clinical texts. ||| 310 ||| 311 ||| 312 ||| 313 ||| 314 ||| 315 ||| 
2020 ||| automatic lyrics transcription using dilated convolutional neural networks with self-attention. ||| 316 ||| 317 ||| 318 ||| 319 ||| 
2021 ||| main: multihead-attention imputation networks. ||| 320 ||| 321 ||| 322 ||| 323 ||| 
2019 ||| self-attention based network for medical query expansion. ||| 324 ||| 325 ||| 326 ||| 327 ||| 328 ||| 329 ||| 
2019 ||| dynamic fusion of convolutional features based on spatial and temporal attention for visual tracking. ||| 330 ||| 331 ||| 
2021 ||| attention-based multi-proximity preserved attributed network embedding. ||| 332 ||| 333 ||| 334 ||| 335 ||| 
2020 ||| neighborhood-aware attention network for semi-supervised face recognition. ||| 336 ||| 337 ||| 338 ||| 
2020 ||| m3la: a novel approach based on encoder-decoder with attention framework for multi-modal multi-label learning. ||| 339 ||| 340 ||| 
2020 ||| double attention for pathology image diagnosis network with visual interpretability. ||| 341 ||| 342 ||| 343 ||| 344 ||| 345 ||| 346 ||| 347 ||| 
2018 ||| a neural generation-based conversation model using fine-grained emotion-guide attention. ||| 348 ||| 349 ||| 350 ||| 
2019 ||| character-aware convolutional recurrent networks with self-attention for emotion detection on twitter. ||| 351 ||| 352 ||| 353 ||| 354 ||| 355 ||| 
2020 ||| federated multi-task learning with hierarchical attention for sensor data analytics. ||| 356 ||| 357 ||| 358 ||| 359 ||| 
2021 ||| multi-actor-attention-critic reinforcement learning for central place foraging swarms. ||| 360 ||| 361 ||| 362 ||| 363 ||| 364 ||| 
2019 ||| directional attention based video frame prediction using graph convolutional networks. ||| 365 ||| 366 ||| 
2021 ||| multiplicative attention mechanism for multi-horizon time series forecasting. ||| 367 ||| 368 ||| 369 ||| 
2020 ||| multi-channel co-attention network for visual question answering. ||| 145 ||| 370 ||| 371 ||| 147 ||| 
2017 ||| classification of radiology reports using neural attention models. ||| 372 ||| 373 ||| 374 ||| 375 ||| 
2020 ||| heterogeneous graph attention networks for early detection of rumors on twitter. ||| 376 ||| 377 ||| 378 ||| 379 ||| 
2019 ||| pyramid attention dense network for image super-resolution. ||| 380 ||| 381 ||| 382 ||| 383 ||| 384 ||| 
2021 ||| a method of relation extraction: integrating graph convolutional networks, relative entity position attention and back-multi-head-attention mechanism. ||| 385 ||| 386 ||| 387 ||| 
2021 ||| parallel scale-wise attention network for effective scene text recognition. ||| 388 ||| 389 ||| 390 ||| 391 ||| 392 ||| 
2020 ||| heterogeneous multi-modal sensor fusion with hybrid attention for exercise recognition. ||| 393 ||| 394 ||| 395 ||| 
2020 ||| scene attention mechanism for remote sensing image caption generation. ||| 396 ||| 397 ||| 398 ||| 399 ||| 400 ||| 
2020 ||| wavelet denoising and attention-based rnn- arima model to predict forex price. ||| 401 ||| 402 ||| 
2021 ||| a multimodal classification of noisy hate speech using character level embedding and attention. ||| 403 ||| 404 ||| 405 ||| 
2020 ||| attention and graph matching network for retrieval-based dialogue system with domain knowledge. ||| 210 ||| 406 ||| 
2021 ||| local frequency domain transformer networks for video prediction. ||| 407 ||| 408 ||| 409 ||| 
2018 ||| distant supervision for relation extraction with hierarchical attention and entity descriptions. ||| 410 ||| 411 ||| 412 ||| 413 ||| 
2021 ||| universal transformer hawkes process. ||| 414 ||| 415 ||| 416 ||| 417 ||| 418 ||| 419 ||| 
2021 ||| edge prior and spatial attention fusion enhanced hierarchical multi-patch network for image deblurring. ||| 420 ||| 421 ||| 422 ||| 423 ||| 
2021 ||| automatic topic labeling model with paired-attention based on pre-trained deep neural network. ||| 424 ||| 425 ||| 426 ||| 427 ||| 428 ||| 429 ||| 
2020 ||| dynamic global-local attention network based on capsules for text classification. ||| 430 ||| 431 ||| 432 ||| 433 ||| 434 ||| 
2019 ||| attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| 435 ||| 436 ||| 437 ||| 127 ||| 
2021 ||| multiple self-attention network for intracranial vessel segmentation. ||| 438 ||| 439 ||| 440 ||| 441 ||| 
2021 ||| embedding extra knowledge and a dependency tree based on a graph attention network for aspect-based sentiment analysis. ||| 442 ||| 443 ||| 444 ||| 
2019 ||| an end-to-end location and regression tracker with attention-based fused features. ||| 445 ||| 446 ||| 447 ||| 
2020 ||| knowledge-based context-aware multi-turn conversational model with hierarchical attention. ||| 448 ||| 449 ||| 
2020 ||| nasabn: a neural architecture search framework for attention-based networks. ||| 450 ||| 451 ||| 452 ||| 
2021 ||| a multi -role graph attention network for knowledge graph alignment. ||| 453 ||| 454 ||| 284 ||| 286 ||| 
2020 ||| heterogeneous information network embedding with convolutional graph attention networks. ||| 455 ||| 456 ||| 457 ||| 458 ||| 459 ||| 
2021 ||| generating fake cyber threat intelligence using transformer-based models. ||| 460 ||| 461 ||| 462 ||| 463 ||| 464 ||| 
2021 ||| spatial generation of molecules with transformers. ||| 465 ||| 466 ||| 467 ||| 
2021 ||| novel soft sensor model based on spatio-temporal attention. ||| 468 ||| 469 ||| 470 ||| 471 ||| 472 ||| 473 ||| 
2019 ||| improving sentence representations with local and global attention for classification. ||| 474 ||| 475 ||| 476 ||| 477 ||| 478 ||| 479 ||| 
2021 ||| transformer with local-feature extractor for relation extraction. ||| 480 ||| 481 ||| 
2021 ||| deep personalized glucose level forecasting using attention-based recurrent neural networks. ||| 482 ||| 483 ||| 484 ||| 485 ||| 
2020 ||| vocoder-free end-to-end voice conversion with transformer network. ||| 486 ||| 487 ||| 488 ||| 
2021 ||| dense video captioning with hierarchical attention-based encoder-decoder networks. ||| 489 ||| 490 ||| 491 ||| 
2018 ||| classify sentence from multiple perspectives with category expert attention network. ||| 492 ||| 493 ||| 494 ||| 329 ||| 
2021 ||| link prediction with multiple structural attentions in multiplex networks. ||| 495 ||| 496 ||| 497 ||| 498 ||| 
2021 ||| attention-based lstm for motion switching detection of particles in living cells. ||| 499 ||| 500 ||| 
2018 ||| a fully attention-based information retriever. ||| 501 ||| 502 ||| 503 ||| 504 ||| 505 ||| 
2020 ||| dsmith: compiler fuzzing through generative deep learning model with attention. ||| 506 ||| 507 ||| 508 ||| 509 ||| 510 ||| 
2021 ||| pgmanet: pose-guided mixed attention network for occluded person re-identification. ||| 511 ||| 512 ||| 513 ||| 514 ||| 515 ||| 
2020 ||| grammatical error detection with self attention by pairwise training. ||| 516 ||| 517 ||| 
2019 ||| text classification using gated and transposed attention networks. ||| 518 ||| 519 ||| 
2019 ||| abstractive summarization with keyword and generated word attention. ||| 520 ||| 521 ||| 
2021 ||| concept-based topic attention for a convolutional sequence document summarization model. ||| 522 ||| 523 ||| 524 ||| 
2021 ||| sintactical distance attention guided graph convolutional network for aspect-based sentiment analisis. ||| 525 ||| 526 ||| 527 ||| 528 ||| 529 ||| 
2020 ||| a dual transformer model for intelligent decision support for maintenance of wind turbines. ||| 530 ||| 531 ||| 
2021 ||| dual multi-task network with bridge-temporal-attention for student emotion recognition via classroom video. ||| 532 ||| 533 ||| 534 ||| 535 ||| 536 ||| 
2021 ||| fake news detection on news-oriented heterogeneous information networks through hierarchical graph attention. ||| 537 ||| 538 ||| 
2021 ||| attention-based spatio-temporal graphic lstm for eeg emotion recognition. ||| 539 ||| 540 ||| 541 ||| 542 ||| 543 ||| 
2021 ||| weakly labeled semi-supervised sound event detection with multi-scale residual attention. ||| 544 ||| 545 ||| 546 ||| 
2019 ||| focalnet - foveal attention for post-processing dnn outputs. ||| 547 ||| 548 ||| 
2020 ||| transkp: transformer based key-phrase extraction. ||| 549 ||| 550 ||| 551 ||| 552 ||| 553 ||| 
2021 ||| personalized session-based recommendation using graph attention networks. ||| 554 ||| 555 ||| 556 ||| 557 ||| 558 ||| 559 ||| 308 ||| 
2017 ||| mipal: multiple-instance passive aggressive learning for identification of attention deficit hyperactive disorder from fmri. ||| 560 ||| 561 ||| 562 ||| 
2021 ||| prototypical inception network with cross branch attention for time series classification. ||| 563 ||| 564 ||| 565 ||| 
2021 ||| curvilinear collaborative metric learning with macro-micro attentions. ||| 566 ||| 567 ||| 568 ||| 
2021 ||| multi-scale attention network based on multi-feature fusion for person re-identification. ||| 569 ||| 570 ||| 571 ||| 572 ||| 573 ||| 574 ||| 
2021 ||| ssrnas: search space reduced one-shot nas by a recursive attention-based predictor with cell tensor-flow diagram. ||| 575 ||| 457 ||| 576 ||| 
2020 ||| a dynamic-attention on crowd region with physical optical flow features for crowd counting. ||| 577 ||| 578 ||| 579 ||| 580 ||| 
2021 ||| simdet: cross similarity attention for one-shot object detection. ||| 581 ||| 582 ||| 583 ||| 584 ||| 
2018 ||| multi-granularity hierarchical attention siamese network for visual tracking. ||| 585 ||| 586 ||| 587 ||| 588 ||| 589 ||| 590 ||| 
2021 ||| ada: adaptive depth attention model for click - through rate prediction. ||| 591 ||| 592 ||| 155 ||| 
2021 ||| discrete auto-regressive variational attention models for text modeling. ||| 593 ||| 594 ||| 595 ||| 596 ||| 597 ||| 598 ||| 
2021 ||| attention-based multi-filter convolutional neural network for inappropriate speech detection. ||| 599 ||| 600 ||| 
2019 ||| visual relationship attention for image captioning. ||| 601 ||| 602 ||| 603 ||| 604 ||| 
2018 ||| automatic chromosome classification using deep attention based sequence learning of chromosome bands. ||| 605 ||| 606 ||| 607 ||| 
2021 ||| 3d multi-branch encoder-decoder networks with attentional feature fusion for pulmonary nodule detection in ct scans. ||| 608 ||| 609 ||| 610 ||| 611 ||| 
2019 ||| image captioning based on sentence-level and word-level attention. ||| 612 ||| 264 ||| 613 ||| 614 ||| 615 ||| 
2021 ||| hierarchical attention transformer networks for long document classification. ||| 616 ||| 617 ||| 618 ||| 619 ||| 620 ||| 621 ||| 
2018 ||| attention-based bilstm network with lexical feature for emotion classification. ||| 622 ||| 623 ||| 624 ||| 625 ||| 626 ||| 627 ||| 
2021 ||| transformer-based relation detect model for aspect-based sentiment analysis. ||| 141 ||| 143 ||| 628 ||| 629 ||| 144 ||| 
2020 ||| adaptive graph convolutional networks with attention mechanism for relation extraction. ||| 264 ||| 630 ||| 631 ||| 613 ||| 262 ||| 
2020 ||| toward tag-free aspect based sentiment analysis: a multiple attention network approach. ||| 632 ||| 633 ||| 634 ||| 
2020 ||| cccnet: an attention based deep learning framework for categorized counting of crowd in different body states. ||| 635 ||| 636 ||| 637 ||| 
2018 ||| fine-grained air quality prediction using attention based neural network. ||| 638 ||| 639 ||| 640 ||| 641 ||| 642 ||| 
2020 ||| hierarchical component-attention based speaker turn embedding for emotion recognition. ||| 643 ||| 644 ||| 645 ||| 646 ||| 647 ||| 648 ||| 649 ||| 
2020 ||| a bert-based approach with relation-aware attention for knowledge base question answering. ||| 650 ||| 651 ||| 652 ||| 
2019 ||| distant supervised why-question generation with passage self-matching attention. ||| 653 ||| 654 ||| 655 ||| 656 ||| 657 ||| 658 ||| 659 ||| 660 ||| 
2020 ||| forecasting photovoltaic power production using a deep learning sequence to sequence model with attention. ||| 661 ||| 662 ||| 663 ||| 664 ||| 
2021 ||| predicting conversation outcomes using multimodal transformer. ||| 665 ||| 666 ||| 667 ||| 668 ||| 669 ||| 
2021 ||| tag-aware attentional graph neural networks for personalized tag recommendation. ||| 670 ||| 671 ||| 672 ||| 
2018 ||| ta4rec: recurrent neural networks with time attention factors for session-based recommendations. ||| 673 ||| 674 ||| 675 ||| 
2021 ||| deep-attention model to analyze reliable customers via federated learning. ||| 676 ||| 677 ||| 678 ||| 
2020 ||| transformer decoder based reinforcement learning approach for conversational response generation. ||| 679 ||| 680 ||| 681 ||| 
2020 ||| multimodal emotion recognition using deep generalized canonical correlation analysis with an attention mechanism. ||| 682 ||| 683 ||| 684 ||| 
2020 ||| dynamic attention aggregation with bert for neural machine translation. ||| 685 ||| 686 ||| 687 ||| 688 ||| 689 ||| 690 ||| 
2019 ||| neural networks applied in the prediction of top oil temperature of transformer. ||| 691 ||| 692 ||| 693 ||| 694 ||| 
2020 ||| cascade modeling with multihead self-attention. ||| 695 ||| 696 ||| 697 ||| 698 ||| 699 ||| 
2021 ||| cacnet: cube attentional cnn for automatic speech recognition. ||| 700 ||| 701 ||| 702 ||| 703 ||| 704 ||| 705 ||| 
2019 ||| spatial-temporal attention network for malware detection using micro-architecture features. ||| 706 ||| 707 ||| 708 ||| 709 ||| 
2021 ||| fa-gal-resnet: lightweight residual network using focused attention mechanism and generative adversarial learning via knowledge distillation. ||| 710 ||| 711 ||| 712 ||| 713 ||| 714 ||| 715 ||| 
2021 ||| a relation-aware attention neural network for modeling the usage of scientific online resources. ||| 716 ||| 688 ||| 717 ||| 718 ||| 719 ||| 689 ||| 
2020 ||| robust semi-supervised semantic segmentation based on self-attention and spectral normalization. ||| 720 ||| 264 ||| 613 ||| 262 ||| 
2019 ||| a transformer-based variational autoencoder for sentence generation. ||| 721 ||| 286 ||| 
2021 ||| visual explanation using attention mechanism in actor-critic-based deep reinforcement learning. ||| 722 ||| 723 ||| 724 ||| 725 ||| 726 ||| 
2019 ||| text attention and focal negative loss for scene text detection. ||| 727 ||| 728 ||| 
2021 ||| eacoupledcf: an enhanced attention-based coupled collaborative filtering approach for recommendation. ||| 729 ||| 730 ||| 731 ||| 732 ||| 
2021 ||| nvnet: an enhanced attention network for segmenting neck vascular from ultrasound images. ||| 733 ||| 734 ||| 735 ||| 
2020 ||| dual semantic relationship attention network for image-text matching. ||| 736 ||| 737 ||| 
2020 ||| convolutional transformer with sentiment-aware attention for sentiment analysis. ||| 481 ||| 738 ||| 739 ||| 740 ||| 
2019 ||| stochastic imputation and uncertainty-aware attention to ehr for mortality prediction. ||| 741 ||| 742 ||| 743 ||| 
2021 ||| driving maneuver detection using features of driver's attention and face shift through deeping learning. ||| 307 ||| 308 ||| 
2021 ||| gap-wf: graph attention pooling network for fine-grained ssl/tls website fingerprinting. ||| 744 ||| 745 ||| 746 ||| 747 ||| 748 ||| 749 ||| 750 ||| 
2021 ||| cap-gan: towards adversarial robustness with cycle-consistent attentional purification. ||| 751 ||| 752 ||| 753 ||| 754 ||| 
2020 ||| big-transformer: integrating hierarchical features for transformer via bipartite graph. ||| 755 ||| 756 ||| 757 ||| 758 ||| 759 ||| 
2021 ||| integrating argument-level attention with multi-level scores to predict what happen next. ||| 760 ||| 507 ||| 761 ||| 762 ||| 
2021 ||| fa-iati: a framework of frequency adaptive and iterative attention interaction for image-text matching. ||| 763 ||| 764 ||| 765 ||| 766 ||| 
2021 ||| cans: coupled-attention networks for sarcasm detection on social media. ||| 767 ||| 768 ||| 769 ||| 
2018 ||| interpretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling. ||| 770 ||| 771 ||| 772 ||| 773 ||| 774 ||| 775 ||| 776 ||| 
2021 ||| heterogeneous graph gated attention network. ||| 777 ||| 415 ||| 417 ||| 418 ||| 
2020 ||| scene text recognition by attention network with gated embedding. ||| 778 ||| 779 ||| 
2019 ||| data-to-text generation with attention recurrent unit. ||| 780 ||| 781 ||| 782 ||| 783 ||| 
2017 ||| designing an adaptive attention mechanism for relation classification. ||| 784 ||| 785 ||| 786 ||| 
2021 ||| mcdalnet: multi-scale contextual dual attention learning network for medical image segmentation. ||| 787 ||| 788 ||| 789 ||| 790 ||| 
2021 ||| bilingual self-attention network: generating headlines for online linguistic questions. ||| 791 ||| 792 ||| 602 ||| 793 ||| 
2021 ||| spatio-temporal action detector with self-attention. ||| 794 ||| 589 ||| 586 ||| 795 ||| 796 ||| 797 ||| 
2019 ||| dagcn: dual attention graph convolutional networks. ||| 798 ||| 799 ||| 800 ||| 801 ||| 802 ||| 
2021 ||| attention-based seq2seq regularisation for relation extraction. ||| 803 ||| 567 ||| 
2020 ||| predicting outcomes of chemical reactions: a seq2seq approach with multi-view attention and edge embedding. ||| 804 ||| 805 ||| 806 ||| 807 ||| 
2021 ||| boost transformer with bert and copying mechanism for asr error correction. ||| 808 ||| 809 ||| 157 ||| 810 ||| 811 ||| 
2021 ||| information reuse attention in convolutional neural networks for facial expression recognition in the wild. ||| 812 ||| 813 ||| 
2021 ||| a contextual attention network for multimodal emotion recognition in conversation. ||| 814 ||| 815 ||| 816 ||| 817 ||| 
2021 ||| a visual self-attention network for facial expression recognition. ||| 818 ||| 819 ||| 
2021 ||| out-of-distribution detection with uncertainty enhanced attention maps. ||| 820 ||| 821 ||| 
2019 ||| hierarchical multi-dimensional attention model for answer selection. ||| 683 ||| 241 ||| 240 ||| 822 ||| 729 ||| 
2020 ||| gcn-lrp explanation: exploring latent attention of graph convolutional networks. ||| 823 ||| 824 ||| 825 ||| 
2019 ||| attention-driven multi-sensor selection. ||| 826 ||| 827 ||| 828 ||| 829 ||| 830 ||| 
2021 ||| multi-scale spatial transformer network for lidar-camera 3d object detection. ||| 831 ||| 832 ||| 833 ||| 834 ||| 835 ||| 836 ||| 
2021 ||| hcov: a target attention-based filter pruning with retaining high-covariance feature map. ||| 837 ||| 838 ||| 839 ||| 840 ||| 
2020 ||| an improved template representation-based transformer for abstractive text summarization. ||| 841 ||| 842 ||| 843 ||| 
2021 ||| multi-attention based spatial-temporal graph convolution networks for traffic flow forecasting. ||| 844 ||| 845 ||| 
2019 ||| attention-based multi-instance neural network for medical diagnosis from incomplete and low quality data. ||| 846 ||| 847 ||| 848 ||| 849 ||| 
2017 ||| fusing attention with visual question answering. ||| 850 ||| 851 ||| 852 ||| 853 ||| 854 ||| 
2021 ||| func-esim: a dual pairwise attention network for cross-version binary function matching. ||| 855 ||| 856 ||| 857 ||| 858 ||| 859 ||| 860 ||| 861 ||| 
2021 ||| dtgan: dual attention generative adversarial networks for text-to-image generation. ||| 862 ||| 863 ||| 
2021 ||| a transformer based multi-task model for domain classification, intent detection and slot-filling. ||| 864 ||| 865 ||| 404 ||| 405 ||| 
2021 ||| context and knowledge enriched transformer framework for emotion recognition in conversations. ||| 866 ||| 867 ||| 165 ||| 405 ||| 
2021 ||| attention-based graph neural network for news recommendation. ||| 868 ||| 869 ||| 870 ||| 852 ||| 871 ||| 872 ||| 873 ||| 
2020 ||| personalized destination prediction using transformers in a contextless data setting. ||| 874 ||| 875 ||| 876 ||| 877 ||| 
2021 ||| ptwa: pre-training with word attention for chinese named entity recognition. ||| 878 ||| 879 ||| 880 ||| 881 ||| 882 ||| 
2021 ||| author name disambiguation using multiple graph attention networks. ||| 883 ||| 884 ||| 885 ||| 886 ||| 887 ||| 888 ||| 889 ||| 890 ||| 
2019 ||| an attention-based model for learning dynamic interaction networks. ||| 891 ||| 892 ||| 893 ||| 894 ||| 895 ||| 
2020 ||| toward improving the evaluation of visual attention models: a crowdsourcing approach. ||| 896 ||| 897 ||| 898 ||| 
2021 ||| transfake: multi-task transformer for multimodal enhanced fake news detection. ||| 899 ||| 900 ||| 901 ||| 902 ||| 903 ||| 904 ||| 905 ||| 
2020 ||| attention-based multi-model ensemble for automatic cataract detection in b-scan eye ultrasound images. ||| 906 ||| 907 ||| 908 ||| 909 ||| 
2021 ||| a generative bayesian graph attention network for semi-supervised classification on scarce data. ||| 910 ||| 911 ||| 912 ||| 913 ||| 914 ||| 
2020 ||| attention-based deep learning model for text readability evaluation. ||| 915 ||| 916 ||| 917 ||| 918 ||| 
2019 ||| adpr: an attention-based deep learning point-of-interest recommendation framework. ||| 919 ||| 920 ||| 921 ||| 922 ||| 923 ||| 924 ||| 
2021 ||| on the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition. ||| 925 ||| 926 ||| 
2019 ||| graph convolutional networks with structural attention model for aspect based sentiment analysis. ||| 927 ||| 928 ||| 929 ||| 930 ||| 
2021 ||| dual sequential recommendation integrating high-order collaborative relations via graph attention networks. ||| 931 ||| 932 ||| 
2021 ||| revisiting the onsets and frames model with additive attention. ||| 933 ||| 934 ||| 935 ||| 936 ||| 
2018 ||| robust human action recognition using global spatial-temporal attention for human skeleton data. ||| 937 ||| 938 ||| 939 ||| 940 ||| 941 ||| 942 ||| 
2021 ||| attention and adaptive bilinear matching network for cross-domain few-shot defect classification of industrial parts. ||| 943 ||| 944 ||| 945 ||| 946 ||| 947 ||| 
2020 ||| bilinear semi-tensor product attention (bstpa) model for visual question answering. ||| 948 ||| 949 ||| 950 ||| 951 ||| 952 ||| 953 ||| 954 ||| 
2021 ||| multitask learning using bert with task-embedded attention. ||| 955 ||| 956 ||| 
2020 ||| att: attention-based timbre transfer. ||| 957 ||| 958 ||| 959 ||| 960 ||| 961 ||| 
2020 ||| a semantic subgraphs based link prediction method for heterogeneous social networks with graph attention networks. ||| 457 ||| 455 ||| 
2020 ||| event recognition with automatic album detection based on sequential grouping of confidence scores and neural attention. ||| 962 ||| 
2021 ||| paying attention: using a siamese pyramid network for the prediction of protein-protein interactions with folding and self-binding primary sequences. ||| 963 ||| 964 ||| 965 ||| 966 ||| 
2020 ||| rad: reinforced attention decoder model on question generation. ||| 633 ||| 967 ||| 968 ||| 969 ||| 970 ||| 971 ||| 972 ||| 
2021 ||| tt2inet: text to photo-realistic image synthesis with transformer as text encoder. ||| 973 ||| 264 ||| 262 ||| 
2020 ||| multi-object tracking via multi-attention. ||| 974 ||| 975 ||| 976 ||| 977 ||| 
2019 ||| ta-stan: a deep spatial-temporal attention learning framework for regional traffic accident risk prediction. ||| 978 ||| 979 ||| 980 ||| 
2020 ||| att-darts: differentiable neural architecture search for attention. ||| 981 ||| 982 ||| 983 ||| 
2019 ||| abstractive text summarization with multi-head attention. ||| 984 ||| 985 ||| 986 ||| 987 ||| 988 ||| 989 ||| 
2021 ||| an image captioning approach using dynamical attention. ||| 990 ||| 737 ||| 
2020 ||| multivariate time series classification with an attention-based multivariate convolutional neural network. ||| 991 ||| 992 ||| 
2019 ||| spinal stenosis detection in mri using modular coordinate convolutional attention networks. ||| 993 ||| 994 ||| 995 ||| 
2021 ||| temporal convolutional attention neural networks for time series forecasting. ||| 996 ||| 997 ||| 998 ||| 
2018 ||| semantic image segmentation based on attentions to intra scales and inner channels. ||| 999 ||| 1000 ||| 1001 ||| 
2020 ||| a transformer based approach for identification of tweet acts. ||| 864 ||| 1002 ||| 404 ||| 405 ||| 
2021 ||| critic boosting attention network on local descriptor for few-shot learning. ||| 1003 ||| 1004 ||| 1005 ||| 1006 ||| 
2021 ||| extended attention mechanism for tsp problem. ||| 1007 ||| 
2019 ||| deep fusion: an attention guided factorized bilinear pooling for audio-video emotion recognition. ||| 1008 ||| 1009 ||| 1010 ||| 
2021 ||| ia-cnn: a generalised interpretable convolutional neural network with attention mechanism. ||| 1011 ||| 1012 ||| 1013 ||| 1014 ||| 
2019 ||| question answering with hierarchical attention networks. ||| 4 ||| 1015 ||| 1016 ||| 1017 ||| 
2020 ||| pointing direction estimation for attention target extraction using body-mounted camera. ||| 1018 ||| 1019 ||| 1020 ||| 
2018 ||| transformer-darwin: a hybrid locomotion humanoid designed to walk or roll. ||| 1021 ||| 1022 ||| 
2020 ||| attention-model guided image enhancement for robotic vision applications. ||| 1023 ||| 1024 ||| 1025 ||| 1026 ||| 
2021 ||| syntactic knowledge-infused transformer and bert models. ||| 1027 ||| 1028 ||| 1029 ||| 1030 ||| 1031 ||| 952 ||| 1032 ||| 
2021 ||| review-aware neural recommendation with cross-modality mutual attention. ||| 1033 ||| 1034 ||| 1035 ||| 1036 ||| 
2017 ||| movie fill in the blank with adaptive temporal attention and description update. ||| 1037 ||| 155 ||| 1038 ||| 154 ||| 1039 ||| 1040 ||| 
2020 ||| beyond 512 tokens: siamese multi-depth transformer-based hierarchical encoder for long-form document matching. ||| 1041 ||| 1042 ||| 130 ||| 1043 ||| 1044 ||| 
2019 ||| a dynamic co-attention network for session-based recommendation. ||| 1045 ||| 1046 ||| 1047 ||| 1048 ||| 
2021 ||| litegt: efficient and lightweight graph transformers. ||| 1049 ||| 1050 ||| 1051 ||| 
2021 ||| modeling heterogeneous graph network on fraud detection: a community-based framework with attention mechanism. ||| 1052 ||| 1053 ||| 1054 ||| 1055 ||| 1056 ||| 
2021 ||| disenkgat: knowledge graph embedding with disentangled graph attention network. ||| 1057 ||| 1058 ||| 1059 ||| 1060 ||| 1061 ||| 1062 ||| 293 ||| 1063 ||| 
2019 ||| attention-residual network with cnn for rumor detection. ||| 1064 ||| 1065 ||| 1066 ||| 1067 ||| 
2020 ||| lsan: modeling long-term dependencies and short-term correlations with hierarchical attention for risk prediction. ||| 1068 ||| 1069 ||| 1070 ||| 1071 ||| 
2019 |||  attention-based feature extractor. ||| 1072 ||| 1073 ||| 1074 ||| 1075 ||| 
2021 ||| attention based dynamic graph learning framework for asset pricing. ||| 1076 ||| 1077 ||| 1078 ||| 
2019 ||| knowledge-aware textual entailment with graph attention network. ||| 1079 ||| 1080 ||| 1081 ||| 793 ||| 1082 ||| 
2021 ||| conditional graph attention networks for distilling and refining knowledge graphs in recommendation. ||| 1083 ||| 1084 ||| 1085 ||| 883 ||| 1086 ||| 1087 ||| 1088 ||| 
2021 ||| improving chinese character representation with formation graph attention network. ||| 1089 ||| 1090 ||| 1091 ||| 1092 ||| 1093 ||| 1094 ||| 
2020 ||| rkt: relation-aware self-attention for knowledge tracing. ||| 1095 ||| 1096 ||| 
2021 ||| trilateral spatiotemporal attention network for user behavior modeling in location-based search. ||| 1097 ||| 1098 ||| 1099 ||| 1100 ||| 1101 ||| 
2021 ||| determining subjective bias in text through linguistically informed transformer based multi-task network. ||| 1102 ||| 1103 ||| 
2021 ||| bert-qpp: contextualized pre-trained transformers for query performance prediction. ||| 1104 ||| 1105 ||| 1106 ||| 
2019 ||| query-based interactive recommendation by meta-path and adapted attention-gru. ||| 1107 ||| 1108 ||| 1109 ||| 1110 ||| 1111 ||| 1112 ||| 1113 ||| 1114 ||| 1115 ||| 
2021 ||| agcnt: adaptive graph convolutional network for transformer-based long sequence time-series forecasting. ||| 1116 ||| 1117 ||| 1118 ||| 
2018 ||| aqupr: attention based query passage retrieval. ||| 1119 ||| 1120 ||| 1121 ||| 1122 ||| 
2020 ||| spatial-temporal convolutional graph attention networks for citywide traffic flow forecasting. ||| 1123 ||| 1124 ||| 1125 ||| 1126 ||| 
2020 ||| time-aware graph relational attention network for stock recommendation. ||| 1127 ||| 1128 ||| 1129 ||| 1130 ||| 885 ||| 
2017 ||| interacting attention-gated recurrent networks for recommendation. ||| 1131 ||| 1132 ||| 1133 ||| 1134 ||| 1135 ||| 1136 ||| 
2020 ||| do people and neural nets pay attention to the same words: studying eye-tracking data for non-factoid qa evaluation. ||| 1137 ||| 1138 ||| 1139 ||| 1140 ||| 1141 ||| 1142 ||| 
2020 ||| deep multifaceted transformers for multi-objective ranking in large-scale e-commerce recommender systems. ||| 1143 ||| 1144 ||| 1145 ||| 1146 ||| 1147 ||| 1148 ||| 
2021 ||| multi-factors aware dual-attentional knowledge tracing. ||| 1149 ||| 1150 ||| 1151 ||| 1152 ||| 1153 ||| 1154 ||| 
2021 ||| you are what and where you are: graph enhanced attention network for explainable poi recommendation. ||| 1155 ||| 1156 ||| 1157 ||| 1158 ||| 1159 ||| 1160 ||| 
2019 ||| dsanet: dual self-attention network for multivariate time series forecasting. ||| 1161 ||| 1162 ||| 1163 ||| 1164 ||| 
2019 ||| regularized adversarial sampling and deep time-aware attention for click-through rate prediction. ||| 1165 ||| 1166 ||| 1167 ||| 1168 ||| 1099 ||| 1169 ||| 1170 ||| 1171 ||| 
2020 ||| gaeat: graph auto-encoder attention networks for knowledge graph completion. ||| 1172 ||| 1173 ||| 844 ||| 1174 ||| 1175 ||| 
2018 ||| a sequential neural information diffusion model with structure attention. ||| 1176 ||| 1177 ||| 1178 ||| 
2019 ||| recommender system using sequential and global preference via attention mechanism and topic modeling. ||| 1179 ||| 1180 ||| 1181 ||| 1182 ||| 1183 ||| 
2018 ||| hram: a hybrid recurrent attention machine for news recommendation. ||| 1184 ||| 961 ||| 1185 ||| 1186 ||| 
2021 ||| dcap: deep cross attentional product network for user response prediction. ||| 1187 ||| 1188 ||| 1189 ||| 1190 ||| 1191 ||| 1192 ||| 
2019 ||| graph convolutional networks with motif-based attention. ||| 1193 ||| 1194 ||| 1195 ||| 1196 ||| 1197 ||| 1198 ||| 
2021 ||| a knowledge-aware recommender with attention-enhanced dynamic convolutional network. ||| 1199 ||| 1200 ||| 1201 ||| 1202 ||| 1203 ||| 
2021 ||| block access pattern discovery via compressed full tensor transformer. ||| 1204 ||| 1205 ||| 1206 ||| 1207 ||| 1208 ||| 1209 ||| 1210 ||| 1211 ||| 1212 ||| 
2021 ||| understanding the property of long term memory for the lstm with attention mechanism. ||| 1213 ||| 1214 ||| 1215 ||| 1216 ||| 
2018 ||| learning multi-touch conversion attribution with dual-attention mechanisms for online advertising. ||| 1217 ||| 1218 ||| 1219 ||| 1220 ||| 1221 ||| 1222 ||| 1223 ||| 1224 ||| 
2021 ||| how to leverage a multi-layered transformer language model for text clustering: an ensemble approach. ||| 1225 ||| 1226 ||| 1227 ||| 1228 ||| 
2021 ||| mixed attention transformer for leveraging word-level knowledge to neural cross-lingual information retrieval. ||| 1229 ||| 1230 ||| 1231 ||| 1232 ||| 1233 ||| 
2019 ||| bert4rec: sequential recommendation with bidirectional encoder representations from transformer. ||| 1234 ||| 1235 ||| 1236 ||| 1237 ||| 1238 ||| 1111 ||| 1239 ||| 
2019 ||| cross-domain attention network with wasserstein regularizers for e-commerce search. ||| 1240 ||| 1241 ||| 1242 ||| 1243 ||| 1244 ||| 1115 ||| 1245 ||| 1246 ||| 
2017 ||| an attention-based collaboration framework for multi-view network representation learning. ||| 1247 ||| 1248 ||| 1249 ||| 1250 ||| 1251 ||| 1252 ||| 
2018 ||| attention-based adaptive model to unify warm and cold starts recommendation. ||| 1253 ||| 1254 ||| 1255 ||| 1256 ||| 
2020 ||| cola-gnn: cross-location attention based graph neural networks for long-term ili prediction. ||| 1257 ||| 1258 ||| 359 ||| 1259 ||| 357 ||| 
2021 ||| spectral graph attention network with fast eigen-approximation. ||| 1260 ||| 1261 ||| 1262 ||| 1263 ||| 1264 ||| 1265 ||| 1088 ||| 
2020 ||| disenhan: disentangled heterogeneous graph attention network for recommendation. ||| 1266 ||| 1267 ||| 1268 ||| 1269 ||| 1270 ||| 1251 ||| 
2019 ||| ehr coding with multi-scale feature attention and structured knowledge graph propagation. ||| 1271 ||| 1090 ||| 1094 ||| 1093 ||| 
2019 ||| hican: hierarchical convolutional attention network for sequence modeling. ||| 1272 ||| 1273 ||| 1274 ||| 1275 ||| 
2019 ||| a zero attention model for personalized product search. ||| 1276 ||| 1277 ||| 1278 ||| 1140 ||| 
2020 ||| logic enhanced commonsense inference with chain transformer. ||| 1279 ||| 1280 ||| 1281 ||| 1282 ||| 
2019 ||| attributed multi-relational attention network for fact-checking url recommendation. ||| 1283 ||| 1284 ||| 1285 ||| 1073 ||| 
2018 ||| hierarchical complementary attention network for predicting stock price movements with news. ||| 1286 ||| 1287 ||| 1288 ||| 1289 ||| 
2021 ||| tabular data concept type detection using star-transformers. ||| 1290 ||| 1291 ||| 1292 ||| 
2021 ||| dynstgat: dynamic spatial-temporal graph attention network for traffic signal control. ||| 213 ||| 214 ||| 216 ||| 378 ||| 
2019 ||| towards explainable representation of time-evolving graphs via spatial-temporal graph attention networks. ||| 1293 ||| 1294 ||| 1295 ||| 
2018 ||| kame: knowledge-based attention model for diagnosis prediction in healthcare. ||| 1071 ||| 1296 ||| 1297 ||| 1298 ||| 1299 ||| 930 ||| 
2018 ||| ready for use: subject-independent movement intention recognition via a convolutional attention model. ||| 773 ||| 771 ||| 770 ||| 1300 ||| 
2019 ||| hierarchical multi-label text classification: an attention-based recurrent network approach. ||| 471 ||| 1301 ||| 1302 ||| 1303 ||| 1304 ||| 1305 ||| 1306 ||| 1307 ||| 1308 ||| 
2021 ||| span-level emotion cause analysis by bert-based graph attention network. ||| 1309 ||| 1310 ||| 1311 ||| 1312 ||| 1313 ||| 
2021 ||| poshan: cardinal pos pattern guided attention for news headline incongruence. ||| 1314 ||| 1315 ||| 
2018 ||| rumor detection with hierarchical social attention network. ||| 1316 ||| 1317 ||| 1318 ||| 1319 ||| 1320 ||| 
2020 ||| qsan: a quantum-probability based signed attention network for explainable false information detection. ||| 1321 ||| 1322 ||| 1323 ||| 1324 ||| 1325 ||| 1326 ||| 
2021 ||| assorted attention network for cross-lingual language-to-vision retrieval. ||| 1327 ||| 208 ||| 1328 ||| 1329 ||| 1330 ||| 977 ||| 
2019 ||| how does bert answer questions?: a layer-wise analysis of transformer representations. ||| 1331 ||| 1332 ||| 1333 ||| 1334 ||| 1335 ||| 
2019 ||| collective link prediction oriented network embedding with hierarchical graph attention. ||| 1336 ||| 1090 ||| 538 ||| 1093 ||| 
2017 ||| aspect-level sentiment classification with heat (hierarchical attention) network. ||| 1337 ||| 1338 ||| 1339 ||| 598 ||| 1340 ||| 1341 ||| 
2020 ||| stp-udgat: spatial-temporal-preference user dimensional graph attention network for next poi recommendation. ||| 1342 ||| 1343 ||| 1344 ||| 1345 ||| 1346 ||| 1347 ||| 1348 ||| 
2018 ||| multiresolution graph attention networks for relevance matching. ||| 1349 ||| 1350 ||| 1351 ||| 1352 ||| 1353 ||| 
2021 ||| attention based subgraph classification for link prediction by network re-weighting. ||| 1354 ||| 1355 ||| 1356 ||| 1357 ||| 1358 ||| 1359 ||| 
2020 ||| agatha: automatic graph mining and transformer based hypothesis generation approach. ||| 1360 ||| 1361 ||| 1362 ||| 1363 ||| 
2021 ||| grad-sam: explaining transformers via gradient self-attention maps. ||| 1364 ||| 1365 ||| 1366 ||| 1367 ||| 1368 ||| 1369 ||| 1370 ||| 
2021 ||| neural information diffusion prediction with topic-aware attention network. ||| 1371 ||| 1372 ||| 1373 ||| 
2018 ||| personalizing search results using hierarchical rnn with query-aware attention. ||| 1374 ||| 1375 ||| 1376 ||| 1377 ||| 1378 ||| 
2019 ||| neighborhood interaction attention network for link prediction. ||| 1176 ||| 1379 ||| 1178 ||| 
2017 ||| a compare-aggregate model with dynamic-clip attention for answer selection. ||| 1380 ||| 449 ||| 1381 ||| 1382 ||| 1383 ||| 
2021 ||| multivariate and propagation graph attention network for spatial-temporal prediction with outdoor cellular traffic. ||| 1384 ||| 1385 ||| 1386 ||| 1387 ||| 
2017 ||| hierarchical rnn with static sentence-level attention for text-based speaker change detection. ||| 1388 ||| 1389 ||| 1390 ||| 
2021 ||| gdfm: gene vectors embodied deep attentional factorization machines for interaction prediction. ||| 1391 ||| 1392 ||| 1393 ||| 1394 ||| 1395 ||| 1203 ||| 
2017 ||| neupl: attention-based semantic matching and pair-linking for entity disambiguation. ||| 1396 ||| 1397 ||| 1398 ||| 1399 ||| 1400 ||| 
2021 ||| spatiotemporal swin-transformer network for short time weather forecasting. ||| 1401 ||| 1402 ||| 1403 ||| 
2017 ||| a temporal attentional model for rumor stance classification. ||| 1404 ||| 1405 ||| 1406 ||| 1407 ||| 
2020 ||| st-grat: a novel spatio-temporal graph attention networks for accurately forecasting dynamically changing road speed. ||| 1408 ||| 1409 ||| 1410 ||| 1411 ||| 1412 ||| 1413 ||| 1414 ||| 1183 ||| 
2020 ||| dual head-wise coattention network for machine comprehension with multiple-choice questions. ||| 1415 ||| 1416 ||| 1417 ||| 1415 ||| 1418 ||| 
2021 ||| improving irregularly sampled time series learning with time-aware dual-attention memory-augmented networks. ||| 1419 ||| 1420 ||| 1421 ||| 1422 ||| 885 ||| 1423 ||| 1424 ||| 1425 ||| 1426 ||| 
2021 ||| continuous-time sequential recommendation with temporal graph collaborative transformer. ||| 1427 ||| 1428 ||| 538 ||| 1090 ||| 1429 ||| 1094 ||| 
2018 ||| understanding reading attention distribution during relevance judgement. ||| 1430 ||| 1255 ||| 1431 ||| 1432 ||| 1254 ||| 1256 ||| 
2020 ||| transformer models for recommending related questions in web search. ||| 1433 ||| 1186 ||| 1434 ||| 
2021 ||| age inference using a hierarchical attention neural network. ||| 1435 ||| 1436 ||| 
2019 ||| interactive variance attention based online spoiler detection for time-sync comments. ||| 1437 ||| 1438 ||| 1439 ||| 1440 ||| 1441 ||| 
2020 ||| diversifying search results using self-attention network. ||| 1442 ||| 1375 ||| 1378 ||| 
2021 ||| match-ignition: plugging pagerank into transformer for long-form text matching. ||| 1443 ||| 1444 ||| 1445 ||| 
2021 ||| extractive-abstractive summarization of judgment documents using multiple attention networks. ||| 1446 ||| 1447 ||| 1448 ||| 1449 ||| 1450 ||| 
2017 ||| looking away and catching up: dealing with brief attentional disconnection in synchronous groupware. ||| 1451 ||| 1452 ||| 1453 ||| 1454 ||| 
2017 ||| coattention based bilstm for answer selection. ||| 241 ||| 240 ||| 
2021 ||| entity relation extraction based on multi-attention mechanism and bigru network. ||| 1455 ||| 1456 ||| 1457 ||| 1458 ||| 
2021 ||| attention - based non-profiled side-channel attack. ||| 1459 ||| 1460 ||| 1461 ||| 
2020 ||| malware classification through attention residual network based visualization. ||| 1462 ||| 1463 ||| 1464 ||| 1465 ||| 1466 ||| 
2019 ||| dual band quadrature vco using switched-transformer coupling for wireless robot applications. ||| 1467 ||| 1468 ||| 1469 ||| 
2020 ||| instrument recognition in transformer substation base on image recognition algorithm. ||| 1470 ||| 1471 ||| 1472 ||| 1473 ||| 
2017 ||| improving rnn with attention and embedding for adverse drug reactions. ||| 1474 ||| 1475 ||| 1476 ||| 1477 ||| 1478 ||| 
2021 ||| self-attention for audio super-resolution. ||| 1479 ||| 1480 ||| 
2021 ||| grad-cam guided channel-spatial attention module for fine-grained visual classification. ||| 1481 ||| 1482 ||| 1483 ||| 1484 ||| 
2019 ||| interpretable online banking fraud detection based on hierarchical attention mechanism. ||| 1485 ||| 1486 ||| 1487 ||| 
2018 ||| supportive attention in end-to-end memory networks. ||| 1488 ||| 1489 ||| 
2020 ||| cognitive-driven convolutional beamforming using eeg-based auditory attention decoding. ||| 1490 ||| 1491 ||| 1492 ||| 1493 ||| 1494 ||| 1495 ||| 
2021 ||| small moving target mot tracking with gm-phd filter and attention-based cnn. ||| 1496 ||| 1497 ||| 1498 ||| 
2020 ||| deep spatio-temporal attention model for grain storage temperature forecasting. ||| 1499 ||| 1500 ||| 1501 ||| 1502 ||| 1503 ||| 
2020 ||| s-gat: accelerating graph attention networks inference on fpga platform with shift operation. ||| 1504 ||| 1505 ||| 1506 ||| 
2022 ||| system of predicting dementia using transformer based ensemble learning. ||| 1507 ||| 1508 ||| 1509 ||| 1510 ||| 
2020 ||| study on mistype correction support using attention in japanese input. ||| 1511 ||| 1508 ||| 1509 ||| 1512 ||| 1510 ||| 
2022 ||| multi-exposure image fusion using cross-attention mechanism. ||| 1513 ||| 1514 ||| 1515 ||| 
2022 ||| trafficformer: a transformer-based traffic predictor. ||| 1516 ||| 1517 ||| 1518 ||| 1519 ||| 1520 ||| 1521 ||| 
2021 ||| logonet: layer-aggregated attention centernet for logo detection. ||| 1522 ||| 1523 ||| 1524 ||| 1525 ||| 1526 ||| 1527 ||| 1528 ||| 
2021 ||| differences in visual attention for hdr and sdr content. ||| 1529 ||| 1530 ||| 1531 ||| 1532 ||| 
2021 ||| robust malware detection using residual attention network. ||| 1533 ||| 1534 ||| 1535 ||| 1536 ||| 1537 ||| 1538 ||| 
2022 ||| highly reliable vehicle detection through cnn with attention mechanism. ||| 1539 ||| 1540 ||| 1541 ||| 1542 ||| 1543 ||| 
2018 ||| transformer-darwin: a miniature humanoid for potential robot companion. ||| 1021 ||| 1022 ||| 
2022 ||| frame attention recurrent back-projection network for accurate video super-resolution. ||| 1544 ||| 1526 ||| 1528 ||| 
2022 ||| fault detection of electric motor coil by yolov3 with spatial attention. ||| 1545 ||| 1526 ||| 1528 ||| 1546 ||| 1547 ||| 
2021 ||| automatic detection and segmentation of liver tumors in multi- phase ct images by phase attention mask r-cnn. ||| 1548 ||| 1526 ||| 1549 ||| 1550 ||| 1551 ||| 1552 ||| 1528 ||| 
2020 ||| study on automatic defect report classification system with self attention visualization. ||| 1508 ||| 1553 ||| 1510 ||| 
2019 ||| multi-headed self-attention-based hierarchical model for extractive summarization. ||| 1554 ||| 1555 ||| 
2020 ||| a co-attention model with sequential behaviors and side information for session- based recommendation. ||| 1556 ||| 1557 ||| 1558 ||| 1559 ||| 
2021 ||| sieve: attention-based sampling of end-to-end trace data in distributed microservice systems. ||| 1560 ||| 174 ||| 1561 ||| 1562 ||| 1563 ||| 
2019 ||| outcome-oriented predictive process monitoring with attention-based bidirectional lstm neural networks. ||| 1564 ||| 1565 ||| 1566 ||| 1567 ||| 
2020 ||| nafm: neural and attentional factorization machine for web api recommendation. ||| 1568 ||| 162 ||| 1569 ||| 1570 ||| 
2021 ||| combining label-wise attention and adversarial training for tag prediction of web services. ||| 1571 ||| 1572 ||| 1573 ||| 1574 ||| 1575 ||| 
2021 ||| mattrip: multi-functional attention-based neural network for semantic travel route recommendation. ||| 1576 ||| 1577 ||| 1578 ||| 1579 ||| 
2021 ||| service recommendation for composition creation based on collaborative attention convolutional network. ||| 1580 ||| 1581 ||| 720 ||| 1582 ||| 1583 ||| 
2021 ||| sequence and distance aware transformer for recommendation systems. ||| 1584 ||| 1585 ||| 1586 ||| 1587 ||| 1588 ||| 
2020 ||| an attention-based neural model for popularity prediction in social service. ||| 1589 ||| 1590 ||| 1578 ||| 1579 ||| 
2021 ||| heterogeneous graph attention network-enhanced web service classification. ||| 1591 ||| 1569 ||| 927 ||| 1568 ||| 162 ||| 1592 ||| 
2020 ||| adaptation of multilingual transformer encoder for robust enhanced universal dependency parsing. ||| 1593 ||| 375 ||| 
2018 ||| director's cut: a combined dataset for visual attention analysis in cinematic vr content. ||| 1594 ||| 1595 ||| 1596 ||| 1597 ||| 
2017 ||| work-in-progress: analysis of meditation and attention level of human brain. ||| 1598 ||| 1599 ||| 1600 ||| 1601 ||| 
2020 ||| instant and accurate instance segmentation equipped with path aggregation and attention gate. ||| 1602 ||| 1603 ||| 
2021 ||| a 2-ghz reconfigurable transmitter using a class-d pa and a multi-tapped transformer. ||| 1604 ||| 1605 ||| 1606 ||| 1607 ||| 1608 ||| 
2018 ||| human visual attention analysis-based image segmentation using color histogram. ||| 1609 ||| 1610 ||| 
2021 ||| layer-wise pruning of transformer attention heads for efficient language modeling. ||| 1611 ||| 1612 ||| 1613 ||| 1614 ||| 
2021 ||| graph attention network with dependency parsing for aspect-level sentiment classification. ||| 1615 ||| 1616 ||| 
2021 ||| chinese text similarity calculation model based on multi-attention siamese bi-lstm. ||| 1617 ||| 1618 ||| 
2018 ||| digitalization of regulation unit selection process in power transformer design. ||| 1619 ||| 1620 ||| 
2020 ||| semi-analytical estimation of on-chip intertwined rectangular transformer parameters in 180 nm cmos technology. ||| 1621 ||| 1622 ||| 1623 ||| 1624 ||| 
2017 ||| fem analysis and design of a voltage instrument transformer for digital sampling wattmeter. ||| 1625 ||| 1626 ||| 1627 ||| 
2017 ||| a multi-task framework for monitoring health conditions via attention-based recurrent neural networks. ||| 1628 ||| 1071 ||| 1629 ||| 930 ||| 1630 ||| 1631 ||| 1632 ||| 
2018 ||| hierarchical attention-based prediction model for discovering the persistence of chronic opioid therapy from a large clinical dataset. ||| 1633 ||| 1634 ||| 1635 ||| 1636 ||| 1637 ||| 
2020 ||| patient cohort retrieval using transformer language models. ||| 1638 ||| 1639 ||| 
2020 ||| interpretable automatic adverse event detection with hierarchical attention networks. ||| 1640 ||| 1641 ||| 1642 ||| 1643 ||| 1644 ||| 1645 ||| 1646 ||| 
2020 ||| learning hierarchical transformer-based representations of clinical notes. ||| 1647 ||| 1648 ||| 1649 ||| 1650 ||| 
2020 ||| extracting angina symptoms from clinical notes using pre-trained transformer architectures. ||| 1651 ||| 1652 ||| 1653 ||| 1654 ||| 1655 ||| 1656 ||| 1657 ||| 
2020 ||| automated diagnosis coding from clinical notes using attention-augmented recurrent convolutional neural networks. ||| 1658 ||| 1659 ||| 1660 ||| 
2020 ||| face completion with pyramid semantic attention and latent codes. ||| 1661 ||| 1662 ||| 
2021 ||| anticipative video transformer. ||| 1663 ||| 1664 ||| 
2021 ||| generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. ||| 1665 ||| 1666 ||| 1667 ||| 
2021 ||| a latent transformer for disentangled face editing in images and videos. ||| 1668 ||| 1669 ||| 1670 ||| 1671 ||| 
2021 |||  image visual attention modelling with unsupervised learning. ||| 1672 ||| 1673 ||| 1674 ||| 1675 ||| 
2019 ||| segeqa: video segmentation based visual attention for embodied question answering. ||| 1676 ||| 1677 ||| 1678 ||| 1679 ||| 1680 ||| 498 ||| 
2021 ||| voxel transformer for 3d object detection. ||| 1681 ||| 1682 ||| 1683 ||| 1684 ||| 1685 ||| 1686 ||| 1687 ||| 1688 ||| 
2021 ||| high-resolution optical flow from 1d attention and correlation. ||| 1689 ||| 1690 ||| 1691 ||| 1692 ||| 1693 ||| 
2021 ||| stvgbert: a visual-linguistic transformer based framework for spatio-temporal video grounding. ||| 1694 ||| 1695 ||| 1696 ||| 
2021 ||| learning spatio-temporal transformer for visual tracking. ||| 1697 ||| 1698 ||| 1699 ||| 952 ||| 1700 ||| 
2021 ||| transreid: transformer-based object re-identification. ||| 1701 ||| 1702 ||| 1703 ||| 1704 ||| 1705 ||| 1706 ||| 
2019 ||| second-order non-local attention networks for person re-identification. ||| 1707 ||| 1708 ||| 1709 ||| 1710 ||| 
2021 ||| localtrans: a multiscale local transformer network for cross-resolution homography estimation. ||| 1711 ||| 1712 ||| 1713 ||| 1714 ||| 1715 ||| 1716 ||| 
2019 ||| group-wise deep object co-segmentation with co-attention recurrent neural network. ||| 1717 ||| 1718 ||| 1719 ||| 1720 ||| 1721 ||| 
2021 ||| tokens-to-token vit: training vision transformers from scratch on imagenet. ||| 1722 ||| 1723 ||| 128 ||| 1724 ||| 1725 ||| 1726 ||| 1727 ||| 1685 ||| 1728 ||| 
2021 ||| dynamic high-pass filtering and multi-spectral attention for image super-resolution. ||| 1729 ||| 1730 ||| 1731 ||| 1732 ||| 1733 ||| 1734 ||| 1735 ||| 
2019 ||| deep floor plan recognition using a multi-task network with room-boundary-guided attention. ||| 1736 ||| 1737 ||| 1738 ||| 1739 ||| 
2021 ||| an empirical study of training self-supervised vision transformers. ||| 1740 ||| 1741 ||| 1742 ||| 
2019 ||| sharpen focus: learning with attention separability and consistency. ||| 1743 ||| 1744 ||| 1745 ||| 1746 ||| 1747 ||| 1748 ||| 1749 ||| 
2017 ||| video fill in the blank using lr/rl lstms with spatial-temporal attentions. ||| 1750 ||| 1751 ||| 1752 ||| 
2017 ||| multi-modal factorized bilinear pooling with co-attention learning for visual question answering. ||| 1753 ||| 1754 ||| 1755 ||| 1756 ||| 
2021 ||| paint transformer: feed forward neural painting with stroke prediction. ||| 1757 ||| 1758 ||| 1759 ||| 136 ||| 1760 ||| 633 ||| 1761 ||| 1371 ||| 
2021 ||| the benefit of distraction: denoising camera-based physiological measurements using inverse attention. ||| 1762 ||| 1763 ||| 1764 ||| 
2021 ||| swin transformer: hierarchical vision transformer using shifted windows. ||| 1765 ||| 1766 ||| 1767 ||| 1768 ||| 1769 ||| 1770 ||| 1771 ||| 1772 ||| 
2021 ||| rethinking and improving relative position encoding for vision transformer. ||| 1773 ||| 1698 ||| 1774 ||| 1699 ||| 1775 ||| 
2021 ||| bossnas: exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. ||| 1776 ||| 1777 ||| 1778 ||| 1779 ||| 1780 ||| 1686 ||| 1781 ||| 
2019 ||| griddehazenet: attention-based multi-scale network for image dehazing. ||| 1782 ||| 1783 ||| 1784 ||| 1785 ||| 
2021 ||| topic scene graph generation by attention distillation from caption. ||| 1786 ||| 1787 ||| 1788 ||| 
2021 ||| thundr: transformer-based 3d human reconstruction with markers. ||| 1789 ||| 1790 ||| 1791 ||| 1792 ||| 1793 ||| 1794 ||| 
2021 ||| attentional pyramid pooling of salient visual residuals for place recognition. ||| 1795 ||| 1796 ||| 1797 ||| 1798 ||| 
2019 ||| dynamic graph attention for referring expression comprehension. ||| 1799 ||| 1800 ||| 1801 ||| 
2021 ||| geometry-free view synthesis: transformers and no 3d priors. ||| 1802 ||| 1803 ||| 648 ||| 1804 ||| 
2021 ||| learning deep local features with multiple dynamic attentions for large-scale image retrieval. ||| 1805 ||| 214 ||| 1806 ||| 1807 ||| 
2019 ||| contextual attention for hand detection in the wild. ||| 1808 ||| 1809 ||| 602 ||| 1810 ||| 1811 ||| 
2021 ||| co-scale conv-attentional image transformers. ||| 1812 ||| 1813 ||| 1814 ||| 1815 ||| 
2021 ||| fashionmirror: co-attention feature-remapping virtual try-on with sequential template poses. ||| 1816 ||| 1817 ||| 1818 ||| 1819 ||| 1820 ||| 
2021 ||| pyramid point cloud transformer for large-scale place recognition. ||| 1821 ||| 1822 ||| 1823 ||| 1824 ||| 1825 ||| 
2019 ||| pyramid graph networks with connection attentions for region-based one-shot semantic segmentation. ||| 1460 ||| 1677 ||| 1679 ||| 1826 ||| 1827 ||| 1828 ||| 
2019 ||| attention-aware polarity sensitive embedding for affective image retrieval. ||| 1829 ||| 1830 ||| 1831 ||| 1832 ||| 1833 ||| 1834 ||| 
2017 ||| structured attentions for visual question answering. ||| 1835 ||| 1836 ||| 1837 ||| 1838 ||| 1839 ||| 
2021 ||| fuseformer: fusing fine-grained information in transformers for video inpainting. ||| 1840 ||| 1841 ||| 1842 ||| 1843 ||| 1844 ||| 1845 ||| 1846 ||| 1847 ||| 1848 ||| 
2017 ||| segmentation-aware convolutional networks using local attention masks. ||| 1849 ||| 1850 ||| 1851 ||| 
2017 ||| paying attention to descriptions generated by image captioning models. ||| 1852 ||| 1853 ||| 1854 ||| 1855 ||| 
2021 ||| uncertainty-guided transformer reasoning for camouflaged object detection. ||| 1856 ||| 1857 ||| 633 ||| 1858 ||| 1859 ||| 1860 ||| 1861 ||| 
2021 ||| residual attention: a simple but effective method for multi-label recognition. ||| 1862 ||| 1863 ||| 
2019 ||| sid4vam: a benchmark dataset with synthetic images for visual attention modeling. ||| 1864 ||| 1865 ||| 1866 ||| 1867 ||| 1868 ||| 1869 ||| 1865 ||| 1870 ||| 
2017 ||| learning visual attention to identify people with autism spectrum disorder. ||| 1871 ||| 1872 ||| 
2021 ||| scouter: slot attention-based classifier for explainable image recognition. ||| 1873 ||| 1874 ||| 1875 ||| 1876 ||| 1877 ||| 1878 ||| 
2021 ||| unit: multimodal multitask learning with a unified transformer. ||| 1879 ||| 1880 ||| 
2021 ||| group-free 3d object detection via transformers. ||| 1765 ||| 1770 ||| 1767 ||| 1768 ||| 1693 ||| 
2021 ||| vision transformers for dense prediction. ||| 1881 ||| 1882 ||| 1883 ||| 1884 ||| 
2021 ||| levit: a vision transformer in convnet's clothing for faster inference. ||| 1885 ||| 1886 ||| 1887 ||| 1888 ||| 1889 ||| 1890 ||| 1891 ||| 1892 ||| 1893 ||| 
2021 ||| oadtr: online action detection with transformers. ||| 1894 ||| 1895 ||| 1896 ||| 1897 ||| 1898 ||| 1899 ||| 1900 ||| 
2019 ||| integral object mining via online attention accumulation. ||| 1901 ||| 1902 ||| 1903 ||| 1904 ||| 1905 ||| 1906 ||| 
2021 ||| adaattn: revisit attention mechanism in arbitrary neural style transfer. ||| 1757 ||| 1758 ||| 1759 ||| 136 ||| 1907 ||| 633 ||| 1718 ||| 1719 ||| 1761 ||| 
2019 ||| co-segmentation inspired attention networks for video-based person re-identification. ||| 1908 ||| 1909 ||| 1910 ||| 
2021 ||| attention-based multi-reference learning for image super-resolution. ||| 1911 ||| 1912 ||| 1913 ||| 
2017 ||| recursive spatial transformer (rest) for alignment-free face recognition. ||| 1914 ||| 1915 ||| 189 ||| 208 ||| 1916 ||| 1788 ||| 
2019 ||| self-critical attention learning for person re-identification. ||| 1917 ||| 1918 ||| 1919 ||| 1920 ||| 1921 ||| 
2021 ||| pcam: product of cross-attention matrices for rigid registration of point clouds. ||| 1922 ||| 1923 ||| 1924 ||| 1925 ||| 
2019 ||| attentional neural fields for crowd counting. ||| 1926 ||| 1927 ||| 1928 ||| 1929 ||| 1930 ||| 1931 ||| 1932 ||| 
2021 ||| transforensics: image forgery localization with dense self-attention. ||| 1933 ||| 1934 ||| 1935 ||| 1936 ||| 1937 ||| 
2021 ||| groupformer: group activity recognition with clustered spatial-temporal transformer. ||| 1938 ||| 1939 ||| 1940 ||| 1941 ||| 1942 ||| 1943 ||| 1944 ||| 
2021 ||| layouttransformer: layout generation and completion with self-attention. ||| 1945 ||| 1946 ||| 1947 ||| 1948 ||| 1949 ||| 1950 ||| 
2021 ||| cloud transformers: a universal approach to point cloud processing tasks. ||| 1951 ||| 1952 ||| 
2021 ||| multi-scale vision longformer: a new vision transformer for high-resolution image encoding. ||| 1953 ||| 1954 ||| 1955 ||| 1956 ||| 1957 ||| 241 ||| 1958 ||| 
2021 ||| dynamic detr: end-to-end object detection with dynamic attention. ||| 1954 ||| 1959 ||| 1955 ||| 1953 ||| 1957 ||| 241 ||| 
2021 ||| stochastic transformer networks with linear competing units: application to end-to-end sl translation. ||| 1960 ||| 1961 ||| 1962 ||| 1749 ||| 1963 ||| 
2021 ||| sotr: segmenting objects with transformers. ||| 1964 ||| 1965 ||| 1966 ||| 1967 ||| 
2021 ||| handwriting transformers. ||| 1968 ||| 1969 ||| 1970 ||| 1971 ||| 1972 ||| 1752 ||| 
2019 ||| entangled transformer for image captioning. ||| 1973 ||| 1974 ||| 1975 ||| 208 ||| 
2021 ||| deep symmetric network for underexposed image enhancement with recurrent attentional learning. ||| 1976 ||| 1977 ||| 1978 ||| 1979 ||| 1980 ||| 
2021 ||| unified questioner transformer for descriptive question generation in goal-oriented visual dialogue. ||| 1981 ||| 1982 ||| 1983 ||| 1984 ||| 726 ||| 1985 ||| 
2021 ||| revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. ||| 1986 ||| 1987 ||| 1988 ||| 1989 ||| 1990 ||| 1991 ||| 1992 ||| 
2019 ||| controllable attention for structured layered video decomposition. ||| 1993 ||| 1994 ||| 1995 ||| 1996 ||| 1997 ||| 
2021 ||| crackformer: transformer network for fine-grained crack detection. ||| 1998 ||| 1999 ||| 2000 ||| 2001 ||| 2002 ||| 
2021 ||| instance-level image retrieval using reranking transformers. ||| 2003 ||| 2004 ||| 2005 ||| 
2021 ||| pyramid vision transformer: a versatile backbone for dense prediction without convolutions. ||| 2006 ||| 2007 ||| 2008 ||| 1861 ||| 2009 ||| 2010 ||| 173 ||| 2011 ||| 1932 ||| 
2019 ||| ranet: ranking attention network for fast video object segmentation. ||| 2012 ||| 2013 ||| 2014 ||| 1929 ||| 1932 ||| 
2021 ||| variational attention: propagating domain-specific knowledge for multi-domain learning in crowd counting. ||| 2015 ||| 2016 ||| 1424 ||| 2017 ||| 890 ||| 2018 ||| 241 ||| 
2021 ||| t-automl: automated machine learning for lesion segmentation using transformers in 3d medical imaging. ||| 2019 ||| 2020 ||| 2021 ||| 2022 ||| 2023 ||| 2024 ||| 
2021 ||| multiscale vision transformers. ||| 2025 ||| 2026 ||| 2027 ||| 2028 ||| 2029 ||| 2030 ||| 2031 ||| 
2019 ||| 3d scene reconstruction with multi-layer depth and epipolar transformers. ||| 2032 ||| 2033 ||| 2034 ||| 2035 ||| 
2021 ||| learning motion-appearance co-attention for zero-shot video object segmentation. ||| 2036 ||| 2037 ||| 2038 ||| 1700 ||| 2039 ||| 2040 ||| 
2017 ||| interpretable learning for self-driving cars by visualizing causal attention. ||| 2041 ||| 2042 ||| 
2019 ||| relation-aware graph attention network for visual question answering. ||| 2043 ||| 2044 ||| 2045 ||| 2046 ||| 
2021 ||| diagonal attention and style-based gan for content-style disentanglement in image generation and translation. ||| 2047 ||| 2048 ||| 
2021 ||| star: a structure-aware lightweight transformer for real-time image enhancement. ||| 2049 ||| 2050 ||| 2051 ||| 1846 ||| 2011 ||| 2052 ||| 
2021 ||| context reasoning attention network for image super-resolution. ||| 1730 ||| 1731 ||| 2053 ||| 2054 ||| 1735 ||| 1734 ||| 
2021 ||| an end-to-end transformer model for 3d object detection. ||| 2055 ||| 1663 ||| 1889 ||| 
2021 ||| scalable vision transformers with hierarchical pooling. ||| 2056 ||| 2057 ||| 2058 ||| 2059 ||| 1691 ||| 
2019 ||| attpool: towards hierarchical feature representation in graph convolutional networks via attention mechanism. ||| 2060 ||| 2061 ||| 2062 ||| 2063 ||| 2064 ||| 
2019 ||| depth-induced multi-scale recurrent attention network for saliency detection. ||| 2065 ||| 2066 ||| 2067 ||| 2068 ||| 1700 ||| 
2021 ||| visual relationship detection using part-and-sum transformers with composite queries. ||| 2069 ||| 1815 ||| 2070 ||| 2071 ||| 1949 ||| 2072 ||| 
2021 ||| high-performance discriminative tracking with transformers. ||| 2073 ||| 2074 ||| 2075 ||| 2076 ||| 2077 ||| 2078 ||| 2079 ||| 2080 ||| 
2021 ||| planetr: structure-guided transformers for 3d plane recovery. ||| 2081 ||| 2082 ||| 2083 ||| 2084 ||| 2085 ||| 
2021 ||| rethinking spatial dimensions of vision transformers. ||| 2086 ||| 2087 ||| 2088 ||| 2089 ||| 2090 ||| 2091 ||| 
2021 ||| episodic transformer for vision-and-language navigation. ||| 2092 ||| 2093 ||| 2094 ||| 
2021 ||| transpose: keypoint localization via transformer. ||| 2095 ||| 2096 ||| 2097 ||| 2098 ||| 
2021 ||| action-conditioned 3d human motion synthesis with transformer vae. ||| 2099 ||| 2100 ||| 2101 ||| 2102 ||| 
2021 ||| parts: unsupervised segmentation with slots, attention and independence maximization. ||| 2103 ||| 2104 ||| 2105 ||| 2106 ||| 
2021 ||| medirl: predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning. ||| 2107 ||| 2108 ||| 2109 ||| 2110 ||| 2005 ||| 2111 ||| 
2019 ||| motion guided attention for video salient object detection. ||| 2112 ||| 2113 ||| 1800 ||| 1801 ||| 
2021 ||| pointr: diverse point cloud completion with geometry-aware transformers. ||| 2114 ||| 2115 ||| 2116 ||| 2117 ||| 1920 ||| 1921 ||| 
2021 ||| going deeper with image transformers. ||| 1887 ||| 2118 ||| 2119 ||| 2120 ||| 1890 ||| 1891 ||| 1892 ||| 
2021 ||| pare: part attention regressor for 3d human body estimation. ||| 2121 ||| 2122 ||| 2123 ||| 2100 ||| 
2021 ||| neat: neural attention fields for end-to-end autonomous driving. ||| 2124 ||| 2125 ||| 2126 ||| 
2019 ||| bilinear attention networks for person retrieval. ||| 2127 ||| 2128 ||| 2129 ||| 2130 ||| 2131 ||| 
2021 ||| the right to talk: an audio-visual transformer approach. ||| 2132 ||| 2133 ||| 2134 ||| 2135 ||| 2136 ||| 2137 ||| 2138 ||| 
2019 ||| align, attend and locate: chest x-ray diagnosis via contrast induced attention network with limited supervision. ||| 2139 ||| 2140 ||| 2141 ||| 1251 ||| 2142 ||| 1801 ||| 
2021 ||| hit: hierarchical transformer with momentum contrast for video-text retrieval. ||| 2143 ||| 2025 ||| 1174 ||| 2144 ||| 2145 ||| 2146 ||| 
2017 ||| rpan: an end-to-end recurrent pose-attention network for action recognition in videos. ||| 2147 ||| 2148 ||| 2149 ||| 
2019 ||| fingerspelling recognition in the wild with iterative visual attention. ||| 2150 ||| 2151 ||| 2152 ||| 2153 ||| 2154 ||| 2155 ||| 
2021 ||| vision transformer with progressive sampling. ||| 2156 ||| 2157 ||| 2158 ||| 2159 ||| 2160 ||| 2161 ||| 2162 ||| 
2021 ||| autoformer: searching transformers for visual recognition. ||| 1774 ||| 1698 ||| 1699 ||| 2163 ||| 
2017 ||| learning multi-attention convolutional neural network for fine-grained image recognition. ||| 2164 ||| 1699 ||| 2165 ||| 2166 ||| 
2021 ||| on the robustness of vision transformers to adversarial examples. ||| 2167 ||| 2168 ||| 2169 ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| 2170 ||| 2171 ||| 1846 ||| 1847 ||| 1848 ||| 
2021 ||| segmenter: transformer for semantic segmentation. ||| 2172 ||| 2173 ||| 2174 ||| 2093 ||| 
2019 ||| attentional feature-pair relation networks for accurate face recognition. ||| 2175 ||| 2176 ||| 2177 ||| 2178 ||| 
2021 ||| glimpse-attend-and-explore: self-attention for active visual exploration. ||| 2179 ||| 2180 ||| 2181 ||| 
2021 ||| counterfactual attention learning for fine-grained visual categorization and re-identification. ||| 2115 ||| 1917 ||| 1920 ||| 1921 ||| 
2021 ||| multi-view 3d reconstruction with transformers. ||| 2182 ||| 2183 ||| 2184 ||| 2185 ||| 2186 ||| 2187 ||| 2188 ||| 2189 ||| 
2017 ||| vqs: linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. ||| 2190 ||| 2191 ||| 2192 ||| 2094 ||| 2193 ||| 
2021 ||| simpler is better: few-shot semantic segmentation with classifier weight transformer. ||| 2194 ||| 2195 ||| 2196 ||| 254 ||| 2197 ||| 2198 ||| 
2019 ||| attention on attention for image captioning. ||| 2199 ||| 2200 ||| 1037 ||| 2201 ||| 
2019 ||| agss-vos: attention guided single-shot video object segmentation. ||| 2202 ||| 2203 ||| 2204 ||| 
2021 ||| event-based video reconstruction using transformer. ||| 2205 ||| 2206 ||| 2207 ||| 
2019 ||| a dual-path model with adaptive attention for vehicle re-identification. ||| 2208 ||| 2209 ||| 2210 ||| 2211 ||| 2212 ||| 2213 ||| 
2021 ||| relaxed transformer decoders for direct action proposal generation. ||| 2214 ||| 2215 ||| 2216 ||| 2217 ||| 
2019 ||| ccnet: criss-cross attention for semantic segmentation. ||| 2218 ||| 2219 ||| 2220 ||| 2221 ||| 1905 ||| 2222 ||| 
2019 ||| what would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention. ||| 2223 ||| 2224 ||| 
2021 ||| 3d human pose estimation with spatial and temporal transformers. ||| 2225 ||| 2226 ||| 2227 ||| 2228 ||| 2229 ||| 2230 ||| 2231 ||| 
2019 ||| attention bridging network for knowledge transfer. ||| 2232 ||| 1730 ||| 2233 ||| 2234 ||| 1734 ||| 
2019 ||| attention-based autism spectrum disorder screening with privileged modality. ||| 2235 ||| 1872 ||| 
2021 ||| neural image compression via attentional multi-scale back projection and frequency decomposition. ||| 2236 ||| 2237 ||| 2238 ||| 2239 ||| 1008 ||| 2240 ||| 2241 ||| 
2019 ||| hierarchical self-attention network for action localization in videos. ||| 2242 ||| 2243 ||| 2244 ||| 
2019 ||| accelerate learning of deep hashing with gradient attention. ||| 2245 ||| 2246 ||| 2247 ||| 
2019 ||| progressive sparse local attention for video object detection. ||| 2248 ||| 2249 ||| 2250 ||| 2251 ||| 2252 ||| 2253 ||| 2254 ||| 2255 ||| 
2021 ||| fcanet: frequency channel attention networks. ||| 2256 ||| 2257 ||| 2258 ||| 2259 ||| 
2021 ||| learning multi-scene absolute pose regression with transformers. ||| 2260 ||| 2261 ||| 2262 ||| 
2021 ||| emerging properties in self-supervised vision transformers. ||| 2263 ||| 1887 ||| 2055 ||| 1890 ||| 1891 ||| 1892 ||| 2264 ||| 2265 ||| 1889 ||| 
2021 ||| vision-language transformer and query generation for referring segmentation. ||| 2266 ||| 748 ||| 2267 ||| 2268 ||| 
2021 ||| rain: reinforced hybrid attention inference network for motion forecasting. ||| 2269 ||| 1856 ||| 2270 ||| 2271 ||| 2272 ||| 2273 ||| 
2017 ||| deep spatial-semantic attention for fine-grained sketch-based image retrieval. ||| 2274 ||| 1695 ||| 2197 ||| 2198 ||| 2275 ||| 
2019 ||| saliency-guided attention network for image-sentence matching. ||| 2276 ||| 2277 ||| 2278 ||| 2279 ||| 
2019 ||| dual attention matching for audio-visual event localization. ||| 2280 ||| 1974 ||| 127 ||| 208 ||| 
2021 ||| dual-camera super-resolution with aligned attention modules. ||| 2281 ||| 2282 ||| 1845 ||| 2283 ||| 2284 ||| 
2021 ||| oscar-net: object-centric scene graph attention for image attribution. ||| 2285 ||| 2286 ||| 2287 ||| 2288 ||| 
2019 ||| learning lightweight lane detection cnns by self attention distillation. ||| 2289 ||| 312 ||| 2290 ||| 2291 ||| 
2021 ||| vivit: a video vision transformer. ||| 2292 ||| 2293 ||| 2294 ||| 2094 ||| 2295 ||| 2093 ||| 
2021 ||| agentformer: agent-aware transformers for socio-temporal multi-agent forecasting. ||| 2296 ||| 2297 ||| 2298 ||| 2299 ||| 
2017 ||| attention-aware deep reinforcement learning for video face recognition. ||| 2115 ||| 1920 ||| 1921 ||| 
2019 ||| mixture-kernel graph attention network for situation recognition. ||| 2300 ||| 2301 ||| 
2017 ||| online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. ||| 2302 ||| 2303 ||| 1848 ||| 1846 ||| 2304 ||| 2305 ||| 
2019 ||| asymmetric cross-guided attention network for actor and action video segmentation from natural language query. ||| 1371 ||| 2306 ||| 2307 ||| 1756 ||| 
2019 ||| mask-guided attention network for occluded pedestrian detection. ||| 2279 ||| 1824 ||| 2308 ||| 1971 ||| 1972 ||| 1932 ||| 
2021 ||| panoptic segmentation of satellite image time series with convolutional temporal attention networks. ||| 2309 ||| 2310 ||| 2311 ||| 
2017 ||| multi-label image recognition by recurrently discovering attentional regions. ||| 2312 ||| 2313 ||| 1800 ||| 2314 ||| 2315 ||| 
2021 ||| musiq: multi-scale image quality transformer. ||| 2316 ||| 2317 ||| 2318 ||| 2319 ||| 2320 ||| 
2021 ||| transfer: learning relation-aware facial expression representations with transformers. ||| 2321 ||| 2322 ||| 2323 ||| 
2021 ||| describing and localizing multiple changes with transformers. ||| 2324 ||| 2325 ||| 2326 ||| 2327 ||| 2328 ||| 2329 ||| 2330 ||| 
2021 ||| 3d human texture estimation from a single image with transformers. ||| 2331 ||| 2291 ||| 
2017 ||| identity-aware textual-visual matching with latent co-attention. ||| 2332 ||| 2333 ||| 1848 ||| 2334 ||| 1846 ||| 
2021 ||| point transformer. ||| 2335 ||| 2336 ||| 2204 ||| 2160 ||| 1884 ||| 
2021 ||| motion guided attention fusion to recognize interactions from videos. ||| 2337 ||| 2338 ||| 2339 ||| 
2017 ||| egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules. ||| 2340 ||| 2341 ||| 2342 ||| 2080 ||| 2343 ||| 
2021 ||| unsupervised point cloud object co-segmentation by co-contrastive learning and mutual attention sampling. ||| 2344 ||| 2345 ||| 2346 ||| 
2021 ||| attention is not enough: mitigating the distribution discrepancy in asynchronous multimodal sequence fusion. ||| 2347 ||| 1677 ||| 2348 ||| 2349 ||| 2350 ||| 
2017 ||| areas of attention for image captioning. ||| 2351 ||| 2352 ||| 2093 ||| 2353 ||| 
2019 ||| discriminative feature learning with consistent attention regularization for person re-identification. ||| 2354 ||| 2355 ||| 2356 ||| 2357 ||| 
2019 ||| attribute attention for semantic disambiguation in zero-shot learning. ||| 1305 ||| 2358 ||| 1115 ||| 2359 ||| 
2021 ||| occlude them all: occlusion-aware attention network for occluded person re-id. ||| 2360 ||| 2361 ||| 2362 ||| 2363 ||| 2364 ||| 2365 ||| 2366 ||| 2367 ||| 
2019 ||| reasoning about human-object interactions through dual attention networks. ||| 2368 ||| 2369 ||| 2370 ||| 2371 ||| 2372 ||| 2373 ||| 
2021 ||| saccadecam: adaptive visual attention for monocular depth sensing. ||| 2374 ||| 2375 ||| 
2021 ||| visio-temporal attention for multi-camera multi-target association. ||| 2376 ||| 2297 ||| 2377 ||| 2299 ||| 
2021 ||| transformer-based dual relation graph for multi-label image recognition. ||| 2378 ||| 2379 ||| 2380 ||| 2381 ||| 2382 ||| 2383 ||| 
2021 ||| glit: neural architecture search for global and local image transformer. ||| 2384 ||| 2385 ||| 2386 ||| 2387 ||| 2388 ||| 2389 ||| 2390 ||| 2391 ||| 2303 ||| 
2021 ||| dance with self-attention: a new look of conditional random fields on anomaly detection in videos. ||| 2392 ||| 2243 ||| 2244 ||| 
2021 ||| self-supervised geometric features discovery via interpretable attention for vehicle re-identification and beyond. ||| 765 ||| 2393 ||| 2394 ||| 
2021 ||| ts-cam: token semantic coupled attention map for weakly supervised object localization. ||| 1310 ||| 2395 ||| 2396 ||| 2397 ||| 2398 ||| 2399 ||| 2373 ||| 2364 ||| 
2019 ||| mixed high-order attention network for person re-identification. ||| 2015 ||| 2400 ||| 2401 ||| 
2021 ||| salient object ranking with position-preserved attention. ||| 2402 ||| 2403 ||| 340 ||| 1774 ||| 2404 ||| 2405 ||| 1115 ||| 2359 ||| 
2021 ||| spatial-temporal transformer for dynamic scene graph generation. ||| 2406 ||| 2407 ||| 2408 ||| 2409 ||| 2410 ||| 
2021 ||| visual saliency transformer. ||| 2411 ||| 2412 ||| 2413 ||| 1932 ||| 2414 ||| 
2021 ||| transvg: end-to-end visual grounding with transformers. ||| 2415 ||| 2416 ||| 2417 ||| 1806 ||| 1807 ||| 
2021 ||| vidtr: video transformer without convolutions. ||| 2418 ||| 2419 ||| 2420 ||| 2421 ||| 2422 ||| 2423 ||| 2424 ||| 2425 ||| 2426 ||| 
2019 ||| relational attention network for crowd counting. ||| 1926 ||| 1928 ||| 2427 ||| 1929 ||| 1930 ||| 1931 ||| 1932 ||| 
2021 ||| cvt: introducing convolutions to vision transformers. ||| 2428 ||| 1956 ||| 2429 ||| 2430 ||| 1954 ||| 1957 ||| 241 ||| 
2021 ||| multimodal co-attention transformer for survival prediction in gigapixel whole slide images. ||| 2431 ||| 2432 ||| 2433 ||| 2434 ||| 2435 ||| 2436 ||| 2437 ||| 2438 ||| 
2021 ||| ensemble attention distillation for privacy-preserving federated learning. ||| 2439 ||| 2440 ||| 1745 ||| 1744 ||| 2441 ||| 2442 ||| 2443 ||| 
2017 ||| deep cropping via attention box prediction and aesthetics assessment. ||| 2444 ||| 2445 ||| 
2017 ||| single shot text detector with regional attention. ||| 2446 ||| 2447 ||| 2448 ||| 2449 ||| 2149 ||| 2450 ||| 
2021 ||| encoder-decoder with multi-level attention for 3d human shape and pose estimation. ||| 2451 ||| 2452 ||| 2453 ||| 2454 ||| 1944 ||| 1848 ||| 
2021 ||| dual contrastive loss and attention for gans. ||| 2455 ||| 2456 ||| 2457 ||| 2458 ||| 2459 ||| 1948 ||| 108 ||| 
2017 ||| faster than real-time facial alignment: a 3d spatial transformer network approach in unconstrained poses. ||| 2460 ||| 2461 ||| 2138 ||| 2462 ||| 
2021 ||| pnp-detr: towards efficient visual analysis with transformers. ||| 128 ||| 1722 ||| 1723 ||| 1685 ||| 1728 ||| 
2019 ||| attention augmented convolutional networks. ||| 2463 ||| 2464 ||| 2465 ||| 2466 ||| 2467 ||| 
2021 ||| ctrl-c: camera calibration transformer with line-classification. ||| 2468 ||| 2469 ||| 2470 ||| 2471 ||| 2472 ||| 2473 ||| 
2019 ||| real image denoising with feature attention. ||| 2474 ||| 2475 ||| 
2021 ||| visformer: the vision-friendly transformer. ||| 2476 ||| 2477 ||| 2478 ||| 2479 ||| 2480 ||| 2398 ||| 
2021 ||| causal attention for unbiased visual recognition. ||| 2481 ||| 2482 ||| 2483 ||| 2484 ||| 
2021 ||| improving 3d object detection with channel-wise transformer. ||| 2485 ||| 2486 ||| 2487 ||| 2488 ||| 2489 ||| 2490 ||| 2491 ||| 
2021 ||| high-fidelity pluralistic image completion with transformers. ||| 2492 ||| 2493 ||| 2494 ||| 2495 ||| 
2021 ||| incorporating convolution designs into visual transformers. ||| 2496 ||| 2497 ||| 2498 ||| 2499 ||| 2500 ||| 293 ||| 
2021 ||| trar: routing the attention spans in transformer for visual question answering. ||| 2501 ||| 2502 ||| 2503 ||| 2504 ||| 2363 ||| 2505 ||| 2365 ||| 2367 ||| 
2019 ||| attentionrnn: a structured spatial attention mechanism. ||| 2506 ||| 2301 ||| 
2017 ||| attention-based multimodal fusion for video description. ||| 2507 ||| 2508 ||| 2509 ||| 2394 ||| 2510 ||| 2511 ||| 2512 ||| 2513 ||| 
2019 ||| expectation-maximization attention networks for semantic segmentation. ||| 2514 ||| 2515 ||| 2516 ||| 2517 ||| 2518 ||| 2519 ||| 
2021 ||| crossvit: cross-attention multi-scale vision transformer for image classification. ||| 2520 ||| 2369 ||| 2521 ||| 
2021 ||| transformer-based attention networks for continuous pixel-wise prediction. ||| 2522 ||| 435 ||| 2523 ||| 437 ||| 2524 ||| 
2021 ||| frequency-aware spatiotemporal transformers for video inpainting detection. ||| 2525 ||| 2526 ||| 2527 ||| 1920 ||| 1921 ||| 
2019 ||| image inpainting with learnable bidirectional attention maps. ||| 2528 ||| 2529 ||| 242 ||| 1904 ||| 2018 ||| 2530 ||| 2531 ||| 1761 ||| 
2019 ||| acfnet: attentional class feature network for semantic segmentation. ||| 2532 ||| 2533 ||| 2534 ||| 2535 ||| 2536 ||| 2537 ||| 2538 ||| 1761 ||| 
2021 ||| temporal-wise attention spiking neural networks for event streams classification. ||| 2539 ||| 2540 ||| 2541 ||| 2542 ||| 2543 ||| 2544 ||| 2545 ||| 
2019 ||| deep contextual attention for human-object interaction detection. ||| 2546 ||| 1971 ||| 2308 ||| 1972 ||| 2279 ||| 1932 ||| 1855 ||| 
2021 ||| wb-detr: transformer-based detector without backbone. ||| 2547 ||| 2548 ||| 2549 ||| 2550 ||| 2551 ||| 2552 ||| 
2021 ||| cotr: correspondence transformer for matching across images. ||| 1706 ||| 2553 ||| 2554 ||| 2555 ||| 2556 ||| 
2021 ||| snowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer. ||| 2557 ||| 2558 ||| 2559 ||| 2560 ||| 2561 ||| 2562 ||| 2563 ||| 
2021 ||| 3dvg-transformer: relation modeling for visual grounding on point clouds. ||| 2564 ||| 2565 ||| 2566 ||| 1696 ||| 
2021 ||| understanding robustness of transformers for image classification. ||| 2567 ||| 2568 ||| 2569 ||| 2570 ||| 2571 ||| 2572 ||| 
2021 ||| the center of attention: center-keypoint grouping via attention for multi-person pose estimation. ||| 2573 ||| 2574 ||| 2575 ||| 
2021 ||| context-aware scene graph generation with seq2seq transformers. ||| 2576 ||| 2577 ||| 2578 ||| 2579 ||| 2580 ||| 2581 ||| 2582 ||| 2583 ||| 
2021 ||| class semantics-based attention for action detection. ||| 2584 ||| 2585 ||| 2586 ||| 2587 ||| 2588 ||| 2589 ||| 
2021 ||| visual transformers: where do transformers really belong in vision models? ||| 2590 ||| 2591 ||| 2592 ||| 2593 ||| 2594 ||| 2029 ||| 2272 ||| 2595 ||| 2596 ||| 2597 ||| 
2021 ||| the animation transformer: visual correspondence via segment matching. ||| 2598 ||| 2253 ||| 2599 ||| 2600 ||| 2601 ||| 
2019 ||| human attention in image captioning: dataset and analysis. ||| 2195 ||| 2602 ||| 1854 ||| 2603 ||| 
2021 ||| hift: hierarchical feature transformer for aerial tracking. ||| 2604 ||| 2605 ||| 2606 ||| 2607 ||| 2608 ||| 
2017 ||| focusing attention: towards accurate text recognition in natural images. ||| 2609 ||| 2610 ||| 2611 ||| 2612 ||| 1937 ||| 2613 ||| 
2021 ||| agkd-bml: defense against adversarial attack by attention guided knowledge distillation and bi-directional metric learning. ||| 2614 ||| 2615 ||| 2616 ||| 2163 ||| 2617 ||| 
2019 ||| coherent semantic attention for image inpainting. ||| 2618 ||| 170 ||| 2619 ||| 497 ||| 
2021 ||| rethinking transformer-based set prediction for object detection. ||| 2620 ||| 2621 ||| 2622 ||| 2299 ||| 
2021 ||| image harmonization with transformer. ||| 2623 ||| 2624 ||| 2625 ||| 2626 ||| 2627 ||| 2628 ||| 
2021 ||| hierarchical graph attention network for few-shot visual-semantic learning. ||| 2629 ||| 2630 ||| 2631 ||| 2632 ||| 2633 ||| 1248 ||| 
2019 ||| patchwork: a patch-wise attention network for efficient object detection and segmentation in video streams. ||| 2634 ||| 
2021 ||| contrastive attention maps for self-supervised co-localization. ||| 2635 ||| 2636 ||| 2090 ||| 2637 ||| 
2019 ||| an empirical study of spatial attention mechanisms in deep networks. ||| 2638 ||| 2639 ||| 1770 ||| 1771 ||| 1847 ||| 
2021 ||| docformer: end-to-end transformer for document understanding. ||| 2640 ||| 2641 ||| 2642 ||| 2643 ||| 2644 ||| 
2020 ||| geometric attention for prediction of differential properties in 3d point clouds. ||| 2645 ||| 2646 ||| 2647 ||| 2648 ||| 
2017 ||| analysis of human attentions for face recognition on natural videos and comparison with cv algorithm on performance. ||| 2649 ||| 2650 ||| 2651 ||| 2652 ||| 2653 ||| 2654 ||| 
2017 ||| a deep neural model for emotion-driven multimodal attention. ||| 2655 ||| 2656 ||| 887 ||| 2657 ||| 2658 ||| 2659 ||| 1017 ||| 
2020 ||| using pre-trained transformer deep learning models to identify named entities and syntactic relations for clinical protocol analysis. ||| 2660 ||| 2661 ||| 2662 ||| 2663 ||| 
2021 ||| self-adaptive physics-informed neural networks using a soft attention mechanism. ||| 2664 ||| 2665 ||| 
2019 ||| artificial agency requires attention: the case of intentional action. ||| 2666 ||| 2667 ||| 2668 ||| 
2018 ||| neural networks with attention for word sense induction. ||| 2669 ||| 2670 ||| 
2021 ||| multimodal fusion with bert and attention mechanism for fake news detection. ||| 2671 ||| 2672 ||| 
2021 ||| an improved deep neural network based on a novel visual attention mechanism for text recognition. ||| 2673 ||| 2674 ||| 2675 ||| 
2018 ||| design and modeling of new configuration of three phase transformer for high voltage operation using in microwave industrial. ||| 2676 ||| 2677 ||| 2678 ||| 2679 ||| 2680 ||| 2681 ||| 
2021 ||| contextual bi-directional attention flow with embeddings from language models: a generative approach to emotion detection. ||| 2682 ||| 2683 ||| 
2017 ||| power transformer condition forecast with time-series extrapolation. ||| 2684 ||| 2685 ||| 2686 ||| 2687 ||| 2688 ||| 
2021 ||| towards efficient cross-modal visual textual retrieval using transformer-encoder deep features. ||| 2689 ||| 2690 ||| 2691 ||| 2692 ||| 2693 ||| 2694 ||| 
2017 ||| connoisseur: classification of styles of mexican architectural heritage with deep learning and visual attention prediction. ||| 2695 ||| 2696 ||| 2697 ||| 2698 ||| 2699 ||| 2700 ||| 2701 ||| 
2019 ||| dropping activations in convolutional neural networks with visual attention maps. ||| 2695 ||| 2701 ||| 2702 ||| 2703 ||| 2697 ||| 2698 ||| 2699 ||| 2700 ||| 
2021 ||| operation-wise attention network for tampering localization fusion. ||| 2704 ||| 2705 ||| 2706 ||| 2707 ||| 
2021 ||| a gru neural network with attention mechanism for detection of risk situations on multimodal lifelog data. ||| 2708 ||| 2709 ||| 2701 ||| 2710 ||| 2711 ||| 2712 ||| 2713 ||| 2714 ||| 
2021 ||| bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification. ||| 2715 ||| 2716 ||| 2717 ||| 
2021 ||| elsa: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. ||| 2718 ||| 2719 ||| 2720 ||| 2721 ||| 2722 ||| 2723 ||| 2724 ||| 
2019 ||| an attention-based cnn for ecg classification. ||| 2725 ||| 2726 ||| 
2022 ||| exploring transformers for intruder detection in complex maritime environment. ||| 2727 ||| 2728 ||| 2729 ||| 2730 ||| 
2022 ||| predicting outcomes for cancer patients with transformer-based multi-task learning. ||| 2731 ||| 2732 ||| 2733 ||| 2734 ||| 800 ||| 
2021 ||| lcl: light contactless low-delay load monitoring via compressive attentional multi-label learning. ||| 2735 ||| 2736 ||| 2737 ||| 2738 ||| 2739 ||| 2740 ||| 2741 ||| 2742 ||| 
2021 ||| byte-label joint attention learning for packet-grained network traffic classification. ||| 2743 ||| 2744 ||| 2745 ||| 2746 ||| 2747 ||| 2748 ||| 
2020 ||| back-guard: wireless backscattering based user activity recognition and identification with parallel attention model. ||| 2749 ||| 2742 ||| 2750 ||| 2751 ||| 2752 ||| 
2021 ||| a-ddpg: attention mechanism-based deep reinforcement learning for nfv. ||| 2753 ||| 2754 ||| 2755 ||| 2756 ||| 2757 ||| 2758 ||| 
2021 ||| ikan: interactive knowledge-aware attention network for recommendation. ||| 2759 ||| 2760 ||| 2761 ||| 2762 ||| 
2021 ||| emotion cause extraction by combining intra-clause sentiment-enhanced attention and inter-clause consistency interaction. ||| 2763 ||| 2764 ||| 
2021 ||| research on crowd counting based on attention mechanism and dilation convolution. ||| 2765 ||| 2766 ||| 2767 ||| 2768 ||| 2736 ||| 2769 ||| 
2020 ||| feature retrieving for human action recognition by mixed scale deep feature combined with attention model. ||| 2770 ||| 2771 ||| 2772 ||| 2773 ||| 
2021 ||| temperature prediction based on integrated deep learning and attention mechanism. ||| 767 ||| 2774 ||| 2775 ||| 
2021 ||| a multi-grained attention network for multi-labeled distant supervision relation extraction. ||| 2776 ||| 2760 ||| 2777 ||| 
2021 ||| a new siamese co-attention network for unsupervised video object segmentation. ||| 2778 ||| 2779 ||| 2780 ||| 2781 ||| 
2021 ||| a new method of wave parameter retrieve based on transformer. ||| 2782 ||| 2781 ||| 2779 ||| 
2021 ||| attention-based efficient lightweight model for accurate real-time face verification on embedded device. ||| 2783 ||| 2784 ||| 2785 ||| 2786 ||| 2787 ||| 
2019 ||| implementation of marketing communication strategy in attention, interest, search, action, and share (aisas) model through vlog. ||| 2788 ||| 2789 ||| 2790 ||| 
2020 ||| text steganalysis with attentional l stm-cnn. ||| 2791 ||| 2792 ||| 2793 ||| 2794 ||| 2795 ||| 
2020 ||| self-attention for cyberbullying detection. ||| 2796 ||| 2797 ||| 2798 ||| 
2017 ||| efficient speaker naming via deep audio-face fusion and end-to-end attention model. ||| 189 ||| 2799 ||| 2163 ||| 
2019 ||| early diagnosis of alzheimer's disease based on selective kernel network with spatial attention. ||| 2800 ||| 2801 ||| 2802 ||| 2803 ||| 
2019 ||| attention guided unsupervised image-to-image translation with progressively growing strategy. ||| 2804 ||| 2805 ||| 2806 ||| 
2019 ||| spatial-temporal graph attention network for video-based gait recognition. ||| 2807 ||| 2808 ||| 2809 ||| 2810 ||| 2811 ||| 2812 ||| 
2019 ||| dual-attention graph convolutional network. ||| 2813 ||| 2814 ||| 2815 ||| 2816 ||| 1825 ||| 
2019 ||| attention recurrent neural networks for image-based sequence text recognition. ||| 2817 ||| 2818 ||| 
2019 ||| residual attention encoding neural network for terrain texture classification. ||| 2819 ||| 2820 ||| 2821 ||| 
2017 ||| attention-set based metric learning for video face recognition. ||| 2822 ||| 2823 ||| 2824 ||| 
2019 ||| aggregating motion and attention for video object detection. ||| 2825 ||| 2826 ||| 2827 ||| 2828 ||| 
2019 ||| action recognition in untrimmed videos with composite self-attention two-stream framework. ||| 2829 ||| 2830 ||| 2831 ||| 
2019 ||| ssa-gan: end-to-end time-lapse video generation with spatial self-attention. ||| 2832 ||| 2806 ||| 
2020 ||| attention-aware linear depthwise convolution for single image super-resolution. ||| 2833 ||| 2834 ||| 2835 ||| 2836 ||| 
2020 ||| multimodal fusion with attention mechanism for trustworthiness prediction in car advertisements. ||| 2837 ||| 2838 ||| 2839 ||| 2840 ||| 2841 ||| 
2020 ||| comparison of attention module for acoustic scene classification. ||| 2842 ||| 2843 ||| 
2021 ||| msa-cnn: face morphing detection via a multiple scales attention convolutional neural network. ||| 2844 ||| 2845 ||| 2846 ||| 2847 ||| 
2021 ||| double-stream segmentation network with temporal self-attention for deepfake video detection. ||| 2848 ||| 2849 ||| 2850 ||| 2851 ||| 2852 ||| 
2019 ||| rca-net: image recovery network with channel attention group for image dehazing. ||| 2853 ||| 2854 ||| 2855 ||| 2856 ||| 2857 ||| 2858 ||| 
2019 ||| improving the attention span of elementary school children in mexico through a s4 technology platform. ||| 2859 ||| 2860 ||| 2861 ||| 2862 ||| 2863 ||| 2864 ||| 2865 ||| 
2018 ||| detecting attention in pivotal response treatment video probes. ||| 2866 ||| 2867 ||| 2868 ||| 2869 ||| 
2021 ||| transformer-based language models for semantic search and mobile applications retrieval. ||| 1994 ||| 2870 ||| 2871 ||| 2872 ||| 2873 ||| 2874 ||| 1994 ||| 2875 ||| 2876 ||| 2877 ||| 
2020 ||| multi-label classification for clinical text with feature-level attention. ||| 2878 ||| 2879 ||| 2880 ||| 2881 ||| 2882 ||| 2883 ||| 2884 ||| 2885 ||| 
2019 ||| design of air-cooled control system for intelligent transformer. ||| 2886 ||| 2887 ||| 2888 ||| 2889 ||| 2890 ||| 
2019 ||| using eye movement to assess auditory attention. ||| 2891 ||| 2892 ||| 2893 ||| 2894 ||| 2895 ||| 2896 ||| 2897 ||| 2898 ||| 
2021 ||| fault recognition of analog circuits based on ultra-lightweight subspace attention module. ||| 2899 ||| 2900 ||| 1420 ||| 
2021 ||| a deep attention-driven model to forecast solar irradiance. ||| 2901 ||| 2902 ||| 2903 ||| 
2019 ||| better predictability of arbitrage strategies trained among less attentionstocks. ||| 2904 ||| 2905 ||| 
2019 ||| neurofeedback and ai for analyzing child temperament and attention levels. ||| 2906 ||| 2907 ||| 2908 ||| 
2022 ||| meta-learning fine-tuned feature extractor for few-shot image classification: a case study on fine-tuning cnn backbone with transformer for few-shot learning. ||| 2909 ||| 2910 ||| 
2020 ||| guiding the illumination estimation using the attention mechanism. ||| 2911 ||| 2912 ||| 2913 ||| 
2022 ||| transformer-based neural texture synthesis and style transfer. ||| 2914 ||| 
2018 ||| time-variant visual attention in 360-degree video playback. ||| 2915 ||| 2916 ||| 2917 ||| 2918 ||| 2919 ||| 
2021 ||| evolving transformer architecture for neural machine translation. ||| 2920 ||| 2921 ||| 2922 ||| 
2021 ||| a tag-based transformer community question answering learning-to-rank model in the home improvement domain. ||| 2923 ||| 2924 ||| 2925 ||| 
2021 ||| gace: graph-attention-network-based cardinality estimator. ||| 2926 ||| 2927 ||| 2928 ||| 2929 ||| 2930 ||| 
2021 ||| log-based anomaly detection with multi-head scaled dot-product attention mechanism. ||| 2931 ||| 2932 ||| 2933 ||| 2934 ||| 2935 ||| 
2018 ||| a transformerless high gain switched-inductor switched-capacitor cuk converter in step-up mode. ||| 2936 ||| 2937 ||| 2938 ||| 
2018 ||| internal fault analysis in disk-type transformer winding using network reduction. ||| 2939 ||| 2940 ||| 
2017 ||| high efficiency three level transformerless inverter based on sic mosfets for pv applications. ||| 2941 ||| 2942 ||| 2938 ||| 
2021 ||| material texture recognition using ultrasonic images with transformer neural networks. ||| 1340 ||| 2943 ||| 
2019 ||| friend recommendation model based on multi-dimensional academic feature and attention mechanism. ||| 2944 ||| 2945 ||| 2946 ||| 949 ||| 2947 ||| 2948 ||| 
2019 ||| chinese-vietnamese news documents summarization based on feature-related attention mechanism. ||| 2949 ||| 2950 ||| 2951 ||| 2952 ||| 2953 ||| 
2019 ||| a graph representation learning algorithm based on attention mechanism and node similarity. ||| 2954 ||| 2955 ||| 2956 ||| 2957 ||| 2958 ||| 2959 ||| 
2019 ||| charge prediction for multi-defendant cases with multi-scale attention. ||| 2960 ||| 2961 ||| 2962 ||| 2963 ||| 2964 ||| 
2019 ||| a mobile application classification method with enhanced topic attention mechanism. ||| 927 ||| 1569 ||| 2965 ||| 162 ||| 2966 ||| 1592 ||| 
2019 ||| measuring node similarity for the collective attention flow network. ||| 2967 ||| 2968 ||| 2969 ||| 2970 ||| 817 ||| 2971 ||| 2972 ||| 
2019 ||| a multi-domain named entity recognition method based on part-of-speech attention mechanism. ||| 2973 ||| 2974 ||| 2975 ||| 2976 ||| 2977 ||| 2978 ||| 
2020 ||| modeling limited attention in opinion dynamics by topological interactions. ||| 2979 ||| 2980 ||| 2981 ||| 
2017 ||| deep learning based automatic diagnoses of attention deficit hyperactive disorder. ||| 2982 ||| 2983 ||| 2984 ||| 
2019 ||| a divide-and-conquer framework for attention-based combination of multiple investment strategies. ||| 2985 ||| 2986 ||| 2987 ||| 2988 ||| 2989 ||| 
2019 ||| an attention based deep neural network for automatic lexical stress detection. ||| 2990 ||| 2991 ||| 2992 ||| 2993 ||| 2994 ||| 2995 ||| 
2019 ||| an: a temporal-frequency fusion attention network for spectrum energy level prediction. ||| 2996 ||| 2997 ||| 2998 ||| 2999 ||| 
2019 ||| fovr: attention-based vr streaming through bandwidth-limited wireless networks. ||| 3000 ||| 3001 ||| 3002 ||| 
2020 ||| extended abstract - transformers: intrusion detection data in disguise. ||| 3003 ||| 3004 ||| 3005 ||| 
2020 ||| evaluating pretrained transformer models for citation recommendation. ||| 3006 ||| 3007 ||| 3008 ||| 3009 ||| 
2017 ||| distributing attention between environment and navigation system to increase spatial knowledge acquisition during assisted wayfinding. ||| 3010 ||| 3011 ||| 3012 ||| 3013 ||| 
2020 ||| sentiment prediction using attention on user-specific rating distribution. ||| 3014 ||| 1397 ||| 
2018 ||| product question intent detection using indicative clause attention and adversarial learning. ||| 1695 ||| 3015 ||| 
2019 ||| sadhan: hierarchical attention networks to learn latent aspect embeddings for fake news detection. ||| 1314 ||| 3016 ||| 
2017 ||| estimation of students' attention in the classroom from kinect features. ||| 3017 ||| 
2021 ||| pneumoxttention: a cnn compensating for human fallibility when detecting pneumonia through cxr images with attention. ||| 3018 ||| 
2021 ||| illuminant estimation error detection for outdoor scenes using transformers. ||| 3019 ||| 3020 ||| 2912 ||| 2913 ||| 
2019 ||| attention-based convolutional neural network for computer vision color constancy. ||| 2911 ||| 2912 ||| 2913 ||| 
2021 ||| effective attention sheds light on interpretability. ||| 3021 ||| 3022 ||| 
2021 ||| transforming term extraction: transformer-based approaches to multilingual term extraction across domains. ||| 3023 ||| 3024 ||| 3025 ||| 3026 ||| 
2018 ||| efficient large-scale neural domain classification with personalized attention. ||| 3027 ||| 3028 ||| 3029 ||| 3030 ||| 
2020 ||| sentibert: a transferable transformer-based architecture for compositional sentiment semantics. ||| 3031 ||| 3032 ||| 3033 ||| 
2021 ||| personalized transformer for explainable recommendation. ||| 3034 ||| 3035 ||| 3036 ||| 
2019 ||| assessing the ability of self-attention networks to learn word order. ||| 3037 ||| 3038 ||| 3039 ||| 3040 ||| 3041 ||| 
2021 ||| attention calibration for transformer in neural machine translation. ||| 3042 ||| 3043 ||| 3044 ||| 3045 ||| 3046 ||| 
2020 ||| regularized context gates on transformer for machine translation. ||| 3047 ||| 3048 ||| 3049 ||| 3050 ||| 3051 ||| 
2021 ||| error detection in large-scale natural language understanding systems using transformer models. ||| 3052 ||| 3053 ||| 3054 ||| 3055 ||| 
2019 ||| learning attention-based embeddings for relation prediction in knowledge graphs. ||| 3056 ||| 3057 ||| 3058 ||| 3059 ||| 
2021 ||| length-adaptive transformer: train once with length drop, use anytime with search. ||| 3060 ||| 3008 ||| 
2021 ||| multimodal graph-based transformer framework for biomedical relation extraction. ||| 3061 ||| 3062 ||| 3063 ||| 404 ||| 
2020 ||| learning to deceive with attention-based explanations. ||| 3064 ||| 3065 ||| 3066 ||| 3067 ||| 3068 ||| 
2020 ||| entity-aware dependency-based deep graph attention network for comparative preference classification. ||| 3069 ||| 3070 ||| 3071 ||| 1371 ||| 3072 ||| 
2019 ||| incremental transformer with deliberation decoder for document grounded conversations. ||| 3073 ||| 3074 ||| 3075 ||| 3076 ||| 1719 ||| 1921 ||| 
2017 ||| learning attention for historical text normalization by learning to pronounce. ||| 3077 ||| 3078 ||| 3079 ||| 3080 ||| 
2020 ||| inset: sentence infilling with inter-sentential transformer. ||| 3081 ||| 1709 ||| 3082 ||| 2045 ||| 
2019 ||| aspect sentiment classification towards question-answering with reinforced bidirectional attention network. ||| 3083 ||| 3084 ||| 3085 ||| 3086 ||| 3087 ||| 1254 ||| 3088 ||| 
2020 ||| enhancing machine translation with dependency-aware self-attention. ||| 3089 ||| 3090 ||| 
2019 ||| extracting multiple-relations in one-pass with pre-trained transformers. ||| 3091 ||| 3092 ||| 3093 ||| 3094 ||| 3095 ||| 3096 ||| 3097 ||| 3098 ||| 
2020 ||| relational graph attention network for aspect-based sentiment analysis. ||| 333 ||| 3099 ||| 3100 ||| 3101 ||| 3049 ||| 
2021 ||| enhancing transformers with gradient boosted decision trees for nli fine-tuning. ||| 3102 ||| 3103 ||| 3104 ||| 
2021 ||| can transformer models measure coherence in text: re-thinking the shuffle test. ||| 3105 ||| 3106 ||| 3107 ||| 3108 ||| 
2019 ||| lattice-based transformer encoder for neural machine translation. ||| 3109 ||| 3110 ||| 3111 ||| 3049 ||| 3112 ||| 
2020 ||| hiring now: a skill-aware multi-attention model for job posting generation. ||| 3113 ||| 3114 ||| 3115 ||| 3116 ||| 3117 ||| 3118 ||| 
2020 ||| heterogeneous graph transformer for graph-to-sequence learning. ||| 3119 ||| 3120 ||| 3121 ||| 
2020 ||| specter: document-level representation learning using citation-informed transformers. ||| 3122 ||| 3123 ||| 3124 ||| 3125 ||| 3126 ||| 
2017 ||| exploiting argument information to improve event detection via supervised attention mechanisms. ||| 3127 ||| 3128 ||| 3129 ||| 1418 ||| 
2020 ||| understanding attention for text classification. ||| 3130 ||| 3131 ||| 
2019 ||| improving textual network embedding with global attention via optimal transport. ||| 3132 ||| 1029 ||| 3133 ||| 1031 ||| 3134 ||| 3135 ||| 3136 ||| 1709 ||| 1032 ||| 
2021 ||| cascaded head-colliding attention. ||| 3137 ||| 3138 ||| 3139 ||| 
2019 ||| self-attention architectures for answer-agnostic neural question generation. ||| 3140 ||| 3141 ||| 3142 ||| 
2019 ||| multilingual constituency parsing with self-attention and pre-training. ||| 3143 ||| 3144 ||| 3145 ||| 
2020 ||| line graph enhanced amr-to-text generation with mix-order graph attention networks. ||| 3146 ||| 3147 ||| 3148 ||| 3149 ||| 3150 ||| 3151 ||| 
2021 |||  marbert: deep bidirectional transformers for arabic. ||| 3152 ||| 3153 ||| 3154 ||| 
2017 ||| domain attention with an ensemble of experts. ||| 3027 ||| 3155 ||| 3028 ||| 
2018 ||| cross-target stance classification with self-attention networks. ||| 3156 ||| 3157 ||| 3158 ||| 3159 ||| 3160 ||| 
2020 ||| hat: hardware-aware transformers for efficient natural language processing. ||| 3161 ||| 3162 ||| 3163 ||| 3164 ||| 3165 ||| 2190 ||| 3166 ||| 
2020 ||| lexically constrained neural machine translation with levenshtein transformer. ||| 3167 ||| 3168 ||| 3169 ||| 
2021 ||| memory-efficient differentiable transformer architecture search. ||| 3170 ||| 3171 ||| 3172 ||| 3173 ||| 3174 ||| 3175 ||| 
2018 ||| a multi-sentiment-resource enhanced attention network for sentiment classification. ||| 3176 ||| 3177 ||| 1081 ||| 1199 ||| 
2021 ||| diagnosing transformers in task-oriented semantic parsing. ||| 3178 ||| 3179 ||| 
2018 ||| accelerating neural transformer via an average attention network. ||| 3180 ||| 3181 ||| 3182 ||| 
2017 ||| a nested attention neural hybrid model for grammatical error correction. ||| 3183 ||| 3184 ||| 3185 ||| 3186 ||| 3187 ||| 1958 ||| 
2020 ||| automated topical component extraction using neural network attention scores from source-based essay scoring. ||| 789 ||| 3188 ||| 
2020 ||| quantifying attention flow in transformers. ||| 3189 ||| 3190 ||| 
2019 ||| using human attention to extract keyphrase from microblog post. ||| 3191 ||| 3192 ||| 
2017 ||| an unsupervised neural attention model for aspect extraction. ||| 3193 ||| 3194 ||| 3195 ||| 3196 ||| 
2020 ||| joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge. ||| 3197 ||| 3198 ||| 3199 ||| 3200 ||| 3101 ||| 2814 ||| 3201 ||| 
2021 ||| the case for translation-invariant self-attention in transformer-based language models. ||| 3202 ||| 3203 ||| 
2021 ||| what context features can transformer language models use? ||| 3204 ||| 3205 ||| 
2021 ||| a bidirectional transformer based alignment model for unsupervised word alignment. ||| 3206 ||| 3207 ||| 
2021 ||| multimodal fusion with co-attention networks for fake news detection. ||| 3208 ||| 3209 ||| 3210 ||| 3211 ||| 3212 ||| 
2019 ||| multi-grained attention with object-level grounding for visual question answering. ||| 3213 ||| 3214 ||| 3215 ||| 3216 ||| 3217 ||| 
2021 ||| a span-based dynamic local attention model for sequential sentence classification. ||| 3218 ||| 3219 ||| 3220 ||| 3221 ||| 3222 ||| 
2019 ||| you only need attention to traverse trees. ||| 3223 ||| 3224 ||| 3225 ||| 
2018 ||| document embedding enhanced event detection with hierarchical and supervised attention. ||| 3226 ||| 3227 ||| 3228 ||| 1445 ||| 
2021 ||| transformer-exclusive cross-modal representation for vision and language. ||| 3229 ||| 3230 ||| 
2017 ||| neural relation extraction with multi-lingual attention. ||| 3231 ||| 3232 ||| 3233 ||| 
2021 ||| tan-ntm: topic attention networks for neural topic modeling. ||| 3234 ||| 3235 ||| 3236 ||| 3237 ||| 
2021 ||| select, extract and generate: neural keyphrase generation with layer-wise coverage attention. ||| 3238 ||| 3239 ||| 3240 ||| 3033 ||| 
2020 ||| highway transformer: self-gating enhanced self-attentive networks. ||| 3241 ||| 3242 ||| 3243 ||| 
2017 ||| gated-attention readers for text comprehension. ||| 3066 ||| 3244 ||| 3245 ||| 3246 ||| 3247 ||| 
2018 ||| neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering. ||| 3248 ||| 3157 ||| 3249 ||| 3250 ||| 3251 ||| 3252 ||| 
2019 ||| attention and lexicon regularized lstm for aspect-based sentiment analysis. ||| 3253 ||| 3254 ||| 3255 ||| 
2021 ||| mention flags (mf): constraining transformer-based text generators. ||| 2852 ||| 3256 ||| 3257 ||| 3258 ||| 3259 ||| 
2020 ||| lipschitz constrained parameter initialization for deep transformers. ||| 8 ||| 3260 ||| 3207 ||| 3181 ||| 3206 ||| 
2020 ||| exbert: a visual analysis tool to explore learned representations in transformer models. ||| 3261 ||| 3262 ||| 3263 ||| 
2020 ||| dtca: decision tree-based co-attention networks for explainable claim verification. ||| 3264 ||| 3265 ||| 3266 ||| 3267 ||| 3268 ||| 
2018 ||| improving slot filling in spoken language understanding with joint pointer and attention. ||| 1976 ||| 3269 ||| 
2019 ||| style transformer: unpaired text style transfer without disentangled latent representation. ||| 3270 ||| 3271 ||| 3272 ||| 3273 ||| 
2021 ||| on the interplay between fine-tuning and composition in transformers. ||| 3274 ||| 3275 ||| 
2019 ||| is attention interpretable? ||| 3276 ||| 3277 ||| 
2019 ||| modeling intra-relation in math word problems with different functional multi-head attentions. ||| 3278 ||| 3279 ||| 3280 ||| 247 ||| 3281 ||| 3282 ||| 
2021 ||| unsupervised out-of-domain detection via pre-trained transformers. ||| 3283 ||| 3284 ||| 3285 ||| 3286 ||| 3287 ||| 
2019 ||| leveraging local and global patterns for self-attention networks. ||| 3288 ||| 3039 ||| 3037 ||| 3289 ||| 3040 ||| 
2021 ||| are pretrained convolutions better than pretrained transformers? ||| 1398 ||| 2293 ||| 3290 ||| 3291 ||| 3292 ||| 3293 ||| 3294 ||| 
2021 ||| irene: interpretable energy prediction for transformers. ||| 3295 ||| 3296 ||| 3297 ||| 3298 ||| 3299 ||| 
2019 ||| multimodal transformer networks for end-to-end video-grounded dialogue systems. ||| 3300 ||| 3301 ||| 3302 ||| 3303 ||| 
2019 ||| learning deep transformer models for machine translation. ||| 3304 ||| 3305 ||| 2333 ||| 3306 ||| 3307 ||| 3039 ||| 3040 ||| 
2020 ||| how does selective mechanism improve self-attention networks? ||| 3308 ||| 3038 ||| 3309 ||| 3310 ||| 3311 ||| 3041 ||| 
2021 ||| ma-bert: learning representation by incorporating multi-attribute knowledge in transformers. ||| 3312 ||| 3313 ||| 3314 ||| 3315 ||| 
2021 ||| a multi-level attention model for evidence-based fact checking. ||| 3316 ||| 3317 ||| 398 ||| 
2021 ||| readonce transformers: reusable representations of text for transformers. ||| 3318 ||| 3319 ||| 3320 ||| 
2020 ||| fine-grained fact verification with kernel graph attention network. ||| 3321 ||| 3322 ||| 3233 ||| 3232 ||| 
2019 ||| syntactically supervised transformers for faster neural machine translation. ||| 3323 ||| 3324 ||| 3325 ||| 
2020 ||| enhancing pre-trained chinese character representation with word-aligned attention. ||| 757 ||| 3326 ||| 756 ||| 759 ||| 
2017 ||| diversity driven attention model for query-based abstractive summarization. ||| 3327 ||| 3328 ||| 3329 ||| 3330 ||| 
2019 ||| monotonic infinite lookback attention for simultaneous machine translation. ||| 3331 ||| 3332 ||| 3333 ||| 3334 ||| 3335 ||| 3336 ||| 3337 ||| 3338 ||| 
2020 ||| location attention for extrapolation to longer sequences. ||| 3339 ||| 3340 ||| 3341 ||| 3342 ||| 
2021 ||| xeroalign: zero-shot cross-lingual transformer alignment. ||| 3103 ||| 3104 ||| 
2021 ||| attending self-attention: a case study of visually grounded supervision in vision-and-language transformers. ||| 3343 ||| 3344 ||| 3345 ||| 3346 ||| 1876 ||| 
2020 ||| improving transformer models by reordering their sublayers. ||| 3347 ||| 3277 ||| 3348 ||| 
2021 ||| on-the-fly attention modulation for neural generation. ||| 3349 ||| 3350 ||| 3351 ||| 3352 ||| 3353 ||| 3354 ||| 3355 ||| 
2020 ||| how does bert's attention change when you fine-tune? an analysis methodology and a case study in negation scope. ||| 3356 ||| 3357 ||| 
2019 ||| token-level dynamic self-attention network for multi-passage reading comprehension. ||| 3358 ||| 3359 ||| 
2021 ||| on orthogonality constraints for transformers. ||| 3360 ||| 3361 ||| 1398 ||| 3362 ||| 3363 ||| 3364 ||| 3365 ||| 3366 ||| 3367 ||| 
2019 ||| scheduled sampling for transformers. ||| 3368 ||| 3369 ||| 3370 ||| 
2019 ||| rumor detection by exploiting user credibility information, attention and multi-task learning. ||| 3371 ||| 3372 ||| 3087 ||| 
2020 ||| the cascade transformer: an application for efficient answer sentence selection. ||| 3373 ||| 3374 ||| 
2021 ||| parameter selection: why we should pay more attention to it. ||| 3375 ||| 3376 ||| 3377 ||| 3378 ||| 
2021 ||| bertac: enhancing transformer-based language models with adversarially pretrained convolutional neural networks. ||| 3379 ||| 3380 ||| 3381 ||| 3382 ||| 
2021 ||| argument pair extraction via attention-guided multi-layer multi-cross encoding. ||| 3383 ||| 3384 ||| 3385 ||| 3087 ||| 
2021 ||| parallel attention network with sequence matching for video grounding. ||| 3386 ||| 1397 ||| 3387 ||| 3388 ||| 3389 ||| 3390 ||| 
2021 ||| global attention decoder for chinese spelling error correction. ||| 3391 ||| 3392 ||| 3393 ||| 3394 ||| 3395 ||| 
2017 ||| segmentation guided attention networks for visual question answering. ||| 3396 ||| 3397 ||| 3398 ||| 
2018 ||| visual attention model for name tagging in multimodal social media. ||| 3399 ||| 3400 ||| 3401 ||| 3402 ||| 3403 ||| 
2018 ||| document modeling with external attention for sentence extraction. ||| 3404 ||| 3405 ||| 3406 ||| 3407 ||| 3408 ||| 3409 ||| 3410 ||| 
2020 ||| adaptive transformers for learning multimodal representations. ||| 3411 ||| 
2021 ||| ernie-doc: a retrospective long-document modeling transformer. ||| 3412 ||| 3413 ||| 3414 ||| 673 ||| 3415 ||| 3416 ||| 3417 ||| 
2019 ||| pay attention when you pay the bills. a multilingual corpus with dependency-based and semantic annotation of collocations. ||| 3418 ||| 3419 ||| 3418 ||| 3420 ||| 3421 ||| 3422 ||| 2600 ||| 3423 ||| 
2020 ||| dynamically adjusting transformer batch size by monitoring gradient direction change. ||| 8 ||| 3207 ||| 3181 ||| 3260 ||| 
2020 ||| hard-coded gaussian attention for neural machine translation. ||| 3424 ||| 3425 ||| 3325 ||| 
2021 ||| glancing transformer for non-autoregressive neural machine translation. ||| 3426 ||| 2736 ||| 3427 ||| 3428 ||| 3429 ||| 1219 ||| 1223 ||| 3034 ||| 
2021 ||| it's all in the heads: using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. ||| 3430 ||| 3431 ||| 
2021 ||| contributions of transformer attention heads in multi- and cross-lingual tasks. ||| 3432 ||| 3433 ||| 3434 ||| 3435 ||| 3436 ||| 
2019 ||| arnor: attention regularization based noise reduction for distant supervision relation classification. ||| 3437 ||| 3438 ||| 3439 ||| 3416 ||| 
2017 ||| morphological inflection generation with hard monotonic attention. ||| 3440 ||| 3441 ||| 
2017 ||| incorporating word reordering knowledge into attention-based neural machine translation. ||| 3442 ||| 3428 ||| 3443 ||| 1921 ||| 
2019 ||| adversarial attention modeling for multi-dimensional emotion regression. ||| 3444 ||| 3085 ||| 3088 ||| 
2020 ||| a transformer-based approach for source code summarization. ||| 3238 ||| 3445 ||| 3446 ||| 3033 ||| 
2019 ||| a multiscale visualization of attention in the transformer model. ||| 3447 ||| 
2019 ||| fine-tuning pre-trained transformer language models to distantly supervised relation extraction. ||| 3448 ||| 3449 ||| 3450 ||| 3451 ||| 
2020 ||| successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. ||| 3452 ||| 3453 ||| 3454 ||| 
2021 ||| bert busters: outlier dimensions that disrupt transformers. ||| 3455 ||| 3456 ||| 3457 ||| 3458 ||| 
2020 ||| pretrained transformers improve out-of-distribution robustness. ||| 3459 ||| 3460 ||| 3461 ||| 3462 ||| 3463 ||| 3464 ||| 
2020 ||| combining subword representations into word-level representations in the transformer architecture. ||| 3465 ||| 3466 ||| 852 ||| 3467 ||| 
2019 ||| zero-shot cross-lingual abstractive sentence summarization through teaching generation and attention. ||| 3468 ||| 3469 ||| 1254 ||| 3470 ||| 3471 ||| 
2019 ||| multi-step reasoning via recurrent dual attention for visual dialog. ||| 2044 ||| 2045 ||| 3472 ||| 2043 ||| 2046 ||| 1958 ||| 
2021 ||| incorporating global information in local attention for knowledge representation learning. ||| 3473 ||| 3474 ||| 3475 ||| 3476 ||| 3477 ||| 3478 ||| 
2019 ||| hibert: document level pre-training of hierarchical bidirectional transformers for document summarization. ||| 3479 ||| 3174 ||| 3480 ||| 
2021 ||| video-guided machine translation with spatial hierarchical attention network. ||| 3481 ||| 3482 ||| 3346 ||| 3483 ||| 
2019 ||| exact hard monotonic attention for character-level transduction. ||| 3484 ||| 3485 ||| 
2020 ||| differentiable window for dynamic local attention. ||| 3486 ||| 3487 ||| 1313 ||| 3488 ||| 
2020 ||| transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. ||| 3489 ||| 375 ||| 
2021 ||| focus attention: promoting faithfulness and diversity in summarization. ||| 3490 ||| 3404 ||| 3491 ||| 3492 ||| 3493 ||| 
2019 ||| look harder: a neural machine translation model with hard attention. ||| 3494 ||| 3495 ||| 3496 ||| 
2021 ||| minilmv2: multi-head self-attention relation distillation for compressing pretrained transformers. ||| 3497 ||| 3498 ||| 3499 ||| 3171 ||| 3174 ||| 
2018 ||| multi-turn response selection for chatbots with deep attention matching network. ||| 3500 ||| 3501 ||| 3502 ||| 1199 ||| 3503 ||| 3504 ||| 3505 ||| 3416 ||| 
2020 ||| gated convolutional bidirectional attention-based model for off-topic spoken response detection. ||| 3506 ||| 3507 ||| 3508 ||| 
2021 ||| do multilingual neural machine translation models contain language pair specific attention heads? ||| 3509 ||| 3510 ||| 3511 ||| 3512 ||| 
2020 ||| towards transparent and explainable attention models. ||| 3513 ||| 3327 ||| 3514 ||| 3328 ||| 3515 ||| 3330 ||| 
2019 ||| self-attentional models for lattice inputs. ||| 3516 ||| 3067 ||| 3517 ||| 3518 ||| 
2021 ||| assessing the syntactic capabilities of transformer-based multilingual language models. ||| 3519 ||| 3520 ||| 3521 ||| 3522 ||| 3419 ||| 3523 ||| 3524 ||| 
2020 ||| human attention maps for text classification: do humans and neural networks focus on the same words? ||| 3525 ||| 3526 ||| 3527 ||| 1195 ||| 3528 ||| 
2020 ||| a contextual hierarchical attention network with adaptive objective for dialogue state tracking. ||| 3529 ||| 3073 ||| 3442 ||| 3075 ||| 3076 ||| 3074 ||| 1921 ||| 
2021 ||| best of both worlds: making high accuracy non-incremental transformer-based disfluency detection incremental. ||| 3530 ||| 3531 ||| 
2021 ||| multi-scale progressive attention network for video question answering. ||| 3532 ||| 3533 ||| 400 ||| 3534 ||| 3535 ||| 
2019 ||| comet: commonsense transformers for automatic knowledge graph construction. ||| 3353 ||| 3536 ||| 3537 ||| 3538 ||| 3539 ||| 3355 ||| 
2018 ||| sparse and constrained attention for neural machine translation. ||| 3538 ||| 3540 ||| 3369 ||| 3370 ||| 
2017 ||| a recurrent neural model with attention for the recognition of chinese implicit discourse relations. ||| 3541 ||| 3542 ||| 3543 ||| 3544 ||| 
2019 ||| reading turn by turn: hierarchical attention architecture for spoken dialogue comprehension. ||| 3545 ||| 3302 ||| 
2021 ||| do context-aware translation models pay the right attention? ||| 3546 ||| 3547 ||| 3064 ||| 3548 ||| 3369 ||| 3370 ||| 3067 ||| 
2017 ||| joint ctc/attention decoding for end-to-end speech recognition. ||| 2508 ||| 3549 ||| 2511 ||| 
2021 ||| structural guidance for transformer language models. ||| 3550 ||| 3551 ||| 3552 ||| 1633 ||| 3553 ||| 
2021 ||| realformer: transformer likes residual attention. ||| 3554 ||| 3555 ||| 3556 ||| 3557 ||| 
2020 ||| self-attention guided copy mechanism for abstractive summarization. ||| 3558 ||| 1013 ||| 3559 ||| 3560 ||| 3561 ||| 3562 ||| 
2018 ||| attention focusing for neural machine translation by bridging source and target embeddings. ||| 3563 ||| 3564 ||| 2871 ||| 3565 ||| 3471 ||| 3181 ||| 
2021 ||| recursive tree-structured self-attention for answer sentence selection. ||| 3566 ||| 3567 ||| 3568 ||| 
2017 ||| a generative attentional neural network model for dialogue act classification. ||| 3569 ||| 3570 ||| 3571 ||| 
2019 ||| attention is (not) all you need for commonsense reasoning. ||| 3572 ||| 3573 ||| 
2021 ||| cluster-former: clustering-based sparse transformer for question answering. ||| 3363 ||| 3574 ||| 2044 ||| 3575 ||| 3576 ||| 3577 ||| 2045 ||| 2046 ||| 
2019 ||| hierarchical transformers for multi-document summarization. ||| 1305 ||| 3408 ||| 
2021 ||| space efficient context encoding for non-task-oriented dialogue generation with graph attention transformer. ||| 3578 ||| 3579 ||| 3580 ||| 3581 ||| 
2021 ||| attention flows are shapley value explanations. ||| 3582 ||| 3583 ||| 
2021 ||| is sparse attention more interpretable? ||| 3584 ||| 3585 ||| 3586 ||| 3485 ||| 
2021 ||| enhancing content preservation in text style transfer using reverse attention and conditional layer normalization. ||| 3587 ||| 3588 ||| 3589 ||| 3590 ||| 
2017 ||| attention strategies for multi-source sequence-to-sequence learning. ||| 3591 ||| 3592 ||| 
2020 ||| character-level translation with self-attention. ||| 3593 ||| 3594 ||| 3595 ||| 3596 ||| 
2019 ||| multimodal transformer for unaligned multimodal language sequences. ||| 3597 ||| 3598 ||| 3599 ||| 3600 ||| 3601 ||| 3247 ||| 
2021 ||| how knowledge graph and attention help? a qualitative analysis into bag-level relation extraction. ||| 3602 ||| 3603 ||| 3604 ||| 3605 ||| 
2021 ||| g-transformer for document-level machine translation. ||| 3606 ||| 3289 ||| 3607 ||| 3470 ||| 3471 ||| 
2021 ||| mect: multi-metadata embedding based cross-transformer for chinese named entity recognition. ||| 3608 ||| 3609 ||| 3610 ||| 
2021 ||| generalized supervised attention for text generation. ||| 3611 ||| 3612 ||| 3613 ||| 3614 ||| 3289 ||| 1838 ||| 
2021 ||| improving bert with syntax-aware local attention. ||| 3615 ||| 3616 ||| 242 ||| 3617 ||| 3618 ||| 
2019 ||| opendialkg: explainable conversational reasoning with attention-based walks over knowledge graphs. ||| 3619 ||| 3620 ||| 3621 ||| 3622 ||| 
2021 ||| graph relational topic model with higher-order graph attention auto-encoders. ||| 3623 ||| 3624 ||| 3625 ||| 3626 ||| 
2018 ||| multimodal affective analysis using hierarchical attention strategy with word-level alignment. ||| 3627 ||| 3628 ||| 3629 ||| 3630 ||| 2419 ||| 2425 ||| 
2021 ||| tilgan: transformer-based implicit latent gan for diverse and coherent text generation. ||| 3631 ||| 3632 ||| 3633 ||| 3198 ||| 2814 ||| 
2021 ||| realtrans: end-to-end simultaneous speech translation with convolutional weighted-shrinking transformer. ||| 3634 ||| 3635 ||| 3443 ||| 
2020 ||| integrating multimodal information in large pretrained transformers. ||| 3636 ||| 3637 ||| 3638 ||| 3639 ||| 3640 ||| 3601 ||| 3641 ||| 
2017 ||| attention-over-attention neural networks for reading comprehension. ||| 3642 ||| 3643 ||| 3644 ||| 1308 ||| 3311 ||| 3645 ||| 
2019 ||| sentence-level evidence embedding for claim verification with hierarchical attention networks. ||| 3646 ||| 1310 ||| 1313 ||| 3647 ||| 
2021 ||| self-attention networks can process bounded hierarchical languages. ||| 3648 ||| 3649 ||| 3650 ||| 3651 ||| 
2019 ||| progressive self-supervised attention learning for aspect-level sentiment analysis. ||| 3652 ||| 3653 ||| 3182 ||| 3654 ||| 3655 ||| 3656 ||| 2166 ||| 
2021 ||| more identifiable yet equally performant transformers for text classification. ||| 3657 ||| 3658 ||| 892 ||| 3659 ||| 
2021 ||| topic-driven and knowledge-aware transformer for dialogue emotion detection. ||| 3660 ||| 3661 ||| 3662 ||| 3663 ||| 3664 ||| 
2020 ||| sas: dialogue state tracking via slot attention and slot information sharing. ||| 3665 ||| 3666 ||| 3667 ||| 329 ||| 1753 ||| 
2018 ||| multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. ||| 1160 ||| 3668 ||| 3669 ||| 
2021 ||| parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. ||| 3670 ||| 3671 ||| 2293 ||| 3672 ||| 
2021 ||| multimodal incremental transformer with visual grounding for visual dialogue generation. ||| 3673 ||| 3075 ||| 3674 ||| 3675 ||| 1921 ||| 
2020 ||| deformer: decomposing pre-trained transformers for faster question answering. ||| 3295 ||| 3297 ||| 3298 ||| 3299 ||| 
2021 ||| optimizing deeper transformers on small datasets. ||| 3676 ||| 3677 ||| 2334 ||| 3678 ||| 3679 ||| 3680 ||| 3354 ||| 3681 ||| 3682 ||| 
2021 ||| a semantics-aware transformer model of relation linking for knowledge base question answering. ||| 3551 ||| 3683 ||| 3684 ||| 3685 ||| 3686 ||| 3687 ||| 3688 ||| 3689 ||| 3690 ||| 
2021 ||| learning slice-aware representations with mixture of attentions. ||| 3691 ||| 3692 ||| 3693 ||| 3694 ||| 3027 ||| 3030 ||| 
2021 ||| on the distribution, sparsity, and inference-time quantization of attention values in transformers. ||| 3695 ||| 3696 ||| 3697 ||| 3698 ||| 3699 ||| 3299 ||| 
2017 ||| end-to-end non-factoid question answering with an interactive visualization of neural attention weights. ||| 3700 ||| 3701 ||| 3702 ||| 
2020 ||| document modeling with graph attention networks for multi-grained machine reading comprehension. ||| 3703 ||| 3704 ||| 3705 ||| 3706 ||| 3707 ||| 3708 ||| 3480 ||| 3311 ||| 
2020 ||| do transformers need deep long-range memory? ||| 3709 ||| 3710 ||| 
2020 ||| gcan: graph-aware co-attention networks for explainable fake news detection on social media. ||| 3711 ||| 3712 ||| 
2018 ||| cold-start aware user and product attention for sentiment classification. ||| 3713 ||| 3714 ||| 3715 ||| 3716 ||| 
2021 ||| probing image-language transformers for verb understanding. ||| 3717 ||| 3718 ||| 
2018 ||| ruminating reader: reasoning with gated multi-hop attention. ||| 3719 ||| 3720 ||| 
2020 ||| multimodal and multiresolution speech recognition with transformers. ||| 3721 ||| 3722 ||| 3723 ||| 3724 ||| 
2020 ||| multimodal transformer for multimodal machine translation. ||| 3119 ||| 3121 ||| 
2020 ||| dependency graph enhanced dual-transformer structure for aspect-based sentiment classification. ||| 435 ||| 3725 ||| 1400 ||| 3726 ||| 
2019 ||| attention-based conditioning methods for external knowledge integration. ||| 3727 ||| 3728 ||| 3729 ||| 
2021 ||| h-transformer-1d: fast one-dimensional hierarchical attention for sequences. ||| 3730 ||| 3731 ||| 
2021 ||| transformer-based direct hidden markov model for machine translation. ||| 3732 ||| 3733 ||| 3734 ||| 3454 ||| 
2021 ||| improving the faithfulness of attention-based explanations with task-specific information for text classification. ||| 3735 ||| 3736 ||| 
2019 ||| recosa: detecting the relevant contexts with self-attention for multi-turn dialogue generation. ||| 3737 ||| 1444 ||| 1443 ||| 3738 ||| 1445 ||| 
2021 ||| code summarization with structure-induced transformer. ||| 3739 ||| 3111 ||| 1254 ||| 
2019 ||| attention over heads: a multi-hop attention for neural machine translation. ||| 3740 ||| 3741 ||| 3742 ||| 3743 ||| 3744 ||| 3745 ||| 
2021 ||| convolutions and self-attention: re-interpreting relative positions in pre-trained language models. ||| 1814 ||| 1813 ||| 1812 ||| 1815 ||| 
2021 ||| contrastive attention for automatic chest x-ray report generation. ||| 3746 ||| 3747 ||| 3748 ||| 3749 ||| 3750 ||| 3751 ||| 
2018 ||| multi-input attention for unsupervised ocr correction. ||| 3752 ||| 3753 ||| 
2021 ||| hi-transformer: hierarchical interactive transformer for efficient and effective long document modeling. ||| 3754 ||| 3755 ||| 3756 ||| 2795 ||| 
2021 ||| hierarchical task learning from language instructions with unified transformers and self-monitoring. ||| 3757 ||| 3758 ||| 
2017 ||| abstractive document summarization with a graph-based attentional neural model. ||| 3759 ||| 3121 ||| 3760 ||| 
2021 ||| how does attention affect the model? ||| 3761 ||| 3762 ||| 3763 ||| 3764 ||| 
2021 ||| how many layers and why? an analysis of the model depth in transformers. ||| 3765 ||| 3766 ||| 3767 ||| 
2021 ||| r2d2: recursive transformer based on differentiable tree for interpretable hierarchical language modeling. ||| 3768 ||| 3769 ||| 3770 ||| 3771 ||| 3772 ||| 3773 ||| 3774 ||| 
2021 ||| n-best asr transformer: enhancing slu performance using multiple asr hypotheses. ||| 3775 ||| 3776 ||| 3777 ||| 3778 ||| 3779 ||| 
2019 ||| transformer-xl: attentive language models beyond a fixed-length context. ||| 3780 ||| 3245 ||| 2622 ||| 3781 ||| 3782 ||| 3247 ||| 
2021 ||| synchronous syntactic attention for transformer neural machine translation. ||| 3783 ||| 3784 ||| 3785 ||| 
2021 ||| neural retrieval for question answering with cross-attention supervised data augmentation. ||| 3786 ||| 3787 ||| 3788 ||| 3789 ||| 3790 ||| 
2017 ||| pay attention to the ending: strong neural baselines for the roc story cloze task. ||| 3791 ||| 3792 ||| 3793 ||| 
2020 ||| self-attention with cross-lingual position representation. ||| 3794 ||| 3038 ||| 1756 ||| 
2021 ||| long-span summarization via local attention and content selection. ||| 3795 ||| 3796 ||| 
2019 ||| adaptive attention span in transformers. ||| 3797 ||| 3798 ||| 2265 ||| 1889 ||| 
2019 ||| semantically conditioned dialog response generation via hierarchical disentangled self-attention. ||| 3799 ||| 3800 ||| 784 ||| 3801 ||| 3802 ||| 
2019 ||| lattice transformer for speech translation. ||| 3803 ||| 3804 ||| 3470 ||| 3805 ||| 
2020 ||| mart: memory-augmented recurrent transformer for coherent video paragraph captioning. ||| 3806 ||| 3807 ||| 3172 ||| 3808 ||| 3809 ||| 3810 ||| 
2017 ||| an end-to-end model for question answering over knowledge base with cross-attention combining global knowledge. ||| 3811 ||| 3812 ||| 3129 ||| 3813 ||| 3814 ||| 3416 ||| 1418 ||| 
2020 ||| flat: chinese ner using flat-lattice transformer. ||| 3815 ||| 3816 ||| 3272 ||| 3273 ||| 
2021 ||| can the transformer learn nested recursion with symbol masking? ||| 3817 ||| 3818 ||| 3819 ||| 
2019 ||| attention guided graph convolutional networks for relation extraction. ||| 3820 ||| 2349 ||| 3131 ||| 
2018 ||| how much attention do you need? a granular analysis of neural machine translation architectures. ||| 3821 ||| 
2021 ||| reservoir transformers. ||| 3822 ||| 3823 ||| 3824 ||| 2596 ||| 3825 ||| 3826 ||| 
2020 ||| improving multimodal named entity recognition via entity span detection with unified multimodal transformer. ||| 3827 ||| 800 ||| 2884 ||| 3828 ||| 
2021 ||| dot: an efficient double transformer for nlp tasks with tables. ||| 3829 ||| 3830 ||| 3831 ||| 3832 ||| 
2021 ||| hit - a hierarchically fused deep attention network for robust code-mixed language representation. ||| 3833 ||| 3834 ||| 3835 ||| 3836 ||| 
2020 ||| hierarchical modeling for user personality prediction: the role of message-level attention. ||| 3837 ||| 3299 ||| 3699 ||| 
2021 ||| attention-based contextual language model adaptation for speech recognition. ||| 3838 ||| 3839 ||| 3840 ||| 3841 ||| 3842 ||| 3843 ||| 
2019 ||| analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be pruned. ||| 3844 ||| 3845 ||| 3846 ||| 3847 ||| 3848 ||| 
2021 ||| an exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers. ||| 3849 ||| 3850 ||| 3851 ||| 
2020 ||| roles and utilization of attention heads in transformer-based neural language models. ||| 3852 ||| 3853 ||| 
2021 ||| highlight-transformer: leveraging key phrase aware attention to improve abstractive multi-document summarization. ||| 3854 ||| 3855 ||| 3856 ||| 3857 ||| 
2019 ||| topic sensitive attention on generic corpora corrects sense bias in pretrained embeddings. ||| 3858 ||| 3859 ||| 3860 ||| 
2019 ||| the convex hull of finitely generable subsets and its predicate transformer. ||| 3861 ||| 3862 ||| 3369 ||| 3863 ||| 
2020 ||| channel-wise attention and channel combination for knowledge distillation. ||| 3864 ||| 3865 ||| 
2018 ||| abstractive summarization by neural attention model with document content memory. ||| 3866 ||| 3867 ||| 3868 ||| 
2020 ||| semantic classification of emf-related literature using deep learning models with attention mechanism. ||| 3869 ||| 3870 ||| 3871 ||| 3872 ||| 
2020 ||| spiking neural network transformer for deploying into a deep learning framework. ||| 3864 ||| 3865 ||| 
2021 ||| ast-transformer: encoding abstract syntax trees efficiently for code summarization. ||| 3873 ||| 3874 ||| 3875 ||| 3876 ||| 3877 ||| 382 ||| 
2017 ||| fib: squeezing loop invariants by interpolation between forward/backward predicate transformers. ||| 3878 ||| 3879 ||| 3880 ||| 1305 ||| 3881 ||| 3882 ||| 3883 ||| 
2021 ||| thinking like a developer? comparing the attention of humans with neural models of code. ||| 3884 ||| 3885 ||| 
2021 ||| javabert: training a transformer-based model for the java programming language. ||| 3886 ||| 3887 ||| 
2020 ||| detecting and explaining self-admitted technical debts with attention-based neural networks. ||| 398 ||| 3888 ||| 144 ||| 3889 ||| 2530 ||| 3890 ||| 
2019 ||| multi-modal attention network learning for semantic source code retrieval. ||| 3891 ||| 3892 ||| 3893 ||| 3894 ||| 1306 ||| 1236 ||| 1094 ||| 
2019 ||| autofocus: interpreting attention-based neural networks by code perturbation. ||| 3895 ||| 3896 ||| 3897 ||| 
2021 ||| to pay or not to pay attention: classifying and interpreting visual selective attention frequency features. ||| 3898 ||| 3899 ||| 3900 ||| 3901 ||| 
2021 ||| attention actor-critic algorithm for multi-agent constrained co-operative reinforcement learning. ||| 3902 ||| 3903 ||| 3904 ||| 3905 ||| 
2020 ||| an interpretable multimodal visual question answering system using attention-based weighted contextual features. ||| 3906 ||| 78 ||| 79 ||| 
2020 ||| anchor attention for hybrid crowd forecasts aggregation. ||| 3907 ||| 3369 ||| 3908 ||| 3909 ||| 3910 ||| 3911 ||| 
2019 ||| active attention-modified policy shaping: socially interactive agents track. ||| 3912 ||| 3913 ||| 3914 ||| 3915 ||| 3916 ||| 
2019 ||| modelling the dynamic joint policy of teammates with attention multi-agent ddpg. ||| 3917 ||| 3918 ||| 3919 ||| 3920 ||| 
2021 ||| self-attention meta-learner for continual learning. ||| 3921 ||| 3922 ||| 3923 ||| 
2019 ||| attention-based deep reinforcement learning for multi-view environments. ||| 3924 ||| 3925 ||| 3926 ||| 
2021 ||| multi-agent graph-attention communication and teaming. ||| 3927 ||| 3928 ||| 3929 ||| 
2018 ||| attention-aware generative adversarial networks (ata-gans). ||| 3930 ||| 3931 ||| 3932 ||| 3933 ||| 3934 ||| 
2021 ||| retrieval-augmented transformer-xl for close-domain dialog generation. ||| 3935 ||| 3936 ||| 3937 ||| 3938 ||| 
2020 ||| emptransfo: a multi-head transformer architecture for creating empathetic dialog systems. ||| 3939 ||| 3940 ||| 
2021 ||| modeling age of acquisition norms using transformer networks. ||| 3941 ||| 3942 ||| 
2019 ||| visual attention model for cross-sectional stock return prediction and end-to-end multimodal market representation learning. ||| 3943 ||| 3944 ||| 3945 ||| 3946 ||| 3947 ||| 3948 ||| 
2018 ||| self-attention for synopsis-based multi-label movie genre classification. ||| 228 ||| 3949 ||| 229 ||| 
2019 ||| different flavors of attention networks for argument mining. ||| 3950 ||| 3951 ||| 3952 ||| 3953 ||| 
2020 ||| attend to the beginning: a study on bidirectional attention for extractive summarization. ||| 3954 ||| 3955 ||| 
2019 ||| opinion spam detection with attention-based neural networks. ||| 3956 ||| 3957 ||| 3958 ||| 3959 ||| 
2017 ||| toward extractive summarization of online forum discussions via hierarchical attention networks. ||| 3960 ||| 523 ||| 3961 ||| 
2021 ||| towards improving open student answer assessment using pretrained transformers. ||| 3962 ||| 3963 ||| 3964 ||| 
2020 ||| attention based transformer for student answers assessment. ||| 3962 ||| 3963 ||| 
2021 ||| show me what you're looking for visualizing abstracted transformer attention for enhancing their local interpretability on time series data. ||| 3965 ||| 3966 ||| 
2021 ||| uhf rfid chip impedance and sensitivity measurement using a transmission line transformer. ||| 3967 ||| 3968 ||| 3969 ||| 3970 ||| 3971 ||| 
2021 ||| dualnet: locate then detect effective payload with deep attention network. ||| 3972 ||| 3973 ||| 3974 ||| 
2020 ||| 3d visualization of temporal data: exploring visual attention and machine learning. ||| 3975 ||| 3976 ||| 3977 ||| 3978 ||| 504 ||| 3979 ||| 
2020 ||| escape room virtual reality: a tool for diagnosis and treatment of attention deficit disorder. ||| 3369 ||| 3980 ||| 3981 ||| 3982 ||| 
2017 ||| cognitive investigation on pilot attention during take-offs and landings using flight simulator. ||| 3983 ||| 3984 ||| 3985 ||| 
2020 ||| pre-training polish transformer-based language models at scale. ||| 3986 ||| 3987 ||| 3988 ||| 
2019 ||| text language identification using attention-based recurrent neural networks. ||| 3987 ||| 3988 ||| 
2021 ||| cagu-net: category attention guidance u-net for retinal blood vessel segmentation. ||| 3989 ||| 3990 ||| 3991 ||| 3992 ||| 3993 ||| 3994 ||| 
2021 ||| transfer learning and attention mechanism for breast cancer classification. ||| 3995 ||| 3996 ||| 3997 ||| 3998 ||| 3999 ||| 4000 ||| 
2020 ||| domain-specific chinese transformer-xl language model with part-of-speech information. ||| 4001 ||| 4002 ||| 398 ||| 
2019 ||| a self-attention-based approach for named entity recognition in cybersecurity. ||| 4003 ||| 4004 ||| 4005 ||| 
2019 ||| relation extraction via attention-based cnns using token-level representations. ||| 247 ||| 4006 ||| 4007 ||| 
2018 ||| siamese networks with discriminant correlation filters and channel attention. ||| 4008 ||| 4009 ||| 4010 ||| 
2019 ||| chinese ner by span-level self-attention. ||| 4011 ||| 4006 ||| 4007 ||| 
2021 ||| a sample-based training method for distantly supervised relation extraction with pre-trained transformers. ||| 4012 ||| 4013 ||| 4014 ||| 
2021 ||| bloomnet: a robust transformer based model for bloom's learning outcome classification. ||| 4015 ||| 4016 ||| 4017 ||| 4018 ||| 4019 ||| 4020 ||| 
2021 ||| arabic named entity recognition using transformer-based-crf model. ||| 4021 ||| 4022 ||| 
2020 ||| the driver's attention level. ||| 4023 ||| 4024 ||| 4025 ||| 
2020 ||| automatic polyp segmentation using channel-spatial attention with deep supervision. ||| 4026 ||| 2843 ||| 
2020 ||| emotion and themes recognition in music with convolutional and recurrent attention-blocks. ||| 4027 ||| 4028 ||| 4029 ||| 4030 ||| 648 ||| 649 ||| 
2019 ||| predicting media memorability using deep features with attention and recurrent network. ||| 4031 ||| 4032 ||| 4033 ||| 
2020 ||| transfer of knowledge: fine-tuning for polyp segmentation with attention. ||| 4034 ||| 
2020 ||| efficient supervision net: polyp segmentation using efficientnet and attention unit. ||| 4035 ||| 4036 ||| 
2019 ||| music theme recognition using cnn and self-attention. ||| 4037 ||| 4038 ||| 
2020 ||| a temporal-spatial attention model for medical image detection. ||| 4039 ||| 4040 ||| 4041 ||| 4042 ||| 4043 ||| 
2020 ||| predicting media memorability from a multimodal late fusion of self-attention and lstm models. ||| 4044 ||| 4045 ||| 4046 ||| 4047 ||| 4048 ||| 4049 ||| 4046 ||| 
2020 ||| emotion and theme recognition in music using attention-based methods. ||| 4030 ||| 4050 ||| 648 ||| 649 ||| 
2020 ||| recognizing music mood and theme using convolutional neural networks and attention. ||| 4051 ||| 4052 ||| 4053 ||| 
2020 ||| automatic polyp segmentation via parallel reverse attention network. ||| 4054 ||| 1861 ||| 614 ||| 4055 ||| 4056 ||| 1932 ||| 
2018 ||| double attention mechanism for sentence embedding. ||| 4057 ||| 4058 ||| 4059 ||| 
2021 ||| named entity recognition of bert-bilstm-crf combined with self-attention. ||| 4060 ||| 2332 ||| 4061 ||| 4062 ||| 
2021 ||| graph-encoder and multi-decoders solution framework with multi-attention. ||| 4063 ||| 4064 ||| 4065 ||| 4066 ||| 4067 ||| 
2020 ||| collaborative filtering: graph neural network with attention. ||| 4068 ||| 4069 ||| 
2021 ||| entity alignment of knowledge graph by joint graph attention and translation representation. ||| 4070 ||| 4071 ||| 4072 ||| 4073 ||| 4067 ||| 
2017 ||| the influence of the attention decay in an information spreading model. ||| 4074 ||| 4075 ||| 4076 ||| 4077 ||| 
2019 ||| a sequence-to-sequence text summarization model with topic based attention mechanism. ||| 4078 ||| 4079 ||| 4080 ||| 
2020 ||| cross-language generative automatic summarization based on attention mechanism. ||| 4081 ||| 4082 ||| 4083 ||| 4084 ||| 
2020 ||| explainable enterprise rating using attention based convolutional neural network. ||| 2810 ||| 4085 ||| 4086 ||| 
2018 ||| modularized and attention-based recurrent convolutional neural network for automatic academic paper aspect scoring. ||| 4087 ||| 4062 ||| 4088 ||| 
2021 ||| the code generation method based on gated attention and interaction-lstm. ||| 4089 ||| 4090 ||| 
2021 ||| robust graph collaborative filtering algorithm based on hierarchical attention. ||| 4091 ||| 4092 ||| 4093 ||| 4094 ||| 4095 ||| 
2021 ||| egat: edge-featured graph attention network. ||| 4096 ||| 1785 ||| 4097 ||| 
2019 ||| targeted sentiment classification with attentional encoder network. ||| 4098 ||| 4099 ||| 4100 ||| 4101 ||| 4102 ||| 
2018 ||| a multi-level attention model for text matching. ||| 4103 ||| 4104 ||| 
2019 ||| attention and edge memory convolution for bioactivity prediction. ||| 4105 ||| 4106 ||| 504 ||| 4107 ||| 4108 ||| 
2020 ||| attention based mechanism for load time series forecasting: an-lstm. ||| 4109 ||| 
2021 ||| interpretable visual understanding with cognitive attention network. ||| 4110 ||| 4111 ||| 4112 ||| 4113 ||| 4114 ||| 4115 ||| 4116 ||| 
2019 ||| a recurrent attention network for judgment prediction. ||| 4117 ||| 822 ||| 241 ||| 4118 ||| 4119 ||| 
2019 ||| signed graph attention networks. ||| 4120 ||| 4121 ||| 4122 ||| 1445 ||| 
2019 ||| a transformer model for retrosynthesis. ||| 4123 ||| 4124 ||| 4125 ||| 
2019 ||| spatial attention network for few-shot learning. ||| 4126 ||| 4127 ||| 4128 ||| 4129 ||| 
2021 ||| gattanet: global attention agreement for convolutional neural networks. ||| 4130 ||| 4131 ||| 
2021 ||| attention-based 3d neural architectures for predicting cracks in designs. ||| 4132 ||| 4133 ||| 4134 ||| 4135 ||| 4136 ||| 
2021 ||| daema: denoising autoencoder with mask attention. ||| 4137 ||| 4138 ||| 4139 ||| 4140 ||| 4141 ||| 
2021 ||| tstnet: a sequence to sequence transformer network for spatial-temporal traffic prediction. ||| 4142 ||| 4143 ||| 4144 ||| 
2019 ||| hierarchical attentional hybrid neural networks for document classification. ||| 4145 ||| 4146 ||| 280 ||| 281 ||| 282 ||| 
2020 ||| cabin: a novel cooperative attention based location prediction network using internal-external trajectory dependencies. ||| 4147 ||| 2355 ||| 4148 ||| 4149 ||| 4150 ||| 1223 ||| 
2017 ||| instance-adaptive attention mechanism for relation classification. ||| 4151 ||| 4152 ||| 785 ||| 
2021 ||| attention-based bi-lstm for anomaly detection on time-series data. ||| 4153 ||| 4154 ||| 4155 ||| 4156 ||| 
2021 ||| stgatp: a spatio-temporal graph attention network for long-term traffic prediction. ||| 4157 ||| 4158 ||| 4159 ||| 
2019 ||| an attention-based id-cnns-crf model for named entity recognition on clinical electronic medical records. ||| 4160 ||| 4161 ||| 4162 ||| 4163 ||| 
2019 ||| surrounding-based attention networks for aspect-level sentiment classification. ||| 4164 ||| 4165 ||| 4166 ||| 696 ||| 697 ||| 
2021 ||| tinet: multi-dimensional traffic data imputation via transformer network. ||| 4142 ||| 4167 ||| 4168 ||| 
2019 ||| lstm with uniqueness attention for human activity recognition. ||| 4169 ||| 4170 ||| 4171 ||| 917 ||| 4172 ||| 
2019 ||| improving deep image clustering with spatial transformer layers. ||| 4173 ||| 282 ||| 
2021 ||| spatial-temporal traffic data imputation via graph attention convolutional network. ||| 4167 ||| 4174 ||| 4168 ||| 
2020 ||| more attentional local descriptors for few-shot learning. ||| 4175 ||| 1041 ||| 4176 ||| 
2021 ||| entity-aware biaffine attention for constituent parsing. ||| 4177 ||| 4178 ||| 586 ||| 398 ||| 589 ||| 
2019 ||| referring expression comprehension via co-attention and visual context. ||| 4179 ||| 4180 ||| 4181 ||| 4182 ||| 4183 ||| 
2021 ||| knowledge graph enhanced transformer for generative question answering tasks. ||| 4184 ||| 4185 ||| 4186 ||| 
2018 ||| hierarchical attention networks for user profile inference in social media systems. ||| 4187 ||| 4188 ||| 987 ||| 4189 ||| 4190 ||| 4191 ||| 
2019 ||| a reinforcement learning approach for sequential spatial transformer networks. ||| 4192 ||| 4193 ||| 4194 ||| 4195 ||| 4196 ||| 
2021 ||| attention-based multi-view feature fusion for cross-domain recommendation. ||| 4197 ||| 4198 ||| 4199 ||| 1717 ||| 4200 ||| 4201 ||| 
2019 ||| towards attention based vulnerability discovery using source code representation. ||| 4202 ||| 4203 ||| 4204 ||| 
2019 ||| revising attention with position for aspect-level sentiment classification. ||| 952 ||| 759 ||| 379 ||| 
2020 ||| multi-scale cross-modal spatial attention fusion for multi-label image recognition. ||| 4205 ||| 4206 ||| 4207 ||| 4208 ||| 
2017 ||| attention focused spatial pyramid pooling for boxless action recognition in still images. ||| 4209 ||| 586 ||| 590 ||| 589 ||| 
2017 ||| attention aware semi-supervised framework for sentiment analysis. ||| 4210 ||| 4211 ||| 4212 ||| 4213 ||| 4214 ||| 
2019 ||| a label-specific attention-based network with regularized loss for multi-label classification. ||| 4215 ||| 4216 ||| 4217 ||| 4218 ||| 459 ||| 
2019 ||| heterogeneous information network embedding with meta-path based graph attention networks. ||| 455 ||| 456 ||| 458 ||| 459 ||| 
2019 ||| the same size dilated attention network for keypoint detection. ||| 4219 ||| 4220 ||| 4221 ||| 
2019 ||| scaffolding haptic attention with controller gating. ||| 4222 ||| 4223 ||| 4224 ||| 
2019 ||| attention-based improved blstm-cnn for relation classification. ||| 4161 ||| 4160 ||| 4162 ||| 4225 ||| 
2021 ||| an attention module for convolutional neural networks. ||| 4226 ||| 4227 ||| 4228 ||| 4229 ||| 
2019 ||| attentional residual dense factorized network for real-time semantic segmentation. ||| 4230 ||| 588 ||| 586 ||| 590 ||| 589 ||| 
2020 ||| adversarial defense via attention-based randomized smoothing. ||| 4231 ||| 4232 ||| 369 ||| 4233 ||| 4234 ||| 
2018 ||| attention-based rnn model for joint extraction of intent and word slot based on a tagging strategy. ||| 4235 ||| 4236 ||| 987 ||| 4190 ||| 986 ||| 4237 ||| 
2020 ||| face anti-spoofing with a noise-attention network using color-channel difference images. ||| 4238 ||| 2850 ||| 2848 ||| 4239 ||| 2852 ||| 
2020 ||| bilinear fusion of commonsense knowledge with attention-based nli models. ||| 4240 ||| 4241 ||| 914 ||| 4242 ||| 
2018 ||| attention enhanced chinese word embeddings. ||| 4243 ||| 4244 ||| 4245 ||| 4246 ||| 3285 ||| 
2021 ||| covit-gan: vision transformer forcovid-19 detection in ct scan imageswith self-attention gan fordataaugmentation. ||| 4247 ||| 4248 ||| 4249 ||| 
2019 ||| capsule networks for attention under occlusion. ||| 4250 ||| 4251 ||| 4252 ||| 4253 ||| 
2019 ||| capturing user and product information for sentiment classification via hierarchical separated attention and neural collaborative filtering. ||| 4254 ||| 969 ||| 4255 ||| 
2018 ||| ynamic multi-agent cooperation via attentional communication model. ||| 4256 ||| 4257 ||| 4258 ||| 4259 ||| 
2019 ||| hybrid attention driven text-to-image synthesis via generative adversarial networks. ||| 4260 ||| 737 ||| 
2021 ||| dvamn: dual visual attention matching network for zero-shot action recognition. ||| 4261 ||| 259 ||| 4262 ||| 4263 ||| 
2020 ||| cross-domain sentiment classification using topic attention and dual-task adversarial training. ||| 4264 ||| 4265 ||| 3015 ||| 
2019 ||| self-attention stargan for multi-domain image-to-image translation. ||| 4266 ||| 4267 ||| 4268 ||| 4269 ||| 3477 ||| 4270 ||| 
2019 ||| collaborative attention network with word and n-gram sequences modeling for sentiment classification. ||| 4271 ||| 1166 ||| 4272 ||| 
2018 ||| design of 65-nm cmos transformer-based impedance matching for lte power amplifier applications. ||| 4273 ||| 4274 ||| 4275 ||| 4276 ||| 
2018 ||| compact transformerless k-band pa with more than 33% pae and 14.8 dbm output power in 65 nm bulk cmos. ||| 4277 ||| 4278 ||| 318 ||| 
2018 ||| on 60 ghz solid-state transformers designed in 65 nm cmos technology. ||| 4279 ||| 
2020 ||| divide-by-2 injection-locked frequency divider using 3-path transformer-coupled resonator. ||| 1467 ||| 1468 ||| 4280 ||| 4281 ||| 
2019 ||| multi-level stereo attention model for center channel extraction. ||| 4282 ||| 4283 ||| 4284 ||| 
2019 ||| hierarchically channel-wise attention model for clean and polluted water images classification. ||| 4285 ||| 4286 ||| 4287 ||| 
2019 ||| coattention-based recurrent neural networks for sentiment analysis of chinese texts. ||| 4288 ||| 4289 ||| 4290 ||| 4291 ||| 4292 ||| 4293 ||| 1717 ||| 
2021 ||| chinese fine-grained sentiment classification based on pre-trained language model and attention mechanism. ||| 4294 ||| 875 ||| 4295 ||| 
2017 ||| attention-aware path-based relation extraction for medical knowledge graph. ||| 4296 ||| 4297 ||| 4298 ||| 4299 ||| 1082 ||| 
2021 ||| real-time prediction of ocean observation data based on transformer model. ||| 4300 ||| 2008 ||| 4301 ||| 4302 ||| 4303 ||| 4304 ||| 
2021 ||| intelligent detection of marine organisms with deep learning based on attention mechanism. ||| 4305 ||| 4306 ||| 4303 ||| 4307 ||| 
2021 ||| d-rex: static detection of relevant runtime exceptions with location aware transformer. ||| 4308 ||| 4309 ||| 4310 ||| 4311 ||| 4312 ||| 
2019 ||| power system upgrade planning with on-load tap-changing transformers, switchable topology and operating policies. ||| 4313 ||| 4314 ||| 
2021 ||| indonesia's fake news detection using transformer network. ||| 4315 ||| 4316 ||| 4317 ||| 4318 ||| 
2021 ||| a study of english-indonesian neural machine translation with attention (seq2seq, convseq2seq, rnn, and mha): a comparative study of nmt on english-indonesian. ||| 4319 ||| 
2021 ||| research on copper foil winding of transformer surface defect detection based on machine vision. ||| 4320 ||| 4321 ||| 
2019 ||| design of electronic current transformer power supply system. ||| 4322 ||| 4323 ||| 4324 ||| 
2021 ||| research on transformer fault diagnosis based on sparrow algorithm optimization probabilistic neural network. ||| 4325 ||| 4326 ||| 4327 ||| 
2021 ||| abstracting local transformer attention for enhancing interpretability on time series data. ||| 3965 ||| 3966 ||| 
2017 ||| auto-transformer-based power amplifier with totem-pole driver. ||| 4328 ||| 4329 ||| 4330 ||| 
2021 ||| robustness of smart transformer based-on sliding mode controller under grid perturbations. ||| 4331 ||| 4332 ||| 
2018 ||| optimum design approach of high frequency transformer: including the effects of eddy currents. ||| 4333 ||| 
2021 ||| review on solid state transformer based on microgrids architectures. ||| 4334 ||| 4335 ||| 
2021 ||| backstepping controller design for the medium and low voltage stages of smart transformer. ||| 4331 ||| 4332 ||| 
2021 ||| study of spectral response of transformer oil under low electrical discharge and thermal stress. ||| 4336 ||| 4337 ||| 4338 ||| 4339 ||| 
2018 ||| dc transformer for optimal power flow with interior point algorithm. ||| 4340 ||| 4341 ||| 
2018 ||| improved performance of piezoelectric transformer mode thickness by nonlinear methods. ||| 4342 ||| 4343 ||| 4344 ||| 4345 ||| 
2020 ||| validation of emotions as a measure of selective attention in children with autism spectrum disorder. ||| 4346 ||| 4347 ||| 4348 ||| 
2020 ||| critical thinking development in collaborative learning: case study of transformer topic through lesson study at junior high school. ||| 4349 ||| 4350 ||| 4351 ||| 
2020 ||| faster r-cnn with attention feature map for robust object detection. ||| 4352 ||| 4353 ||| 
2021 ||| saliency prediction with relation-aware global attention module. ||| 4354 ||| 4353 ||| 
2021 ||| efficient spatial-attention module for human pose estimation. ||| 4355 ||| 4356 ||| 4357 ||| 4353 ||| 
2019 ||| analysing coreference in transformer outputs. ||| 4358 ||| 4359 ||| 4360 ||| 3207 ||| 
2021 ||| attention edgeconv for 3d point cloud classification. ||| 4361 ||| 4362 ||| 4363 ||| 4364 ||| 
2021 ||| semi-supervised sound event detection using self-attention and multiple techniques of consistency training. ||| 4365 ||| 4366 ||| 4367 ||| 4368 ||| 
2019 ||| phonetic-attention scoring for deep speaker features in speaker verification. ||| 4369 ||| 4370 ||| 4371 ||| 952 ||| 
2017 ||| music thumbnailing via neural attention modeling of music emotion. ||| 4372 ||| 4373 ||| 4374 ||| 
2021 ||| dual-path transformer for machine condition monitoring. ||| 4375 ||| 4376 ||| 4377 ||| 
2020 ||| speaker verification system based on deformable cnn and time-frequency attention. ||| 4134 ||| 304 ||| 1484 ||| 
2021 ||| attention-based multi-channel speaker verification with ad-hoc microphone arrays. ||| 4378 ||| 4379 ||| 4380 ||| 4381 ||| 
2021 ||| cyclegan-based non-parallel speech enhancement with an adaptive attention-in-attention mechanism. ||| 4382 ||| 4383 ||| 4384 ||| 1341 ||| 4385 ||| 
2021 ||| ensemble of one model: creating model variations for transformer with layer permutation. ||| 4386 ||| 4387 ||| 4388 ||| 
2020 ||| module comparison of transformer-tts for speaker adaptation based on fine-tuning. ||| 4389 ||| 4390 ||| 4391 ||| 
2020 ||| class attention network for semantic segmentation of remote sensing images. ||| 4392 ||| 4393 ||| 2240 ||| 
2019 ||| bidirectional temporal convolution with self-attention network for ctc-based acoustic modeling. ||| 4394 ||| 4395 ||| 4396 ||| 4397 ||| 
2017 ||| visual attention guided eye movements for 360 degree images. ||| 4398 ||| 4399 ||| 4400 ||| 
2019 ||| classification of causes of speech recognition errors using attention-based bidirectional long short-term memory and modulation spectrum. ||| 4401 ||| 4402 ||| 4403 ||| 
2021 ||| non-parallel voice conversion with generative attentional networks. ||| 4404 ||| 4405 ||| 4406 ||| 
2019 ||| disfluency detection based on speech-aware token-by-token sequence labeling with blstm-crfs and attention mechanisms. ||| 4407 ||| 4408 ||| 4409 ||| 4410 ||| 4411 ||| 
2021 ||| effect of visual attention and driving experiences on the event-related potential p300 in the perception of traffic scenes. ||| 4412 ||| 4413 ||| 4414 ||| 4415 ||| 
2019 ||| multi-lingual transformer training for khmer automatic speech recognition. ||| 4416 ||| 4417 ||| 4418 ||| 4419 ||| 
2019 ||| towards generation of visual attention map for source code. ||| 4420 ||| 4421 ||| 4422 ||| 4423 ||| 4424 ||| 4425 ||| 4426 ||| 4427 ||| 
2020 ||| context-adaptive gaussian attention for text-independent speaker verification. ||| 4428 ||| 4429 ||| 789 ||| 4430 ||| 
2021 ||| improvement of spatial ambiguity in multi-channel speech separation using channel attention. ||| 4431 ||| 4388 ||| 4432 ||| 4433 ||| 
2018 ||| online speaker adaptation for lvcsr based on attention mechanism. ||| 4434 ||| 4435 ||| 4436 ||| 1010 ||| 4437 ||| 4438 ||| 
2021 ||| positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition. ||| 4439 ||| 4440 ||| 3890 ||| 4441 ||| 
2020 ||| adversarial training using inter/intra-attention architecture for speech enhancement network. ||| 4442 ||| 4443 ||| 
2018 ||| user's intention understanding in question-answering system using attention-based lstm. ||| 4444 ||| 4445 ||| 4446 ||| 
2019 ||| image compression with deeper learned transformer. ||| 4447 ||| 4448 ||| 4449 ||| 
2021 ||| an attention based expert inspection system for smart scalp. ||| 4450 ||| 4451 ||| 4452 ||| 
2020 ||| speaker-invariant psychological stress detection using attention-based network. ||| 4453 ||| 4454 ||| 4455 ||| 4456 ||| 
2019 ||| learning contextual representation with convolution bank and multi-head self-attention for speech emphasis detection. ||| 4457 ||| 3138 ||| 4458 ||| 4459 ||| 4460 ||| 
2020 ||| attentive fusion enhanced audio-visual encoding for transformer based robust speech recognition. ||| 4461 ||| 1134 ||| 4462 ||| 4463 ||| 
2021 ||| comparison of low complexity self-attention mechanisms for acoustic event detection. ||| 4464 ||| 4465 ||| 
2021 ||| end-to-end speaker age and height estimation using attention mechanism and triplet loss. ||| 4466 ||| 4467 ||| 4468 ||| 4469 ||| 
2019 ||| dynamic attention loss for small-sample image classification. ||| 4470 ||| 4471 ||| 1482 ||| 539 ||| 1484 ||| 
2020 ||| rate-distortion optimization for 360-degree image considering visual attention. ||| 4472 ||| 4473 ||| 4474 ||| 
2021 ||| image captioning based on an improved transformer with iou position encoding. ||| 4475 ||| 4476 ||| 4477 ||| 4478 ||| 1484 ||| 
2021 ||| ca-vc: a novel zero-shot voice conversion method with channel attention. ||| 4479 ||| 4480 ||| 4481 ||| 4482 ||| 
2021 ||| efficient conformer-based speech recognition with linear attention. ||| 4483 ||| 4484 ||| 4381 ||| 
2019 ||| end-to-end tibetan ando dialect speech recognition based on hybrid ctc/attention architecture. ||| 4485 ||| 4486 ||| 4487 ||| 4488 ||| 
2018 ||| attention based fully convolutional network for speech emotion recognition. ||| 1008 ||| 1010 ||| 1009 ||| 4489 ||| 4490 ||| 
2021 ||| speech enhancement network with unsupervised attention using invariant information clustering. ||| 4442 ||| 4491 ||| 4443 ||| 
2019 ||| voice conversion by dual-domain bidirectional long short-term memory networks with temporal attention. ||| 4492 ||| 4493 ||| 4494 ||| 
2021 ||| an empirical study on transformer-based end-to-end speech recognition with novel decoder masking. ||| 4495 ||| 4496 ||| 4497 ||| 
2019 ||| dynamic-attention based encoder-decoder model for speaker extraction with anchor speech. ||| 1705 ||| 4498 ||| 4499 ||| 
2019 ||| convolutional attention model for retinal edema segmentation. ||| 4500 ||| 4501 ||| 4502 ||| 
2021 ||| a study on virtual reality sickness and visual attention. ||| 4503 ||| 4504 ||| 4505 ||| 4506 ||| 
2020 ||| temporal attention feature encoding for video captioning. ||| 4507 ||| 4508 ||| 4509 ||| 
2020 ||| supportive and self attentions for image caption. ||| 1488 ||| 1489 ||| 
2021 ||| a self-attention-based ensemble convolution neural network approach for sleep stage classification with merged spectrogram. ||| 4510 ||| 4511 ||| 4512 ||| 
2019 ||| mixed attention mechanism for small-sample fine-grained image classification. ||| 539 ||| 4513 ||| 1482 ||| 4514 ||| 1484 ||| 4470 ||| 
2020 ||| self-attention for multi-channel speech separation in noisy and reverberant environments. ||| 4515 ||| 4516 ||| 
2021 ||| real-time edge attention-based learning for low-light one-stage object detection. ||| 4517 ||| 4518 ||| 4519 ||| 
2021 ||| rethinking singing voice separation with spectral- temporal transformer. ||| 4520 ||| 4521 ||| 4522 ||| 4523 ||| 
2020 ||| an improved method for instantaneous frequency estimation using a finite order hilbert transformer. ||| 4524 ||| 4525 ||| 4526 ||| 4527 ||| 
2019 ||| prosodic structure prediction using deep self-attention neural network. ||| 4528 ||| 3138 ||| 4529 ||| 4530 ||| 3808 ||| 4460 ||| 
2021 ||| an investigation of enhancing ctc model for triggered attention-based streaming asr. ||| 4531 ||| 4532 ||| 4533 ||| 4534 ||| 
2020 ||| efficient diverse response generation in attention-based neural conversational model with maximum mutual information. ||| 4535 ||| 4536 ||| 4537 ||| 4538 ||| 4539 ||| 
2021 ||| self-supervised visual transformers for breast cancer diagnosis. ||| 4540 ||| 4541 ||| 4542 ||| 4543 ||| 4544 ||| 
2018 ||| mmann: multimodal multilevel attention neural network for horror clip detection. ||| 4545 ||| 4546 ||| 4547 ||| 4548 ||| 
2021 ||| separable temporal convolution plus temporally pooled attention for lightweight high-performance keyword spotting. ||| 4549 ||| 4550 ||| 4551 ||| 4552 ||| 
2018 ||| hierarchical attention networks for different types of documents with smaller size of datasets. ||| 4553 ||| 4554 ||| 4555 ||| 4556 ||| 
2019 ||| attention neural networks for pan-tilt-zoom control with active hand-off. ||| 4557 ||| 4558 ||| 
2017 ||| expanding the scope of learning analytics data: preliminary findings on attention and self-regulation using wearable technology. ||| 4559 ||| 4560 ||| 4561 ||| 
2021 ||| are you with me? measurement of learners' video-watching attention with eye tracking. ||| 4562 ||| 4563 ||| 4564 ||| 4565 ||| 4566 ||| 4567 ||| 4568 ||| 4569 ||| 
2020 ||| visualization of focal cues for visuomotor coordination by gradient-based methods: a recurrent neural network shifts the attention depending on task requirements. ||| 4570 ||| 4571 ||| 4572 ||| 4573 ||| 4574 ||| 
2018 ||| more attention and less repetitive and stereotyped behaviors using a robot with children with autism. ||| 4575 ||| 4576 ||| 4577 ||| 4578 ||| 4579 ||| 4580 ||| 4581 ||| 4582 ||| 
2019 ||| surprise! predicting infant visual attention in a socially assistive robot contingent learning paradigm. ||| 4583 ||| 4584 ||| 4585 ||| 4586 ||| 4587 ||| 4588 ||| 
2021 ||| designing interface aids to assist collaborative robot operators in attention management. ||| 4589 ||| 4590 ||| 4591 ||| 
2017 ||| sociable driving agents to maintain driver's attention in autonomous driving. ||| 4592 ||| 4593 ||| 4594 ||| 4595 ||| 4596 ||| 4597 ||| 
2019 ||| verbal explanations for deep reinforcement learning neural networks with attention on extracted features. ||| 4598 ||| 4599 ||| 4600 ||| 4601 ||| 4602 ||| 
2021 ||| attention deep learning based model for predicting the 3d human body pose using the robot human handover phases. ||| 4603 ||| 4604 ||| 4605 ||| 4606 ||| 
2019 ||| smak-net: self-supervised multi-level spatial attention network for knowledge representation towards imitation learning. ||| 4607 ||| 4608 ||| 4609 ||| 4610 ||| 4611 ||| 
2018 ||| predicting response to joint attention performance in human-human interaction based on human-robot interaction for young children with autism spectrum disorder. ||| 4612 ||| 4613 ||| 4614 ||| 4615 ||| 4616 ||| 4617 ||| 4618 ||| 
2021 ||| attention distribution graph: visualizing student's attention transition in error-finding tasks. ||| 2883 ||| 683 ||| 4619 ||| 4620 ||| 4621 ||| 
2021 ||| medical assistant diagnosis method based on graph neural network and attention mechanism. ||| 4622 ||| 4623 ||| 4624 ||| 
2017 ||| eeg analysis of brain activity in attention deficit hyperactivity disorder during an attention task. ||| 4625 ||| 4626 ||| 4627 ||| 4628 ||| 4629 ||| 4630 ||| 
2021 ||| semantic-guided high-order region attention embedding for zero-shot learning. ||| 3248 ||| 2331 ||| 4631 ||| 
2017 ||| fast recognition of human climbing fences in transformer substations. ||| 4632 ||| 4633 ||| 4634 ||| 4635 ||| 4636 ||| 2989 ||| 4637 ||| 
2018 ||| chinese metaphor sentiment analysis based on attention-based lstm. ||| 4638 ||| 4639 ||| 4640 ||| 
2020 ||| robust finger vein recognition based on deep cnn with spatial attention and bias field correction. ||| 4641 ||| 4642 ||| 
2018 ||| sentence representation and classification using attention and additional language information. ||| 4643 ||| 241 ||| 4644 ||| 
2019 ||| transformer fault on-line diagnosis system. ||| 4645 ||| 1160 ||| 4646 ||| 
2018 ||| history attention for source-target alignment in neural machine translation. ||| 4647 ||| 4648 ||| 4649 ||| 4650 ||| 
2021 ||| automatic fetus head segmentation in ultrasound images by attention based encoder decoder network. ||| 4651 ||| 4652 ||| 4653 ||| 4654 ||| 
2020 ||| an approach for bengali automatic question answering system using attention mechanism. ||| 4655 ||| 4656 ||| 4657 ||| 4658 ||| 4659 ||| 
2020 ||| an attention based approach for sentiment analysis of food review dataset. ||| 4655 ||| 4660 ||| 4661 ||| 4662 ||| 4658 ||| 
2021 ||| image caption generator using attention mechanism. ||| 4663 ||| 4664 ||| 4665 ||| 4666 ||| 
2021 ||| understanding and evaluating commonsense reasoning in transformer-based architectures. ||| 4667 ||| 4668 ||| 4669 ||| 
2021 ||| clustering text using attention. ||| 4670 ||| 
2021 ||| an attention on sentiment analysis of child abusive public comments towards bangla text and ml. ||| 4671 ||| 4672 ||| 4673 ||| 4674 ||| 4675 ||| 
2018 ||| bandwidth enhancement of dual band impedance transformer with transmission zero. ||| 4676 ||| 4677 ||| 4678 ||| 
2019 ||| virtual reality based avatar-mediated joint attention task for children with autism: implication on performance and physiology. ||| 4679 ||| 4680 ||| 4681 ||| 
2020 ||| driver inattention monitoring system based on the orientation of the face using convolutional neural network. ||| 4682 ||| 4683 ||| 4684 ||| 
2021 ||| loss optimised video captioning using deep-lstm, attention mechanism and weighted loss metrices. ||| 4685 ||| 4686 ||| 
2020 ||| csta-2p1d unet: consecutive spatio-temporal attention for multi-scale 3d pancreas segmentation. ||| 4687 ||| 4688 ||| 4689 ||| 4690 ||| 
2021 ||| attention based image captioning using depth-wise separable convolution. ||| 4691 ||| 4686 ||| 
2017 ||| demo: radio rate transformer: a portable vhf/uhf and gigahertz radio combo providing broadband ad-hoc networking services to mobile vehicles. ||| 4692 ||| 
2019 ||| worldly eyes on video: learnt vs. reactive deployment of attention to dynamic stimuli. ||| 4693 ||| 4694 ||| 4695 ||| 4696 ||| 
2019 ||| video-based convolutional attention for person re-identification. ||| 4697 ||| 4698 ||| 4699 ||| 4700 ||| 4701 ||| 4702 ||| 4703 ||| 4704 ||| 
2019 ||| image memorability using diverse visual features and soft attention. ||| 4705 ||| 4706 ||| 4707 ||| 4708 ||| 4709 ||| 4710 ||| 4711 ||| 
2017 ||| exploiting visual saliency algorithms for object-based attention: a new color and scale-based approach. ||| 4712 ||| 4713 ||| 4714 ||| 
2019 ||| a sequence-to-sequence transformer premised temporal convolutional network for chinese word segmentation. ||| 1706 ||| 4715 ||| 4716 ||| 
2020 ||| cross-database micro expression recognition based on apex frame optical flow and multi-head self-attention. ||| 4717 ||| 4718 ||| 4719 ||| 4720 ||| 4721 ||| 4722 ||| 
2020 ||| a novel attention model of deep learning in image classification. ||| 4723 ||| 4724 ||| 4725 ||| 4726 ||| 185 ||| 
2021 ||| integrated training for sequence-to-sequence models using non-autoregressive transformer. ||| 4727 ||| 4728 ||| 3732 ||| 4729 ||| 4730 ||| 4731 ||| 3454 ||| 
2019 ||| how transformer revitalizes character-based neural machine translation: an investigation on japanese-vietnamese translation systems. ||| 4732 ||| 4733 ||| 4734 ||| 4735 ||| 
2019 ||| controlling utterance length in nmt-based word segmentation with attention. ||| 4736 ||| 3510 ||| 1226 ||| 4737 ||| 
2021 ||| multilingual speech translation with unified transformer: huawei noah's ark lab at iwslt 2021. ||| 3634 ||| 3635 ||| 3443 ||| 
2019 ||| transformers without tears: improving the normalization of self-attention. ||| 4738 ||| 4739 ||| 
2020 ||| efficient automatic punctuation restoration using bidirectional transformers with robust inference. ||| 4740 ||| 4741 ||| 4742 ||| 
2019 ||| transformer-based cascaded multimodal speech translation. ||| 4743 ||| 4744 ||| 72 ||| 4745 ||| 4746 ||| 
2018 ||| new immersive media to broaden attention and awareness. ||| 4747 ||| 4748 ||| 4749 ||| 4750 ||| 
2021 ||| efficient attentions for long document summarization. ||| 4751 ||| 4752 ||| 4753 ||| 3403 ||| 4754 ||| 
2019 ||| convolutional self-attention networks. ||| 3037 ||| 3038 ||| 3039 ||| 3040 ||| 3041 ||| 
2021 ||| enriching transformers with structured tensor-product representations for abstractive summarization. ||| 4755 ||| 3539 ||| 4756 ||| 4757 ||| 4758 ||| 4759 ||| 4760 ||| 4761 ||| 3810 ||| 1958 ||| 
2019 ||| text generation from knowledge graphs with graph transformers. ||| 4762 ||| 4763 ||| 4764 ||| 3408 ||| 4765 ||| 
2021 ||| capturing row and column semantics in transformer based question answering over tables. ||| 4766 ||| 4767 ||| 3689 ||| 4768 ||| 4769 ||| 4770 ||| 4771 ||| 4772 ||| 4773 ||| 4774 ||| 
2019 ||| joint multi-label attention networks for social text annotation. ||| 4775 ||| 1160 ||| 4776 ||| 4777 ||| 
2019 ||| bert: pre-training of deep bidirectional transformers for language understanding. ||| 4778 ||| 4779 ||| 4780 ||| 3185 ||| 
2021 ||| lightseq: a high performance inference library for transformers. ||| 4781 ||| 4782 ||| 4783 ||| 3428 ||| 3034 ||| 
2021 ||| unidrop: a simple yet effective technique to improve transformer without extra cost. ||| 4784 ||| 4785 ||| 4786 ||| 4787 ||| 4788 ||| 4789 ||| 4790 ||| 4791 ||| 
2021 ||| attention head masking for inference time content selection in abstractive summarization. ||| 4752 ||| 4754 ||| 
2019 ||| hierarchical user and item representation with three-tier attention for recommendation. ||| 3754 ||| 3755 ||| 4792 ||| 2795 ||| 
2021 ||| predicting discourse trees from transformer-based neural summarizers. ||| 4793 ||| 4794 ||| 4795 ||| 
2019 ||| modeling recurrence for transformer. ||| 4796 ||| 3309 ||| 3037 ||| 3038 ||| 4797 ||| 3041 ||| 
2021 ||| towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers. ||| 4798 ||| 4799 ||| 3929 ||| 
2019 ||| relation classification using segment-level attention-based cnn and dependency-based rnn. ||| 4800 ||| 4801 ||| 4802 ||| 4803 ||| 
2021 ||| on the transformer growth for progressive bert training. ||| 4804 ||| 4805 ||| 4806 ||| 4807 ||| 2230 ||| 1252 ||| 
2021 ||| an architecture for accelerated large-scale inference of transformer-based language models. ||| 4808 ||| 4809 ||| 4810 ||| 4811 ||| 
2021 ||| mask attention networks: rethinking and strengthen transformer. ||| 4812 ||| 4813 ||| 2921 ||| 4814 ||| 4815 ||| 4816 ||| 3706 ||| 4817 ||| 3273 ||| 
2021 ||| amr parsing with action-pointer transformer. ||| 4818 ||| 3551 ||| 1633 ||| 3553 ||| 4819 ||| 
2021 ||| mt5: a massively multilingual pre-trained text-to-text transformer. ||| 4820 ||| 4821 ||| 4822 ||| 4823 ||| 4824 ||| 4825 ||| 4826 ||| 3338 ||| 
2021 ||| da-transformer: distance-aware transformer. ||| 3754 ||| 3755 ||| 2795 ||| 
2021 ||| event time extraction and propagation via graph attention networks. ||| 3704 ||| 4827 ||| 3403 ||| 4828 ||| 1252 ||| 4771 ||| 4829 ||| 4830 ||| 
2021 ||| probing for bridging inference in transformer language models. ||| 4831 ||| 4832 ||| 
2019 ||| giving attention to the unexpected: using prosody innovations in disfluency detection. ||| 4833 ||| 4834 ||| 
2021 ||| target-specified sequence labeling with multi-head self-attention for target-oriented opinion words extraction. ||| 4835 ||| 4102 ||| 4836 ||| 4837 ||| 4838 ||| 
2019 ||| simple attention-based representation learning for ranking short social media posts. ||| 4839 ||| 4840 ||| 3009 ||| 
2018 ||| how time matters: learning time-decay attention for contextual spoken language understanding in dialogues. ||| 4841 ||| 4842 ||| 4843 ||| 
2018 ||| combining character and word information in neural machine translation using a multi-level attention. ||| 4844 ||| 4845 ||| 4846 ||| 4790 ||| 4847 ||| 
2021 ||| sparta: efficient open-domain question answering via sparse transformer matching retrieval. ||| 4848 ||| 4849 ||| 4850 ||| 
2021 ||| on attention redundancy: a comprehensive study. ||| 4851 ||| 4852 ||| 4853 ||| 4854 ||| 4855 ||| 
2018 ||| generating topic-oriented summaries using neural attention. ||| 4856 ||| 3515 ||| 
2021 ||| spanpredict: extraction of predictive document spans with neural attention. ||| 1028 ||| 4857 ||| 4858 ||| 3132 ||| 4859 ||| 1032 ||| 
2021 ||| on biasing transformer attention towards monotonicity. ||| 4860 ||| 4861 ||| 4862 ||| 4863 ||| 3847 ||| 
2021 ||| multi-hop transformer for document-level machine translation. ||| 4864 ||| 2814 ||| 4865 ||| 3037 ||| 4245 ||| 3285 ||| 
2021 ||| too much in common: shifting of embeddings in transformer language models and its implications. ||| 4866 ||| 4867 ||| 4868 ||| 
2019 ||| dialogue act classification with context-aware self-attention. ||| 4869 ||| 4870 ||| 
2019 ||| tensorized self-attention: efficiently modeling pairwise and global dependencies together. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 4873 ||| 
2019 ||| attentivechecker: a bi-directional attention flow mechanism for fact verification. ||| 4874 ||| 4875 ||| 4876 ||| 4877 ||| 
2021 ||| incorporating syntax and semantics in coreference resolution with heterogeneous graph attention network. ||| 4878 ||| 4879 ||| 
2019 ||| can-ner: convolutional attention network for chinese named entity recognition. ||| 4880 ||| 4881 ||| 
2018 ||| target foresight based attention for neural machine translation. ||| 3047 ||| 3048 ||| 3041 ||| 4882 ||| 3051 ||| 
2021 ||| emotion classification in a resource constrained language using transformer-based approach. ||| 4883 ||| 4884 ||| 4885 ||| 4886 ||| 
2021 ||| focused attention improves document-grounded generation. ||| 4887 ||| 4888 ||| 4889 ||| 4890 ||| 3247 ||| 
2019 ||| sequential attention with keyword mask model for community-based question answering. ||| 4891 ||| 4211 ||| 4892 ||| 4214 ||| 
2019 ||| distant supervision relation extraction with intra-bag and inter-bag attentions. ||| 4893 ||| 4894 ||| 
2021 ||| reconsider: improved re-ranking using span-focused cross-attention for open domain question answering. ||| 4895 ||| 4896 ||| 4897 ||| 4898 ||| 
2021 ||| script: self-critic pretraining of transformers. ||| 4899 ||| 4900 ||| 4901 ||| 3287 ||| 
2021 ||| empirical evaluation of pre-trained transformers for human-level nlp: the role of sample size and dimensionality. ||| 4902 ||| 4903 ||| 4904 ||| 4905 ||| 3699 ||| 
2019 ||| incorporating word attention into character-based word segmentation. ||| 4906 ||| 4907 ||| 4908 ||| 4909 ||| 4910 ||| 4911 ||| 4912 ||| 
2019 ||| saliency learning: teaching the model where to pay attention. ||| 4913 ||| 4914 ||| 4915 ||| 4916 ||| 
2018 ||| generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization. ||| 3327 ||| 4917 ||| 4918 ||| 3329 ||| 4919 ||| 3328 ||| 
2018 ||| read and comprehend by gated-attention reader with more belief. ||| 4920 ||| 4921 ||| 
2021 ||| compositional generalization for neural semantic parsing via span-level supervised attention. ||| 4922 ||| 2402 ||| 3067 ||| 4923 ||| 4924 ||| 4925 ||| 4926 ||| 3205 ||| 
2021 ||| cort: complementary rankings from transformers. ||| 4927 ||| 4928 ||| 
2019 ||| topic spotting using hierarchical networks with self attention. ||| 4929 ||| 4930 ||| 4931 ||| 4932 ||| 4933 ||| 
2019 ||| selective attention for context-aware neural machine translation. ||| 4934 ||| 3369 ||| 3370 ||| 3570 ||| 
2021 ||| a million tweets are worth a few points: tuning transformers for customer service tasks. ||| 4935 ||| 4936 ||| 2253 ||| 4937 ||| 4938 ||| 4939 ||| 
2019 ||| decay-function-free time-aware attention to context and speaker indicator for spoken language understanding. ||| 4940 ||| 4941 ||| 
2021 ||| practical transformer-based multilingual text classification. ||| 4942 ||| 4943 ||| 
2021 ||| mtag: modal-temporal attention graph for unaligned human multimodal language sequences. ||| 4944 ||| 4945 ||| 4946 ||| 4880 ||| 4947 ||| 4948 ||| 892 ||| 3601 ||| 
2019 ||| attention is not explanation. ||| 4949 ||| 4950 ||| 
2021 ||| syntax-based attention masking for neural machine translation. ||| 4951 ||| 4846 ||| 
2018 ||| a discourse-aware attention model for abstractive summarization of long documents. ||| 3122 ||| 4952 ||| 4953 ||| 4954 ||| 4955 ||| 4956 ||| 4957 ||| 
2021 ||| probing word translations in the transformer and trading decoder for encoder layers. ||| 8 ||| 3207 ||| 3260 ||| 3181 ||| 
2018 ||| watch, listen, and describe: globally and locally aligned cross-modal attentions for video captioning. ||| 398 ||| 4958 ||| 3802 ||| 
2018 ||| self-attention with relative position representations. ||| 4959 ||| 4960 ||| 2466 ||| 
2018 ||| a mixed hierarchical attention based encoder-decoder approach for standard table summarization. ||| 4918 ||| 3329 ||| 4919 ||| 3327 ||| 3328 ||| 4917 ||| 
2021 ||| template filling with generative transformers. ||| 4961 ||| 4962 ||| 4963 ||| 
2019 ||| tweet stance detection using an attention based neural ensemble model. ||| 4964 ||| 4965 ||| 4966 ||| 
2019 ||| star-transformer. ||| 4967 ||| 3272 ||| 4968 ||| 4969 ||| 4970 ||| 1770 ||| 
2021 ||| hierarchical transformer for task oriented dialog systems. ||| 4971 ||| 4972 ||| 4973 ||| 
2021 ||| date: detecting anomalies in text via self-supervision of transformers. ||| 4974 ||| 4975 ||| 4976 ||| 
2018 ||| knowledge-enriched two-layered attention network for sentiment analysis. ||| 4977 ||| 4978 ||| 3483 ||| 
2019 ||| bag: bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. ||| 4979 ||| 4980 ||| 1756 ||| 
2019 ||| information aggregation for multi-head attention with routing-by-agreement. ||| 595 ||| 3037 ||| 4981 ||| 3309 ||| 597 ||| 3041 ||| 
2018 ||| higher-order syntactic attention network for longer sentence compression. ||| 4982 ||| 4983 ||| 4984 ||| 3745 ||| 
2021 ||| distantly supervised transformers for e-commerce product qa. ||| 4985 ||| 4986 ||| 4987 ||| 4988 ||| 4989 ||| 
2021 ||| an effective intrusion detection model for class-imbalanced learning based on smote and attention mechanism. ||| 4990 ||| 4991 ||| 
2021 ||| user identification in online social networks using graph transformer networks. ||| 4992 ||| 4993 ||| 
2018 ||| differences in working-memory capacity modulate top-down control of social attention. ||| 4994 ||| 4995 ||| 
2021 ||| multimodal attention creates the visual input for infant word learning. ||| 4996 ||| 4997 ||| 
2021 ||| unsupervised learning of shape-invariant lie group transformer by embedding ordinary differential equation. ||| 4998 ||| 4999 ||| 5000 ||| 
2020 ||| a visually explainable learning system for skin lesion detection using multiscale input with attention u-net. ||| 5001 ||| 5002 ||| 5003 ||| 5004 ||| 
2021 ||| combining transformer generators with convolutional discriminators. ||| 5005 ||| 5006 ||| 4194 ||| 4195 ||| 4193 ||| 5007 ||| 4196 ||| 5008 ||| 
2021 ||| an attention method to introduce prior knowledge in dialogue state tracking. ||| 5009 ||| 5010 ||| 
2020 ||| proposing two different feature extraction methods from multi-fractal detrended fluctuation analysis of electroencephalography signals: a case study on attention-deficit hyperactivity disorder. ||| 5011 ||| 5012 ||| 5013 ||| 5014 ||| 
2020 ||| correlation-aware next basket recommendation using graph attention networks. ||| 3812 ||| 5015 ||| 5016 ||| 5017 ||| 602 ||| 5018 ||| 
2021 ||| saliency detection framework based on deep enhanced attention network. ||| 5019 ||| 5020 ||| 5021 ||| 5022 ||| 5023 ||| 5024 ||| 5025 ||| 
2021 ||| classmates enhanced diversity-self-attention network for dropout prediction in moocs. ||| 5026 ||| 5027 ||| 5028 ||| 5029 ||| 5030 ||| 
2021 ||| spatio-temporal dynamic multi-graph attention network for ride-hailing demand prediction. ||| 5031 ||| 5032 ||| 5033 ||| 5034 ||| 
2021 ||| speaker verification with disentangled self-attention. ||| 5035 ||| 5036 ||| 5037 ||| 286 ||| 5038 ||| 
2021 ||| rethinking the effectiveness of selective attention in neural networks. ||| 5039 ||| 5040 ||| 1086 ||| 5041 ||| 
2019 ||| weakly supervised fine-grained visual recognition via adversarial complementary attentions and hierarchical bilinear pooling. ||| 5042 ||| 5043 ||| 5044 ||| 
2019 ||| exploration of different attention mechanisms on medical image segmentation. ||| 344 ||| 342 ||| 343 ||| 341 ||| 5045 ||| 
2019 ||| attention network for product characteristics prediction based on reviews. ||| 5046 ||| 241 ||| 4008 ||| 
2021 ||| transformer with prior language knowledge for image captioning. ||| 5047 ||| 5048 ||| 883 ||| 5049 ||| 
2020 ||| a hybrid self-attention model for pedestrians detection. ||| 4715 ||| 5050 ||| 5051 ||| 
2017 ||| temporal attention neural network for video understanding. ||| 5052 ||| 5053 ||| 488 ||| 
2020 ||| dpast-rnn: a dual-phase attention-based recurrent neural network using spatiotemporal lstms for time series prediction. ||| 5054 ||| 5055 ||| 923 ||| 921 ||| 920 ||| 
2021 ||| self-attention long-term dependency modelling in electroencephalography sleep stage prediction. ||| 5056 ||| 5057 ||| 5058 ||| 5059 ||| 5060 ||| 
2019 ||| discriminant feature learning with self-attention for person re-identification. ||| 438 ||| 5061 ||| 5062 ||| 
2017 ||| a multi-attention-based bidirectional long short-term memory network for relation extraction. ||| 5063 ||| 5064 ||| 5065 ||| 5066 ||| 
2021 ||| gru with level-aware attention for rumor early detection in social networks. ||| 3906 ||| 5067 ||| 5068 ||| 5069 ||| 5070 ||| 5071 ||| 
2019 ||| attention-based deep q-network in complex systems. ||| 5072 ||| 5073 ||| 5074 ||| 
2019 ||| link prediction with attention-based semantic influence of multiple neighbors. ||| 5075 ||| 1241 ||| 5076 ||| 5077 ||| 398 ||| 5078 ||| 3764 ||| 
2020 ||| prediction of taxi demand based on cnn-bilstm-attention neural network. ||| 5079 ||| 
2018 ||| memory-based model with multiple attentions for multi-turn response selection. ||| 5080 ||| 349 ||| 350 ||| 
2018 ||| mulattenrec: a multi-level attention-based model for recommendation. ||| 5081 ||| 4552 ||| 5082 ||| 5083 ||| 5084 ||| 
2018 ||| attention-based network for cross-view gait recognition. ||| 5085 ||| 5086 ||| 5087 ||| 5088 ||| 
2020 ||| multitask learning based on constrained hierarchical attention network for multi-aspect sentiment classification. ||| 5089 ||| 162 ||| 5090 ||| 5091 ||| 3559 ||| 
2020 ||| adversarial shared-private attention network for joint slot filling and intent detection. ||| 5092 ||| 5093 ||| 5094 ||| 5095 ||| 
2021 ||| a multi-channel graph attention network for chinese ner. ||| 5096 ||| 284 ||| 286 ||| 
2018 ||| hashtag recommendation with attention-based neural image hashtagging network. ||| 5097 ||| 5098 ||| 5099 ||| 234 ||| 5100 ||| 5101 ||| 
2020 ||| springnet: transformer and spring dtw for time series forecasting. ||| 996 ||| 997 ||| 998 ||| 
2021 ||| coordinate attention residual deformable u-net for vessel segmentation. ||| 5102 ||| 2530 ||| 5103 ||| 5104 ||| 
2021 ||| pathsage: spatial graph attention neural networks with random path sampling. ||| 5105 ||| 1221 ||| 209 ||| 210 ||| 
2020 ||| stga-lstm: a spatial-temporal graph attentional lstm scheme for multi-agent cooperation. ||| 5106 ||| 5107 ||| 5108 ||| 5109 ||| 
2020 ||| coarse-to-fine attention network via opinion approximate representation for aspect-level sentiment classification. ||| 5110 ||| 5048 ||| 5111 ||| 5112 ||| 5111 ||| 
2019 ||| hie-transformer: a hierarchical hybrid transformer for abstractive article summarization. ||| 5113 ||| 284 ||| 286 ||| 
2021 ||| multi-task perceptual occlusion face detection with semantic attention network. ||| 5114 ||| 5115 ||| 5116 ||| 
2018 ||| text simplification with self-attention-based pointer-generator networks. ||| 5117 ||| 920 ||| 5118 ||| 5119 ||| 
2019 ||| high-performance light field reconstruction with channel-wise and sai-wise attention. ||| 5120 ||| 5121 ||| 5122 ||| 2303 ||| 5123 ||| 5124 ||| 
2021 ||| global fusion capsule network with pairwise-relation attention graph routing. ||| 5125 ||| 5126 ||| 515 ||| 
2017 ||| position-based content attention for time series forecasting with sequence-to-sequence rnns. ||| 5127 ||| 5128 ||| 5129 ||| 5130 ||| 5131 ||| 5132 ||| 5133 ||| 
2019 ||| attention-based audio-visual fusion for video summarization. ||| 5134 ||| 5135 ||| 5136 ||| 
2017 ||| aggregating class interactions for hierarchical attention relation extraction. ||| 1416 ||| 449 ||| 1382 ||| 
2019 ||| attention based shared representation for multi-task stance detection and sentiment analysis. ||| 5137 ||| 5138 ||| 165 ||| 
2021 ||| cpsam: channel and position squeeze attention module. ||| 5139 ||| 5140 ||| 2778 ||| 5141 ||| 
2019 ||| delving into precise attention in image captioning. ||| 5142 ||| 5143 ||| 5144 ||| 5145 ||| 5146 ||| 
2020 ||| sparse hierarchical modeling of deep contextual attention for document-level neural machine translation. ||| 285 ||| 283 ||| 5147 ||| 286 ||| 
2019 ||| gcnda: graph convolutional networks with dual attention mechanisms for aspect based sentiment analysis. ||| 927 ||| 928 ||| 930 ||| 929 ||| 5148 ||| 5149 ||| 
2018 ||| attention-based combination of cnn and rnn for relation classification. ||| 5150 ||| 4600 ||| 1840 ||| 5151 ||| 5152 ||| 5153 ||| 
2020 ||| an attention-based interaction-aware spatio-temporal graph neural network for trajectory prediction. ||| 2736 ||| 5154 ||| 5155 ||| 5156 ||| 5157 ||| 5158 ||| 
2017 ||| a width-variable window attention model for environmental sensors. ||| 5159 ||| 5160 ||| 3879 ||| 5161 ||| 5162 ||| 5163 ||| 
2019 ||| sparse graphic attention lstm for eeg emotion recognition. ||| 5164 ||| 540 ||| 5165 ||| 541 ||| 
2021 ||| edge guided attention based densely connected network for single image super-resolution. ||| 5166 ||| 4151 ||| 5167 ||| 
2021 ||| spatial-temporal attention network with multi-similarity loss for fine-grained skeleton-based action recognition. ||| 2008 ||| 5168 ||| 5169 ||| 5170 ||| 5171 ||| 5172 ||| 5173 ||| 5174 ||| 
2019 ||| reinforcement learning with attention that works: a self-supervised approach. ||| 5175 ||| 5176 ||| 5177 ||| 
2017 ||| bi-directional lstm with quantum attention mechanism for sentence modeling. ||| 5178 ||| 5078 ||| 5179 ||| 
2018 ||| aspect-level sentiment classification with conv-attention mechanism. ||| 5180 ||| 3114 ||| 5181 ||| 5182 ||| 
2019 ||| raunet: residual attention u-net for semantic segmentation of cataract surgical instruments. ||| 5183 ||| 5184 ||| 5185 ||| 271 ||| 5186 ||| 5187 ||| 276 ||| 5188 ||| 5189 ||| 
2021 ||| attention-based 3d resnet for detection of alzheimer's disease process. ||| 5190 ||| 5191 ||| 5048 ||| 5112 ||| 
2019 ||| a fast convolutional self-attention based speech dereverberation method for robust speech recognition. ||| 5192 ||| 5193 ||| 5093 ||| 5095 ||| 
2021 ||| end-to-end edge detection via improved transformer model. ||| 5194 ||| 5195 ||| 5196 ||| 907 ||| 
2021 ||| srgat: social relational graph attention network for human trajectory prediction. ||| 5197 ||| 5198 ||| 5199 ||| 5200 ||| 
2018 ||| attention based dialogue context selection model. ||| 5201 ||| 5202 ||| 517 ||| 
2021 ||| hierarchical features integration and attention iteration network for juvenile refractive power prediction. ||| 1420 ||| 5203 ||| 802 ||| 5204 ||| 5205 ||| 5206 ||| 
2021 ||| a transformer-based model for low-resource event detection. ||| 5207 ||| 5208 ||| 5209 ||| 5210 ||| 
2021 ||| learning discriminative representation with attention and diversity for large-scale face recognition. ||| 5211 ||| 5212 ||| 5213 ||| 
2019 ||| target-based attention model for aspect-level sentiment analysis. ||| 5110 ||| 5048 ||| 883 ||| 5214 ||| 5215 ||| 5216 ||| 5217 ||| 5111 ||| 5218 ||| 
2020 ||| multi-scale attention consistency for multi-label image classification. ||| 5219 ||| 5220 ||| 5221 ||| 4776 ||| 
2021 ||| a lightweight multidimensional self-attention network for fine-grained action recognition. ||| 5170 ||| 5168 ||| 5172 ||| 5222 ||| 2008 ||| 5223 ||| 
2019 ||| sacic: a semantics-aware convolutional image captioner using multi-level pervasive attention. ||| 5224 ||| 366 ||| 
2021 ||| sta3dcnn: spatial-temporal attention 3d convolutional neural network for citywide crowd flow prediction. ||| 5225 ||| 348 ||| 1717 ||| 
2019 ||| pay attention to deep feature fusion in crowd density estimation. ||| 5226 ||| 5227 ||| 5228 ||| 2505 ||| 5229 ||| 
2017 ||| hierarchical attention blstm for modeling sentences and documents. ||| 5178 ||| 5078 ||| 
2021 ||| multimodal named entity recognition via co-attention-based method with dynamic visual concept expansion. ||| 5230 ||| 5231 ||| 
2019 ||| intra-modality feature interaction using self-attention for visual question answering. ||| 5232 ||| 4182 ||| 4180 ||| 5233 ||| 4183 ||| 
2017 ||| boxless action recognition in still images via recurrent visual attention. ||| 4209 ||| 586 ||| 590 ||| 589 ||| 
2019 ||| hierarchical attention cnn and entity-aware for relation extraction. ||| 5234 ||| 286 ||| 5235 ||| 
2020 ||| spotfast networks with memory augmented lateral transformers for lipreading. ||| 5236 ||| 
2020 ||| diabetic retinopathy detection using multi-layer neural networks and split attention with focal loss. ||| 5237 ||| 402 ||| 5238 ||| 5239 ||| 5240 ||| 5241 ||| 5242 ||| 849 ||| 
2019 ||| aspect-level sentiment classification with dependency rules and dual attention. ||| 5243 ||| 5244 ||| 5245 ||| 
2019 ||| time-frequency deep representation learning for speech emotion recognition integrating self-attention. ||| 5246 ||| 5247 ||| 5093 ||| 5248 ||| 5095 ||| 
2018 ||| a refined spatial transformer network. ||| 5249 ||| 5250 ||| 5251 ||| 5252 ||| 
2020 ||| res2u-net: image inpainting via multi-scale backbone and channel attention. ||| 2792 ||| 5253 ||| 
2020 ||| predicting information diffusion cascades using graph attention networks. ||| 444 ||| 5254 ||| 
2020 ||| multi-modal feature attention for cervical lymph node segmentation in ultrasound and doppler images. ||| 5255 ||| 5256 ||| 2487 ||| 5257 ||| 5258 ||| 5259 ||| 5260 ||| 
2021 ||| trufm: a transformer-guided framework for fine-grained urban flow inference. ||| 5261 ||| 5262 ||| 1940 ||| 
2021 ||| bertdan: question-answer dual attention fusion networks with pre-trained models for answer selection. ||| 769 ||| 5263 ||| 767 ||| 247 ||| 776 ||| 5264 ||| 336 ||| 861 ||| 
2017 ||| relation classification via target-concentrated attention cnns. ||| 5265 ||| 5266 ||| 5267 ||| 1445 ||| 
2018 ||| a comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese. ||| 5268 ||| 5269 ||| 5270 ||| 728 ||| 
2020 ||| routing attention shift network for image classification and segmentation. ||| 5271 ||| 5272 ||| 5273 ||| 5274 ||| 
2019 ||| improve image captioning by self-attention. ||| 5275 ||| 5276 ||| 5277 ||| 
2021 ||| metric learning based vision transformer for product matching. ||| 5278 ||| 5279 ||| 5280 ||| 5281 ||| 5282 ||| 
2018 ||| attentional payload anomaly detector for web applications. ||| 5283 ||| 5284 ||| 5285 ||| 
2020 ||| triple attention network for clothing parsing. ||| 5286 ||| 5287 ||| 5288 ||| 5289 ||| 5290 ||| 5291 ||| 
2019 ||| fusion convolutional attention network for opinion spam detection. ||| 5292 ||| 5293 ||| 5294 ||| 5067 ||| 233 ||| 5295 ||| 
2021 ||| generative adversarial domain generalization via cross-task feature attention learning for prostate segmentation. ||| 5296 ||| 5297 ||| 5298 ||| 5299 ||| 5300 ||| 
2020 ||| deep cardiovascular disease prediction with risk factors powered bi-attention. ||| 5301 ||| 5302 ||| 5303 ||| 5304 ||| 
2019 ||| keyphrase generation with word attention. ||| 5158 ||| 5305 ||| 240 ||| 241 ||| 
2017 ||| end-to-end chinese image text recognition with attention model. ||| 5306 ||| 5307 ||| 5308 ||| 728 ||| 
2020 ||| multi-agent cooperation and competition with two-level attention network. ||| 5309 ||| 5108 ||| 5109 ||| 5106 ||| 
2020 ||| multi-view subspace adaptive learning via autoencoder and attention. ||| 5310 ||| 5311 ||| 5312 ||| 5313 ||| 
2018 ||| estimation of student classroom attention using a novel measure of head motion coherence. ||| 5314 ||| 5315 ||| 
2020 ||| multiple sclerosis lesion filling using a non-lesion attention based convolutional network. ||| 5316 ||| 5317 ||| 5318 ||| 5319 ||| 
2018 ||| two-stage attention network for aspect-level sentiment classification. ||| 622 ||| 623 ||| 624 ||| 627 ||| 626 ||| 5320 ||| 
2019 ||| transformer-dw: a transformer network with dynamic and weighted head. ||| 5321 ||| 5322 ||| 5235 ||| 286 ||| 
2019 ||| deep residual-dense attention network for image super-resolution. ||| 5323 ||| 737 ||| 
2019 ||| attention-based image captioning using densenet features. ||| 5324 ||| 5325 ||| 5326 ||| 5327 ||| 5328 ||| 
2021 ||| gated channel attention network for cataract classification on as-oct image. ||| 5329 ||| 5330 ||| 5203 ||| 5331 ||| 5332 ||| 5333 ||| 5206 ||| 
2020 ||| residual spatial attention network for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 5337 ||| 5067 ||| 5338 ||| 
2021 ||| raidu-net: image inpainting via residual attention fusion and gated information distillation. ||| 5339 ||| 5253 ||| 5340 ||| 2792 ||| 
2021 ||| an lstm-based plagiarism detection via attention mechanism and a population-based approach for pre-training parameters with imbalanced classes. ||| 5341 ||| 5342 ||| 5343 ||| 5344 ||| 
2021 ||| tri-transformer hawkes process: three heads are better than one. ||| 416 ||| 5310 ||| 414 ||| 5345 ||| 
2020 ||| attention-based multi-component lstm for internet traffic prediction. ||| 5346 ||| 5347 ||| 4490 ||| 5348 ||| 
2017 ||| hierarchical hybrid attention networks for chinese conversation topic classification. ||| 5349 ||| 3307 ||| 728 ||| 5350 ||| 4470 ||| 728 ||| 
2021 ||| multi-attention network for arbitrary style transfer. ||| 5351 ||| 5352 ||| 
2020 ||| facial expression recognition with an attention network using a single depth image. ||| 5353 ||| 5354 ||| 5355 ||| 5356 ||| 
2021 ||| jstrack: enriching malicious javascript detection based on ast graph analysis and attention mechanism. ||| 5357 ||| 5358 ||| 5359 ||| 5360 ||| 5361 ||| 5362 ||| 
2021 ||| a novel multi-scale key-point detector using residual dense block and coordinate attention. ||| 5363 ||| 5364 ||| 5365 ||| 4398 ||| 5250 ||| 
2020 ||| joint optic disc and optic cup segmentation based on new skip-link attention guidance network and polar transformation. ||| 5366 ||| 930 ||| 5367 ||| 
2019 ||| multi-task gated contextual cross-modal attention framework for sentiment and emotion analysis. ||| 5368 ||| 5137 ||| 3836 ||| 165 ||| 405 ||| 
2019 ||| fraud detection with multi-modal attention and correspondence learning. ||| 5369 ||| 5370 ||| 5371 ||| 5372 ||| 5373 ||| 
2021 ||| quantitative analysis and simple monitoring for partial discharge from transformer. ||| 5374 ||| 5375 ||| 
2019 ||| improvement of residual attention network for image classification. ||| 5376 ||| 5377 ||| 5378 ||| 5379 ||| 
2019 ||| slicenet: mask guided efficient feature augmentation for attention-aware person re-identification. ||| 5380 ||| 241 ||| 
2019 ||| proposal-aware visual saliency detection with semantic attention. ||| 4754 ||| 5381 ||| 5382 ||| 5383 ||| 
2019 ||| improved ctc-attention based end-to-end speech recognition on air traffic control. ||| 5384 ||| 5385 ||| 5386 ||| 5387 ||| 5388 ||| 
2019 ||| an attention bi-box regression network for traffic light detection. ||| 5389 ||| 4400 ||| 5390 ||| 5391 ||| 3311 ||| 5392 ||| 
2019 ||| attention relational network for few-shot learning. ||| 5393 ||| 5394 ||| 5395 ||| 
2020 ||| an attention-enhanced edge-cloud collaborative framework for multi-task application. ||| 5396 ||| 5397 ||| 4398 ||| 5398 ||| 5399 ||| 5400 ||| 
2021 ||| ecg-based heart arrhythmia diagnosis through attentional convolutional neural networks. ||| 5401 ||| 586 ||| 
2021 ||| behavioral disorder test to identify attention-deficit / hyperactivity disorder (adhd) in children using fuzzy algorithm. ||| 5402 ||| 5403 ||| 5404 ||| 
2017 ||| autonomous, self-calibrating binocular vision based on learned attention and active efficient coding. ||| 5405 ||| 5406 ||| 5407 ||| 
2018 ||| object detection and localization with artificial foveal visual attention. ||| 5408 ||| 5409 ||| 5410 ||| 5411 ||| 5412 ||| 852 ||| 5413 ||| 
2020 ||| learning over the attentional space with mobile robots. ||| 5414 ||| 5415 ||| 5416 ||| 5417 ||| 5418 ||| 5419 ||| 227 ||| 5420 ||| 5421 ||| 
2017 ||| imitation learning and attentional supervision of dual-arm structured tasks. ||| 5422 ||| 5423 ||| 5424 ||| 5425 ||| 5426 ||| 5427 ||| 
2019 ||| toddlers' hands organize parent-toddler attention across different social contexts. ||| 5428 ||| 5429 ||| 5430 ||| 4997 ||| 
2017 ||| shape-based attention for identification and localization of cylindrical objects. ||| 5410 ||| 5431 ||| 5412 ||| 852 ||| 5413 ||| 5432 ||| 1994 ||| 
2021 ||| hybrid chinese grammar error checking model based on transformer. ||| 5433 ||| 5434 ||| 5435 ||| 
2019 ||| dense attentional network for pancreas segmentation in abdominal ct scans. ||| 1724 ||| 5436 ||| 5437 ||| 
2021 ||| shoeprint image retrieval based on dual attention light hash network. ||| 5438 ||| 438 ||| 5439 ||| 
2021 ||| text classification method based on bigru-attention and cnn hybrid model. ||| 5440 ||| 5441 ||| 5442 ||| 5443 ||| 5444 ||| 5445 ||| 
2021 ||| fcos small target detection algorithm combined with multi-layer hybrid attention mechanism. ||| 5439 ||| 5446 ||| 5447 ||| 5448 ||| 
2020 ||| a network combining local features and attention mechanisms for vehicle re-identification. ||| 5449 ||| 5450 ||| 2377 ||| 
2021 ||| multi-head attention with disagreement regularization for multimodal sentiment analysis. ||| 5451 ||| 5439 ||| 2904 ||| 
2021 ||| floor plan semantic segmentation using deep learning with boundary attention aggregated mechanism. ||| 5452 ||| 1372 ||| 5453 ||| 5454 ||| 5455 ||| 5456 ||| 
2021 ||| text matching model that fuse position-encoding with multiple attentional mechanisms. ||| 5457 ||| 5458 ||| 5459 ||| 
2021 ||| a transformer district line loss anomaly discrimination model incorporating cross-attention and deep learning algorithm. ||| 5460 ||| 5461 ||| 5462 ||| 5463 ||| 5464 ||| 1825 ||| 5465 ||| 5466 ||| 
2020 ||| an lstm-based traffic prediction algorithm with attention mechanism for satellite network. ||| 5467 ||| 5468 ||| 5469 ||| 
2021 ||| deep facial expression recognition algorithm combining channel attention. ||| 5470 ||| 5439 ||| 5471 ||| 5472 ||| 
2021 ||| sentiment analysis model based on multi-head attention in multimodality. ||| 5473 ||| 5474 ||| 5475 ||| 
2021 ||| graph neural network based on geometric and appearance attention for 6d pose estimation. ||| 5476 ||| 5477 ||| 
2021 ||| single infrared image super-resolution with lightweight self-corrected attention network. ||| 5478 ||| 5479 ||| 5480 ||| 5442 ||| 
2021 ||| vehicle re-identification based on multi-view and convolutional block attention. ||| 3386 ||| 5481 ||| 5482 ||| 
2021 ||| an unmanned aerial vehicle video object tracking algorithm based on siamese attention network. ||| 5483 ||| 5484 ||| 5485 ||| 5486 ||| 5448 ||| 
2021 ||| oriented target detection algorithm based on transformer. ||| 5487 ||| 5488 ||| 5489 ||| 
2020 ||| a spatial attention-enhanced multi-timescale graph convolutional network for skeleton-based action recognition. ||| 5490 ||| 5491 ||| 5492 ||| 5400 ||| 
2021 ||| image caption based on bigru and attention hybrid model. ||| 5493 ||| 5494 ||| 
2019 ||| attention-based bidirectional gated recurrent unit neural networks for sentiment analysis. ||| 5495 ||| 5496 ||| 5497 ||| 
2020 ||| retinal blood vessel segmentation via attention gate network. ||| 5498 ||| 5499 ||| 5500 ||| 5501 ||| 5502 ||| 5166 ||| 
2021 ||| automatic diagnosis of multiple lesions in fundus images based on dual attention mechanism. ||| 5503 ||| 5504 ||| 5505 ||| 5506 ||| 5507 ||| 683 ||| 
2021 ||| what should we pay attention to when classifying violent videos? ||| 5508 ||| 5509 ||| 5510 ||| 5511 ||| 
2018 ||| let's talk about refugees: network effects drive contributor attention to wikipedia articles about migration-related topics. ||| 4194 ||| 5512 ||| 5513 ||| 
2021 ||| an improved yolov3 algorithm combined with attention mechanism for flame and smoke detection. ||| 3386 ||| 5514 ||| 5515 ||| 5516 ||| 5517 ||| 5518 ||| 
2021 ||| subspace classification of attention deficit hyperactivity disorder with laplacian regularization. ||| 4715 ||| 5519 ||| 5520 ||| 5521 ||| 5522 ||| 
2018 ||| attention-based bidirectional recurrent neural networks for description generation of videos. ||| 5523 ||| 5524 ||| 5525 ||| 
2020 ||| joint extraction of entity and semantic relation using encoder - decoder model based on attention mechanism. ||| 5526 ||| 5527 ||| 5528 ||| 5529 ||| 
2020 ||| research on user preference film recommendation based on attention mechanism. ||| 978 ||| 5530 ||| 781 ||| 5531 ||| 
2021 ||| graph attention network for word embeddings. ||| 5532 ||| 5533 ||| 5534 ||| 5535 ||| 5536 ||| 
2019 ||| cbam-gan: generative adversarial networks based on convolutional block attention module. ||| 5537 ||| 133 ||| 5538 ||| 136 ||| 5539 ||| 
2018 ||| attention-based chinese word embedding. ||| 5540 ||| 781 ||| 5531 ||| 
2021 ||| corrosion detection in transformers based on hierarchical annotation. ||| 5541 ||| 5542 ||| 5543 ||| 1572 ||| 5544 ||| 5545 ||| 5546 ||| 
2020 ||| ms-sae: a general model of sentiment analysis based on multimode semantic extraction and sentiment attention enhancement mechanism. ||| 5492 ||| 131 ||| 133 ||| 136 ||| 135 ||| 5547 ||| 
2020 ||| method of multi-feature fusion based on attention mechanism in malicious software detection. ||| 5548 ||| 5549 ||| 
2020 ||| dual residual global context attention network for super-resolution. ||| 5550 ||| 5551 ||| 4175 ||| 2058 ||| 5552 ||| 5553 ||| 5554 ||| 
2017 ||| mindfulness based stress reduction improves tactile selective attention bci accuracy. ||| 5555 ||| 5556 ||| 5112 ||| 
2017 ||| detection of attention alteration of bci users based on eeg analysis. ||| 5557 ||| 5558 ||| 5112 ||| 5559 ||| 5560 ||| 
2017 ||| electroencephalography (eeg)-derived markers to measure components of attention processing. ||| 5561 ||| 5562 ||| 5563 ||| 5564 ||| 5565 ||| 5566 ||| 5567 ||| 
2021 ||| development of long-term prediction algorithm based on component states using bilstm and attention mechanism. ||| 5568 ||| 5569 ||| 5570 ||| 5571 ||| 5572 ||| 
2021 ||| machine learning based anomaly detection of log files using ensemble learning and self-attention. ||| 5573 ||| 5574 ||| 5575 ||| 5335 ||| 5576 ||| 
2018 ||| intelligent fault diagnosis for power transformer based on dga data using support vector machine (svm). ||| 5577 ||| 5578 ||| 5579 ||| 5580 ||| 
2017 ||| a novel nanocrystalline-based current transformer working on saturated region. ||| 5581 ||| 5582 ||| 5583 ||| 5584 ||| 
2021 ||| miniaturized magnetic energy harvester: lightweight and safe transformer design. ||| 5585 ||| 5586 ||| 5587 ||| 5588 ||| 5589 ||| 
2020 ||| analysis of ratio and phase errors over time for low power voltage transformers. ||| 5590 ||| 5591 ||| 5592 ||| 5593 ||| 5594 ||| 
2018 ||| home automation system using brain computer interface paradigm based on auditory selection attention. ||| 5595 ||| 5596 ||| 5597 ||| 5598 ||| 
2020 ||| measurement of dynamic voltage variation effect on instrument transformers for power grid applications. ||| 5599 ||| 5600 ||| 5601 ||| 5602 ||| 5603 ||| 5604 ||| 5605 ||| 5606 ||| 5607 ||| 5608 ||| 5609 ||| 
2017 ||| calibration of mv voltage instrument transformer in a wide frequency range. ||| 5599 ||| 5603 ||| 5600 ||| 5604 ||| 5605 ||| 5610 ||| 
2021 ||| measurement of very fast transient overvoltages in current transformers at open air hv substations. ||| 5611 ||| 5612 ||| 5613 ||| 5614 ||| 5615 ||| 5616 ||| 5617 ||| 
2020 ||| calibration of burdens for instrument transformers. ||| 5618 ||| 5619 ||| 5620 ||| 5621 ||| 
2018 ||| study on zno-based gas sensor for detection of acetylene dissolved in transformer oil. ||| 5622 ||| 5623 ||| 5624 ||| 5625 ||| 
2020 ||| procedure for ratio error and phase displacement prediction of inductive current transformers at different operating conditions. ||| 5626 ||| 5590 ||| 5591 ||| 5627 ||| 
2021 ||| analysis and visualization of time-varying harmonics in transformer inrush currents. ||| 5628 ||| 5629 ||| 5630 ||| 1633 ||| 5631 ||| 
2021 ||| improving harmonic measurements with instrument transformers: a comparison among two techniques. ||| 5601 ||| 5632 ||| 5604 ||| 5633 ||| 5606 ||| 5605 ||| 5634 ||| 5635 ||| 
2019 ||| improving the accuracy of current transformers through harmonic distortion compensation. ||| 5633 ||| 5635 ||| 5636 ||| 
2018 ||| fault tracing method for high voltage electronic current transformer during its performance test based on the fmea. ||| 5637 ||| 5638 ||| 5639 ||| 
2018 ||| effect of temperature on the accuracy of inductive current transformers. ||| 5590 ||| 5640 ||| 5591 ||| 5627 ||| 
2020 ||| magnetic energy harvesting on overhead high voltage lines: weight optimized transformer design for high power output. ||| 5586 ||| 5587 ||| 5585 ||| 5641 ||| 
2021 ||| esa-net: a network with efficient spatial attention for smoky vehicle detection. ||| 5642 ||| 5643 ||| 5644 ||| 5645 ||| 5646 ||| 
2020 ||| combined impact of voltage transformer and estimation algorithm on harmonic synchrophasors measurements. ||| 5633 ||| 5635 ||| 5636 ||| 5647 ||| 5648 ||| 5649 ||| 
2019 ||| behavioral modeling of an inductive voltage transformer: comparison between x-parameters and simplified volterra approaches. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2018 ||| low power voltage transformer accuracy class effects on the residual voltage measurement. ||| 5590 ||| 5591 ||| 5627 ||| 
2019 ||| low cost procedure for frequency characterization of voltage instrument transformers. ||| 5606 ||| 5599 ||| 5600 ||| 5602 ||| 5603 ||| 5604 ||| 5605 ||| 
2021 ||| attention based inception model for robust eeg motor imagery classification. ||| 5650 ||| 5651 ||| 5652 ||| 5653 ||| 5654 ||| 
2018 ||| metrological performances of voltage and current instrument transformers in harmonics measurements. ||| 5655 ||| 5656 ||| 5599 ||| 5600 ||| 5610 ||| 5657 ||| 5658 ||| 5603 ||| 5604 ||| 5605 ||| 
2018 ||| context-aware end-to-end relation extracting from clinical texts with attention-based bi-tree-gru. ||| 5659 ||| 5660 ||| 5661 ||| 5662 ||| 
2018 ||| attention based residual network for micro-gesture recognition. ||| 3626 ||| 5663 ||| 5664 ||| 
2017 ||| improving children's gaze prediction via separate facial areas and attention shift cue. ||| 5665 ||| 5666 ||| 5667 ||| 1052 ||| 765 ||| 5668 ||| 
2020 ||| dual-attention gan for large-pose face frontalization. ||| 5669 ||| 5670 ||| 5671 ||| 1734 ||| 
2020 ||| end-to-end spatial attention network with feature mimicking for head detection. ||| 5672 ||| 5673 ||| 5674 ||| 4128 ||| 
2020 ||| understanding consumer attention on mobile devices. ||| 5675 ||| 5676 ||| 5677 ||| 5678 ||| 
2021 ||| demystifying attention mechanisms for deepfake detection. ||| 5679 ||| 5680 ||| 5681 ||| 
2021 ||| efficient human pose estimation by maximizing fusion and high-level spatial attention. ||| 5682 ||| 5683 ||| 5684 ||| 5685 ||| 5686 ||| 
2021 ||| high-accuracy rgb-d face recognition via segmentation-aware face depth estimation and mask-guided attention network. ||| 5687 ||| 5688 ||| 5689 ||| 5690 ||| 
2021 ||| a coarse-to-fine dual attention network for blind face completion. ||| 5691 ||| 5692 ||| 5693 ||| 5694 ||| 5695 ||| 
2020 ||| multimodality pain and related behaviors recognition based on attention learning. ||| 2837 ||| 2838 ||| 5696 ||| 2841 ||| 
2021 ||| trouspi-net: spatio-temporal attention on parallel atrous convolutions and u-grus for skeletal pedestrian crossing prediction. ||| 5697 ||| 5698 ||| 5699 ||| 5700 ||| 
2019 ||| discriminative attention-based convolutional neural network for 3d facial expression recognition. ||| 5701 ||| 5702 ||| 5703 ||| 5704 ||| 5705 ||| 5706 ||| 
2017 ||| attention-based template adaptation for face verification. ||| 5707 ||| 5708 ||| 5709 ||| 2400 ||| 
2021 ||| your "attention" deserves attention: a self-diversified multi-channel attention for facial action analysis. ||| 5710 ||| 5711 ||| 5712 ||| 5713 ||| 5714 ||| 
2021 ||| headposr: end-to-end trainable head pose estimation using transformer encoders. ||| 5715 ||| 
2018 ||| online attention for interpretable conflict estimation in political debates. ||| 5716 ||| 5717 ||| 5718 ||| 5719 ||| 
2020 ||| learning guided attention masks for facial action unit recognition. ||| 5720 ||| 5721 ||| 5722 ||| 
2018 ||| lcanet: end-to-end lipreading with cascaded attention-ctc. ||| 5723 ||| 5724 ||| 5725 ||| 1117 ||| 
2021 ||| replay attention and data augmentation network for 3d dense alignment and face reconstruction. ||| 5726 ||| 3034 ||| 5727 ||| 
2021 ||| multi-modal learning for au detection based on multi-head fused transformers. ||| 586 ||| 5714 ||| 
2020 ||| spatio-temporal attention and magnification for classification of parkinson's disease from videos collected via the internet. ||| 5728 ||| 5729 ||| 5730 ||| 5731 ||| 5732 ||| 
2020 ||| deep entwined learning head pose and face alignment inside an attentional cascade with doubly-conditional fusion. ||| 5733 ||| 5734 ||| 2118 ||| 
2021 ||| sign, attend and tell: spatial attention for sign language recognition. ||| 5735 ||| 5736 ||| 
2021 ||| adversarial attacks on kinship verification using transformer. ||| 5737 ||| 5738 ||| 5739 ||| 5740 ||| 5741 ||| 
2021 ||| two-stream global-guided attention network for facial expression recognition. ||| 5742 ||| 4482 ||| 5743 ||| 4480 ||| 5744 ||| 
2019 ||| stacked hourglass network joint with salient region attention refinement for face alignment. ||| 5745 ||| 5746 ||| 
2021 ||| cross attentional audio-visual fusion for dimensional emotion recognition. ||| 5747 ||| 5748 ||| 5749 ||| 
2019 ||| robust remote heart rate estimation from face utilizing spatial-temporal attention. ||| 5750 ||| 5751 ||| 5752 ||| 5679 ||| 5681 ||| 1916 ||| 1788 ||| 
2021 ||| toward personalized emotion recognition: a face recognition based attention method for facial emotion recognition. ||| 5753 ||| 602 ||| 5754 ||| 5755 ||| 5756 ||| 
2020 ||| attention fusion for audio-visual person verification using multi-scale features. ||| 5691 ||| 5692 ||| 5757 ||| 5694 ||| 5695 ||| 
2021 ||| skeleton-based action recognition for human-robot interaction using self-attention mechanism. ||| 5758 ||| 5759 ||| 
2017 ||| a 350uw 2ghz fbar transformer coupled colpitts oscillator with close-in phase noise reduction. ||| 5760 ||| 5761 ||| 5762 ||| 5763 ||| 
2017 ||| an isolated dc-dc converter with fully integrated magnetic core transformer. ||| 5764 ||| 5765 ||| 5766 ||| 
2020 ||| keyram: a 0.34 uj/decision 18 k decisions/s recurrent attention in-memory processor for keyword spotting. ||| 5767 ||| 5768 ||| 5769 ||| 5770 ||| 
2021 ||| an ultra-compact 16-to-45 ghz power amplifier within a single inductor footprint using folded transformer technique. ||| 5771 ||| 5772 ||| 5773 ||| 5774 ||| 5775 ||| 
2020 ||| mixer-first extremely wideband 43-97 ghz rx frontend with broadband quadrature input matching and current mode transformer-based image rejection for massive mimo applications. ||| 5776 ||| 5777 ||| 300 ||| 
2017 ||| on-chip transformer design and application to rf and mm-wave front-ends. ||| 5778 ||| 
2020 ||| channel-wise spatial attention with spatiotemporal heterogeneous framework for action recognition. ||| 5779 ||| 5780 ||| 5781 ||| 
2019 ||| face presentation attack detection based on exclusivity regularized attention maps. ||| 5782 ||| 545 ||| 
2021 ||| a facial expression recognition system for smart learning based on yolo and vision transformer. ||| 5783 ||| 5784 ||| 952 ||| 1132 ||| 
2020 ||| attention-based graph convolution collaborative filtering. ||| 5785 ||| 5786 ||| 
2021 ||| a self-attention based method for facial expression recognition. ||| 5783 ||| 5784 ||| 1132 ||| 
2021 ||| a new head pose estimation method using vision transformer model. ||| 5783 ||| 952 ||| 1132 ||| 
2020 ||| attention-based joint representation learning network for short text classification. ||| 5787 ||| 5788 ||| 
2019 ||| an attention-based sequence learning model for scene text recognition with text correction. ||| 604 ||| 515 ||| 5789 ||| 
2019 ||| e-dam: encoder-decoder with attention mechanism for city-scale taxi trajectory prediction. ||| 5790 ||| 5791 ||| 5792 ||| 5793 ||| 5794 ||| 
2018 ||| eeg-based attention feedback to improve focus in e-learning. ||| 5795 ||| 5796 ||| 5797 ||| 5798 ||| 5799 ||| 
2020 ||| multi-attention mechanism for chinese description of videos. ||| 5525 ||| 5800 ||| 5524 ||| 
2020 ||| multi-scale attention net for retina blood vessel segmentation. ||| 5801 ||| 5802 ||| 1207 ||| 5803 ||| 5804 ||| 5805 ||| 
2019 ||| an improved model of multi-attention lstm for multimodal sentiment analysis. ||| 5806 ||| 5807 ||| 5808 ||| 
2020 ||| translating natural language instructions for behavioral robot indoor navigation with attention-history based attention. ||| 5809 ||| 5810 ||| 
2018 ||| combining gated recurrent unit and attention pooling for sentimental classification. ||| 5811 ||| 5812 ||| 5813 ||| 5814 ||| 5815 ||| 5816 ||| 
2018 ||| an unmanned aerial vehicle detection algorithm based on semantic segmentation and visual attention mechanism. ||| 5817 ||| 817 ||| 5818 ||| 
2020 ||| region-attentioned network with location scoring dynamic-threshold nms for object detection in remote sensing images. ||| 5819 ||| 5820 ||| 5821 ||| 5822 ||| 
2018 ||| improvement of embedding channel-wise activation in soft-attention neural image captioning. ||| 5823 ||| 
2020 ||| generative adversarial and self-attention based fine-grained cross-media retrieval. ||| 5824 ||| 1676 ||| 498 ||| 1680 ||| 
2021 ||| multi-scale spatial-temporal transformer for 3d human pose estimation. ||| 5825 ||| 5826 ||| 
2017 ||| power line communication through distribution transformers in smart grid. ||| 53 ||| 5827 ||| 5828 ||| 
2017 ||| tree-lstm guided attention pooling of dcnn for semantic sentence modeling. ||| 5829 ||| 5830 ||| 5831 ||| 5832 ||| 
2021 ||| anomalous state detection of power transformer based on k-means clustering algorithm. ||| 5833 ||| 5834 ||| 5835 ||| 5836 ||| 5837 ||| 5838 ||| 
2021 ||| perfecting short-term stock predictions with multi-attention networks in noise-free settings. ||| 5839 ||| 5840 ||| 5841 ||| 5842 ||| 5843 ||| 
2021 ||| attention transition prediction based on multi-scale spatiotemporal features. ||| 5844 ||| 5845 ||| 5846 ||| 5847 ||| 5848 ||| 5849 ||| 
2021 ||| weakly supervised mitosis detection using ellipse label on attention mask r-cnn. ||| 5850 ||| 5851 ||| 781 ||| 5852 ||| 5853 ||| 5854 ||| 
2019 ||| joint visual-textual sentiment analysis based on cross-modality attention mechanism. ||| 5855 ||| 5856 ||| 1481 ||| 1748 ||| 5857 ||| 
2020 ||| classroom attention analysis based on multiple euler angles constraint and head pose estimation. ||| 5858 ||| 5859 ||| 
2021 ||| multi-branch and multi-scale attention learning for fine-grained visual categorization. ||| 2532 ||| 5860 ||| 5861 ||| 5862 ||| 
2020 ||| a novel attention enhanced dense network for image super-resolution. ||| 5863 ||| 5864 ||| 5865 ||| 5866 ||| 
2020 ||| unsupervised video summarization via attention-driven adversarial learning. ||| 5867 ||| 5868 ||| 5869 ||| 5870 ||| 5871 ||| 
2022 ||| personalized fashion recommendation using pairwise attention. ||| 5872 ||| 5873 ||| 5874 ||| 
2021 ||| multi-granularity recurrent attention graph neural network for few-shot learning. ||| 181 ||| 5875 ||| 5876 ||| 
2022 ||| learning image representation via attribute-aware attention networks for fashion classification. ||| 5877 ||| 5878 ||| 5879 ||| 5880 ||| 
2019 ||| spatio-temporal attention model based on multi-view for social relation understanding. ||| 5881 ||| 411 ||| 
2022 ||| classroom attention estimation method based on mining facial landmarks of students. ||| 5882 ||| 5883 ||| 5884 ||| 
2019 ||| two-level attention with multi-task learning for facial emotion estimation. ||| 5885 ||| 5886 ||| 5887 ||| 5888 ||| 5889 ||| 5890 ||| 
2021 ||| eeg emotion recognition based on channel attention for e-healthcare applications. ||| 181 ||| 5891 ||| 5876 ||| 
2019 ||| enhancing scene text detection via fused semantic segmentation network with attention. ||| 859 ||| 4430 ||| 5892 ||| 
2020 ||| attennet: deep attention based retinal disease classification in oct images. ||| 1035 ||| 5893 ||| 5894 ||| 5895 ||| 5896 ||| 5897 ||| 5898 ||| 5899 ||| 5900 ||| 5901 ||| 5902 ||| 5903 ||| 5904 ||| 5905 ||| 5906 ||| 5907 ||| 5908 ||| 1755 ||| 
2019 ||| single-stage detector with semantic attention for occluded pedestrian detection. ||| 5909 ||| 5910 ||| 4267 ||| 4270 ||| 
2022 ||| non-uniform attention network for multi-modal sentiment analysis. ||| 5911 ||| 5912 ||| 5913 ||| 5914 ||| 5915 ||| 5916 ||| 
2020 ||| an attention based speaker-independent audio-visual deep learning model for speech enhancement. ||| 5917 ||| 5918 ||| 5919 ||| 
2020 ||| image captioning based on visual and semantic attention. ||| 612 ||| 264 ||| 613 ||| 
2022 ||| mevit: motion enhanced video transformer for video classification. ||| 144 ||| 5920 ||| 
2019 ||| video summarization with lstm and deep attention models. ||| 5921 ||| 5922 ||| 
2022 ||| multi-scale cross-modal transformer network for rgb-d object detection. ||| 5923 ||| 5924 ||| 5925 ||| 
2021 ||| generative image inpainting by hybrid contextual attention network. ||| 5926 ||| 5927 ||| 
2021 ||| dense attention-guided network for boundary-aware salient object detection. ||| 2855 ||| 5928 ||| 5929 ||| 5930 ||| 
2021 ||| musicoder: a universal music-acoustic encoder based on transformer. ||| 5931 ||| 5932 ||| 
2021 ||| a sentiment similarity-oriented attention model with multi-task learning for text-based emotion recognition. ||| 5933 ||| 5248 ||| 5093 ||| 5247 ||| 5246 ||| 5095 ||| 
2021 ||| a hybrid music recommendation algorithm based on attention mechanism. ||| 5934 ||| 5935 ||| 5936 ||| 5937 ||| 
2021 ||| unsupervised temporal attention summarization model for user created videos. ||| 5888 ||| 813 ||| 5938 ||| 5939 ||| 
2022 ||| bi-attention modal separation network for multimodal video fusion. ||| 5940 ||| 5941 ||| 5038 ||| 
2022 ||| time-frequency attention for speech emotion recognition with squeeze-and-excitation blocks. ||| 5942 ||| 5187 ||| 5943 ||| 4287 ||| 
2022 ||| one-stage image inpainting with hybrid attention. ||| 5944 ||| 5945 ||| 5946 ||| 
2020 ||| adversarial query-by-image video retrieval based on attention mechanism. ||| 5947 ||| 5948 ||| 5088 ||| 
2019 ||| action recognition using visual attention with reinforcement learning. ||| 5949 ||| 1785 ||| 813 ||| 5950 ||| 5951 ||| 5952 ||| 
2018 ||| recursive pyramid network with joint attention for cross-media retrieval. ||| 5953 ||| 5954 ||| 
2022 ||| conditional context-aware feature alignment for domain adaptive detection transformer. ||| 5955 ||| 
2022 ||| sam: self attention mechanism for scene text recognition based on swin transformer. ||| 5956 ||| 5957 ||| 1160 ||| 5958 ||| 5858 ||| 
2021 ||| confidence-based global attention guided network for image inpainting. ||| 5959 ||| 5960 ||| 3034 ||| 5961 ||| 782 ||| 
2020 ||| compact position-aware attention network for image semantic segmentation. ||| 5962 ||| 5963 ||| 989 ||| 379 ||| 
2022 ||| adaptive speech intelligibility enhancement for far-and-near-end noise environments based on self-attention stargan. ||| 5964 ||| 5965 ||| 705 ||| 5966 ||| 5967 ||| 5968 ||| 
2017 ||| large-scale product classification via spatial attention based cnn learning and multi-class regression. ||| 5969 ||| 5970 ||| 5308 ||| 
2021 ||| a multi-modal transformer-based code summarization approach for smart contracts. ||| 5937 ||| 5971 ||| 5972 ||| 737 ||| 5973 ||| 5974 ||| 2068 ||| 
2021 ||| locating faulty methods with a mixed rnn and attention model. ||| 5975 ||| 5976 ||| 5977 ||| 5978 ||| 5979 ||| 
2020 ||| exploiting code knowledge graph for bug localization via bi-directional attention. ||| 5980 ||| 5981 ||| 4245 ||| 5982 ||| 3285 ||| 
2020 ||| a self-attentional neural architecture for code completion with multi-task learning. ||| 5743 ||| 2064 ||| 5983 ||| 5984 ||| 5985 ||| 1390 ||| 
2020 ||| attention mechanism in predictive business process monitoring. ||| 5986 ||| 5987 ||| 5988 ||| 5989 ||| 5990 ||| 
2019 ||| attention bilinear pooling for fine-grained facial expression recognition. ||| 4805 ||| 5991 ||| 5992 ||| 
2020 ||| robust gan based on attention mechanism. ||| 5993 ||| 5994 ||| 5995 ||| 5996 ||| 
2019 ||| dense inception attention neural network for in-loop filter. ||| 5997 ||| 5998 ||| 5999 ||| 6000 ||| 6001 ||| 6002 ||| 6003 ||| 
2018 ||| automatic generation of pseudocode with attention seq2seq model. ||| 6004 ||| 1090 ||| 
2021 ||| fine-grained pseudo-code generation method via code feature extraction and transformer. ||| 6005 ||| 6006 ||| 6007 ||| 6008 ||| 
2020 ||| software defect prediction and localization with attention-based models and ensemble learning. ||| 6009 ||| 2931 ||| 2933 ||| 6010 ||| 6011 ||| 
2019 ||| visual attention system based on fuzzy classifier to define priority of traffic signs for intelligent robotic vehicle navigation purposes. ||| 6012 ||| 6013 ||| 6014 ||| 
2018 ||| attention-based view of online information dissemination. ||| 6015 ||| 
2021 ||| urltran: improving phishing url detection using transformers. ||| 6016 ||| 6017 ||| 6018 ||| 6019 ||| 6020 ||| 6021 ||| 
2021 ||| towards transformer-based real-time object detection at the edge: a benchmarking study. ||| 6022 ||| 6023 ||| 
2019 ||| automatic fault detection in a cascaded transformer multilevel inverter using pattern recognition techniques. ||| 6024 ||| 6025 ||| 6026 ||| 6027 ||| 6028 ||| 
2019 ||| frame by frame pain estimation using locally spatial attention learning. ||| 1754 ||| 6029 ||| 6030 ||| 
2021 ||| trace: early detection of chronic kidney disease onset with transformer-enhanced feature embedding. ||| 3906 ||| 6031 ||| 6032 ||| 6033 ||| 
2020 ||| the transformers for polystores - the next frontier for polystore research. ||| 104 ||| 103 ||| 105 ||| 
2021 ||| madc: multi-scale attention-based deep clustering for workload prediction. ||| 6034 ||| 6035 ||| 6036 ||| 6037 ||| 542 ||| 
2020 ||| feature envy detection based on bi-lstm with self-attention mechanism. ||| 6038 ||| 2058 ||| 6039 ||| 6040 ||| 6041 ||| 1341 ||| 
2020 ||| an ego network embedding model via neighbors sampling and self-attention mechanism. ||| 6042 ||| 6043 ||| 6044 ||| 6045 ||| 
2018 ||| hierarchical attention based recurrent neural network framework for mobile moba game recommender systems. ||| 6046 ||| 6047 ||| 6048 ||| 
2019 ||| automatic text summarization based on transformer and switchable normalization. ||| 6049 ||| 2954 ||| 6050 ||| 
2019 ||| an efficient machine reading comprehension method based on attention mechanism. ||| 6051 ||| 6052 ||| 6053 ||| 
2021 ||| adaptive attention encoder for attribute graph embedding. ||| 6054 ||| 6055 ||| 6056 ||| 
2019 ||| counting attention based on classification confidence for visual question answering. ||| 6057 ||| 6058 ||| 6059 ||| 6060 ||| 
2021 ||| self-attention based automated vulnerability detection with effective data representation. ||| 6061 ||| 6062 ||| 6063 ||| 6064 ||| 6065 ||| 
2020 ||| avdhram: automated vulnerability detection based on hierarchical representation and attention mechanism. ||| 6066 ||| 6062 ||| 6067 ||| 6063 ||| 6065 ||| 709 ||| 
2019 ||| an attention-based recommendation algorithm. ||| 6068 ||| 6069 ||| 6070 ||| 6071 ||| 157 ||| 6072 ||| 
2020 ||| answer graph-based interactive attention network for question answering over knowledge base. ||| 6073 ||| 989 ||| 6074 ||| 6075 ||| 6076 ||| 379 ||| 
2021 ||| attention-based encoder-decoder recurrent neural networks for http payload anomaly detection. ||| 6077 ||| 6078 ||| 
2019 ||| attention alignment by linear space projection for video features extraction. ||| 6079 ||| 6080 ||| 6081 ||| 6082 ||| 
2019 ||| resfpa-gan: text-to-image synthesis with generative adversarial network based on residual block feature pyramid attention. ||| 6083 ||| 6084 ||| 2747 ||| 
2019 ||| recognising human-object interactions using attention-based lstms. ||| 6085 ||| 6086 ||| 
2019 ||| 5g-transformer service orchestrator: design, implementation, and evaluation. ||| 6087 ||| 6088 ||| 6089 ||| 6090 ||| 6091 ||| 4046 ||| 6092 ||| 6093 ||| 6094 ||| 6095 ||| 6096 ||| 6097 ||| 6098 ||| 2600 ||| 6099 ||| 6100 ||| 6101 ||| 2259 ||| 6102 ||| 
2021 ||| interpreting deep learning based cerebral palsy prediction with channel attention. ||| 6103 ||| 6104 ||| 6105 ||| 6106 ||| 6107 ||| 
2021 ||| multimodal breast lesion classification using cross-attention deep networks. ||| 6108 ||| 6109 ||| 6110 ||| 6111 ||| 6112 ||| 
2018 ||| deep learning from electronic medical records using attention-based cross-modal convolutional neural networks. ||| 6113 ||| 6114 ||| 
2021 ||| afa-rn: an abnormal feature attention relation network for multi-class disease classification in gastrointestinal endoscopic images. ||| 6115 ||| 6116 ||| 6117 ||| 
2018 ||| a hierarchical lstm model with attention for modeling eeg non-stationarity for human decision prediction. ||| 6118 ||| 6119 ||| 6120 ||| 
2018 ||| a novel channel-aware attention framework for multi-channel eeg seizure detection via multi-view deep learning. ||| 2296 ||| 6121 ||| 1071 ||| 1628 ||| 6122 ||| 6123 ||| 1630 ||| 
2019 ||| ecgnet: learning where to attend for detection of atrial fibrillation with deep visual attention. ||| 6124 ||| 6125 ||| 6126 ||| 6127 ||| 
2021 ||| multi-module recurrent convolutional neural network with transformer encoder for ecg arrhythmia classification. ||| 6128 ||| 6129 ||| 6130 ||| 6131 ||| 6132 ||| 2137 ||| 
2020 ||| spontaneous expression recognition based on visual attention mechanism and co-salient features. ||| 6133 ||| 6134 ||| 6135 ||| 6136 ||| 
2018 ||| privacy-preserving network bmi decoding of covert spatial attention. ||| 6137 ||| 6138 ||| 6139 ||| 
2021 ||| speech emotion recognition using xgboost and cnn blstm with attention. ||| 6140 ||| 6141 ||| 
2018 ||| generating expert's review from the crowds': integrating a multi-attention mechanism with encoder-decoder framework. ||| 6142 ||| 6143 ||| 6144 ||| 
2019 ||| sentiment analysis based on attention mechanisms and bi-directional lstm fusion model. ||| 6145 ||| 6146 ||| 3127 ||| 6147 ||| 369 ||| 6148 ||| 6149 ||| 
2019 ||| attention-based adaptive sampling for continuous emg data streams. ||| 6150 ||| 6151 ||| 3248 ||| 6152 ||| 
2018 ||| embedding-level attention and multi-scale convolutional neural networks for behaviour modelling. ||| 6153 ||| 6154 ||| 6155 ||| 
2019 ||| visual attention-based object detection in cluttered environments. ||| 6156 ||| 6157 ||| 6158 ||| 5706 ||| 
2019 ||| a driving attention detection method based on head pose. ||| 6159 ||| 6160 ||| 6161 ||| 6162 ||| 6163 ||| 
2019 ||| msahta: mixed spatial attention and hierarchical temporal aggregation for action recognition. ||| 6164 ||| 6165 ||| 6166 ||| 6167 ||| 6168 ||| 6169 ||| 
2019 ||| a novel relationship extraction scheme based on negative feedback attention. ||| 6170 ||| 2058 ||| 6171 ||| 6172 ||| 6173 ||| 6174 ||| 
2017 ||| dnn-based approach for identification of the level of attention of the tv-viewers using iot network. ||| 6175 ||| 6176 ||| 6177 ||| 
2019 ||| lstm based semi-supervised attention framework for sentiment analysis. ||| 6178 ||| 4211 ||| 4210 ||| 6179 ||| 4214 ||| 
2017 ||| a votable concept mapping approach to promoting students' attentional behavior: based on the evidence of mining sequential behavioral patterns. ||| 6180 ||| 6181 ||| 6182 ||| 6183 ||| 6184 ||| 6185 ||| 
2020 ||| deephop on edge: hop-by-hop routing bydistributed learning with semantic attention. ||| 6186 ||| 6187 ||| 6188 ||| 6189 ||| 6190 ||| 5010 ||| 6191 ||| 
2020 ||| deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence. ||| 6192 ||| 6193 ||| 6194 ||| 6195 ||| 
2022 ||| early rumor detection based on data augmentation and pre-training transformer. ||| 6196 ||| 6197 ||| 6198 ||| 6199 ||| 6200 ||| 6201 ||| 6202 ||| 
2021 ||| video anomaly detection method based on future frame prediction and attention mechanism. ||| 6203 ||| 6204 ||| 6205 ||| 
2020 ||| improve accuracy of speech emotion recognition with attention head fusion. ||| 6206 ||| 2532 ||| 6207 ||| 
2020 ||| evaluating human-machine translation with attention mechanisms for industry 4.0 environment sql-based systems. ||| 6208 ||| 6209 ||| 6210 ||| 6211 ||| 6212 ||| 
2019 ||| use of comtrade fault current data to test inductive current transformers. ||| 5590 ||| 5591 ||| 5627 ||| 6213 ||| 
2019 ||| graph-based attention networks for aspect level sentiment analysis. ||| 927 ||| 928 ||| 929 ||| 930 ||| 5148 ||| 
2019 ||| sentiment-aware short text classification based on convolutional neural network and attention. ||| 6214 ||| 4716 ||| 6215 ||| 6216 ||| 6217 ||| 
2021 ||| transformer based multi-output regression learning for wastewater treatment. ||| 6218 ||| 6219 ||| 340 ||| 6220 ||| 6221 ||| 
2020 ||| cfgat: a coarse-to-fine graph attention network for semi-supervised node classification. ||| 6222 ||| 6223 ||| 6224 ||| 6225 ||| 
2021 ||| fashion landmark detection via deep residual spatial attention network. ||| 3049 ||| 4287 ||| 6226 ||| 
2020 ||| enhanced soft attention mechanism with an inception-like module for image captioning. ||| 6227 ||| 6228 ||| 3049 ||| 528 ||| 
2017 ||| an attention mechanism for neural answer selection using a combined global and local view. ||| 6229 ||| 6230 ||| 6231 ||| 6232 ||| 6233 ||| 852 ||| 6234 ||| 6235 ||| 6236 ||| 6237 ||| 
2020 ||| sentiment-aware transformer using joint training. ||| 6238 ||| 6239 ||| 6240 ||| 
2021 ||| a supervisory mask attentional network for person re-identification in uniform dress scenes. ||| 1717 ||| 6241 ||| 602 ||| 6242 ||| 6243 ||| 
2021 ||| query-based summarization using reinforcement learning and transformer model. ||| 6244 ||| 6245 ||| 
2020 ||| solving open shop scheduling problem via graph attention neural network. ||| 4807 ||| 6246 ||| 3433 ||| 6247 ||| 
2019 ||| graph attention networks for neural social recommendation. ||| 6248 ||| 6249 ||| 6250 ||| 6251 ||| 
2019 ||| sparse high-level attention networks for person re-identification. ||| 6252 ||| 613 ||| 264 ||| 6253 ||| 
2021 ||| kgat-sr: knowledge-enhanced graph attention network for session-based recommendation. ||| 6254 ||| 6255 ||| 6256 ||| 4716 ||| 
2021 ||| cvt-assd: convolutional vision-transformer based attentive single shot multibox detector. ||| 6257 ||| 6258 ||| 6259 ||| 
2021 ||| expert-guided policy optimization by latent space planning with attention. ||| 6260 ||| 6261 ||| 6262 ||| 6263 ||| 
2021 ||| enhanced-memory transformer for coherent paragraph video captioning. ||| 6264 ||| 6265 ||| 227 ||| 6266 ||| 6267 ||| 
2019 ||| math expression image retrieval via attention-based framework. ||| 6268 ||| 6269 ||| 6270 ||| 6271 ||| 329 ||| 
2019 ||| aerns: attention-based entity region networks for multi-grained named entity recognition. ||| 6272 ||| 717 ||| 6273 ||| 6274 ||| 6275 ||| 
2021 ||| global attention augmentation ghost module: more features from lightweight global attention extraction. ||| 6276 ||| 6277 ||| 6278 ||| 6279 ||| 6280 ||| 6281 ||| 
2020 ||| st-mgat: spatial-temporal multi-head graph attention networks for traffic forecasting. ||| 6282 ||| 6283 ||| 6284 ||| 6285 ||| 
2021 ||| quantum-inspired hierarchical attention mechanism for question answering. ||| 6286 ||| 
2021 ||| hierarchical triplet attention pooling for graph classification. ||| 6287 ||| 6288 ||| 6289 ||| 2628 ||| 
2019 ||| targeted sentiment classification with knowledge powered attention network. ||| 6290 ||| 717 ||| 6291 ||| 6274 ||| 6292 ||| 
2021 ||| multi-task learning with attention : constructing auxiliary tasks for learning to learn. ||| 6293 ||| 6294 ||| 
2021 ||| user-guided image inpatinting with transformer. ||| 6295 ||| 1446 ||| 
2020 ||| graph attention auto-encoders. ||| 6296 ||| 6297 ||| 
2019 ||| distant-supervised relation extraction with hierarchical attention based on knowledge graph. ||| 6298 ||| 6299 ||| 6300 ||| 6301 ||| 6302 ||| 6303 ||| 
2021 ||| zero or few shot knowledge graph completions by text enhancement with multi-grained attention. ||| 6304 ||| 
2019 ||| integrating an attention mechanism and deep neural network for detection of dga domain names. ||| 6305 ||| 6306 ||| 6307 ||| 
2020 ||| position and channel attention for image inpainting by semantic structure. ||| 6295 ||| 1446 ||| 
2017 ||| neural named entity recognition using a self-attention mechanism. ||| 6230 ||| 6229 ||| 6308 ||| 6231 ||| 6233 ||| 
2021 ||| utilizing external knowledge with multi-granularity attention for review reading comprehension. ||| 6309 ||| 6310 ||| 
2020 ||| bi-directional self-attention with relative positional encoding for video summarization. ||| 6311 ||| 6312 ||| 
2021 ||| fragan-vsr: frame-recurrent attention generative adversarial network for video super-resolution. ||| 169 ||| 6313 ||| 6314 ||| 6249 ||| 398 ||| 1976 ||| 3279 ||| 
2019 ||| ecpnet: an efficient attention-based convolution network with pseudo-3d block for human action recognition. ||| 6315 ||| 5524 ||| 6316 ||| 
2018 ||| multivariate attention network for image captioning. ||| 6317 ||| 6318 ||| 5746 ||| 
2020 ||| sequential view synthesis with transformer. ||| 6319 ||| 6320 ||| 6321 ||| 6322 ||| 
2018 ||| skeleton transformer networks: 3d human pose and skinned mesh from single rgb image. ||| 6323 ||| 6324 ||| 6325 ||| 6326 ||| 
2020 ||| transforming multi-concept attention into video summarization. ||| 6327 ||| 2376 ||| 6328 ||| 
2020 ||| attention-aware feature aggregation for real-time stereo matching on edge devices. ||| 6329 ||| 6330 ||| 6331 ||| 
2018 ||| coarse-to-fine: a rnn-based hierarchical attention model for vehicle re-identification. ||| 6332 ||| 6333 ||| 6334 ||| 6335 ||| 1863 ||| 
2020 ||| channel recurrent attention networks for video pedestrian retrieval. ||| 2127 ||| 6336 ||| 2128 ||| 2130 ||| 2131 ||| 
2018 ||| large scale scene text verification with guided attention. ||| 6337 ||| 6338 ||| 6339 ||| 6340 ||| 6341 ||| 1695 ||| 6342 ||| 6343 ||| 
2020 ||| rgb-d co-attention network for semantic segmentation. ||| 2736 ||| 6344 ||| 6345 ||| 5158 ||| 5157 ||| 
2018 ||| paying attention to style: recognizing photo styles with convolutional attentional units. ||| 6346 ||| 6347 ||| 6348 ||| 
2020 ||| spatial temporal attention graph convolutional networks with mechanics-stream for skeleton-based action recognition. ||| 6349 ||| 723 ||| 724 ||| 725 ||| 
2018 ||| automatic graphics program generation using attention-based hierarchical decoder. ||| 2958 ||| 6350 ||| 6351 ||| 
2018 ||| deep attention-based classification network for robust depth prediction. ||| 6352 ||| 6353 ||| 6335 ||| 6354 ||| 6355 ||| 6356 ||| 
2020 ||| bi-directional attention for joint instance and semantic segmentation in point clouds. ||| 6357 ||| 6358 ||| 1239 ||| 6359 ||| 
2018 ||| a2a: attention to attention reasoning for movie question answering. ||| 6360 ||| 6361 ||| 6362 ||| 6363 ||| 
2020 ||| second order enhanced multi-glimpse attention in visual question answering. ||| 4103 ||| 6364 ||| 6365 ||| 
2020 ||| hierarchical x-ray report generation via pathology tags and multi head attention. ||| 6366 ||| 6367 ||| 6368 ||| 6369 ||| 
2018 ||| gated hierarchical attention for image captioning. ||| 6370 ||| 6371 ||| 
2020 ||| epsnet: efficient panoptic segmentation network with cross-layer attention fusion. ||| 6372 ||| 6373 ||| 6374 ||| 6375 ||| 
2020 ||| audiovisual transformer with instance attention for audio-visual event localization. ||| 6376 ||| 6328 ||| 
2020 ||| frequency attention network: blind noise removal for real images. ||| 6377 ||| 6378 ||| 6379 ||| 6380 ||| 6381 ||| 6382 ||| 
2020 ||| rotation axis focused attention network (rafa-net) for estimating head pose. ||| 6383 ||| 6384 ||| 6385 ||| 4611 ||| 
2018 ||| video-based person re-identification via 3d convolutional networks and non-local attention. ||| 6386 ||| 6387 ||| 6388 ||| 1460 ||| 
2020 ||| decoupled spatial-temporal attention network for skeleton-based action-gesture recognition. ||| 6389 ||| 2341 ||| 2343 ||| 2080 ||| 
2020 ||| attention-based fine-grained classification of bone marrow cells. ||| 6390 ||| 6391 ||| 6392 ||| 6393 ||| 6394 ||| 6395 ||| 4482 ||| 
2020 ||| in-sample contrastive learning and consistent attention for weakly supervised object localization. ||| 2635 ||| 2636 ||| 6396 ||| 2637 ||| 
2020 ||| local context attention for salient object segmentation. ||| 2214 ||| 6397 ||| 6398 ||| 6399 ||| 6400 ||| 
2018 ||| summarizing videos with attention. ||| 6401 ||| 6402 ||| 6403 ||| 6404 ||| 6405 ||| 
2020 ||| part-aware attention network for person re-identification. ||| 6406 ||| 2489 ||| 2490 ||| 241 ||| 
2020 ||| afn: attentional feedback network based 3d terrain super-resolution. ||| 6407 ||| 6408 ||| 6409 ||| 6410 ||| 
2020 ||| modular graph attention network for complex visual relational reasoning. ||| 6411 ||| 6412 ||| 6413 ||| 6414 ||| 6415 ||| 6416 ||| 6417 ||| 
2018 ||| semantic aware attention based deep object co-segmentation. ||| 6418 ||| 6419 ||| 6420 ||| 
2020 ||| image captioning through image transformer. ||| 2195 ||| 2407 ||| 1852 ||| 2410 ||| 2409 ||| 2603 ||| 
2020 ||| spatial and channel attention modulated network for medical image segmentation. ||| 6421 ||| 6422 ||| 
2018 ||| predicting driver attention in critical situations. ||| 6423 ||| 6424 ||| 2041 ||| 6425 ||| 6426 ||| 6427 ||| 
2020 ||| parallel-connected residual channel attention network for remote sensing image super-resolution. ||| 6428 ||| 1526 ||| 1550 ||| 1528 ||| 
2018 ||| multilevel collaborative attention network for person search. ||| 6429 ||| 6430 ||| 6431 ||| 5277 ||| 
2020 ||| accurate and efficient single image super-resolution with matrix channel attention network. ||| 6432 ||| 6433 ||| 1099 ||| 
2020 ||| class: cross-level attention and supervision for salient objects detection. ||| 6434 ||| 1717 ||| 
2020 ||| multi-label x-ray imagery classification via bottom-up attention and meta fusion. ||| 6435 ||| 1460 ||| 6436 ||| 6437 ||| 6438 ||| 
2018 ||| attentionmask: attentive, efficient object proposal generation focusing on small objects. ||| 6439 ||| 5736 ||| 
2018 ||| transformer design for 77-ghz down-converter in 28-nm fd-soi cmos technology. ||| 6440 ||| 6441 ||| 6442 ||| 6443 ||| 6444 ||| 
2018 ||| an attention-based recurrent neural networks framework for health data analysis. ||| 1628 ||| 1071 ||| 1629 ||| 930 ||| 1630 ||| 1632 ||| 6445 ||| 1631 ||| 
2021 ||| assessing internal and external attention in ar using brain computer interfaces: a pilot study. ||| 6446 ||| 6447 ||| 6448 ||| 6449 ||| 6450 ||| 6451 ||| 
2017 ||| modeling and detecting student attention and interest level using wearable computers. ||| 6452 ||| 6453 ||| 6454 ||| 
2021 ||| dual attention based network with hierarchical convlstm for video object segmentation. ||| 6455 ||| 6456 ||| 
2021 ||| lf-magnet: learning mutual attention guidance of sub-aperture images for light field image super-resolution. ||| 5166 ||| 4151 ||| 6457 ||| 6458 ||| 6459 ||| 6460 ||| 
2021 ||| attention-based node-edge graph convolutional networks for identification of autism spectrum disorder using multi-modal mri data. ||| 2957 ||| 6461 ||| 6462 ||| 6463 ||| 6464 ||| 6465 ||| 6466 ||| 6467 ||| 
2019 ||| spatial-temporal fusion network with residual learning and attention mechanism: a benchmark for video-based group re-id. ||| 6468 ||| 1007 ||| 6469 ||| 
2018 ||| attention forest for semantic segmentation. ||| 6470 ||| 6471 ||| 6472 ||| 
2021 ||| saliencybert: recurrent attention network for target-oriented multimodal sentiment classification. ||| 6473 ||| 6474 ||| 6475 ||| 6476 ||| 6477 ||| 
2019 ||| an automated method with attention network for cervical cancer scanning. ||| 6478 ||| 6479 ||| 6480 ||| 6481 ||| 6482 ||| 6483 ||| 
2021 ||| attention template update model for siamese tracker. ||| 6484 ||| 6485 ||| 116 ||| 
2020 ||| h-at: hybrid attention transfer for knowledge distillation. ||| 6486 ||| 2400 ||| 2401 ||| 
2019 ||| learning attention regularization correlation filter for visual tracking. ||| 6487 ||| 6488 ||| 6489 ||| 6490 ||| 
2021 ||| ipe transformer for depth completion with input-aware positional embeddings. ||| 6491 ||| 2550 ||| 6492 ||| 6493 ||| 6494 ||| 6495 ||| 1700 ||| 
2021 ||| joint attention mechanism for unsupervised video object segmentation. ||| 1828 ||| 5858 ||| 6496 ||| 6497 ||| 6498 ||| 
2021 ||| image tampering localization using unified two-stream features enhanced with channel and spatial attention. ||| 6499 ||| 6500 ||| 6501 ||| 6502 ||| 
2021 ||| cetransformer: casual effect estimation via transformer based representation learning. ||| 6503 ||| 6504 ||| 6505 ||| 6506 ||| 6507 ||| 
2021 ||| idanet: iterative d-linknets with attention for road extraction from high-resolution satellite imagery. ||| 6508 ||| 6509 ||| 5200 ||| 5198 ||| 6510 ||| 
2020 ||| pavement crack detection using attention u-net with multiple sources. ||| 6511 ||| 295 ||| 6512 ||| 6513 ||| 6514 ||| 
2020 ||| weakly supervised pedestrian attribute recognition with attention in latent space. ||| 6515 ||| 1007 ||| 6516 ||| 
2021 ||| early diagnosis of alzheimer's disease using 3d residual attention network based on hippocampal multi-indices feature fusion. ||| 6517 ||| 6518 ||| 692 ||| 6519 ||| 6520 ||| 6521 ||| 6522 ||| 
2021 ||| insight on attention modules for skeleton-based action recognition. ||| 6523 ||| 6524 ||| 6525 ||| 
2021 ||| group re-identification based on single feature attention learning network (sfaln). ||| 6526 ||| 6527 ||| 6528 ||| 
2020 ||| attention-based network for semantic image segmentation via adversarial learning. ||| 6529 ||| 6530 ||| 6531 ||| 6532 ||| 6533 ||| 6534 ||| 
2019 ||| shellfish detection based on fusion attention mechanism in end-to-end network. ||| 6535 ||| 1967 ||| 6536 ||| 6537 ||| 6538 ||| 
2021 ||| feature enhancement and multi-scale cross-modal attention for rgb-d salient object detection. ||| 6539 ||| 5908 ||| 6540 ||| 748 ||| 6541 ||| 6542 ||| 
2021 ||| magan: multi-attention generative adversarial networks for text-to-image generation. ||| 6543 ||| 6544 ||| 6545 ||| 
2020 ||| mhasiam: mixed high-order attention siamese network for real-time visual tracking. ||| 6546 ||| 6547 ||| 6548 ||| 6549 ||| 6488 ||| 6550 ||| 
2021 ||| interactive attention sampling network for clinical skin disease image classification. ||| 6551 ||| 6552 ||| 6553 ||| 6554 ||| 
2018 ||| deep word association: a flexible chinese word association method with iterative attention mechanism. ||| 6555 ||| 6556 ||| 6557 ||| 6558 ||| 6559 ||| 
2020 ||| residual attention siameserpn for visual tracking. ||| 6560 ||| 6561 ||| 6562 ||| 
2019 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| 6563 ||| 6564 ||| 6565 ||| 6566 ||| 6567 ||| 
2020 ||| lsam: local spatial attention module. ||| 6568 ||| 6569 ||| 382 ||| 
2019 ||| a simple and robust attentional encoder-decoder model for license plate recognition. ||| 6570 ||| 5845 ||| 6571 ||| 6572 ||| 
2021 ||| multi-directional attention network for segmentation of pediatric echocardiographic. ||| 6573 ||| 6574 ||| 6575 ||| 6576 ||| 6577 ||| 6578 ||| 6579 ||| 6580 ||| 6581 ||| 5476 ||| 6582 ||| 
2021 ||| relational attention with textual enhanced transformer for image captioning. ||| 6583 ||| 6584 ||| 6585 ||| 6586 ||| 2252 ||| 
2021 ||| hpcreg-net: unsupervised u-net integrating dilated convolution and residual attention for hippocampus registration. ||| 6587 ||| 6518 ||| 692 ||| 6519 ||| 6520 ||| 6521 ||| 6522 ||| 
2021 ||| a guided attention 4d convolutional neural network for modeling spatio-temporal patterns of functional brain networks. ||| 6461 ||| 3473 ||| 6462 ||| 6588 ||| 6589 ||| 6590 ||| 2957 ||| 6463 ||| 6591 ||| 6592 ||| 6593 ||| 6466 ||| 6467 ||| 
2018 ||| attention enhanced convnet-rnn for chinese vehicle license plate recognition. ||| 6594 ||| 6595 ||| 6596 ||| 3337 ||| 6597 ||| 
2020 ||| an attention enhanced graph convolutional network for semantic segmentation. ||| 6598 ||| 6599 ||| 
2018 ||| facial expression recognition based on region-wise attention and geometry difference. ||| 6600 ||| 490 ||| 489 ||| 
2020 ||| inception parallel attention network for small object detection in remote sensing images. ||| 6601 ||| 6602 ||| 6603 ||| 6604 ||| 1307 ||| 6605 ||| 5819 ||| 2058 ||| 
2021 ||| non-significant information enhancement based attention network for face anti-spoofing. ||| 6606 ||| 6607 ||| 1825 ||| 
2021 ||| facial expression recognition based on multi-scale feature fusion convolutional neural network and attention mechanism. ||| 6608 ||| 6123 ||| 6609 ||| 
2019 ||| adsrnet: attention-based densely connected network for image super-resolution. ||| 6610 ||| 4151 ||| 6611 ||| 6612 ||| 5166 ||| 
2020 ||| multi-cue and temporal attention for person recognition in videos. ||| 6613 ||| 411 ||| 6614 ||| 6615 ||| 
2021 ||| pyramid self-attention for semantic segmentation. ||| 6616 ||| 2219 ||| 2405 ||| 6617 ||| 2222 ||| 
2018 ||| multi-attention guided activation propagation in cnns. ||| 6618 ||| 5954 ||| 
2020 ||| lightweight image super-resolution with local attention enhancement. ||| 6619 ||| 6620 ||| 6621 ||| 6622 ||| 
2019 ||| pointnet-based channel attention vlad network. ||| 6623 ||| 6624 ||| 6625 ||| 
2021 ||| relation-guided actor attention for group activity recognition. ||| 6626 ||| 6627 ||| 1155 ||| 6628 ||| 6629 ||| 
2020 ||| cross-view image synthesis with deformable convolution and attention mechanism. ||| 6630 ||| 6631 ||| 435 ||| 2258 ||| 6632 ||| 6633 ||| 
2020 ||| principal semantic feature analysis with covariance attention. ||| 6634 ||| 6635 ||| 6636 ||| 6637 ||| 
2021 ||| tmd-fs: improving few-shot object detection with transformer multi-modal directing. ||| 6638 ||| 6478 ||| 6639 ||| 6640 ||| 
2018 ||| domain attention model for domain generalization in object detection. ||| 6641 ||| 490 ||| 6528 ||| 
2021 ||| multi-level residual attention network for speckle suppression. ||| 1379 ||| 3854 ||| 6642 ||| 6643 ||| 6644 ||| 
2019 ||| multi-scale spatial-temporal attention for action recognition. ||| 6645 ||| 4257 ||| 4259 ||| 
2021 ||| cross-modality attention method for medical image enhancement. ||| 6646 ||| 5170 ||| 6647 ||| 6648 ||| 
2019 ||| attention-based label consistency for semi-supervised deep learning. ||| 5394 ||| 5395 ||| 
2019 ||| local and global feature learning for subtle facial expression recognition from attention perspective. ||| 6649 ||| 6650 ||| 6651 ||| 
2018 ||| attention-based convolutional networks for ship detection in high-resolution remote sensing images. ||| 6652 ||| 6653 ||| 6654 ||| 
2021 ||| attention guided spatio-temporal artifacts extraction for deepfake detection. ||| 6655 ||| 633 ||| 6656 ||| 4400 ||| 
2019 ||| facial expression recognition: disentangling expression based on self-attention conditional generative adversarial nets. ||| 6657 ||| 6658 ||| 6659 ||| 6660 ||| 6661 ||| 
2018 ||| prohibited item detection in airport x-ray security images via attention mechanism based cnn. ||| 6662 ||| 6663 ||| 6664 ||| 
2021 |||  13.3 db gain sub-6 ghz to 28 ghz transformer-coupled low-voltage upconversion mixer for 5g applications. ||| 6665 ||| 6666 ||| 
2018 ||| a 15mw -105dbm image-sparse-sliding-if receiver with transformer-based on-chip q-enhanced rf matching network for a 113db-link-budget ble 5.0 trx. ||| 6667 ||| 6668 ||| 6669 ||| 6670 ||| 6671 ||| 6672 ||| 6673 ||| 
2019 ||| a 78 fs rms jitter injection-locked clock multiplier using transformer-based ultra-low-power vco. ||| 6674 ||| 6675 ||| 6676 ||| 6677 ||| 6678 ||| 6679 ||| 6680 ||| 6681 ||| 6682 ||| 
2017 ||| gender, age, colour, position and stress: how they influence attention at workplace? ||| 6683 ||| 6684 ||| 6685 ||| 6686 ||| 
2020 ||| detection of obstacle features using neural networks with attention in the task of autonomous navigation of mobile robots. ||| 6687 ||| 6688 ||| 6689 ||| 6690 ||| 6691 ||| 
2018 ||| mental war: an attention-based single/multiplayer brain-computer interface game. ||| 6692 ||| 6693 ||| 6694 ||| 
2019 ||| scenes segmentation in self-driving car navigation system using neural network models with attention. ||| 6687 ||| 6695 ||| 6696 ||| 6689 ||| 6697 ||| 
2020 ||| spectral super-resolution using hybrid 2d-3d structure tensor attention networks with camera spectral sensitivity prior. ||| 6698 ||| 6699 ||| 6700 ||| 6701 ||| 
2019 ||| attention based residual network for high-resolution remote sensing imagery scene classification. ||| 6702 ||| 6703 ||| 6704 ||| 6705 ||| 
2018 ||| salient object detection via double sparse representations under visual attention guidance. ||| 1894 ||| 5082 ||| 6706 ||| 6707 ||| 
2018 ||| ai-net: attention inception neural networks for hyperspectral image classification. ||| 6708 ||| 6650 ||| 6627 ||| 
2021 ||| remote sensing image change detection based on fully convolutional network with pyramid attention. ||| 6709 ||| 6710 ||| 
2020 ||| dense docked ship detection via spatial group-wise enhance attention in sar images. ||| 6711 ||| 6712 ||| 6713 ||| 6714 ||| 
2021 ||| self-attention fusion module for single remote sensing image super-resolution. ||| 6715 ||| 6716 ||| 6717 ||| 
2021 ||| siammraan: siamese multi-level residual attention adaptive network for hyperspectral videos tracking. ||| 6718 ||| 6719 ||| 2973 ||| 6720 ||| 
2019 ||| multiscale ship detection based on dense attention pyramid network in sar images. ||| 6721 ||| 6722 ||| 6712 ||| 6723 ||| 6724 ||| 
2021 ||| deep vision transformers for remote sensing scene classification. ||| 6725 ||| 6726 ||| 6727 ||| 
2021 ||| hyperspectral imagery super-resolution based on self-calibrated attention residual network. ||| 6728 ||| 6719 ||| 6729 ||| 6720 ||| 
2021 ||| small vessel detection based on adaptive dual-polarimetric sar feature fusion and attention-enhanced feature pyramid network. ||| 6730 ||| 6731 ||| 2532 ||| 6732 ||| 6733 ||| 
2017 ||| sar image change detection method based on visual attention. ||| 2349 ||| 1589 ||| 6734 ||| 6735 ||| 6736 ||| 
2019 ||| attention networks for band weighting and selection in hyperspectral remote sensing image classification. ||| 4550 ||| 1086 ||| 861 ||| 6737 ||| 
2020 ||| hyperspectral image classification based on semi-supervised dual-branch convolutional autoencoder with self-attention. ||| 6738 ||| 6739 ||| 951 ||| 6740 ||| 6617 ||| 397 ||| 
2019 ||| attention-based domain adaptation for hyperspectral image classification. ||| 6741 ||| 6742 ||| 6720 ||| 6743 ||| 
2020 ||| spatial attention network for road extraction. ||| 6744 ||| 6745 ||| 6746 ||| 6747 ||| 
2019 ||| high-order self-attention network for remote sensing scene classification. ||| 6748 ||| 6749 ||| 1329 ||| 6750 ||| 
2021 ||| attention neural network semblance velocity auto picking with reference velocity curve data augmentation. ||| 6751 ||| 6752 ||| 6753 ||| 6754 ||| 5860 ||| 6755 ||| 
2021 ||| triplet attention feature fusion network for sar and optical image land cover classification. ||| 6756 ||| 6757 ||| 6758 ||| 6759 ||| 6760 ||| 
2019 ||| a global point-sift attention network for 3d point cloud semantic segmentation. ||| 6761 ||| 6762 ||| 6763 ||| 
2019 ||| aircraft detection from remote sensing image based on a weakly supervised attention model. ||| 6764 ||| 6514 ||| 5937 ||| 6765 ||| 6766 ||| 6767 ||| 
2020 ||| instance-aware remote sensing image captioning with cross-hierarchy attention. ||| 6768 ||| 6769 ||| 6650 ||| 
2021 ||| attention based semantic segmentation on uav dataset for natural disaster damage assessment. ||| 6770 ||| 6771 ||| 
2021 ||| a multi-branch network based on weight sharing and attention mechanism for hyperspectral image classification. ||| 6772 ||| 6773 ||| 1199 ||| 
2021 ||| attention mechanism for land cover mapping with image-level labels. ||| 6774 ||| 6775 ||| 6776 ||| 6777 ||| 6778 ||| 6779 ||| 
2019 ||| super-resolution of sentinel-2 images based on deep channel-attention residual network. ||| 6780 ||| 6781 ||| 6782 ||| 
2021 ||| channel-based attention for land cover classification using sentinel-2 time series. ||| 6783 ||| 6784 ||| 6785 ||| 6786 ||| 6787 ||| 6788 ||| 6789 ||| 
2021 ||| self-attention generative adversarial networks for times series vhr multispectral image generation. ||| 6790 ||| 6791 ||| 6792 ||| 6793 ||| 
2020 ||| dilated residual network based on dual expectation maximization attention for semantic segmentation of remote sensing images. ||| 6794 ||| 6795 ||| 6699 ||| 6698 ||| 6700 ||| 
2019 ||| multiscale ship detection based on dense attention pyramid network in sar images. ||| 6721 ||| 6722 ||| 6712 ||| 6723 ||| 6724 ||| 
2021 ||| daff-net: dual attention feature fusion network for aircraft detection in remote sensing images. ||| 6796 ||| 6797 ||| 778 ||| 1321 ||| 6798 ||| 
2020 ||| se-hrnet: a deep high-resolution network with attention for remote sensing scene classification. ||| 3535 ||| 1321 ||| 6799 ||| 6703 ||| 
2021 ||| hyperspectral classification based on spectral indices learned through soft attention units. ||| 6800 ||| 2253 ||| 6801 ||| 6802 ||| 
2021 ||| a spatial-temporal-channel attention unet++ for high resolution remote sensing image change detection. ||| 6803 ||| 6804 ||| 6805 ||| 6806 ||| 6807 ||| 6808 ||| 
2021 ||| hyperspectral and lidar data classification based on linear self-attention. ||| 6809 ||| 6810 ||| 6811 ||| 2628 ||| 
2021 ||| spectral and spatial residual attention network for joint hyperspectral and lidar data classification. ||| 4550 ||| 1086 ||| 6812 ||| 6813 ||| 
2021 ||| glacier calving front segmentation using attention u-net. ||| 6814 ||| 6815 ||| 6816 ||| 6817 ||| 6818 ||| 6819 ||| 
2021 ||| a multi-scale feature aggregation network based on channel-spatial attention for remote sensing scene classification. ||| 765 ||| 6820 ||| 6821 ||| 6822 ||| 
2021 ||| multi-view attention network for remote sensing image captioning. ||| 6823 ||| 6824 ||| 6825 ||| 6826 ||| 6827 ||| 6828 ||| 6829 ||| 400 ||| 
2019 ||| semantic segmentation of high resolution remote sensing image based on batch-attention mechanism. ||| 6830 ||| 6831 ||| 214 ||| 6832 ||| 2343 ||| 
2020 ||| remote sensing scene classification using spatial transformer fusion network. ||| 6833 ||| 6834 ||| 6835 ||| 6836 ||| 497 ||| 6837 ||| 
2021 ||| dual lightweight network with attention and feature fusion for semantic segmentation of high-resolution remote sensing images. ||| 6838 ||| 6839 ||| 6840 ||| 6841 ||| 2343 ||| 
2021 ||| ptgan: a proposal-weighted two-stage gan with attention for hyperspectral target detection. ||| 6842 ||| 6843 ||| 6701 ||| 6844 ||| 3806 ||| 6720 ||| 
2021 ||| research on fracture recognition in well logging images: adversarial learning with attention. ||| 781 ||| 6746 ||| 5145 ||| 6845 ||| 6846 ||| 
2021 ||| modified deep transformers for gnss time series prediction. ||| 6847 ||| 6848 ||| 
2018 ||| attention-based convolutional neural network for the detection of built-up areas in high-resolution sar images. ||| 6849 ||| 6465 ||| 6850 ||| 
2021 ||| self-attention and mutual-attention for few-shot hyperspectral image classification. ||| 1215 ||| 6759 ||| 6758 ||| 6760 ||| 
2020 ||| cloud detection using gabor filters and attention-based convolutional neural network for remote sensing images. ||| 875 ||| 6851 ||| 1341 ||| 4061 ||| 6701 ||| 
2021 ||| sar image object detection based on improved cross-entropy loss function with the attention of hard samples. ||| 6852 ||| 6853 ||| 6854 ||| 400 ||| 6855 ||| 6856 ||| 
2019 ||| spectral-spatial classification of hyperspectral image based on a joint attention network. ||| 6857 ||| 6858 ||| 6859 ||| 6860 ||| 6861 ||| 6862 ||| 6863 ||| 
2021 ||| attention-driven cross-modal remote sensing image retrieval. ||| 6864 ||| 6865 ||| 6866 ||| 6867 ||| 
2020 ||| deep residual spatial attention network for hyperspectral pansharpening. ||| 6868 ||| 6699 ||| 6701 ||| 6869 ||| 6870 ||| 
2020 ||| towards natural language question answering over earth observation linked data using attention-based neural machine translation. ||| 6871 ||| 6872 ||| 6873 ||| 
2021 ||| triple attention network for multi-class semantic segmentation in aerial images. ||| 6874 ||| 6875 ||| 6876 ||| 6877 ||| 6878 ||| 6879 ||| 
2021 ||| an attention-based system for damage assessment using satellite imagery. ||| 6880 ||| 6881 ||| 6882 ||| 6883 ||| 6884 ||| 6885 ||| 6886 ||| 6887 ||| 6888 ||| 
2020 ||| complex-valued spatial-scattering separated attention network for polsar image classification. ||| 6889 ||| 6890 ||| 6891 ||| 128 ||| 6892 ||| 6637 ||| 
2020 ||| light-weight attention semantic segmentation network for high-resolution remote sensing images. ||| 6893 ||| 6841 ||| 6894 ||| 6838 ||| 2343 ||| 
2021 ||| attention residual u-net for building segmentation in aerial images. ||| 6895 ||| 6896 ||| 6897 ||| 6898 ||| 6899 ||| 6900 ||| 6901 ||| 
2019 ||| a novel multi-attention driven system for multi-label remote sensing image classification. ||| 6902 ||| 6903 ||| 6904 ||| 
2021 ||| adaptive channel attention and feature super-resolution for remote sensing images spatiotemporal fusion. ||| 6905 ||| 6906 ||| 1903 ||| 875 ||| 6907 ||| 
2020 ||| hierarchical attention for ship detection in sar images. ||| 6908 ||| 6909 ||| 6910 ||| 6911 ||| 
2019 ||| hyperspectral target detection via deep multiple instance self-attention neural network. ||| 6912 ||| 6913 ||| 6914 ||| 6915 ||| 6916 ||| 6617 ||| 6917 ||| 
2021 ||| dsamnet: a deeply supervised attention metric based network for change detection of high-resolution images. ||| 6918 ||| 6919 ||| 
2021 ||| attention based convolution autoencoder for dimensionality reduction in hyperspectral images. ||| 6920 ||| 6865 ||| 
2018 ||| attention based network for remote sensing scene classification. ||| 6921 ||| 6627 ||| 6922 ||| 
2021 ||| polsar image classification with complex-valued residual attention enhanced u-net. ||| 6923 ||| 6924 ||| 
2020 ||| semantic segmentation of urban buildings from vhr remotely sensed imagery using attention-based cnn. ||| 6925 ||| 6926 ||| 6170 ||| 
2021 ||| residual attention mechanism for construction disturbance detection from satellite image. ||| 6927 ||| 6928 ||| 2230 ||| 6929 ||| 6930 ||| 6931 ||| 1007 ||| 
2020 ||| acmu-nets: attention cascading modular u-nets incorporating squeeze and excitation blocks. ||| 6932 ||| 6933 ||| 6934 ||| 
2020 ||| an improved convolutional block attention module for chinese character recognition. ||| 5384 ||| 6731 ||| 3248 ||| 6935 ||| 
2020 ||| multi-tensor fusion network with hybrid attention for multimodal sentiment analysis. ||| 6936 ||| 6937 ||| 6938 ||| 6939 ||| 
2020 ||| improving self-attention based news recommendation with document classification. ||| 6940 ||| 
2018 ||| identification of attracting attention in thai handicraft products using self-organizing map. ||| 6941 ||| 6942 ||| 
2020 ||| a novel chinese reading comprehension model based on attention mechanism and convolutional neural networks. ||| 6943 ||| 6944 ||| 6945 ||| 
2019 ||| short-text question classification based on dependency parsing and attention mechanism. ||| 6946 ||| 
2019 ||| atvr: an attention training system using multitasking and neurofeedback on virtual reality platform. ||| 6947 ||| 6948 ||| 1751 ||| 
2020 ||| a qoe and visual attention evaluation on the influence of spatial audio in 360 videos. ||| 6949 ||| 6950 ||| 6951 ||| 
2021 ||| a technical report for visual attention estimation in hmd challenge. ||| 6952 ||| 6953 ||| 
2019 ||| deep learning on vr-induced attention. ||| 858 ||| 6954 ||| 
2020 ||| attention estimation in virtual reality with eeg based image regression. ||| 6955 ||| 6956 ||| 6957 ||| 6958 ||| 6959 ||| 
2019 ||| development of internet of things platform and its application in remote monitoring and control of transformer operation. ||| 6960 ||| 6961 ||| 6962 ||| 
2019 ||| attention model for massive mimo csi compression feedback and recovery. ||| 6963 ||| 6964 ||| 6965 ||| 
2021 ||| graph attention network-based drl for network slicing management in dense cellular networks. ||| 6966 ||| 6967 ||| 6968 ||| 675 ||| 
2021 ||| qaan: question answering attention network for community question classification. ||| 6969 ||| 861 ||| 
2021 ||| ble beacon with user traffic awareness using deep correlation and attention network. ||| 6970 ||| 6971 ||| 
2021 ||| can: complementary attention network for aspect level sentiment classification in social e-commerce. ||| 6972 ||| 6306 ||| 2051 ||| 6973 ||| 6974 ||| 3433 ||| 
2019 ||| resilient combination of complementary cnn and rnn features for text classification through attention and ensembling. ||| 6975 ||| 6976 ||| 6977 ||| 6978 ||| 6979 ||| 
2020 ||| text sentiment analysis based on parallel tcn model and attention model. ||| 2829 ||| 6980 ||| 6981 ||| 
2020 ||| color recognition of vehicle based on low light enhancement and pixel-wise contextual attention. ||| 6982 ||| 6983 ||| 6984 ||| 5479 ||| 
2020 ||| investigation on transformer oil parameters using support vector machine. ||| 6985 ||| 6986 ||| 6987 ||| 
2021 ||| melanoma skin cancer detection using efficientnet and channel attention module. ||| 6988 ||| 6989 ||| 
2017 ||| human attention estimation by a domestic service robot using upper body skeletal information. ||| 6990 ||| 6991 ||| 6992 ||| 
2017 ||| possibility of blending sesame oil with field aged mineral oil for transformer applications. ||| 6993 ||| 6994 ||| 6995 ||| 6996 ||| 
2017 ||| application of sfra techniques to discriminate short circuit faults of transformer winding. ||| 6997 ||| 6998 ||| 6999 ||| 6994 ||| 6995 ||| 
2017 ||| comparison of coconut/sesame/castor oils and their blends for transformer insulation. ||| 6994 ||| 6995 ||| 6996 ||| 
2020 ||| single image super-resolution using residual channel attention network. ||| 7000 ||| 7001 ||| 7002 ||| 7003 ||| 
2021 ||| mining medication-effect relations from twitter data using pre-trained transformer language model. ||| 7004 ||| 7005 ||| 7006 ||| 
2020 ||| modeling dynamic heterogeneous network for link prediction using hierarchical attention with temporal rnn. ||| 7007 ||| 7008 ||| 6760 ||| 7009 ||| 7010 ||| 7011 ||| 
2020 ||| amqan: adaptive multi-attention question-answer networks for answer selection. ||| 769 ||| 861 ||| 767 ||| 247 ||| 7012 ||| 7013 ||| 7014 ||| 7015 ||| 
2018 ||| intelligent monitoring of transformer insulation using convolutional neural networks. ||| 7016 ||| 7017 ||| 7018 ||| 
2019 ||| semantically corroborating neural attention for biomedical question answering. ||| 7019 ||| 7020 ||| 7021 ||| 
2019 ||| transformer models for question answering at bioasq 2019. ||| 7022 ||| 7023 ||| 7024 ||| 7025 ||| 
2018 ||| multimodal tweet sentiment classification algorithm based on attention mechanism. ||| 7026 ||| 7027 ||| 
2021 ||| concad: contrastive learning-based cross attention for sleep apnea detection. ||| 7028 ||| 1071 ||| 
2018 ||| polar: attention-based cnn for one-shot personalized article recommendation. ||| 7029 ||| 7030 ||| 7031 ||| 
2019 ||| learning to detect online harassment on twitter with the transformer. ||| 7032 ||| 7033 ||| 7034 ||| 
2020 ||| a self-attention network based node embedding model. ||| 7035 ||| 7036 ||| 7037 ||| 
2019 ||| attention-based deep tropical cyclone rapid intensification prediction. ||| 7038 ||| 7039 ||| 7040 ||| 
2020 ||| gram-smot: top-n personalized bundle recommendation via graph attention mechanism and submodular optimization. ||| 7041 ||| 7042 ||| 7043 ||| 
2021 ||| the joy of dressing is an art: outfit generation using self-attention bi-lstm. ||| 7044 ||| 7045 ||| 7046 ||| 
2020 ||| self-attention enhanced patient journey understanding in healthcare system. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 
2021 ||| transformers: "the end of history" for natural language processing? ||| 7047 ||| 7048 ||| 7049 ||| 
2019 ||| sorecgat: leveraging graph attention mechanism for top-n social recommendation. ||| 7050 ||| 7042 ||| 7043 ||| 
2017 ||| sequence generation with target attention. ||| 4787 ||| 7051 ||| 4789 ||| 2305 ||| 4791 ||| 
2020 ||| benchmarking tropical cyclone rapid intensification with satellite images and attention-based deep models. ||| 7038 ||| 7039 ||| 7040 ||| 
2019 ||| attention-based method for categorizing different types of online harassment language. ||| 7052 ||| 7053 ||| 
2018 ||| discovering spatio-temporal latent influence in geographical attention dynamics. ||| 7054 ||| 7055 ||| 7056 ||| 7057 ||| 
2020 ||| lightweight temporal self-attention for classifying satellite images time series. ||| 2309 ||| 2310 ||| 2311 ||| 
2017 ||| sentiment analysis model based on structure attention mechanism. ||| 7058 ||| 7059 ||| 7060 ||| 
2020 ||| visualizing transformers for nlp: a brief survey. ||| 7061 ||| 7062 ||| 
2018 ||| extending attention span for children adhd using an attentive visual interface. ||| 7063 ||| 7064 ||| 7065 ||| 7066 ||| 
2020 ||| attention support with soft visual cues in control room environments. ||| 7067 ||| 7068 ||| 7069 ||| 
2019 ||| condition monitoring of interconnecting transformer through ann approach. ||| 7070 ||| 7071 ||| 
2021 ||| multimodal emotion recognition using a modified dense co-attention symmetric network. ||| 7072 ||| 683 ||| 684 ||| 
2019 ||| attentional correlates in somatosensory potentials evoked by ultrasound induced virtual objects in mid-air. ||| 7073 ||| 7074 ||| 
2019 ||| deep neural network with attention mechanism for classification of motor imagery eeg. ||| 7075 ||| 6329 ||| 7076 ||| 6331 ||| 
2017 ||| circular organization of the instantaneous phase in erps and the ongoing eeg due to selective attention. ||| 7077 ||| 7074 ||| 
2017 ||| effect of attention division on movement detection and execution in dual-task conditions. ||| 5557 ||| 5558 ||| 5559 ||| 5560 ||| 
2019 ||| phase transfer entropy between frontal and posterior regions during visual spatial attention. ||| 7078 ||| 7079 ||| 7080 ||| 7081 ||| 7082 ||| 
2019 ||| cortical tracking of vocoded speech streams with a competing speaker based on attentional selection. ||| 3279 ||| 7083 ||| 7084 ||| 
2017 ||| preliminary study of neurocognitive differences in attention and fluency in schizophrenia using fnirs. ||| 7085 ||| 7086 ||| 7080 ||| 7087 ||| 7088 ||| 7089 ||| 7081 ||| 
2017 ||| attention evaluation with eye tracking glasses for eeg-based emotion recognition. ||| 7090 ||| 2482 ||| 7091 ||| 684 ||| 
2019 ||| stock volatility prediction based on self-attention networks with social information. ||| 6837 ||| 7092 ||| 7093 ||| 7094 ||| 7095 ||| 
2019 ||| attention-based lstm for insider threat detection. ||| 7096 ||| 4189 ||| 4190 ||| 987 ||| 4237 ||| 
2018 ||| visual attention and memory augmented activity recognition and behavioral prediction. ||| 7097 ||| 
2019 ||| automated deployment and scaling of automotive safety services in 5g-transformer. ||| 6088 ||| 7098 ||| 7099 ||| 6087 ||| 6092 ||| 6091 ||| 4046 ||| 7100 ||| 7101 ||| 7102 ||| 7103 ||| 
2018 ||| deploying a containerized ns-3/lena-based lte mobile network service through the 5g-transformer platform. ||| 6088 ||| 6089 ||| 6090 ||| 7104 ||| 6087 ||| 
2017 ||| top-down deep appearance attention for action recognition. ||| 1971 ||| 1972 ||| 7105 ||| 1855 ||| 
2017 ||| wearable gaze trackers: mapping visual attention in 3d. ||| 7106 ||| 7107 ||| 7108 ||| 7109 ||| 7110 ||| 7111 ||| 
2020 ||| knowledge based transformer model for information retrieval. ||| 7112 ||| 3512 ||| 7113 ||| 
2019 ||| power transformer fault diagnosis based on improved bat algorithms to optimize rnn. ||| 7114 ||| 7115 ||| 683 ||| 
2017 ||| the optimal maintenance strategy of power transformers based on the life cycle cost. ||| 7116 ||| 7117 ||| 7118 ||| 
2017 ||| the optimal switching strategy of transformers based on the cost. ||| 7117 ||| 7119 ||| 7120 ||| 7121 ||| 
2018 ||| particle filtering based visual attention model for moving target detection. ||| 7122 ||| 7123 ||| 1781 ||| 
2019 ||| elimination of transformer inrush current by three-phase linkage circuit breakers. ||| 7124 ||| 7125 ||| 7126 ||| 6070 ||| 6074 ||| 7127 ||| 7128 ||| 
2017 ||| moving object attention selection using optical flow and pulse coupled neural network. ||| 7129 ||| 737 ||| 
2021 ||| audio-visual event localization via recursive fusion by joint co-attention. ||| 7130 ||| 435 ||| 1160 ||| 7131 ||| 7132 ||| 127 ||| 
2021 ||| spatial context-aware self-attention model for multi-organ segmentation. ||| 435 ||| 7133 ||| 7134 ||| 7135 ||| 7136 ||| 7137 ||| 4297 ||| 7138 ||| 7139 ||| 
2022 ||| video salient object detection via contrastive features and attention modules. ||| 7140 ||| 7141 ||| 7142 ||| 7143 ||| 
2021 ||| attentional feature fusion. ||| 7144 ||| 7145 ||| 7146 ||| 7147 ||| 7148 ||| 
2021 ||| self supervision for attention networks. ||| 7149 ||| 7150 ||| 7151 ||| 7152 ||| 
2021 ||| an explainable attention-guided iris presentation attack detector. ||| 7153 ||| 7154 ||| 
2022 ||| siamese transformer pyramid networks for real-time uav tracking. ||| 7155 ||| 7156 ||| 7157 ||| 7158 ||| 
2021 ||| attention-based spatial guidance for image-to-image translation. ||| 7011 ||| 7159 ||| 7160 ||| 5089 ||| 7161 ||| 7162 ||| 
2022 ||| mm-vit: multi-modal video transformer for compressed video action recognition. ||| 1060 ||| 7163 ||| 
2022 ||| leaky gated cross-attention for weakly supervised multi-modal temporal action localization. ||| 7164 ||| 7165 ||| 7166 ||| 
2018 ||| fine-grained and semantic-guided visual attention for image captioning. ||| 601 ||| 603 ||| 602 ||| 604 ||| 
2019 ||| crowd counting using scale-aware attention networks. ||| 7167 ||| 7168 ||| 7169 ||| 602 ||| 
2022 ||| learnable multi-level frequency decomposition and hierarchical attention mechanism for generalized face presentation attack detection. ||| 7170 ||| 7171 ||| 7172 ||| 7173 ||| 
2020 ||| iterative and adaptive sampling with spatial attention for black-box model explanations. ||| 7174 ||| 7175 ||| 
2021 ||| rotate to attend: convolutional triplet attention module. ||| 7176 ||| 7177 ||| 7178 ||| 1902 ||| 
2021 ||| atm: attentional text matting. ||| 7179 ||| 7180 ||| 7181 ||| 7182 ||| 
2022 ||| billion-scale pretraining with vision transformers for multi-task visual representations. ||| 7183 ||| 7184 ||| 7185 ||| 7186 ||| 7187 ||| 
2019 ||| mask r-cnn with pyramid attention network for scene text detection. ||| 7188 ||| 7189 ||| 7190 ||| 7191 ||| 
2020 ||| david: dual-attentional video deblurring. ||| 7192 ||| 7193 ||| 3937 ||| 7194 ||| 7195 ||| 
2022 ||| unsupervised sounding object localization with bottom-up and top-down attention. ||| 7196 ||| 5264 ||| 
2020 ||| actor conditioned attention maps for video action detection. ||| 7197 ||| 7198 ||| 7199 ||| 7200 ||| 7201 ||| 
2019 ||| no-reference image quality assessment: an attention driven approach. ||| 7202 ||| 2142 ||| 7203 ||| 7204 ||| 
2022 ||| unetr: transformers for 3d medical image segmentation. ||| 7205 ||| 7206 ||| 7207 ||| 2019 ||| 2020 ||| 7208 ||| 2023 ||| 2024 ||| 
2020 ||| adversarial discriminative attention for robust anomaly detection. ||| 7209 ||| 7210 ||| 7211 ||| 7212 ||| 7213 ||| 
2020 ||| attention flow: end-to-end joint attention estimation. ||| 7214 ||| 7215 ||| 7216 ||| 7217 ||| 7218 ||| 
2021 ||| unsupervised attention based instance discriminative learning for person re-identification. ||| 7219 ||| 7220 ||| 
2021 ||| logan: latent graph co-attention network for weakly-supervised video moment retrieval. ||| 7221 ||| 7222 ||| 7223 ||| 7224 ||| 
2022 ||| edgeconv with attention module for monocular depth estimation. ||| 7225 ||| 7226 ||| 7227 ||| 7228 ||| 
2022 ||| sega: semantic guided attention on visual prototype for few-shot learning. ||| 7229 ||| 1787 ||| 1788 ||| 
2022 ||| visualizing paired image similarity in transformer networks. ||| 7230 ||| 7231 ||| 1191 ||| 7232 ||| 
2020 ||| temporal aggregation with clip-level attention for video-based person re-identification. ||| 7233 ||| 7234 ||| 2357 ||| 7235 ||| 7236 ||| 
2021 ||| long-range attention network for multi-view stereo. ||| 7237 ||| 7238 ||| 7239 ||| 1931 ||| 7240 ||| 
2022 ||| maps: multimodal attention for product similarity. ||| 7241 ||| 7242 ||| 7243 ||| 7244 ||| 
2019 ||| scene parsing via dense recurrent neural networks with attentional selection. ||| 7245 ||| 7246 ||| 7247 ||| 2163 ||| 
2018 ||| predicting facial attributes in video using temporal coherence and motion-attention. ||| 7248 ||| 7249 ||| 2213 ||| 
2022 ||| sac: semantic attention composition for text-conditioned image retrieval. ||| 7250 ||| 7251 ||| 7252 ||| 7253 ||| 7254 ||| 3237 ||| 
2022 ||| detecting arbitrary intermediate keypoints for human pose estimation with vision transformers. ||| 7255 ||| 7256 ||| 7257 ||| 
2022 ||| idea-net: adaptive dual self-attention network for single image denoising. ||| 7258 ||| 7259 ||| 7234 ||| 4634 ||| 7260 ||| 7261 ||| 7262 ||| 
2021 ||| coarse temporal attention network (cta-net) for driver's activity recognition. ||| 6384 ||| 6383 ||| 7263 ||| 7264 ||| 
2021 ||| efficient attention: attention with linear complexities. ||| 7265 ||| 7266 ||| 7267 ||| 1944 ||| 1848 ||| 
2022 ||| spatial-temporal transformer for 3d point cloud sequences. ||| 7268 ||| 5170 ||| 7269 ||| 7270 ||| 7271 ||| 
2018 ||| "seeing is believing": pedestrian trajectory forecasting using visual frustum of attention. ||| 7272 ||| 7273 ||| 7274 ||| 7275 ||| 7276 ||| 7277 ||| 
2021 ||| dualsanet: dual spatial attention network for iris recognition. ||| 5492 ||| 7278 ||| 7279 ||| 
2022 ||| robust lane detection via expanded self attention. ||| 7225 ||| 7280 ||| 7281 ||| 7282 ||| 7226 ||| 7228 ||| 
2020 ||| spatio-temporal ranked-attention networks for video captioning. ||| 7283 ||| 7284 ||| 2507 ||| 2512 ||| 
2018 ||| multichannel attention network for analyzing visual behavior in public speaking. ||| 7285 ||| 7286 ||| 7287 ||| 
2019 ||| ventral-dorsal neural networks: object detection via selective attention. ||| 7288 ||| 7289 ||| 7290 ||| 7291 ||| 7292 ||| 7143 ||| 7293 ||| 
2022 ||| image-adaptive hint generation via vision transformer for outpainting. ||| 7294 ||| 7295 ||| 7296 ||| 7297 ||| 7298 ||| 
2020 ||| multi-label visual feature learning with attentional aggregation. ||| 6031 ||| 7299 ||| 1078 ||| 7300 ||| 
2022 ||| beyond mono to binaural: generating binaural audio from mono audio with depth and cross modal attention. ||| 7301 ||| 7302 ||| 7287 ||| 
2020 ||| periphery-fovea multi-resolution driving model guided by human attention. ||| 6423 ||| 2041 ||| 2042 ||| 6426 ||| 7303 ||| 6427 ||| 
2020 ||| self-attention network for skeleton-based human action recognition. ||| 7304 ||| 7305 ||| 523 ||| 7306 ||| 
2019 ||| attention mechanisms for object recognition with event-based cameras. ||| 7307 ||| 7308 ||| 7309 ||| 7310 ||| 
2022 ||| m3detr: multi-representation, multi-scale, mutual-relation 3d object detection with transformers. ||| 7311 ||| 1224 ||| 7312 ||| 7313 ||| 7314 ||| 1948 ||| 7315 ||| 
2021 ||| pdan: pyramid dilated attention network for action detection. ||| 7316 ||| 5680 ||| 7317 ||| 7318 ||| 7319 ||| 1226 ||| 7320 ||| 7321 ||| 
2019 ||| location-velocity attention for pedestrian trajectory prediction. ||| 7322 ||| 7323 ||| 152 ||| 
2019 ||| pixel-wise attentional gating for scene parsing. ||| 7324 ||| 2035 ||| 
2021 ||| coarse- and fine-grained attention network with background-aware loss for crowd density map estimation. ||| 7325 ||| 7326 ||| 
2021 ||| spike-thrift: towards energy-efficient deep spiking neural networks by limiting spiking activity via attention-guided compression. ||| 7327 ||| 7328 ||| 7329 ||| 7330 ||| 
2021 ||| regional attention networks with context-aware fusion for group emotion recognition. ||| 7331 ||| 7332 ||| 7333 ||| 7334 ||| 
2022 ||| aa3dnet: attention augmented real time 3d object detection. ||| 7335 ||| 
2022 ||| densely-packed object detection via hard negative-aware anchor attention. ||| 7336 ||| 7337 ||| 7338 ||| 
2019 ||| fashion attributes-to-image synthesis using attention-based generative adversarial network. ||| 7339 ||| 7340 ||| 
2021 ||| integrating human gaze into attention for egocentric activity recognition. ||| 7341 ||| 7342 ||| 
2021 ||| end-to-end lane shape prediction with transformers. ||| 7343 ||| 6351 ||| 7344 ||| 7345 ||| 
2020 ||| defraudnet: end2end fingerprint spoof detection using patch level attention. ||| 7346 ||| 7347 ||| 7348 ||| 
2020 ||| attention-based fusion for multi-source human image generation. ||| 2693 ||| 7349 ||| 7350 ||| 7351 ||| 7352 ||| 437 ||| 
2021 ||| super - sam: using the supervision signal from a pose estimator to train a spatial attention module for personal protective equipment recognition. ||| 7353 ||| 7354 ||| 7355 ||| 7356 ||| 
2022 ||| 3dreftransformer: fine-grained object identification in real-world scenes using natural language. ||| 7357 ||| 7358 ||| 7359 ||| 7360 ||| 1785 ||| 7361 ||| 
2021 ||| supervoxel attention graphs for long-range video modeling. ||| 602 ||| 7362 ||| 7363 ||| 7364 ||| 7365 ||| 7366 ||| 
2021 ||| kernel self-attention for weakly-supervised image classification using deep multiple instance learning. ||| 7367 ||| 7368 ||| 7369 ||| 7370 ||| 
2022 ||| cloth-changing person re-identification with self-attention. ||| 7371 ||| 4704 ||| 4699 ||| 
2022 ||| variational stacked local attention networks for diverse video captioning. ||| 7372 ||| 7373 ||| 7374 ||| 7375 ||| 7376 ||| 7377 ||| 
2019 ||| cascade attention machine for occluded landmark detection in 2d x-ray angiography. ||| 7378 ||| 7379 ||| 7380 ||| 2441 ||| 
2020 ||| mhsan: multi-head self-attention network for visual semantic embedding. ||| 7381 ||| 7382 ||| 7383 ||| 7384 ||| 
2022 ||| fassst: fast attention based single-stage segmentation net for real-time instance segmentation. ||| 7385 ||| 1056 ||| 7386 ||| 7387 ||| 7388 ||| 7389 ||| 5447 ||| 1051 ||| 
2019 ||| attention based natural language grounding by navigating virtual environment. ||| 7390 ||| 7391 ||| 7254 ||| 3237 ||| 
2020 ||| improving object detection with inverted attention. ||| 2356 ||| 7392 ||| 7393 ||| 
2020 ||| ulsam: ultra-lightweight subspace attention module for compact convolutional neural networks. ||| 7394 ||| 7395 ||| 7396 ||| 7397 ||| 7398 ||| 
2020 ||| pointgrow: autoregressively learned point cloud generation with self-attention. ||| 7399 ||| 7400 ||| 2498 ||| 7401 ||| 7402 ||| 
2022 ||| time-space transformers for video panoptic segmentation. ||| 7403 ||| 7404 ||| 
2017 ||| enriched deep recurrent visual attention model for multiple object recognition. ||| 7405 ||| 7406 ||| 1691 ||| 
2020 ||| component attention guided face super-resolution network: cagface. ||| 7407 ||| 4003 ||| 7408 ||| 
2022 ||| after-unet: axial fusion transformer unet for medical image segmentation. ||| 7409 ||| 435 ||| 7138 ||| 7410 ||| 7411 ||| 7135 ||| 
2022 ||| megan: memory enhanced graph attention network for space-time video super-resolution. ||| 7412 ||| 7413 ||| 7414 ||| 7415 ||| 7416 ||| 7417 ||| 
2022 ||| monocular depth estimation with adaptive geometric attention. ||| 7418 ||| 7419 ||| 7420 ||| 7421 ||| 
2022 ||| facial attribute transformers for precise and robust makeup transfer. ||| 7422 ||| 7423 ||| 7424 ||| 7425 ||| 7426 ||| 2166 ||| 
2021 ||| text-to-image generation grounded by fine-grained user attention. ||| 7427 ||| 7428 ||| 7429 ||| 3786 ||| 
2019 ||| interpretable visual question answering by visual grounding from attention supervision mining. ||| 7430 ||| 7431 ||| 7432 ||| 
2022 ||| all the attention you need: global-local, spatial-channel attention for image retrieval. ||| 7433 ||| 7434 ||| 7435 ||| 
2018 ||| structured triplet learning with pos-tag guided attention for visual question answering. ||| 7436 ||| 7437 ||| 2216 ||| 2149 ||| 7135 ||| 2035 ||| 
2022 ||| sign pose-based transformer for word-level sign language recognition. ||| 7438 ||| 7439 ||| 7440 ||| 7441 ||| 7442 ||| 
2022 ||| no-reference image quality assessment via transformers, relative ranking, and self-consistency. ||| 7443 ||| 7444 ||| 7445 ||| 
2020 ||| proposal-free temporal moment localization of a natural-language query in video using guided attention. ||| 7446 ||| 7447 ||| 7448 ||| 7449 ||| 7450 ||| 
2022 ||| attention guided cosine margin to overcome class-imbalance in few-shot road object detection. ||| 7451 ||| 7452 ||| 7453 ||| 7454 ||| 
2022 ||| non-local attention improves description generation for retinal images. ||| 7455 ||| 7456 ||| 7457 ||| 7458 ||| 7459 ||| 7460 ||| 59 ||| 7461 ||| 
2018 ||| hierarchical attention-based anomaly detection model for embedded operating systems. ||| 7462 ||| 7463 ||| 7464 ||| 
2019 ||| a multivariate time series classification method based on self-attention. ||| 7465 ||| 7466 ||| 7467 ||| 7468 ||| 
2019 ||| insulation faults diagnosis of power transformer by decision tree with fuzzy logic. ||| 7469 ||| 7470 ||| 7471 ||| 7472 ||| 
2017 ||| development of audio and visual attention assessment system in combination with brain wave instrument: apply to children with attention deficit hyperactivity disorder. ||| 7473 ||| 7474 ||| 7475 ||| 7476 ||| 
2019 ||| improvement of chromatographic peaks qualitative analysis for power transformer base on decision tree. ||| 7470 ||| 7469 ||| 7477 ||| 7472 ||| 
2017 ||| research on temperature rising prediction of distribution transformer by artificial neural networks. ||| 7478 ||| 7472 ||| 7479 ||| 
2022 ||| pre-routing path delay estimation based on transformer and residual framework. ||| 7480 ||| 7481 ||| 7482 ||| 
2021 ||| net2: a graph attention network method customized for pre-placement net length estimation. ||| 7483 ||| 7484 ||| 7485 ||| 7486 ||| 7487 ||| 7488 ||| 
2022 ||| tenet: temporal cnn with attention for anomaly detection in automotive cyber-physical systems. ||| 7489 ||| 7490 ||| 7491 ||| 
2021 ||| attention-in-memory for few-shot learning with configurable ferroelectric fet arrays. ||| 7492 ||| 7493 ||| 7494 ||| 7495 ||| 
2019 ||| implementing neural machine translation with bi-directional gru and attention mechanism on fpgas using hls. ||| 7496 ||| 7497 ||| 7498 ||| 7499 ||| 7500 ||| 
2021 ||| improving graph convolutional networks with transformer layer in social-based items recommendation. ||| 7501 ||| 7502 ||| 7503 ||| 
2019 ||| attention-based multi-input deep learning architecture for biological activity prediction: an application in egfr inhibitors. ||| 7504 ||| 7505 ||| 
2021 ||| attention-based deep learning model for aspect classification on vietnamese e-commerce data. ||| 7506 ||| 7507 ||| 7508 ||| 7509 ||| 7510 ||| 7511 ||| 
2020 ||| integrating transformer into global and residual image feature extractor in visual question answering for blind people. ||| 7512 ||| 7513 ||| 7514 ||| 
2021 ||| influence prediction on social media network through contents and interaction behaviors using attention-based knowledge graph. ||| 7515 ||| 7516 ||| 7517 ||| 7518 ||| 7519 ||| 
2021 ||| improving the readability of unformatted text using multitask attention networks. ||| 7520 ||| 7521 ||| 7522 ||| 7523 ||| 
2019 ||| recovering capitalization for automatic speech recognition of vietnamese using transformer and chunk merging. ||| 7524 ||| 7525 ||| 7526 ||| 7527 ||| 7528 ||| 7529 ||| 
2021 ||| aspect-based sentiment analysis using mini-window locating attention for vietnamese e-commerce reviews. ||| 7530 ||| 7531 ||| 7532 ||| 7533 ||| 7511 ||| 7508 ||| 7534 ||| 7509 ||| 
2020 ||| transformer-based summarization by exploiting social information. ||| 7521 ||| 7535 ||| 7536 ||| 7537 ||| 
2018 ||| effective attention networks for aspect-level sentiment classification. ||| 7538 ||| 7539 ||| 
2020 |||  movie cuts in users' attention. ||| 7540 ||| 227 ||| 7541 ||| 7542 ||| 
2020 ||| joint attention for automated video editing. ||| 7543 ||| 7544 ||| 7545 ||| 7546 ||| 7547 ||| 
2019 ||| jass: joint attention strategies for paraphrase generation. ||| 7548 ||| 7549 ||| 7550 ||| 7551 ||| 
2019 ||| a study on self-attention mechanism for amr-to-text generation. ||| 7552 ||| 7514 ||| 
2020 ||| natural language generation using transformer network in an open-domain setting. ||| 867 ||| 165 ||| 7553 ||| 7554 ||| 7555 ||| 405 ||| 
2020 ||| improving the community question retrieval performance using attention-based siamese lstm. ||| 7556 ||| 7557 ||| 7558 ||| 7559 ||| 
2017 ||| a hierarchical iterative attention model for machine comprehension. ||| 1415 ||| 1417 ||| 7560 ||| 7561 ||| 
2019 ||| bidirectional transformer based multi-task learning for natural language understanding. ||| 7562 ||| 7563 ||| 7564 ||| 7565 ||| 7566 ||| 
2020 ||| studying attention models in sentiment attitude extraction task. ||| 7567 ||| 7568 ||| 
2019 ||| exploring the attention mechanism in deep models: a case study on sentiment analysis. ||| 7569 ||| 7570 ||| 
2017 ||| perceptions of the effect of fragmented attention on mobile web search tasks. ||| 7571 ||| 7572 ||| 
2021 ||| classifying speech acts using multi-channel deep attention network for task-oriented conversational search agents. ||| 7573 ||| 7574 ||| 
2021 ||| a user study on user attention for an interactive content-based image search system. ||| 7575 ||| 7576 ||| 
2020 ||| studying how health literacy influences attention during online information seeking. ||| 7577 ||| 7578 ||| 
2020 ||| position-aware communication via self-attention for multi-agent reinforcement learning. ||| 7579 ||| 7580 ||| 
2021 ||| evaluation of attention mechanisms on text to speech. ||| 7581 ||| 7582 ||| 7583 ||| 4502 ||| 
2021 ||| deep residual and deep dense attentions in english chinese translation. ||| 7581 ||| 7582 ||| 7584 ||| 4502 ||| 
2021 ||| modified attention spatial convolution model for skin lesion segmentation. ||| 7531 ||| 7585 ||| 4502 ||| 
2019 ||| a methodology for intuitive and low-attention tv remote control on smart phones. ||| 7586 ||| 227 ||| 7587 ||| 3369 ||| 7588 ||| 59 ||| 7589 ||| 5409 ||| 7590 ||| 7591 ||| 7592 ||| 
2021 ||| poi recommendation model based on attention-based gated recurrent unit network. ||| 7593 ||| 7594 ||| 7595 ||| 7596 ||| 
2021 ||| green coffee beans classification using attention-based features and knowledge transfer. ||| 4451 ||| 4450 ||| 4452 ||| 
2020 ||| prediction of time series data based on transformer with soft dynamic time wrapping. ||| 7597 ||| 7598 ||| 7599 ||| 7600 ||| 
2019 ||| inferring student's attention in a machine learning approach: a feasibility study. ||| 7601 ||| 7602 ||| 7603 ||| 7604 ||| 7605 ||| 7606 ||| 
2020 ||| estimation of person-specific visual attention via selection of similar persons. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| degradation level estimation of road structures via attention branch network with text data. ||| 7611 ||| 7608 ||| 7609 ||| 7610 ||| 
2019 ||| user-specific visual attention estimation based on visual similarity and spatial information in images. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2019 ||| field-of-view prediction in 360-degree videos with attention-based neural encoder-decoder networks. ||| 7612 ||| 4297 ||| 
2020 ||| attention-based learning for missing data imputation in holoclean. ||| 7613 ||| 7614 ||| 7615 ||| 7616 ||| 
2020 ||| optimus: optimized matrix multiplication structure for transformer neural network accelerator. ||| 7617 ||| 7618 ||| 7619 ||| 1614 ||| 7620 ||| 
2017 ||| reachability analysis of transformer-isolated dc-dc converters. ||| 7621 ||| 7622 ||| 7623 ||| 
2018 ||| power transformer anomaly detection based on adaptive kernel fuzzy c-means clustering and kernel principal component analysis. ||| 7624 ||| 7625 ||| 7626 ||| 7627 ||| 7628 ||| 
2020 ||| neural topic model with attention for supervised learning. ||| 7629 ||| 208 ||| 
2017 ||| information transfer within human robot teams: multimodal attention management in human-robot interaction. ||| 7630 ||| 7631 ||| 
2017 ||| rightward attentional bias in windshield displays: implication towards external human machine interfaces for self-driving cars. ||| 1073 ||| 7632 ||| 7633 ||| 7634 ||| 7635 ||| 7636 ||| 7637 ||| 
2020 ||| on-line monitoring method of power transformer insulation fault based on bayesian network. ||| 7638 ||| 7639 ||| 7640 ||| 
2020 ||| design of sealing transformer for vacuum packaging machine based on single chip microcomputer control. ||| 7641 ||| 1340 ||| 
2019 ||| analysis and visualization of attention area of tweets during disasters. ||| 7642 ||| 7643 ||| 7644 ||| 7645 ||| 
2017 ||| initialized frame attention networks for video question answering. ||| 7646 ||| 7647 ||| 7648 ||| 
2018 ||| video question answering via multi-granularity temporal attention network learning. ||| 7649 ||| 7650 ||| 7651 ||| 1306 ||| 7652 ||| 2258 ||| 7653 ||| 7654 ||| 
2018 ||| object-oriented video prediction with pixel-level attention. ||| 7655 ||| 7656 ||| 7657 ||| 
2021 ||| multi scale attention network for crowd counting. ||| 7658 ||| 7659 ||| 
2021 ||| object tracking algorithm based on channel-interconnection-spatial attention mechanism and siamese region proposal network. ||| 7660 ||| 7661 ||| 
2020 ||| text sentiment classification based on lstm-tcn hybrid model and attention mechanism. ||| 2829 ||| 6980 ||| 4175 ||| 7662 ||| 7663 ||| 6981 ||| 
2021 ||| smoky vehicle detection based on improved vision transformer. ||| 1722 ||| 7664 ||| 7659 ||| 
2021 ||| implicit discourse relation classification based on semantic graph attention networks. ||| 7665 ||| 7666 ||| 3114 ||| 
2021 ||| ssd target detection algorithm based on multi-scale fusion and attention. ||| 7667 ||| 3034 ||| 7668 ||| 7669 ||| 
2021 ||| attention mechanism driven yolov3 on fpga acceleration for efficient vision based defect inspection. ||| 7670 ||| 6115 ||| 7671 ||| 
2020 ||| attention v-net: a residual u-net with attention gate block for lung organs at risk segmentation. ||| 7672 ||| 7673 ||| 7674 ||| 7675 ||| 7676 ||| 
2021 ||| spatial-temporal multi-head attention networks for traffic flow forecasting. ||| 5543 ||| 124 ||| 7677 ||| 
2019 ||| knowledge graph completion via complete attention between knowledge graph and entity descriptions. ||| 7678 ||| 387 ||| 7679 ||| 
2020 ||| self-attention and dynamic convolution hybrid model for neural machine translation. ||| 7680 ||| 7681 ||| 1216 ||| 7682 ||| 
2021 ||| a character-word graph attention networks for chinese text classification. ||| 7683 ||| 7684 ||| 
2018 ||| short-attention mechanism for generative dialogue system. ||| 7685 ||| 3177 ||| 1199 ||| 
2018 ||| stance detection with target and target towards attention. ||| 7686 ||| 3177 ||| 1199 ||| 
2020 ||| prediction of financial big data stock trends based on attention mechanism. ||| 7687 ||| 7688 ||| 7689 ||| 7690 ||| 
2018 ||| sentiment and semantic deep hierarchical attention neural network for fine grained news classification. ||| 7691 ||| 7692 ||| 7693 ||| 
2020 ||| heterogeneous dynamic graph attention network. ||| 7694 ||| 7695 ||| 7696 ||| 7697 ||| 
2019 ||| adversarial graph attention network for multi-modal cross-modal retrieval. ||| 7698 ||| 1114 ||| 7699 ||| 7700 ||| 7701 ||| 7702 ||| 7703 ||| 
2019 ||| open-domain document-based automatic qa models based on cnn and attention mechanism. ||| 7704 ||| 7705 ||| 7706 ||| 7707 ||| 
2019 ||| large-scale video-based person re-identification via non-local attention and feature erasing. ||| 1381 ||| 7708 ||| 202 ||| 
2019 ||| convolutional-block-attention dual path networks for slide transition detection in lecture videos. ||| 7709 ||| 2233 ||| 7710 ||| 7711 ||| 
2019 ||| attention-based top-down single-task action recognition in still images. ||| 7712 ||| 7713 ||| 1007 ||| 
2019 ||| preliminary study on visual attention maps of experts and nonexperts when examining pathological microscopic images. ||| 7714 ||| 7715 ||| 5549 ||| 7716 ||| 
2017 ||| fault classification in transformer using low frequency component. ||| 7717 ||| 7718 ||| 7719 ||| 7720 ||| 
2021 ||| a 6.54-to-26.03 tops/w computing-in-memory rnn processor using input similarity optimization and attention-based context-breaking with output speculation. ||| 7721 ||| 1705 ||| 7722 ||| 7723 ||| 7724 ||| 7725 ||| 7726 ||| 7727 ||| 7728 ||| 7729 ||| 
2017 ||| attracting versus sustaining attention in the information economy. ||| 7730 ||| 7731 ||| 
2020 ||| homicidal event forecasting and interpretable analysis using hierarchical attention model. ||| 7732 ||| 7733 ||| 7734 ||| 359 ||| 
2020 ||| optimizing attention-aware opinion seeding strategies. ||| 7735 ||| 7736 ||| 7737 ||| 
2018 ||| aspect level sentiment classification with attention-over-attention neural networks. ||| 7738 ||| 2298 ||| 7739 ||| 
2021 ||| identifying shifts in collective attention to topics on social media. ||| 7740 ||| 7741 ||| 7742 ||| 7743 ||| 
2021 ||| quasi character-level transformers to improve neural machine translation on small datasets. ||| 7744 ||| 3882 ||| 7745 ||| 
2021 ||| predicting human behavior with transformer considering the mutual relationship between categories and regions. ||| 7746 ||| 7747 ||| 7748 ||| 7749 ||| 7750 ||| 7751 ||| 
2020 ||| attention-based lstm for automatic evaluation of press conferences. ||| 7752 ||| 7753 ||| 7754 ||| 7755 ||| 7756 ||| 
2020 ||| aesthetics-assisted multi-task learning with attention for image memorability prediction. ||| 7757 ||| 7758 ||| 7759 ||| 7760 ||| 
2020 ||| cross-domain visual attention model adaption with one-shot gan. ||| 7761 ||| 7762 ||| 2380 ||| 7763 ||| 2383 ||| 
2021 ||| xm2a: multi-scale multi-head attention with cross-talk for multi-variate time series analysis. ||| 7764 ||| 7765 ||| 7766 ||| 
2019 ||| automatic generation of chinese couplets with attention based encoder-decoder model. ||| 7767 ||| 7768 ||| 1556 ||| 3248 ||| 
2020 ||| video review analysis via transformer-based sentiment change detection. ||| 7769 ||| 7770 ||| 3248 ||| 1556 ||| 
2021 ||| multimodal machine translation enhancement by fusing multimodal-attention and fine-grained image features. ||| 1556 ||| 7771 ||| 
2021 ||| transformer based neural network for fine-grained classification of vehicle color. ||| 7772 ||| 7773 ||| 7774 ||| 7775 ||| 7776 ||| 
2020 ||| learn to pay attention via switchable attention for image recognition. ||| 7777 ||| 7778 ||| 7779 ||| 7780 ||| 7781 ||| 7782 ||| 
2021 ||| scarf: a semantic constrained attention refinement network for semantic segmentation. ||| 7783 ||| 7784 ||| 2631 ||| 7785 ||| 7786 ||| 
2021 ||| scat: stride consistency with auto-regressive regressor and transformer for hand pose estimation. ||| 7787 ||| 7788 ||| 6627 ||| 7789 ||| 7790 ||| 7791 ||| 
2019 ||| weakly-supervised completion moment detection using temporal attention. ||| 7792 ||| 7793 ||| 7794 ||| 
2021 ||| where did i see it? object instance re-identification with attention. ||| 7371 ||| 4704 ||| 4699 ||| 
2017 ||| spatial attention improves object localization: a biologically plausible neuro-computational model for use in virtual reality. ||| 7795 ||| 7796 ||| 7797 ||| 
2021 ||| vtgan: semi-supervised retinal image synthesis and disease prediction using vision transformers. ||| 7798 ||| 7799 ||| 7800 ||| 7801 ||| 7802 ||| 
2017 ||| dynamic computational time for visual attention. ||| 7803 ||| 208 ||| 2530 ||| 6924 ||| 2531 ||| 7804 ||| 
2019 ||| spatial attention for multi-scale feature refinement for object detection. ||| 2277 ||| 7805 ||| 6761 ||| 6762 ||| 7806 ||| 7807 ||| 400 ||| 
2019 ||| indoor depth completion with boundary consistency and self-attention. ||| 7808 ||| 7809 ||| 7810 ||| 1387 ||| 
2021 ||| swinir: image restoration using swin transformer. ||| 7811 ||| 7812 ||| 7813 ||| 3433 ||| 7814 ||| 7815 ||| 
2019 ||| learning spatiotemporal attention for egocentric action recognition. ||| 7816 ||| 7817 ||| 7818 ||| 
2021 ||| semantic segmentation with multi scale spatial attention for self driving cars. ||| 7335 ||| 7819 ||| 
2019 ||| part matching with multi-level attention for person re-identification. ||| 7820 ||| 
2019 ||| a hvs-inspired attention to improve loss metrics for cnn-based perception-oriented super-resolution. ||| 7821 ||| 7822 ||| 7823 ||| 
2021 ||| self-attention agreement among capsules. ||| 7824 ||| 4702 ||| 4699 ||| 
2021 ||| semi-autoregressive transformer for image captioning. ||| 7825 ||| 7826 ||| 7827 ||| 444 ||| 
2021 ||| siamsta: spatio-temporal attention based siamese tracker for tracking uavs. ||| 7828 ||| 927 ||| 7829 ||| 7830 ||| 7831 ||| 7832 ||| 3279 ||| 7833 ||| 
2019 ||| image super-resolution via attention based back projection networks. ||| 1542 ||| 1539 ||| 7834 ||| 1540 ||| 7835 ||| 
2019 ||| retinal image classification via vasculature-guided sequential attention. ||| 7836 ||| 7837 ||| 
2021 ||| a transformer-based framework for automatic covid19 diagnosis in chest cts. ||| 241 ||| 7838 ||| 
2019 ||| eyenet: attention based convolutional encoder-decoder network for eye region segmentation. ||| 7839 ||| 7840 ||| 
2021 ||| video transformer network. ||| 7841 ||| 7842 ||| 7843 ||| 7844 ||| 
2021 ||| transblast: self-supervised learning using augmented subspace with transformer for background/foreground separation. ||| 7845 ||| 7846 ||| 7847 ||| 
2017 ||| understanding scenery quality: a visual attention measure and its computational model. ||| 7848 ||| 7849 ||| 7850 ||| 7851 ||| 7852 ||| 
2019 ||| dual attention mobdensenet(damdnet) for robust 3d face alignment. ||| 479 ||| 7853 ||| 6525 ||| 
2019 ||| great ape detection in challenging jungle camera trap footage via attention-based spatial and temporal feature blending. ||| 7854 ||| 7793 ||| 7855 ||| 
2021 ||| trans4trans: efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world. ||| 7856 ||| 7857 ||| 7858 ||| 7859 ||| 7860 ||| 3831 ||| 7861 ||| 
2021 ||| manet: a motion-driven attention network for detecting the pulse from a facial video with drastic motions. ||| 7862 ||| 7863 ||| 7864 ||| 6718 ||| 1134 ||| 7865 ||| 
2021 ||| a computer vision-based attention generator using dqn. ||| 7866 ||| 7867 ||| 7868 ||| 7869 ||| 
2017 ||| attending to distinctive moments: weakly-supervised attention models for action localization in video. ||| 1207 ||| 7870 ||| 7871 ||| 
2019 ||| interpretable spatio-temporal attention for video action recognition. ||| 7872 ||| 7873 ||| 7874 ||| 7875 ||| 4217 ||| 7876 ||| 2301 ||| 
2019 ||| free-lunch saliency via attention in atari agents. ||| 7877 ||| 7878 ||| 7879 ||| 7880 ||| 
2021 ||| tiled squeeze-and-excite: channel attention with local spatial context. ||| 7881 ||| 7882 ||| 7883 ||| 
2021 ||| ressanet: a hybrid backbone of residual block and self-attention module for masked face recognition. ||| 7884 ||| 7885 ||| 7886 ||| 
2019 ||| forced spatial attention for driver foot activity classification. ||| 7887 ||| 7888 ||| 
2021 ||| sa-det3d: self-attention based context-aware 3d object detection. ||| 7889 ||| 7890 ||| 7891 ||| 
2019 ||| manipulation-skill assessment from videos with spatial attention network. ||| 7892 ||| 6419 ||| 7893 ||| 7894 ||| 
2019 ||| image super-resolution via residual block attention networks. ||| 7895 ||| 7896 ||| 3614 ||| 7897 ||| 
2021 ||| efficientarl: improving skin cancer diagnoses by combining lightweight attention on efficientnet. ||| 7898 ||| 7899 ||| 7900 ||| 
2021 ||| multimodal continuous visual attention mechanisms. ||| 2871 ||| 7901 ||| 3369 ||| 3370 ||| 7902 ||| 
2019 ||| semantically consistent hierarchical text to fashion image synthesis with an enhanced-attentional generative adversarial network. ||| 7903 ||| 7904 ||| 7905 ||| 7906 ||| 
2019 ||| residual attention graph convolutional network for geometric 3d scene classification. ||| 7907 ||| 7908 ||| 
2021 ||| learning tracking representations via dual-branch fully transformer networks. ||| 7909 ||| 7910 ||| 7911 ||| 2098 ||| 7912 ||| 
2021 ||| convnets vs. transformers: whose visual representations are more transferable? ||| 7913 ||| 7914 ||| 1799 ||| 1801 ||| 
2021 ||| studying the effects of self-attention for medical image analysis. ||| 7915 ||| 5369 ||| 7916 ||| 7917 ||| 7918 ||| 
2019 ||| pose guided attention for multi-label fashion image classification. ||| 7919 ||| 1994 ||| 7920 ||| 7921 ||| 7922 ||| 1994 ||| 7923 ||| 
2017 ||| temporal localization and spatial segmentation of joint attention in multiple first-person videos. ||| 6419 ||| 7893 ||| 7924 ||| 7925 ||| 7926 ||| 7894 ||| 
2019 ||| attention-translation-relation network for scalable scene graph generation. ||| 7927 ||| 7928 ||| 7929 ||| 7930 ||| 
2021 ||| attention aware debiasing for unbiased model prediction. ||| 7931 ||| 7932 ||| 7933 ||| 
2019 ||| extreme low resolution action recognition with spatial-temporal multi-head self-attention and knowledge distillation. ||| 2392 ||| 2242 ||| 2243 ||| 2244 ||| 
2019 ||| reverse and boundary attention network for road segmentation. ||| 7934 ||| 7935 ||| 7936 ||| 7937 ||| 7938 ||| 
2019 ||| photometric transformer networks and label adjustment for breast density prediction. ||| 7939 ||| 7940 ||| 7941 ||| 
2021 ||| a unified efficient pyramid transformer for semantic segmentation. ||| 7942 ||| 2422 ||| 254 ||| 7943 ||| 6365 ||| 3046 ||| 
2019 ||| adherent raindrop removal with self-supervised attention maps and spatio-temporal generative adversarial networks. ||| 7944 ||| 7945 ||| 7946 ||| 7947 ||| 7948 ||| 
2019 ||| video multitask transformer network. ||| 7949 ||| 7950 ||| 7951 ||| 
2021 ||| audio-visual transformer based crowd counting. ||| 388 ||| 7952 ||| 7953 ||| 391 ||| 392 ||| 
2021 ||| mila: multi-task learning from videos via efficient inter-frame attention. ||| 7954 ||| 7955 ||| 7956 ||| 7957 ||| 7224 ||| 7958 ||| 7959 ||| 2101 ||| 7960 ||| 
2019 ||| attention-aware age-agnostic visual place recognition. ||| 7961 ||| 7962 ||| 7963 ||| 7964 ||| 
2019 ||| motion-guided spatial time attention for video object segmentation. ||| 7965 ||| 2218 ||| 2220 ||| 7966 ||| 7967 ||| 2222 ||| 2219 ||| 
2017 ||| human action recognition: pose-based attention draws focus to hands. ||| 7968 ||| 7969 ||| 7970 ||| 
2019 ||| spatio-temporal attention network for video instance segmentation. ||| 7971 ||| 7972 ||| 7973 ||| 
2021 ||| medskip: medical report generation using skip connections and integrated attention. ||| 7974 ||| 7975 ||| 7976 ||| 7977 ||| 7978 ||| 
2019 ||| detecting visual relationships using box attention. ||| 7979 ||| 7980 ||| 7981 ||| 7982 ||| 
2021 ||| the value of visual attention for covid-19 classification in ct scans. ||| 7915 ||| 5369 ||| 7918 ||| 
2021 ||| investigating transformers in the decomposition of polygonal shapes as point collections. ||| 7983 ||| 7984 ||| 7985 ||| 
2021 ||| background/foreground separation: guided attention based adversarial modeling (gaam) versus robust subspace learning methods. ||| 7986 ||| 7987 ||| 7988 ||| 2308 ||| 7989 ||| 
2017 ||| two-stream flow-guided convolutional attention networks for action recognition. ||| 7990 ||| 7991 ||| 
2021 ||| saliency-guided transformer network combined with local embedding for no-reference image quality assessment. ||| 7992 ||| 7993 ||| 7994 ||| 7995 ||| 7996 ||| 7997 ||| 
2019 ||| an indoor crowd detection network framework based on feature aggregation module and hybrid attention selection module. ||| 7998 ||| 7999 ||| 8000 ||| 
2021 ||| vit-yolo: transformer-based yolo for object detection. ||| 8001 ||| 8002 ||| 8003 ||| 8004 ||| 400 ||| 5743 ||| 
2017 ||| 3d morphable models as spatial transformer networks. ||| 8005 ||| 8006 ||| 8007 ||| 8008 ||| 6525 ||| 
2019 ||| improving fashion landmark detection by dual attention feature enhancement. ||| 8009 ||| 582 ||| 583 ||| 584 ||| 
2021 ||| tph-yolov5: improved yolov5 based on transformer prediction head for object detection on drone-captured scenarios. ||| 8010 ||| 8011 ||| 8012 ||| 1872 ||| 
2021 ||| transformer meets part model: adaptive part division for person re-identification. ||| 8013 ||| 6660 ||| 6935 ||| 
2019 ||| two-stream video classification with cross-modality attention. ||| 8014 ||| 8015 ||| 8016 ||| 2398 ||| 
2021 ||| leveraging batch normalization for vision transformers. ||| 8017 ||| 1767 ||| 1766 ||| 1765 ||| 1770 ||| 1768 ||| 
2021 ||| an investigation of attention mechanisms in histopathology whole-slide-image analysis for regression objectives. ||| 8018 ||| 8019 ||| 8020 ||| 8021 ||| 
2019 ||| cross-granularity attention network for semantic segmentation. ||| 8022 ||| 8023 ||| 8024 ||| 8025 ||| 
2021 ||| dyadformer: a multi-modal transformer for long-range modeling of dyadic interactions. ||| 8026 ||| 8027 ||| 7111 ||| 8028 ||| 8029 ||| 4194 ||| 8030 ||| 7591 ||| 8031 ||| 8032 ||| 8033 ||| 8034 ||| 8035 ||| 8036 ||| 
2021 ||| abd-net: attention based decomposition network for 3d point cloud decomposition. ||| 8037 ||| 8038 ||| 8039 ||| 8040 ||| 8041 ||| 
2019 ||| attention routing between capsules. ||| 8042 ||| 8043 ||| 8044 ||| 8045 ||| 
2021 ||| skeletonnetv2: a dense channel attention blocks for skeleton extraction. ||| 4035 ||| 7839 ||| 
2021 ||| pose transformers (potr): human motion prediction with non-autoregressive transformers. ||| 8046 ||| 8047 ||| 8048 ||| 8049 ||| 8050 ||| 
2020 ||| coordinated movement for prosthesis reference trajectory generation: temporal factors and attention. ||| 8051 ||| 2440 ||| 8052 ||| 8053 ||| 
2018 ||| preliminary testing of a telerobotic haptic system and analysis of visual attention during a playful activity. ||| 8054 ||| 8055 ||| 8056 ||| 8057 ||| 8058 ||| 
2019 ||| patenttransformer-1.5: measuring patent claim generation by span relevancy. ||| 8059 ||| 8060 ||| 
2019 ||| eeg-based decoding of auditory attention to a target instrument in polyphonic music. ||| 8061 ||| 8062 ||| 8063 ||| 8064 ||| 
2021 ||| df-conformer: integrated architecture of conv-tasnet and conformer using linear complexity self-attention for speech enhancement. ||| 8065 ||| 8066 ||| 8067 ||| 8068 ||| 2511 ||| 8069 ||| 8070 ||| 
2019 ||| end-to-end melody note transcription based on a beat-synchronous attention mechanism. ||| 8071 ||| 8072 ||| 8073 ||| 8074 ||| 
2019 ||| attention wave-u-net for speech enhancement. ||| 8075 ||| 8076 ||| 8077 ||| 
2021 ||| detecting android malware based on dynamic feature sequence and attention mechanism. ||| 8078 ||| 8079 ||| 1305 ||| 
2019 ||| a biologically-inspired attentional approach for face recognition. ||| 8080 ||| 8081 ||| 
2019 ||| automatic short answer grading via multiway attention networks. ||| 8082 ||| 8083 ||| 8084 ||| 8085 ||| 8086 ||| 8087 ||| 
2019 ||| improving short answer grading using transformer-based pre-training. ||| 8088 ||| 8089 ||| 8090 ||| 
2021 ||| paraphrasing academic text: a study of back-translating anatomy and physiology with transformers. ||| 8091 ||| 
2020 ||| learning from interpretable analysis: attention-based knowledge tracing. ||| 8092 ||| 1724 ||| 8093 ||| 8094 ||| 4080 ||| 8095 ||| 
2020 ||| the sound of inattention: predicting mind wandering with automatically derived features of instructor speech. ||| 8096 ||| 8097 ||| 8098 ||| 8099 ||| 8100 ||| 8101 ||| 
2020 ||| deep knowledge tracing with transformers. ||| 8102 ||| 8103 ||| 8104 ||| 8105 ||| 
2021 ||| dyadic joint visual attention interaction in face-to-face collaborative problem-solving at k-12 maths education: a multimodal approach. ||| 8106 ||| 8107 ||| 8108 ||| 
2020 ||| deep-cross-attention recommendation model for knowledge sharing micro learning service. ||| 8109 ||| 8110 ||| 8111 ||| 8112 ||| 8113 ||| 8114 ||| 144 ||| 8115 ||| 8116 ||| 
2019 ||| automatic construction of a phonics curriculum for reading education using the transformer neural network. ||| 8117 ||| 8118 ||| 8119 ||| 
2020 ||| investigating transformers for automatic short answer grading. ||| 8120 ||| 8121 ||| 
2020 ||| scanpath analysis of student attention during problem solving with worked examples. ||| 8122 ||| 8123 ||| 
2017 ||| the impact of student individual differences and visual attention to pedagogical agents during learning with metatutor. ||| 7111 ||| 8124 ||| 8125 ||| 8126 ||| 8127 ||| 8128 ||| 
2019 ||| development of serious games for neurorehabilitation of children with attention-deficit/hyperactivity disorder through neurofeedback. ||| 8129 ||| 8130 ||| 8131 ||| 8132 ||| 
2019 ||| trust-aware group recommendation with attention mechanism in social network. ||| 406 ||| 7803 ||| 8133 ||| 4297 ||| 
2020 ||| call attention to stances: detect rumor with a stance attention network. ||| 8134 ||| 411 ||| 412 ||| 
2018 ||| computer vision and internet of things: attention system in educational context. ||| 8135 ||| 8136 ||| 8137 ||| 
2021 ||| dyngraphtrans: dynamic graph embedding via modified universal transformer networks for financial transaction data. ||| 8138 ||| 8139 ||| 254 ||| 
2020 ||| m2nn: rare event inference through multi-variate multi-scale attention. ||| 8140 ||| 7765 ||| 7766 ||| 8141 ||| 
2021 ||| an efficient link prediction model in dynamic heterogeneous information networks based on multiple self-attention. ||| 8142 ||| 8143 ||| 
2017 ||| an effective gated and attention-based neural network model for fine-grained financial target-dependent sentiment analysis. ||| 8144 ||| 8145 ||| 349 ||| 350 ||| 
2019 ||| uafa: unsupervised attribute-friendship attention framework for user representation. ||| 8146 ||| 4189 ||| 8147 ||| 4190 ||| 4237 ||| 
2021 ||| landscape-enhanced graph attention network for rumor detection. ||| 860 ||| 1073 ||| 857 ||| 858 ||| 8148 ||| 859 ||| 861 ||| 
2021 ||| medication combination prediction via attention neural networks with prior medical knowledge. ||| 8149 ||| 8150 ||| 8151 ||| 8152 ||| 8153 ||| 8154 ||| 
2020 ||| edge features enhanced graph attention network for relation extraction. ||| 6273 ||| 717 ||| 6275 ||| 8155 ||| 
2021 ||| attentional neural factorization machines for knowledge tracing. ||| 8156 ||| 144 ||| 
2018 ||| attention aware bidirectional gated recurrent unit based framework for sentiment analysis. ||| 8157 ||| 4211 ||| 4892 ||| 4210 ||| 4214 ||| 
2021 ||| combining knowledge with attention neural networks for short text classification. ||| 3337 ||| 144 ||| 
2020 ||| document-improved hierarchical modular attention for event detection. ||| 8158 ||| 2931 ||| 2933 ||| 
2018 ||| fine-grained correlation learning with stacked co-attention networks for cross-modal information retrieval. ||| 8159 ||| 8160 ||| 4190 ||| 4237 ||| 4191 ||| 1273 ||| 
2021 ||| graph attention mechanism with cardinality preservation for knowledge graph completion. ||| 8161 ||| 8162 ||| 8163 ||| 8164 ||| 
2021 ||| text-aware recommendation model based on multi-attention neural networks. ||| 8165 ||| 8166 ||| 8167 ||| 8168 ||| 
2020 ||| robotic pushing and grasping knowledge learning via attention deep q-learning network. ||| 8169 ||| 8170 ||| 
2020 ||| moocrec: an attention meta-path based model for top-k recommendation in mooc. ||| 8171 ||| 8172 ||| 8173 ||| 8174 ||| 
2021 ||| integrating task information into few-shot classifier by channel attention. ||| 8175 ||| 8176 ||| 
2019 ||| multi-attention item recommendation model based on social relations. ||| 8177 ||| 8176 ||| 
2019 ||| knowledge-aware self-attention networks for document grounded dialogue generation. ||| 8178 ||| 8179 ||| 
2021 ||| logattn: unsupervised log anomaly detection with an autoencoder based attention mechanism. ||| 8180 ||| 8181 ||| 6925 ||| 8182 ||| 8183 ||| 8184 ||| 8185 ||| 8186 ||| 
2021 ||| the novel efficient transformer for nlp. ||| 8187 ||| 8188 ||| 8189 ||| 8190 ||| 
2021 ||| attention based short-term metro passenger flow prediction. ||| 8191 ||| 8192 ||| 8193 ||| 8194 ||| 8195 ||| 8196 ||| 
2019 ||| nrsa: neural recommendation with summary-aware attention. ||| 8197 ||| 8198 ||| 696 ||| 4166 ||| 699 ||| 697 ||| 
2020 ||| fine-tuned transformer model for sentiment analysis. ||| 142 ||| 140 ||| 8156 ||| 8199 ||| 144 ||| 124 ||| 
2019 ||| tagdeeprec: tag recommendation for software information sites using attention-based bi-lstm. ||| 665 ||| 8200 ||| 8201 ||| 8202 ||| 8203 ||| 
2021 ||| readmission prediction with knowledge graph attention and rnn-based ordinary differential equations. ||| 8204 ||| 8205 ||| 2732 ||| 8206 ||| 
2021 ||| similarity-based heterogeneous graph attention network for knowledge-enhanced recommendation. ||| 2532 ||| 8207 ||| 3617 ||| 8208 ||| 
2021 ||| a deep learning model based on neural bag-of-words attention for sentiment analysis. ||| 2495 ||| 8209 ||| 
2020 ||| a hybrid model with pre-trained entity-aware transformer for relation extraction. ||| 8210 ||| 1254 ||| 8211 ||| 8212 ||| 
2020 ||| attention-based knowledge tracing with heterogeneous information network embedding. ||| 700 ||| 8213 ||| 8214 ||| 144 ||| 8111 ||| 8110 ||| 
2021 ||| aspect and opinion terms co-extraction using position-aware attention and auxiliary labels. ||| 859 ||| 8215 ||| 857 ||| 858 ||| 8216 ||| 860 ||| 861 ||| 
2019 ||| sequential recommendation based on long-term and short-term user behavior with self-attention. ||| 8217 ||| 8218 ||| 2760 ||| 
2021 ||| sentence matching with deep self-attention and co-attention features. ||| 8219 ||| 8220 ||| 
2019 ||| self-supervised attention model for weakly labeled audio event classification. ||| 8221 ||| 8222 ||| 
2021 ||| spontaneous speech summarization: transformers all the way through. ||| 8223 ||| 8224 ||| 8225 ||| 8226 ||| 8227 ||| 
2021 ||| attention-based distributed speech enhancement for unconstrained microphone arrays with varying number of nodes. ||| 8228 ||| 8229 ||| 8062 ||| 8230 ||| 
2020 ||| selective adaptation of end-to-end speech recognition using hybrid ctc/attention architecture for noise robustness. ||| 8231 ||| 8232 ||| 8233 ||| 
2020 ||| noise-robust attention learning for end-to-end speech recognition. ||| 4532 ||| 8234 ||| 8235 ||| 8236 ||| 4534 ||| 4533 ||| 
2021 ||| anomalous sound detection based on attention mechanism. ||| 8237 ||| 8238 ||| 8239 ||| 
2021 ||| wavetransformer: an architecture for audio captioning based on learning temporal and time-frequency information. ||| 7990 ||| 8240 ||| 8241 ||| 
2020 ||| diagnosis of attention deficit and hyperactivity disorder (adhd) using hidden markov models. ||| 8242 ||| 8243 ||| 8244 ||| 3419 ||| 8245 ||| 8246 ||| 
2021 ||| attention augmented cnns for musical instrument identification. ||| 8247 ||| 8248 ||| 8249 ||| 
2020 ||| exploiting attention-based sequence-to-sequence architectures for sound event localization. ||| 8250 ||| 8251 ||| 1491 ||| 1493 ||| 1492 ||| 1494 ||| 8252 ||| 
2019 ||| a new metric to evaluate auditory attention detection performance based on a markov chain. ||| 8253 ||| 8254 ||| 8255 ||| 
2020 ||| few-shot learning of signal modulation recognition based on attention relation network. ||| 8256 ||| 185 ||| 8257 ||| 
2021 ||| auditory attention decoding from eeg using convolutional recurrent neural network. ||| 8258 ||| 1241 ||| 8259 ||| 8260 ||| 
2017 ||| eeg-based attention-driven speech enhancement for noisy speech mixtures using n-fold multi-channel wiener filters. ||| 8261 ||| 8262 ||| 8254 ||| 8255 ||| 
2021 ||| speaker-aware speech enhancement with self-attention. ||| 8263 ||| 8264 ||| 8265 ||| 8266 ||| 
2021 ||| waveglove: transformer-based hand gesture recognition using multiple inertial sensors. ||| 8267 ||| 8268 ||| 8269 ||| 
2021 ||| attention vs non-attention for a shapley-based explanation method. ||| 8270 ||| 8271 ||| 8272 ||| 3341 ||| 
2021 ||| kw-attn: knowledge infused attention for accurate and interpretable text classification. ||| 8273 ||| 8274 ||| 4793 ||| 4795 ||| 8275 ||| 8276 ||| 
2021 ||| transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. ||| 8277 ||| 8278 ||| 8279 ||| 8280 ||| 
2019 ||| amcnet: attention-based multiscale convolutional network for dcm mri segmentation. ||| 8281 ||| 8282 ||| 8283 ||| 8284 ||| 8285 ||| 8286 ||| 
2020 ||| modeling an ar serious game to increase attention of adhd patients. ||| 8287 ||| 
2021 ||| learning to match workers and tasks via a multi-view graph attention network. ||| 8288 ||| 8289 ||| 5978 ||| 8290 ||| 
2021 ||| casr: a collaborative attention model for session-based recommendation. ||| 8291 ||| 8292 ||| 8293 ||| 8294 ||| 
2021 ||| using cognitive interest graph and knowledge-activated attention for learning resource recommendation. ||| 8295 ||| 8296 ||| 8297 ||| 8298 ||| 
2021 ||| predicting entity relations across different security databases by using graph attention network. ||| 8299 ||| 8300 ||| 8301 ||| 8302 ||| 8303 ||| 1000 ||| 
2020 ||| accuracy improvement for neural program synthesis via attention mechanism and program slicing. ||| 8304 ||| 8305 ||| 8306 ||| 8307 ||| 8308 ||| 
2021 ||| p4 transformer: towards unified programming for the data plane of software defined network. ||| 8309 ||| 8310 ||| 8311 ||| 
2020 ||| few-shot ontology alignment model with attribute attentions. ||| 563 ||| 564 ||| 565 ||| 
2021 ||| attention guidance agents with eye-tracking - a use-case based on the matbii cockpit task. ||| 8312 ||| 8313 ||| 8314 ||| 8315 ||| 8316 ||| 
2020 ||| : accelerating attention mechanisms in neural networks with approximation. ||| 2718 ||| 8317 ||| 8318 ||| 8319 ||| 8320 ||| 8321 ||| 8322 ||| 8323 ||| 8324 ||| 2724 ||| 8325 ||| 
2021 ||| spatten: efficient sparse attention architecture with cascade token and head pruning. ||| 3161 ||| 8326 ||| 3166 ||| 
2021 ||| loglab: attention-based labeling of log data anomalies via weak supervision. ||| 8327 ||| 8328 ||| 8329 ||| 8330 ||| 
2021 ||| an attention-based forecasting network for intelligent services in manufacturing. ||| 8331 ||| 1578 ||| 
2021 ||| mma-net: a multimodal-attention-based deep neural network for web services classification. ||| 875 ||| 8332 ||| 8333 ||| 8334 ||| 8335 ||| 
2021 ||| video abnormal event detection and location based on spatial attention. ||| 8336 ||| 8337 ||| 8338 ||| 
2021 ||| $\mathcal{laja}{-}$ label attention transformer architectures for icd-10 coding of unstructured clinical notes. ||| 8339 ||| 8340 ||| 8341 ||| 
2017 ||| attention estimation system via smart glasses. ||| 8342 ||| 8343 ||| 8344 ||| 
2018 ||| forecasting user attention during everyday mobile interactions using device-integrated and wearable sensors. ||| 8345 ||| 8346 ||| 3831 ||| 8347 ||| 8348 ||| 
2018 ||| image to latex with densenet encoder and joint attention. ||| 8349 ||| 8350 ||| 8351 ||| 
2019 ||| using automated state space planning for effective management of visual information and learner's attention in virtual reality. ||| 8352 ||| 8353 ||| 8354 ||| 
2019 ||| an introductory survey on attention mechanisms in nlp problems. ||| 8355 ||| 
2018 ||| effective strategies for combining attention mechanism with lstm for aspect-level sentiment classification. ||| 8356 ||| 8357 ||| 8358 ||| 8359 ||| 3676 ||| 
2021 ||| enhancing lstm models with self-attention and stateful training. ||| 8360 ||| 8361 ||| 
2021 ||| sbilsan: stacked bidirectional self-attention lstm network for anomaly detection and diagnosis from system logs. ||| 7412 ||| 8362 ||| 766 ||| 
2021 ||| an attention-based deep learning model with interpretable patch-weight sharing for diagnosing cervical dysplasia. ||| 8363 ||| 4646 ||| 8364 ||| 8365 ||| 8366 ||| 
2021 ||| one-class self-attention model for anomaly detection in manufacturing lines. ||| 8367 ||| 8368 ||| 8369 ||| 8370 ||| 
2020 ||| adaptive attention mechanism based semantic compositional network for video captioning. ||| 8371 ||| 8372 ||| 8373 ||| 8374 ||| 8375 ||| 7768 ||| 
2021 ||| reputation analysis based on weakly-supervised bi-lstm-attention network. ||| 8376 ||| 8377 ||| 
2021 ||| attention-enabled object detection to improve one-stage tracker. ||| 8378 ||| 8379 ||| 8034 ||| 
2020 ||| image denoising using attention-residual convolutional neural networks. ||| 8380 ||| 8381 ||| 8382 ||| 8383 ||| 8384 ||| 1994 ||| 8385 ||| 
2020 ||| a lightweight 2d pose machine with attention enhancement. ||| 8386 ||| 8387 ||| 8388 ||| 5409 ||| 8389 ||| 8390 ||| 2712 ||| 8391 ||| 
2020 ||| superpixel image classification with graph attention networks. ||| 8392 ||| 8393 ||| 8394 ||| 8382 ||| 8395 ||| 8396 ||| 8397 ||| 
2021 ||| tvanet: a spatial and feature-based attention model for self-driving car. ||| 8398 ||| 8399 ||| 8400 ||| 
2021 ||| fast spatial-temporal transformer network. ||| 8401 ||| 8402 ||| 8403 ||| 
2021 ||| gaze estimation via self-attention augmented convolutions. ||| 8404 ||| 8405 ||| 
2021 ||| sgat: semantic graph attention for 3d human pose estimation. ||| 8406 ||| 8407 ||| 8408 ||| 8409 ||| 8390 ||| 2712 ||| 8391 ||| 
2020 ||| multilingual joint fine-tuning of transformer models for identifying trolling, aggression and cyberbullying at trac 2020. ||| 8410 ||| 8411 ||| 8412 ||| 
2020 ||| the go transformer: natural language modeling for game play. ||| 8413 ||| 8414 ||| 8415 ||| 
2021 ||| towards vulnerability types classification using pure self-attention: a common weakness enumeration based approach. ||| 8416 ||| 8417 ||| 8418 ||| 
2021 ||| extracting discriminative features for cross-view gait recognition based on the attention mechanism. ||| 8419 ||| 8420 ||| 8421 ||| 8422 ||| 8423 ||| 8424 ||| 
2021 ||| multimodal aesthetic analysis assisted by styles through a multimodal co-transformer model. ||| 8425 ||| 8426 ||| 1312 ||| 1311 ||| 
2021 ||| a novel sentiment classification based on "word-phrase" attention mechanism. ||| 8427 ||| 8428 ||| 8429 ||| 8430 ||| 3666 ||| 8431 ||| 
2017 ||| modelling an intelligent interaction system for increasing the level of attention. ||| 8432 ||| 227 ||| 8433 ||| 8434 ||| 8435 ||| 
2018 ||| supervising attention in an e-learning system. ||| 8432 ||| 227 ||| 8434 ||| 8435 ||| 
2017 ||| an algorithm for simulating human selective attention. ||| 8436 ||| 8437 ||| 8438 ||| 8439 ||| 
2021 ||| detectornet: transformer-enhanced spatial temporal graph neural network for traffic prediction. ||| 8440 ||| 8441 ||| 8442 ||| 8443 ||| 8444 ||| 8445 ||| 8446 ||| 8447 ||| 8448 ||| 
2021 ||| attention-based spatial interpolation for house price prediction. ||| 8449 ||| 8450 ||| 
2021 ||| geo-attention network for traffic condition prediction and travel time estimation. ||| 8451 ||| 8452 ||| 8453 ||| 8454 ||| 
2021 ||| dual-attention multi-scale graph convolutional networks for highway accident delay time prediction. ||| 8455 ||| 8456 ||| 8457 ||| 
2019 ||| bike-share demand prediction using attention based sequence to sequence and conditional variational autoencoder. ||| 8458 ||| 8459 ||| 8460 ||| 8461 ||| 
2020 ||| multi-scale feature fusion uav image object detection method based on dilated convolution and attention mechanism. ||| 8462 ||| 8463 ||| 1903 ||| 8464 ||| 
2019 ||| session-based recommendation with context-aware attention network. ||| 8465 ||| 8466 ||| 8467 ||| 
2019 ||| an efficient non-local attention network for video-based person re-identification. ||| 1419 ||| 8468 ||| 8469 ||| 8470 ||| 8471 ||| 
2019 ||| traffic flow prediction based on self-attention mechanism and deep packet residual network. ||| 8472 ||| 5935 ||| 8473 ||| 8474 ||| 8475 ||| 6473 ||| 
2020 ||| a hierarchical attention-based neural network model for socialbot detection in osn. ||| 8476 ||| 8477 ||| 8478 ||| 
2018 ||| fine-grained deep knowledge-aware network for news recommendation with self-attention. ||| 8479 ||| 4006 ||| 8480 ||| 3049 ||| 811 ||| 8481 ||| 8482 ||| 4007 ||| 
2017 ||| emotion and attention: predicting electrodermal activity through video visual descriptors. ||| 8483 ||| 8484 ||| 3419 ||| 4048 ||| 4049 ||| 4046 ||| 8485 ||| 8486 ||| 3419 ||| 
2020 ||| ggtan: graph gated talking-heads attention networks for traveling salesman problem. ||| 8487 ||| 8488 ||| 8489 ||| 
2019 ||| an efficient co-attention neural network for social recommendation. ||| 8490 ||| 8491 ||| 8492 ||| 
2021 ||| visual attention analysis and user guidance in cinematic vr film. ||| 8493 ||| 1596 ||| 8494 ||| 8495 ||| 1597 ||| 1594 ||| 
2017 ||| ethical issues of a smart system to enhance students' attention. ||| 8496 ||| 8497 ||| 1994 ||| 8498 ||| 8499 ||| 8500 ||| 8501 ||| 8502 ||| 
2020 ||| history repeats itself: human motion prediction via motion attention. ||| 8503 ||| 8504 ||| 8505 ||| 
2018 ||| knowing when to look for what and where: evaluating generation of spatial descriptions with adaptive attention. ||| 8506 ||| 8507 ||| 
2020 ||| character region attention for text spotting. ||| 8508 ||| 8509 ||| 8510 ||| 8511 ||| 8512 ||| 8513 ||| 8514 ||| 
2020 ||| a dual residual network with channel attention for image restoration. ||| 8515 ||| 8516 ||| 8517 ||| 8518 ||| 2277 ||| 400 ||| 5743 ||| 
2020 ||| example-guided image synthesis using masked spatial-channel attention and self-supervision. ||| 8519 ||| 2070 ||| 8520 ||| 6579 ||| 2417 ||| 2166 ||| 
2020 ||| a recurrent transformer network for novel view action synthesis. ||| 8521 ||| 8522 ||| 8523 ||| 8524 ||| 
2018 ||| dependency-aware attention control for unconstrained face recognition with image sets. ||| 8525 ||| 8526 ||| 497 ||| 8527 ||| 5379 ||| 
2020 ||| empowering relational network by self-attention augmented conditional random fields for group activity recognition. ||| 2242 ||| 2243 ||| 2244 ||| 
2020 ||| progressive transformers for end-to-end sign language production. ||| 8528 ||| 8529 ||| 7442 ||| 8530 ||| 
2018 ||| attention-gan for object transfiguration in wild images. ||| 8531 ||| 3156 ||| 8532 ||| 1756 ||| 
2020 ||| gatcluster: self-supervised gaussian-attention network for image clustering. ||| 8533 ||| 1796 ||| 8534 ||| 8535 ||| 
2020 ||| feature pyramid transformer. ||| 1751 ||| 2484 ||| 8536 ||| 444 ||| 8537 ||| 2483 ||| 
2018 ||| look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss. ||| 8538 ||| 8539 ||| 8540 ||| 8541 ||| 
2020 ||| multi-modal transformer for video retrieval. ||| 8542 ||| 2094 ||| 8543 ||| 2093 ||| 
2020 ||| hand-transformer: non-autoregressive structured modeling for 3d hand pose estimation. ||| 8544 ||| 8545 ||| 3478 ||| 8546 ||| 
2018 ||| pairwise body-part attention for recognizing human-object interactions. ||| 8547 ||| 8548 ||| 8549 ||| 5136 ||| 
2020 ||| efficient image super-resolution using pixel attention. ||| 8550 ||| 8551 ||| 8552 ||| 2149 ||| 6964 ||| 
2020 ||| an attention-driven two-stage clustering method for unsupervised person re-identification. ||| 8553 ||| 8554 ||| 8555 ||| 2530 ||| 8556 ||| 8557 ||| 
2020 ||| cafe-gan: arbitrary face attribute editing with complementary attention feature. ||| 8558 ||| 8559 ||| 8560 ||| 
2018 ||| image super-resolution using very deep residual channel attention networks. ||| 1730 ||| 2232 ||| 2233 ||| 8561 ||| 8562 ||| 1734 ||| 
2020 ||| spatially aware multimodal transformers for textvqa. ||| 8563 ||| 8564 ||| 8565 ||| 8566 ||| 8567 ||| 8568 ||| 8569 ||| 
2018 ||| deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization. ||| 8570 ||| 8571 ||| 127 ||| 208 ||| 
2018 ||| reinforced temporal attention and split-rate transfer for depth-based person re-identification. ||| 8572 ||| 8573 ||| 1959 ||| 2072 ||| 
2018 ||| temporal attention mechanism with conditional inference for large-scale multi-label video classification. ||| 8574 ||| 8575 ||| 8576 ||| 8577 ||| 8578 ||| 8579 ||| 8580 ||| 
2020 ||| c4av: learning cross-modal representations from transformers. ||| 8581 ||| 8582 ||| 1932 ||| 8583 ||| 
2020 ||| solar: second-order loss and attention for image retrieval. ||| 8584 ||| 8585 ||| 8586 ||| 8587 ||| 
2020 ||| few-shot semantic segmentation with democratic attention networks. ||| 7239 ||| 7237 ||| 7238 ||| 8588 ||| 1931 ||| 1930 ||| 
2018 ||| attention-aware deep adversarial hashing for cross-modal retrieval. ||| 1325 ||| 8589 ||| 1685 ||| 
2020 ||| attentionnas: spatiotemporal attention cell search for video classification. ||| 8590 ||| 8591 ||| 8592 ||| 8593 ||| 8594 ||| 8595 ||| 7445 ||| 8596 ||| 
2020 ||| monocular expressive body regression through body-driven attention. ||| 8597 ||| 8598 ||| 8599 ||| 8600 ||| 2100 ||| 
2018 ||| reverse attention for salient object detection. ||| 8601 ||| 8602 ||| 8603 ||| 8604 ||| 
2018 ||| cbam: convolutional block attention module. ||| 7916 ||| 5369 ||| 7917 ||| 5372 ||| 
2020 ||| da4ad: end-to-end deep attention-based visual localization for autonomous driving. ||| 8605 ||| 8606 ||| 8607 ||| 5999 ||| 8608 ||| 8609 ||| 8610 ||| 
2018 ||| question type guided attention in visual question answering. ||| 8611 ||| 8612 ||| 8613 ||| 8614 ||| 
2020 ||| box2seg: attention weighted loss and discriminative feature learning for weakly supervised segmentation. ||| 8615 ||| 8616 ||| 8617 ||| 2160 ||| 8618 ||| 
2020 ||| single image super-resolution via a holistic attention network. ||| 8619 ||| 8620 ||| 8621 ||| 8622 ||| 8623 ||| 8624 ||| 8625 ||| 8626 ||| 7064 ||| 
2020 ||| attention-based query expansion learning. ||| 8627 ||| 8628 ||| 8629 ||| 
2020 ||| semantic line detection using mirror attention and comparative ranking and matching. ||| 8630 ||| 7164 ||| 8631 ||| 
2020 ||| weight excitation: built-in attention mechanisms in convolutional neural networks. ||| 2585 ||| 8632 ||| 2589 ||| 2588 ||| 3337 ||| 
2018 ||| bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. ||| 978 ||| 8633 ||| 8634 ||| 1739 ||| 8635 ||| 8636 ||| 8637 ||| 
2020 ||| assemblenet++: assembling modality representations via attention connections. ||| 8594 ||| 8593 ||| 8638 ||| 8595 ||| 
2020 ||| read: reciprocal attention discriminator for image-to-video re-identification. ||| 8639 ||| 8640 ||| 8641 ||| 8642 ||| 
2018 ||| multimodal dual attention memory for video story question answering. ||| 8643 ||| 8578 ||| 8644 ||| 8580 ||| 
2020 ||| dmd: a large-scale multi-modal driver monitoring dataset for attention and alertness analysis. ||| 8645 ||| 8646 ||| 8647 ||| 8648 ||| 8649 ||| 8650 ||| 8651 ||| 8652 ||| 8653 ||| 
2018 ||| psanet: point-wise spatial attention network for scene parsing. ||| 2335 ||| 340 ||| 8654 ||| 8655 ||| 2291 ||| 2162 ||| 2204 ||| 
2020 ||| axial-deeplab: stand-alone axial-attention for panoptic segmentation. ||| 8656 ||| 8657 ||| 8658 ||| 8659 ||| 8660 ||| 8661 ||| 
2020 ||| span: spatial pyramid attention network for image manipulation localization. ||| 8662 ||| 8663 ||| 8664 ||| 8665 ||| 8666 ||| 8667 ||| 
2018 ||| attend and rectify: a gated attention mechanism for fine-grained recovery. ||| 8668 ||| 6235 ||| 8669 ||| 8670 ||| 8671 ||| 8672 ||| 8048 ||| 
2018 ||| mancs: a multi-task attentional network with curriculum sampling for person re-identification. ||| 3691 ||| 2251 ||| 2221 ||| 2222 ||| 2219 ||| 
2018 ||| stacked cross attention for image-text matching. ||| 8673 ||| 5250 ||| 8674 ||| 8675 ||| 3561 ||| 
2018 ||| single image water hazard detection using fcn with reflection attention units. ||| 8676 ||| 8677 ||| 8678 ||| 836 ||| 
2018 ||| boosted attention: leveraging human attention for image captioning. ||| 2235 ||| 1872 ||| 
2020 ||| end-to-end low cost compressive spectral imaging with spatial-spectral self-attention. ||| 8679 ||| 8680 ||| 5958 ||| 
2020 ||| end-to-end object detection with transformers. ||| 8681 ||| 8682 ||| 2120 ||| 8683 ||| 8684 ||| 8685 ||| 
2018 ||| spatial-temporal attention res-tcn for skeleton-based dynamic hand gesture recognition. ||| 8686 ||| 5925 ||| 8687 ||| 8688 ||| 8473 ||| 8689 ||| 
2018 ||| connecting gaze, scene, and attention: generalized attention estimation via joint modeling of gaze and scene saliency. ||| 22 ||| 8690 ||| 4945 ||| 6553 ||| 8691 ||| 8692 ||| 
2020 ||| few-shot action recognition with permutation-invariant attention. ||| 8693 ||| 254 ||| 2203 ||| 7449 ||| 2160 ||| 8694 ||| 
2020 ||| air: attention with reasoning capability. ||| 2235 ||| 1871 ||| 8695 ||| 1872 ||| 
2020 ||| spatio-temporal graph transformer networks for pedestrian trajectory prediction. ||| 8696 ||| 8697 ||| 8698 ||| 7267 ||| 1944 ||| 
2020 ||| password-conditioned anonymization and deanonymization with face identity transformers. ||| 8699 ||| 8700 ||| 8594 ||| 8701 ||| 
2018 ||| generative adversarial network with spatial attention for face attribute editing. ||| 8702 ||| 1915 ||| 1916 ||| 1788 ||| 
2018 ||| deep imbalanced attribute classification using visual attention aggregation. ||| 8703 ||| 8704 ||| 8705 ||| 
2020 ||| deep reinforced attention learning for quality-aware visual recognition. ||| 8706 ||| 2284 ||| 
2018 ||| deep residual attention network for spectral image super-resolution. ||| 8707 ||| 8708 ||| 2207 ||| 8709 ||| 8710 ||| 8711 ||| 
2020 ||| pynet-ca: enhanced pynet with channel attention for end-to-end mobile image signal processing. ||| 8712 ||| 8713 ||| 2048 ||| 8714 ||| 
2020 ||| pyramidal edge-maps and attention based guided thermal super-resolution. ||| 8715 ||| 8716 ||| 
2020 ||| deep surface normal estimation on the 2-sphere with confidence guided semantic attention. ||| 8717 ||| 8718 ||| 8719 ||| 8720 ||| 1845 ||| 8721 ||| 8722 ||| 
2018 ||| fine-grained video categorization with redundancy reduction attention. ||| 1835 ||| 8723 ||| 6924 ||| 2530 ||| 8724 ||| 1761 ||| 1839 ||| 
2018 ||| knowing where to look? analysis on attention of visual question answering system. ||| 3337 ||| 8725 ||| 8726 ||| 8727 ||| 
2020 ||| learning trailer moments in full-length movies with co-contrastive attention. ||| 1743 ||| 8709 ||| 8728 ||| 1749 ||| 
2020 ||| attend and segment: attention guided active semantic segmentation. ||| 2179 ||| 2181 ||| 
2020 ||| feedback attention for cell image segmentation. ||| 8729 ||| 8730 ||| 8731 ||| 
2020 ||| attention guided anomaly localization in images. ||| 8732 ||| 1746 ||| 1747 ||| 8733 ||| 
2020 ||| the devil is in the details: self-supervised attention for vehicle re-identification. ||| 2208 ||| 2210 ||| 2212 ||| 2213 ||| 
2018 ||| attention-based ensemble for deep metric learning. ||| 8734 ||| 8735 ||| 8736 ||| 8737 ||| 8738 ||| 
2020 ||| efficient attention mechanism for visual dialog that can handle all the interactions between multiple inputs. ||| 8739 ||| 8740 ||| 8741 ||| 
2020 ||| multi-attention based ultra lightweight image super-resolution. ||| 8742 ||| 8743 ||| 8744 ||| 8745 ||| 8746 ||| 8747 ||| 
2020 ||| spatial attention pyramid network for unsupervised domain adaptation. ||| 8748 ||| 8749 ||| 8750 ||| 8751 ||| 8752 ||| 8753 ||| 8754 ||| 
2018 ||| spatio-temporal transformer network for video restoration. ||| 8755 ||| 8756 ||| 8757 ||| 8758 ||| 8759 ||| 
2018 ||| "factual" or "emotional": stylized image captioning with adaptive learning and attention. ||| 2417 ||| 8760 ||| 1296 ||| 8761 ||| 8762 ||| 8763 ||| 2166 ||| 
2020 ||| forecasting human-object interaction: joint prediction of motor attention and actions in first person video. ||| 8764 ||| 8765 ||| 8766 ||| 8692 ||| 
2020 ||| multi-channel transformers for multi-articulatory sign language translation. ||| 8529 ||| 7442 ||| 8767 ||| 8768 ||| 8530 ||| 
2020 ||| suppressing mislabeled data via grouping and self-attention. ||| 8769 ||| 333 ||| 8770 ||| 3477 ||| 8771 ||| 2149 ||| 
2020 ||| unsupervised domain attention adaptation network for caricature attribute recognition. ||| 8772 ||| 8773 ||| 8774 ||| 8775 ||| 5089 ||| 
2018 ||| video object segmentation with joint re-identification and attention-aware mask propagation. ||| 8776 ||| 2291 ||| 
2020 ||| look here! a parametric learning based approach to redirect visual attention. ||| 8777 ||| 8778 ||| 8779 ||| 8780 ||| 8781 ||| 
2020 ||| attention-driven dynamic graph convolutional network for multi-label image recognition. ||| 8782 ||| 8783 ||| 8769 ||| 8784 ||| 2149 ||| 
2018 ||| learning visual question answering by bootstrapping hard attention. ||| 8785 ||| 8786 ||| 8787 ||| 8788 ||| 
2018 ||| predicting gaze in egocentric video by learning task-dependent attention transition. ||| 6419 ||| 7893 ||| 7892 ||| 7894 ||| 
2020 ||| supervised edge attention network for accurate image instance segmentation. ||| 8789 ||| 8790 ||| 400 ||| 2277 ||| 8791 ||| 8792 ||| 
2020 ||| attention enhanced single stage multimodal reasoner. ||| 8793 ||| 8794 ||| 
2018 ||| deep adaptive attention for joint facial action unit detection and face alignment. ||| 8795 ||| 5247 ||| 1691 ||| 5141 ||| 
2018 ||| give ear to my face: modelling multimodal attention to social interactions. ||| 8796 ||| 4693 ||| 4694 ||| 4695 ||| 4696 ||| 
2020 ||| volumetric transformer networks. ||| 8797 ||| 8798 ||| 8799 ||| 8505 ||| 
2018 ||| online multi-object tracking with dual matching attention networks. ||| 8800 ||| 1007 ||| 2411 ||| 8801 ||| 8802 ||| 7143 ||| 
2018 ||| deepphys: video-based physiological measurement using convolutional attention networks. ||| 8803 ||| 5732 ||| 
2020 ||| attention deeplabv3+: multi-level context attention mechanism for skin lesion segmentation. ||| 8804 ||| 8805 ||| 8806 ||| 8035 ||| 
2020 ||| how to track your dragon: a multi-attentional framework for real-time rgb-d 6-dof object pose tracking. ||| 8807 ||| 7929 ||| 8808 ||| 8809 ||| 8810 ||| 7930 ||| 
2020 ||| attngrounder: talking to cars with attention. ||| 8811 ||| 
2020 ||| self-calibrated attention neural network for real-world super resolution. ||| 8812 ||| 8813 ||| 
2020 ||| orientation-aware vehicle re-identification with semantics-guided part attention network. ||| 8814 ||| 8815 ||| 8816 ||| 8817 ||| 
2020 ||| take an emotion walk: perceiving emotions from gaits using hierarchical attention pooling and affective mapping. ||| 8818 ||| 8819 ||| 8820 ||| 7313 ||| 8821 ||| 8822 ||| 8823 ||| 7315 ||| 
2018 ||| agil: learning attention from human for visuomotor tasks. ||| 8824 ||| 8825 ||| 8826 ||| 8827 ||| 8828 ||| 8829 ||| 8830 ||| 
2018 ||| egocentric activity prediction via event modulated attention. ||| 8831 ||| 8832 ||| 8833 ||| 8834 ||| 
2018 ||| multi-attention multi-class constraint for fine-grained image recognition. ||| 2390 ||| 8835 ||| 6924 ||| 1761 ||| 
2018 ||| interaction-aware spatio-temporal pyramid attention networks for action classification. ||| 8836 ||| 8837 ||| 8838 ||| 8839 ||| 8840 ||| 8841 ||| 
2018 ||| deep fashion analysis with feature map upsampling and landmark-driven attention. ||| 8842 ||| 8843 ||| 
2020 ||| we learn better road pothole detection: from attention aggregation to adversarial domain adaptation. ||| 8844 ||| 8845 ||| 8846 ||| 124 ||| 
2020 ||| cross-attention in coupled unmixing nets for unsupervised hyperspectral super-resolution. ||| 8847 ||| 8848 ||| 8849 ||| 8850 ||| 8851 ||| 8852 ||| 
2020 ||| unsupervised deep metric learning with transformed attention consistency and contrastive clustering loss. ||| 438 ||| 8853 ||| 8854 ||| 
2020 ||| guiding monocular depth estimation using depth-attention volume. ||| 6320 ||| 6319 ||| 8855 ||| 6321 ||| 6322 ||| 
2018 ||| deep attention neural tensor network for visual question answering. ||| 8856 ||| 1699 ||| 880 ||| 2165 ||| 
2020 ||| visbert: hidden-state visualizations for transformers. ||| 1331 ||| 1332 ||| 1333 ||| 1334 ||| 1335 ||| 
2019 ||| dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. ||| 8857 ||| 8858 ||| 1578 ||| 8859 ||| 8860 ||| 8861 ||| 1579 ||| 
2018 ||| deepmove: predicting human mobility with attentional recurrent networks. ||| 6738 ||| 2969 ||| 8862 ||| 8863 ||| 8864 ||| 8865 ||| 8866 ||| 
2021 ||| galaxc: graph neural networks with labelwise attention for extreme classification. ||| 8867 ||| 8868 ||| 8869 ||| 4816 ||| 8870 ||| 4817 ||| 8871 ||| 
2020 ||| html: hierarchical transformer-based multi-task learning for volatility prediction. ||| 8872 ||| 8873 ||| 8874 ||| 8875 ||| 
2020 ||| domain adaptation with category attention network for deep sentiment analysis. ||| 8876 ||| 3476 ||| 8877 ||| 8878 ||| 8879 ||| 8880 ||| 
2020 |||  vu: a contextualized temporal attention mechanism for sequential recommendation. ||| 8881 ||| 8882 ||| 8883 ||| 
2021 ||| ntam: neighborhood-temporal attention model for disk failure prediction in cloud platforms. ||| 8884 ||| 8885 ||| 8886 ||| 8887 ||| 8888 ||| 293 ||| 8889 ||| 8890 ||| 8891 ||| 8892 ||| 8893 ||| 
2018 ||| : attention based autoencoder for rap lyrics representation learning. ||| 8894 ||| 1719 ||| 8895 ||| 6799 ||| 8896 ||| 1979 ||| 
2021 ||| l3i_lbpam at the finsim-2 task: learning financial semantic similarities with siamese transformers. ||| 8897 ||| 8898 ||| 8063 ||| 8899 ||| 8900 ||| 8901 ||| 
2020 ||| earn more social attention: user popularity based tag recommendation system. ||| 7755 ||| 8902 ||| 7756 ||| 
2020 ||| an end-to-end topic-enhanced self-attention network for social emotion classification. ||| 8903 ||| 8904 ||| 
2018 ||| an attention factor graph model for tweet entity linking. ||| 8905 ||| 8906 ||| 8907 ||| 
2019 ||| inferring search queries from web documents via a graph-augmented sequence to attention network. ||| 8908 ||| 1351 ||| 1352 ||| 8909 ||| 8910 ||| 1353 ||| 
2021 ||| linear-time self attention with codeword histogram for efficient recommendation. ||| 8911 ||| 8912 ||| 8913 ||| 8914 ||| 8915 ||| 1245 ||| 8916 ||| 
2020 ||| hierarchically structured transformer networks for fine-grained spatial event forecasting. ||| 3748 ||| 1124 ||| 8917 ||| 8918 ||| 
2020 ||| multi-context attention for entity matching. ||| 3282 ||| 8919 ||| 7681 ||| 8920 ||| 8921 ||| 
2018 ||| content attention model for aspect based sentiment analysis. ||| 8922 ||| 8923 ||| 8924 ||| 8925 ||| 8926 ||| 
2020 ||| attention please: your attention check questions in survey studies can be automatically answered. ||| 8927 ||| 8928 ||| 8929 ||| 8930 ||| 
2017 ||| attention to science. ||| 8931 ||| 8932 ||| 
2018 ||| attention network for information diffusion prediction. ||| 1176 ||| 1177 ||| 1178 ||| 
2020 ||| domain adaptive multi-modality neural attention network for financial forecasting. ||| 1294 ||| 8933 ||| 8934 ||| 8935 ||| 1295 ||| 
2017 ||| the effect of aging on visual attention shifting in collaborative document editing. ||| 8936 ||| 8937 ||| 8938 ||| 
2019 ||| link prediction with mutual attention for text-attributed networks. ||| 8939 ||| 8940 ||| 8941 ||| 
2021 ||| tcs_witm_2021 @finsim-2: transformer based models for automatic classification of financial terms. ||| 8942 ||| 8943 ||| 8944 ||| 1103 ||| 8945 ||| 
2018 ||| when e-commerce meets social media: identifying business on wechat moment using bilateral-attention lstm. ||| 2417 ||| 8946 ||| 1316 ||| 2166 ||| 
2021 ||| deep co-attention network for multi-view subspace learning. ||| 8933 ||| 2045 ||| 8916 ||| 8947 ||| 1295 ||| 
2018 ||| user-guided hierarchical attention network for multi-modal social image popularity prediction. ||| 781 ||| 8948 ||| 1224 ||| 8949 ||| 
2020 ||| transmodality: an end2end fusion method with transformer for multimodal sentiment analysis. ||| 8950 ||| 8951 ||| 3121 ||| 
2019 ||| a hierarchical attention retrieval model for healthcare question answering. ||| 8952 ||| 8953 ||| 5474 ||| 8954 ||| 
2021 ||| polyu-cbs at the finsim-2 task: combining distributional, string-based and transformers-based features for hypernymy detection in the financial domain. ||| 8955 ||| 8956 ||| 
2018 ||| neural attentional rating regression with review-level explanations. ||| 8957 ||| 1254 ||| 1255 ||| 1256 ||| 
2020 ||| heterogeneous graph transformer. ||| 8958 ||| 8959 ||| 8960 ||| 8961 ||| 
2021 ||| cross-positional attention for debiasing clicks. ||| 8962 ||| 3293 ||| 8963 ||| 1043 ||| 8964 ||| 8179 ||| 8965 ||| 
2021 ||| stan: spatio-temporal attention network for next location recommendation. ||| 8966 ||| 1073 ||| 1074 ||| 
2020 ||| graph attention topic modeling network. ||| 8967 ||| 8968 ||| 8969 ||| 8970 ||| 8626 ||| 8971 ||| 8972 ||| 
2019 ||| heterographic pun recognition via pronunciation and spelling understanding gated attention network. ||| 8973 ||| 8974 ||| 8967 ||| 8975 ||| 8976 ||| 8977 ||| 8978 ||| 
2020 ||| learning bi-directional social influence in information cascades using graph sequence attention networks. ||| 8979 ||| 8980 ||| 3248 ||| 8981 ||| 8982 ||| 
2019 ||| event detection using hierarchical multi-aspect attention. ||| 8983 ||| 8984 ||| 359 ||| 8985 ||| 
2020 ||| iart: intent-aware response ranking with transformers in information-seeking conversation systems. ||| 1041 ||| 1240 ||| 8986 ||| 1242 ||| 3738 ||| 3035 ||| 1140 ||| 8987 ||| 
2021 ||| using prior knowledge to guide bert's attention in semantic textual matching tasks. ||| 8988 ||| 7400 ||| 5904 ||| 3410 ||| 
2019 ||| black hat trolling, white hat trolling, and hacking the attention landscape. ||| 8989 ||| 8990 ||| 
2020 ||| probabilistic logic graph attention networks for reasoning. ||| 8991 ||| 8992 ||| 8993 ||| 
2020 ||| condition aware and revise transformer for question answering. ||| 8994 ||| 3996 ||| 8995 ||| 8996 ||| 8997 ||| 
2020 ||| herding a deluge of good samaritans: how github projects respond to increased attention. ||| 8998 ||| 8999 ||| 9000 ||| 9001 ||| 
2019 ||| tissa: a time slice self-attention approach for modeling sequential user behaviors. ||| 9002 ||| 9003 ||| 885 ||| 
2018 ||| attention convolutional neural network for advertiser-level click-through rate forecasting. ||| 9004 ||| 9005 ||| 9006 ||| 3239 ||| 1825 ||| 
2019 ||| neural multimodal belief tracker with adaptive attention for dialogue systems. ||| 1770 ||| 9007 ||| 9008 ||| 9009 ||| 3605 ||| 
2019 ||| attention - from neuroscience to the web and wellbeing. ||| 
2019 ||| predicting human mobility via variational attention. ||| 2887 ||| 9010 ||| 9011 ||| 9012 ||| 9013 ||| 9014 ||| 
2018 ||| latent relational metric learning via memory-based attention for collaborative ranking. ||| 1398 ||| 9015 ||| 9016 ||| 
2018 ||| may i have your attention, please: - building a dystopian attention economy. ||| 9017 ||| 
2019 ||| an attention-based model for joint extraction of entities and relations with implicit entity features. ||| 9018 ||| 9019 ||| 9020 ||| 5295 ||| 233 ||| 
2019 ||| heterogeneous graph attention network. ||| 5957 ||| 9021 ||| 1373 ||| 412 ||| 9022 ||| 1084 ||| 1094 ||| 
2020 ||| high quality candidate generation and sequential graph attention network for entity linking. ||| 4236 ||| 987 ||| 9023 ||| 758 ||| 4190 ||| 9024 ||| 
2021 ||| : dual attention matching network with normalized hard sample mining. ||| 9025 ||| 9026 ||| 350 ||| 349 ||| 
2021 ||| beyond outlier detection: outlier interpretation by attention-guided triplet deviation network. ||| 761 ||| 6078 ||| 762 ||| 760 ||| 507 ||| 9027 ||| 9028 ||| 
2020 ||| weakly supervised attention for hashtag recommendation using graph data. ||| 9029 ||| 9030 ||| 9031 ||| 9032 ||| 9033 ||| 
2019 ||| quantifying the impact of user attentionon fair group representation in ranked lists. ||| 9034 ||| 9035 ||| 9036 ||| 9037 ||| 9038 ||| 
2019 ||| focusing attention network for answer ranking. ||| 9039 ||| 9040 ||| 9041 ||| 9042 ||| 9043 ||| 
2018 ||| what we read, what we search: media attention and public attention among 193 countries. ||| 9044 ||| 9045 ||| 9046 ||| 9047 ||| 9048 ||| 
2020 ||| outfitnet: fashion outfit recommendation with attention-based multiple instance learning. ||| 9049 ||| 9050 ||| 2792 ||| 
2021 ||| tweet-aware news summarization with dual-attention mechanism. ||| 9051 ||| 1397 ||| 9052 ||| 
2020 ||| towards fine-grained flow forecasting: a graph attention approach for bike sharing systems. ||| 9053 ||| 9054 ||| 
2018 ||| laan: a linguistic-aware attention network for sentiment analysis. ||| 3176 ||| 3177 ||| 1199 ||| 
2020 ||| multiple knowledge syncretic transformer for natural dialogue generation. ||| 9055 ||| 5093 ||| 9056 ||| 9057 ||| 9058 ||| 9059 ||| 
2019 ||| user-video co-attention network for personalized micro-video recommendation. ||| 9060 ||| 9061 ||| 9062 ||| 9063 ||| 
2020 ||| dual-attentional factorization-machines based neural network for user response prediction. ||| 968 ||| 5819 ||| 9064 ||| 9065 ||| 7466 ||| 9066 ||| 
2021 ||| predicting customer value with social relationships via motif-based graph attention networks. ||| 9067 ||| 9068 ||| 9069 ||| 9070 ||| 2969 ||| 
2018 ||| monitoring students' attention in a classroom through computer vision. ||| 9071 ||| 9072 ||| 2871 ||| 9073 ||| 
2021 ||| an attentional model for earthquake prediction using seismic data. ||| 9074 ||| 9075 ||| 9076 ||| 5511 ||| 5421 ||| 
2020 ||| attention in recurrent neural networks for energy disaggregation. ||| 9077 ||| 9078 ||| 9079 ||| 
2018 ||| using stop-motion video as visual indicator to strength children with asd's attention focus on specific nonverbal social cues to enhance perception judgments and situational awareness. ||| 9080 ||| 
2019 ||| mixed attention-aware network for person re-identification. ||| 9081 ||| 9082 ||| 9083 ||| 
2017 ||| power transformer fault diagnosis using support vector machine and particle swarm optimization. ||| 9084 ||| 9085 ||| 9086 ||| 9087 ||| 9088 ||| 
2021 ||| attention-based joint feature extraction model for static music emotion classification. ||| 9089 ||| 9090 ||| 9091 ||| 9092 ||| 9093 ||| 8802 ||| 
2019 ||| a densely connected transformer for machine translation. ||| 9094 ||| 9095 ||| 9096 ||| 9097 ||| 9098 ||| 9099 ||| 
2020 ||| short text classification model based on multi-attention. ||| 9100 ||| 9101 ||| 
2019 ||| attention and multi-layer fusion for real-time semantic segmentation. ||| 9102 ||| 613 ||| 264 ||| 9103 ||| 
2019 ||| generating topical and emotional responses using topic attention. ||| 9104 ||| 9105 ||| 9106 ||| 9107 ||| 9108 ||| 
2021 ||| malaria parasite detection using residual attention u-net. ||| 9109 ||| 9110 ||| 9111 ||| 9112 ||| 9113 ||| 
2020 ||| the application of transformer model architecture for the dependency parsing task. ||| 9114 ||| 9115 ||| 9116 ||| 9117 ||| 
2019 ||| a bioinspired model of decision making considering spatial attention for goal-driven behaviour. ||| 9118 ||| 9119 ||| 9120 ||| 9121 ||| 504 ||| 9122 ||| 
2018 ||| modeling spatial auditory attention in act-r: a constraint-based approach. ||| 9123 ||| 9124 ||| 9125 ||| 9126 ||| 
2019 ||| input-cell attention reduces vanishing saliency of recurrent neural networks. ||| 9127 ||| 9128 ||| 9129 ||| 2712 ||| 9130 ||| 9131 ||| 
2017 ||| attention is all you need. ||| 2466 ||| 9132 ||| 9133 ||| 4960 ||| 8069 ||| 9134 ||| 9135 ||| 9136 ||| 
2017 ||| attend and predict: understanding gene regulation by selective attention on chromatin. ||| 9137 ||| 9138 ||| 9139 ||| 9140 ||| 
2019 ||| ouroboros: on accelerating training of transformer-based language models. ||| 9141 ||| 9142 ||| 3136 ||| 9143 ||| 1032 ||| 
2020 ||| attention-gated brain propagation: how the brain can implement reward-based error backpropagation. ||| 9144 ||| 9145 ||| 9146 ||| 
2020 ||| ranet: region attention network for semantic segmentation. ||| 9147 ||| 9148 ||| 977 ||| 9149 ||| 9150 ||| 
2018 ||| unsupervised attention-guided image-to-image translation. ||| 9151 ||| 9152 ||| 9153 ||| 9154 ||| 8779 ||| 
2020 ||| o(n) connections are expressive enough: universal approximability of sparse transformers. ||| 9155 ||| 9156 ||| 2567 ||| 9157 ||| 9158 ||| 9159 ||| 
2020 ||| coot: cooperative hierarchical transformer for video-text representation learning. ||| 9160 ||| 9161 ||| 9162 ||| 9163 ||| 
2020 ||| modern hopfield networks and attention for immune repertoire classification. ||| 9164 ||| 8758 ||| 277 ||| 9165 ||| 9166 ||| 9167 ||| 9168 ||| 9169 ||| 9170 ||| 9171 ||| 9172 ||| 2101 ||| 9173 ||| 
2019 ||| attentionxml: label tree-based attention-aware deep model for high-performance extreme multi-label text classification. ||| 9174 ||| 9175 ||| 9176 ||| 9177 ||| 9178 ||| 9179 ||| 
2020 ||| smyrf - efficient attention using asymmetric clustering. ||| 9180 ||| 3143 ||| 9181 ||| 9182 ||| 
2017 ||| vain: attentional multi-agent predictive modeling. ||| 9183 ||| 
2019 ||| semantic-guided multi-attention localization for zero-shot learning. ||| 9184 ||| 9185 ||| 9186 ||| 9187 ||| 9188 ||| 
2020 ||| self-supervised graph transformer on large-scale molecular data. ||| 1261 ||| 9189 ||| 1262 ||| 9190 ||| 9191 ||| 1263 ||| 1265 ||| 
2020 ||| untangling tradeoffs between recurrence and self-attention in artificial neural networks. ||| 9192 ||| 9193 ||| 9194 ||| 9195 ||| 9196 ||| 9197 ||| 
2020 ||| auto learning attention. ||| 9198 ||| 875 ||| 9199 ||| 1756 ||| 
2020 ||| sac: accelerating and structuring self-attention via sparse adaptive connection. ||| 9200 ||| 9201 ||| 9202 ||| 9203 ||| 2258 ||| 9204 ||| 
2018 ||| learning attentional communication for multi-agent cooperation. ||| 9205 ||| 9206 ||| 
2018 ||| watch your step: learning node embeddings via graph attention. ||| 9207 ||| 9208 ||| 4824 ||| 9209 ||| 
2017 ||| a regularized framework for sparse and structured neural attention. ||| 9210 ||| 9211 ||| 
2019 ||| social-bigat: multimodal trajectory forecasting using bicycle-gan and graph attention networks. ||| 9212 ||| 9213 ||| 9214 ||| 9215 ||| 3882 ||| 9216 ||| 9217 ||| 9218 ||| 
2019 ||| induced attention invariance: defending vqa models against adversarial attacks. ||| 3396 ||| 9219 ||| 3601 ||| 
2019 ||| incremental few-shot learning with attention attractor networks. ||| 9220 ||| 9221 ||| 9222 ||| 9223 ||| 
2020 ||| accelerating training of transformer-based language models with progressive layer dropping. ||| 9224 ||| 9225 ||| 
2020 ||| deep reinforcement learning with stacked hierarchical attention for text-based games. ||| 9226 ||| 4980 ||| 9227 ||| 9228 ||| 3389 ||| 4873 ||| 
2020 ||| big bird: transformers for longer sequences. ||| 9229 ||| 9230 ||| 9231 ||| 3557 ||| 9232 ||| 9233 ||| 3882 ||| 9234 ||| 3555 ||| 9235 ||| 2884 ||| 5776 ||| 
2019 ||| efficient graph generation with graph recurrent attention networks. ||| 9221 ||| 231 ||| 326 ||| 9236 ||| 9237 ||| 9238 ||| 9239 ||| 9223 ||| 
2019 ||| supervised multimodal bitransformers for classifying images and text. ||| 3826 ||| 9240 ||| 9241 ||| 9242 ||| 
2020 ||| comprehensive attention self-distillation for weakly-supervised object detection. ||| 2356 ||| 9243 ||| 8526 ||| 7393 ||| 
2019 ||| novel positional encodings to enable tree-based transformers. ||| 9244 ||| 9245 ||| 
2019 ||| adaptively aligned image captioning via adaptive attention time. ||| 2199 ||| 2200 ||| 9246 ||| 1037 ||| 
2019 ||| graph transformer networks. ||| 9247 ||| 9248 ||| 9249 ||| 9250 ||| 9251 ||| 
2017 ||| variational laws of visual attention for dynamic scenes. ||| 896 ||| 898 ||| 
2020 ||| se(3)-transformers: 3d roto-translation equivariant attention networks. ||| 9252 ||| 9253 ||| 9254 ||| 9255 ||| 
2020 ||| why are adaptive methods good for attention models? ||| 9256 ||| 9257 ||| 2572 ||| 9258 ||| 9158 ||| 9159 ||| 9259 ||| 
2018 ||| bilinear attention networks. ||| 8644 ||| 9260 ||| 8580 ||| 
2018 ||| densely connected attention propagation for reading comprehension. ||| 1398 ||| 9261 ||| 9016 ||| 9262 ||| 
2017 ||| attentional pooling for action recognition. ||| 1663 ||| 9263 ||| 
2019 ||| enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. ||| 9264 ||| 9265 ||| 9266 ||| 9267 ||| 3799 ||| 9268 ||| 3801 ||| 
2020 ||| improving natural language processing tasks with human gaze-guided neural attention. ||| 9269 ||| 9270 ||| 8346 ||| 3831 ||| 8348 ||| 
2019 ||| a tensorized transformer for language modeling. ||| 5076 ||| 989 ||| 3364 ||| 3706 ||| 5078 ||| 3480 ||| 3764 ||| 
2019 ||| towards interpretable reinforcement learning using attention augmented agents. ||| 9271 ||| 2103 ||| 9272 ||| 9273 ||| 9274 ||| 
2017 ||| visual reference resolution using attention memory for visual dialog. ||| 9275 ||| 9276 ||| 9277 ||| 2301 ||| 
2020 ||| relationnet++: bridging visual representations for object detection via transformer decoder. ||| 9278 ||| 9279 ||| 1768 ||| 
2019 ||| modulated self-attention convolutional network for vqa. ||| 9280 ||| 
2020 ||| bayesian attention modules. ||| 9281 ||| 9282 ||| 9283 ||| 9284 ||| 
2017 ||| learning deep structured multi-scale features using attention-gated crfs for contour prediction. ||| 436 ||| 2303 ||| 9285 ||| 2524 ||| 1846 ||| 437 ||| 
2018 ||| recurrent transformer networks for semantic correspondence. ||| 8797 ||| 1771 ||| 9286 ||| 9287 ||| 1515 ||| 
2020 ||| object-centric learning with slot attention. ||| 9288 ||| 9289 ||| 2571 ||| 9290 ||| 2294 ||| 4960 ||| 9291 ||| 9292 ||| 
2019 ||| leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog. ||| 9293 ||| 9294 ||| 9295 ||| 9296 ||| 9297 ||| 
2020 ||| cascaded text generation with markov transformers. ||| 3944 ||| 4962 ||| 
2019 ||| visually grounded video reasoning in selective attention memory. ||| 9298 ||| 9299 ||| 9300 ||| 9301 ||| 9302 ||| 
2019 ||| compositional de-attention networks. ||| 1398 ||| 9261 ||| 3360 ||| 3363 ||| 9016 ||| 
2018 ||| a^2-nets: double attention networks. ||| 1723 ||| 9303 ||| 9304 ||| 1728 ||| 1685 ||| 
2017 ||| high-order attention models for visual question answering. ||| 9305 ||| 8566 ||| 9306 ||| 
2019 ||| one-shot object detection with co-attention and co-excitation. ||| 9307 ||| 9308 ||| 6362 ||| 6363 ||| 
2020 ||| multi-task temporal shift attention networks for on-device contactless vitals measurement. ||| 189 ||| 9309 ||| 9310 ||| 5732 ||| 
2018 ||| uncertainty-aware attention for reliable interpretation and prediction. ||| 9311 ||| 9312 ||| 9313 ||| 9314 ||| 9315 ||| 9316 ||| 9317 ||| 
2019 ||| levenshtein transformer. ||| 9318 ||| 9319 ||| 9320 ||| 
2020 ||| limits to depth efficiencies of self-attention. ||| 9321 ||| 9322 ||| 9323 ||| 9324 ||| 9325 ||| 
2020 ||| attend and decode: 4d fmri task state decoding using attention models. ||| 9326 ||| 9327 ||| 9328 ||| 9329 ||| 
2020 ||| ratt: recurrent attention to transient tasks for continual image captioning. ||| 9330 ||| 9331 ||| 9332 ||| 7105 ||| 
2019 ||| understanding attention and generalization in graph neural networks. ||| 2579 ||| 2582 ||| 9333 ||| 
2020 ||| multi-agent trajectory prediction with fuzzy query attention. ||| 9334 ||| 9335 ||| 9336 ||| 1251 ||| 9337 ||| 
2020 ||| semg gesture recognition with a simple model of attention. ||| 9338 ||| 9339 ||| 9340 ||| 9341 ||| 
2017 ||| saliency-based sequential image attention with multiset prediction. ||| 9342 ||| 9343 ||| 3008 ||| 1770 ||| 
2020 ||| adversarial sparse transformer for time series forecasting. ||| 9344 ||| 2744 ||| 9345 ||| 9346 ||| 9191 ||| 1265 ||| 
2020 ||| fast transformers with clustered attention. ||| 9347 ||| 9348 ||| 1226 ||| 9349 ||| 
2020 ||| prophet attention: predicting attention with future attention. ||| 3746 ||| 9350 ||| 3748 ||| 3749 ||| 7417 ||| 4430 ||| 3751 ||| 
2018 ||| attention in convolutional lstm for gesture recognition. ||| 1166 ||| 9351 ||| 9352 ||| 9353 ||| 9354 ||| 5328 ||| 
2019 ||| learning dynamics of attention: human prior for interpretable machine reasoning. ||| 9355 ||| 9356 ||| 
2020 ||| learning to execute programs with instruction pointer attention graph neural networks. ||| 9357 ||| 9358 ||| 9359 ||| 9360 ||| 
2019 ||| predicting utilization of healthcare services from individual disease trajectories using rnns with multi-headed attention. ||| 9361 ||| 9362 ||| 9363 ||| 9364 ||| 9365 ||| 9366 ||| 
2020 ||| sparse and continuous attention mechanisms. ||| 3369 ||| 3370 ||| 2871 ||| 7901 ||| 9367 ||| 9210 ||| 7902 ||| 5335 ||| 9368 ||| 
2020 ||| funnel-transformer: filtering out sequential redundancy for efficient language processing. ||| 3780 ||| 9369 ||| 2622 ||| 2465 ||| 
2019 ||| saccader: improving accuracy of hard attention models for vision. ||| 9370 ||| 9371 ||| 9372 ||| 
2019 ||| nat: neural architecture transformer for accurate and compact architectures. ||| 9373 ||| 9374 ||| 6413 ||| 6415 ||| 9375 ||| 9346 ||| 1265 ||| 
2019 ||| cross attention network for few-shot classification. ||| 9376 ||| 9377 ||| 9378 ||| 1916 ||| 1788 ||| 
2020 ||| attendlight: universal attention-based reinforcement learning model for traffic signal control. ||| 9379 ||| 9380 ||| 9381 ||| 9382 ||| 
2019 ||| a self validation network for object-level human attention estimation. ||| 9383 ||| 4997 ||| 9384 ||| 
2020 ||| neural encoding with visual attention. ||| 9385 ||| 9386 ||| 9387 ||| 9388 ||| 9389 ||| 
2020 ||| deep transformers with latent depth. ||| 9390 ||| 9391 ||| 9392 ||| 9393 ||| 
2018 ||| stacked semantics-guided attention model for fine-grained zero-shot learning. ||| 9394 ||| 2276 ||| 6365 ||| 9395 ||| 2279 ||| 9396 ||| 
2020 ||| focus of attention improves information transfer in visual features. ||| 9397 ||| 897 ||| 9398 ||| 9399 ||| 898 ||| 
2020 ||| musical speech: a transformer-based composition tool. ||| 9400 ||| 9401 ||| 9402 ||| 9403 ||| 9404 ||| 
2020 ||| kalman filtering attention for user behavior modeling in ctr prediction. ||| 5525 ||| 811 ||| 9405 ||| 9406 ||| 9407 ||| 9408 ||| 9383 ||| 595 ||| 9409 ||| 1171 ||| 1170 ||| 
2018 ||| latent alignment and variational attention. ||| 3944 ||| 9410 ||| 9411 ||| 9412 ||| 4962 ||| 
2020 ||| minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. ||| 3497 ||| 3174 ||| 3171 ||| 3498 ||| 9413 ||| 3480 ||| 
2020 ||| crosstransformers: spatially-aware few-shot transfer. ||| 8786 ||| 9414 ||| 1997 ||| 
2020 ||| measuring systematic generalization in neural proof generation with transformers. ||| 9415 ||| 9416 ||| 9417 ||| 9418 ||| 
2019 ||| self-attention with functional time representation learning. ||| 9419 ||| 9420 ||| 9421 ||| 9422 ||| 9423 ||| 9424 ||| 
2018 ||| self-erasing network for integral object attention. ||| 1902 ||| 1901 ||| 1905 ||| 1904 ||| 
2020 ||| neurosymbolic transformers for multi-agent communication. ||| 9425 ||| 9426 ||| 9427 ||| 9428 ||| 9429 ||| 9430 ||| 9431 ||| 9432 ||| 
2019 ||| stand-alone self-attention in vision models. ||| 9133 ||| 9433 ||| 2466 ||| 2463 ||| 9434 ||| 9435 ||| 
2018 ||| attention-based semantic priming for slot-filling. ||| 9436 ||| 9437 ||| 9438 ||| 9439 ||| 3302 ||| 
2019 ||| residual attention regression for 3d hand pose estimation. ||| 4807 ||| 4864 ||| 9440 ||| 
2021 ||| gaze based implicit intention inference with historical information of visual attention for human-robot interaction. ||| 9441 ||| 9442 ||| 
2019 ||| the effectiveness of eeg-feedback on attention in 3d virtual environment. ||| 7400 ||| 9443 ||| 9444 ||| 9445 ||| 9446 ||| 9447 ||| 
2020 ||| progressive attentional learning for underwater image super-resolution. ||| 9448 ||| 9449 ||| 9450 ||| 9451 ||| 9452 ||| 
2019 ||| select and focus: action recognition with spatial-temporal attention. ||| 9453 ||| 9454 ||| 9455 ||| 9456 ||| 9457 ||| 
2021 ||| bearing fault diagnosis based on attentional multi-scale cnn. ||| 9458 ||| 9337 ||| 9459 ||| 9460 ||| 
2021 ||| research on chinese text summarization based on core word attention mechanism. ||| 1457 ||| 1456 ||| 9461 ||| 
2021 ||| improved am-lstm for power transformer error forecasting model. ||| 9462 ||| 9463 ||| 9464 ||| 2054 ||| 9465 ||| 9466 ||| 9467 ||| 
2020 ||| attention-based state decoupler of vibration transmission in multi-working conditions of rolling bearings. ||| 9468 ||| 9469 ||| 
2021 ||| self-attention based multitasking sequential recom mendation. ||| 9470 ||| 9442 ||| 9471 ||| 9472 ||| 9473 ||| 
2021 ||| facial expression recognition based on hybrid attention mechanism. ||| 9467 ||| 9474 ||| 9475 ||| 9476 ||| 9477 ||| 
2020 ||| dynamic facial expression recognition model based on bilstm-attention. ||| 9478 ||| 9479 ||| 9480 ||| 9481 ||| 
2021 ||| calculation of error of multi-winding voltage transformer under arbitrary secondary load by determinant method. ||| 9463 ||| 9462 ||| 9464 ||| 9465 ||| 9482 ||| 9483 ||| 
2017 ||| an attention based model for off-topic spontaneous spoken response detection: an initial study. ||| 9484 ||| 9485 ||| 9486 ||| 3906 ||| 3796 ||| 
2019 ||| gaming the attention with a ssvep-based brain-computer interface. ||| 9487 ||| 9488 ||| 9489 ||| 
2017 ||| robust joint visual attention for hri using a laser pointer for perspective alignment and deictic referring. ||| 9490 ||| 9491 ||| 9492 ||| 9493 ||| 
2017 ||| a neurologically inspired network model for graziano's attention schema theory for consciousness. ||| 9494 ||| 9495 ||| 9496 ||| 
2020 ||| lvbert: transformer-based model for latvian language understanding. ||| 9497 ||| 9498 ||| 
2021 ||| progressive guidance categorization using transformer-based deep neural network architecture. ||| 9499 ||| 9500 ||| 9501 ||| 9502 ||| 9503 ||| 9504 ||| 
2021 ||| wifimod: transformer-based indoor human mobility modeling using passive sensing. ||| 9505 ||| 9506 ||| 9507 ||| 9508 ||| 3325 ||| 
2019 ||| nuclei detection using residual attention feature pyramid networks. ||| 9509 ||| 9510 ||| 9511 ||| 
2020 ||| evaluation of hyperbolic attention in histopathology images. ||| 9512 ||| 9513 ||| 9514 ||| 
2021 ||| detecting attention in hilbert-transformed eeg brain signals from simple-reaction and choice-reaction cognitive tasks. ||| 9515 ||| 9516 ||| 9517 ||| 
2018 ||| interpretable prediction of vascular diseases from electronic health records via deep attention networks. ||| 9518 ||| 9519 ||| 9520 ||| 9521 ||| 9522 ||| 9523 ||| 
2018 ||| attention spanned: comprehensive vulnerability analysis of at commands within the android ecosystem. ||| 9524 ||| 9525 ||| 9526 ||| 9527 ||| 9528 ||| 9529 ||| 9530 ||| 9531 ||| 9532 ||| 9533 ||| 9534 ||| 
2021 ||| reducing test cases with attention mechanism of neural networks. ||| 9535 ||| 9536 ||| 9537 ||| 9538 ||| 9539 ||| 2747 ||| 9540 ||| 9541 ||| 
2021 ||| siamhan: ipv6 address correlation attacks on tls encrypted traffic via siamese heterogeneous graph attention network. ||| 9542 ||| 745 ||| 9543 ||| 5189 ||| 9544 ||| 748 ||| 
2017 ||| leveraging contextual sentence relations for extractive summarization using a neural attention model. ||| 9545 ||| 1189 ||| 9546 ||| 3174 ||| 9547 ||| 1048 ||| 
2021 ||| hierarchical multi-modal contextual attention network for fake news detection. ||| 1174 ||| 9548 ||| 844 ||| 1173 ||| 1175 ||| 
2018 ||| equity of attention: amortizing individual fairness in rankings. ||| 9549 ||| 9550 ||| 9551 ||| 
2021 ||| cross-graph attention enhanced multi-modal correlation learning for fine-grained image-text retrieval. ||| 2944 ||| 189 ||| 9552 ||| 9553 ||| 9554 ||| 9555 ||| 
2018 ||| ca-lstm: search task identification with context attention based lstm. ||| 9556 ||| 9557 ||| 2969 ||| 
2018 ||| modeling dynamic pairwise attention for crime classification over legal articles. ||| 822 ||| 4117 ||| 9558 ||| 3035 ||| 241 ||| 9559 ||| 
2021 ||| presize: predicting size in e-commerce using transformers. ||| 9560 ||| 9561 ||| 9562 ||| 9563 ||| 
2018 ||| multihop attention networks for question answer matching. ||| 9564 ||| 9565 ||| 9566 ||| 
2021 ||| augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. ||| 1428 ||| 1427 ||| 3906 ||| 1094 ||| 
2017 ||| searching on the go: the effects of fragmented attention on mobile web search tasks. ||| 7571 ||| 7572 ||| 
2021 ||| package recommendation with intra- and inter-package attention networks. ||| 399 ||| 9567 ||| 1160 ||| 1373 ||| 3475 ||| 9568 ||| 1372 ||| 181 ||| 9569 ||| 
2017 ||| video question answering via attribute-augmented attention network learning. ||| 7651 ||| 1306 ||| 7650 ||| 9570 ||| 7652 ||| 7654 ||| 
2020 ||| a transformer-based embedding model for personalized product search. ||| 9571 ||| 1276 ||| 1140 ||| 
2018 ||| attention-driven factor model for explainable personalized recommendation. ||| 9572 ||| 3476 ||| 9573 ||| 3199 ||| 9574 ||| 8880 ||| 
2020 ||| multiplex behavioral relation learning for recommendation via memory augmented transformer network. ||| 1126 ||| 1124 ||| 1125 ||| 2588 ||| 1099 ||| 9575 ||| 
2020 ||| 3d self-attention for unsupervised video quantization. ||| 9576 ||| 9577 ||| 9578 ||| 9579 ||| 1039 ||| 1040 ||| 
2020 ||| choppy: cut transformer for ranked list truncation. ||| 3292 ||| 1398 ||| 9580 ||| 3294 ||| 9581 ||| 
2021 ||| improving transformer-kernel ranking model using conformer and query term independence. ||| 9582 ||| 9583 ||| 9584 ||| 9585 ||| 9586 ||| 
2020 ||| spatio-temporal dual graph attention network for query-poi matching. ||| 9587 ||| 5170 ||| 9588 ||| 9589 ||| 9590 ||| 9591 ||| 9592 ||| 
2020 ||| multi-level multimodal transformer network for multimodal recipe comprehension. ||| 9593 ||| 9594 ||| 9595 ||| 9596 ||| 9597 ||| 9598 ||| 596 ||| 
2020 ||| neural unified review recommendation with cross attention. ||| 4166 ||| 696 ||| 9599 ||| 8197 ||| 697 ||| 
2021 ||| hybrid fusion with intra- and cross-modality attention for image-recipe retrieval. ||| 9600 ||| 9579 ||| 9601 ||| 1038 ||| 9602 ||| 9603 ||| 1040 ||| 
2017 ||| enhancing recurrent neural networks with positional attention for question answering. ||| 9604 ||| 9605 ||| 9606 ||| 329 ||| 9607 ||| 
2021 ||| transformer-based banking products recommender system. ||| 9608 ||| 9609 ||| 9610 ||| 
2020 ||| creating a children-friendly reading environment via joint learning of content and human attention. ||| 9611 ||| 9612 ||| 9613 ||| 9614 ||| 3084 ||| 3086 ||| 3131 ||| 
2020 ||| local self-attention over long text for efficient document retrieval. ||| 9583 ||| 9584 ||| 9585 ||| 9582 ||| 9586 ||| 9615 ||| 
2020 ||| deep interest with hierarchical attention network for click-through rate prediction. ||| 9616 ||| 9617 ||| 9618 ||| 9619 ||| 9620 ||| 9621 ||| 
2019 ||| adaptive multi-attention network incorporating answer information for duplicate question detection. ||| 9622 ||| 9623 ||| 9624 ||| 336 ||| 9625 ||| 9626 ||| 9627 ||| 3273 ||| 
2018 ||| large scale taxonomy classification using bilstm with self-attention. ||| 9628 ||| 9629 ||| 
2017 ||| attentive collaborative filtering: multimedia recommendation with item- and component-level attention. ||| 9630 ||| 2484 ||| 1063 ||| 9631 ||| 683 ||| 3605 ||| 
2020 ||| a knowledge-enhanced recommendation model with attribute-level co-attention. ||| 9632 ||| 9633 ||| 9634 ||| 9635 ||| 
2018 ||| saan: a sentiment-aware attention network for sentiment analysis. ||| 3176 ||| 3177 ||| 1081 ||| 
2020 ||| efficient document re-ranking for transformers by precomputing term representations. ||| 9636 ||| 9637 ||| 9638 ||| 9639 ||| 4957 ||| 9640 ||| 
2020 ||| attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view. ||| 9641 ||| 9642 ||| 9643 ||| 9644 ||| 9407 ||| 7030 ||| 1094 ||| 
2021 ||| transformer reasoning network for personalized review summarization. ||| 9599 ||| 4166 ||| 697 ||| 696 ||| 
2019 ||| personalized fashion recommendation with visual explanations based on multimodal attention network: towards visually explainable recommendation. ||| 9645 ||| 9646 ||| 9647 ||| 3035 ||| 3603 ||| 5146 ||| 8949 ||| 
2020 ||| guided transformer: leveraging multiple external sources for representation learning in conversational search. ||| 9648 ||| 9585 ||| 1140 ||| 
2020 ||| relevance transformer: generating concise code snippets with relevance feedback. ||| 9649 ||| 9650 ||| 9651 ||| 
2018 ||| update delivery mechanisms for prospective information needs: an analysis of attention in mobile users. ||| 3009 ||| 9652 ||| 9653 ||| 9654 ||| 
2019 ||| nrpa: neural recommendation with personalized attention. ||| 4166 ||| 3755 ||| 696 ||| 4165 ||| 697 ||| 3754 ||| 9574 ||| 
2021 ||| fast attention-based learning-to-rank model for structured map search. ||| 9655 ||| 9656 ||| 9657 ||| 9658 ||| 
2020 ||| learning efficient representations of mouse movements to predict user attention. ||| 9659 ||| 9660 ||| 
2021 ||| looking at ctr prediction again: is attention all you need? ||| 7385 ||| 9661 ||| 
2021 ||| heterogeneous attention network for effective and efficient cross-modal retrieval. ||| 1327 ||| 208 ||| 1329 ||| 5071 ||| 1328 ||| 977 ||| 
2021 ||| position enhanced mention graph attention network for dialogue relation extraction. ||| 9662 ||| 9558 ||| 9663 ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond. ||| 9664 ||| 3006 ||| 3009 ||| 
2021 ||| learning a fine-grained review-based transformer model for personalized product search. ||| 9571 ||| 1276 ||| 1140 ||| 
2021 ||| dsgpt: domain-specific generative pre-training of transformers for text generation in e-commerce title and review summarization. ||| 9665 ||| 9666 ||| 9667 ||| 9668 ||| 1460 ||| 9669 ||| 9670 ||| 9671 ||| 
2019 ||| user attention-guided multimodal dialog systems. ||| 9672 ||| 9673 ||| 9674 ||| 9008 ||| 9675 ||| 9631 ||| 
2020 ||| improving neural chinese word segmentation with lexicon-enhanced adaptive attention. ||| 9676 ||| 1081 ||| 9677 ||| 9678 ||| 
2020 ||| reranking for efficient transformer-based answer selection. ||| 9679 ||| 9680 ||| 3374 ||| 
2020 ||| social media user geolocation via hybrid attention. ||| 9681 ||| 9682 ||| 9683 ||| 9684 ||| 1160 ||| 
2017 ||| a hierarchical multimodal attention-based neural network for image captioning. ||| 9685 ||| 9686 ||| 9687 ||| 9688 ||| 9689 ||| 6514 ||| 
2020 ||| contextual re-ranking with behavior aware transformers. ||| 8986 ||| 3322 ||| 1709 ||| 9690 ||| 1140 ||| 9691 ||| 
2018 ||| attention-based hierarchical neural query suggestion. ||| 1045 ||| 1046 ||| 1047 ||| 1048 ||| 
2017 ||| learning to diversify search results via subtopic attention. ||| 1376 ||| 1378 ||| 1375 ||| 3504 ||| 1377 ||| 9692 ||| 
2018 ||| mention recommendation for multimodal microblog with cross-attention memory network. ||| 9693 ||| 336 ||| 9694 ||| 9695 ||| 3273 ||| 
2018 ||| a contextual attention recurrent architecture for context-aware venue recommendation. ||| 9696 ||| 9697 ||| 9698 ||| 
2021 ||| does bert pay attention to cyberbullying? ||| 9699 ||| 9700 ||| 9701 ||| 9702 ||| 
2019 ||| video dialog via multi-grained convolutional self-attention context networks. ||| 9703 ||| 1306 ||| 9704 ||| 1754 ||| 7652 ||| 7654 ||| 
2021 ||| dual attention transfer in session-based recommendation with multi-dimensional integration. ||| 2230 ||| 8718 ||| 9705 ||| 
2021 ||| lighter and better: low-rank decomposed self-attention networks for next-item recommendation. ||| 9706 ||| 921 ||| 9707 ||| 3504 ||| 9574 ||| 1378 ||| 
2019 ||| interact and decide: medley of sub-attention networks for effective group recommendation. ||| 9708 ||| 9709 ||| 1398 ||| 1147 ||| 9710 ||| 3488 ||| 
2017 ||| toroidal vector-potential transformer. ||| 9711 ||| 
2020 ||| process outcome prediction: cnn vs. lstm (with attention). ||| 9712 ||| 9713 ||| 
2021 ||| combating informational denial-of-service (idos) attacks: modeling and mitigation of attentional human vulnerability. ||| 9714 ||| 9715 ||| 
2020 ||| moving target defense for robust monitoring of electric grid transformers in adversarial environments. ||| 9716 ||| 9717 ||| 9718 ||| 9719 ||| 
2019 ||| amas: attention model for attributed sequence classification. ||| 9720 ||| 1195 ||| 3528 ||| 
2018 ||| deep attention model for triage of emergency department patients. ||| 9721 ||| 9722 ||| 9723 ||| 9724 ||| 9725 ||| 9726 ||| 9727 ||| 
2019 ||| hierarchical attention networks for cyberbullying detection on the instagram social network. ||| 9728 ||| 9729 ||| 9730 ||| 9731 ||| 5791 ||| 
2021 ||| attention-based autoregression for accurate and efficient multivariate time series forecasting. ||| 9732 ||| 9733 ||| 
2021 ||| deep multi-instance contrastive learning with dual attention for anomaly precursor detection. ||| 9734 ||| 1156 ||| 9735 ||| 9736 ||| 9737 ||| 9738 ||| 9739 ||| 1159 ||| 586 ||| 
2019 ||| attentional heterogeneous graph neural network: application to program reidentification. ||| 9642 ||| 9740 ||| 9741 ||| 9742 ||| 9743 ||| 9735 ||| 9744 ||| 1159 ||| 1094 ||| 
2019 ||| geoattn: localization of social media messages via attentional memory network. ||| 9745 ||| 8862 ||| 9746 ||| 9747 ||| 1252 ||| 
2020 ||| attention-aware answers of the crowd. ||| 9748 ||| 9749 ||| 1224 ||| 9750 ||| 9751 ||| 
2021 ||| inter-series attention model for covid-19 forecasting. ||| 9265 ||| 9268 ||| 3801 ||| 
2019 ||| predicting multiple demographic attributes with task specific embedding transformation and attention network. ||| 9249 ||| 9752 ||| 9753 ||| 9250 ||| 
2021 ||| session-based recommendation with hypergraph attention networks. ||| 9754 ||| 9755 ||| 6452 ||| 9756 ||| 
2019 ||| bus travel speed prediction using attention network of heterogeneous correlation features. ||| 9757 ||| 9758 ||| 9759 ||| 9760 ||| 9761 ||| 
2020 ||| semi-supervised classification using attention-based regularization on coarse-resolution data. ||| 9762 ||| 9763 ||| 9764 ||| 9765 ||| 9766 ||| 
2020 ||| dual-attention recurrent networks for affine registration of neuroimaging data. ||| 9767 ||| 1195 ||| 5787 ||| 1193 ||| 9768 ||| 
2017 ||| improving distantly supervised relation extraction using word and entity based attention. ||| 9769 ||| 2506 ||| 9770 ||| 
2021 ||| reasoning with transformer-based models: deep learning, but shallow reasoning. ||| 9771 ||| 9772 ||| 9773 ||| 9774 ||| 
2021 ||| multi-branch recurrent attention convolutional neural network with evidence theory for fine-grained image classification. ||| 9775 ||| 5879 ||| 9776 ||| 9777 ||| 9778 ||| 
2020 ||| improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. ||| 9779 ||| 6559 ||| 9780 ||| 9781 ||| 
2020 ||| attention combination of sequence models for handwritten chinese text recognition. ||| 9782 ||| 9783 ||| 9784 ||| 
2020 ||| handwritten historical music recognition by sequence-to-sequence with attention mechanism. ||| 9785 ||| 9786 ||| 9787 ||| 7111 ||| 
2018 ||| memory-augmented attention model for scene text recognition. ||| 778 ||| 9783 ||| 779 ||| 
2020 ||| attention based writer independent verification. ||| 9788 ||| 9789 ||| 9790 ||| 9791 ||| 
2020 ||| attention augmented convolutional recurrent network for handwritten japanese text recognition. ||| 9792 ||| 9793 ||| 9794 ||| 
2020 ||| an attention based method for offline handwritten urdu text recognition. ||| 9795 ||| 9796 ||| 
2021 ||| exploring the digital identity divide: a call for attention to computing identity at hbcus. ||| 9797 ||| 9798 ||| 
2022 ||| communicating alternative grading schemes: how to shift students' attention to their learning from grades. ||| 9799 ||| 9800 ||| 9801 ||| 
2020 ||| look at me and grab this! materiality and the practices around negotiation of social attention with children on the autistic spectrum. ||| 9802 ||| 9803 ||| 9804 ||| 9805 ||| 9806 ||| 9807 ||| 9808 ||| 
2020 ||| using mouse movement heatmaps to visualize user attention to words. ||| 9809 ||| 
2020 ||| grounding dialogue history: strengths and weaknesses of pre-trained transformers. ||| 9810 ||| 9811 ||| 9812 ||| 
2018 ||| the cowriter robot: improving attention in a learning-by-teaching setup. ||| 9813 ||| 9814 ||| 9815 ||| 9816 ||| 9817 ||| 9818 ||| 
2017 ||| detecting attention breakdowns in robotic neurofeedback systems. ||| 9819 ||| 9815 ||| 9817 ||| 9818 ||| 
2019 ||| applying self-interaction attention for extracting drug-drug interactions. ||| 9820 ||| 9821 ||| 9822 ||| 9823 ||| 
2020 ||| explainable attentional neural recommendations for personalized social learning. ||| 9824 ||| 9825 ||| 9826 ||| 9827 ||| 9828 ||| 9829 ||| 
2018 ||| cross attention for selection-based question answering. ||| 9830 ||| 9650 ||| 9831 ||| 7025 ||| 
2020 ||| which turn do neural models exploit the most to solve guesswhat? diving into the dialogue history encoding in transformers and lstms. ||| 9810 ||| 9811 ||| 9812 ||| 
2018 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| 9832 ||| 9833 ||| 
2021 ||| evaluating transformer models for punctuation restoration in italian. ||| 9834 ||| 9835 ||| 9836 ||| 
2019 ||| inspecting unification of encoding and matching with transformer: a case study of machine reading comprehension. ||| 3498 ||| 3171 ||| 3174 ||| 3497 ||| 9413 ||| 9837 ||| 9838 ||| 3480 ||| 
2019 ||| question answering using hierarchical attention on top of bert features. ||| 9839 ||| 9840 ||| 9841 ||| 
2020 ||| paying attention to rhythm in hci: some thoughts on methods. ||| 9842 ||| 
2020 ||| jointly learning to align and transcribe using attention-based alignment and uncertainty-to-weigh losses. ||| 9843 ||| 9844 ||| 9845 ||| 9846 ||| 9847 ||| 9848 ||| 
2020 ||| dendrogram based clustering and separation of individual and simultaneously active incipient discharges in transformer insulation. ||| 9849 ||| 9850 ||| 9851 ||| 9852 ||| 
2020 ||| multi-target hybrid ctc-attentional decoder for joint phoneme-grapheme recognition. ||| 9843 ||| 9847 ||| 9848 ||| 
2020 ||| joint language identification of code-switching speech using attention-based e2e network. ||| 9853 ||| 9854 ||| 9855 ||| 9856 ||| 
2020 ||| improved feed forward attention mechanism in bidirectional recurrent neural networks for robust sequence classification. ||| 9857 ||| 9858 ||| 9859 ||| 9860 ||| 
2021 ||| hotspot detection via multi-task learning and transformer encoder. ||| 9861 ||| 9862 ||| 9863 ||| 1856 ||| 9864 ||| 9865 ||| 9866 ||| 
2021 ||| accelerating framework of transformer by hardware design and model compression co-optimization. ||| 9867 ||| 9868 ||| 9869 ||| 9870 ||| 9871 ||| 9872 ||| 9873 ||| 9874 ||| 
2020 ||| retransformer: reram-based processing-in-memory architecture for transformer acceleration. ||| 9875 ||| 9876 ||| 9877 ||| 7488 ||| 
2020 ||| hotspot detection via attention-based deep layout metric learning. ||| 9878 ||| 9879 ||| 2037 ||| 9880 ||| 1856 ||| 9864 ||| 9865 ||| 
2021 ||| bit-transformer: transforming bit-level sparsity into higher preformance in reram-based accelerator. ||| 9881 ||| 9882 ||| 9883 ||| 9884 ||| 9885 ||| 9886 ||| 2336 ||| 
2021 ||| autogtco: graph and tensor co-optimize for image recognition with transformers on gpu. ||| 1281 ||| 9887 ||| 433 ||| 9865 ||| 
2019 ||| attention neural network for user behavior modeling. ||| 9888 ||| 406 ||| 
2019 ||| relation extraction based on dual attention mechanism. ||| 9889 ||| 3265 ||| 9890 ||| 9891 ||| 
2021 ||| attention residual convolution neural network based on u-net for covid-19 lung infection segmentation. ||| 9892 ||| 9893 ||| 9894 ||| 
2020 ||| feature extraction by using attention mechanism in text classification. ||| 9895 ||| 7400 ||| 
2021 ||| ecg-based arrhythmia detection using attention-based convolutional neural network. ||| 9896 ||| 9897 ||| 
2021 ||| a transformer model-based approach to bearing fault diagnosis. ||| 9898 ||| 9899 ||| 9900 ||| 9901 ||| 9902 ||| 9903 ||| 
2020 ||| cp-net: channel attention and pixel attention network for single image dehazing. ||| 9904 ||| 406 ||| 3666 ||| 
2021 ||| channel context and dual-domain attention based u-net for skin lesion attributes segmentation. ||| 9905 ||| 9906 ||| 9907 ||| 9908 ||| 9909 ||| 9910 ||| 
2020 ||| deeper attention-based network for structured data. ||| 9911 ||| 9912 ||| 9913 ||| 9914 ||| 9915 ||| 
2019 ||| superimposed attention mechanism-based cnn network for reading comprehension and question answering. ||| 9916 ||| 9917 ||| 9918 ||| 622 ||| 
2020 ||| poi recommendations using self-attention based on side information. ||| 8133 ||| 406 ||| 1315 ||| 9919 ||| 
2019 ||| visual sentiment analysis with local object regions attention. ||| 9920 ||| 9921 ||| 9922 ||| 
2021 ||| combining global and local attention with positional encoding for video summarization. ||| 5867 ||| 9923 ||| 5870 ||| 5871 ||| 
2021 ||| adaptation and attention for neural video coding. ||| 9924 ||| 9925 ||| 9926 ||| 9927 ||| 1852 ||| 9928 ||| 8024 ||| 9929 ||| 6321 ||| 
2019 ||| versatile video coding of 360-degree video using frame-based fov and visual attention. ||| 1994 ||| 1995 ||| 7111 ||| 9930 ||| 9931 ||| 9932 ||| 9933 ||| 7033 ||| 
2017 ||| sustained attention function evaluation during cooking based on egocentric vision. ||| 9934 ||| 9935 ||| 9936 ||| 9937 ||| 9938 ||| 
2021 ||| sailboat detection based on automated search attention mechanism and deep learning models. ||| 9939 ||| 9940 ||| 2802 ||| 
2020 ||| salient motion features for visual attention models. ||| 9941 ||| 9942 ||| 9943 ||| 
2017 ||| active shift attention based object tracking system. ||| 9941 ||| 9943 ||| 9944 ||| 
2018 ||| a comparison of rgb and hsv colour spaces for visual attention models. ||| 9941 ||| 9943 ||| 9944 ||| 9942 ||| 
2018 ||| universal model for millimeter-wave integrated transformers. ||| 9945 ||| 9946 ||| 6666 ||| 
2018 ||| geometric form factors-based power transformers design. ||| 9947 ||| 9948 ||| 
2018 ||| visual attention toward socially rich context information for autism spectrum disorder (asd) and normal developing children: an eye tracking study. ||| 9949 ||| 9950 ||| 9951 ||| 9952 ||| 
2017 ||| prioritizing attention in analytic monitoring. ||| 9953 ||| 9954 ||| 9955 ||| 9956 ||| 
2018 ||| proposal of robot-interaction based intervention for joint-attention development. ||| 9957 ||| 9958 ||| 9959 ||| 9960 ||| 3419 ||| 
2018 ||| captioning with language-based attention. ||| 9961 ||| 9962 ||| 9963 ||| 9964 ||| 9965 ||| 
2021 ||| interpretable prediction of diabetes from tabular health screening records using an attentional neural network. ||| 9966 ||| 9967 ||| 9968 ||| 9969 ||| 
2020 ||| hanke: hierarchical attention networks for knowledge extraction in political science domain. ||| 9970 ||| 7162 ||| 9971 ||| 9972 ||| 9973 ||| 9974 ||| 
2021 ||| improving portuguese semantic role labeling with transformers and transfer learning. ||| 9975 ||| 9976 ||| 9977 ||| 9978 ||| 
2021 ||| 3m-transformers for event coding on organized crime domain. ||| 9970 ||| 7162 ||| 9971 ||| 9973 ||| 9972 ||| 9974 ||| 
2020 ||| sesamebert: attention for anywhere. ||| 9979 ||| 9980 ||| 
2021 ||| a neural network architecture with an attention-based layer for spatial prediction of fine particulate matter. ||| 9981 ||| 9982 ||| 852 ||| 9983 ||| 
2021 ||| resgcn: attention-based deep residual modeling for anomaly detection on attributed networks. ||| 9984 ||| 9985 ||| 9986 ||| 3923 ||| 
2019 ||| mars: memory attention-aware recommender system. ||| 1429 ||| 9987 ||| 9988 ||| 9989 ||| 9990 ||| 9991 ||| 9992 ||| 9993 ||| 1094 ||| 
2021 ||| constructing global coherence representations: identifying interpretability and coherences of transformer attention in time series data. ||| 3965 ||| 3966 ||| 
2019 ||| transformer fault diagnosis model based on iterative nearest neighbor interpolation and ensemble learning. ||| 9994 ||| 4807 ||| 885 ||| 9995 ||| 9996 ||| 9997 ||| 
2021 ||| human fall detection algorithm based on mixed attention mechanism. ||| 9998 ||| 9999 ||| 10000 ||| 10001 ||| 
2021 ||| a study on the cognitive efficiency of visual attention in pop-up ads based on eye-movement experiments. ||| 10002 ||| 
2020 ||| power load forecasting based on vmd and attention-lstm. ||| 10003 ||| 10004 ||| 7472 ||| 10005 ||| 10006 ||| 
2021 ||| sa-hardnest: a self-attention network for polyp segmentation. ||| 10007 ||| 
2021 ||| multi-head self-attention transformer for dogecoin price prediction. ||| 10008 ||| 10009 ||| 
2020 ||| a convolutional neural network with word-level attention for text classification. ||| 10010 ||| 
2021 ||| a multi-scale spatial and temporal attention network on dynamic connectivity to localize the eloquent cortex in brain tumor patients. ||| 10011 ||| 10012 ||| 10013 ||| 10014 ||| 10015 ||| 10016 ||| 10017 ||| 
2019 ||| ultrasound image representation learning by modeling sonographer visual attention. ||| 10018 ||| 10019 ||| 10020 ||| 10021 ||| 10022 ||| 10023 ||| 10024 ||| 
2019 ||| melanoma recognition via visual attention. ||| 10025 ||| 10026 ||| 7837 ||| 
2020 ||| bidirectional transformer language models for smart autocompletion of source code. ||| 10027 ||| 223 ||| 224 ||| 
2020 ||| an entity relation extraction algorithm based on bert(wwm-ext)-bigru-attention. ||| 10028 ||| 10029 ||| 10030 ||| 10031 ||| 5536 ||| 5527 ||| 
2019 ||| student attention evaluation system using machine learning for decision making. ||| 8432 ||| 227 ||| 
2021 ||| neural text categorization with transformers for learning portuguese as a second language. ||| 10032 ||| 1994 ||| 10033 ||| 2871 ||| 3565 ||| 10034 ||| 
2019 ||| hyper-parameter optimization of multi-attention recurrent neural network for battery state-of-charge forecasting. ||| 10035 ||| 10036 ||| 10037 ||| 10038 ||| 10039 ||| 
2021 ||| answering fill-in-the-blank questions in portuguese with transformer language models. ||| 10040 ||| 10041 ||| 
2019 ||| spatio-temporal attention deep recurrent q-network for pomdps. ||| 10042 ||| 10043 ||| 10044 ||| 
2018 ||| wireless linear variable differential transformer design and structural performance analysis. ||| 10045 ||| 10046 ||| 10047 ||| 10048 ||| 10049 ||| 
2019 ||| a study of primary school pupils' motivation, emotional intelligence and attentional control ability. ||| 10050 ||| 10051 ||| 10052 ||| 
2017 ||| saliency attention and sift keypoints combination for automatic target recognition on mstar dataset. ||| 10053 ||| 10054 ||| 10055 ||| 10056 ||| 
2018 ||| a visual attention model based on human visual cognition. ||| 10057 ||| 10058 ||| 10059 ||| 10060 ||| 
2019 ||| long short-term attention. ||| 2817 ||| 10061 ||| 10062 ||| 10063 ||| 4776 ||| 
2018 ||| attend to knowledge: memory-enhanced attention network for image captioning. ||| 10064 ||| 10065 ||| 10066 ||| 10067 ||| 2278 ||| 
2018 ||| dau-gan: unsupervised object transfiguration via deep attention unit. ||| 10068 ||| 10069 ||| 10070 ||| 673 ||| 10071 ||| 10072 ||| 
2019 ||| msa-net: multiscale spatial attention network for the classification of breast histology images. ||| 10073 ||| 10074 ||| 9199 ||| 10075 ||| 
2018 ||| a study of the role of attention in classifying covert and overt motor activities. ||| 10076 ||| 9643 ||| 10077 ||| 10078 ||| 10079 ||| 
2021 ||| optimizing person re-identification using generated attention masks. ||| 10080 ||| 1994 ||| 10081 ||| 10082 ||| 10083 ||| 
2019 ||| applying self-attention for stance classification. ||| 7032 ||| 7033 ||| 7034 ||| 
2020 ||| creating dialogue between a tutee agent and a tutor in a lecture video improves students' attention. ||| 10084 ||| 10085 ||| 10086 ||| 10087 ||| 10088 ||| 
2020 ||| multi-stage attention convolutional neural networks for hevc in-loop filtering. ||| 10089 ||| 10090 ||| 
2019 ||| sparse autoencoder with attention mechanism for speech emotion recognition. ||| 10091 ||| 10092 ||| 
2020 ||| event-based attention and tracking on neuromorphic hardware. ||| 10093 ||| 10094 ||| 10095 ||| 10096 ||| 
2017 ||| miniattention: attention management in minimal invasive surgery using an iot synchronization approach. ||| 10097 ||| 10098 ||| 10099 ||| 10100 ||| 10101 ||| 10102 ||| 10103 ||| 
2020 ||| malware family classification using lstm with attention. ||| 10104 ||| 507 ||| 10105 ||| 
2019 ||| on-line detection method of transformer measurement error based on bp neural network. ||| 10106 ||| 10107 ||| 10108 ||| 10109 ||| 10110 ||| 
2019 ||| a bi-sru neural network based on soft attention for hrrp target recognition. ||| 633 ||| 10111 ||| 
2019 ||| translate and summarize complaints of patient to electronic health record by bilstm-cnn attention model. ||| 10112 ||| 10113 ||| 10114 ||| 10115 ||| 10116 ||| 10117 ||| 
2019 ||| sentiment analysis of web text based on deep learning with a attention mechanism. ||| 5495 ||| 5496 ||| 10118 ||| 
2019 ||| experimental analysis on auditory attention saliency calculation models. ||| 10119 ||| 10120 ||| 10121 ||| 
2019 ||| summarizing articles into sentences by hierarchical attention model and rnn language model. ||| 10122 ||| 10123 ||| 10124 ||| 
2020 ||| single-image super-resolution based on a self-attention deep neural network. ||| 10125 ||| 10126 ||| 10127 ||| 
2020 ||| synchrony detection of epileptic eeg signals based on attention and pearson's correlation coefficient. ||| 10128 ||| 10129 ||| 10130 ||| 10131 ||| 
2020 ||| spatial-temporal graph attention model on traffic forecasting. ||| 10132 ||| 10133 ||| 10134 ||| 
2019 ||| ocdad: an overlapping community detecting algorithm using attention degree in directed ex-egonet. ||| 10135 ||| 5879 ||| 10136 ||| 3226 ||| 10137 ||| 10138 ||| 
2020 ||| attention-based bidirectional long short-term memory networks for relation classification using knowledge distillation from bert. ||| 10139 ||| 2760 ||| 
2021 ||| self-attention based text matching model with generative pre-training. ||| 10140 ||| 10141 ||| 10142 ||| 
2021 ||| st-tap: a traffic accident prediction framework based on spatio-temporal transformer. ||| 10143 ||| 10144 ||| 10145 ||| 10146 ||| 10147 ||| 10148 ||| 10149 ||| 2740 ||| 10150 ||| 
2019 ||| multi-factor based stock price prediction using hybrid neural networks with attention mechanism. ||| 399 ||| 181 ||| 10151 ||| 10152 ||| 10153 ||| 10154 ||| 
2021 ||| sfnet: stage spatial attention and feature waterfall fusion for mathematical document text line detection. ||| 748 ||| 8308 ||| 7826 ||| 5278 ||| 
2021 ||| a deep learning algorithm for groundwater level prediction based on spatial-temporal attention mechanism. ||| 8957 ||| 10155 ||| 10156 ||| 3474 ||| 
2021 ||| improving domestic nilm using an attention-enabled seq2point learning approach. ||| 875 ||| 10157 ||| 10158 ||| 1302 ||| 10159 ||| 
2021 ||| design and implementation of novel power electronic transformer for smart grid. ||| 10160 ||| 
2019 ||| academic performance estimation with attention-based graph convolutional networks. ||| 6797 ||| 359 ||| 
2021 ||| math question solving and mcq distractor generation with attentional gru networks. ||| 10161 ||| 10162 ||| 10163 ||| 6343 ||| 
2019 ||| a meta-learning augmented bidirectional transformer model for automatic short answer grading. ||| 10164 ||| 10165 ||| 10166 ||| 10167 ||| 10168 ||| 
2020 ||| legal language modeling with transformers. ||| 10169 ||| 10170 ||| 10171 ||| 10172 ||| 
2019 ||| patenttransformer: a framework for personalized patent claim generation. ||| 8059 ||| 
2020 ||| cross-domain generalization and knowledge transfer in transformers trained on legal data. ||| 10173 ||| 10174 ||| 10175 ||| 10176 ||| 
2019 ||| weakly supervised one-shot classification using recurrent neural networks with attention: application to claim acceptance detection. ||| 10177 ||| 7111 ||| 10178 ||| 2693 ||| 10179 ||| 10180 ||| 
2020 ||| transformers for classifying fourth amendment elements and factors tests. ||| 10181 ||| 10182 ||| 10183 ||| 
2021 ||| named entity recognition of wa cultural information resources based on attention mechanism. ||| 10184 ||| 6588 ||| 1224 ||| 10185 ||| 
2018 ||| a transfer learning based hierarchical attention neural network for sentiment classification. ||| 131 ||| 4715 ||| 133 ||| 10186 ||| 
2020 ||| adaptive and dynamic knowledge transfer in multi-task learning with attention networks. ||| 10187 ||| 517 ||| 
2021 ||| lightweight object tracking algorithm based on siamese network with efficient attention. ||| 10188 ||| 10189 ||| 
2021 ||| prediction of oil temperature for transformers using gated recurrent unit. ||| 10190 ||| 10191 ||| 10192 ||| 
2020 ||| attention u-net based adversarial architectures for chest x-ray lung segmentation information. ||| 10193 ||| 10194 ||| 2713 ||| 10195 ||| 10196 ||| 3369 ||| 10197 ||| 10198 ||| 
2020 ||| multimodal matching transformer for live commenting. ||| 10199 ||| 9837 ||| 10200 ||| 3174 ||| 10201 ||| 880 ||| 
2020 ||| human activity recognition from wearable sensor data using self-attention. ||| 10202 ||| 10203 ||| 7374 ||| 10204 ||| 7376 ||| 10205 ||| 10206 ||| 7375 ||| 
2020 ||| saliency detection with deformable convolution and feature attention. ||| 2855 ||| 5928 ||| 5929 ||| 5930 ||| 
2020 ||| transsketchnet: attention-based sketch recognition using transformers. ||| 10207 ||| 10208 ||| 10209 ||| 10210 ||| 
2020 ||| mid-weight image super-resolution with bypass connection attention network. ||| 10211 ||| 10212 ||| 
2020 ||| dual attention-based adversarial autoencoder for attributed network embedding. ||| 124 ||| 6191 ||| 6187 ||| 6188 ||| 6189 ||| 
2020 ||| feature importance estimation with self-attention networks. ||| 10213 ||| 10214 ||| 10215 ||| 10216 ||| 
2020 ||| forecaster: a graph transformer for forecasting spatial and time-dependent data. ||| 438 ||| 852 ||| 10217 ||| 
2020 ||| self-attention-based fully-inception networks for continuous sign language recognition. ||| 10218 ||| 10219 ||| 10220 ||| 10221 ||| 
2020 ||| transformer-based argument mining for healthcare applications. ||| 10222 ||| 10223 ||| 3953 ||| 
2020 ||| si-agan: spatial interpolation with attentional generative adversarial networks for environment monitoring. ||| 10224 ||| 10225 ||| 1460 ||| 5957 ||| 7776 ||| 
2020 ||| a lightweight recurrent attention network for real-time guidewire segmentation and tracking in interventional x-ray fluoroscopy. ||| 276 ||| 5186 ||| 5184 ||| 271 ||| 
2020 ||| group behavior recognition using attention- and graph-based neural networks. ||| 10226 ||| 10227 ||| 10228 ||| 5335 ||| 10229 ||| 10230 ||| 10231 ||| 
2020 ||| simplifying graph attention networks with source-target separation. ||| 10232 ||| 10233 ||| 10234 ||| 10235 ||| 10236 ||| 
2020 ||| span-based joint entity and relation extraction with transformer pre-training. ||| 10237 ||| 224 ||| 
2018 ||| towards crossmodal learning for smooth multimodal attention orientation. ||| 10238 ||| 10239 ||| 10240 ||| 10241 ||| 10242 ||| 10243 ||| 10244 ||| 6787 ||| 10245 ||| 
2018 ||| autonomous assistance control based on inattention of the driver when driving a truck tract. ||| 10246 ||| 2253 ||| 10247 ||| 
2020 ||| towards the design of a robot for supporting children's attention during long distance learning. ||| 10248 ||| 10249 ||| 10250 ||| 10251 ||| 10252 ||| 
2018 ||| an attention-aware model for human action recognition on tree-based skeleton sequences. ||| 10253 ||| 748 ||| 2519 ||| 
2018 ||| training autistic children on joint attention skills with a robot. ||| 10254 ||| 10255 ||| 10256 ||| 10257 ||| 10258 ||| 10259 ||| 
2021 ||| developing a robot's empathetic reactive response inspired by a bottom-up attention model. ||| 10260 ||| 10261 ||| 10262 ||| 10263 ||| 10264 ||| 10265 ||| 10266 ||| 10267 ||| 10268 ||| 
2020 ||| a fault ride through strategy of multi-ports dc transformer. ||| 3890 ||| 
2020 ||| a method of prediction for transformer malfunction based on oil chromatography. ||| 3890 ||| 6931 ||| 10269 ||| 10270 ||| 10271 ||| 10272 ||| 
2020 ||| a decoupling control method for multi-ports dc transformer. ||| 3890 ||| 
2020 ||| stable style transformer: delete and generate approach with encoder-decoder for text style transfer. ||| 10273 ||| 
2020 ||| memory attentive fusion: external language model integration for transformer-based sequence-to-sequence model. ||| 10274 ||| 4408 ||| 10275 ||| 4407 ||| 10276 ||| 10277 ||| 
2021 ||| biomedical data-to-text generation via fine-tuning transformers. ||| 10278 ||| 10279 ||| 10280 ||| 
2021 ||| controllable sentence simplification with a unified text-to-text transfer transformer. ||| 10281 ||| 10282 ||| 
2019 ||| can neural image captioning be controlled via forced attention? ||| 10283 ||| 10284 ||| 3580 ||| 
2021 ||| attention is indeed all you need: semantically attention-guided decoding for data-to-text nlg. ||| 10285 ||| 10286 ||| 
2020 ||| generating quantified referring expressions through attention-driven incremental perception. ||| 10287 ||| 
2020 ||| chart-to-text: generating natural language descriptions for charts by adapting the transformer model. ||| 10288 ||| 10289 ||| 
2020 ||| overview of the transformer-based models for nlp tasks. ||| 10290 ||| 10291 ||| 10292 ||| 10293 ||| 
2021 ||| evaluation of neural network transformer models for named-entity recognition on low-resourced languages. ||| 10294 ||| 
2017 ||| refocusing attention on unobserved attributes to reach consensus in decision making problems involving a heterogeneous group of experts. ||| 10295 ||| 10296 ||| 10297 ||| 
2021 ||| towards a question answering assistant for software development using a transformer-based language model. ||| 10298 ||| 10299 ||| 
2021 ||| exploring use of transformer based models on incident reports in aviation. ||| 10300 ||| 10301 ||| 10302 ||| 
2021 ||| multilingual machine translation systems at wat 2021: one-to-many and many-to-one transformer based nmt. ||| 10303 ||| 10304 ||| 10305 ||| 405 ||| 
2019 ||| nlprl at wat2019: transformer-based tamil - english indic task neural machine translation system. ||| 2209 ||| 10306 ||| 
2020 ||| transformer-based double-token bidirectional autoregressive decoding in neural machine translation. ||| 10307 ||| 4908 ||| 
2018 ||| iitp-mt at wat2018: transformer-based multilingual indic-english neural machine translation system. ||| 10308 ||| 10309 ||| 165 ||| 405 ||| 
2021 ||| a fuzzy logic proposal for diagnosis multiple incipient faults in a power transformer. ||| 10310 ||| 10311 ||| 10312 ||| 504 ||| 10313 ||| 10314 ||| 10315 ||| 10316 ||| 852 ||| 10317 ||| 10318 ||| 
2021 ||| multi-level graph attention network based unsupervised network alignment. ||| 10319 ||| 813 ||| 5964 ||| 10320 ||| 10321 ||| 10322 ||| 
2019 ||| a neural attention model for real-time network intrusion detection. ||| 10323 ||| 10324 ||| 10325 ||| 178 ||| 
2020 ||| a novel data-to-text generation model with transformer planning and a wasserstein auto-encoder. ||| 10326 ||| 10327 ||| 10328 ||| 
2020 ||| personality traits prediction based on users' digital footprints in social networks via attention rnn. ||| 10329 ||| 9695 ||| 10330 ||| 10331 ||| 10332 ||| 
2019 ||| service recommendation based on attentional factorization machine. ||| 2965 ||| 162 ||| 160 ||| 1569 ||| 10333 ||| 1592 ||| 
2019 ||| tuning multilingual transformers for language-specific named entity recognition. ||| 10334 ||| 10335 ||| 10336 ||| 10337 ||| 
2019 ||| multilingual named entity recognition using pretrained embeddings, attention mechanism and ncrf. ||| 10338 ||| 10339 ||| 
2021 ||| empirical based approaches to evaluating the residual life for oil-immersed transformers - a case study. ||| 10340 ||| 10341 ||| 10342 ||| 10343 ||| 10344 ||| 
2017 ||| transformer condition assessment for maintenance ranking: a comparison of three standards and different weighting techniques. ||| 10345 ||| 10341 ||| 10346 ||| 
2019 ||| improving the real power capacity of a furnace transformer through capacitance injection. ||| 10347 ||| 10343 ||| 10341 ||| 10348 ||| 
2017 ||| off-load tap-change transformer impact under high distributed pv generation. ||| 10349 ||| 10350 ||| 10351 ||| 
2019 ||| a review of detection and mitigation of negative dissipation factor in high voltage power transformers. ||| 10352 ||| 10353 ||| 10354 ||| 
2021 ||| on the impact of grid harmonics in transformers: a case study. ||| 10340 ||| 10341 ||| 10342 ||| 10343 ||| 10344 ||| 
2021 ||| residual life estimation of power transformer based on karl fischer and adaptive neuro-fuzzy interference system. ||| 10355 ||| 10356 ||| 
2019 ||| simulation studies on the grounding of clustered single phase transformers in rural electrification. ||| 10357 ||| 10358 ||| 10359 ||| 
2017 ||| the design of an integrated diplexer-power divider based on dual-band impedance transformers and a five-port power-divider. ||| 10360 ||| 10361 ||| 
2021 ||| method for increasing the accuracy of tracking the center of attention of the gaze. ||| 10362 ||| 10363 ||| 10364 ||| 
2021 ||| power grid stability prediction model based on bilstm with attention. ||| 2349 ||| 10365 ||| 1422 ||| 10366 ||| 10367 ||| 
2021 ||| batae-gru: attention-based aspect sentiment analysis model. ||| 4715 ||| 577 ||| 
2019 ||| dividi2: reinforcing divided attention in children with ad/hd through a mobile application. ||| 10368 ||| 10369 ||| 10370 ||| 10371 ||| 10372 ||| 3157 ||| 10373 ||| 10318 ||| 10374 ||| 
2019 ||| development of a system for the identification of adhd in children: attention monitor. ||| 10375 ||| 10376 ||| 8048 ||| 10377 ||| 3419 ||| 10378 ||| 
2017 ||| feasibility of detecting adhd patients' attention levels by classifying their eeg signals. ||| 10379 ||| 10380 ||| 10381 ||| 10382 ||| 
2017 ||| optoelectronic method for determining the aluminium involved in symptoms of attention deficit hyperactivity disorder children. ||| 10383 ||| 10384 ||| 10385 ||| 10386 ||| 10387 ||| 10388 ||| 10389 ||| 10390 ||| 10391 ||| 10392 ||| 
2017 ||| diagnosis of attention deficit hyperactivity disorder using deep belief network based on greedy approach. ||| 10393 ||| 10394 ||| 10395 ||| 
2021 ||| fast haptic terrain classification for legged robots using transformer. ||| 10396 ||| 10397 ||| 10398 ||| 10399 ||| 10400 ||| 
2019 ||| forward-backward visual saliency propagation in deep nns vs internal attentional mechanisms. ||| 2695 ||| 2701 ||| 2702 ||| 2703 ||| 2697 ||| 2698 ||| 2699 ||| 2700 ||| 
2019 ||| attention-guided deep convolutional neural networks for skin cancer classification. ||| 10401 ||| 10402 ||| 10403 ||| 
2019 ||| an optimized modeling method for transformer design. ||| 10404 ||| 10405 ||| 10406 ||| 
2019 ||| a 60 ghz single-to-differential lna using slow-wave cpw and transformer coupling in 28 nm cmos. ||| 10407 ||| 10408 ||| 10409 ||| 1785 ||| 
2019 ||| a35.2 dbm cmos rf power amplifier using an 8-way current-voltage combining transformer with harmonic control. ||| 10410 ||| 5331 ||| 10411 ||| 
2021 ||| facial expressions and body postures emotion recognition based on convolutional attention network. ||| 10412 ||| 10413 ||| 10414 ||| 10415 ||| 
2020 ||| effects of different representation styles on user attention in mooc. ||| 10416 ||| 
2019 ||| method name suggestion with hierarchical attention networks. ||| 10417 ||| 10418 ||| 10419 ||| 10420 ||| 10421 ||| 4620 ||| 
2020 ||| iitk at semeval-2020 task 10: transformers for emphasis selection. ||| 10422 ||| 10423 ||| 10424 ||| 4930 ||| 
2020 ||| iscas at semeval-2020 task 5: pre-trained transformers for counterfactual statement modeling. ||| 10425 ||| 10426 ||| 10427 ||| 10428 ||| 3656 ||| 
2018 ||| yuanfudao at semeval-2018 task 11: three-way attention and relational knowledge for commonsense machine comprehension. ||| 10429 ||| 4493 ||| 7700 ||| 10430 ||| 10431 ||| 
2020 ||| kafk at semeval-2020 task 12: checkpoint ensemble of transformers for hate speech classification. ||| 10432 ||| 10433 ||| 10434 ||| 10435 ||| 
2021 ||| nlp-iis@ut at semeval-2021 task 4: machine reading comprehension using the long document transformer. ||| 10436 ||| 10437 ||| 10438 ||| 10439 ||| 10440 ||| 
2019 ||| ubc-nlp at semeval-2019 task 4: hyperpartisan news detection with attention-based bi-lstms. ||| 10441 ||| 10442 ||| 3152 ||| 
2020 ||| ujnlp at semeval-2020 task 12: detecting offensive language using bidirectional transformers. ||| 10443 ||| 10444 ||| 10445 ||| 
2020 ||| uaics at semeval-2020 task 4: using a bidirectional transformer for task a. ||| 10446 ||| 10447 ||| 10448 ||| 
2020 ||| gorynych transformer at semeval-2020 task 6: multi-task learning for definition extraction. ||| 10449 ||| 2670 ||| 10450 ||| 10451 ||| 10452 ||| 
2020 ||| csecu_kde_ma at semeval-2020 task 8: a neural attention model for memotion analysis. ||| 4965 ||| 4964 ||| 4966 ||| 
2020 ||| xd at semeval-2020 task 12: ensemble approach to offensive language identification in social media using transformer encoders. ||| 10453 ||| 375 ||| 
2017 ||| datastories at semeval-2017 task 6: siamese lstm with attention for humorous text comparison. ||| 3728 ||| 10454 ||| 10455 ||| 
2021 ||| rg pa at semeval-2021 task 1: a contextual attention-based model with roberta for lexical complexity prediction. ||| 10456 ||| 10457 ||| 10458 ||| 10459 ||| 10460 ||| 10461 ||| 
2018 ||| tcs research at semeval-2018 task 1: learning robust representations using multi-attention architecture. ||| 10462 ||| 8945 ||| 
2018 ||| amobee at semeval-2018 task 1: gru neural network with a cnn attention mechanism for sentiment classification. ||| 10463 ||| 10464 ||| 
2017 ||| datastories at semeval-2017 task 4: deep lstm with attention for message-level and topic-based sentiment analysis. ||| 3728 ||| 10454 ||| 10455 ||| 
2018 ||| ynu deep at semeval-2018 task 12: a bilstm model with neural attention for argument reasoning comprehension. ||| 10465 ||| 10466 ||| 
2020 ||| hitachi at semeval-2020 task 3: exploring the representation spaces of transformers for human sense word similarity. ||| 10467 ||| 10468 ||| 10469 ||| 10470 ||| 
2018 ||| thu_ngn at semeval-2018 task 2: residual cnn-lstm network with attention for english emoji prediction. ||| 3754 ||| 3755 ||| 10471 ||| 10472 ||| 4792 ||| 2795 ||| 
2019 ||| nuli at semeval-2019 task 6: transfer learning for offensive language detection using bidirectional transformers. ||| 1975 ||| 10473 ||| 2982 ||| 
2019 ||| uc davis at semeval-2019 task 1: dag semantic parsing with attention-based decoder. ||| 10474 ||| 10475 ||| 
2019 ||| cn-hit-mi.t at semeval-2019 task 6: offensive language identification based on bilstm with double attention. ||| 10476 ||| 7679 ||| 880 ||| 
2021 ||| ta-mamc at semeval-2021 task 4: task-adaptive pretraining and multi-head attention for abstract meaning reading comprehension. ||| 875 ||| 3358 ||| 10477 ||| 
2018 ||| ynu-hpcc at semeval-2018 task 1: bilstm with attention based sentiment analysis for affect in tweets. ||| 3312 ||| 3313 ||| 3315 ||| 
2018 ||| emojiit at semeval-2018 task 2: an effective attention-based recurrent neural network model for emoji prediction with characters gated words. ||| 492 ||| 493 ||| 329 ||| 
2020 ||| hr@just team at semeval-2020 task 4: the impact of roberta transformer for evaluation common sense understanding. ||| 10478 ||| 10479 ||| 10480 ||| 
2020 ||| lt3 at semeval-2020 task 7: comparing feature-based and transformer-based approaches to detect funny headlines. ||| 10481 ||| 4936 ||| 10482 ||| 10483 ||| 2253 ||| 4937 ||| 
2020 ||| dothemath at semeval-2020 task 12 : deep neural networks with self attention for arabic offensive language detection. ||| 10484 ||| 10485 ||| 10486 ||| 10487 ||| 
2020 ||| upb at semeval-2020 task 9: identifying sentiment in code-mixed social media texts using transformers and multi-task learning. ||| 10488 ||| 10489 ||| 10490 ||| 10491 ||| 10492 ||| 
2017 ||| neobility at semeval-2017 task 1: an attention-based sentence similarity model. ||| 10493 ||| 10494 ||| 
2017 ||| adullam at semeval-2017 task 4: sentiment analyzer using lexicon integrated convolutional neural networks with attention. ||| 10495 ||| 10496 ||| 10497 ||| 
2019 ||| but-fit at semeval-2019 task 7: determining the rumour stance with pre-trained deep bidirectional transformers. ||| 10498 ||| 10499 ||| 10500 ||| 10501 ||| 
2021 ||| humorhunter at semeval-2021 task 7: humor and offense recognition with disentangled attention. ||| 10502 ||| 10503 ||| 10504 ||| 
2017 ||| lipn-iimas at semeval-2017 task 1: subword embeddings, attention recurrent neural networks and cross word alignment for semantic textual similarity. ||| 10505 ||| 10314 ||| 10506 ||| 10507 ||| 7442 ||| 
2021 ||| iiith at semeval-2021 task 7: leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining. ||| 10508 ||| 10509 ||| 10510 ||| 1185 ||| 
2020 ||| uor at semeval-2020 task 4: pre-trained sentence transformer models for commonsense validation and explanation. ||| 10511 ||| 10512 ||| 10513 ||| 10514 ||| 
2018 ||| ynu-hpcc at semeval-2018 task 2: multi-ensemble bi-gru model with attention mechanism for multilingual emoji prediction. ||| 8292 ||| 3313 ||| 3315 ||| 
2021 ||| csecu-dsg at semeval-2021 task 1: fusion of transformer models for lexical complexity prediction. ||| 10515 ||| 10516 ||| 4965 ||| 
2017 ||| oxford at semeval-2017 task 9: neural amr parsing with pointer-augmented attention. ||| 10517 ||| 10518 ||| 
2021 ||| roma at semeval-2021 task 7: a transformer-based approach for detecting and rating humor and offense. ||| 10519 ||| 10520 ||| 10521 ||| 10522 ||| 
2018 ||| ynu-hpcc at semeval-2018 task 11: using an attention-based cnn-lstm for machine comprehension using commonsense knowledge. ||| 10523 ||| 3313 ||| 3315 ||| 
2021 ||| 1213li at semeval-2021 task 6: detection of propaganda with multi-modal attention and pre-trained models. ||| 10524 ||| 315 ||| 10525 ||| 
2020 ||| uhh-lt at semeval-2020 task 12: fine-tuning of pre-trained transformer networks for offensive language detection. ||| 10526 ||| 10527 ||| 10528 ||| 
2019 ||| clp at semeval-2019 task 3: multi-encoder in hierarchical attention networks for contextual emotion detection. ||| 10529 ||| 10530 ||| 
2018 ||| ntua-slp at semeval-2018 task 2: predicting emojis using rnns with context-aware attention. ||| 3728 ||| 10531 ||| 10532 ||| 3721 ||| 10533 ||| 3729 ||| 
2020 ||| palomino-ochoa at semeval-2020 task 9: robust system based on transformer for code-mixed sentiment classification. ||| 10534 ||| 852 ||| 9983 ||| 
2018 ||| ynu_deep at semeval-2018 task 11: an ensemble of attention-based bilstm models for machine comprehension. ||| 10465 ||| 10466 ||| 
2021 ||| ecnu_ica_1 semeval-2021 task 4: leveraging knowledge-enhanced graph attention networks for reading comprehension of abstract meaning. ||| 10535 ||| 10536 ||| 6115 ||| 2424 ||| 10537 ||| 10061 ||| 329 ||| 
2019 ||| ynu nlp at semeval-2019 task 5: attention and capsule ensemble for identifying hate speech. ||| 379 ||| 10538 ||| 
2020 ||| wessa at semeval-2020 task 9: code-mixed sentiment analysis using transformers. ||| 10539 ||| 10540 ||| 10541 ||| 10542 ||| 
2018 ||| itnlp-arc at semeval-2018 task 12: argument reasoning comprehension with attention. ||| 10543 ||| 10544 ||| 10545 ||| 10546 ||| 
2021 ||| utnlp at semeval-2021 task 5: a comparative analysis of toxic span detection using attention-based, named entity recognition, and ensemble models. ||| 10547 ||| 10548 ||| 10549 ||| 10550 ||| 10439 ||| 
2021 ||| cs-um6p at semeval-2021 task 1: a deep learning model-based pre-trained transformer encoder for lexical complexity. ||| 10551 ||| 10552 ||| 10553 ||| 10554 ||| 10555 ||| 
2021 ||| cs60075_team2 at semeval-2021 task 1 : lexical complexity prediction using transformer-based language models pre-trained on various text corpora. ||| 10556 ||| 10557 ||| 10558 ||| 10559 ||| 
2019 ||| pkuse at semeval-2019 task 3: emotion detection with emotion-oriented neural attention network. ||| 10560 ||| 4864 ||| 4245 ||| 10561 ||| 
2020 ||| lmml at semeval-2020 task 7: siamese transformers for rating humor in edited news headlines. ||| 10562 ||| 
2020 ||| hitachi at semeval-2020 task 11: an empirical study of pre-trained transformer family for propaganda detection. ||| 10468 ||| 10467 ||| 10469 ||| 10470 ||| 
2018 ||| attnconvnet at semeval-2018 task 1: attention-based convolutional neural networks for multi-label emotion classification. ||| 10563 ||| 10564 ||| 10565 ||| 
2020 ||| transformers at semeval-2020 task 11: propaganda fragment detection using diversified bert architectures based ensemble learning. ||| 10566 ||| 10567 ||| 10568 ||| 
2021 ||| ynu-hpcc at semeval-2021 task 10: using a transformer-based source-free domain adaptation model for semantic processing. ||| 10569 ||| 3313 ||| 3315 ||| 
2021 ||| t at semeval-2021 task 5: integrating transformer and crf for toxic spans detection. ||| 10570 ||| 10571 ||| 880 ||| 
2019 ||| ynu-hpcc at semeval-2019 task 8: using a lstm-attention model for fact-checking in community forums. ||| 10572 ||| 3313 ||| 3315 ||| 
2019 ||| amrita school of engineering - cse at semeval-2019 task 6: manipulating attention with temporal convolutional neural network for offense identification and classification. ||| 10573 ||| 10574 ||| 
2020 ||| baksa at semeval-2020 task 9: bolstering cnn with self-attention for sentiment analysis of code mixed text. ||| 10575 ||| 10576 ||| 10577 ||| 4930 ||| 
2020 ||| ynu-oxz at semeval-2020 task 5: detecting counterfactuals based on ordered neurons lstm and hierarchical attention network. ||| 10578 ||| 10579 ||| 10580 ||| 
2021 ||| alpha at semeval-2021 task 6: transformer based propaganda classification. ||| 10581 ||| 10582 ||| 10583 ||| 10584 ||| 10585 ||| 673 ||| 3036 ||| 
2020 ||| brums at semeval-2020 task 12: transformer based multilingual offensive language identification in social media. ||| 3849 ||| 10586 ||| 
2020 ||| ks@lth at semeval-2020 task 12: fine-tuning multi- and monolingual transformer models for offensive language detection. ||| 10587 ||| 
2021 ||| liori at semeval-2021 task 8: ask transformer for measurements. ||| 10449 ||| 10451 ||| 2670 ||| 10588 ||| 
2018 ||| jiangnan at semeval-2018 task 11: deep neural network with attention method for machine comprehension task. ||| 10589 ||| 
2018 ||| ecnu at semeval-2018 task 12: an end-to-end attention-based neural network for the argument reasoning comprehension task. ||| 10590 ||| 349 ||| 350 ||| 
2020 ||| cnrl at semeval-2020 task 5: modelling causal reasoning in language with multi-head self-attention weights based counterfactual detection. ||| 10591 ||| 4053 ||| 
2021 ||| lecun at semeval-2021 task 6: detecting persuasion techniques in text using ensembled pretrained transformers and data augmentation. ||| 10592 ||| 10593 ||| 10594 ||| 
2018 ||| thu_ngn at semeval-2018 task 1: fine-grained tweet sentiment intensity analysis with attention cnn-lstm. ||| 3754 ||| 3755 ||| 4792 ||| 10472 ||| 10471 ||| 2795 ||| 
2018 ||| ynu-hpcc at semeval-2018 task 12: the argument reasoning comprehension task using a bi-directional lstm with attention model. ||| 10595 ||| 10596 ||| 3313 ||| 3315 ||| 
2017 ||| swissalps at semeval-2017 task 3: attention-based convolutional neural network for community question answering. ||| 10597 ||| 10598 ||| 
2019 ||| lijunyi at semeval-2019 task 9: an attention-based lstm and ensemble of different models for suggestion mining from online reviews and forums. ||| 10599 ||| 
2018 ||| tweety at semeval-2018 task 2: predicting emojis using hierarchical attention neural networks and support vector machine. ||| 10600 ||| 10601 ||| 10602 ||| 10603 ||| 10604 ||| 10605 ||| 10606 ||| 
2021 ||| mcl@iitk at semeval-2021 task 2: multilingual and cross-lingual word-in-context disambiguation using augmented data, signals, and transformers. ||| 10607 ||| 10608 ||| 10609 ||| 4930 ||| 
2018 ||| joker at semeval-2018 task 12: the argument reasoning comprehension with neural attention. ||| 10610 ||| 10611 ||| 10612 ||| 
2020 ||| justers at semeval-2020 task 4: evaluating transformer models against commonsense validation and explanation. ||| 10613 ||| 10614 ||| 893 ||| 
2021 ||| iapucp at semeval-2021 task 1: stacking fine-tuned transformers is almost all you need for lexical complexity prediction. ||| 10615 ||| 10616 ||| 
2019 ||| lastus/taln at semeval-2019 task 6: identification and categorization of offensive language in social media with attention-based bi-lstm model. ||| 10617 ||| 10618 ||| 10282 ||| 
2020 ||| ecnu at semeval-2020 task 7: assessing humor in edited news headlines using bilstm with attention. ||| 10619 ||| 10620 ||| 349 ||| 
2019 ||| thu_ngn at semeval-2019 task 3: dialog emotion classification using attentional lstm-cnn. ||| 10621 ||| 3756 ||| 3754 ||| 2795 ||| 
2021 ||| wlv-rit at semeval-2021 task 5: a neural transformer framework for detecting toxic spans. ||| 3849 ||| 10622 ||| 10623 ||| 10624 ||| 
2021 ||| xrjl-hkust at semeval-2021 task 4: wordnet-enhanced dual multi-head co-attention for reading comprehension of abstract meaning. ||| 10625 ||| 10626 ||| 10627 ||| 3890 ||| 10628 ||| 
2021 ||| ynu-hpcc at semeval-2021 task 5: using a transformer-based model with auxiliary information for toxic span detection. ||| 10629 ||| 3313 ||| 3315 ||| 
2021 ||| csecu-dsg at semeval-2021 task 7: detecting and rating humor and offense employing transformers. ||| 10630 ||| 10631 ||| 4965 ||| 
2020 ||| uld@nuig at semeval-2020 task 9: generative morphemes with an attention model for sentiment analysis in code-mixed text. ||| 10632 ||| 10633 ||| 10634 ||| 10635 ||| 10636 ||| 
2021 ||| uot-uwf-partai at semeval-2021 task 5: self attention based bi-gru with multi-embedding representation for toxicity highlighter. ||| 10637 ||| 10638 ||| 10639 ||| 10640 ||| 
2021 ||| aimh at semeval-2021 task 6: multimodal classification using an ensemble of transformer models. ||| 2689 ||| 2691 ||| 2692 ||| 2690 ||| 
2020 ||| problemconquero at semeval-2020 task 12: transformer and soft label-based approaches. ||| 10641 ||| 10642 ||| 10643 ||| 4930 ||| 
2021 ||| transwic at semeval-2021 task 2: transformer-based multilingual and cross-lingual word-in-context disambiguation. ||| 10586 ||| 3849 ||| 
2018 ||| blcu_nlp at semeval-2018 task 12: an ensemble model for argument reasoning based on hierarchical attention. ||| 10644 ||| 10645 ||| 6174 ||| 10646 ||| 3808 ||| 
2020 ||| pum at semeval-2020 task 12: aggregation of transformer-based models' features for offensive language recognition. ||| 10647 ||| 10648 ||| 10649 ||| 
2019 ||| caire_hkust at semeval-2019 task 3: hierarchical attention for dialogue emotion classification. ||| 10650 ||| 10651 ||| 10652 ||| 10653 ||| 2377 ||| 3676 ||| 10654 ||| 
2020 ||| boun-rex at clef-2020 chemu task 2: evaluating pretrained transformers for event extraction. ||| 10655 ||| 10656 ||| 55 ||| 56 ||| 10657 ||| 10658 ||| 57 ||| 58 ||| 59 ||| 
2019 ||| recurrent attention networks for medical concept prediction. ||| 10659 ||| 10660 ||| 10661 ||| 
2019 ||| a hierarchical attention network for bots and gender profiling. ||| 10662 ||| 10663 ||| 10490 ||| 10664 ||| 
2020 ||| 2020: if you say so: post-hoc fact-checking of claims using transformer-based models. ||| 10665 ||| 10666 ||| 10667 ||| 
2020 ||| transformers in semantic indexing of clinical codes. ||| 10668 ||| 10669 ||| 10670 ||| 10671 ||| 10672 ||| 
2020 ||| exploring argument retrieval with transformers. ||| 10673 ||| 10674 ||| 
2021 ||| attention-based cnn-gru model for automatic medical images captioning: imageclef 2021. ||| 10675 ||| 10676 ||| 10677 ||| 10678 ||| 
2021 ||| efficientnets and vision transformers for snake species identification using image and location information. ||| 10679 ||| 10680 ||| 
2021 ||| comparing transformer-based ner approaches for analysing textual medical diagnoses. ||| 10681 ||| 10682 ||| 10683 ||| 
2021 ||| lijie at imageclefmed vqa-med 2021: attention model-based efficient interaction between multimodality. ||| 4634 ||| 10579 ||| 
2020 ||| transformer-based open domain biomedical question answering at bioasq8 challenge. ||| 10684 ||| 10685 ||| 10686 ||| 
2020 ||| check_square at checkthat! 2020 claim detection in social media via fusion of transformer and syntactic features. ||| 10687 ||| 10688 ||| 10689 ||| 
2020 ||| early risk detection of self-harm and depression severity using bert-based transformers. ||| 10690 ||| 10691 ||| 7033 ||| 10692 ||| 10693 ||| 10694 ||| 
2021 ||| 2021: ensemble transformer model for fake news classification. ||| 10695 ||| 10696 ||| 
2020 ||| 2020: approaching fact checking from a sentence similarity perspective through the lens of transformers. ||| 10697 ||| 10698 ||| 10699 ||| 10700 ||| 
2020 ||| convolutional attention models with post-processing heuristics at clef ehealth 2020. ||| 10701 ||| 10702 ||| 
2021 ||| ranked list fusion and re-ranking with pre-trained transformers for arqmath lab. ||| 10703 ||| 1236 ||| 6343 ||| 
2017 ||| author profiling with word+character neural attention network. ||| 10704 ||| 10705 ||| 10706 ||| 10707 ||| 
2020 ||| harendrakv at vqa-med 2020: sequential vqa with attention for medical visual question answering. ||| 10708 ||| 10709 ||| 
2021 ||| identify hate speech spreaders on twitter using transformer embeddings features and automl classifiers. ||| 10710 ||| 
2020 ||| multilingual icd-10 code assignment with transformer architectures using mimic-iii discharge summaries. ||| 10711 ||| 10712 ||| 10680 ||| 
2017 ||| audio bird classification with inception-v4 extended with time and time-frequency attention mechanisms. ||| 10713 ||| 1890 ||| 10714 ||| 
2021 ||| bert-based transformers for early detection of mental health illnesses. ||| 10690 ||| 10691 ||| 7033 ||| 10692 ||| 10693 ||| 10694 ||| 
2021 ||| 2021: check-worthiness estimation as a regression problem on transformers. ||| 10715 ||| 
2021 ||| transformer-based language models for factoid question answering at bioasq9b. ||| 10716 ||| 10717 ||| 
2019 ||| event sentence detection task using attention model. ||| 10718 ||| 
2019 ||| extracting protests from news using lstm models with different attention mechanisms. ||| 10670 ||| 10672 ||| 10719 ||| 10720 ||| 10721 ||| 
2021 ||| 2021: check-worthiness estimation and fake news detection using transformer models. ||| 10722 ||| 10723 ||| 10724 ||| 10725 ||| 
2021 ||| 2021: fado-fake news detection and domain identification using transformers ensembling. ||| 10726 ||| 10727 ||| 10728 ||| 
2020 ||| 2020: identifying check-worthy tweets with transformer models. ||| 10729 ||| 10730 ||| 10604 ||| 7049 ||| 
2017 ||| attention-based medical caption generation with image modality classification and clinical concept mapping. ||| 10731 ||| 10732 ||| 10733 ||| 10734 ||| 10735 ||| 10736 ||| 10737 ||| 10738 ||| 10739 ||| 10740 ||| 10741 ||| 
2021 ||| profiling hate speech spreaders on twitter: transformers and mixed pooling. ||| 10742 ||| 3419 ||| 10743 ||| 10744 ||| 3882 ||| 10745 ||| 
2017 ||| author profiling with bidirectional rnns using attention with grus. ||| 10746 ||| 10747 ||| 10748 ||| 10598 ||| 
2021 ||| stft transformers for bird song recognition. ||| 10749 ||| 
2020 ||| 2020: tweet check worthiness using transformers, convolutional neural networks and support vector machines. ||| 10750 ||| 10751 ||| 10752 ||| 10753 ||| 
2021 ||| 2021: integration of transformers in misinformation detection and topic classification. ||| 10742 ||| 3419 ||| 10743 ||| 10744 ||| 3882 ||| 10745 ||| 
2018 ||| a parallel hierarchical attention network for style change detection: notebook for pan at clef 2018. ||| 10754 ||| 10755 ||| 
2020 ||| lifelog moment retrieval with self-attention based joint embedding model. ||| 10756 ||| 10757 ||| 4033 ||| 
2017 ||| deep networks for human visual attention: a hybrid model using foveal vision. ||| 5411 ||| 5410 ||| 5412 ||| 852 ||| 5413 ||| 
2018 ||| integrating global attention for pairwise text comparison. ||| 10758 ||| 5391 ||| 10759 ||| 10760 ||| 10761 ||| 
2021 ||| ordering sentences and paragraphs with pre-trained encoder-decoder transformers and pointer ensembles. ||| 59 ||| 10762 ||| 10763 ||| 10764 ||| 
2019 ||| quantifying the effects of temperature and noise on attention-level using eda and eeg sensors. ||| 10765 ||| 10766 ||| 10767 ||| 10768 ||| 10769 ||| 
2021 ||| a wideband cmos power amplifier with integrated digital linearizer and tunable transformer. ||| 10770 ||| 10771 ||| 10772 ||| 10773 ||| 10774 ||| 10775 ||| 
2021 ||| a broadband tri-coil based transformer design for mm-wave cascode amplifiers. ||| 10776 ||| 10777 ||| 10778 ||| 
2021 ||| a differential rf front-end cmos transformer matching for ambient rf energy harvesting systems. ||| 10779 ||| 10780 ||| 10781 ||| 10782 ||| 
2019 ||| convolutional neural network and attention mechanism for bone age prediction. ||| 10783 ||| 10784 ||| 10785 ||| 
2022 ||| ami: attention based adaptative feedback with augmented reality to improve takeover performances in highly automated vehicles. ||| 10786 ||| 10787 ||| 10788 ||| 
2019 ||| action units: directing user attention in 360-degree video based vr. ||| 10789 ||| 10790 ||| 10791 ||| 
2020 ||| using multiple perspective projections to guide visual attention in glyph-based data visualisations in vr. ||| 10792 ||| 10793 ||| 10794 ||| 10795 ||| 
2019 ||| impact of gamified interaction with virtual nature on sustained attention and self-reported restoration - a pilot study. ||| 10796 ||| 10797 ||| 10798 ||| 
2019 ||| esn-ner: entity storage network using attention mechanism for chinese ner. ||| 10799 ||| 10800 ||| 10801 ||| 10802 ||| 5110 ||| 5445 ||| 
2020 ||| extending equational monadic reasoning with monad transformers. ||| 10803 ||| 10804 ||| 
2018 ||| web service discovery based on information gain theory and bilstm with attention mechanism. ||| 10805 ||| 162 ||| 1569 ||| 10806 ||| 1592 ||| 
2019 ||| relation extraction toward patent domain based on keyword strategy and attention+bilstm model (short paper). ||| 10807 ||| 10808 ||| 10809 ||| 10810 ||| 10811 ||| 
2019 ||| a next location predicting approach based on a recurrent neural network and self-attention. ||| 5069 ||| 10812 ||| 10813 ||| 5068 ||| 
2019 ||| web services classification with topical attention based bi-lstm. ||| 2965 ||| 162 ||| 1569 ||| 160 ||| 1592 ||| 10814 ||| 
2019 ||| attention-based bilinear joint learning framework for entity linking. ||| 10815 ||| 10816 ||| 10817 ||| 10818 ||| 10819 ||| 10820 ||| 
2021 ||| question classification using universal sentence encoder and deep contextualized transformer. ||| 10821 ||| 10822 ||| 10823 ||| 
2021 ||| operationalizing a national digital library: the case for a norwegian transformer model. ||| 10824 ||| 10825 ||| 10826 ||| 10827 ||| 
2022 ||| a predicate transformer for choreographies - computing preconditions in choreographic programming. ||| 10828 ||| 10829 ||| 
2020 ||| an answer sorting method combining multiple neural networks and attentional mechanisms. ||| 10830 ||| 390 ||| 10831 ||| 10832 ||| 10833 ||| 
2020 ||| link prediction of attention flow network based on maximum entropy model. ||| 2969 ||| 10834 ||| 2968 ||| 817 ||| 2971 ||| 10835 ||| 10836 ||| 2972 ||| 
2020 ||| graph representation learning using attention network. ||| 10837 ||| 10838 ||| 10839 ||| 10840 ||| 10841 ||| 10842 ||| 
2019 ||| attentional transformer networks for target-oriented sentiment classification. ||| 10843 ||| 5110 ||| 10844 ||| 
2021 ||| driver attention assistance by pedestrian/cyclist distance estimation from a single rgb image: a cnn-based semantic segmentation approach. ||| 10845 ||| 10846 ||| 10847 ||| 10848 ||| 10849 ||| 
2019 ||| optimal design and experimental validation of a novel line-frequency zig-zag transformer employed in a unified ac-dc system. ||| 10850 ||| 10851 ||| 10852 ||| 
2020 ||| averaged models of a six-phase, dual-interleaved dc-dc buck-boost converter with interphase transformers. ||| 10853 ||| 10854 ||| 10855 ||| 10856 ||| 10857 ||| 
2018 ||| analysis of 132kv/33kv 15mva power transformer dissolved gas using transport-x kelman kit through duval's triangle and roger's ratio prediction. ||| 10858 ||| 10859 ||| 10860 ||| 10861 ||| 10862 ||| 
2021 ||| flexible control structure of a smart transformer for universal operation. ||| 10863 ||| 10864 ||| 10865 ||| 10866 ||| 10867 ||| 10868 ||| 
2017 ||| comparative assessment of three-phase transformerless grid-connected solar inverters. ||| 10869 ||| 10870 ||| 10871 ||| 10872 ||| 
2018 ||| research on a novel hybrid transformer for smart distribution network. ||| 1235 ||| 10873 ||| 10874 ||| 10875 ||| 10876 ||| 10877 ||| 10878 ||| 
2018 ||| development of a partial discharge testing system for potential transformers. ||| 10879 ||| 10880 ||| 10881 ||| 
2019 ||| solid state transformer with integrated input stage. ||| 10882 ||| 2698 ||| 10868 ||| 
2018 ||| design of a 4-phase intercell transformer converter for a space charge measuring system. ||| 10883 ||| 10884 ||| 10885 ||| 10886 ||| 10887 ||| 
2018 ||| fault diagnosis of transformer based on kpca and elman neural network. ||| 10888 ||| 10889 ||| 10890 ||| 10891 ||| 10892 ||| 
2017 ||| power control of a multi-cell solid-state transformer with extended functions. ||| 10893 ||| 10894 ||| 10895 ||| 10896 ||| 10897 ||| 10898 ||| 10899 ||| 3882 ||| 
2021 ||| modulation strategy and control of modular cascade h-bridge converters as input-side of a multi-port smart transformer. ||| 10900 ||| 10901 ||| 10902 ||| 10903 ||| 10904 ||| 10905 ||| 10906 ||| 10907 ||| 10908 ||| 10909 ||| 10910 ||| 10911 ||| 8048 ||| 
2018 ||| high-temperature coplanar transformer. ||| 10912 ||| 10913 ||| 10914 ||| 10915 ||| 
2017 ||| computation of rectifier transformers employed in railway networks. ||| 10916 ||| 10917 ||| 
2019 ||| a doubly-grounded transformer-less single-phase pv inverter with boost capability. ||| 10918 ||| 10919 ||| 6379 ||| 10920 ||| 10921 ||| 5463 ||| 10922 ||| 10923 ||| 
2018 ||| testing of non-toroidal shape primary pass-through current transformer for electrical machine monitoring and protection. ||| 10924 ||| 10925 ||| 10926 ||| 2698 ||| 10927 ||| 
2019 ||| impacts of linear controllers for power interfaces in ideal transformer model based power hardware-in-the-loop. ||| 10928 ||| 10929 ||| 10930 ||| 
2018 ||| model predictive control of packed u cells based transformerless single-phase dynamic voltage restorer. ||| 10931 ||| 10932 ||| 10933 ||| 10934 ||| 
2021 ||| vocabulary-constrained question generation with rare word masking and dual attention. ||| 10935 ||| 
2018 ||| a neural attention based approach for clickstream mining. ||| 10936 ||| 3330 ||| 
2020 ||| unitor @ dankmeme: combining convolutional models and transformer-based architectures for accurate meme management. ||| 10937 ||| 10938 ||| 10939 ||| 10940 ||| 
2020 ||| unitor @ sardistance2020: combining transformer-based architectures and transfer learning for robust stance detection. ||| 10941 ||| 10942 ||| 10943 ||| 10940 ||| 10939 ||| 
2020 ||| ssn nlp @ sardistance : stance detection from italian tweets using rnn and transformers (short paper). ||| 10669 ||| 10944 ||| 10945 ||| 
2018 ||| hate speech detection using attention-based lstm. ||| 10946 ||| 10947 ||| 3882 ||| 10948 ||| 10949 ||| 10950 ||| 10522 ||| 
2020 ||| fontana-unipi @ haspeede2: ensemble of transformers for the hate speech task at evalita (short paper). ||| 10951 ||| 7025 ||| 
2018 ||| bidirectional attentional lstm for aspect based sentiment analysis on italian. ||| 10952 ||| 
2021 ||| anomaly detection in unstructured logs using attention-based bi-lstm network. ||| 10953 ||| 10954 ||| 10955 ||| 10956 ||| 247 ||| 7015 ||| 
2021 ||| a novel fabric defect detection network based on attention mechanism and multi-task fusion. ||| 10957 ||| 10958 ||| 10959 ||| 10960 ||| 10961 ||| 10962 ||| 
2021 ||| speech separation based on dptnet with sparse attention. ||| 10963 ||| 10964 ||| 10965 ||| 10966 ||| 
2021 ||| a safety-helmet detection algorithm based on attention mechanism. ||| 10967 ||| 10968 ||| 
2021 ||| dsamt: dual-source aligned multimodal transformers for textcaps. ||| 10969 ||| 10970 ||| 10971 ||| 
2021 ||| ancient chinese recognition method based on attention mechanism. ||| 10972 ||| 985 ||| 10973 ||| 10974 ||| 
2021 ||| combined coverage, attention and pointer networks for improving slot filling in spoken language understanding. ||| 10975 ||| 10976 ||| 5189 ||| 10977 ||| 10978 ||| 
2021 ||| multi-scene safety helmet detection with multi-scale spatial attention feature. ||| 10979 ||| 10980 ||| 10981 ||| 10982 ||| 
2021 ||| zero-shot voice cloning using variational embedding with attention mechanism. ||| 10983 ||| 10984 ||| 10985 ||| 
2021 ||| attention-guided soft ranking loss for resource-constrained head pose estimation. ||| 10986 ||| 10987 ||| 683 ||| 10988 ||| 
2020 ||| current political news translation model based on attention mechanism. ||| 10989 ||| 10990 ||| 7259 ||| 10991 ||| 10992 ||| 10993 ||| 10994 ||| 
2018 ||| visual attention mechanisms revisited. ||| 10995 ||| 10996 ||| 10997 ||| 10998 ||| 
2018 ||| attentional mechanism based on a microphone array for embedded devices and a single camera. ||| 10999 ||| 852 ||| 11000 ||| 11001 ||| 11002 ||| 11003 ||| 11004 ||| 11005 ||| 
2020 ||| integrating openface 2.0 toolkit for driver attention estimation in challenging accidental scenarios. ||| 11006 ||| 11007 ||| 11008 ||| 11009 ||| 11010 ||| 11011 ||| 11012 ||| 11013 ||| 3882 ||| 11014 ||| 11015 ||| 11016 ||| 
2020 ||| ftrans: energy-efficient acceleration of transformers using fpga. ||| 9874 ||| 11017 ||| 11018 ||| 11019 ||| 9747 ||| 11020 ||| 11021 ||| 11022 ||| 11023 ||| 11024 ||| 
2019 ||| cnn-based camera-less user attention detection for smartphone power management. ||| 11025 ||| 11026 ||| 11027 ||| 11028 ||| 
2017 ||| why the mapping process in ontology integration deserves attention. ||| 11029 ||| 11030 ||| 11031 ||| 11032 ||| 11033 ||| 
2017 ||| exploring alternative security warning dialog for attracting user attention: evaluation of "kawaii" effect and its additional stimulus combination. ||| 11034 ||| 11035 ||| 
2021 ||| analysis of graphsum's attention weights to improve the explainability of multi-document summarization. ||| 11036 ||| 11037 ||| 11038 ||| 11039 ||| 11040 ||| 11041 ||| 11042 ||| 
2019 ||| low supply voltage control oscillator with transformer feedback for photoplethysmography. ||| 1467 ||| 1468 ||| 11043 ||| 
2021 ||| transformer empowered csi feedback for massive mimo systems. ||| 6781 ||| 11044 ||| 11045 ||| 
2021 ||| activity recognition based on fr-cnn and attention-based lstm network. ||| 11046 ||| 11047 ||| 11048 ||| 11049 ||| 
2020 ||| requirements for monitoring inattention of the responsible human in an autonomous vehicle: the recall and precision tradeoff. ||| 11050 ||| 11051 ||| 7891 ||| 
2018 ||| inattention-management middleware for human-in-the-loop multi-display applications. ||| 11052 ||| 11053 ||| 
2017 ||| attention and engagement-awareness in the wild: a large-scale study with adaptive notifications. ||| 11054 ||| 11055 ||| 11056 ||| 11057 ||| 11058 ||| 
2021 ||| deep triplet networks with attention for sensor-based human activity recognition. ||| 11059 ||| 11060 ||| 11061 ||| 
2021 ||| tap: a transformer based activity prediction exploiting temporal relations in collaborative tasks. ||| 11062 ||| 11063 ||| 
2019 ||| outcome-driven clustering of acute coronary syndrome patients using multi-task neural network with attention. ||| 11064 ||| 11065 ||| 11066 ||| 11067 ||| 11068 ||| 11069 ||| 11070 ||| 595 ||| 11071 ||| 11072 ||| 11073 ||| 
2021 ||| a joint self-attention model for aspect category detection in e-commerce reviews. ||| 11074 ||| 11075 ||| 11076 ||| 
2021 ||| catching audiences' attention through narrative sensory cues on digital distribution platforms. ||| 11077 ||| 11078 ||| 11079 ||| 11080 ||| 11081 ||| 
2018 ||| using virtual reality for museum exhibitions: the effects of attention and engagement for national palace museum. ||| 11082 ||| 11083 ||| 333 ||| 11084 ||| 11085 ||| 
2019 ||| artificial intelligence-powered cognitive training applications for children with attention deficit hyperactivity disorder: a brief review. ||| 11086 ||| 11087 ||| 11088 ||| 
2020 ||| relation aware attention model for uncertainty detection in text. ||| 1102 ||| 11089 ||| 1103 ||| 
2019 ||| episodic memory network with self-attention for emotion detection. ||| 351 ||| 11090 ||| 189 ||| 
2020 ||| code2text: dual attention syntax annotation networks for structure-aware code translation. ||| 1090 ||| 6004 ||| 11091 ||| 5787 ||| 1195 ||| 10768 ||| 1094 ||| 1093 ||| 
2019 ||| dmmam: deep multi-source multi-task attention model for intensive care unit diagnosis. ||| 11092 ||| 11093 ||| 11094 ||| 11095 ||| 11096 ||| 11097 ||| 
2021 ||| dcan: deep co-attention network by modeling user preference and news lifecycle for news recommendation. ||| 11098 ||| 2764 ||| 11099 ||| 11100 ||| 
2020 ||| aspect category sentiment analysis with self-attention fusion networks. ||| 11101 ||| 5496 ||| 11102 ||| 11103 ||| 11104 ||| 
2021 ||| neural adversarial review summarization with hierarchical personalized attention. ||| 9599 ||| 4166 ||| 696 ||| 697 ||| 
2021 ||| sans: setwise attentional neural similarity method for few-shot recommendation. ||| 2778 ||| 2961 ||| 11105 ||| 989 ||| 11106 ||| 2962 ||| 
2019 ||| agree: attention-based tour group recommendation with multi-modal data. ||| 11107 ||| 11108 ||| 1578 ||| 1579 ||| 
2019 ||| sparsemaac: sparse attention for multi-agent reinforcement learning. ||| 11109 ||| 11110 ||| 11111 ||| 
2021 ||| an attention-based approach to rule learning in large knowledge graphs. ||| 11112 ||| 11113 ||| 7436 ||| 11114 ||| 259 ||| 
2019 ||| adaptive attention-aware gated recurrent unit for sequential recommendation. ||| 11115 ||| 659 ||| 9588 ||| 658 ||| 654 ||| 660 ||| 6475 ||| 11116 ||| 
2019 ||| neural review rating prediction with hierarchical attentions and latent factors. ||| 4165 ||| 4166 ||| 8198 ||| 3755 ||| 9599 ||| 696 ||| 9574 ||| 
2019 ||| combining meta-graph and attention for recommendation over heterogenous information network. ||| 11117 ||| 11118 ||| 8177 ||| 8176 ||| 
2018 ||| exploiting context graph attention for poi recommendation in location-based social networks. ||| 11119 ||| 1860 ||| 
2021 ||| relation-aware alignment attention network for multi-view multi-label learning. ||| 340 ||| 11120 ||| 11121 ||| 459 ||| 
2019 ||| attention and convolution enhanced memory network for sequential recommendation. ||| 6307 ||| 659 ||| 9588 ||| 658 ||| 11122 ||| 660 ||| 6475 ||| 11116 ||| 
2021 ||| multi-scale gated inpainting network with patch-wise spacial attention. ||| 5291 ||| 11123 ||| 5288 ||| 5290 ||| 11124 ||| 11125 ||| 11126 ||| 5286 ||| 5289 ||| 
2021 ||| dfilan: domain-based feature interactions learning via attention networks for ctr prediction. ||| 11127 ||| 11128 ||| 11129 ||| 11130 ||| 1862 ||| 
2019 ||| attention-based abnormal-aware fusion network for radiology report generation. ||| 1271 ||| 1090 ||| 1094 ||| 11131 ||| 11132 ||| 1093 ||| 
2019 ||| attention-based neural tag recommendation. ||| 11133 ||| 11134 ||| 11135 ||| 11136 ||| 
2021 ||| graph attention networks for new product sales forecasting in e-commerce. ||| 11137 ||| 11138 ||| 11139 ||| 11140 ||| 11141 ||| 11142 ||| 11143 ||| 
2020 ||| detection of wrong disease information using knowledge-based embedding and attention. ||| 9998 ||| 5819 ||| 9695 ||| 4175 ||| 11144 ||| 
2021 ||| attention-based multimodal entity linking with high-quality images. ||| 254 ||| 654 ||| 11145 ||| 
2020 ||| modeling periodic pattern with self-attention network for sequential recommendation. ||| 9547 ||| 659 ||| 9588 ||| 6475 ||| 658 ||| 660 ||| 
2020 ||| attention with long-term interval-based gated recurrent units for modeling sequential user behaviors. ||| 885 ||| 9002 ||| 11146 ||| 11147 ||| 11148 ||| 11149 ||| 9003 ||| 1129 ||| 
2021 ||| zh-ner: chinese named entity recognition with adversarial multi-task learning and self-attentions. ||| 6489 ||| 11150 ||| 11151 ||| 11152 ||| 11153 ||| 11154 ||| 
2018 ||| improving short text modeling by two-level attention networks for sentiment classification. ||| 11155 ||| 11156 ||| 11157 ||| 3477 ||| 
2021 ||| scsg attention: a self-centered star graph with attention for pedestrian trajectory prediction. ||| 9645 ||| 11158 ||| 11159 ||| 11160 ||| 11161 ||| 11162 ||| 11163 ||| 
2021 ||| graph attention collaborative similarity embedding for recommender system. ||| 11164 ||| 11165 ||| 1234 ||| 11166 ||| 11167 ||| 1239 ||| 
2021 ||| entity resolution with hybrid attention-based networks. ||| 11168 ||| 4072 ||| 
2020 ||| hybrid attention based neural architecture for text semantics similarity measurement. ||| 11169 ||| 7826 ||| 11170 ||| 
2020 ||| hierarchical variational attention for sequential recommendation. ||| 764 ||| 659 ||| 9588 ||| 6475 ||| 654 ||| 660 ||| 
2020 ||| mutual self attention recommendation with gated fusion between ratings and reviews. ||| 8197 ||| 4166 ||| 882 ||| 9599 ||| 11171 ||| 697 ||| 
2019 ||| mdal: multi-task dual attention lstm model for semi-supervised network embedding. ||| 11172 ||| 1312 ||| 1311 ||| 8426 ||| 4067 ||| 
2020 ||| sast-gnn: a self-attention based spatio-temporal graph neural network for traffic prediction. ||| 11173 ||| 1090 ||| 1093 ||| 
2021 ||| an attention-based bi-gru for route planning and order dispatch of bus-booking platform. ||| 11174 ||| 11175 ||| 11176 ||| 1578 ||| 2008 ||| 1579 ||| 
2019 ||| hospitalization behavior prediction based on attention and time adjustment factors in bidirectional lstm. ||| 7087 ||| 1559 ||| 1558 ||| 6044 ||| 1557 ||| 
2019 ||| modeling more globally: a hierarchical attention network via multi-task learning for aspect-based sentiment analysis. ||| 4216 ||| 11177 ||| 4217 ||| 459 ||| 
2021 ||| multi-head attention with hint mechanisms for joint extraction of entity and relation. ||| 11178 ||| 600 ||| 11179 ||| 11180 ||| 
2021 ||| spatial-temporal attention network for temporal knowledge graph completion. ||| 11181 ||| 11182 ||| 11183 ||| 155 ||| 
2020 ||| question answering over knowledge base with symmetric complementary attention. ||| 11184 ||| 11185 ||| 
2019 ||| align reviews with topics in attention network for rating prediction. ||| 11186 ||| 5244 ||| 11187 ||| 
2020 ||| modeling long-term and short-term interests with parallel attentions for session-based recommendation. ||| 11188 ||| 11189 ||| 11190 ||| 
2018 ||| core loss evaluation of high-frequency transformers in high-power dc-dc converters. ||| 11191 ||| 
2018 ||| transformerless dc/dc converter based on the autotransformer concept for the interconnection of hvdc grids. ||| 11192 ||| 11193 ||| 11194 ||| 11195 ||| 11196 ||| 
2020 ||| a medium frequency transformer design tool with methodologies adapted to various structures. ||| 11197 ||| 11198 ||| 11199 ||| 11200 ||| 11201 ||| 
2021 ||| medium-frequency transformers for fast rise time pwm voltages: modelling and design considerations. ||| 11202 ||| 11203 ||| 11204 ||| 11205 ||| 
2017 ||| individual trait oriented scanpath prediction for visual attention analysis. ||| 11206 ||| 9061 ||| 
2021 ||| attention toward neighbors: a context aware framework for high resolution image segmentation. ||| 11207 ||| 7376 ||| 7375 ||| 11208 ||| 
2019 ||| temporal regularized spatial attention for video-based person re-identification. ||| 11209 ||| 7662 ||| 
2021 ||| two-stream hybrid attention network for multimodal classification. ||| 11210 ||| 11211 ||| 11212 ||| 11213 ||| 5272 ||| 
2020 ||| dcm: a dense-attention context module for semantic segmentation. ||| 11214 ||| 11215 ||| 10107 ||| 5894 ||| 11216 ||| 11217 ||| 7247 ||| 
2018 ||| learning semantics-guided visual attention for few-shot image classification. ||| 11218 ||| 6328 ||| 
2019 ||| cascade attention: multiple feature based learning for image captioning. ||| 11219 ||| 11220 ||| 11221 ||| 
2018 ||| infrared and visible image registration using transformer adversarial network. ||| 11222 ||| 11223 ||| 3226 ||| 11224 ||| 11225 ||| 
2017 ||| cham: action recognition using convolutional hierarchical attention model. ||| 13 ||| 11226 ||| 11227 ||| 11228 ||| 
2018 ||| action recognition: first-and second-order 3d feature in bi-directional attention network. ||| 11229 ||| 11230 ||| 11231 ||| 
2019 ||| cross attention network for semantic segmentation. ||| 11232 ||| 11233 ||| 
2021 ||| single image super-resolution via global-context attention networks. ||| 11234 ||| 11235 ||| 11236 ||| 11237 ||| 11238 ||| 
2020 ||| graph pattern loss based diversified attention network for cross-modal retrieval. ||| 11239 ||| 6465 ||| 6850 ||| 
2021 ||| spectral-spatial fused attention network for hyperspectral image classification. ||| 11240 ||| 11241 ||| 
2021 ||| sparse and structured visual attention. ||| 11242 ||| 9210 ||| 11243 ||| 3369 ||| 3370 ||| 
2019 ||| frame attention networks for facial expression recognition in videos. ||| 11244 ||| 8769 ||| 333 ||| 2149 ||| 
2019 ||| feature-attentioned object detection in remote sensing imagery. ||| 11245 ||| 11246 ||| 2816 ||| 2182 ||| 2814 ||| 1825 ||| 
2021 ||| transformer for image quality assessment. ||| 11247 ||| 11248 ||| 
2021 ||| attention-based multi-task learning for fine-grained image classification. ||| 11249 ||| 3906 ||| 11250 ||| 11251 ||| 
2020 ||| automatic measurement of fetal cavum septum pellucidum from ultrasound images using deep attention network. ||| 11252 ||| 11253 ||| 11254 ||| 378 ||| 
2019 ||| a ranking based attention approach for visual tracking. ||| 11255 ||| 11256 ||| 11257 ||| 
2021 ||| analysis of the novel transformer module combination for scene text recognition. ||| 11258 ||| 11259 ||| 11260 ||| 11261 ||| 11262 ||| 11263 ||| 
2021 ||| improving the robustness of convolutional neural networks via sketch attention. ||| 11264 ||| 11265 ||| 1132 ||| 11266 ||| 
2019 ||| spatial temporal attentional glimpse for human activity classification in video. ||| 11267 ||| 11268 ||| 11269 ||| 2233 ||| 11270 ||| 
2020 ||| convolutional attention model for restaurant recommendation with multi-view visual features. ||| 11271 ||| 11272 ||| 11273 ||| 
2019 ||| dual reverse attention networks for person re-identification. ||| 11274 ||| 11275 ||| 11276 ||| 11277 ||| 
2017 ||| multi-layer linear model for top-down modulation of visual attention in natural egocentric vision. ||| 11278 ||| 2652 ||| 11279 ||| 2653 ||| 11280 ||| 1872 ||| 
2020 ||| attention boosted deep networks for video classification. ||| 11247 ||| 11248 ||| 
2019 ||| bira-net: bilinear attention net for diabetic retinopathy grading. ||| 11281 ||| 11282 ||| 11283 ||| 11284 ||| 11285 ||| 3036 ||| 5858 ||| 
2017 ||| an integrated approach to visual attention modelling using spatial-temporal saliency and objectness. ||| 11286 ||| 11287 ||| 7406 ||| 
2019 ||| cascade attention network for person re-identification. ||| 11288 ||| 11289 ||| 11290 ||| 11291 ||| 2077 ||| 2080 ||| 
2020 ||| wavelet channel attention module with a fusion network for single image deraining. ||| 11292 ||| 7457 ||| 6328 ||| 
2020 ||| attention-enhanced and more balanced r-cnn for object detection. ||| 11293 ||| 11294 ||| 11295 ||| 
2019 ||| segmenting hepatic lesions using residual attention u-net with an adaptive weighted dice loss. ||| 11296 ||| 11297 ||| 11298 ||| 1820 ||| 11299 ||| 
2021 ||| light-field view synthesis using a convolutional block attention module. ||| 11300 ||| 11301 ||| 11302 ||| 11303 ||| 7111 ||| 11304 ||| 11305 ||| 
2017 ||| person re-identification using visual attention. ||| 11306 ||| 11307 ||| 11308 ||| 326 ||| 7421 ||| 
2021 ||| a heterogeneous face recognition via part adaptive and relation attention module. ||| 11309 ||| 11310 ||| 7228 ||| 
2021 ||| mask guided attention for fine-grained patchy image classification. ||| 1224 ||| 11311 ||| 11312 ||| 
2021 ||| explore connection pattern and attention mechanism for lightweightimage super-resolution. ||| 11313 ||| 11314 ||| 
2020 ||| gsanet: semantic segmentation with global and selective attention. ||| 4437 ||| 11315 ||| 11316 ||| 11317 ||| 
2018 ||| image captioning with word level attention. ||| 11318 ||| 7656 ||| 11319 ||| 
2021 ||| semantic role aware correlation transformer for text to video retrieval. ||| 11320 ||| 11321 ||| 11322 ||| 7904 ||| 
2019 ||| learning hierarchical self-attention for video summarization. ||| 6327 ||| 2376 ||| 11323 ||| 11324 ||| 6328 ||| 
2021 ||| semantic-compensated and attention-guided network for scene text detection. ||| 11325 ||| 11326 ||| 11327 ||| 
2018 ||| hierarchical relational attention for video question answering. ||| 11328 ||| 11329 ||| 11330 ||| 11331 ||| 
2020 ||| infrared target detection using intensity saliency and self-attention. ||| 11332 ||| 11333 ||| 11334 ||| 11335 ||| 11336 ||| 11337 ||| 
2017 ||| visual comfort assessment of stereoscopic images using deep visual and disparity features based on human attention. ||| 11338 ||| 11339 ||| 11340 ||| 
2021 ||| self attention based semantic segmentation on a natural disaster dataset. ||| 6770 ||| 6771 ||| 
2021 ||| attention based network for no-reference ugc video quality assessment. ||| 11341 ||| 11342 ||| 4217 ||| 11343 ||| 5904 ||| 6516 ||| 
2019 ||| dressing for attention: outfit based fashion popularity prediction. ||| 1817 ||| 11344 ||| 11345 ||| 11346 ||| 1819 ||| 1820 ||| 
2021 ||| co-saliency detection via unified hierarchical graph neural network with geometric attention. ||| 11347 ||| 11348 ||| 11349 ||| 11350 ||| 3072 ||| 
2018 ||| topic-guided attention for image captioning. ||| 2958 ||| 6350 ||| 6351 ||| 
2020 ||| feature aggregation attention network for single image dehazing. ||| 11351 ||| 11352 ||| 11353 ||| 11354 ||| 
2021 ||| rar-u-net: a residual encoder to attention decoder by residual connections framework for spine segmentation under noisy labels. ||| 11355 ||| 11356 ||| 11357 ||| 
2021 ||| deep features fusion with mutual attention transformer for skin lesion diagnosis. ||| 11358 ||| 11359 ||| 
2020 ||| knowledge-guided and hyper-attention aware joint network for benign-malignant lung nodule classification. ||| 11360 ||| 11361 ||| 11362 ||| 11363 ||| 11364 ||| 832 ||| 
2021 ||| pan: personalized attention network for outfit recommendation. ||| 11365 ||| 11366 ||| 
2021 ||| inter-modality fusion based attention for zero-shot cross-modal retrieval. ||| 11367 ||| 5845 ||| 3279 ||| 
2020 ||| deep regression forest with soft-attention for head pose estimation. ||| 11368 ||| 11369 ||| 11370 ||| 11371 ||| 11372 ||| 
2017 ||| deep partial person re-identification via attention model. ||| 11230 ||| 11231 ||| 
2021 ||| learning regional attention over multi-resolution deep convolutional features for trademark retrieval. ||| 11373 ||| 11374 ||| 11330 ||| 11331 ||| 
2021 ||| resolution-invariant person reid based on feature transformation and self-weighted attention. ||| 11375 ||| 11376 ||| 11377 ||| 11378 ||| 
2019 ||| a convolutional neural network for pavement surface crack segmentation using residual connections and attention gating. ||| 11379 ||| 11380 ||| 11381 ||| 11382 ||| 11383 ||| 11384 ||| 
2020 ||| triple attention for robust video crowd counting. ||| 11385 ||| 11386 ||| 11387 ||| 11388 ||| 11389 ||| 
2020 ||| point set attention network for semantic segmentation. ||| 11390 ||| 2058 ||| 11391 ||| 11392 ||| 2080 ||| 
2017 ||| convolutional feature pyramid fusion via attention network. ||| 9286 ||| 8797 ||| 1515 ||| 
2018 ||| bottom-up attention guidance for recurrent image recognition. ||| 1852 ||| 1854 ||| 1971 ||| 6321 ||| 11393 ||| 
2020 ||| semi-supervised multi-spectral land cover classification with multi-attention and adaptive kernel. ||| 11394 ||| 1007 ||| 
2020 ||| attention unet++: a nested attention-aware u-net for liver ct image segmentation. ||| 399 ||| 11395 ||| 5110 ||| 9999 ||| 11396 ||| 11397 ||| 11398 ||| 
2021 ||| blind image deblurring based on dual attention network and 2d blur kernel estimation. ||| 11399 ||| 11400 ||| 11401 ||| 
2020 ||| fusion target attention mask generation network for video segmentation. ||| 11402 ||| 11403 ||| 1856 ||| 8177 ||| 11404 ||| 11405 ||| 
2021 ||| robust monocular 3d lane detection with dual attention. ||| 11406 ||| 11407 ||| 11408 ||| 9624 ||| 
2020 ||| gapnet: generic-attribute-pose network for fine-grained visual categorization using multi-attribute attention module. ||| 11409 ||| 11410 ||| 11411 ||| 11412 ||| 
2020 ||| detection features as attention (defat): a keypoint-free approach to amur tiger re-identification. ||| 11413 ||| 11414 ||| 700 ||| 577 ||| 545 ||| 
2021 ||| sagan: skip-attention gan for anomaly detection. ||| 11415 ||| 11416 ||| 1349 ||| 11417 ||| 11418 ||| 
2020 ||| context-aware hierarchical feature attention network for multi-scale object detection. ||| 11419 ||| 6259 ||| 11420 ||| 
2021 ||| attention-based local region aggregation network for hierarchical point cloud learning. ||| 11421 ||| 11422 ||| 11423 ||| 11424 ||| 
2021 ||| filter pruning via softmax attention. ||| 7336 ||| 11425 ||| 7338 ||| 
2017 ||| person re-identification with coarse-to-fine visual attention. ||| 11426 ||| 11427 ||| 11428 ||| 11429 ||| 
2021 ||| attention-driven tile splitting method for improved efficiency of omnidirectional versatile video coding. ||| 11430 ||| 7111 ||| 9930 ||| 9931 ||| 9932 ||| 9933 ||| 7033 ||| 
2021 ||| joint co-attention and co-reconstruction representation learning for one-shot object detection. ||| 11431 ||| 11432 ||| 11433 ||| 3131 ||| 
2020 ||| self-attention dense depth estimation network for unrectified video sequences. ||| 11434 ||| 1002 ||| 11435 ||| 
2021 ||| transformer and node-compressed dnn based dual-path system for manipulated face detection. ||| 11436 ||| 11256 ||| 11437 ||| 
2019 ||| local to global with multi-scale attention network for person re-identification. ||| 11438 ||| 11439 ||| 11440 ||| 11441 ||| 
2018 ||| temporal attention network for action proposal. ||| 11442 ||| 2331 ||| 11443 ||| 
2019 ||| text recognition in images based on transformer with hierarchical attention. ||| 11444 ||| 11445 ||| 11446 ||| 472 ||| 
2020 ||| feature comparison based channel attention for fine-grained visual classification. ||| 11447 ||| 11448 ||| 875 ||| 
2019 ||| weakly-supervised learning for attention-guided skull fracture classification in computed tomography imaging. ||| 11449 ||| 11450 ||| 11451 ||| 11452 ||| 6328 ||| 
2019 ||| learning target-oriented dual attention for robust rgb-t tracking. ||| 11453 ||| 11454 ||| 5957 ||| 11455 ||| 5755 ||| 
2021 ||| cascade attention blend residual network for single image super-resolution. ||| 11456 ||| 515 ||| 11457 ||| 512 ||| 513 ||| 514 ||| 
2021 ||| attention-based partial face recognition. ||| 5691 ||| 5692 ||| 11458 ||| 5694 ||| 11459 ||| 5695 ||| 
2021 ||| captioning transformer with scene graph guiding. ||| 11460 ||| 7830 ||| 7676 ||| 4634 ||| 
2020 ||| spatio-temporal slowfast self-attention network for action recognition. ||| 11461 ||| 11462 ||| 2178 ||| 
2021 ||| two-pathway transformer network for video action recognition. ||| 2632 ||| 11463 ||| 11464 ||| 11465 ||| 11466 ||| 
2019 ||| a dual-attention dilated residual network for liver lesion classification and localization on ct images. ||| 3889 ||| 1236 ||| 1550 ||| 11467 ||| 1551 ||| 11468 ||| 1526 ||| 6422 ||| 1528 ||| 11469 ||| 
2021 ||| correlation-aware attention branch network using multi-modal data for deterioration level estimation of infrastructures. ||| 7611 ||| 7608 ||| 7609 ||| 7610 ||| 
2020 ||| multimodal attention-mechanism for temporal emotion recognition. ||| 11060 ||| 11470 ||| 11061 ||| 
2021 ||| localization uncertainty-based attention for object detection. ||| 11471 ||| 11472 ||| 11473 ||| 2178 ||| 
2019 ||| spatial constraint multiple granularity attention network for clothesretrieval. ||| 11474 ||| 11475 ||| 1132 ||| 11476 ||| 
2017 ||| improving human action recognitionby temporal attention. ||| 11477 ||| 11478 ||| 11479 ||| 
2020 ||| chroma intra prediction with attention-based cnn architectures. ||| 11480 ||| 11481 ||| 11482 ||| 11483 ||| 1675 ||| 11484 ||| 
2019 ||| efficient motion deblurring with feature transformation and spatial attention. ||| 11485 ||| 11486 ||| 
2018 ||| attention-enhanced sensorimotor object recognition. ||| 11487 ||| 11488 ||| 11489 ||| 11490 ||| 
2021 ||| linked attention-based dynamic graph convolution module for point cloud classification. ||| 11491 ||| 11492 ||| 11493 ||| 3433 ||| 11494 ||| 11495 ||| 
2020 ||| single image super-resolution via residual neuron attention networks. ||| 11496 ||| 11497 ||| 11498 ||| 11499 ||| 
2021 ||| vision and text transformer for predicting answerability on visual question answering. ||| 7512 ||| 7513 ||| 11500 ||| 
2021 ||| afdn: attention-based feedback dehazing network for uav remote sensing image haze removal. ||| 11501 ||| 11502 ||| 11503 ||| 
2021 ||| hyperspectral classification using cooperative spatial-spectral attention network with tensor low-rank reconstruction. ||| 11504 ||| 11505 ||| 11506 ||| 3034 ||| 11507 ||| 11508 ||| 
2019 ||| compression artifact removal with stacked multi-context channel-wise attention network. ||| 11509 ||| 1832 ||| 602 ||| 
2021 ||| deep gaussian denoiser epistemic uncertainty and decoupled dual-attention fusion. ||| 11510 ||| 11511 ||| 11512 ||| 8798 ||| 8799 ||| 
2020 ||| sea-net: squeeze-and-excitation attention net for diabetic retinopathy grading. ||| 11281 ||| 11513 ||| 11514 ||| 3488 ||| 
2021 ||| advanced deep network with attention and genetic-driven reinforcement learning layer for an efficient cancer treatment outcome prediction. ||| 10847 ||| 11515 ||| 11516 ||| 11517 ||| 
2019 ||| estimation of emotion labels via tensor-based spatiotemporal visual attention analysis. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| pocformer: a lightweight transformer architecture for detection of covid-19 using point of care ultrasound. ||| 11518 ||| 11519 ||| 11520 ||| 
2020 ||| bae-net: a band attention aware ensemble network for hyperspectral object tracking. ||| 11521 ||| 11522 ||| 1086 ||| 4550 ||| 836 ||| 11523 ||| 
2019 ||| acnet: attention based network to exploit complementary features for rgbd semantic segmentation. ||| 11524 ||| 7857 ||| 11525 ||| 5906 ||| 
2021 ||| an attention fusion network for event-based vehicle object detection. ||| 11526 ||| 11527 ||| 11528 ||| 621 ||| 
2021 ||| cyclic diffeomorphic transformer nets for contour alignment. ||| 11529 ||| 11530 ||| 11531 ||| 
2021 ||| sparse spatial attention network for semantic segmentation. ||| 11232 ||| 11233 ||| 
2020 ||| jitter-robust video retargeting with kalman filter and attention saliency fusion network. ||| 11532 ||| 11533 ||| 11534 ||| 
2021 ||| action segmentation on representations of skeleton sequences using transformer networks. ||| 11535 ||| 11536 ||| 11537 ||| 11538 ||| 
2021 ||| attention-based self-supervised learning monocular depth estimation with edge refinement. ||| 11539 ||| 11540 ||| 11541 ||| 11542 ||| 
2021 ||| get to the point: content classification of animated graphics interchange formats with key-frame attention. ||| 11543 ||| 3906 ||| 8754 ||| 11544 ||| 11545 ||| 
2021 ||| enhanced back projection network based stereo image super-resolution considering parallax attention. ||| 11546 ||| 11326 ||| 
2020 ||| complex spatial-temporal attention aggregation for video person re-identification. ||| 11547 ||| 8217 ||| 11548 ||| 11549 ||| 
2021 ||| self-supervised bodymap-to-appearance co-attention for partial person re-identification. ||| 11550 ||| 6328 ||| 
2020 ||| fake video detection with certainty-based attention network. ||| 11551 ||| 11552 ||| 11553 ||| 11554 ||| 11340 ||| 
2021 ||| deepfake video detection using 3d-attentional inception convolutional neural network. ||| 11555 ||| 2304 ||| 11556 ||| 2302 ||| 2305 ||| 
2021 ||| attend, correct and focus: a bidirectional correct attention network for image-text matching. ||| 1305 ||| 11557 ||| 11558 ||| 11559 ||| 2519 ||| 
2017 ||| audio-visual attention: eye-tracking dataset and analysis toolbox. ||| 11560 ||| 11561 ||| 11562 ||| 11563 ||| 11564 ||| 11565 ||| 11566 ||| 7350 ||| 
2020 ||| a convlstm-combined hierarchical attention network for saliency detection. ||| 3279 ||| 11567 ||| 
2019 ||| 3d deep attention network for survival prediction from magnetic resonance images in glioblastoma. ||| 11568 ||| 11569 ||| 11570 ||| 11571 ||| 11572 ||| 11573 ||| 
2021 ||| salypath: a deep-based architecture for visual attention prediction. ||| 11574 ||| 11575 ||| 11576 ||| 11577 ||| 
2018 ||| dense chained attention network for scene text recognition. ||| 11578 ||| 11579 ||| 2077 ||| 2074 ||| 2080 ||| 
2021 ||| plnl-3dssd: part-aware 3d single stage detector using local and non-local attention. ||| 11580 ||| 11581 ||| 11582 ||| 2259 ||| 11583 ||| 
2019 ||| a coarse-to-fine framework for learned color enhancement with non-local attention. ||| 11584 ||| 11585 ||| 11586 ||| 
2019 ||| insect classification using squeeze-and-excitation and attention modules - a benchmark study. ||| 11587 ||| 11588 ||| 1086 ||| 
2019 ||| optical flow estimation using spatial-channel combinational attention-based pyramid networks. ||| 11589 ||| 11590 ||| 11591 ||| 6927 ||| 11592 ||| 
2019 ||| group re-identification with hybrid attention model and residual distance. ||| 6468 ||| 1007 ||| 6469 ||| 6516 ||| 
2018 ||| general recurrent attention model for jointly multiple object recognition and weakly supervised localization. ||| 11593 ||| 11594 ||| 11595 ||| 11596 ||| 
2020 ||| improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images. ||| 7798 ||| 7800 ||| 7801 ||| 
2020 ||| learning discriminative part features through attentions for effective and scalable person search. ||| 11597 ||| 11598 ||| 11599 ||| 11600 ||| 11601 ||| 
2019 ||| attentional road safety networks. ||| 11602 ||| 11603 ||| 11604 ||| 11605 ||| 
2021 ||| zero-shot object detection with transformers. ||| 11606 ||| 672 ||| 
2021 ||| cmdm-vac: improving a perceptual quality metric for 3d graphics by integrating a visual attention complexity measure. ||| 11607 ||| 11608 ||| 11609 ||| 11610 ||| 11611 ||| 
2019 ||| deeply supervised multimodal attentional translation embeddings for visual relationship detection. ||| 7927 ||| 7928 ||| 7929 ||| 11612 ||| 7930 ||| 
2020 ||| video summarization with anchors and multi-head attention. ||| 11613 ||| 11614 ||| 11615 ||| 6363 ||| 
2021 ||| temporal memory attention for video semantic segmentation. ||| 1371 ||| 6390 ||| 2058 ||| 
2019 ||| showcasing deeply supervised multimodal attentional translation embeddings: a demo for visual relationship detection. ||| 7927 ||| 7928 ||| 7929 ||| 11612 ||| 7930 ||| 
2020 ||| few-shot learning with attention-weighted graph convolutional networks for hyperspectral image classification. ||| 11616 ||| 11508 ||| 11617 ||| 11618 ||| 
2020 ||| attention selective network for face synthesis and pose-invariant face recognition. ||| 11619 ||| 11620 ||| 7286 ||| 11621 ||| 
2019 ||| monad transformers and modular algebraic effects: what binds them together. ||| 11622 ||| 11623 ||| 2101 ||| 11624 ||| 11625 ||| 
2018 ||| effective attention modeling for aspect-level sentiment classification. ||| 3193 ||| 3194 ||| 3195 ||| 3196 ||| 
2020 ||| porous lattice transformer encoder for chinese ner. ||| 756 ||| 3326 ||| 759 ||| 3289 ||| 11626 ||| 379 ||| 
2020 ||| interactively-propagative attention learning for implicit discourse relation recognition. ||| 11627 ||| 11628 ||| 6781 ||| 967 ||| 3088 ||| 1254 ||| 
2020 ||| mixup-transformer: dynamic data augmentation for nlp tasks. ||| 11629 ||| 11630 ||| 11631 ||| 11632 ||| 1094 ||| 9988 ||| 
2020 ||| attention transfer network for aspect-level sentiment classification. ||| 11633 ||| 4784 ||| 4790 ||| 
2020 ||| bayes-enhanced lifelong attention networks for sentiment classification. ||| 1371 ||| 11634 ||| 11635 ||| 3072 ||| 3666 ||| 979 ||| 
2020 ||| dual-decoder transformer for joint automatic speech recognition and multilingual speech translation. ||| 11636 ||| 11637 ||| 9319 ||| 9318 ||| 3512 ||| 3510 ||| 
2020 ||| seeing both the forest and the trees: multi-head attention for joint classification on different compositional levels. ||| 11638 ||| 11639 ||| 
2020 ||| interactive key-value memory-augmented attention for image paragraph captioning. ||| 11640 ||| 11641 ||| 11642 ||| 3199 ||| 1081 ||| 11643 ||| 
2020 ||| hierarchical bi-directional self-attention networks for paper review rating recommendation. ||| 11644 ||| 9407 ||| 11630 ||| 215 ||| 9988 ||| 1094 ||| 
2020 ||| flight of the pegasus? comparing transformers on few-shot and zero-shot multi-document abstractive summarization. ||| 11645 ||| 11646 ||| 11647 ||| 
2020 ||| graph enhanced dual attention network for document-level relation extraction. ||| 1717 ||| 4245 ||| 11648 ||| 5981 ||| 11649 ||| 3285 ||| 
2018 ||| dynamic feature selection with attention in incremental parsing. ||| 11650 ||| 11651 ||| 4803 ||| 
2020 ||| attention word embedding. ||| 11652 ||| 10166 ||| 10168 ||| 
2020 ||| language model transformers as evaluators for open-domain dialogues. ||| 11653 ||| 3581 ||| 11654 ||| 
2020 ||| complaint identification in social media with transformer networks. ||| 11655 ||| 3736 ||| 
2020 ||| rethinking the value of transformer components. ||| 11656 ||| 3041 ||| 
2020 ||| catching attention with automatic pull quote selection. ||| 11657 ||| 11658 ||| 
2020 ||| syntax-aware graph attention network for aspect-level sentiment classification. ||| 11659 ||| 6288 ||| 11660 ||| 11661 ||| 11662 ||| 
2018 ||| lstms with attention for aggression detection. ||| 11663 ||| 11664 ||| 11665 ||| 11666 ||| 
2020 ||| how relevant are selectional preferences for transformer-based language models? ||| 11667 ||| 11668 ||| 11669 ||| 
2020 ||| forcereader: a bert-based interactive machine reading comprehension model with attention separation. ||| 11670 ||| 11671 ||| 
2018 ||| one vs. many qa matching with both word-level and sentence-level attention network. ||| 4754 ||| 3085 ||| 3084 ||| 3087 ||| 3086 ||| 1254 ||| 3088 ||| 
2020 ||| generating plausible counterfactual explanations for deep transformers in financial text classification. ||| 8872 ||| 11672 ||| 8873 ||| 208 ||| 8874 ||| 8875 ||| 
2020 ||| supervised visual attention for multimodal neural machine translation. ||| 11673 ||| 3784 ||| 3785 ||| 11674 ||| 6420 ||| 
2020 ||| span-based joint entity and relation extraction with attention-based span-specific and contextual semantic representations. ||| 11675 ||| 11676 ||| 11677 ||| 9547 ||| 7779 ||| 11395 ||| 11678 ||| 
2018 ||| variational attention for sequence-to-sequence models. ||| 11679 ||| 1389 ||| 11680 ||| 11681 ||| 
2018 ||| zero pronoun resolution with attention-based neural network. ||| 11682 ||| 9472 ||| 1219 ||| 3311 ||| 3802 ||| 
2020 ||| how far does bert look at: distance-based clustering and analysis of bert's attention. ||| 11683 ||| 11684 ||| 242 ||| 11685 ||| 11686 ||| 
2018 ||| implicit discourse relation recognition using neural tensor network with interactive attention and sparse learning. ||| 11687 ||| 9056 ||| 8971 ||| 5095 ||| 5093 ||| 11688 ||| 
2020 ||| dual attention network for cross-lingual entity alignment. ||| 4394 ||| 11689 ||| 11690 ||| 
2020 ||| dual attention model for citation recommendation. ||| 1420 ||| 11691 ||| 
2020 ||| improving long-tail relation extraction with collaborating relation-augmented attention. ||| 438 ||| 4871 ||| 802 ||| 800 ||| 4872 ||| 4873 ||| 
2020 ||| optimizing transformer for low-resource neural machine translation. ||| 11692 ||| 11693 ||| 
2018 ||| a position-aware bidirectional attention network for aspect-level sentiment analysis. ||| 11694 ||| 11695 ||| 5078 ||| 11696 ||| 
2020 ||| joint chinese word segmentation and part-of-speech tagging via multi-channel attention of character n-grams. ||| 3197 ||| 3198 ||| 3200 ||| 
2020 ||| multi-task learning of spoken language understanding by integrating n-best hypotheses with hierarchical attention. ||| 11697 ||| 5787 ||| 11698 ||| 3373 ||| 11699 ||| 11700 ||| 
2020 ||| debunking rumors on twitter with tree transformer. ||| 3646 ||| 1310 ||| 
2020 ||| hierarchical chinese legal event extraction via pedal attention mechanism. ||| 11701 ||| 5528 ||| 5189 ||| 11702 ||| 11703 ||| 
2020 ||| learning to decouple relations: few-shot relation classification with entity-guided attention and confusion-aware training. ||| 11704 ||| 4271 ||| 11705 ||| 3560 ||| 3561 ||| 3562 ||| 880 ||| 
2018 ||| interaction-aware topic model for microblog conversations through network embedding and user attention. ||| 9056 ||| 11706 ||| 8971 ||| 5093 ||| 5095 ||| 11688 ||| 
2020 ||| a contextual alignment enhanced cross graph attention network for cross-lingual entity alignment. ||| 11707 ||| 11708 ||| 11709 ||| 3888 ||| 11710 ||| 9606 ||| 
2018 ||| multilingual neural machine translation with task-specific attention. ||| 11711 ||| 11712 ||| 11713 ||| 
2020 ||| increasing learning efficiency of self-attention networks through direct position interactions, learnable temperature, and convoluted attention. ||| 11714 ||| 11715 ||| 11716 ||| 11717 ||| 
2018 ||| a comparison of transformer and recurrent neural networks on multilingual neural machine translation. ||| 11718 ||| 11719 ||| 11720 ||| 
2018 ||| a lexicon-based supervised attention model for neural sentiment analysis. ||| 11721 ||| 9627 ||| 336 ||| 3273 ||| 
2018 ||| incorporating argument-level interactions for persuasion comments evaluation using co-attention model. ||| 11722 ||| 4814 ||| 11723 ||| 1305 ||| 336 ||| 3273 ||| 
2020 ||| hitrans: a transformer-based context- and speaker-sensitive model for emotion detection in conversations. ||| 11724 ||| 3725 ||| 9028 ||| 11725 ||| 11726 ||| 
2018 ||| hybrid attention based multimodal network for spoken language classification. ||| 3627 ||| 3628 ||| 3629 ||| 3630 ||| 2419 ||| 2425 ||| 
2020 ||| autoregressive reasoning over chains of facts with transformers. ||| 11727 ||| 11728 ||| 10702 ||| 
2020 ||| better sign language translation with stmc-transformer. ||| 3546 ||| 11729 ||| 
2020 ||| improving sentiment analysis over non-english tweets using multilingual transformers and automatic translation for data-augmentation. ||| 11730 ||| 7350 ||| 11731 ||| 
2020 ||| interpretable multi-headed attention for abstractive summarization at controllable lengths. ||| 11732 ||| 11733 ||| 11734 ||| 11735 ||| 
2020 ||| aprile: attention with pseudo residual connection for knowledge graph embedding. ||| 11736 ||| 5845 ||| 11737 ||| 11738 ||| 11739 ||| 
2020 ||| knowledge aware emotion recognition in textual conversations via multi-task incremental transformer. ||| 11740 ||| 3674 ||| 5270 ||| 728 ||| 
2018 ||| visual question answering dataset for bilingual image understanding: a study of cross-lingual transfer using attention maps. ||| 11741 ||| 11742 ||| 11743 ||| 
2018 ||| who is killed by police: introducing supervised attention for hierarchical lstms. ||| 9940 ||| 11744 ||| 
2018 ||| neural machine translation with decoding history enhanced attention. ||| 3428 ||| 11745 ||| 11746 ||| 3182 ||| 3181 ||| 11747 ||| 
2020 ||| evaluating pretrained transformer-based models on the task of fine-grained named entity recognition. ||| 11748 ||| 11749 ||| 11750 ||| 11751 ||| 11752 ||| 11753 ||| 
2020 ||| incorporating noisy length constraints into transformer with length-aware positional encodings. ||| 11754 ||| 11755 ||| 11756 ||| 11757 ||| 
2020 ||| scale down transformer by grouping features for a lightweight character-level language model. ||| 8511 ||| 11758 ||| 8512 ||| 11759 ||| 11760 ||| 8514 ||| 
2018 ||| a multi-attention based neural network with external knowledge for story ending predicting task. ||| 1719 ||| 11761 ||| 8896 ||| 11762 ||| 11763 ||| 1979 ||| 
2020 ||| the devil is in the details: evaluating limitations of transformer-based methods for granular tasks. ||| 11764 ||| 11765 ||| 11766 ||| 3400 ||| 
2020 ||| joint transformer/rnn architecture for gesture typing in indic languages. ||| 10935 ||| 11767 ||| 3328 ||| 11768 ||| 
2020 ||| transquest: translation quality estimation with cross-lingual transformers. ||| 3849 ||| 3850 ||| 3851 ||| 
2020 ||| conan: a complementary neighboring-based attention network for referring expression generation. ||| 11769 ||| 11770 ||| 11771 ||| 
2020 ||| context-aware cross-attention for non-autoregressive translation. ||| 3794 ||| 3038 ||| 8976 ||| 1756 ||| 3041 ||| 
2018 ||| stance detection with hierarchical attention network. ||| 11772 ||| 11773 ||| 222 ||| 3088 ||| 
2018 ||| identification of internal faults in indirect symmetrical phase shift transformers using ensemble learning. ||| 11774 ||| 11775 ||| 11776 ||| 
2018 ||| sam-gcnn: a gated convolutional neural network with segment-level attention mechanism for home activity monitoring. ||| 11777 ||| 11778 ||| 11779 ||| 
2018 ||| using attention to process rf for cognitive radio. ||| 11780 ||| 11781 ||| 
2020 ||| a joint detection-classification model for weakly supervised sound event detection using multi-scale attention method. ||| 11782 ||| 329 ||| 
2021 ||| improving stateful premise selection with transformers. ||| 11783 ||| 11784 ||| 4194 ||| 11785 ||| 
2021 |||  attention: a self-attention based neural network for remaining useful lifetime predictions. ||| 11786 ||| 11787 ||| 
2018 ||| a reading comprehension style question answering model based on attention mechanism. ||| 11788 ||| 11789 ||| 6052 ||| 
2019 ||| reconstructing attention with dynamic regularization. ||| 11790 ||| 11791 ||| 11792 ||| 
2021 ||| face shows your intention: visual search based on full-face gaze estimation with channel-spatial attention. ||| 2143 ||| 11793 ||| 11794 ||| 11795 ||| 11796 ||| 
2021 ||| divided caption model with global attention. ||| 11797 ||| 11798 ||| 11799 ||| 11800 ||| 
2021 ||| optical flow estimation with foreground attention guided network. ||| 11801 ||| 11802 ||| 
2021 ||| soft-gated self-supervision network for action reasoning: soft-gated self-supervision network with attention mechanism and joint multi-task training strategy for action reasoning. ||| 11803 ||| 11804 ||| 11805 ||| 11806 ||| 11807 ||| 11808 ||| 
2021 ||| deep multi-scale recursive residual attention network for spectral super resolution. ||| 11809 ||| 11810 ||| 11811 ||| 11812 ||| 
2020 ||| an object detection algorithm based on attention mechanism and lightweight network (amln). ||| 11813 ||| 11814 ||| 11815 ||| 11816 ||| 
2021 ||| parallel attention with weighted efficient network for video-based person re-identification. ||| 11817 ||| 11818 ||| 1299 ||| 11819 ||| 11820 ||| 11821 ||| 
2021 ||| chinese description of videos incorporating multimodal features and attention mechanism. ||| 5525 ||| 5800 ||| 5524 ||| 
2021 ||| deep recommendation model based on local attention and gru. ||| 406 ||| 11822 ||| 11823 ||| 
2020 ||| attention guided multi-scale regression for scene text detection. ||| 11824 ||| 
2017 ||| evaluating bad and good eeg segments based on extracted features: towards an automated understanding of infant behavior and attention. ||| 11825 ||| 11826 ||| 11827 ||| 
2017 ||| a systematic review: attention assessment of virtual reality based intervention for learning in children with autism spectrum disorder. ||| 4346 ||| 4347 ||| 4348 ||| 11828 ||| 
2018 ||| multisensory virtual game with use of the device leap motion to improve the lack of attention in children of 7-12 years with adhd. ||| 11829 ||| 11830 ||| 11831 ||| 4252 ||| 11832 ||| 11833 ||| 11834 ||| 
2021 ||| games, attention and brain signals. ||| 11835 ||| 11836 ||| 3882 ||| 3369 ||| 11837 ||| 11838 ||| 11839 ||| 
2021 ||| exploring transformer-based language recognition using phonotactic information. ||| 11840 ||| 9438 ||| 11841 ||| 
2018 ||| end-to-end speech translation with the transformer. ||| 11842 ||| 11843 ||| 852 ||| 3467 ||| 3466 ||| 
2018 ||| self-attention linguistic-acoustic decoder. ||| 11844 ||| 11845 ||| 11846 ||| 
2017 ||| recognizing entailments in legal texts using sentence encoding-based and decomposable attention models. ||| 11847 ||| 7520 ||| 7514 ||| 
2019 ||| combining similarity and transformer methods for case law entailment. ||| 11848 ||| 11849 ||| 11850 ||| 
2021 ||| using transformers to improve answer retrieval for legal questions. ||| 11851 ||| 11852 ||| 
2019 ||| neural attention learning for legal query reformulation. ||| 11853 ||| 11854 ||| 
2017 ||| legal question answering system using neural attention. ||| 11855 ||| 11856 ||| 11857 ||| 4802 ||| 4803 ||| 
2020 ||| alphanet: an attention guided deep network for automatic image matting. ||| 11858 ||| 11859 ||| 11860 ||| 
2021 ||| a microcontroller is all you need: enabling transformer execution on low-power iot endnodes. ||| 11861 ||| 11862 ||| 11863 ||| 11864 ||| 11865 ||| 
2021 ||| attention-based reinforcement learning for real-time uav semantic communication. ||| 11866 ||| 11867 ||| 11868 ||| 11869 ||| 11870 ||| 11871 ||| 11872 ||| 
2019 ||| customized hidden layered ann based pattern recognition technique for differential protection of power transformer. ||| 11873 ||| 11874 ||| 
2020 ||| guiding symbolic natural language grammar induction via transformer-based sequence probabilities. ||| 11875 ||| 3369 ||| 11876 ||| 11877 ||| 11878 ||| 
2020 ||| an attentional control mechanism for reasoning and learning. ||| 11879 ||| 11880 ||| 
2017 ||| development of microtransformers using mcm and electronic packaging technologies. ||| 11881 ||| 11882 ||| 11883 ||| 
2017 ||| measuring readiness-to-hand through differences in attention to the task vs. attention to the tool. ||| 11884 ||| 11885 ||| 11886 ||| 
2020 ||| towards computational identification of visual attention on interactive tabletops. ||| 11887 ||| 8097 ||| 11888 ||| 11889 ||| 
2017 ||| lyrics-based music genre classification using a hierarchical attention network. ||| 11890 ||| 
2017 ||| automatic drum transcription for polyphonic recordings using soft attention mechanisms and convolutional neural networks. ||| 11891 ||| 11892 ||| 11893 ||| 
2019 ||| learning soft-attention models for tempo-invariant audio-sheet music retrieval. ||| 11894 ||| 11895 ||| 11896 ||| 11897 ||| 11898 ||| 
2019 ||| harmony transformer: incorporating chord segmentation into harmony recognition. ||| 11899 ||| 11900 ||| 
2020 ||| the jazz transformer on the front line: exploring the shortcomings of ai-composed music through quantitative measures. ||| 11901 ||| 4374 ||| 
2019 ||| a bi-directional transformer for musical chord recognition. ||| 11902 ||| 11903 ||| 11904 ||| 11905 ||| 11906 ||| 
2019 ||| an attention mechanism for musical instrument recognition. ||| 11907 ||| 11908 ||| 11909 ||| 
2021 ||| sequence-to-sequence piano transcription with transformers. ||| 11910 ||| 11911 ||| 11912 ||| 11913 ||| 11914 ||| 
2021 ||| semi-supervised music tagging transformer. ||| 11915 ||| 11916 ||| 11917 ||| 
2020 ||| automatic composition of guitar tabs by transformers and groove modeling. ||| 11918 ||| 4372 ||| 11919 ||| 4374 ||| 
2021 ||| spectnt: a time-frequency transformer for music audio. ||| 11920 ||| 11921 ||| 11915 ||| 11916 ||| 11922 ||| 
2020 ||| sentiment analysis with contextual embeddings and self-attention. ||| 11923 ||| 11924 ||| 11925 ||| 
2020 ||| attention to emotions: detecting mental disorders in social media. ||| 11926 ||| 3882 ||| 11927 ||| 11928 ||| 11929 ||| 11930 ||| 11931 ||| 11932 ||| 11933 ||| 
2021 ||| transformer-based automatic punctuation prediction and word casing reconstruction of the asr output. ||| 11934 ||| 11935 ||| 11936 ||| 11937 ||| 11938 ||| 
2021 ||| attention-based end-to-end named entity recognition from speech. ||| 11939 ||| 11940 ||| 11941 ||| 
2021 ||| lstm-xl: attention enhanced long-term memory for lstm cells. ||| 11942 ||| 11943 ||| 11944 ||| 11941 ||| 
2018 ||| application of amplified reality to the cognitive effect of children with attention deficit hyperactivity disorder(adhd) - an example of italian chicco-app interactive building blocks. ||| 11945 ||| 11946 ||| 
2019 ||| adjustment of the power factor and the impedance by the phase of the flux in a transformer circuit. ||| 11947 ||| 11948 ||| 11949 ||| 
2021 ||| ask2transformers: zero-shot domain labelling with pretrained language models. ||| 11950 ||| 11951 ||| 
2021 ||| a review of methods to detect divided attention impairments in alzheimer's disease. ||| 11952 ||| 11953 ||| 11954 ||| 11955 ||| 11956 ||| 11957 ||| 
2020 ||| toward semantic iot load inference attention management for facilitating healthcare and public health collaboration: a survey. ||| 11958 ||| 11959 ||| 
2018 ||| 3d vae-attention network: a parallel system for single-view 3d reconstruction. ||| 11960 ||| 11961 ||| 11962 ||| 11963 ||| 4385 ||| 
2019 ||| gaze attention and flow visualization using the smudge effect. ||| 11964 ||| 11965 ||| 11966 ||| 11967 ||| 
2020 ||| book rating model based on self-attention and lstm. ||| 11968 ||| 
2020 ||| where attention goes, energy flows: enhancing individual sustainability in software engineering. ||| 11969 ||| 
2019 ||| sequence-level knowledge distillation for model compression of attention-based sequence-to-sequence speech recognition. ||| 11970 ||| 11971 ||| 11972 ||| 
2021 ||| transformer in action: a comparative study of transformer-based acoustic models for large scale speech recognition applications. ||| 11973 ||| 11974 ||| 11975 ||| 11976 ||| 11977 ||| 11978 ||| 11979 ||| 
2019 ||| triggered attention for end-to-end speech recognition. ||| 11980 ||| 2508 ||| 11981 ||| 
2021 ||| frequency-temporal attention network for singing melody extraction. ||| 4520 ||| 11982 ||| 4112 ||| 3337 ||| 
2019 ||| attention-augmented end-to-end multi-task learning for emotion prediction from speech. ||| 11983 ||| 11984 ||| 648 ||| 649 ||| 
2021 ||| attention-guided second-order pooling convolutional networks. ||| 11985 ||| 11986 ||| 11987 ||| 11988 ||| 817 ||| 
2021 ||| topic-aware dialogue generation with two-hop based graph attention. ||| 11989 ||| 4211 ||| 11990 ||| 11991 ||| 4892 ||| 4214 ||| 
2020 ||| deep monocular video depth estimation using temporal attention. ||| 11992 ||| 11315 ||| 11317 ||| 
2020 ||| lightweight and efficient end-to-end speech recognition using low-rank transformer. ||| 10650 ||| 11993 ||| 10652 ||| 11994 ||| 10654 ||| 
2019 ||| windowed attention mechanisms for speech recognition. ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2021 ||| tstnn: two-stage transformer based neural network for speech enhancement in the time domain. ||| 333 ||| 11998 ||| 11999 ||| 
2020 ||| improving end-to-end speech synthesis with local recurrent neural network enhanced transformer. ||| 12000 ||| 12001 ||| 12002 ||| 12003 ||| 
2020 ||| a regularized attention mechanism for graph attention networks. ||| 12004 ||| 12005 ||| 12006 ||| 
2021 ||| domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition. ||| 5519 ||| 5246 ||| 5093 ||| 5095 ||| 
2020 ||| multi-resolution multi-head attention in deep speaker embedding. ||| 12007 ||| 12008 ||| 12009 ||| 12010 ||| 
2021 ||| transformer language models with lstm-based cross-utterance information representation. ||| 12011 ||| 8862 ||| 12012 ||| 
2021 ||| top-down attention in end-to-end spoken language understanding. ||| 5348 ||| 12013 ||| 12014 ||| 12015 ||| 12016 ||| 12017 ||| 12018 ||| 
2021 ||| riemannian geometry-based decoding of the directional focus of auditory attention using eeg. ||| 8253 ||| 8254 ||| 8255 ||| 
2021 ||| hierarchical refined attention for scene text recognition. ||| 1254 ||| 2882 ||| 2885 ||| 
2020 ||| self-attention and retrieval enhanced neural networks for essay generation. ||| 1160 ||| 793 ||| 12019 ||| 
2019 ||| end-to-end audio visual scene-aware dialog using multimodal attention-based video features. ||| 2507 ||| 12020 ||| 7284 ||| 12021 ||| 2508 ||| 7283 ||| 2512 ||| 12022 ||| 12023 ||| 12024 ||| 12025 ||| 8564 ||| 8567 ||| 
2020 ||| deblurring and super-resolution using deep gated fusion attention networks for face images. ||| 12026 ||| 12027 ||| 
2019 ||| single-channel speech extraction using speaker inventory and attention network. ||| 12028 ||| 12029 ||| 12030 ||| 8068 ||| 12031 ||| 12032 ||| 12016 ||| 12033 ||| 
2021 ||| attention-embedded decomposed network with unpaired ct images prior for metal artifact reduction. ||| 422 ||| 12034 ||| 12035 ||| 420 ||| 
2020 ||| cross-view attention network for breast cancer screening from multi-view mammograms. ||| 12036 ||| 12037 ||| 12038 ||| 
2019 ||| discriminative saliency-pose-attention covariance for action recognition. ||| 12039 ||| 259 ||| 4263 ||| 4262 ||| 
2021 ||| multimodal cross- and self-attention network for speech emotion recognition. ||| 12040 ||| 2304 ||| 12041 ||| 6227 ||| 
2021 ||| adaptable multi-domain language model for transformer asr. ||| 12042 ||| 12043 ||| 12044 ||| 12045 ||| 12046 ||| 12047 ||| 12048 ||| 12049 ||| 12050 ||| 12051 ||| 12052 ||| 12053 ||| 12054 ||| 
2018 ||| speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. ||| 5269 ||| 5270 ||| 728 ||| 
2020 ||| parsing map guided multi-scale attention network for face hallucination. ||| 3995 ||| 12055 ||| 12056 ||| 12057 ||| 12058 ||| 
2021 ||| cascade attention fusion for fine-grained image captioning based on multi-layer lstm. ||| 6827 ||| 6823 ||| 6824 ||| 241 ||| 6825 ||| 6826 ||| 400 ||| 
2020 ||| multi-head attention for speech emotion recognition with auxiliary learning of gender recognition. ||| 12059 ||| 12060 ||| 7243 ||| 
2020 ||| attention-guided deraining network via stage-wise learning. ||| 12061 ||| 2146 ||| 12062 ||| 2230 ||| 12063 ||| 6862 ||| 12056 ||| 
2020 ||| deep exposure fusion with deghosting via homography estimation and attention learning. ||| 12064 ||| 2345 ||| 
2018 ||| minimum word error rate training for attention-based sequence-to-sequence models. ||| 12065 ||| 12066 ||| 12067 ||| 12068 ||| 12069 ||| 3334 ||| 12070 ||| 
2021 ||| joint alignment learning-attention based model for grapheme-to-phoneme conversion. ||| 12071 ||| 790 ||| 4600 ||| 4499 ||| 
2020 ||| improving the performance of transformer based low resource speech recognition for indian languages. ||| 12072 ||| 12073 ||| 12074 ||| 
2021 ||| attention-based multi-encoder automatic pronunciation assessment. ||| 12075 ||| 12076 ||| 
2020 ||| key action and joint ctc-attention based sign language recognition. ||| 12077 ||| 12078 ||| 12079 ||| 12080 ||| 12081 ||| 
2020 ||| improving auditory attention decoding performance of linear and non-linear methods using state-space model. ||| 1490 ||| 12082 ||| 1495 ||| 
2020 ||| speech enhancement using self-adaptation and multi-head self-attention. ||| 8065 ||| 12083 ||| 1491 ||| 12084 ||| 12085 ||| 
2020 ||| fcem: a novel fast correlation extract model for real time steganalysis of voip stream via multi-head attention. ||| 2792 ||| 2793 ||| 2791 ||| 2794 ||| 2795 ||| 
2020 ||| h-vectors: utterance-level speaker embedding using a hierarchical attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2021 ||| streaming simultaneous speech translation with augmented memory transformer. ||| 12088 ||| 11973 ||| 12089 ||| 12090 ||| 11637 ||| 
2020 ||| facial emotion recognition using light field images with deep attention-based bidirectional lstm. ||| 12091 ||| 12092 ||| 12093 ||| 12094 ||| 
2020 ||| attention-based curiosity-driven exploration in deep reinforcement learning. ||| 12095 ||| 5335 ||| 5336 ||| 
2020 ||| t-gsa: transformer with gaussian-weighted self-attention for speech enhancement. ||| 12096 ||| 11315 ||| 11317 ||| 
2020 ||| an attention-based joint acoustic and text on-device end-to-end model. ||| 12066 ||| 3336 ||| 12097 ||| 12098 ||| 3334 ||| 12099 ||| 
2021 ||| pre-training transformer decoder for end-to-end asr model with unpaired text data. ||| 12100 ||| 12101 ||| 12102 ||| 12103 ||| 12104 ||| 8298 ||| 
2020 ||| structured sparse attention for end-to-end automatic speech recognition. ||| 12105 ||| 12106 ||| 12107 ||| 
2019 ||| models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. ||| 12108 ||| 12109 ||| 3510 ||| 
2018 ||| efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. ||| 12110 ||| 12111 ||| 12112 ||| 
2020 ||| improved end-to-end spoken utterance classification with a self-attention acoustic classifier. ||| 12113 ||| 12114 ||| 12115 ||| 
2021 ||| hybrid model for network anomaly detection with gradient boosting decision trees and tabtransformer. ||| 12116 ||| 12117 ||| 
2020 ||| bio-mimetic attentional feedback in music source separation. ||| 12118 ||| 12119 ||| 
2020 ||| residual attention network for wavelet domain super-resolution. ||| 2058 ||| 12120 ||| 12121 ||| 12122 ||| 5141 ||| 
2020 ||| interpretable self-attention temporal reasoning for driving behavior understanding. ||| 12123 ||| 12124 ||| 12125 ||| 7457 ||| 7460 ||| 59 ||| 12126 ||| 
2020 ||| channel-attention dense u-net for multichannel speech enhancement. ||| 12127 ||| 8075 ||| 12128 ||| 8076 ||| 8077 ||| 
2020 ||| weakly-supervised sound event detection with self-attention. ||| 12129 ||| 4464 ||| 8223 ||| 3549 ||| 12130 ||| 12131 ||| 
2019 ||| cognitive-driven binaural lcmv beamformer using eeg-based auditory attention decoding. ||| 1490 ||| 1495 ||| 
2021 ||| vset: a multimodal transformer for visual speech enhancement. ||| 12132 ||| 12133 ||| 12134 ||| 952 ||| 3889 ||| 
2020 ||| frequency and temporal convolutional attention for text-independent speaker recognition. ||| 12135 ||| 12136 ||| 
2019 ||| dilated residual network with multi-head self-attention for speech emotion recognition. ||| 4458 ||| 3138 ||| 4459 ||| 12137 ||| 4460 ||| 
2019 ||| non-local self-attention structure for function approximation in deep reinforcement learning. ||| 12138 ||| 2744 ||| 2745 ||| 12139 ||| 12140 ||| 12141 ||| 3477 ||| 2748 ||| 
2021 ||| sub-band grouping spectral feature-attention block for hyperspectral image classification. ||| 12142 ||| 11256 ||| 11436 ||| 
2019 ||| replay attack detection using magnitude and phase information with attention-based adaptive filters. ||| 6736 ||| 5093 ||| 5095 ||| 12143 ||| 12144 ||| 11688 ||| 
2021 ||| mutually-constrained monotonic multihead attention for online asr. ||| 12145 ||| 12146 ||| 9316 ||| 
2019 ||| context modelling using hierarchical attention networks for sentiment and self-assessed emotion detection in spoken narratives. ||| 12147 ||| 647 ||| 12148 ||| 12149 ||| 12150 ||| 646 ||| 648 ||| 649 ||| 
2021 ||| meta-learning with attention for improved few-shot learning. ||| 12151 ||| 12152 ||| 12153 ||| 
2020 ||| voice conversion with transformer network. ||| 12154 ||| 3889 ||| 12155 ||| 
2019 ||| automatic singing transcription based on encoder-decoder recurrent neural networks with a weakly-supervised attention mechanism. ||| 8071 ||| 8072 ||| 12156 ||| 8073 ||| 8074 ||| 
2019 ||| attention-based graph convolutional network for recommendation system. ||| 12157 ||| 12158 ||| 12159 ||| 12160 ||| 
2020 ||| deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition. ||| 12161 ||| 12162 ||| 12163 ||| 
2020 ||| arnet: attention-based refinement network for few-shot semantic segmentation. ||| 12164 ||| 12165 ||| 782 ||| 783 ||| 
2021 ||| attentionlite: towards efficient self-attention models for vision. ||| 7327 ||| 12166 ||| 
2020 ||| efficient scene text detection with textual attention tower. ||| 1166 ||| 12167 ||| 12168 ||| 12169 ||| 9351 ||| 9354 ||| 5328 ||| 9353 ||| 
2018 ||| eeg-based auditory attention decoding using steerable binaural superdirective beamformer. ||| 1490 ||| 12170 ||| 1495 ||| 
2021 ||| speech emotion recognition with multiscale area attention and data augmentation. ||| 6206 ||| 2532 ||| 12171 ||| 781 ||| 
2021 ||| catiloc: camera image transformer for indoor localization. ||| 12172 ||| 12173 ||| 12174 ||| 
2021 ||| attention on attention sparse dense convolutional network for financial signal processing. ||| 12175 ||| 2404 ||| 12176 ||| 3614 ||| 7897 ||| 
2017 ||| joint ctc-attention based end-to-end speech recognition using multi-task learning. ||| 12177 ||| 2508 ||| 3549 ||| 
2018 ||| advancing connectionist temporal classification with attention modeling. ||| 12178 ||| 12179 ||| 8164 ||| 12033 ||| 
2019 ||| video quality assessment for encrypted http adaptive streaming: attention-based hybrid rnn-hmm model. ||| 12180 ||| 12181 ||| 12182 ||| 12183 ||| 
2021 ||| improving audio anomalies recognition using temporal convolutional attention networks. ||| 12087 ||| 8233 ||| 
2020 ||| attention mechanism enhanced kernel prediction networks for denoising of burst images. ||| 2747 ||| 12184 ||| 12185 ||| 12186 ||| 12187 ||| 
2019 ||| seq2seq attentional siamese neural networks for text-dependent speaker verification. ||| 3757 ||| 12188 ||| 10057 ||| 12189 ||| 12190 ||| 3808 ||| 
2018 ||| a time-restricted self-attention layer for asr. ||| 12191 ||| 12192 ||| 12193 ||| 1424 ||| 12194 ||| 
2021 ||| synergic feature attention for image restoration. ||| 12195 ||| 12196 ||| 
2020 ||| full-reference speech quality estimation with attentional siamese neural networks. ||| 12197 ||| 12198 ||| 3831 ||| 
2019 ||| visual relationship recognition via language and position guided attention. ||| 2736 ||| 12199 ||| 11386 ||| 12200 ||| 
2021 ||| monaural speech enhancement with complex convolutional block attention module and joint time frequency losses. ||| 12201 ||| 12202 ||| 12203 ||| 
2020 ||| emet: embeddings from multilingual-encoder transformer for fake news detection. ||| 12204 ||| 2871 ||| 12205 ||| 12206 ||| 12207 ||| 
2021 ||| towards immediate backchannel generation using attention-based early prediction model. ||| 12208 ||| 12209 ||| 10470 ||| 
2019 ||| a sequential guiding network with attention for image captioning. ||| 12210 ||| 7095 ||| 12211 ||| 7094 ||| 
2020 ||| bba-net: a bi-branch attention network for crowd counting. ||| 12212 ||| 12213 ||| 1856 ||| 2827 ||| 12214 ||| 8177 ||| 11404 ||| 11405 ||| 
2020 ||| stock movement prediction that integrates heterogeneous data sources using dilated causal convolution networks with attention. ||| 12215 ||| 12216 ||| 12217 ||| 
2021 ||| atvio: attention guided visual-inertial odometry. ||| 2014 ||| 2064 ||| 12218 ||| 
2020 ||| distilling attention weights for ctc-based asr systems. ||| 4409 ||| 12219 ||| 4407 ||| 12220 ||| 4408 ||| 12221 ||| 
2021 ||| jointly trained transformers models for spoken language translation. ||| 12222 ||| 12223 ||| 6785 ||| 12224 ||| 12225 ||| 10500 ||| 10501 ||| 12226 ||| 
2021 ||| blind deinterleaving of signals in time series with self-attention based soft min-cost flow learning. ||| 12227 ||| 12228 ||| 12229 ||| 7442 ||| 12230 ||| 12231 ||| 
2021 ||| image super-resolution using multi-resolution attention network. ||| 5802 ||| 11326 ||| 12232 ||| 
2021 ||| co-attentional transformers for story-based video understanding. ||| 648 ||| 12233 ||| 8580 ||| 
2021 ||| multi-task transformer with input feature reconstruction for dysarthric speech recognition. ||| 12234 ||| 12235 ||| 764 ||| 
2018 ||| attention-based lstm for psychological stress detection from spoken language using distant supervision. ||| 10650 ||| 12236 ||| 10654 ||| 
2018 ||| attention-based dialog state tracking for conversational interview coaching. ||| 12237 ||| 4388 ||| 12238 ||| 12239 ||| 
2021 ||| real image super-resolution using token based contextual attention. ||| 12240 ||| 2387 ||| 
2020 ||| synchronous transformers for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12243 ||| 12041 ||| 3364 ||| 12244 ||| 
2020 ||| end-to-end multi-speaker speech recognition with transformer. ||| 12245 ||| 12246 ||| 12247 ||| 11981 ||| 3549 ||| 
2021 ||| transformer-based end-to-end speech recognition with local dense synthesizer attention. ||| 4484 ||| 4483 ||| 4381 ||| 
2021 ||| an attention model for hypernasality prediction in children with cleft palate. ||| 12248 ||| 12249 ||| 12250 ||| 12251 ||| 12252 ||| 
2019 ||| noise-tolerant audio-visual online person verification using an attention-based neural network fusion. ||| 12253 ||| 7363 ||| 12254 ||| 
2019 ||| knowledge distillation using output errors for self-attention end-to-end models. ||| 12050 ||| 12255 ||| 12256 ||| 12052 ||| 12044 ||| 12043 ||| 12054 ||| 
2021 ||| a further study of unsupervised pretraining for transformer based speech recognition. ||| 12257 ||| 12258 ||| 12259 ||| 12260 ||| 12261 ||| 12262 ||| 12263 ||| 7134 ||| 11688 ||| 
2019 ||| attention in recurrent neural networks for ransomware detection. ||| 12264 ||| 6017 ||| 12265 ||| 12266 ||| 
2020 ||| weakly labelled audio tagging via convolutional networks with spatial and channel-wise attention. ||| 12267 ||| 4430 ||| 11418 ||| 455 ||| 
2020 ||| trilingual semantic embeddings of visually grounded speech with self-attention mechanisms. ||| 12268 ||| 12269 ||| 12270 ||| 12271 ||| 12272 ||| 12254 ||| 
2019 ||| an attention-aware bidirectional multi-residual recurrent neural network (abmrnn): a study about better short-term text classification. ||| 6718 ||| 12273 ||| 12274 ||| 12275 ||| 12276 ||| 12277 ||| 
2020 ||| adrn: attention-based deep residual network for hyperspectral image denoising. ||| 12278 ||| 12057 ||| 12056 ||| 12058 ||| 
2020 ||| spatial attentional bilinear 3d convolutional network for video-based autism spectrum disorder detection. ||| 12279 ||| 1556 ||| 12280 ||| 12281 ||| 12282 ||| 
2020 ||| audio-attention discriminative language model for asr rescoring. ||| 3843 ||| 3841 ||| 
2021 ||| multi-dialect speech recognition in english using attention on ensemble of experts. ||| 12178 ||| 12283 ||| 1236 ||| 
2021 ||| attention is all you need in speech separation. ||| 12284 ||| 12285 ||| 12286 ||| 12287 ||| 12288 ||| 
2017 ||| dynamic tracking attention model for action recognition. ||| 12289 ||| 12290 ||| 12291 ||| 4502 ||| 
2021 ||| bayesian transformer language models for speech recognition. ||| 12292 ||| 12293 ||| 12294 ||| 12295 ||| 12296 ||| 12297 ||| 12298 ||| 12299 ||| 4460 ||| 
2019 ||| modality attention for end-to-end audio-visual speech recognition. ||| 12300 ||| 12301 ||| 5110 ||| 12302 ||| 4459 ||| 
2020 ||| deja-vu: double feature presentation and iterated loss in deep transformer networks. ||| 12303 ||| 12304 ||| 11975 ||| 5450 ||| 11973 ||| 2120 ||| 11757 ||| 12305 ||| 
2020 ||| transformer-based text-to-speech with weighted forced attention. ||| 12306 ||| 12130 ||| 12307 ||| 12308 ||| 
2020 ||| hierarchical attention transfer networks for depression assessment from speech. ||| 645 ||| 12309 ||| 11983 ||| 647 ||| 12310 ||| 648 ||| 649 ||| 
2020 ||| speaker-aware training of attention-based end-to-end speech recognition using neural speaker embeddings. ||| 12311 ||| 12312 ||| 11941 ||| 
2019 ||| improving facial attractiveness prediction via co-attention learning. ||| 12313 ||| 4176 ||| 12314 ||| 12315 ||| 12316 ||| 
2021 ||| low-dimensional denoising embedding transformer for ecg classification. ||| 12317 ||| 666 ||| 12318 ||| 12319 ||| 11418 ||| 
2020 ||| deep audio-visual speech separation with attention mechanism. ||| 12320 ||| 12247 ||| 
2021 ||| head-synchronous decoding for transformer-based streaming asr. ||| 12321 ||| 12322 ||| 12323 ||| 
2020 ||| fixed-point optimization of transformer neural network. ||| 12324 ||| 1613 ||| 
2020 ||| compare learning: bi-attention network for few-shot learning. ||| 12325 ||| 12326 ||| 12327 ||| 6552 ||| 
2020 ||| preference-aware mask for session-based recommendation with bidirectional transformer. ||| 12328 ||| 12329 ||| 12330 ||| 6469 ||| 12331 ||| 12332 ||| 12333 ||| 12334 ||| 
2021 ||| hierarchical transformer-based large-context end-to-end asr with large-context knowledge distillation. ||| 4408 ||| 10275 ||| 10274 ||| 10276 ||| 4407 ||| 10277 ||| 
2019 ||| ad-net: attention guided network for optical flow estimation using dilated convolution. ||| 11590 ||| 11589 ||| 11591 ||| 6927 ||| 11592 ||| 
2021 ||| image-assisted transformer in zero-resource multi-modal translation. ||| 12335 ||| 12235 ||| 2792 ||| 
2020 ||| video question generation via semantic rich cross-modal self-attention networks learning. ||| 12336 ||| 1385 ||| 12337 ||| 12338 ||| 1387 ||| 
2020 ||| weakly supervised crowd-wise attention for robust crowd counting. ||| 11387 ||| 11388 ||| 2736 ||| 11386 ||| 
2019 ||| detecting attention shift from neural response based on beat-frequency-modulated musical excerpts. ||| 12339 ||| 12340 ||| 12341 ||| 
2020 ||| iq-stan: image quality guided spatio-temporal attention network for license plate recognition. ||| 12342 ||| 6627 ||| 6922 ||| 
2021 ||| channel attention residual u-net for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 12343 ||| 12344 ||| 5067 ||| 5337 ||| 
2019 ||| an interaction-aware attention network for speech emotion recognition in spoken dialogs. ||| 12345 ||| 12346 ||| 12347 ||| 
2021 ||| a co-interactive transformer for joint slot filling and intent detection. ||| 12348 ||| 12349 ||| 3707 ||| 12350 ||| 12351 ||| 3311 ||| 
2021 ||| self-attention generative adversarial network for speech enhancement. ||| 12352 ||| 12353 ||| 12354 ||| 3882 ||| 12355 ||| 12356 ||| 12357 ||| 12358 ||| 
2019 ||| self-attention based model for punctuation prediction using word and speech embeddings. ||| 12242 ||| 12041 ||| 
2019 ||| deep recurrent neural networks with layer-wise multi-head attentions for punctuation restoration. ||| 4955 ||| 
2019 ||| self-attention networks for connectionist temporal classification in speech recognition. ||| 4739 ||| 12359 ||| 12360 ||| 
2020 ||| hka: a hierarchical knowledge attention mechanism for multi-turn dialogue system. ||| 12361 ||| 12362 ||| 12363 ||| 5259 ||| 
2021 ||| gaussian kernelized self-attention for long sequence data and its application to ctc-based speech recognition. ||| 12364 ||| 12365 ||| 3549 ||| 
2019 ||| utterance-level end-to-end language identification using attention-based cnn-blstm. ||| 12366 ||| 12367 ||| 12368 ||| 765 ||| 
2020 ||| attention-mask dense merger (attendense) deep hdr for ghost removal. ||| 12369 ||| 12370 ||| 
2019 ||| a region based attention method for weakly supervised sound event detection and classification. ||| 12371 ||| 3198 ||| 4395 ||| 12372 ||| 12357 ||| 12373 ||| 
2019 ||| multi-step self-attention network for cross-modal retrieval based on a limited text space. ||| 12374 ||| 2200 ||| 2064 ||| 
2020 ||| design-gan: cross-category fashion translation driven by landmark attention. ||| 12375 ||| 3001 ||| 12376 ||| 1856 ||| 12377 ||| 
2021 ||| memory layers with multi-head attention mechanisms for text-dependent speaker verification. ||| 12378 ||| 12379 ||| 12380 ||| 4046 ||| 12381 ||| 
2021 ||| wake word detection with streaming transformers. ||| 12382 ||| 12383 ||| 12191 ||| 12384 ||| 12194 ||| 
2020 ||| channel attention based generative network for robust visual tracking. ||| 12385 ||| 12386 ||| 1825 ||| 127 ||| 
2021 ||| developing real-time streaming transformer transducer for speech recognition on large-scale dataset. ||| 12387 ||| 2280 ||| 12388 ||| 12389 ||| 12179 ||| 
2021 ||| hcag: a hierarchical context-aware graph attention model for depression detection. ||| 12390 ||| 472 ||| 12391 ||| 12392 ||| 
2019 ||| an attention-based neural network approach for single channel speech enhancement. ||| 12393 ||| 12394 ||| 1125 ||| 12395 ||| 12384 ||| 
2020 ||| unsupervised speaker adaptation using attention-based speaker memory for end-to-end asr. ||| 12396 ||| 11980 ||| 2508 ||| 11981 ||| 
2019 ||| end-to-end language recognition using attention based hierarchical gated recurrent unit models. ||| 12397 ||| 12398 ||| 12399 ||| 
2021 ||| history utterance embedding transformer lm for speech recognition. ||| 12400 ||| 12101 ||| 12401 ||| 12104 ||| 8298 ||| 
2020 ||| high-resolution attention network with acoustic segment model for acoustic scene classification. ||| 12402 ||| 1010 ||| 4434 ||| 12403 ||| 4490 ||| 12404 ||| 
2021 ||| transformer-transducers for code-switched speech recognition. ||| 12405 ||| 12406 ||| 12407 ||| 12359 ||| 
2019 ||| spatial and channel attention based convolutional neural networks for modeling noisy speech. ||| 12408 ||| 12409 ||| 
2021 ||| end-to-end spoken language understanding using transformer networks and self-supervised pre-trained features. ||| 12410 ||| 12411 ||| 12412 ||| 12413 ||| 12414 ||| 12415 ||| 12416 ||| 
2019 ||| the speechtransformer for large-scale mandarin chinese speech recognition. ||| 12417 ||| 4634 ||| 4523 ||| 185 ||| 
2018 ||| incorporating asr errors with attention-based, jointly trained rnn for intent detection and slot filling. ||| 12418 ||| 12419 ||| 
2020 ||| transformer-based online ctc/attention end-to-end speech recognition architecture. ||| 12401 ||| 12101 ||| 12100 ||| 12104 ||| 8298 ||| 
2019 ||| exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed. ||| 12420 ||| 12421 ||| 12422 ||| 12423 ||| 
2019 ||| self-attention aligner: a latency-control end-to-end model for asr using self-attention network and chunk-hopping. ||| 5269 ||| 6832 ||| 728 ||| 
2021 ||| dense attention module for accurate pulmonary nodule detection. ||| 12424 ||| 4634 ||| 12425 ||| 12426 ||| 
2021 ||| transformer based unsupervised pre-training for acoustic representation learning. ||| 12259 ||| 12427 ||| 12258 ||| 12257 ||| 12263 ||| 11688 ||| 
2019 ||| decoupling category-wise independence and relevance with self-attention for multi-label image classification. ||| 12428 ||| 12429 ||| 2447 ||| 12430 ||| 
2021 ||| graph attention and interaction network with multi-task learning for fact verification. ||| 11453 ||| 12431 ||| 4894 ||| 
2019 ||| attention-based transfer learning for brain-computer interface. ||| 12432 ||| 1168 ||| 12433 ||| 12434 ||| 12435 ||| 
2021 ||| efficient speech emotion recognition using multi-scale cnn and attention. ||| 12436 ||| 3042 ||| 12437 ||| 12438 ||| 
2021 ||| hierarchical attention fusion for geo-localization. ||| 12439 ||| 3642 ||| 12440 ||| 12441 ||| 
2021 ||| attention enhanced spatial temporal neural network for hrrp recognition. ||| 12442 ||| 10111 ||| 
2021 ||| multitask learning and joint optimization for transformer-rnn-transducer speech recognition. ||| 12443 ||| 12444 ||| 
2020 ||| spatial attention for far-field speech recognition with deep beamforming neural networks. ||| 12445 ||| 218 ||| 12446 ||| 12447 ||| 12448 ||| 12449 ||| 
2021 ||| tabular transformers for modeling multivariate time series. ||| 12450 ||| 12451 ||| 12452 ||| 12453 ||| 12454 ||| 12455 ||| 12456 ||| 12457 ||| 12458 ||| 
2021 ||| a multi-channel temporal attention convolutional neural network model for environmental sound classification. ||| 12459 ||| 12460 ||| 12461 ||| 
2019 ||| speech emotion recognition using multi-hop attention mechanism. ||| 12462 ||| 12463 ||| 12464 ||| 10565 ||| 
2019 ||| learning to match transient sound events using attentional similarity for few-shot sound recognition. ||| 4373 ||| 12465 ||| 12466 ||| 4374 ||| 
2019 ||| adversarial examples for improving end-to-end attention-based small-footprint keyword spotting. ||| 12467 ||| 12395 ||| 12394 ||| 12468 ||| 12384 ||| 12469 ||| 12470 ||| 
2020 ||| strategic attention learning for modality translation. ||| 12471 ||| 12472 ||| 12473 ||| 6454 ||| 
2021 ||| double multi-head attention for speaker verification. ||| 12474 ||| 12475 ||| 12476 ||| 
2020 ||| non-local nested residual attention network for stereo image super-resolution. ||| 12477 ||| 12196 ||| 12478 ||| 455 ||| 11819 ||| 
2019 ||| phonemic-level duration control using attention alignment for natural speech synthesis. ||| 12479 ||| 12480 ||| 12481 ||| 12482 ||| 
2021 ||| end-to-end multi-channel transformer for speech recognition. ||| 12483 ||| 12484 ||| 12485 ||| 12486 ||| 12487 ||| 
2020 ||| transformer-based acoustic modeling for hybrid speech recognition. ||| 11973 ||| 12488 ||| 12489 ||| 12304 ||| 11979 ||| 12447 ||| 12490 ||| 12303 ||| 5450 ||| 11975 ||| 12449 ||| 12305 ||| 12491 ||| 
2021 ||| neuro-steered music source separation with eeg-based auditory attention decoding and contrastive-nmf. ||| 8061 ||| 8062 ||| 8063 ||| 8064 ||| 
2020 ||| focusing on attention: prosody transfer and adaptative optimization strategy for multi-speaker end-to-end speech synthesis. ||| 12492 ||| 12041 ||| 12244 ||| 12242 ||| 128 ||| 
2021 ||| graphspeech: syntax-aware graph attention network for neural speech synthesis. ||| 1840 ||| 12493 ||| 12494 ||| 
2019 ||| investigation of enhanced tacotron text-to-speech synthesis systems with self-attention for pitch accent language. ||| 12495 ||| 398 ||| 12496 ||| 3317 ||| 
2021 ||| an end-to-end speech accent recognition method based on hybrid ctc/attention transformer asr. ||| 2887 ||| 12427 ||| 12497 ||| 12498 ||| 
2020 ||| attention-based gated scaling adaptive acoustic model for ctc-based speech recognition. ||| 12499 ||| 4395 ||| 4463 ||| 1010 ||| 
2020 ||| all in one network for driver attention monitoring. ||| 12500 ||| 12501 ||| 12502 ||| 3248 ||| 583 ||| 12503 ||| 12504 ||| 
2020 ||| attention-based asr with lightweight and dynamic convolutions. ||| 12505 ||| 12506 ||| 12507 ||| 3549 ||| 
2021 ||| sa-net: shuffle attention for deep convolutional neural networks. ||| 12508 ||| 5865 ||| 
2021 ||| unidirectional memory-self-attention transducer for online speech recognition. ||| 12509 ||| 701 ||| 704 ||| 705 ||| 
2020 ||| selective attention encoders by syntactic graph convolutional networks for document summarization. ||| 12510 ||| 12511 ||| 7134 ||| 12512 ||| 12513 ||| 11688 ||| 
2020 ||| redundant convolutional network with attention mechanism for monaural speech enhancement. ||| 7955 ||| 12514 ||| 12515 ||| 12516 ||| 11504 ||| 8922 ||| 
2020 ||| automotive radar signal interference mitigation using rnn with self attention. ||| 12517 ||| 12518 ||| 12519 ||| 
2021 ||| patnet : a phoneme-level autoregressive transformer network for speech synthesis. ||| 12520 ||| 12521 ||| 12492 ||| 12242 ||| 12041 ||| 
2020 ||| a hybrid text normalization system using multi-head self-attention for mandarin. ||| 12522 ||| 12523 ||| 12524 ||| 399 ||| 12525 ||| 1420 ||| 4089 ||| 12526 ||| 
2021 ||| reinforcement stacked learning with semantic-associated attention for visual question answering. ||| 6585 ||| 6586 ||| 2252 ||| 2255 ||| 
2020 ||| streaming automatic speech recognition with the transformer model. ||| 11980 ||| 2508 ||| 11981 ||| 
2021 ||| multi-rate attention architecture for fast streamable text-to-speech spectrum modeling. ||| 8880 ||| 12527 ||| 12528 ||| 12529 ||| 12530 ||| 
2020 ||| generating synthetic audio data for attention-based speech recognition systems. ||| 12531 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2019 ||| a bayesian attention neural network layer for speaker recognition. ||| 12535 ||| 12536 ||| 
2021 ||| mixed precision quantization of transformer language models for speech recognition. ||| 12294 ||| 12296 ||| 12293 ||| 12299 ||| 4460 ||| 
2019 ||| atts2s-vc: sequence-to-sequence voice conversion with attention and context preservation mechanisms. ||| 12537 ||| 12538 ||| 12539 ||| 12540 ||| 
2019 ||| attention-based wavenet autoencoder for universal voice conversion. ||| 12541 ||| 1667 ||| 
2020 ||| attentional fused temporal transformation network for video action recognition. ||| 12542 ||| 12543 ||| 12544 ||| 12545 ||| 4127 ||| 4129 ||| 11390 ||| 11105 ||| 4128 ||| 
2020 ||| mixup multi-attention multi-tasking model for early-stage leukemia identification. ||| 12546 ||| 12547 ||| 12548 ||| 12549 ||| 12550 ||| 
2021 ||| capturing multi-resolution context by dilated self-attention. ||| 11980 ||| 2508 ||| 11981 ||| 
2020 ||| sequence-to-sequence singing synthesis using the feed-forward transformer. ||| 12551 ||| 12552 ||| 
2020 ||| signal-aware broadband doa estimation using attention mechanisms. ||| 12553 ||| 12554 ||| 12555 ||| 12556 ||| 12557 ||| 
2020 ||| complex transformer: a framework for modeling complex-valued sequence. ||| 12558 ||| 12559 ||| 12560 ||| 3597 ||| 3247 ||| 
2021 ||| don't shoot butterfly with rifles: multi-channel continuous speech separation with early exit transformer. ||| 12561 ||| 2280 ||| 12029 ||| 12030 ||| 12389 ||| 12562 ||| 12563 ||| 
2020 ||| gated mechanism for attention based multi modal sentiment analysis. ||| 10575 ||| 12564 ||| 
2021 ||| representation learning with spectro-temporal-channel attention for speech emotion recognition. ||| 5248 ||| 5093 ||| 12565 ||| 5095 ||| 4469 ||| 12494 ||| 
2020 ||| investigation of methods to improve the recognition performance of tamil-english code-switched data in transformer framework. ||| 12073 ||| 12072 ||| 12074 ||| 
2020 ||| attention driven fusion for multi-modal emotion recognition. ||| 12566 ||| 12567 ||| 11374 ||| 11330 ||| 11331 ||| 
2021 ||| arrhythmia classification with heartbeat-aware transformer. ||| 379 ||| 748 ||| 12568 ||| 12569 ||| 12570 ||| 
2019 ||| enhancing hybrid self-attention structure with relative-position-aware bias for speech synthesis. ||| 12571 ||| 12572 ||| 12573 ||| 12384 ||| 3808 ||| 
2020 ||| spidernet: attention network for one-shot anomaly detection in sounds. ||| 8065 ||| 12574 ||| 12575 ||| 12576 ||| 12577 ||| 12578 ||| 
2021 ||| focus on the present: a regularization method for the asr source-target attention layer. ||| 12579 ||| 12580 ||| 12581 ||| 12582 ||| 12583 ||| 
2021 ||| non-autoregressive transformer asr with ctc-enhanced decoder input. ||| 12584 ||| 3138 ||| 12585 ||| 12586 ||| 4530 ||| 12587 ||| 
2021 ||| stock movement prediction and portfolio management via multimodal learning with transformer. ||| 12215 ||| 12217 ||| 
2019 ||| co-attention network and low-rank bilinear pooling for aspect based sentiment analysis. ||| 12588 ||| 12589 ||| 12590 ||| 12591 ||| 
2019 ||| self-attention based prosodic boundary prediction for chinese speech synthesis. ||| 12592 ||| 12104 ||| 8298 ||| 
2020 ||| fast domain adaptation for goal-oriented dialogue using a hybrid generative-retrieval transformer. ||| 12593 ||| 12594 ||| 12595 ||| 12596 ||| 
2017 ||| automatic speech emotion recognition using recurrent neural networks with local attention. ||| 12597 ||| 12598 ||| 12599 ||| 
2021 ||| skip attention gan for remote sensing image synthesis. ||| 12600 ||| 1558 ||| 12601 ||| 12602 ||| 8859 ||| 
2019 ||| dynamically context-sensitive time-decay attention for dialogue modeling. ||| 4841 ||| 4842 ||| 4843 ||| 
2018 ||| fault detection using attention models based on visual saliency. ||| 12603 ||| 12604 ||| 12605 ||| 12606 ||| 12607 ||| 
2019 ||| stream attention-based multi-array end-to-end speech recognition. ||| 12608 ||| 12609 ||| 12610 ||| 2508 ||| 3549 ||| 12611 ||| 
2020 ||| transformer transducer: a streamable speech recognition model with transformer encoders and rnn-t loss. ||| 2251 ||| 12612 ||| 12613 ||| 12614 ||| 12615 ||| 12616 ||| 12617 ||| 
2018 ||| audio set classification with attention model: a probabilistic perspective. ||| 12618 ||| 1125 ||| 11418 ||| 12619 ||| 
2021 ||| an attention based wavelet convolutional model for visual saliency detection. ||| 12620 ||| 12621 ||| 
2021 ||| emformer: efficient memory transformer based acoustic model for low latency streaming speech recognition. ||| 11974 ||| 11973 ||| 11976 ||| 11978 ||| 11977 ||| 11975 ||| 12489 ||| 12622 ||| 
2021 ||| adaptive bi-directional attention: exploring multi-granularity representations for machine reading comprehension. ||| 12623 ||| 3746 ||| 7412 ||| 12624 ||| 4430 ||| 
2018 ||| effective attention mechanism in dynamic models for speech emotion recognition. ||| 12625 ||| 4366 ||| 
2020 ||| attention guided region division for crowd counting. ||| 12626 ||| 12627 ||| 12628 ||| 293 ||| 
2018 ||| spatiotemporal attention based deep neural networks for emotion recognition. ||| 12629 ||| 12630 ||| 8797 ||| 1515 ||| 
2021 ||| spatiotemporal attention for multivariate time series prediction and interpretation. ||| 12631 ||| 12632 ||| 12633 ||| 12634 ||| 12635 ||| 
2021 ||| a global-local attention framework for weakly labelled audio tagging. ||| 12636 ||| 4430 ||| 11418 ||| 
2021 ||| bidirectional focused semantic alignment attention network for cross-modal retrieval. ||| 12637 ||| 4719 ||| 12638 ||| 12639 ||| 
2020 ||| exploring a zero-order direct hmm based on latent attention for automatic speech recognition. ||| 3453 ||| 12640 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| fragmentvc: any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention. ||| 12641 ||| 12642 ||| 12643 ||| 12644 ||| 12645 ||| 
2021 ||| an attention-seq2seq model based on crnn encoding for automatic labanotation generation from motion capture data. ||| 12646 ||| 2826 ||| 12647 ||| 12648 ||| 
2018 ||| hierarchical attention and context modeling for group activity recognition. ||| 12649 ||| 12650 ||| 5704 ||| 5705 ||| 7814 ||| 
2018 ||| attention-based models for text-dependent speaker verification. ||| 12651 ||| 12652 ||| 12653 ||| 12654 ||| 
2019 ||| token-wise training for attention based end-to-end speech recognition. ||| 12655 ||| 12190 ||| 12586 ||| 3808 ||| 
2021 ||| geometric scattering attention networks. ||| 12656 ||| 12657 ||| 12658 ||| 
2020 |||  trading attention for feed-forward layers. ||| 12659 ||| 12660 ||| 12533 ||| 12534 ||| 3454 ||| 
2018 ||| forward attention in sequence- to-sequence acoustic modeling for speech synthesis. ||| 12661 ||| 4894 ||| 12372 ||| 
2021 ||| continuous-time self-attention in neural differential equation. ||| 1488 ||| 12662 ||| 
2020 ||| look globally, age locally: face aging with an attention mechanism. ||| 12663 ||| 12664 ||| 12665 ||| 12666 ||| 
2021 ||| dnanet: dense nested attention network for single image dehazing. ||| 12667 ||| 12035 ||| 12668 ||| 12669 ||| 
2020 ||| characterizing speech adversarial examples using self-attention u-net enhancement. ||| 7457 ||| 12670 ||| 12671 ||| 12672 ||| 12404 ||| 
2020 ||| audio sound determination using feature space attention based convolution recurrent neural network. ||| 12673 ||| 12674 ||| 5918 ||| 
2019 ||| scanet: spatial-channel attention network for 3d object detection. ||| 12675 ||| 12676 ||| 12677 ||| 12678 ||| 12679 ||| 11819 ||| 
2021 ||| a novel attention-based gated recurrent unit and its efficacy in speech emotion recognition. ||| 4030 ||| 4050 ||| 12680 ||| 643 ||| 648 ||| 649 ||| 
2018 ||| acoustic-to-word attention-based model complemented with character-level ctc-based model. ||| 12681 ||| 12682 ||| 12683 ||| 4418 ||| 
2021 ||| transmask: a compact and fast speech separation model based on transformer. ||| 12684 ||| 12685 ||| 12686 ||| 
2020 ||| a time-frequency network with channel attention and non-local modules for artificial bandwidth extension. ||| 12687 ||| 12688 ||| 12689 ||| 12690 ||| 2182 ||| 12691 ||| 12692 ||| 
2021 ||| hsan: a hierarchical self-attention network for multi-turn dialogue generation. ||| 12693 ||| 2037 ||| 12694 ||| 12695 ||| 
2020 ||| correction of automatic speech recognition with transformer sequence-to-sequence model. ||| 12696 ||| 12697 ||| 12698 ||| 
2021 ||| a novel end-to-end speech emotion recognition network with stacked transformer layers. ||| 12699 ||| 214 ||| 12700 ||| 12701 ||| 12702 ||| 12703 ||| 
2021 ||| query-by-example keyword spotting system using multi-head attention and soft-triple loss. ||| 12704 ||| 12705 ||| 12706 ||| 12707 ||| 
2019 ||| a joint auditory attention decoding and adaptive binaural beamforming algorithm for hearing devices. ||| 12708 ||| 12709 ||| 6514 ||| 12710 ||| 
2020 ||| an attention enhanced multi-task model for objective speech assessment in real-world environments. ||| 12711 ||| 12712 ||| 
2020 ||| enhanced non-local cascading network with attention mechanism for hyperspectral image denoising. ||| 12713 ||| 12714 ||| 6650 ||| 
2021 ||| confidence estimation for attention-based sequence-to-sequence models for speech recognition. ||| 12715 ||| 12716 ||| 9472 ||| 1717 ||| 12098 ||| 12012 ||| 12717 ||| 12099 ||| 
2021 ||| hoca: higher-order channel attention for single image super-resolution. ||| 12718 ||| 7895 ||| 12719 ||| 12720 ||| 7897 ||| 12721 ||| 
2020 ||| mockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders. ||| 12722 ||| 12723 ||| 12724 ||| 12725 ||| 12644 ||| 
2020 ||| an empirical study of transformer-based neural language model adaptation. ||| 1424 ||| 6474 ||| 12726 ||| 12490 ||| 12727 ||| 12191 ||| 12194 ||| 
2021 ||| graph attention networks for speaker verification. ||| 12728 ||| 12729 ||| 12730 ||| 12731 ||| 
2020 ||| towards decoding selective attention from single-trial eeg data in cochlear implant users based on deep neural networks. ||| 12732 ||| 12733 ||| 
2021 ||| hierarchical attention-based temporal convolutional networks for eeg-based emotion recognition. ||| 242 ||| 12734 ||| 645 ||| 647 ||| 648 ||| 649 ||| 
2020 ||| high-accuracy classification of attention deficit hyperactivity disorder with l2, 1-norm linear discriminant analysis. ||| 5522 ||| 12735 ||| 3503 ||| 12736 ||| 12737 ||| 8525 ||| 
2019 ||| perceptual quality preserving image super-resolution via channel attention. ||| 12738 ||| 12739 ||| 6328 ||| 
2018 ||| query-by-example spoken term detection using attention-based multi-hop networks. ||| 12740 ||| 12644 ||| 
2020 ||| evaluation of joint auditory attention decoding and adaptive binaural beamforming approach for hearing devices with attention switching. ||| 12708 ||| 12741 ||| 12709 ||| 6514 ||| 12710 ||| 
2020 ||| reversal no longer matters: attention-based arrhythmia detection with lead-reversal ecg data. ||| 12742 ||| 12743 ||| 5259 ||| 
2021 ||| fma-eta: estimating travel time entirely based on ffn with attention. ||| 12744 ||| 12745 ||| 12746 ||| 369 ||| 12747 ||| 12748 ||| 12749 ||| 
2020 ||| transformer vae: a hierarchical model for structure-aware and interpretable music representation learning. ||| 12750 ||| 12751 ||| 12752 ||| 12753 ||| 12754 ||| 
2019 ||| multi-attention network for thoracic disease classification and localization. ||| 12679 ||| 12678 ||| 12676 ||| 12675 ||| 11819 ||| 
2020 ||| addressing the polysemy problem in language modeling with attentional multi-sense embeddings. ||| 12755 ||| 12756 ||| 1302 ||| 3147 ||| 3151 ||| 
2018 ||| attention-based end-to-end speech recognition on voice search. ||| 12394 ||| 12757 ||| 4551 ||| 12384 ||| 
2020 ||| multimodal transformer fusion for continuous emotion recognition. ||| 12758 ||| 12041 ||| 2304 ||| 6227 ||| 12759 ||| 
2020 ||| controllable time-delay transformer for real-time punctuation prediction and disfluency detection. ||| 12760 ||| 12761 ||| 1717 ||| 8948 ||| 
2019 ||| attention-based atrous convolutional neural networks: visualisation and understanding perspectives of acoustic scenes. ||| 12762 ||| 12618 ||| 12763 ||| 12619 ||| 648 ||| 649 ||| 
2020 ||| location-relative attention mechanisms for robust long-form speech synthesis. ||| 12764 ||| 12765 ||| 12766 ||| 12767 ||| 12768 ||| 12769 ||| 12770 ||| 
2021 ||| cass-nat: ctc alignment-based single step non-autoregressive transformer for speech recognition. ||| 12771 ||| 12772 ||| 12773 ||| 705 ||| 
2021 ||| decoding music attention from "eeg headphones": a user-friendly auditory brain-computer interface. ||| 12774 ||| 12775 ||| 12776 ||| 12777 ||| 12778 ||| 12779 ||| 12780 ||| 12781 ||| 12782 ||| 12783 ||| 
2020 ||| spectrogram analysis via self-attention for realizing cross-model visual-audio generation. ||| 12784 ||| 12785 ||| 12786 ||| 12787 ||| 
2017 ||| a multiple bandwidth objective speech intelligibility estimator based on articulation index band correlations and attention. ||| 12788 ||| 
2019 ||| multi-scale attention aided multi-resolution network for human pose estimation. ||| 12789 ||| 12790 ||| 
2019 ||| gradually growing residual and self-attention based dense deep back projection network for large scale super-resolution of image. ||| 12791 ||| 12792 ||| 12793 ||| 12794 ||| 12795 ||| 12796 ||| 12797 ||| 12790 ||| 12798 ||| 
2018 ||| research on investment decision model of distribution grid project based on transformer district. ||| 12799 ||| 12800 ||| 1224 ||| 5957 ||| 12801 ||| 12802 ||| 
2019 ||| find the overwhelming transformers in power grid with an optimized clustering method. ||| 12801 ||| 12800 ||| 12803 ||| 
2021 ||| incorporating attention mechanism in enhancing classification of alzheimer's disease. ||| 12804 ||| 12805 ||| 12806 ||| 12807 ||| 12808 ||| 12809 ||| 
2021 ||| what attracts people's attention in banner advertisements? a study on banner advertisements using a human attention model. ||| 12810 ||| 12811 ||| 12812 ||| 12813 ||| 
2021 ||| a one-stage temporal detector with attentional lstm for video object detection. ||| 12814 ||| 9440 ||| 12815 ||| 12816 ||| 
2018 ||| detection for joint attention based on a multi-sensor visual system. ||| 12817 ||| 5018 ||| 12818 ||| 12819 ||| 
2021 ||| mag-net: multi-task attention guided network for brain tumor segmentation and classification. ||| 12820 ||| 12821 ||| 12822 ||| 12823 ||| 
2022 ||| towards efficient vision transformer inference: a first study of transformers on mobile devices. ||| 12824 ||| 12825 ||| 602 ||| 12826 ||| 
2020 ||| pair-wise convolution network with transformers for sequential recommendation. ||| 12827 ||| 291 ||| 292 ||| 
2019 ||| change detection in synthetic aperture radar images based on convolutional block attention module. ||| 952 ||| 6810 ||| 2628 ||| 12828 ||| 
2022 |||  sur transformers. ||| 12829 ||| 12830 ||| 12831 ||| 
2020 ||| can i get your (robot) attention? human sensitivity to subtle hints of human-likeness in a humanoid robot's behavior. ||| 12832 ||| 12833 ||| 12834 ||| 12835 ||| 10259 ||| 
2017 ||| goal-directed deployment of attention in a computational model: a study in multiple-object tracking. ||| 12836 ||| 2668 ||| 2666 ||| 
2017 ||| children's eeg indices of directed attention during somatosensory anticipation: relations with executive function. ||| 12837 ||| 12838 ||| 
2020 ||| may i have your attention? testing a subjective attention scale. ||| 12839 ||| 
2018 ||| how do pragmatic and object cues affect monolingual and bilingual toddlers' visual attention during word learning? ||| 12840 ||| 12841 ||| 12842 ||| 
2018 ||| effects of illustration details on attention and comprehension in beginning readers. ||| 12843 ||| 12844 ||| 12845 ||| 12846 ||| 
2018 ||| dimension-based attention in learning and understanding spoken language. ||| 12847 ||| 12848 ||| 12849 ||| 12850 ||| 12775 ||| 
2017 ||| evidence for overt visual attention to hand gestures as a function of redundancy and speech disfluency. ||| 12851 ||| 12852 ||| 
2017 ||| exploitative and exploratory attention in a four-armed bandit task. ||| 12853 ||| 12854 ||| 12855 ||| 
2017 ||| cognitive and attentional process in insight problem solving of the puzzle game "tangram". ||| 12856 ||| 
2018 ||| the impact of gesture and prior knowledge on visual attention during math instruction. ||| 12857 ||| 12858 ||| 12859 ||| 12860 ||| 12861 ||| 12862 ||| 
2017 ||| a computational model of the role of attention in subitizing and enumeration. ||| 10287 ||| 2668 ||| 2666 ||| 
2019 ||| movements and visuospatial working memory: examining the role of movement and attention to movement. ||| 12863 ||| 12864 ||| 12865 ||| 
2018 ||| computational model of spatial auditory attention in act-r. ||| 9123 ||| 9125 ||| 9124 ||| 9126 ||| 
2018 ||| how to open the "window of attention" in serial verb constructions. ||| 12866 ||| 
2019 ||| book design, attention, and reading performance: current practices and opportunities for optimization. ||| 12844 ||| 12843 ||| 12867 ||| 12846 ||| 
2020 ||| investigation of attentional decay: implications for instruction. ||| 12844 ||| 12868 ||| 
2018 |||  personalized attention tasks for children with developmental disorders. ||| 12869 ||| 12870 ||| 
2017 ||| computational modeling of auditory spatial attention. ||| 9126 ||| 9124 ||| 9123 ||| 9125 ||| 
2019 ||| look out, it's going to fall!: does physical instability capture attention and lead to distraction? ||| 12871 ||| 12872 ||| 12873 ||| 12874 ||| 
2017 ||| executive function and attention predict low-income preschoolers' active category learning. ||| 12875 ||| 12876 ||| 
2018 ||| object-based attention in multiple frames of reference. ||| 12877 ||| 12878 ||| 12879 ||| 12880 ||| 8843 ||| 12881 ||| 
2020 ||| does looking time predict choice in domestic dogs? examining visual attention in man's best friend. ||| 12882 ||| 12883 ||| 12884 ||| 
2020 ||| covert attention shift by sequence-space synesthesia (sss): a cognitive grammar approach. ||| 12885 ||| 12886 ||| 
2017 ||| effects of attention to emergent phenomena on rule discovery. ||| 12887 ||| 12888 ||| 12889 ||| 12890 ||| 12891 ||| 
2018 ||| an attention-driven computational model of human causal reasoning. ||| 2666 ||| 12836 ||| 10287 ||| 2667 ||| 
2017 ||| a model-based approach for assessing attentional biases in people with depressive symptoms. ||| 12892 ||| 12893 ||| 12894 ||| 12895 ||| 
2020 ||| better together: exploration prior to instruction facilitates rule-learning and modifies attention to demonstration. ||| 12896 ||| 12897 ||| 12898 ||| 12899 ||| 
2020 ||| do environmental resource distributions affect attentional styles? ||| 12900 ||| 12901 ||| 
2018 ||| rapid learning in early attentional processing: bayesian estimation of trial-by-trial updating. ||| 12902 ||| 12903 ||| 12904 ||| 12905 ||| 
2020 ||| striatal and cortical components of inattentional responses: an experimental and computational study of thewisconsin card sorting test in adults with adhd traits. ||| 12906 ||| 12907 ||| 
2020 ||| a large-scale analysis of attentional deployment across one hundred sessions of adaptive multitask training. ||| 12908 ||| 
2019 ||| does children's shape knowledge contribute to age-related improvements in selective sustained attention measured in a trackit task? ||| 12909 ||| 12910 ||| 12911 ||| 12846 ||| 
2020 ||| using the trackit task to measure the development of selective sustained attention in children ages 2-7. ||| 12909 ||| 12910 ||| 12912 ||| 12911 ||| 12846 ||| 
2017 ||| connecting stimulus-driven attention to the properties of infant-directed speech - is exaggerated intonation also more surprising? ||| 12913 ||| 7111 ||| 10678 ||| 12914 ||| 12915 ||| 
2020 ||| dynamics of spatio-temporal scope of attention: temporal correlations in reaction time data. ||| 12916 ||| 12917 ||| 
2018 ||| hand-eye coordination and visual attention in infancy. ||| 12918 ||| 12919 ||| 5430 ||| 4997 ||| 
2020 ||| does the effect of labels on sustained attention depend on target familiarity? ||| 12909 ||| 12920 ||| 12910 ||| 12921 ||| 12846 ||| 
2017 ||| attention modulation effects on visual feature-selectivity of neurons in brain-inspired categorization models. ||| 12922 ||| 12923 ||| 12924 ||| 12925 ||| 
2018 ||| contingent responsiveness in digital storybooks: effects on children's comprehension and the role of individual differences in attention. ||| 12843 ||| 12926 ||| 12911 ||| 
2020 ||| attentional allocation as optimal compression in visual search. ||| 12927 ||| 12928 ||| 
2018 ||| contextual separation shifts attentional biases. ||| 12929 ||| 12841 ||| 
2017 ||| variables involved in selective sustained attention development: advances in measurement. ||| 12910 ||| 12930 ||| 12911 ||| 12846 ||| 
2018 ||| attention selectively boosts learning of statistical structure. ||| 12931 ||| 12932 ||| 
2018 ||| a dynamic neural field model of memory, attention and cross-situational word learning. ||| 12933 ||| 12934 ||| 12935 ||| 
2018 ||| enumeration by pattern recognition requires attention: evidence against immediate holistic processing of canonical patterns. ||| 10287 ||| 12936 ||| 2666 ||| 
2018 ||| the 'goldilocks effect' in preschoolers' attention to spoken language. ||| 12937 ||| 12938 ||| 12939 ||| 
2017 ||| numbers uniquely bias spatial attention: a novel paradigm for understanding spatial-numerical associations. ||| 12940 ||| 12941 ||| 12942 ||| 
2018 ||| measuring attention control abilities with a gaze following antisaccade paradigm. ||| 12943 ||| 12944 ||| 
2019 ||| "give me a break": can brief bouts of physical activity reduce elementary children's attentional failures and improve learning? ||| 12867 ||| 12844 ||| 
2020 ||| how to navigate everyday distractions: leveraging optimal feedback to train attention control. ||| 12945 ||| 12946 ||| 12947 ||| 12948 ||| 12949 ||| 12950 ||| 12951 ||| 12952 ||| 
2020 ||| attentional competition in genuine classrooms: analysis of the classroom visual environment. ||| 12844 ||| 12953 ||| 12954 ||| 12846 ||| 
2020 ||| nonlinear probability weighting can reflect attentional biases in sequential sampling. ||| 12955 ||| 12956 ||| 
2017 ||| it's time: quantifying the relevant timescales for joint attention. ||| 12918 ||| 5430 ||| 4997 ||| 
2020 ||| visual attention during e-learning: eye-tracking shows that making salient areas more prominent helps learning in online tutors. ||| 12957 ||| 12958 ||| 12959 ||| 
2019 ||| measuring selective sustained attention in children with trackit and eyetracking. ||| 12910 ||| 12960 ||| 12909 ||| 12911 ||| 12846 ||| 
2018 ||| inferring attention through cursor trajectories. ||| 12961 ||| 12962 ||| 12963 ||| 
2017 ||| children's attention to semantic content versus emotional tone: differences between two cultural groups. ||| 11466 ||| 1052 ||| 6627 ||| 
2018 ||| beyond principles and outcomes: children determine fairness based on attention and exactness. ||| 12964 ||| 12965 ||| 12966 ||| 
2019 ||| a computational model of feature formation, event prediction, and attention switching. ||| 12967 ||| 12968 ||| 
2020 ||| staying and returning dynamics of sustained attention in young children. ||| 12910 ||| 12960 ||| 12911 ||| 12846 ||| 
2017 ||| seeing is not enough for sustained visual attention. ||| 12969 ||| 12970 ||| 4997 ||| 5430 ||| 
2020 ||| an evidence accumulation model of motivational and developmental influences over sustained attention. ||| 12971 ||| 12972 ||| 12973 ||| 12974 ||| 12951 ||| 
2019 ||| attentional capture: modeling automatic mechanisms and top-down control. ||| 12836 ||| 2668 ||| 2666 ||| 
2020 ||| the impact of semantic versus perceptual attention on memory representation. ||| 12975 ||| 12976 ||| 12977 ||| 
2018 ||| spatial language and visual attention: a new approach to test linguistic relativity. ||| 12978 ||| 12979 ||| 12980 ||| 
2019 ||| predicting learned inattention from attentional selectivity and optimization. ||| 12981 ||| 12982 ||| 
2018 ||| tuning to the task at hand: processing goals shape adults' attention to unfolding activity. ||| 12983 ||| 12984 ||| 
2020 ||| visual attention and real-world decision making: sharing photos on social media. ||| 12985 ||| 12986 ||| 12987 ||| 12988 ||| 12989 ||| 
2019 ||| individual differences in bodily attention: variability in anticipatory mu rhythm power is associated with executive function abilities and processing speed. ||| 12837 ||| 12990 ||| 12838 ||| 
2018 ||| sign language experience affects comprehension and attention to gesture. ||| 12991 ||| 12992 ||| 12862 ||| 12899 ||| 
2018 ||| exploration and attention in young children. ||| 12981 ||| 12982 ||| 
2017 ||| bottom-up attentional cueing in category learning in children. ||| 12981 ||| 12982 ||| 
2018 ||| sequences of discrete attentional shifts emerge from a neural dynamic architecture for conjunctive visual search that operates in continuous time. ||| 12993 ||| 12994 ||| 12995 ||| 12996 ||| 12997 ||| 12998 ||| 12149 ||| 
2020 ||| online article comprehension in monolingual spanish-speaking preschoolers with specific language impairment: a language-mediated visual attention study. ||| 12999 ||| 13000 ||| 13001 ||| 13002 ||| 
2017 ||| perspective-taking in referential communication: does stimulated attention to addressee's perspective influence speakers' reference production? ||| 13003 ||| 13004 ||| 13005 ||| 13006 ||| 
2019 ||| visual spatial attention skills and holistic processing in high school students with and without dyslexia. ||| 13007 ||| 13008 ||| 13009 ||| 13010 ||| 
2017 ||| predicting preschool-aged children's behavior regulation from attention tasks in the lab. ||| 13011 ||| 13012 ||| 13013 ||| 12904 ||| 
2019 ||| effects of instructor presence in video lectures: rapport, attention, and learning. ||| 13014 ||| 13015 ||| 13016 ||| 13017 ||| 13018 ||| 
2018 ||| modeling morphological affixation with interpretable recurrent networks: sequential rebinding controlled by hierarchical attention. ||| 13019 ||| 
2019 ||| novel labels modify visual attention in 2-year-old children. ||| 13020 ||| 12859 ||| 13021 ||| 
2020 ||| does children's visual attention to objects influence their verb learning? ||| 13022 ||| 13023 ||| 13024 ||| 13025 ||| 13026 ||| 
2020 ||| examining sustained attention in child-parent interaction: a comparative study of typically developing children and children with autism spectrum disorder. ||| 13027 ||| 13028 ||| 13029 ||| 13030 ||| 13031 ||| 13032 ||| 13033 ||| 4997 ||| 
2018 ||| changing minds: the effect of stimulated attention to another's different point of view on visual perspective-taking. ||| 13003 ||| 13005 ||| 13004 ||| 13006 ||| 
2019 ||| a re-examination of the interrelationships between attention, eye behavior, and creative thought. ||| 13034 ||| 13035 ||| 13036 ||| 
2018 ||| evidence that the attention blink reflects categorical perceptual dynamics. ||| 13037 ||| 13038 ||| 
2018 ||| wiggleometer: measuring selective sustained attention in children. ||| 12844 ||| 12846 ||| 
2019 ||| exploring the role of social priming in alcohol attentional bias. ||| 13039 ||| 13040 ||| 
2019 ||| parametric control of distractor-oriented attention. ||| 12971 ||| 12951 ||| 
2019 ||| the impact of speech complexity on preschooler attention, speaker preference, and learning. ||| 12937 ||| 12939 ||| 12938 ||| 
2019 ||| controlling attention in a memory-augmented neural network to solve working memory tasks. ||| 9298 ||| 13041 ||| 9300 ||| 13042 ||| 13043 ||| 9302 ||| 
2019 ||| hands in mind: learning to write with both hands improves inhibitory control, but not attention. ||| 13044 ||| 13045 ||| 13046 ||| 13047 ||| 13048 ||| 
2017 ||| brief mindfulness meditation improves attention in novices. ||| 13049 ||| 13050 ||| 
2020 ||| examining a developmental pathway of early word learning: from qualitative characteristics of parent speech, to sustained attention, to vocabulary size. ||| 13051 ||| 4997 ||| 
2020 ||| the attentional demands of learning by doing: a developmental study. ||| 12844 ||| 13052 ||| 
2018 ||| understanding attentional selectivity, flexibility, and stability: a dynamic neural field model predicts behavior in 3- and 4-year-olds. ||| 13053 ||| 13054 ||| 13055 ||| 
2019 ||| numerosity capture of attention. ||| 13056 ||| 13057 ||| 
2019 ||| distinguishing learned categorical perception from selective attention to a dimension: preliminary evidence from a new method. ||| 13058 ||| 13059 ||| 13060 ||| 13061 ||| 13062 ||| 
2020 ||| do language effects on attention persist in complex task contexts? ||| 13063 ||| 13064 ||| 
2020 ||| self-reference effect for faces is mediated by attention. ||| 13065 ||| 12916 ||| 
2019 ||| inattentional blindness in visual search. ||| 13066 ||| 13067 ||| 13068 ||| 
2018 ||| coupling dynamical and connectionist models: representation of spatial attention via learned deictic gestures in human-robot interaction. ||| 13069 ||| 12934 ||| 13070 ||| 
2021 ||| novel design and simulation of heric transformerless pv inverter in matlab/simulink. ||| 13071 ||| 13072 ||| 13073 ||| 13074 ||| 13075 ||| 
2018 ||| automatic text summarization using customizable fuzzy features and attention on the context and vocabulary. ||| 13076 ||| 13077 ||| 13078 ||| 13079 ||| 
2019 ||| composing services in 5g-transformer. ||| 6088 ||| 6087 ||| 6092 ||| 6091 ||| 4046 ||| 6093 ||| 6096 ||| 
2018 ||| attention-based neural network for joint diarization and speaker extraction. ||| 13080 ||| 13081 ||| 1487 ||| 
2018 ||| internet of tangibles: exploring the interaction-attention continuum. ||| 13082 ||| 10292 ||| 10293 ||| 13083 ||| 13084 ||| 13085 ||| 
2021 ||| assessing the effectiveness of multilingual transformer-based text embeddings for named entity recognition in portuguese. ||| 13086 ||| 13087 ||| 13088 ||| 13089 ||| 7033 ||| 
2018 ||| research on human-robot interaction security strategy of movement authorization for service robot based on people's attention monitoring. ||| 13090 ||| 2614 ||| 13091 ||| 13092 ||| 13093 ||| 
2019 ||| statcom evaluation in electrified railway using v/v and scott power transformers. ||| 13094 ||| 13095 ||| 2871 ||| 13096 ||| 1994 ||| 13097 ||| 13098 ||| 
2021 ||| acgvd: vulnerability detection based on comprehensive graph via graph neural network with attention. ||| 12646 ||| 13099 ||| 13100 ||| 13101 ||| 13102 ||| 13103 ||| 
2019 ||| a character-level bigru-attention for phishing classification. ||| 13104 ||| 13105 ||| 13106 ||| 13107 ||| 13108 ||| 
2021 ||| convolutional recurrent neural network with attention gates for real-time single-channel speech enhancement. ||| 13109 ||| 13110 ||| 7582 ||| 4406 ||| 
2017 ||| study on suppressing harmonic flux density of converter transformer core under the dc bias. ||| 13111 ||| 13112 ||| 13113 ||| 2424 ||| 
2021 ||| diabetic retinopathy detection using cnn, transformer and mlp based architectures. ||| 13114 ||| 13115 ||| 
2021 ||| towards an attention-based accurate intrusion detection approach. ||| 13116 ||| 13117 ||| 13118 ||| 13119 ||| 
2021 ||| h-bert: enhancing chinese pretrained models with attention to hownet. ||| 3394 ||| 
2021 ||| matching with transformers in melt. ||| 13120 ||| 13121 ||| 13122 ||| 
2019 ||| easy web api development with sparql transformer. ||| 13123 ||| 13124 ||| 13125 ||| 13126 ||| 13127 ||| 13128 ||| 13129 ||| 
2019 ||| pretrained transformers for simple question answering over knowledge graphs. ||| 13130 ||| 13131 ||| 3581 ||| 
2019 ||| explaining customer activation with deep attention models. ||| 13132 ||| 13133 ||| 13134 ||| 
2019 ||| designing user-adaptive information dashboards: considering limited attention and working memory. ||| 13135 ||| 13136 ||| 
2019 ||| the impact of using it artefacts on organizational attention: the case of a city hall. ||| 13137 ||| 13138 ||| 
2019 ||| unfolding the clickbait: a siren's call in the attention economy. ||| 13139 ||| 13140 ||| 13141 ||| 
2018 ||| designing attention-aware business intelligence and analytics dashboards to support task resumption. ||| 13135 ||| 13136 ||| 13142 ||| 
2020 ||| do transformers dream of inference, or can pretrained generative models learn implicit inferential rules? ||| 13143 ||| 13144 ||| 
2020 ||| on task-level dialogue composition of generative transformer model. ||| 13145 ||| 13146 ||| 13147 ||| 
2020 ||| sentiment analysis for software engineering: how far can pre-trained transformer models go? ||| 1349 ||| 13148 ||| 13149 ||| 13150 ||| 13151 ||| 3897 ||| 
2018 ||| recurrent attention for deep neural object detection. ||| 13152 ||| 13153 ||| 
2020 ||| drug-drug interaction classification using attention based neural networks. ||| 13154 ||| 13155 ||| 
2018 ||| dialog state tracking for unseen values using an extended attention mechanism. ||| 13156 ||| 2328 ||| 13157 ||| 13158 ||| 
2018 ||| attention based joint model with negative sampling for new slot values recognition. ||| 13159 ||| 13160 ||| 13161 ||| 13162 ||| 13163 ||| 13164 ||| 
2020 ||| dialog state tracking with incorporation of target values in attention models. ||| 13156 ||| 2328 ||| 13165 ||| 13157 ||| 
2018 ||| hierarchical feature fusion with text attention for multi-scale text detection. ||| 859 ||| 4430 ||| 13166 ||| 
2018 ||| information distance based self-attention-bgru layer for end-to-end speech recognition. ||| 13167 ||| 13168 ||| 13169 ||| 13170 ||| 
2020 ||| mlab-bilstm: online web attack detection via attention-based deep neural networks. ||| 13171 ||| 13172 ||| 13173 ||| 
2019 ||| verifying asynchronous event-driven programs using partial abstract transformers. ||| 13174 ||| 13175 ||| 13176 ||| 
2017 ||| drug-drug interaction extraction via recurrent neural network with multiple attention layers. ||| 13177 ||| 11677 ||| 11676 ||| 11395 ||| 7779 ||| 13178 ||| 13179 ||| 
2020 ||| cross product and attention based deep neural collaborative filtering. ||| 13180 ||| 8636 ||| 4398 ||| 379 ||| 
2019 ||| damtrnn: a delta attention-based multi-task rnn for intention recognition. ||| 11094 ||| 11095 ||| 1200 ||| 13181 ||| 13182 ||| 
2021 ||| stct: spatial-temporal conv-transformer network for cardiac arrhythmias recognition. ||| 13183 ||| 11094 ||| 11095 ||| 13184 ||| 13185 ||| 
2018 ||| event extraction with deep contextualized word representation and multi-attention layer. ||| 13186 ||| 843 ||| 
2020 ||| interprocedural shape analysis using separation logic-based transformer summaries. ||| 13187 ||| 13188 ||| 13189 ||| 
2017 ||| a new abstraction framework for affine transformers. ||| 13190 ||| 13191 ||| 
2020 ||| improving auto-encoder novelty detection using channel attention and entropy minimization. ||| 13192 ||| 13193 ||| 13194 ||| 13195 ||| 13196 ||| 
2021 ||| attention-based dual-branches localization network for weakly supervised object localization. ||| 13197 ||| 13198 ||| 13199 ||| 
2019 ||| dense attention network for facial expression recognition in the wild. ||| 778 ||| 13200 ||| 13201 ||| 13202 ||| 
2021 ||| deep adaptive attention triple hashing. ||| 8611 ||| 13203 ||| 11215 ||| 13204 ||| 13205 ||| 
2020 ||| integrating aspect-aware interactive attention and emotional position-aware for multi-aspect sentiment analysis. ||| 13206 ||| 13207 ||| 13208 ||| 6578 ||| 571 ||| 13209 ||| 
2020 ||| attention feature matching for weakly-supervised video relocalization. ||| 13210 ||| 13211 ||| 13208 ||| 13212 ||| 13213 ||| 
2021 ||| score transformer: generating musical score from note-level representation. ||| 13214 ||| 
2020 ||| scene graph generation via multi-relation classification and cross-modal attention coordinator. ||| 13215 ||| 369 ||| 9579 ||| 13216 ||| 11466 ||| 
2021 ||| local self-attention on fine-grained cross-media retrieval. ||| 5187 ||| 498 ||| 13217 ||| 1680 ||| 
2021 ||| hard-boundary attention network for nuclei instance segmentation. ||| 13218 ||| 13219 ||| 13220 ||| 13221 ||| 1037 ||| 
2019 ||| multi-label image classification with attention mechanism and graph convolutional networks. ||| 13222 ||| 13223 ||| 
2021 ||| focusing attention across multiple images for multimodal event detection. ||| 6852 ||| 5536 ||| 13224 ||| 275 ||| 
2019 ||| attention-aware feature pyramid ordinal hashing for image retrieval. ||| 13225 ||| 13226 ||| 13227 ||| 
2020 ||| graph convolution network with node feature optimization using cross attention for few-shot learning. ||| 5439 ||| 13228 ||| 13229 ||| 
2019 ||| selective attention network for image dehazing and deraining. ||| 13230 ||| 13231 ||| 8536 ||| 
2020 ||| attention-constraint facial expression recognition. ||| 13232 ||| 
2020 ||| table detection and cell segmentation in online handwritten documents with graph attention networks. ||| 5439 ||| 5538 ||| 13233 ||| 13234 ||| 779 ||| 
2020 ||| motion-transformer: self-supervised pre-training for skeleton-based action recognition. ||| 13235 ||| 13236 ||| 8977 ||| 2315 ||| 
2020 ||| multi-level expression guided attention network for referring expression comprehension. ||| 275 ||| 11466 ||| 9579 ||| 2067 ||| 13237 ||| 
2018 ||| size-invariant attention accuracy metric for image captioning with high-resolution residual attention. ||| 601 ||| 603 ||| 602 ||| 604 ||| 
2021 ||| attention-based long-term modeling for deep visual odometry. ||| 13238 ||| 5316 ||| 13239 ||| 5018 ||| 
2017 ||| recurrent highway networks with attention mechanism for scene text recognition. ||| 6533 ||| 13240 ||| 13241 ||| 13242 ||| 1796 ||| 
2017 ||| attention to the scale: deep multi-scale salient object detection. ||| 875 ||| 2240 ||| 1717 ||| 4393 ||| 
2019 ||| ogaze: gaze prediction in egocentric videos for attentional object selection. ||| 13243 ||| 13244 ||| 13245 ||| 13246 ||| 13247 ||| 13248 ||| 4196 ||| 
2021 ||| ear-net: error attention refining network for retinal vessel segmentation. ||| 1224 ||| 13249 ||| 13250 ||| 11311 ||| 11312 ||| 
2018 ||| convolutional 3d attention network for video based freezing of gait recognition. ||| 13251 ||| 5018 ||| 13252 ||| 13253 ||| 
2019 ||| multi-pooling attention learning for melanoma recognition. ||| 13254 ||| 13239 ||| 13255 ||| 
2020 ||| a-deeppixbis: attentional angular margin for face anti-spoofing. ||| 13256 ||| 13257 ||| 13258 ||| 13259 ||| 13260 ||| 13261 ||| 
2020 ||| evolutionary attention network for medical image segmentation. ||| 13262 ||| 13263 ||| 13264 ||| 
2019 ||| bi-san-cap: bi-directional self-attention for image captioning. ||| 5324 ||| 5325 ||| 5326 ||| 5327 ||| 5328 ||| 
2020 ||| data augmentation for transformer-based g2p. ||| 13265 ||| 13266 ||| 
2020 ||| grapheme-to-phoneme conversion with a multilingual transformer model. ||| 13267 ||| 13268 ||| 
2020 ||| one model to pronounce them all: multilingual grapheme-to-phoneme conversion with a transformer ensemble. ||| 13269 ||| 3152 ||| 13270 ||| 
2021 ||| m3d-cam - a pytorch library to generate 3d attention maps for medical deep learning. ||| 13271 ||| 13272 ||| 8048 ||| 13273 ||| 13274 ||| 
2021 ||| ultrasound breast lesion detection using extracted attention maps from a weakly supervised convolutional neural network. ||| 13275 ||| 13276 ||| 13277 ||| 13278 ||| 13279 ||| 
2020 ||| sos syphilis: smartphone application for the mapping of syphilis attention networks. ||| 13280 ||| 13281 ||| 13282 ||| 13283 ||| 13284 ||| 13285 ||| 13286 ||| 
2020 ||| two computational models for analyzing political attention in social media. ||| 13287 ||| 13288 ||| 13289 ||| 
2021 ||| ceam: the effectiveness of cyclic and ephemeral attention models of user behavior on social platforms. ||| 13290 ||| 13291 ||| 13292 ||| 13293 ||| 3400 ||| 11765 ||| 13294 ||| 
2021 ||| sudden attention shifts on wikipedia during the covid-19 crisis. ||| 13295 ||| 13296 ||| 13297 ||| 13298 ||| 13299 ||| 13300 ||| 
2020 ||| the effects of an informational intervention on attention to anti-vaccination content on youtube. ||| 13301 ||| 13302 ||| 13303 ||| 13304 ||| 13305 ||| 13306 ||| 
2017 ||| what gets media attention and how media attention evolves over time: large-scale empirical evidence from 196 countries. ||| 9045 ||| 9044 ||| 
2017 ||| why do men get more attention? exploring factors behind success in an online design community. ||| 13307 ||| 13308 ||| 3369 ||| 13309 ||| 59 ||| 7111 ||| 13310 ||| 13311 ||| 13312 ||| 
2021 ||| exercise? i thought you said 'extra fries': leveraging sentence demarcations and multi-hop attention for meme affect analysis. ||| 13313 ||| 3836 ||| 3835 ||| 
2018 ||| couplenet: paying attention to couples with coupled attention for relationship recommendation. ||| 1398 ||| 9015 ||| 9016 ||| 
2020 ||| characterizing collective attention via descriptor context: a case study of public discussions of crisis events. ||| 13314 ||| 13315 ||| 13316 ||| 
2018 ||| sustained attention driving task analysis based on recurrent residual neural network using eeg data. ||| 13317 ||| 13318 ||| 10962 ||| 13319 ||| 13320 ||| 
2021 ||| hierarchical fuzzy graph attention network for group recommendation. ||| 13321 ||| 2251 ||| 368 ||| 
2021 ||| fuzzy explainable attention-based deep active learning on mental-health data. ||| 676 ||| 677 ||| 678 ||| 
2019 ||| sepsis prediction: an attention-based interpretable approach. ||| 13322 ||| 13323 ||| 
2019 ||| hierarchical attention-based fuzzy neural network for subject classification of power customer service work orders. ||| 13324 ||| 13325 ||| 13326 ||| 13327 ||| 
2019 ||| fuzzattention on session-based recommender system. ||| 13328 ||| 13329 ||| 
2019 ||| aleap: attention-based lstm with event embedding for attack projection. ||| 13330 ||| 13331 ||| 13332 ||| 13333 ||| 13334 ||| 13335 ||| 13336 ||| 
2020 ||| att: a fault-tolerant reram accelerator for attention-based neural networks. ||| 13337 ||| 13338 ||| 12196 ||| 13339 ||| 13340 ||| 
2021 ||| aidetectorx: a vulnerability detector based on tcn and self-attention mechanism. ||| 13341 ||| 1748 ||| 13342 ||| 13343 ||| 13344 ||| 
2018 ||| analyzing distribution transformers at city scale and the impact of evs and storage. ||| 13345 ||| 13346 ||| 9508 ||| 13347 ||| 
2020 ||| convolutional network with densely backward attention for facial expression recognition. ||| 13348 ||| 13349 ||| 13350 ||| 13351 ||| 
2019 ||| dilated lstm with attention for classification of suicide notes. ||| 13352 ||| 13353 ||| 13354 ||| 531 ||| 
2018 ||| patient risk assessment and warning symptom detection using deep attention-based neural networks. ||| 13355 ||| 13356 ||| 13357 ||| 13358 ||| 13359 ||| 13360 ||| 13361 ||| 13362 ||| 
2019 ||| ontological attention ensembles for capturing semantic concepts in icd code prediction from clinical text. ||| 2227 ||| 13363 ||| 13364 ||| 13365 ||| 13366 ||| 13367 ||| 13368 ||| 13369 ||| 13370 ||| 
2020 ||| multitask learning of negation and speculation using transformers. ||| 13371 ||| 13372 ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| 13373 ||| 3430 ||| 13374 ||| 
2020 ||| from genome to phenome: predicting multiple cancer phenotypes based on somatic genomicalterations via the genomic impact transformer. ||| 13375 ||| 13376 ||| 3246 ||| 13377 ||| 
2020 ||| multilevel self-attention model and its use on medical risk prediction. ||| 13378 ||| 13379 ||| 13380 ||| 13381 ||| 13382 ||| 748 ||| 
2020 ||| compressed-transformer: distilling knowledge from transformer for neural machine translation. ||| 13383 ||| 2238 ||| 
2018 ||| multi-attention network for sentiment analysis. ||| 13384 ||| 13385 ||| 3748 ||| 13386 ||| 
2020 ||| character-level transformer-based neural machine translation. ||| 13387 ||| 13388 ||| 13389 ||| 
2019 ||| effect of attention adaptive personal audio deliverable system on digital signage. ||| 13390 ||| 13391 ||| 13392 ||| 13393 ||| 13394 ||| 
2019 ||| human motion denoising using attention-based bidirectional recurrent neural network. ||| 13395 ||| 13396 ||| 13397 ||| 
2019 ||| structure-aware image expansion with global attention. ||| 13398 ||| 6738 ||| 13399 ||| 
2020 ||| covr: co-located virtual reality experience sharing for facilitating joint attention via projected view of hmd users. ||| 13400 ||| 13401 ||| 13402 ||| 13403 ||| 13404 ||| 
2021 ||| occlusion robust part-aware object classification through part attention and redundant features suppression. ||| 13405 ||| 13406 ||| 
2021 ||| learning english to chinese character: calligraphic art production based on transformer. ||| 13407 ||| 340 ||| 13408 ||| 
2020 ||| dmcr-gan: adversarial denoising for monte carlo renderings with residual attention networks and hierarchical features modulation of auxiliary buffers. ||| 13409 ||| 13410 ||| 1040 ||| 
2018 ||| single shot attention-based face detector. ||| 13411 ||| 13412 ||| 13413 ||| 337 ||| 338 ||| 
2021 ||| wavelet-based face inpainting with channel relation attention. ||| 13414 ||| 13415 ||| 
2018 ||| attention detection by learning hierarchy feature fusion on eye movement. ||| 3072 ||| 13416 ||| 2355 ||| 13417 ||| 13418 ||| 13419 ||| 
2021 ||| multi-lingual hybrid handwritten signature recognition based on deep residual attention network. ||| 13420 ||| 13421 ||| 13422 ||| 13423 ||| 13424 ||| 
2021 ||| a deep attention transformer network for pain estimation with facial expression video. ||| 13425 ||| 13426 ||| 
2021 ||| one-class face anti-spoofing based on attention auto-encoder. ||| 13427 ||| 13428 ||| 13429 ||| 
2021 ||| an improved finger vein recognition model with a residual attention mechanism. ||| 13430 ||| 13431 ||| 13432 ||| 1266 ||| 13433 ||| 
2021 ||| attention network with gmm based feature for asv spoofing detection. ||| 13434 ||| 13435 ||| 13436 ||| 13437 ||| 
2019 ||| global and local spatial-attention network for isolated gesture recognition. ||| 13438 ||| 13439 ||| 13440 ||| 13441 ||| 13442 ||| 338 ||| 13443 ||| 13444 ||| 
2021 ||| rect: a recursive transformer architecture for generalizable mathematical reasoning. ||| 13445 ||| 13446 ||| 13447 ||| 
2020 ||| dacnn: dynamic weighted attention with multi-channel convolutional neural network for emotion recognition. ||| 13448 ||| 600 ||| 
2019 ||| attention based stack resnet for citywide traffic accident prediction. ||| 13449 ||| 
2019 ||| clustering noisy trajectories via robust deep attention auto-encoders. ||| 3248 ||| 13450 ||| 13451 ||| 13452 ||| 5187 ||| 13453 ||| 
2021 ||| dual sequence transformer for query-based interactive recommendation. ||| 13454 ||| 13455 ||| 1167 ||| 8608 ||| 13456 ||| 13457 ||| 9066 ||| 13458 ||| 
2019 ||| sequence-aware recommendation with long-term and short-term attention memory networks. ||| 13459 ||| 3248 ||| 13460 ||| 
2020 ||| attention based caption augmented w2vv++ adhoc video search (avs) trecvid task. ||| 7285 ||| 12790 ||| 13461 ||| 
2020 ||| a multimodal fusion model based on hybrid attention mechanism for gesture recognition. ||| 13462 ||| 13463 ||| 13464 ||| 13465 ||| 
2020 ||| graph transformer: learning better representations for graph neural networks. ||| 13466 ||| 13467 ||| 13468 ||| 13469 ||| 
2020 ||| selecting features from time series using attention-based recurrent neural networks. ||| 13470 ||| 13471 ||| 13472 ||| 
2020 ||| predicting polypharmacy side effects through a relation-wise graph attention network. ||| 13473 ||| 13474 ||| 13475 ||| 13476 ||| 13477 ||| 
2021 ||| channel estimation for full-duplex ris-assisted haps backhauling with graph attention networks. ||| 13478 ||| 13479 ||| 2101 ||| 13480 ||| 13481 ||| 13482 ||| 13483 ||| 
2021 ||| neighboring-aware caching in heterogeneous edge networks by actor-attention-critic learning. ||| 13484 ||| 13485 ||| 3995 ||| 12608 ||| 13486 ||| 
2021 ||| joint localization and radio map generation using transformer networks with limited rss samples. ||| 13487 ||| 13488 ||| 13489 ||| 
2019 ||| agrm: attention-based graph representation model for telecom fraud detection. ||| 124 ||| 6191 ||| 6187 ||| 6188 ||| 
2019 ||| a novel attention mechanism considering decoder input for abstractive text summarization. ||| 2478 ||| 13490 ||| 13491 ||| 2479 ||| 
2021 ||| enhancing transformer with horizontal and vertical guiding mechanisms for neural language modeling. ||| 13492 ||| 2478 ||| 13493 ||| 
2021 ||| assessment of self-attention on learned features for sound event localization and detection. ||| 13494 ||| 13495 ||| 8240 ||| 
2021 ||| semi-supervised sound event detection using multiscale channel attention and multiple consistency training. ||| 4365 ||| 4366 ||| 4367 ||| 4368 ||| 
2020 ||| audio tag representation guided dual attention network for acoustic scene classification. ||| 13496 ||| 12728 ||| 13497 ||| 12730 ||| 
2021 ||| many-to-many audio spectrogram tansformer: transformer for sound event localization and detection. ||| 13498 ||| 13499 ||| 4284 ||| 
2021 ||| toward interpretable polyphonic sound event detection with attention maps based on local prototypes. ||| 13500 ||| 13501 ||| 13502 ||| 13503 ||| 13504 ||| 11917 ||| 
2021 ||| multi-scale network based on split attention for semi-supervised sound event detection. ||| 13505 ||| 13506 ||| 
2021 ||| transfer learning followed by transformer for automated audio captioning. ||| 13507 ||| 13508 ||| 13509 ||| 13510 ||| 
2020 ||| audio captioning based on transformer and pre-trained cnn. ||| 13511 ||| 13512 ||| 13513 ||| 13514 ||| 13515 ||| 13516 ||| 13517 ||| 
2018 ||| multi-level attention model for weakly supervised audio classification. ||| 13518 ||| 13519 ||| 12618 ||| 10875 ||| 
2018 ||| attention-based convolutional neural networks for acoustic scene classification. ||| 12762 ||| 12618 ||| 13520 ||| 12619 ||| 648 ||| 649 ||| 
2021 ||| audio captioning transformer. ||| 13521 ||| 13522 ||| 13523 ||| 12619 ||| 11418 ||| 
2019 ||| registration and analysis of a pilot's attention using a mobile eyetracking system. ||| 3983 ||| 3984 ||| 3985 ||| 13524 ||| 
2019 ||| hierarchical text-label integrated attention network for document classification. ||| 13525 ||| 13526 ||| 13527 ||| 
2019 ||| amnet: convolutional neural network embeded with attention mechanism for semantic segmentation. ||| 13528 ||| 13529 ||| 1107 ||| 
2019 ||| text sentiment classification based on layered attention network. ||| 13530 ||| 11162 ||| 3879 ||| 
2019 ||| rational, emotional, and attentional choice models for recommender systems. ||| 13531 ||| 13532 ||| 13533 ||| 13534 ||| 3419 ||| 13535 ||| 4252 ||| 
2017 ||| quantifying the effects of learning styles on attention. ||| 8432 ||| 227 ||| 13536 ||| 8434 ||| 8435 ||| 
2021 ||| improved multi-scale fusion of attention network for hyperspectral image classification. ||| 13537 ||| 13538 ||| 13539 ||| 13540 ||| 
2021 ||| w-core transformer model for chinese word segmentation. ||| 13541 ||| 13538 ||| 13540 ||| 
2021 ||| a deep learning approach based on feature reconstruction and multi-dimensional attention mechanism for drug-drug interaction prediction. ||| 13542 ||| 13543 ||| 13544 ||| 13545 ||| 13546 ||| 
2021 ||| bindtransnet: a transferable transformer-based architecture for cross-cell type dna-protein binding sites prediction. ||| 13547 ||| 13548 ||| 13549 ||| 263 ||| 13550 ||| 13551 ||| 13552 ||| 13553 ||| 
2021 ||| litetrans: reconstruct transformer with convolution for medical image segmentation. ||| 13554 ||| 13555 ||| 
2021 ||| ecg arrhythmia detection based on hidden attention residual neural network. ||| 13556 ||| 13557 ||| 9027 ||| 1130 ||| 13558 ||| 
2021 ||| improved depression recognition using attention and multitask learning of gender recognition. ||| 1305 ||| 13559 ||| 13560 ||| 13561 ||| 13562 ||| 13563 ||| 
2019 ||| duplicate question detection based on neural networks and multi-head attention. ||| 5538 ||| 13564 ||| 
2019 ||| employing gated attention and multi-similarities to resolve document-level chinese event coreference. ||| 13565 ||| 221 ||| 222 ||| 
2019 ||| using mention segmentation to improve event detection with multi-head attention. ||| 13566 ||| 11628 ||| 13567 ||| 13568 ||| 
2018 ||| a hybrid algorithm for text classification based on cnn-blstm with attention. ||| 13569 ||| 13570 ||| 398 ||| 1199 ||| 
2021 ||| aspect-based sentiment classification with dependency relation and structured attention. ||| 13571 ||| 13572 ||| 13573 ||| 10104 ||| 10891 ||| 13574 ||| 
2017 ||| recursive annotations for attention-based neural machine translation. ||| 13575 ||| 4395 ||| 
2018 ||| shared representation learning with self-attention for cross-domain chinese hedge cue recognition. ||| 13576 ||| 13577 ||| 6474 ||| 1415 ||| 13578 ||| 
2020 ||| transformer-based arabic dialect identification. ||| 13579 ||| 13580 ||| 13581 ||| 12494 ||| 
2020 ||| sentiment classification with syntactic relationship and attention for teaching evaluation texts. ||| 13582 ||| 13583 ||| 13573 ||| 
2018 ||| topical-relevance detection using attention-based neural network. ||| 2514 ||| 13584 ||| 13585 ||| 13586 ||| 
2019 ||| fusion of image-text attention for transformer-based multimodal machine translation. ||| 13587 ||| 13588 ||| 13589 ||| 2514 ||| 13590 ||| 
2019 ||| extremely low resource text simplification with pre-trained transformer language model. ||| 13591 ||| 13592 ||| 
2019 ||| syntax-aware transformer encoder for neural machine translation. ||| 13593 ||| 3111 ||| 13594 ||| 3049 ||| 
2020 ||| detect turn-takings in subtitle streams with semantic recall transformer encoder. ||| 13595 ||| 7965 ||| 
2021 ||| resa: relation enhanced self-attention for low-resource neural machine translation. ||| 610 ||| 687 ||| 688 ||| 
2020 ||| enhancing attention models via multi-head collaboration. ||| 3359 ||| 13596 ||| 
2020 ||| structurally enhanced interactive attention network for aspect-level sentiment classification. ||| 13597 ||| 13598 ||| 13599 ||| 
2021 ||| vortx: volumetric 3d reconstruction with transformers for voxelwise view selection and fusion. ||| 13600 ||| 13601 ||| 13602 ||| 13603 ||| 13604 ||| 
2020 ||| spatial attention improves iterative 6d object pose estimation. ||| 13605 ||| 2123 ||| 
2020 ||| a transformer-based network for dynamic hand gesture recognition. ||| 13606 ||| 13607 ||| 13608 ||| 13609 ||| 13610 ||| 13611 ||| 
2019 ||| pairwise attention encoding for point cloud feature learning. ||| 13612 ||| 13613 ||| 11188 ||| 13614 ||| 
2021 ||| sceneformer: indoor scene generation with transformers. ||| 13615 ||| 13616 ||| 13617 ||| 12149 ||| 
2019 ||| res3atn - deep 3d residual attention network for hand gesture recognition in videos. ||| 5715 ||| 13618 ||| 
2021 ||| investigating attention mechanism in 3d point cloud object detection. ||| 13619 ||| 13620 ||| 2474 ||| 13621 ||| 
2021 ||| air-nets: an attention-based framework for locally conditioned implicit representations. ||| 13622 ||| 13623 ||| 13624 ||| 
2021 ||| attention meets geometry: geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation. ||| 13625 ||| 13626 ||| 13627 ||| 13628 ||| 13629 ||| 
2021 ||| channel-wise attention-based network for self-supervised monocular depth estimation. ||| 13630 ||| 13631 ||| 13632 ||| 13633 ||| 
2021 ||| gascn: graph attention shape completion network. ||| 803 ||| 13634 ||| 13635 ||| 
2021 ||| a spatio-temporal transformer for 3d human motion prediction. ||| 13636 ||| 13637 ||| 7482 ||| 2123 ||| 
2018 ||| detail preserving depth estimation from a single image using attention guided networks. ||| 13638 ||| 11641 ||| 8678 ||| 13639 ||| 
2021 ||| named entity recognition and relation extraction for covid-19: explainable active learning with word2vec embeddings and transformer-based bert models. ||| 13640 ||| 13641 ||| 13642 ||| 13643 ||| 13644 ||| 13645 ||| 13646 ||| 13647 ||| 13648 ||| 13649 ||| 13650 ||| 13651 ||| 13652 ||| 13653 ||| 
2020 ||| forecasting corporate financial time series using multi-phase attention recurrent neural networks. ||| 13654 ||| 13655 ||| 
2020 ||| entity matching with transformer architectures - a step forward in data integration. ||| 13656 ||| 13657 ||| 
2021 ||| accelerating transformer-based deep learning models on fpgas using column balanced block pruning. ||| 9870 ||| 9871 ||| 13658 ||| 13659 ||| 13660 ||| 11023 ||| 1258 ||| 11024 ||| 
2021 ||| three-dimensional memristive deep neural network with programmable attention mechanism. ||| 13661 ||| 13662 ||| 2771 ||| 
2021 ||| attention analysis in flipped classroom using 1d multi-point local ternary patterns. ||| 13663 ||| 13664 ||| 13665 ||| 13666 ||| 
2018 ||| code-switched named entity recognition with embedding attention. ||| 9319 ||| 3008 ||| 3826 ||| 
2019 ||| inriafbk drawing attention to offensive language at germeval2019. ||| 13667 ||| 13668 ||| 10223 ||| 13669 ||| 3953 ||| 
2019 ||| neural classification with attention assessment of the implicit-association test omt and prediction of subsequent academic success. ||| 13670 ||| 13671 ||| 10528 ||| 
2021 ||| incorporating distinct translation system outputs into statistical and transformer model. ||| 13672 ||| 13673 ||| 
2019 ||| transmembrane topology identification by fusing evolutionary and co-evolutionary information with cascaded bidirectional transformers. ||| 5189 ||| 13674 ||| 13675 ||| 13676 ||| 13677 ||| 1270 ||| 
2020 ||| cross-global attention graph kernel network prediction of drug prescription. ||| 13678 ||| 13679 ||| 9640 ||| 13680 ||| 13681 ||| 13682 ||| 
2020 ||| exam: an explainable attention-based model for covid-19 automatic diagnosis. ||| 13683 ||| 13684 ||| 13685 ||| 13686 ||| 13687 ||| 
2017 ||| protein-protein interaction extraction using attention-based convolution neural networks. ||| 3386 ||| 13688 ||| 13689 ||| 13690 ||| 13691 ||| 13692 ||| 
2020 ||| transforming the language of life: transformer neural networks for protein prediction tasks. ||| 13693 ||| 13694 ||| 13695 ||| 13696 ||| 13697 ||| 13698 ||| 
2021 ||| transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. ||| 13378 ||| 13382 ||| 748 ||| 
2017 ||| interpretable predictions of clinical outcomes with an attention-based recurrent neural network. ||| 4255 ||| 13687 ||| 
2021 ||| transformer-based named entity recognition for parsing clinical trial eligibility criteria. ||| 13699 ||| 13700 ||| 13408 ||| 13701 ||| 12067 ||| 4797 ||| 2989 ||| 13702 ||| 
2021 ||| kgdal: knowledge graph guided double attention lstm for rolling mortality prediction for aki-d patients. ||| 13703 ||| 13704 ||| 13705 ||| 13706 ||| 
2019 ||| transmembrane topology identification by fusing evolutionary and co-evolutionary information with cascaded bidirectional transformers. ||| 5189 ||| 13674 ||| 13675 ||| 13676 ||| 13677 ||| 1270 ||| 
2020 ||| a deep learning framework based on spatio-temporal attention mechanism for traffic prediction. ||| 844 ||| 1717 ||| 
2019 ||| word image representation based on sequence to sequence model with attention mechanism for out-of-vocabulary keyword spotting. ||| 13707 ||| 13708 ||| 4600 ||| 
2018 ||| aspect level sentiment classification with memory network using word sentiment vectors and a new attention mechanism am-pposc. ||| 13709 ||| 3279 ||| 13710 ||| 13711 ||| 
2019 ||| extract, attend, predict: aspect-based sentiment analysis with deep self-attention network. ||| 13712 ||| 970 ||| 497 ||| 13713 ||| 13714 ||| 
2019 ||| attention-based neural network: a novel approach for predicting the popularity of online content. ||| 13715 ||| 13716 ||| 13717 ||| 13718 ||| 13719 ||| 
2020 ||| attention-guided multi-view stereo network for depth estimation. ||| 13720 ||| 5727 ||| 13721 ||| 
2018 ||| head movements are correlated with other measures of visual attention at smaller spatial scales. ||| 13722 ||| 13723 ||| 13724 ||| 13725 ||| 
2021 ||| predicting acute kidney injury via interpretable ensemble learning and attention weighted convoutional-recurrent neural networks. ||| 13726 ||| 13727 ||| 13728 ||| 13729 ||| 10017 ||| 
2018 ||| recognizing unconstrained vietnamese handwriting by attention based encoder decoder model. ||| 13730 ||| 13731 ||| 9794 ||| 
2019 ||| microblog sentiment classification method based on dual attention mechanism and bidirectional lstm. ||| 13732 ||| 13733 ||| 13734 ||| 13735 ||| 
2019 ||| linguistic knowledge based on attention neural network for targeted sentiment classification. ||| 13736 ||| 13737 ||| 
2018 ||| the attention to safety issues from mainland china and taiwan. ||| 11501 ||| 13738 ||| 
2018 ||| attention-based bi-lstm for chinese named entity recognition. ||| 3433 ||| 13739 ||| 13733 ||| 
2021 ||| the next 700 program transformers. ||| 13740 ||| 
2019 ||| deep residual network with self attention improves person re-identification accuracy. ||| 13741 ||| 13742 ||| 13743 ||| 13744 ||| 
2019 ||| attention based echo state network: a novel approach for fault prognosis. ||| 13745 ||| 13746 ||| 13747 ||| 13748 ||| 
2020 ||| weakly supervised fine-grained recognition in a segmentation-attention network. ||| 13749 ||| 13750 ||| 13751 ||| 
2021 ||| leveraging cnn and bi-lstm in indonesian g2p using transformer. ||| 13752 ||| 13753 ||| 13754 ||| 
2019 ||| feature fusion attention visual question answering. ||| 13755 ||| 13756 ||| 13757 ||| 
2020 ||| an attention-based deep network for ctr prediction. ||| 13758 ||| 13759 ||| 1503 ||| 
2019 ||| a deep attention network for chinese word segment. ||| 13760 ||| 10968 ||| 13761 ||| 
2020 ||| the study of pso-svm and pso-grnn algorithm used in the fault pattern classification of transformer. ||| 13762 ||| 
2020 ||| dual pyramid attention network for high-resolution remotely sensed image change detection. ||| 13763 ||| 13764 ||| 13765 ||| 13766 ||| 13767 ||| 
2019 ||| bidirectional-gru based on attention mechanism for aspect-level sentiment analysis. ||| 13768 ||| 13769 ||| 
2017 ||| attention-based experience replay in deep q-learning. ||| 13770 ||| 13771 ||| 
2021 ||| gcn2-naa: two-stage graph convolutional networks with node-aware attention for joint entity and relation extraction. ||| 13772 ||| 11685 ||| 13773 ||| 13774 ||| 13775 ||| 
2021 ||| low light image enhancement in usv imaging system via u-net and attention mechanism. ||| 13776 ||| 13777 ||| 13778 ||| 
2021 ||| spectral-wise attention-based residual network for hyperspectral image classification. ||| 13779 ||| 13780 ||| 1132 ||| 
2021 ||| accelerating transformer for neural machine translation. ||| 13781 ||| 13782 ||| 13783 ||| 
2021 ||| multi-perspective reasoning transformers. ||| 13784 ||| 13785 ||| 13783 ||| 
2021 ||| intelligent gastric histopathology image classification using hierarchical conditional random field based attention mechanism. ||| 13786 ||| 13787 ||| 399 ||| 13788 ||| 5378 ||| 13789 ||| 7826 ||| 
2017 ||| towards enhanced hierarchical attention networks in icd-9 tagging of clinical notes. ||| 13790 ||| 13791 ||| 13792 ||| 
2021 ||| large power transformer overload detection using sound analysis. ||| 13793 ||| 
2019 ||| multi-scale deep convolutional nets with attention model and conditional random fields for semantic image segmentation. ||| 124 ||| 13794 ||| 5543 ||| 
2019 ||| an attention-enhanced recurrent graph convolutional network for skeleton-based action recognition. ||| 5491 ||| 5492 ||| 5400 ||| 
2017 ||| global-local feature attention network with reranking strategy for image caption generation. ||| 13795 ||| 13796 ||| 13797 ||| 13798 ||| 
2021 ||| how software architects focus their attention. ||| 13799 ||| 13800 ||| 
2019 ||| modeling of transformer-rectifier sets for the energization of electrostatic precipitators using modelica. ||| 13801 ||| 13802 ||| 2855 ||| 13803 ||| 13804 ||| 
2018 ||| reference based on adaptive attention mechanism for image captioning. ||| 13805 ||| 13806 ||| 13807 ||| 2277 ||| 
2019 ||| visual attention and haptic control: a cross-study. ||| 2917 ||| 2919 ||| 13808 ||| 13809 ||| 13810 ||| 13811 ||| 
2019 ||| are you paying attention? detecting distracted driving in real-time. ||| 13812 ||| 13813 ||| 12550 ||| 13814 ||| 8364 ||| 
2019 ||| related attention network for person re-identification. ||| 13815 ||| 13816 ||| 13817 ||| 2398 ||| 
2019 ||| pedestrian detection based on spatial attention module for outdoor video surveillance. ||| 13818 ||| 13819 ||| 13820 ||| 
2018 ||| spatial- temporal attention for image captioning. ||| 13821 ||| 232 ||| 233 ||| 5295 ||| 230 ||| 
2019 ||| impression prediction of oral presentation using lstm and dot-product attention mechanism. ||| 7752 ||| 7755 ||| 7756 ||| 
2018 ||| saliency-based spatiotemporal attention for video captioning. ||| 13822 ||| 13223 ||| 13823 ||| 13824 ||| 13825 ||| 
2020 ||| classification of propagation path and tweets for rumor detection using graphical convolutional networks and transformer based encodings. ||| 13826 ||| 13827 ||| 
2019 ||| video summarization using global attention with memory network and lstm. ||| 13828 ||| 13829 ||| 13830 ||| 13831 ||| 13832 ||| 12550 ||| 8364 ||| 
2018 ||| enhanced text-guided attention model for image captioning. ||| 7825 ||| 7827 ||| 13833 ||| 13834 ||| 5946 ||| 
2021 ||| a-a kd: attention and activation knowledge distillation. ||| 13835 ||| 859 ||| 13836 ||| 13837 ||| 5217 ||| 
2021 ||| missformer: (in-)attention-based handling of missing observations for trajectory filtering and prediction. ||| 13838 ||| 13839 ||| 13840 ||| 3450 ||| 13841 ||| 13842 ||| 
2020 ||| domain adaptive transfer learning on visual attention aware data augmentation for fine-grained visual categorization. ||| 13843 ||| 13844 ||| 
2019 ||| improving visual reasoning with attention alignment. ||| 13845 ||| 13846 ||| 9629 ||| 
2018 ||| denssiam: end-to-end densely-siamese network with self-attention model for object tracking. ||| 7846 ||| 7847 ||| 13847 ||| 
2019 ||| adaptive attention model for lidar instance segmentation. ||| 13848 ||| 13849 ||| 13850 ||| 13851 ||| 
2021 ||| non-homogeneous haze removal through a multiple attention module architecture. ||| 13852 ||| 2600 ||| 13853 ||| 13854 ||| 
2019 ||| cnns and transfer learning for lecture venue occupancy and student attention monitoring. ||| 13855 ||| 13856 ||| 13857 ||| 
2021 ||| mobile application behavior recognition based on dual-domain attention and meta-learning. ||| 8802 ||| 
2021 ||| analysis of network public opinion based on bilstm and self-attention fusion mechanism. ||| 13858 ||| 13859 ||| 
2022 ||| differentiating endogenous and exogenous attention shifts based on fixation-related potentials. ||| 13860 ||| 13861 ||| 13862 ||| 
2021 ||| ruite: refining ui layout aesthetics using transformer encoder. ||| 13863 ||| 13864 ||| 13865 ||| 
2017 ||| evaluation of attention guiding techniques for augmented reality-based assistance in picking and assembly tasks. ||| 13866 ||| 13867 ||| 
2018 ||| providing adaptive and personalized visual support based on behavioral tracking of children with autism for assessing reciprocity and coordination skills in a joint attention training application. ||| 13868 ||| 13869 ||| 
2019 ||| transformer: a database-driven approach to generating forms for constrained interaction. ||| 13870 ||| 11734 ||| 
2020 ||| multi-attention deep recurrent neural network for nursing action evaluation using wearable sensor. ||| 13871 ||| 13872 ||| 13873 ||| 13874 ||| 
2018 ||| visualizing gaze direction to support video coding of social attention for children with autism spectrum disorder. ||| 7926 ||| 13875 ||| 13876 ||| 13877 ||| 8347 ||| 13878 ||| 7894 ||| 
2020 ||| deep attention-based model for helpfulness prediction of healthcare online reviews. ||| 13879 ||| 13880 ||| 13881 ||| 13882 ||| 
2018 ||| webcam-based attention tracking in online learning: a feasibility study. ||| 13883 ||| 3226 ||| 13884 ||| 13885 ||| 
2021 ||| multi-modal multi-scale attention guidance in cyber-physical environments. ||| 13886 ||| 13887 ||| 
2019 ||| streaming end-to-end speech recognition with joint ctc-attention based models. ||| 11980 ||| 2508 ||| 11981 ||| 
2021 ||| relaxed attention: a simple method to boost performance of end-to-end automatic speech recognition. ||| 13888 ||| 13889 ||| 13890 ||| 13891 ||| 
2019 ||| a comparison of transformer and lstm encoder decoder models for asr. ||| 12532 ||| 3453 ||| 12659 ||| 12533 ||| 12534 ||| 3454 ||| 
2019 ||| character-aware attention-based end-to-end speech recognition. ||| 13892 ||| 13893 ||| 12179 ||| 12033 ||| 
2021 ||| cross-attention conformer for context modeling in speech enhancement for asr. ||| 13894 ||| 3334 ||| 13895 ||| 12652 ||| 12098 ||| 
2019 ||| end-to-end neural speaker diarization with self-attention. ||| 13896 ||| 13897 ||| 13898 ||| 13899 ||| 13900 ||| 3549 ||| 
2021 ||| learning how long to wait: adaptively-constrained monotonic multihead attention for streaming asr. ||| 12145 ||| 12146 ||| 9316 ||| 
2019 ||| improved multi-stage training of online attention-based encoder-decoder models. ||| 13901 ||| 13902 ||| 13903 ||| 13904 ||| 13905 ||| 13906 ||| 
2017 ||| attention-based wav2text with feature transfer learning. ||| 12303 ||| 13907 ||| 11757 ||| 
2019 ||| explicit alignment of text and speech encodings for attention-based end-to-end speech recognition. ||| 13908 ||| 12254 ||| 
2019 ||| cnn with phonetic attention for text-independent speaker verification. ||| 13909 ||| 11819 ||| 12179 ||| 12033 ||| 1236 ||| 
2021 ||| action item detection in meetings using pretrained transformers. ||| 13910 ||| 3491 ||| 13911 ||| 
2017 ||| dynamic time-aware attention to speaker roles and contexts for spoken language understanding. ||| 13912 ||| 13913 ||| 4841 ||| 4843 ||| 
2019 ||| speaker-aware speech-transformer. ||| 13914 ||| 4634 ||| 5268 ||| 728 ||| 
2021 ||| improving hybrid ctc/attention end-to-end speech recognition with pretrained acoustic and language models. ||| 12400 ||| 13915 ||| 13916 ||| 13917 ||| 
2019 ||| orthogonality constrained multi-head attention for keyword spotting. ||| 13918 ||| 13919 ||| 13920 ||| 13921 ||| 13922 ||| 13923 ||| 
2017 ||| a hierarchical attention based model for off-topic spontaneous spoken response detection. ||| 9484 ||| 9485 ||| 3796 ||| 
2019 ||| improving mandarin end-to-end speech synthesis by self-attention and learnable gaussian bias. ||| 13924 ||| 12571 ||| 13925 ||| 13926 ||| 12384 ||| 
2021 ||| attention based model for segmental pronunciation error detection. ||| 13927 ||| 13928 ||| 13929 ||| 8233 ||| 
2021 ||| duality temporal-channel-frequency attention enhanced speaker representation learning. ||| 254 ||| 13930 ||| 12384 ||| 
2021 ||| efficient conformer: progressive downsampling and grouped attention for automatic speech recognition. ||| 13931 ||| 13932 ||| 
2019 ||| attention based on-device streaming speech recognition with large speech corpus. ||| 13904 ||| 13933 ||| 12048 ||| 13934 ||| 13906 ||| 13935 ||| 13902 ||| 13936 ||| 13937 ||| 13938 ||| 13939 ||| 13940 ||| 13941 ||| 
2017 ||| unwritten languages demand attention too! word discovery with encoder-decoder models. ||| 19 ||| 13942 ||| 13943 ||| 3510 ||| 
2021 ||| improving hs-dacs based streaming transformer asr with deep reinforcement learning. ||| 12321 ||| 12323 ||| 
2021 ||| attention-based multi-hypothesis fusion for speech summarization. ||| 13944 ||| 8235 ||| 1491 ||| 3549 ||| 
2021 ||| attention-based scaling adaptation for target speech extraction. ||| 13945 ||| 13946 ||| 13947 ||| 13948 ||| 
2021 ||| context-aware transformer transducer for speech recognition. ||| 12483 ||| 2058 ||| 12484 ||| 12485 ||| 13949 ||| 3841 ||| 12487 ||| 
2019 ||| hierarchical transformers for long document classification. ||| 13950 ||| 12580 ||| 12581 ||| 12582 ||| 13951 ||| 12583 ||| 
2019 ||| adapting pretrained transformer to lattices for spoken language understanding. ||| 13952 ||| 4843 ||| 
2019 ||| integrating source-channel and attention-based sequence-to-sequence models for speech recognition. ||| 12715 ||| 8862 ||| 12012 ||| 
2019 ||| transformer asr with contextual block processing. ||| 12365 ||| 12364 ||| 13953 ||| 3549 ||| 
2021 ||| distilling knowledge from ensembles of acoustic models for joint ctc-attention end-to-end speech recognition. ||| 1446 ||| 13954 ||| 13955 ||| 
2019 ||| a comparative study on transformer vs rnn in speech applications. ||| 8066 ||| 12608 ||| 3549 ||| 8224 ||| 12246 ||| 12579 ||| 8223 ||| 2508 ||| 12682 ||| 13956 ||| 13957 ||| 13958 ||| 13959 ||| 
2019 ||| attention-based speech recognition using gaze information. ||| 8227 ||| 8223 ||| 12131 ||| 
2021 ||| multi-task learning with cross attention for keyword spotting. ||| 13960 ||| 13961 ||| 13962 ||| 
2019 ||| state-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions. ||| 13963 ||| 13964 ||| 10187 ||| 
2021 ||| using self attention dnns to discover phonemic features for audio deep fake detection. ||| 13965 ||| 13966 ||| 13967 ||| 13968 ||| 
2021 ||| fully integrated transformer less floating gate driver for 3d power supply on chip. ||| 13969 ||| 13970 ||| 
2019 ||| transformer-less floating gate driver for 3d power soc. ||| 13971 ||| 13972 ||| 13970 ||| 
2019 ||| dependency-based self-attention for transformer nmt. ||| 3783 ||| 3784 ||| 3785 ||| 
2021 ||| towards the application of calibrated transformers to the unsupervised estimation of question difficulty from text. ||| 13973 ||| 13974 ||| 13975 ||| 13976 ||| 
2021 ||| are the multilingual models better? improving czech sentiment with transformers. ||| 13977 ||| 3882 ||| 13978 ||| 
2021 ||| on the contribution of per-icd attention mechanisms to classify health records in languages with fewer resources than english. ||| 13979 ||| 13980 ||| 13981 ||| 2600 ||| 13982 ||| 13983 ||| 
2021 ||| predicting the factuality of reporting of news media using observations about user attention in their youtube channels. ||| 13984 ||| 13985 ||| 10604 ||| 13986 ||| 13987 ||| 7049 ||| 
2019 ||| self-attention networks for intent detection. ||| 13988 ||| 2101 ||| 13989 ||| 13990 ||| 13310 ||| 13991 ||| 13992 ||| 
2021 ||| enriching the transformer with linguistic factors for low-resource machine translation. ||| 13993 ||| 3466 ||| 11843 ||| 
2019 ||| persistence pays off: paying attention to what the lstm gating mechanism persists. ||| 13994 ||| 13995 ||| 
2019 ||| turkish tweet classification with transformer encoder. ||| 13996 ||| 13997 ||| 13998 ||| 13999 ||| 57 ||| 58 ||| 59 ||| 14000 ||| 
2021 ||| can the transformer be used as a drop-in replacement for rnns in text-generating gans? ||| 14001 ||| 14002 ||| 
2021 ||| masking and transformer-based models for hyperpartisanship detection in news. ||| 14003 ||| 14004 ||| 10522 ||| 11932 ||| 11933 ||| 14005 ||| 
2019 ||| dependency-based relative positional encoding for transformer nmt. ||| 11674 ||| 3784 ||| 3785 ||| 
2021 ||| transformer with syntactic position encoding for machine translation. ||| 14006 ||| 14007 ||| 14008 ||| 8880 ||| 
2019 ||| discourse-aware hierarchical attention network for extractive single-document summarization. ||| 14009 ||| 4982 ||| 14010 ||| 14011 ||| 
2021 ||| can multilingual transformers fight the covid-19 infodemic? ||| 14012 ||| 3849 ||| 10586 ||| 
2021 ||| on the usability of transformers-based models for a french question-answering task. ||| 14013 ||| 14014 ||| 14015 ||| 
2021 ||| a pre-trained transformer and cnn model with joint language id and part-of-speech tagging for code-mixed social-media text. ||| 14016 ||| 10510 ||| 
2017 ||| real-time news summarization with adaptation to media attention. ||| 3700 ||| 3701 ||| 3702 ||| 
2019 ||| quasi bidirectional encoder representations from transformers for word sense disambiguation. ||| 14017 ||| 14018 ||| 
2021 ||| decoupled transformer for scalable inference in open-domain question answering. ||| 14019 ||| 14020 ||| 
2019 ||| self-attentional models application in task-oriented dialogue generation systems. ||| 14021 ||| 14022 ||| 14023 ||| 14024 ||| 
2021 ||| character-based thai word segmentation with multiple attentions. ||| 14025 ||| 4982 ||| 14011 ||| 
2020 ||| garnet: graph attention residual networks based on adversarial learning for 3d human pose estimation. ||| 14026 ||| 14027 ||| 14028 ||| 977 ||| 
2021 ||| add-net: attention u-net with dilated skip connection and dense connected decoder for retinal vessel segmentation. ||| 14029 ||| 8358 ||| 3289 ||| 
2019 ||| realistic pedestrian simulation based on the environmental attention model. ||| 14030 ||| 14031 ||| 14032 ||| 14033 ||| 
2021 ||| a classification network for ocular diseases based on structure feature and visual attention. ||| 14034 ||| 14035 ||| 14036 ||| 14037 ||| 14038 ||| 14039 ||| 14040 ||| 14041 ||| 
2021 ||| compact double attention module embedded cnn for palmprint recognition. ||| 14042 ||| 14043 ||| 3437 ||| 14044 ||| 14045 ||| 14046 ||| 
2020 ||| broad-classifier for remote sensing scene classification with spatial and channel-wise attention. ||| 14026 ||| 14047 ||| 14048 ||| 14037 ||| 977 ||| 14049 ||| 
2018 ||| open-domain question answering using feature encoded dynamic coattention networks. ||| 14050 ||| 14051 ||| 14052 ||| 14053 ||| 14054 ||| 14055 ||| 14056 ||| 14057 ||| 
2021 ||| an attention-based cnn-lstm model with limb synergy for joint angles prediction. ||| 14058 ||| 14059 ||| 14060 ||| 14061 ||| 14062 ||| 
2021 ||| accommodating transformer onto fpga: coupling the balanced model compression and fpga-implementation optimization. ||| 9867 ||| 9873 ||| 9870 ||| 9871 ||| 9869 ||| 9868 ||| 
2021 ||| hmc-tran: a tensor-core inspired hierarchical model compression for transformer-based dnns on gpu. ||| 9871 ||| 14063 ||| 9870 ||| 14064 ||| 9872 ||| 14065 ||| 14066 ||| 1258 ||| 11023 ||| 11024 ||| 
2020 ||| effective piecewise cnn with attention mechanism for distant supervision on relation extraction task. ||| 14067 ||| 14068 ||| 10113 ||| 14069 ||| 
2020 ||| can a transformer assist in scientific writing? generating semantic web paper snippets with gpt-2. ||| 13124 ||| 13125 ||| 13126 ||| 14070 ||| 
2019 ||| a hybrid approach for aspect-based sentiment analysis using a lexicalized domain ontology and attentional neural models. ||| 14071 ||| 14072 ||| 
2021 ||| grounding dialogue systems via knowledge graph aware decoding with pre-trained transformers. ||| 14073 ||| 14074 ||| 3581 ||| 
2018 ||| knowledge guided attention and inference for describing images containing unseen objects. ||| 14075 ||| 14076 ||| 14077 ||| 14078 ||| 
2021 ||| retra: recurrent transformers for learning temporally contextualized knowledge graph embeddings. ||| 14079 ||| 14078 ||| 14080 ||| 4194 ||| 14081 ||| 14082 ||| 
2021 ||| context transformer with stacked pointer networks for conversational question answering over knowledge graphs. ||| 14083 ||| 14084 ||| 14085 ||| 14086 ||| 3581 ||| 
2021 ||| extractive summarization for explainable sentiment analysis using transformers. ||| 14087 ||| 14088 ||| 9836 ||| 14089 ||| 
2021 ||| structurally guided channel attention networks: sgca-net. ||| 14090 ||| 14091 ||| 14092 ||| 14093 ||| 14094 ||| 14095 ||| 14096 ||| 
2020 ||| ar-glasses-based attention guiding for complex environments: requirements, classification and evaluation. ||| 13866 ||| 13867 ||| 
2020 ||| a three-module proposed solution to improve cognitive and social skills of students with attention deficit disorder (add) and high functioning autism (hfa): innovative technological advancements for students with neurodevelopmental disorders. ||| 14097 ||| 14098 ||| 14099 ||| 14100 ||| 14101 ||| 14102 ||| 
2020 ||| chinese zero pronoun resolution based on biaffine attention mechanism. ||| 14103 ||| 14104 ||| 14105 ||| 14106 ||| 11762 ||| 6502 ||| 
2018 ||| collective attention in wechat public platform. ||| 14107 ||| 14108 ||| 14109 ||| 14110 ||| 14111 ||| 
2020 ||| an efficient intrusion detection model combined bidirectional gated recurrent units with attention mechanism. ||| 1072 ||| 14112 ||| 14113 ||| 14114 ||| 14115 ||| 
2018 ||| uncovering determinants of discontinuous intention of attention to mobile line-p messages. ||| 14116 ||| 14117 ||| 4043 ||| 
2019 ||| design of efficient distribution transformer: a deep learning approach. ||| 14118 ||| 14119 ||| 14120 ||| 14121 ||| 14122 ||| 14123 ||| 14124 ||| 14125 ||| 14126 ||| 
2019 ||| transformer design software with parallel processing. ||| 14118 ||| 14119 ||| 14120 ||| 14121 ||| 14122 ||| 14123 ||| 14124 ||| 14125 ||| 14126 ||| 
2017 ||| video attention using multiple temporal scales coding reconstruction. ||| 7011 ||| 14127 ||| 
2019 ||| a method for human parsing based on deep learning and attention mechanism. ||| 11453 ||| 14128 ||| 
2021 ||| efficient channel attention u-net for mesh crack detection. ||| 14129 ||| 14130 ||| 1705 ||| 14131 ||| 
2017 ||| an anti-power theft method for secondary circuit of energy meter current transformer. ||| 14132 ||| 14133 ||| 10106 ||| 10110 ||| 14134 ||| 14135 ||| 875 ||| 14136 ||| 
2019 ||| driver action recognition based on attention mechanism. ||| 14137 ||| 7659 ||| 14138 ||| 14139 ||| 14140 ||| 
2021 ||| vehicle trajectory prediction based on attention mechanism and gan. ||| 9149 ||| 14141 ||| 1589 ||| 6827 ||| 
2021 ||| uav-based cross-view geo-localization fusion spatial attention mechanism and netvlad. ||| 14142 ||| 5902 ||| 9283 ||| 14143 ||| 326 ||| 14144 ||| 
2021 ||| a multiscale attention mechanism based vehicle re-identification. ||| 14145 ||| 14146 ||| 14147 ||| 14148 ||| 14130 ||| 14149 ||| 
2021 ||| spiking transformer networks: a rate coded approach for processing sequential data. ||| 14150 ||| 14151 ||| 14152 ||| 14153 ||| 
2018 ||| the location of partial discharge source in the power transformer with uhf sensors. ||| 14154 ||| 14155 ||| 
2019 ||| transformer image recognition system based on deep learning. ||| 1353 ||| 14156 ||| 14157 ||| 14158 ||| 14159 ||| 4217 ||| 14160 ||| 14161 ||| 
2021 ||| an automatic data acquisition device for transformer oscillating switching impulse voltage tests. ||| 14162 ||| 14163 ||| 14164 ||| 14165 ||| 14166 ||| 
2021 ||| comment text grading for chinese graduate academic dissertation using attention convolutional neural networks. ||| 14167 ||| 14168 ||| 14169 ||| 14170 ||| 
2018 ||| a short text semantic classification method for power grid service based on attention_gated recurrent unit (at_gru) neural network. ||| 14171 ||| 14172 ||| 
2019 ||| abstract model based on location attention and competition mechanism. ||| 1239 ||| 14173 ||| 14174 ||| 14175 ||| 
2021 ||| supervised classification of plant image based on attention mechanism. ||| 4634 ||| 1132 ||| 
2021 ||| deep homography estimation based on attention mechanism. ||| 6827 ||| 14176 ||| 9283 ||| 14144 ||| 14141 ||| 9149 ||| 
2017 ||| a hybrid method for current transformer saturation detection and compensation in smart grid. ||| 14177 ||| 14178 ||| 14179 ||| 1153 ||| 1455 ||| 
2019 ||| an attention convolutional neural network for forest fire smoke recognition. ||| 14180 ||| 14181 ||| 14182 ||| 7659 ||| 
2019 ||| outlier detection for transformer's oil chromatographic data based on metric learning and the weighted local outlier factor. ||| 14183 ||| 208 ||| 14184 ||| 14185 ||| 14186 ||| 
2019 ||| a residual network of water scene recognition based on optimized inception module and convolutional block attention module. ||| 14187 ||| 14128 ||| 
2018 ||| hierarchical gated convolutional networks with multi-head attention for text classification. ||| 14188 ||| 14189 ||| 
2019 ||| attention please: consider mockito when evaluating newly proposed automated program repair techniques. ||| 14190 ||| 14191 ||| 14192 ||| 14193 ||| 
2019 ||| experiences of studying attention through eeg in the context of review tasks. ||| 14194 ||| 14195 ||| 14196 ||| 14197 ||| 14198 ||| 
2018 ||| pixel-parallel architecture for neuromorphic smart image sensor with visual attention. ||| 14199 ||| 14200 ||| 14201 ||| 
2021 ||| re-transformer: a self-attention based model for machine translation. ||| 14202 ||| 14203 ||| 
2021 ||| attention based abstractive summarization of malayalam document. ||| 14204 ||| 14205 ||| 14206 ||| 
2017 ||| arabic machine transliteration using an attention-based encoder-decoder model. ||| 14207 ||| 14208 ||| 14209 ||| 
2020 ||| double attention-based multimodal neural machine translation with semantic image regions. ||| 14210 ||| 14211 ||| 14212 ||| 3346 ||| 
2020 ||| fine-grained human evaluation of transformer and recurrent approaches to neural machine translation for english-to-chinese. ||| 14213 ||| 14214 ||| 
2021 ||| a co-attention method based on generative adversarial networks for multi-view images. ||| 14215 ||| 14216 ||| 14217 ||| 14218 ||| 14219 ||| 
2019 ||| share price trend prediction using attention with lstm structure. ||| 14220 ||| 14221 ||| 14222 ||| 14223 ||| 
2017 ||| classifying semantic clause types: modeling context and genre characteristics with recurrent neural networks and attention. ||| 14224 ||| 14225 ||| 14226 ||| 14227 ||| 14228 ||| 
2021 ||| can transformer language models predict psychometric properties? ||| 3941 ||| 14229 ||| 14230 ||| 3942 ||| 
2021 ||| did the cat drink the coffee? challenging transformers with generalized event knowledge. ||| 14231 ||| 14232 ||| 8955 ||| 14233 ||| 10699 ||| 14234 ||| 
2020 ||| generalized admittance matrix model of transformers. ||| 14235 ||| 14236 ||| 11005 ||| 14237 ||| 14238 ||| 
2017 ||| neurofeedback training system with audiovisual stimuli for the attention state induction during cognitive processes. ||| 14239 ||| 14240 ||| 14241 ||| 14242 ||| 14243 ||| 852 ||| 14244 ||| 14245 ||| 
2019 ||| a new time-frequency attention mechanism for tdnn and cnn-lstm-tdnn, with application to language identification. ||| 14246 ||| 12357 ||| 8298 ||| 
2020 ||| multi-modal attention for speech emotion recognition. ||| 14247 ||| 14248 ||| 4481 ||| 12494 ||| 
2021 ||| keyword transformer: a self-attention model for keyword spotting. ||| 14249 ||| 14250 ||| 14251 ||| 
2020 ||| a cross-channel attention-based wave-u-net for multi-channel speech enhancement. ||| 14252 ||| 14253 ||| 14254 ||| 14255 ||| 4456 ||| 
2020 ||| environment sound classification using multiple feature channels and attention based deep convolutional neural network. ||| 14256 ||| 14257 ||| 14258 ||| 
2021 ||| event specific attention for polyphonic sound event detection. ||| 14259 ||| 2390 ||| 1589 ||| 
2019 ||| improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration. ||| 8066 ||| 13958 ||| 3549 ||| 1491 ||| 8235 ||| 1492 ||| 
2018 ||| temporal transformer networks for acoustic scene classification. ||| 6163 ||| 12362 ||| 5259 ||| 
2020 ||| attention to indexical information improves voice recall. ||| 14260 ||| 14261 ||| 
2018 ||| improving attention based sequence-to-sequence models for end-to-end english conversational speech recognition. ||| 12586 ||| 12190 ||| 14262 ||| 1224 ||| 12189 ||| 4530 ||| 3808 ||| 
2020 ||| memory controlled sequential self attention for sound recognition. ||| 14263 ||| 14264 ||| 14265 ||| 935 ||| 
2020 ||| attention-based speaker embeddings for one-shot voice conversion. ||| 14266 ||| 14267 ||| 
2018 ||| exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition. ||| 645 ||| 14268 ||| 11983 ||| 12310 ||| 14269 ||| 242 ||| 
2021 ||| tvqvc: transformer based vector quantized variational autoencoder with ctc loss for voice conversion. ||| 14270 ||| 12104 ||| 
2020 ||| durian-sc: duration informed attention network based singing voice conversion system. ||| 14271 ||| 12189 ||| 12572 ||| 12586 ||| 14272 ||| 13512 ||| 14273 ||| 14274 ||| 3808 ||| 
2021 ||| weakly-supervised speech-to-text mapping with visually connected non-parallel speech-text data using cyclic partially-aligned transformer. ||| 14275 ||| 13907 ||| 11757 ||| 
2017 ||| advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm. ||| 2508 ||| 3549 ||| 9472 ||| 14276 ||| 
2019 ||| speaker adaptation for attention-based end-to-end speech recognition. ||| 13892 ||| 13893 ||| 12179 ||| 12033 ||| 
2020 ||| attention wave-u-net for acoustic echo cancellation. ||| 14277 ||| 10985 ||| 
2020 ||| enhancing monotonic multihead attention for streaming asr. ||| 12682 ||| 12683 ||| 4418 ||| 
2020 ||| speech transformer with speaker aware persistent memory. ||| 14278 ||| 14279 ||| 14280 ||| 1313 ||| 4469 ||| 12203 ||| 
2019 ||| attentive to individual: a multimodal emotion recognition network with personalized attention profile. ||| 14281 ||| 12347 ||| 
2021 ||| knowledge distillation for streaming transformer-transducer. ||| 14282 ||| 
2020 ||| robust beam search for encoder-decoder attention based speech recognition without length bias. ||| 5067 ||| 12533 ||| 12534 ||| 3454 ||| 
2018 ||| sequence-to-sequence neural network model with 2d attention for learning japanese pitch accents. ||| 14283 ||| 14284 ||| 14285 ||| 
2021 ||| end-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention. ||| 14286 ||| 14287 ||| 6514 ||| 14288 ||| 14289 ||| 14290 ||| 14291 ||| 14292 ||| 14293 ||| 
2021 ||| attention-based keyword localisation in speech using visual grounding. ||| 14294 ||| 14295 ||| 
2020 ||| a noise-aware memory-attention network architecture for regression-based speech enhancement. ||| 14296 ||| 1010 ||| 14297 ||| 12404 ||| 4434 ||| 
2021 ||| efficient conformer with prob-sparse attention mechanism for end-to-end speech recognition. ||| 12467 ||| 12395 ||| 12384 ||| 13917 ||| 
2021 ||| sequence-to-sequence learning for deep gaussian process based speech synthesis using self-attention gp layer. ||| 14298 ||| 14299 ||| 14300 ||| 
2019 ||| automatic hierarchical attention neural network for detecting ad. ||| 14301 ||| 14302 ||| 14303 ||| 14304 ||| 14305 ||| 14306 ||| 
2021 ||| improving polyphone disambiguation for mandarin chinese by combining mix-pooling strategy and window-based attention. ||| 14307 ||| 14308 ||| 14309 ||| 9547 ||| 2994 ||| 705 ||| 
2020 ||| exploring transformers for large-scale speech recognition. ||| 14310 ||| 12031 ||| 12179 ||| 12033 ||| 
2020 ||| understanding self-attention of self-supervised audio transformers. ||| 12723 ||| 12722 ||| 12644 ||| 
2021 ||| pilot: introducing transformers for probabilistic sound event localization. ||| 8250 ||| 14311 ||| 14312 ||| 8251 ||| 1491 ||| 1493 ||| 1492 ||| 1494 ||| 8252 ||| 
2020 ||| ecapa-tdnn: emphasized channel attention, propagation and aggregation in tdnn based speaker verification. ||| 14313 ||| 14314 ||| 14315 ||| 
2019 ||| multi-stride self-attention for speech recognition. ||| 13963 ||| 14316 ||| 14317 ||| 3561 ||| 3562 ||| 
2020 ||| transformer vq-vae for unsupervised unit discovery and speech synthesis: zerospeech 2020 challenge. ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| investigating methods to improve language model integration for attention-based encoder-decoder asr models. ||| 14318 ||| 14319 ||| 14320 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2020 ||| tmt: a transformer-based modal translator for improving multimodal sequence representations in audio visual scene-aware dialog. ||| 12258 ||| 12257 ||| 12263 ||| 11688 ||| 
2019 ||| online hybrid ctc/attention architecture for end-to-end speech recognition. ||| 12401 ||| 12101 ||| 12104 ||| 14321 ||| 8298 ||| 
2021 ||| advanced long-context end-to-end speech recognition using context-expanded transformers. ||| 2508 ||| 11980 ||| 2507 ||| 11981 ||| 
2020 ||| enhancing monotonicity for robust autoregressive transformer tts. ||| 14322 ||| 3138 ||| 4458 ||| 14323 ||| 12137 ||| 4460 ||| 
2021 ||| end-to-end speaker-attributed asr with transformer. ||| 13897 ||| 14324 ||| 13893 ||| 12608 ||| 13892 ||| 12029 ||| 12030 ||| 
2018 ||| multi-channel attention for end-to-end speech recognition. ||| 826 ||| 827 ||| 828 ||| 829 ||| 830 ||| 
2021 ||| transcribing paralinguistic acoustic cues to target language text in transformer-based speech-to-text translation. ||| 14325 ||| 13907 ||| 11756 ||| 11757 ||| 
2019 ||| attention-based word vector prediction with lstms and its application to the oov problem in asr. ||| 14326 ||| 4048 ||| 4049 ||| 4046 ||| 14327 ||| 14328 ||| 14329 ||| 11005 ||| 
2020 ||| environmental sound classification with parallel temporal-spectral attention. ||| 12636 ||| 4430 ||| 14330 ||| 11418 ||| 
2021 ||| modular multi-modal attention network for alzheimer's disease detection using patient audio and language data. ||| 6003 ||| 14331 ||| 14332 ||| 14333 ||| 14334 ||| 
2018 ||| multi-task learning with augmentation strategy for acoustic-to-word attention-based encoder-decoder speech recognition. ||| 4409 ||| 12681 ||| 12221 ||| 1491 ||| 14335 ||| 4411 ||| 
2020 ||| end-to-end text-to-speech synthesis with unaligned multiple language units based on attention. ||| 14336 ||| 14337 ||| 14300 ||| 
2020 ||| attention and encoder-decoder based models for transforming articulatory movements at different speaking rates. ||| 14338 ||| 14339 ||| 14340 ||| 
2018 ||| an investigation of convolution attention based models for multilingual speech synthesis of indian languages. ||| 14341 ||| 14342 ||| 4890 ||| 
2021 ||| lightweight causal transformer with local self-attention for real-time speech enhancement. ||| 14343 ||| 13930 ||| 1010 ||| 
2021 ||| hierarchical context-aware transformers for non-autoregressive text to speech. ||| 14344 ||| 14345 ||| 14346 ||| 14347 ||| 
2021 ||| federated learning with dynamic transformer for text to speech. ||| 14348 ||| 701 ||| 703 ||| 3114 ||| 14349 ||| 705 ||| 
2019 ||| using attention networks and adversarial augmentation for styrian dialect continuous sleepiness and baby sound recognition. ||| 12345 ||| 14350 ||| 14351 ||| 14352 ||| 14353 ||| 14354 ||| 14355 ||| 14356 ||| 14357 ||| 14358 ||| 14359 ||| 14360 ||| 12347 ||| 
2020 ||| multi-encoder-decoder transformer for code-switching speech recognition. ||| 14361 ||| 14362 ||| 13947 ||| 14363 ||| 12494 ||| 
2021 ||| multi-encoder learning and stream fusion for transformer-based end-to-end automatic speech recognition. ||| 13888 ||| 13890 ||| 13891 ||| 
2021 ||| detection of lexical stress errors in non-native (l2) english with data augmentation and attention. ||| 14364 ||| 14365 ||| 14366 ||| 14367 ||| 14368 ||| 14369 ||| 12016 ||| 14370 ||| 14371 ||| 
2021 ||| avatr: one-shot speaker extraction with transformers. ||| 14372 ||| 14373 ||| 14374 ||| 4051 ||| 14375 ||| 14376 ||| 
2021 ||| knowledge distillation from bert transformer to speech transformer for intent classification. ||| 14377 ||| 14378 ||| 13580 ||| 12494 ||| 
2021 ||| stochastic attention head removal: a simple and effective method for improving transformer based asr models. ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2021 ||| stableemit: selection probability discount for reducing emission latency of streaming monotonic attention asr. ||| 12682 ||| 4418 ||| 
2020 ||| gated multi-head attention pooling for weakly labelled audio tagging. ||| 12267 ||| 4430 ||| 11418 ||| 
2019 ||| detecting mismatch between speech and transcription using cross-modal attention. ||| 12087 ||| 8233 ||| 
2019 ||| conversational emotion analysis via attention mechanisms. ||| 6227 ||| 12041 ||| 2304 ||| 12758 ||| 
2021 ||| tdca-net: time-domain channel attention network for depression detection. ||| 14379 ||| 12759 ||| 2304 ||| 12041 ||| 14380 ||| 
2017 ||| directing attention during perceptual training: a preliminary study of phonetic learning in southern min by mandarin speakers. ||| 3503 ||| 14381 ||| 
2019 ||| large margin training for attention based end-to-end speech recognition. ||| 12655 ||| 12190 ||| 12586 ||| 3808 ||| 
2018 ||| encoder transfer for attention-based acoustic-to-word speech recognition. ||| 12681 ||| 4409 ||| 12683 ||| 14382 ||| 12221 ||| 14335 ||| 4411 ||| 4418 ||| 
2021 ||| automatic lip-reading with hierarchical pyramidal convolution and self-attention for image sequences with no word boundaries. ||| 14383 ||| 1010 ||| 14384 ||| 12372 ||| 14385 ||| 12404 ||| 
2020 ||| the implication of sound level on spatial selective auditory attention for cochlear implant users: behavioral and electrophysiological measurement. ||| 14386 ||| 14387 ||| 14388 ||| 
2019 ||| a saliency-based attention lstm model for cognitive load classification from speech. ||| 14389 ||| 14390 ||| 3882 ||| 14391 ||| 
2019 ||| multi-scale time-frequency attention for acoustic event detection. ||| 14392 ||| 14393 ||| 14394 ||| 329 ||| 
2020 ||| dual attention in time and frequency domain for voice activity detection. ||| 14395 ||| 14396 ||| 14397 ||| 
2019 ||| self attention in variational sequential learning for summarization. ||| 1488 ||| 14398 ||| 
2019 ||| multi-stream network with temporal attention for environmental sound classification. ||| 2419 ||| 14399 ||| 12359 ||| 
2019 ||| multi-task multi-resolution char-to-bpe cross-attention decoder for end-to-end speech recognition. ||| 13902 ||| 13901 ||| 13904 ||| 13905 ||| 13906 ||| 
2021 ||| contextualized attention-based knowledge transfer for spoken conversational question answering. ||| 7412 ||| 12623 ||| 4430 ||| 
2018 ||| an interlocutor-modulated attentional lstm for differentiating between subgroups of autism spectrum disorder. ||| 12346 ||| 14400 ||| 12347 ||| 
2019 ||| the influence of distraction on speech processing: how selective is selective attention? ||| 14401 ||| 14402 ||| 14403 ||| 14404 ||| 14405 ||| 14406 ||| 14407 ||| 14408 ||| 
2019 ||| attention model for articulatory features detection. ||| 14409 ||| 14410 ||| 
2021 ||| online compressive transformer for end-to-end speech recognition. ||| 14411 ||| 14412 ||| 1488 ||| 
2020 ||| end-to-end neural transformer based spoken language understanding. ||| 12484 ||| 12485 ||| 12487 ||| 
2021 ||| optimizing latency for online video captioning using audio-visual transformers. ||| 2507 ||| 2508 ||| 11981 ||| 
2020 ||| removing bias with residual mixture of multi-view attention for speech emotion recognition. ||| 13928 ||| 13929 ||| 8233 ||| 14413 ||| 
2018 ||| conversational analysis using utterance-level attention-based bidirectional recurrent neural networks. ||| 14414 ||| 14415 ||| 14416 ||| 1017 ||| 
2019 ||| transformer based grapheme-to-phoneme conversion. ||| 13988 ||| 2101 ||| 13989 ||| 13990 ||| 13310 ||| 13991 ||| 13992 ||| 
2020 ||| weakly supervised training of hierarchical attention networks for speaker identification. ||| 12086 ||| 12087 ||| 8233 ||| 
2020 ||| the effect of language dominance on the selective attention of segments and tones in urdu-cantonese speakers. ||| 1199 ||| 14417 ||| 
2017 ||| jointly trained sequential labeling and classification by sparse attention neural networks. ||| 14418 ||| 14419 ||| 14420 ||| 3251 ||| 3562 ||| 
2018 ||| triplet network with attention for speaker diarization. ||| 14421 ||| 14422 ||| 12005 ||| 12252 ||| 12006 ||| 
2021 ||| self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition. ||| 14423 ||| 14424 ||| 14425 ||| 14426 ||| 852 ||| 14427 ||| 4046 ||| 14428 ||| 
2020 ||| empirical interpretation of speech emotion perception with attention based model for speech emotion recognition. ||| 14429 ||| 13929 ||| 8233 ||| 
2020 ||| streaming transformer-based acoustic models using self-attention with augmented memory. ||| 11976 ||| 11973 ||| 11974 ||| 11978 ||| 11975 ||| 
2021 ||| ast: audio spectrogram transformer. ||| 1708 ||| 14430 ||| 12254 ||| 
2019 ||| language modeling with deep transformers. ||| 12659 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| real-time multi-channel speech enhancement based on neural network masking with attention model. ||| 14431 ||| 14432 ||| 14433 ||| 14434 ||| 
2017 ||| online adaptation of an attention-based neural network for natural language generation. ||| 14435 ||| 14436 ||| 2693 ||| 14437 ||| 14438 ||| 14439 ||| 
2020 ||| weak-attention suppression for transformer based speech recognition. ||| 11974 ||| 11973 ||| 11976 ||| 12449 ||| 11975 ||| 12489 ||| 11978 ||| 12491 ||| 
2020 ||| congruent audiovisual speech enhances cortical envelope tracking during auditory selective attention. ||| 8258 ||| 8260 ||| 
2017 ||| attention based cldnns for short-duration acoustic scene classification. ||| 14440 ||| 7957 ||| 14441 ||| 14442 ||| 
2019 ||| latent topic attention for domain classification. ||| 14443 ||| 14444 ||| 14445 ||| 14446 ||| 14447 ||| 
2020 ||| conversational emotion recognition using self-attention mechanisms and graph neural networks. ||| 6227 ||| 12041 ||| 2304 ||| 12758 ||| 14448 ||| 14449 ||| 
2020 ||| self-attention encoding and pooling for speaker recognition. ||| 12475 ||| 12474 ||| 12476 ||| 
2020 ||| all-in-one transformer: unifying speech recognition, audio tagging, and event detection. ||| 11980 ||| 12021 ||| 2508 ||| 11981 ||| 
2020 ||| voice transformer network: sequence-to-sequence voice conversion using transformer with text-to-speech pretraining. ||| 14450 ||| 8223 ||| 14451 ||| 12538 ||| 12130 ||| 
2021 ||| speech emotion recognition based on attention weight correction using word-level confidence measure. ||| 4401 ||| 4402 ||| 4403 ||| 14452 ||| 14453 ||| 
2020 ||| attention-driven projections for soundscape classification. ||| 14454 ||| 14455 ||| 14456 ||| 14457 ||| 
2021 ||| rapid speaker adaptation for conformer transducer: attention and bias are all you need. ||| 4648 ||| 14324 ||| 12179 ||| 12033 ||| 
2017 ||| attention networks for modeling behaviors in addiction counseling. ||| 14458 ||| 14459 ||| 14460 ||| 14461 ||| 14462 ||| 
2020 ||| mlnet: an adaptive multiple receptive-field attention neural network for voice activity detection. ||| 14463 ||| 701 ||| 704 ||| 12509 ||| 705 ||| 
2020 ||| speaker-utterance dual attention for speaker and utterance verification. ||| 14464 ||| 13581 ||| 13580 ||| 14465 ||| 12494 ||| 
2017 ||| attentional factors in listeners' uptake of gesture cues during speech processing. ||| 14466 ||| 14467 ||| 
2020 ||| affective conditioning on hierarchical attention networks applied to depression detection from transcribed clinical interviews. ||| 14468 ||| 3721 ||| 3729 ||| 14469 ||| 
2021 ||| streaming end-to-end speech recognition for hybrid rnn-t/attention architecture. ||| 4409 ||| 4407 ||| 12220 ||| 8251 ||| 12219 ||| 14470 ||| 4408 ||| 1491 ||| 14471 ||| 
2020 ||| parallel rescoring with transformer for streaming on-device speech recognition. ||| 3337 ||| 14472 ||| 3334 ||| 3336 ||| 12098 ||| 
2021 ||| multi-channel transformer transducer for speech recognition. ||| 12483 ||| 12484 ||| 12485 ||| 13949 ||| 
2021 ||| noise robust acoustic modeling for single-channel speech recognition based on a stream-wise transformer architecture. ||| 14473 ||| 12308 ||| 
2019 ||| spatio-temporal attention pooling for audio scene classification. ||| 12352 ||| 12354 ||| 3882 ||| 14474 ||| 12355 ||| 14475 ||| 12357 ||| 12358 ||| 
2020 ||| multi-stream attention-based blstm with feature segmentation for speech emotion recognition. ||| 14476 ||| 14477 ||| 14478 ||| 
2020 ||| improved hybrid streaming asr with transformer language models. ||| 14479 ||| 14480 ||| 11927 ||| 14481 ||| 4046 ||| 14482 ||| 14483 ||| 4252 ||| 14484 ||| 7111 ||| 14485 ||| 14486 ||| 
2019 ||| deep attention gated dilated temporal convolutional networks with intra-parallel convolutional modules for end-to-end monaural speech separation. ||| 14487 ||| 14488 ||| 11307 ||| 14489 ||| 12107 ||| 14490 ||| 
2019 ||| few-shot audio classification with attentional graph neural networks. ||| 8138 ||| 14491 ||| 14492 ||| 14493 ||| 
2020 ||| transformer-based long-context end-to-end speech recognition. ||| 2508 ||| 11980 ||| 2507 ||| 11981 ||| 
2017 ||| gaussian prediction based attention for online end-to-end speech recognition. ||| 4462 ||| 14494 ||| 12372 ||| 
2018 ||| analysing the focus of a hierarchical attention network: the importance of enjambments when classifying post-modern poetry. ||| 14495 ||| 14496 ||| 14497 ||| 
2020 ||| improving transformer-based speech recognition with unsupervised pre-training and multi-task semantic knowledge learning. ||| 14498 ||| 1556 ||| 14499 ||| 14500 ||| 
2020 ||| speech-xlnet: unsupervised acoustic model pretraining for self-attention networks. ||| 12584 ||| 14262 ||| 12585 ||| 3138 ||| 4530 ||| 4460 ||| 
2020 ||| exploration of audio quality assessment and anomaly localisation using attention models. ||| 12087 ||| 8233 ||| 
2021 ||| residual echo and noise cancellation with feature attention module and multi-domain loss function. ||| 14501 ||| 14502 ||| 14503 ||| 14504 ||| 8298 ||| 
2020 ||| speaker identification for household scenarios with self-attention and adversarial training. ||| 6596 ||| 9682 ||| 3748 ||| 14505 ||| 3842 ||| 
2020 ||| multispeech: multi-speaker text to speech with transformer. ||| 14506 ||| 14507 ||| 14508 ||| 14509 ||| 7725 ||| 12137 ||| 4789 ||| 
2020 ||| eeg-based short-time auditory attention detection using multi-task deep learning. ||| 14510 ||| 14511 ||| 5095 ||| 3608 ||| 14512 ||| 5093 ||| 
2020 ||| transformer with bidirectional decoder for speech recognition. ||| 5250 ||| 14513 ||| 14514 ||| 14515 ||| 7729 ||| 
2020 ||| durian: duration informed attention network for speech synthesis. ||| 12189 ||| 12572 ||| 14516 ||| 12188 ||| 12586 ||| 3096 ||| 10572 ||| 14517 ||| 4529 ||| 14518 ||| 4530 ||| 3808 ||| 
2020 ||| advancing multiple instance learning with attention modeling for categorical speech emotion recognition. ||| 14519 ||| 14520 ||| 14521 ||| 14522 ||| 
2021 ||| cross-modal transformer-based neural correction models for automatic speech recognition. ||| 4407 ||| 4408 ||| 10274 ||| 10276 ||| 4409 ||| 12220 ||| 10277 ||| 10275 ||| 
2021 ||| visual transformers for primates classification and covid detection. ||| 14523 ||| 14524 ||| 3831 ||| 14525 ||| 14526 ||| 
2020 ||| self-and-mixed attention decoder with deep acoustic structure for transformer-based lvcsr. ||| 14361 ||| 14527 ||| 14362 ||| 13947 ||| 13948 ||| 12494 ||| 
2020 ||| cross attention with monotonic alignment for speech transformer. ||| 14278 ||| 14279 ||| 14280 ||| 1313 ||| 4469 ||| 12203 ||| 
2020 ||| end-to-end asr with adaptive span self-attention. ||| 12245 ||| 12506 ||| 787 ||| 3549 ||| 12505 ||| 12507 ||| 
2020 ||| ctc-synchronous training for monotonic attention model. ||| 12682 ||| 12683 ||| 4418 ||| 
2019 ||| phonetically-aware embeddings, wide residual networks with time-delay neural networks and self attention models for the 2018 nist speaker recognition evaluation. ||| 14528 ||| 14529 ||| 14530 ||| 12378 ||| 14531 ||| 14532 ||| 12379 ||| 12380 ||| 4046 ||| 12381 ||| 
2021 ||| cough-based covid-19 detection with contextual attention convolutional neural networks and gender information. ||| 12680 ||| 14533 ||| 14534 ||| 11933 ||| 648 ||| 649 ||| 
2020 ||| self-distillation for improving ctc-transformer-based asr systems. ||| 4409 ||| 8251 ||| 8066 ||| 12219 ||| 4407 ||| 12220 ||| 4408 ||| 12221 ||| 1491 ||| 
2018 ||| stream attention for distributed multi-microphone speech recognition. ||| 12608 ||| 12609 ||| 12611 ||| 
2020 ||| a transformer-based audio captioning model with keyword estimation. ||| 8065 ||| 4408 ||| 14535 ||| 12574 ||| 12576 ||| 
2020 ||| temporal attention convolutional network for speech emotion recognition with latent representation. ||| 5246 ||| 5247 ||| 5093 ||| 5519 ||| 5248 ||| 5095 ||| 
2021 ||| estimating articulatory movements in speech production with transformer networks. ||| 14536 ||| 14537 ||| 14338 ||| 14339 ||| 14340 ||| 
2021 ||| nisqa: a deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. ||| 12197 ||| 14538 ||| 14539 ||| 12198 ||| 3831 ||| 
2019 ||| robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts. ||| 14540 ||| 14541 ||| 6879 ||| 
2020 ||| conformer: convolution-augmented transformer for speech recognition. ||| 14542 ||| 14472 ||| 3334 ||| 9133 ||| 9472 ||| 12814 ||| 14543 ||| 14544 ||| 11356 ||| 12067 ||| 3336 ||| 
2020 ||| identify speakers in cocktail parties with end-to-end attention. ||| 14545 ||| 14546 ||| 12396 ||| 
2019 ||| bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. ||| 14547 ||| 14548 ||| 
2019 ||| attention based hybrid i-vector blstm model for language recognition. ||| 12397 ||| 12398 ||| 12399 ||| 
2018 ||| monaural multi-talker speech recognition with attention mechanism and gated convolutional networks. ||| 12245 ||| 12247 ||| 3808 ||| 
2019 ||| very deep self-attention networks for end-to-end speech recognition. ||| 3517 ||| 14549 ||| 11470 ||| 14550 ||| 3831 ||| 3518 ||| 
2021 ||| temporal convolutional network with frequency dimension adaptive attention for speech enhancement. ||| 14551 ||| 14552 ||| 14553 ||| 7955 ||| 12494 ||| 
2018 ||| forward-backward attention decoder. ||| 12683 ||| 14382 ||| 4418 ||| 
2020 ||| hybrid transformer/ctc networks for hardware efficient voice triggering. ||| 14554 ||| 14555 ||| 14556 ||| 14557 ||| 13962 ||| 
2018 ||| self-assessed affect recognition using fusion of attentional blstm and static acoustic features. ||| 14351 ||| 12345 ||| 14558 ||| 14559 ||| 14560 ||| 14281 ||| 12347 ||| 
2021 ||| serialized multi-layer multi-head attention for neural speaker embedding. ||| 14561 ||| 14562 ||| 12494 ||| 
2018 ||| improved training of end-to-end attention models for speech recognition. ||| 12532 ||| 12659 ||| 12533 ||| 12534 ||| 3454 ||| 
2018 ||| attention-based sequence classification for affect detection. ||| 14563 ||| 14564 ||| 14565 ||| 14566 ||| 14567 ||| 14568 ||| 14569 ||| 
2019 ||| an online attention-based model for speech recognition. ||| 12771 ||| 12300 ||| 5110 ||| 4459 ||| 14570 ||| 
2019 ||| a hierarchical attention network-based approach for depression detection from transcribed clinical interviews. ||| 12680 ||| 645 ||| 12147 ||| 647 ||| 648 ||| 649 ||| 
2021 ||| graph attention networks for anti-spoofing. ||| 14571 ||| 12728 ||| 14572 ||| 14573 ||| 14574 ||| 
2020 ||| low latency auditory attention detection with common spatial pattern analysis of eeg signals. ||| 14575 ||| 14576 ||| 14577 ||| 14578 ||| 12494 ||| 
2020 ||| attention forcing for speech synthesis. ||| 14579 ||| 14580 ||| 3796 ||| 
2019 ||| learning how to listen: a temporal-frequential attention model for sound event detection. ||| 11777 ||| 11778 ||| 11779 ||| 
2021 ||| out-of-vocabulary words detection with attention and ctc alignments in an end-to-end asr system. ||| 14581 ||| 12222 ||| 10500 ||| 10501 ||| 14582 ||| 
2019 ||| attention-enhanced connectionist temporal classification for discrete speech emotion recognition. ||| 645 ||| 12309 ||| 11983 ||| 647 ||| 12310 ||| 648 ||| 649 ||| 
2021 ||| streaming transformer for hardware efficient voice trigger detection and false trigger mitigation. ||| 14555 ||| 13922 ||| 14556 ||| 14554 ||| 14557 ||| 14583 ||| 13962 ||| 
2020 ||| multi-task network for noise-robust keyword spotting and speaker verification using ctc-based soft vad and global query attention. ||| 14584 ||| 14396 ||| 14585 ||| 14397 ||| 
2019 ||| predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models. ||| 4044 ||| 4045 ||| 4046 ||| 14391 ||| 4047 ||| 4048 ||| 4049 ||| 4046 ||| 
2020 ||| attentron: few-shot text-to-speech utilizing attention-based variable-length embedding. ||| 14586 ||| 14587 ||| 14588 ||| 14589 ||| 
2017 ||| prosodic analysis of attention-drawing speech. ||| 14590 ||| 14591 ||| 14592 ||| 
2021 ||| t5g2p: using text-to-text transfer transformer for grapheme-to-phoneme conversion. ||| 14593 ||| 14594 ||| 14595 ||| 11934 ||| 14596 ||| 
2019 ||| improved end-to-end speech emotion recognition using self attention mechanism and multitask learning. ||| 14597 ||| 14598 ||| 4418 ||| 
2019 ||| investigation of transformer based spelling correction model for ctc-based end-to-end mandarin speech recognition. ||| 14494 ||| 14599 ||| 14600 ||| 
2018 ||| unsupervised word segmentation from speech with attention. ||| 4736 ||| 19 ||| 14601 ||| 13942 ||| 1226 ||| 4737 ||| 13943 ||| 3510 ||| 
2019 ||| end-to-end multi-channel speech enhancement using inter-channel time-restricted attention on raw waveform. ||| 14602 ||| 10964 ||| 14603 ||| 10965 ||| 10966 ||| 
2020 ||| end-to-end keyword search based on attention and energy scorer for low resource languages. ||| 14604 ||| 11779 ||| 
2021 ||| end-to-end neural diarization: from transformer to conformer. ||| 12123 ||| 14605 ||| 14606 ||| 3842 ||| 
2020 ||| peking opera synthesis via duration informed attention network. ||| 13512 ||| 13516 ||| 12189 ||| 12572 ||| 12586 ||| 14271 ||| 3808 ||| 
2019 ||| variational attention using articulatory priors for generating code mixed speech using monolingual corpora. ||| 14342 ||| 4890 ||| 
2018 ||| syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese. ||| 5268 ||| 5269 ||| 5270 ||| 728 ||| 
2018 ||| self-attentional acoustic models. ||| 3516 ||| 11470 ||| 3067 ||| 14607 ||| 69 ||| 3518 ||| 
2021 ||| detection and analysis of attention errors in sequence-to-sequence text-to-speech. ||| 14608 ||| 14609 ||| 
2020 ||| speaker adaptive training for speech recognition based on attention-over-attention mechanism. ||| 4436 ||| 4434 ||| 14610 ||| 14611 ||| 4438 ||| 
2021 ||| dropout regularization for self-supervised learning of transformer encoder speech representation. ||| 12509 ||| 701 ||| 704 ||| 705 ||| 
2019 ||| self-attention for speech emotion recognition. ||| 14612 ||| 14613 ||| 14614 ||| 
2021 ||| simulating reading mistakes for child speech transformer-based phone recognition. ||| 14615 ||| 14616 ||| 14617 ||| 14618 ||| 
2019 ||| individual differences in implicit attention to phonetic detail in speech perception. ||| 14619 ||| 14620 ||| 
2020 ||| atss-net: target speaker separation via attention-based neural network. ||| 14621 ||| 14622 ||| 14623 ||| 765 ||| 
2021 ||| transformer based end-to-end mispronunciation detection and diagnosis. ||| 14624 ||| 8293 ||| 14625 ||| 4460 ||| 
2019 ||| adapting transformer to end-to-end spoken language translation. ||| 14626 ||| 14627 ||| 14628 ||| 
2017 ||| attention-based lstm with multi-task learning for distant speech recognition. ||| 9472 ||| 12104 ||| 8298 ||| 
2021 ||| transformer-based asr incorporating time-reduction layer and fine-tuning with self-knowledge distillation. ||| 14629 ||| 12133 ||| 14630 ||| 
2019 ||| an analysis of local monotonic attention variants. ||| 3369 ||| 14631 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| optimally encoding inductive biases into the transformer improves end-to-end speech translation. ||| 14632 ||| 14633 ||| 12712 ||| 
2020 ||| group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition. ||| 4968 ||| 8293 ||| 4460 ||| 
2021 ||| neural speaker extraction with speaker-speech cross-attention network. ||| 12134 ||| 12565 ||| 5193 ||| 12494 ||| 
2021 ||| tecanet: temporal-contextual attention network for environment-aware speech dereverberation. ||| 12636 ||| 11346 ||| 14634 ||| 12188 ||| 12293 ||| 1125 ||| 14635 ||| 12586 ||| 4530 ||| 3808 ||| 
2021 ||| an improved single step non-autoregressive transformer for automatic speech recognition. ||| 12771 ||| 12772 ||| 12773 ||| 705 ||| 14442 ||| 
2019 ||| lattice generation in attention-based speech recognition models. ||| 14636 ||| 14637 ||| 14638 ||| 14639 ||| 
2020 ||| should we hard-code the recurrence concept or learn it instead ? exploring the transformer architecture for audio-visual speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2021 ||| factorization-aware training of transformers for natural language understanding on the edge. ||| 14643 ||| 14644 ||| 14645 ||| 14646 ||| 
2021 ||| dual causal/non-causal self-attention for streaming end-to-end speech recognition. ||| 11980 ||| 2508 ||| 11981 ||| 
2018 ||| an attention pooling based representation learning method for speech emotion recognition. ||| 14647 ||| 3198 ||| 12357 ||| 4395 ||| 4463 ||| 
2020 ||| universal speech transformer. ||| 14278 ||| 14279 ||| 14280 ||| 1313 ||| 4469 ||| 12203 ||| 
2019 ||| self multi-head attention for speaker recognition. ||| 12474 ||| 12475 ||| 12476 ||| 
2020 ||| dual-path transformer network: direct context-aware modeling for end-to-end monaural speech separation. ||| 14648 ||| 14649 ||| 8709 ||| 
2019 ||| environment-dependent attention-driven recurrent convolutional neural network for robust speech enhancement. ||| 5193 ||| 5093 ||| 5192 ||| 14650 ||| 5095 ||| 11688 ||| 
2021 ||| multimodal sentiment analysis with temporal modality attention. ||| 14651 ||| 12107 ||| 
2018 ||| multi-modal attention mechanisms in lstm and its application to acoustic scene classification. ||| 6163 ||| 12362 ||| 5259 ||| 
2019 ||| sequence-to-sequence learning via attention transfer for incremental speech recognition. ||| 14652 ||| 12303 ||| 13907 ||| 11757 ||| 
2020 ||| text-independent speaker verification with dual attention network. ||| 14653 ||| 14522 ||| 
2021 ||| improving multilingual transformer transducer models by reducing language confusions. ||| 14654 ||| 12179 ||| 13892 ||| 2280 ||| 13201 ||| 12389 ||| 12033 ||| 
2017 ||| attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging. ||| 1125 ||| 12618 ||| 12087 ||| 11418 ||| 12619 ||| 
2020 ||| whisper activity detection using cnn-lstm based attention pooling network trained for a speaker identification task. ||| 14655 ||| 14656 ||| 14340 ||| 
2020 ||| evolved speech-transformer: applying neural architecture search to end-to-end automatic speech recognition. ||| 14657 ||| 14658 ||| 14659 ||| 14660 ||| 
2021 ||| transformer-based acoustic modeling for streaming speech synthesis. ||| 11976 ||| 12527 ||| 11974 ||| 14661 ||| 12449 ||| 12528 ||| 12529 ||| 8880 ||| 
2020 ||| spike-triggered non-autoregressive transformer for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 12243 ||| 3364 ||| 12244 ||| 
2019 ||| an attention-based hybrid network for automatic detection of alzheimer's disease from narrative speech. ||| 1785 ||| 8800 ||| 12749 ||| 
2021 ||| mixture model attention: flexible streaming and non-streaming automatic speech recognition. ||| 14662 ||| 14663 ||| 14664 ||| 14665 ||| 
2021 ||| feature fusion by attention networks for robust doa estimation. ||| 14666 ||| 14667 ||| 5250 ||| 
2020 ||| streaming chunk-aware multihead attention for online end-to-end speech recognition. ||| 14494 ||| 14668 ||| 14669 ||| 14599 ||| 8479 ||| 14600 ||| 12384 ||| 
2019 ||| self-attention transducers for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 12243 ||| 12244 ||| 
2020 ||| lvcsr with transformer language models. ||| 14670 ||| 12533 ||| 12534 ||| 3454 ||| 
2020 ||| multimodal speech emotion recognition using cross attention with aligned audio and text. ||| 14671 ||| 12462 ||| 10565 ||| 
2020 ||| singing voice extraction with attention-based spectrograms fusion. ||| 14650 ||| 5093 ||| 4417 ||| 14672 ||| 5193 ||| 5192 ||| 5095 ||| 14673 ||| 
2020 ||| a recursive network with dynamic attention for monaural speech enhancement. ||| 14674 ||| 4384 ||| 14675 ||| 14676 ||| 14677 ||| 
2021 ||| domain-aware self-attention for multi-domain neural machine translation. ||| 14678 ||| 9337 ||| 3181 ||| 3803 ||| 3470 ||| 
2021 ||| on-device streaming transformer-based end-to-end speech recognition. ||| 14679 ||| 14680 ||| 
2020 ||| jdi-t: jointly trained duration informed transformer for text-to-speech without explicit alignment. ||| 14681 ||| 14682 ||| 14683 ||| 14684 ||| 14685 ||| 14686 ||| 
2020 ||| asr error correction with augmented transformer for entity retrieval. ||| 3091 ||| 14687 ||| 575 ||| 14688 ||| 14689 ||| 1305 ||| 
2019 ||| vectorized beam search for ctc-attention-based speech recognition. ||| 14673 ||| 2508 ||| 3549 ||| 11980 ||| 11981 ||| 
2018 ||| improving mandarin tone recognition using convolutional bidirectional long short-term memory with attention. ||| 14690 ||| 14691 ||| 14692 ||| 
2020 ||| semantic mask for transformer based end-to-end speech recognition. ||| 14693 ||| 2280 ||| 14694 ||| 12179 ||| 12389 ||| 14310 ||| 14695 ||| 14324 ||| 12137 ||| 3480 ||| 
2021 ||| end to end transformer-based contextual speech recognition based on pointer network. ||| 12075 ||| 12076 ||| 
2019 ||| rwth asr systems for librispeech: hybrid vs attention. ||| 14696 ||| 14697 ||| 14670 ||| 12659 ||| 14698 ||| 14320 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2020 ||| singing synthesis: with a little help from my attention. ||| 14699 ||| 14700 ||| 14701 ||| 14370 ||| 
2021 ||| multi-mode transformer transducer with stochastic future context. ||| 13904 ||| 14702 ||| 14703 ||| 13963 ||| 3549 ||| 
2019 ||| improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation. ||| 4417 ||| 14704 ||| 14705 ||| 14706 ||| 4418 ||| 12308 ||| 
2020 ||| abstractive spoken document summarization using hierarchical model with multi-stage attention diversity optimization. ||| 3795 ||| 3796 ||| 10536 ||| 
2020 ||| speech driven talking head generation via attentional landmarks based representation. ||| 14707 ||| 247 ||| 14708 ||| 14709 ||| 13948 ||| 14710 ||| 
2021 ||| real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network. ||| 14711 ||| 14712 ||| 
2020 ||| acoustic scene analysis with multi-head attention networks. ||| 14713 ||| 14714 ||| 2390 ||| 1589 ||| 
2018 ||| who are you listening to? towards a dynamic measure of auditory attention to speech-on-speech. ||| 14715 ||| 14716 ||| 14717 ||| 14718 ||| 14719 ||| 14720 ||| 
2019 ||| cross-attention end-to-end asr for two-party conversations. ||| 12177 ||| 12405 ||| 14721 ||| 
2020 ||| naagn: noise-aware attention-gated network for speech enhancement. ||| 4522 ||| 4100 ||| 4523 ||| 10922 ||| 185 ||| 
2020 ||| finnish asr with deep transformer models. ||| 14722 ||| 12311 ||| 14723 ||| 14724 ||| 11941 ||| 
2020 ||| noisy-reverberant speech enhancement using denseunet with time-frequency attention. ||| 10646 ||| 14725 ||| 
2020 ||| bi-encoder transformer network for mandarin-english code-switching speech recognition using mixture of experts. ||| 14726 ||| 14727 ||| 1705 ||| 14728 ||| 12247 ||| 
2021 ||| attention-based convolutional neural network for asv spoofing detection. ||| 975 ||| 14729 ||| 14730 ||| 14731 ||| 977 ||| 
2020 ||| multilingual speech recognition with self-attention structured parameterization. ||| 6577 ||| 14732 ||| 12614 ||| 14664 ||| 14733 ||| 14734 ||| 12612 ||| 12613 ||| 14735 ||| 14736 ||| 14665 ||| 2251 ||| 
2021 ||| attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams. ||| 14737 ||| 14738 ||| 14739 ||| 14740 ||| 14741 ||| 12526 ||| 14742 ||| 
2021 ||| vad-free streaming hybrid ctc/attention asr for unsegmented recording. ||| 12682 ||| 4418 ||| 
2019 ||| a time delay neural network with shared weight self-attention for small-footprint keyword spotting. ||| 12243 ||| 12242 ||| 12041 ||| 12244 ||| 12241 ||| 14743 ||| 14675 ||| 
2021 ||| shallow convolution-augmented transformer with differentiable neural computer for low-complexity classification of variable-length acoustic scene. ||| 14744 ||| 14745 ||| 14746 ||| 
2019 ||| pyramid memory block and timestep attention for speech emotion recognition. ||| 12260 ||| 14747 ||| 14748 ||| 5051 ||| 
2018 ||| end-to-end audio replay attack detection using deep convolutional networks with attention. ||| 14749 ||| 14750 ||| 14751 ||| 
2021 ||| triple m: a practical text-to-speech synthesis system with multi-guidance attention and multi-band multi-time lpcnet. ||| 14752 ||| 12002 ||| 14753 ||| 12001 ||| 12003 ||| 
2020 ||| multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks. ||| 14754 ||| 14755 ||| 
2020 ||| conv-transformer transducer: low latency, low frame rate, streamable end-to-end speech recognition. ||| 14756 ||| 14757 ||| 14758 ||| 3889 ||| 
2021 ||| an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech. ||| 14759 ||| 14760 ||| 5950 ||| 14761 ||| 14762 ||| 2014 ||| 
2019 ||| neural text clustering with document-level attention based on dynamic soft labels. ||| 3148 ||| 4395 ||| 12372 ||| 4894 ||| 1010 ||| 
2020 ||| san-m: memory equipped self-attention for end-to-end speech recognition. ||| 14668 ||| 14494 ||| 14599 ||| 12357 ||| 
2018 ||| attention-based end-to-end models for small-footprint keyword spotting. ||| 12394 ||| 12757 ||| 4551 ||| 12384 ||| 
2017 ||| parallel hierarchical attention networks with shared memory reader for multi-stream conversational document classification. ||| 14763 ||| 4408 ||| 14764 ||| 
2020 ||| single headed attention based sequence-to-sequence model for state-of-the-art results on switchboard. ||| 12413 ||| 12414 ||| 12415 ||| 14765 ||| 14662 ||| 12416 ||| 
2017 ||| an analysis of "attention" in sequence-to-sequence models. ||| 12065 ||| 12066 ||| 1717 ||| 14766 ||| 14767 ||| 
2021 ||| improving streaming transformer based asr under a framework of self-supervised learning. ||| 13915 ||| 14768 ||| 14769 ||| 14770 ||| 12395 ||| 13916 ||| 13917 ||| 
2021 ||| learning mutual correlation in multimodal transformer for speech emotion recognition. ||| 14771 ||| 14772 ||| 14773 ||| 14774 ||| 14775 ||| 
2017 ||| end-to-end speech recognition with auditory attention for multi-microphone distance speech recognition. ||| 12177 ||| 14548 ||| 
2018 ||| image semantic description based on deep learning with multi-attention mechanisms. ||| 1825 ||| 14776 ||| 
2018 ||| attention-based temporal weighted convolutional neural network for action recognition. ||| 14777 ||| 6436 ||| 14778 ||| 6437 ||| 8674 ||| 14779 ||| 
2018 ||| content-aware attention network for action recognition. ||| 14778 ||| 6436 ||| 14779 ||| 
2018 ||| the cognitive philosophical problems in visual attention and its influence on artificial intelligence modeling. ||| 14780 ||| 
2019 ||| an eye-tracking dataset for visual attention modelling in a virtual museum context. ||| 14781 ||| 14782 ||| 14783 ||| 14784 ||| 14785 ||| 14786 ||| 
2021 ||| dg-trans: automatic code summarization via dynamic graph attention-based transformer. ||| 14787 ||| 6514 ||| 14788 ||| 
2021 ||| aclm: software aging prediction of virtual machine monitor based on attention mechanism of cnn-lstm model. ||| 14789 ||| 2058 ||| 
2021 ||| a novel api recommendation approach by using graph attention network. ||| 14790 ||| 6514 ||| 14791 ||| 
2020 ||| a sentiment classification model based on bi-directional lstm with positional attention for fresh food consumer reviews. ||| 14792 ||| 14793 ||| 14794 ||| 369 ||| 
2021 ||| a transformer based sales prediction of smart container in new retail era. ||| 14795 ||| 4160 ||| 14796 ||| 
2019 ||| practice in caption generation with keras: the design and evaluation for attention models. ||| 14797 ||| 14798 ||| 
2018 ||| attention-based neural network for short-text question answering. ||| 14799 ||| 1748 ||| 
2020 ||| now, over here: leveraging extended attentional capabilities in human-robot interaction. ||| 14800 ||| 14801 ||| 14802 ||| 14803 ||| 
2020 ||| modeling the interplay of trust and attention in hri: an autonomous vehicle study. ||| 14804 ||| 14805 ||| 14806 ||| 14807 ||| 
2020 ||| attention-based multimodal fusion for estimating human emotion in real-world hri. ||| 14597 ||| 14598 ||| 14808 ||| 
2019 ||| welcoming robot behaviors for drawing attention. ||| 14809 ||| 14810 ||| 14811 ||| 
2019 ||| welcoming robot behaviors for drawing attention. ||| 14809 ||| 14810 ||| 14811 ||| 
2017 ||| movers, shakers, and those who stand still: visual attention-grabbing techniques in robot teleoperation. ||| 14812 ||| 14813 ||| 14814 ||| 14815 ||| 
2020 ||| addressing attention difficulties in autistic children using multimodal cues from a humanoid robot. ||| 14816 ||| 14817 ||| 14818 ||| 14819 ||| 14820 ||| 14821 ||| 
2021 ||| effects of gaze and arm motion kinesics on a humanoid's perceived confidence, eagerness to learn, and attention to the task in a teaching scenario. ||| 14822 ||| 14823 ||| 14824 ||| 14825 ||| 
2020 ||| joint attention estimator. ||| 14826 ||| 14827 ||| 14828 ||| 14829 ||| 
2021 ||| design and cost analysis of two different types of 400 kva distribution transformers. ||| 14830 ||| 14831 ||| 
2017 ||| evaluation of orientation performance of attention patterns for blind person. ||| 14832 ||| 14833 ||| 14834 ||| 14835 ||| 14836 ||| 
2019 ||| attention-guided model for robust face detection system. ||| 14837 ||| 4353 ||| 
2021 ||| the influence of media attention and equity incentives on corporate tax avoidance in the information age. ||| 14838 ||| 
2018 ||| does the research question structure impact the attention model? user study experiment. ||| 14839 ||| 14840 ||| 14841 ||| 
2020 ||| locating cephalometric x-ray landmarks with foveated pyramid attention. ||| 14842 ||| 14843 ||| 
2021 ||| feedback graph attention convolutional network for mr images enhancement by exploring self-similarity features. ||| 14844 ||| 14845 ||| 8621 ||| 14846 ||| 14847 ||| 3473 ||| 14848 ||| 
2021 ||| image sequence generation and analysis via gru and attention for trachomatous trichiasis classification. ||| 14849 ||| 14850 ||| 14851 ||| 14852 ||| 14853 ||| 14854 ||| 14855 ||| 14856 ||| 14857 ||| 
2020 ||| prostate cancer semantic segmentation by gleason score group in bi-parametric mri with self attention model on the peripheral zone. ||| 14858 ||| 14859 ||| 14860 ||| 
2020 ||| automated labelling using an attention model for radiology reports of mri scans (alarm). ||| 14861 ||| 14862 ||| 14863 ||| 14864 ||| 14865 ||| 14866 ||| 14867 ||| 14868 ||| 14869 ||| 14870 ||| 14871 ||| 14872 ||| 14873 ||| 7111 ||| 14874 ||| 14875 ||| 14876 ||| 
2020 ||| automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study. ||| 14877 ||| 14878 ||| 14879 ||| 14880 ||| 14881 ||| 14882 ||| 
2021 ||| unsupervised brain anomaly detection and segmentation with transformers. ||| 14883 ||| 14884 ||| 14885 ||| 14886 ||| 14887 ||| 7111 ||| 14874 ||| 14888 ||| 
2019 ||| group-attention single-shot detector (ga-ssd): finding pulmonary nodules in large-scale ct images. ||| 14889 ||| 2008 ||| 14846 ||| 14848 ||| 14890 ||| 14891 ||| 14892 ||| 
2019 ||| care: class attention to regions of lesion for classification on imbalanced data. ||| 14893 ||| 14894 ||| 14895 ||| 14896 ||| 14897 ||| 
2021 ||| attention via scattering transforms for segmentation of small intravascular ultrasound data sets. ||| 14898 ||| 14899 ||| 14900 ||| 14901 ||| 14902 ||| 14903 ||| 14904 ||| 14905 ||| 14906 ||| 
2020 ||| sau-net: efficient 3d spine mri segmentation using inter-slice attention. ||| 3757 ||| 14907 ||| 14908 ||| 14909 ||| 
2019 ||| xlsor: a robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. ||| 14910 ||| 14911 ||| 705 ||| 14912 ||| 
2019 ||| assessing knee oa severity with cnn attention-based end-to-end architectures. ||| 11480 ||| 14913 ||| 14914 ||| 1674 ||| 14915 ||| 14916 ||| 1675 ||| 
2021 ||| security requirements classification into groups using nlp transformers. ||| 14917 ||| 14918 ||| 
2021 ||| power grid cascading failure prediction based on transformer. ||| 14919 ||| 2008 ||| 14920 ||| 
2021 ||| incorporating transformer models for sentiment analysis and news classification in khmer. ||| 14921 ||| 14922 ||| 
2021 ||| deep bangla authorship attribution using transformer models. ||| 14922 ||| 14923 ||| 
2018 ||| enable an innovative prolonged exposure therapy of attention deficits on autism spectrum through adaptive virtual environments. ||| 14924 ||| 14925 ||| 
2019 ||| improving visual attention guiding by differentiation between fine and coarse navigation. ||| 14926 ||| 14927 ||| 14928 ||| 
2017 ||| ynu-hpcc at ijcnlp-2017 task 5: multi-choice question answering in exams using an attention-based lstm model. ||| 10523 ||| 3312 ||| 3313 ||| 3315 ||| 
2017 ||| ynu-hpcc at ijcnlp-2017 task 4: attention-based bi-directional gru model for customer feedback analysis task of english. ||| 8292 ||| 3313 ||| 3315 ||| 
2020 ||| reconstructing event regions for event extraction via graph attention networks. ||| 14929 ||| 1822 ||| 3129 ||| 14930 ||| 3128 ||| 14931 ||| 1418 ||| 
2017 ||| sentence modeling with deep neural architecture using lexicon and character attention mechanism for sentiment classification. ||| 7538 ||| 7539 ||| 
2020 ||| comparing probabilistic, distributional and transformer-based models on logical metonymy interpretation. ||| 14232 ||| 8955 ||| 10699 ||| 14234 ||| 8956 ||| 
2017 ||| key-value attention mechanism for neural machine translation. ||| 14932 ||| 4907 ||| 4908 ||| 14933 ||| 
2020 ||| graph attention network with memory fusion for aspect-level sentiment analysis. ||| 1722 ||| 3313 ||| 3314 ||| 3315 ||| 
2017 ||| cascading multiway attentions for document-level sentiment classification. ||| 14934 ||| 11660 ||| 14935 ||| 11662 ||| 3751 ||| 
2017 ||| what does attention in neural machine translation pay attention to? ||| 14936 ||| 11693 ||| 
2020 ||| two-headed monster and crossed co-attention networks. ||| 14937 ||| 800 ||| 
2017 ||| ynudlg at ijcnlp-2017 task 5: a cnn-lstm model with attention for multi-choice question answering in examinations. ||| 214 ||| 14938 ||| 10465 ||| 14939 ||| 10466 ||| 
2017 ||| supervised attention for sequence-to-sequence constituency parsing. ||| 4982 ||| 4983 ||| 4984 ||| 14010 ||| 14011 ||| 3745 ||| 
2017 ||| multilingual hierarchical attention networks for document classification. ||| 14940 ||| 14941 ||| 
2020 ||| training with adversaries to improve faithfulness of attention in neural machine translation. ||| 14942 ||| 14943 ||| 14944 ||| 
2020 ||| making a point: pointer-generator transformers for disjoint vocabularies. ||| 14945 ||| 14946 ||| 
2017 ||| local monotonic attention mechanism for end-to-end speech and language processing. ||| 12303 ||| 13907 ||| 11757 ||| 
2020 ||| heads-up! unsupervised constituency parsing via self-attention heads. ||| 2607 ||| 14947 ||| 3713 ||| 13068 ||| 
2017 ||| cky-based convolutional attention for neural machine translation. ||| 14948 ||| 3784 ||| 3785 ||| 
2020 ||| transformer-based approach for predicting chemical compound structures. ||| 11674 ||| 14949 ||| 14950 ||| 3784 ||| 3785 ||| 
2020 ||| multi-source attention for unsupervised domain adaptation. ||| 14951 ||| 14952 ||| 
2021 ||| attention convolutional u-net for automatic liver tumor segmentation. ||| 14953 ||| 14954 ||| 
2018 ||| generating abstractive summaries using sequence to sequence attention model. ||| 14955 ||| 14956 ||| 
2021 ||| a vision transformer with improved leff and vision combinative self-attention mechanism for waste image classification. ||| 14957 ||| 14958 ||| 14959 ||| 14960 ||| 14961 ||| 14962 ||| 14963 ||| 
2021 ||| influence and simulation of multibarrier isolation facilities on noise attenuation distribution of transformer. ||| 14964 ||| 14965 ||| 14966 ||| 
2021 ||| it is time to laugh: discovering specific contexts for laughter with attention mechanism. ||| 14967 ||| 14968 ||| 14969 ||| 
2021 ||| influence and simulation of transformer firewall device on audible noise propagation characteristics. ||| 14964 ||| 14965 ||| 1858 ||| 
2020 ||| evaluating the effect of user-given guiding attention on the learning process. ||| 14970 ||| 14971 ||| 14972 ||| 4194 ||| 14973 ||| 14974 ||| 
2017 ||| convergence of media attention across 129 countries. ||| 9045 ||| 14975 ||| 9044 ||| 
2018 ||| quantifying media influence and partisan attention on twitter during the uk eu referendum. ||| 14976 ||| 86 ||| 14977 ||| 14978 ||| 14979 ||| 14980 ||| 
2018 ||| assessing competition for social media attention among non-profits. ||| 14981 ||| 14982 ||| 11005 ||| 
2017 ||| attention please! - exploring attention management on wikipedia in the context of the ukrainian crisis. ||| 14983 ||| 14984 ||| 
2021 ||| multi-head fusion attention for transformer-based end-to-end automatic speech recognition. ||| 13888 ||| 13889 ||| 13890 ||| 13891 ||| 
2018 ||| a gaze-based attention model for spatially-aware hearing aids. ||| 14985 ||| 14986 ||| 14987 ||| 14988 ||| 
2020 ||| blac: a named entity recognition model incorporating part-of-speech attention in irregular short text. ||| 8952 ||| 14989 ||| 14990 ||| 5218 ||| 
2021 ||| attention residual network with 3d convolutional neural network for 3d human pose estimation. ||| 14991 ||| 14992 ||| 
2021 ||| towards autonomous driving decision by combining self-attention and deep reinforcement learning. ||| 14993 ||| 14994 ||| 1302 ||| 14995 ||| 14996 ||| 14997 ||| 
2021 ||| attention mechanism-based monocular depth estimation and visual odometry. ||| 14998 ||| 14999 ||| 15000 ||| 15001 ||| 15002 ||| 15003 ||| 
2017 ||| attention guiding techniques using peripheral vision and eye tracking for feedback in augmented-reality-based assistance systems. ||| 13866 ||| 13867 ||| 
2020 ||| attention-based neural networks for sentiment attitude extraction using distant supervision. ||| 7567 ||| 7568 ||| 
2020 ||| attention-based text recognition in the wild. ||| 15004 ||| 15005 ||| 
2021 ||| selective and divided attention for vibrotactile stimuli on both arms. ||| 15006 ||| 10370 ||| 15007 ||| 4046 ||| 15008 ||| 
2017 ||| can tactile suppression be explained by attentional capture? ||| 15009 ||| 15010 ||| 
2019 ||| exogenous cueing of visual attention using small, directional, tactile cues applied to the fingertip. ||| 15011 ||| 15012 ||| 15013 ||| 15014 ||| 15015 ||| 
2019 ||| electromagnetic force of power transformer with different short circuit current based on fem. ||| 15016 ||| 15017 ||| 15018 ||| 15019 ||| 
2019 ||| operator functional state: measure it with attention intensity and selectivity, explain it with cognitive control. ||| 15020 ||| 15021 ||| 15022 ||| 15023 ||| 
2019 ||| enhancing transformer for end-to-end speech-to-text translation. ||| 14626 ||| 14627 ||| 15024 ||| 15025 ||| 14628 ||| 
2021 ||| product review translation using phrase replacement and attention guided noise augmentation. ||| 10309 ||| 15026 ||| 15027 ||| 165 ||| 
2019 ||| a multi-hop attention for rnn based neural machine translation. ||| 3740 ||| 3741 ||| 3742 ||| 3743 ||| 3744 ||| 3745 ||| 
2021 ||| frozen pretrained transformers for neural sign language translation. ||| 15028 ||| 15029 ||| 15030 ||| 15031 ||| 15032 ||| 15033 ||| 15034 ||| 
2020 ||| leveraging multilingual transformers for hate speech detection. ||| 15035 ||| 15036 ||| 10508 ||| 15037 ||| 1185 ||| 
2020 ||| cfilt iit bombay@hasoc-dravidian-codemix fire 2020: assisting ensemble of transformers with random transliteration. ||| 15038 ||| 405 ||| 
2019 ||| ynu_wb at hasoc 2019: ordered neurons lstm with attention for identifying hate speech and offensive language. ||| 379 ||| 15039 ||| 10579 ||| 10466 ||| 
2020 ||| huiping shi@hasoc 2020: multi-top k self-attention with k-max pooling for discrimination between hate profane and offensive posts. ||| 15040 ||| 10466 ||| 
2020 ||| astralis @ hasoc 2020: analysis on identification of hate speech in indo-european languages with fine-tuned transformers. ||| 15041 ||| 15042 ||| 15043 ||| 
2019 ||| 3idiots at hasoc 2019: fine-tuning transformer neural networks for hate speech identification in indo-european languages. ||| 8412 ||| 8410 ||| 
2020 ||| bi-directional encoder representation of transformer model for sequential music recommender system. ||| 15044 ||| 10306 ||| 
2021 ||| attention based end to end speech recognition for voice search in hindi and english. ||| 15045 ||| 15046 ||| 
2020 ||| cmsaone@dravidian-codemix-fire2020: a meta embedding and transformer model for code-mixed sentiment analysis on social media text. ||| 14016 ||| 10510 ||| 
2019 ||| multi-task bidirectional transformer representations for irony detection. ||| 10441 ||| 3152 ||| 
2020 ||| hub@hasoc 2020: fine-tuning pre-trained transformer language models for hate speech and offensive content identification in indo-european languages. ||| 7828 ||| 1281 ||| 
2020 ||| spectre@aila-fire2020: supervised rhetorical role labeling for legal judgments using transformers. ||| 15047 ||| 15048 ||| 15049 ||| 
2020 ||| attention based anaphora resolution for code-mixed social media text for hindi language. ||| 15050 ||| 15051 ||| 405 ||| 
2021 ||| e.t.: re-thinking self-attention for transformer models on gpus. ||| 14063 ||| 9871 ||| 11017 ||| 9874 ||| 15052 ||| 15053 ||| 11024 ||| 11023 ||| 
2020 ||| calling attention to passages for biomedical question answering. ||| 15054 ||| 7111 ||| 15055 ||| 
2018 ||| attention-based neural text segmentation. ||| 7251 ||| 15056 ||| 1186 ||| 1185 ||| 
2021 ||| multi-head self-attention with role-guided masks. ||| 15057 ||| 15058 ||| 15059 ||| 15060 ||| 15061 ||| 15062 ||| 15063 ||| 
2019 ||| end-to-end neural relation extraction using deep biaffine attention. ||| 15064 ||| 15065 ||| 
2020 ||| dake: document-level attention for keyphrase extraction. ||| 15066 ||| 15067 ||| 15068 ||| 15069 ||| 
2021 ||| answer sentence selection using local and global context in transformer models. ||| 15070 ||| 3374 ||| 
2020 ||| an attention model of customer expectation to improve review helpfulness prediction. ||| 15071 ||| 15072 ||| 15073 ||| 15074 ||| 
2021 ||| drug and disease interpretation learning with biomedical entity representation transformer. ||| 15075 ||| 15076 ||| 15077 ||| 15078 ||| 
2020 ||| recognizing semantic relations: attention-based transformers vs. recurrent models. ||| 15079 ||| 15080 ||| 15081 ||| 
2021 ||| a multi-task approach to neural multi-label hierarchical patent classification using transformers. ||| 15082 ||| 15083 ||| 15084 ||| 15085 ||| 15086 ||| 
2020 ||| temporal embeddings and transformer models for narrative text understanding. ||| 15087 ||| 15088 ||| 15089 ||| 
2017 ||| a neural attention model for categorizing patient safety events. ||| 3122 ||| 15090 ||| 4957 ||| 15091 ||| 
2020 ||| dynamic heterogeneous graph embedding using hierarchical attentions. ||| 7008 ||| 15092 ||| 6760 ||| 7009 ||| 7010 ||| 1371 ||| 
2021 ||| classifying scientific publications with bert - is self-attention a feature selection method? ||| 3369 ||| 15093 ||| 15094 ||| 852 ||| 15095 ||| 15096 ||| 15097 ||| 2600 ||| 
2020 ||| readnet: a hierarchical transformer framework for web article readability analysis. ||| 15098 ||| 15099 ||| 15100 ||| 15101 ||| 
2021 ||| mitigating the position bias of transformer models in passage re-ranking. ||| 9583 ||| 9584 ||| 15102 ||| 15103 ||| 15104 ||| 9615 ||| 
2019 ||| zero-shot language transfer for cross-lingual sentence retrieval using bidirectional attention model. ||| 15105 ||| 15106 ||| 
2021 ||| comparing score aggregation approaches for document retrieval with pretrained transformers. ||| 3613 ||| 9664 ||| 3009 ||| 
2021 ||| transformer-based approach towards music emotion recognition from lyrics. ||| 15107 ||| 15108 ||| 15109 ||| 
2020 ||| inductive document network embedding with topic-word attention. ||| 8939 ||| 8940 ||| 8941 ||| 
2018 ||| topical stance detection for twitter: a two-phase lstm model using attention. ||| 10435 ||| 15110 ||| 15111 ||| 
2021 ||| pgt: pseudo relevance feedback using a graph-based transformer. ||| 15112 ||| 15113 ||| 15114 ||| 
2021 ||| open-domain conversational search assistant with transformers. ||| 15115 ||| 15116 ||| 15117 ||| 1994 ||| 15118 ||| 227 ||| 
2018 ||| malware analysis of imaged binary samples by convolutional neural network with attention mechanism. ||| 15119 ||| 15120 ||| 15121 ||| 15122 ||| 15123 ||| 
2019 ||| attention-based recurrent neural network for urban vehicle trajectory prediction. ||| 15124 ||| 15125 ||| 15126 ||| 
2019 ||| attention-based autoencoder topic model for short texts. ||| 1321 ||| 15127 ||| 
2021 ||| smart contracts implementation based on bidirectional encoder representations from transformers. ||| 15128 ||| 15129 ||| 15130 ||| 15131 ||| 
2021 ||| a fast detection method for polynomial fitting lane with self-attention module added. ||| 2259 ||| 967 ||| 15132 ||| 15133 ||| 
2021 ||| speech enhancement based on attention mechanism and pg-lstm neural network. ||| 15134 ||| 15135 ||| 
2021 ||| an improved speech recognition system based on transformer language model. ||| 15136 ||| 15137 ||| 2944 ||| 15138 ||| 15139 ||| 15140 ||| 
2021 ||| a novel view image generation network based on attention mechanism refining features. ||| 15141 ||| 15142 ||| 15143 ||| 15144 ||| 
2020 ||| boosting toponym interlinking by paying attention to both machine and deep learning. ||| 15145 ||| 15146 ||| 15147 ||| 
2017 ||| macrobase: prioritizing attention in fast data. ||| 9953 ||| 9954 ||| 15148 ||| 15149 ||| 9955 ||| 9956 ||| 
2021 ||| apan: asynchronous propagation attention network for real-time temporal graph embedding. ||| 15150 ||| 15151 ||| 15152 ||| 15153 ||| 5101 ||| 15154 ||| 15155 ||| 15156 ||| 11792 ||| 15157 ||| 6503 ||| 
2021 ||| combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of covid-19 fake tweets. ||| 15158 ||| 15159 ||| 15160 ||| 15161 ||| 3835 ||| 
2021 ||| glad-paw: graph-based log anomaly detection by position aware weighted graph attention network. ||| 15162 ||| 15163 ||| 952 ||| 15164 ||| 
2019 ||| aaane: attention-based adversarial autoencoder for multi-scale network embedding. ||| 15165 ||| 11333 ||| 1174 ||| 15166 ||| 
2020 ||| attention-based graph evolution. ||| 15167 ||| 15168 ||| 
2021 ||| iacn: influence-aware and attention-based co-evolutionary network for recommendation. ||| 1095 ||| 15169 ||| 1096 ||| 
2019 ||| text feature extraction and selection based on attention mechanism. ||| 240 ||| 241 ||| 
2018 ||| call attention to rumors: deep attention based recurrent neural networks for early rumor detection. ||| 5664 ||| 9889 ||| 1203 ||| 1796 ||| 
2019 ||| early churn user classification in social networking service using attention-based long short-term memory. ||| 15170 ||| 15171 ||| 15172 ||| 
2021 ||| graph attention networks with positional embeddings. ||| 15173 ||| 15174 ||| 15175 ||| 
2021 ||| hierarchical self attention based autoencoder for open-set human activity recognition. ||| 10203 ||| 10202 ||| 7377 ||| 7376 ||| 7375 ||| 
2021 ||| adaptive graph co-attention networks for traffic forecasting. ||| 15176 ||| 15177 ||| 602 ||| 15178 ||| 604 ||| 
2021 ||| a deep hybrid pooling architecture for graph classification with hierarchical attention. ||| 15179 ||| 15180 ||| 7043 ||| 
2020 ||| slgat: soft labels guided graph attention networks. ||| 15181 ||| 758 ||| 759 ||| 4191 ||| 
2019 ||| aspect level sentiment analysis with aspect attention. ||| 3307 ||| 15182 ||| 15183 ||| 
2021 ||| tantp: conversational emotion recognition using tree-based attention networks with transformer pre-training. ||| 15184 ||| 15185 ||| 1382 ||| 
2021 ||| using transformer based ensemble learning to classify scientific articles. ||| 15186 ||| 15187 ||| 
2018 ||| research and application of mapping relationship based on learning attention mechanism. ||| 15188 ||| 15189 ||| 11676 ||| 15190 ||| 
2020 ||| attention-based aggregation graph networks for knowledge graph information transfer. ||| 15191 ||| 1438 ||| 15192 ||| 
2020 ||| gamma: a graph and multi-view memory attention mechanism for top-n heterogeneous recommendation. ||| 7050 ||| 7042 ||| 7043 ||| 
2020 ||| optimized transformer models for faq answering. ||| 15193 ||| 15194 ||| 15195 ||| 1186 ||| 15196 ||| 
2020 ||| cacrnn: a context-aware attention-based convolutional recurrent neural network for fine-grained taxi demand prediction. ||| 15197 ||| 15198 ||| 15199 ||| 
2020 ||| role equivalence attention for label propagation in graph neural networks. ||| 15200 ||| 15101 ||| 
2021 ||| upgraded attention-based local feature learning block for speech emotion recognition. ||| 15201 ||| 15202 ||| 15203 ||| 
2019 ||| context-aware dual-attention network for natural language inference. ||| 1558 ||| 15204 ||| 1301 ||| 15205 ||| 1302 ||| 15206 ||| 
2021 ||| transformer-based multi-task learning for queuing time aware next poi recommendation. ||| 15207 ||| 15208 ||| 15209 ||| 15210 ||| 
2021 ||| heterogeneous graph attention network for small and medium-sized enterprises bankruptcy prediction. ||| 15211 ||| 15212 ||| 15213 ||| 799 ||| 
2020 ||| msfcnet: multi-scale feature-crossing attention network for multi-field sparse data. ||| 15214 ||| 15215 ||| 15216 ||| 5141 ||| 
2019 ||| complaint classification using hybrid-attention gru neural network. ||| 15217 ||| 411 ||| 412 ||| 15218 ||| 
2019 ||| topic attentional neural network for abstractive document summarization. ||| 5170 ||| 793 ||| 1160 ||| 
2018 ||| adaptive attention network for review sentiment classification. ||| 15219 ||| 15220 ||| 894 ||| 15221 ||| 
2021 ||| learning attention-based translational knowledge graph embedding via nonlinear dynamic mapping. ||| 15222 ||| 15223 ||| 633 ||| 15224 ||| 
2019 ||| multivariate time series early classification with interpretability using deep learning and attention mechanism. ||| 15225 ||| 15226 ||| 15227 ||| 
2018 ||| a deep neural spoiler detection model using a genre-aware attention mechanism. ||| 15228 ||| 9752 ||| 9249 ||| 15229 ||| 9250 ||| 
2019 ||| sentiment analysis based on lstm architecture with emoticon attention. ||| 3307 ||| 15230 ||| 13737 ||| 
2021 ||| raga: relation-aware graph attention networks for global entity alignment. ||| 15231 ||| 2882 ||| 2885 ||| 
2021 ||| meta-context transformers for domain-specific response generation. ||| 15232 ||| 15233 ||| 15234 ||| 
2019 ||| atnet: answering cloze-style questions via intra-attention and inter-attention. ||| 15235 ||| 15236 ||| 2349 ||| 
2019 ||| attention-based hierarchical recurrent neural network for phenotype classification. ||| 15237 ||| 8920 ||| 11190 ||| 
2020 ||| mask-guided region attention network for person re-identification. ||| 15238 ||| 15239 ||| 
2020 ||| relational metric learning with dual graph attention networks for social recommendation. ||| 15240 ||| 5107 ||| 15241 ||| 9555 ||| 
2019 ||| dependency-aware attention model for emotion analysis for online news. ||| 15242 ||| 4646 ||| 15243 ||| 
2021 ||| scarlet: explainable attention based graph neural network for fake news spreader prediction. ||| 15244 ||| 15245 ||| 1096 ||| 
2019 ||| semi-interactive attention network for answer understanding in reverse-qa. ||| 15246 ||| 15247 ||| 15248 ||| 5077 ||| 15249 ||| 
2021 ||| densely connected graph attention network based on iterative path reasoning for document-level relation extraction. ||| 15250 ||| 967 ||| 15251 ||| 11105 ||| 968 ||| 
2020 ||| temporalgat: attention-based dynamic graph representation learning. ||| 15252 ||| 5254 ||| 
2019 ||| consistency checking of attention aware systems. ||| 15253 ||| 3882 ||| 15254 ||| 15255 ||| 15256 ||| 15257 ||| 15258 ||| 
2021 ||| fast and precise certification of transformers. ||| 15259 ||| 15260 ||| 15261 ||| 15262 ||| 
2021 ||| generating bug-fixes using pretrained transformers. ||| 15263 ||| 3668 ||| 15264 ||| 15265 ||| 
2020 ||| prediction of protein tertiary structure using pre-trained self-supervised learning based on transformer. ||| 15266 ||| 15267 ||| 15268 ||| 15269 ||| 
2021 ||| canonical segmentation using affix characters as a unit on transformer for javanese language. ||| 15270 ||| 15271 ||| 15272 ||| 15267 ||| 
2021 ||| clamp: cross-level attention for multi-party conversational emotion recognition. ||| 15273 ||| 15274 ||| 15275 ||| 15276 ||| 
2020 ||| attention-guided generative adversarial network to address atypical anatomy in synthetic ct generation. ||| 15277 ||| 15278 ||| 15279 ||| 
2021 ||| learning dynamic connectivity with residual-attention network for autism classification in 4d fmri brain images. ||| 15280 ||| 15281 ||| 15282 ||| 
2019 ||| orchids classification using spatial transformer network with adaptive scaling. ||| 15283 ||| 15284 ||| 15285 ||| 15286 ||| 
2021 ||| directional graph transformer-based control flow embedding for malware classification. ||| 15287 ||| 15281 ||| 15282 ||| 
2020 ||| driver monitoring system based on cnn models: an approach for attention level detection. ||| 15288 ||| 15289 ||| 15290 ||| 2600 ||| 15291 ||| 
2020 ||| an automatic glioma segmentation system based on a separable attention u-net (saunet). ||| 758 ||| 15292 ||| 11446 ||| 
2020 ||| boundary-attention loss function in neural network for pathological lymph nodes segmentation based on pet/ct images. ||| 15293 ||| 15294 ||| 15295 ||| 
2021 ||| a multi-scale self-attention network for diabetic retinopathy retrieval. ||| 9964 ||| 15296 ||| 15297 ||| 15298 ||| 5206 ||| 
2017 ||| attention-based recurrent neural network for location recommendation. ||| 923 ||| 920 ||| 924 ||| 4003 ||| 
2019 ||| the attention based blstm model integrating sentence embeddings for biomedical event trigger identification. ||| 4089 ||| 15299 ||| 15300 ||| 15301 ||| 15302 ||| 
2019 ||| image caption model based on multi-head attention and encoder-decoder framework. ||| 15303 ||| 11546 ||| 
2019 ||| inter-person relation classification via attentionbased bidirectional gated recurrent unit. ||| 15304 ||| 1417 ||| 15305 ||| 875 ||| 15306 ||| 15307 ||| 
2019 ||| msanet: a multi-scale attention module. ||| 15308 ||| 683 ||| 242 ||| 15309 ||| 15310 ||| 11558 ||| 
2019 ||| english drug name entity recognition method based on attention mechanism bilstm-crf. ||| 9472 ||| 4191 ||| 1417 ||| 1416 ||| 15311 ||| 15312 ||| 
2019 ||| pay attention, please: formal language improves attention in volunteer and paid online experiments. ||| 15313 ||| 15314 ||| 
2017 ||| attention allocation aid for visual search. ||| 15315 ||| 15316 ||| 15317 ||| 15318 ||| 15319 ||| 
2018 ||| intelligent interruptions for ivr: investigating the interplay between presence, workload and attention. ||| 15320 ||| 15321 ||| 15322 ||| 
2020 ||| red alert: a cognitive countermeasure to mitigate attentional tunneling. ||| 15323 ||| 15324 ||| 15325 ||| 15326 ||| 15327 ||| 
2018 ||| increasing user attention with a comic-based policy. ||| 15328 ||| 15329 ||| 15330 ||| 15331 ||| 
2017 ||| attention, comprehension, execution: effects of different designs of biofeedback display. ||| 15332 ||| 8947 ||| 15333 ||| 
2020 ||| turkeyes: a web-based toolbox for crowdsourcing attention data. ||| 15334 ||| 15335 ||| 15336 ||| 15337 ||| 15338 ||| 15339 ||| 15340 ||| 8781 ||| 
2020 ||| enhancing social attention using eye-movement modeling and simulated dyadic social interactions. ||| 15341 ||| 
2019 ||| dynamics of visual attention in multiparty collaborative problem solving using multidimensional recurrence quantification analysis. ||| 15342 ||| 15343 ||| 15344 ||| 15345 ||| 
2021 ||| do cross-cultural differences in visual attention patterns affect search efficiency on websites? ||| 15346 ||| 15347 ||| 15313 ||| 15348 ||| 15314 ||| 
2020 ||| quantification of users' visual attention during everyday mobile device interactions. ||| 15349 ||| 15350 ||| 15351 ||| 8348 ||| 
2020 ||| attention-aware brain computer interface to avoid distractions in augmented reality. ||| 13860 ||| 13862 ||| 
2017 ||| modeling sub-document attention using viewport time. ||| 15352 ||| 15353 ||| 15354 ||| 15355 ||| 15356 ||| 15357 ||| 
2017 ||| facial thermography for attention tracking on smart eyewear: an initial study. ||| 15358 ||| 15359 ||| 15360 ||| 15361 ||| 15362 ||| 15363 ||| 
2017 ||| a framework for interactive mindfulness meditation using attention-regulation process. ||| 15364 ||| 15365 ||| 15366 ||| 15367 ||| 15368 ||| 
2020 ||| hivefive: immersion preserving attention guidance in virtual reality. ||| 15369 ||| 15370 ||| 15371 ||| 15372 ||| 
2021 ||| breaking out of the lab: mitigating mind wandering with gaze-based attention-aware technology in classrooms. ||| 15373 ||| 15374 ||| 15375 ||| 15345 ||| 
2019 ||| aila: attentive interactive labeling assistant for document classification through attention-based deep neural networks. ||| 15376 ||| 1408 ||| 15377 ||| 15378 ||| 1183 ||| 15379 ||| 
2017 ||| undertanding and detecting divided attention in mobile mooc learning. ||| 15380 ||| 15381 ||| 
2020 ||| faces of focus: a study on the facial cues of attentional states. ||| 15382 ||| 4562 ||| 4564 ||| 15383 ||| 15384 ||| 4566 ||| 
2021 ||| human-ai interactive and continuous sensemaking: a case study of image classification using scribble attention maps. ||| 7064 ||| 15385 ||| 15386 ||| 15387 ||| 15388 ||| 5177 ||| 15389 ||| 
2019 ||| search as news curator: the role of google in shaping attention to news information. ||| 15390 ||| 15391 ||| 
2021 ||| mindless attractor: a false-positive resistant intervention for drawing attention using auditory perturbation. ||| 15392 ||| 15119 ||| 
2020 ||| using mobile augmented reality to improve attention in adults with autism spectrum disorder. ||| 15393 ||| 15394 ||| 15395 ||| 
2020 ||| classification of functional attention in video meetings. ||| 15396 ||| 15397 ||| 
2020 ||| the interaction attention continuum: an education case study. ||| 15398 ||| 15399 ||| 
2021 ||| impact of task on attentional tunneling in handheld augmented reality. ||| 15400 ||| 15401 ||| 15402 ||| 8500 ||| 4566 ||| 15384 ||| 
2019 ||| "watch out!": semi-autonomous vehicles using assertive voices to grab distracted drivers' attention. ||| 15403 ||| 15404 ||| 15405 ||| 15406 ||| 
2019 ||| towards novel urban planning methods - using eye-tracking systems to understand human attention in urban environments. ||| 15407 ||| 15408 ||| 15409 ||| 15410 ||| 
2019 ||| feature extraction and classification of odor using attention based neural network. ||| 15411 ||| 15412 ||| 15413 ||| 15414 ||| 15415 ||| 
2020 ||| dynamic multi-level attention models for dialogue response generation. ||| 11991 ||| 4211 ||| 11989 ||| 6179 ||| 4214 ||| 
2020 ||| s-cogit: a natural language processing tool for linguistic analysis of the social interaction between individuals with attention-deficit disorder. ||| 15416 ||| 8048 ||| 15417 ||| 15418 ||| 15419 ||| 
2021 ||| malware analysis with artificial intelligence and a particular attention on results interpretability. ||| 15420 ||| 15421 ||| 15422 ||| 
2019 ||| ld-parser: leaf detection based dependency parsing using bilstm and attention mechanism. ||| 15423 ||| 15424 ||| 2454 ||| 2230 ||| 15425 ||| 
2021 ||| social media named entity recognition based on graph attention network. ||| 781 ||| 15426 ||| 5531 ||| 
2021 ||| transformer based refinement network for accurate crack detection. ||| 15427 ||| 15428 ||| 
2021 ||| impact of virtual reality head mounted display on the attentional visual field. ||| 15429 ||| 15430 ||| 13310 ||| 15431 ||| 15432 ||| 
2017 ||| tilers, tilemakers, transformers! ||| 15433 ||| 
2019 ||| an attention-based recurrent convolutional network for vehicle taillight recognition. ||| 15434 ||| 15435 ||| 15436 ||| 15437 ||| 15438 ||| 
2020 ||| deep learning with attention mechanism for predicting driver intention at intersection. ||| 15439 ||| 15440 ||| 15441 ||| 15442 ||| 15443 ||| 
2021 ||| stgt: forecasting pedestrian motion using spatio-temporal graph transformer. ||| 15444 ||| 15445 ||| 
2021 ||| bi-directional attention feature enhancement for video instance segmentation. ||| 15446 ||| 15447 ||| 
2021 ||| predicting vehicles trajectories in urban scenarios with transformer networks and augmented information. ||| 15448 ||| 15449 ||| 15450 ||| 15451 ||| 14327 ||| 15452 ||| 15453 ||| 15454 ||| 
2020 ||| multi-head attention based probabilistic vehicle trajectory prediction. ||| 15455 ||| 3028 ||| 15456 ||| 15457 ||| 15458 ||| 
2020 ||| attention r-cnn for accident detection. ||| 15459 ||| 15460 ||| 15461 ||| 15462 ||| 
2021 ||| trajectory prediction for autonomous driving based on multi-head attention with joint agent-map representation. ||| 15463 ||| 15464 ||| 7888 ||| 15465 ||| 
2017 ||| a computational framework for driver's visual attention using a fully convolutional architecture. ||| 15466 ||| 15467 ||| 
2021 ||| scout: socially-consistent and understandable graph attention network for trajectory prediction of vehicles and vrus. ||| 15468 ||| 15449 ||| 15450 ||| 15453 ||| 15454 ||| 
2019 ||| attention monitoring and hazard assessment with bio-sensing and vision: empirical analysis utilizing cnns on the kitti dataset. ||| 15469 ||| 7888 ||| 
2021 ||| novelty detection and analysis of traffic scenario infrastructures in the latent space of a vision transformer-based triplet autoencoder. ||| 15470 ||| 15471 ||| 15472 ||| 15473 ||| 
2019 ||| visual explanation by attention branch network for end-to-end learning-based self-driving. ||| 15474 ||| 15475 ||| 15476 ||| 723 ||| 724 ||| 725 ||| 
2017 ||| detection and recognition of traffic signs inside the attentional visual field of drivers. ||| 15477 ||| 15478 ||| 15479 ||| 15480 ||| 
2021 ||| pedestrian trajectory prediction via spatial interaction transformer network. ||| 15481 ||| 15482 ||| 2377 ||| 
2021 ||| eeg-based system using deep learning and attention mechanism for driver drowsiness detection. ||| 15483 ||| 15484 ||| 15485 ||| 15486 ||| 15487 ||| 15488 ||| 15489 ||| 
2021 ||| dr-tanet: dynamic receptive temporal attention network for street scene change detection. ||| 9996 ||| 7857 ||| 7861 ||| 
2020 ||| traffic agent trajectory prediction using social convolution and attention mechanism. ||| 15490 ||| 15491 ||| 6828 ||| 15492 ||| 14779 ||| 
2018 ||| on the novel approach to parallel coupled-line bandpass filters that have diverse wavelenght impedance scaling i/o transformers. ||| 15493 ||| 
2017 ||| compact thermal model of planar transformers. ||| 15494 ||| 15495 ||| 15496 ||| 
2020 ||| histopathologic cancer detection by dense-attention network with incorporation of prior knowledge. ||| 15497 ||| 882 ||| 15498 ||| 14909 ||| 
2021 ||| two-stream attention spatio-temporal network for classification of echocardiography videos. ||| 15499 ||| 15500 ||| 15501 ||| 
2020 ||| synaptic partner assignment using attentional voxel association networks. ||| 15502 ||| 15503 ||| 15504 ||| 10834 ||| 15505 ||| 15506 ||| 
2021 ||| deep transformers for fast small intestine grounding in capsule endoscope video. ||| 15507 ||| 15508 ||| 6810 ||| 15509 ||| 15510 ||| 1800 ||| 
2020 ||| spectral graph transformer networks for brain surface parcellation. ||| 2824 ||| 15511 ||| 15512 ||| 15513 ||| 
2019 ||| mri reconstruction via cascaded channel-wise attention network. ||| 15514 ||| 2019 ||| 15515 ||| 15516 ||| 15517 ||| 1749 ||| 
2021 ||| smocam: smooth conditional attention mask for 3d-regression models. ||| 15518 ||| 2713 ||| 15519 ||| 15520 ||| 15521 ||| 15522 ||| 15523 ||| 15524 ||| 11331 ||| 15525 ||| 
2021 ||| adasan: adaptive cosine similarity self-attention network for gastrointestinal endoscopy image classification. ||| 6115 ||| 6116 ||| 6117 ||| 
2021 ||| parallel res2net-based network with reverseattention for polyp segmentation. ||| 15526 ||| 15527 ||| 2527 ||| 
2019 ||| epithelial segmentation from in situ hybridisation histological samples using a deep central attention learning approach. ||| 15528 ||| 15529 ||| 15530 ||| 15531 ||| 
2019 ||| attentionnet: learning where to focus via attention mechanism for anatomical segmentation of whole breast ultrasound images. ||| 6799 ||| 15532 ||| 15533 ||| 8636 ||| 15534 ||| 6582 ||| 
2020 ||| relational learning between multiple pulmonary nodules via deep set attention transformers. ||| 15535 ||| 15536 ||| 15537 ||| 8832 ||| 15538 ||| 
2017 ||| decoding dynamic auditory attention during naturalistic experience. ||| 15539 ||| 15540 ||| 444 ||| 15541 ||| 2414 ||| 15542 ||| 15543 ||| 15544 ||| 6593 ||| 
2020 ||| fine-grained multi-instance classification in microscopy through deep attention. ||| 15545 ||| 15546 ||| 15547 ||| 2377 ||| 15548 ||| 
2020 ||| efficient aortic valve multilabel segmentation using a spatial transformer network. ||| 15549 ||| 3369 ||| 15550 ||| 4217 ||| 15551 ||| 
2019 ||| residual attention based network for hand bone age assessment. ||| 15552 ||| 15553 ||| 398 ||| 15554 ||| 9891 ||| 6810 ||| 15555 ||| 15556 ||| 14552 ||| 15557 ||| 15558 ||| 
2020 ||| longitudinal analysis of mild cognitive impairment via sparse smooth network and attention-based stacked bi-directional long-short term memory. ||| 15559 ||| 15560 ||| 440 ||| 6578 ||| 1160 ||| 5476 ||| 6582 ||| 
2020 ||| weakly-supervised balanced attention network for gastric pathology image localization and classification. ||| 15561 ||| 5151 ||| 15562 ||| 15563 ||| 
2019 ||| recurrent attention mechanism networks for enhanced classification of biomedical images. ||| 15564 ||| 15565 ||| 15566 ||| 
2021 ||| mga-net: multi-scale guided attention models for an automated diagnosis of idiopathic pulmonary fibrosis (ipf). ||| 15567 ||| 15568 ||| 15569 ||| 15570 ||| 15571 ||| 
2020 ||| super-resolution and self-attention with generative adversarial network for improving malignancy characterization of hepatocellular carcinoma. ||| 15572 ||| 6238 ||| 15573 ||| 15574 ||| 15575 ||| 15576 ||| 
2021 ||| multi-channel sparse graph transformer network for early alzheimer's disease identification. ||| 6576 ||| 15577 ||| 15578 ||| 15559 ||| 15579 ||| 5476 ||| 6582 ||| 
2021 ||| automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. ||| 15580 ||| 15581 ||| 15582 ||| 15583 ||| 15584 ||| 15585 ||| 15586 ||| 15587 ||| 15588 ||| 
2019 ||| a novel focal tversky loss function with improved attention u-net for lesion segmentation. ||| 15589 ||| 15590 ||| 
2021 ||| retinal vessel segmentation via context guide attention net with joint hard sample mining strategy. ||| 15591 ||| 15592 ||| 15593 ||| 15594 ||| 15595 ||| 
2020 ||| jointly analyzing alzheimer's disease related structure-function using deep cross-model attention network. ||| 2037 ||| 1052 ||| 15596 ||| 
2020 ||| classification of ocular diseases employing attention-based unilateral and bilateral feature weighting and fusion. ||| 8783 ||| 130 ||| 8782 ||| 15597 ||| 2149 ||| 15598 ||| 
2019 ||| look, investigate, and classify: a deep hybrid attention method for breast cancer classification. ||| 15599 ||| 15600 ||| 15601 ||| 15602 ||| 15603 ||| 15604 ||| 15605 ||| 13429 ||| 15606 ||| 
2020 ||| a multi-modality fusion network based on attention mechanism for brain tumor segmentation. ||| 15607 ||| 15608 ||| 15609 ||| 2693 ||| 15610 ||| 
2021 ||| mssa-net: multi-scale self-attention network for breast ultrasound image segmentation. ||| 15611 ||| 15612 ||| 15613 ||| 15614 ||| 
2020 ||| ceus-net: lesion segmentation in dynamic contrast-enhanced ultrasound with feature-reweighted attention mechanism. ||| 15615 ||| 604 ||| 15616 ||| 15617 ||| 15618 ||| 15619 ||| 15620 ||| 
2020 ||| csaf-cnn: cross-layer spatial attention map fusion network for organ-at-risk segmentation in head and neck ct images. ||| 15621 ||| 2054 ||| 15622 ||| 15623 ||| 
2021 ||| structural visual guidance attention networks in retinopathy of prematurity. ||| 14090 ||| 14095 ||| 15624 ||| 15625 ||| 15626 ||| 14093 ||| 15627 ||| 14094 ||| 14096 ||| 14091 ||| 
2021 ||| medical image enhancement for lesion detection based on class-aware attention and deep colorization. ||| 15628 ||| 1785 ||| 15629 ||| 15630 ||| 
2020 ||| attentionanatomy: a unified framework for whole-body organs at risk segmentation using multiple partially annotated datasets. ||| 7138 ||| 1305 ||| 7139 ||| 435 ||| 7136 ||| 7137 ||| 4297 ||| 7135 ||| 
2020 ||| deep learning fast mri using channel attention in magnitude domain. ||| 15631 ||| 15632 ||| 15633 ||| 2048 ||| 
2021 ||| asymmetric attention upsampling: rethinking upsampling for biological image segmentation. ||| 15634 ||| 15635 ||| 13511 ||| 11266 ||| 
2019 ||| focusnet: an attention-based fully convolutional network for medical image segmentation. ||| 15636 ||| 15637 ||| 15638 ||| 
2021 ||| focal-balanced attention u-net with dynamic thresholding by spatial regression for segmentation of aortic dissection in ct imagery. ||| 15639 ||| 15640 ||| 15641 ||| 15642 ||| 15643 ||| 
2020 ||| robust brain magnetic resonance image segmentation for hydrocephalus patients: hard and soft attention. ||| 15644 ||| 15645 ||| 15646 ||| 15647 ||| 15648 ||| 577 ||| 
2021 ||| an attention-based hybrid deep learning framework integrating temporal coherence and dynamics for discriminating schizophrenia. ||| 15649 ||| 15650 ||| 15592 ||| 15651 ||| 15652 ||| 15653 ||| 15654 ||| 15655 ||| 
2021 ||| global multi-level attention network for the segmentation of clinical target volume in the planning ct for cervical cancer. ||| 15656 ||| 15657 ||| 15658 ||| 15659 ||| 15660 ||| 15661 ||| 
2020 ||| attention-based cnn for kl grade classification: data from the osteoarthritis initiative. ||| 15662 ||| 15663 ||| 3008 ||| 15664 ||| 15665 ||| 
2021 ||| attention-guided deep multi-instance learning for staging retinopathy of prematurity. ||| 15666 ||| 15667 ||| 15668 ||| 15669 ||| 5476 ||| 15670 ||| 6582 ||| 
2021 ||| mda-net: multi-dimensional attention-based neural network for 3d image segmentation. ||| 15671 ||| 15672 ||| 
2019 ||| region proposal networks with contextual selective attention for real-time organ detection. ||| 15673 ||| 15674 ||| 15675 ||| 
2019 ||| self-attention equipped graph convolutions for disease prediction. ||| 15676 ||| 15677 ||| 15678 ||| 15679 ||| 15680 ||| 13628 ||| 
2020 ||| surround sound spreads visual attention and increases cognitive effort in immersive media reproductions. ||| 15681 ||| 3419 ||| 15682 ||| 
2018 ||| a prototype mixer to improve cross-modal attention during audio mixing. ||| 15683 ||| 15684 ||| 15685 ||| 
2018 ||| my sound space: an attentional shield for immersive redirection. ||| 15686 ||| 15687 ||| 15688 ||| 15689 ||| 
2021 ||| audio-visual interactive art: investigating the effect of gaze-controlled audio on visual attention and short term memory. ||| 15690 ||| 15691 ||| 15692 ||| 15693 ||| 
2020 ||| bi-lattice lstm model with self-attention for chinese ner. ||| 15694 ||| 191 ||| 
2021 ||| short-term power load probability density forecasting based on a double-layer lstm-attention quantile regression. ||| 15695 ||| 15696 ||| 15697 ||| 
2020 ||| modeling and analysis of three-phase distribution transformer connections by phase-coordinates based on matrix operation method. ||| 15698 ||| 15699 ||| 15700 ||| 
2019 ||| multi-layer attention mechanism based speech separation model. ||| 5860 ||| 7955 ||| 15701 ||| 15702 ||| 8922 ||| 
2020 ||| pos scaling attention model for joint slot filling and intent classification. ||| 15703 ||| 15704 ||| 15705 ||| 
2020 ||| ampa-net: optimization-inspired attention neural network for deep compressed sensing. ||| 15706 ||| 15707 ||| 
2021 ||| tpe-mha: a malicious traffic detection model based on time position encoding and multi-head attention. ||| 15708 ||| 15709 ||| 15710 ||| 
2020 ||| a comparative research on the influence of commercial complex waterscape atrium on human emotion based on computer visual attention and eeg data. ||| 15711 ||| 15712 ||| 
2020 ||| semantic segmentation of high resolution remote sensing images with extra context attention mechanism. ||| 15713 ||| 15714 ||| 15715 ||| 11499 ||| 15716 ||| 4398 ||| 
2020 ||| electromagnetic parameters optimization design of industrial dc transformer based on improved genetic algorithm. ||| 15717 ||| 15718 ||| 
2021 ||| cross-channel fusion image dehazing network with feature attention. ||| 4297 ||| 15719 ||| 
2021 ||| attention mechanism-driven potential fault cause identification in optical networks. ||| 15720 ||| 15721 ||| 15722 ||| 5898 ||| 15723 ||| 15724 ||| 1254 ||| 
2020 ||| business model canvas should pay more attention to the software startup team. ||| 15725 ||| 15726 ||| 15727 ||| 15728 ||| 15729 ||| 15730 ||| 15731 ||| 15732 ||| 15733 ||| 15734 ||| 15735 ||| 15736 ||| 15737 ||| 15738 ||| 15739 ||| 
2021 ||| classification of autism spectrum disorder severity using eye tracking data based on visual attention model. ||| 15740 ||| 15741 ||| 15742 ||| 15743 ||| 15744 ||| 3369 ||| 15745 ||| 15746 ||| 504 ||| 3979 ||| 15747 ||| 
2021 ||| assessing the clinical validity of attention-based and shap temporal explanations for adverse drug event predictions. ||| 15748 ||| 15749 ||| 15750 ||| 15751 ||| 
2021 ||| personalised short-term glucose prediction via recurrent self-attention network. ||| 15752 ||| 15753 ||| 15754 ||| 15755 ||| 15756 ||| 
2017 ||| an eye tracker based computer system to support oculomotor and attention deficit investigations. ||| 15757 ||| 15758 ||| 15759 ||| 10849 ||| 15760 ||| 15761 ||| 15762 ||| 15763 ||| 
2020 ||| exploring visual attention and machine learning in 3d visualization of medical temporal data. ||| 3975 ||| 3976 ||| 3977 ||| 3978 ||| 504 ||| 15764 ||| 
2021 ||| apehr: automated prognosis in electronic health records using multi-head self-attention. ||| 15765 ||| 15766 ||| 15767 ||| 852 ||| 15768 ||| 
2021 ||| trident: change point detection for multivariate time series via dual-level attention learning. ||| 15769 ||| 14188 ||| 15770 ||| 
2020 ||| the impact of constant field of attention on properties of contextual neural networks. ||| 15771 ||| 15772 ||| 15773 ||| 15774 ||| 
2021 ||| residual attention network vs real attention on aesthetic assessment. ||| 15775 ||| 15776 ||| 15777 ||| 15778 ||| 
2020 ||| dynamic prototype selection by fusing attention mechanism for few-shot relation classification. ||| 15779 ||| 15780 ||| 15781 ||| 189 ||| 622 ||| 
2021 ||| the impact of aggregation window width on properties of contextual neural networks with constant field of attention. ||| 15782 ||| 15783 ||| 15784 ||| 15771 ||| 15772 ||| 
2020 ||| stock return prediction using dual-stage attention model with stock relation inference. ||| 15785 ||| 15786 ||| 
2021 ||| empirical study of tweets topic classification using transformer-based language models. ||| 15775 ||| 15787 ||| 15776 ||| 15778 ||| 
2020 ||| antidote: attention-based dynamic optimization for neural network runtime efficiency. ||| 15788 ||| 15789 ||| 15790 ||| 15791 ||| 6007 ||| 
2021 ||| in-memory computing based accelerator for transformer networks for long sequences. ||| 7493 ||| 15792 ||| 7494 ||| 15793 ||| 
2019 ||| ean: event attention network for stock price trend prediction based on sentimental embedding. ||| 6416 ||| 3477 ||| 15794 ||| 15795 ||| 
2018 ||| collective attention towards scientists and research topics. ||| 15796 ||| 15797 ||| 15798 ||| 15799 ||| 
2021 ||| efficient detection of multilingual hate speech by using interactive attention network with minimal human feedback. ||| 15800 ||| 15801 ||| 15802 ||| 
2019 ||| characterizing attention cascades in whatsapp groups. ||| 15803 ||| 15804 ||| 15805 ||| 15806 ||| 8500 ||| 15807 ||| 15808 ||| 15809 ||| 15810 ||| 
2020 ||| act : automatic fake news classification through self-attention. ||| 15811 ||| 
2017 ||| the effect of collective attention on controversial debates on social media. ||| 8931 ||| 15812 ||| 15813 ||| 15814 ||| 
2019 ||| frequency domain transformer networks for video prediction. ||| 407 ||| 409 ||| 
2018 ||| image-to-text transduction with spatial self-attention. ||| 15815 ||| 15816 ||| 14416 ||| 1017 ||| 
2020 ||| motion segmentation using frequency domain transformer networks. ||| 407 ||| 409 ||| 
2018 ||| regularize and explicit collaborative filtering with textual attention. ||| 15817 ||| 15818 ||| 15819 ||| 
2017 ||| attention-based information fusion using multi-encoder-decoder recurrent neural networks. ||| 15820 ||| 15821 ||| 15822 ||| 
2021 ||| attention-based hybrid precoding for mmwave mimo systems. ||| 15823 ||| 3042 ||| 15824 ||| 15825 ||| 15826 ||| 15827 ||| 
2021 ||| adaptive parking slot occupancy detection using vision transformer and llie. ||| 15828 ||| 
2020 ||| attention-enabled network-level traffic speed prediction. ||| 15829 ||| 15830 ||| 15831 ||| 15832 ||| 
2018 ||| coarse to fine: multi-label image classification with global/local attention. ||| 10069 ||| 10072 ||| 6475 ||| 15833 ||| 10071 ||| 15834 ||| 
2021 ||| towards a real-time system based on regression model to evaluate driver's attention. ||| 15835 ||| 15836 ||| 15837 ||| 8048 ||| 15838 ||| 
2018 ||| eye-tracking for user attention evaluation in adaptive serious games. ||| 15839 ||| 15840 ||| 15841 ||| 
2020 ||| a novel collaborative filtering framework based on variational self-attention gan. ||| 15842 ||| 15843 ||| 15844 ||| 15845 ||| 
2021 ||| augmented convolutional neural networks with transformer for wireless interference identification. ||| 15846 ||| 15847 ||| 15848 ||| 
2021 ||| attention-aware multi-encoder for session-based recommendation. ||| 15849 ||| 15850 ||| 9009 ||| 15851 ||| 
2021 ||| mrainf: multilayer relation attention based social influence prediction net with local stimulation. ||| 15852 ||| 2755 ||| 15853 ||| 
2021 ||| ppdtsa: privacy-preserving deep transformation self-attention framework for object detection. ||| 15854 ||| 15855 ||| 15856 ||| 15857 ||| 
2020 ||| a video popularity prediction scheme with attention-based lstm and feature embedding. ||| 15858 ||| 15859 ||| 15860 ||| 5110 ||| 
2021 ||| distributed signal strength prediction using satellite map empowered by deep vision transformer. ||| 15861 ||| 15862 ||| 15863 ||| 15367 ||| 2303 ||| 15864 ||| 15865 ||| 
2021 ||| mcformer: a transformer based deep neural network for automatic modulation classification. ||| 15866 ||| 15867 ||| 
2021 ||| an attention-aided deep neural network design for channel estimation in massive mimo systems. ||| 15868 ||| 15869 ||| 15870 ||| 2049 ||| 15871 ||| 
2021 ||| performance optimization for semantic communications: an attention-based learning approach. ||| 15872 ||| 15873 ||| 15874 ||| 6049 ||| 13677 ||| 15875 ||| 
2021 ||| deep learning based ofdm channel estimation using frequency-time division and attention mechanism. ||| 15876 ||| 15877 ||| 15878 ||| 15879 ||| 14174 ||| 
2019 ||| adaptive multi-attention convolutional neural network for fine-grained image recognition. ||| 13659 ||| 15880 ||| 15881 ||| 117 ||| 15882 ||| 
2018 ||| decoding behavioral accuracy in an attention task using brain fmri data. ||| 7436 ||| 14268 ||| 15883 ||| 15884 ||| 15885 ||| 15886 ||| 15887 ||| 
2021 ||| packet routing with graph attention multi-agent reinforcement learning. ||| 15888 ||| 15889 ||| 15890 ||| 
2019 ||| graph attention spatial-temporal network for deep learning based mobile traffic prediction. ||| 15891 ||| 15892 ||| 9645 ||| 15893 ||| 4520 ||| 
2019 ||| a deep learning framework with spatial-temporal attention mechanism for cellular traffic prediction. ||| 116 ||| 115 ||| 15894 ||| 119 ||| 
2021 ||| attention mechanism based resnext network for automatic modulation classification. ||| 15895 ||| 10415 ||| 15896 ||| 15897 ||| 7676 ||| 
2021 ||| explainable health state prediction for social iots through multi-channel attention. ||| 15898 ||| 1819 ||| 
2020 ||| machine learning-based regression and classification models for oil assessment of power transformers. ||| 15899 ||| 7018 ||| 15900 ||| 
2021 ||| siamese attention and point adaptive network for visual tracking. ||| 15901 ||| 15902 ||| 15903 ||| 
2021 ||| quantum attention based language model for answer selection. ||| 15904 ||| 15905 ||| 15906 ||| 
2018 ||| sentiment analysis based on hybrid bi-attention mechanism in mobile application. ||| 13925 ||| 3177 ||| 1199 ||| 
2021 ||| multimodal social media sentiment analysis based on cross-modal hierarchical attention fusion. ||| 15907 ||| 15908 ||| 
2020 ||| attention-based asymmetric fusion network for saliency prediction in 3d images. ||| 15909 ||| 15908 ||| 
2020 ||| attention-based interaction trajectory prediction. ||| 6474 ||| 4289 ||| 15910 ||| 13743 ||| 
2020 ||| deep reinforcement learning with transformers for text adventure games. ||| 9226 ||| 9227 ||| 4980 ||| 602 ||| 4873 ||| 
2020 ||| influencing the affective state and attention restoration in vr-supported psychotherapy. ||| 15911 ||| 15912 ||| 15913 ||| 15914 ||| 15915 ||| 
2021 ||| designing vr games with gaze control for directing attention of children with adhd. ||| 15916 ||| 15917 ||| 15918 ||| 
2021 ||| inventory management with attention-based meta actions. ||| 15919 ||| 15920 ||| 
2021 ||| towards federated learning with attention transfer to mitigate system and data heterogeneity of clients. ||| 15921 ||| 15922 ||| 
2022 ||| real-time style transfer with efficient vision transformers. ||| 15923 ||| 15924 ||| 15925 ||| 15926 ||| 15927 ||| 
2019 ||| gesture class prediction by recurrent neural network and attention mechanism. ||| 15928 ||| 9772 ||| 9773 ||| 15929 ||| 
2017 ||| you can leave your head on - attention management and turn-taking in multi-party interaction with a virtual human/robot duo. ||| 15930 ||| 15931 ||| 15932 ||| 15933 ||| 15934 ||| 15935 ||| 15936 ||| 
2021 ||| attention-guidance method based on conforming behavior of multiple virtual agents for pedestrians. ||| 15937 ||| 15938 ||| 
2019 ||| effects of a virtual human appearance fidelity continuum on visual attention in virtual reality. ||| 15939 ||| 15940 ||| 15941 ||| 
2019 ||| can a signing virtual human engage a baby's attention? ||| 15942 ||| 15943 ||| 15944 ||| 15945 ||| 15946 ||| 15947 ||| 15948 ||| 15949 ||| 15950 ||| 
2017 ||| integration of multi-modal cues in synthetic attention processes to drive virtual agent behavior. ||| 15951 ||| 15952 ||| 15953 ||| 15954 ||| 15955 ||| 15956 ||| 
2021 ||| power transformer design resorting to metaheuristics techniques. ||| 15957 ||| 15958 ||| 15959 ||| 
2021 ||| design of an attention tool using hci and work-related variables. ||| 15960 ||| 8382 ||| 15961 ||| 15962 ||| 15963 ||| 7033 ||| 15964 ||| 15965 ||| 15966 ||| 1994 ||| 10033 ||| 3157 ||| 15967 ||| 15968 ||| 
2017 ||| student's attention improvement supported by physiological measurements analysis. ||| 8496 ||| 8497 ||| 15969 ||| 15970 ||| 1994 ||| 8498 ||| 8499 ||| 8500 ||| 
2018 ||| high-frequency transformer isolated ac-dc converter for resilient low voltage dc residential grids. ||| 15971 ||| 15972 ||| 15973 ||| 
2017 ||| a generalized geometric programming sub-problem of transformer design optimization. ||| 11942 ||| 15974 ||| 11942 ||| 15975 ||| 12413 ||| 15976 ||| 15326 ||| 15977 ||| 
2020 ||| cascaded solid state transformer structure to power fast ev charging stations from medium voltage transmission lines. ||| 15978 ||| 15979 ||| 15980 ||| 15981 ||| 
2021 ||| rational inattention in choice overload: clustering for discrete choices. ||| 15982 ||| 15983 ||| 
2021 ||| attention-based deep feature learning network for scene classification of hyperspectral images. ||| 15984 ||| 15985 ||| 15986 ||| 
2017 ||| modulation classification using convolutional neural networks and spatial transformer networks. ||| 15987 ||| 15988 ||| 15989 ||| 
2019 ||| meda: multi-output encoder-decoder for spatial attention in convolutional neural networks. ||| 15990 ||| 6126 ||| 
2021 ||| two-exposure image fusion based on cross attention fusion. ||| 15991 ||| 15992 ||| 15993 ||| 15994 ||| 
2021 ||| synthesized speech detection using convolutional transformer-based spectrogram analysis. ||| 6882 ||| 6887 ||| 
2020 ||| pay attention to categories: syntax-based sentence modeling with metadata projection matrix. ||| 15995 ||| 10966 ||| 
2020 ||| attention-based domain adaption using transfer learning for part-of-speech tagging: an experiment on the hindi language. ||| 15996 ||| 15997 ||| 15998 ||| 10306 ||| 
2018 ||| customized attention mechanism for relation classification. ||| 11701 ||| 14034 ||| 15999 ||| 13573 ||| 
2020 ||| understanding transformers for information extraction with limited data. ||| 7521 ||| 16000 ||| 16001 ||| 16002 ||| 16003 ||| 16004 ||| 
2018 ||| attention-based blstm-crf architecture for mongolian named entity recognition. ||| 16005 ||| 16006 ||| 
2020 ||| tdp - a hybrid diacritic restoration with transformer decoder. ||| 16007 ||| 16008 ||| 
2018 ||| feature attention network: interpretable depression detection from social media. ||| 16009 ||| 16010 ||| 16011 ||| 16012 ||| 
2017 ||| extracting important tweets for news writers using recurrent neural network with attention mechanism and multi-task learning. ||| 16013 ||| 16014 ||| 16015 ||| 16016 ||| 16017 ||| 
2020 ||| improving sequence tagging for vietnamese text using transformer-based neural models. ||| 16018 ||| 16019 ||| 16020 ||| 
2018 ||| detecting free translation in parallel corpora from attention scores. ||| 6415 ||| 16021 ||| 3306 ||| 
2018 ||| metaphor identification with paragraph and word vectorization: an attention-based neural approach. ||| 16022 ||| 16023 ||| 
2018 ||| japanese sentiment classification using a tree-structured long short-term memory with attention. ||| 16024 ||| 14211 ||| 
2018 ||| semantic role labeling in conversational chat using deep bi-directional long short-term memory networks with attention mechanism. ||| 16025 ||| 16026 ||| 16027 ||| 16028 ||| 16029 ||| 
2020 ||| imbalanced chinese multi-label text classification based on alternating attention. ||| 16030 ||| 1768 ||| 13737 ||| 
2020 ||| attention-based bidirectional long short-term memory neural network for short answer scoring. ||| 16031 ||| 16032 ||| 1235 ||| 16033 ||| 16034 ||| 
2019 ||| cyberbullying detection with birnn and attention mechanism. ||| 16035 ||| 1200 ||| 16036 ||| 333 ||| 
2020 ||| guiding the operator's attention among a plurality of operator workstation screens. ||| 16037 ||| 
2020 ||| grid structure attention for natural language interface to bash commands. ||| 16038 ||| 16039 ||| 16040 ||| 
2020 ||| evaluation of fatigue and attention levels in multi-target scenario using cnn. ||| 16041 ||| 16042 ||| 
2020 ||| android malware detection system integrating block feature extraction and multi-head attention mechanism. ||| 16043 ||| 16044 ||| 16045 ||| 16046 ||| 
2018 ||| multimodal attention agents in visual conversation. ||| 16047 ||| 16048 ||| 9566 ||| 
2020 ||| attentional neural mechanisms for social recommendations in educational platforms. ||| 9826 ||| 9827 ||| 9828 ||| 9825 ||| 9824 ||| 9829 ||| 
2020 ||| classification of students' conceptual understanding in stem education using their visual attention distributions: a comparison of three machine-learning approaches. ||| 16049 ||| 16050 ||| 16051 ||| 16052 ||| 16053 ||| 16054 ||| 
2018 ||| reducing the split-attention effect in assembly based instruction by merging physical parts with holograms in mixed reality. ||| 16055 ||| 16056 ||| 16057 ||| 
2020 ||| constraining the transformer nmt model with heuristic grid beam search. ||| 16058 ||| 16059 ||| 
2020 ||| investigation of transformer-based latent attention models for neural machine translation. ||| 3453 ||| 12640 ||| 3454 ||| 
2021 |||  (transformer-based argument mining for healthcare applications). ||| 10222 ||| 10223 ||| 3953 ||| 
2020 |||  base de transformers (transformer based approach for answer generation). ||| 16060 ||| 16061 ||| 15325 ||| 15326 ||| 16062 ||| 
2018 ||| lective pour classification de microblogs (deft 2018 : selective attention for microblogging classification ). ||| 15817 ||| 16063 ||| 15819 ||| 15818 ||| 
2021 ||| ais (generative pre-trained transformer in______ (french) we introduce a french adaptation from the well-known gpt model). ||| 3765 ||| 3766 ||| 3767 ||| 
2021 ||| dire l'aspect linguistique en anglais au moyen de transformers (classifying linguistic aspect in english with transformers ). ||| 11667 ||| 11668 ||| 11669 ||| 
2018 ||| canisme d'attention (customer satisfaction prediction with attention-based rnns from a chat contact center corpus). ||| 4194 ||| 59 ||| 16064 ||| 16065 ||| 2101 ||| 16066 ||| 3766 ||| 16067 ||| 15325 ||| 15326 ||| 16068 ||| 16069 ||| 
2020 ||| decoding auditory and tactile attention for use in an eeg-based brain-computer interface. ||| 12774 ||| 16070 ||| 16071 ||| 12776 ||| 16072 ||| 16073 ||| 12778 ||| 12779 ||| 12777 ||| 12780 ||| 12781 ||| 12783 ||| 
2020 ||| importance of reliable eeg data in motor imagery classification: attention level-based approach. ||| 16074 ||| 16075 ||| 16076 ||| 16077 ||| 16078 ||| 
2021 ||| fine-grained temporal attention network for eeg-based seizure detection. ||| 16079 ||| 16080 ||| 16081 ||| 743 ||| 
2021 ||| attention-based spatio-temporal-spectral feature learning for subject-specific eeg classification. ||| 16082 ||| 16083 ||| 16084 ||| 
2017 ||| the effect of selective attention on multiple assrs for future bci application. ||| 16085 ||| 16086 ||| 
2020 ||| classification of selective attention based on steady-state somatosensory evoked potentials using high-frequency vibration stimuli. ||| 16087 ||| 16088 ||| 16089 ||| 16090 ||| 
2022 ||| decoding 3d representation of visual imagery eeg using attention-based dual-stream convolutional neural network. ||| 16091 ||| 16092 ||| 
2017 ||| identification of attention state for menu-selection using in-ear eeg recording. ||| 16093 ||| 16094 ||| 16095 ||| 16096 ||| 
2021 ||| classification of tactile perception and attention on natural textures from eeg signals. ||| 16097 ||| 16098 ||| 16099 ||| 
2022 ||| eeg-transformer: self-attention from transformer architecture for decoding eeg of imagined speech. ||| 16100 ||| 16101 ||| 
2022 ||| decoding high-level imagined speech using attention-based deep neural networks. ||| 16092 ||| 16102 ||| 16103 ||| 
2017 ||| multimodal integration, attention and sensory augmentation? ||| 16104 ||| 16105 ||| 11380 ||| 
2020 ||| domain adaptation of transformers for english word segmentation. ||| 16106 ||| 16107 ||| 16108 ||| 16109 ||| 
2021 ||| mrat-sql+gap: a portuguese text-to-sql transformer. ||| 16110 ||| 504 ||| 505 ||| 
2021 ||| code autocomplete using transformers. ||| 16111 ||| 16112 ||| 16113 ||| 16114 ||| 
2020 ||| bidirectional transformer based on online text-based information to implement convolutional neural network model for secure business investment. ||| 16115 ||| 16116 ||| 
2020 ||| the effect of spatial reference on visual attention and workload during viewpoint guidance in augmented reality. ||| 16117 ||| 16118 ||| 16119 ||| 16120 ||| 16121 ||| 
2019 ||| visual cues to restore student attention based on eye gaze drift, and application to an offshore training system. ||| 16122 ||| 16123 ||| 16124 ||| 
2021 ||| altering non-verbal cues to implicitly direct attention in social vr. ||| 16125 ||| 16126 ||| 8346 ||| 3831 ||| 16127 ||| 8348 ||| 16128 ||| 
2019 ||| multi-label aerial image classification using a bidirectional class-wise attention network. ||| 16129 ||| 16130 ||| 16131 ||| 
2019 ||| mapping human settlements with multi-seasonal sentinel-2 imagery and attention-based resnext. ||| 16132 ||| 16133 ||| 16134 ||| 318 ||| 16131 ||| 
2020 ||| attention-based secure feature extraction in near sensor processing: work-in-progress. ||| 14200 ||| 14199 ||| 16135 ||| 14201 ||| 
2021 ||| graph attention network based object detection and classification in crowded scenario. ||| 16136 ||| 16137 ||| 
2021 ||| hierarchical transformer encoders for vietnamese spelling correction. ||| 16138 ||| 16139 ||| 16140 ||| 16141 ||| 
2021 ||| an efficient transformer-based model for vietnamese punctuation prediction. ||| 16138 ||| 16139 ||| 16142 ||| 7517 ||| 
2021 ||| key point matching with transformers. ||| 16143 ||| 
2019 ||| is it worth the attention? a comparative evaluation of attention layers for argument unit segmentation. ||| 16144 ||| 16145 ||| 16146 ||| 16147 ||| 
2017 ||| technology demo of using real-time biofeedback of heart rate variability measures to track and help improve levels of attention and relaxation. ||| 16148 ||| 16149 ||| 
2017 ||| using real-time biofeedback of heart rate variability measures to track and help improve levels of attention and relaxation. ||| 16148 ||| 16149 ||| 16150 ||| 
2021 ||| what you see is what you get? - relating eye-tracking metrics to students' attention to game elements. ||| 16151 ||| 16152 ||| 16153 ||| 16154 ||| 3882 ||| 
2021 ||| non-intrusive classroom attention tracking system (nicats). ||| 16155 ||| 16156 ||| 16157 ||| 16158 ||| 
2018 ||| improving the teaching of vector group of three-phase transformer by integrating software and hardware tools into classroom. ||| 16159 ||| 
2020 ||| data mining approach for determining student attention pattern. ||| 16160 ||| 16161 ||| 16162 ||| 
2020 ||| teaching computational thinking to a student with attention deficit through programming. ||| 16163 ||| 16164 ||| 16165 ||| 59 ||| 
2017 ||| visual attention based evaluation for multiple-choice tests in e-learning applications. ||| 683 ||| 16166 ||| 16167 ||| 4620 ||| 5904 ||| 
2020 ||| analysis of balance controllers for cascaded modular solid-state transformer during steady-state, transient and fault conditions. ||| 16168 ||| 16169 ||| 16170 ||| 16171 ||| 
2020 ||| study of the impact of processes in electric power systems with res on the operation of numerical differential transformer protection. ||| 16172 ||| 16173 ||| 16174 ||| 16175 ||| 16176 ||| 16177 ||| 16178 ||| 16179 ||| 16180 ||| 16181 ||| 
2019 ||| the opportunities for efficiency increase of phase-shifting transformers in power transmission operational modes. ||| 16182 ||| 16183 ||| 16184 ||| 16185 ||| 
2021 ||| wide voltage-regulation range tap-changing transformer model for power system studies. ||| 852 ||| 16186 ||| 16187 ||| 16188 ||| 
2019 ||| a new transformerless configuration for grid-connected photovoltaic inverters. ||| 16189 ||| 16190 ||| 
2020 ||| application of demand response and smart battery electric vehicles charging for capacity utilization of the distribution transformer. ||| 16191 ||| 16192 ||| 16193 ||| 
2020 ||| comparative analysis of transformer-energizing and fault-caused voltage dips on the dynamic behavior of dfig-based wind turbines. ||| 16194 ||| 10980 ||| 16195 ||| 16196 ||| 
2021 ||| a novel approach for incipient fault diagnosis in power transformers by artificial neural networks. ||| 16197 ||| 16198 ||| 16199 ||| 
2018 ||| smart transformer based loop power controller in radial power distribution grid. ||| 16200 ||| 16201 ||| 10868 ||| 
2017 ||| an autonomous voltage control for distribution power system using pole-transformer. ||| 16202 ||| 16203 ||| 16204 ||| 
2017 ||| impacts of tap stagger on currents of power transformers. ||| 16205 ||| 16206 ||| 16207 ||| 16208 ||| 16209 ||| 
2019 ||| transformer loss of life mitigation in the presence of energy storage and pv generation. ||| 16210 ||| 16211 ||| 16212 ||| 
2019 ||| planning of oltc transformers in lv systems under conservation voltage reduction strategy. ||| 16213 ||| 16214 ||| 
2021 ||| assessing the impact of high penetration pv on the power transformer loss of life on a distribution system. ||| 16215 ||| 16216 ||| 16217 ||| 16218 ||| 
2020 ||| instantaneous flicker control strategy with oltc-fitted distribution transformers in lv networks. ||| 16219 ||| 16220 ||| 
2021 ||| influence of the power factor on the vibration behavior of transformers for primary and secondary distribution. ||| 16221 ||| 16222 ||| 16223 ||| 16224 ||| 16225 ||| 
2019 ||| novel method for numerical transformer differential protection setting up using its detailed mathematical model. ||| 16172 ||| 16181 ||| 16173 ||| 16174 ||| 16175 ||| 16176 ||| 16177 ||| 16178 ||| 16179 ||| 16180 ||| 
2019 ||| modeling students' attention in the classroom using eyetrackers. ||| 16226 ||| 16227 ||| 16158 ||| 16228 ||| 16229 ||| 
2021 ||| benefits of combining dimensional attention and working memory for partially observable reinforcement learning problems. ||| 16230 ||| 16231 ||| 
2021 ||| fast streaming translation using machine learning with transformer. ||| 16232 ||| 16233 ||| 16234 ||| 
2020 ||| attention patterns detection using brain computer interfaces. ||| 16235 ||| 16236 ||| 16237 ||| 16238 ||| 16239 ||| 16240 ||| 
2021 ||| emotion detection on greek social media using bidirectional encoder representations from transformers. ||| 16241 ||| 16242 ||| 16243 ||| 16244 ||| 16245 ||| 
2020 ||| banner advertisement effectiveness using big-5 personality traits, advertisement recall, and visual attention. ||| 16246 ||| 16247 ||| 
2018 ||| a study of micro-augmentations: personality, gender, emotions and effects on attention and brain waves. ||| 16248 ||| 16249 ||| 16250 ||| 16251 ||| 16252 ||| 
2020 ||| frequency-based multi task learning with attention mechanism for fault detection in power systems. ||| 16253 ||| 16254 ||| 
2021 ||| defense against power system time delay attacks via attention-based multivariate deep learning. ||| 16255 ||| 16256 ||| 16257 ||| 10325 ||| 16258 ||| 
2019 ||| a methodology for detecting stealthy transformer tap command injection attacks in smart grids. ||| 16259 ||| 16260 ||| 
2020 ||| dynamic state estimation based monitoring of high frequency transformer. ||| 16261 ||| 16262 ||| 16263 ||| 16264 ||| 3532 ||| 16265 ||| 
2021 ||| generative adversarial networks based on mixed-attentions for citation intent classification in scientific publications. ||| 16266 ||| 16267 ||| 16268 ||| 
2021 ||| incorporating domain knowledge into language transformers for multi-label classification of chinese medical questions. ||| 16269 ||| 16270 ||| 16268 ||| 
2021 ||| multi-label classification of chinese humor texts using hypergraph attention networks. ||| 16271 ||| 16272 ||| 16268 ||| 16273 ||| 
2017 |||  (two-stage attentional auditory model inspired neural network and its application to speaker identification) [in chinese]. ||| 16274 ||| 16275 ||| 16276 ||| 
2021 ||| ncu-nlp at rocling-2021 shared task: using macbert transformers for dimensional sentiment analysis. ||| 16272 ||| 16267 ||| 16277 ||| 16268 ||| 
2019 ||| spatial attention lesion detection on automated breast ultrasound. ||| 16278 ||| 16279 ||| 16280 ||| 16281 ||| 16282 ||| 3747 ||| 16283 ||| 16284 ||| 9442 ||| 16285 ||| 
2019 ||| investigation on the dependencies between hrv, physical training, and focus of attention in virtual environment. ||| 16286 ||| 16287 ||| 16288 ||| 16289 ||| 
2021 ||| bslkt: a bagging model with self-attention and lightgbm for knowledge tracing. ||| 16290 ||| 16291 ||| 
2021 ||| ship detection in large-scale sar images based on dense spatial attention and multi-level feature fusion. ||| 6900 ||| 6896 ||| 6899 ||| 6897 ||| 6898 ||| 6901 ||| 
2019 ||| graph attention propagation for few-shot learning. ||| 16292 ||| 16293 ||| 2313 ||| 
2020 ||| fast and precise energy consumption prediction based on fully convolutional attention res2net. ||| 497 ||| 16294 ||| 2487 ||| 
2020 ||| kt-xl: a knowledge tracing model for predicting learning performance based on transformer-xl. ||| 16295 ||| 16296 ||| 16297 ||| 16298 ||| 
2019 ||| an attention-based ambient network with 3d convolutional network for incomplete traffic flow prediction. ||| 16299 ||| 16300 ||| 16301 ||| 
2020 ||| feeling scarcity: augmenting human feelings through physicalizations of energy consumption, attention depletion and animal murder. ||| 16302 ||| 16303 ||| 2101 ||| 16304 ||| 16305 ||| 
2019 ||| attention guidance in second screen applications. ||| 16306 ||| 3831 ||| 16307 ||| 16308 ||| 16309 ||| 16310 ||| 
2021 ||| seneca: an attention support tool for context-related content learning. ||| 16311 ||| 16312 ||| 16313 ||| 16314 ||| 16315 ||| 
2021 ||| ut-atd: universal transformer for anomalous trajectory detection by embedding trajectory information. ||| 6553 ||| 16316 ||| 5809 ||| 411 ||| 
2017 ||| 3d memristor-based adjustable deep recurrent neural network with programmable attention mechanism. ||| 13661 ||| 16317 ||| 2771 ||| 
2021 ||| marl: multimodal attentional representation learning for disease prediction. ||| 16318 ||| 16319 ||| 16320 ||| 
2017 ||| selection and execution of simple actions via visual attention and direct parameter specification. ||| 16321 ||| 16322 ||| 16323 ||| 16324 ||| 13310 ||| 16325 ||| 
2021 ||| thermal image super-resolution using second-order channel attention with varying receptive fields. ||| 16326 ||| 16327 ||| 
2021 ||| object localization with attribute preference based on top-down attention. ||| 16328 ||| 16329 ||| 14153 ||| 5736 ||| 
2020 ||| language-oriented sentiment analysis based on the grammar structure and improved self-attention network. ||| 7516 ||| 16330 ||| 16331 ||| 7518 ||| 16332 ||| 
2019 ||| specialized visual sensor coupled to a dynamic neural field for embedded attentional process. ||| 16333 ||| 16334 ||| 16335 ||| 3766 ||| 16336 ||| 
2021 ||| occtransformers: learning occupancy using attention. ||| 16337 ||| 7404 ||| 
2019 ||| investigation of automatic video summarization using viewer's physiological, facial and attentional features. ||| 7111 ||| 16338 ||| 16339 ||| 
2019 ||| accuracy improvement of fashion style estimation with attention control of a classifier. ||| 16340 ||| 16341 ||| 16342 ||| 16343 ||| 
2020 ||| fake news detection on fake.br using hierarchical attention networks. ||| 16344 ||| 16345 ||| 3725 ||| 16346 ||| 
2022 ||| a targeted assessment of the syntactic abilities of transformer models for galician-portuguese. ||| 3418 ||| 3419 ||| 16347 ||| 
2020 ||| hardware accelerator for multi-head attention and position-wise feed-forward in the transformer. ||| 16348 ||| 16349 ||| 11182 ||| 10888 ||| 16350 ||| 
2019 ||| abstractive text summarization using enhanced attention model. ||| 16351 ||| 16352 ||| 16353 ||| 
2020 ||| grabbing pedestrian attention with interactive signboard for street advertising. ||| 16354 ||| 16355 ||| 
2017 ||| the design of a virtual reality game for stroke-induced attention deficits. ||| 16356 ||| 3157 ||| 16357 ||| 16358 ||| 16359 ||| 
2018 ||| the transmutation of perception: research of attention and visual guidance in virtual reality context. ||| 16360 ||| 
2019 ||| getting the player's attention: comparing the effectiveness of common notification types in task management games. ||| 16361 ||| 16362 ||| 16363 ||| 16364 ||| 
2019 ||| bi-directional attention flow for video alignment. ||| 16365 ||| 9841 ||| 16366 ||| 16367 ||| 
2021 ||| multi-task architecture with attention for imaging atmospheric cherenkov telescope data analysis. ||| 16368 ||| 16369 ||| 16370 ||| 6784 ||| 6785 ||| 16371 ||| 16372 ||| 
2022 ||| attention-based gender recognition on masked faces. ||| 13473 ||| 13475 ||| 16373 ||| 13477 ||| 
2022 ||| study of lidar segmentation and model's uncertainty using transformer for different pre-trainings. ||| 16374 ||| 16375 ||| 16376 ||| 
2019 ||| supervised spatial transformer networks for attention learning in fine-grained action recognition. ||| 11249 ||| 3906 ||| 11251 ||| 
2021 ||| embedding human knowledge into deep neural network via attention map. ||| 16377 ||| 15475 ||| 16378 ||| 16379 ||| 723 ||| 724 ||| 725 ||| 
2019 ||| real time eye gaze tracking system using cnn-based facial features for human attention measurement. ||| 16380 ||| 5759 ||| 
2020 ||| localizing visitors in natural sites exploiting modality attention on egocentric images and gps data. ||| 16381 ||| 16382 ||| 2223 ||| 2224 ||| 
2021 ||| latent video transformer. ||| 16383 ||| 16384 ||| 2646 ||| 2647 ||| 2648 ||| 
2022 ||| multimodal personality recognition using cross-attention transformer and behaviour encoding. ||| 16385 ||| 16386 ||| 16387 ||| 16388 ||| 1226 ||| 7320 ||| 7321 ||| 
2022 ||| bispectral pedestrian detection augmented with saliency maps using transformer. ||| 16389 ||| 16390 ||| 16391 ||| 16392 ||| 16393 ||| 
2022 ||| transformers in self-supervised monocular depth estimation with unknown camera intrinsics. ||| 16394 ||| 16395 ||| 16396 ||| 16397 ||| 
2022 ||| skeleton-based online sign language recognition using monotonic attention. ||| 16398 ||| 16399 ||| 16400 ||| 
2021 ||| upsampling attention network for single image super-resolution. ||| 16401 ||| 16402 ||| 16403 ||| 
2020 ||| semantic segmentation using light attention mechanism. ||| 16404 ||| 8731 ||| 
2021 ||| interpretation of human behavior from multi-modal brain mri images based on graph deep neural networks and attention mechanism. ||| 16405 ||| 16406 ||| 12831 ||| 
2022 ||| structurenet: deep context attention learning for structural component recognition. ||| 16407 ||| 2651 ||| 16408 ||| 
2022 ||| a comprehensive study of vision transformers on dense prediction tasks. ||| 16409 ||| 16410 ||| 16394 ||| 16411 ||| 16396 ||| 16397 ||| 
2019 ||| subjective annotations for vision-based attention level estimation. ||| 16412 ||| 16413 ||| 16414 ||| 16415 ||| 16416 ||| 16417 ||| 
2021 ||| long-term behaviour recognition in videos with actor-focused region attention. ||| 16418 ||| 16419 ||| 16420 ||| 
2020 ||| learn more from context: joint modeling of local and global attention for aspect sentiment classification. ||| 4815 ||| 10572 ||| 16421 ||| 16422 ||| 16423 ||| 16424 ||| 
2019 ||| effective self attention modeling for aspect based sentiment analysis. ||| 16425 ||| 12694 ||| 4201 ||| 709 ||| 
2017 ||| investigation of the visual attention role in clinical bioethics decision-making using machine learning algorithms. ||| 16426 ||| 16427 ||| 16428 ||| 16429 ||| 16430 ||| 16431 ||| 1994 ||| 16432 ||| 16433 ||| 16434 ||| 
2019 ||| meta-graph based attention-aware recommendation over heterogeneous information networks. ||| 4197 ||| 4198 ||| 1717 ||| 3442 ||| 4200 ||| 4201 ||| 
2020 ||| an empirical evaluation of attention and pointer networks for paraphrase generation. ||| 16435 ||| 16436 ||| 
2020 ||| detecting the most insightful parts of documents using a regularized attention-based model. ||| 16437 ||| 
2021 ||| transformer based models in fake news detection. ||| 16438 ||| 16439 ||| 16440 ||| 16441 ||| 
2019 ||| rumor detection on social media: a multi-view model using self-attention mechanism. ||| 16442 ||| 16443 ||| 6891 ||| 4201 ||| 
2021 ||| exploiting extensive external information for event detection through semantic networks word representation and attention map. ||| 16444 ||| 16445 ||| 241 ||| 16446 ||| 
2021 ||| automated method for evaluating neural network's attention focus. ||| 16447 ||| 16448 ||| 
2019 ||| short-term traffic congestion forecasting using attention-based long short-term memory recurrent neural network. ||| 16449 ||| 5439 ||| 16450 ||| 16451 ||| 16452 ||| 1166 ||| 
2021 ||| combining transformer-based models with traditional machine learning approaches for sexism identification in social networks at exist 2021. ||| 16453 ||| 16454 ||| 16455 ||| 
2018 ||| attention mechanism for aggressive detection. ||| 10949 ||| 10950 ||| 10946 ||| 10947 ||| 3882 ||| 10522 ||| 
2019 ||| from recurrency to attention in opinion analysis: comparing rnn vs transformer models. ||| 16456 ||| 16457 ||| 16458 ||| 16459 ||| 16460 ||| 16461 ||| 
2021 ||| umuteam at meoffendes 2021: ensemble learning for offensive language identification using linguistic features, fine-grained negation, and transformers. ||| 852 ||| 16462 ||| 16463 ||| 16464 ||| 16465 ||| 16466 ||| 16467 ||| 16468 ||| 3419 ||| 
2021 ||| transformer based offensive language identification in spanish. ||| 16469 ||| 16470 ||| 16471 ||| 
2019 ||| elirf-upv at irosva: transformer encoders for spanish irony detection. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 16474 ||| 16475 ||| 16476 ||| 
2020 ||| a parallel-attention model for tumor named entity recognition in spanish. ||| 6669 ||| 16477 ||| 14939 ||| 
2021 ||| umuteam at emoevales 2021: emotion analysis for spanish based on explainable linguistic features and transformers. ||| 852 ||| 16462 ||| 16463 ||| 16464 ||| 16478 ||| 16468 ||| 3419 ||| 
2020 ||| identification of cancer entities in clinical text combining transformers with dictionary features. ||| 16479 ||| 16480 ||| 16481 ||| 16482 ||| 
2021 ||| ai-upv at iberlef-2021 detoxis task: toxicity detection in immigration-related web news comments using transformers and statistical models. ||| 16483 ||| 16484 ||| 16485 ||| 
2020 ||| transformers and data augmentation for aggressiveness detection in mexican spanish. ||| 16486 ||| 16487 ||| 11927 ||| 11928 ||| 11929 ||| 
2021 ||| boosting transformers for job expression extraction and classification in a low-resource setting. ||| 16488 ||| 16489 ||| 15085 ||| 15086 ||| 
2019 ||| elirf-upv at tass 2019: transformer encoders for twitter sentiment analysis in spanish. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 16474 ||| 16475 ||| 16476 ||| 
2021 ||| haha@iberlef2021: humor analysis using ensembles of simple transformers. ||| 16490 ||| 16491 ||| 
2021 ||| exist2021: detecting sexism with transformers and translation-augmented data. ||| 16492 ||| 16493 ||| 
2020 ||| detecting aggressiveness in mexican spanish social media content by fine-tuning transformer-based models. ||| 16494 ||| 10488 ||| 10490 ||| 16495 ||| 
2021 ||| everything transformers: recognition, classification and normalisation of professions and family relations. ||| 16496 ||| 16497 ||| 
2021 ||| transformers pipeline for offensiveness detection in mexican spanish social media. ||| 16498 ||| 16499 ||| 16500 ||| 16501 ||| 11927 ||| 11928 ||| 11929 ||| 
2021 ||| emotion detection for spanish with data augmentation and transformer-based models. ||| 16502 ||| 
2020 ||| a tumor named entity recognition model based on pre-trained language model and attention mechanism. ||| 16503 ||| 16504 ||| 10466 ||| 
2020 ||| palomino-ochoa at tass 2020: transformer-based data augmentation for overcoming few-shot learning. ||| 10534 ||| 852 ||| 9983 ||| 
2021 ||| automatic sexism detection with multilingual transformer models ait fhstp@exist2021. ||| 16505 ||| 11303 ||| 16506 ||| 16507 ||| 16508 ||| 16509 ||| 16510 ||| 16511 ||| 16512 ||| 16513 ||| 16514 ||| 
2021 ||| transformer ensembles for sexism detection. ||| 16515 ||| 16516 ||| 16517 ||| 16518 ||| 
2021 ||| umuteam at exist 2021: sexist language identification based on linguistic features and transformers in spanish and english. ||| 852 ||| 16462 ||| 16463 ||| 16464 ||| 16478 ||| 16468 ||| 3419 ||| 
2021 ||| system description for exist shared task at iberlef 2021: automatic misogyny identification using pretrained transformers. ||| 16519 ||| 16520 ||| 16521 ||| 
2020 ||| automatic icd code classification with label description attention mechanism. ||| 16522 ||| 2101 ||| 16523 ||| 
2021 ||| umuteam at haha 2021: linguistic features and transformers for analysing spanish humor. the what, the how, and to whom. ||| 852 ||| 16462 ||| 16463 ||| 16464 ||| 16468 ||| 3419 ||| 
2019 ||| effectiveness of facial animated avatar and voice transformer in elearning programming course. ||| 16524 ||| 16525 ||| 16526 ||| 
2018 ||| the virtual schoolyard: attention training in virtual reality for children with attentional disorders. ||| 16527 ||| 16528 ||| 16529 ||| 16530 ||| 16531 ||| 16532 ||| 16533 ||| 16534 ||| 
2020 ||| video captioning using attention based visual fusion with bi-temporal context and bi-modal semantic feature learning. ||| 16535 ||| 16536 ||| 16537 ||| 
2019 ||| winding deformation detection of transformer based on sweep frequency impedance. ||| 4600 ||| 128 ||| 16538 ||| 16539 ||| 16540 ||| 16541 ||| 16542 ||| 16543 ||| 
2019 ||| analysis winding deformation of power transformer detection using sweep frequency impedance technology. ||| 128 ||| 16544 ||| 16545 ||| 16546 ||| 16547 ||| 241 ||| 16540 ||| 16548 ||| 
2021 ||| a pre-ln transformer network model with lexical features for fine-grained sentiment classification. ||| 168 ||| 16549 ||| 16550 ||| 16551 ||| 
2018 ||| prior knowledge integrated with self-attention for event detection. ||| 185 ||| 1400 ||| 785 ||| 16552 ||| 
2020 ||| position-aware hybrid attention network for aspect-level sentiment analysis. ||| 16553 ||| 2514 ||| 16554 ||| 13587 ||| 16555 ||| 
2018 ||| question-answering aspect classification with multi-attention representation. ||| 16556 ||| 16557 ||| 3083 ||| 16558 ||| 3085 ||| 
2020 ||| hierarchical attention network in stock prediction. ||| 16559 ||| 16560 ||| 16561 ||| 16562 ||| 16563 ||| 8957 ||| 16564 ||| 
2021 ||| lda-transformer model in chinese poetry authorship attribution. ||| 16565 ||| 16566 ||| 16567 ||| 16568 ||| 
2017 ||| combine non-text features with deep learning structures based on attention-lstm for answer selection. ||| 16569 ||| 10544 ||| 10546 ||| 10545 ||| 
2018 ||| joint attention lstm network for aspect-level sentiment analysis. ||| 9920 ||| 16570 ||| 
2017 ||| more attention, less deficit: wearable eeg-based serious game for focus improvement. ||| 10379 ||| 10382 ||| 10381 ||| 
2020 ||| vera: virtual environments recording attention. ||| 6955 ||| 6958 ||| 6959 ||| 6956 ||| 6957 ||| 
2018 ||| assessing attention in visual and textual programming using neuroeducation approaches. ||| 16571 ||| 16572 ||| 16573 ||| 16574 ||| 16575 ||| 
2020 ||| notional machines in computing education: the education of attention. ||| 16576 ||| 16577 ||| 16578 ||| 16579 ||| 16580 ||| 16581 ||| 16582 ||| 16583 ||| 16584 ||| 16585 ||| 16586 ||| 16587 ||| 16588 ||| 
2020 ||| extracting biomedical relations via a multi-head attention based graph convolutional network. ||| 16589 ||| 1704 ||| 16590 ||| 3279 ||| 16591 ||| 8974 ||| 8349 ||| 
2020 ||| an end-to-end oxford nanopore basecaller using convolution-augmented transformer. ||| 16592 ||| 16593 ||| 16594 ||| 16595 ||| 
2020 ||| predicting drugs for covid-19/sars-cov-2 via heterogeneous graph attention networks. ||| 16596 ||| 9472 ||| 16597 ||| 16598 ||| 16599 ||| 16600 ||| 3488 ||| 
2020 ||| multi-view multi-label learning with dual-attention networks for stroke screen. ||| 11120 ||| 340 ||| 11121 ||| 459 ||| 
2020 ||| deeparc: an attention-based hybrid model for predicting transcription factor binding sites from positional embedded dna sequence. ||| 16601 ||| 16602 ||| 
2020 ||| structured information extraction of pathology reports with attention-based graph convolutional network. ||| 16603 ||| 16604 ||| 16605 ||| 16606 ||| 399 ||| 
2021 ||| sgat: a self-supervised graph attention network for biomedical relation extraction. ||| 16607 ||| 16590 ||| 3279 ||| 16591 ||| 8974 ||| 16608 ||| 
2019 ||| disease prediction model based on bilstm and attention mechanism. ||| 11466 ||| 16609 ||| 16610 ||| 
2021 ||| low-dimensional depth local dual-view features embedded transformer for electrocardiogram signal quality assessment. ||| 16611 ||| 16612 ||| 16613 ||| 16614 ||| 
2019 ||| attentiondta: prediction of drug-target binding affinity using attention model. ||| 16615 ||| 16616 ||| 16617 ||| 16618 ||| 1130 ||| 
2021 ||| deeppppred: deep ensemble learning with transformers, recurrent and convolutional neural networks for human protein-phenotype co-mention classification. ||| 16619 ||| 16620 ||| 16621 ||| 16622 ||| 
2021 ||| hydrogen bonds meet self-attention: all you need for protein structure embedding. ||| 10980 ||| 16623 ||| 16624 ||| 16625 ||| 16626 ||| 
2020 ||| attention based detection for central serious chorioretinopathy in fundus image. ||| 718 ||| 16627 ||| 16628 ||| 14034 ||| 16629 ||| 16630 ||| 
2021 ||| document-level biomedical relation extraction with generative adversarial network and dual-attention multi-instance learning. ||| 16631 ||| 16632 ||| 16633 ||| 
2019 ||| an attention-based neural network basecaller for oxford nanopore sequencing data. ||| 16634 ||| 16635 ||| 16636 ||| 16637 ||| 1130 ||| 
2020 ||| brain functional connectivity pattern recognition for attention-deficit/hyperactivity disorder diagnosis. ||| 16638 ||| 16639 ||| 3417 ||| 
2021 ||| attent: domain-adaptive medical image segmentation via attention-aware translation and adversarial entropy minimization. ||| 399 ||| 9999 ||| 5110 ||| 16640 ||| 16641 ||| 11395 ||| 
2020 ||| attention-based saliency hashing for ophthalmic image retrieval. ||| 15296 ||| 15560 ||| 5330 ||| 5331 ||| 5206 ||| 
2018 ||| protein-protein interaction article classification: a knowledge-enriched self-attention convolutional neural network approach. ||| 5015 ||| 16590 ||| 3279 ||| 16591 ||| 8974 ||| 8349 ||| 8967 ||| 8978 ||| 16566 ||| 
2021 ||| multi-scale hierarchical transformer structure for 3d medical image segmentation. ||| 16642 ||| 13818 ||| 16643 ||| 16644 ||| 5030 ||| 16645 ||| 16646 ||| 
2020 ||| deep multi-instance learning with induced self-attention for medical image classification. ||| 16647 ||| 570 ||| 16648 ||| 16649 ||| 571 ||| 
2021 ||| pg-tfnet: transformer-based fusion network integrating pathological images and genomic data for cancer survival analysis. ||| 16650 ||| 16651 ||| 10233 ||| 16652 ||| 7830 ||| 16653 ||| 
2020 ||| constructing a relevance-oriented dataset for training transformer rankers for medical search. ||| 4613 ||| 16654 ||| 
2021 ||| haunet-3d: a novel hierarchical attention 3d unet for lung nodule segmentation. ||| 16655 ||| 16656 ||| 16657 ||| 16658 ||| 16659 ||| 16660 ||| 524 ||| 968 ||| 
2021 ||| transmixnet: an attention based double-branch model for white blood cell classification and its training with the fuzzified training data. ||| 16661 ||| 16662 ||| 16663 ||| 16664 ||| 16665 ||| 16666 ||| 16667 ||| 
2021 ||| paenet: a progressive attention-enhanced network for 3d to 2d retinal vessel segmentation. ||| 16668 ||| 5166 ||| 16669 ||| 16670 ||| 16671 ||| 16672 ||| 16673 ||| 
2021 ||| attention-based convolutional neural networks for protein-protein interaction site prediction. ||| 16674 ||| 16675 ||| 16676 ||| 16677 ||| 
2018 ||| breast cancer classification with electronic medical records using hierarchical attention bidirectional networks. ||| 5659 ||| 16678 ||| 5662 ||| 
2021 ||| hgna-hti: heterogeneous graph neural network with attention mechanism for prediction of herb-target interactions. ||| 16679 ||| 3890 ||| 16680 ||| 
2021 ||| transformer-based multi-target regression on electronic health records for primordial prevention of cardiovascular disease. ||| 16681 ||| 16682 ||| 16683 ||| 16684 ||| 
2021 ||| a graph attention neural network for diagnosing asd with fmri data. ||| 16685 ||| 16686 ||| 16687 ||| 
2021 ||| ammasurv: asymmetrical multi-modal attention for accurate survival analysis with whole slide images and gene expression data. ||| 16688 ||| 16689 ||| 16690 ||| 16691 ||| 
2021 ||| personalized clinical pathway recommendation via attention based pre-training. ||| 16692 ||| 8177 ||| 16693 ||| 5819 ||| 16694 ||| 16695 ||| 9695 ||| 16696 ||| 
2020 ||| attention-based transformers for instance segmentation of cells in microstructures. ||| 16697 ||| 16698 ||| 16699 ||| 
2021 ||| sasa-net: a spatial-aware self-attention mechanism for building protein 3d structure directly from inter-residue distances. ||| 16700 ||| 16701 ||| 16702 ||| 16703 ||| 
2021 ||| darnet: dual-attention residual network for automatic diagnosis of covid-19 via ct images. ||| 15657 ||| 15656 ||| 16704 ||| 11241 ||| 16705 ||| 15660 ||| 5474 ||| 
2020 ||| multi-class metabolic pathway prediction by graph attention-based deep learning method. ||| 16706 ||| 16662 ||| 16707 ||| 16708 ||| 16665 ||| 
2019 ||| predicting disease-related rna associations based on graph convolutional attention network. ||| 16709 ||| 16710 ||| 16711 ||| 1274 ||| 16712 ||| 11670 ||| 
2021 ||| a transformer-based network for pathology image classification. ||| 16713 ||| 16714 ||| 16715 ||| 3267 ||| 
2021 ||| emotion transformer fusion: complementary representation properties of eeg and eye movements on recognizing anger and surprise. ||| 16716 ||| 16717 ||| 8207 ||| 684 ||| 
2019 ||| deeptriager: a neural attention model for emergency triage with electronic health records. ||| 16718 ||| 1782 ||| 16719 ||| 16720 ||| 10333 ||| 
2019 ||| dense encoder-decoder network based on two-level context enhanced residual attention mechanism for segmentation of breast tumors in magnetic resonance imaging. ||| 16721 ||| 16722 ||| 16723 ||| 16724 ||| 16725 ||| 
2019 ||| semi-supervised attention-guided cyclegan for data augmentation on medical images. ||| 16726 ||| 16727 ||| 16728 ||| 
2019 ||| fusing transformer model with temporal features for ecg heartbeat classification. ||| 16729 ||| 16730 ||| 301 ||| 295 ||| 
2021 ||| accurate brain age prediction model for healthy children and adolescents using 3d-cnn and dimensional attention. ||| 16731 ||| 16732 ||| 16733 ||| 16734 ||| 
2021 ||| deepanis: predicting antibody paratope from concatenated cdr sequences by integrating bidirectional long-short-term memory and transformer neural networks. ||| 16735 ||| 16736 ||| 16737 ||| 16738 ||| 16595 ||| 
2021 ||| cac-emvt: efficient coronary artery calcium segmentation with multi-scale vision transformers. ||| 16739 ||| 16740 ||| 16741 ||| 8718 ||| 16742 ||| 13794 ||| 
2017 ||| chemical-induced disease extraction via convolutional neural networks with attention. ||| 16743 ||| 12391 ||| 5231 ||| 1117 ||| 
2021 ||| an interpretable multi-level enhanced graph attention network for disease diagnosis with gene expression data. ||| 16744 ||| 1856 ||| 6799 ||| 1796 ||| 3473 ||| 16745 ||| 1265 ||| 16746 ||| 
2019 ||| emotion recognition from children speech signals using attention based time series deep learning. ||| 16747 ||| 16748 ||| 16749 ||| 16750 ||| 
2021 ||| radiology report generation for rare diseases via few-shot transformer. ||| 16751 ||| 1090 ||| 538 ||| 5893 ||| 16752 ||| 1093 ||| 16753 ||| 
2021 ||| bioie: biomedical information extraction with multi-head attention enhanced graph convolutional network. ||| 16603 ||| 16754 ||| 16755 ||| 1305 ||| 16606 ||| 399 ||| 
2020 ||| multi-scale strategy based 3d dual-encoder brain tumor segmentation network with attention mechanism. ||| 16756 ||| 13195 ||| 11188 ||| 16757 ||| 
2021 ||| emotion recognition from multi-channel eeg data through a dual-pipeline graph attention network. ||| 2008 ||| 4807 ||| 16758 ||| 16759 ||| 
2021 ||| wearable sensor gait analysis of fall detection using attention network. ||| 16760 ||| 16761 ||| 16762 ||| 16763 ||| 
2019 ||| fine-grained thyroid nodule classification via multi-semantic attention network. ||| 16764 ||| 16765 ||| 16766 ||| 16767 ||| 16768 ||| 1099 ||| 7300 ||| 
2020 ||| msdan: multi-scale self-attention unsupervised domain adaptation network for thyroid ultrasound images. ||| 16769 ||| 16770 ||| 16771 ||| 5950 ||| 16772 ||| 8479 ||| 16773 ||| 14761 ||| 16774 ||| 
2021 ||| transpicker: a transformer-based framework for particle picking in cryoem micrographs. ||| 1460 ||| 16775 ||| 16776 ||| 16777 ||| 16652 ||| 16778 ||| 16653 ||| 
2021 ||| agmi: attention-guided multi-omics integration for drug response prediction with graph neural networks. ||| 16779 ||| 16780 ||| 16781 ||| 16782 ||| 16783 ||| 1236 ||| 
2020 ||| extraction and classification of tcm medical records based on bert and bi-lstm with attention mechanism. ||| 16784 ||| 16785 ||| 16786 ||| 16787 ||| 2829 ||| 
2019 ||| automatic epileptic seizure detection via attention-based cnn-birnn. ||| 16788 ||| 16789 ||| 16747 ||| 
2021 ||| main: multimodal attention-based fusion networks for diagnosis prediction. ||| 13558 ||| 16790 ||| 16791 ||| 1130 ||| 16792 ||| 
2021 ||| structure-based protein-drug affinity prediction with spatial attention mechanisms. ||| 16793 ||| 16794 ||| 16795 ||| 10980 ||| 16796 ||| 16626 ||| 
2018 ||| attention-based recurrent multi-channel neural network for influenza epidemic prediction. ||| 16797 ||| 16798 ||| 16799 ||| 16800 ||| 16801 ||| 13805 ||| 16802 ||| 16803 ||| 7647 ||| 
2020 ||| modeling multivariate time series via prototype learning: a multi-level attention-based perspective. ||| 16804 ||| 299 ||| 16805 ||| 775 ||| 16806 ||| 296 ||| 
2019 ||| drug target interaction prediction using multi-task learning and co-attention. ||| 16807 ||| 2389 ||| 16808 ||| 16809 ||| 
2020 ||| a diversified supervised based u-shape colorectal lesion segmentor with meaningful feature supplement and multi-level residual attention mechanism. ||| 16810 ||| 16723 ||| 16811 ||| 16721 ||| 
2020 ||| structure enhanced protein-drug interaction prediction using transformer and graph embedding. ||| 16812 ||| 16813 ||| 16814 ||| 16815 ||| 16816 ||| 
2021 ||| detecting chronic vascular damage with attention-guided neural system. ||| 16817 ||| 16818 ||| 16819 ||| 16820 ||| 
2020 ||| cross-modal self-attention distillation for prostate cancer segmentation. ||| 16821 ||| 16822 ||| 5298 ||| 16823 ||| 16824 ||| 16825 ||| 16826 ||| 5300 ||| 
2018 ||| full-attention based drug drug interaction extraction exploiting user-generated content. ||| 728 ||| 16827 ||| 16551 ||| 16828 ||| 8974 ||| 16590 ||| 8349 ||| 16829 ||| 
2020 ||| attention u-net for interpretable classification on chest x-ray image. ||| 13514 ||| 10333 ||| 
2019 ||| multi-stage attention-unet for wireless capsule endoscopy image bleeding area segmentation. ||| 16830 ||| 538 ||| 16831 ||| 301 ||| 
2021 ||| rcga-net: an improved multi-hybrid attention mechanism network in biomedical image segmentation. ||| 3996 ||| 16832 ||| 16833 ||| 16834 ||| 13196 ||| 16835 ||| 16836 ||| 
2019 ||| an attention-based semi-supervised neural network for thyroid nodules segmentation. ||| 14759 ||| 16837 ||| 16771 ||| 14761 ||| 5950 ||| 16772 ||| 8479 ||| 16773 ||| 16774 ||| 
2021 ||| arsc-net: adventitious respiratory sound classification network using parallel paths with channel-spatial attention. ||| 4060 ||| 16838 ||| 3888 ||| 16839 ||| 8968 ||| 1130 ||| 
2021 ||| attention-enhanced graph cross-convolution for protein-ligand binding affinity prediction. ||| 16840 ||| 16841 ||| 16842 ||| 16843 ||| 16844 ||| 16845 ||| 
2017 ||| exploiting argument information to improve biomedical event trigger identification via recurrent neural networks and supervised attention mechanisms. ||| 16631 ||| 1305 ||| 
2021 ||| exploring feasibility of truth-involved automatic sleep staging combined with transformer. ||| 16846 ||| 952 ||| 11670 ||| 16847 ||| 16848 ||| 16849 ||| 16850 ||| 
2018 ||| correlated attention networks for multimodal emotion recognition. ||| 16851 ||| 16852 ||| 16853 ||| 
2019 ||| multi-level glioma segmentation using 3d u-net combined attention mechanism with atrous convolution. ||| 16838 ||| 3888 ||| 16854 ||| 16855 ||| 1130 ||| 
2020 ||| dce-mri based breast intratumor heterogeneity analysis via dual attention deep clustering network and its application in molecular typing. ||| 16856 ||| 13195 ||| 16757 ||| 
2020 ||| predicting prescriptions via dsca-dual sequences with cross attention network. ||| 16857 ||| 1557 ||| 7087 ||| 16858 ||| 4069 ||| 
2020 ||| chemical-protein interaction extraction via chemicalbert and attention guided graph convolutional networks in parallel. ||| 16859 ||| 16860 ||| 8284 ||| 
2021 ||| jointly learning to align and aggregate with cross attention pooling for peptide-mhc class i binding prediction. ||| 10980 ||| 16794 ||| 16652 ||| 2073 ||| 16626 ||| 
2019 ||| cascaded convolutional neural network with attention mechanism for mobile eeg-based driver drowsiness detection system. ||| 16861 ||| 16614 ||| 16862 ||| 16863 ||| 16864 ||| 16613 ||| 
2021 ||| fam: fully attention module for medical image segmentation. ||| 15293 ||| 16865 ||| 
2020 ||| respiratory sound classification based on bigru-attention network with xgboost. ||| 16866 ||| 16867 ||| 16868 ||| 16869 ||| 10417 ||| 
2018 ||| biomedical event trigger detection based on bilstm integrating attention mechanism and sentence vector. ||| 15301 ||| 16631 ||| 16870 ||| 16871 ||| 16872 ||| 16873 ||| 
2019 ||| combined self-attention mechanism for biomedical event trigger identification. ||| 5302 ||| 16874 ||| 
2021 ||| a meta-path based drug-target prediction model with collaborative attention mechanisms. ||| 16875 ||| 16829 ||| 16876 ||| 16877 ||| 16878 ||| 
2021 ||| an attention based deep learning model for direct estimation of pharmacokinetic maps from dce-mri images. ||| 16879 ||| 15576 ||| 
2020 ||| a two-level attention-based sequence-to-sequence model for accurate inter-patient arrhythmia detection. ||| 16880 ||| 16730 ||| 16881 ||| 301 ||| 5845 ||| 1160 ||| 
2018 ||| an attention-based bi-gru-capsnet model for hypernymy detection between compound entities. ||| 6627 ||| 16882 ||| 16883 ||| 16884 ||| 16885 ||| 16886 ||| 
2019 ||| an improved biomedical event trigger identification framework via modeling document with hierarchical attention. ||| 16887 ||| 16888 ||| 16889 ||| 16890 ||| 16891 ||| 16892 ||| 16893 ||| 
2021 ||| disease correlation enhanced attention network for icd coding. ||| 16894 ||| 2754 ||| 16895 ||| 16896 ||| 
2020 ||| hierarchical attention-based multiple instance learning network for patient-level lung cancer diagnosis. ||| 16897 ||| 2883 ||| 1244 ||| 16898 ||| 16899 ||| 16900 ||| 15532 ||| 
2021 ||| predicting drug-mirna resistance with layer attention graph convolution network and multi channel feature extraction. ||| 16901 ||| 16902 ||| 12525 ||| 16903 ||| 13735 ||| 
2021 ||| cc-denseunet: densely connected u-net with criss-cross attention for liver and tumor segmentation in ct volumes. ||| 16895 ||| 16904 ||| 16905 ||| 16906 ||| 16907 ||| 16908 ||| 1825 ||| 
2021 ||| automatic icd-10 coding based on multi-head attention mechanism and gated residual network. ||| 5457 ||| 5458 ||| 16909 ||| 5459 ||| 452 ||| 
2021 ||| dcet-net: dual-stream convolution expanded transformer for breast cancer histopathological image classification. ||| 16910 ||| 11985 ||| 11986 ||| 2304 ||| 11988 ||| 
2021 ||| ct-cad: context-aware transformers for end-to-end chest abnormality detection on x-rays. ||| 16911 ||| 4285 ||| 16912 ||| 16913 ||| 
2021 ||| automated grading of knee osteoarthritis x-ray images based on attention mechanism. ||| 16914 ||| 2058 ||| 16915 ||| 16916 ||| 
2021 ||| ect-nas: searching efficient cnn-transformers architecture for medical image segmentation. ||| 13554 ||| 13555 ||| 
2019 ||| cross attention densely connected networks for multiple sclerosis lesion segmentation. ||| 16917 ||| 5213 ||| 5858 ||| 16918 ||| 
2018 ||| attention-based multi-task learning in pharmacovigilance. ||| 16919 ||| 16920 ||| 16921 ||| 16922 ||| 
2020 ||| brain mr image super-resolution using 3d feature attention network. ||| 609 ||| 16923 ||| 16924 ||| 611 ||| 16925 ||| 
2021 ||| graph attention mechanism-based deep tensor factorization for predicting disease-associated mirna-mirna pairs. ||| 16600 ||| 16926 ||| 16832 ||| 16927 ||| 16928 ||| 
2018 ||| attention and concentration in normal and deaf gamers. ||| 16929 ||| 16930 ||| 8396 ||| 16931 ||| 16932 ||| 
2021 ||| an acne grading framework on face images via skin attention and sfnet. ||| 16933 ||| 16934 ||| 16935 ||| 16936 ||| 16937 ||| 16938 ||| 
2021 ||| dual attention feature fusion network for monocular depth estimation. ||| 5296 ||| 765 ||| 16939 ||| 438 ||| 16940 ||| 
2021 ||| attention scale-aware deformable network for inshore ship detection in surveillance videos. ||| 16941 ||| 2349 ||| 10646 ||| 9472 ||| 
2021 ||| a hierarchical multi-label classification algorithm for scientific papers based on graph attention networks. ||| 16942 ||| 7689 ||| 7688 ||| 7690 ||| 16943 ||| 16944 ||| 
2021 ||| syllable level speech emotion recognition based on formant attention. ||| 16945 ||| 16946 ||| 16947 ||| 
2021 ||| dga-net: dynamic gaussian attention network for sentence semantic matching. ||| 1558 ||| 15204 ||| 444 ||| 1301 ||| 
2021 ||| unsupervised domain adaptation via attention augmented mutual networks for person re-identification. ||| 16948 ||| 16949 ||| 
2021 ||| enhanced attribute alignment based on semantic co-attention for text-based person search. ||| 1371 ||| 7827 ||| 
2021 ||| attention guided retinex architecture search for robust low-light image enhancement. ||| 16950 ||| 16951 ||| 13917 ||| 16952 ||| 16953 ||| 
2021 ||| multi-view relevance matching model of scientific papers based on graph convolutional network and attention mechanism. ||| 16954 ||| 7689 ||| 7688 ||| 7690 ||| 16943 ||| 16944 ||| 
2019 ||| collaborative attention network for natural language inference. ||| 16955 ||| 16956 ||| 16957 ||| 16958 ||| 
2017 ||| chinese dialects identification using attention-based deep neural networks. ||| 16959 ||| 6858 ||| 16960 ||| 16961 ||| 16962 ||| 
2019 ||| a study of visual attention elements with experiment analysis based on composition. ||| 1706 ||| 16963 ||| 6827 ||| 
2018 ||| a multi-label image classification algorithm based on attention model. ||| 16964 ||| 16965 ||| 
2021 ||| transformer-ic: the solution to information loss. ||| 16966 ||| 16967 ||| 16968 ||| 16969 ||| 
2019 ||| design of intelligent artificial agents: its application in joint attention task for children with autism. ||| 4679 ||| 4681 ||| 
2018 ||| image-based attention level estimation of interaction scene by head pose and gaze information. ||| 16970 ||| 16971 ||| 16972 ||| 
2020 ||| interpretable deep attention model for multivariate time series prediction in building energy systems. ||| 12631 ||| 12632 ||| 12633 ||| 12635 ||| 
2017 ||| a guided spatial transformer network for histology cell differentiation. ||| 16973 ||| 16974 ||| 16975 ||| 16976 ||| 6818 ||| 
2019 ||| inter and intra document attention for depression risk assessment. ||| 16977 ||| 16978 ||| 16979 ||| 
2019 ||| efficient transformer-based sentence encoding for sentence pair modelling. ||| 3223 ||| 3225 ||| 
2020 ||| query focused abstractive summarization via incorporating query relevance and transfer learning with transformer models. ||| 16980 ||| 10289 ||| 768 ||| 
2019 ||| the unreasonable effectiveness of transformer language models in grammatical error correction. ||| 16981 ||| 4869 ||| 4870 ||| 
2018 ||| co-attention based neural network for source-dependent essay scoring. ||| 789 ||| 3188 ||| 
2021 ||| on the application of transformers for estimating the difficulty of multiple-choice questions from text. ||| 13974 ||| 16982 ||| 13976 ||| 16983 ||| 16984 ||| 16985 ||| 
2020 ||| an exploratory study of argumentative writing by young students: a transformer-based approach. ||| 16986 ||| 16987 ||| 16988 ||| 
2019 ||| tmu transformer system using bert for re-ranking at bea 2019 grammatical error correction on restricted track. ||| 16989 ||| 16990 ||| 16991 ||| 14211 ||| 
2021 ||| unsupervised methods for the study of transformer embeddings. ||| 1225 ||| 1226 ||| 1227 ||| 1228 ||| 
2018 ||| analysis of resting state eeg signals of adults with attention-deficit hyperactivity disorder. ||| 16992 ||| 16993 ||| 16994 ||| 16995 ||| 
2020 ||| assessing differences on eye fixations by attention levels in an assembly environment. ||| 16996 ||| 16997 ||| 
2020 ||| detection of subject attention in an active environment through facial expressions using deep learning techniques and computer vision. ||| 16998 ||| 16999 ||| 17000 ||| 17001 ||| 17002 ||| 17003 ||| 17004 ||| 
2021 ||| attentional and emotional engagement of sustainability in tourism marketing: electroencephalographic (eeg) and peripheral neuroscientific approach. ||| 17005 ||| 17006 ||| 17007 ||| 
2020 ||| technological innovation to assess cognitive functions in attention deficit hyperactivity disorder. ||| 17008 ||| 5335 ||| 17009 ||| 17010 ||| 17011 ||| 
2021 ||| modeling the dynamic visual attention resource allocation in cockpit with discrete event simulation. ||| 17012 ||| 13805 ||| 17013 ||| 2424 ||| 17014 ||| 
2020 ||| evaluation of the attention effect of the fraser-wilcox illusion in a visual discrimination task. ||| 17015 ||| 17016 ||| 17017 ||| 17018 ||| 
2020 ||| the impact of a biological driver state monitoring system on visual attention during partially automated driving. ||| 17019 ||| 17020 ||| 17021 ||| 17022 ||| 
2021 ||| measure of the attentional bias in children using eye tracking during a psychological test. ||| 17023 ||| 17024 ||| 17025 ||| 17026 ||| 17027 ||| 17028 ||| 17029 ||| 
2020 ||| driver's visual attention analysis in smart car with fhud. ||| 17030 ||| 16834 ||| 17031 ||| 17032 ||| 17033 ||| 
2020 ||| collaborative cognitive training game to enhance selective sustained attention in preschoolers. ||| 17034 ||| 17035 ||| 17036 ||| 
2020 ||| characterizing driver workload and attention in a simulated automated vehicle. ||| 17037 ||| 17038 ||| 
2017 ||| eeg-engagement index and auditory alarm misperception: an inattentional deafness study in actual flight condition. ||| 15325 ||| 15326 ||| 15327 ||| 13128 ||| 17039 ||| 17040 ||| 17041 ||| 17042 ||| 
2019 ||| effect of different visual stimuli on joint attention of asd children using nao robot. ||| 17043 ||| 17044 ||| 17045 ||| 17001 ||| 17046 ||| 
2020 ||| exploring attention in vr: effects of visual and auditory modalities. ||| 17047 ||| 17048 ||| 17049 ||| 17050 ||| 17051 ||| 
2019 ||| gear shifter design - lack of dedicated positions and the contribution to cognitive load and inattention. ||| 17052 ||| 5574 ||| 17053 ||| 17054 ||| 
2021 ||| visual attention of pedestrians in traffic scenes: a crowdsourcing experiment. ||| 17055 ||| 17056 ||| 17057 ||| 
2021 ||| the influence of e-commerce web page format on information area under attention mechanism. ||| 2532 ||| 3772 ||| 3114 ||| 700 ||| 6810 ||| 
2020 ||| engagement in the cinematography of videogames: proposal of an algorithm for colors and foci of attention analysis. ||| 17058 ||| 17059 ||| 
2019 ||| emotion measurement from attention analysis on imagery in virtual reality. ||| 17060 ||| 17061 ||| 17062 ||| 
2020 ||| the role of attentional networks in secondary task engagement in the context of partially automated driving. ||| 1056 ||| 17063 ||| 781 ||| 
2021 ||| an interactive guide based on learning objects to train teachers on the detection and support of children with attention deficit hyperactivity disorder. ||| 17064 ||| 17065 ||| 17066 ||| 17067 ||| 17068 ||| 17069 ||| 7442 ||| 
2019 ||| happiness on instagram - content analysis and engagement based on attention theory. ||| 17070 ||| 17071 ||| 
2021 ||| the hierarchy in the temporary interaction micro-processes that precede the breaking points of focal attention in an object of the new media. ||| 17072 ||| 17073 ||| 
2020 ||| augmented reality assisted sensory integration therapy for improving attention of children with autism. ||| 10812 ||| 17074 ||| 
2021 ||| sharing photos on social media: visual attention affects real-world decision making. ||| 17075 ||| 12986 ||| 12987 ||| 12988 ||| 12989 ||| 
2020 ||| effect of paired stimuli on joint attention of children with asd. ||| 17043 ||| 17044 ||| 17045 ||| 17046 ||| 17001 ||| 
2017 ||| visual attention and recall in website advertisements: an eye tracking study. ||| 17076 ||| 17077 ||| 
2019 ||| towards the development of a universal testing environment for attention guiding techniques. ||| 14927 ||| 14926 ||| 3369 ||| 17078 ||| 14928 ||| 
2017 ||| predicting stimulus-driven attentional selection within mobile interfaces. ||| 17079 ||| 17080 ||| 17081 ||| 17082 ||| 
2020 ||| comparing effect of active vs. passive robotic interaction on joint attention of children with asd. ||| 17044 ||| 17043 ||| 17045 ||| 17046 ||| 17001 ||| 
2021 ||| spatio-temporal 3d action recognition with hierarchical self-attention mechanism. ||| 17083 ||| 17084 ||| 
2021 ||| attribute-image similarity measure for multimodal attention mechanism. ||| 17085 ||| 17084 ||| 
2019 ||| patient-level classification on clinical note sequences guided by attributed hierarchical attention. ||| 3525 ||| 3526 ||| 1195 ||| 3528 ||| 
2020 ||| t-egat: a temporal edge enhanced graph attention network for tax evasion detection. ||| 17086 ||| 17087 ||| 17088 ||| 17089 ||| 17090 ||| 17091 ||| 17092 ||| 
2018 ||| applied attention-based lstm neural networks in stock prediction. ||| 17093 ||| 17094 ||| 17095 ||| 
2020 ||| hyper-parameter optimization with reinforce and masked attention auto-regressive density estimators. ||| 17096 ||| 17097 ||| 17098 ||| 2577 ||| 17099 ||| 
2021 ||| come-ke: a new transformers based approach for knowledge extraction in conflict and mediation domain. ||| 9970 ||| 2822 ||| 7162 ||| 9971 ||| 9973 ||| 9972 ||| 
2019 ||| non-local attention learning on large heterogeneous information networks. ||| 17100 ||| 17101 ||| 17102 ||| 17103 ||| 
2021 ||| two-stage image dehazing with depth information and cross-scale non-local attention. ||| 9728 ||| 17104 ||| 
2020 ||| egad: evolving graph representation learning with self-attention and knowledge distillation for live video streaming events. ||| 17105 ||| 17106 ||| 17107 ||| 
2021 ||| session-aware item-combination recommendation with transformer network. ||| 17108 ||| 17109 ||| 
2018 ||| comparative study of cnn and lstm based attention neural networks for aspect-level opinion mining. ||| 16712 ||| 11670 ||| 1129 ||| 17110 ||| 
2021 ||| impact of attention on adversarial robustness of image classification models. ||| 17111 ||| 12821 ||| 12822 ||| 12823 ||| 
2020 ||| hypergraph attention isomorphism network by learning line graph expansion. ||| 15179 ||| 17112 ||| 7043 ||| 
2021 ||| human-like explanation for text classification with limited attention supervision. ||| 8977 ||| 3525 ||| 17113 ||| 3526 ||| 1195 ||| 3528 ||| 
2020 ||| evanet: an extreme value attention network for long-term air quality prediction. ||| 17114 ||| 17115 ||| 17116 ||| 17117 ||| 17118 ||| 
2018 ||| spatio-temporal attention based recurrent neural network for next location prediction. ||| 17119 ||| 17120 ||| 9751 ||| 
2021 ||| dra u-net: an attention based u-net framework for 2d medical image segmentation. ||| 8283 ||| 17121 ||| 17122 ||| 17123 ||| 17124 ||| 17125 ||| 1099 ||| 17126 ||| 
2019 ||| hierarchical-document-structure-aware attention with adaptive cost sensitive learning for biomedical document classification. ||| 16888 ||| 16887 ||| 16889 ||| 6278 ||| 16892 ||| 16710 ||| 16893 ||| 
2021 ||| hamlet: hierarchical attention-based model with multi-task self-training for user profiling. ||| 17127 ||| 17128 ||| 17129 ||| 11800 ||| 1288 ||| 1094 ||| 
2019 ||| attention-based multi-task learning for sensor analytics. ||| 356 ||| 359 ||| 
2017 ||| product function need recognition via semi-supervised attention network. ||| 17130 ||| 9989 ||| 17131 ||| 1094 ||| 
2021 ||| an ensemble of transformer and lstm approach for multivariate time series data classification. ||| 17132 ||| 17133 ||| 17134 ||| 17135 ||| 17136 ||| 
2019 ||| modeling human attention by learning from large amount of emotional images. ||| 17137 ||| 
2020 ||| explainable software vulnerability detection based on attention-based bidirectional recurrent neural networks. ||| 17138 ||| 920 ||| 17139 ||| 5348 ||| 
2019 ||| costock: a deepfm model for stock market prediction with attentional embeddings. ||| 17140 ||| 1325 ||| 1326 ||| 
2021 ||| performance profile of transformer fine-tuning in multi-gpu cloud environments. ||| 104 ||| 17141 ||| 103 ||| 
2021 ||| glow : global weighted self-attention network for web search. ||| 17142 ||| 17143 ||| 17144 ||| 6415 ||| 17145 ||| 9755 ||| 3705 ||| 17146 ||| 17147 ||| 
2021 ||| transformer oil temperature prediction based on long and short-term memory networks. ||| 17148 ||| 17149 ||| 17150 ||| 17151 ||| 17152 ||| 
2020 ||| graphsanet: a graph neural network and self attention based approach for spatial temporal prediction in sensor network. ||| 8440 ||| 8441 ||| 8443 ||| 8444 ||| 8445 ||| 8442 ||| 
2019 ||| explainable authorship verification in social media via attention-based similarity learning. ||| 17153 ||| 17154 ||| 8252 ||| 17155 ||| 
2021 ||| attention-augmented spatio-temporal segmentation for land cover mapping. ||| 9763 ||| 17156 ||| 9764 ||| 17157 ||| 17158 ||| 9766 ||| 
2019 ||| abr-hic: attention based bidirectional rnn for hierarchical industry classification. ||| 17159 ||| 17087 ||| 17092 ||| 17160 ||| 17161 ||| 17088 ||| 
2019 ||| metapath enhanced graph attention encoder for hins representation learning. ||| 17162 ||| 1090 ||| 1094 ||| 17163 ||| 1093 ||| 
2021 ||| multi-input-output fusion attention module for deblurring networks. ||| 17164 ||| 17165 ||| 15240 ||| 17166 ||| 17167 ||| 
2021 ||| on exploring attention-based explanation for transformer models in text classification. ||| 17168 ||| 17169 ||| 17170 ||| 17171 ||| 
2020 ||| 2d-att: causal inference for mobile game organic installs with 2-dimensional attentional neural network. ||| 15845 ||| 17172 ||| 17173 ||| 17174 ||| 
2020 ||| attention-based lstm network for covid-19 clinical trial parsing. ||| 17175 ||| 17176 ||| 17177 ||| 17178 ||| 
2018 ||| how to become instagram famous: post popularity prediction with dual-attention. ||| 8760 ||| 2417 ||| 17179 ||| 17180 ||| 2166 ||| 
2021 ||| transforming fake news: robust generalisable news classification using transformers. ||| 17181 ||| 17182 ||| 
2018 ||| market abnormality period detection via co-movement attention model. ||| 7400 ||| 17183 ||| 9642 ||| 1094 ||| 13468 ||| 13467 ||| 
2021 ||| non-parallel text style transfer using self-attentional discriminator as supervisor. ||| 17184 ||| 11190 ||| 17185 ||| 
2018 ||| cam: a combined attention model for natural language inference. ||| 4240 ||| 17186 ||| 914 ||| 4242 ||| 17187 ||| 
2021 ||| a hierarchical attention graph convolutional network for traffic incident impact forecasting. ||| 17188 ||| 17189 ||| 17190 ||| 17191 ||| 17192 ||| 
2020 ||| cosine similarity distance pruning algorithm based on graph attention mechanism. ||| 17193 ||| 17194 ||| 17195 ||| 17196 ||| 
2020 ||| link prediction based on heuristics and graph attention. ||| 17197 ||| 17198 ||| 16833 ||| 17199 ||| 
2021 ||| a re-thinking asr modeling framework using attention mechanisms. ||| 17200 ||| 17201 ||| 
2019 ||| attention-based multi-layer chinese word embedding. ||| 5537 ||| 6189 ||| 6187 ||| 6188 ||| 
2018 ||| efficient super resolution for large-scale images using attentional gan. ||| 17202 ||| 17203 ||| 17204 ||| 17205 ||| 
2020 ||| mastgn: multi-attention spatio-temporal graph networks for air pollution prediction. ||| 17206 ||| 17207 ||| 
2021 ||| soft sensing transformer: hundreds of sensors are worth a single word. ||| 8862 ||| 17208 ||| 17209 ||| 17210 ||| 17211 ||| 17212 ||| 17213 ||| 
2020 ||| hardening soft information: a transformer-based approach to forecasting stock return volatility. ||| 17214 ||| 17215 ||| 3831 ||| 
2020 ||| self-calibrated attention residual network for image super-resolution. ||| 17216 ||| 17104 ||| 17217 ||| 17218 ||| 
2021 ||| multi-feature urban traffic prediction based on unconstrained graph attention network. ||| 17219 ||| 6284 ||| 6285 ||| 
2019 ||| study of the effects of visual complexity and consumer experience on visual attention and purchase behavior through the use of eye tracking. ||| 17220 ||| 17221 ||| 17222 ||| 
2021 ||| soft-sensing conformer: a curriculum learning-based convolutional transformer. ||| 17208 ||| 8862 ||| 17211 ||| 17209 ||| 17210 ||| 17223 ||| 17213 ||| 
2021 ||| object interaction recommendation with multi-modal attention-based hierarchical graph neural network. ||| 17224 ||| 17225 ||| 17226 ||| 
2020 ||| urban crowdsensing using social media: an empirical study on transformer and recurrent neural networks. ||| 17227 ||| 17228 ||| 15208 ||| 
2020 ||| glima: global and local time series imputation with multi-directional attention learning. ||| 1628 ||| 17229 ||| 6121 ||| 17230 ||| 17231 ||| 1630 ||| 
2019 ||| learning to generate diverse and authentic reviews via an encoder-decoder model with transformer and gru. ||| 17232 ||| 1325 ||| 17233 ||| 
2021 ||| spatiotemporal vision transformer for short time weather forecasting. ||| 1401 ||| 1402 ||| 1403 ||| 
2019 ||| ctc-attention based non-parametric inference modeling for clinical state progression. ||| 17234 ||| 17235 ||| 5738 ||| 
2021 ||| temporal and spatial attention network model based evolution model for bulk commodity price fluctuation risk. ||| 17236 ||| 17237 ||| 
2021 ||| ise-yolo: improved squeeze-and-excitation attention module based yolo for blood cells detection. ||| 5010 ||| 17238 ||| 17239 ||| 
2018 ||| per: a probabilistic attentional model for personalized text recommendations. ||| 1429 ||| 17240 ||| 9988 ||| 9989 ||| 17241 ||| 1094 ||| 
2019 ||| deep multi-head attention network for aspect-based sentiment analysis. ||| 8220 ||| 17242 ||| 17243 ||| 17244 ||| 17245 ||| 
2019 ||| weighted focus-attention deep network for fine-grained image classification. ||| 17246 ||| 3049 ||| 8626 ||| 17247 ||| 
2019 ||| human-object contour for action recognition with attentional multi-modal fusion network. ||| 17248 ||| 17249 ||| 17250 ||| 1589 ||| 4634 ||| 
2022 ||| ciafill: lightweight and fast image inpainting with channel independent attention. ||| 17251 ||| 17252 ||| 17253 ||| 
2020 ||| predictive analysis of business processes using neural networks with attention mechanism. ||| 17254 ||| 17255 ||| 17256 ||| 4194 ||| 17257 ||| 
2022 ||| multiview attention for 3d object detection in lidar point cloud. ||| 17258 ||| 17259 ||| 17260 ||| 
2019 ||| chinese story generation with fasttext transformer network. ||| 17261 ||| 17262 ||| 17263 ||| 
2022 ||| multi-head cnn and lstm with attention for user status estimation from biometric information. ||| 17264 ||| 17265 ||| 17266 ||| 17267 ||| 17268 ||| 17269 ||| 
2020 ||| effective-target representation via lstm with attention for aspect-level sentiment analysis. ||| 14059 ||| 17270 ||| 
2022 ||| whole slide image analysis and detection of prostate cancer using vision transformers. ||| 17271 ||| 17272 ||| 17273 ||| 17274 ||| 17275 ||| 17276 ||| 
2021 ||| small object detection using context and attention. ||| 17277 ||| 17278 ||| 17279 ||| 17280 ||| 
2020 ||| multi person pose estimation with attention. ||| 17281 ||| 17282 ||| 
2018 ||| document level polarity classification with attention gated recurrent unit. ||| 17283 ||| 4554 ||| 4555 ||| 17284 ||| 4556 ||| 
2022 ||| economic denial of sustainability (edos) attack detection by attention on flow-based in software defined network (sdn). ||| 17285 ||| 17286 ||| 
2021 ||| non-local self-attention mechanism for real-time context embedding deep shadow removal network. ||| 17287 ||| 11871 ||| 
2018 ||| classification of human attention to multimedia lecture. ||| 17288 ||| 17289 ||| 17290 ||| 
2019 ||| application of granger causality in decoding covert selective attention with human eeg. ||| 17291 ||| 17292 ||| 11443 ||| 1340 ||| 17293 ||| 
2021 ||| trans-attention multiple instance learning for cancer tissue classification in digital histopathology images. ||| 17294 ||| 17295 ||| 17296 ||| 
2021 ||| crossmodal matching transformer based x-ray and ct image registration for tevar. ||| 5860 ||| 17297 ||| 17298 ||| 17299 ||| 16833 ||| 5480 ||| 17300 ||| 17301 ||| 
2021 ||| fa-net: attention-based fusion network for malware https traffic classification. ||| 17302 ||| 17303 ||| 17304 ||| 17305 ||| 
2020 ||| an ai-based visual attention model for vehicle make and model recognition. ||| 17306 ||| 17307 ||| 
2021 ||| fktan: fusion keystroke time-textual attention networks for continuous authentication. ||| 769 ||| 855 ||| 247 ||| 17308 ||| 7015 ||| 861 ||| 
2021 ||| improved face detector on fisheye images via spherical-domain attention. ||| 17309 ||| 17310 ||| 17311 ||| 17312 ||| 3212 ||| 17303 ||| 
2021 ||| multi-modal fake news detection on social media with dual attention fusion networks. ||| 769 ||| 767 ||| 855 ||| 247 ||| 17308 ||| 5264 ||| 861 ||| 
2021 ||| interpretable deep learning method for attack detection based on spatial domain attention. ||| 2618 ||| 17313 ||| 17314 ||| 17315 ||| 
2020 ||| a multivariate time series prediction schema based on multi-attention in recurrent neural network. ||| 12524 ||| 17303 ||| 17316 ||| 3212 ||| 14133 ||| 17317 ||| 
2018 ||| multi-agent communication with attentional and recurrent message integration. ||| 17318 ||| 8750 ||| 8752 ||| 
2021 ||| improved cnn-based magnetic indoor positioning system using attention mechanism. ||| 17319 ||| 17320 ||| 17321 ||| 17322 ||| 
2021 ||| drug-drug interaction extraction from biomedical texts based on multi-attention mechanism. ||| 17323 ||| 1160 ||| 13408 ||| 17324 ||| 
2018 ||| exploring the correlation between attention and cognitive load of students when attending different classes. ||| 17325 ||| 17326 ||| 17327 ||| 17328 ||| 
2019 ||| visual attention analysis during program debugging using virtual reality eye tracker. ||| 17329 ||| 17330 ||| 17331 ||| 17328 ||| 
2019 ||| a comparative study of attention-based encoder-decoder approaches to natural scene text recognition. ||| 17332 ||| 17333 ||| 7191 ||| 4191 ||| 
2021 ||| going full-tilt boogie on document understanding with text-image-layout transformer. ||| 17334 ||| 17335 ||| 17336 ||| 17337 ||| 17338 ||| 17339 ||| 
2019 ||| reelfa: a scene text recognizer with encoded location and focused attention. ||| 17340 ||| 17341 ||| 5123 ||| 17342 ||| 2730 ||| 17343 ||| 17344 ||| 
2021 ||| recognizing handwritten chinese texts with insertion and swapping using a structural attention network. ||| 17345 ||| 17346 ||| 9783 ||| 779 ||| 
2019 ||| recurrent comparator with attention models to detect counterfeit documents. ||| 17347 ||| 17348 ||| 17349 ||| 17350 ||| 17351 ||| 17352 ||| 
2017 ||| attention-based extraction of structured information from street view imagery. ||| 17353 ||| 6339 ||| 17354 ||| 17355 ||| 1695 ||| 6338 ||| 6341 ||| 
2017 ||| visual attention models for scene text recognition. ||| 17356 ||| 17357 ||| 9332 ||| 
2021 ||| on the use of attention in deep learning based denoising method for ancient cham inscription images. ||| 17358 ||| 17359 ||| 17360 ||| 17361 ||| 17362 ||| 
2021 ||| multi-task learning for newspaper image segmentation and baseline detection using attention-based u-net architecture. ||| 17363 ||| 17364 ||| 17365 ||| 17366 ||| 17367 ||| 
2021 ||| multimodal attention-based learning for imbalanced corporate documents classification. ||| 17368 ||| 17369 ||| 17370 ||| 17371 ||| 17372 ||| 17373 ||| 17374 ||| 17375 ||| 
2021 ||| a transformer-based math language model for handwritten math expression recognition. ||| 17376 ||| 9793 ||| 13731 ||| 17377 ||| 9794 ||| 
2019 ||| woodblock-printing mongolian words recognition by bi-lstm with attention mechanism. ||| 13708 ||| 13707 ||| 4600 ||| 4499 ||| 
2019 ||| document binarization via multi-resolutional attention model with drd loss. ||| 17378 ||| 1589 ||| 17379 ||| 
2019 ||| lpga: line-of-sight parsing with graph-based attention for math formula recognition. ||| 17380 ||| 17381 ||| 17382 ||| 17383 ||| 
2021 ||| a-vlad: an end-to-end attention-based neural network for writer identification in historical documents. ||| 17384 ||| 13731 ||| 9794 ||| 
2019 ||| an attention-based end-to-end model for multiple text lines recognition in japanese historical documents. ||| 9792 ||| 9793 ||| 9794 ||| 
2021 ||| labeling document images for e-commence products with tree-based segment re-organizing and hierarchical transformer. ||| 3675 ||| 17385 ||| 2969 ||| 1171 ||| 1170 ||| 
2021 ||| 2d self-attention convolutional recurrent network for offline handwritten text recognition. ||| 9792 ||| 13731 ||| 9794 ||| 
2017 ||| attention based rnn model for document image quality assessment. ||| 17386 ||| 17387 ||| 17388 ||| 17389 ||| 17390 ||| 
2021 ||| attention to warp: deep metric learning for multivariate time series. ||| 17391 ||| 17392 ||| 17393 ||| 12269 ||| 12271 ||| 6933 ||| 6934 ||| 
2021 ||| fast recognition for multidirectional and multi-type license plates with 2d spatial attention. ||| 1302 ||| 17394 ||| 17395 ||| 14747 ||| 17396 ||| 5051 ||| 
2019 ||| on the use of attention mechanism in a seq2seq based approach for off-line handwritten digit string recognition. ||| 17397 ||| 17398 ||| 15326 ||| 17399 ||| 
2021 ||| a transcription is all you need: learning to align through attention. ||| 17400 ||| 17401 ||| 17402 ||| 9787 ||| 7111 ||| 
2021 ||| key-guided identity document classification method by graph attention network. ||| 17403 ||| 683 ||| 4646 ||| 17404 ||| 3879 ||| 
2021 ||| vision transformer for fast and efficient scene text recognition. ||| 17405 ||| 
2017 ||| scan, attend and read: end-to-end handwritten paragraph recognition with mdlstm attention. ||| 13992 ||| 17406 ||| 4194 ||| 59 ||| 17407 ||| 17408 ||| 
2019 ||| recognition of japanese historical text lines by an attention-based encoder-decoder and text line generation. ||| 13730 ||| 17409 ||| 17410 ||| 17411 ||| 9792 ||| 
2019 ||| eaten: entity-aware attention for single shot visual text extraction. ||| 17412 ||| 17413 ||| 17414 ||| 2538 ||| 2536 ||| 1761 ||| 
2021 ||| dynamic receptive field adaptation for attention-based text recognition. ||| 17415 ||| 14747 ||| 17416 ||| 5051 ||| 
2019 ||| attention after attention: reading text in the wild with cross attention. ||| 17417 ||| 17418 ||| 6559 ||| 17419 ||| 17420 ||| 
2019 ||| a character attention generative adversarial network for degraded historical document restoration. ||| 17421 ||| 9793 ||| 17422 ||| 9794 ||| 
2021 ||| transformer for handwritten text recognition using bidirectional post-decoding. ||| 17423 ||| 17424 ||| 17425 ||| 17426 ||| 17427 ||| 
2019 ||| multiple comparative attention network for offline handwritten chinese character recognition. ||| 17428 ||| 17429 ||| 2222 ||| 
2017 ||| a gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. ||| 4489 ||| 1010 ||| 4463 ||| 
2021 ||| attention based multiple siamese network for offline signature verification. ||| 17430 ||| 17431 ||| 
2017 ||| weakly supervised text attention network for generating text proposals in scene images. ||| 5204 ||| 17432 ||| 17433 ||| 8923 ||| 
2019 ||| multi-modal attention network for handwritten mathematical expression recognition. ||| 17434 ||| 1010 ||| 4489 ||| 1009 ||| 
2021 ||| handwritten mathematical expression recognition with bidirectionally trained transformer. ||| 17435 ||| 17436 ||| 17437 ||| 17438 ||| 16785 ||| 17439 ||| 
2017 ||| segmentation-free printed traditional mongolian ocr using sequence to sequence with attention model. ||| 4600 ||| 13707 ||| 790 ||| 4499 ||| 
2019 ||| contextual stroke classification in online handwritten documents with graph attention networks. ||| 13234 ||| 17440 ||| 17441 ||| 779 ||| 
2017 ||| convolutional neural network with attention mechanism for historical chinese character recognition. ||| 17442 ||| 17387 ||| 
2021 ||| hcadecoder: a hybrid ctc-attention decoder for chinese text recognition. ||| 14575 ||| 17443 ||| 17117 ||| 17444 ||| 
2021 ||| an encoder-decoder approach to handwritten mathematical expression recognition with multi-head attention and stacked decoder. ||| 17445 ||| 472 ||| 7191 ||| 
2019 ||| a deep learning method for automatic visual attention detection in older drivers. ||| 17446 ||| 17447 ||| 17448 ||| 17449 ||| 
2019 ||| poster: attention-based spatio-temporal model for har using multivariate time series. ||| 17450 ||| 765 ||| 17451 ||| 2930 ||| 
2019 ||| poster: watchyouwatch a web-cam based natural customer attention tracking shelf. ||| 17452 ||| 3232 ||| 17453 ||| 1973 ||| 
2020 ||| short-term load forecasting based on cnn-bilstm with bayesian optimization and attention mechanism. ||| 17454 ||| 4723 ||| 17455 ||| 
2019 ||| attention-based supply-demand prediction for autonomous vehicles. ||| 17456 ||| 17457 ||| 14115 ||| 17458 ||| 17459 ||| 
2019 ||| an affect computing based attention estimation. ||| 17460 ||| 17461 ||| 17462 ||| 17463 ||| 
2021 ||| structure optimization method of power transformer based on intelligent algorithm. ||| 5904 ||| 6578 ||| 17464 ||| 17465 ||| 
2021 ||| short text generation based on adversarial graph attention networks. ||| 17466 ||| 
2021 ||| interactive attention graph convolution networks for aspect-level sentiment classification. ||| 5752 ||| 17467 ||| 17468 ||| 
2021 ||| partial discharge detection of transformer winding. ||| 17469 ||| 
2021 ||| intelligent detection system of transformer winding temperature based on distributed optical fiber. ||| 17469 ||| 
2021 ||| research on cloud-edge collaborative processing method of distribution internet of things based on attention-lstm. ||| 17470 ||| 16735 ||| 17471 ||| 17472 ||| 1558 ||| 17473 ||| 
2021 ||| attention-based feature fusion network for fake reviews detection. ||| 17474 ||| 6721 ||| 
2021 ||| the cause of transformer zero sequence overcurrent protection act. ||| 17475 ||| 6579 ||| 
2019 ||| a hierarchy transformer network for extractive summaries. ||| 17476 ||| 817 ||| 
2021 ||| using graph attention network to predicte urban traffic flow. ||| 17477 ||| 17478 ||| 17479 ||| 
2018 ||| an attention-based long-short-term-memory model for paraphrase generation. ||| 17480 ||| 17481 ||| 17482 ||| 
2020 ||| track-assignment detailed routing using attention-based policy model with supervision. ||| 17483 ||| 17484 ||| 17485 ||| 17486 ||| 17487 ||| 
2021 ||| a circuit attention network-based actor-critic learning approach to robust analog transistor sizing. ||| 17488 ||| 17489 ||| 17490 ||| 17491 ||| 17492 ||| 17493 ||| 7486 ||| 
2021 ||| dynamic transformer for efficient machine translation on embedded devices. ||| 17494 ||| 17495 ||| 17496 ||| 17497 ||| 17498 ||| 17499 ||| 
2020 ||| 5g-transformer meets network service federation: design, implementation and evaluation. ||| 6088 ||| 6087 ||| 6091 ||| 4046 ||| 6092 ||| 6096 ||| 17500 ||| 2259 ||| 
2022 ||| remot: a hardware-software architecture for attention-guided multi-object tracking with dynamic vision sensors on fpgas. ||| 17501 ||| 307 ||| 17502 ||| 
2018 ||| scaling notifications beyond alerts: from subtly drawing attention up to forcing the user to take action. ||| 17503 ||| 17504 ||| 17505 ||| 
2020 ||| flashattention: data-centric interaction for data transformation using programming-by-example. ||| 7211 ||| 17506 ||| 9891 ||| 17507 ||| 
2018 ||| augmented collaboration in shared space design with shared attention and manipulation. ||| 17508 ||| 17509 ||| 17510 ||| 16094 ||| 17511 ||| 
2021 ||| motion improvisation: 3d human motion synthesis with a transformer. ||| 17512 ||| 17513 ||| 
2019 ||| attention-based recurrent neural networks (rnns) for short text classification: an application in public health monitoring. ||| 17514 ||| 17515 ||| 17516 ||| 17517 ||| 
2021 ||| detection of tumor morphology mentions in clinical reports in spanish using transformers. ||| 17518 ||| 17519 ||| 3419 ||| 852 ||| 17520 ||| 17521 ||| 17522 ||| 17523 ||| 
2019 ||| document model with attention bidirectional recurrent network for gender identification. ||| 17524 ||| 17525 ||| 
2017 ||| enjoyment, immersion, and attentional focus in a virtual reality exergame with differing visual environments. ||| 17526 ||| 17527 ||| 17528 ||| 17529 ||| 17530 ||| 17531 ||| 17532 ||| 
2020 ||| attention, awareness, and analysis: video clubs as meaningful venues for teacher noticing and culturally-sustaining pedagogy. ||| 17533 ||| 17534 ||| 17535 ||| 
2020 ||| designing for joint attention and co-presence across parallel realities. ||| 17536 ||| 
2020 ||| simultaneous paraphrasing and translation by fine-tuning transformer models. ||| 3052 ||| 
2017 ||| an empirical study of adequate vision span for attention-based neural machine translation. ||| 17537 ||| 6420 ||| 
2018 ||| a shared attention mechanism for interpretation of neural automatic post-editing systems. ||| 17538 ||| 17539 ||| 17540 ||| 
2018 ||| enhancement of encoder and attention using target monolingual corpora in neural machine translation. ||| 10307 ||| 17541 ||| 4908 ||| 
2020 ||| balancing cost and benefit with tied-multi transformers. ||| 14704 ||| 17542 ||| 17541 ||| 
2019 ||| a weakly supervised text detection based on attention mechanism. ||| 17543 ||| 17544 ||| 17545 ||| 
2017 ||| combining object-based attention and attributes for image captioning. ||| 17546 ||| 17547 ||| 17548 ||| 17549 ||| 
2021 ||| towards boosting channel attention for real image denoising: sub-band pyramid attention. ||| 15990 ||| 17550 ||| 17551 ||| 1371 ||| 6126 ||| 
2021 ||| sa-gnn: stereo attention and graph neural network for stereo image super-resolution. ||| 17552 ||| 6658 ||| 17553 ||| 
2021 ||| dual attention guided r2 u-net architecture for right ventricle segmentation in mri images. ||| 479 ||| 17554 ||| 17555 ||| 10075 ||| 
2021 ||| 6d object pose estimation with mutual attention fusion. ||| 17556 ||| 17557 ||| 17558 ||| 
2017 ||| neural image caption generation with global feature based attention scheme. ||| 17559 ||| 1906 ||| 
2019 ||| residual joint attention network with graph structure inference for object detection. ||| 17560 ||| 17561 ||| 17562 ||| 
2021 ||| multi-scale attention-based feature pyramid networks for object detection. ||| 17563 ||| 17564 ||| 17565 ||| 3993 ||| 13429 ||| 
2019 ||| attention to head locations for crowd counting. ||| 17566 ||| 17567 ||| 17568 ||| 11620 ||| 781 ||| 
2021 ||| facial action unit detection based on transformer and attention mechanism. ||| 17569 ||| 17570 ||| 17561 ||| 
2021 ||| pst-net: point cloud sampling via point-based transformer. ||| 8012 ||| 14114 ||| 17571 ||| 17572 ||| 14115 ||| 
2021 ||| aroa: attention refinement one-stage anchor-free detector for objects in remote sensing imagery. ||| 17573 ||| 17574 ||| 17575 ||| 6490 ||| 17576 ||| 17577 ||| 
2017 ||| attention-sharing correlation learning for cross-media retrieval. ||| 17578 ||| 17579 ||| 5954 ||| 
2017 ||| an unsupervised change detection approach for remote sensing image using visual attention mechanism. ||| 17580 ||| 17581 ||| 17582 ||| 
2019 ||| attention-aware invertible hashing network. ||| 10768 ||| 17583 ||| 17584 ||| 17585 ||| 17586 ||| 17587 ||| 
2021 ||| multi-level features selection network based on multi-attention for salient object detection. ||| 17588 ||| 369 ||| 17589 ||| 
2021 ||| human-object interaction detection based on multi-scale attention fusion. ||| 17590 ||| 17591 ||| 
2021 ||| attention-guided siamese network for clothes-changing person re-identification. ||| 17592 ||| 17593 ||| 6528 ||| 
2019 ||| online handwritten diagram recognition with graph attention networks. ||| 13233 ||| 17440 ||| 13234 ||| 779 ||| 
2021 ||| learning cross-domain descriptors for 2d-3d matching with hard triplet loss and spatial transformer network. ||| 17594 ||| 17595 ||| 3691 ||| 17596 ||| 17597 ||| 17598 ||| 17599 ||| 17600 ||| 5287 ||| 
2021 ||| cab-net: channel attention block network for pathological image cell nucleus segmentation. ||| 7115 ||| 17601 ||| 12500 ||| 
2019 ||| visual tracking with attentional convolutional siamese networks. ||| 17602 ||| 17603 ||| 
2021 ||| hetero-stan: crowd flow prediction by heterogeneous spatio-temporal attention network. ||| 17604 ||| 17605 ||| 17606 ||| 17607 ||| 
2019 ||| u-net with attention mechanism for retinal vessel segmentation. ||| 17608 ||| 17609 ||| 17610 ||| 
2019 ||| a stackable attention-guided multi-scale cnn for number plate detection. ||| 17611 ||| 17612 ||| 7804 ||| 6781 ||| 17613 ||| 17614 ||| 6782 ||| 17615 ||| 
2021 ||| gscam: global spatial coordinate attention module for fine-grained image recognition. ||| 17616 ||| 17617 ||| 17618 ||| 
2021 ||| 3d-resnet fused attention for autism spectrum disorder classification. ||| 17619 ||| 11241 ||| 17620 ||| 17621 ||| 
2019 ||| mma: motion memory attention network for video object detection. ||| 17622 ||| 17623 ||| 17624 ||| 382 ||| 
2019 ||| spatial-temporal bottom-up top-down attention model for action recognition. ||| 17625 ||| 17626 ||| 
2021 ||| hpcseg-net: hippocampus segmentation network integrating autofocus attention mechanism and feature recombination and recalibration module. ||| 2304 ||| 6518 ||| 692 ||| 6519 ||| 6520 ||| 6521 ||| 6522 ||| 
2021 ||| uav track planning algorithm based on graph attention network and deep q network. ||| 17627 ||| 17628 ||| 17629 ||| 
2021 ||| semi-supervised attention-guided vnet for breast cancer detection via multi-task learning. ||| 17630 ||| 208 ||| 1706 ||| 5476 ||| 6582 ||| 
2020 ||| gate-fusion transformer for multimodal sentiment analysis. ||| 17631 ||| 17632 ||| 
2019 ||| transferable attention for domain adaptation. ||| 17633 ||| 13824 ||| 17634 ||| 17635 ||| 17636 ||| 
2017 ||| let your photos talk: generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. ||| 16550 ||| 1699 ||| 2165 ||| 13811 ||| 
2019 ||| context-aware self-attention networks. ||| 3037 ||| 595 ||| 3039 ||| 3040 ||| 3309 ||| 3041 ||| 
2018 ||| path-based attention neural model for fine-grained entity typing. ||| 9589 ||| 17637 ||| 17638 ||| 17639 ||| 3228 ||| 
2020 ||| partial correlation-based attention for multivariate time series forecasting. ||| 17640 ||| 
2021 ||| an attention based multi-view model for sarcasm cause detection (student abstract). ||| 17641 ||| 17642 ||| 17643 ||| 17644 ||| 
2019 ||| recurrent attention model for pedestrian attribute recognition. ||| 17645 ||| 17646 ||| 10065 ||| 2278 ||| 17647 ||| 17648 ||| 
2019 ||| structured two-stream attention network for video question answering. ||| 1039 ||| 17649 ||| 9576 ||| 17650 ||| 17651 ||| 2165 ||| 1040 ||| 
2019 ||| sta: spatial-temporal attention for large-scale video-based person re-identification. ||| 9107 ||| 17652 ||| 1905 ||| 17653 ||| 
2021 ||| attention beam: an image captioning approach (student abstract). ||| 17654 ||| 3835 ||| 
2018 ||| hierarchical attention flow for multiple-choice reading comprehension. ||| 17655 ||| 3174 ||| 3310 ||| 3311 ||| 
2019 ||| connecting language to images: a progressive attention-guided network for simultaneous image captioning and language grounding. ||| 17656 ||| 1235 ||| 16280 ||| 17657 ||| 
2021 ||| dual-level collaborative transformer for image captioning. ||| 17658 ||| 17659 ||| 2504 ||| 17660 ||| 6831 ||| 2382 ||| 17661 ||| 2367 ||| 
2020 ||| a knowledge-aware attentional reasoning network for recommendation. ||| 17662 ||| 17663 ||| 378 ||| 4237 ||| 4191 ||| 
2017 ||| battrae: bidimensional attention-based recursive autoencoders for learning bilingual phrase embeddings. ||| 3180 ||| 3181 ||| 3182 ||| 
2021 ||| act: an attentive convolutional transformer for efficient text classification. ||| 481 ||| 738 ||| 740 ||| 17664 ||| 17665 ||| 12438 ||| 17666 ||| 17667 ||| 
2021 ||| cascade network with guided loss and hybrid attention for finding good correspondences. ||| 3148 ||| 1856 ||| 17668 ||| 
2020 ||| sneq: semi-supervised attributed network embedding with attention-based quantisation. ||| 17669 ||| 1039 ||| 9576 ||| 398 ||| 17670 ||| 17671 ||| 
2020 ||| context-transformer: tackling object confusion for few-shot detection. ||| 4117 ||| 2148 ||| 17672 ||| 2363 ||| 2149 ||| 
2021 ||| improving image captioning by leveraging intra- and inter-layer global representation in transformer network. ||| 17659 ||| 17658 ||| 2504 ||| 17673 ||| 17674 ||| 6831 ||| 820 ||| 2367 ||| 
2021 ||| transformer-style relational reasoning with dynamic memory updating for temporal network modeling. ||| 9734 ||| 17675 ||| 1156 ||| 17676 ||| 1159 ||| 586 ||| 
2021 ||| learning light-weight translation models from deep transformer. ||| 3305 ||| 11355 ||| 17677 ||| 17678 ||| 2333 ||| 17679 ||| 3306 ||| 
2021 ||| modeling the momentum spillover effect for stock prediction via attribute-driven graph attention networks. ||| 16649 ||| 3477 ||| 
2020 ||| decoupled attention network for text recognition. ||| 17680 ||| 17681 ||| 6559 ||| 17418 ||| 17682 ||| 17683 ||| 17684 ||| 17685 ||| 
2018 ||| adaptive co-attention network for named entity recognition in tweets. ||| 336 ||| 9625 ||| 7971 ||| 3273 ||| 
2020 ||| ultrafast video attention prediction with coupled knowledge distillation. ||| 7762 ||| 17686 ||| 17687 ||| 17688 ||| 17689 ||| 2383 ||| 
2020 ||| graph-based transformer with cross-candidate verification for semantic parsing. ||| 17690 ||| 4813 ||| 17691 ||| 17692 ||| 3183 ||| 17693 ||| 
2020 ||| graph attention based proposal 3d convnets for action detection. ||| 17694 ||| 17695 ||| 17696 ||| 17697 ||| 7266 ||| 9576 ||| 
2019 ||| logic attention based neighborhood aggregation for inductive knowledge graph embedding. ||| 17698 ||| 1399 ||| 1400 ||| 2238 ||| 
2020 ||| trimodal attention module for multimodal sentiment analysis (student abstract). ||| 17699 ||| 17700 ||| 
2019 ||| a radical-aware attention-based model for chinese text classification. ||| 17701 ||| 17702 ||| 17703 ||| 17704 ||| 17705 ||| 1302 ||| 
2019 ||| residual attribute attention network for face image super-resolution. ||| 17706 ||| 17707 ||| 6621 ||| 4634 ||| 
2020 ||| weakly-supervised video re-localization with multiscale attention model. ||| 17708 ||| 17709 ||| 17710 ||| 2346 ||| 
2020 ||| learning long- and short-term user literal-preference with multimodal hierarchical transformer network for personalized image caption. ||| 781 ||| 17711 ||| 17712 ||| 8949 ||| 
2019 ||| a dual attention network with semantic embedding for few-shot learning. ||| 17713 ||| 14513 ||| 17714 ||| 
2020 ||| multi-view deep attention network for reinforcement learning (student abstract). ||| 17715 ||| 12235 ||| 5858 ||| 764 ||| 
2021 ||| an efficient transformer decoder with compressed sub-layers. ||| 17716 ||| 17717 ||| 2333 ||| 3306 ||| 
2020 ||| salsac: a video saliency prediction model with shuffled attentions and correlation-based convlstm. ||| 17718 ||| 17719 ||| 17720 ||| 17721 ||| 307 ||| 
2019 ||| cross-relation cross-bag attention for distantly-supervised relation extraction. ||| 17722 ||| 4805 ||| 17723 ||| 17724 ||| 7654 ||| 1937 ||| 2258 ||| 1250 ||| 
2017 ||| coupled multi-layer attentions for co-extraction of aspect and opinion terms. ||| 17725 ||| 2247 ||| 3196 ||| 17726 ||| 
2018 ||| modeling attention and memory for auditory selection in a cocktail party environment. ||| 5350 ||| 17727 ||| 17728 ||| 3674 ||| 728 ||| 
2018 ||| facial landmarks detection by self-iterative regression based landmarks-attention network. ||| 17729 ||| 17730 ||| 17731 ||| 13825 ||| 
2017 ||| deterministic attention for sequence-to-sequence constituent parsing. ||| 17732 ||| 3048 ||| 3784 ||| 880 ||| 4908 ||| 
2017 ||| attention correctness in neural image captioning. ||| 17733 ||| 17734 ||| 17735 ||| 8660 ||| 
2017 ||| text-guided attention model for image captioning. ||| 17736 ||| 17737 ||| 9277 ||| 
2019 ||| cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. ||| 11139 ||| 883 ||| 1373 ||| 1086 ||| 12009 ||| 1087 ||| 
2018 ||| a question-focused multi-factor attention network for question answering. ||| 7327 ||| 3195 ||| 
2019 ||| beyond rnns: positional self-attention with co-attention for video question answering. ||| 17738 ||| 9576 ||| 1039 ||| 17695 ||| 1263 ||| 1063 ||| 2190 ||| 
2021 ||| tackling the infodemic: analysis using transformer based models. ||| 17739 ||| 17740 ||| 
2018 ||| improving neural fine-grained entity typing with knowledge attention. ||| 17741 ||| 3231 ||| 3232 ||| 3233 ||| 
2018 ||| "attention" for detecting unreliable news in the information age. ||| 17742 ||| 
2020 ||| crowd-assisted disaster scene assessment with human-ai interactive attention. ||| 17743 ||| 17744 ||| 1420 ||| 952 ||| 
2021 ||| dast: unsupervised domain adaptation in semantic segmentation based on discriminator attention and self-training. ||| 17745 ||| 17746 ||| 17747 ||| 17748 ||| 5707 ||| 254 ||| 
2019 ||| wais: word attention for joint intent detection and slot filling. ||| 17749 ||| 4520 ||| 
2021 ||| efficient folded attention for medical image reconstruction and segmentation. ||| 17750 ||| 17751 ||| 17752 ||| 17753 ||| 17754 ||| 17755 ||| 9149 ||| 
2019 ||| self-ensembling attention networks: addressing domain shift for semantic segmentation. ||| 17756 ||| 17757 ||| 17758 ||| 2251 ||| 17759 ||| 17760 ||| 
2021 ||| explicitly modeled attention maps for image classification. ||| 17761 ||| 17762 ||| 17763 ||| 13617 ||| 12149 ||| 9163 ||| 
2021 ||| self-supervised attention-aware reinforcement learning. ||| 2428 ||| 17764 ||| 17765 ||| 
2017 ||| attention based lstm for target dependent sentiment classification. ||| 1081 ||| 17766 ||| 17767 ||| 12938 ||| 986 ||| 
2018 ||| gated-attention architectures for task-oriented language grounding. ||| 17768 ||| 17769 ||| 17770 ||| 17771 ||| 3247 ||| 
2019 ||| skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module. ||| 17772 ||| 1340 ||| 17773 ||| 6559 ||| 17774 ||| 
2020 ||| attention-guide walk model in heterogeneous information network for multi-style recommendation explanation. ||| 398 ||| 7830 ||| 17775 ||| 
2018 ||| a neural attention model for urban air quality inference: learning the weights of monitoring stations. ||| 17776 ||| 8920 ||| 11190 ||| 17777 ||| 
2021 ||| a hybrid attention mechanism for weakly-supervised temporal action localization. ||| 17778 ||| 7175 ||| 17779 ||| 
2018 ||| atrank: an attention-based user behavior modeling framework for recommendation. ||| 2482 ||| 17780 ||| 17781 ||| 17782 ||| 17783 ||| 17784 ||| 1423 ||| 
2020 ||| sentence generation for entity description with content-plan attention. ||| 17785 ||| 17786 ||| 3248 ||| 
2019 ||| semantic adversarial network with multi-scale pyramid attention for video classification. ||| 17787 ||| 2306 ||| 1371 ||| 242 ||| 17788 ||| 
2021 ||| show, attend and distill: knowledge distillation via attention-based feature matching. ||| 17789 ||| 2086 ||| 8511 ||| 
2020 ||| generative attention networks for multi-agent behavioral modeling. ||| 17790 ||| 2632 ||| 9335 ||| 2631 ||| 9337 ||| 
2020 ||| gret: global representation enhanced transformer. ||| 17791 ||| 17792 ||| 4845 ||| 17793 ||| 3385 ||| 3471 ||| 4847 ||| 
2019 ||| afs: an attention-based mechanism for supervised feature selection. ||| 17794 ||| 17795 ||| 17796 ||| 
2019 ||| hybrid attention-based prototypical networks for noisy few-shot relation classification. ||| 17797 ||| 17798 ||| 3232 ||| 3233 ||| 
2019 ||| neural speech synthesis with transformer network. ||| 17799 ||| 12389 ||| 14323 ||| 12137 ||| 124 ||| 
2018 ||| lateral inhibition-inspired convolutional neural network for visual attention and saliency detection. ||| 17800 ||| 17801 ||| 11479 ||| 10429 ||| 17802 ||| 17803 ||| 
2018 ||| mention and entity description co-attention for entity disambiguation. ||| 17804 ||| 3618 ||| 17625 ||| 17805 ||| 2238 ||| 
2021 ||| mango: a mask attention guided one-stage scene text spotter. ||| 17806 ||| 3503 ||| 2609 ||| 2611 ||| 17807 ||| 1937 ||| 2258 ||| 
2019 ||| motiontransformer: transferring neural inertial tracking between domains. ||| 17808 ||| 17809 ||| 17810 ||| 17811 ||| 10518 ||| 17812 ||| 17813 ||| 
2020 ||| fine-grained machine teaching with attention modeling. ||| 17814 ||| 17815 ||| 17816 ||| 
2018 ||| disan: directional self-attention network for rnn/cnn-free language understanding. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 799 ||| 4873 ||| 
2021 ||| alp-kd: attention-based layer projection for knowledge distillation. ||| 17817 ||| 17818 ||| 14630 ||| 3443 ||| 
2021 ||| modular graph transformer networks for multi-label image classification. ||| 17819 ||| 17820 ||| 17821 ||| 
2018 ||| rnn-based sequence-preserved attention for dependency parsing. ||| 17822 ||| 17823 ||| 6174 ||| 17824 ||| 17825 ||| 17826 ||| 
2020 ||| neural simile recognition with cyclic multitask learning and local attention. ||| 3043 ||| 3655 ||| 3182 ||| 11745 ||| 17827 ||| 2166 ||| 
2020 ||| attention-over-attention field-aware factorization machine. ||| 17828 ||| 17829 ||| 17830 ||| 577 ||| 17831 ||| 15877 ||| 
2017 ||| distant supervision for relation extraction with sentence-level attention and entity descriptions. ||| 17832 ||| 3129 ||| 3813 ||| 1418 ||| 
2020 ||| compressed self-attention for deep metric learning. ||| 17833 ||| 17834 ||| 15560 ||| 17835 ||| 1558 ||| 17757 ||| 
2017 ||| coherent dialogue with attention-based language models. ||| 17836 ||| 3810 ||| 17837 ||| 
2021 ||| continuous self-attention models with neural ode networks. ||| 875 ||| 989 ||| 17838 ||| 17839 ||| 17840 ||| 
2019 ||| motion guided spatial attention for video captioning. ||| 17841 ||| 17842 ||| 
2019 ||| fully convolutional video captioning with coarse-to-fine and inherited attention. ||| 17843 ||| 9687 ||| 9688 ||| 9689 ||| 17844 ||| 6514 ||| 17845 ||| 
2020 ||| multi-scale self-attention for text classification. ||| 4967 ||| 3272 ||| 4968 ||| 4970 ||| 1770 ||| 
2020 ||| tapnet: multivariate time series classification with attentional prototypical network. ||| 17846 ||| 17847 ||| 17848 ||| 17192 ||| 
2018 ||| hierarchical recurrent attention network for response generation. ||| 17849 ||| 2280 ||| 293 ||| 3118 ||| 3480 ||| 
2021 ||| knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. ||| 1126 ||| 1124 ||| 1125 ||| 2588 ||| 1123 ||| 17850 ||| 17851 ||| 9575 ||| 
2021 ||| confidence-aware non-repetitive multimodal transformers for textcaps. ||| 17852 ||| 17853 ||| 6417 ||| 17854 ||| 
2021 ||| graph-based tri-attention network for answer ranking in cqa. ||| 781 ||| 17855 ||| 6964 ||| 8948 ||| 8949 ||| 8907 ||| 
2021 ||| split then refine: stacked attention-guided resunets for blind single image visible watermark removal. ||| 17856 ||| 17857 ||| 
2019 ||| hierarchical attention network for image captioning. ||| 6317 ||| 6318 ||| 5746 ||| 
2019 ||| a two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels. ||| 17858 ||| 17859 ||| 8710 ||| 8711 ||| 17860 ||| 
2019 ||| deep short text classification with knowledge powered attention. ||| 17861 ||| 17862 ||| 17863 ||| 9635 ||| 17864 ||| 
2020 ||| a new dataset and boundary-attention semantic segmentation for face parsing. ||| 17865 ||| 17866 ||| 10211 ||| 17867 ||| 17868 ||| 2165 ||| 
2018 ||| event detection via gated multilingual attention mechanism. ||| 6307 ||| 3128 ||| 3129 ||| 1418 ||| 
2020 ||| self-attention convlstm for spatiotemporal prediction. ||| 17869 ||| 17870 ||| 17871 ||| 17872 ||| 1280 ||| 
2021 ||| gradient-based localization and spatial attention for confidence measure in fine-grained recognition using deep neural networks. ||| 17873 ||| 2713 ||| 17874 ||| 17875 ||| 17876 ||| 
2019 ||| convolutional spatial attention model for reading comprehension with multiple-choice questions. ||| 3643 ||| 3642 ||| 17877 ||| 1308 ||| 3645 ||| 
2019 ||| character-level language modeling with deeper self-attention. ||| 4824 ||| 17878 ||| 4821 ||| 3789 ||| 8069 ||| 
2021 ||| dynamic multi-context attention networks for citation forecasting of scientific publications. ||| 17189 ||| 17190 ||| 17188 ||| 17191 ||| 8985 ||| 17192 ||| 
2020 ||| an attention-based graph neural network for heterogeneous structural learning. ||| 17879 ||| 10232 ||| 17880 ||| 17881 ||| 17882 ||| 12749 ||| 
2021 ||| stock selection via spatiotemporal hypergraph attention network: a learning to rank approach. ||| 12548 ||| 17883 ||| 17884 ||| 4114 ||| 12550 ||| 
2021 ||| patch-wise attention network for monocular depth estimation. ||| 17885 ||| 17886 ||| 17887 ||| 17888 ||| 17889 ||| 
2021 ||| rarebert: transformer architecture for rare disease patient identification using administrative claims. ||| 17890 ||| 17891 ||| 17892 ||| 17893 ||| 
2018 ||| sentiment lexicon enhanced attention-based lstm for sentiment classification. ||| 3176 ||| 3177 ||| 1081 ||| 
2019 ||| to find where you talk: temporal sentence localization in video with attention based location regression. ||| 17894 ||| 2165 ||| 1088 ||| 
2020 ||| robutrans: a robust transformer-based text-to-speech model. ||| 17799 ||| 14323 ||| 2280 ||| 12389 ||| 12137 ||| 124 ||| 
2020 ||| schema-guided multi-domain dialogue state tracking with graph attention neural networks. ||| 3147 ||| 17895 ||| 4171 ||| 3150 ||| 17896 ||| 3151 ||| 
2020 ||| relation-guided spatial attention and temporal refinement for video-based person re-identification. ||| 17897 ||| 1806 ||| 17898 ||| 1807 ||| 
2020 ||| syntactically look-ahead attention network for sentence compression. ||| 4982 ||| 14011 ||| 
2020 ||| multi-label patent categorization with non-local attention-based graph convolutional network. ||| 17899 ||| 17900 ||| 17901 ||| 17902 ||| 17903 ||| 8918 ||| 
2017 ||| title learning latent subevents in activity videos using temporal attention filters. ||| 8593 ||| 17904 ||| 8594 ||| 
2020 ||| joint entity and relation extraction with a hybrid transformer and reinforcement learning based model. ||| 17905 ||| 17906 ||| 17907 ||| 5346 ||| 17908 ||| 
2021 ||| dynamic graph representation learning for video dialog via multi-modal shuffled transformers. ||| 17909 ||| 2170 ||| 17910 ||| 2507 ||| 11981 ||| 3035 ||| 1848 ||| 7283 ||| 
2021 ||| abusive language detection in heterogeneous contexts: dataset collection and the role of supervised attention. ||| 17911 ||| 17912 ||| 17913 ||| 17914 ||| 17915 ||| 17916 ||| 
2019 ||| sam-net: integrating event-level and chain-level attentions to predict what happens next. ||| 17917 ||| 17918 ||| 9019 ||| 233 ||| 5295 ||| 
2020 ||| cawa: an attention-network for credit attribution. ||| 17919 ||| 15169 ||| 
2019 ||| deliberate attention networks for image captioning. ||| 1039 ||| 17920 ||| 9576 ||| 17695 ||| 9579 ||| 1040 ||| 
2018 ||| syntax-directed attention for neural machine translation. ||| 3112 ||| 3049 ||| 4907 ||| 4908 ||| 880 ||| 
2020 ||| dianet: dense-and-implicit attention network. ||| 17921 ||| 17922 ||| 17923 ||| 17924 ||| 
2020 ||| data-gru: dual-attention time-aware gated recurrent unit for irregular multivariate time series. ||| 17925 ||| 17926 ||| 17927 ||| 17302 ||| 17928 ||| 17929 ||| 17930 ||| 17931 ||| 
2021 ||| encoding syntactic knowledge in transformer encoder for intent detection and slot filling. ||| 17932 ||| 17933 ||| 12484 ||| 16905 ||| 14646 ||| 
2021 ||| attnmove: history enhanced trajectory recovery via attentional network. ||| 17934 ||| 17935 ||| 6738 ||| 9069 ||| 8863 ||| 17936 ||| 2969 ||| 
2021 ||| melodic phrase attention network for symbolic data-based music genre classification (student abstract). ||| 144 ||| 3248 ||| 8980 ||| 
2019 ||| dan: deep attention neural network for news recommendation. ||| 17662 ||| 17663 ||| 17937 ||| 4237 ||| 4191 ||| 
2020 ||| ma-dst: multi-attention-based scalable dialog state tracking. ||| 17938 ||| 17939 ||| 17940 ||| 17941 ||| 17942 ||| 59 ||| 
2020 ||| bidirectional dilated lstm with attention for fine-grained emotion classification in tweets. ||| 13352 ||| 13354 ||| 531 ||| 
2020 ||| graph transformer for graph-to-sequence learning. ||| 1115 ||| 3015 ||| 
2018 ||| word attention for sequence to sequence text understanding. ||| 4785 ||| 7051 ||| 17104 ||| 6528 ||| 4791 ||| 
2021 ||| attanet: attention-augmented network for fast and accurate scene parsing. ||| 14552 ||| 17943 ||| 1858 ||| 
2020 ||| age progression and regression with spatial attention modules. ||| 6721 ||| 17944 ||| 17945 ||| 
2020 ||| multiple positional self-attention network for text classification. ||| 17946 ||| 17947 ||| 17948 ||| 
2021 ||| global fusion attention for vision and language understanding (student abstract). ||| 17949 ||| 17950 ||| 2492 ||| 1281 ||| 
2019 ||| exploring answer stance detection with recurrent conditional attention. ||| 17951 ||| 6314 ||| 17952 ||| 3310 ||| 
2020 ||| cross-modal attention network for temporal inconsistent audio-visual event localization. ||| 12386 ||| 758 ||| 9996 ||| 1825 ||| 127 ||| 
2021 ||| understood in translation: transformers for domain understanding. ||| 17953 ||| 17954 ||| 17955 ||| 17956 ||| 
2021 ||| tdaf: top-down attention framework for vision tasks. ||| 4900 ||| 17957 ||| 17958 ||| 17959 ||| 17960 ||| 5136 ||| 
2020 ||| loss-based attention for deep multiple instance learning. ||| 17961 ||| 17962 ||| 17963 ||| 17964 ||| 9837 ||| 17965 ||| 
2020 ||| sequential recommendation with relation-aware kernelized self-attention. ||| 17789 ||| 17966 ||| 17967 ||| 17968 ||| 17969 ||| 
2018 ||| multi-attention recurrent network for human communication comprehension. ||| 4948 ||| 3599 ||| 892 ||| 17970 ||| 893 ||| 3601 ||| 
2020 ||| tanet: robust 3d object detection from point clouds with triple attention. ||| 6474 ||| 17645 ||| 17971 ||| 17972 ||| 11689 ||| 17429 ||| 
2020 ||| generating diverse translation by manipulating multi-head attention. ||| 17973 ||| 4845 ||| 17792 ||| 4790 ||| 4847 ||| 
2020 ||| multi-type self-attention guided degraded saliency detection. ||| 17974 ||| 369 ||| 1700 ||| 307 ||| 17589 ||| 
2020 ||| pyramid constrained self-attention network for fast video salient object detection. ||| 17975 ||| 17976 ||| 2012 ||| 4477 ||| 1904 ||| 1977 ||| 
2021 ||| knowledge-aware dialogue generation with hybrid attention (student abstract). ||| 17977 ||| 17978 ||| 17979 ||| 
2021 ||| future-guided incremental transformer for simultaneous translation. ||| 11809 ||| 3076 ||| 3635 ||| 
2020 ||| multi-agent actor-critic with hierarchical graph attention network. ||| 17980 ||| 17981 ||| 17982 ||| 
2021 ||| named entity recognition from synthesis procedural text in materials science domain with attention-based approach. ||| 17983 ||| 17984 ||| 
2020 ||| atloc: attention guided camera localization. ||| 1780 ||| 17808 ||| 17810 ||| 17985 ||| 17813 ||| 17812 ||| 
2021 ||| contrastive triple extraction with generative transformer. ||| 17986 ||| 17987 ||| 17988 ||| 17989 ||| 12432 ||| 9686 ||| 17990 ||| 
2021 ||| two-stream convolution augmented transformer for human activity recognition. ||| 8838 ||| 17991 ||| 1160 ||| 17992 ||| 17993 ||| 16597 ||| 
2021 ||| object relation attention for image paragraph captioning. ||| 17994 ||| 17995 ||| 17996 ||| 
2019 ||| attention based spatial-temporal graph convolutional networks for traffic flow forecasting. ||| 17997 ||| 17998 ||| 17999 ||| 183 ||| 18000 ||| 
2018 ||| recurrent attentional reinforcement learning for multi-label image recognition. ||| 2313 ||| 2312 ||| 1800 ||| 2315 ||| 
2021 ||| attributes-guided and pure-visual attention alignment for few-shot recognition. ||| 1161 ||| 1254 ||| 18001 ||| 1162 ||| 
2020 ||| domain adaptive attention learning for unsupervised person re-identification. ||| 18002 ||| 18003 ||| 14114 ||| 14115 ||| 11269 ||| 
2019 ||| attention guided imitation learning and reinforcement learning. ||| 8824 ||| 
2021 ||| informer: beyond efficient transformer for long sequence time-series forecasting. ||| 18004 ||| 18005 ||| 18006 ||| 3364 ||| 215 ||| 9592 ||| 18007 ||| 
2021 ||| relation-aware graph attention model with adaptive self-adversarial training. ||| 5289 ||| 18008 ||| 18009 ||| 18010 ||| 
2020 ||| not all attention is needed: gated attention network for sequence data. ||| 3589 ||| 15072 ||| 3590 ||| 
2019 ||| tied transformers: neural machine translation with shared encoder and decoder. ||| 4787 ||| 18011 ||| 14507 ||| 7051 ||| 18012 ||| 4789 ||| 
2020 ||| re-attention for visual question answering. ||| 18013 ||| 4646 ||| 18014 ||| 1834 ||| 18015 ||| 15243 ||| 
2018 ||| multimodal keyless attention fusion for video classification. ||| 18016 ||| 2190 ||| 3774 ||| 2530 ||| 2191 ||| 136 ||| 2531 ||| 
2021 ||| a supervised multi-head self-attention network for nested named entity recognition. ||| 716 ||| 688 ||| 717 ||| 689 ||| 
2020 ||| tanda: transfer and adapt pre-trained transformer models for answer sentence selection. ||| 18017 ||| 9680 ||| 3374 ||| 
2021 ||| audio-oriented multimodal machine comprehension via dynamic inter- and intra-modality attention. ||| 1229 ||| 3746 ||| 3748 ||| 3749 ||| 12636 ||| 7417 ||| 4430 ||| 
2021 ||| exploring text-transformers in aaai 2021 shared task: covid-19 fake news detection in english. ||| 18018 ||| 18019 ||| 18016 ||| 6679 ||| 11660 ||| 
2021 ||| gate: graph attention transformer encoder for cross-lingual relation and event extraction. ||| 3238 ||| 18020 ||| 3033 ||| 
2021 ||| uag: uncertainty-aware attention graph neural network for defending adversarial attacks. ||| 18021 ||| 18022 ||| 18023 ||| 
2017 ||| recurrent attentional topic model. ||| 18024 ||| 9472 ||| 2238 ||| 18025 ||| 11466 ||| 
2020 ||| transformer-capsule model for intent detection (student abstract). ||| 18026 ||| 18027 ||| 
2021 ||| m-based algorithm for approximating self-attention. ||| 18028 ||| 18029 ||| 18030 ||| 18031 ||| 18032 ||| 8766 ||| 18033 ||| 
2021 ||| hargan: heterogeneous argument attention network for persuasiveness prediction. ||| 18034 ||| 18035 ||| 18036 ||| 
2021 ||| let: linguistic knowledge enhanced graph transformer for chinese short text matching. ||| 18037 ||| 3147 ||| 3150 ||| 3151 ||| 
2021 ||| implicit kernel attention. ||| 17967 ||| 18038 ||| 18039 ||| 17969 ||| 
2020 ||| attention-based multi-modal fusion network for semantic scene completion. ||| 18040 ||| 18041 ||| 18042 ||| 18043 ||| 820 ||| 
2021 ||| segatron: segment-aware transformer for language modeling and understanding. ||| 18044 ||| 4839 ||| 3009 ||| 18045 ||| 9654 ||| 18046 ||| 7204 ||| 765 ||| 
2019 ||| difficulty-aware attention network with confidence learning for medical image segmentation. ||| 18047 ||| 1052 ||| 18048 ||| 18049 ||| 18050 ||| 18051 ||| 
2021 ||| salnet: semi-supervised few-shot text classification with attention-based lexicon construction. ||| 18052 ||| 18053 ||| 18054 ||| 
2021 ||| co-gat: a co-interactive graph attention network for joint dialog act recognition and sentiment classification. ||| 12348 ||| 18055 ||| 3707 ||| 18056 ||| 3311 ||| 
2021 ||| systems at sdu-2021 task 1: transformers for sentence level sequence label. ||| 4398 ||| 18057 ||| 18058 ||| 18059 ||| 6167 ||| 7627 ||| 9624 ||| 
2019 ||| deep metric learning by online soft mining and class-aware attention. ||| 18060 ||| 18061 ||| 18062 ||| 18063 ||| 18064 ||| 
2021 ||| regional attention with architecture-rebuilt 3d network for rgb-d gesture recognition. ||| 18065 ||| 13441 ||| 13439 ||| 
2021 ||| the heads hypothesis: a unifying statistical approach towards understanding multi-headed attention in bert. ||| 18066 ||| 18067 ||| 3327 ||| 11768 ||| 3328 ||| 
2020 ||| high tissue contrast mri synthesis using multi-stage attention-gan for segmentation. ||| 18068 ||| 6582 ||| 5476 ||| 
2021 ||| attention-based multi-level fusion network for light field depth estimation. ||| 18069 ||| 1315 ||| 17998 ||| 
2020 ||| filtration and distillation: enhancing region attention for fine-grained visual categorization. ||| 18070 ||| 18071 ||| 8710 ||| 18072 ||| 18073 ||| 17860 ||| 
2021 ||| context-aware attentional pooling (cap) for fine-grained visual classification. ||| 6383 ||| 6384 ||| 18074 ||| 18075 ||| 
2018 ||| dual attention network for product compatibility and function satisfiability analysis. ||| 17130 ||| 9989 ||| 17131 ||| 1094 ||| 
2019 ||| vistanet: visual aspect attention network for multimodal sentiment analysis. ||| 18076 ||| 18077 ||| 
2021 ||| tune-in: training under negative environments with interference for attention networks simulating cocktail party effect. ||| 1224 ||| 18078 ||| 4530 ||| 3808 ||| 
2020 ||| can eruptions be predicted? short-term prediction of volcanic eruptions via attention-based long short-term memory. ||| 18079 ||| 18080 ||| 18081 ||| 
2021 ||| dynamic memory based attention network for sequential recommendation. ||| 18082 ||| 18083 ||| 18084 ||| 18085 ||| 8916 ||| 1245 ||| 18086 ||| 
2021 ||| multi-document transformer for personality detection. ||| 18087 ||| 3101 ||| 3100 ||| 18088 ||| 
2017 ||| image caption with global-local attention. ||| 5449 ||| 18089 ||| 18090 ||| 17860 ||| 2398 ||| 
2020 ||| path ranking with attention to type hierarchies. ||| 18091 ||| 18092 ||| 18093 ||| 18094 ||| 
2021 ||| revisiting mahalanobis distance for transformer-based out-of-domain detection. ||| 18095 ||| 18096 ||| 18097 ||| 10339 ||| 18098 ||| 
2021 ||| over-map: structural attention mechanism and automated semantic segmentation ensembled for uncertainty prediction. ||| 17873 ||| 2713 ||| 17874 ||| 17875 ||| 17876 ||| 
2020 ||| pose-guided multi-granularity attention network for text-based person search. ||| 18099 ||| 18100 ||| 18101 ||| 1160 ||| 10429 ||| 17803 ||| 
2018 ||| hierarchical attention transfer network for cross-domain sentiment classification. ||| 6679 ||| 9191 ||| 9472 ||| 11145 ||| 
2020 ||| learning the graphical structure of electronic health records with graph convolutional transformer. ||| 18102 ||| 3212 ||| 231 ||| 18103 ||| 18104 ||| 18105 ||| 18106 ||| 
2020 ||| adversarial cross-domain action recognition with co-attention. ||| 18107 ||| 18108 ||| 18050 ||| 7431 ||| 
2021 ||| classification by attention: scene graph classification with prior knowledge. ||| 18109 ||| 18110 ||| 15822 ||| 
2017 ||| localizing by describing: attribute-guided attention localization for fine-grained recognition. ||| 2530 ||| 18111 ||| 2531 ||| 1761 ||| 18112 ||| 
2021 ||| on scalar embedding of relative positions in attention models. ||| 18113 ||| 18114 ||| 18115 ||| 18116 ||| 
2021 ||| regularizing attention networks for anomaly detection in visual question answering. ||| 18117 ||| 18118 ||| 18119 ||| 
2020 ||| who did they respond to? conversation structure modeling using masked hierarchical transformer. ||| 18120 ||| 18121 ||| 18122 ||| 18123 ||| 3251 ||| 
2020 ||| hierarchical attention network with pairwise loss for chinese zero pronoun resolution. ||| 18124 ||| 5395 ||| 
2019 ||| dynamic capsule attention for visual question answering. ||| 2501 ||| 2367 ||| 3182 ||| 2504 ||| 18125 ||| 
2020 ||| real-time emotion recognition via attention gated hierarchical memory network. ||| 18126 ||| 597 ||| 598 ||| 
2020 ||| guiding attention in sequence-to-sequence models for dialogue act prediction. ||| 18127 ||| 18128 ||| 17954 ||| 18129 ||| 18130 ||| 9772 ||| 9773 ||| 
2019 ||| interactive attention transfer network for cross-domain sentiment classification. ||| 3433 ||| 18131 ||| 1302 ||| 17703 ||| 18132 ||| 1301 ||| 
2021 ||| dynamic modeling cross- and self-lattice attention network for chinese ner. ||| 18133 ||| 970 ||| 18134 ||| 18135 ||| 5743 ||| 
2019 ||| hirenet: a hierarchical attention model for the automatic analysis of asynchronous video job interviews. ||| 2713 ||| 18136 ||| 18137 ||| 18138 ||| 18139 ||| 9772 ||| 9773 ||| 
2021 ||| humor knowledge enriched transformer for understanding multimodal humor. ||| 3637 ||| 3638 ||| 3636 ||| 4948 ||| 18140 ||| 3601 ||| 5731 ||| 
2021 ||| lightxml: transformer with dynamic negative sampling for high-performance extreme multi-label text classification. ||| 18141 ||| 18142 ||| 18143 ||| 18144 ||| 18145 ||| 3476 ||| 
2020 ||| shallow feature based dense attention network for crowd counting. ||| 18146 ||| 10066 ||| 10065 ||| 2278 ||| 
2020 ||| understanding medical conversations with scattered keyword attention and weak supervision from responses. ||| 18147 ||| 5746 ||| 18148 ||| 18149 ||| 18150 ||| 3311 ||| 1265 ||| 
2020 ||| gman: a graph multi-attention network for traffic prediction. ||| 18151 ||| 18152 ||| 3691 ||| 17786 ||| 
2021 ||| dual sparse attention network for session-based recommendation. ||| 11133 ||| 18153 ||| 18154 ||| 11136 ||| 3504 ||| 
2018 ||| deep semantic role labeling with self-attention. ||| 11746 ||| 3428 ||| 11745 ||| 18155 ||| 18156 ||| 
2020 ||| symbiotic attention with privileged information for egocentric action recognition. ||| 18157 ||| 2280 ||| 1974 ||| 208 ||| 
2021 ||| faster depth-adaptive transformers. ||| 18158 ||| 3075 ||| 1921 ||| 18159 ||| 18160 ||| 
2020 ||| natural image matting via guided contextual attention. ||| 5276 ||| 5277 ||| 
2021 ||| hot-vae: learning high-order label correlation for multi-label classification via attention-based variational autoencoders. ||| 2815 ||| 18161 ||| 18162 ||| 18163 ||| 18164 ||| 
2021 ||| paragraph-level commonsense transformers with recurrent memory. ||| 18165 ||| 3350 ||| 18166 ||| 18167 ||| 18168 ||| 3355 ||| 
2018 ||| learning attention model from human for visuomotor tasks. ||| 8826 ||| 8824 ||| 8825 ||| 8829 ||| 8830 ||| 
2021 ||| gta: graph truncated attention for retrosynthesis. ||| 18169 ||| 18170 ||| 18171 ||| 18172 ||| 18173 ||| 18174 ||| 9317 ||| 9316 ||| 
2020 ||| self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction. ||| 438 ||| 802 ||| 4871 ||| 4872 ||| 771 ||| 801 ||| 800 ||| 
2020 ||| spatio-temporal attention-based neural network for credit card fraud detection. ||| 11150 ||| 18175 ||| 18176 ||| 18177 ||| 11151 ||| 5088 ||| 
2019 ||| multi-task learning with multi-view attention for answer selection and knowledge base question answering. ||| 18178 ||| 18179 ||| 1080 ||| 1081 ||| 18180 ||| 7417 ||| 18181 ||| 1082 ||| 
2020 ||| fact: fused attention for clothing transfer with generative adversarial networks. ||| 18182 ||| 3034 ||| 18183 ||| 18184 ||| 8802 ||| 
2021 ||| structured co-reference graph attention for video-grounded dialogue. ||| 11230 ||| 18185 ||| 18186 ||| 11231 ||| 
2018 ||| order-free rnn with visual attention for multi-label classification. ||| 11324 ||| 18187 ||| 18188 ||| 6328 ||| 
2021 ||| efficient license plate recognition via holistic position attention. ||| 18189 ||| 11479 ||| 18190 ||| 
2019 ||| learning a key-value memory co-attention matching network for person re-identification. ||| 18191 ||| 2259 ||| 17724 ||| 
2020 ||| cross-modality attention with semantic graph embedding for multi-label classification. ||| 18192 ||| 18193 ||| 9837 ||| 18016 ||| 18194 ||| 2531 ||| 
2020 ||| an end-to-end visual-audio attention network for emotion recognition in user-generated videos. ||| 1831 ||| 18195 ||| 13464 ||| 1834 ||| 18196 ||| 18197 ||| 18198 ||| 18199 ||| 2596 ||| 
2020 ||| motif-matching based subgraph-level attentional convolutional network for graph classification. ||| 9407 ||| 215 ||| 18200 ||| 18201 ||| 18202 ||| 9988 ||| 
2021 ||| effective ensembling of transformer based language models for acronyms identification. ||| 18203 ||| 18204 ||| 
2020 ||| learning signed network embedding via graph attention. ||| 11641 ||| 5904 ||| 538 ||| 3410 ||| 
2020 ||| multi-agent game abstraction via graph attention neural network. ||| 4297 ||| 18205 ||| 18206 ||| 16800 ||| 18207 ||| 5089 ||| 
2021 ||| task adaptive pretraining of transformers for hostility detection. ||| 10508 ||| 15035 ||| 15036 ||| 15037 ||| 1185 ||| 
2020 ||| treegen: a tree-based transformer architecture for code generation. ||| 18208 ||| 18209 ||| 18210 ||| 18211 ||| 1389 ||| 2037 ||| 
2020 ||| multi-level head-wise match and aggregation in transformer for textual sequence matching. ||| 3363 ||| 18212 ||| 1398 ||| 800 ||| 2046 ||| 
2018 ||| attention-based transactional context embedding for next-item recommendation. ||| 18213 ||| 1066 ||| 18214 ||| 18215 ||| 8912 ||| 683 ||| 
2021 ||| noninvasive self-attention for side information fusion in sequential recommendation. ||| 748 ||| 13455 ||| 13454 ||| 13456 ||| 6053 ||| 13458 ||| 
2020 ||| two-level transformer and auxiliary coherence modeling for improved text segmentation. ||| 15105 ||| 18216 ||| 
2020 ||| channel attention is all you need for video frame interpolation. ||| 18217 ||| 18218 ||| 9277 ||| 7957 ||| 18219 ||| 
2019 ||| hierarchical attention networks for sentence ordering. ||| 3120 ||| 3121 ||| 
2020 ||| type-aware anchor link prediction across heterogeneous networks based on graph attention network. ||| 4188 ||| 4189 ||| 987 ||| 8840 ||| 4237 ||| 4190 ||| 
2021 ||| continuous-time attention for sequential learning. ||| 1488 ||| 12662 ||| 
2019 ||| attention-based multi-context guiding for few-shot semantic segmentation. ||| 17729 ||| 18220 ||| 18221 ||| 18222 ||| 8016 ||| 18223 ||| 
2020 ||| distraction-aware feature learning for human attribute recognition via coarse-to-fine attention mechanism. ||| 18224 ||| 5704 ||| 8972 ||| 5705 ||| 
2018 ||| a cascaded inception of inception network with attention modulated feature fusion for human pose estimation. ||| 18225 ||| 1037 ||| 130 ||| 18226 ||| 18227 ||| 18228 ||| 
2019 ||| point2sequence: learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. ||| 18229 ||| 2563 ||| 2559 ||| 18230 ||| 
2020 ||| co-attention hierarchical network: generating coherent long distractors for reading comprehension. ||| 18231 ||| 18232 ||| 18233 ||| 
2019 ||| an affect-rich neural conversational model with biased attention and weighted cross-entropy loss. ||| 738 ||| 15790 ||| 16696 ||| 
2021 ||| self-attention attribution: interpreting information interactions inside transformer. ||| 18234 ||| 3171 ||| 3174 ||| 3617 ||| 
2021 ||| kan: knowledge-aware attention network for fake news detection. ||| 18235 ||| 18236 ||| 2230 ||| 18237 ||| 15243 ||| 
2020 ||| attention-based view selection networks for light-field disparity estimation. ||| 18238 ||| 18239 ||| 18240 ||| 2345 ||| 
2019 ||| gaussian transformer: a lightweight approach for natural language inference. ||| 18241 ||| 9472 ||| 3311 ||| 
2020 ||| attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems. ||| 11994 ||| 10650 ||| 10652 ||| 3676 ||| 10654 ||| 
2017 ||| multi-focus attention network for efficient deep reinforcement learning. ||| 18242 ||| 18243 ||| 8580 ||| 
2020 ||| aateam: achieving the ad hoc teamwork by employing the attention mechanism. ||| 9996 ||| 18244 ||| 18245 ||| 1134 ||| 
2022 ||| comparing vision transformers and convolutional nets for safety critical systems. ||| 18246 ||| 18247 ||| 
2020 ||| relational graph neural network with hierarchical attention for knowledge graph completion. ||| 5543 ||| 3476 ||| 18132 ||| 18248 ||| 9592 ||| 8880 ||| 
2020 ||| ffa-net: feature fusion attention network for single image dehazing. ||| 18249 ||| 18250 ||| 18251 ||| 11405 ||| 11404 ||| 
2020 ||| attention based data hiding with generative adversarial networks. ||| 5251 ||| 
2018 ||| chinese liwc lexicon expansion via hierarchical classification of word embeddings with sememe attention. ||| 18252 ||| 1372 ||| 18253 ||| 3232 ||| 3233 ||| 
2018 ||| attention-via-attention neural machine translation. ||| 18254 ||| 3173 ||| 
2018 ||| attend and diagnose: clinical time series analysis using attention models. ||| 14421 ||| 14878 ||| 12005 ||| 12006 ||| 
2020 ||| predicting students' attention level with interpretable facial and head dynamic features in an online tutoring system (student abstract). ||| 18255 ||| 18256 ||| 18257 ||| 18258 ||| 
2019 ||| attention-aware sampling via deep reinforcement learning for action recognition. ||| 18259 ||| 18260 ||| 17803 ||| 
2020 ||| divide and conquer: question-guided spatio-temporal contextual attention for video question answering. ||| 18261 ||| 18262 ||| 18263 ||| 18043 ||| 820 ||| 
2018 ||| neural knowledge acquisition via mutual attention between knowledge graph and text. ||| 17798 ||| 3232 ||| 3233 ||| 
2018 ||| an unsupervised model with attention autoencoders for question retrieval. ||| 18264 ||| 18233 ||| 
2021 ||| multi-decoder attention model with embedding glimpse for solving vehicle routing problems. ||| 18265 ||| 18266 ||| 18245 ||| 1134 ||| 
2020 ||| explanation vs attention: a two-player game to obtain attention for vqa. ||| 7149 ||| 18267 ||| 18268 ||| 
2021 ||| compound word transformer: learning to compose full-song music over dynamic directed hypergraphs. ||| 11919 ||| 18269 ||| 18270 ||| 4374 ||| 
2020 ||| why attention? analyze bilstm deficiency and its remedies in the case of ner. ||| 18271 ||| 18272 ||| 18273 ||| 
2020 ||| convolutional hierarchical attention network for query-focused video summarization. ||| 18274 ||| 1306 ||| 18275 ||| 18276 ||| 1081 ||| 
2017 ||| an end-to-end spatio-temporal attention model for human action recognition from skeleton data. ||| 18277 ||| 18278 ||| 11269 ||| 7912 ||| 18279 ||| 
2020 ||| an attentional recurrent neural network for personalized next location recommendation. ||| 18280 ||| 1133 ||| 1134 ||| 18281 ||| 
2018 ||| exploring human-like attention supervision in visual question answering. ||| 18282 ||| 12376 ||| 18283 ||| 
2018 ||| improving review representations with user attention and product attention for sentiment classification. ||| 4784 ||| 18284 ||| 18285 ||| 4845 ||| 4847 ||| 
2020 ||| alignment-enhanced transformer for constraining nmt with pre-specified translations. ||| 18286 ||| 11361 ||| 17793 ||| 3289 ||| 84 ||| 3471 ||| 3468 ||| 1254 ||| 
2020 ||| message passing attention networks for document understanding. ||| 18287 ||| 18288 ||| 18289 ||| 
2020 ||| pyramid attention aggregation network for semantic segmentation of surgical instruments. ||| 5183 ||| 5184 ||| 18290 ||| 5185 ||| 271 ||| 18291 ||| 5186 ||| 
2021 ||| symbolic music generation with transformer-gans. ||| 18292 ||| 13824 ||| 18293 ||| 18294 ||| 18295 ||| 18296 ||| 18297 ||| 3068 ||| 18298 ||| 
2021 ||| transformer-based language model fine-tuning methods for covid-19 fake news detection. ||| 18299 ||| 12719 ||| 18300 ||| 18301 ||| 11142 ||| 18302 ||| 11143 ||| 6931 ||| 
2021 ||| mau-net: multiple attention 3d u-net for lung cancer segmentation on ct images. ||| 5110 ||| 18303 ||| 18304 ||| 5858 ||| 18305 ||| 
2021 ||| improving user attention to chatbots through a controlled intensity of changes within the interface. ||| 18306 ||| 18307 ||| 18308 ||| 
2020 ||| towards expert gaze modeling and recognition of a user's attention in realtime. ||| 18309 ||| 18310 ||| 18311 ||| 18312 ||| 18313 ||| 18314 ||| 7218 ||| 
2021 ||| a single-run recognition of nested named entities with transformers. ||| 18315 ||| 18316 ||| 
2020 ||| attracting user attention to visual elements within website with the use of fitts's law and flickering effect. ||| 18307 ||| 18308 ||| 18317 ||| 18148 ||| 18318 ||| 18319 ||| 
2021 ||| attention-based network for effective action recognition from multi-view video. ||| 18320 ||| 18321 ||| 
2021 ||| pseudo-labeling with transformers for improving question answering systems. ||| 18322 ||| 18323 ||| 
2021 ||| semi-supervised anomaly detection in business process event data using self-attention based classification. ||| 18324 ||| 18325 ||| 
2017 ||| mitigation of residual flux for high-temperature superconductor (hts) transformer by controlled switching of hts breaker arc model. ||| 18326 ||| 16192 ||| 18327 ||| 18328 ||| 
2021 ||| modular high-frequency high-power transformers for offshore wind turbines. ||| 18329 ||| 18330 ||| 18331 ||| 18332 ||| 
2021 ||| sizing transformer considering transformer thermal limits and wind farm wake effect. ||| 18333 ||| 18334 ||| 18335 ||| 18336 ||| 18337 ||| 
2021 ||| assessment of effect of winding geometry on thermal performance of retrofilled transformers. ||| 18338 ||| 18339 ||| 
2021 ||| integration of solid-state transformer of off-shore wind turbine systems. ||| 18330 ||| 18329 ||| 18331 ||| 
2021 ||| vibration profile comparison of grid connected and battery-grid connected transformers. ||| 18340 ||| 18341 ||| 18342 ||| 
2021 ||| sfra based deterioration index for transformer condition monitoring. ||| 18343 ||| 18344 ||| 18345 ||| 18346 ||| 18347 ||| 
2021 ||| study on down-sizing inverter transformers in solar farms. ||| 18348 ||| 
2017 ||| an investigation into improving the measurement of the water content of transformer electrical insulation. ||| 18349 ||| 18350 ||| 18351 ||| 18352 ||| 18353 ||| 18354 ||| 
2017 ||| solid state transformer control aspects for various smart grid scenarios. ||| 16168 ||| 18355 ||| 16171 ||| 
2021 ||| transformer through fault protection - challenges and improvements in asset monitoring for precise predictive maintenance. ||| 18356 ||| 18357 ||| 18358 ||| 
2021 ||| study of unbalance reduction in 25kv ac traction system by different transformer configurations. ||| 18359 ||| 18360 ||| 18361 ||| 
2017 ||| solid state transformer parallel operation with a tap changing line frequency transformer. ||| 18362 ||| 18363 ||| 18364 ||| 
2021 ||| impact of battery energy storage system fed super grid transformer on distance protection. ||| 18365 ||| 18366 ||| 
2018 ||| attention patterns for code animations: using eye trackers to evaluate dynamic code presentation techniques. ||| 18367 ||| 18368 ||| 18369 ||| 
2020 ||| aspect level sentiment classification with unbiased attention and target enhanced representations. ||| 10572 ||| 759 ||| 16421 ||| 16422 ||| 16424 ||| 16423 ||| 
2021 ||| explaining a neural attention model for aspect-based sentiment classification using diagnostic classification. ||| 18370 ||| 14072 ||| 18371 ||| 
2020 ||| attention history-based attention for abstractive text summarization. ||| 18372 ||| 3866 ||| 3868 ||| 
2019 ||| aldona: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalised domain ontology and a neural attention model. ||| 18373 ||| 14072 ||| 
2021 ||| pay attention to the cough: early diagnosis of covid-19 using interpretable symptoms embeddings with cough sound signal processing. ||| 18374 ||| 18375 ||| 
2020 ||| de novo drug design using self attention mechanism. ||| 18376 ||| 18377 ||| 
2019 ||| amv-lstm: an attention-based model with multiple positional text matching. ||| 18 ||| 18378 ||| 18379 ||| 18380 ||| 
2018 ||| affectiveroad system and database to assess driver's attention. ||| 18381 ||| 18382 ||| 18383 ||| 18384 ||| 5335 ||| 18385 ||| 18386 ||| 
2021 ||| adela: attention based deep ensemble learning for activity recognition in smart collaborative environments. ||| 11062 ||| 11063 ||| 
2021 ||| attention-based stress detection exploiting non-contact monitoring of movement patterns with ir-uwb radar. ||| 18387 ||| 18388 ||| 18389 ||| 18390 ||| 18391 ||| 18392 ||| 
2017 ||| cogvis: attention-driven cognitive architecture for visual change detection. ||| 18393 ||| 18394 ||| 18395 ||| 18396 ||| 18397 ||| 18398 ||| 18399 ||| 18400 ||| 
2019 ||| library and information science papers discussed on twitter: a new network-based approach for measuring public attention. ||| 18401 ||| 18402 ||| 18403 ||| 
2019 ||| social media attention of the esi highly cited papers: an altmetrics-based overview. ||| 852 ||| 18404 ||| 18405 ||| 18406 ||| 18407 ||| 18408 ||| 18409 ||| 4252 ||| 18410 ||| 
2019 ||| online attention of scholarly papers on psychosocial hazards - job stress, bullying and burnout. ||| 18411 ||| 18412 ||| 
2019 ||| the impact of preprints in library and information science: citations, usage, and social attention. ||| 18413 ||| 18414 ||| 18415 ||| 18416 ||| 
2017 ||| why do some research articles receive more online attention? reasons for online success as measured with altmetrics. ||| 18417 ||| 18418 ||| 
2019 ||| altmetrics - on the way to the "economy of attention"? feasibility study altmetrics for the german ministry of science and research (bmbf). ||| 18419 ||| 
2019 ||| specialized user attention on twitter: identifying scientific fields of interest among social users of science. ||| 18420 ||| 18421 ||| 
2019 ||| unsupervised keyphrase extraction in academic publications using human attention. ||| 3191 ||| 3192 ||| 
2021 ||| event attention network for stock trend prediction. ||| 18422 ||| 18423 ||| 18424 ||| 18425 ||| 
2021 ||| deepqsc: a gnn and attention mechanism-based framework for qos-aware service composition. ||| 18426 ||| 8802 ||| 18427 ||| 18428 ||| 11634 ||| 18429 ||| 18430 ||| 
2021 ||| pyramid dilated attention network for action segmentation. ||| 18431 ||| 18432 ||| 18433 ||| 13930 ||| 
2021 ||| a cybertwin-driven task offloading scheme based on deep reinforcement learning and graph attention networks. ||| 18434 ||| 18435 ||| 
2021 ||| dual attention fusion network for single image dehazing. ||| 6053 ||| 18436 ||| 18437 ||| 
2021 ||| an attention-based bidirectional gated recurrent unit network for location prediction. ||| 4979 ||| 13659 ||| 18438 ||| 18439 ||| 15882 ||| 15881 ||| 
2021 ||| expression recognition based on attention mechanism and length feature of facial landmark. ||| 9141 ||| 968 ||| 18440 ||| 
2020 ||| channel estimation method based on transformer in high dynamic environment. ||| 18441 ||| 18442 ||| 18443 ||| 
2021 ||| capsule network based on self-attention mechanism. ||| 18444 ||| 7957 ||| 18445 ||| 18446 ||| 
2021 ||| cran: an hybrid cnn-rnn attention-based model for arabic machine translation. ||| 18447 ||| 18448 ||| 18449 ||| 18450 ||| 
2020 ||| simulation design of power electronic transformer with dual-pwm. ||| 18451 ||| 18452 ||| 18453 ||| 18454 ||| 
2019 ||| attention-based hybrid model for automatic short answer scoring. ||| 18455 ||| 7400 ||| 18456 ||| 18457 ||| 18458 ||| 
2020 ||| attention aware deep learning object detection and simulation. ||| 18459 ||| 18460 ||| 18461 ||| 18462 ||| 
2020 ||| algorithm for double-layer structure multi-label classification with optimal sequence based on attention mechanism. ||| 18463 ||| 18464 ||| 
2021 ||| semi-supervised graph attention networks for event representation learning. ||| 1994 ||| 18465 ||| 18466 ||| 
2017 ||| dataset construction via attention for aspect term extraction with distant supervision. ||| 6975 ||| 18467 ||| 6979 ||| 6977 ||| 6978 ||| 
2019 ||| spatiotemporal attention networks for wind power forecasting. ||| 18468 ||| 6810 ||| 18469 ||| 18470 ||| 18471 ||| 
2019 ||| temporal self-attention network for medical concept embedding. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 2730 ||| 
2019 ||| an augmented transformer architecture for natural language generation tasks. ||| 18472 ||| 18473 ||| 1305 ||| 18474 ||| 18475 ||| 18476 ||| 
2021 ||| sting: self-attention based time-series imputation networks using gan. ||| 18477 ||| 11462 ||| 18478 ||| 18479 ||| 
2018 ||| next point-of-interest recommendation with temporal and multi-level context attention. ||| 18480 ||| 8920 ||| 11190 ||| 
2017 ||| multi-level multiple attentions for contextual multimodal sentiment analysis. ||| 892 ||| 893 ||| 18481 ||| 3658 ||| 4948 ||| 3601 ||| 
2019 ||| dynamic news recommendation with hierarchical attention network. ||| 4600 ||| 9645 ||| 777 ||| 
2020 ||| multi-attention 3d residual neural network for origin-destination crowd flow prediction. ||| 18482 ||| 15209 ||| 18483 ||| 18484 ||| 18485 ||| 
2018 ||| astm: an attentional segmentation based topic model for short texts. ||| 18486 ||| 9227 ||| 18487 ||| 15166 ||| 
2021 ||| deep reinforced attention regression for partial sketch based image retrieval. ||| 18488 ||| 18489 ||| 18490 ||| 18491 ||| 
2021 ||| attention-based feature interaction for efficient online knowledge distillation. ||| 18492 ||| 18493 ||| 14692 ||| 18494 ||| 8608 ||| 18495 ||| 
2020 ||| attentionfm: incorporating attention mechanism and factorization machine for credit scoring. ||| 5439 ||| 1160 ||| 16449 ||| 16450 ||| 
2020 ||| multivariate time-series anomaly detection via graph attention network. ||| 18496 ||| 18497 ||| 18498 ||| 18499 ||| 18500 ||| 18501 ||| 18502 ||| 18503 ||| 18504 ||| 336 ||| 
2020 ||| learning latent correlation of heterogeneous sensors using attention based temporal convolutional network. ||| 398 ||| 18505 ||| 775 ||| 16806 ||| 
2020 ||| multi-task time series forecasting with shared attention. ||| 1187 ||| 18506 ||| 1190 ||| 18507 ||| 18508 ||| 
2020 ||| community attention network for semi-supervised node classification. ||| 18509 ||| 12273 ||| 1305 ||| 18510 ||| 18511 ||| 18512 ||| 
2017 ||| an cnn-lstm attention approach to understanding user query intent from online health communities. ||| 18513 ||| 18514 ||| 18515 ||| 18516 ||| 18517 ||| 4270 ||| 
2021 ||| gcn-se: attention as explainability for node classification in dynamic graphs. ||| 18518 ||| 18519 ||| 18520 ||| 
2021 ||| psanet - subspace attention for personalized compatibility. ||| 18521 ||| 18522 ||| 18523 ||| 1948 ||| 
2018 ||| selective graph attention networks for account takeover detection. ||| 18524 ||| 1341 ||| 12590 ||| 
2018 ||| tada: trend alignment with dual-attention multi-task recurrent neural networks for sales prediction. ||| 5664 ||| 1203 ||| 18525 ||| 17580 ||| 1371 ||| 18526 ||| 9889 ||| 
2020 ||| interactive knowledge graph attention network for recommender systems. ||| 2884 ||| 18527 ||| 18528 ||| 18529 ||| 
2021 ||| joint scence network and attention-guided for image captioning. ||| 18530 ||| 6271 ||| 613 ||| 18531 ||| 
2019 ||| an attention ensemble based approach for multilabel profanity detection. ||| 18532 ||| 12790 ||| 
2019 ||| an integrated multimodal attention-based approach for bank stress test prediction. ||| 18533 ||| 9590 ||| 11466 ||| 9592 ||| 
2021 ||| hypertenet: hypergraph and transformer-based neural network for personalized list continuation. ||| 7041 ||| 18534 ||| 7042 ||| 
2018 ||| attend2trend: attention model for real-time detecting and forecasting of trending topics. ||| 18535 ||| 11042 ||| 
2017 ||| discovering cooperative structure among online items for attention dynamics. ||| 7055 ||| 7056 ||| 7057 ||| 18536 ||| 18537 ||| 18538 ||| 
2019 ||| learning attentional temporal cues of brainwaves with spatial embedding for motion intent detection. ||| 773 ||| 770 ||| 18539 ||| 771 ||| 1300 ||| 18540 ||| 
2019 ||| counterfactual attention supervision. ||| 18541 ||| 18542 ||| 3716 ||| 
2021 ||| bi-level attention graph neural networks. ||| 18543 ||| 1160 ||| 8961 ||| 
2019 ||| few-shot learning based on attention relation compare network. ||| 18544 ||| 944 ||| 7676 ||| 18545 ||| 
2021 ||| label dependent attention model for disease risk prediction using multimodal electronic health records. ||| 18546 ||| 15246 ||| 18547 ||| 18548 ||| 18549 ||| 
2019 ||| st-attn: spatial-temporal attention mechanism for multi-step citywide crowd flow prediction. ||| 18550 ||| 2424 ||| 5536 ||| 18551 ||| 18552 ||| 18553 ||| 
2020 ||| agstn: learning attention-adjusted graph spatio-temporal networks for short-term urban sensor value forecasting. ||| 3711 ||| 3712 ||| 
2021 ||| stargat: star-shaped hierarchical graph attentional network for heterogeneous network representation learning. ||| 18554 ||| 335 ||| 334 ||| 18555 ||| 
2020 ||| tado: time-varying attention with dual-optimizer model. ||| 18556 ||| 17797 ||| 18557 ||| 18558 ||| 
2021 ||| sequential diagnosis prediction with transformer and ontological representation. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 
2018 ||| muvan: a multi-view attention network for multivariate temporal data. ||| 2296 ||| 6121 ||| 1071 ||| 18559 ||| 18180 ||| 6123 ||| 18560 ||| 1630 ||| 
2019 ||| camp: co-attention memory networks for diagnosis prediction in healthcare. ||| 18561 ||| 18562 ||| 18563 ||| 1381 ||| 18564 ||| 18565 ||| 18566 ||| 9574 ||| 
2017 ||| live on tv, alive on twitter: quantifying continuous partial attention of viewers during live television telecasts. ||| 18567 ||| 18568 ||| 18569 ||| 
2020 ||| learning space-time-frequency representation with two-stream attention based 3d network for motor imagery classification. ||| 18570 ||| 4550 ||| 18571 ||| 17998 ||| 
2020 ||| interactive attention networks for semantic text matching. ||| 12351 ||| 18572 ||| 4639 ||| 18573 ||| 2355 ||| 
2018 ||| diagnosis prediction via medical context attention networks using deep generative modeling. ||| 18574 ||| 8511 ||| 17966 ||| 17969 ||| 
2021 ||| bat: beat-aligned transformer for electrocardiogram classification. ||| 18575 ||| 399 ||| 18576 ||| 18577 ||| 16285 ||| 2008 ||| 16280 ||| 
2020 ||| camta: causal attention model for multi-touch attribution. ||| 18578 ||| 18579 ||| 18580 ||| 18581 ||| 607 ||| 18582 ||| 
2020 ||| predict the next attack location via an attention-based fused-spatialtemporal lstm. ||| 1415 ||| 18583 ||| 18584 ||| 18585 ||| 
2021 ||| child face age progression and regression using self-attention multi-scale patch gan. ||| 18586 ||| 18587 ||| 
2019 ||| sclerasegnet: an improved u-net model with attention for accurate sclera segmentation. ||| 18588 ||| 18589 ||| 17944 ||| 18590 ||| 2824 ||| 17945 ||| 
2021 ||| visual-semantic transformer for face forgery detection. ||| 18591 ||| 18592 ||| 18593 ||| 18594 ||| 2824 ||| 
2021 ||| iris presentation attack detection by attention-based and deep pixel-wise binary supervision network. ||| 7170 ||| 7171 ||| 18595 ||| 7172 ||| 7173 ||| 
2021 ||| bita-net: bi-temporal attention network for facial video forgery detection. ||| 18596 ||| 16672 ||| 17944 ||| 18597 ||| 6721 ||| 
2021 ||| attention aware wavelet-based detection of morphed face images. ||| 18598 ||| 18599 ||| 18600 ||| 18601 ||| 18602 ||| 
2021 ||| video-based physiological measurement using 3d central difference convolution attention network. ||| 3473 ||| 18603 ||| 1856 ||| 18604 ||| 18605 ||| 6915 ||| 
2019 ||| polarimetric thermal to visible face verification via self-attention guided synthesis. ||| 18606 ||| 7220 ||| 18607 ||| 18608 ||| 18609 ||| 
2019 ||| sanet: smoothed attention network for single stage face detector. ||| 6389 ||| 8704 ||| 8705 ||| 
2021 ||| attention-guided progressive mapping for profile face recognition. ||| 18610 ||| 18611 ||| 
2020 ||| partial fingerprint verification via spatial transformer networks. ||| 18612 ||| 18613 ||| 18614 ||| 
2018 ||| improving face recognition by exploring local features with visual attention. ||| 18615 ||| 18616 ||| 
2021 ||| on the effectiveness of vision transformers for zero-shot face anti-spoofing. ||| 18617 ||| 7111 ||| 18618 ||| 
2021 ||| latent space transformers for generalizing deep networks. ||| 18619 ||| 18620 ||| 15160 ||| 8396 ||| 18621 ||| 18622 ||| 7033 ||| 18623 ||| 18624 ||| 8396 ||| 18625 ||| 
2019 ||| a non-invasive tool for attention-deficit disorder analysis based on gaze tracks. ||| 18626 ||| 18627 ||| 18628 ||| 18629 ||| 10314 ||| 18630 ||| 
2019 ||| named entity recognition in chinese electronic medical record using attention mechanism. ||| 18631 ||| 9472 ||| 18632 ||| 8260 ||| 18633 ||| 
2018 ||| social image captioning with tags-based attention model. ||| 18634 ||| 18635 ||| 18636 ||| 18637 ||| 18638 ||| 
2020 ||| gaussian image denoiser based on deep convolutional sparse coding with attention mechanism. ||| 11796 ||| 18639 ||| 18640 ||| 18641 ||| 18642 ||| 18643 ||| 
2020 ||| energy-efficient inference service of transformer-based deep learning models on gpus. ||| 18644 ||| 3304 ||| 18645 ||| 
2018 ||| the distribution of transient magnetic field and eddy current losses of three-phase five-legged transformer under dc bias. ||| 18646 ||| 
2020 ||| attention-based hierarchical convolution neural network for fine-grained crop image classification. ||| 18647 ||| 2532 ||| 18648 ||| 
2021 ||| safsn: a self-attention based neural network for encrypted mobile traffic classification. ||| 18649 ||| 18650 ||| 18651 ||| 18652 ||| 18653 ||| 18654 ||| 
2019 ||| spatio-temporal attention lstm model for flood forecasting. ||| 18655 ||| 18656 ||| 4285 ||| 18657 ||| 18658 ||| 
2021 ||| a mm-wave gm-assisted transformer-based matching network 2x2 phased-array receiver for 5g communication and radar systems. ||| 18659 ||| 18660 ||| 
2019 ||| transformer-based 24 ghz power amplifier in 65nm cmos technology for fmcw applications. ||| 18661 ||| 18662 ||| 18663 ||| 18664 ||| 
2021 ||| transformer interpretability beyond attention visualization. ||| 1665 ||| 1666 ||| 1667 ||| 
2020 ||| boosting the transferability of adversarial samples via attention. ||| 18665 ||| 18666 ||| 18667 ||| 1338 ||| 598 ||| 597 ||| 8549 ||| 
2020 ||| correlation-guided attention for corner detection based visual tracking. ||| 18668 ||| 10572 ||| 7700 ||| 18669 ||| 
2019 ||| temporal transformer networks: joint learning of invariant and discriminative time warping. ||| 18670 ||| 18671 ||| 18672 ||| 
2020 ||| modality shifting attention network for multi-modal video question answering. ||| 11230 ||| 18673 ||| 18674 ||| 18675 ||| 11231 ||| 
2019 ||| towards universal object detection by domain attention. ||| 12824 ||| 18676 ||| 18677 ||| 18678 ||| 
2017 ||| attentional correlation filter network for adaptive visual tracking. ||| 18679 ||| 18680 ||| 2087 ||| 18681 ||| 18682 ||| 18683 ||| 
2020 ||| recognizing handwritten mathematical expressions via paired dual loss attention network and printed mathematical expressions. ||| 13730 ||| 
2021 ||| dynamic head: unifying object detection heads with attentions. ||| 1954 ||| 1959 ||| 1956 ||| 2494 ||| 2430 ||| 1957 ||| 241 ||| 
2021 ||| general multi-label image classification with transformers. ||| 9138 ||| 18684 ||| 2005 ||| 9140 ||| 
2021 ||| self-supervised video hashing via bidirectional transformers. ||| 18685 ||| 2527 ||| 1920 ||| 1921 ||| 
2019 ||| attention-guided unified network for panoptic segmentation. ||| 18686 ||| 18687 ||| 18688 ||| 2477 ||| 18689 ||| 18690 ||| 11787 ||| 
2019 ||| multi-modal face presentation attack detection via spatial and channel attentions. ||| 18691 ||| 18692 ||| 5752 ||| 1916 ||| 1788 ||| 
2019 ||| masked graph attention network for person re-identification. ||| 18693 ||| 9378 ||| 9377 ||| 1788 ||| 
2019 ||| attention-based dropout layer for weakly supervised object localization. ||| 2090 ||| 18694 ||| 
2019 ||| feratt: facial expression recognition with attention net. ||| 18695 ||| 10314 ||| 18696 ||| 3419 ||| 18697 ||| 18698 ||| 
2021 ||| wide receptive field and channel attention network for jpeg compressed image deblurring. ||| 18699 ||| 18700 ||| 18701 ||| 
2018 ||| focal visual-text attention for visual question answering. ||| 18702 ||| 18703 ||| 12717 ||| 14441 ||| 18704 ||| 
2018 ||| picanet: learning pixel-wise contextual attention for saliency detection. ||| 2411 ||| 2414 ||| 7143 ||| 
2020 ||| lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. ||| 18705 ||| 2445 ||| 18706 ||| 18707 ||| 18708 ||| 
2018 ||| deep-bcn: deep networks meet biased competition to create a brain-inspired model of attention control. ||| 18709 ||| 18710 ||| 
2019 ||| adcrowdnet: an attention-injective deformable convolutional network for crowd understanding. ||| 9027 ||| 18711 ||| 18041 ||| 18712 ||| 6044 ||| 18713 ||| 
2017 ||| temporally steered gaussian attention for video understanding. ||| 18714 ||| 18715 ||| 18716 ||| 18717 ||| 18718 ||| 18719 ||| 
2018 ||| st-gan: spatial transformer generative adversarial networks for image compositing. ||| 18720 ||| 18721 ||| 18722 ||| 8780 ||| 18723 ||| 
2020 ||| learning oracle attention for high-fidelity face completion. ||| 18724 ||| 18611 ||| 18725 ||| 18726 ||| 1756 ||| 
2018 ||| hierarchical recurrent attention networks for structured online maps. ||| 18727 ||| 18728 ||| 18729 ||| 9239 ||| 
2020 ||| attention convolutional binary neural tree for fine-grained visual categorization. ||| 18730 ||| 8751 ||| 8750 ||| 8749 ||| 8753 ||| 18731 ||| 17695 ||| 2382 ||| 
2020 ||| attention scaling for crowd counting. ||| 18732 ||| 254 ||| 2365 ||| 18733 ||| 18734 ||| 18735 ||| 7676 ||| 2279 ||| 
2019 ||| attentional pointnet for 3d-object detection in point clouds. ||| 18736 ||| 58 ||| 18737 ||| 7969 ||| 18738 ||| 
2020 ||| learning selective self-mutual attention for rgb-d saliency detection. ||| 2411 ||| 2412 ||| 2414 ||| 
2017 ||| learning dynamic gmm for attention distribution on single-face videos. ||| 18739 ||| 18740 ||| 18741 ||| 18742 ||| 18743 ||| 
2018 ||| global pose estimation with an attention-based recurrent network. ||| 18744 ||| 17768 ||| 12196 ||| 3247 ||| 
2020 ||| fusatnet: dual attention based spectrospatial multimodal fusion network for hyperspectral and lidar classification. ||| 18745 ||| 6920 ||| 6865 ||| 7348 ||| 
2021 ||| end-to-end human pose and mesh reconstruction with transformers. ||| 18746 ||| 17976 ||| 8573 ||| 
2021 ||| an improved attention for visual question answering. ||| 18747 ||| 18748 ||| 2301 ||| 4795 ||| 
2018 ||| attentional shapecontextnet for point cloud recognition. ||| 1741 ||| 24 ||| 6214 ||| 1815 ||| 
2021 ||| soe-net: a self-attention and orientation encoding network for point cloud based place recognition. ||| 18749 ||| 18750 ||| 2332 ||| 3049 ||| 2853 ||| 18751 ||| 18752 ||| 
2018 ||| an end-to-end textspotter with explicit alignment and attention. ||| 2448 ||| 15886 ||| 2447 ||| 6335 ||| 2149 ||| 18753 ||| 
2021 ||| line segment detection using transformers without edges. ||| 1813 ||| 1812 ||| 18754 ||| 1815 ||| 
2020 ||| learning temporal co-attention models for unsupervised video action localization. ||| 18755 ||| 18756 ||| 8016 ||| 2398 ||| 
2021 ||| isolated sign recognition from rgb video using pose flow and self-attention. ||| 15028 ||| 15033 ||| 15034 ||| 
2020 ||| imram: iterative matching with recurrent attention memory for cross-modal image-text retrieval. ||| 10064 ||| 10065 ||| 12569 ||| 10066 ||| 3478 ||| 2278 ||| 
2017 ||| episodic camn: contextual attention-based memory networks with iterative feedback for scene labeling. ||| 18757 ||| 2421 ||| 18758 ||| 8608 ||| 
2020 ||| end-to-end learning for video frame compression with self-attention. ||| 9924 ||| 9925 ||| 9926 ||| 1852 ||| 9928 ||| 8024 ||| 9929 ||| 6321 ||| 
2018 ||| learning attentions: residual attentional siamese network for high performance online visual tracking. ||| 3304 ||| 18759 ||| 11269 ||| 18760 ||| 8841 ||| 18761 ||| 
2019 ||| a dual attention network with semantic embedding for few-shot learning. ||| 17713 ||| 14513 ||| 17714 ||| 
2019 ||| graph attention convolution for point cloud semantic segmentation. ||| 3279 ||| 18762 ||| 18763 ||| 18764 ||| 7470 ||| 
2018 ||| mattnet: modular attention network for referring expression comprehension. ||| 18765 ||| 18766 ||| 7142 ||| 18767 ||| 18768 ||| 3810 ||| 3809 ||| 
2017 ||| end-to-end instance segmentation with recurrent attention. ||| 9220 ||| 9223 ||| 
2020 ||| attention-driven cropping for very high resolution facial landmark detection. ||| 18769 ||| 18770 ||| 18771 ||| 18772 ||| 
2021 ||| (asna) an attention-based siamese-difference neural network with surrogate ranking loss function for perceptual image quality assessment. ||| 18773 ||| 18774 ||| 
2021 ||| thinking fast and slow: efficient text-to-visual retrieval with transformers. ||| 18775 ||| 1993 ||| 2174 ||| 18776 ||| 1997 ||| 
2019 ||| scan: spatial color attention networks for real single image super-resolution. ||| 18777 ||| 633 ||| 
2018 ||| amnet: memorability estimation with attention. ||| 6401 ||| 6403 ||| 6404 ||| 6405 ||| 
2021 ||| ednet: efficient disparity estimation with cost volume combination and attention-based spatial residual. ||| 18778 ||| 18779 ||| 3304 ||| 18780 ||| 18781 ||| 18645 ||| 
2019 ||| looking for the devil in the details: learning trilinear attention sampling network for fine-grained image recognition. ||| 2164 ||| 1699 ||| 8710 ||| 2166 ||| 
2020 ||| dynamic attention-based visual odometry. ||| 18782 ||| 18783 ||| 18784 ||| 18785 ||| 
2019 ||| residual attention-based fusion for video classification. ||| 18786 ||| 8416 ||| 18787 ||| 
2021 ||| adaptive image transformer for one-shot object detection. ||| 6361 ||| 18788 ||| 6363 ||| 
2019 ||| visual attention in multi-label image classification. ||| 11359 ||| 1871 ||| 1872 ||| 
2017 ||| residual attention network for image classification. ||| 2355 ||| 18789 ||| 18226 ||| 18790 ||| 130 ||| 675 ||| 1846 ||| 18791 ||| 
2017 ||| look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition. ||| 1699 ||| 2164 ||| 2165 ||| 
2020 ||| weakly-supervised action localization by generative attention modeling. ||| 18792 ||| 6545 ||| 8016 ||| 18793 ||| 
2019 ||| multi-view vehicle re-identification using temporal attention model and metadata re-ranking. ||| 18794 ||| 18795 ||| 2792 ||| 18796 ||| 5062 ||| 
2018 ||| end-to-end flow correlation tracking with spatial-temporal attention. ||| 18688 ||| 293 ||| 12263 ||| 2391 ||| 
2021 ||| visual navigation with spatial attention. ||| 18797 ||| 9306 ||| 18798 ||| 
2019 ||| end-to-end multi-task learning with attention. ||| 18799 ||| 18800 ||| 18801 ||| 
2020 ||| relation-aware global attention for person re-identification. ||| 11585 ||| 18278 ||| 7912 ||| 18802 ||| 11586 ||| 
2019 ||| improving referring expression grounding with cross-modal attention-guided erasing. ||| 18803 ||| 18804 ||| 18805 ||| 1846 ||| 1848 ||| 
2021 ||| pixel-guided dual-branch attention network for joint image deblurring and super-resolution. ||| 18806 ||| 18807 ||| 9624 ||| 
2017 ||| amc: attention guided multi-modal correlation learning for image search. ||| 18808 ||| 4954 ||| 8761 ||| 8762 ||| 8667 ||| 
2019 ||| attention driven vehicle re-identification and unsupervised anomaly detection for traffic understanding. ||| 2208 ||| 2210 ||| 2209 ||| 18809 ||| 2213 ||| 
2020 ||| dynamic convolution: attention over convolution kernels. ||| 1959 ||| 1954 ||| 2430 ||| 2494 ||| 1957 ||| 8573 ||| 
2019 ||| classification of computer generated and natural images based on efficient deep convolutional recurrent attention model. ||| 1462 ||| 18810 ||| 18811 ||| 1466 ||| 1464 ||| 
2021 ||| multi-task learning with attention for end-to-end autonomous driving. ||| 18812 ||| 18813 ||| 18814 ||| 18815 ||| 18816 ||| 
2019 ||| depth-attentional features for single-image rain removal. ||| 8634 ||| 1739 ||| 978 ||| 8637 ||| 
2021 ||| toward accurate and realistic outfits visualization with attention to details. ||| 18817 ||| 18818 ||| 18819 ||| 18820 ||| 
2019 ||| recursive visual attention in visual dialog. ||| 18821 ||| 2484 ||| 5212 ||| 18822 ||| 18823 ||| 1378 ||| 
2021 ||| 3d-man: 3d multi-frame attention network for object detection. ||| 18824 ||| 18825 ||| 12069 ||| 18826 ||| 
2018 ||| deep diffeomorphic transformer networks. ||| 18827 ||| 11531 ||| 7111 ||| 18828 ||| 
2018 ||| weakly supervised phrase localization with multi-scale anchored transformer network. ||| 18829 ||| 9304 ||| 4095 ||| 1685 ||| 
2019 ||| cross-modal self-attention network for referring image segmentation. ||| 18830 ||| 18831 ||| 2740 ||| 602 ||| 
2018 ||| image caption generation with hierarchical contextual visual spatial attention. ||| 18832 ||| 18833 ||| 
2021 ||| perceptual image quality assessment with transformers. ||| 18834 ||| 18835 ||| 18836 ||| 18837 ||| 
2018 ||| decidenet: counting varying density crowds through attention guided detection and density estimation. ||| 5206 ||| 11223 ||| 8850 ||| 18704 ||| 
2018 ||| attention in multimodal neural networks for person re-identification. ||| 18838 ||| 18839 ||| 18840 ||| 8379 ||| 8034 ||| 
2021 ||| up-detr: unsupervised pre-training for object detection with transformers. ||| 18841 ||| 18842 ||| 18843 ||| 18844 ||| 
2020 ||| polytransform: deep polygon transformer for instance segmentation. ||| 18845 ||| 18727 ||| 18728 ||| 18846 ||| 18847 ||| 9239 ||| 
2021 ||| bottleneck transformers for visual recognition. ||| 18848 ||| 18849 ||| 9133 ||| 2467 ||| 18850 ||| 2466 ||| 
2020 ||| cars can't fly up in the sky: improving urban-scene segmentation via height-driven attention networks. ||| 18851 ||| 18852 ||| 1183 ||| 
2021 ||| self-attention based text knowledge mining for text detection. ||| 18853 ||| 18854 ||| 13429 ||| 
2017 ||| attentional push: a deep convolutional network for augmenting image salience with shared attention modeling in social scenes. ||| 18855 ||| 18856 ||| 
2021 ||| dual attention suppression attack: generate adversarial camouflage in physical world. ||| 18857 ||| 18858 ||| 18859 ||| 18860 ||| 18861 ||| 17695 ||| 
2020 ||| towards robust image classification using sequential attention models. ||| 2103 ||| 9272 ||| 18862 ||| 18863 ||| 18864 ||| 18865 ||| 
2020 ||| s2a: wasserstein gan with spatio-spectral laplacian attention for multi-spectral band synthesis. ||| 18866 ||| 18867 ||| 18868 ||| 18869 ||| 
2020 ||| prime sample attention in object detection. ||| 18870 ||| 472 ||| 2291 ||| 2162 ||| 
2020 ||| interactive image segmentation with first click attention. ||| 16443 ||| 5543 ||| 18871 ||| 1904 ||| 1977 ||| 
2020 ||| tesa: tensor element self-attention via matricization. ||| 18872 ||| 18873 ||| 18874 ||| 18875 ||| 
2020 ||| self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. ||| 18876 ||| 18877 ||| 
2021 ||| a joint spatial and magnification based attention framework for large scale histopathology classification. ||| 18878 ||| 18879 ||| 18880 ||| 18881 ||| 18882 ||| 18883 ||| 18884 ||| 
2019 ||| pyramid feature attention network for saliency detection. ||| 18885 ||| 18886 ||| 
2020 ||| your local gan: designing two dimensional local attention mechanisms for generative models. ||| 9180 ||| 9181 ||| 14048 ||| 9182 ||| 
2020 ||| epipolar transformers. ||| 18887 ||| 10233 ||| 18888 ||| 18889 ||| 
2021 ||| multimodal motion prediction with stacked transformers. ||| 18890 ||| 18891 ||| 18892 ||| 18893 ||| 2373 ||| 
2017 ||| temporal attention-gated model for robust sequence classification. ||| 1131 ||| 18894 ||| 1136 ||| 3601 ||| 
2021 ||| layouttransformer: scene layout generation with conceptual and spatial diversity. ||| 18895 ||| 18896 ||| 11323 ||| 6328 ||| 
2019 ||| progressive pose attention transfer for person image generation. ||| 18897 ||| 17971 ||| 18898 ||| 17248 ||| 18899 ||| 17429 ||| 
2017 ||| inverse compositional spatial transformer networks. ||| 18720 ||| 18723 ||| 
2019 ||| factor graph attention. ||| 9305 ||| 18900 ||| 9306 ||| 8566 ||| 
2021 ||| graph attention tracking. ||| 13193 ||| 18901 ||| 13194 ||| 18902 ||| 18903 ||| 6335 ||| 
2021 ||| mr image super-resolution with squeeze and excitation reasoning attention network. ||| 1730 ||| 2233 ||| 2232 ||| 1734 ||| 
2020 ||| deformable siamese attention networks for visual object tracking. ||| 18904 ||| 18905 ||| 2447 ||| 12430 ||| 
2019 ||| learning roi transformer for oriented object detection in aerial images. ||| 18906 ||| 2082 ||| 18907 ||| 2085 ||| 18908 ||| 
2019 ||| attention-aware multi-stroke style transfer. ||| 18909 ||| 18910 ||| 18911 ||| 18912 ||| 18913 ||| 1224 ||| 
2021 ||| all you can embed: natural language based vehicle retrieval with spatio-temporal transformers. ||| 18914 ||| 18915 ||| 18916 ||| 18917 ||| 18918 ||| 
2021 ||| rstnet: captioning with adaptive attention on visual and non-visual words. ||| 18919 ||| 2504 ||| 17658 ||| 17659 ||| 2501 ||| 6831 ||| 2382 ||| 2367 ||| 
2019 ||| the pros and cons: rank-aware temporal attention for skill determination in long videos. ||| 18920 ||| 18921 ||| 7794 ||| 
2019 ||| learning unsupervised video object segmentation through visual attention. ||| 2444 ||| 18922 ||| 18923 ||| 2445 ||| 6456 ||| 3303 ||| 2163 ||| 
2021 ||| co-grounding networks with semantic attention for referring expression comprehension in videos. ||| 18277 ||| 18924 ||| 18279 ||| 18925 ||| 18926 ||| 
2019 ||| multi-layer depth and epipolar feature transformers for 3d scene reconstruction. ||| 2032 ||| 2033 ||| 2034 ||| 2035 ||| 
2019 ||| kernel transformer networks for compact spherical convolution. ||| 18927 ||| 1664 ||| 
2021 ||| revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning. ||| 18928 ||| 18929 ||| 18930 ||| 18931 ||| 
2021 ||| reinforced attention for few-shot learning and beyond. ||| 18932 ||| 2127 ||| 18933 ||| 2814 ||| 18934 ||| 2131 ||| 2130 ||| 
2018 ||| diversity regularized spatiotemporal attention for video-based person re-identification. ||| 2332 ||| 18935 ||| 18936 ||| 1846 ||| 
2017 ||| global context-aware attention lstm networks for 3d action recognition. ||| 1235 ||| 8608 ||| 18937 ||| 18938 ||| 11620 ||| 
2020 ||| context-aware group captioning via self-attention and contrastive features. ||| 18939 ||| 18940 ||| 18941 ||| 18766 ||| 8660 ||| 
2017 ||| dual attention networks for multimodal reasoning and matching. ||| 18942 ||| 9523 ||| 18943 ||| 
2021 ||| end-to-end video instance segmentation with transformers. ||| 10192 ||| 18944 ||| 18945 ||| 6335 ||| 18946 ||| 10211 ||| 5155 ||| 
2018 ||| da-gan: instance-level image translation by deep attention generative adversarial networks. ||| 18947 ||| 1699 ||| 13811 ||| 2165 ||| 
2017 ||| sca-cnn: spatial and channel-wise attention in convolutional networks for image captioning. ||| 9570 ||| 2484 ||| 7652 ||| 9631 ||| 18948 ||| 683 ||| 3605 ||| 
2020 ||| learning texture transformer network for image super-resolution. ||| 18949 ||| 18950 ||| 1699 ||| 5277 ||| 1772 ||| 
2020 ||| multi-modality cross attention network for image and sentence matching. ||| 16771 ||| 18733 ||| 185 ||| 17860 ||| 8711 ||| 
2018 ||| stacked latent attention for multimodal reasoning. ||| 2025 ||| 18951 ||| 
2018 ||| multimodal attention for fusion of audio and spatiotemporal features for video description. ||| 2507 ||| 2508 ||| 12021 ||| 7284 ||| 2509 ||| 7283 ||| 2512 ||| 
2021 ||| festa: flow estimation via spatial-temporal attention for scene point clouds. ||| 18952 ||| 18953 ||| 18954 ||| 18955 ||| 18956 ||| 
2018 ||| attention clusters: purely attention based local feature integration for video classification. ||| 18016 ||| 2190 ||| 3774 ||| 18957 ||| 2530 ||| 2531 ||| 
2019 ||| multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation. ||| 435 ||| 436 ||| 437 ||| 15791 ||| 7342 ||| 127 ||| 
2021 ||| rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. ||| 18958 ||| 18959 ||| 2335 ||| 2196 ||| 18960 ||| 18961 ||| 6365 ||| 18962 ||| 2198 ||| 2160 ||| 254 ||| 
2021 ||| temporal-relational crosstransformers for few-shot action recognition. ||| 18963 ||| 18964 ||| 7855 ||| 7793 ||| 7794 ||| 
2020 ||| learned image compression with discretized gaussian mixture likelihoods and attention modules. ||| 18965 ||| 13836 ||| 18966 ||| 16367 ||| 
2020 ||| robust superpixel-guided attentional adversarial attack. ||| 18967 ||| 18968 ||| 2494 ||| 18969 ||| 18970 ||| 18971 ||| 1848 ||| 1846 ||| 18972 ||| 2305 ||| 
2019 ||| are you paying attention? classifying attention in pivotal response treatment videos. ||| 2866 ||| 2867 ||| 2869 ||| 
2018 ||| bottom-up and top-down attention for image captioning and visual question answering. ||| 8565 ||| 3561 ||| 18973 ||| 18974 ||| 3259 ||| 7450 ||| 241 ||| 
2020 ||| attention-aware multi-view stereo. ||| 18975 ||| 18976 ||| 17721 ||| 18977 ||| 12029 ||| 18978 ||| 
2020 ||| channel attention based iterative residual learning for depth map super-resolution. ||| 18979 ||| 2240 ||| 18707 ||| 11307 ||| 3337 ||| 7449 ||| 18708 ||| 
2018 ||| where and why are they looking? jointly inferring human attention and intentions in complex tasks. ||| 18980 ||| 1305 ||| 18981 ||| 14779 ||| 18982 ||| 
2020 ||| hypergraph attention networks for multimodal learning. ||| 8574 ||| 18983 ||| 8575 ||| 8577 ||| 8580 ||| 
2018 ||| differential attention for visual question answering. ||| 7149 ||| 7152 ||| 
2017 ||| supervising neural attention models for video captioning by human gaze data. ||| 18984 ||| 18985 ||| 18986 ||| 18987 ||| 18988 ||| 18989 ||| 
2021 ||| multi-stage aggregated transformer network for temporal language localization in videos. ||| 18990 ||| 11466 ||| 18991 ||| 18992 ||| 9579 ||| 2067 ||| 1040 ||| 
2019 ||| second-order attention network for single image super-resolution. ||| 7895 ||| 18993 ||| 18994 ||| 7897 ||| 241 ||| 
2021 ||| transformer-based text detection in the wild. ||| 18995 ||| 18996 ||| 18997 ||| 18998 ||| 18999 ||| 
2020 ||| hierarchical graph attention network for visual relationship detection. ||| 19000 ||| 9061 ||| 
2020 ||| inflated episodic memory with region self-attention for long-tailed visual recognition. ||| 1974 ||| 208 ||| 
2020 ||| fantastic answers and where to find them: immersive question-directed visual attention. ||| 1871 ||| 2235 ||| 8695 ||| 1872 ||| 
2020 ||| meshed-memory transformer for image captioning. ||| 19001 ||| 19002 ||| 19003 ||| 13611 ||| 
2019 ||| mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks. ||| 5672 ||| 6417 ||| 12196 ||| 6335 ||| 836 ||| 
2021 ||| gated spatio-temporal attention-guided video deblurring. ||| 19004 ||| 11486 ||| 
2020 ||| normalized and geometry-aware self-attention network for image captioning. ||| 19005 ||| 2058 ||| 11392 ||| 19006 ||| 19007 ||| 2080 ||| 
2020 ||| nonlocal channel attention for nonhomogeneous image dehazing. ||| 12369 ||| 19008 ||| 19009 ||| 12370 ||| 
2019 ||| kernel transformer networks for compact spherical convolution. ||| 18927 ||| 1664 ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| 19010 ||| 19011 ||| 19012 ||| 7869 ||| 19013 ||| 19014 ||| 
2019 ||| attention-guided network for ghost-free high dynamic range imaging. ||| 19015 ||| 19016 ||| 19017 ||| 5177 ||| 6335 ||| 9216 ||| 10075 ||| 
2021 ||| point 4d transformer networks for spatio-temporal modeling in point cloud videos. ||| 19018 ||| 208 ||| 19019 ||| 
2021 ||| unsupervised visual attention and invariance for reinforcement learning. ||| 12824 ||| 19020 ||| 19021 ||| 
2019 ||| pay attention! - robustifying a deep visuomotor policy through task-focused visual attention. ||| 19022 ||| 1750 ||| 1752 ||| 19023 ||| 2713 ||| 19024 ||| 
2019 ||| salient object detection with pyramid attention and salient edges. ||| 2444 ||| 18923 ||| 2445 ||| 3303 ||| 1854 ||| 
2018 ||| attngan: fine-grained text to image generation with attentional generative adversarial networks. ||| 19025 ||| 1953 ||| 19026 ||| 14048 ||| 2044 ||| 19027 ||| 3561 ||| 
2020 ||| attention-based context aware reasoning for situation recognition. ||| 19028 ||| 10325 ||| 3131 ||| 
2020 ||| video super-resolution with temporal group attention. ||| 19029 ||| 5665 ||| 19030 ||| 19031 ||| 18874 ||| 1688 ||| 19032 ||| 11221 ||| 2398 ||| 
2020 ||| large scale vehicle re-identification by knowledge transfer from simulated data and temporal attention. ||| 19033 ||| 19034 ||| 19035 ||| 
2020 ||| explaining autonomous driving by learning end-to-end visual attention. ||| 19036 ||| 19037 ||| 19038 ||| 19039 ||| 19040 ||| 
2019 ||| multi-scale body-part mask guided attention for person re-identification. ||| 19041 ||| 19042 ||| 19043 ||| 
2017 ||| multi-attention network for one shot learning. ||| 5845 ||| 6334 ||| 6335 ||| 19044 ||| 5177 ||| 1040 ||| 
2020 ||| non-local neural networks with grouped bilinear attentional transforms. ||| 8014 ||| 8725 ||| 8016 ||| 8727 ||| 
2020 ||| adaptive weighted attention network with camera spectral sensitivity prior for spectral reconstruction from rgb images. ||| 6699 ||| 6698 ||| 6700 ||| 6701 ||| 523 ||| 
2019 ||| lsta: long short-term attention for egocentric action recognition. ||| 9832 ||| 8035 ||| 9833 ||| 
2018 ||| fooling vision and language models despite localization and attention mechanism. ||| 19045 ||| 19046 ||| 748 ||| 19047 ||| 19048 ||| 3464 ||| 
2018 ||| inferring shared attention in social scene videos. ||| 19049 ||| 5348 ||| 18980 ||| 2444 ||| 18982 ||| 
2021 ||| attention! stay focus! ||| 19050 ||| 
2020 ||| point cloud completion by skip-attention network with hierarchical folding. ||| 2558 ||| 19051 ||| 2563 ||| 2559 ||| 
2021 ||| person re-identification using heterogeneous local graph attention networks. ||| 19052 ||| 19053 ||| 13805 ||| 
2019 ||| object detection with location-aware deformable convolution and backward attention filtering. ||| 10922 ||| 19054 ||| 
2019 ||| practical stacked non-local attention modules for image compression. ||| 19055 ||| 5664 ||| 19056 ||| 19057 ||| 
2019 ||| deep attention model for the hierarchical diagnosis of skin lesions. ||| 19058 ||| 19059 ||| 19060 ||| 
2021 ||| ssan: separable self-attention network for video representation learning. ||| 5079 ||| 19061 ||| 19062 ||| 
2021 ||| sstvos: sparse spatiotemporal transformers for video object segmentation. ||| 19063 ||| 19064 ||| 7969 ||| 19065 ||| 2582 ||| 
2021 ||| transformer tracking. ||| 19066 ||| 1697 ||| 19067 ||| 952 ||| 19068 ||| 1700 ||| 
2021 ||| expectation-maximization attention cross residual network for single image super-resolution. ||| 248 ||| 19069 ||| 249 ||| 
2019 ||| visual attention consistency under image transforms for multi-label image classification. ||| 8358 ||| 19070 ||| 9669 ||| 19071 ||| 307 ||| 
2021 ||| manipulation detection in satellite images using vision transformer. ||| 4194 ||| 19072 ||| 13992 ||| 6881 ||| 6880 ||| 19073 ||| 6887 ||| 
2021 ||| transformer meets tracker: exploiting temporal context for robust visual tracking. ||| 6003 ||| 1806 ||| 5894 ||| 1807 ||| 
2020 ||| color-wise attention network for low-light image enhancement. ||| 19074 ||| 19075 ||| 19076 ||| 19077 ||| 10405 ||| 
2017 ||| attention-based natural language person retrieval. ||| 614 ||| 15099 ||| 11676 ||| 19078 ||| 
2021 ||| loftr: detector-free local feature matching with transformers. ||| 841 ||| 19079 ||| 19080 ||| 19081 ||| 19082 ||| 
2021 ||| dual attention guided gaze target detection in the wild. ||| 13614 ||| 19083 ||| 19084 ||| 8906 ||| 19085 ||| 18183 ||| 6516 ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| 19010 ||| 19011 ||| 19012 ||| 7869 ||| 19013 ||| 19014 ||| 
2019 ||| dual attention network for scene segmentation. ||| 11391 ||| 2058 ||| 19086 ||| 2969 ||| 1171 ||| 19087 ||| 2080 ||| 
2018 ||| generative image inpainting with contextual attention. ||| 12814 ||| 18766 ||| 18767 ||| 7142 ||| 18768 ||| 17653 ||| 
2020 ||| hierarchical pyramid diverse attention networks for face recognition. ||| 2322 ||| 19088 ||| 19089 ||| 2323 ||| 
2021 ||| coordinate attention for efficient mobile network design. ||| 1902 ||| 19090 ||| 1685 ||| 
2021 ||| introvert: human trajectory prediction via conditional 3d attention. ||| 19091 ||| 19092 ||| 19093 ||| 
2021 ||| hotr: end-to-end human-object interaction detection with transformers. ||| 19094 ||| 19095 ||| 9250 ||| 8574 ||| 9251 ||| 
2021 ||| facial action unit detection with transformers. ||| 19096 ||| 648 ||| 19097 ||| 
2019 ||| emotion-aware human attention prediction. ||| 17137 ||| 19098 ||| 19099 ||| 19019 ||| 
2020 ||| image search with text feedback by visiolinguistic attention learning. ||| 19100 ||| 19101 ||| 18930 ||| 
2020 ||| residual pixel attention network for spectral reconstruction from rgb images. ||| 9407 ||| 19102 ||| 6644 ||| 
2021 ||| discrete-continuous action space policy gradient-based attention for image-text matching. ||| 13 ||| 5999 ||| 12120 ||| 
2020 ||| context-aware attention network for image-text retrieval. ||| 336 ||| 337 ||| 18260 ||| 338 ||| 
2018 ||| attention-aware compositional network for person re-identification. ||| 4620 ||| 8164 ||| 7758 ||| 19103 ||| 2303 ||| 
2021 ||| topological planning with transformers for vision-and-language navigation. ||| 19104 ||| 19105 ||| 19106 ||| 19107 ||| 2698 ||| 9218 ||| 
2021 ||| a2-fpn: attention aggregation based feature pyramid network for instance segmentation. ||| 19108 ||| 11220 ||| 1715 ||| 11221 ||| 
2020 ||| fake news detection using higher-order user to user mutual-attention progression in propagation paths. ||| 1314 ||| 
2020 ||| further non-local and channel attention networks for vehicle re-identification. ||| 19109 ||| 19110 ||| 19111 ||| 19112 ||| 19113 ||| 
2019 ||| directing dnns attention for facial attribution classification using gradient-weighted class activation mapping. ||| 13408 ||| 19114 ||| 19115 ||| 17507 ||| 
2020 ||| x-linear attention networks for image captioning. ||| 19116 ||| 19117 ||| 19118 ||| 2165 ||| 
2020 ||| leaf spot attention network for apple leaf disease identification. ||| 19119 ||| 19120 ||| 
2020 ||| end-to-end adversarial-attention network for multi-modal clustering. ||| 19121 ||| 19122 ||| 
2021 ||| keep your eyes on the lane: real-time attention-guided lane detection. ||| 19123 ||| 19124 ||| 19125 ||| 7033 ||| 19126 ||| 19127 ||| 19128 ||| 
2021 ||| multi-modal fusion transformer for end-to-end autonomous driving. ||| 2125 ||| 2124 ||| 2126 ||| 
2020 ||| sign language transformers: joint end-to-end sign language recognition and translation. ||| 8529 ||| 7442 ||| 8767 ||| 8768 ||| 8530 ||| 
2019 ||| attention based image compression post-processing convlutional neural network. ||| 19129 ||| 
2019 ||| an attention enhanced graph convolutional lstm network for skeleton-based action recognition. ||| 18100 ||| 19130 ||| 1160 ||| 10429 ||| 17803 ||| 
2020 ||| iterative answer prediction with pointer-augmented multimodal transformers for textvqa. ||| 1879 ||| 1880 ||| 19048 ||| 19131 ||| 
2021 ||| mega-cda: memory guided attention for category-aware unsupervised domain adaptive object detection. ||| 19132 ||| 19133 ||| 19134 ||| 19135 ||| 18609 ||| 
2019 ||| interpretation of feature space using multi-channel attentional sub-networks. ||| 19136 ||| 19137 ||| 
2019 ||| learning parallax attention for stereo image super-resolution. ||| 19138 ||| 19139 ||| 19140 ||| 19141 ||| 19142 ||| 19143 ||| 7271 ||| 
2018 ||| occluded pedestrian detection through guided attention in cnns. ||| 19144 ||| 1825 ||| 19145 ||| 
2019 ||| densenet with deep residual channel-attention blocks for single image super resolution. ||| 19146 ||| 19147 ||| 
2021 ||| delving deep into many-to-many attention for few-shot video object segmentation. ||| 19148 ||| 19149 ||| 19150 ||| 19151 ||| 19152 ||| 
2020 ||| satellite image time series classification with pixel-set encoders and temporal self-attention. ||| 2309 ||| 2310 ||| 2311 ||| 7111 ||| 19153 ||| 19154 ||| 
2018 ||| progressive attention guided recurrent network for salient object detection. ||| 19155 ||| 19156 ||| 2038 ||| 1700 ||| 8608 ||| 
2021 ||| clusformer: a transformer based clustering approach to unsupervised large-scale face and visual landmark recognition. ||| 19157 ||| 19158 ||| 2133 ||| 19159 ||| 2138 ||| 
2020 ||| scatter: selective context attentional scene text recognizer. ||| 19160 ||| 19161 ||| 19162 ||| 19163 ||| 19164 ||| 2644 ||| 
2021 ||| attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton. ||| 1325 ||| 19165 ||| 
2020 ||| one-shot adversarial attacks on visual tracking with dual attention. ||| 12676 ||| 19166 ||| 19167 ||| 3614 ||| 7897 ||| 11819 ||| 2367 ||| 
2019 ||| enhancing salient object segmentation through attention. ||| 19168 ||| 19169 ||| 19170 ||| 19171 ||| 
2019 ||| medical time series classification with hierarchical attention-based temporal convolutional networks: a case study of myotonic dystrophy diagnosis. ||| 10545 ||| 19172 ||| 19173 ||| 19174 ||| 19175 ||| 
2020 ||| residual channel attention generative adversarial network for image super-resolution and noise reduction. ||| 7333 ||| 19176 ||| 7163 ||| 
2019 ||| scene memory transformer for embodied agents in long-horizon tasks. ||| 19177 ||| 19178 ||| 19179 ||| 9218 ||| 
2020 ||| focus longer to see better: recursively refined attention for fine-grained image classification. ||| 19180 ||| 19181 ||| 1905 ||| 7195 ||| 
2021 ||| deep rgb-d saliency detection with depth-sensitive attention and automatic multi-modal fusion. ||| 15877 ||| 19182 ||| 19183 ||| 19184 ||| 2259 ||| 
2020 ||| vsgnet: spatial attention network for detecting human object interactions using graph convolutions. ||| 7197 ||| 19185 ||| 7201 ||| 
2021 ||| micro-expression classification based on landmark relations with graph attention convolutional network. ||| 19186 ||| 19187 ||| 
2019 ||| end-to-end optimized image compression with attention mechanism. ||| 11464 ||| 19188 ||| 19189 ||| 19190 ||| 
2020 ||| squeeze-and-attention networks for semantic segmentation. ||| 19191 ||| 19192 ||| 19193 ||| 19194 ||| 19195 ||| 19196 ||| 14892 ||| 19197 ||| 7865 ||| 
2021 ||| csanet: high speed channel spatial attention network for mobile isp. ||| 19198 ||| 19199 ||| 19200 ||| 19201 ||| 19202 ||| 
2020 ||| post-processing network based on dense inception attention for video compression. ||| 19203 ||| 5998 ||| 5999 ||| 6000 ||| 19204 ||| 6002 ||| 6003 ||| 6001 ||| 
2018 ||| dual attention matching network for context-aware feature sequence based person re-identification. ||| 19205 ||| 675 ||| 19206 ||| 19207 ||| 19208 ||| 11620 ||| 8608 ||| 
2020 ||| an accurate segmentation-based scene text detector with context attention and repulsive text border. ||| 19209 ||| 19210 ||| 3248 ||| 6935 ||| 
2021 ||| bgt-net: bidirectional gru transformer network for scene graph generation. ||| 5715 ||| 19211 ||| 13618 ||| 
2021 ||| skeletor: skeletal transformers for robust body-pose estimation. ||| 4100 ||| 8529 ||| 7442 ||| 8530 ||| 
2020 ||| on recognizing texts of arbitrary shapes with 2d self-attention. ||| 8512 ||| 8511 ||| 8510 ||| 2091 ||| 19212 ||| 8514 ||| 
2018 ||| estimating attention of faces due to its growing level of emotions. ||| 19213 ||| 19214 ||| 19215 ||| 19216 ||| 
2018 ||| parallel attention: a unified framework for visual object discovery through dialogs and queries. ||| 2057 ||| 6417 ||| 6335 ||| 9216 ||| 5177 ||| 
2018 ||| going from image to video saliency: augmenting image salience with dynamic attentional push. ||| 18855 ||| 18856 ||| 
2021 ||| end-to-end human object interaction detection with hoi transformer. ||| 19217 ||| 19218 ||| 689 ||| 19219 ||| 5993 ||| 3473 ||| 19220 ||| 19221 ||| 1460 ||| 19222 ||| 4394 ||| 
2017 ||| attention-aware face hallucination via deep reinforcement learning. ||| 19223 ||| 2315 ||| 19224 ||| 1686 ||| 1800 ||| 
2020 ||| self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. ||| 19225 ||| 1134 ||| 1915 ||| 1916 ||| 1788 ||| 
2019 ||| progressive attention memory network for movie story question answering. ||| 11230 ||| 18673 ||| 18675 ||| 1606 ||| 11231 ||| 
2019 ||| pcan: 3d attention map learning using contextual information for point cloud based retrieval. ||| 19226 ||| 19227 ||| 
2020 ||| eca-net: efficient channel attention for deep convolutional neural networks. ||| 19228 ||| 19229 ||| 8754 ||| 19230 ||| 2018 ||| 5077 ||| 
2021 ||| adnet: attention-guided deformable convolutional network for high dynamic range imaging. ||| 5107 ||| 19231 ||| 19232 ||| 19233 ||| 18141 ||| 19234 ||| 19235 ||| 4394 ||| 19236 ||| 
2020 ||| adaptive graph convolutional network with attention graph clustering for co-saliency detection. ||| 19237 ||| 19238 ||| 19239 ||| 1748 ||| 13706 ||| 6625 ||| 
2017 ||| multi-context attention for human pose estimation. ||| 18227 ||| 2334 ||| 2303 ||| 19240 ||| 8660 ||| 1846 ||| 
2021 ||| encoder fusion network with co-attention embedding for referring image segmentation. ||| 19241 ||| 19242 ||| 19243 ||| 1700 ||| 
2020 ||| few-shot object detection with attention-rpn and multi-relation detector. ||| 19244 ||| 19245 ||| 19246 ||| 8549 ||| 
2020 ||| inferring attention shift ranks of objects for image saliency. ||| 19247 ||| 8538 ||| 19248 ||| 19249 ||| 8541 ||| 
2021 ||| lesion-aware transformers for diabetic retinopathy grading. ||| 19250 ||| 19251 ||| 18733 ||| 5963 ||| 8711 ||| 17860 ||| 
2021 ||| gaussian context transformer. ||| 19252 ||| 19253 ||| 19254 ||| 19255 ||| 19256 ||| 
2021 ||| variational transformer networks for layout generation. ||| 19257 ||| 19258 ||| 19259 ||| 19260 ||| 
2021 ||| connecting what to say with where to look by modeling human attention traces. ||| 19261 ||| 18765 ||| 3402 ||| 3809 ||| 19262 ||| 18033 ||| 19263 ||| 
2021 ||| line art colorization with concatenated spatial attention. ||| 19264 ||| 15920 ||| 
2021 ||| appearance-based gaze estimation using attention and difference mechanism. ||| 19265 ||| 19266 ||| 
2017 ||| knowing when to look: adaptive attention via a visual sentinel for image captioning. ||| 8568 ||| 3287 ||| 8567 ||| 19267 ||| 
2021 ||| guided interactive video object segmentation using reliability-based attention maps. ||| 19268 ||| 19269 ||| 8631 ||| 
2021 ||| image super-resolution with non-local sparse attention. ||| 19270 ||| 19271 ||| 19272 ||| 
2020 ||| a shared multi-attention framework for multi-label zero-shot learning. ||| 19273 ||| 19093 ||| 
2021 ||| co-attention for conditioned image matching. ||| 19274 ||| 7111 ||| 19275 ||| 1997 ||| 
2020 ||| visual parsing with query-driven global graph attention (qd-gga): preliminary results for handwritten math formula recognition. ||| 17380 ||| 18143 ||| 17383 ||| 
2019 ||| attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions. ||| 8740 ||| 5902 ||| 8741 ||| 
2019 ||| hybrid-attention based decoupled metric learning for zero-shot image retrieval. ||| 2015 ||| 2400 ||| 
2021 ||| mdmmt: multidomain multimodal transformer for video retrieval. ||| 19276 ||| 19277 ||| 19278 ||| 19279 ||| 
2021 ||| visual focus of attention estimation in 3d scene with an arbitrary number of targets. ||| 59 ||| 19280 ||| 8050 ||| 
2018 ||| visual grounding via accumulated attention. ||| 19281 ||| 6417 ||| 1827 ||| 10072 ||| 10069 ||| 6413 ||| 
2019 ||| attentional pointnet for 3d-object detection in point clouds. ||| 18736 ||| 58 ||| 18737 ||| 7969 ||| 18738 ||| 
2021 ||| causal attention for vision-language tasks. ||| 5157 ||| 2484 ||| 19282 ||| 1691 ||| 
2020 ||| attention mechanism exploits temporal contexts: real-time 3d human pose reconstruction. ||| 19283 ||| 19284 ||| 19285 ||| 2230 ||| 19286 ||| 19287 ||| 
2020 ||| attention-guided hierarchical structure aggregation for image matting. ||| 2149 ||| 19288 ||| 7676 ||| 816 ||| 2365 ||| 817 ||| 19289 ||| 
2018 ||| end-to-end dense video captioning with masked transformer. ||| 3574 ||| 4889 ||| 7342 ||| 19267 ||| 3287 ||| 
2018 ||| emotional attention: a study of image sentiment and visual attention. ||| 19098 ||| 19099 ||| 1871 ||| 19290 ||| 19291 ||| 19019 ||| 1872 ||| 
2018 ||| improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. ||| 19292 ||| 8741 ||| 
2019 ||| aanet: attribute attention network for person re-identifications. ||| 19293 ||| 19294 ||| 19295 ||| 
2021 ||| max-deeplab: end-to-end panoptic segmentation with mask transformers. ||| 8656 ||| 8657 ||| 8659 ||| 8660 ||| 8661 ||| 
2019 ||| dynamic fusion with intra- and inter-modality attention flow for visual question answering. ||| 2170 ||| 19296 ||| 19297 ||| 17712 ||| 3303 ||| 1846 ||| 1848 ||| 
2020 ||| attentional bottleneck: towards an interpretable deep driving network. ||| 2041 ||| 19298 ||| 
2019 ||| heterogeneous memory enhanced multimodal attention model for video question answering. ||| 17904 ||| 7497 ||| 6588 ||| 19299 ||| 1460 ||| 9143 ||| 
2020 ||| sketchformer: transformer-based representation for sketched structure. ||| 19300 ||| 2286 ||| 2288 ||| 19301 ||| 
2021 ||| taming transformers for high-resolution image synthesis. ||| 1803 ||| 1802 ||| 648 ||| 1804 ||| 
2020 ||| actor-transformers for group activity recognition. ||| 19302 ||| 19303 ||| 19304 ||| 18223 ||| 
2021 ||| rethinking the self-attention in vision transformers. ||| 19305 ||| 2590 ||| 2592 ||| 2594 ||| 2029 ||| 2597 ||| 19306 ||| 
2021 ||| symmetric parallax attention for stereo image super-resolution. ||| 19139 ||| 19307 ||| 19138 ||| 19308 ||| 19143 ||| 7271 ||| 
2021 ||| improving accuracy of respiratory rate estimation by restoring high resolution features with transformers and recursive convolutional models. ||| 19309 ||| 19310 ||| 19311 ||| 19312 ||| 19313 ||| 
2020 ||| sct: set constrained temporal transformer for set supervised action segmentation. ||| 19314 ||| 4194 ||| 19315 ||| 
2019 ||| see more, know more: unsupervised video object segmentation with co-attention siamese networks. ||| 19316 ||| 2444 ||| 5264 ||| 2445 ||| 1932 ||| 7408 ||| 
2019 ||| channel attention networks. ||| 19317 ||| 19318 ||| 
2019 ||| on attention modules for audio-visual synchronization. ||| 19319 ||| 19320 ||| 8728 ||| 
2020 ||| exploring self-attention for image recognition. ||| 2335 ||| 2204 ||| 1884 ||| 
2020 ||| predicting goal-directed human attention using inverse reinforcement learning. ||| 19321 ||| 19322 ||| 19323 ||| 19324 ||| 19325 ||| 18710 ||| 18884 ||| 7365 ||| 
2020 ||| fine-grained generalized zero-shot learning via dense attribute-based attention. ||| 19273 ||| 19093 ||| 
2019 ||| improved automating seismic facies analysis using deep dilated attention autoencoders. ||| 19326 ||| 19327 ||| 19328 ||| 19329 ||| 
2018 ||| mask-guided contrastive attention model for person re-identification. ||| 6147 ||| 4648 ||| 2303 ||| 10429 ||| 
2018 ||| tell me where to look: guided attention inference network. ||| 2232 ||| 1744 ||| 1746 ||| 19330 ||| 1734 ||| 
2019 ||| video action transformer network. ||| 1663 ||| 1994 ||| 1995 ||| 8786 ||| 1997 ||| 
2021 ||| diverse part discovery: occluded person re-identification with part-aware transformer. ||| 5780 ||| 19331 ||| 18733 ||| 19332 ||| 17860 ||| 8711 ||| 
2019 ||| neighbourhood watch: referring expression comprehension via language-guided graph attention networks. ||| 5845 ||| 6417 ||| 19333 ||| 6335 ||| 1039 ||| 5177 ||| 
2018 ||| structured attention guided convolutional neural fields for monocular depth estimation. ||| 436 ||| 1160 ||| 435 ||| 2519 ||| 437 ||| 2524 ||| 
2018 ||| pay attention to virality: understanding popularity of social media videos with the attention mechanism. ||| 19334 ||| 19335 ||| 
2019 ||| deep modular co-attention networks for visual question answering. ||| 1753 ||| 1754 ||| 19336 ||| 1756 ||| 2398 ||| 
2020 ||| visual-semantic matching by exploring high-order attention and distraction. ||| 19337 ||| 19338 ||| 8016 ||| 
2019 ||| attention branch network: learning of attention mechanism for visual explanation. ||| 15475 ||| 723 ||| 724 ||| 725 ||| 
2018 ||| jersey number recognition with semi-supervised spatial transformer network. ||| 19339 ||| 19340 ||| 19332 ||| 3034 ||| 8727 ||| 
2019 ||| modeling point clouds with self-attention and gumbel subset sampling. ||| 15535 ||| 817 ||| 8832 ||| 19341 ||| 19342 ||| 19343 ||| 2398 ||| 
2017 ||| multi-level attention networks for visual question answering. ||| 19344 ||| 1699 ||| 2165 ||| 19345 ||| 
2021 ||| embedded discriminative attention mechanism for weakly supervised semantic segmentation. ||| 6746 ||| 19346 ||| 19347 ||| 6659 ||| 6935 ||| 19348 ||| 19349 ||| 
2017 ||| dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting. ||| 3610 ||| 6525 ||| 19350 ||| 8006 ||| 6524 ||| 
2017 ||| generating the future with adversarial transformers. ||| 19351 ||| 19352 ||| 
2020 ||| image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. ||| 19270 ||| 19271 ||| 19272 ||| 2220 ||| 17653 ||| 19353 ||| 
2021 ||| pose recognition with cascade transformers. ||| 1424 ||| 19354 ||| 586 ||| 1813 ||| 1812 ||| 1815 ||| 
2019 ||| shifting more attention to video salient object detection. ||| 1861 ||| 2444 ||| 1904 ||| 2445 ||| 
2021 ||| scaling local self-attention for parameter efficient visual backbones. ||| 2466 ||| 9433 ||| 18848 ||| 9133 ||| 19355 ||| 2467 ||| 
2021 ||| hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. ||| 19356 ||| 19357 ||| 19358 ||| 5845 ||| 7141 ||| 18823 ||| 2011 ||| 
2018 ||| harmonious attention network for person re-identification. ||| 3337 ||| 2196 ||| 19101 ||| 
2021 ||| mrscatt: a spatio-channel attention-guided network for mars rover image classification. ||| 19359 ||| 19360 ||| 17156 ||| 
2021 ||| pre-trained image processing transformer. ||| 19361 ||| 19362 ||| 19363 ||| 3156 ||| 19364 ||| 19365 ||| 19366 ||| 1688 ||| 19367 ||| 7204 ||| 
2020 ||| epipolar transformer for multi-view human pose estimation. ||| 18887 ||| 10233 ||| 18888 ||| 18889 ||| 
2021 ||| multi-attentional deepfake detection. ||| 19368 ||| 11556 ||| 2494 ||| 19369 ||| 18972 ||| 2305 ||| 
2020 ||| sketch-bert: learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. ||| 19370 ||| 6365 ||| 4970 ||| 17842 ||| 
2021 ||| mist: multiple instance spatial transformer. ||| 19371 ||| 19372 ||| 9371 ||| 2555 ||| 2556 ||| 
2019 ||| attention based glaucoma detection: a large-scale database and cnn model. ||| 19373 ||| 18741 ||| 12608 ||| 19374 ||| 19375 ||| 
2019 ||| event-based attention and tracking on neuromorphic hardware. ||| 10093 ||| 10094 ||| 10096 ||| 
2019 ||| arbitrary style transfer with style-attentional networks. ||| 19376 ||| 19377 ||| 
2019 ||| predicting methylation from sequence and gene expression using deep learning with attention. ||| 19378 ||| 19379 ||| 19380 ||| 19381 ||| 
2021 ||| diverse image inpainting with bidirectional and autoregressive transformers. ||| 19382 ||| 19383 ||| 19384 ||| 19385 ||| 19386 ||| 7406 ||| 19387 ||| 18911 ||| 16696 ||| 
2021 ||| multimodal sentiment analysis based on recurrent neural network and multimodal attention. ||| 14379 ||| 16295 ||| 12040 ||| 6227 ||| 2304 ||| 12041 ||| 19388 ||| 19389 ||| 
2018 ||| multi-scale context attention network for image retrieval. ||| 19390 ||| 11448 ||| 19391 ||| 18938 ||| 
2018 ||| csan: contextual self-attention network for user sequential recommendation. ||| 19392 ||| 1174 ||| 1173 ||| 19393 ||| 1175 ||| 
2021 ||| fingerspelling recognition in the wild with fixed-query based visual attention. ||| 19394 ||| 19395 ||| 19396 ||| 19397 ||| 19398 ||| 
2020 ||| fine-grained iterative attention network for temporal language localization in videos. ||| 19399 ||| 19400 ||| 19401 ||| 2045 ||| 12376 ||| 12300 ||| 19402 ||| 
2021 ||| video background music generation with controllable music transformer. ||| 19403 ||| 19404 ||| 17854 ||| 17852 ||| 19405 ||| 19406 ||| 19407 ||| 1728 ||| 
2021 ||| gccn: geometric constraint co-attention network for 6d object pose estimation. ||| 19408 ||| 19409 ||| 19410 ||| 19411 ||| 19412 ||| 
2019 ||| cra-net: composed relation attention network for visual question answering. ||| 275 ||| 11466 ||| 369 ||| 19413 ||| 19044 ||| 
2018 ||| attention-based multi-patch aggregation for image aesthetic assessment. ||| 19414 ||| 19415 ||| 19416 ||| 19417 ||| 2382 ||| 19418 ||| 
2020 ||| multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism. ||| 12040 ||| 6227 ||| 12041 ||| 2304 ||| 12759 ||| 
2021 ||| tsa-net: tube self-attention network for action quality assessment. ||| 19419 ||| 19420 ||| 19421 ||| 19422 ||| 19423 ||| 
2021 ||| attention-driven graph clustering network. ||| 19424 ||| 17677 ||| 19425 ||| 19426 ||| 
2017 ||| video description with spatial-temporal attention. ||| 19427 ||| 19428 ||| 19429 ||| 17648 ||| 
2019 ||| small and dense commodity object detection with multi-scale receptive field attention. ||| 2276 ||| 19430 ||| 2277 ||| 2279 ||| 
2021 ||| underwater species detection using channel sharpening attention. ||| 19431 ||| 9149 ||| 19432 ||| 19433 ||| 16550 ||| 8482 ||| 19434 ||| 19435 ||| 19436 ||| 19437 ||| 
2021 ||| vehicle counting network with attention-based mask refinement and spatial-awareness block loss. ||| 1422 ||| 19438 ||| 19413 ||| 3337 ||| 
2021 ||| video relation detection via tracklet based visual transformer. ||| 19439 ||| 9570 ||| 17744 ||| 7652 ||| 
2021 ||| simullr: simultaneous lip reading transducer with attention-guided adaptive memory. ||| 19440 ||| 1306 ||| 19441 ||| 19442 ||| 5475 ||| 3634 ||| 2359 ||| 
2018 ||| your attention is unique: detecting 360-degree video saliency in head-mounted display for head movement prediction. ||| 19443 ||| 19444 ||| 19445 ||| 
2019 ||| multi-level attention network using text, audio and video for depression prediction. ||| 19446 ||| 19447 ||| 19448 ||| 17364 ||| 19449 ||| 
2019 ||| pdanet: polarity-consistent deep attention network for fine-grained visual emotion regression. ||| 1831 ||| 19450 ||| 10064 ||| 7760 ||| 10065 ||| 2596 ||| 
2021 ||| long short-term convolutional transformer for no-reference video quality assessment. ||| 11247 ||| 
2018 ||| twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm. ||| 8946 ||| 1036 ||| 1296 ||| 2166 ||| 
2020 ||| object-level attention for aesthetic rating distribution prediction. ||| 19451 ||| 19452 ||| 19453 ||| 
2018 ||| facial expression recognition in the wild: a cycle-consistent adversarial attention transfer approach. ||| 19454 ||| 18733 ||| 14649 ||| 19455 ||| 1175 ||| 
2021 ||| distributed attention for grounded image captioning. ||| 19456 ||| 2396 ||| 19457 ||| 14066 ||| 19458 ||| 19459 ||| 19460 ||| 2381 ||| 2382 ||| 19461 ||| 
2020 ||| one-shot text field labeling using attention and belief propagation for structure information extraction. ||| 19462 ||| 1240 ||| 19463 ||| 1244 ||| 19464 ||| 
2020 ||| attention based dual branches fingertip detection network and virtual key system. ||| 12195 ||| 1340 ||| 
2017 ||| learning deep contextual attention network for narrative photo stream captioning. ||| 19465 ||| 17723 ||| 16591 ||| 2165 ||| 7654 ||| 2258 ||| 
2019 ||| fine-grained cross-media representation learning with deep quantization attention network. ||| 16943 ||| 7688 ||| 17651 ||| 7689 ||| 16442 ||| 19466 ||| 
2021 ||| m3tr: multi-modal multi-label recognition with transformer. ||| 2378 ||| 2380 ||| 2383 ||| 
2020 ||| privacy-preserving visual content tagging using graph transformer networks. ||| 17820 ||| 17821 ||| 19467 ||| 19468 ||| 17819 ||| 
2020 ||| cross-modal non-linear guided attention and temporal coherence in multi-modal deep video models. ||| 19469 ||| 19470 ||| 19471 ||| 14606 ||| 
2019 ||| critic-based attention network for event-based video captioning. ||| 3924 ||| 3925 ||| 
2017 ||| attention transfer from web images for video recognition. ||| 19472 ||| 19473 ||| 1872 ||| 19019 ||| 
2018 ||| content-based video relevance prediction with second-order relevance and attention modeling. ||| 19474 ||| 8164 ||| 19475 ||| 8709 ||| 8710 ||| 
2017 ||| watch what you just said: image captioning with text-conditional attention. ||| 3574 ||| 19476 ||| 19477 ||| 7342 ||| 
2021 ||| group-based distinctive image captioning with memory attention. ||| 19478 ||| 19479 ||| 6370 ||| 6371 ||| 
2021 ||| dpt: deformable patch-based transformer for visual recognition. ||| 19480 ||| 19481 ||| 11290 ||| 18063 ||| 19482 ||| 2077 ||| 2074 ||| 
2021 ||| recursive fusion and deformable spatiotemporal attention for video compression artifact reduction. ||| 19483 ||| 15538 ||| 2613 ||| 
2021 ||| transformer-based feature reconstruction network for robust multimodal sentiment analysis. ||| 19484 ||| 3337 ||| 623 ||| 19485 ||| 
2019 ||| l2g auto-encoder: understanding point clouds by local-to-global reconstruction with hierarchical self-attention. ||| 18229 ||| 2563 ||| 2558 ||| 2559 ||| 18230 ||| 
2017 ||| cross-domain image retrieval with attention modeling. ||| 19486 ||| 1160 ||| 149 ||| 11466 ||| 
2020 ||| cluster attention contrast for video anomaly detection. ||| 4096 ||| 4430 ||| 19487 ||| 
2019 ||| aberrance-aware gradient-sensitive attentions for scene recognition with rgb-d videos. ||| 19488 ||| 19489 ||| 19490 ||| 19491 ||| 
2020 ||| hierarchical gumbel attention network for text-based person search. ||| 19492 ||| 17651 ||| 9614 ||| 8710 ||| 2165 ||| 
2020 ||| beyond the attention: distinguish the discriminative and confusable features for fine-grained image classification. ||| 19493 ||| 19494 ||| 822 ||| 19495 ||| 19496 ||| 17651 ||| 
2020 ||| compact bilinear augmented query structured attention for sport highlights classification. ||| 19497 ||| 3386 ||| 19498 ||| 1073 ||| 19499 ||| 
2020 ||| hot-net: non-autoregressive transformer for 3d hand-object pose estimation. ||| 8544 ||| 8545 ||| 19500 ||| 3478 ||| 8546 ||| 
2019 ||| attention transfer (ant) network for view-invariant action recognition. ||| 18992 ||| 19501 ||| 11466 ||| 13410 ||| 1040 ||| 19502 ||| 
2019 ||| fine-grained fitting experience prediction: a 3d-slicing attention approach. ||| 15534 ||| 11800 ||| 19503 ||| 3614 ||| 19504 ||| 
2020 ||| guided attention network for object detection and counting on drones. ||| 19505 ||| 8749 ||| 8750 ||| 8751 ||| 19506 ||| 8753 ||| 15557 ||| 
2021 ||| yes, "attention is all you need", for exemplar based colorization. ||| 19507 ||| 19508 ||| 19509 ||| 17378 ||| 
2017 ||| image caption with synchronous cross-attention. ||| 7400 ||| 19510 ||| 13160 ||| 
2020 ||| multimodal attention with image text spatial relationship for ocr-based image captioning. ||| 4550 ||| 8536 ||| 2166 ||| 
2020 ||| attention cube network for image restoration. ||| 19511 ||| 6117 ||| 6116 ||| 19512 ||| 1921 ||| 
2021 ||| multiview detection with shadow transformer (and view-coherent data augmentation). ||| 19513 ||| 8571 ||| 
2019 ||| self-attention and ingredient-attention based model for recipe retrieval from image queries. ||| 19514 ||| 19515 ||| 19516 ||| 
2021 ||| attention-guided temporally coherent video object matting. ||| 19517 ||| 4171 ||| 19518 ||| 19519 ||| 18911 ||| 2490 ||| 19081 ||| 19520 ||| 19521 ||| 
2020 ||| deeprhythm: exposing deepfakes with attentional visual heartbeat rhythms. ||| 19522 ||| 18280 ||| 19523 ||| 19524 ||| 6805 ||| 12081 ||| 1305 ||| 19525 ||| 
2021 ||| multi-label pattern image retrieval via attention mechanism driven graph convolutional network. ||| 949 ||| 19526 ||| 19527 ||| 19528 ||| 
2021 ||| combining attention with flow for person image synthesis. ||| 19529 ||| 19530 ||| 12218 ||| 2063 ||| 2064 ||| 
2019 ||| gastrointestinal tract diseases detection with deep attention neural network. ||| 4219 ||| 4220 ||| 19531 ||| 4221 ||| 
2020 ||| curriculum learning for wide multimedia-based transformer with graph target detection. ||| 19532 ||| 19533 ||| 19534 ||| 19535 ||| 3049 ||| 3475 ||| 16829 ||| 9569 ||| 19536 ||| 247 ||| 
2019 ||| action recognition with bootstrapping based long-range temporal context attention. ||| 6910 ||| 19347 ||| 19537 ||| 6746 ||| 19349 ||| 
2021 ||| cascade cross-modal attention network for video actor and action segmentation from a sentence. ||| 19538 ||| 19539 ||| 19540 ||| 19541 ||| 13823 ||| 13825 ||| 
2017 ||| manet: a modal attention network for describing videos. ||| 19542 ||| 19543 ||| 5874 ||| 
2017 ||| learning multimodal attention lstm networks for video captioning. ||| 2013 ||| 19117 ||| 17860 ||| 2165 ||| 
2020 ||| multimodal deep learning for social media popularity prediction with attention mechanism. ||| 362 ||| 19544 ||| 19545 ||| 19546 ||| 5773 ||| 19547 ||| 
2021 ||| pixel-wise graph attention networks for person re-identification. ||| 19548 ||| 19549 ||| 19550 ||| 1839 ||| 19551 ||| 
2020 ||| transformer-based label set generation for multi-modal multi-label emotion detection. ||| 19552 ||| 1751 ||| 3564 ||| 3088 ||| 
2021 ||| few-shot fine-grained action recognition via bidirectional attention and contrastive meta-learning. ||| 19553 ||| 5705 ||| 2794 ||| 10426 ||| 
2021 ||| former-dfer: dynamic facial expression recognition transformer. ||| 19554 ||| 6625 ||| 
2020 ||| pay attention selectively and comprehensively: pyramid gating network for human pose estimation without pre-training. ||| 19555 ||| 4776 ||| 19556 ||| 19557 ||| 19558 ||| 
2020 ||| multi-group multi-attention: towards discriminative spatiotemporal representation. ||| 19559 ||| 19560 ||| 19561 ||| 19562 ||| 19563 ||| 2626 ||| 2625 ||| 2627 ||| 
2020 ||| unsupervised representation learning with attention and sequence to sequence autoencoders to predict sleepiness from speech. ||| 4028 ||| 19564 ||| 19565 ||| 4029 ||| 4027 ||| 648 ||| 649 ||| 
2018 ||| examine before you answer: multi-task learning with adaptive-attentions for multiple-choice vqa. ||| 1039 ||| 17649 ||| 9576 ||| 17695 ||| 1040 ||| 
2020 ||| au-assisted graph attention convolutional network for micro-expression recognition. ||| 19566 ||| 1817 ||| 1819 ||| 1820 ||| 
2019 ||| audiovisual transformer architectures for large-scale classification and synchronization of weakly labeled audio events. ||| 19567 ||| 19568 ||| 
2021 ||| stst: spatial-temporal specialized transformer for skeleton-based action recognition. ||| 5982 ||| 11346 ||| 10473 ||| 19569 ||| 2190 ||| 
2020 ||| sequential attention gan for interactive image editing. ||| 2045 ||| 2044 ||| 19570 ||| 2046 ||| 1958 ||| 
2020 ||| hybrid dynamic-static context-aware attention network for action assessment in long videos. ||| 19571 ||| 19572 ||| 14892 ||| 19573 ||| 19482 ||| 19574 ||| 19575 ||| 
2017 ||| multi-scale context based attention for dynamic music emotion prediction. ||| 19576 ||| 19577 ||| 4547 ||| 4459 ||| 19578 ||| 
2021 ||| convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. ||| 19579 ||| 9738 ||| 19580 ||| 9740 ||| 9735 ||| 1159 ||| 
2018 ||| new feature-level video classification via temporal attention model. ||| 7949 ||| 7950 ||| 19581 ||| 19582 ||| 19583 ||| 7951 ||| 
2019 ||| multi-modal knowledge-aware hierarchical attention network for explainable medical question answering. ||| 17979 ||| 1174 ||| 1173 ||| 1175 ||| 
2020 ||| codan: counting-driven attention network for vehicle detection in congested scenes. ||| 3337 ||| 19584 ||| 19413 ||| 1422 ||| 19585 ||| 7778 ||| 
2020 ||| perceptual characterization of 3d graphical contents based on attention complexity measures. ||| 11608 ||| 11610 ||| 11611 ||| 
2021 ||| direction relation transformer for image captioning. ||| 17937 ||| 17663 ||| 19586 ||| 4237 ||| 4191 ||| 
2020 ||| jointly cross- and self-modal graph attention network for query-based moment localization. ||| 19587 ||| 19399 ||| 19588 ||| 12376 ||| 12300 ||| 19402 ||| 
2021 ||| progressive graph attention network for video question answering. ||| 275 ||| 19589 ||| 19590 ||| 18691 ||| 
2020 ||| query twice: dual mixture attention meta learning for video summarization. ||| 19591 ||| 1281 ||| 18907 ||| 19592 ||| 6660 ||| 19593 ||| 6935 ||| 
2021 ||| dual graph convolutional networks with transformer and curriculum learning for image captioning. ||| 19594 ||| 7175 ||| 19595 ||| 19227 ||| 
2021 ||| multimodal video summarization via time-aware transformers. ||| 19596 ||| 8725 ||| 19597 ||| 8727 ||| 
2017 ||| modeling image virality with pairwise spatial transformer networks. ||| 19598 ||| 19599 ||| 
2018 ||| object-difference attention: a simple relational attention for visual question answering. ||| 19600 ||| 19510 ||| 13160 ||| 12711 ||| 
2019 ||| ingredient-guided cascaded multi-attention network for food recognition. ||| 19601 ||| 19602 ||| 19603 ||| 19491 ||| 
2017 ||| the role of visual attention in sentiment prediction. ||| 19098 ||| 1871 ||| 19099 ||| 19290 ||| 19019 ||| 1872 ||| 
2021 ||| end-to-end video object detection with spatial-temporal transformers. ||| 19604 ||| 19605 ||| 19606 ||| 5948 ||| 19607 ||| 6821 ||| 8374 ||| 18501 ||| 5141 ||| 5088 ||| 
2019 ||| understanding the teaching styles by an attention based multi-task cross-media dimensional modeling. ||| 19608 ||| 4459 ||| 19609 ||| 2008 ||| 19610 ||| 4646 ||| 19611 ||| 19612 ||| 4648 ||| 19613 ||| 
2020 ||| alanet: adaptive latent attention network for joint video deblurring and interpolation. ||| 19614 ||| 19615 ||| 19616 ||| 
2021 ||| ftaface: context-enhanced face detector with fine-grained task attention. ||| 19617 ||| 19618 ||| 19619 ||| 19620 ||| 19621 ||| 19622 ||| 19623 ||| 19624 ||| 
2021 ||| sensor-augmented egocentric-video captioning with dynamic modal attention. ||| 19625 ||| 13245 ||| 19626 ||| 
2019 ||| deep adversarial graph attention convolution network for text-based person search. ||| 9614 ||| 8710 ||| 5946 ||| 444 ||| 17860 ||| 
2017 ||| video question answering via hierarchical dual-level attention network learning. ||| 1306 ||| 19627 ||| 19628 ||| 1115 ||| 2359 ||| 7654 ||| 
2021 ||| multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model. ||| 12040 ||| 19388 ||| 6227 ||| 2304 ||| 12041 ||| 444 ||| 7385 ||| 
2019 ||| erasing-based attention learning for visual question answering. ||| 523 ||| 2058 ||| 5946 ||| 2080 ||| 
2019 ||| joint-attention discriminator for accurate super-resolution via adversarial training. ||| 19629 ||| 12120 ||| 19630 ||| 19631 ||| 19632 ||| 
2021 ||| latent memory-augmented graph transformer for visual storytelling. ||| 19633 ||| 12650 ||| 5704 ||| 19634 ||| 208 ||| 2166 ||| 
2021 ||| exploring sequence feature alignment for domain adaptive detection transformers. ||| 8948 ||| 1903 ||| 875 ||| 19635 ||| 8710 ||| 19636 ||| 1756 ||| 
2020 ||| asta-net: adaptive spatio-temporal attention network for person re-identification in videos. ||| 19637 ||| 9614 ||| 19638 ||| 444 ||| 8710 ||| 
2018 ||| regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval. ||| 5910 ||| 4267 ||| 19639 ||| 19640 ||| 
2019 ||| visual relation detection with multi-level attention. ||| 19641 ||| 19642 ||| 19643 ||| 
2021 ||| anchor-free 3d single stage detector with mask-guided attention for point cloud. ||| 19644 ||| 8582 ||| 1932 ||| 8583 ||| 
2019 ||| dadnet: dilated-attention-deformable convnet for crowd counting. ||| 19645 ||| 8293 ||| 8710 ||| 444 ||| 
2020 ||| joint self-attention and scale-aggregation for self-calibrated deraining network. ||| 778 ||| 19646 ||| 19647 ||| 19648 ||| 
2017 ||| temporally selective attention model for social and affective state recognition in multimedia content. ||| 19649 ||| 19650 ||| 19651 ||| 19652 ||| 19653 ||| 3601 ||| 
2021 ||| spatio-temporal convolutional attention network for spotting macro- and micro-expression intervals. ||| 19654 ||| 19655 ||| 13332 ||| 
2018 ||| visual spatial attention network for relationship detection. ||| 19656 ||| 1038 ||| 2014 ||| 11466 ||| 1040 ||| 
2019 ||| pedestrian attribute recognition via hierarchical multi-task learning and relationship attention. ||| 19657 ||| 5704 ||| 8972 ||| 5705 ||| 
2021 ||| multifocal attention-based cross-scale network for image de-raining. ||| 19658 ||| 19659 ||| 19660 ||| 2207 ||| 8710 ||| 8711 ||| 
2021 ||| heterogeneous face recognition with attention-guided feature disentangling. ||| 19661 ||| 2985 ||| 16933 ||| 15367 ||| 340 ||| 18083 ||| 
2021 ||| tritransnet: rgb-d salient object detection with a triplet transformer embedding network. ||| 19662 ||| 4715 ||| 19663 ||| 9670 ||| 19664 ||| 
2020 ||| multi-scale generalized attention-based regional maximum activation of convolutions for beauty product retrieval. ||| 362 ||| 19665 ||| 19666 ||| 19545 ||| 19547 ||| 19667 ||| 
2019 ||| multi-level fusion based class-aware attention model for weakly labeled audio tagging. ||| 13814 ||| 19668 ||| 19669 ||| 19670 ||| 12550 ||| 8364 ||| 
2021 ||| information-growth attention network for image super-resolution. ||| 17584 ||| 2064 ||| 12218 ||| 2063 ||| 1310 ||| 
2021 ||| pre-training graph transformer with multimodal side information for recommendation. ||| 4297 ||| 19671 ||| 9002 ||| 4881 ||| 19672 ||| 1692 ||| 1397 ||| 16696 ||| 
2021 ||| token shift transformer for video classification. ||| 3386 ||| 19497 ||| 19498 ||| 
2021 ||| rams-trans: recurrent attention multi-scale transformer for fine-grained image recognition. ||| 19673 ||| 19674 ||| 16591 ||| 19675 ||| 19676 ||| 3001 ||| 12377 ||| 
2020 ||| pop music transformer: beat-based modeling and generation of expressive pop piano compositions. ||| 4372 ||| 4374 ||| 
2020 ||| exploring language prior for mode-sensitive visual attention modeling. ||| 2504 ||| 18919 ||| 17660 ||| 6831 ||| 2382 ||| 2367 ||| 
2021 ||| position-augmented transformers with entity-aligned mesh for textvqa. ||| 19677 ||| 17441 ||| 
2021 ||| unifying multimodal transformer for bi-directional image and text generation. ||| 19678 ||| 19679 ||| 19680 ||| 16594 ||| 
2020 ||| dual-view attention networks for single image super-resolution. ||| 19681 ||| 19682 ||| 1134 ||| 19683 ||| 19684 ||| 
2020 ||| sst-emotionnet: spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition. ||| 18571 ||| 17998 ||| 19685 ||| 19686 ||| 19687 ||| 4550 ||| 
2021 ||| group-level focus of visual attention for improved next speaker prediction. ||| 19688 ||| 19689 ||| 4588 ||| 
2021 ||| hat: hierarchical aggregation transformers for person re-identification. ||| 19690 ||| 19691 ||| 2038 ||| 1700 ||| 
2019 ||| what i see is what you see: joint attention learning for first and third person video co-analysis. ||| 19692 ||| 7893 ||| 9994 ||| 13639 ||| 
2021 ||| a transformer based approach for image manipulation chain detection. ||| 19693 ||| 19694 ||| 19695 ||| 19696 ||| 19697 ||| 2514 ||| 
2021 ||| video semantic segmentation via sparse temporal transformer. ||| 3110 ||| 14707 ||| 927 ||| 5948 ||| 19205 ||| 18226 ||| 5088 ||| 
2021 ||| knowing when to quit: selective cascaded regression with patch attention for real-time face alignment. ||| 19698 ||| 19699 ||| 19700 ||| 19701 ||| 
2018 ||| attention and language ensemble for scene text recognition with convolutional sequence modeling. ||| 19702 ||| 18071 ||| 8710 ||| 19703 ||| 4237 ||| 17860 ||| 
2020 ||| look, listen, and attend: co-attention network for self-supervised audio-visual representation learning. ||| 19704 ||| 19705 ||| 19706 ||| 580 ||| 9689 ||| 
2020 ||| cascade grouped attention network for referring expression segmentation. ||| 17674 ||| 2501 ||| 2367 ||| 2504 ||| 3182 ||| 17661 ||| 2398 ||| 
2020 ||| attention-driven unsupervised image retrieval for beauty products with visual and textual clues. ||| 19451 ||| 19707 ||| 19708 ||| 
2021 ||| hda-net: horizontal deformable attention network for stereo matching. ||| 336 ||| 19709 ||| 19710 ||| 2957 ||| 19711 ||| 
2021 ||| video transformer for deepfake detection with incremental learning. ||| 19712 ||| 8582 ||| 
2020 ||| single image deraining via scale-space invariant attention neural network. ||| 4900 ||| 12057 ||| 12056 ||| 12058 ||| 
2019 ||| focus your attention: a bidirectional focal attention network for image-text matching. ||| 2290 ||| 5963 ||| 19713 ||| 18733 ||| 379 ||| 17860 ||| 
2021 ||| learning hierarchal channel attention for fine-grained visual classification. ||| 19714 ||| 18691 ||| 9579 ||| 19590 ||| 
2020 ||| cf-sis: semantic-instance segmentation of 3d point clouds by context fusion with self-attention. ||| 2558 ||| 2563 ||| 19715 ||| 2559 ||| 
2021 ||| mix-order attention networks for image restoration. ||| 7895 ||| 12718 ||| 12719 ||| 11800 ||| 19716 ||| 7897 ||| 
2021 ||| structext: structured text understanding with multi-modal transformers. ||| 5780 ||| 19717 ||| 18904 ||| 17413 ||| 19718 ||| 9337 ||| 19719 ||| 2538 ||| 2536 ||| 1761 ||| 
2021 ||| zero-shot video emotion recognition via multimodal protagonist-aware transformer network. ||| 19720 ||| 19721 ||| 1175 ||| 
2017 ||| learning social image embedding with deep multimodal attention networks. ||| 19722 ||| 5320 ||| 843 ||| 2165 ||| 19723 ||| 19724 ||| 
2020 ||| unpaired image enhancement with quality-attention generative adversarial network. ||| 19725 ||| 19726 ||| 19391 ||| 19727 ||| 19728 ||| 
2020 ||| isia food-500: a dataset for large-scale food recognition via stacked global-local attention network. ||| 19601 ||| 19602 ||| 19729 ||| 19603 ||| 6659 ||| 6935 ||| 19491 ||| 
2018 ||| net: contextual-attentional attribute-appearance network for person re-identification. ||| 9614 ||| 8710 ||| 18071 ||| 2207 ||| 17860 ||| 
2018 ||| audio-visual attention networks for emotion recognition. ||| 12629 ||| 12630 ||| 8797 ||| 1515 ||| 
2019 ||| linestofacephoto: face photo generation from lines with conditional self-attention generative adversarial networks. ||| 19730 ||| 17859 ||| 8711 ||| 8710 ||| 
2019 ||| beauty product retrieval based on regional maximum activation of convolutions with generalized attention. ||| 1754 ||| 19731 ||| 19732 ||| 19733 ||| 18073 ||| 
2021 ||| transrefer3d: entity-and-relation aware transformer for fine-grained 3d visual grounding. ||| 19734 ||| 19735 ||| 1069 ||| 19736 ||| 19737 ||| 19738 ||| 17854 ||| 
2017 ||| video question answering via gradually refined attention over appearance and motion. ||| 19739 ||| 1306 ||| 7652 ||| 2258 ||| 2484 ||| 1063 ||| 7654 ||| 
2017 ||| unconstrained fashion landmark detection via hierarchical recurrent transformer networks. ||| 19740 ||| 2498 ||| 2011 ||| 13619 ||| 1846 ||| 18791 ||| 
2020 ||| light field super-resolution via attention-guided fusion of hybrid lenses. ||| 10406 ||| 19426 ||| 1037 ||| 19728 ||| 19741 ||| 
2021 ||| svhan: sequential view based hierarchical attention network for 3d shape recognition. ||| 3226 ||| 19742 ||| 19713 ||| 13208 ||| 19743 ||| 
2018 ||| attribute-aware attention model for fine-grained representation learning. ||| 19744 ||| 19745 ||| 8862 ||| 19746 ||| 
2021 ||| image search with text feedback by deep hierarchical attention mutual information maximization. ||| 19747 ||| 19748 ||| 19749 ||| 19750 ||| 19751 ||| 1160 ||| 
2021 ||| uacanet: uncertainty augmented context attention for polyp segmentation. ||| 11462 ||| 19752 ||| 2178 ||| 
2021 ||| pose-guided inter- and intra-part relational transformer for occluded person re-identification. ||| 19753 ||| 2380 ||| 2383 ||| 
2018 ||| temporal hierarchical attention at category- and item-level for micro-video click-through prediction. ||| 19474 ||| 8709 ||| 8710 ||| 1806 ||| 2207 ||| 185 ||| 
2019 ||| an attentional-lstm for improved classification of brain activities evoked by images. ||| 6312 ||| 19754 ||| 19755 ||| 
2020 ||| dual attention gans for semantic image synthesis. ||| 435 ||| 2083 ||| 437 ||| 
2021 ||| face hallucination via split-attention in split-attention network. ||| 19756 ||| 19757 ||| 19758 ||| 3906 ||| 683 ||| 2146 ||| 12056 ||| 
2020 ||| context-aware attention network for predicting image aesthetic subjectivity. ||| 19759 ||| 19760 ||| 19529 ||| 2063 ||| 2064 ||| 
2019 ||| attention-based densely connected lstm for video captioning. ||| 19761 ||| 19491 ||| 
2018 ||| learning joint multimodal representation with adversarial attention networks. ||| 19722 ||| 5320 ||| 843 ||| 
2021 ||| scene text image super-resolution via parallelly contextual attention network. ||| 19762 ||| 19763 ||| 19764 ||| 19765 ||| 1035 ||| 1038 ||| 1040 ||| 
2021 ||| learning contextual transformer network for image inpainting. ||| 19766 ||| 19767 ||| 2354 ||| 8850 ||| 2357 ||| 
2018 ||| attention-based pyramid aggregation network for visual place recognition. ||| 19768 ||| 19769 ||| 2477 ||| 8571 ||| 
2017 ||| generative attention model with adversarial self-learning for visual question answering. ||| 19770 ||| 1685 ||| 
2018 ||| mining semantics-preserving attention for group activity recognition. ||| 19771 ||| 19772 ||| 19773 ||| 1920 ||| 19774 ||| 1921 ||| 
2020 ||| attention based beauty product retrieval using global and local descriptors. ||| 1754 ||| 19731 ||| 19732 ||| 19733 ||| 19775 ||| 19776 ||| 19777 ||| 
2021 ||| doctr: document image transformer for geometric unwarping and illumination correction. ||| 2078 ||| 19778 ||| 1806 ||| 2415 ||| 1807 ||| 
2019 ||| bert4sessrec: content-based video relevance prediction with bidirectional encoder representations from transformer. ||| 19474 ||| 8709 ||| 9002 ||| 8207 ||| 8710 ||| 2207 ||| 
2020 ||| desmoothgan: recovering details of smoothed images via spatial feature-wise transformation and full attention. ||| 6419 ||| 735 ||| 19779 ||| 2495 ||| 19780 ||| 734 ||| 
2020 ||| a structured graph attention network for vehicle re-identification. ||| 19781 ||| 8710 ||| 18733 ||| 9614 ||| 2166 ||| 
2019 ||| learning fragment self-attention embeddings for image-text matching. ||| 19782 ||| 13823 ||| 13221 ||| 13825 ||| 
2021 ||| hierarchical multi-task learning for diagram question answering with multi-modal transformer. ||| 19783 ||| 14791 ||| 19413 ||| 1175 ||| 
2020 ||| occluded prohibited items detection: an x-ray security inspection benchmark and de-occlusion attention module. ||| 19784 ||| 19785 ||| 19786 ||| 19787 ||| 8750 ||| 17695 ||| 
2020 ||| deep learning-based person search with visual attention embedding. ||| 19788 ||| 19789 ||| 19790 ||| 19791 ||| 19792 ||| 
2020 ||| fused recurrent network via channel attention for remote sensing satellite image super-resolution. ||| 19793 ||| 19794 ||| 19795 ||| 19796 ||| 155 ||| 
2021 ||| hierarchical transformer: unsupervised representation learning for skeleton-based human action recognition. ||| 13235 ||| 13236 ||| 19640 ||| 19797 ||| 8977 ||| 2315 ||| 
2021 ||| non-local attention learning for medical image classification. ||| 14034 ||| 16628 ||| 19798 ||| 19799 ||| 12866 ||| 9886 ||| 718 ||| 
2021 ||| multimodal transformer networks with latent interaction for audio-visual event localization. ||| 19800 ||| 9579 ||| 189 ||| 19801 ||| 13431 ||| 
2021 ||| pyramid orthogonal attention network based on dual self-similarity for accurate mr image super-resolution. ||| 19802 ||| 19803 ||| 19804 ||| 19805 ||| 1730 ||| 
2020 ||| spanet: spatial pyramid attention network for enhanced image recognition. ||| 19806 ||| 19807 ||| 19808 ||| 19809 ||| 19810 ||| 6415 ||| 19811 ||| 17441 ||| 19812 ||| 
2021 ||| astm: an attention based spatiotemporal model for video prediction using 3d convolutional neural networks. ||| 19813 ||| 19540 ||| 19814 ||| 19366 ||| 19815 ||| 7204 ||| 
2017 ||| learning attentional recurrent neural network for visual tracking. ||| 19816 ||| 1280 ||| 17869 ||| 
2021 ||| depth-guided adain and shift attention network for vision-and-language navigation. ||| 4103 ||| 19817 ||| 19818 ||| 6365 ||| 4970 ||| 
2019 ||| multimodal semantic attention network for video captioning. ||| 19819 ||| 8838 ||| 8837 ||| 19820 ||| 8841 ||| 
2021 ||| driving video fixation prediction model via spatio-temporal networks and attention gates. ||| 19821 ||| 19822 ||| 19823 ||| 
2019 ||| locality-constrained spatial transformer network for video crowd counting. ||| 19824 ||| 19825 ||| 19826 ||| 19827 ||| 19828 ||| 
2019 ||| personalized image recommendation with photo importance and user-item interactive attention. ||| 19829 ||| 19830 ||| 1978 ||| 
2019 ||| video prediction with temporal-spatial attention mechanism and deep perceptual similarity branch. ||| 5993 ||| 2200 ||| 19831 ||| 19832 ||| 
2019 ||| attention based semi-supervised dictionary learning for diagnosis of autism spectrum disorders. ||| 5395 ||| 19833 ||| 6469 ||| 19834 ||| 6582 ||| 
2021 ||| truth inference with bipartite attention graph neural network from a comprehensive view. ||| 17814 ||| 17816 ||| 19835 ||| 
2021 ||| graph attention-based deep neural network for 3d point cloud processing. ||| 19836 ||| 19837 ||| 6915 ||| 19838 ||| 5017 ||| 
2019 ||| deep color image demosaicking with feature pyramid channel attention. ||| 19839 ||| 1714 ||| 19840 ||| 
2021 ||| pyramid feature attention network for monocular depth prediction. ||| 5296 ||| 16939 ||| 765 ||| 438 ||| 16940 ||| 
2021 ||| stereo superpixel segmentation via dual-attention fusion networks. ||| 19841 ||| 19842 ||| 19843 ||| 19844 ||| 
2019 ||| audio2face: generating speech/face animation from single audio with attention-based bidirectional lstm networks. ||| 19845 ||| 19846 ||| 4297 ||| 
2021 ||| star-net: spatial-temporal attention residual network for video deraining. ||| 11962 ||| 19847 ||| 13917 ||| 19435 ||| 8482 ||| 19848 ||| 
2019 ||| multi-modal language analysis with hierarchical interaction-level and selection-level attentions. ||| 1751 ||| 19849 ||| 3085 ||| 222 ||| 3088 ||| 
2021 ||| multi-scale attention constraint network for fine-grained visual classification. ||| 815 ||| 19850 ||| 816 ||| 19851 ||| 817 ||| 19289 ||| 
2021 ||| improving convolutional networks with boosting attention convolutions. ||| 242 ||| 15309 ||| 15310 ||| 11558 ||| 683 ||| 19852 ||| 
2019 ||| channel-wise temporal attention network for video action recognition. ||| 19853 ||| 19854 ||| 19855 ||| 13825 ||| 
2021 ||| relationship-aware primal-dual graph attention network for scene graph generation. ||| 2736 ||| 19856 ||| 1796 ||| 1101 ||| 13240 ||| 
2019 ||| learning recurrent structure-guided attention network for multi-person pose estimation. ||| 19857 ||| 19858 ||| 1699 ||| 17609 ||| 
2021 ||| attention-guided knowledge distillation for efficient single-stage detector. ||| 6669 ||| 19481 ||| 11290 ||| 7662 ||| 2077 ||| 2074 ||| 
2021 ||| depth super-resolution by texture-depth transformer. ||| 19859 ||| 19860 ||| 19861 ||| 19862 ||| 19863 ||| 
2021 ||| learning outfit compatibility with graph attention network and visual-semantic embedding. ||| 292 ||| 291 ||| 19864 ||| 2529 ||| 
2020 ||| attention-based network for low-light image enhancement. ||| 3761 ||| 19015 ||| 1107 ||| 19865 ||| 19866 ||| 10075 ||| 
2021 ||| qau-net: quartet attention u-net for liver and liver-tumor segmentation. ||| 19867 ||| 19868 ||| 19869 ||| 19870 ||| 19871 ||| 
2021 ||| graph attention neural network for image restoration. ||| 12195 ||| 12196 ||| 
2021 ||| supervised video summarization via multiple feature sets with parallel attention. ||| 19872 ||| 10688 ||| 10689 ||| 
2019 ||| herding effect based attention for personalized time-sync video recommendation. ||| 1437 ||| 1439 ||| 1440 ||| 1438 ||| 19873 ||| 1441 ||| 
2019 ||| convolutional temporal attention model for video-based person re-identification. ||| 18747 ||| 18831 ||| 602 ||| 
2019 ||| residual dilated network with attention for image blind denoising. ||| 7993 ||| 3177 ||| 8688 ||| 
2021 ||| vanet: a view attention guided network for 3d reconstruction from single and multi-view images. ||| 19846 ||| 19874 ||| 2185 ||| 
2020 ||| self-adaptive embedding for few-shot classification by hierarchical attention. ||| 19875 ||| 8711 ||| 5894 ||| 
2021 ||| a novel attention enhanced residual-in-residual dense network for text image super-resolution. ||| 19876 ||| 12360 ||| 19877 ||| 173 ||| 
2019 ||| graph attention neural networks for point cloud recognition. ||| 19878 ||| 1796 ||| 19879 ||| 19880 ||| 19881 ||| 
2021 ||| salient object detection via attention-aware cascaded bottom-up feature aggregation. ||| 19882 ||| 313 ||| 19883 ||| 19884 ||| 
2019 ||| spatial-aware non-local attention for fashion landmark detection. ||| 13786 ||| 19885 ||| 19886 ||| 19887 ||| 
2021 ||| saliency-guided complementary attention for improved few-shot learning. ||| 19888 ||| 19889 ||| 19890 ||| 3337 ||| 8726 ||| 
2019 ||| multi-level attention model with deep scattering spectrum for acoustic scene classification. ||| 19891 ||| 14737 ||| 14273 ||| 13516 ||| 14271 ||| 19892 ||| 683 ||| 
2019 ||| context attention module for human hand detection. ||| 19893 ||| 19894 ||| 19895 ||| 19896 ||| 
2020 ||| graph attention model embedded with multi-modal knowledge for depression detection. ||| 11352 ||| 11351 ||| 11353 ||| 11354 ||| 
2019 ||| self-attention relation network for few-shot learning. ||| 19897 ||| 8754 ||| 5077 ||| 19228 ||| 
2021 ||| multiple hub-driven attention graph network for scene graph generation. ||| 19610 ||| 19898 ||| 
2019 ||| attentiondrop for convolutional neural networks. ||| 19899 ||| 6729 ||| 19900 ||| 19901 ||| 7895 ||| 7897 ||| 
2020 ||| ransp: ranking attention network for saliency prediction on omnidirectional images. ||| 19902 ||| 19903 ||| 19904 ||| 19905 ||| 19906 ||| 19907 ||| 6516 ||| 8532 ||| 
2021 ||| cranet: cascade residual attention network for crowd counting. ||| 19908 ||| 19909 ||| 4371 ||| 1302 ||| 1900 ||| 5787 ||| 
2021 ||| a keypoint transformer to discover spine structure for cobb angle estimation. ||| 19910 ||| 19911 ||| 19082 ||| 19912 ||| 
2021 ||| multi-pretext attention network for few-shot learning with self-supervision. ||| 19913 ||| 19785 ||| 5536 ||| 19914 ||| 19915 ||| 2039 ||| 17695 ||| 
2021 ||| learning connected attentions for convolutional neural networks. ||| 19807 ||| 19806 ||| 19811 ||| 19916 ||| 6415 ||| 17441 ||| 19812 ||| 19917 ||| 17707 ||| 232 ||| 
2020 ||| double shot: preserve and erase based class attention networks for weakly supervised localization (peca-net). ||| 19918 ||| 1280 ||| 19919 ||| 3614 ||| 19920 ||| 9925 ||| 
2018 ||| refining attention: a sequential attention model for image captioning. ||| 11318 ||| 7657 ||| 7656 ||| 11319 ||| 
2021 ||| distance restricted transformer encoder for multi-label classification. ||| 8155 ||| 19921 ||| 19922 ||| 19923 ||| 6365 ||| 4970 ||| 
2017 ||| top attention in line with time: a light-weight strategy. ||| 19924 ||| 19925 ||| 7648 ||| 5077 ||| 2258 ||| 
2021 ||| multi-view face recognition using deep attention-based face frontalization. ||| 19926 ||| 11269 ||| 19927 ||| 19928 ||| 19929 ||| 11796 ||| 
2021 ||| person retrieval with conv-transformer. ||| 19930 ||| 11448 ||| 19931 ||| 19455 ||| 
2019 ||| text-attentional conditional generative adversarial network for super-resolution of text images. ||| 19932 ||| 19933 ||| 19934 ||| 
2019 ||| unsupervised monocular depth estimation based on dual attention mechanism and depth-aware loss. ||| 19935 ||| 19936 ||| 345 ||| 11962 ||| 8482 ||| 19937 ||| 19938 ||| 
2021 ||| multi-scale gated attention for weakly labelled sound event detection. ||| 19939 ||| 19940 ||| 
2019 ||| visual attention modeling for autism spectrum disorder by semantic features. ||| 19941 ||| 19942 ||| 19943 ||| 19944 ||| 
2019 ||| sequential behavior modeling for next micro-video recommendation with collaborative transformer. ||| 9060 ||| 9061 ||| 
2020 ||| saan: semantic attention adaptation network for face super-resolution. ||| 14598 ||| 4206 ||| 
2019 ||| improving human pose estimation with self-attention generative adversarial networks. ||| 19945 ||| 3049 ||| 19946 ||| 2740 ||| 19947 ||| 
2020 ||| two-way feature-aligned and attention-rectified adversarial training. ||| 19948 ||| 19949 ||| 19950 ||| 7648 ||| 19951 ||| 19952 ||| 
2017 ||| a simple method to obtain visual attention data in head mounted virtual reality. ||| 19953 ||| 19954 ||| 
2019 ||| knowledge distillation with category-aware attention and discriminant logit losses. ||| 479 ||| 1806 ||| 1807 ||| 
2019 ||| learning to segment unseen category objects using gradient gaussian attention. ||| 19955 ||| 19956 ||| 19957 ||| 19434 ||| 19958 ||| 
2020 ||| lightweight single image super-resolution through efficient second-order attention spindle network. ||| 19959 ||| 19960 ||| 8688 ||| 6116 ||| 6117 ||| 
2021 ||| residual attention block search for lightweight image super-resolution. ||| 19961 ||| 10212 ||| 10211 ||| 145 ||| 
2019 ||| low-light image enhancement with attention and multi-level feature fusion. ||| 3279 ||| 19962 ||| 11441 ||| 19963 ||| 11295 ||| 
2021 ||| self-attention recurrent summarization network with reinforcement learning for video summarization task. ||| 19964 ||| 13701 ||| 19965 ||| 19966 ||| 19967 ||| 
2021 ||| pmae: pseudo multi-label attention ensemble. ||| 4207 ||| 4208 ||| 4205 ||| 
2021 ||| attention-guided semantic hashing for unsupervised cross-modal retrieval. ||| 19968 ||| 835 ||| 19969 ||| 2014 ||| 
2020 ||| pa-ggan: session-based recommendation with position-aware gated graph attention network. ||| 19970 ||| 19971 ||| 19972 ||| 19973 ||| 19974 ||| 
2021 ||| cognition-driven real-time personality detection via language-guided contrastive visual attention. ||| 19975 ||| 3083 ||| 3085 ||| 3088 ||| 
2021 ||| spatial attention-based non-reference perceptual quality prediction network for omnidirectional images. ||| 2884 ||| 18741 ||| 19976 ||| 19977 ||| 
2017 ||| a joint model for action localization and classification in untrimmed video with visual attention. ||| 19832 ||| 2200 ||| 19831 ||| 19978 ||| 2064 ||| 
2017 ||| deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition. ||| 19979 ||| 14462 ||| 
2021 ||| spatial reasoning and context-aware attention network for skeleton-based action recognition. ||| 19980 ||| 10415 ||| 19981 ||| 19982 ||| 19983 ||| 19984 ||| 
2021 ||| attention-based relation reasoning network for video-text retrieval. ||| 19985 ||| 369 ||| 9579 ||| 1038 ||| 11466 ||| 1040 ||| 
2017 ||| scanpath mining of eye movement trajectories for visual attention analysis. ||| 11206 ||| 19986 ||| 9061 ||| 
2019 ||| towards accurate instance-level text spotting with guided attention. ||| 18952 ||| 19987 ||| 18955 ||| 
2019 ||| salient object detection via recurrently aggregating spatial attention weighted cross-level deep features. ||| 19988 ||| 19989 ||| 19990 ||| 1703 ||| 
2019 ||| multi-scale capsule attention-based salient object detection with multi-crossed layer connections. ||| 6188 ||| 6456 ||| 2445 ||| 19991 ||| 
2021 ||| underexposed image enhancement via unsupervised feature attention network. ||| 19992 ||| 19993 ||| 
2020 ||| video anomaly detection via predictive autoencoder with gradient-based attention. ||| 19994 ||| 1840 ||| 7648 ||| 
2020 ||| local-variance-based attention for visual tracking. ||| 19995 ||| 571 ||| 570 ||| 16648 ||| 
2019 ||| cross-database micro-expression recognition: a style aggregated and attention transfer approach. ||| 19996 ||| 14649 ||| 19997 ||| 
2019 ||| an attention residual neural network with recurrent greedy approach as loop filter for inter frames. ||| 19998 ||| 1052 ||| 19999 ||| 20000 ||| 1937 ||| 
2019 ||| self-attention guided deep features for action recognition. ||| 20001 ||| 20002 ||| 20003 ||| 20004 ||| 1703 ||| 20005 ||| 
2019 ||| scene text recognition via gated cascade attention. ||| 20006 ||| 20007 ||| 20008 ||| 20009 ||| 16845 ||| 
2021 ||| halder: hierarchical attention-guided learning with detail-refinement for multi-exposure image fusion. ||| 20010 ||| 16951 ||| 19435 ||| 8482 ||| 
2021 ||| let's find fluorescein: cross-modal dual attention learning for fluorescein leakage segmentation in fundus fluorescein angiography. ||| 14034 ||| 16628 ||| 20011 ||| 12866 ||| 19798 ||| 16627 ||| 718 ||| 
2021 ||| improved lightcnn with attention modules for asv spoofing detection. ||| 20012 ||| 20013 ||| 19144 ||| 12368 ||| 329 ||| 
2021 ||| attention driven self-similarity capture for motion deblurring. ||| 1134 ||| 20014 ||| 20015 ||| 20016 ||| 20017 ||| 12503 ||| 
2020 ||| dual focus attention network for video emotion recognition. ||| 20018 ||| 329 ||| 6832 ||| 
2020 ||| residual attention based network for automatic classification of phonation modes. ||| 11982 ||| 20019 ||| 3337 ||| 
2019 ||| skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention. ||| 20020 ||| 20021 ||| 17293 ||| 
2019 ||| pasiam: predicting attention inspired siamese network, for space-borne satellite video tracking. ||| 20022 ||| 17757 ||| 3668 ||| 20023 ||| 
2017 ||| adaptive attention fusion network for visual question answering. ||| 20024 ||| 20025 ||| 11340 ||| 
2020 ||| attention meets normalization and beyond. ||| 19807 ||| 19806 ||| 6415 ||| 19811 ||| 17441 ||| 19812 ||| 
2020 ||| a spatial attention based convolutional neural network for gesture recognition with hd-semg signals. ||| 20026 ||| 20027 ||| 20028 ||| 11494 ||| 
2020 ||| attention bias in emotional conflict in major depression disorder: an eye tracking study. ||| 11188 ||| 20029 ||| 10139 ||| 20030 ||| 20031 ||| 15710 ||| 
2020 ||| an adaptive attention regulation method based on biocybernetic loop. ||| 340 ||| 8932 ||| 12196 ||| 20032 ||| 
2020 ||| bgsga: combining bi-gru and syntactic graph attention for improving distant supervision relation extraction. ||| 20033 ||| 411 ||| 20034 ||| 
2021 ||| fu covid-19 ai agent built on attention algorithm using a combination of transformer, albert model, and rasa framework. ||| 20035 ||| 20036 ||| 20037 ||| 20038 ||| 20039 ||| 20040 ||| 
2019 ||| multi-attention network for aspect sentiment analysis. ||| 20041 ||| 5434 ||| 20042 ||| 20043 ||| 
2019 ||| spectral normalization and relativistic adversarial training for conditional pose generation with self-attention. ||| 20044 ||| 20045 ||| 15920 ||| 20046 ||| 
2021 ||| attention mining branch for optimizing attention map. ||| 20047 ||| 16377 ||| 20048 ||| 723 ||| 724 ||| 725 ||| 
2019 ||| automatic measurement of visual attention to video content using deep learning. ||| 20049 ||| 20050 ||| 20051 ||| 5719 ||| 
2019 ||| welding joints inspection via residual attention network. ||| 20052 ||| 6351 ||| 7344 ||| 
2021 ||| action spotting and temporal attention analysis in soccer videos. ||| 20053 ||| 723 ||| 724 ||| 725 ||| 20054 ||| 20055 ||| 648 ||| 19097 ||| 
2021 ||| video summarization with frame index vision transformer. ||| 20056 ||| 20057 ||| 20058 ||| 
2021 ||| hma-depth: a new monocular depth estimation model using hierarchical multi-scale attention. ||| 20059 ||| 20060 ||| 20061 ||| 20062 ||| 
2017 ||| attention to describe products with attributes. ||| 13240 ||| 20063 ||| 
2021 ||| recurrent rlcn-guided attention network for single image deraining. ||| 20064 ||| 20065 ||| 20066 ||| 
2017 ||| transformer monitoring using kalman filtering. ||| 20067 ||| 20068 ||| 20069 ||| 
2017 ||| infusing autonomy in power distribution networks using smart transformers. ||| 20070 ||| 
2020 ||| coordinated charging of large electric vehicle fleet in a charging station with limited transformer power. ||| 20071 ||| 13128 ||| 20072 ||| 20073 ||| 20074 ||| 
2020 ||| makeup style transfer on low-quality images with weighted multi-scale attention. ||| 20075 ||| 6105 ||| 6107 ||| 
2020 ||| manet: multimodal attention network based point-view fusion for 3d shape recognition. ||| 20076 ||| 20077 ||| 7015 ||| 20078 ||| 
2020 ||| attention stereo matching network. ||| 20079 ||| 20080 ||| 20081 ||| 13208 ||| 16546 ||| 
2020 ||| taan: task-aware attention network for few-shot classification. ||| 7436 ||| 2014 ||| 20082 ||| 
2020 ||| self and channel attention network for person re-identification. ||| 20083 ||| 4699 ||| 4702 ||| 
2020 ||| selective kernel and motion-emphasized loss based attention-guided network for hdr imaging of dynamic scenes. ||| 20084 ||| 20085 ||| 20086 ||| 
2020 ||| two-level attention-based fusion learning for rgb-d face recognition. ||| 20087 ||| 12091 ||| 20088 ||| 20089 ||| 
2018 ||| focusing on what is relevant: time-series learning and understanding using attention. ||| 20090 ||| 7210 ||| 7212 ||| 20091 ||| 20092 ||| 7209 ||| 7213 ||| 
2020 ||| equation attention relationship network (earn) : a geometric deep metric framework for learning similar math expression embedding. ||| 20093 ||| 17382 ||| 5721 ||| 5722 ||| 
2018 ||| action recognition with visual attention on skeleton images. ||| 2416 ||| 20094 ||| 20095 ||| 2166 ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| 20096 ||| 7307 ||| 7310 ||| 
2020 ||| sca net: sparse channel attention module for action recognition. ||| 20097 ||| 20098 ||| 20099 ||| 
2020 ||| motion attention deep transfer network for cross-database micro-expression recognition. ||| 20100 ||| 540 ||| 541 ||| 20101 ||| 
2020 ||| automatic fake news detection with pre-trained transformer models. ||| 16505 ||| 11303 ||| 16513 ||| 20102 ||| 20103 ||| 
2020 ||| answer-checking in context: a multi-modal fully attention network for visual question answering. ||| 20104 ||| 20105 ||| 14543 ||| 20106 ||| 20107 ||| 
2020 ||| acrm: attention cascade r-cnn with mix-nms for metallic surface defect detection. ||| 20108 ||| 20109 ||| 20110 ||| 
2020 ||| transformer networks for trajectory forecasting. ||| 20111 ||| 7272 ||| 7276 ||| 7277 ||| 
2020 ||| a novel disaster image data-set and characteristics analysis using attention model. ||| 11207 ||| 20112 ||| 20113 ||| 20114 ||| 20115 ||| 7376 ||| 7375 ||| 20116 ||| 7377 ||| 
2020 ||| wavelet attention embedding networks for video super-resolution. ||| 20117 ||| 20118 ||| 20119 ||| 
2018 ||| conditional transfer with dense residual attention: synthesizing traffic signs from street-view imagery. ||| 20120 ||| 20121 ||| 20122 ||| 20123 ||| 20124 ||| 
2020 ||| dual-attention guided dropblock module for weakly supervised object localization. ||| 20125 ||| 20126 ||| 1482 ||| 1484 ||| 786 ||| 
2020 ||| bcau-net: a novel architecture with binary channel attention module for mri brain segmentation. ||| 20127 ||| 20128 ||| 20129 ||| 20130 ||| 
2020 ||| multi-scale residual pyramid attention network for monocular depth estimation. ||| 2058 ||| 20131 ||| 20132 ||| 20133 ||| 
2020 ||| deep learning for astrophysics, understanding the impact of attention on variability induced by parameter initialization. ||| 16368 ||| 16369 ||| 16370 ||| 6784 ||| 6785 ||| 16371 ||| 16372 ||| 
2020 ||| a transformer-based radical analysis network for chinese character recognition. ||| 749 ||| 13930 ||| 1010 ||| 4489 ||| 20134 ||| 20135 ||| 
2020 ||| weakly supervised attention rectification for scene text recognition. ||| 20136 ||| 11445 ||| 11444 ||| 11446 ||| 472 ||| 
2020 ||| free-form image inpainting via contrastive attention network. ||| 9442 ||| 20137 ||| 18593 ||| 6660 ||| 6935 ||| 2824 ||| 
2020 ||| progressive scene segmentation based on self-attention mechanism. ||| 20138 ||| 20139 ||| 14036 ||| 2349 ||| 
2020 ||| cascade saliency attention network for object detection in remote sensing images. ||| 20140 ||| 6465 ||| 20141 ||| 
2020 ||| arbitrary style transfer with parallel self-attention. ||| 20142 ||| 16721 ||| 6810 ||| 11275 ||| 2628 ||| 
2020 ||| transformer reasoning network for image- text matching and retrieval. ||| 2689 ||| 2691 ||| 20143 ||| 2690 ||| 
2020 ||| magnet: multi-region attention-assisted grounding of natural language queries at phrase level. ||| 20144 ||| 20145 ||| 11018 ||| 20146 ||| 
2020 ||| region and relations based multi attention network for graph classification. ||| 15180 ||| 20147 ||| 
2020 ||| a lightweight spatial attention module with adaptive receptive fields in 3d convolutional neural network for alzheimer's disease classification. ||| 17745 ||| 20148 ||| 20149 ||| 6925 ||| 20150 ||| 20151 ||| 
2020 ||| nested attention u-net: a splicing detection method for satellite images. ||| 4194 ||| 19072 ||| 13992 ||| 19073 ||| 6887 ||| 
2020 ||| second-order attention guided convolutional activations for visual recognition. ||| 11985 ||| 577 ||| 11986 ||| 2304 ||| 11988 ||| 817 ||| 
2020 ||| single image deblurring using bi-attention network. ||| 20152 ||| 5298 ||| 5300 ||| 
2020 ||| open set domain recognition via attention-based gcn and semantic matching optimization. ||| 20153 ||| 6650 ||| 6769 ||| 
2020 ||| foanet: a focus of attention network with application to myocardium segmentation. ||| 1306 ||| 20154 ||| 20155 ||| 20156 ||| 20157 ||| 
2020 ||| vtt: long-term visual tracking with transformers. ||| 20158 ||| 18061 ||| 20159 ||| 20160 ||| 20161 ||| 20162 ||| 20163 ||| 
2020 ||| object detection model based on scene-level region proposal self-attention. ||| 615 ||| 264 ||| 613 ||| 262 ||| 
2020 ||| ddanet: dual decoder attention network for automatic polyp segmentation. ||| 20164 ||| 20165 ||| 20166 ||| 2712 ||| 20167 ||| 20168 ||| 20169 ||| 16413 ||| 20170 ||| 
2020 ||| stroke based posterior attention for online handwritten mathematical expression recognition. ||| 20134 ||| 13930 ||| 4489 ||| 1010 ||| 17434 ||| 20171 ||| 20172 ||| 
2020 ||| exploiting saliency in attention based convolutional neural network for classification of vertical root fractures. ||| 20173 ||| 15615 ||| 20174 ||| 15620 ||| 
2020 ||| reads: a rectified attentional double supervised network for scene text recognition. ||| 14552 ||| 20175 ||| 5192 ||| 3248 ||| 6935 ||| 
2020 ||| local attention and global representation collaborating for fine-grained classification. ||| 6828 ||| 20176 ||| 4600 ||| 2058 ||| 20177 ||| 18590 ||| 
2020 ||| attention based multi-instance thyroid cytopathological diagnosis with multi-scale feature fusion. ||| 20178 ||| 20179 ||| 20180 ||| 20181 ||| 20182 ||| 
2020 ||| cspa-dn: channel and spatial attention dense network for fusing pet and mri images. ||| 20183 ||| 20184 ||| 20185 ||| 5062 ||| 3879 ||| 20186 ||| 
2020 ||| few-shot few-shot learning and the role of spatial attention. ||| 20187 ||| 7435 ||| 20188 ||| 
2020 ||| decoupled self-attention module for person re-identification. ||| 20189 ||| 758 ||| 20190 ||| 127 ||| 
2020 ||| multi-branch attention networks for classifying galaxy clusters. ||| 9472 ||| 20191 ||| 20192 ||| 20193 ||| 
2020 ||| exploring spatial-temporal representations for fnirs-based intimacy detection via an attention-enhanced cascade convolutional recurrent neural network. ||| 242 ||| 2251 ||| 645 ||| 20194 ||| 648 ||| 649 ||| 
2018 ||| revised spatial transformer network towards improved image super-resolutions. ||| 20195 ||| 20196 ||| 19755 ||| 
2020 ||| deep multiple instance learning with spatial attention for rop case classification, instance selection and abnormality localization. ||| 5907 ||| 20197 ||| 6931 ||| 5895 ||| 20198 ||| 20199 ||| 20200 ||| 20201 ||| 20202 ||| 20203 ||| 20204 ||| 5896 ||| 20205 ||| 
2020 ||| question-agnostic attention for visual question answering. ||| 20206 ||| 1969 ||| 2475 ||| 
2020 ||| ordinal depth classification using region-based self-attention. ||| 20207 ||| 20208 ||| 20209 ||| 
2020 ||| attention-oriented action recognition for real- time human-robot interaction. ||| 20210 ||| 20211 ||| 6351 ||| 20212 ||| 20213 ||| 20214 ||| 20215 ||| 
2020 ||| accurate cell segmentation in digital pathology images via attention enforced networks. ||| 5499 ||| 5498 ||| 5500 ||| 20216 ||| 16673 ||| 20217 ||| 
2018 ||| attention-based neural network for traffic sign detection. ||| 875 ||| 1821 ||| 836 ||| 20218 ||| 
2020 ||| nighttime pedestrian detection based on feature attention and transformation. ||| 858 ||| 19144 ||| 1825 ||| 
2020 ||| attention-based multi-modal emotion recognition from art. ||| 20219 ||| 20220 ||| 20221 ||| 13992 ||| 
2020 ||| attention as activation. ||| 7144 ||| 7146 ||| 7145 ||| 7147 ||| 7148 ||| 
2020 ||| attention based pruning for shift networks. ||| 20222 ||| 20223 ||| 20224 ||| 20225 ||| 9196 ||| 
2020 ||| sat-net: self-attention and temporal fusion for facial action unit detection. ||| 5711 ||| 1770 ||| 5714 ||| 
2020 ||| cross-regional attention network for point cloud completion. ||| 20226 ||| 20227 ||| 
2020 ||| cascade attention guided residue learning gan for cross-modal translation. ||| 7130 ||| 1160 ||| 435 ||| 20228 ||| 127 ||| 
2020 ||| sa-unet: spatial attention u-net for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 5337 ||| 12344 ||| 20229 ||| 20230 ||| 
2020 ||| coarse-to-fine foreground segmentation based on co-occurrence pixel-block and spatio-temporal attention model. ||| 11467 ||| 20231 ||| 
2020 ||| a novel attention-based aggregation function to combine vision and language. ||| 19002 ||| 19001 ||| 19003 ||| 13611 ||| 
2020 ||| edge-aware graph attention network for ratio of edge-user estimation in mobile networks. ||| 20232 ||| 20233 ||| 1894 ||| 20234 ||| 11266 ||| 1132 ||| 20235 ||| 
2020 ||| collaborative human machine attention module for character recognition. ||| 20236 ||| 20237 ||| 12797 ||| 
2020 ||| flow-guided spatial attention tracking for egocentric activity recognition. ||| 20238 ||| 19991 ||| 
2020 ||| attention based coupled framework for road and pothole segmentation. ||| 20239 ||| 19449 ||| 17364 ||| 19446 ||| 
2020 ||| mean: multi - element attention network for scene text recognition. ||| 20240 ||| 17387 ||| 20241 ||| 20242 ||| 20243 ||| 
2020 ||| recurrent deep attention network for person re-identification. ||| 20244 ||| 1086 ||| 20245 ||| 20246 ||| 5067 ||| 
2020 ||| hierarchical multimodal attention for deep video summarization. ||| 20247 ||| 15325 ||| 15326 ||| 20248 ||| 20249 ||| 
2018 ||| scene text detection via deep semantic feature fusion and attention-based refinement. ||| 20250 ||| 20251 ||| 5752 ||| 1916 ||| 1788 ||| 
2020 ||| aggregating object features based on attention weights for fine-grained image retrieval. ||| 20252 ||| 20253 ||| 20254 ||| 20255 ||| 20256 ||| 
2020 ||| attention-driven body pose encoding for human activity recognition. ||| 20257 ||| 20258 ||| 4611 ||| 6383 ||| 
2020 ||| cardiogan: an attention-based generative adversarial network for generation of electrocardiograms. ||| 20259 ||| 20260 ||| 20261 ||| 
2020 ||| temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition. ||| 925 ||| 926 ||| 
2020 ||| segmentation of intracranial aneurysm remnant in mra using dual-attention atrous net. ||| 20262 ||| 20263 ||| 20264 ||| 5335 ||| 20265 ||| 
2020 ||| integrating historical states and co-attention mechanism for visual dialog. ||| 20266 ||| 4180 ||| 4183 ||| 
2020 ||| interpretable attention guided network for fine-grained visual classification. ||| 20267 ||| 20268 ||| 7873 ||| 20269 ||| 7240 ||| 
2020 ||| efficient-receptive field block with group spatial attention mechanism for object detection. ||| 494 ||| 19112 ||| 19113 ||| 
2020 ||| hanet: hybrid attention-aware network for crowd counting. ||| 20270 ||| 8835 ||| 20271 ||| 19401 ||| 2531 ||| 12300 ||| 
2020 ||| generalized local attention pooling for deep metric learning. ||| 20272 ||| 20273 ||| 20274 ||| 20275 ||| 20276 ||| 
2020 ||| attentional wavelet network for traditional chinese painting transfer. ||| 3049 ||| 18593 ||| 17624 ||| 2824 ||| 
2020 ||| deep residual attention network for hyperspectral image reconstruction. ||| 20277 ||| 6422 ||| 
2020 ||| 3d attention mechanism for fine-grained classification of table tennis strokes using a twin spatio-temporal convolutional neural networks. ||| 20278 ||| 2701 ||| 20279 ||| 20280 ||| 20281 ||| 
2020 ||| reinforcement learning with dual attention guided graph convolution for relation extraction. ||| 264 ||| 20282 ||| 631 ||| 613 ||| 262 ||| 
2020 ||| multi-scale relational reasoning with regional attention for visual question answering. ||| 20283 ||| 173 ||| 4285 ||| 
2020 ||| ma-lstm: a multi-attention based lstm for complex pattern extraction. ||| 6283 ||| 6282 ||| 6284 ||| 6285 ||| 
2020 ||| transformer-encoder detector module: using context to improve robustness to adversarial attacks on object detection. ||| 20284 ||| 20285 ||| 2603 ||| 
2020 ||| trajectory-user link with attention recurrent networks. ||| 4150 ||| 4148 ||| 2355 ||| 17580 ||| 4147 ||| 20286 ||| 
2020 ||| attention-based model with attribute classification for cross-domain person re-identification. ||| 20287 ||| 20288 ||| 20289 ||| 
2020 ||| privattnet: predicting privacy risks in images using visual attention. ||| 20290 ||| 20291 ||| 562 ||| 
2020 ||| deep attention based semi-supervised 2d-pose estimation for surgical instruments. ||| 20292 ||| 20293 ||| 16413 ||| 20294 ||| 20295 ||| 20296 ||| 20297 ||| 5695 ||| 
2020 ||| range-doppler hand gesture recognition using deep residual-3dcnn with transformer network. ||| 20298 ||| 20299 ||| 20300 ||| 
2020 ||| anchors vs attention: comparing xai on a real-life use case. ||| 8063 ||| 20301 ||| 20302 ||| 7350 ||| 20303 ||| 20304 ||| 
2018 ||| weakly supervised domain-specific color naming based on attention. ||| 17120 ||| 20305 ||| 7105 ||| 
2020 ||| attention2angiogan: synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks. ||| 7798 ||| 7799 ||| 7800 ||| 7801 ||| 
2020 ||| tcatd: text contour attention for scene text detection. ||| 20306 ||| 20307 ||| 6271 ||| 
2020 ||| sdma: saliency-driven mutual cross attention for multi-variate time series. ||| 7764 ||| 7765 ||| 7766 ||| 
2020 ||| context matters: self-attention for sign language recognition. ||| 20308 ||| 20309 ||| 
2018 ||| multi-scale attention with dense encoder for handwritten mathematical expression recognition. ||| 4489 ||| 1010 ||| 4463 ||| 
2020 ||| attention pyramid module for scene recognition. ||| 19916 ||| 19838 ||| 20310 ||| 20311 ||| 
2018 ||| an attention-based approach for single image super resolution. ||| 2487 ||| 20312 ||| 5192 ||| 6560 ||| 20313 ||| 12186 ||| 20314 ||| 
2020 ||| global context-based network with transformer for image2latex. ||| 20315 ||| 14747 ||| 17416 ||| 20316 ||| 5051 ||| 
2020 ||| predicting chemical properties using self-attention multi-task learning based on smiles representation. ||| 20317 ||| 20318 ||| 
2020 ||| improving reliability of attention branch network by introducing uncertainty. ||| 20319 ||| 723 ||| 724 ||| 725 ||| 
2020 ||| multi-stage attention based visual question answering. ||| 20320 ||| 20321 ||| 20322 ||| 
2018 ||| a multi-part convolutional attention network for fine-grained image recognition. ||| 6766 ||| 6765 ||| 6514 ||| 6764 ||| 6767 ||| 
2020 ||| avd-net: attention value decomposition network for deep multi-agent reinforcement learning. ||| 20323 ||| 11581 ||| 3906 ||| 
2020 ||| attention-based selection strategy for weakly supervised object localization. ||| 20324 ||| 19159 ||| 
2020 ||| global-local attention network for semantic segmentation in aerial images. ||| 20325 ||| 20326 ||| 20327 ||| 1281 ||| 20328 ||| 19506 ||| 20329 ||| 382 ||| 6569 ||| 
2020 ||| a transformer-based network for anisotropic 3d medical image segmentation. ||| 20330 ||| 19078 ||| 
2020 ||| utilising visual attention cues for vehicle detection and tracking. ||| 20331 ||| 20332 ||| 1675 ||| 11483 ||| 20333 ||| 
2020 ||| 3d point cloud registration based on cascaded mutual information attention network. ||| 13195 ||| 20334 ||| 20335 ||| 
2018 ||| context-aware attention lstm network for flood prediction. ||| 4285 ||| 20336 ||| 20337 ||| 4287 ||| 20338 ||| 173 ||| 
2020 ||| gaussian constrained attention network for scene text recognition. ||| 20339 ||| 20340 ||| 11689 ||| 20341 ||| 4201 ||| 
2018 ||| aggregated sparse attention for steering angle prediction. ||| 2195 ||| 20342 ||| 20343 ||| 2603 ||| 
2020 ||| atsal: an attention based architecture for saliency prediction in 360$^\circ $ videos. ||| 20344 ||| 11575 ||| 1674 ||| 1675 ||| 
2020 ||| on the global self-attention mechanism for graph convolutional networks. ||| 5187 ||| 20345 ||| 
2020 ||| rsan: residual subtraction and attention network for single image super-resolution. ||| 20346 ||| 6288 ||| 20347 ||| 2628 ||| 
2020 ||| attention-based deep metric learning for near-duplicate video retrieval. ||| 20348 ||| 20349 ||| 600 ||| 20350 ||| 5690 ||| 
2020 ||| fuzzy-based pseudo segmentation approach for handwritten word recognition using a sequence to sequence model with attention. ||| 20351 ||| 20352 ||| 20353 ||| 20354 ||| 
2020 ||| $p^{2}$ net: augmented parallel-pyramid net for attention guided pose estimation. ||| 20355 ||| 4470 ||| 20356 ||| 7064 ||| 1248 ||| 2824 ||| 
2020 ||| cross-media hash retrieval using multi-head attention network. ||| 264 ||| 20357 ||| 17560 ||| 613 ||| 262 ||| 
2020 ||| da-refinenet: dual-inputs attention refinenet for whole slide image segmentation. ||| 20358 ||| 20359 ||| 20360 ||| 6502 ||| 
2020 ||| multiscale attention-based prototypical network for few-shot semantic segmentation. ||| 8426 ||| 15326 ||| 20361 ||| 20362 ||| 20363 ||| 20364 ||| 20365 ||| 
2020 ||| multi-scale and attention based resnet for heartbeat classification. ||| 20366 ||| 20367 ||| 20368 ||| 20369 ||| 13205 ||| 
2020 ||| feature-aware unsupervised learning with joint variational attention and automatic clustering. ||| 20370 ||| 1556 ||| 20371 ||| 20372 ||| 20373 ||| 
2020 ||| attendaffectnet: self-attention based networks for predicting affective responses from movies. ||| 20374 ||| 20375 ||| 936 ||| 20376 ||| 
2020 ||| understanding when spatial transformer networks do not support invariance, and what to do about it. ||| 20377 ||| 20378 ||| 20379 ||| 
2018 ||| robust attentional pooling via feature selection. ||| 20380 ||| 2509 ||| 20381 ||| 20382 ||| 2394 ||| 
2020 ||| video summarization with a dual attention capsule network. ||| 5033 ||| 20383 ||| 5233 ||| 
2018 ||| time-dependent pre-attention model for image captioning. ||| 20384 ||| 20385 ||| 17777 ||| 
2018 ||| self-attention based network for punctuation restoration. ||| 6832 ||| 5110 ||| 5937 ||| 728 ||| 
2020 ||| translating adult's focus of attention to elderly's. ||| 20386 ||| 20387 ||| 12270 ||| 12271 ||| 20388 ||| 
2021 ||| root cause analysis in the industrial domain using knowledge graphs: a case study on power transformers. ||| 6097 ||| 20389 ||| 20390 ||| 20391 ||| 20392 ||| 20393 ||| 20394 ||| 
2020 ||| utilization of residual cnn-gru with attention mechanism for classification of 12-lead ecg. ||| 20395 ||| 20396 ||| 20397 ||| 20398 ||| 20399 ||| 20400 ||| 13478 ||| 20401 ||| 
2021 ||| generative pre-trained transformer for cardiac abnormality detection. ||| 20402 ||| 20403 ||| 8382 ||| 20404 ||| 4194 ||| 59 ||| 20405 ||| 20406 ||| 20407 ||| 
2021 ||| classification of ecg using ensemble of residual cnns with attention mechanism. ||| 20395 ||| 20396 ||| 20408 ||| 20409 ||| 20397 ||| 20410 ||| 20400 ||| 13478 ||| 20401 ||| 
2020 ||| classification of 12-lead ecgs using intra-heartbeat discrete-time fourier transform and inter-heartbeat attention. ||| 20411 ||| 20412 ||| 20413 ||| 
2020 ||| a wide and deep transformer neural network for 12-lead ecg classification. ||| 20414 ||| 20415 ||| 20416 ||| 20417 ||| 20418 ||| 20419 ||| 20420 ||| 
2021 ||| a mixed-domain self-attention network for multilabel cardiac irregularity classification using reduced-lead electrocardiogram. ||| 20421 ||| 20422 ||| 20423 ||| 
2020 ||| multi-label classification of 12-lead ecgs by using residual cnn and class-wise attention. ||| 1305 ||| 20424 ||| 20425 ||| 20426 ||| 20427 ||| 20428 ||| 20429 ||| 
2020 ||| automatic classification of arrhythmias by residual network and bigru with attention mechanism. ||| 9897 ||| 20424 ||| 20430 ||| 4103 ||| 20427 ||| 20426 ||| 20429 ||| 
2018 ||| pay more attention with fewer parameters: a novel 1-d convolutional neural network for heart sounds classification. ||| 9226 ||| 1956 ||| 20431 ||| 20432 ||| 12522 ||| 19807 ||| 
2020 ||| madnn: a multi-scale attention deep neural network for arrhythmia classification. ||| 20433 ||| 3561 ||| 20434 ||| 
2021 ||| convolution-free waveform transformers for multi-lead ecg classification. ||| 20414 ||| 20418 ||| 20415 ||| 20435 ||| 20420 ||| 
2021 ||| channel self-attention deep learning framework for multi-cardiac abnormality diagnosis from varied-lead ecg signals. ||| 20436 ||| 20437 ||| 20438 ||| 20439 ||| 20440 ||| 20441 ||| 20442 ||| 
2021 ||| cardiac abnormalities recognition in ecg using a convolutional network with attention and input with an adaptable number of leads. ||| 20220 ||| 20443 ||| 20444 ||| 20445 ||| 20446 ||| 20447 ||| 
2019 ||| pay attention and watch temporal correlation: a novel 1-d convolutional neural network for ecg record classification. ||| 20448 ||| 1956 ||| 20431 ||| 20432 ||| 12522 ||| 19807 ||| 
2021 ||| multi-label classification of multi-lead ecg based on deep 1d convolutional neural networks with residual and attention mechanism. ||| 20449 ||| 20450 ||| 20451 ||| 20452 ||| 8968 ||| 20453 ||| 20454 ||| 
2019 ||| identifying protein-protein interaction using tree lstm and structured attention. ||| 3223 ||| 20455 ||| 3224 ||| 3225 ||| 
2019 ||| improving tree-lstm with tree attention. ||| 3223 ||| 3224 ||| 3225 ||| 
2020 ||| ican: introspective convolutional attention network for semantic text classification. ||| 20456 ||| 20457 ||| 20458 ||| 20459 ||| 20460 ||| 
2021 ||| automatic title generation for text with pre-trained transformer language model. ||| 20461 ||| 20462 ||| 20463 ||| 20464 ||| 
2020 ||| deepcontext: parameterized compatibility-based attention cnn for human context recognition. ||| 20465 ||| 20466 ||| 20467 ||| 20468 ||| 20469 ||| 3528 ||| 
2020 ||| similarity of speech emotion in different languages revealed by a neural network with attention. ||| 20470 ||| 20471 ||| 20472 ||| 20473 ||| 20474 ||| 
2022 ||| modeling review helpfulness with augmented transformer neural networks. ||| 20475 ||| 16842 ||| 20476 ||| 20477 ||| 326 ||| 20478 ||| 20479 ||| 20480 ||| 20481 ||| 
2019 ||| arabic poem generation with hierarchical recurrent attentional network. ||| 20482 ||| 20483 ||| 
2020 ||| attentional adversarial variational video generation via decomposing motion and content. ||| 20482 ||| 20483 ||| 20484 ||| 20485 ||| 
2020 ||| deep neural attention-based model for the evaluation of italian sentences complexity. ||| 20486 ||| 20487 ||| 20488 ||| 20489 ||| 
2019 ||| sadeepsense: self-attention deep learning framework for heterogeneous on-device sensors in internet of things applications. ||| 3366 ||| 20490 ||| 3365 ||| 20491 ||| 17168 ||| 20492 ||| 20493 ||| 5142 ||| 20494 ||| 17171 ||| 
2020 ||| image translation with attention mechanism based on generative adversarial networks. ||| 3042 ||| 20495 ||| 20496 ||| 20497 ||| 20498 ||| 10235 ||| 
2021 ||| dual attention-based federated learning for wireless traffic prediction. ||| 20499 ||| 20500 ||| 20501 ||| 20502 ||| 
2017 ||| computing continuous-time markov chains as transformers of unbounded observables. ||| 20503 ||| 20504 ||| 20505 ||| 15062 ||| 
2021 ||| epistemic planning with attention as a bounded resource. ||| 20506 ||| 20507 ||| 
2020 ||| improving passage re-ranking with word n-gram aware coattention encoder. ||| 20508 ||| 20509 ||| 
2020 ||| native-language identification with attention. ||| 20510 ||| 648 ||| 20511 ||| 318 ||| 
2020 ||| solving arithmetic word problems using transformer and pre-processing of problem texts. ||| 20512 ||| 20513 ||| 
2021 ||| softermax: hardware/software co-design of an efficient softmax for transformers. ||| 20514 ||| 20515 ||| 20516 ||| 20517 ||| 20518 ||| 
2021 ||| dancing along battery: enabling transformer with run-time reconfigurability on mobile devices. ||| 9873 ||| 13660 ||| 9874 ||| 9867 ||| 9869 ||| 9868 ||| 20519 ||| 20520 ||| 11024 ||| 
2021 ||| mat: processing in-memory acceleration for long-sequence attention. ||| 20521 ||| 20522 ||| 20523 ||| 6502 ||| 20524 ||| 20525 ||| 
2021 ||| late breaking results: attention in graph2seq neural networks towards push-button analog ic placement. ||| 2871 ||| 20526 ||| 7033 ||| 20527 ||| 20528 ||| 7033 ||| 20529 ||| 
2021 ||| attentional transfer is all you need: technology-aware layout pattern generation. ||| 15595 ||| 9879 ||| 20530 ||| 
2019 ||| improved densenet with convolutional attention module for brain tumor segmentation. ||| 12719 ||| 20531 ||| 20532 ||| 
2019 ||| cardiac motion scoring based on cnn with attention mechanism. ||| 20533 ||| 20534 ||| 5476 ||| 20535 ||| 
2018 ||| the effect of spatial consistence on multisensory integration in a divided attention task. ||| 20536 ||| 20537 ||| 20538 ||| 20539 ||| 
2020 ||| amil: an attentional multi-instance learning for computer-aided diagnosis of skin diagnosis. ||| 20540 ||| 20541 ||| 7482 ||| 
2019 ||| left ventricle segmentation and quantification with attention-enhanced segmentation and shape correction. ||| 20542 ||| 20534 ||| 20535 ||| 
2017 ||| experimental design and collection of brain and respiratory data for detection of driver's attention. ||| 20543 ||| 10500 ||| 20544 ||| 10500 ||| 20545 ||| 20220 ||| 20546 ||| 20547 ||| 
2022 ||| selection of representative instances using ant colony: a case study in a database of children and adolescents with attention-deficit/hyperactivity disorder. ||| 20548 ||| 20549 ||| 20550 ||| 20551 ||| 15326 ||| 20552 ||| 20553 ||| 20554 ||| 20555 ||| 20556 ||| 
2020 ||| an attention-based architecture for eeg classification. ||| 9826 ||| 20557 ||| 9827 ||| 20558 ||| 20559 ||| 20560 ||| 9828 ||| 
2022 ||| vision transformers for brain tumor classification. ||| 20561 ||| 20562 ||| 
2022 ||| improved mri-based pseudo-ct synthesis via segmentation guided attention networks. ||| 20563 ||| 20564 ||| 20565 ||| 20566 ||| 20567 ||| 
2022 ||| learning embeddings from free-text triage notes using pretrained transformer models. ||| 20568 ||| 20569 ||| 20570 ||| 20571 ||| 
2020 ||| data mining in clinical trial text: transformers for classification and question answering tasks. ||| 20572 ||| 20573 ||| 20574 ||| 
2021 ||| the application of multichannel neuro-electrostimulation for working memory and attention improvement of young subjects. ||| 20575 ||| 20576 ||| 
2021 ||| analysis of school performance of children and adolescents with attention-deficit/hyperactivity disorder: a dimensionality reduction approach. ||| 20549 ||| 20577 ||| 20578 ||| 15326 ||| 20579 ||| 20554 ||| 20555 ||| 20580 ||| 
2017 ||| correction of attention in a learning ability task with using non-invasive neurostimulation of peripheral nervous system. ||| 20576 ||| 20575 ||| 20581 ||| 
2020 ||| attention-based sequential generative conversational agent. ||| 20582 ||| 20583 ||| 20584 ||| 20585 ||| 
2020 ||| machine learning based transformer health monitoring using iot edge computing. ||| 20586 ||| 20587 ||| 20588 ||| 
2021 ||| eeg classification with transformer-based models. ||| 20589 ||| 1824 ||| 20590 ||| 
2021 ||| improvement of visual attention by dotted background noise. ||| 20591 ||| 20592 ||| 20593 ||| 
2019 ||| estimation of visual attention via canonical correlation between visual and gaze-based features. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2020 ||| distress level classification of road infrastructures via cnn generating attention map. ||| 7611 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| attention-based model for predicting question relatedness on stack overflow. ||| 20594 ||| 20595 ||| 20596 ||| 20597 ||| 20598 ||| 
2020 ||| improved automatic summarization of subroutines via attention to file context. ||| 20599 ||| 20600 ||| 18010 ||| 20601 ||| 
2019 ||| the transformer substation scenario modeling and visual perception visualization system. ||| 20602 ||| 20603 ||| 
2021 ||| comparative analysis of high- and low-performing factory workers with attention-based neural networks. ||| 20604 ||| 20605 ||| 20606 ||| 20607 ||| 20473 ||| 
2019 ||| a deep spatio-temporal attention-based neural network for passenger flow prediction. ||| 20608 ||| 20609 ||| 20610 ||| 20611 ||| 
2021 ||| ar-t: temporal relation embedded transformer for the real world activity recognition. ||| 11062 ||| 11063 ||| 
2021 ||| use of a computational tool for the assessment of attention of medical residents after a day on duty. ||| 20612 ||| 20613 ||| 852 ||| 20614 ||| 20615 ||| 3419 ||| 20616 ||| 20617 ||| 
2021 ||| dual-pathway attention based supervised adversarial hashing for cross-modal retrieval. ||| 20618 ||| 16943 ||| 20619 ||| 7688 ||| 
2021 ||| air quality prediction with 1-dimensional convolution and attention on multi-modal features. ||| 20620 ||| 20621 ||| 10565 ||| 
2020 ||| source code summarization using attention-based keyword memory networks. ||| 3866 ||| 20622 ||| 3868 ||| 
2019 ||| ceam: a novel approach using cycle embeddings with attention mechanism for stock price prediction. ||| 20623 ||| 20624 ||| 
2022 ||| cross-attention model for multi-modal bio-signal processing. ||| 20625 ||| 20626 ||| 20627 ||| 
2021 ||| attention on personalized clinical decision support system: federated learning approach. ||| 20628 ||| 20629 ||| 20630 ||| 20631 ||| 
2019 ||| query-focused summarization enhanced with sentence attention mechanism. ||| 20632 ||| 20633 ||| 20634 ||| 
2020 ||| paraphrase bidirectional transformer with multi-task learning. ||| 20635 ||| 20636 ||| 
2022 ||| aptamer- protein interaction prediction using transformer. ||| 20637 ||| 20638 ||| 
2018 ||| convolutional neural network with sdp-based attention for relation classification. ||| 7015 ||| 4600 ||| 20639 ||| 
2022 ||| transformer-based embedding applied to classify bacterial species using sequencing reads. ||| 20640 ||| 20641 ||| 
2020 ||| multi-label patent classification using attention-aware deep learning model. ||| 20642 ||| 20643 ||| 20644 ||| 20645 ||| 
2020 ||| unsupervised image-to-image translation with self-attention networks. ||| 20646 ||| 19377 ||| 
2022 ||| transformer networks for trajectory classification. ||| 20647 ||| 20648 ||| 20645 ||| 
2017 ||| head pose-free eye gaze prediction for driver attention study. ||| 20649 ||| 20650 ||| 20651 ||| 20652 ||| 20653 ||| 
2017 ||| an attention-based neural popularity prediction model for social media events. ||| 20654 ||| 20655 ||| 20656 ||| 
2019 ||| privacy protection in transformer-based neural network. ||| 20657 ||| 20658 ||| 20659 ||| 20660 ||| 
2017 ||| analyzing multimodal public sentiment based on hierarchical semantic attentional network. ||| 15237 ||| 
2019 ||| a prior knowledge based neural attention model for opioid topic identification. ||| 20661 ||| 17642 ||| 20662 ||| 20663 ||| 
2019 ||| attention allocation of twitter users in geopolitics. ||| 15183 ||| 3307 ||| 15182 ||| 3002 ||| 20664 ||| 1564 ||| 20660 ||| 
2019 ||| sentiment analysis based on background knowledge attention. ||| 3307 ||| 5349 ||| 15183 ||| 15182 ||| 
2018 ||| attention-based multi-hop reasoning for knowledge graph. ||| 20665 ||| 20658 ||| 20663 ||| 18416 ||| 
2019 ||| context-aware multi-view attention networks for emotion cause extraction. ||| 20666 ||| 20667 ||| 20656 ||| 3279 ||| 
2021 ||| gatps: an attention-based graph neural network for predicting sdc-causing instructions. ||| 20668 ||| 5793 ||| 20669 ||| 
2019 ||| lame: light-controlled attention guidance for multi-monitor environments. ||| 15370 ||| 20670 ||| 15372 ||| 
2018 ||| exploring vibrotactile and peripheral cues for spatial attention guidance. ||| 15370 ||| 20671 ||| 20672 ||| 15371 ||| 20673 ||| 15372 ||| 
2020 ||| time-aware transformer-based network for clinical notes series prediction. ||| 8977 ||| 17113 ||| 3525 ||| 3528 ||| 
2019 ||| self-attention based molecule representation for predicting drug-target interaction. ||| 372 ||| 20674 ||| 20675 ||| 20676 ||| 
2018 ||| learning to exploit invariances in clinical time-series data using sequence transformer networks. ||| 20677 ||| 20678 ||| 20679 ||| 
2020 ||| dynamically extracting outcome-specific problem lists from clinical notes with guided multi-headed attention. ||| 20680 ||| 20681 ||| 20682 ||| 20683 ||| 
2020 ||| attention-based network for weak labels in neonatal seizure detection. ||| 20684 ||| 20685 ||| 20686 ||| 20687 ||| 20688 ||| 4046 ||| 13501 ||| 20689 ||| 3882 ||| 20690 ||| 20691 ||| 
2020 ||| predicting drug sensitivity of cancer cell lines via collaborative filtering with contextual attention. ||| 13375 ||| 20692 ||| 20693 ||| 20694 ||| 13377 ||| 
2020 ||| students need more attention: bert-based attention model for small data with application to automatic patient message triage. ||| 1030 ||| 3049 ||| 20695 ||| 3386 ||| 20696 ||| 1029 ||| 1032 ||| 
2018 ||| towards understanding ecg rhythm classification using convolutional neural networks and attention mappings. ||| 20697 ||| 20698 ||| 20699 ||| 20700 ||| 20701 ||| 20702 ||| 
2021 ||| an end-to-end attention transfer network for cross-domain service recommendation. ||| 1580 ||| 1581 ||| 
2020 ||| multi-indicators prediction in microservice using granger causality test and attention lstm. ||| 20703 ||| 1572 ||| 20704 ||| 
2021 ||| natural language interface for covid-19 amharic database using lstm encoder decoder architecture with attention. ||| 20705 ||| 20706 ||| 
2020 ||| magnet: multi-label text classification using attention-based graph neural network. ||| 18374 ||| 20707 ||| 18375 ||| 
2022 ||| subtst: a combination of sub-word latent topics and sentence transformer for semantic similarity detection. ||| 20708 ||| 20709 ||| 4735 ||| 
2018 ||| personalized sentiment analysis and a framework with attention-based hawkes process model. ||| 20710 ||| 20711 ||| 20712 ||| 20713 ||| 20714 ||| 
2017 ||| which saliency detection method is the best to estimate the human attention for adjective noun concepts?. ||| 20715 ||| 20716 ||| 13243 ||| 20717 ||| 20718 ||| 4196 ||| 
2020 ||| vmrfanet: view-specific multi-receptive field attention network for person re-identification. ||| 19041 ||| 20719 ||| 19042 ||| 20720 ||| 19043 ||| 
2021 ||| informer, an information organization transformer architecture. ||| 20721 ||| 20722 ||| 20723 ||| 20724 ||| 20725 ||| 
2022 ||| bilinear multi-head attention graph neural network for traffic prediction. ||| 20726 ||| 19744 ||| 20727 ||| 
2022 ||| transformers for low-resource neural machine translation. ||| 20728 ||| 20729 ||| 20730 ||| 
2022 ||| quantifying student attention using convolutional neural networks. ||| 20731 ||| 20732 ||| 
2022 ||| comparing rnn and transformer context representations in the czech answer selection task. ||| 20733 ||| 20734 ||| 20735 ||| 13478 ||| 
2021 ||| human sentence processing: recurrence or attention? ||| 20736 ||| 20737 ||| 
2021 ||| accounting for agreement phenomena in sentence comprehension with transformer language models: effects of similarity-based interference on surprisal and attention. ||| 20738 ||| 20739 ||| 
2021 ||| relation classification with cognitive attention supervision. ||| 20740 ||| 20741 ||| 
2021 ||| cognlp-sheffield at cmcl 2021 shared task: blending cognitively inspired features with transformer-based language models for predicting eye tracking patterns. ||| 20742 ||| 20743 ||| 20744 ||| 13943 ||| 
2021 ||| tempera: spatial transformer feature pyramid network for cardiac mri segmentation. ||| 20745 ||| 20746 ||| 20747 ||| 20748 ||| 20749 ||| 20750 ||| 
2021 ||| a multi-view crossover attention u-net cascade with fourier domain adaptation for multi-domain cardiac mri segmentation. ||| 20751 ||| 20752 ||| 20753 ||| 
2021 ||| consistency based co-segmentation for multi-view cardiac mri using vision transformer. ||| 20754 ||| 20755 ||| 
2019 ||| towards predicting attention and workload during math problem solving. ||| 20756 ||| 20757 ||| 20758 ||| 
2021 ||| emotional robbert and insensitive bertje: combining transformers and affect lexica for dutch emotion detection. ||| 20759 ||| 20760 ||| 20761 ||| 2253 ||| 4937 ||| 
2021 ||| wassa@iitk at wassa 2021: multi-task learning and transformer finetuning for emotion classification and empathy prediction. ||| 10608 ||| 10607 ||| 20762 ||| 
2017 ||| lexicon integrated cnn models with attention for sentiment analysis. ||| 372 ||| 374 ||| 375 ||| 
2017 ||| mining fine-grained opinions on closed captions of youtube videos with an attention-rnn. ||| 7447 ||| 20763 ||| 20764 ||| 
2021 ||| towards emotion recognition in hindi-english code-mixed data: a transformer based approach. ||| 20765 ||| 20766 ||| 
2019 ||| online abuse detection: the value of preprocessing and neural attention models. ||| 3677 ||| 20767 ||| 20768 ||| 
2021 ||| pvg at wassa 2021: a multi-input, multi-task, transformer-based architecture for empathy and distress prediction. ||| 20769 ||| 20770 ||| 20771 ||| 20772 ||| 
2018 ||| self-attention: a better building block for sentiment analysis neural network classifiers. ||| 20773 ||| 20774 ||| 
2017 ||| emoatt at emoint-2017: inner attention sentence embedding for emotion intensity. ||| 7447 ||| 20764 ||| 
2018 ||| nlp at iest 2018: bilstm-attention and lstm-attention via soft voting in emotion classification. ||| 20775 ||| 3890 ||| 
2017 ||| improving attention to security in software design with analytics and cognitive techniques. ||| 20776 ||| 20777 ||| 
2019 ||| attention-based gated convolutional neural networks for distant supervised relation extraction. ||| 20778 ||| 18159 ||| 18160 ||| 20779 ||| 
2018 ||| coherence-based automated essay scoring using self-attention. ||| 2514 ||| 13585 ||| 20780 ||| 20781 ||| 20782 ||| 20783 ||| 
2018 ||| tsabcnn: two-stage attention-based convolutional neural network for frame identification. ||| 20784 ||| 20785 ||| 20786 ||| 20787 ||| 20788 ||| 
2019 ||| contextualized word representations with effective attention for aspect-based sentiment analysis. ||| 20789 ||| 20790 ||| 20791 ||| 20792 ||| 
2018 ||| attention-based convolutional neural networks for chinese relation extraction. ||| 20793 ||| 18159 ||| 18160 ||| 20779 ||| 
2018 ||| trigger words detection by integrating attention mechanism into bi-lstm neural network - a case study in pubmed-wide trigger words detection for pancreatic cancer. ||| 20794 ||| 20795 ||| 20796 ||| 20797 ||| 20798 ||| 20799 ||| 20800 ||| 20801 ||| 
2019 ||| a comprehensive verification of transformer in text classification. ||| 20802 ||| 8967 ||| 20803 ||| 8974 ||| 
2020 ||| clickbait detection with style-aware title modeling and co-attention. ||| 3754 ||| 3755 ||| 3756 ||| 2795 ||| 
2019 ||| an attention-based approach for mongolian news named entity recognition. ||| 20804 ||| 790 ||| 4499 ||| 20805 ||| 
2018 ||| medical knowledge attention enhanced neural model for named entity recognition in chinese emr. ||| 5302 ||| 9472 ||| 18724 ||| 
2019 ||| multi-label aspect classification on question-answering text with contextualized attention-based neural network. ||| 16556 ||| 20806 ||| 3083 ||| 16557 ||| 3085 ||| 
2018 ||| an end-to-end entity and relation extraction network with multi-head attention. ||| 16631 ||| 20807 ||| 20808 ||| 20809 ||| 
2017 ||| memory augmented attention model for chinese implicit discourse relation recognition. ||| 1305 ||| 3044 ||| 11690 ||| 
2019 ||| syntax-aware attention for natural language inference with phrase-level matching. ||| 20810 ||| 20811 ||| 20779 ||| 18160 ||| 18159 ||| 
2019 ||| leveraging multi-head attention mechanism to improve event detection. ||| 20812 ||| 92 ||| 20813 ||| 20814 ||| 11634 ||| 
2020 ||| mongolian questions classification based on multi-head attention. ||| 15574 ||| 790 ||| 20805 ||| 
2020 ||| attention-based graph neural network with global context awareness for document understanding. ||| 20815 ||| 11446 ||| 8718 ||| 20816 ||| 
2018 ||| question-answering aspect classification with hierarchical attention network. ||| 16556 ||| 16557 ||| 3083 ||| 16558 ||| 20817 ||| 
2018 ||| attention-based cnn-blstm networks for joint intent detection and slot filling. ||| 16708 ||| 310 ||| 16893 ||| 
2021 ||| multi-level emotion cause analysis by multi-head attention based multi-task learning. ||| 1309 ||| 1311 ||| 8426 ||| 1312 ||| 
2020 ||| message structure aided attentional convolution network for rf device fingerprinting. ||| 20818 ||| 20819 ||| 20820 ||| 20821 ||| 
2020 ||| double attention-based deformable convolutional network for recommendation. ||| 9779 ||| 20822 ||| 7058 ||| 20823 ||| 20824 ||| 
2020 ||| gatae: graph attention-based anomaly detection on attributed networks. ||| 20825 ||| 20826 ||| 20827 ||| 1419 ||| 
2019 ||| learning spatial-channel attention for visual tracking. ||| 20828 ||| 11294 ||| 711 ||| 
2021 ||| meta-neighbor aggregated graph attention network for heterogeneous graph representation. ||| 20829 ||| 20826 ||| 
2020 ||| interactive attention encoder network with local context features for aspect-level sentiment analysis. ||| 20830 ||| 20831 ||| 
2019 ||| eye tracking as a method of neuromarketing for attention research - an empirical analysis using the online appointment booking platform from mercedes-benz. ||| 20832 ||| 20833 ||| 20834 ||| 
2020 ||| multimodal fusion with co-attention mechanism. ||| 5090 ||| 20835 ||| 
2021 ||| improving the performance of transformer context encoders for ner. ||| 20836 ||| 20837 ||| 20838 ||| 20839 ||| 
2020 ||| a method for dissolved gas forecasting in power transformers using ls-svm. ||| 20840 ||| 20841 ||| 
2021 ||| next generation multitarget trackers: random finite set methods vs transformer-based deep learning. ||| 20842 ||| 20843 ||| 20844 ||| 20845 ||| 20846 ||| 20847 ||| 
2019 ||| dual stream spatio-temporal motion fusion with self-attention for action recognition. ||| 13928 ||| 20848 ||| 14413 ||| 20849 ||| 
2021 ||| emotiv insight with convolutional neural network: visual attention test classification. ||| 20850 ||| 20851 ||| 20852 ||| 
2020 ||| aspect level sentiment analysis using bi-directional lstm encoder with the attention mechanism. ||| 20853 ||| 20854 ||| 
2021 ||| scale input adapted attention for image denoising using a densely connected u-net: sade-net. ||| 20855 ||| 20856 ||| 
2020 ||| simple fine-tuning attention modules for human pose estimation. ||| 4355 ||| 4356 ||| 20857 ||| 4353 ||| 
2021 ||| hybrid vision transformer for domain adaptable person re-identification. ||| 20858 ||| 20859 ||| 20860 ||| 
2019 ||| an hybrid attention-based system for the prediction of facial attributes. ||| 8080 ||| 17945 ||| 8081 ||| 
2019 ||| emotion recognition with spatial attention and temporal softmax pooling. ||| 20861 ||| 2351 ||| 5749 ||| 5748 ||| 
2018 ||| a computational model of multi-scale spatiotemporal attention in video data. ||| 20862 ||| 20863 ||| 20864 ||| 20865 ||| 
2021 ||| a super-resolution method of remote sensing image using transformers. ||| 20866 ||| 9467 ||| 20867 ||| 20868 ||| 1132 ||| 20869 ||| 
2021 ||| a static and dynamic co-attention network for social recommendation. ||| 20870 ||| 9467 ||| 20871 ||| 20872 ||| 20867 ||| 1132 ||| 
2017 ||| comparison of direct and transformer drive high voltage ultrasonic pulser topologies. ||| 20873 ||| 20874 ||| 20875 ||| 20876 ||| 20877 ||| 
2019 ||| a low-cost autonomous attention assessment system for robot intervention with autistic children. ||| 20878 ||| 20879 ||| 20880 ||| 20881 ||| 
2021 ||| a 3d rhythm-based serious game for collaboration improvement of children with attention deficit hyperactivity disorder (adhd). ||| 20882 ||| 20883 ||| 20884 ||| 20885 ||| 20886 ||| 
2021 ||| metrics based on attention metadata for learning resource and assessment repository. ||| 20887 ||| 20888 ||| 20889 ||| 
2017 ||| how to throw chocolate at students: a survey of extrinsic means for increased audience attention. ||| 10598 ||| 20890 ||| 20891 ||| 
2020 ||| urse forum with self-attention. ||| 1251 ||| 20892 ||| 1266 ||| 
2019 ||| domain specific nmt based on knowledge graph embedding and attention. ||| 2792 ||| 20893 ||| 20894 ||| 20895 ||| 
2022 ||| sea surface temperature prediction approach based on 3d cnn and lstm with attention mechanism. ||| 20896 ||| 20897 ||| 20898 ||| 20899 ||| 
2021 ||| sea surface temperature prediction approach based on 3d cnn and lstm with attention mechanism. ||| 20896 ||| 20897 ||| 20898 ||| 20899 ||| 
2022 ||| recognition of transformer high frequency partial discharge based on time domain feature. ||| 6736 ||| 20900 ||| 20901 ||| 20902 ||| 208 ||| 14184 ||| 20903 ||| 20904 ||| 20905 ||| 14183 ||| 8349 ||| 20906 ||| 20907 ||| 
2018 ||| attention based neural architecture for rumor detection with author context awareness. ||| 3960 ||| 3961 ||| 
2018 ||| lead-lag relationship between investor sentiment in social media9 investor attention in google, and stock return. ||| 20908 ||| 20909 ||| 20910 ||| 20911 ||| 20912 ||| 20913 ||| 
2018 ||| text classification based on lstm and attention. ||| 20914 ||| 
2021 ||| bronchial light microscopy image segmentation based on boundary attention. ||| 20915 ||| 20916 ||| 18890 ||| 20917 ||| 20918 ||| 20919 ||| 20920 ||| 
2020 ||| study of transformer fault diagnosis based on sparrow optimization algorithm. ||| 20921 ||| 7826 ||| 
2021 ||| cross-modal retrieval based on big transfer and regional maximum activation of convolutions with generalized attention. ||| 12301 ||| 20922 ||| 
2020 ||| survey on automatic text summarization and transformer models applicability. ||| 20923 ||| 20924 ||| 20925 ||| 
2017 ||| base on transmission line model to investigate the power margins of main transformers. ||| 7479 ||| 20926 ||| 7472 ||| 20927 ||| 20928 ||| 
2017 ||| capacity reduction of distribution transformer by harmonic effect. ||| 7479 ||| 20929 ||| 7472 ||| 20930 ||| 20931 ||| 
2017 ||| facial emotion recognition skills and measures in children and adolescents with attention deficit hyperactivity disorder (adhd). ||| 20932 ||| 20933 ||| 20934 ||| 20935 ||| 
2021 ||| a comparison of evolutionary and neural attention modeling relative to adversarial learning. ||| 20936 ||| 20937 ||| 20938 ||| 20939 ||| 20940 ||| 
2021 ||| attention-oriented brain storm optimization for multimodal optimization problems. ||| 1825 ||| 20941 ||| 
2020 ||| safety isolating transformer design using hyde-df algorithm. ||| 1994 ||| 20942 ||| 20943 ||| 20944 ||| 20945 ||| 20946 ||| 20947 ||| 
2020 ||| enhanced interactive estimation of distribution algorithms with attention mechanism and restricted boltzmann machine. ||| 20948 ||| 20949 ||| 20950 ||| 7826 ||| 20951 ||| 
2020 ||| the elephant in the interpretability room: why use attention as explanation when we have saliency methods? ||| 20952 ||| 20953 ||| 
2019 ||| transcoding compositionally: using attention to find more generalizable solutions. ||| 20954 ||| 3341 ||| 20955 ||| 3342 ||| 
2020 ||| unsupervised evaluation for question answering with transformers. ||| 20956 ||| 3586 ||| 20957 ||| 
2019 ||| from balustrades to pierre vinken: looking for syntax in transformer self-attentions. ||| 20958 ||| 20959 ||| 
2019 ||| analyzing the structure of attention in a transformer language model. ||| 3447 ||| 20960 ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| 20961 ||| 20962 ||| 20963 ||| 20964 ||| 
2019 ||| what does bert look at? an analysis of bert's attention. ||| 20965 ||| 20966 ||| 3348 ||| 20967 ||| 
2020 ||| probing for multilingual numerical understanding in transformer-based language models. ||| 20968 ||| 20969 ||| 20970 ||| 20971 ||| 
2020 ||| dissecting lottery ticket transformers: structural and behavioral study of sparse neural machine translation. ||| 20972 ||| 20973 ||| 
2020 ||| structured self-attentionweights encode semantics in sentiment analysis. ||| 20974 ||| 20975 ||| 20976 ||| 
2019 ||| detecting political bias in news articles using headline attention. ||| 20977 ||| 20978 ||| 10510 ||| 
2019 ||| learning the dyck language with attention-based seq2seq models. ||| 7193 ||| 20979 ||| 20980 ||| 
2017 ||| visual attention during simulated autonomous driving in the us and japan. ||| 20981 ||| 20982 ||| 20983 ||| 20984 ||| 
2018 ||| led visualizations for drivers' attention: an exploratory study on experience and associated information contents. ||| 20985 ||| 20986 ||| 20987 ||| 20988 ||| 20989 ||| 
2018 ||| reducing the attentional demands of in-vehicle touchscreens with stencil overlays. ||| 20990 ||| 20991 ||| 20992 ||| 20993 ||| 20994 ||| 1451 ||| 
2018 ||| let me finish before i take over: towards attention aware device integration in highly automated vehicles. ||| 20995 ||| 20996 ||| 20997 ||| 3831 ||| 20998 ||| 20999 ||| 
2018 ||| where to look: exploring peripheral cues for shifting attention to spatially distributed out-of-view objects. ||| 15371 ||| 20671 ||| 20672 ||| 21000 ||| 318 ||| 15372 ||| 20673 ||| 
2017 ||| tutorial: how does your hmi design affect the visual attention of the driver. ||| 21001 ||| 21002 ||| 
2019 ||| voices in self-driving cars should be assertive to more quickly grab a distracted driver's attention. ||| 15403 ||| 15404 ||| 15405 ||| 15406 ||| 
2017 ||| guiding driver visual attention with leds. ||| 21003 ||| 21004 ||| 
2020 ||| attention mechanism based adversarial attack against deep reinforcement learning. ||| 21005 ||| 21006 ||| 2349 ||| 21007 ||| 9003 ||| 
2020 ||| carnet: context attention refine network for semantic segmentation. ||| 21008 ||| 5126 ||| 515 ||| 
2021 ||| a transformer-based chinese non-autoregressive speech synthesis scheme. ||| 21009 ||| 21010 ||| 
2021 ||| meta-learning based siamese network with channel-wise self-attention for visual tracking. ||| 3049 ||| 15881 ||| 11999 ||| 
2019 ||| exploring the usability of nesplora aquarium, a virtual reality system for neuropsychological assessment of attention and executive functioning. ||| 17047 ||| 17048 ||| 17049 ||| 17050 ||| 21011 ||| 8648 ||| 17051 ||| 
2020 ||| modified playback of avatar clip sequences based on student attention in educational vr. ||| 16123 ||| 16122 ||| 16124 ||| 
2020 |||  movie cuts in users' attention. ||| 7540 ||| 227 ||| 7541 ||| 7542 ||| 
2018 ||| a path-based attention guiding technique for assembly environments with target occlusions. ||| 13866 ||| 21012 ||| 13867 ||| 
2018 ||| attention guiding using augmented reality in complex environments. ||| 13866 ||| 13867 ||| 
2020 ||| a methodology of eye gazing attention determination for vr training. ||| 21013 ||| 21014 ||| 1329 ||| 14924 ||| 
2019 ||| optimizing visual element placement via visual attention analysis. ||| 21015 ||| 21016 ||| 21017 ||| 21018 ||| 21019 ||| 21020 ||| 
2017 ||| coordinating attention and cooperation in multi-user virtual reality narratives. ||| 21021 ||| 21022 ||| 21023 ||| 21024 ||| 21025 ||| 
2021 ||| an interface for enhanced teacher awareness of student actions and attention in a vr classroom. ||| 21026 ||| 21027 ||| 21028 ||| 16124 ||| 
2017 ||| attention guidance for immersive video content in head-mounted displays. ||| 21029 ||| 21030 ||| 21031 ||| 
2018 ||| the relationship between visual attention and simulator sickness: a driving simulation study. ||| 21032 ||| 21033 ||| 21034 ||| 21035 ||| 21036 ||| 
2021 |||  task attention in cybersickness: a call for a standardized approach to data sharing. ||| 21037 ||| 21038 ||| 21039 ||| 21040 ||| 21041 ||| 21042 ||| 
2019 ||| the matter of attention and motivation - understanding unexpected results from auditory localization training using augmented reality. ||| 21043 ||| 21044 ||| 
2018 ||| empirical evaluation of virtual human conversational and affective animations on visual attention in inter-personal simulations. ||| 15939 ||| 21045 ||| 15940 ||| 15941 ||| 
2021 ||| text2gestures: a transformer-based network for generating emotive body gestures for virtual agents**this work has been supported in part by aro grants w911nf1910069 and w911nf1910315, and intel. code and additional materials available at: https: //gamma.umd.edu/t2g. ||| 8818 ||| 21046 ||| 21047 ||| 21048 ||| 8823 ||| 7315 ||| 
2019 ||| eye-gaze-triggered visual cues to restore attention in educational vr. ||| 16122 ||| 16123 ||| 16124 ||| 
2021 ||| exploiting object-of-interest information to understand attention in vr classrooms. ||| 21049 ||| 21050 ||| 21051 ||| 21052 ||| 21053 ||| 7218 ||| 21054 ||| 17425 ||| 
2018 ||| towards joint attention training for children with asd - a vr game approach and eye gaze exploration. ||| 14924 ||| 21055 ||| 21056 ||| 21057 ||| 
2020 ||| directing versus attracting attention: exploring the effectiveness of central and peripheral cues in panoramic videos. ||| 21058 ||| 21059 ||| 21060 ||| 21061 ||| 21062 ||| 
2020 ||| infosalgail: visual attention-empowered imitation learning of pedestrian behavior in critical traffic scenarios. ||| 21063 ||| 21064 ||| 3369 ||| 21065 ||| 21066 ||| 3831 ||| 
2021 ||| collecting data for machine learning on office workers' attention, fatigue, overload, and stress during computer use. ||| 21067 ||| 
2021 ||| spatio-temporal attention mechanism and knowledge distillation for lip reading. ||| 21068 ||| 21069 ||| 21070 ||| 21071 ||| 21072 ||| 21073 ||| 21074 ||| 
2020 ||| combining angiodysplasia classification and segmentation on capsule endoscopy images using attentional albunet. ||| 21075 ||| 15786 ||| 
2019 ||| attention guided relation network for few-shot image classification. ||| 21076 ||| 21077 ||| 21078 ||| 21079 ||| 
2020 ||| enhance attentional lstm models for power consumption forecasting using asymmetric loss and renewable energy factors. ||| 21080 ||| 15786 ||| 
2020 ||| machine reading comprehension on multiclass questions using bidirectional attention flow models with contextual embeddings and transfer learning in thai corpus. ||| 21081 ||| 15786 ||| 
2021 ||| sanger: a co-design framework for enabling sparse attention using reconfigurable architecture. ||| 21082 ||| 21083 ||| 21084 ||| 21085 ||| 3675 ||| 128 ||| 16809 ||| 
2020 ||| gobo: quantizing attention-based nlp models for low latency and energy efficient inference. ||| 21086 ||| 21087 ||| 21088 ||| 21089 ||| 
2020 ||| abstract podcast summarization using bart with longformer attention. ||| 21090 ||| 21091 ||| 
2020 ||| evaluating transformer-kernel models at trec deep learning 2020. ||| 9583 ||| 9584 ||| 9615 ||| 
2020 ||| impact of tokenization, pretraining task, and transformer depth on text ranking. ||| 21092 ||| 21093 ||| 21094 ||| 
2019 ||| a method to estimate request sentences using lstm with self-attention mechanism. ||| 21095 ||| 21096 ||| 21097 ||| 21098 ||| 
2017 ||| effects of design factors of game-based english vocabulary learning app on learning performance, sustained attention, emotional state, and memory retention. ||| 21099 ||| 21100 ||| 21101 ||| 
2017 ||| an english diagnosis and review system based on brainwave attention recognition technology for the paper-based learning context with digital-pen support. ||| 21102 ||| 21100 ||| 21103 ||| 
2017 ||| internal fault classification algorithm in power transformer based on discrete wavelet transform and fuzzy logic. ||| 21104 ||| 21105 ||| 7717 ||| 21106 ||| 7718 ||| 7719 ||| 
2018 ||| advertising visual attention to facebook social network: evidence from eye movements. ||| 17329 ||| 17330 ||| 21107 ||| 21108 ||| 
2018 ||| using brainwave characteristics for exploring the effect of integrating graduated-prompting strategy into interactive e-books on students' learning attention. ||| 21109 ||| 21110 ||| 21111 ||| 
2020 ||| text entailment generation with attention-based sequence-to-sequence model. ||| 21112 ||| 21113 ||| 
2020 ||| multiple perspective caption generation with attention mechanism. ||| 21113 ||| 21114 ||| 
2019 ||| combining virtual reality advertising and eye tracking to understand visual attention: a pilot study. ||| 17329 ||| 17331 ||| 21115 ||| 
2019 ||| a learning environment attracting attention to the nested structure of metacognition of self-regulated learning. ||| 21116 ||| 21117 ||| 21118 ||| 21119 ||| 
2018 ||| improving effectiveness of learners' review of video lectures by using an attention-based video lecture review mechanism based on brainwave signals. ||| 21120 ||| 21100 ||| 21121 ||| 
2019 ||| attention analysis in caption generation. ||| 21114 ||| 21113 ||| 
2021 ||| multi-intent attention and top-k network with interactive framework for joint multiple intent detection and slot filling. ||| 19030 ||| 21122 ||| 21123 ||| 3626 ||| 
2020 ||| multi-layer joint learning of chinese nested named entity recognition based on self-attention mechanism. ||| 21124 ||| 21125 ||| 21126 ||| 3088 ||| 
2021 ||| adaptive transformer for multilingual neural machine translation. ||| 21127 ||| 1416 ||| 15311 ||| 5791 ||| 1417 ||| 
2020 ||| transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis. ||| 4784 ||| 21128 ||| 4790 ||| 4845 ||| 4847 ||| 
2020 ||| sentence constituent-aware aspect-category sentiment analysis with graph attention networks. ||| 21129 ||| 21130 ||| 6312 ||| 
2018 ||| a3net: adversarial-and-attention network for machine reading comprehension. ||| 19478 ||| 21131 ||| 21132 ||| 21133 ||| 945 ||| 4783 ||| 21134 ||| 
2020 ||| memory attention neural network for multi-domain dialogue state tracking. ||| 21135 ||| 3148 ||| 3147 ||| 3150 ||| 3151 ||| 
2019 ||| a transformer-based semantic parser for nlpcc-2019 shared task 2. ||| 21136 ||| 3564 ||| 21137 ||| 
2020 ||| encoding sentences with a syntax-aware self-attention neural network for emotion distribution prediction. ||| 8903 ||| 8904 ||| 
2018 ||| cross-lingual emotion classification with auxiliary and attention neural networks. ||| 2037 ||| 19849 ||| 3085 ||| 11773 ||| 3088 ||| 
2021 ||| autonlu: architecture search for sentence and cross-sentence attention modeling with re-designed search space. ||| 3394 ||| 
2019 ||| attentional neural network for emotion detection in conversations with speaker influence awareness. ||| 18807 ||| 1311 ||| 1312 ||| 8426 ||| 1309 ||| 
2021 ||| attention based reinforcement learning with reward shaping for knowledge graph reasoning. ||| 1270 ||| 6913 ||| 12692 ||| 
2020 ||| deep hierarchical attention flow for visual commonsense reasoning. ||| 21138 ||| 21139 ||| 
2021 ||| autotrans: automating transformer design via reinforced architecture search. ||| 3394 ||| 11136 ||| 3392 ||| 3395 ||| 
2021 ||| searching effective transformer for seq2seq keyphrase generation. ||| 18640 ||| 21140 ||| 21141 ||| 21142 ||| 336 ||| 3272 ||| 3273 ||| 
2020 ||| dca: diversified co-attention towards informative live video commenting. ||| 8663 ||| 21143 ||| 21144 ||| 21145 ||| 21146 ||| 
2019 ||| interpretable spatial-temporal attention graph convolution network for service part hierarchical demand forecast. ||| 21147 ||| 21148 ||| 21149 ||| 21150 ||| 21151 ||| 21152 ||| 7417 ||| 
2019 ||| many vs. many query matching with hierarchical bert and transformer. ||| 6781 ||| 21153 ||| 1751 ||| 3085 ||| 3088 ||| 
2019 ||| charge prediction with legal attention. ||| 21154 ||| 13573 ||| 21155 ||| 21156 ||| 9635 ||| 
2019 ||| feature-level attention based sentence encoding for neural relation extraction. ||| 21157 ||| 728 ||| 21158 ||| 
2021 ||| multi-modal sarcasm detection based on contrastive attention mechanism. ||| 21159 ||| 3503 ||| 21160 ||| 
2018 ||| using entity relation to improve event detection via attention mechanism. ||| 13567 ||| 21161 ||| 11628 ||| 13568 ||| 1254 ||| 
2018 ||| hierarchical attention based semi-supervised network representation learning. ||| 3114 ||| 21162 ||| 21163 ||| 21164 ||| 
2019 ||| using bidirectional transformer-crf for spoken language understanding. ||| 11661 ||| 11662 ||| 
2020 ||| a submodular optimization-based vae-transformer framework for paraphrase generation. ||| 21165 ||| 721 ||| 21166 ||| 1147 ||| 286 ||| 5235 ||| 
2017 ||| abstractive document summarization via neural model with joint attention. ||| 21167 ||| 8179 ||| 21168 ||| 
2021 ||| a dual-attention neural network for pun location and using pun-gloss pairs for interpretation. ||| 21169 ||| 21170 ||| 6928 ||| 21171 ||| 350 ||| 349 ||| 
2018 ||| employing multiple decomposable attention networks to resolve event coreference. ||| 2904 ||| 221 ||| 3088 ||| 
2019 ||| co-attention networks for aspect-level sentiment analysis. ||| 21172 ||| 527 ||| 21173 ||| 528 ||| 21174 ||| 
2017 ||| a convolutional attention model for text classification. ||| 21175 ||| 3662 ||| 15906 ||| 3664 ||| 
2021 ||| context-aware and data-augmented transformer for interactive argument pair identification. ||| 21176 ||| 21177 ||| 2532 ||| 21178 ||| 8967 ||| 8974 ||| 
2019 ||| neural machine translation with bilingual history involved attention. ||| 21179 ||| 3076 ||| 1283 ||| 13735 ||| 14653 ||| 
2019 ||| co-attention and aggregation based chinese recognizing textual entailment model. ||| 21180 ||| 13582 ||| 13573 ||| 
2019 ||| variational attention for commonsense knowledge aware conversation generation. ||| 21181 ||| 3813 ||| 3129 ||| 1418 ||| 
2017 ||| look-ahead attention for generation in neural machine translation. ||| 21182 ||| 3044 ||| 11690 ||| 
2019 ||| kg-to-text generation with slot-attention and link-attention. ||| 21183 ||| 6275 ||| 21184 ||| 21185 ||| 
2021 ||| knowledge enhanced transformers system for claim stance classification. ||| 18018 ||| 6679 ||| 11660 ||| 21186 ||| 6590 ||| 
2020 ||| hierarchical multimodal transformer with localness and speaker aware attention for emotion recognition in conversations. ||| 21187 ||| 3827 ||| 21188 ||| 3828 ||| 21189 ||| 10842 ||| 
2020 ||| hierarchical multi-view attention for neural review-based recommendation. ||| 4166 ||| 696 ||| 21190 ||| 478 ||| 8197 ||| 21191 ||| 697 ||| 
2019 ||| using dependency information to enhance attention mechanism for aspect-based sentiment analysis. ||| 21192 ||| 4430 ||| 12196 ||| 384 ||| 5556 ||| 
2019 ||| multi-task multi-head attention memory network for fine-grained sentiment analysis. ||| 21193 ||| 7697 ||| 19365 ||| 21194 ||| 21195 ||| 21196 ||| 21197 ||| 21198 ||| 
2020 ||| rumor detection on hierarchical attention network with user and sentiment information. ||| 21199 ||| 21200 ||| 221 ||| 21201 ||| 222 ||| 
2019 ||| improving multi-head attention with capsule networks. ||| 21202 ||| 3076 ||| 
2018 ||| a novel attention based cnn model for emotion intensity prediction. ||| 5354 ||| 1311 ||| 1312 ||| 8426 ||| 
2019 ||| cross aggregation of multi-head attention for neural machine translation. ||| 21203 ||| 3111 ||| 3151 ||| 
2018 ||| dependency parsing and attention network for aspect-level sentiment classification. ||| 21204 ||| 651 ||| 
2021 ||| skeleton-based sign language recognition with attention-enhanced graph convolutional networks. ||| 21205 ||| 21206 ||| 
2019 ||| extending the transformer with context and multi-dimensional mechanism for dialogue response generation. ||| 5321 ||| 5322 ||| 5235 ||| 286 ||| 
2018 ||| summary++: summarizing chinese news articles with attention. ||| 21207 ||| 21208 ||| 92 ||| 21209 ||| 
2017 ||| ahnn: an attention-based hybrid neural network for sentence modeling. ||| 21210 ||| 13781 ||| 13783 ||| 
2021 ||| variational autoencoder with interactive attention for affective text generation. ||| 10629 ||| 3313 ||| 3315 ||| 
2018 ||| research on construction method of chinese nt clause based on attention-lstm. ||| 21211 ||| 21212 ||| 21213 ||| 13733 ||| 
2019 ||| improving transformer with sequential context representations for abstractive text summarization. ||| 476 ||| 21214 ||| 21215 ||| 479 ||| 21216 ||| 
2017 ||| an effective approach for chinese news headline classification based on multi-representation mixed model with attention and ensemble learning. ||| 21217 ||| 21218 ||| 21219 ||| 21220 ||| 21221 ||| 
2017 ||| detecting deceptive review spam via attention-based neural networks. ||| 21222 ||| 3129 ||| 1418 ||| 
2020 ||| non-local second-order attention network for single image super resolution. ||| 21223 ||| 21224 ||| 
2021 ||| transformer-based hierarchical encoder for document classification. ||| 21225 ||| 21226 ||| 18532 ||| 
2021 ||| attention augmented convolutional transformer for tabular time-series. ||| 21227 ||| 21228 ||| 
2021 ||| transformers4rec: bridging the gap between nlp and sequential / session-based recommendation. ||| 21229 ||| 21230 ||| 21231 ||| 21232 ||| 21233 ||| 
2020 ||| tafa: two-headed attention fused autoencoder for context-aware recommendations. ||| 21234 ||| 21235 ||| 21236 ||| 2600 ||| 2583 ||| 
2021 ||| tops, bottoms, and shoes: building capsule wardrobes via cross-attention tensor network. ||| 21237 ||| 9049 ||| 2355 ||| 2792 ||| 
2018 ||| risk "attention" or "adventure": a qualitative study of novelty and familiarity in music listening. ||| 21238 ||| 21239 ||| 21240 ||| 21241 ||| 
2020 ||| fissa: fusing item similarity models with self-attention networks for sequential recommendation. ||| 21242 ||| 21243 ||| 21244 ||| 
2020 ||| meantime: mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. ||| 21245 ||| 21246 ||| 21247 ||| 
2017 ||| modeling user session and intent with an attention-based encoder-decoder architecture. ||| 21248 ||| 4811 ||| 21249 ||| 
2020 ||| sse-pt: sequential recommendation via personalized transformer. ||| 21250 ||| 21251 ||| 21252 ||| 21253 ||| 
2017 ||| interpretable convolutional neural networks with dual local and global attention for review rating prediction. ||| 21254 ||| 14316 ||| 2792 ||| 9337 ||| 
2018 ||| qualitydeepsense: quality-aware deep learning framework for internet of things applications with sensor-temporal attention. ||| 3366 ||| 20490 ||| 5142 ||| 17171 ||| 
2020 ||| a novel three-phase transformerless cascaded multilevel inverter topology for grid-connected solar pv applications. ||| 21255 ||| 21256 ||| 21257 ||| 
2017 ||| different shapes and dimensions of laminated core on characteristics of a practical single-phase distribution transformer using finite-element analysis. ||| 1052 ||| 21258 ||| 21259 ||| 
2019 ||| a new magnetic linked active neutral point clamp converter for transformer-less direct grid integration of solar photovoltaic systems. ||| 21260 ||| 21261 ||| 21262 ||| 21263 ||| 
2021 ||| a new h7 transformer-less single-phase inverter to improve the performance of grid-connected solar photovoltaic systems. ||| 21264 ||| 21265 ||| 21261 ||| 21266 ||| 21262 ||| 
2020 ||| portable device for transformer oil inhibitor content analysis using near-infrared spectroscopy wavelength. ||| 21267 ||| 21268 ||| 21269 ||| 21270 ||| 21271 ||| 21272 ||| 21273 ||| 
2019 ||| efficiency characterization and optimal power sharing in a unified ac-dc system employing a line-frequency zig-zag transformer with high winding leakage inductance. ||| 10850 ||| 10851 ||| 10852 ||| 
2017 ||| mitigation of electric arc furnace transformer inrush current using soft-starter-based controlled energization. ||| 21274 ||| 21275 ||| 21276 ||| 
2021 ||| a charging strategy for electric vehicle fast charging station to mitigate distribution transformer aging and reduce operation cost. ||| 21277 ||| 21278 ||| 21279 ||| 21280 ||| 21281 ||| 21282 ||| 
2018 ||| a seamless transition scheme of position sensorless control in industrial permanent magnet motor (pmm) drives with output filter and transformer for electric submersible pumps. ||| 21283 ||| 21284 ||| 21285 ||| 21286 ||| 21287 ||| 
2018 ||| design and implementation of 10-kv mw-level electronic power transformer (ept). ||| 2182 ||| 21288 ||| 344 ||| 21289 ||| 5745 ||| 
2019 ||| physics-based design optimization of high frequency transformers for solid state transformer applications. ||| 21290 ||| 21291 ||| 21292 ||| 21293 ||| 
2021 ||| a multilevel solid-state transformer-based grid-connected solar photovoltaic systems. ||| 21294 ||| 21295 ||| 21296 ||| 21297 ||| 
2019 ||| comparative analysis of oil-filled transformer and solid-state transformer for electric arc furnace. ||| 21298 ||| 3882 ||| 21299 ||| 21300 ||| 21301 ||| 3882 ||| 
2020 ||| design and implementation of a smart solid-state transformer based power converter for solar pv system. ||| 21302 ||| 21295 ||| 
2017 ||| transformer management system for energy control of customer demand response and pv systems. ||| 21303 ||| 21304 ||| 21305 ||| 21306 ||| 
2018 ||| analysis of a shunt phase-shift transformer for multi-generator harmonic elimination. ||| 21307 ||| 21308 ||| 
2020 ||| calculation of model based capacitances of a two-winding high-frequency transformer to predict its natural resonance frequencies. ||| 10850 ||| 10852 ||| 
2020 |||  solid-state transformers. ||| 21257 ||| 21309 ||| 
2020 ||| a soft-switching transformer-less step-down converter based on resonant current balance module. ||| 21310 ||| 21311 ||| 21312 ||| 6078 ||| 1160 ||| 21313 ||| 
2018 ||| condition monitoring techniques of dielectrics in liquid immersed power transformers - a review. ||| 21314 ||| 21315 ||| 
2021 ||| h9 and h10 transformer-less solar photovoltaic inverters for leakage current suppression and harmonic current reduction. ||| 21316 ||| 21317 ||| 21265 ||| 21261 ||| 21266 ||| 21262 ||| 
2020 ||| soft-switching, self-tuning and optimization technique for solid state transformers based on direct ac-ac matrix converter topology. ||| 21290 ||| 21292 ||| 21293 ||| 
2018 ||| a controlled switching approach to reduction of three-phase transformer inrush currents. ||| 21318 ||| 21319 ||| 21320 ||| 
2020 ||| transformerless six-switch (h6)-based single-phase inverter for grid-connected photovoltaic system with reducd leakage current. ||| 21321 ||| 21322 ||| 21323 ||| 21324 ||| 
2017 ||| reduction of three-phase voltage unbalance subject to special winding connections of two single-phase distribution transformers of a microgrid system using a designed d-statcom controller. ||| 1052 ||| 21325 ||| 21326 ||| 21327 ||| 21328 ||| 21329 ||| 21259 ||| 
2019 |||  converter transformers. ||| 21257 ||| 21330 ||| 21331 ||| 21309 ||| 
2018 ||| enhancement of the dbd power for current-mode converters using the step-up transformer elements. ||| 21332 ||| 21333 ||| 21334 ||| 10318 ||| 21335 ||| 
2017 ||| a cascaded dstatcom integrated with d-y connection distribution transformer for reactive power compensation. ||| 16833 ||| 21336 ||| 21337 ||| 21338 ||| 21339 ||| 1419 ||| 
2018 ||| exploring a unified attention-based pooling framework for speaker verification. ||| 1199 ||| 329 ||| 21340 ||| 10107 ||| 
2021 ||| sams-net: a sliced attention-based neural network for music source separation. ||| 14621 ||| 1060 ||| 21341 ||| 765 ||| 
2021 ||| leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning. ||| 21342 ||| 4467 ||| 21343 ||| 21344 ||| 4469 ||| 14279 ||| 12203 ||| 
2021 ||| transformer-based empathetic response generation using dialogue situation and advanced-level definition of empathy. ||| 21345 ||| 4387 ||| 4388 ||| 21346 ||| 
2018 ||| hybrid ctc-attention based end-to-end speech recognition using subword units. ||| 21347 ||| 21348 ||| 12772 ||| 3508 ||| 
2021 ||| context-dependent label smoothing regularization for attention-based end-to-end code-switching speech recognition. ||| 21349 ||| 3675 ||| 21350 ||| 12104 ||| 8298 ||| 
2021 ||| non-autoregressive deliberation-attention based end-to-end asr. ||| 12100 ||| 12101 ||| 1086 ||| 12104 ||| 8298 ||| 
2021 ||| improving attention-based end-to-end asr by incorporating an n-gram neural network. ||| 21351 ||| 21352 ||| 
2021 ||| dialogue act recognition using branch architecture with attention mechanism for imbalanced data. ||| 5092 ||| 5093 ||| 5094 ||| 5095 ||| 
2018 ||| multi-head attention for end-to-end neural machine translation. ||| 21353 ||| 21354 ||| 
2021 ||| an attention-augmented fully convolutional neural network for monaural speech enhancement. ||| 21355 ||| 18141 ||| 242 ||| 21356 ||| 
2018 ||| an analysis of decoding for attention-based end-to-end mandarin speech recognition. ||| 12257 ||| 12263 ||| 21357 ||| 21358 ||| 11688 ||| 
2021 ||| an investigation of positional encoding in transformer-based end-to-end speech recognition. ||| 21359 ||| 21352 ||| 
2021 ||| attention-guided cutmix data augmentation network for fine-grained bird recognition. ||| 21360 ||| 21361 ||| 21362 ||| 
2021 ||| chinese entity relation extraction based on multi-level gated recurrent mechanism and self-attention. ||| 21363 ||| 
2021 ||| target-specific sentiment analysis of dual-channel interactive attention network. ||| 21364 ||| 21365 ||| 21366 ||| 21367 ||| 
2021 ||| application of channel attention for speaker recognition in the wild. ||| 3148 ||| 3279 ||| 
2021 ||| a transformer network for captcha recognition. ||| 1557 ||| 189 ||| 3166 ||| 21368 ||| 21369 ||| 
2021 ||| entity relationship extraction based on bi-lstm and attention mechanism. ||| 9476 ||| 21370 ||| 21371 ||| 
2021 ||| irony recognition combined with lda and improved one-dimensional intra-attention model. ||| 21364 ||| 21372 ||| 21373 ||| 21374 ||| 
2021 ||| audio-visual salieny network with audio attention module. ||| 21375 ||| 21376 ||| 21377 ||| 21378 ||| 
2020 ||| imshell-dec: pay more attention to external links in powershell. ||| 21379 ||| 497 ||| 15851 ||| 21380 ||| 21381 ||| 4398 ||| 
2017 ||| on active sharing and responses to joint attention bids by children with autism in a loosely coupled collaborative play environment. ||| 13868 ||| 13869 ||| 21382 ||| 
2017 ||| a multi-user tabletop application to train children with autism social attention coordination skills without forcing eye-gaze following. ||| 13869 ||| 13868 ||| 
2020 ||| cross-lingual transformers for neural automatic post-editing. ||| 21383 ||| 
2018 ||| a transformer-based multi-source automatic post-editing system. ||| 21384 ||| 21385 ||| 21386 ||| 6787 ||| 3207 ||| 
2018 ||| input combination strategies for multi-source transformer decoder. ||| 3591 ||| 3592 ||| 20958 ||| 
2018 ||| parameter sharing methods for multilingual self-attentional translation models. ||| 21387 ||| 3067 ||| 
2018 ||| cuni transformer neural mt system for wmt18. ||| 21388 ||| 
2018 ||| an analysis of attention mechanisms: the case of word sense disambiguation in neural machine translation. ||| 21389 ||| 3847 ||| 21390 ||| 
2019 ||| transformer-based automatic post-editing model with joint encoder and multi-source attention of decoder. ||| 21391 ||| 21392 ||| 4941 ||| 
2018 ||| neural machine translation with the transformer and multi-source romance languages for the biomedical wmt 2018 task. ||| 21393 ||| 3466 ||| 
2019 ||| the en-ru two-way integrated machine translation system based on transformer. ||| 21394 ||| 
2020 ||| translating similar languages: role of mutual intelligibility in multilingual transformers. ||| 21395 ||| 3154 ||| 3152 ||| 
2020 ||| attention transformer model for translation of similar languages. ||| 21396 ||| 21397 ||| 
2018 ||| multi-source transformer with combined losses for automatic post editing. ||| 21398 ||| 21399 ||| 14627 ||| 14628 ||| 
2018 ||| multi-encoder transformer network for automatic post-editing. ||| 21392 ||| 4941 ||| 
2020 ||| transformer-based neural machine translation system for hindi - marathi: wmt20 shared task. ||| 2209 ||| 21400 ||| 15996 ||| 10306 ||| 
2021 ||| translation transformers rediscover inherent data domains. ||| 21401 ||| 21402 ||| 21403 ||| 
2018 ||| quality estimation with force-decoded attention and cross-lingual embeddings. ||| 21404 ||| 21405 ||| 21406 ||| 21403 ||| 
2020 ||| filtering noisy parallel corpus using transformers with proxy task learning. ||| 21407 ||| 21408 ||| 6089 ||| 21409 ||| 21410 ||| 21411 ||| 21412 ||| 21413 ||| 21414 ||| 
2019 ||| incorporating source syntax into transformer-based neural machine translation. ||| 21415 ||| 21416 ||| 
2018 ||| ms-uedin submission to the wmt2018 ape shared task: dual-source transformer for automatic post-editing. ||| 21417 ||| 21418 ||| 
2019 ||| english-czech systems in wmt19: document-level transformer. ||| 21388 ||| 21419 ||| 7440 ||| 21420 ||| 21421 ||| 21422 ||| 
2021 ||| direct exploitation of attention weights for translation quality estimation. ||| 21423 ||| 21403 ||| 
2018 ||| on the alignment problem in multi-head attention-based neural machine translation. ||| 21424 ||| 21425 ||| 3454 ||| 
2019 ||| multi-source transformer for kazakh-russian-english neural machine translation. ||| 21426 ||| 21427 ||| 21428 ||| 21429 ||| 
2017 ||| biasing attention-based recurrent neural networks using external alignment information. ||| 21424 ||| 3454 ||| 
2019 ||| fast and accurate capitalization and punctuation for automatic speech recognition using transformer and chunk merging. ||| 21430 ||| 7526 ||| 21431 ||| 21432 ||| 21433 ||| 7527 ||| 7528 ||| 
2021 ||| simultaneous speech-to-speech translation system with transformer-based incremental asr, mt, and tts. ||| 21434 ||| 14652 ||| 11754 ||| 21435 ||| 21436 ||| 21437 ||| 14325 ||| 21438 ||| 21439 ||| 13907 ||| 11756 ||| 11757 ||| 
2021 ||| multi-encoder sequential attention network for context-aware speech recognition in japanese dialog conversation. ||| 21440 ||| 13907 ||| 11757 ||| 
2018 ||| from coarse attention to fine-grained gaze: a two-stage 3d fully convolutional network for predicting eye gaze in first person video. ||| 9383 ||| 9384 ||| 4997 ||| 21441 ||| 
2019 ||| construct dynamic graphs for hand gesture recognition via spatial-temporal attention. ||| 8946 ||| 21442 ||| 9187 ||| 1036 ||| 1749 ||| 
2020 ||| image harmonization with attention-based deep feature modulation. ||| 21443 ||| 20045 ||| 21444 ||| 
2018 ||| stacked dense u-nets with dual transformers for robust face alignment. ||| 5932 ||| 21445 ||| 21446 ||| 18875 ||| 
2018 ||| reciprocal attention fusion for visual question answering. ||| 20206 ||| 1969 ||| 
2017 ||| deep reinforcement learning attention selection for person re-identification. ||| 21447 ||| 21448 ||| 19101 ||| 2196 ||| 
2019 ||| learning target-aware attention for robust tracking with conditional adversarial network. ||| 5957 ||| 11453 ||| 4150 ||| 382 ||| 
2019 ||| body part alignment and temporal attention pooling for video-based person re-identification. ||| 2211 ||| 21449 ||| 
2018 ||| ican: instance-centric attention network for human-object interaction detection. ||| 17109 ||| 21450 ||| 21451 ||| 
2019 ||| joint spatial and layer attention for convolutional networks. ||| 21452 ||| 1850 ||| 21453 ||| 
2019 ||| document binarization using recurrent attention generative model. ||| 9040 ||| 21454 ||| 21455 ||| 9039 ||| 2446 ||| 155 ||| 
2020 ||| attention distillation for learning video representations. ||| 8764 ||| 19066 ||| 6553 ||| 8766 ||| 8692 ||| 
2019 ||| spatial transformer spectral kernels for deformable image registration. ||| 21456 ||| 21457 ||| 
2017 ||| autoscaler: scale-attention networks for visual correspondence. ||| 9236 ||| 21458 ||| 3402 ||| 2383 ||| 
2019 ||| relation-aware multiple attention siamese networks for robust visual tracking. ||| 21459 ||| 9378 ||| 9377 ||| 1916 ||| 1788 ||| 
2020 ||| asap-net: attention and structure aware point cloud sequence segmentation. ||| 17960 ||| 21460 ||| 4900 ||| 5136 ||| 8660 ||| 286 ||| 
2019 ||| pcas: pruning channels with attention statistics for deep network compression. ||| 21461 ||| 21462 ||| 
2018 ||| regional attention based deep feature for image retrieval. ||| 21463 ||| 21464 ||| 
2020 ||| contrastively-reinforced attention convolutional neural network for fine-grained image recognition. ||| 11249 ||| 3906 ||| 11251 ||| 11250 ||| 
2018 ||| human activity recognition with pose-driven attention to rgb. ||| 7968 ||| 7969 ||| 7970 ||| 
2018 ||| destnet: densely fused spatial transformer networks. ||| 21465 ||| 21466 ||| 21467 ||| 
2020 ||| paying more attention to snapshots of iterative pruning: improving model compression via ensemble distillation. ||| 13716 ||| 21468 ||| 13719 ||| 
2018 ||| point attention network for gesture recognition using point cloud data. ||| 21469 ||| 21470 ||| 21471 ||| 
2020 ||| two-stream spatiotemporal compositional attention network for videoqa. ||| 21472 ||| 20473 ||| 21473 ||| 
2020 ||| a better use of audio-visual cues: dense video captioning with bi-modal transformer. ||| 21474 ||| 6321 ||| 
2020 ||| marginalized graph attention hashing for zero-shot image retrieval. ||| 21475 ||| 21476 ||| 21477 ||| 21478 ||| 1717 ||| 4201 ||| 
2019 ||| ms-gan: text to image synthesis with attention-modulated generators and similarity-aware discriminators. ||| 21479 ||| 9378 ||| 9377 ||| 1916 ||| 1788 ||| 
2019 ||| attention-based facial behavior analytics insocial communication. ||| 1743 ||| 21480 ||| 21481 ||| 21482 ||| 21483 ||| 21484 ||| 1749 ||| 
2018 ||| progressive attention networks for visual attribute prediction. ||| 9275 ||| 18766 ||| 21485 ||| 7142 ||| 9277 ||| 
2018 ||| attentional alignment networks. ||| 1927 ||| 21486 ||| 21487 ||| 7240 ||| 1930 ||| 1931 ||| 
2019 ||| graph-based knowledge distillation by multi-head attention network. ||| 21488 ||| 21489 ||| 
2018 ||| attention is all we need: nailing down object-centric attention for egocentric activity recognition. ||| 9832 ||| 9833 ||| 
2019 ||| spatially and temporally efficient non-local attention network for video-based person re-identification. ||| 8815 ||| 8816 ||| 6328 ||| 8817 ||| 
2018 ||| deep attentional structured representation learning for visual recognition. ||| 21490 ||| 8505 ||| 
2018 ||| bam: bottleneck attention module. ||| 5369 ||| 7916 ||| 7917 ||| 5372 ||| 
2020 ||| learning to pay attention to mistakes. ||| 21491 ||| 21492 ||| 21493 ||| 21494 ||| 
2019 ||| progressive face super-resolution via attention to facial landmark. ||| 21495 ||| 21496 ||| 2047 ||| 7383 ||| 
2020 ||| sofa-net: second-order and first-order attention network for crowd counting. ||| 21497 ||| 833 ||| 19593 ||| 
2019 ||| attentional demand estimation with attentive driving models. ||| 21498 ||| 21499 ||| 21500 ||| 
2019 ||| end-to-end information extraction by character-level embedding and multi-stage attentional u-net. ||| 21501 ||| 21502 ||| 
2018 ||| recurrent transformer network for remote sensing scene categorisation. ||| 21503 ||| 833 ||| 21504 ||| 1932 ||| 
2019 ||| focused attention for action recognition. ||| 21505 ||| 8543 ||| 2093 ||| 
2018 ||| self-attention learning for person re-identification. ||| 21506 ||| 6650 ||| 6627 ||| 
2017 ||| semantic segmentation with reverse attention. ||| 21507 ||| 21508 ||| 21509 ||| 6718 ||| 14521 ||| 
2020 ||| conditional attention for content-based image retrieval. ||| 21510 ||| 21511 ||| 
2018 ||| pyramid attention network for semantic segmentation. ||| 21512 ||| 6397 ||| 7424 ||| 21513 ||| 
2020 ||| taxi demand prediction based on lstm with residuals and multi-head attention. ||| 21514 ||| 21515 ||| 
2020 ||| light field image compression using multi-branch spatial transformer networks based view synthesis. ||| 3313 ||| 21516 ||| 21517 ||| 21518 ||| 621 ||| 
2020 ||| wide and deep learning for video summarization via attention mechanism and independently recurrent neural network. ||| 21519 ||| 218 ||| 
2017 ||| analyzing students' attention in class using wearable devices. ||| 1340 ||| 21520 ||| 21521 ||| 21522 ||| 21523 ||| 
2021 ||| sign language translation using multi context transformer. ||| 21524 ||| 21525 ||| 21526 ||| 21527 ||| 21528 ||| 
2021 ||| nahuatl neural machine translation using attention based architectures: a comparative analysis for rnns and transformers as a mobile application service. ||| 21529 ||| 3419 ||| 13535 ||| 21530 ||| 21531 ||| 852 ||| 21532 ||| 21533 ||| 21534 ||| 10314 ||| 852 ||| 21535 ||| 21536 ||| 21537 ||| 10314 ||| 
2020 ||| data augmentation with transformers for text classification. ||| 852 ||| 21538 ||| 21539 ||| 21540 ||| 
2021 ||| modelling of high frequency coreless planar transformer with twr hexagonal winding. ||| 21541 ||| 21542 ||| 21543 ||| 
2021 ||| effect of harmonics current on the performance of current transformers. ||| 21544 ||| 21545 ||| 
2018 ||| retargeting 4k video for mobile access using visual attention and temporal stabilization. ||| 21546 ||| 21547 ||| 9933 ||| 7033 ||| 9932 ||| 
2019 ||| entity synonym discovery via multiple attentions. ||| 21548 ||| 21549 ||| 7804 ||| 21550 ||| 
2019 ||| attention-based direct interaction model for knowledge graph embedding. ||| 21551 ||| 3128 ||| 3129 ||| 1418 ||| 
2019 ||| dispute generation in law documents via joint context and topic attention. ||| 11702 ||| 21552 ||| 21553 ||| 5528 ||| 444 ||| 21554 ||| 11703 ||| 
2021 ||| identification of dietary supplement use from electronic health records using transformer-based language models. ||| 21555 ||| 21556 ||| 21557 ||| 21558 ||| 21559 ||| 13702 ||| 21560 ||| 3248 ||| 
2021 ||| automatic assignment of icd-10 codes to diagnostic texts using transformers based techniques. ||| 21561 ||| 21562 ||| 21563 ||| 21564 ||| 
2021 ||| identify diabetic retinopathy-related clinical concepts using transformer-based natural language processing methods. ||| 21565 ||| 13408 ||| 21566 ||| 21567 ||| 21568 ||| 21569 ||| 12067 ||| 
2021 ||| catan: chart-aware temporal attention network for adverse outcome prediction. ||| 21570 ||| 20676 ||| 
2021 ||| (m)slae-net: multi-scale multi-level attention embedded network for retinal vessel segmentation. ||| 21571 ||| 21572 ||| 
2019 ||| multiple mace risk prediction using multi-task recurrent neural network with attention. ||| 21573 ||| 21574 ||| 11066 ||| 11064 ||| 21575 ||| 21576 ||| 
2019 ||| combined attention mechanism for named entity recognition in chinese electronic medical records. ||| 21577 ||| 21578 ||| 
2019 ||| multimodal attention network for trauma activity recognition from spoken language and environmental sound. ||| 3627 ||| 21579 ||| 21580 ||| 3630 ||| 21581 ||| 2425 ||| 21582 ||| 21583 ||| 
2018 ||| chinese clinical entity recognition via attention-based cnn-lstm-crf. ||| 21584 ||| 1117 ||| 12391 ||| 5231 ||| 
2019 ||| a self-attention based deep learning method for lesion attribute detection from ct reports. ||| 21585 ||| 2379 ||| 21586 ||| 14912 ||| 21587 ||| 
2021 ||| dementia detection using transformer-based deep learning and natural language processing models. ||| 21588 ||| 21589 ||| 21590 ||| 21591 ||| 
2020 ||| traffic flow forecasting using a spatial-temporal attention graph convolutional network predictor. ||| 21592 ||| 21593 ||| 17694 ||| 
2021 ||| st-gwann: a novel spatial-temporal graph wavelet attention neural network for traffic prediction. ||| 21594 ||| 8463 ||| 21595 ||| 12969 ||| 21596 ||| 21597 ||| 
2020 ||| environment classification for global navigation satellite systems using attention-based recurrent neural networks. ||| 11540 ||| 21598 ||| 21599 ||| 1160 ||| 11541 ||| 11542 ||| 21600 ||| 
2020 ||| attention u-net for road extraction in remote sensing images. ||| 21601 ||| 8463 ||| 1903 ||| 
2021 ||| a dynamic traffic community prediction model based on hierarchical graph attention network. ||| 21596 ||| 8464 ||| 8463 ||| 21594 ||| 21597 ||| 
2018 ||| modeling of rosen-type piezoelectric transformer by mean of a polynomial approach. ||| 21602 ||| 21603 ||| 21604 ||| 21605 ||| 21606 ||| 21607 ||| 21608 ||| 21609 ||| 
2021 ||| the effect of outdoor monitor on people's attention. ||| 21610 ||| 21611 ||| 21612 ||| 21613 ||| 
2019 ||| lean model of services for the improvement in the times of attention of the emergency areas of the health sector. ||| 21614 ||| 21615 ||| 21616 ||| 
2019 ||| improvement of attention times and efficiency of container movements in a port terminal using a truck appointment system, lifo management and poka yoke. ||| 21617 ||| 7033 ||| 21618 ||| 21619 ||| 21616 ||| 
2021 ||| lost people: how national ai-strategies paying attention to users. ||| 21620 ||| 21621 ||| 21622 ||| 
2021 ||| iteration of children with attention deficit disorder, impulsivity and hyperactivity, cognitive behavioral therapy, and artificial intelligence. ||| 21623 ||| 21624 ||| 21625 ||| 8048 ||| 21626 ||| 
2019 ||| genesis of attention in the process of interaction weak visual person - work system in a local environment. ||| 17073 ||| 17072 ||| 
2019 ||| visual attention convergence index for virtual reality experiences. ||| 21627 ||| 21628 ||| 
2021 ||| the impact of virtual reality, augmented reality, and interactive whiteboards on the attention management in secondary school stem teaching. ||| 21629 ||| 21630 ||| 
2019 ||| positional self-attention based hierarchical image captioning. ||| 21631 ||| 21632 ||| 6514 ||| 
2019 ||| an attention module for multi-person pose estimation. ||| 21633 ||| 21634 ||| 21635 ||| 21636 ||| 
2019 ||| attention grasping network: a real-time approach to generating grasp synthesis. ||| 21637 ||| 21638 ||| 21639 ||| 
2019 ||| how does algorithmic filtering influence attention inequality on social media? ||| 21640 ||| 21641 ||| 21642 ||| 21643 ||| 
2020 ||| assessing ad attention through clustering viewport trajectories. ||| 21644 ||| 21645 ||| 
2020 ||| emerging leaders in digital work: toward a theory of attentional leadership. ||| 21646 ||| 21647 ||| 21648 ||| 21649 ||| 
2017 ||| helping employees to be digital transformers - the olympus.connect case. ||| 21650 ||| 21651 ||| 
2019 ||| detecting senior executives' personalities for predicting corporate behaviors: an attention-based deep learning approach. ||| 5492 ||| 21652 ||| 
2019 ||| impact of it use on the collective attentional engagement to innovation: the case of a organization in the cork sector. ||| 13138 ||| 13137 ||| 
2019 ||| selling information when attention is limited: an empirical analysis of an online investment advisory platform. ||| 9741 ||| 21653 ||| 21654 ||| 
2018 ||| i or you: whom should online reviewers direct their attention to, and when? ||| 21655 ||| 21656 ||| 14048 ||| 
2020 ||| exploring transformer text generation for medical dataset augmentation. ||| 21657 ||| 72 ||| 76 ||| 
2020 ||| sign language recognition with transformer networks. ||| 15028 ||| 15033 ||| 15034 ||| 
2020 ||| recognizing semantic relations by combining transformers and fully connected models. ||| 15079 ||| 15080 ||| 15081 ||| 
2020 ||| why attention is not explanation: surgical intervention and causal reasoning about neural models. ||| 21658 ||| 21659 ||| 21660 ||| 
2018 ||| incorporating semantic attention in video description generation. ||| 21661 ||| 3090 ||| 6420 ||| 
2020 ||| unior nlp at mwsa task - globalex 2020: siamese lstm with attention for word sense alignment. ||| 21662 ||| 21663 ||| 21664 ||| 21665 ||| 
2020 ||| adaptation of deep bidirectional transformers for afrikaans language. ||| 21666 ||| 
2020 ||| transfer learning from transformers to fake news challenge stance detection (fnc-1) task. ||| 21667 ||| 7025 ||| 
2020 ||| evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering. ||| 1638 ||| 1639 ||| 
2018 ||| a multimodal corpus for mutual gaze and joint attention in multiparty situated interaction. ||| 21668 ||| 21669 ||| 21670 ||| 21671 ||| 21672 ||| 21673 ||| 21674 ||| 21675 ||| 
2020 ||| sentence level human translation quality estimation with attention-based neural networks. ||| 21676 ||| 15080 ||| 
2018 ||| attention for implicit discourse relation recognition. ||| 21677 ||| 3959 ||| 
2020 ||| stress test evaluation of transformer-based models in natural language understanding tasks. ||| 21678 ||| 3369 ||| 21679 ||| 21680 ||| 
2020 ||| contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task. ||| 16980 ||| 9606 ||| 10289 ||| 
2022 ||| dota: detect and omit weak attentions for scalable transformer acceleration. ||| 21681 ||| 11307 ||| 21682 ||| 21683 ||| 18023 ||| 12120 ||| 
2020 ||| densegats: a graph-attention-based network for nonlinear character deformation. ||| 21684 ||| 21685 ||| 21686 ||| 
2020 ||| a multi-input cnns with attention for skin lesion classification. ||| 839 ||| 6595 ||| 21687 ||| 21688 ||| 
2020 ||| dual-level attention based on heterogeneous graph convolution network for aspect-based sentiment classification. ||| 3559 ||| 479 ||| 162 ||| 5091 ||| 5090 ||| 5089 ||| 
2020 ||| a framework of distribution transformer health early warning system based on edc-gru network. ||| 683 ||| 9262 ||| 21689 ||| 17209 ||| 21690 ||| 1302 ||| 21691 ||| 21692 ||| 21693 ||| 
2021 ||| comformer: code comment generation via transformer and fusion method-based hybrid code representation. ||| 6005 ||| 6007 ||| 21694 ||| 21695 ||| 21696 ||| 6008 ||| 5942 ||| 
2021 ||| use of deep learning model with attention mechanism for software fault prediction. ||| 21697 ||| 21698 ||| 21699 ||| 
2020 ||| reliable and robust weakly supervised attention networks for surface defect detection. ||| 18275 ||| 21700 ||| 17589 ||| 369 ||| 
2020 ||| no-reference stereoscopic image quality assessment based on visual attention mechanism. ||| 11326 ||| 21701 ||| 12232 ||| 
2018 ||| channel attention and multi-level features fusion for single image super-resolution. ||| 17342 ||| 17898 ||| 11441 ||| 21702 ||| 21703 ||| 
2021 ||| progressive co-attention network for fine-grained visual classification. ||| 16627 ||| 1482 ||| 1484 ||| 786 ||| 
2018 ||| convolutional neural networks with generalized attentional pooling for action recognition. ||| 21704 ||| 1806 ||| 6437 ||| 1807 ||| 
2019 ||| attentional part-based network for person re-identification. ||| 21705 ||| 11441 ||| 11295 ||| 21706 ||| 19963 ||| 2760 ||| 
2021 ||| underwater image enhancement with multi-scale residual attention network. ||| 21707 ||| 21708 ||| 
2018 ||| potential of attention mechanism for classification of optical coherence tomography images. ||| 21709 ||| 21710 ||| 18070 ||| 18071 ||| 17860 ||| 
2021 ||| data transformer for anomalous trajectory detection. ||| 21711 ||| 21712 ||| 
2019 ||| efficient dual attention module for real-time visual tracking. ||| 20828 ||| 21702 ||| 11294 ||| 21713 ||| 711 ||| 
2019 ||| dsan: double supervised network with attention mechanism for scene text recognition. ||| 21714 ||| 11446 ||| 21715 ||| 21716 ||| 472 ||| 8718 ||| 
2019 ||| enhanced semantic features via attention for real-time visual tracking. ||| 21713 ||| 11294 ||| 20828 ||| 
2020 ||| learning graph topology representation with attention networks. ||| 21717 ||| 21718 ||| 785 ||| 786 ||| 675 ||| 
2020 ||| attention-guided fusion network of point cloud and multiple views for 3d shape recognition. ||| 19855 ||| 21719 ||| 19853 ||| 21720 ||| 
2021 ||| attention-guided convolutional neural network for lightweight jpeg compression artifacts removal. ||| 8702 ||| 21721 ||| 21722 ||| 21723 ||| 
2021 ||| hcit: deepfake video detection using a hybrid model of cnn features and vision transformer. ||| 21724 ||| 21725 ||| 21726 ||| 21727 ||| 21728 ||| 
2021 ||| maps: joint multimodal attention and pos sequence generation for video captioning. ||| 17246 ||| 21729 ||| 21730 ||| 9061 ||| 2063 ||| 
2017 ||| object localization in weakly labeled data using regularized attention networks. ||| 21731 ||| 6503 ||| 602 ||| 
2021 ||| action recognition improved by correlations and attention of subjects and scene. ||| 21732 ||| 21733 ||| 
2018 ||| spatiotemporal attention on sliced parts for video-based person re-identification. ||| 5157 ||| 2747 ||| 21734 ||| 21735 ||| 21736 ||| 
2018 ||| deep network with spatial and channel attention for person re-identification. ||| 21737 ||| 21738 ||| 11441 ||| 11295 ||| 17898 ||| 
2019 ||| multi-heads attention graph convolutional networks for skeleton-based action recognition. ||| 21739 ||| 1340 ||| 
2018 ||| omnidirectional video streaming using visual attention-driven dynamic tiling for vr. ||| 1595 ||| 21740 ||| 21741 ||| 1597 ||| 
2017 ||| clothing retrieval with visual attention model. ||| 21742 ||| 21743 ||| 1222 ||| 1086 ||| 19085 ||| 
2020 ||| learning convolution feature aggregation via edge attention convolution network for person re-identification. ||| 19973 ||| 21744 ||| 21745 ||| 21746 ||| 19206 ||| 
2019 ||| facial attention based convolutional neural network for 2d+3d facial expression recognition. ||| 4135 ||| 17807 ||| 2071 ||| 136 ||| 21747 ||| 21748 ||| 
2020 ||| a hybrid intelligent system for insider threat detection using iterative attention. ||| 21749 ||| 3211 ||| 
2019 ||| a novel attention-based neural network for video scene classification in complex background. ||| 21750 ||| 21751 ||| 21752 ||| 
2018 ||| learning to communicate via supervised attentional message processing. ||| 17318 ||| 8750 ||| 8752 ||| 
2021 ||| smat: an attention-based deep learning solution to the automation of schema matching. ||| 875 ||| 372 ||| 375 ||| 20676 ||| 
2021 ||| attention-guided memory model for video object segmentation. ||| 21753 ||| 21754 ||| 
2019 ||| self-attention deep saliency network for fabric defect detection. ||| 21755 ||| 20184 ||| 14165 ||| 21756 ||| 20183 ||| 
2017 ||| siamese network with soft attention for semantic text understanding. ||| 21757 ||| 21758 ||| 21759 ||| 
2021 ||| llod-driven bilingual word embeddings rivaling cross-lingual transformers in quality of life concept detection from french online health communities. ||| 21760 ||| 21761 ||| 21762 ||| 21763 ||| 21764 ||| 21765 ||| 
2021 ||| attentional learn-able pooling for human activity recognition. ||| 20257 ||| 20258 ||| 4611 ||| 6383 ||| 
2021 ||| semantic reinforced attention learning for visual place recognition. ||| 1795 ||| 21766 ||| 1796 ||| 21767 ||| 21768 ||| 1798 ||| 
2020 ||| attention-guided lightweight network for real-time segmentation of robotic surgical instruments. ||| 5183 ||| 5184 ||| 271 ||| 5185 ||| 5186 ||| 5189 ||| 
2021 ||| sct-cnn: a spatio-channel-temporal attention cnn for grasp stability prediction. ||| 21769 ||| 21770 ||| 21771 ||| 21772 ||| 21773 ||| 15489 ||| 
2021 ||| perceive, attend, and drive: learning spatial attention for safe self-driving. ||| 21774 ||| 9220 ||| 21775 ||| 21776 ||| 10875 ||| 9239 ||| 
2021 ||| attentional-gcnn: adaptive pedestrian trajectory prediction towards generic autonomous vehicle use cases. ||| 21777 ||| 21778 ||| 21779 ||| 21780 ||| 21781 ||| 21782 ||| 
2021 ||| referring image segmentation via language-driven attention. ||| 6361 ||| 18788 ||| 6363 ||| 
2017 ||| show, attend and interact: perceivable human-robot social interaction through neural attention q-network. ||| 21783 ||| 21784 ||| 21785 ||| 20474 ||| 
2021 ||| gcc-phat with speech-oriented attention for robotic sound source localization. ||| 21786 ||| 21787 ||| 21788 ||| 21789 ||| 12494 ||| 
2019 ||| learning to write anywhere with spatial transformer image-to-motion encoder-decoder networks. ||| 21790 ||| 21791 ||| 21792 ||| 21793 ||| 
2019 ||| a cane-based low cost sensor to implement attention mechanisms in telecare robots. ||| 21794 ||| 21795 ||| 21796 ||| 21797 ||| 21798 ||| 
2021 ||| ndt-transformer: large-scale 3d point cloud localisation using the normal distribution transform representation. ||| 21799 ||| 6574 ||| 21800 ||| 21801 ||| 5089 ||| 21802 ||| 21803 ||| 
2021 ||| avgcn: trajectory prediction using graph convolutional networks guided by human attention. ||| 21804 ||| 1303 ||| 124 ||| 5407 ||| 
2017 ||| attention and anticipation in fast visual-inertial navigation. ||| 21805 ||| 21806 ||| 
2021 ||| deep3dranker: a novel framework for learning to rank 3d models with self-attention in robotic vision. ||| 21807 ||| 20179 ||| 21808 ||| 21809 ||| 21810 ||| 
2020 ||| attentive task-net: self supervised task-attention network for imitation learning using video demonstration. ||| 4607 ||| 4608 ||| 4609 ||| 4610 ||| 4611 ||| 
2021 ||| attention-based probabilistic planning with active perception. ||| 21811 ||| 3362 ||| 
2021 ||| bidirectional attention network for monocular depth estimation. ||| 21812 ||| 21813 ||| 21814 ||| 21815 ||| 21816 ||| 
2021 ||| covariance self-attention dual path unet for rectal tumor segmentation. ||| 21817 ||| 21818 ||| 21819 ||| 21820 ||| 
2020 ||| ap-mtl: attention pruned multi-task learning model for real-time instrument detection and segmentation in robot-assisted surgery. ||| 21821 ||| 19132 ||| 21822 ||| 
2020 ||| ccan: constraint co-attention network for instance grasping. ||| 19410 ||| 21823 ||| 19412 ||| 21824 ||| 
2021 ||| multibranch learning for angiodysplasia segmentation with attention-guided networks and domain adaptation. ||| 21825 ||| 21826 ||| 16744 ||| 21827 ||| 21828 ||| 21829 ||| 
2019 ||| crowd-robot interaction: crowd-aware robot navigation with attention-based deep reinforcement learning. ||| 21830 ||| 21831 ||| 21832 ||| 21833 ||| 
2021 ||| verbal focus-of-attention system for learning-from-observation. ||| 21834 ||| 21835 ||| 21836 ||| 21837 ||| 
2021 ||| maast: map attention with semantic transformers for efficient visual navigation. ||| 21838 ||| 21839 ||| 21840 ||| 21841 ||| 21842 ||| 21843 ||| 
2021 ||| long-range hand gesture recognition via attention-based ssd network. ||| 21844 ||| 21845 ||| 21846 ||| 21847 ||| 21848 ||| 
2021 ||| a graph attention spatio-temporal convolutional network for 3d human pose estimation in video. ||| 21849 ||| 21850 ||| 21851 ||| 21852 ||| 21853 ||| 21854 ||| 21855 ||| 
2019 ||| fast radar motion estimation with a learnt focus of attention using weak supervision. ||| 21856 ||| 21857 ||| 21858 ||| 21859 ||| 
2020 ||| multi-head attention for multi-modal joint vehicle motion forecasting. ||| 21860 ||| 21861 ||| 21862 ||| 21863 ||| 21864 ||| 21865 ||| 
2020 ||| smart: training shallow memory-aware transformers for robotic explainability. ||| 19001 ||| 19003 ||| 13611 ||| 
2018 ||| social attention: modeling attention in human crowds. ||| 21866 ||| 19013 ||| 21867 ||| 
2019 ||| lightweight contrast modeling for attention-aware visual localization. ||| 21868 ||| 1800 ||| 6159 ||| 2315 ||| 
2019 ||| attention-based lane change prediction. ||| 21869 ||| 21870 ||| 21871 ||| 13628 ||| 19260 ||| 
2017 ||| tweeting mass shootings: the dynamics of issue attention on social media. ||| 21872 ||| 21873 ||| 21874 ||| 21875 ||| 21876 ||| 
2018 ||| #thanksfortheinvite: examining attention to social exclusion signals online. ||| 21877 ||| 21878 ||| 21879 ||| 21880 ||| 21881 ||| 
2020 ||| salient attention model and classes imbalance remission for video anomaly analysis with weak label. ||| 21882 ||| 21883 ||| 17591 ||| 14649 ||| 
2020 ||| image fusion method for transformer substation based on nsct and visual saliency. ||| 21884 ||| 13546 ||| 21885 ||| 21886 ||| 
2020 ||| scale-aware network with attentional selection for human pose estimation. ||| 21887 ||| 21888 ||| 21889 ||| 21890 ||| 21891 ||| 
2019 ||| character-level attention convolutional neural networks for short-text classification. ||| 21892 ||| 21893 ||| 10107 ||| 
2018 ||| wi-fi attention network for indoor fingerprint positioning. ||| 1349 ||| 21894 ||| 21895 ||| 21896 ||| 21897 ||| 
2021 ||| a novel wifi gesture recognition method based on cnn-lstm and channel attention. ||| 6824 ||| 21898 ||| 
2021 ||| combined metapath based attention network for heterogenous networks node classification. ||| 10062 ||| 21899 ||| 
2021 ||| an improved gail based on object detection, gru, and attention. ||| 21900 ||| 21901 ||| 
2021 ||| attention-based sub-word network for multilingual short text classification. ||| 20282 ||| 21902 ||| 21903 ||| 
2021 ||| facial expression recognition based on deep learning and attention mechanism. ||| 21904 ||| 14128 ||| 
2020 ||| learning target-specific response attention for siamese network based visual tracking. ||| 21905 ||| 21906 ||| 21907 ||| 127 ||| 21908 ||| 
2021 ||| research on 110kv oil impregnated paper capac-itance graded transformer bushings based on the design principle of equal capacitance and steps. ||| 21909 ||| 21910 ||| 9472 ||| 21911 ||| 
2021 ||| lightweight url-based phishing detection using natural language processing transformers for mobile devices. ||| 21912 ||| 21913 ||| 21914 ||| 
2021 ||| an improved cnn based on attention mechanism with multi-domain feature fusion for bearing fault diagnosis. ||| 21915 ||| 21916 ||| 21917 ||| 21918 ||| 21919 ||| 21920 ||| 
2021 ||| control of soft switching solid state transformer based on lyapunov energy function for three-phase ac-ac power conversion. ||| 21921 ||| 21280 ||| 21281 ||| 
2021 ||| breadth-first search leakage tolerant commutation method for matrix converters in three-phase solid state transformers. ||| 21922 ||| 7111 ||| 21923 ||| 21924 ||| 
2020 ||| dual transformer based dual active bridge for solid state transformer in distribution system. ||| 21925 ||| 21926 ||| 21927 ||| 21928 ||| 
2020 ||| a single stage multilevel converter based on transformer multi-tap voltages control fed by low dc voltage source. ||| 21929 ||| 21930 ||| 1310 ||| 21931 ||| 21932 ||| 21933 ||| 
2017 ||| operation and control of smart transformer-based electric vehicles charging system. ||| 16200 ||| 21934 ||| 10868 ||| 
2019 ||| a new fault tolerant single phase 5-level inverter topology with capacitor voltage balancing for solid state transformer. ||| 21935 ||| 21936 ||| 21937 ||| 21938 ||| 21939 ||| 
2018 ||| design of nonlinear dry-type transformer for all-electric ship and marine applications. ||| 21940 ||| 21941 ||| 21942 ||| 21943 ||| 21944 ||| 
2018 ||| analysis and simulation of transformer isolated high current 48 v dc power supply with dc-ups capability based on scaldo technique for google's new open rack power architecture. ||| 21945 ||| 21946 ||| 21947 ||| 
2018 ||| a solid state transformer based fast charging station for all categories of electric vehicles. ||| 21948 ||| 21949 ||| 
2019 ||| load management design methods by displacement and affectation in ageing level of distribution transformers. ||| 21950 ||| 21951 ||| 10506 ||| 21952 ||| 21953 ||| 
2020 ||| bi-directional cllc resonant converter with integrated planar transformer for energy storage systems. ||| 21954 ||| 21955 ||| 
2021 ||| series resonant converter with embedded filters for dcx of solid-state transformer. ||| 21956 ||| 21957 ||| 21958 ||| 21959 ||| 
2019 ||| lanet: a ladder attention network for image semantic segmentation. ||| 21960 ||| 1241 ||| 9018 ||| 
2017 ||| simplified rail power conditioner based on a half-bridge indirect ac/dc/ac modular multilevel converter and a v/v power transformer. ||| 13095 ||| 21961 ||| 21962 ||| 13098 ||| 2871 ||| 13096 ||| 21963 ||| 3369 ||| 21964 ||| 10314 ||| 1994 ||| 13097 ||| 
2018 ||| optimal sizing of a power electronic traction transformer for railway applications. ||| 21965 ||| 21966 ||| 21967 ||| 11197 ||| 21968 ||| 21969 ||| 
2017 ||| visual attention distribution map for artificial misdirection. ||| 21970 ||| 21971 ||| 
2020 ||| an asymmetrical loosely coupled transformer and constant current wireless charging scheme for warehouse vehicles. ||| 21972 ||| 13166 ||| 21973 ||| 21974 ||| 1299 ||| 
2020 ||| analysis, design and modelling of two fully- integrated transformers with segmental magnetic shunt for llc resonant converters. ||| 21975 ||| 21976 ||| 21977 ||| 
2017 ||| voltage quality improvement in smart transformer integrated distribution grid. ||| 21978 ||| 16200 ||| 10868 ||| 
2019 ||| smart transformer modelling in optimal power flow analysis. ||| 21979 ||| 21980 ||| 21981 ||| 16214 ||| 21982 ||| 21983 ||| 
2020 ||| six-switch and seven-switch grid-connected current source inverters for transformerless photovoltaic applications. ||| 21984 ||| 21985 ||| 21986 ||| 21987 ||| 
2017 ||| thermal modeling and transient behavior analysis of a medium-frequency high-power transformer. ||| 10850 ||| 21988 ||| 21989 ||| 21990 ||| 21991 ||| 12614 ||| 21992 ||| 
2020 ||| design of a high-voltage-insulation and high-efficiency medium frequency transformer. ||| 21993 ||| 21994 ||| 8008 ||| 21995 ||| 
2017 ||| topology and control of transformerless high voltage grid-connected pv systems with a cascade step-up structure. ||| 21996 ||| 7436 ||| 4646 ||| 2855 ||| 
2019 ||| methodology for assessment of the impact of smart transformers on power system reliability. ||| 21979 ||| 16213 ||| 16214 ||| 21983 ||| 
2021 ||| multilevel inverter with a new modulation method applied to solid-state transformer in pv applications. ||| 21997 ||| 21998 ||| 21999 ||| 22000 ||| 22001 ||| 852 ||| 6234 ||| 6235 ||| 
2020 ||| a multiport power electronic transformer with shared medium-frequency transformer. ||| 22002 ||| 22003 ||| 22004 ||| 
2018 ||| smart transformer for the provision of coordinated voltage and frequency support in the grid. ||| 21979 ||| 10867 ||| 22005 ||| 22006 ||| 10868 ||| 22007 ||| 21983 ||| 
2021 ||| on-line method for high-sensitivity leakage current measurement of converter-connected transformers in microgrids. ||| 22008 ||| 3208 ||| 22009 ||| 22010 ||| 
2019 ||| tab based multiport converter with optimized transformer rms current and improved zvs range for dc microgrid applications. ||| 22011 ||| 22012 ||| 22013 ||| 
2017 ||| dynamic demand minimization using a smart transformer. ||| 21979 ||| 22014 ||| 21983 ||| 
2017 ||| corrosion evaluation of the grounding grid in transformer substation using electrical impedance tomography technology. ||| 22015 ||| 22016 ||| 22017 ||| 22018 ||| 22019 ||| 22020 ||| 
2020 ||| smart transformer based meshed hybrid microgrid with mvdc interconnection. ||| 22021 ||| 16200 ||| 
2017 ||| post-fault operation of hybrid dc-dc converter for solid-state transformer. ||| 22022 ||| 22023 ||| 22024 ||| 22025 ||| 
2021 ||| a two-phase interleaved high-voltage gain dc-dc converter with coupled inductor and built-in transformer for photovoltaic applications. ||| 22026 ||| 22027 ||| 22028 ||| 22029 ||| 
2020 ||| multi-port converter integrating automatic current balancing interleaved pwm converter and dual active bridge converter with improved transformer utilization. ||| 11857 ||| 22030 ||| 22031 ||| 
2021 ||| control of cross-circulating currents in a mmc with parallel connected arms in solid state transformers. ||| 22032 ||| 22033 ||| 2600 ||| 22034 ||| 22025 ||| 
2019 ||| design of an igbt-series-based solid-state circuit breaker for battery energy storage system terminal in solid-state transformer. ||| 3049 ||| 3180 ||| 22035 ||| 22036 ||| 16833 ||| 
2018 ||| sensorless starting control of permanent magnet synchronous motors with step-up transformer for downhole electric drilling. ||| 22037 ||| 22038 ||| 22039 ||| 22040 ||| 3313 ||| 22041 ||| 22042 ||| 22043 ||| 22044 ||| 22045 ||| 
2019 ||| high-efficiency solid state transformer architecture for large-scale pv application. ||| 22046 ||| 10867 ||| 22047 ||| 10868 ||| 
2018 ||| high power quality voltage control of smart transformer-fed distribution grid. ||| 10867 ||| 22048 ||| 10868 ||| 
2017 ||| investigation of load compensation features of smart transformer in medium voltage grid. ||| 16200 ||| 10867 ||| 10868 ||| 
2018 ||| laboratory investigations of parallel connected inverters feeding medium voltage transformer. ||| 22049 ||| 
2020 ||| a new transformer model with separate common-mode and differential-mode capacitance. ||| 22050 ||| 22051 ||| 22052 ||| 22053 ||| 
2019 ||| distribution network hybrid transformer for load current and grid voltage compensation. ||| 22054 ||| 22055 ||| 22056 ||| 852 ||| 22057 ||| 
2019 ||| hybrid transformers with virtual inertia for future distribution networks. ||| 22056 ||| 22058 ||| 22033 ||| 2600 ||| 22059 ||| 10895 ||| 22060 ||| 22061 ||| 
2019 ||| optimum design of planar transformer for llc resonant converter using metaheuristic method. ||| 22062 ||| 22063 ||| 
2020 ||| a bidirectional matrix-based ac-dc dual-active bridge for modular solid-state-transformers. ||| 16169 ||| 16168 ||| 16171 ||| 
2018 ||| zero-sequence injection technique for capacitor lifetime extension on the low-voltage converter of a smart transformer. ||| 10867 ||| 22064 ||| 10868 ||| 
2020 ||| application of gen-3 10 kv sic mosfets in xhv-6 packaging for a mobile utility support equipment based solid state transformer (muse-sst). ||| 22065 ||| 22066 ||| 22067 ||| 22068 ||| 
2018 ||| modular ev fast charging station architectures based on multiphase-medium-frequency transformer. ||| 22069 ||| 22070 ||| 3419 ||| 10868 ||| 
2017 ||| two-transformer-series approach in developing a transistor based-ac voltage regulator for consumer-end applications. ||| 22071 ||| 21946 ||| 22072 ||| 21947 ||| 
2020 ||| improving automated visual fault detection by combining a biologically plausible model of visual attention with deep learning. ||| 1 ||| 22073 ||| 22074 ||| 22075 ||| 
2018 ||| robustness analysis of voltage control strategies of smart transformer. ||| 22076 ||| 22077 ||| 10867 ||| 22048 ||| 10868 ||| 
2017 ||| modeling and analysis on the sensing characteristic of fiber optical current transformer. ||| 16562 ||| 1780 ||| 1235 ||| 
2017 ||| an improved partially interleaved transformer structure for high-voltage high-frequency multiple-output applications. ||| 22078 ||| 22079 ||| 22080 ||| 22081 ||| 22082 ||| 
2019 ||| a frequency multiplying circuit containing a high-frequency output inverter and an impedance matching transformer. ||| 22083 ||| 22084 ||| 22085 ||| 22086 ||| 
2021 ||| computational feasibility of multi-objective optimal design techniques for grid-connected multi-cell solid-state-transformers. ||| 16169 ||| 16168 ||| 22087 ||| 16171 ||| 
2018 ||| energy storage systems to prevent distribution transformers overload with high nzeb penetration. ||| 22088 ||| 22089 ||| 1994 ||| 22090 ||| 
2018 ||| an ac-ac modular multilevel converter-based partially-rated solid-state transformer for power flow control. ||| 22091 ||| 22092 ||| 
2018 ||| experimental verification on thermal modeling of medium frequency transformers. ||| 21991 ||| 21988 ||| 22093 ||| 21989 ||| 12614 ||| 22094 ||| 
2020 ||| protection scheme for a medium voltage mobile utility support equipment based solid state transformer (muse-sst). ||| 22065 ||| 22066 ||| 22095 ||| 22067 ||| 22068 ||| 
2019 ||| unbalanced load compensation for solid-state transformer using smoothing capacitors of cascaded h-bridges as energy buffer. ||| 22096 ||| 22097 ||| 22098 ||| 22099 ||| 
2019 ||| high performances voltage control of bidirectional-asymmetrical dc/dc converter in smart transformer for limited reverse power flow. ||| 10867 ||| 10868 ||| 10882 ||| 2698 ||| 
2017 ||| voltage control strategies of smart transformer considering dc capacitor lifetime. ||| 10867 ||| 22100 ||| 10868 ||| 
2021 ||| comparison of different multi-winding transformer models in multi-port ac-coupled converter application. ||| 22101 ||| 16915 ||| 124 ||| 22102 ||| 
2017 ||| integration of ripple correlation mppt technique with one-cycle-controlled transformerless inverter for single-phase single-stage photovoltaic systems. ||| 22103 ||| 22104 ||| 
2017 ||| multi-bus flexible interconnection scheme for balancing power transformers in low-voltage distribution systems. ||| 22105 ||| 22106 ||| 22107 ||| 22108 ||| 22109 ||| 
2020 ||| transformerless common ground quasi-z-source three phase inverter for pv applications. ||| 22110 ||| 22111 ||| 22112 ||| 
2019 ||| modeling and design of a 1.2 pf common-mode capacitance transformer for powering mv sic mosfets gate drivers. ||| 22113 ||| 22114 ||| 22115 ||| 22116 ||| 22117 ||| 22118 ||| 22119 ||| 22120 ||| 22121 ||| 
2021 ||| attention based object classification for drone imagery. ||| 22122 ||| 4353 ||| 
2019 ||| multi-port system for braking energy recovery in diesel-electric locomotives - focus on the multi-interphase transformer design. ||| 22123 ||| 22124 ||| 22125 ||| 22126 ||| 22127 ||| 22128 ||| 
2021 ||| new operation opportunities for the solid-state transformer in smart homes: a comprehensive analysis. ||| 21961 ||| 1994 ||| 22129 ||| 22130 ||| 22131 ||| 1994 ||| 13097 ||| 
2021 ||| potentials and challenges of multiwinding transformer-based dc-dc converters for solid-state transformer. ||| 22132 ||| 22069 ||| 10868 ||| 
2018 ||| design strategy and simulation of medium-frequency transformers for a three-phase dual active bridge. ||| 22133 ||| 22134 ||| 22135 ||| 
2018 ||| flexible power transfer in smart transformer interconnected microgrids. ||| 21978 ||| 16200 ||| 10868 ||| 
2019 ||| 3-phase medium frequency transformer for a 100kw 1.2kv 20khz dual active bridge converter. ||| 22136 ||| 22137 ||| 22138 ||| 11199 ||| 22139 ||| 
2018 ||| model predictive control of h5 inverter for transformerless pv systems with maximum power point tracking and leakage current reduction. ||| 22140 ||| 22141 ||| 22142 ||| 22143 ||| 
2021 ||| ff-gat: feature fusion using graph attention networks. ||| 22144 ||| 22145 ||| 22146 ||| 22147 ||| 22148 ||| 
2021 ||| a novel current limiting control strategy for three-phase three-wire inverter with transformer. ||| 22149 ||| 533 ||| 
2018 ||| high frequency transformer based improved gamma zsi with lossless snubber. ||| 22150 ||| 22151 ||| 22152 ||| 22153 ||| 
2021 ||| novel topology of modular-matrix-converter-based smart transformer for hybrid microgrid. ||| 22154 ||| 22155 ||| 22156 ||| 22157 ||| 21922 ||| 7111 ||| 21923 ||| 852 ||| 22158 ||| 
2017 ||| research on a dual active bridge based power electronics transformer using nanocrystalline and silicon carbide. ||| 12333 ||| 22159 ||| 22160 ||| 22161 ||| 18040 ||| 128 ||| 17992 ||| 
2018 ||| sensitivity of leakage inductance for detecting winding movements in transformers. ||| 22162 ||| 22163 ||| 22164 ||| 16171 ||| 
2020 ||| control and stabilization of grid-connected converters operating as constant power load in a smart transformer grid scenario. ||| 1460 ||| 21934 ||| 22165 ||| 22166 ||| 
2018 ||| protection design considerations of a 10 kv sic mosfet enabled mobile utilities support equipment based solid state transformer (muse-sst). ||| 22167 ||| 22066 ||| 22065 ||| 22168 ||| 22169 ||| 22170 ||| 22067 ||| 
2020 ||| startup scheme for the active front end converter in a medium voltage mobile utility support equipment based solid state transformer (muse-sst). ||| 22065 ||| 22066 ||| 22095 ||| 22067 ||| 22068 ||| 
2018 ||| grid fault detection technique of microgrid inverter according to the structure of three phase output transformer. ||| 22171 ||| 22172 ||| 22173 ||| 22174 ||| 22175 ||| 22176 ||| 
2018 ||| voltage feedback of an llc resonant converter with a rotary transformer. ||| 22177 ||| 22178 ||| 22179 ||| 22180 ||| 
2021 ||| reduced dc voltage fed grid connected transformer-less shunt compensator with ac-side impedance-source configuration. ||| 22181 ||| 22182 ||| 22183 ||| 
2020 ||| a dual attention module for real-time facial expression recognition. ||| 22184 ||| 4357 ||| 4353 ||| 
2019 ||| reliability modeling and assessment of dual active bridge based dc transformer for dc power distribution application. ||| 22185 ||| 22186 ||| 22187 ||| 22188 ||| 22189 ||| 6832 ||| 719 ||| 
2020 ||| modeling and control of a hybrid transformer based on a cascaded h-bridge multilevel converter. ||| 22054 ||| 22055 ||| 22056 ||| 852 ||| 22057 ||| 
2019 ||| a fast simulation model of cascaded h bridge-power electronic transformer. ||| 12938 ||| 22190 ||| 22191 ||| 22192 ||| 2885 ||| 22193 ||| 
2020 ||| power loss minimization in smart transformer based meshed hybrid distribution network. ||| 16200 ||| 22194 ||| 22195 ||| 10868 ||| 
2018 ||| modular multilevel converter based topology for electric locomotive with medium frequency step-down transformer. ||| 22196 ||| 22197 ||| 
2018 ||| low cm leakage current and high efficiency h6 inverter with active clamping for transformerless pv system. ||| 22198 ||| 22199 ||| 1099 ||| 22200 ||| 22201 ||| 
2020 ||| an iterative algorithm for optimum design of high frequency transformer in sst application. ||| 22202 ||| 22203 ||| 22204 ||| 22205 ||| 22206 ||| 
2017 ||| a novel multi-port solid state transformer enabled isolated hybrid microgrid architecture. ||| 21948 ||| 21949 ||| 
2017 ||| transformerless single-phase upqc for large scale led lighting networks. ||| 22207 ||| 22201 ||| 
2017 ||| optimization of lcl filter with intercell transformer for interleaved voltage source converter. ||| 22208 ||| 22209 ||| 22210 ||| 22211 ||| 22212 ||| 
2017 ||| single-phase transformer-less buck-boost inverter with zero leakage current for pv systems. ||| 22213 ||| 22214 ||| 22215 ||| 22216 ||| 
2020 ||| virtual transformer operation of solid state transformer (sst). ||| 22217 ||| 22218 ||| 22219 ||| 22220 ||| 22221 ||| 22222 ||| 
2020 ||| a magnetic saturation suppression scheme of the output line-frequency transformer in photovoltaic inverter. ||| 3558 ||| 22223 ||| 22224 ||| 
2017 ||| retiring strategies of transformer lcc with reliability of power transmission system considered. ||| 22225 ||| 22226 ||| 22227 ||| 8260 ||| 22228 ||| 22229 ||| 22230 ||| 22231 ||| 
2020 ||| temporal pattern attention-based sequence to sequence model for multistep individual load forecasting. ||| 22232 ||| 22233 ||| 22234 ||| 
2021 ||| transformer oil-paper insulation aging evaluation system based on different aging characteristics. ||| 22235 ||| 22236 ||| 22237 ||| 
2020 ||| xss detection technology based on lstm-attention. ||| 22238 ||| 8009 ||| 22239 ||| 22240 ||| 
2021 ||| a visual target tracking algorithm integrating attention mechanism. ||| 19836 ||| 22241 ||| 19758 ||| 
2019 ||| speeding up reinforcement learning by combining attention and agency features. ||| 22242 ||| 13501 ||| 22243 ||| 22244 ||| 
2021 ||| prevent the occurrence of false signals in the power transformer internal protection in substation to improve reliability with differential operational amplifier safety. ||| 22245 ||| 22246 ||| 
2019 ||| optical qpsk signal generation based on the circular trajectory of phase-shifted optical vsb modulation using high-pass hilbert transformers. ||| 22247 ||| 22248 ||| 22249 ||| 
2017 ||| efficient inverse discrete wavelet transformer. ||| 22250 ||| 22251 ||| 22252 ||| 22253 ||| 
2017 ||| efficient forward discrete wavelet transformer. ||| 22250 ||| 22251 ||| 22252 ||| 22253 ||| 
2020 ||| transformer region proposal for object detection. ||| 22254 ||| 22255 ||| 22256 ||| 22257 ||| 
2018 ||| vision-based joint attention detection for autism spectrum disorders. ||| 12817 ||| 5018 ||| 12819 ||| 
2018 ||| sar image fast online atr based on visual attention and scale analysis. ||| 11811 ||| 22258 ||| 22259 ||| 11809 ||| 11810 ||| 
2020 ||| automatic leaf recognition based on attention densenet. ||| 22260 ||| 22261 ||| 22262 ||| 22263 ||| 1168 ||| 
2018 ||| computer-based attention training improves brain cognitive control function: evidences from event-related potentials. ||| 1429 ||| 22264 ||| 9149 ||| 22265 ||| 
2020 ||| mwoa auxiliary diagnosis via rsn-based 3d deep multiple instance learning with spatial attention mechanism. ||| 2008 ||| 22266 ||| 19051 ||| 22267 ||| 
2020 ||| a sequence-to-sequence model based on attention mechanism for wave spectrum prediction. ||| 22268 ||| 11275 ||| 22269 ||| 15198 ||| 
2020 ||| a lightweight transformer with convolutional attention. ||| 22270 ||| 22271 ||| 
2017 ||| neurofeedback based attention training for children with adhd. ||| 7473 ||| 7474 ||| 22272 ||| 22273 ||| 
2021 ||| coordinate attention unet. ||| 22274 ||| 22275 ||| 
2018 ||| modified transformerless dual buck inverter with improved lifetime for pv applications. ||| 22276 ||| 22216 ||| 
2019 ||| perceptual attention-based predictive control. ||| 22277 ||| 22278 ||| 22279 ||| 22280 ||| 
2018 ||| learning 6-dof grasping and pick-place using attention focus. ||| 22281 ||| 22282 ||| 
2020 ||| attentional separation-and-aggregation network for self-supervised depth-pose learning in dynamic scenes. ||| 6810 ||| 22283 ||| 10211 ||| 3906 ||| 8689 ||| 
2020 ||| transformers for one-shot visual imitation. ||| 22284 ||| 7364 ||| 
2019 ||| multimodal attention branch network for perspective-free sentence generation. ||| 22285 ||| 726 ||| 12308 ||| 
2020 ||| attention-privileged reinforcement learning. ||| 22286 ||| 22287 ||| 22288 ||| 22289 ||| 22290 ||| 
2021 ||| security threat modeling for power transformers in cyber-physical environments. ||| 22291 ||| 22292 ||| 22293 ||| 22294 ||| 22295 ||| 
2017 ||| voltage control by using capacitor banks and tap changing transformers in a renewable microgrid. ||| 22296 ||| 22297 ||| 22298 ||| 22299 ||| 
2021 ||| on ambient temperature of transformer substations in desert climates. ||| 22300 ||| 1241 ||| 22301 ||| 22302 ||| 22303 ||| 22304 ||| 
2020 ||| distribution transformer health monitoring using smart meter data. ||| 22305 ||| 22306 ||| 21281 ||| 22307 ||| 
2021 ||| radial deformation emplacement in power transformers using long short-term memory networks. ||| 22308 ||| 22309 ||| 22310 ||| 22311 ||| 22312 ||| 
2019 ||| transformer rating due to high penetrations of pv, ev charging, and energy storage. ||| 22313 ||| 22314 ||| 22315 ||| 
2020 ||| secondary network parameter estimation for distribution transformers. ||| 22305 ||| 22316 ||| 21281 ||| 
2020 ||| sensing service transformer secondary currents using planar magnetic pick-up coils. ||| 22317 ||| 21281 ||| 
2020 ||| is attention contagious? estimating the spillover effect of investor attention in digital networks. ||| 22318 ||| 5250 ||| 22319 ||| 
2018 ||| eco-feedback interventions: selective attention and actual behavior change. ||| 22320 ||| 22321 ||| 22322 ||| 22323 ||| 
2018 ||| the role of attention and neutralization in posting malicious comments online. ||| 22324 ||| 22325 ||| 
2017 ||| paying attention to news briefs about innovative technologies. ||| 22326 ||| 22327 ||| 
2021 ||| vitbis: vision transformer for biomedical image segmentation. ||| 7335 ||| 
2021 ||| attention-guided pancreatic duct segmentation from abdominal ct volumes. ||| 22328 ||| 2023 ||| 22329 ||| 22330 ||| 22331 ||| 22332 ||| 22333 ||| 
2019 ||| a comparison of word-embeddings in emotion detection from text using bilstm, cnn and self-attention. ||| 10681 ||| 22334 ||| 10682 ||| 10683 ||| 
2017 ||| "out of the fr-eye-ing pan": towards gaze-based models of attention during learning with technology in the classroom. ||| 15373 ||| 8097 ||| 8098 ||| 15374 ||| 15375 ||| 15345 ||| 
2019 ||| personalized serious games for self-regulated attention training. ||| 22335 ||| 
2018 ||| a joint attention model for automated editing. ||| 7543 ||| 7547 ||| 
2020 ||| field simulation method for secondary signal of zero-flux current transformer. ||| 22336 ||| 22337 ||| 22338 ||| 144 ||| 22339 ||| 22340 ||| 22341 ||| 
2020 ||| stock movement classification from twitter via mogrifier based memory cells with attention mechanism. ||| 556 ||| 
2019 ||| finite element three-dimensional modeling of cooling system for strong oil circulating air-cooled transformer. ||| 22342 ||| 6721 ||| 22343 ||| 22344 ||| 22345 ||| 
2019 ||| calculation and analysis of short circuit performance of transformer based on finite element method. ||| 22343 ||| 22342 ||| 22346 ||| 
2019 ||| experimental study on insulation characteristics of oil-immersed transformer in alpine environment. ||| 22347 ||| 22348 ||| 22349 ||| 989 ||| 4217 ||| 1556 ||| 
2019 ||| study on suppressing the voltage fluctuation in testing of transformer sudden short circuit based on grid supply. ||| 22350 ||| 22351 ||| 20899 ||| 22352 ||| 22353 ||| 22354 ||| 
2019 ||| transient current limit control to transformer short circuit based on harmonics voltage injection method. ||| 22350 ||| 22355 ||| 22356 ||| 20899 ||| 22357 ||| 22358 ||| 
2017 ||| malware analysis of imaged binary samples by convolutional neural network with attention mechanism. ||| 15119 ||| 15120 ||| 15121 ||| 15122 ||| 15123 ||| 
2018 ||| gender differences in selective attention and shopping intention in the case of taobao live-show: an eye-tracking study. ||| 22359 ||| 22360 ||| 22361 ||| 
2018 ||| the invisible gorilla revisited: using eye tracking to investigate inattentional blindness in interface design. ||| 22362 ||| 22363 ||| 
2018 ||| snap-changes: a dynamic editing strategy for directing viewer's attention in streaming virtual reality videos. ||| 22364 ||| 22365 ||| 22366 ||| 22367 ||| 22368 ||| 22369 ||| 22370 ||| 
2021 ||| two-stream graph attention convolutional for video action recognition. ||| 22371 ||| 12815 ||| 22372 ||| 22373 ||| 
2021 ||| dual attention-based interest network for personalized recommendation system. ||| 22374 ||| 22375 ||| 8427 ||| 22376 ||| 2561 ||| 22377 ||| 
2020 ||| real-time social media analytics with deep transformer language models: a big data approach. ||| 22378 ||| 22379 ||| 
2017 ||| attention-based memory network for sentence-level question answering. ||| 16927 ||| 1151 ||| 18972 ||| 22380 ||| 22381 ||| 
2017 ||| dependency-attention-based lstm for target-dependent sentiment analysis. ||| 22382 ||| 1382 ||| 
2019 ||| risk avoidance through reliable attention management at control room workstations. ||| 22383 ||| 22384 ||| 22385 ||| 22386 ||| 22387 ||| 22388 ||| 
2020 ||| a 28-37 ghz triple-stage transformer-coupled sige lna with 2.5 db minimum nf for low power wideband phased array receivers. ||| 22389 ||| 22390 ||| 
2019 ||| a compact ka-band transformer-coupled power amplifier for 5g in 0.15um gaas. ||| 22391 ||| 22392 ||| 
2021 ||| a 268-325 ghz 5.2 dbm psat frequency doubler using transformer-based mode separation in sige bicmos technology. ||| 22393 ||| 22394 ||| 22395 ||| 22396 ||| 18664 ||| 
2019 ||| a 6-12 ghz reconfigurable transformer-based outphasing combiner in 250-nm gaas. ||| 22397 ||| 22398 ||| 22399 ||| 22400 ||| 
2020 ||| attention vs. precision: latency scheduling for uncertainty resilient control systems. ||| 22401 ||| 11005 ||| 22402 ||| 7111 ||| 22403 ||| 7111 ||| 
2021 ||| dynamic allocation of visual attention for vision-based autonomous navigation under data rate constraints. ||| 22404 ||| 22405 ||| 22406 ||| 
2020 ||| an approach to minimum attention control by sparse derivative. ||| 22407 ||| 22408 ||| 
2020 ||| using reverse interactive audio systems (rias) to direct attention in virtual reality narrative practices: a case study. ||| 22409 ||| 
2021 ||| lean-back machina: attention-based skippable segments in interactive cinema. ||| 22410 ||| 22411 ||| 22412 ||| 22413 ||| 22414 ||| 22415 ||| 22416 ||| 22417 ||| 
2018 ||| visual question answering using explicit visual attention. ||| 22418 ||| 22419 ||| 13153 ||| 
2020 ||| transformer-combining digital pa with efficiency peaking at 0, -6, and -12 db backoff in 32nm cmos. ||| 22420 ||| 22421 ||| 22422 ||| 22423 ||| 22424 ||| 22425 ||| 
2019 ||| a 28 ghz 8-bit calibration-free lo-path phase shifter using transformer-based vector summing topology in 40 nm cmos. ||| 22426 ||| 22427 ||| 22428 ||| 22429 ||| 22430 ||| 22431 ||| 17228 ||| 22432 ||| 
2021 ||| knowledge distillation based on positive-unlabeled classification and attention mechanism. ||| 5191 ||| 5190 ||| 5112 ||| 5048 ||| 22433 ||| 22434 ||| 
2020 ||| mam: mixed attention module with random disruption augmentation for image classification. ||| 22435 ||| 22436 ||| 22437 ||| 22438 ||| 22439 ||| 
2020 ||| investigation of effect of stray capacitances in air-core toroidal transformer at high-frequency oscillation based on internal magnetic flux density. ||| 22440 ||| 22441 ||| 22442 ||| 
2020 ||| pedestrian tracking with gated recurrent units and attention mechanisms. ||| 22443 ||| 2393 ||| 
2020 ||| unsupervised multiple granularities attention-attribute learning for person re-identification. ||| 11453 ||| 5126 ||| 515 ||| 
2021 ||| sdan: stacked diverse attention network for video action recognition. ||| 22444 ||| 22445 ||| 22446 ||| 22447 ||| 22448 ||| 22449 ||| 
2017 ||| better deep visual attention with reinforcement learning in action recognition. ||| 8608 ||| 2200 ||| 22450 ||| 22451 ||| 
2021 ||| a novel low-complexity attention-driven composite model for speech enhancement. ||| 22452 ||| 11999 ||| 3766 ||| 22453 ||| 
2020 ||| through-the-barrier communications in isolated class-e converters embedding a low-k transformer. ||| 22454 ||| 22455 ||| 22456 ||| 22457 ||| 22458 ||| 
2017 ||| a transformer-less duplexer with out-of-band filtering for same-channel full-duplex radios. ||| 22459 ||| 22460 ||| 
2019 ||| transformer-based ultra-wide band 43 ghz vco in 28 nm cmos for fmcw radar system. ||| 22461 ||| 22462 ||| 3222 ||| 5775 ||| 22463 ||| 
2021 ||| spatial and channel dimensions attention feature transfer for better convolutional neural networks. ||| 5191 ||| 5190 ||| 5112 ||| 5048 ||| 22433 ||| 
2020 ||| generative image inpainting based on wavelet transform attention model. ||| 5187 ||| 3313 ||| 21518 ||| 621 ||| 
2018 ||| cmos rectifier with on-chip transformer-coupled tunable matching network for biomedical implants. ||| 22464 ||| 22465 ||| 
2020 ||| dynamic spatial-temporal graph attention graph convolutional network for short-term traffic flow forecasting. ||| 22466 ||| 22467 ||| 22468 ||| 
2021 ||| attention-based bidirectional lstm-cnn model for remaining useful life estimation. ||| 22469 ||| 22470 ||| 22471 ||| 22472 ||| 7298 ||| 
2019 ||| spatial-temporal visual attention model for video quality assessment. ||| 22473 ||| 22474 ||| 22475 ||| 22476 ||| 22477 ||| 
2021 ||| dual-path deep supervision network with self-attention for visible-infrared person re-identification. ||| 22478 ||| 5125 ||| 515 ||| 513 ||| 514 ||| 
2021 ||| attention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario. ||| 22479 ||| 22480 ||| 11121 ||| 22481 ||| 22482 ||| 16276 ||| 
2020 ||| tweet stance detection: a two-stage dc-bilstm model based on semantic attention. ||| 22483 ||| 411 ||| 14419 ||| 22484 ||| 
2019 ||| short-term traffic flow prediction using attention-based long short-term memory network. ||| 22485 ||| 22486 ||| 22487 ||| 22488 ||| 1199 ||| 22489 ||| 22490 ||| 
2020 ||| word level domain-diversity attention based lstm model for sentiment classification. ||| 22491 ||| 22492 ||| 16421 ||| 759 ||| 22493 ||| 
2020 ||| blhnn: a novel charge prediction model based on bi-attention lstm-cnn hybrid neural network. ||| 5932 ||| 411 ||| 5809 ||| 
2019 ||| attention-based text recognition in image. ||| 11444 ||| 11445 ||| 11446 ||| 472 ||| 22494 ||| 22495 ||| 
2019 ||| stns-csg: syntax tree networks with self-attention for complex sql generation. ||| 22496 ||| 411 ||| 412 ||| 5809 ||| 
2020 ||| a transformer-based model for sentence-level chinese mandarin lipreading. ||| 22497 ||| 11445 ||| 22494 ||| 
2019 ||| evaluating performance and accuracy improvements for attention-ocr. ||| 22498 ||| 22499 ||| 22500 ||| 22501 ||| 
2021 ||| bapm: block attention profiling model for multi-tab website fingerprinting attacks on tor. ||| 22502 ||| 9543 ||| 745 ||| 5189 ||| 9544 ||| 748 ||| 
2019 ||| attention learning with retrievable acoustic embedding of personality for emotion recognition. ||| 14281 ||| 12347 ||| 
2021 ||| using multimodal transformers in affective computing. ||| 22503 ||| 
2021 ||| modality fusion network and personalized attention in momentary stress detection in the wild. ||| 15239 ||| 22504 ||| 22505 ||| 22506 ||| 
2019 ||| slices of attention in asynchronous video job interviews. ||| 2713 ||| 18136 ||| 18137 ||| 18139 ||| 9772 ||| 9773 ||| 
2021 ||| using knowledge-embedded attention to augment pre-trained language models for fine-grained emotion recognition. ||| 22507 ||| 20976 ||| 
2017 ||| visual attention in schizophrenia: eye contact and gaze aversion during clinical interactions. ||| 22508 ||| 18894 ||| 22509 ||| 22510 ||| 22511 ||| 3601 ||| 
2017 ||| effect of different music genre: attention vs. meditation. ||| 22512 ||| 22513 ||| 
2019 ||| attention/distraction estimation for surgeons during laparoscopic cholecystectomies. ||| 22514 ||| 10100 ||| 22515 ||| 
2021 ||| emotion-aware transformer encoder for empathetic dialogue generation. ||| 22516 ||| 22517 ||| 22518 ||| 22519 ||| 
2019 ||| learning temporal and bodily attention in protective movement behavior detection. ||| 5663 ||| 3626 ||| 22520 ||| 13955 ||| 22521 ||| 22522 ||| 
2019 ||| combining gated convolutional networks and self-attention mechanism for speech emotion recognition. ||| 242 ||| 644 ||| 14269 ||| 645 ||| 
2020 ||| unseen filler generalization in attention-based natural language reasoning models. ||| 22523 ||| 22524 ||| 22525 ||| 22526 ||| 
2018 ||| seeing signs of danger: attention-accelerated hazmat label detection. ||| 22527 ||| 16321 ||| 16322 ||| 13310 ||| 16325 ||| 
2018 ||| human spatio-temporal attention modeling using head pose tracking for implicit object of interest discrimination in robot agents. ||| 22528 ||| 22529 ||| 
2020 ||| eye-tracking study of direction influence of user's attention for intelligence system design. ||| 22530 ||| 22531 ||| 22532 ||| 20220 ||| 22533 ||| 22534 ||| 22535 ||| 22536 ||| 
2017 ||| intelligent notification and attention management on mobile devices. ||| 22537 ||| 22538 ||| 22539 ||| 22540 ||| 22541 ||| 22542 ||| 22543 ||| 11054 ||| 
2021 ||| the attention kitchen: comparing modalities for smart home notifications in a cooking scenario. ||| 22538 ||| 22544 ||| 22545 ||| 22546 ||| 22547 ||| 
2019 ||| evaluation of attention inducing effects using ubiquitous humanlike face robots. ||| 22548 ||| 22549 ||| 22550 ||| 22551 ||| 
2020 ||| addressing inattentional blindness with smart eyewear and vibrotactile feedback on the finger, wrist, and forearm. ||| 22552 ||| 22553 ||| 22554 ||| 22555 ||| 22556 ||| 
2021 ||| simplifying paragraph-level question generation via transformer language models. ||| 22557 ||| 22558 ||| 22559 ||| 22560 ||| 
2021 ||| abae: utilize attention to boost graph auto-encoder. ||| 638 ||| 7160 ||| 22561 ||| 13467 ||| 13468 ||| 
2021 ||| online multi-object tracking with pose-guided object location and dual self-attention network. ||| 1340 ||| 22562 ||| 22563 ||| 22564 ||| 12628 ||| 
2019 ||| hierarchical convolutional attention networks using joint chinese word embedding for text classification. ||| 22565 ||| 16445 ||| 22566 ||| 18432 ||| 22567 ||| 
2019 ||| deep learning method with attention for extreme multi-label text classification. ||| 4008 ||| 22568 ||| 5046 ||| 1558 ||| 
2021 ||| span: subgraph prediction attention network for dynamic graphs. ||| 8177 ||| 22569 ||| 22570 ||| 13541 ||| 
2019 ||| saf: semantic attention fusion mechanism for pedestrian detection. ||| 22571 ||| 6459 ||| 4151 ||| 22572 ||| 22573 ||| 22574 ||| 
2021 ||| adversarial training for image captioning incorporating relation attention. ||| 11456 ||| 264 ||| 613 ||| 262 ||| 
2019 ||| cd-abm: curriculum design with attention branch model for person re-identification. ||| 19640 ||| 22575 ||| 22444 ||| 22576 ||| 22577 ||| 22449 ||| 
2021 ||| vsec: transformer-based model for vietnamese spelling correction. ||| 22578 ||| 22579 ||| 22580 ||| 22581 ||| 
2019 ||| a hierarchical attention based seq2seq model for chinese lyrics generation. ||| 22582 ||| 5894 ||| 22583 ||| 2994 ||| 705 ||| 
2019 ||| multi-label recognition of paintings with cascaded attention network. ||| 6898 ||| 22584 ||| 22585 ||| 22586 ||| 
2019 ||| hint-embedding attention-based lstm for aspect identification sentiment analysis. ||| 22587 ||| 22588 ||| 22589 ||| 22590 ||| 
2019 ||| discriminative deep attention-aware hashing for face image retrieval. ||| 21478 ||| 1717 ||| 4198 ||| 22591 ||| 4201 ||| 
2018 ||| phonologically aware bilstm model for mongolian phrase break prediction with attention mechanism. ||| 1840 ||| 790 ||| 4499 ||| 4600 ||| 12071 ||| 
2021 ||| pupilface: a cascaded face detection and location network fusing attention. ||| 2008 ||| 22592 ||| 
2021 ||| van: voting and attention based network for unsupervised medical image registration. ||| 22593 ||| 22594 ||| 7786 ||| 22595 ||| 7784 ||| 
2018 ||| attention-based linguistically constraints network for aspect-level sentiment. ||| 22596 ||| 5078 ||| 
2018 ||| reading more efficiently: multi-sentence summarization with a dual attention and copy-generator network. ||| 1325 ||| 15780 ||| 660 ||| 
2018 ||| attention based meta path fusion for heterogeneous information network embedding. ||| 9021 ||| 1373 ||| 412 ||| 
2021 ||| graph attention convolutional network with motion tempo enhancement for skeleton-based action recognition. ||| 22597 ||| 22598 ||| 22599 ||| 22600 ||| 22601 ||| 11466 ||| 12646 ||| 855 ||| 
2019 ||| time-guided high-order attention model of longitudinal heterogeneous healthcare data. ||| 22602 ||| 19721 ||| 1175 ||| 
2021 ||| anf: attention-based noise filtering strategy for unsupervised few-shot classification. ||| 22603 ||| 8693 ||| 764 ||| 22604 ||| 4552 ||| 588 ||| 
2018 ||| matching attention network for domain adaptation optimized by joint gans and kl-mmd. ||| 22605 ||| 22606 ||| 22607 ||| 5865 ||| 
2021 ||| an attention-based approach to accelerating sequence generative adversarial nets. ||| 22608 ||| 22609 ||| 22610 ||| 259 ||| 
2021 ||| random walk erasing with attention calibration for action recognition. ||| 22611 ||| 8372 ||| 8374 ||| 22612 ||| 22613 ||| 17926 ||| 
2021 ||| off-tanet: a lightweight neural micro-expression recognizer with optical flow features and integrated attention mechanism. ||| 22614 ||| 968 ||| 22615 ||| 
2021 ||| heterogeneous graph attention network for user geolocation. ||| 13514 ||| 22616 ||| 22617 ||| 22618 ||| 1748 ||| 
2018 ||| two-step multi-factor attention neural network for answer selection. ||| 22619 ||| 5078 ||| 22620 ||| 3772 ||| 
2019 ||| a light-weight context-aware self-attention model for skin lesion segmentation. ||| 22621 ||| 3890 ||| 3879 ||| 22622 ||| 2014 ||| 
2019 ||| duo attention with deep learning on tomato yield prediction and factor interpretation. ||| 22623 ||| 22624 ||| 22625 ||| 858 ||| 
2021 ||| punctuation prediction in vietnamese asrs using transformer-based models. ||| 22626 ||| 22627 ||| 
2018 ||| unrest news amount prediction with context-aware attention lstm. ||| 22628 ||| 2424 ||| 843 ||| 19724 ||| 
2019 ||| kb-transformer: incorporating knowledge into end-to-end task-oriented dialog systems. ||| 22629 ||| 22630 ||| 8467 ||| 
2019 ||| distribution transformer condition monitoring based on edge intelligence for industrial iot. ||| 22631 ||| 22632 ||| 22633 ||| 
2021 ||| environmental sound classification with tiny transformers in noisy edge environments. ||| 22634 ||| 22635 ||| 22636 ||| 22637 ||| 22638 ||| 22639 ||| 22640 ||| 22641 ||| 22642 ||| 22643 ||| 22644 ||| 
2021 ||| malicious login detection using long short-term memory with an attention mechanism. ||| 13101 ||| 22645 ||| 13103 ||| 
2021 ||| pfedatt: attention-based personalized federated learning on heterogeneous clients. ||| 22646 ||| 3042 ||| 18476 ||| 22647 ||| 13677 ||| 
2019 ||| a model of text-enhanced knowledge graph representation learning with collaborative attention. ||| 21183 ||| 6275 ||| 21185 ||| 
2018 ||| stock price prediction using attention-based multi-input lstm. ||| 1705 ||| 8920 ||| 11190 ||| 
2019 ||| multi-scale visual semantics aggregation with self-attention for end-to-end image-text matching. ||| 17871 ||| 22648 ||| 1280 ||| 
2021 ||| relation also need attention: integrating relation information into image captioning. ||| 11456 ||| 264 ||| 22649 ||| 613 ||| 262 ||| 
2019 ||| realistic image generation using region-phrase attention. ||| 22650 ||| 11378 ||| 22651 ||| 
2020 ||| inverse visual question answering with multi-level attentions. ||| 22652 ||| 22653 ||| 
2019 ||| unpaired data based cross-domain synthesis and segmentation using attention neural network. ||| 10405 ||| 22654 ||| 22655 ||| 22656 ||| 
2020 ||| bidirectional dependency-guided attention for relation extraction. ||| 22657 ||| 241 ||| 22658 ||| 22659 ||| 3738 ||| 822 ||| 
2021 ||| s2tnet: spatio-temporal transformer networks for trajectory prediction in autonomous driving. ||| 22660 ||| 22661 ||| 22662 ||| 
2020 ||| aarm: action attention recalibration module for action recognition. ||| 22663 ||| 2771 ||| 22664 ||| 22665 ||| 22666 ||| 
2020 ||| efficient attention calibration network for real-time semantic segmentation. ||| 22667 ||| 1840 ||| 816 ||| 7676 ||| 817 ||| 19289 ||| 
2018 ||| tvt: two-view transformer network for video captioning. ||| 8009 ||| 22668 ||| 17724 ||| 22669 ||| 
2018 ||| adversarial tableqa: attention supervision for question answering on tables. ||| 22670 ||| 3713 ||| 3716 ||| 22671 ||| 
2018 ||| a hierarchical conditional attention-based neural networks for paraphrase generation. ||| 17480 ||| 17481 ||| 17482 ||| 
2017 ||| hierarchical attention network with xgboost for recognizing insufficiently supported argument. ||| 22672 ||| 22673 ||| 22674 ||| 22675 ||| 22676 ||| 22677 ||| 
2020 ||| heterogeneous graph attention networks for scalable multi-robot scheduling with temporospatial constraints. ||| 22678 ||| 3929 ||| 
2020 ||| transformer-based context-aware sarcasm detection in conversation threads from social media. ||| 10453 ||| 3489 ||| 375 ||| 
2020 ||| applying transformers and aspect-based sentiment analysis approaches on sarcasm detection. ||| 22679 ||| 22680 ||| 4014 ||| 
2020 ||| transformers on sarcasm detection with context. ||| 22681 ||| 22682 ||| 
2020 ||| a transformer approach to contextual sarcasm detection in twitter. ||| 22683 ||| 22684 ||| 22685 ||| 22686 ||| 22687 ||| 22688 ||| 
2020 ||| detecting sarcasm in conversation context using transformer-based models. ||| 22689 ||| 22690 ||| 10510 ||| 
2020 ||| go figure! multi-task transformer-based architecture for metaphor detection using idioms: ets team in 2020 metaphor shared task. ||| 22691 ||| 22692 ||| 22693 ||| 16987 ||| 
2020 ||| metaphor detection using contextual word embeddings from transformers. ||| 22694 ||| 22695 ||| 22696 ||| 22687 ||| 22688 ||| 
2021 ||| contextual-semantic-aware linkable knowledge prediction in stack overflow via self-attention. ||| 22697 ||| 8200 ||| 14788 ||| 8201 ||| 22698 ||| 665 ||| 
2021 ||| exception handling recommendation based on self-attention network. ||| 7058 ||| 22699 ||| 22700 ||| 
2021 ||| artificial intelligence in the innovation process - do we pay attention to this participant in innovative projects? ||| 22701 ||| 
2017 ||| a state-based game attention model for cloud gaming. ||| 15382 ||| 22702 ||| 10381 ||| 
2017 ||| towards improving visual attention models using influencing factors in a video gaming context. ||| 22703 ||| 22704 ||| 22705 ||| 12198 ||| 3831 ||| 
2021 ||| msa transformer. ||| 22706 ||| 22707 ||| 22708 ||| 22709 ||| 2042 ||| 18850 ||| 22710 ||| 22711 ||| 
2019 ||| processing megapixel images with deep attention-sampling models. ||| 9348 ||| 1226 ||| 9349 ||| 
2021 ||| thinking like transformers. ||| 22712 ||| 3441 ||| 22713 ||| 
2021 ||| convit: improving vision transformers with soft convolutional inductive biases. ||| 2693 ||| 22714 ||| 1887 ||| 22715 ||| 3824 ||| 22716 ||| 22717 ||| 
2020 ||| learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. ||| 22718 ||| 22719 ||| 9194 ||| 22720 ||| 22721 ||| 9197 ||| 22722 ||| 9196 ||| 
2018 ||| overcoming catastrophic forgetting with hard attention to the task. ||| 11846 ||| 22723 ||| 22724 ||| 22725 ||| 
2021 ||| evolving attention with residual convolutions. ||| 18497 ||| 7703 ||| 22726 ||| 19936 ||| 18503 ||| 8160 ||| 13362 ||| 7875 ||| 18501 ||| 
2021 ||| learning self-modulating attention in continuous time space with applications to sequential recommendation. ||| 6915 ||| 22727 ||| 22728 ||| 2307 ||| 22729 ||| 22730 ||| 8532 ||| 
2020 ||| improving transformer optimization through better initialization. ||| 22731 ||| 21236 ||| 2600 ||| 22732 ||| 2583 ||| 
2021 ||| trees with attention for set prediction tasks. ||| 22733 ||| 22734 ||| 
2020 ||| on layer normalization in the transformer architecture. ||| 22735 ||| 22736 ||| 18012 ||| 11162 ||| 22737 ||| 17849 ||| 22738 ||| 1444 ||| 3807 ||| 4791 ||| 
2021 ||| sparsebert: rethinking the importance analysis in self-attention. ||| 22739 ||| 22740 ||| 22741 ||| 1687 ||| 1686 ||| 22742 ||| 22743 ||| 
2019 ||| self-attention graph pooling. ||| 19095 ||| 22744 ||| 9250 ||| 
2021 ||| synthesizer: rethinking self-attention for transformer models. ||| 1398 ||| 3292 ||| 3294 ||| 22745 ||| 22746 ||| 9580 ||| 
2021 ||| is space-time attention all you need for video understanding? ||| 7362 ||| 22747 ||| 7366 ||| 
2017 ||| online and linear-time attention by enforcing monotonic alignments. ||| 3338 ||| 22748 ||| 22749 ||| 12097 ||| 22750 ||| 
2021 ||| linear transformers are secretly fast weight programmers. ||| 22751 ||| 12659 ||| 4194 ||| 11785 ||| 
2019 ||| the evolved transformer. ||| 22752 ||| 9372 ||| 17950 ||| 
2021 ||| a unified generative adversarial network training via self-labeling and self-attention. ||| 22753 ||| 22754 ||| 
2021 ||| poolingformer: long document modeling with pooling attention. ||| 17750 ||| 4813 ||| 3172 ||| 20432 ||| 907 ||| 3706 ||| 3175 ||| 
2019 ||| insertion transformer: flexible sequence generation via insertion operations. ||| 22755 ||| 14276 ||| 22756 ||| 4960 ||| 
2021 ||| perceiver: general perception with iterative attention. ||| 22757 ||| 22758 ||| 22759 ||| 22760 ||| 1997 ||| 1994 ||| 1995 ||| 
2020 ||| infinite attention: nngp and ntk for deep attention networks. ||| 22761 ||| 22762 ||| 22763 ||| 22764 ||| 
2021 ||| you only sample (almost) once: linear cost self-attention via bernoulli sampling. ||| 18029 ||| 18028 ||| 22765 ||| 22766 ||| 22767 ||| 18033 ||| 
2020 ||| stabilizing transformers for reinforcement learning. ||| 18744 ||| 22768 ||| 3709 ||| 22769 ||| 22770 ||| 2713 ||| 22771 ||| 22772 ||| 22773 ||| 13128 ||| 22774 ||| 22775 ||| 22776 ||| 22777 ||| 22778 ||| 22289 ||| 
2021 ||| el-attention: memory efficient lossless attention for generation. ||| 7666 ||| 22779 ||| 17691 ||| 22780 ||| 4813 ||| 3706 ||| 4817 ||| 
2020 ||| cost-effective interactive attention learning with neural attention processes. ||| 9311 ||| 22781 ||| 22782 ||| 9315 ||| 9314 ||| 9316 ||| 9317 ||| 
2018 ||| image transformer. ||| 9133 ||| 2466 ||| 4960 ||| 9135 ||| 9132 ||| 22783 ||| 22784 ||| 
2020 ||| transformers are rnns: fast autoregressive transformers with linear attention. ||| 9348 ||| 9347 ||| 14940 ||| 1226 ||| 9349 ||| 
2018 ||| differentiable dynamic programming for structured prediction and attention. ||| 22785 ||| 9211 ||| 
2020 ||| low-rank bottleneck in multi-head attention models. ||| 2567 ||| 9155 ||| 9157 ||| 9158 ||| 9159 ||| 
2021 ||| pixeltransformer: sample conditioned signal generation. ||| 22786 ||| 7364 ||| 
2021 ||| omninet: omnidirectional representations from transformers. ||| 1398 ||| 2293 ||| 3291 ||| 3290 ||| 9234 ||| 3293 ||| 3292 ||| 22745 ||| 3294 ||| 
2018 ||| attention-based deep multiple instance learning. ||| 22787 ||| 22788 ||| 9255 ||| 
2021 ||| catformer: designing stable transformers via sensitivity analysis. ||| 22789 ||| 22790 ||| 22791 ||| 22792 ||| 22793 ||| 22794 ||| 22795 ||| 
2021 ||| relative positional encoding for transformers with linear complexity. ||| 22796 ||| 22797 ||| 22798 ||| 11901 ||| 22799 ||| 4374 ||| 8063 ||| 8064 ||| 
2019 ||| set transformer: a framework for attention-based permutation-invariant neural networks. ||| 9314 ||| 9356 ||| 22800 ||| 22801 ||| 22802 ||| 22803 ||| 
2021 ||| lipschitz normalization for self-attention layers with application to graph neural networks. ||| 22804 ||| 22805 ||| 22806 ||| 
2021 ||| autoattend: automated attention representation search. ||| 22807 ||| 398 ||| 1088 ||| 
2020 ||| transformer hawkes process. ||| 22808 ||| 22809 ||| 22810 ||| 22811 ||| 8949 ||| 
2020 ||| powernorm: rethinking batch normalization in transformers. ||| 3822 ||| 22812 ||| 22813 ||| 22814 ||| 2596 ||| 
2019 ||| bert and pals: projected attention layers for efficient adaptation in multi-task learning. ||| 9391 ||| 22815 ||| 
2020 ||| sparse sinkhorn attention. ||| 1398 ||| 3292 ||| 1041 ||| 3294 ||| 22745 ||| 
2021 ||| which transformer architecture fits my data? a vocabulary bottleneck in self-attention. ||| 9322 ||| 9321 ||| 22816 ||| 9325 ||| 
2021 ||| tfix: learning to fix coding errors with a text-to-text transformer. ||| 22817 ||| 22818 ||| 22819 ||| 15262 ||| 
2017 ||| image-to-markup generation with coarse-to-fine attention. ||| 3944 ||| 18813 ||| 22820 ||| 4962 ||| 
2019 ||| actor-attention-critic for multi-agent reinforcement learning. ||| 22821 ||| 17735 ||| 
2021 ||| vilt: vision-and-language transformer without convolution or region supervision. ||| 9355 ||| 22822 ||| 22823 ||| 
2021 |||  distillation through attention. ||| 1887 ||| 2118 ||| 1893 ||| 8682 ||| 2119 ||| 1890 ||| 1891 ||| 1892 ||| 
2020 ||| learning to encode position for transformer with continuous dynamical model. ||| 22824 ||| 22825 ||| 22826 ||| 21252 ||| 
2020 ||| non-autoregressive machine translation with disentangled context transformer. ||| 22827 ||| 22828 ||| 22829 ||| 9318 ||| 
2019 ||| self-attention generative adversarial networks. ||| 14048 ||| 22830 ||| 1749 ||| 9181 ||| 
2021 ||| lietransformer: equivariant self-attention for lie groups. ||| 22831 ||| 22832 ||| 22833 ||| 22834 ||| 22803 ||| 22835 ||| 
2021 ||| generative adversarial transformers. ||| 22836 ||| 22837 ||| 
2020 ||| encoding musical style with transformer autoencoders. ||| 22838 ||| 11910 ||| 11911 ||| 22839 ||| 11914 ||| 
2021 ||| cate: computation-aware neural architecture encoding with transformers. ||| 22840 ||| 22841 ||| 523 ||| 22842 ||| 
2019 ||| area attention. ||| 438 ||| 9135 ||| 22843 ||| 22844 ||| 
2021 ||| the lipschitz constant of self-attention. ||| 22835 ||| 22845 ||| 22846 ||| 
2019 ||| equivariant transformer networks. ||| 22847 ||| 9953 ||| 22848 ||| 
2021 ||| attention is not all you need: pure attention loses rank doubly exponentially with depth. ||| 22849 ||| 22850 ||| 22851 ||| 
2020 ||| train big, then compress: rethinking model size for efficient training and inference of transformers. ||| 22852 ||| 3461 ||| 3822 ||| 18746 ||| 2596 ||| 3145 ||| 22853 ||| 
2021 ||| bayesian attention belief networks. ||| 9282 ||| 9281 ||| 9283 ||| 9284 ||| 
2021 ||| simam: a simple, parameter-free attention module for convolutional neural networks. ||| 22854 ||| 22855 ||| 22856 ||| 22857 ||| 
2021 ||| differentiable spatial planning using transformers. ||| 17768 ||| 22858 ||| 2030 ||| 
2021 ||| pipetransformer: automated elastic pipelining for distributed training of large-scale models. ||| 22859 ||| 12469 ||| 22860 ||| 22861 ||| 
2021 ||| generative video transformer: can objects be the words? ||| 22862 ||| 22863 ||| 22864 ||| 
2017 ||| position shift of phosphene and attention attraction in arbitrary direction with galvanic retina stimulation. ||| 22865 ||| 22866 ||| 22867 ||| 22868 ||| 22869 ||| 
2021 ||| exploiting triangle patterns for heterogeneous graph attention network. ||| 22870 ||| 22871 ||| 
2021 ||| visualizing web users' attention to text with selection heatmaps. ||| 9809 ||| 
2020 ||| detecting rumor on microblogging platforms via a hybrid stance attention mechanism. ||| 8134 ||| 411 ||| 412 ||| 
2020 ||| a hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention. ||| 18371 ||| 22872 ||| 14072 ||| 22873 ||| 
2022 ||| multi-resolution attention for personalized item search. ||| 22874 ||| 22875 ||| 22876 ||| 22877 ||| 22878 ||| 22879 ||| 22880 ||| 22881 ||| 
2020 ||| interpretable click-through rate prediction through hierarchical attention. ||| 1155 ||| 1156 ||| 8335 ||| 1159 ||| 1160 ||| 
2019 ||| social attentional memory network: modeling aspect- and friend-level differences in recommendation. ||| 8957 ||| 1254 ||| 1255 ||| 1256 ||| 
2020 ||| jointly optimized neural coreference resolution with mutual attention. ||| 11423 ||| 1235 ||| 22882 ||| 22883 ||| 22884 ||| 22885 ||| 22886 ||| 
2021 ||| incorporating wide context information for deep knowledge tracing using attentional bi-interaction. ||| 22887 ||| 22888 ||| 22889 ||| 2251 ||| 10707 ||| 
2020 ||| dysat: deep neural representation learning on dynamic graphs via self-attention networks. ||| 22890 ||| 22891 ||| 22892 ||| 781 ||| 2792 ||| 
2022 ||| pretraining multi-modal representations for chinese ner task with cross-modality attention. ||| 22893 ||| 22894 ||| 22895 ||| 22896 ||| 6307 ||| 8837 ||| 22897 ||| 
2022 ||| a neighborhood-attention fine-grained entity typing for knowledge graph completion. ||| 22898 ||| 17662 ||| 22899 ||| 22900 ||| 22901 ||| 
2020 ||| recurrent attention walk for semi-supervised classification. ||| 22902 ||| 22903 ||| 22904 ||| 9751 ||| 
2019 ||| msa: jointly detecting drug name and adverse drug reaction mentioning tweets with multi-head self-attention. ||| 3754 ||| 3755 ||| 10472 ||| 4792 ||| 2795 ||| 9574 ||| 
2020 ||| time interval aware self-attention for sequential recommendation. ||| 5292 ||| 6449 ||| 22905 ||| 
2021 ||| origin-aware next destination recommendation with personalized preference attention. ||| 1342 ||| 1343 ||| 1344 ||| 1345 ||| 1346 ||| 1347 ||| 22906 ||| 
2022 ||| scope-aware re-ranking with gated attention in feed. ||| 22907 ||| 22908 ||| 3433 ||| 883 ||| 22909 ||| 22910 ||| 1086 ||| 22911 ||| 
2019 ||| session-based social recommendation via dynamic graph attention networks. ||| 1269 ||| 22912 ||| 1266 ||| 22913 ||| 1251 ||| 1248 ||| 
2017 ||| multi-column convolutional neural networks with causality-attention for why-question answering. ||| 3379 ||| 3382 ||| 3316 ||| 3380 ||| 3381 ||| 
2021 ||| attentionflow: visualising influence in networks of time series. ||| 22914 ||| 22915 ||| 22916 ||| 22917 ||| 14797 ||| 22918 ||| 14077 ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond. ||| 9664 ||| 3006 ||| 3009 ||| 
2018 ||| predicting multi-step citywide passenger demands using attention-based neural networks. ||| 22919 ||| 8920 ||| 11190 ||| 17777 ||| 
2019 ||| how is attention allocated?: data-driven studies of popularity and engagement in online videos. ||| 22916 ||| 
2022 ||| a personalized cross-platform post style transfer method based on transformer and bi-attention mechanism. ||| 12029 ||| 22920 ||| 989 ||| 2961 ||| 11106 ||| 2962 ||| 
2021 ||| combining rnn with transformer for modeling multi-leg trips. ||| 22921 ||| 
2021 ||| modeling across-context attention for long-tail query classification in e-commerce. ||| 6213 ||| 5201 ||| 22922 ||| 5250 ||| 22923 ||| 22924 ||| 
2022 ||| learning concept prerequisite relations from educational data via multi-head attention variational graph auto-encoders. ||| 22925 ||| 22926 ||| 22927 ||| 17827 ||| 22928 ||| 22929 ||| 
2021 ||| multi-interactive attention network for fine-grained feature learning in ctr prediction. ||| 3433 ||| 22907 ||| 22930 ||| 1302 ||| 22931 ||| 1086 ||| 22932 ||| 1301 ||| 
2021 ||| attention-based neural re-ranking approach for next city in trip recommendations. ||| 22933 ||| 22934 ||| 
2021 ||| test students' attention in class using a mobile application. ||| 22935 ||| 22936 ||| 22937 ||| 
2018 ||| a survey of technologies utilized in the treatment and diagnosis of attention deficit hyperactivity disorder. ||| 22938 ||| 22939 ||| 
2019 ||| goal-oriented conversational system using transfer learning and attention mechanism. ||| 22940 ||| 22941 ||| 22942 ||| 
2021 ||| interactive attention network for chinese address element recognition. ||| 22943 ||| 22944 ||| 399 ||| 
2018 ||| quantifying the attention potential of pervasive display placements. ||| 22945 ||| 22946 ||| 22947 ||| 
2019 ||| attention-based visual-audio fusion for video caption generation. ||| 22948 ||| 22949 ||| 22950 ||| 
2019 ||| dog face recognition algorithm based on dc-attention-ssd neural network. ||| 5845 ||| 22951 ||| 22952 ||| 17613 ||| 
2017 ||| design and application of a compatible clamping fixture for current transformers auto-testing line. ||| 22953 ||| 5475 ||| 22954 ||| 1825 ||| 
2019 ||| running state prediction and evaluation of power transformers. ||| 22955 ||| 9087 ||| 5384 ||| 6915 ||| 22956 ||| 22957 ||| 
2021 ||| learning knowledge graph embeddings by multi-attention mechanism for link prediction. ||| 22958 ||| 3694 ||| 22959 ||| 
2021 ||| temporal convolution network based on attention for intelligent anomaly detection of wind turbine blades. ||| 22960 ||| 22961 ||| 22962 ||| 
2020 ||| automatic medical image report generation with multi-view and multi-modal attention mechanism. ||| 22963 ||| 2478 ||| 22964 ||| 2479 ||| 
2021 ||| attention-based cross-domain gesture recognition using wifi channel state information. ||| 22965 ||| 22966 ||| 6824 ||| 22967 ||| 
2021 ||| transformer-based rating-aware sequential recommendation. ||| 438 ||| 924 ||| 22968 ||| 1943 ||| 
2020 ||| lexicon-enhanced transformer with pointing for domains specific generative question answering. ||| 4185 ||| 4186 ||| 22969 ||| 22970 ||| 
2021 ||| spatio-temporal topology routing algorithm for opportunistic network based on self-attention mechanism. ||| 22971 ||| 22972 ||| 22973 ||| 22966 ||| 22974 ||| 
2021 ||| multi-relational hierarchical attention for top-k recommendation. ||| 22975 ||| 406 ||| 11823 ||| 
2021 ||| an efficient message dissemination scheme for cooperative drivings via multi-agent hierarchical attention reinforcement learning. ||| 22976 ||| 22977 ||| 22978 ||| 9442 ||| 12692 ||| 22979 ||| 22980 ||| 
2021 ||| optimising knee injury detection with spatial attention and validating localisation ability. ||| 22981 ||| 22982 ||| 22983 ||| 22984 ||| 22985 ||| 22986 ||| 22987 ||| 
2019 ||| convolutional attention on images for locating macular edema. ||| 22988 ||| 22989 ||| 22990 ||| 22991 ||| 22992 ||| 
2020 |||  ambient notification and attention management. ||| 22993 ||| 22539 ||| 22994 ||| 22995 ||| 11054 ||| 22996 ||| 22997 ||| 22538 ||| 
2019 ||| toward digital image processing and eye tracking to promote visual attention for people with autism. ||| 22998 ||| 22999 ||| 4614 ||| 23000 ||| 23001 ||| 
2017 |||  ambient notification and attention management. ||| 11054 ||| 22539 ||| 22538 ||| 22537 ||| 23002 ||| 23003 ||| 23004 ||| 23005 ||| 22542 ||| 22543 ||| 23006 ||| 22996 ||| 
2019 ||| capturing attentional problems with smart eyewear. ||| 23007 ||| 23008 ||| 3831 ||| 23009 ||| 23010 ||| 
2017 ||| a user-centered approach towards attention visualization for learning activities. ||| 23011 ||| 23012 ||| 
2019 ||| nurse care activity recognition: a gru-based approach with attention mechanism. ||| 23013 ||| 23014 ||| 23015 ||| 23016 ||| 7375 ||| 
2021 ||| an ensemble of convtransformer networks for the sussex-huawei locomotion-transportation (shl) recognition challenge. ||| 23017 ||| 23018 ||| 23019 ||| 5264 ||| 23020 ||| 
2019 ||| attention computing: overview of mobile sensing applied to measuring attention. ||| 22997 ||| 22993 ||| 
2018 ||| on attention models for human activity recognition. ||| 23021 ||| 23022 ||| 11303 ||| 
2018 ||| attentivu: evaluating the feasibility of biofeedback glasses to monitor and improve attention. ||| 6446 ||| 23023 ||| 6451 ||| 
2021 ||| dark-channel mixed attention based neural networks for smoke detection in fog environment. ||| 23024 ||| 23025 ||| 23026 ||| 23027 ||| 23028 ||| 2755 ||| 
2020 ||| behavification: bypassing human's attentional and cognitive systems for automated behavior change. ||| 11054 ||| 23029 ||| 23030 ||| 
2018 ||| tweet emoji prediction using hierarchical model with attention. ||| 3754 ||| 3755 ||| 10471 ||| 2795 ||| 9574 ||| 
2018 ||| attention management for improved renewable energy usage at households using iot-enabled ambient displays. ||| 23031 ||| 23032 ||| 
2019 ||| gesture recognition based on convlstm-attention implementation of small data semg signals. ||| 23033 ||| 23034 ||| 23035 ||| 23036 ||| 
2017 ||| symbiotic attention management in the context of internet of things. ||| 23037 ||| 23038 ||| 23039 ||| 
2017 ||| using corneal imaging for measuring a human's visual attention. ||| 23040 ||| 23041 ||| 23042 ||| 23005 ||| 
2018 ||| understanding and improving recurrent networks for human activity recognition by continuous attention. ||| 9964 ||| 23043 ||| 23044 ||| 9963 ||| 23045 ||| 14548 ||| 23046 ||| 
2021 ||| tacnet: task-aware electroencephalogram classification for brain-computer interface through a novel temporal attention convolutional network. ||| 23047 ||| 23048 ||| 23049 ||| 11634 ||| 23050 ||| 2903 ||| 1740 ||| 23051 ||| 
2018 |||  ambient notification and attention management. ||| 22537 ||| 22539 ||| 22538 ||| 22996 ||| 23004 ||| 23005 ||| 11054 ||| 
2019 |||  ambient notification and attention management. ||| 22539 ||| 22538 ||| 22537 ||| 23002 ||| 23052 ||| 23005 ||| 11054 ||| 22996 ||| 
2020 ||| blink rate variability: a marker of sustained attention during a visual task. ||| 23053 ||| 23054 ||| 23055 ||| 23056 ||| 23057 ||| 23058 ||| 23059 ||| 
2020 ||| transformer fault diagnosis based on bp neural network by improved apriori algorithm. ||| 23060 ||| 23061 ||| 23062 ||| 23063 ||| 
2018 ||| multi-modal sequence fusion via recursive attention for emotion recognition. ||| 23064 ||| 23065 ||| 23066 ||| 23067 ||| 23068 ||| 23069 ||| 23070 ||| 
2018 ||| attention-free encoder decoder for morphological processing. ||| 23071 ||| 23072 ||| 
2018 ||| hierarchical attention based position-aware network for aspect-level sentiment analysis. ||| 16631 ||| 1305 ||| 20809 ||| 
2019 ||| variational semi-supervised aspect-term sentiment analysis via transformer. ||| 23073 ||| 5201 ||| 14931 ||| 12772 ||| 23074 ||| 23075 ||| 23076 ||| 
2017 ||| character sequence-to-sequence model with global attention for universal morphological reinflection. ||| 2449 ||| 6845 ||| 2450 ||| 
2017 ||| attention-based recurrent convolutional neural network for automatic essay scoring. ||| 23077 ||| 3289 ||| 1132 ||| 
2021 ||| do pretrained transformers infer telicity like humans? ||| 3356 ||| 23078 ||| 23079 ||| 3357 ||| 
2017 ||| su-rug at the conll-sigmorphon 2017 shared task: morphological inflection with attentional sequence-to-sequence models. ||| 23080 ||| 23081 ||| 20957 ||| 
2018 ||| sequence classification with human attention. ||| 23082 ||| 3078 ||| 13358 ||| 11639 ||| 3079 ||| 3080 ||| 
2020 ||| on the computational power of transformers and its implications in sequence modeling. ||| 23083 ||| 23084 ||| 23085 ||| 
2019 ||| a dual-attention hierarchical recurrent neural network for dialogue act classification. ||| 23086 ||| 23087 ||| 23088 ||| 6821 ||| 23089 ||| 
2019 ||| effective attention modeling for neural relation extraction. ||| 23090 ||| 3195 ||| 
2019 ||| triplenet: triple attention network for multi-turn response selection in retrieval-based chatbots. ||| 17877 ||| 3642 ||| 23091 ||| 23092 ||| 1219 ||| 3311 ||| 1308 ||| 3645 ||| 
2018 ||| comparing attention-based convolutional and recurrent neural networks: success and limitations in machine reading comprehension. ||| 23093 ||| 23094 ||| 9269 ||| 7193 ||| 20979 ||| 
2018 ||| pervasive attention: 2d convolutional neural networks for sequence-to-sequence prediction. ||| 23095 ||| 3510 ||| 2353 ||| 
2020 ||| interpreting attention models with human visual attention in machine reading comprehension. ||| 9269 ||| 9270 ||| 23096 ||| 8348 ||| 20979 ||| 
2019 ||| jbnu at mrp 2019: multi-level biaffine attention for semantic dependency parsing. ||| 23097 ||| 23098 ||| 23099 ||| 23100 ||| 23101 ||| 
2018 ||| global attention for name tagging. ||| 23102 ||| 23103 ||| 3604 ||| 3403 ||| 
2019 ||| in conclusion not repetition: comprehensive abstractive summarization with diversified attention based on determinantal point processes. ||| 3034 ||| 683 ||| 23104 ||| 23105 ||| 23106 ||| 
2019 ||| mrmep: joint extraction of multiple relations and multiple entity pairs based on triplet attention. ||| 23107 ||| 13161 ||| 13160 ||| 23108 ||| 
2021 ||| vqa-mhug: a gaze dataset to study multimodal neural attention in visual question answering. ||| 9269 ||| 23109 ||| 23110 ||| 23111 ||| 23112 ||| 8348 ||| 
2017 ||| it's not just about attention to details: redefining the talents autistic software developers bring to software development. ||| 23113 ||| 23114 ||| 23115 ||| 
2020 ||| comparison of attention behaviour across user sets through automatic identification of common areas of interest. ||| 23116 ||| 23117 ||| 23118 ||| 7069 ||| 23119 ||| 
2021 ||| could you please pay attention?' comparing in-person and mturk responses on a computer code review task. ||| 23120 ||| 23121 ||| 23122 ||| 23123 ||| 
2020 ||| attention or appreciation? the impact of feedback on online volunteering. ||| 23124 ||| 23125 ||| 23126 ||| 
2019 ||| exposing attention-decision-learning cycles in engineering project teams through collaborative design experiments. ||| 23127 ||| 23128 ||| 23129 ||| 
2017 ||| an exploratory study on consumers' attention towards social media advertising: an electroencephalography approach. ||| 23130 ||| 23131 ||| 
2021 ||| cagan: text-to-image generation with combined attention generative adversarial networks. ||| 23132 ||| 23133 ||| 23134 ||| 
2021 ||| t6d-direct: transformers for multi-object 6d pose direct regression. ||| 23135 ||| 23136 ||| 409 ||| 
2020 ||| pet-guided attention network for segmentation of lung tumors from pet/ct images. ||| 23137 ||| 23138 ||| 23139 ||| 23140 ||| 23141 ||| 23142 ||| 
2018 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in cnns. ||| 23143 ||| 23144 ||| 23145 ||| 23146 ||| 23147 ||| 
2019 ||| exploiting attention for visual relationship detection. ||| 23148 ||| 2407 ||| 2410 ||| 2409 ||| 
2021 ||| implicit and explicit attention for zero-shot learning. ||| 20284 ||| 23149 ||| 
2018 ||| convolve, attend and spell: an attention-based sequence-to-sequence model for handwritten word recognition. ||| 23150 ||| 23151 ||| 23152 ||| 23153 ||| 9787 ||| 7111 ||| 10907 ||| 23154 ||| 23155 ||| 
2021 ||| txt: crossmodal end-to-end learning with transformers. ||| 23156 ||| 23157 ||| 3702 ||| 23158 ||| 
2017 ||| reconstructing readerly attention: citational practices and the canon, 1789-2016. ||| 23159 ||| 23160 ||| 23161 ||| 
2017 ||| the use of the cognitive digital games in school: contributions for the attention. ||| 23162 ||| 23163 ||| 
2020 ||| not all swear words are used equal: attention over word n-grams for abusive language identification. ||| 23164 ||| 23165 ||| 23166 ||| 23167 ||| 11932 ||| 11933 ||| 23168 ||| 23169 ||| 
2020 ||| optimal real-time scheduling of human attention for a human and multi-robot collaboration system. ||| 23170 ||| 23171 ||| 
2020 ||| improved attention models for memory augmented neural network adaptive controllers. ||| 23172 ||| 23173 ||| 23174 ||| 
2019 ||| investigating the effect of lexical segmentation in transformer-based models on medical datasets. ||| 23175 ||| 23176 ||| 8301 ||| 
2021 ||| study on artificial intelligence approaches for power transformer health index assessment. ||| 23177 ||| 23178 ||| 23179 ||| 
2018 ||| recurrent attention lstm model for image chinese caption generation. ||| 23180 ||| 23181 ||| 23182 ||| 23183 ||| 23184 ||| 
2018 ||| rehabilitation support system for attentional deficits based on trail-making test. ||| 23185 ||| 23186 ||| 23187 ||| 23188 ||| 23189 ||| 
2021 ||| depth inpainting via vision transformer. ||| 23190 ||| 23191 ||| 
2020 ||| tga: two-level group attention for assembly state detection. ||| 23192 ||| 23193 ||| 23194 ||| 23195 ||| 23196 ||| 
2018 ||| guiding smombies: augmenting peripheral vision with low-cost glasses to shift the attention of smartphone users. ||| 15371 ||| 15370 ||| 23197 ||| 23198 ||| 23199 ||| 23200 ||| 20673 ||| 
2020 ||| enhancing visitor experience or hindering docent roles: attentional issues in augmented reality supported installations. ||| 15400 ||| 15401 ||| 4566 ||| 15402 ||| 8500 ||| 15384 ||| 
2017 ||| the impact of the frame of reference on attention shifts between augmented reality and real-world environment. ||| 23201 ||| 23202 ||| 23203 ||| 23204 ||| 
2021 ||| two-hand pose estimation from the non-cropped rgb image with self-attention based network. ||| 23205 ||| 23206 ||| 23207 ||| 
2021 ||| subtle attention guidance for real walking in virtual environments. ||| 23208 ||| 23209 ||| 23210 ||| 23211 ||| 13618 ||| 
2019 ||| visualization-guided attention direction in dynamic control tasks. ||| 23212 ||| 748 ||| 23213 ||| 23214 ||| 
2017 ||| handling instrument transformers and pmu errors for the estimation of line parameters in distribution grids. ||| 5649 ||| 5647 ||| 5648 ||| 23215 ||| 23216 ||| 
2018 ||| an efficient digitizer for calibration of instrument transformers. ||| 23217 ||| 23218 ||| 23219 ||| 
2021 ||| instrument transformers for power quality measurements: a review of literature and standards. ||| 5599 ||| 5601 ||| 5604 ||| 5606 ||| 5605 ||| 23220 ||| 18405 ||| 23221 ||| 
2018 ||| a simple method for compensating the harmonic distortion introduced by voltage transformers. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2018 ||| impact of capacitor voltage transformers on phasor measurement units dynamic performance. ||| 23222 ||| 5649 ||| 5635 ||| 
2018 ||| calibration of synchronized measurement system: from the instrument transformer to the pmu. ||| 5590 ||| 5591 ||| 5627 ||| 23223 ||| 23224 ||| 5594 ||| 
2021 ||| compensating the harmonic distortion introduced by instrument transformers: an improved method based on frequency-domain polynomials. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 
2017 ||| assessment of metrological characteristics of calibration systems for accuracy vs. temperature verification of voltage transformer. ||| 5590 ||| 5591 ||| 5627 ||| 23225 ||| 23226 ||| 
2018 ||| on the remote calibration of voltage transformers: validation of opportunity. ||| 23227 ||| 23228 ||| 23229 ||| 23230 ||| 23231 ||| 23232 ||| 
2018 ||| impact of careless current transformer position on current measurement. ||| 23233 ||| 23234 ||| 23235 ||| 23236 ||| 
2017 ||| calibration of commercial test sets for non-conventional instrument transformers. ||| 23237 ||| 23238 ||| 23239 ||| 23240 ||| 23241 ||| 23242 ||| 23243 ||| 
2019 ||| expressing uncertainty of voltage transformers: a proposal. ||| 5632 ||| 23244 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2018 ||| compensation of current transformers' non-linearities by means of frequency coupling matrices. ||| 23245 ||| 5602 ||| 5603 ||| 23246 ||| 5605 ||| 
2021 ||| novel calibration systems for the dynamic and steady-state testing of digital instrument transformers. ||| 23247 ||| 5599 ||| 23248 ||| 5606 ||| 23237 ||| 5605 ||| 23249 ||| 
2019 ||| a proposed method for evaluating the frequency response of 22 kv outdoor current transformers for harmonic measurements in renewable energy plant applications. ||| 23250 ||| 23251 ||| 
2019 ||| setup and characterisation of reference current-to-voltage transformers for wideband current transformers calibration up to 2 ka. ||| 23247 ||| 23237 ||| 23252 ||| 23253 ||| 23254 ||| 5605 ||| 
2020 ||| automatic diacritic restoration with transformer model based neural machine translation for east-central european languages. ||| 2713 ||| 23255 ||| 1891 ||| 23256 ||| 23257 ||| 
2022 ||| measuring human auditory attention with eeg. ||| 23258 ||| 23259 ||| 
2021 ||| mflamegaze: mobile-based flame gazing for improving sustained attention. ||| 5799 ||| 10210 ||| 23260 ||| 23261 ||| 23262 ||| 23263 ||| 23264 ||| 
2022 ||| short term effect of physical exercise on selective attention using eeg and stroop task. ||| 5799 ||| 23265 ||| 23266 ||| 23267 ||| 
2017 ||| an integrated computational framework for attention, reinforcement learning, and working memory. ||| 23268 ||| 
2021 ||| end-to-end russian speech recognition models with multi-head attention. ||| 23269 ||| 
2020 ||| audio adversarial examples for robust hybrid ctc/attention speech recognition. ||| 23270 ||| 23271 ||| 23272 ||| 23273 ||| 23274 ||| 5695 ||| 
2019 ||| exploring hybrid ctc/attention end-to-end speech recognition with gaussian processes. ||| 23270 ||| 23271 ||| 23274 ||| 23273 ||| 23275 ||| 5695 ||| 
2021 ||| an equal data setting for attention-based encoder-decoder and hmm/dnn models: a case study in finnish asr. ||| 12311 ||| 23276 ||| 23277 ||| 19568 ||| 11941 ||| 
2021 ||| regularized forward-backward decoder for attention models. ||| 23274 ||| 23270 ||| 23271 ||| 23273 ||| 5695 ||| 
2020 ||| hate speech detection using transformer ensembles on the hasoc dataset. ||| 23278 ||| 23279 ||| 23280 ||| 23281 ||| 10198 ||| 
2021 ||| induced local attention for transformer models in speech recognition. ||| 23274 ||| 23270 ||| 23271 ||| 23273 ||| 5695 ||| 
2020 ||| experimenting with attention mechanisms in joint ctc-attention models for russian speech recognition. ||| 23269 ||| 23282 ||| 
2019 ||| investigating joint ctc-attention models for end-to-end russian speech recognition. ||| 23282 ||| 23269 ||| 
2020 ||| synchronized forward-backward transformer for end-to-end speech recognition. ||| 23274 ||| 23270 ||| 23271 ||| 23273 ||| 5695 ||| 
2021 ||| human and transformer-based prosodic phrasing in two speech genres. ||| 23283 ||| 3882 ||| 14593 ||| 14594 ||| 14595 ||| 23284 ||| 
2020 ||| testing pre-trained transformer models for lithuanian news clustering. ||| 23285 ||| 23286 ||| 
2020 ||| pert: payload encoding representation from transformer for encrypted traffic classification. ||| 23287 ||| 23288 ||| 23289 ||| 
2021 ||| clinically guided trainable soft attention for early detection of oral cancer. ||| 23290 ||| 6405 ||| 23291 ||| 7852 ||| 23292 ||| 23293 ||| 23294 ||| 23295 ||| 23296 ||| 23297 ||| 23298 ||| 23299 ||| 23300 ||| 23301 ||| 23302 ||| 
2017 ||| attention-based two-phase model for video action detection. ||| 19831 ||| 2200 ||| 19832 ||| 19978 ||| 
2019 ||| patent citation dynamics modeling via multi-attention recurrent networks. ||| 17189 ||| 17191 ||| 17190 ||| 17188 ||| 17192 ||| 8985 ||| 
2019 ||| dual visual attention network for visual dialog. ||| 19645 ||| 1341 ||| 444 ||| 
2019 ||| open-ended long-form video question answering via hierarchical convolutional self-attention networks. ||| 20664 ||| 1306 ||| 19440 ||| 9576 ||| 2359 ||| 
2020 ||| multi-scale group transformer for long sequence modeling in speech separation. ||| 23303 ||| 23304 ||| 8710 ||| 7912 ||| 
2021 ||| adaptive edge attention for graph matching with outliers. ||| 16841 ||| 2163 ||| 837 ||| 16844 ||| 16845 ||| 
2020 ||| compressed self-attention for deep metric learning with low-rank approximation. ||| 17833 ||| 17834 ||| 23305 ||| 17757 ||| 
2018 ||| multiway attention networks for modeling sentence pairs. ||| 12432 ||| 3174 ||| 3497 ||| 23306 ||| 3480 ||| 
2021 ||| guided attention network for concept extraction. ||| 23307 ||| 23308 ||| 23309 ||| 17702 ||| 23310 ||| 23311 ||| 23312 ||| 1302 ||| 
2017 ||| hashtag recommendation for multimodal microblog using co-attention network. ||| 336 ||| 9694 ||| 23313 ||| 3273 ||| 4813 ||| 
2020 ||| cross-interaction hierarchical attention networks for urban anomaly prediction. ||| 1124 ||| 8917 ||| 2588 ||| 9575 ||| 
2017 ||| learning sentence representation with guidance of human attention. ||| 23314 ||| 3044 ||| 11690 ||| 
2018 ||| a^3ncf: an adaptive aspect attention model for rating prediction. ||| 13213 ||| 23315 ||| 1063 ||| 978 ||| 9674 ||| 19019 ||| 
2019 ||| graph contextualized self-attention network for session-based recommendation. ||| 23316 ||| 659 ||| 9588 ||| 6475 ||| 658 ||| 3476 ||| 11122 ||| 18526 ||| 
2018 ||| code completion with neural attention and pointer networks. ||| 595 ||| 7400 ||| 597 ||| 598 ||| 
2020 ||| neighbor combinatorial attention for critical structure mining. ||| 23317 ||| 23318 ||| 14897 ||| 
2018 ||| memory attention networks for skeleton-based action recognition. ||| 23319 ||| 10955 ||| 7240 ||| 2230 ||| 2278 ||| 2363 ||| 
2021 ||| deep neural network loses attention to adversarial images. ||| 23320 ||| 23321 ||| 
2019 ||| pre-training of graph augmented transformers for medication recommendation. ||| 3413 ||| 23322 ||| 1070 ||| 23323 ||| 
2020 ||| evidence-aware hierarchical interactive attention networks for explainable claim verification. ||| 3264 ||| 3265 ||| 23324 ||| 23325 ||| 3268 ||| 
2020 ||| hierarchical multi-scale gaussian transformer for stock movement prediction. ||| 9345 ||| 9344 ||| 7725 ||| 23326 ||| 23327 ||| 
2019 ||| attain: attention-based time-aware lstm networks for disease progression modeling. ||| 1503 ||| 13408 ||| 23328 ||| 23329 ||| 
2019 ||| hierarchical diffusion attention network. ||| 1176 ||| 1178 ||| 
2020 ||| attention-based multi-level feature fusion for named entity recognition. ||| 23330 ||| 23331 ||| 538 ||| 3646 ||| 3410 ||| 
2020 ||| arbitrary talking face generation via attentional audio-visual coherence learning. ||| 9335 ||| 18593 ||| 1329 ||| 17624 ||| 2824 ||| 
2019 ||| an actor-critic-attention mechanism for deep reinforcement learning in multi-view environments. ||| 3924 ||| 3925 ||| 
2021 ||| information bottleneck approach to spatial attention learning. ||| 23332 ||| 11641 ||| 23333 ||| 23334 ||| 23335 ||| 23336 ||| 
2018 ||| aspect term extraction with history attention and selective transformation. ||| 633 ||| 3385 ||| 23337 ||| 3015 ||| 23338 ||| 
2018 ||| multi-turn video question answering via multi-stream hierarchical attention context network. ||| 1306 ||| 19628 ||| 1115 ||| 7652 ||| 2359 ||| 1937 ||| 
2021 ||| medical image segmentation using squeeze-and-expansion transformers. ||| 23339 ||| 23340 ||| 23341 ||| 23342 ||| 4297 ||| 3390 ||| 
2020 ||| attan: attention adversarial networks for 3d point cloud semantic segmentation. ||| 23343 ||| 23344 ||| 400 ||| 5743 ||| 23345 ||| 
2019 ||| adaptive joint attention with reinforcement training for convolutional image caption. ||| 23346 ||| 23347 ||| 15620 ||| 
2020 ||| improving attention mechanism in graph neural networks via cardinality preservation. ||| 1315 ||| 12384 ||| 
2020 ||| hype-han: hyperbolic hierarchical attention network for semantic embedding. ||| 23348 ||| 619 ||| 
2017 ||| an attention-based regression model for grounding textual phrases in images. ||| 23349 ||| 4966 ||| 10267 ||| 23350 ||| 
2021 ||| fine-grained air quality inference via multi-channel attention model. ||| 23351 ||| 23352 ||| 23353 ||| 
2018 ||| attention-fused deep matching network for natural language inference. ||| 10199 ||| 9837 ||| 23354 ||| 3174 ||| 10201 ||| 880 ||| 
2020 ||| a new attention mechanism to classify multivariate time series. ||| 20492 ||| 23355 ||| 
2020 ||| attention as relation: learning supervised multi-head self-attention for relation extraction. ||| 3114 ||| 23356 ||| 23357 ||| 23358 ||| 10057 ||| 17704 ||| 
2021 ||| gasp: gated attention for saliency prediction. ||| 23359 ||| 23360 ||| 1017 ||| 
2021 ||| multi-hop attention graph neural networks. ||| 23361 ||| 23362 ||| 14316 ||| 23363 ||| 
2021 ||| attention-based pyramid dilated lattice network for blind image denoising. ||| 23364 ||| 11312 ||| 1086 ||| 
2021 ||| laughing heads: can transformers detect what makes a sentence funny? ||| 13297 ||| 23365 ||| 13296 ||| 13300 ||| 
2017 ||| video question answering via hierarchical spatio-temporal attention networks. ||| 1306 ||| 23366 ||| 1115 ||| 2359 ||| 7654 ||| 
2019 ||| clvsa: a convolutional lstm based variational sequence-to-sequence model with attention for predicting trends of financial markets. ||| 23367 ||| 23368 ||| 23369 ||| 4979 ||| 23370 ||| 
2018 ||| neural machine translation with key-value memory-augmented attention. ||| 3075 ||| 3041 ||| 9685 ||| 23371 ||| 23372 ||| 23373 ||| 15790 ||| 
2020 ||| self-supervised gait encoding with locality-aware attention for person re-identification. ||| 23374 ||| 23375 ||| 16724 ||| 6413 ||| 23376 ||| 15003 ||| 23377 ||| 
2020 ||| a relation-specific attention network for joint entity and relation extraction. ||| 23378 ||| 17663 ||| 799 ||| 17662 ||| 17937 ||| 4191 ||| 
2019 ||| matching user with item set: collaborative bundle recommendation with deep attention network. ||| 12373 ||| 1305 ||| 1063 ||| 1039 ||| 1563 ||| 
2018 ||| sequential recommender system based on hierarchical attention networks. ||| 23379 ||| 3476 ||| 1062 ||| 9588 ||| 3894 ||| 9574 ||| 9592 ||| 1236 ||| 
2018 ||| cross-media multi-level alignment with relation attention network. ||| 23380 ||| 5954 ||| 5953 ||| 
2020 ||| gesturedet: real-time student gesture analysis with multi-dimensional attention-based detector. ||| 23381 ||| 23382 ||| 23383 ||| 
2019 ||| attnsense: multi-level attention mechanism for multimodal human activity recognition. ||| 23384 ||| 8181 ||| 1190 ||| 23385 ||| 8186 ||| 
2020 ||| transformers as soft reasoners over language. ||| 23386 ||| 23387 ||| 23388 ||| 
2019 ||| t-cvae: transformer-based conditioned variational autoencoder for story completion. ||| 3120 ||| 3121 ||| 
2019 ||| semi-supervised user profiling with heterogeneous graph attention networks. ||| 23389 ||| 1143 ||| 9546 ||| 1063 ||| 18071 ||| 23390 ||| 1148 ||| 17860 ||| 
2020 ||| feature augmented memory with global attention network for videoqa. ||| 23391 ||| 1280 ||| 23392 ||| 3034 ||| 17872 ||| 23393 ||| 
2018 ||| beyond polarity: interpretable financial sentiment analysis with hierarchical query-driven attention. ||| 5015 ||| 3199 ||| 23394 ||| 3313 ||| 23395 ||| 23396 ||| 8880 ||| 
2017 ||| stance classification with target-specific neural attention. ||| 21175 ||| 15906 ||| 3664 ||| 3662 ||| 
2019 ||| locate-then-detect: real-time web attack detection via attention-based deep neural networks. ||| 23397 ||| 23398 ||| 23399 ||| 23400 ||| 
2018 ||| scanpath prediction for visual attention using ior-roi lstm. ||| 9061 ||| 23401 ||| 
2019 ||| 3dviewgraph: learning global features for 3d shapes from a graph of unordered views with attention. ||| 2563 ||| 23402 ||| 23403 ||| 2559 ||| 18230 ||| 15206 ||| 
2019 ||| beyond word attention: using segment attention in neural relation extraction. ||| 3326 ||| 758 ||| 759 ||| 379 ||| 11660 ||| 23404 ||| 
2018 ||| hermitian co-attention networks for text matching in asymmetrical domains. ||| 1398 ||| 9261 ||| 9016 ||| 
2018 ||| learning to recognize transient sound events using attentional supervision. ||| 4373 ||| 12466 ||| 4374 ||| 
2019 ||| risk assessment for networked-guarantee loans using high-order graph attention representation. ||| 11150 ||| 23405 ||| 23406 ||| 23407 ||| 5088 ||| 
2020 ||| infobox-to-text generation with tree-like planning based attention network. ||| 1281 ||| 1282 ||| 23408 ||| 1082 ||| 793 ||| 
2019 ||| addgraph: anomaly detection in dynamic graph using attention-based temporal gcn. ||| 23409 ||| 23410 ||| 595 ||| 885 ||| 1423 ||| 
2021 ||| segmenting transparent objects in the wild with transformer. ||| 2007 ||| 23411 ||| 2006 ||| 23412 ||| 1687 ||| 2010 ||| 2011 ||| 
2020 ||| pay attention to devils: a photometric stereo network for better details. ||| 23413 ||| 19991 ||| 8335 ||| 11275 ||| 2628 ||| 
2019 ||| densely connected attention flow for visual question answering. ||| 523 ||| 2058 ||| 19087 ||| 5946 ||| 2080 ||| 
2020 ||| hierarchical attention based spatial-temporal graph-to-sequence learning for grounded video description. ||| 23414 ||| 18010 ||| 23415 ||| 17723 ||| 7652 ||| 7654 ||| 
2018 ||| multi-modality sensor data classification with selective attention. ||| 586 ||| 771 ||| 23416 ||| 1300 ||| 6413 ||| 802 ||| 13181 ||| 
2021 ||| local representation is not enough: soft point-wise transformer for descriptor and detector of local features. ||| 18804 ||| 21920 ||| 5189 ||| 
2018 ||| reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 1300 ||| 4873 ||| 
2019 ||| a deep bi-directional attention network for human motion recovery. ||| 23417 ||| 23418 ||| 13432 ||| 23419 ||| 
2017 ||| modeling spatial auditory attention: handling equiprobable attended locations. ||| 9123 ||| 9124 ||| 9125 ||| 9126 ||| 
2021 ||| a sketch-transformer network for face photo-sketch synthesis. ||| 23420 ||| 23421 ||| 17707 ||| 2735 ||| 19196 ||| 6621 ||| 
2020 ||| sbat: video captioning with sparse boundary-aware transformer. ||| 14156 ||| 22669 ||| 8009 ||| 22668 ||| 17724 ||| 
2020 ||| learning regional attention convolutional neural network for motion intention recognition based on eeg data. ||| 23422 ||| 270 ||| 274 ||| 269 ||| 272 ||| 273 ||| 23423 ||| 271 ||| 
2018 ||| commonsense knowledge aware conversation generation with graph attention. ||| 2736 ||| 23424 ||| 9008 ||| 23425 ||| 17952 ||| 9009 ||| 
2019 ||| dmran: a hierarchical fine-grained attention-based network for recommendation. ||| 23426 ||| 23427 ||| 657 ||| 654 ||| 11162 ||| 
2019 ||| on retrospecting human dynamics with attention. ||| 23428 ||| 3156 ||| 
2019 ||| detecting robust co-saliency with recurrent co-attention neural network. ||| 1717 ||| 1718 ||| 6434 ||| 23429 ||| 23430 ||| 
2021 ||| kdexplainer: a task-oriented attention model for explaining knowledge distillation. ||| 23431 ||| 16954 ||| 18726 ||| 3503 ||| 23432 ||| 23433 ||| 
2019 ||| spagan: shortest path graph attention network. ||| 23434 ||| 18726 ||| 23433 ||| 8546 ||| 1756 ||| 
2019 ||| position focused attention network for image-text matching. ||| 23435 ||| 2792 ||| 23436 ||| 19727 ||| 811 ||| 8481 ||| 8482 ||| 
2018 ||| listen, think and listen again: capturing top-down auditory attention for speaker-independent speech separation. ||| 17727 ||| 5350 ||| 17728 ||| 728 ||| 
2019 ||| earlier attention? aspect-aware lstm for aspect-based sentiment analysis. ||| 23437 ||| 23438 ||| 14514 ||| 23439 ||| 1062 ||| 2146 ||| 688 ||| 
2018 ||| translating embeddings for knowledge graph completion with relation attention mechanism. ||| 23440 ||| 23441 ||| 1107 ||| 1115 ||| 2359 ||| 
2019 ||| mina: multilevel knowledge-guided attention for modeling electrocardiography signals. ||| 23442 ||| 1070 ||| 23322 ||| 23443 ||| 23323 ||| 
2020 ||| weakly supervised few-shot object segmentation using co-attention with visual and semantic embeddings. ||| 23444 ||| 23445 ||| 23446 ||| 23447 ||| 23448 ||| 23449 ||| 
2020 ||| a graphical and attentional framework for dual-target cross-domain recommendation. ||| 7758 ||| 247 ||| 23450 ||| 23427 ||| 23451 ||| 
2017 ||| interactive attention networks for aspect-level sentiment classification. ||| 14934 ||| 11660 ||| 14935 ||| 11662 ||| 
2017 ||| mam-rnn: multi-level attention model based rnn for video captioning. ||| 6922 ||| 22078 ||| 8002 ||| 
2020 ||| towards fully 8-bit integer inference for the transformer model. ||| 17717 ||| 17716 ||| 23452 ||| 2333 ||| 23453 ||| 3306 ||| 
2018 ||| attentional image retweet modeling via multi-faceted ranking network learning. ||| 1306 ||| 23454 ||| 7652 ||| 1081 ||| 2258 ||| 1115 ||| 2359 ||| 7654 ||| 
2018 ||| a brand-level ranking system with the customized attention-gru model. ||| 1107 ||| 1112 ||| 23455 ||| 23456 ||| 1113 ||| 1114 ||| 1115 ||| 
2017 ||| hierarchical lstm with adjusted temporal attention for video captioning. ||| 9576 ||| 1039 ||| 3391 ||| 17651 ||| 3282 ||| 1040 ||| 
2021 ||| state-aware value function approximation with attention mechanism for restless multi-armed bandits. ||| 3608 ||| 23457 ||| 23458 ||| 1224 ||| 
2018 ||| medical concept embedding with time-aware attention. ||| 18015 ||| 23459 ||| 23460 ||| 23461 ||| 4646 ||| 15243 ||| 
2019 ||| multi-domain sentiment classification based on domain-aware embedding and attention. ||| 23462 ||| 3121 ||| 
2018 ||| show, observe and tell: attribute-driven attention model for image captioning. ||| 10064 ||| 10065 ||| 10066 ||| 1831 ||| 2278 ||| 
2020 ||| self-attentional credit assignment for transfer in reinforcement learning. ||| 23463 ||| 13128 ||| 23464 ||| 23465 ||| 23466 ||| 
2019 ||| musical: multi-scale image contextual attention learning for inpainting. ||| 6003 ||| 23467 ||| 17758 ||| 17757 ||| 
2019 ||| dyat nets: dynamic attention networks for state forecasting in cyber-physical systems. ||| 23468 ||| 23469 ||| 8985 ||| 
2021 ||| proposal-free one-stage referring expression via grid-word cross-attention. ||| 23470 ||| 23471 ||| 5845 ||| 6417 ||| 
2017 ||| predicting human interaction via relative attention model. ||| 23472 ||| 8832 ||| 8532 ||| 
2020 ||| barnet: bilinear attention network with adaptive receptive fields for surgical instrument segmentation. ||| 5183 ||| 5184 ||| 18290 ||| 5185 ||| 271 ||| 5186 ||| 5189 ||| 23473 ||| 
2019 ||| attributed graph clustering: a deep attentional embedding approach. ||| 23474 ||| 799 ||| 23475 ||| 802 ||| 800 ||| 4873 ||| 
2021 ||| posegtac: graph transformer encoder-decoder with atrous convolution for 3d human pose estimation. ||| 23476 ||| 9579 ||| 1038 ||| 18992 ||| 1039 ||| 1040 ||| 
2020 ||| relation-aware transformer for portfolio policy learning. ||| 3617 ||| 2341 ||| 23477 ||| 9346 ||| 6413 ||| 
2017 ||| cascade dynamics modeling with attention-based recurrent neural network. ||| 23478 ||| 4121 ||| 23479 ||| 23480 ||| 1445 ||| 
2018 ||| feature enhancement in attention for visual question answering. ||| 23481 ||| 23482 ||| 23483 ||| 7654 ||| 
2019 ||| emoji-aware attention-based bi-directional gru network model for chinese sentiment analysis. ||| 23484 ||| 23485 ||| 23486 ||| 23487 ||| 
2019 ||| dense transformer networks for brain electron microscopy image segmentation. ||| 5536 ||| 23488 ||| 23489 ||| 23490 ||| 23491 ||| 
2018 ||| aspect sentiment classification with both word-level and clause-level attention networks. ||| 3083 ||| 4634 ||| 3085 ||| 9612 ||| 1254 ||| 3087 ||| 3088 ||| 
2019 ||| hierarchical inter-attention network for document classification with multi-task learning. ||| 23492 ||| 7826 ||| 3313 ||| 11170 ||| 
2020 ||| neural abstractive summarization with structural attention. ||| 23493 ||| 18578 ||| 3835 ||| 
2019 ||| rthn: a rnn-transformer hierarchical network for emotion cause extraction. ||| 3828 ||| 23494 ||| 21188 ||| 
2019 ||| multi-agent attentional activity recognition. ||| 770 ||| 771 ||| 773 ||| 16806 ||| 775 ||| 
2020 ||| a label attention model for icd coding from clinical text. ||| 23495 ||| 15064 ||| 23496 ||| 
2018 ||| geoman: multi-level attention networks for geo-sensory time series prediction. ||| 23497 ||| 23498 ||| 12757 ||| 23499 ||| 14268 ||| 
2017 ||| attentional factorization machines: learning the weight of feature interactions via attention networks. ||| 7652 ||| 6270 ||| 1063 ||| 2484 ||| 2258 ||| 3605 ||| 
2017 ||| inferring human attention by learning latent intentions. ||| 18980 ||| 23500 ||| 14779 ||| 18982 ||| 
2020 ||| a attention network. ||| 22595 ||| 23501 ||| 7804 ||| 
2019 ||| feature-level deeper self-attention network for sequential recommendation. ||| 5576 ||| 659 ||| 9588 ||| 6475 ||| 658 ||| 18142 ||| 23427 ||| 18526 ||| 
2021 ||| learning attributed graph representation with communicative message passing transformer. ||| 16737 ||| 16736 ||| 23502 ||| 23503 ||| 16595 ||| 
2018 ||| multi-modal sentence summarization with modality attention and image filtering. ||| 1013 ||| 23504 ||| 23505 ||| 3044 ||| 11690 ||| 
2019 ||| vulsniper: focus your attention to shoot fine-grained vulnerabilities. ||| 23506 ||| 23507 ||| 9003 ||| 23508 ||| 23509 ||| 23510 ||| 8753 ||| 
2020 ||| multi-attention meta learning for few-shot fine-grained image recognition. ||| 23511 ||| 23512 ||| 19491 ||| 
2020 ||| a structured latent variable recurrent network with stochastic attention for generating weibo comments. ||| 23513 ||| 13824 ||| 13823 ||| 13223 ||| 13825 ||| 2398 ||| 
2017 ||| link prediction via ranking metric dual-level attention network learning. ||| 1306 ||| 23514 ||| 894 ||| 1115 ||| 2359 ||| 7654 ||| 
2018 ||| from pixels to objects: cubic visual attention for visual question answering. ||| 9576 ||| 17649 ||| 1039 ||| 1040 ||| 
2017 ||| completely heterogeneous transfer learning with attention - what and what not to transfer. ||| 3619 ||| 3781 ||| 
2019 ||| knowledge-enhanced hierarchical attention for community question answering with multi-task and adaptive learning. ||| 1081 ||| 1207 ||| 986 ||| 1827 ||| 5067 ||| 1082 ||| 
2019 ||| multi-level visual-semantic alignments with relation-wise dual attention network for image and text matching. ||| 23515 ||| 23516 ||| 23517 ||| 127 ||| 9375 ||| 
2018 ||| co-attention cnns for unsupervised object co-segmentation. ||| 17709 ||| 2346 ||| 2345 ||| 
2018 ||| same representation, different attentions: shareable sentence representation learning from multiple tasks. ||| 23518 ||| 23519 ||| 3272 ||| 
2021 ||| leveraging human attention in novel object captioning. ||| 17672 ||| 1871 ||| 1872 ||| 
2019 ||| sharing attention weights for fast transformer. ||| 2333 ||| 23520 ||| 3306 ||| 2950 ||| 23453 ||| 
2017 ||| a dual-stage attention-based recurrent neural network for time series prediction. ||| 23521 ||| 9738 ||| 1159 ||| 1156 ||| 23522 ||| 23523 ||| 
2021 ||| dynamic lane traffic signal control with group attention and multi-timescale reinforcement learning. ||| 23524 ||| 23525 ||| 19697 ||| 23526 ||| 
2019 ||| applying attention mechanism and deep neural network for medical object segmentation and classification in x-ray fluoroscopy images. ||| 7826 ||| 18517 ||| 23527 ||| 23528 ||| 
2020 ||| action-guided attention mining and relation reasoning network for human-object interaction detection. ||| 23529 ||| 23530 ||| 23531 ||| 
2019 ||| neighborhood-aware attentional representation for multilingual knowledge graphs. ||| 17662 ||| 17663 ||| 378 ||| 4237 ||| 4191 ||| 
2021 ||| bi-isca: bidirectional inter-sentence contextual attention mechanism for detecting sarcasm in user generated noisy short text. ||| 23532 ||| 15111 ||| 10435 ||| 
2020 ||| collaborative self-attention network for session-based recommendation. ||| 11115 ||| 659 ||| 9588 ||| 3476 ||| 18142 ||| 658 ||| 11122 ||| 6475 ||| 
2020 ||| an attention-based model for conversion rate prediction with delayed feedback via post-click calibration. ||| 23533 ||| 1166 ||| 1167 ||| 1099 ||| 13759 ||| 2182 ||| 1171 ||| 9406 ||| 1169 ||| 1170 ||| 
2018 ||| densely connected cnn with multi-scale feature attention for text classification. ||| 23534 ||| 9008 ||| 1000 ||| 
2019 ||| mnn: multimodal attentional neural networks for diagnosis prediction. ||| 20339 ||| 3748 ||| 3749 ||| 7417 ||| 
2021 ||| multimodal transformer networks for pedestrian trajectory prediction. ||| 20211 ||| 7343 ||| 7345 ||| 6351 ||| 
2019 ||| bpam: recommendation based on bp neural network with attention mechanism. ||| 23535 ||| 335 ||| 334 ||| 23536 ||| 6528 ||| 
2018 ||| get the point of my utterance! learning towards effective responses with multi-head attention mechanism. ||| 23537 ||| 23538 ||| 23539 ||| 293 ||| 23540 ||| 10233 ||| 
2020 ||| internal and contextual attention network for cold-start multi-channel matching in recommendation. ||| 3475 ||| 23541 ||| 23542 ||| 1199 ||| 1099 ||| 9569 ||| 
2021 ||| gaen: graph attention evolving networks. ||| 160 ||| 17209 ||| 23543 ||| 161 ||| 23544 ||| 162 ||| 
2017 ||| learning to read irregular text with attention mechanisms. ||| 2985 ||| 6337 ||| 23545 ||| 6342 ||| 6343 ||| 
2019 ||| predicting the visual focus of attention in multi-person discussion videos. ||| 21480 ||| 23546 ||| 23363 ||| 23547 ||| 23548 ||| 21484 ||| 
2020 ||| deep interleaved network for single image super-resolution with asymmetric co-attention. ||| 4398 ||| 23549 ||| 4399 ||| 23550 ||| 
2020 ||| deep specification mining with attention. ||| 23551 ||| 700 ||| 
2017 ||| testing the electrical insulation system of power transformer based on mesuring factor of dielectric losses. ||| 23552 ||| 23553 ||| 23554 ||| 
2019 ||| application of the protective current transformers in measurement systems for energy management purposes. ||| 23555 ||| 23556 ||| 23557 ||| 23558 ||| 
2019 ||| external attention lstm models for cognitive load classification from speech. ||| 14389 ||| 14390 ||| 3882 ||| 14391 ||| 
2018 ||| restoring punctuation and capitalization using transformer models. ||| 23559 ||| 23560 ||| 
2017 ||| attentional parallel rnns for generating punctuation in transcribed speech. ||| 23561 ||| 23562 ||| 23563 ||| 7111 ||| 3524 ||| 
2021 ||| comparison of czech transformers on text classification tasks. ||| 11935 ||| 11934 ||| 
2019 ||| use of intelligent agent through low-cost brain-computer interface to analyze attention and meditation levels by gender. ||| 23564 ||| 23565 ||| 23566 ||| 23567 ||| 23568 ||| 23569 ||| 2253 ||| 23570 ||| 
2020 ||| the wonderful wizard of loc: paying attention to the man behind the curtain of lines-of-code metrics. ||| 23571 ||| 23572 ||| 23573 ||| 
2021 ||| recurrent attention unit: a simple and effective method for traffic prediction. ||| 23574 ||| 23575 ||| 23576 ||| 23577 ||| 23578 ||| 23579 ||| 11354 ||| 
2021 ||| ra-gat: repulsion and attraction graph attention for trajectory prediction. ||| 23580 ||| 23581 ||| 23582 ||| 
2021 ||| tqam: temporal attention for cycle-wise queue length estimation using high-resolution loop detector data. ||| 23583 ||| 23584 ||| 23585 ||| 23586 ||| 
2019 ||| vehicle speed prediction with rnn and attention model under multiple scenarios. ||| 23587 ||| 23588 ||| 23589 ||| 23590 ||| 
2020 ||| graph attention convolutional network: spatiotemporal modeling for urban traffic prediction. ||| 23591 ||| 23592 ||| 15447 ||| 23593 ||| 23594 ||| 
2020 ||| detecting lane and road markings at a distance with perspective transformer layers. ||| 23595 ||| 23596 ||| 23597 ||| 23598 ||| 23599 ||| 
2019 ||| predicting time-series trajectories of human driven vehicles, using an unsupervised attention-based recurrent neural network. ||| 23600 ||| 
2021 ||| learning to drive at unsignalized intersections using attention-based deep reinforcement learning. ||| 23601 ||| 23602 ||| 23603 ||| 23604 ||| 
2019 ||| a benchmark dataset and multi-scale attention network for semantic traffic light detection. ||| 3076 ||| 23605 ||| 18980 ||| 22662 ||| 14779 ||| 
2021 ||| effects of cooperative vehicle infrastructure system on driver's attention with different personal attribute. ||| 14761 ||| 23606 ||| 23607 ||| 23608 ||| 
2021 ||| dsa-gan: driving style attention generative adversarial network for vehicle trajectory prediction. ||| 23609 ||| 23610 ||| 23611 ||| 3028 ||| 23612 ||| 8042 ||| 15458 ||| 
2021 ||| densepass: dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange. ||| 23613 ||| 7856 ||| 7857 ||| 23614 ||| 7861 ||| 
2021 ||| fusionpainting: multimodal fusion with adaptive attention for 3d object detection. ||| 23615 ||| 18707 ||| 23616 ||| 18705 ||| 23617 ||| 23618 ||| 
2019 ||| neural-attention-based deep learning architectures for modeling traffic dynamics on lane graphs. ||| 23619 ||| 23620 ||| 23621 ||| 
2021 ||| attention-based vehicle self-localization with hd feature maps. ||| 23622 ||| 23623 ||| 23624 ||| 
2019 ||| an efficient model for driving focus of attention prediction using deep learning. ||| 23625 ||| 15629 ||| 23626 ||| 
2021 ||| rpfa-net: a 4d radar pillar feature attention network for 3d object detection. ||| 23627 ||| 3613 ||| 1052 ||| 23628 ||| 23629 ||| 23630 ||| 5536 ||| 23631 ||| 
2021 ||| spatio-temporal multi-task learning transformer for joint moving object detection and segmentation. ||| 23632 ||| 23633 ||| 
2020 ||| top-down, spatio-temporal attentional guidance for on-road object detection. ||| 23634 ||| 23635 ||| 23636 ||| 
2021 ||| image transformer for explainable autonomous driving system. ||| 23637 ||| 23638 ||| 23639 ||| 23640 ||| 23641 ||| 
2019 ||| driver attention level estimation using driver model identification. ||| 23642 ||| 23643 ||| 
2019 ||| detection of driver's inattention: a real-time deep learning approach. ||| 23644 ||| 23645 ||| 
2019 ||| safety criteria analysis for negotiating blind corners in personal mobility vehicles based on driver's attention simulation on 3d map. ||| 23646 ||| 23647 ||| 23648 ||| 23649 ||| 
2020 ||| attention based graph bi-lstm networks for traffic forecasting. ||| 23650 ||| 18950 ||| 3906 ||| 1798 ||| 23651 ||| 
2021 ||| an attention-based spatial-temporal traffic flow prediction method with pattern similarity analysis. ||| 23652 ||| 23653 ||| 23654 ||| 
2021 ||| dau-net: dense attention u-net for pavement crack segmentation. ||| 12124 ||| 12126 ||| 
2019 ||| attention-based gated recurrent unit for links traffic speed forecasting. ||| 23655 ||| 23656 ||| 23657 ||| 23658 ||| 23659 ||| 
2021 ||| centralized traffic signal control for multiple intersections based on sequence-to-sequence model and attention mechanism. ||| 23660 ||| 23661 ||| 378 ||| 
2021 ||| attention-based neural network for driving environment complexity perception. ||| 13362 ||| 23662 ||| 23663 ||| 
2019 ||| attention neural baby talk: captioning of risk factors while driving. ||| 23664 ||| 15475 ||| 723 ||| 23665 ||| 724 ||| 725 ||| 
2019 ||| towards vr attention guidance: environment-dependent perceptual threshold for stereo inverse brightness modulation. ||| 23666 ||| 23667 ||| 23668 ||| 23669 ||| 23670 ||| 
2019 ||| empirical evaluation of the interplay of emotion and visual attention in human-virtual human interaction. ||| 15939 ||| 23671 ||| 23672 ||| 15940 ||| 15941 ||| 
2019 ||| assessment of driver attention during a safety critical situation in vr to generate vr-based training. ||| 21049 ||| 18312 ||| 7218 ||| 
2019 ||| real time attention based bidirectional long short-term memory networks for air pollution forecasting. ||| 23673 ||| 23674 ||| 17364 ||| 12796 ||| 
2019 ||| person search based on attention mechanism. ||| 23675 ||| 23676 ||| 19288 ||| 
2019 ||| glosysic framework: transformer for image captioning with sequential attention. ||| 23677 ||| 23678 ||| 23679 ||| 
2022 ||| graves-cpa: a graph-attention verifier selector (competition contribution). ||| 23680 ||| 23681 ||| 
2018 ||| recurrent convolutional neural network with attention for twitter and yelp sentiment classification: arc model for sentiment classification. ||| 23682 ||| 595 ||| 
2021 ||| mulrnn: an enhanced technique of multi-stage-attention network for complex time series prediction. ||| 23683 ||| 4807 ||| 23684 ||| 23685 ||| 
2021 ||| region-level attention network for food and ingredient joint recognition. ||| 23686 ||| 6965 ||| 23687 ||| 
2018 ||| attresnet: attention-based resnet for image captioning. ||| 23688 ||| 588 ||| 586 ||| 23689 ||| 23690 ||| 589 ||| 
2021 ||| multi-level convolutional transformer with adaptive ranking for semi-supervised crowd counting. ||| 19976 ||| 579 ||| 23691 ||| 23692 ||| 
2019 ||| single-shot object detector based on attention mechanism. ||| 12319 ||| 23370 ||| 
2020 ||| leveraging different context for response generation through topic-guided multi-head attention. ||| 23693 ||| 23694 ||| 23695 ||| 
2021 ||| handwritten mathematical expression recognition with self-attention. ||| 23696 ||| 9784 ||| 23697 ||| 23698 ||| 
2021 ||| residual networks with channel attention for single image super-resolution. ||| 13572 ||| 19190 ||| 1341 ||| 
2020 ||| a novel interactive recurrent attention network for emotion-cause pair extraction. ||| 23699 ||| 23700 ||| 23701 ||| 3114 ||| 
2021 ||| attention-based multi-level network for text matching with feature fusion. ||| 23702 ||| 11386 ||| 
2020 ||| probabilistic graph attention for relation extraction for domain of geography. ||| 23703 ||| 23704 ||| 23705 ||| 12692 ||| 
2021 ||| research and application of recommendation algorithm based on bidirectional attention model. ||| 23706 ||| 23707 ||| 23708 ||| 
2021 ||| deep joint convolutional neural network with double-level attention mechanism for multi-sensor bearing performance degradation assessment. ||| 23709 ||| 23710 ||| 23711 ||| 23712 ||| 23713 ||| 
2019 ||| attention in software maintenance: an eye tracking study. ||| 23714 ||| 23715 ||| 23716 ||| 
2019 ||| toward imitating visual attention of experts in software development tasks. ||| 4424 ||| 23717 ||| 4425 ||| 4421 ||| 4426 ||| 
2019 ||| integration of laser scanning and photogrammetry in architecture survey. open issue in geomatics and attention to details. ||| 23718 ||| 23719 ||| 
2021 ||| transformer-based approaches for personality detection using the mbti model. ||| 23720 ||| 23167 ||| 852 ||| 9983 ||| 
2021 ||| visual attention prediction model based on prominence maps, machine learning and biometric data. ||| 23721 ||| 23722 ||| 23723 ||| 504 ||| 23724 ||| 
2018 ||| inference of the definition of the predicate transformer wp with occurrences of the predicate domain based on denotational semantics of gcl on zf set theory. ||| 23725 ||| 
2019 ||| tagging malware intentions by using attention-based sequence-to-sequence neural network. ||| 23726 ||| 23727 ||| 23728 ||| 23729 ||| 23730 ||| 23731 ||| 
2020 ||| sequence-to-set semantic tagging for complex query reformulation and automated text categorization in biomedical ir using self-attention. ||| 23732 ||| 23733 ||| 12409 ||| 13382 ||| 23734 ||| 23735 ||| 23736 ||| 
2019 ||| nlnde: enhancing neural sequence taggers with attention and noisy channel for robust pharmacological entity detection. ||| 16488 ||| 16489 ||| 15085 ||| 15086 ||| 
2019 ||| ncuee at mediqa 2019: medical text inference using ensemble bert-bilstm-attention model. ||| 16268 ||| 9891 ||| 16269 ||| 23737 ||| 23738 ||| 
2021 ||| wbi at mediqa 2021: summarizing consumer health questions with generative transformers. ||| 23739 ||| 23740 ||| 23741 ||| 23742 ||| 
2019 ||| dut-bim at mediqa 2019: utilizing transformer network and medical domain-specific contextualized representations for question answering. ||| 13576 ||| 23743 ||| 6474 ||| 1415 ||| 
2019 ||| saama research at mediqa 2019: pre-trained biobert with attention visualisation for medical natural language inference. ||| 23744 ||| 23745 ||| 23746 ||| 23747 ||| 18375 ||| 
2017 ||| extracting drug-drug interactions with attention cnns. ||| 23748 ||| 23749 ||| 23750 ||| 
2019 ||| bionlp-ost 2019 rdoc tasks: multi-grain neural relevance ranking using topics and attention based query-document-sentence interactions. ||| 23751 ||| 23752 ||| 11716 ||| 11717 ||| 
2021 ||| ncuee-nlp at mediqa 2021: health question summarization using pegasus transformers. ||| 16268 ||| 16269 ||| 16270 ||| 23737 ||| 23738 ||| 
2021 ||| biom-transformers: building large biomedical language models with bert, albert and electra. ||| 23753 ||| 23754 ||| 
2019 ||| lasigebiotm at mediqa 2019: biomedical question answering using bidirectional transformers and named entity recognition. ||| 23755 ||| 23756 ||| 
2019 ||| constructive type-logical supertagging with self-attention networks. ||| 23757 ||| 23758 ||| 23759 ||| 
2020 ||| staying true to your word: (how) can attention become explanation? ||| 23760 ||| 23761 ||| 
2017 ||| sequential attention: a context-aware alignment function for machine reading. ||| 23762 ||| 23763 ||| 3720 ||| 
2019 ||| multilingual nmt with a language-independent attention bridge. ||| 23764 ||| 23765 ||| 2698 ||| 23766 ||| 4194 ||| 23767 ||| 23768 ||| 
2018 ||| hierarchical convolutional attention networks for text classification. ||| 23769 ||| 23770 ||| 23771 ||| 
2019 ||| an evaluation of language-agnostic inner-attention-based representations in machine translation. ||| 23766 ||| 23764 ||| 23765 ||| 2698 ||| 23768 ||| 4194 ||| 23767 ||| 
2020 ||| enhancing transformer with sememe knowledge. ||| 23772 ||| 23773 ||| 23774 ||| 3232 ||| 
2021 ||| an emotion analysis model based on fine-grained emoji attention mechanism for multi-modal. ||| 23775 ||| 23776 ||| 18556 ||| 23777 ||| 
2019 ||| attention based speech model for japanese recognization. ||| 23778 ||| 23779 ||| 23780 ||| 3694 ||| 23781 ||| 23782 ||| 
2021 ||| research on vehicle detection model based on attention mechanism. ||| 19948 ||| 23783 ||| 23784 ||| 23785 ||| 
2018 ||| ship targets detection based on visual attention. ||| 23786 ||| 23787 ||| 
2020 ||| multi-head attention networks for nonintrusive load monitoring. ||| 23788 ||| 23789 ||| 23790 ||| 23791 ||| 
2021 ||| the video captioning method based on the spatial- temporal information and attention mechanism. ||| 21752 ||| 5463 ||| 21750 ||| 23792 ||| 23793 ||| 6553 ||| 
2018 ||| enhancing human cross-linguistic comprehension via cognitive computation and selective attention. ||| 23794 ||| 23795 ||| 
2021 ||| airport small target algorithm based on convolution kernel attention mechanism. ||| 23796 ||| 2736 ||| 
2021 ||| dynamic gesture recognition based on cnn-lstm-attention. ||| 23797 ||| 23798 ||| 23799 ||| 1125 ||| 
2021 ||| a multiscale dual-attention based convolutional neural network for ship classification in sar image. ||| 23800 ||| 23801 ||| 4236 ||| 23802 ||| 
2017 ||| scene emotion detection using closed caption based on hierarchical attention network. ||| 23803 ||| 23804 ||| 23805 ||| 23806 ||| 
2019 ||| depth attention net. ||| 23807 ||| 23808 ||| 23809 ||| 
2020 ||| robust keypoint normalization method for korean sign language translation using transformer. ||| 23810 ||| 23811 ||| 17253 ||| 23812 ||| 23813 ||| 23814 ||| 
2021 ||| generating face images using vqgan and sparse transformer. ||| 23815 ||| 23816 ||| 
2021 ||| arrhythmia detection using convolutional neural networks with temporal attention mechanism. ||| 23817 ||| 23818 ||| 23819 ||| 23820 ||| 
2021 ||| korean traditional document translation using transformer in bidirectional-crf. ||| 23821 ||| 23822 ||| 23823 ||| 5053 ||| 488 ||| 
2018 ||| frequency-aware attention based lstm networks for cardiovascular disease. ||| 23824 ||| 23825 ||| 23826 ||| 
2021 ||| transformer based prediction method for solar power generation data. ||| 23827 ||| 23828 ||| 23829 ||| 23830 ||| 
2018 ||| estimating attentional state of a driver: interacting effects of task demands and cognitive capacities. ||| 23831 ||| 23832 ||| 23833 ||| 
2021 ||| human pose estimation based on attention multi-resolution network. ||| 23834 ||| 23835 ||| 23836 ||| 23837 ||| 13200 ||| 
2019 ||| cross-modal video moment retrieval with spatial and language-temporal attention. ||| 170 ||| 17578 ||| 497 ||| 8546 ||| 
2019 ||| improving what cross-modal retrieval models learn through object-oriented inter- and intra-modal attention networks. ||| 23838 ||| 23839 ||| 1781 ||| 18704 ||| 
2020 ||| dagc: employing dual attention and graph convolution for point cloud based place recognition. ||| 433 ||| 23840 ||| 532 ||| 23841 ||| 23842 ||| 
2020 ||| multi-attention multimodal sentiment analysis. ||| 23843 ||| 23844 ||| 
2020 ||| deep discrete attention guided hashing for face image retrieval. ||| 21478 ||| 21476 ||| 22591 ||| 23845 ||| 1717 ||| 4201 ||| 
2021 ||| reading scene text by fusing visual attention with semantic representations. ||| 23846 ||| 23847 ||| 23848 ||| 
2021 ||| gpt2mvs: generative pre-trained transformer-2 for multi-modal video summarization. ||| 7455 ||| 23849 ||| 11484 ||| 7461 ||| 
2017 ||| frame-transformer emotion classification network. ||| 23850 ||| 6365 ||| 17842 ||| 4970 ||| 
2021 ||| unsupervised training data generation of handwritten formulas using generative adversarial networks with self-attention. ||| 23851 ||| 23852 ||| 23853 ||| 10689 ||| 
2021 ||| relation-aware hierarchical attention framework for video question answering. ||| 6614 ||| 23854 ||| 23855 ||| 6615 ||| 23856 ||| 411 ||| 
2021 ||| be specific, be clear: bridging machine and human captions by scene-guided transformer. ||| 19678 ||| 8770 ||| 16594 ||| 
2021 ||| teach: attention-aware deep cross-modal hashing. ||| 23857 ||| 23858 ||| 23859 ||| 9999 ||| 9675 ||| 
2021 ||| text-enhanced attribute-based attention for generalized zero-shot fine-grained image classification. ||| 23860 ||| 23861 ||| 
2020 ||| attention mechanisms, signal encodings and fusion strategies for improved ad-hoc video search with dual encoding networks. ||| 23862 ||| 5870 ||| 
2020 ||| bidal-hcmus@lsc2020: an interactive multimodal lifelog retrieval with query-to-sample attention-based search engine. ||| 23863 ||| 23864 ||| 23865 ||| 23866 ||| 23867 ||| 17207 ||| 
2021 ||| cross-modal self-attention with multi-task pre-training for medical visual question answering. ||| 23868 ||| 2113 ||| 23869 ||| 1801 ||| 1800 ||| 
2021 ||| scene text recognition with cascade attention network. ||| 1254 ||| 2882 ||| 2885 ||| 
2019 ||| relationship detection based on object semantic inference and attention mechanisms. ||| 1166 ||| 3364 ||| 9353 ||| 9351 ||| 9354 ||| 5328 ||| 
2018 ||| class-aware self-attention for audio event recognition. ||| 19642 ||| 11126 ||| 19643 ||| 18704 ||| 
2021 ||| naster: non-local attentional scene text recognizer. ||| 23870 ||| 13834 ||| 19497 ||| 23871 ||| 5946 ||| 
2019 ||| weakly supervised image retrieval via coarse-scale feature fusion and multi-level attention blocks. ||| 23872 ||| 8843 ||| 5166 ||| 8842 ||| 23873 ||| 
2018 ||| multimodal network embedding via attention based multi-view variational autoencoder. ||| 19722 ||| 5320 ||| 9991 ||| 843 ||| 19723 ||| 19724 ||| 
2019 ||| stacked self-attention networks for visual question answering. ||| 4103 ||| 6365 ||| 
2021 ||| nested dense attention network for single image super-resolution. ||| 23874 ||| 23875 ||| 23876 ||| 
2019 ||| a geographical-temporal awareness hierarchical attention network for next point-of-interest recommendation. ||| 23877 ||| 6191 ||| 23878 ||| 5039 ||| 6187 ||| 
2021 ||| multi-attention audio-visual fusion network for audio spatialization. ||| 13735 ||| 155 ||| 
2021 ||| global relation-aware attention network for image-text retrieval. ||| 4470 ||| 1174 ||| 23879 ||| 1173 ||| 1175 ||| 
2020 ||| a coordinated representation learning enhanced multimodal machine translation approach with multi-attention. ||| 23880 ||| 1556 ||| 18083 ||| 
2021 ||| look back again: dual parallel attention network for accurate and robust scene text recognition. ||| 21710 ||| 18071 ||| 23881 ||| 1319 ||| 
2021 ||| fire detection using transformer network. ||| 23882 ||| 11299 ||| 
2021 ||| multi-feature graph attention network for cross-modal video-text retrieval. ||| 23883 ||| 23884 ||| 21476 ||| 21477 ||| 1717 ||| 4201 ||| 
2019 ||| hierarchical attention based neural network for explainable recommendation. ||| 23885 ||| 6314 ||| 3310 ||| 23886 ||| 23887 ||| 23888 ||| 23889 ||| 
2021 ||| automatic music composition with transformers. ||| 4374 ||| 
2021 ||| rgb-d scene recognition based on object-scene relation and semantics-preserving attention. ||| 23890 ||| 23891 ||| 
2020 ||| learning deep graph matching with channel-independent embedding and hungarian attention. ||| 23892 ||| 23893 ||| 2307 ||| 23894 ||| 
2020 ||| on the relationship between self-attention and convolutional layers. ||| 22850 ||| 22851 ||| 23895 ||| 
2021 ||| attentional constellation nets for few-shot learning. ||| 1812 ||| 1813 ||| 23896 ||| 1815 ||| 
2021 ||| group equivariant stand-alone self-attention for vision. ||| 23897 ||| 22850 ||| 
2020 ||| reducing transformer depth on demand with structured dropout. ||| 23898 ||| 3798 ||| 1889 ||| 
2017 ||| frustratingly short attention spans in neural language modeling. ||| 23899 ||| 23900 ||| 23901 ||| 23902 ||| 23903 ||| 
2021 ||| vtnet: visual transformer network for object goal navigation. ||| 23904 ||| 23905 ||| 8571 ||| 
2020 ||| compressive transformers for long-range sequence modelling. ||| 3709 ||| 23906 ||| 22772 ||| 23907 ||| 23908 ||| 
2021 ||| an image is worth 16x16 words: transformers for image recognition at scale. ||| 9291 ||| 23909 ||| 7979 ||| 9289 ||| 23910 ||| 2571 ||| 2293 ||| 23911 ||| 2294 ||| 23912 ||| 4960 ||| 23913 ||| 
2021 ||| hypergrid transformers: towards a single model for multiple tasks. ||| 1398 ||| 22746 ||| 3292 ||| 3294 ||| 22745 ||| 
2019 ||| marginalized average attentional network for weakly-supervised learning. ||| 6650 ||| 23914 ||| 23915 ||| 23916 ||| 23917 ||| 
2017 ||| deep biaffine attention for neural dependency parsing. ||| 23918 ||| 20967 ||| 
2020 ||| tree-structured attention with hierarchical accumulation. ||| 3487 ||| 1313 ||| 3303 ||| 19267 ||| 
2021 ||| transformer protein language models are unsupervised structure learners. ||| 22706 ||| 22709 ||| 22710 ||| 23919 ||| 22711 ||| 
2020 ||| u-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation. ||| 2473 ||| 23920 ||| 23921 ||| 23922 ||| 
2019 ||| hyperbolic attention networks. ||| 22770 ||| 2713 ||| 22771 ||| 23923 ||| 8785 ||| 3710 ||| 22769 ||| 23924 ||| 8788 ||| 23925 ||| 23926 ||| 8787 ||| 23927 ||| 
2018 ||| dcn+: mixed objective and deep residual coattention for question answering. ||| 3287 ||| 23928 ||| 19267 ||| 
2017 ||| dynamic coattention networks for question answering. ||| 3287 ||| 23928 ||| 19267 ||| 
2019 ||| coarse-grain fine-grain coattention network for multi-evidence question answering. ||| 23928 ||| 3287 ||| 23929 ||| 19267 ||| 
2021 ||| iot: instance-wise layer reordering for transformer structures. ||| 23930 ||| 4785 ||| 4787 ||| 4788 ||| 4789 ||| 1806 ||| 1807 ||| 4791 ||| 
2020 ||| transformer-xh: multi-evidence reasoning with extra hop attention. ||| 18731 ||| 3322 ||| 9690 ||| 23931 ||| 23932 ||| 23933 ||| 
2021 ||| hopper: multi-hop transformer for spatiotemporal reasoning. ||| 23934 ||| 23935 ||| 23936 ||| 23937 ||| 23938 ||| 4933 ||| 23939 ||| 
2018 ||| qanet: combining local convolution with global self-attention for reading comprehension. ||| 23940 ||| 23941 ||| 22748 ||| 8164 ||| 472 ||| 23942 ||| 9372 ||| 
2020 ||| reformer: the efficient transformer. ||| 3143 ||| 9135 ||| 9434 ||| 
2018 ||| designing efficient neural attention systems towards achieving human-level sharp vision. ||| 23943 ||| 23717 ||| 23944 ||| 23945 ||| 23946 ||| 20764 ||| 
2018 ||| compositional attention networks for machine reasoning. ||| 22836 ||| 20967 ||| 
2021 ||| pre-training text-to-text transformers for concept-centric common sense. ||| 23947 ||| 23948 ||| 23949 ||| 23950 ||| 1250 ||| 
2020 ||| robustness verification for transformers. ||| 23951 ||| 16915 ||| 3033 ||| 9008 ||| 21252 ||| 
2018 ||| graph attention networks. ||| 23952 ||| 8670 ||| 23953 ||| 23954 ||| 23955 ||| 9196 ||| 
2017 ||| recurrent mixture density network for spatiotemporal visual attention. ||| 18930 ||| 9359 ||| 7366 ||| 
2017 ||| structured attention networks. ||| 9410 ||| 23956 ||| 23957 ||| 4962 ||| 
2018 ||| learn to pay attention. ||| 23958 ||| 23959 ||| 23960 ||| 2160 ||| 
2018 ||| polar transformer networks. ||| 23961 ||| 23962 ||| 19082 ||| 23963 ||| 
2020 ||| monotonic multihead attention. ||| 12088 ||| 11637 ||| 22828 ||| 23964 ||| 9318 ||| 
2019 ||| posterior attention models for sequence to sequence learning. ||| 23965 ||| 3859 ||| 
2021 ||| a trainable optimal transport embedding for feature aggregation and its relationship to attention. ||| 17321 ||| 23966 ||| 23967 ||| 23968 ||| 2264 ||| 
2018 ||| fusionnet: fusing via fully-aware attention with application to machine comprehension. ||| 23969 ||| 6064 ||| 3172 ||| 3175 ||| 
2021 ||| random feature attention. ||| 9407 ||| 14940 ||| 23970 ||| 23971 ||| 3277 ||| 3139 ||| 
2019 ||| music transformer: generating music with long-term structure. ||| 23972 ||| 2466 ||| 4960 ||| 11911 ||| 11910 ||| 9132 ||| 18106 ||| 23973 ||| 22839 ||| 22750 ||| 
2020 ||| capsules with inverted dot-product attention routing. ||| 3597 ||| 23974 ||| 23975 ||| 3247 ||| 
2020 ||| logic and the 2-simplicial transformer. ||| 23976 ||| 23977 ||| 23978 ||| 23979 ||| 
2020 ||| pay attention to features, transfer learn faster cnns. ||| 23980 ||| 23981 ||| 23982 ||| 23983 ||| 1406 ||| 6285 ||| 
2019 ||| delta: deep learning transfer using feature map with attention for convolutional networks. ||| 23983 ||| 23984 ||| 23985 ||| 23986 ||| 23987 ||| 23988 ||| 
2021 ||| how to find your friendly neighborhood: graph attention design with self-supervision. ||| 23989 ||| 23990 ||| 
2021 ||| a universal representation transformer layer for few-shot image classification. ||| 6174 ||| 9237 ||| 802 ||| 800 ||| 9359 ||| 
2019 ||| attention, learn to solve routing problems! ||| 23991 ||| 23992 ||| 9255 ||| 
2020 ||| adaptive structural fingerprints for graph attention networks. ||| 3433 ||| 23993 ||| 1224 ||| 1134 ||| 
2021 ||| lambdanetworks: modeling long-range interactions without attention. ||| 2463 ||| 
2021 ||| deformable detr: deformable transformers for end-to-end object detection. ||| 2638 ||| 23994 ||| 1844 ||| 6502 ||| 1846 ||| 1847 ||| 
2021 ||| on the dynamics of training attention models. ||| 23995 ||| 18115 ||| 23996 ||| 
2017 ||| paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. ||| 8685 ||| 23997 ||| 
2020 ||| on identifiability in transformers. ||| 23998 ||| 1305 ||| 23999 ||| 24000 ||| 24001 ||| 24002 ||| 
2019 ||| universal transformers. ||| 2293 ||| 24003 ||| 22760 ||| 4960 ||| 9135 ||| 
2021 ||| bertology meets biology: interpreting attention in protein language models. ||| 3447 ||| 24004 ||| 15983 ||| 3287 ||| 19267 ||| 24005 ||| 
2020 ||| space: unsupervised object-oriented scene representation via spatial attention and decomposition. ||| 24006 ||| 22862 ||| 24007 ||| 24008 ||| 24009 ||| 24010 ||| 24011 ||| 22864 ||| 
2021 ||| multi-time attention networks for irregularly sampled time series. ||| 24012 ||| 6023 ||| 
2021 ||| is attention better than matrix decomposition? ||| 24013 ||| 24014 ||| 18525 ||| 2514 ||| 24015 ||| 2518 ||| 
2021 ||| delight: deep and light-weight transformer. ||| 24016 ||| 22829 ||| 4895 ||| 24017 ||| 4765 ||| 
2018 ||| monotonic chunkwise attention. ||| 3334 ||| 3338 ||| 
2020 ||| are transformers universal approximators of sequence-to-sequence functions? ||| 9155 ||| 2567 ||| 9157 ||| 9158 ||| 9159 ||| 
2021 ||| cross-attentional audio-visual fusion for weakly-supervised action localization. ||| 7164 ||| 7166 ||| 24018 ||| 7165 ||| 
2021 ||| updet: universal multi-agent rl via policy decoupling with transformers. ||| 24019 ||| 24020 ||| 1781 ||| 1686 ||| 
2021 ||| parameter efficient multimodal transformers for video representation learning. ||| 24021 ||| 18984 ||| 18989 ||| 24022 ||| 24023 ||| 20350 ||| 
2020 ||| lite transformer with long-short range attention. ||| 3162 ||| 3163 ||| 24024 ||| 24025 ||| 3166 ||| 
2021 ||| colorization transformer. ||| 24026 ||| 9289 ||| 24027 ||| 
2021 ||| neural attention distillation: erasing backdoor triggers from deep neural networks. ||| 24028 ||| 24029 ||| 24030 ||| 24031 ||| 1717 ||| 24032 ||| 
2021 ||| rethinking attention with performers. ||| 24033 ||| 24034 ||| 23941 ||| 24035 ||| 24036 ||| 11942 ||| 24037 ||| 7111 ||| 24038 ||| 22789 ||| 24039 ||| 9135 ||| 24040 ||| 24041 ||| 24042 ||| 
2019 ||| residual non-local attention networks for image restoration. ||| 1730 ||| 2232 ||| 2233 ||| 8562 ||| 1734 ||| 
2019 ||| pay less attention with lightweight and dynamic convolutions. ||| 14702 ||| 23898 ||| 3823 ||| 24043 ||| 3825 ||| 
2021 ||| long range arena : a benchmark for efficient transformers. ||| 1398 ||| 2293 ||| 3189 ||| 24044 ||| 3292 ||| 9234 ||| 4840 ||| 1041 ||| 3671 ||| 3294 ||| 
2018 ||| bi-directional block self-attention for fast and memory-efficient sequence modeling. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 4873 ||| 
2021 ||| efficient transformers in reinforcement learning using actor-learner distillation. ||| 18744 ||| 24045 ||| 
2020 ||| hyper-sagnn: a self-attention based graph neural network for hypergraphs. ||| 24046 ||| 24047 ||| 24048 ||| 
2021 ||| deberta: decoding-enhanced bert with disentangled attention. ||| 24049 ||| 24050 ||| 1958 ||| 3175 ||| 
2020 ||| depth-adaptive transformer. ||| 23095 ||| 9318 ||| 3798 ||| 3825 ||| 
2017 ||| bidirectional attention flow for machine comprehension. ||| 24051 ||| 24052 ||| 24053 ||| 4765 ||| 
2020 ||| deep and shallow feature fusion and recognition of recording devices based on attention mechanism. ||| 24054 ||| 24055 ||| 24056 ||| 24057 ||| 
2020 ||| research on book recommendation system for people with visual impairment based on fusion of preference and user attention. ||| 19750 ||| 19748 ||| 24058 ||| 1160 ||| 24059 ||| 24060 ||| 
2019 ||| spatial attention for pedestrian detection. ||| 24061 ||| 24062 ||| 24063 ||| 1226 ||| 7320 ||| 7321 ||| 
2019 ||| video-based person re-identification using refined attention networks. ||| 18747 ||| 18831 ||| 602 ||| 
2021 ||| from multimodal to unimodal attention in transformers using knowledge distillation. ||| 16386 ||| 16385 ||| 24064 ||| 1226 ||| 7320 ||| 7321 ||| 
2021 ||| dam: dissimilarity attention module for weakly-supervised video anomaly detection. ||| 24065 ||| 5680 ||| 1226 ||| 7320 ||| 7321 ||| 
2019 ||| self-attention temporal convolutional network for long-term daily living activity detection. ||| 7316 ||| 7317 ||| 7318 ||| 7319 ||| 1226 ||| 7320 ||| 7321 ||| 
2019 ||| inverse attention guided deep crowd counting network. ||| 19135 ||| 18609 ||| 
2019 ||| multi-component spatiotemporal attention and its application to object detection in surveillance videos. ||| 24066 ||| 20864 ||| 24067 ||| 20865 ||| 
2020 ||| emphasis: an embedded public attention stress identification system. ||| 24068 ||| 24069 ||| 24070 ||| 24071 ||| 24072 ||| 
2021 ||| super: sub-graph parallelism for transformers. ||| 24073 ||| 24074 ||| 24075 ||| 24076 ||| 24077 ||| 24078 ||| 24079 ||| 24080 ||| 
2021 ||| encoder-attention-based automatic term recognition (ea-atr). ||| 24081 ||| 10636 ||| 
2020 ||| applying multilingual and monolingual transformer-based models for dialect identification. ||| 24082 ||| 24083 ||| 
2021 ||| hierarchical transformer for multilingual machine translation. ||| 24084 ||| 24085 ||| 24086 ||| 24087 ||| 24088 ||| 24089 ||| 
2021 ||| sapn: spatial attention pyramid network for cross-domain person re-identification. ||| 24090 ||| 24091 ||| 24092 ||| 11494 ||| 24093 ||| 
2021 ||| mfagcn: multi-feature based attention graph convolutional network for traffic prediction. ||| 1013 ||| 8935 ||| 24094 ||| 24095 ||| 
2021 ||| aopl: attention enhanced oversampling and parallel deep learning model for attack detection in imbalanced network traffic. ||| 24096 ||| 861 ||| 10956 ||| 247 ||| 24097 ||| 
2019 ||| self-attention based collaborative neural network for recommendation. ||| 24098 ||| 406 ||| 
2021 ||| sequential recommendation via temporal self-attention and multi-preference learning. ||| 24091 ||| 406 ||| 11823 ||| 
2021 ||| person re-identification algorithm based on spatial attention network. ||| 24092 ||| 2420 ||| 24099 ||| 24093 ||| 
2020 ||| attention-based dynamic preference model for next point-of-interest recommendation. ||| 24100 ||| 7596 ||| 
2021 ||| multi-step domain adaption image classification network via attention mechanism and multi-level feature alignment. ||| 24101 ||| 24102 ||| 8217 ||| 15696 ||| 24103 ||| 
2021 ||| light field super-resolution based on spatial and angular attention. ||| 24104 ||| 24105 ||| 24106 ||| 18507 ||| 
2021 ||| temporal attention-based graph convolution network for taxi demand prediction in functional areas. ||| 7400 ||| 8935 ||| 24107 ||| 24094 ||| 24108 ||| 
2021 ||| dual attention network based on knowledge graph for news recommendation. ||| 24109 ||| 22375 ||| 8427 ||| 22376 ||| 2561 ||| 
2017 ||| a preliminary study on the influence of automation over mind wandering frequency in sustained attention. ||| 24110 ||| 24111 ||| 24112 ||| 
2021 ||| imitations of immortality: learning from human imitative examples in transformer poetry generation. ||| 24113 ||| 
2021 ||| improving the efficiency of transformers for resource-constrained devices. ||| 24114 ||| 24115 ||| 24116 ||| 16397 ||| 16396 ||| 
2021 ||| integrating channel context attention and regional association attention for kidney and tumor segmentation. ||| 5439 ||| 421 ||| 24117 ||| 24118 ||| 24119 ||| 
2019 ||| attention-guided convolutional neural network for detecting pneumonia on chest x-rays. ||| 24120 ||| 5213 ||| 24121 ||| 24122 ||| 
2020 ||| correlates of attention in the cingulate cortex during gambling in humans. ||| 24123 ||| 24124 ||| 24125 ||| 24126 ||| 24127 ||| 24128 ||| 24129 ||| 
2020 ||| a haptic-based perception-empathy biofeedback system with vibration transition: verifying the attention amount. ||| 24130 ||| 24131 ||| 24132 ||| 24133 ||| 24134 ||| 24135 ||| 24136 ||| 
2020 ||| development and evaluation of a new virtual reality-based audio-tactile cueing-system to guide visuo-spatial attention. ||| 24137 ||| 24138 ||| 24139 ||| 24140 ||| 1881 ||| 24141 ||| 14195 ||| 24142 ||| 
2021 ||| hierarchical attentional feature fusion for surgical instrument segmentation. ||| 19082 ||| 19910 ||| 19912 ||| 24143 ||| 
2021 ||| placental super micro-vessels segmentation based on resnext with convolutional block attention and u-net. ||| 6581 ||| 6574 ||| 24144 ||| 24145 ||| 5476 ||| 6582 ||| 
2020 ||| metaheuristic spatial transformation (mst) for accurate detection of attention deficit hyperactivity disorder (adhd) using rs-fmri. ||| 24146 ||| 561 ||| 24147 ||| 
2020 ||| a weighted graph attention network based method for multi-label classification of electrocardiogram abnormalities. ||| 24148 ||| 7700 ||| 18570 ||| 24149 ||| 24150 ||| 24151 ||| 15296 ||| 19774 ||| 
2021 ||| ucatr: based on cnn and transformer encoding and cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. ||| 24152 ||| 875 ||| 24153 ||| 24154 ||| 24155 ||| 6479 ||| 
2021 ||| introducing attention mechanism for eeg signals: emotion recognition with vision transformers. ||| 24156 ||| 24157 ||| 24158 ||| 
2019 ||| pandas: paediatric attention-deficit/hyperactivity disorder application software. ||| 1890 ||| 24159 ||| 24160 ||| 24161 ||| 
2021 ||| towards interpretable attention networks for cervical cancer analysis. ||| 24162 ||| 24163 ||| 11374 ||| 2130 ||| 24164 ||| 
2021 ||| high-resolution magnetic resonance spectroscopic imaging using a multi-encoder attention u-net with structural and adversarial loss. ||| 24165 ||| 24166 ||| 24167 ||| 24168 ||| 24169 ||| 24170 ||| 24171 ||| 24172 ||| 24173 ||| 15551 ||| 
2021 ||| transformer-based cnns: mining temporal context information for multi-sound covid-19 diagnosis. ||| 3410 ||| 12762 ||| 648 ||| 649 ||| 
2021 ||| weakly supervised attention map training for histological localization of colonoscopy images. ||| 24174 ||| 24175 ||| 
2018 ||| automatic sleep stage classification using single-channel eeg: learning sequential features with attention-based recurrent neural networks. ||| 12352 ||| 24176 ||| 24177 ||| 12354 ||| 3882 ||| 14475 ||| 
2019 ||| using soft attention mechanisms to classify heart sounds. ||| 24178 ||| 24179 ||| 8382 ||| 24180 ||| 24181 ||| 24182 ||| 24183 ||| 
2019 ||| a blstm with attention network for predicting acute myeloid leukemia patient's prognosis using comprehensive clinical parameters. ||| 24184 ||| 14281 ||| 24185 ||| 24186 ||| 24187 ||| 12347 ||| 
2017 ||| brain connectivity networks at the basis of human attention components: an eeg study. ||| 5561 ||| 5567 ||| 5563 ||| 5565 ||| 5564 ||| 5562 ||| 
2021 ||| cross-subject eeg-based emotion recognition using adversarial domain adaption with attention mechanism. ||| 24188 ||| 24189 ||| 24190 ||| 24191 ||| 24192 ||| 
2021 ||| learning generalized representations of eeg between multiple cognitive attention tasks. ||| 24193 ||| 24194 ||| 24195 ||| 10077 ||| 
2018 ||| research of the regulation effect of transcranial alternating current stimulation on vigilant attention. ||| 24196 ||| 24197 ||| 24198 ||| 24199 ||| 24200 ||| 24201 ||| 24202 ||| 24203 ||| 24204 ||| 
2019 ||| prediction of response time and vigilance score in a sustained attention task from pre-trial phase synchrony using deep neural networks. ||| 24205 ||| 24206 ||| 24207 ||| 24208 ||| 5335 ||| 24209 ||| 24210 ||| 
2017 ||| personalized features for attention detection in children with attention deficit hyperactivity disorder. ||| 24211 ||| 10077 ||| 24212 ||| 24213 ||| 24214 ||| 24215 ||| 
2020 ||| a novel graph attention network architecture for modeling multimodal brain connectivity. ||| 24216 ||| 24217 ||| 24218 ||| 24219 ||| 23955 ||| 
2019 ||| effect of english learning experience on young children's prefrontal cortex functioning for attentional control: an fnirs study. ||| 24220 ||| 24221 ||| 24222 ||| 1299 ||| 24223 ||| 
2021 ||| gated transformer for decoding human brain eeg signals. ||| 24224 ||| 4150 ||| 18292 ||| 24225 ||| 18296 ||| 24226 ||| 18294 ||| 13824 ||| 24227 ||| 
2021 ||| attention based deep multiple instance learning approach for lung cancer prediction using histopathological images. ||| 1994 ||| 24228 ||| 6785 ||| 24229 ||| 24230 ||| 24231 ||| 852 ||| 24232 ||| 2712 ||| 24233 ||| 
2020 ||| an attention-based deep learning method for schizophrenia patients classification using dna methylation data. ||| 21598 ||| 11542 ||| 11540 ||| 24234 ||| 24235 ||| 
2021 ||| c3d-unet: a comprehensive 3d unet for covid-19 segmentation with intact encoding and local attention. ||| 24236 ||| 24237 ||| 24238 ||| 4811 ||| 15648 ||| 24239 ||| 1224 ||| 24240 ||| 
2018 ||| assessment of attention demand for balance control using a smartphone: implementation and evaluation. ||| 24241 ||| 24242 ||| 24243 ||| 24244 ||| 
2020 ||| learning a phenotypic-attribute attentional brain connectivity embedding for adhd classification using rs-fmri. ||| 24245 ||| 24246 ||| 12347 ||| 
2020 ||| analysis of selective attention processing on experienced simultaneous interpreters using eeg phase synchronization. ||| 24247 ||| 24248 ||| 24249 ||| 24250 ||| 24251 ||| 11756 ||| 11757 ||| 
2021 ||| combined dynamic time warping and spatiotemporal attention for myoelectric control. ||| 24252 ||| 24253 ||| 24254 ||| 
2018 ||| vulnerable plaque recognition based on attention model with deep convolutional neural network. ||| 24255 ||| 24256 ||| 24257 ||| 14779 ||| 
2020 ||| adaptive brain-computer interface with attention alterations in patients with amyotrophic lateral sclerosis. ||| 5557 ||| 5560 ||| 
2019 ||| neural activity from attention networks predicts movement errors. ||| 24126 ||| 24258 ||| 24259 ||| 4046 ||| 24260 ||| 24129 ||| 
2020 ||| decoding auditory attention from single-trial eeg for a high-efficiency brain-computer interface. ||| 12774 ||| 24261 ||| 24262 ||| 12775 ||| 
2021 ||| spectrum power and brain functional connectivity of different eeg frequency bands in attention network tests. ||| 3691 ||| 398 ||| 22225 ||| 24263 ||| 24264 ||| 24265 ||| 24266 ||| 24267 ||| 
2020 ||| dran: densely reversed attention based convolutional network for diabetic retinopathy detection. ||| 13348 ||| 13349 ||| 13351 ||| 
2017 ||| feasibility study on the assessment of auditory sustained attention through walking motor parameters in mild cognitive impairments and healthy subjects. ||| 24268 ||| 24269 ||| 24270 ||| 24271 ||| 24272 ||| 24273 ||| 24274 ||| 24275 ||| 
2021 ||| segmentation in diabetic retinopathy using deeply-supervised multiscalar attention. ||| 24276 ||| 24277 ||| 
2020 ||| sequential attention-based detection of semantic incongruities from eeg while listening to speech. ||| 24251 ||| 24248 ||| 11757 ||| 
2021 ||| a study of visual search based calibration protocol for eeg attention detection. ||| 24195 ||| 24278 ||| 10077 ||| 
2021 ||| self-attention based virtual staining for bright-field images of label-free human carotid atherosclerotic plaque tissue section. ||| 24279 ||| 24280 ||| 24281 ||| 24282 ||| 344 ||| 24283 ||| 
2020 ||| 40-hz rhythmic visual stimulation facilitates attention by reshaping the brain functional connectivity. ||| 24284 ||| 24201 ||| 5204 ||| 24285 ||| 13805 ||| 24203 ||| 
2020 ||| multi-shell d-mri reconstruction via residual learning utilizing encoder-decoder network with attention (msr-net). ||| 24286 ||| 6369 ||| 6368 ||| 24287 ||| 24288 ||| 24289 ||| 
2018 ||| regularized spatial filtering method (r-sfm) for detection of attention deficit hyperactivity disorder (adhd) from resting-state functional magnetic resonance imaging (rs-fmri). ||| 24146 ||| 562 ||| 561 ||| 24290 ||| 
2021 ||| dual attention convolutional neural network based on adaptive parametric relu for denoising ecg signals with strong noise. ||| 24291 ||| 6812 ||| 5810 ||| 2054 ||| 
2019 ||| real-time tracking of magnetoencephalographic neuromarkers during a dynamic attention-switching task. ||| 24292 ||| 24293 ||| 24294 ||| 24295 ||| 
2021 ||| 3d attention m-net for short-axis left ventricular myocardium segmentation in mice mr cardiac images. ||| 24296 ||| 24297 ||| 24298 ||| 24299 ||| 24300 ||| 24301 ||| 24302 ||| 
2017 ||| eeg-based auditory attention decoding using unprocessed binaural signals in reverberant and noisy conditions? ||| 1490 ||| 1495 ||| 
2021 ||| dual encoder attention u-net for nuclei segmentation. ||| 24303 ||| 24304 ||| 24305 ||| 
2020 ||| automatic pulmonary vein and left atrium segmentation for tapvc preoperative evaluation using v-net with grouped attention. ||| 16761 ||| 5436 ||| 24306 ||| 24307 ||| 24308 ||| 5437 ||| 
2021 ||| auditory attention detection with eeg channel attention. ||| 14576 ||| 14575 ||| 24309 ||| 14578 ||| 12494 ||| 
2019 ||| a study of the midbrain network for covert attentional orienting in cervical dystonia patients using dynamic causal modelling. ||| 24310 ||| 24311 ||| 24312 ||| 24313 ||| 24314 ||| 24315 ||| 24316 ||| 
2021 ||| s and sequence transformer networks. ||| 24317 ||| 24318 ||| 
2021 ||| low-latency auditory spatial attention detection based on spectro-spatial features from eeg. ||| 14575 ||| 24319 ||| 24320 ||| 12494 ||| 
2021 ||| eeg emotion recognition via graph-based spatio-temporal attention neural networks. ||| 24321 ||| 24205 ||| 5335 ||| 24209 ||| 24210 ||| 
2020 ||| aec-net: attention and edge constraint network for medical image segmentation. ||| 1072 ||| 7662 ||| 24322 ||| 24239 ||| 
2021 ||| the synchronized enhancement effect of rhythmic visual stimulation of 40 hz on selective attention. ||| 5204 ||| 24284 ||| 24201 ||| 24203 ||| 
2021 ||| amf-net: attention-aware multi-scale fusion network for retinal vessel segmentation. ||| 5101 ||| 24323 ||| 421 ||| 423 ||| 
2017 ||| the timing of theta phase synchronization accords with vigilant attention. ||| 24197 ||| 24198 ||| 24200 ||| 24199 ||| 24202 ||| 24203 ||| 24204 ||| 
2020 ||| breathing sound segmentation and detection using transfer learning techniques on an attention-based encoder-decoder architecture. ||| 24324 ||| 24325 ||| 24326 ||| 24327 ||| 24328 ||| 24329 ||| 24330 ||| 
2020 ||| generalizability of eeg-based mental attention modeling with multiple cognitive tasks. ||| 24195 ||| 24331 ||| 10077 ||| 
2021 ||| zoome: efficient melanoma detection using zoom-in attention and metadata embedding deep neural network. ||| 24332 ||| 24333 ||| 3433 ||| 24334 ||| 24335 ||| 
2018 ||| real-time decoding of auditory attention from eeg via bayesian filtering. ||| 24293 ||| 24336 ||| 24337 ||| 24295 ||| 6514 ||| 24294 ||| 
2020 ||| malignancy detection in prostate multi-parametric mr images using u-net with attention. ||| 24338 ||| 24339 ||| 24340 ||| 21457 ||| 
2021 ||| generative adversarial training with dual-attention for vascular segmentation and topological analysis. ||| 11209 ||| 24341 ||| 24342 ||| 24343 ||| 24344 ||| 
2019 ||| lstms and neural attention models for blood glucose prediction: comparative experiments on real and synthetic data. ||| 24345 ||| 24346 ||| 24347 ||| 24348 ||| 
2021 ||| brain tumors classification for mr images based on attention guided deep learning model. ||| 24349 ||| 24350 ||| 24351 ||| 24352 ||| 24353 ||| 
2020 ||| effects of stimulus spatial resolution on ssvep responses under overt and covert attention. ||| 24195 ||| 24354 ||| 15490 ||| 24355 ||| 10077 ||| 
2020 ||| eeg-based depression detection using convolutional neural network with demographic attention mechanism. ||| 24356 ||| 24357 ||| 24358 ||| 23377 ||| 5554 ||| 24359 ||| 
2020 ||| single fundus image super-resolution via cascaded channel-wise attention network. ||| 4812 ||| 24360 ||| 24361 ||| 24362 ||| 24363 ||| 
2019 ||| rasnet: segmentation for tracking surgical instruments in surgical videos using refined attention segmentation network. ||| 5183 ||| 5184 ||| 24364 ||| 271 ||| 5185 ||| 276 ||| 
2017 ||| neural decoding of attentional selection in multi-speaker environments without access to separated sources. ||| 24365 ||| 12029 ||| 24366 ||| 24367 ||| 24368 ||| 24369 ||| 
2021 ||| full scale attention for automated covid-19 diagnosis from ct images. ||| 12742 ||| 24370 ||| 23379 ||| 1236 ||| 
2020 ||| automatic quality assessment of reflectance confocal microscopy mosaics using attention-based deep neural network. ||| 24371 ||| 24372 ||| 24373 ||| 24374 ||| 24375 ||| 24376 ||| 
2021 ||| a novel approach to decode covert spatial attention using ssvep and single-frequency phase-coded stimuli. ||| 24377 ||| 24378 ||| 7402 ||| 
2021 ||| attention-based multi-scale generative adversarial network for synthesizing contrast-enhanced mri. ||| 24379 ||| 4600 ||| 24380 ||| 24381 ||| 344 ||| 
2020 ||| neural correlates of attention lapses during continuous tasks. ||| 24382 ||| 24383 ||| 24384 ||| 23024 ||| 24385 ||| 
2020 ||| an lmmse-based estimation of temporal response function in auditory attention decoding. ||| 24386 ||| 24387 ||| 24388 ||| 
2019 ||| attentional bias for emotional faces in depressed and non-depressed individuals: an eye-tracking study. ||| 24389 ||| 24390 ||| 24391 ||| 24392 ||| 24393 ||| 
2020 ||| triple multi-scale adversarial learning with self-attention and quality loss for unpaired fundus fluorescein angiography synthesis. ||| 24394 ||| 24255 ||| 24395 ||| 24256 ||| 24396 ||| 14779 ||| 
2020 ||| classification of auditory attention focuses during speech perception. ||| 24397 ||| 24201 ||| 24398 ||| 24399 ||| 24203 ||| 
2018 ||| applying entropy to human center of foot pressure data to assess attention investment in balance control. ||| 24242 ||| 24400 ||| 24243 ||| 24244 ||| 
2021 ||| low dose ct image denoising using boosting attention fusion gan with perceptual loss. ||| 24401 ||| 24402 ||| 24403 ||| 
2021 ||| attentional bias towards high and low caloric food on repeated visual food stimuli: an erp study. ||| 24404 ||| 24405 ||| 24406 ||| 
2020 ||| deep learning with skip connection attention for choroid layer segmentation in oct images. ||| 24407 ||| 24408 ||| 24409 ||| 24410 ||| 24411 ||| 24412 ||| 24413 ||| 15003 ||| 5206 ||| 
2018 ||| the effect of miniaturization and galvanic separation of eeg sensor devices in an auditory attention detection task. ||| 24414 ||| 8255 ||| 
2020 ||| attention networks for multi-task signal analysis. ||| 24164 ||| 24163 ||| 11374 ||| 11331 ||| 2130 ||| 
2020 ||| a novel approach for atrial fibrillation signal identification based on temporal attention mechanism. ||| 22355 ||| 2054 ||| 15621 ||| 
2019 ||| spectral analysis versus signal complexity methods for assessing attention related activity in human eeg. ||| 24415 ||| 24416 ||| 24417 ||| 23764 ||| 24418 ||| 24419 ||| 24420 ||| 24421 ||| 
2019 ||| canet: a channel attention network to determine informative multi-channel for image classification from brain signals. ||| 24422 ||| 24423 ||| 24424 ||| 24425 ||| 
2021 ||| 3d deep attentive u-net with transformer for breast tumor segmentation from automated breast volume scanner. ||| 17630 ||| 208 ||| 1706 ||| 5476 ||| 6582 ||| 
2017 ||| the motion influence on respiration rate estimation from low-resolution thermal sequences during attention focusing tasks. ||| 19309 ||| 19311 ||| 24426 ||| 
2019 ||| improving crnn with efficientnet-like feature extractor and multi-head attention for text recognition. ||| 2675 ||| 24427 ||| 
2017 ||| parallel multi-feature attention on neural sentiment classification. ||| 13925 ||| 3177 ||| 
2019 ||| session-based recommendation with self-attention. ||| 24428 ||| 24429 ||| 24430 ||| 
2021 ||| catching patient's attention at the right time to help them undergo behavioural change: stress classification experiment from blood volume pulse. ||| 13365 ||| 24431 ||| 24432 ||| 
2021 ||| transicd: transformer based code-wise attention model for explainable icd coding. ||| 24433 ||| 24434 ||| 3750 ||| 
2021 ||| attention-based explanation in a deep learning model for classifying radiology reports. ||| 9820 ||| 24435 ||| 9822 ||| 24436 ||| 9823 ||| 
2021 ||| transformers for multi-label classification of medical text: an empirical comparison. ||| 24437 ||| 24438 ||| 24439 ||| 24440 ||| 
2021 ||| using event-based web-scraping methods and bidirectional transformers to characterize covid-19 outbreaks in food production and retail settings. ||| 24441 ||| 24442 ||| 24443 ||| 24444 ||| 24445 ||| 24446 ||| 24447 ||| 
2021 ||| cbam-unet++: easier to find the target with the attention module "cbam". ||| 24448 ||| 24449 ||| 24450 ||| 
2021 ||| propagation-based fake news detection using graph neural networks with transformer. ||| 24451 ||| 24452 ||| 24453 ||| 
2020 ||| automatic detection of chewing and swallowing using hybrid ctc/attention. ||| 24454 ||| 24455 ||| 24456 ||| 24457 ||| 24458 ||| 24459 ||| 
2021 ||| text image super resolution using deep attention neural network. ||| 4477 ||| 24460 ||| 24461 ||| 24462 ||| 24463 ||| 24464 ||| 
2017 ||| an intelligent maintenance scheduling of distribution transformers in a smart grid. ||| 24465 ||| 24466 ||| 24467 ||| 24468 ||| 24469 ||| 
2020 ||| self-attention based neural network for few shot classification. ||| 24470 ||| 24461 ||| 
2018 ||| user-centric visual attention estimation based on relationship between image and eye gaze data. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2020 ||| data augmentation using user attention for educational content recommendation based on ffm. ||| 24471 ||| 24472 ||| 24473 ||| 24474 ||| 
2019 ||| the design of wireless power transformer for electronic flower pot. ||| 24475 ||| 24476 ||| 
2020 ||| estimation of user-specific visual attention considering individual tendency toward gazed objects. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| a study of dqn using visiontransformer as an image extractor. ||| 24477 ||| 24478 ||| 24450 ||| 
2021 ||| question generation using knowledge graphs with the t5 language model and masked self-attention. ||| 24479 ||| 24480 ||| 24481 ||| 24459 ||| 
2019 ||| estimation of user-specific visual attention based on gaze information of similar users. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| automatic detection of chewing and swallowing using attention-based fusion. ||| 24454 ||| 24456 ||| 24458 ||| 24455 ||| 24457 ||| 24459 ||| 
2021 ||| four-way bidirectional attention for multiple-choice reading comprehension. ||| 24482 ||| 24483 ||| 24484 ||| 24485 ||| 24486 ||| 24487 ||| 24488 ||| 
2021 ||| cthgat: category-aware and time-aware next point-of-interest via heterogeneous graph attention network. ||| 24489 ||| 24490 ||| 24491 ||| 11453 ||| 24492 ||| 24493 ||| 24494 ||| 
2021 ||| mix attention based convolutional neural network for clothing brand logo recognition and classification. ||| 22476 ||| 24495 ||| 22477 ||| 
2020 ||| a cnn model for herb identification based on part priority attention mechanism. ||| 8981 ||| 24496 ||| 24497 ||| 24498 ||| 24499 ||| 1856 ||| 
2020 ||| decoding visual recognition of objects from eeg signals based on attention-driven convolutional neural network. ||| 24500 ||| 24501 ||| 24502 ||| 
2021 ||| csagan: channel and spatial attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| 11453 ||| 24490 ||| 24489 ||| 24491 ||| 24503 ||| 17614 ||| 24494 ||| 
2019 ||| a pbci to predict attentional error before it happens in real flight conditions. ||| 15325 ||| 15326 ||| 15327 ||| 14046 ||| 13128 ||| 17039 ||| 24504 ||| 24505 ||| 17042 ||| 
2020 ||| rna-net: residual nonlocal attention network for retinal vessel segmentation. ||| 1064 ||| 24335 ||| 340 ||| 3433 ||| 
2021 ||| risc-vtf: risc-v based extended instruction set for transformer. ||| 24506 ||| 6595 ||| 5743 ||| 24507 ||| 
2021 ||| acnet: mask-aware attention with dynamic context enhancement for robust acne detection. ||| 24508 ||| 24509 ||| 24502 ||| 
2021 |||  adaptive attention convolutional neural network. ||| 15439 ||| 15443 ||| 24510 ||| 24511 ||| 24512 ||| 
2020 ||| bmi-vr based cognitive training improves attention switching processing speed. ||| 24513 ||| 24514 ||| 24515 ||| 
2021 ||| box-driven weakly supervised images semantic segmentation algorithm based on attention model. ||| 24516 ||| 24517 ||| 764 ||| 24518 ||| 24519 ||| 
2018 ||| cognitive environment system by joint attention behaviors and relevance theory for robot partners. ||| 23188 ||| 24520 ||| 24521 ||| 
2021 ||| ssvep-aided recognition of internally and externally directed attention from brain activity. ||| 13860 ||| 16146 ||| 24522 ||| 13862 ||| 
2020 ||| learning effective value function factorization via attentional communication. ||| 11346 ||| 24523 ||| 24524 ||| 3049 ||| 528 ||| 5331 ||| 
2021 ||| predictive attention allocation in supervising multiple robots for search and rescue tasks. ||| 24525 ||| 24526 ||| 
2019 ||| predicting auditory spatial attention from eeg using single- and multi-task convolutional neural networks. ||| 24527 ||| 24528 ||| 6120 ||| 9126 ||| 
2019 ||| conversation during partially automated driving: how attention arousal is effective on maintaining situation awareness. ||| 24529 ||| 24530 ||| 24531 ||| 24532 ||| 
2018 ||| a study of ssvep responses in case of overt and covert visual attention with different view angles. ||| 24195 ||| 24533 ||| 24534 ||| 10077 ||| 
2021 ||| transformer-xl with graph neural network for source code summarization. ||| 24535 ||| 24536 ||| 24537 ||| 24538 ||| 24539 ||| 24540 ||| 
2021 ||| non-strict attentional region annotation to improve image classification accuracy. ||| 24541 ||| 24542 ||| 24543 ||| 
2021 ||| sarnet: self-attention assisted ranking network for temporal action proposal generation. ||| 24544 ||| 24545 ||| 
2021 ||| crb-net: a sign language recognition deep learning strategy based on multi-modal fusion with attention mechanism. ||| 3996 ||| 16832 ||| 24546 ||| 13196 ||| 
2018 ||| covert visuospatial attention (vsa) for eeg-based asynchronous control of robot. ||| 24547 ||| 24548 ||| 24549 ||| 
2021 ||| edge attention network for image deblurring and super-resolution. ||| 24550 ||| 24551 ||| 24552 ||| 24553 ||| 
2017 ||| eeg-based auditory attention decoding: impact of reverberation, noise and interference reduction. ||| 1490 ||| 1495 ||| 
2021 ||| tstrack: tracking by detector with target guidance and self-attention. ||| 24554 ||| 24555 ||| 24556 ||| 
2019 ||| morphological landmark detection on lobsters using attention networks. ||| 24557 ||| 24558 ||| 
2019 ||| attention prediction on webpage images using multilabel classification. ||| 24559 ||| 24560 ||| 24561 ||| 
2021 ||| malbert: malware detection using bidirectional encoder representations from transformers. ||| 24562 ||| 24563 ||| 
2021 ||| one-stage attention-based network for image classification and segmentation on optical coherence tomography image. ||| 10405 ||| 24564 ||| 24565 ||| 
2020 ||| ahac: actor hierarchical attention critic for multi-agent reinforcement learning. ||| 24566 ||| 24567 ||| 24568 ||| 15823 ||| 24569 ||| 24570 ||| 
2019 ||| shared attention reflected in eeg, electrodermal activity and heart rate. ||| 24571 ||| 24572 ||| 24573 ||| 
2021 ||| automatic misogyny detection in social media platforms using attention-based bidirectional-lstm. ||| 24562 ||| 24563 ||| 24574 ||| 24575 ||| 
2020 ||| an auxiliary screening system for autism spectrum disorder based on emotion and attention analysis. ||| 5723 ||| 11675 ||| 5018 ||| 2046 ||| 12819 ||| 
2017 ||| visual attention control using peripheral vision stimulation. ||| 24576 ||| 24577 ||| 24578 ||| 24579 ||| 24580 ||| 24581 ||| 
2021 ||| semantic segmentation method of 3d liver image based on contextual attention model. ||| 24582 ||| 24583 ||| 24584 ||| 24585 ||| 
2017 ||| analysis of driver visual attention when driving with different levels of haptic steering guidance. ||| 369 ||| 2760 ||| 24586 ||| 24587 ||| 24588 ||| 
2021 ||| multimodal sentiment analysis based on attention mechanism and tensor fusion network. ||| 24519 ||| 24517 ||| 764 ||| 24518 ||| 24516 ||| 
2021 ||| show why the answer is correct! towards explainable ai using compositional temporal attention. ||| 24589 ||| 24590 ||| 24591 ||| 
2021 ||| a fine-grained visual attention approach for fingerspelling recognition in the wild. ||| 24592 ||| 24593 ||| 392 ||| 
2020 ||| proposal for visual attention level based on microsaccades after saccades. ||| 24594 ||| 24595 ||| 24596 ||| 
2020 ||| self-attention networks for human activity recognition using wearable devices. ||| 24597 ||| 24598 ||| 24599 ||| 
2020 ||| attention bidirectional lstm networks based mime speech recognition using semg data. ||| 24600 ||| 24601 ||| 24602 ||| 1251 ||| 24603 ||| 5192 ||| 1973 ||| 
2021 ||| temporally-aware convolutional block attention module for video text detection. ||| 24604 ||| 24605 ||| 
2021 ||| assessing the significance of co-occurring terms in goods and services tax in india using co-occurrence graphs and attention based deep learning models. ||| 24606 ||| 24607 ||| 
2018 ||| error backpropagation with attention control to learn imbalanced data for regression. ||| 24608 ||| 12482 ||| 
2021 ||| default mode network and attention network in unconscious processing. ||| 16721 ||| 
2021 ||| eeg-based classification of drivers attention using convolutional neural network. ||| 24609 ||| 24610 ||| 
2020 ||| is autism, attention deficit hyperactivity disorder (adhd) and specific learning disorder linked to impaired emotion recognition in primary school aged children? ||| 20932 ||| 24611 ||| 24612 ||| 20934 ||| 24613 ||| 20933 ||| 20935 ||| 
2022 ||| gsap: a hybrid gru and self-attention based model for dual medical nlp tasks. ||| 14202 ||| 24614 ||| 24615 ||| 24616 ||| 24617 ||| 
2019 ||| local features in angry faces capture attention in children with autism spectrum disorders exhibiting local-biased perceptual characteristics. ||| 24618 ||| 24619 ||| 24620 ||| 
2017 ||| visual attention is captured by task-irrelevant faces, but not by pareidolia faces. ||| 24621 ||| 24622 ||| 
2017 ||| impacts of cue reliability and explicit instruction on visual attention. ||| 24623 ||| 24624 ||| 
2020 ||| an improved attention-based lstm for multi-step dissolved oxygen prediction in water environment. ||| 24625 ||| 24626 ||| 24627 ||| 24628 ||| 24629 ||| 
2021 ||| research on digital twin model and visualization of power transformer. ||| 20541 ||| 24630 ||| 24631 ||| 24204 ||| 24632 ||| 
2017 ||| salient locations search based on human visual attention: an experimental analysis. ||| 24633 ||| 24634 ||| 24635 ||| 5107 ||| 24636 ||| 24637 ||| 
2021 ||| improved polarmask with attention for instance segmentation. ||| 1903 ||| 24638 ||| 
2017 ||| rssi-based attention target approach detection for a vehicle reminder system with beaconing devices. ||| 24639 ||| 24640 ||| 
2020 ||| a dual-attention-based neural network for see-through driving decision. ||| 24641 ||| 24642 ||| 24643 ||| 24644 ||| 
2021 ||| a time-efficient and attention-aware deployment strategy for uav networks driven by deep reinforcement learning. ||| 24645 ||| 1287 ||| 24646 ||| 3337 ||| 17822 ||| 
2019 ||| multimodal brain image segmentation and analysis with neuromorphic attention-based learning. ||| 24647 ||| 24648 ||| 
2019 ||| brain tumor segmentation based on attention mechanism and multi-model fusion. ||| 24649 ||| 24650 ||| 24651 ||| 24652 ||| 24653 ||| 24654 ||| 24655 ||| 6669 ||| 24656 ||| 
2020 ||| brain tumor segmentation using dual-path attention u-net in 3d mri images. ||| 24657 ||| 24658 ||| 24659 ||| 
2020 ||| attention u-net with dimension-hybridized fast data density functional theory for automatic brain tumor image segmentation. ||| 24660 ||| 24661 ||| 24662 ||| 24663 ||| 24664 ||| 
2020 ||| automatic brain tumor segmentation with scale attention network. ||| 24665 ||| 
2020 ||| multi-threshold attention u-net (mtau) based model for multimodal brain tumor segmentation in mri scans. ||| 24666 ||| 24667 ||| 24668 ||| 
2019 ||| brain tumor segmentation using attention-based network in 3d mri images. ||| 6278 ||| 24669 ||| 1418 ||| 
2020 ||| multimodal brain image analysis and survival prediction using neuromorphic attention-based neural networks. ||| 24648 ||| 
2019 ||| brain tumor segmentation and survival prediction using 3d attention unet. ||| 21821 ||| 24670 ||| 24671 ||| 24672 ||| 24673 ||| 21822 ||| 
2020 ||| brain tumor segmentation network using attention-based fusion and spatial relationship constraint. ||| 24674 ||| 24675 ||| 3034 ||| 19749 ||| 24676 ||| 24677 ||| 20755 ||| 
2020 ||| a deep supervised u-attention net for pixel-wise brain tumor segmentation. ||| 24678 ||| 24679 ||| 24680 ||| 20729 ||| 20730 ||| 
2020 ||| a two-stage cascade model with variational autoencoders and attention gates for mri brain tumor segmentation. ||| 24681 ||| 24682 ||| 
2021 ||| evaluation of the bubble view metaphor for the crowdsourcing study of visual attention deployment in tone-mapped images. ||| 24683 ||| 24684 ||| 11611 ||| 
2018 ||| viewport-aware omnidirectional video streaming using visual attention and dynamic tiles. ||| 1595 ||| 21740 ||| 21741 ||| 1597 ||| 
2020 ||| group-wise attention fusion network for choroid segmentation in oct images. ||| 24685 ||| 24686 ||| 24687 ||| 24688 ||| 24689 ||| 24690 ||| 14041 ||| 24691 ||| 24692 ||| 
2020 ||| automatic epicardial fat segmentation in cardiac ct imaging using 3d deep attention u-net. ||| 24693 ||| 24694 ||| 24695 ||| 24696 ||| 24697 ||| 24698 ||| 24699 ||| 24700 ||| 
2020 ||| ganet: group attention network for diabetic retinopathy image segmentation. ||| 24701 ||| 24688 ||| 24687 ||| 24686 ||| 
2020 ||| automated retinopathy of prematurity screening using deep neural network with attention mechanism. ||| 24702 ||| 24688 ||| 17396 ||| 24703 ||| 24686 ||| 
2020 ||| attention-guided channel to pixel convolution network for retinal layer segmentation with choroidal neovascularization. ||| 250 ||| 24686 ||| 24689 ||| 
2020 ||| automatic lung segmentation in low-dose ct image with contrastive attention module. ||| 24704 ||| 24705 ||| 24689 ||| 24692 ||| 24688 ||| 24686 ||| 
2019 ||| coronary calcium detection using 3d attention identical dual deep network based on weakly supervised learning. ||| 24706 ||| 24707 ||| 24708 ||| 7207 ||| 24709 ||| 24710 ||| 24711 ||| 24712 ||| 7208 ||| 
2020 ||| super-resolution magnetic resonance imaging reconstruction using deep attention networks. ||| 24693 ||| 24695 ||| 24713 ||| 1208 ||| 24698 ||| 24697 ||| 24700 ||| 
2020 ||| attention multi-scale network for pigment epithelial detachment segmentation in oct images. ||| 24714 ||| 24685 ||| 24688 ||| 24692 ||| 24686 ||| 
2020 ||| enhancing infarct segmentation performance using domain-specific attention in acute ischemic stroke. ||| 24715 ||| 24716 ||| 24717 ||| 24718 ||| 
2020 ||| an attention model for mashup tag recommendation. ||| 24719 ||| 
2019 ||| neural-attention multi-instance learning for predicting user demographics from highly noisy tweets. ||| 13339 ||| 24720 ||| 12196 ||| 
2021 ||| implicit visual attention feedback system for wikipedia users. ||| 24721 ||| 24722 ||| 24723 ||| 24724 ||| 
2019 ||| interpretable multi-task learning for product quality prediction with attention mechanism. ||| 24725 ||| 24726 ||| 24727 ||| 
2021 ||| gallat: a spatiotemporal graph attention network for passenger demand prediction. ||| 24728 ||| 1203 ||| 5664 ||| 24729 ||| 8603 ||| 24730 ||| 24731 ||| 
2019 ||| context-aware co-attention neural network for service recommendations. ||| 3034 ||| 8875 ||| 3036 ||| 
2021 ||| purchase intent forecasting with convolutional hierarchical transformer networks. ||| 1124 ||| 1055 ||| 1148 ||| 
2019 ||| context-aware attention-based data augmentation for poi recommendation. ||| 438 ||| 24732 ||| 1770 ||| 24733 ||| 1084 ||| 
2020 ||| san : scale-space attention networks. ||| 7764 ||| 7765 ||| 7766 ||| 8141 ||| 
2021 ||| variational self-attention network for sequential recommendation. ||| 764 ||| 659 ||| 660 ||| 9588 ||| 6475 ||| 18526 ||| 
2019 ||| air: attentional intention-aware recommender systems. ||| 5664 ||| 1203 ||| 18525 ||| 10233 ||| 24734 ||| 9889 ||| 
2021 ||| structure-aware parameter-free group query via heterogeneous information network transformer. ||| 24735 ||| 1819 ||| 24736 ||| 24737 ||| 1373 ||| 1094 ||| 24738 ||| 
2018 ||| joint bottleneck feature and attention model for speech recognition. ||| 24739 ||| 24740 ||| 
2021 ||| integrating transformers and knowledge graphs for twitter stance detection. ||| 24741 ||| 24742 ||| 24743 ||| 24744 ||| 24745 ||| 24746 ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| 14083 ||| 24747 ||| 
2020 ||| tatl at wnut-2020 task 2: a transformer-based baseline system for identification of informative covid-19 english tweets. ||| 24748 ||| 
2020 ||| upennhlp at wnut-2020 task 2 : transformer models for classification of covid19 posts on twitter. ||| 24749 ||| 24750 ||| 24751 ||| 24752 ||| 24753 ||| 
2020 ||| punctuation restoration using transformer models for high-and low-resource languages. ||| 24754 ||| 24755 ||| 24756 ||| 
2021 ||| does it happen? multi-hop path structures for event factuality prediction with graph transformer networks. ||| 24757 ||| 11744 ||| 
2020 ||| siva at wnut-2020 task 2: fine-tuning transformer neural networks for identification of informative covid-19 tweets. ||| 24758 ||| 
2021 ||| sequence-to-sequence lexical normalization with multilingual transformers. ||| 24759 ||| 24760 ||| 24761 ||| 
2019 ||| weakly supervised attention networks for fine-grained opinion mining and public health. ||| 24762 ||| 24763 ||| 24764 ||| 
2019 ||| geolocation with attention-based multitask learning models. ||| 24765 ||| 24766 ||| 
2021 ||| description-based label attention classifier for explainable icd-9 classification. ||| 24767 ||| 24768 ||| 15103 ||| 15822 ||| 
2020 ||| infominer at wnut-2020 task 2: transformer-based covid-19 informative tweet extraction. ||| 10586 ||| 3849 ||| 
2020 ||| edinburghnlp at wnut-2020 task 2: leveraging transformers with generalized augmentation for identifying informativeness in covid-19 tweets. ||| 24769 ||| 
2021 ||| aspect level sentiment classification with semantic distance attention networks. ||| 24770 ||| 24771 ||| 24772 ||| 765 ||| 24773 ||| 
2018 ||| insertion-loss optimization of transformer-based matching networks for mm-wave applications. ||| 9946 ||| 6666 ||| 
2019 ||| designing at millimeter-wave: lessons from a triple coil variable transformer. ||| 24774 ||| 24775 ||| 24776 ||| 24777 ||| 24778 ||| 24779 ||| 24780 ||| 
2020 ||| predicting of air pollutant concentrations based on spatio-temporal attention convolutional lstm networks. ||| 1239 ||| 24781 ||| 1235 ||| 24782 ||| 
2020 ||| mild cognitive impairment classification convolutional neural network with attention mechanism. ||| 24783 ||| 24784 ||| 10418 ||| 24785 ||| 
2018 ||| multi-column spatial transformer convolution neural network for traffic sign recognition. ||| 390 ||| 24786 ||| 24787 ||| 24788 ||| 
2019 ||| bidirectional gated recurrent unit networks for relation classification with multiple attentions and semantic information. ||| 24789 ||| 24790 ||| 24791 ||| 24792 ||| 24793 ||| 
2019 ||| graph convolution and self attention based non-maximum suppression. ||| 24794 ||| 737 ||| 
2019 ||| automatically generate hymns using variational attention models. ||| 24795 ||| 24796 ||| 24797 ||| 7517 ||| 
2019 ||| dynamic graph cnn with attention module for 3d hand pose estimation. ||| 24798 ||| 24799 ||| 
2021 ||| ambd: attention based multi-block deep learning model for warehouse dwell time prediction. ||| 24800 ||| 7700 ||| 24801 ||| 24802 ||| 11154 ||| 
2020 ||| nsa-net: a netflow sequence attention network for virtual private network traffic detection. ||| 24803 ||| 748 ||| 24804 ||| 15251 ||| 745 ||| 9543 ||| 5189 ||| 
2020 ||| adfr: an attention-based deep learning model for flight ranking. ||| 24805 ||| 17587 ||| 24806 ||| 24807 ||| 24808 ||| 
2017 ||| connecting targets to tweets: semantic attention-based model for target-specific stance detection. ||| 1290 ||| 913 ||| 6389 ||| 
2021 ||| enhancing both local and global entity linking models with attention. ||| 24809 ||| 24810 ||| 24811 ||| 254 ||| 11145 ||| 24812 ||| 654 ||| 
2018 ||| dual: a deep unified attention model with latent relation representations for fake news detection. ||| 24813 ||| 771 ||| 772 ||| 24814 ||| 13182 ||| 24815 ||| 
2019 ||| multiple interaction attention model for open-world knowledge graph completion. ||| 24816 ||| 654 ||| 11145 ||| 11254 ||| 11122 ||| 659 ||| 658 ||| 
2021 ||| cross-modal attention network with orthogonal latent memory for rumor detection. ||| 24817 ||| 24818 ||| 4267 ||| 24819 ||| 24820 ||| 4270 ||| 
2018 ||| combining contextual information by self-attention mechanism in convolutional neural networks for text classification. ||| 24821 ||| 11156 ||| 3477 ||| 24822 ||| 11157 ||| 
2021 ||| multi-task learning with personalized transformer for review recommendation. ||| 15860 ||| 683 ||| 24823 ||| 
2021 ||| interactive pose attention network for human pose transfer. ||| 24824 ||| 24825 ||| 4267 ||| 24826 ||| 24827 ||| 24828 ||| 3477 ||| 4270 ||| 
2020 ||| attention-based high-order feature interactions to enhance the recommender system for web-based knowledge-sharing service. ||| 8109 ||| 8110 ||| 8111 ||| 8113 ||| 8112 ||| 8114 ||| 144 ||| 5474 ||| 8115 ||| 8116 ||| 
2021 ||| mgsan: a multi-granularity self-attention network for next poi recommendation. ||| 24829 ||| 24830 ||| 659 ||| 9588 ||| 6475 ||| 
2019 ||| entity disambiguation based on parse tree neighbours on graph attention network. ||| 24831 ||| 24832 ||| 16550 ||| 18526 ||| 
2019 ||| memory-augmented attention network for sequential recommendation. ||| 24833 ||| 24834 ||| 24835 ||| 24836 ||| 
2019 ||| pattern filtering attention for distant supervised relation extraction via online clustering. ||| 3626 ||| 24837 ||| 24838 ||| 24839 ||| 300 ||| 301 ||| 
2021 ||| rau: an interpretable automatic infection diagnosis of covid-19 pneumonia with residual attention u-net. ||| 24840 ||| 771 ||| 9472 ||| 
2021 ||| efficient feature interactions learning with gated attention transformer. ||| 24841 ||| 11190 ||| 24842 ||| 17185 ||| 
2021 ||| jurassic mark: inattentional blindness for a datasaurus reveals that visualizations are explored, not seen. ||| 24843 ||| 24844 ||| 24845 ||| 
2019 ||| sanvis: visual analytics for understanding self-attention networks. ||| 1408 ||| 1183 ||| 24846 ||| 24847 ||| 24848 ||| 24849 ||| 24850 ||| 4095 ||| 24851 ||| 24852 ||| 
2021 ||| named entity recognition in cyber threat intelligence using transformer-based models. ||| 24853 ||| 24854 ||| 24855 ||| 24856 ||| 24857 ||| 24858 ||| 2707 ||| 
2021 ||| ai-assisted stanford classification of aortic dissection in ct imaging using volumetric 3d cnn with external guided attention. ||| 24859 ||| 15640 ||| 15641 ||| 15642 ||| 15643 ||| 
2017 ||| measurement of energy transmission efficiency of transcutaneous energy transformer in nacl solution for ventricular assist devices by reducing common-mode current in the range of 200-1500 khz. ||| 24860 ||| 24861 ||| 
2021 ||| video based heart rate extraction using skin roi segmentation and attention cnn. ||| 24862 ||| 13249 ||| 24863 ||| 
2021 ||| residual learning attention cnn for motion intention recognition based on eeg data. ||| 13179 ||| 24864 ||| 24865 ||| 24866 ||| 24867 ||| 24868 ||| 
2021 ||| attention state classification with in-ear eeg. ||| 24869 ||| 24870 ||| 24871 ||| 17063 ||| 20525 ||| 24872 ||| 
2021 ||| light field visual attention prediction using fourier disparity layers. ||| 24873 ||| 24874 ||| 24875 ||| 8494 ||| 1597 ||| 
2017 ||| fine-grained vehicle classificationusing deep residual networks with multiscale attention windows. ||| 24876 ||| 24877 ||| 24878 ||| 24879 ||| 
2021 ||| a machine-learning framework to predict tmo preference based on image and visual attention features. ||| 24683 ||| 24684 ||| 11611 ||| 
2019 ||| one-shot video object segmentation using attention transfer. ||| 7169 ||| 602 ||| 
2021 ||| dual attention network for heart rate and respiratory rate estimation. ||| 24880 ||| 24881 ||| 24882 ||| 
2019 ||| accurate small bowel lesions detection in wireless capsule endoscopy images using deep recurrent attention neural network. ||| 59 ||| 24883 ||| 9566 ||| 24884 ||| 11561 ||| 24885 ||| 24886 ||| 20302 ||| 7350 ||| 
2020 ||| profiling actions for sport video summarization: an attention signal analysis. ||| 20247 ||| 15325 ||| 15326 ||| 20248 ||| 20249 ||| 
2019 ||| from speech to facial activity: towards cross-modal sequence-to-sequence attention networks. ||| 12147 ||| 19565 ||| 647 ||| 24887 ||| 24888 ||| 648 ||| 649 ||| 
2020 ||| multianet: a multi-attention network for defocus blur detection. ||| 24889 ||| 14041 ||| 8862 ||| 24890 ||| 
2021 ||| spatial cross-attention rgb-d fusion module for object detection. ||| 24891 ||| 24892 ||| 24893 ||| 
2021 ||| attention-based stylisation for exemplar image colourisation. ||| 11480 ||| 11481 ||| 24894 ||| 1675 ||| 11484 ||| 
2020 ||| station correlation attention learning for data-driven bike sharing system usage prediction. ||| 13408 ||| 9053 ||| 24895 ||| 
2020 ||| gsan: graph self-attention network for interaction measurement in autonomous driving. ||| 24896 ||| 24897 ||| 24898 ||| 22980 ||| 24899 ||| 24900 ||| 
2020 ||| the problem of tracking the center of attention in eye tracking systems. ||| 24901 ||| 24902 ||| 24903 ||| 24904 ||| 
2019 ||| vr experience from data science point of view: how to measure inter-subject dependence in visual attention and spatial behavior. ||| 21627 ||| 21628 ||| 24905 ||| 
2020 ||| the car as a transformer. ||| 24906 ||| 24907 ||| 
2021 ||| do syntax trees help pre-trained transformers extract information? ||| 21387 ||| 24349 ||| 112 ||| 9237 ||| 
2021 ||| how certain is your transformer? ||| 24908 ||| 24909 ||| 24910 ||| 24911 ||| 24912 ||| 24913 ||| 
2021 ||| civil rephrases of toxic texts with self-supervised transformers. ||| 24914 ||| 24915 ||| 24916 ||| 24917 ||| 
2021 ||| trankit: a light-weight transformer-based toolkit for multilingual natural language processing. ||| 24918 ||| 24919 ||| 1404 ||| 11744 ||| 
2021 ||| telling bert's full story: from local attention to global aggregation. ||| 23999 ||| 23998 ||| 24002 ||| 
2021 ||| interpret: an interactive visualization tool for interpreting transformers. ||| 24920 ||| 24921 ||| 24922 ||| 24923 ||| 24924 ||| 24925 ||| 24926 ||| 24927 ||| 24928 ||| 
2021 ||| attention can reflect syntactic structure (if you let it). ||| 24929 ||| 24930 ||| 24931 ||| 3079 ||| 3080 ||| 21390 ||| 
2017 ||| large-scale categorization of japanese product titles using neural attention models. ||| 24932 ||| 24933 ||| 24934 ||| 24935 ||| 24936 ||| 24937 ||| 
2021 ||| multi-split reversible transformers can enhance neural machine translation. ||| 3170 ||| 24938 ||| 3173 ||| 
2021 ||| conversational question answering over knowledge graphs with transformer and graph attention networks. ||| 14084 ||| 14083 ||| 14085 ||| 14086 ||| 3581 ||| 24939 ||| 
2021 ||| enriching non-autoregressive transformer with syntactic and semantic structures for neural machine translation. ||| 23311 ||| 3891 ||| 14896 ||| 2815 ||| 1094 ||| 
2017 ||| attention modeling for targeted sentiment. ||| 24940 ||| 3289 ||| 
2021 ||| attention-based relational graph convolutional network for target-oriented opinion words extraction. ||| 24941 ||| 24942 ||| 24943 ||| 
2021 ||| t2ner: transformers based transfer learning framework for named entity recognition. ||| 24944 ||| 24945 ||| 
2021 ||| bert meets shapley: extending shap explanations to transformer-based classifiers. ||| 24946 ||| 10213 ||| 10215 ||| 24947 ||| 24948 ||| 
2017 ||| exploring different dimensions of attention for uncertainty detection. ||| 16489 ||| 11716 ||| 11717 ||| 
2021 ||| exploring neural language models via analysis of local and global self-attention spaces. ||| 10213 ||| 24949 ||| 24950 ||| 24948 ||| 24951 ||| 24947 ||| 
2021 ||| t-ner: an all-round python library for transformer-based named entity recognition. ||| 24952 ||| 852 ||| 24953 ||| 
2021 ||| dynamic graph transformer for implicit tag recognition. ||| 24954 ||| 24955 ||| 18035 ||| 18036 ||| 
2021 ||| applying the transformer to character-level transduction. ||| 3484 ||| 3485 ||| 13266 ||| 
2021 ||| have attention heads in bert learned constituency grammar? ||| 24956 ||| 
2021 ||| "killing me" is not a spoiler: spoiler detection model using graph neural networks with dependency relation-aware attention mechanism. ||| 15228 ||| 24957 ||| 9752 ||| 9250 ||| 
2017 ||| neural machine translation with recurrent attention modeling. ||| 24958 ||| 24959 ||| 3944 ||| 24960 ||| 18298 ||| 
2017 ||| structural attention neural networks for improved sentiment analysis. ||| 24961 ||| 3729 ||| 
2021 ||| measuring and improving faithfulness of attention in neural machine translation. ||| 14942 ||| 14943 ||| 14944 ||| 
2021 ||| syntax-bert: improving pre-trained transformers with syntax trees. ||| 22726 ||| 18497 ||| 24962 ||| 7703 ||| 18503 ||| 8160 ||| 18501 ||| 
2021 ||| benchmarking a transformer-free model for ad-hoc retrieval. ||| 15054 ||| 7111 ||| 15055 ||| 
2021 ||| globalizing bert-based transformer architectures for long document summarization. ||| 24963 ||| 24964 ||| 5130 ||| 
2021 ||| grit: generative role-filler transformers for document-level event entity extraction. ||| 4961 ||| 4962 ||| 4963 ||| 
2021 ||| enconter: entity constrained progressive sequence generation via insertion-based transformer. ||| 24965 ||| 24966 ||| 24967 ||| 
2021 ||| proformer: towards on-device lsh projection based transformers. ||| 24968 ||| 24969 ||| 24970 ||| 
2021 ||| changing the mind of transformers for topically-controllable language generation. ||| 24971 ||| 24972 ||| 3325 ||| 24973 ||| 
2021 ||| bert prescriptions to avoid unwanted headaches: a comparison of transformer architectures for adverse drug event detection. ||| 24974 ||| 24975 ||| 8955 ||| 4700 ||| 14233 ||| 
2020 ||| malware classification using attention-based transductive learning network. ||| 24976 ||| 24977 ||| 24978 ||| 24979 ||| 24540 ||| 24980 ||| 
2019 ||| spatial attention mechanism for weakly supervised fire and traffic accident scene classification. ||| 24981 ||| 24982 ||| 24983 ||| 
2021 ||| mldt: multi-task learning with denoising transformer for gait identity and emotion recognition. ||| 24984 ||| 24985 ||| 20835 ||| 
2021 ||| deep-learning-based diagnosis of cassava leaf diseases using vision transformer. ||| 24986 ||| 
2020 ||| malware classification on imbalanced data through self-attention. ||| 24987 ||| 16445 ||| 24988 ||| 24989 ||| 24990 ||| 24991 ||| 24992 ||| 24993 ||| 22567 ||| 
2021 ||| gea-net: global embedded attention neural network for image classification. ||| 24566 ||| 24994 ||| 24995 ||| 24996 ||| 24997 ||| 
2020 ||| pyramid pooling channel attention network for esophageal tissue segmentation on oct images. ||| 24998 ||| 2068 ||| 24999 ||| 25000 ||| 25001 ||| 25002 ||| 
2019 ||| phishing url detection via cnn and attention-based hierarchical rnn. ||| 25003 ||| 25004 ||| 25005 ||| 25006 ||| 
2021 ||| simple online unmanned aerial vehicle tracking with transformer. ||| 1305 ||| 25007 ||| 3558 ||| 11800 ||| 25008 ||| 25009 ||| 
2020 ||| exploit internal structural information for iot malware detection based on hierarchical transformer model. ||| 528 ||| 19250 ||| 25010 ||| 25011 ||| 12773 ||| 
2020 ||| hypergraph attention networks. ||| 25012 ||| 25013 ||| 25014 ||| 25015 ||| 
2019 ||| solving a tool-based interaction task using deep reinforcement learning with visual attention. ||| 4223 ||| 4224 ||| 
2020 ||| can nao robot influence the eye gaze and joint attention of mentally impaired young adults? ||| 25016 ||| 2871 ||| 25017 ||| 2253 ||| 25018 ||| 
2021 ||| turbotransformers: an efficient gpu serving system for transformer models. ||| 25019 ||| 882 ||| 25020 ||| 1921 ||| 
2021 ||| a transformer architecture for stress detection from ecg. ||| 25021 ||| 25022 ||| 25023 ||| 25024 ||| 20089 ||| 
2017 ||| wearable social prosthetics: supporting joint attention during communication with artificial eyes. ||| 25025 ||| 25026 ||| 25027 ||| 
2021 ||| a pilot study using covert visuospatial attention as an eeg-based brain computer interface to enhance ar interaction. ||| 6446 ||| 6448 ||| 6449 ||| 6447 ||| 6450 ||| 6451 ||| 
2020 ||| generative text steganography based on lstm network and attention mechanism with keywords. ||| 25028 ||| 25029 ||| 25030 ||| 
2019 ||| attention based residual-time delay neural network for indian language identification. ||| 25031 ||| 25032 ||| 
2019 ||| multi-head self-attention networks for language identification. ||| 25033 ||| 25031 ||| 12222 ||| 25032 ||| 
2021 ||| evaluation of the transformer architecture for univariate time series forecasting. ||| 25034 ||| 10316 ||| 25035 ||| 25036 ||| 3419 ||| 852 ||| 25037 ||| 25038 ||| 
2019 ||| a novel three-stage power electronic transformer for ac/dc conversion. ||| 17750 ||| 22193 ||| 22190 ||| 22191 ||| 22192 ||| 25039 ||| 2885 ||| 
2019 ||| a multi path feedforward control of load current for three-phase inverter with transformer. ||| 25040 ||| 3675 ||| 25041 ||| 
2019 ||| a global redundancy scheme for medium-voltage modular multilevel converter based solid-state transformer. ||| 22109 ||| 22108 ||| 22106 ||| 22105 ||| 
2017 ||| asymmetrical two-phase induction motor speed controlled by multilevel inverter employing cascaded transformers. ||| 25042 ||| 25043 ||| 25044 ||| 25045 ||| 25046 ||| 
2020 ||| secondary-side center-tapped transformer structure with one-turn secondary coils integrating rectifier for reducing copper loss of forward converter. ||| 25047 ||| 25048 ||| 25049 ||| 25050 ||| 
2021 ||| current and voltage model predictive control for a three-stage smart transformer. ||| 21934 ||| 25051 ||| 25052 ||| 21994 ||| 25053 ||| 
2021 ||| disturbance property of high-frequency transformer model for photovoltaic applications. ||| 25054 ||| 25055 ||| 
2017 ||| series transistor array-based linear ac regulator: role of multiple buck-boost transformers in efficiency improvements. ||| 22071 ||| 21946 ||| 22072 ||| 21947 ||| 
2019 ||| control of solid state transformer based on modular multilevel converters with interconnecting dual active bridges. ||| 22033 ||| 2600 ||| 25056 ||| 852 ||| 22057 ||| 22025 ||| 
2019 ||| improving the diagnosis of incipient faults in power transformers using dissolved gas analysis and multi layer perceptron. ||| 25057 ||| 10353 ||| 
2020 ||| an orthogonal decoupled transformer design for inductive power transfer applications. ||| 25058 ||| 25059 ||| 25060 ||| 25061 ||| 25062 ||| 
2019 ||| advanced power management algorithm in dc microgrid subsystem controlled by smart transformer. ||| 25063 ||| 25064 ||| 25065 ||| 22024 ||| 
2019 ||| novel zcs transformerless high gain dc-dc converters for renewable energy conversion systems. ||| 25066 ||| 25067 ||| 25068 ||| 
2021 ||| disturbance rejection and harmonic mitigation for solid state transformer through passivity based control. ||| 25069 ||| 25070 ||| 25071 ||| 
2021 ||| a novel interleaved transformerless ultra-high step-up dc/dc converter. ||| 25072 ||| 25073 ||| 25074 ||| 25075 ||| 
2019 ||| implementation of fpga and dsp combined algorithms for modular single-phase matrix converter with medium frequency transformer for traction drive application. ||| 25076 ||| 59 ||| 25067 ||| 25068 ||| 25077 ||| 
2021 ||| unsupervised person re-identification with transformer-based network for intelligent surveillance systems. ||| 4354 ||| 4353 ||| 
2021 ||| vt-adl: a vision transformer network for image anomaly detection and localization. ||| 25078 ||| 25079 ||| 25080 ||| 25081 ||| 4704 ||| 
2020 ||| quadratic dc/dc converter with autotransformer at the output side. ||| 25082 ||| 25083 ||| 25084 ||| 
2021 ||| a novel framework of cnn for image super-resolution based on attention module. ||| 25085 ||| 17270 ||| 
2021 ||| modeling of cross-circulating currents in a mmc with parallel connected submodules in solid state transformers. ||| 25086 ||| 22055 ||| 22034 ||| 22025 ||| 
2017 ||| enhancing the provision of ancillary services from storage systems using smart transformer and smart meters. ||| 25087 ||| 25088 ||| 25089 ||| 16201 ||| 10868 ||| 
2021 ||| analysis and validation of variable transformers. ||| 25090 ||| 25091 ||| 25050 ||| 
2017 ||| design of transformer load monitoring systems by utilizing smart meter data. ||| 25092 ||| 25093 ||| 25094 ||| 25095 ||| 25096 ||| 
2017 ||| study on operating performance of transformer and scaling model with dc bias. ||| 25097 ||| 25098 ||| 25099 ||| 25100 ||| 25101 ||| 25102 ||| 
2018 ||| a smart transformer-rectifier unit for the more electric aircraft. ||| 21934 ||| 25103 ||| 22047 ||| 10868 ||| 
2019 ||| multi-winding transformer based high resolution power flow controller. ||| 25104 ||| 25105 ||| 25106 ||| 
2019 ||| on design of 60 ghz solid-state transformers modeled as coupled bitter coils. ||| 4279 ||| 
2021 ||| an efficient implementation of fpga-based object detection using multi-scale attention. ||| 25107 ||| 25108 ||| 25109 ||| 25110 ||| 
2018 ||| impact assessment of electric vehicle charging on distribution transformers including state-of-charge. ||| 25111 ||| 25112 ||| 25113 ||| 
2018 ||| a 2: 8 ghz to 12: 8 ghz uwb lna using transformer wide-band input matching for ir-uwb radar applications. ||| 25114 ||| 25115 ||| 4330 ||| 
2017 ||| a wideband simplified transformer-based vco with digital amplitude calibration. ||| 25116 ||| 25117 ||| 10406 ||| 25118 ||| 25119 ||| 
2020 ||| iot device auto-tagging using transformers. ||| 25120 ||| 25121 ||| 25122 ||| 25123 ||| 
2018 ||| jellys: towards a videogame that trains rhythm and visual attention for dyslexia. ||| 25124 ||| 25125 ||| 25126 ||| 3882 ||| 25127 ||| 25128 ||| 25129 ||| 
2020 ||| transformer puf : a highly flexible configurable ro puf based on fpga. ||| 25130 ||| 25131 ||| 1723 ||| 25132 ||| 25133 ||| 25134 ||| 
2018 ||| simulation analysis of railway co-phase power supply system based on inequilateral scott connection transformer. ||| 25135 ||| 25136 ||| 
2019 ||| idmn: a two-pass attention mechanism in dynamic memory network. ||| 17179 ||| 25137 ||| 25138 ||| 5890 ||| 
2020 ||| a cross-cultural study on information architecture: culture differences on attention allocation to web components. ||| 25139 ||| 25140 ||| 25141 ||| 25142 ||| 
2021 ||| attracting attention in online health forums: studies of r/alzheimers and r/dementia. ||| 25143 ||| 25144 ||| 25145 ||| 
2022 ||| exploiting transformer-based multitask learning for the detection of media bias in news articles. ||| 25146 ||| 25147 ||| 25148 ||| 25149 ||| 25150 ||| 25151 ||| 24943 ||| 25152 ||| 
2021 ||| attention: to better stand on the shoulders of giants. ||| 25153 ||| 25154 ||| 9472 ||| 2333 ||| 1266 ||| 
2021 ||| syntax-aware transformers for neural machine translation: the case of text to sign gloss translation. ||| 25155 ||| 11933 ||| 25156 ||| 10282 ||| 
2020 ||| self-attention network for next poi recommendation. ||| 25157 ||| 659 ||| 658 ||| 11122 ||| 654 ||| 24830 ||| 11116 ||| 6475 ||| 
2021 ||| co-authorship prediction based on temporal graph attention. ||| 25158 ||| 15367 ||| 25159 ||| 1207 ||| 
2021 ||| grham: towards group recommendation using hierarchical attention mechanism. ||| 22926 ||| 22925 ||| 22928 ||| 17827 ||| 22929 ||| 
2018 ||| improving clinical named entity recognition with global neural attention. ||| 25160 ||| 25161 ||| 11185 ||| 
2021 ||| sqkt: a student attention-based and question-aware model for knowledge tracing. ||| 25162 ||| 25163 ||| 25164 ||| 25159 ||| 
2020 ||| aligning users across social networks via intra and inter attentions. ||| 25165 ||| 25166 ||| 7466 ||| 
2020 ||| fhan: feature-level hierarchical attention network for group event recommendation. ||| 25167 ||| 25168 ||| 25169 ||| 25170 ||| 
2020 ||| predicting human mobility with self-attention and feature interaction. ||| 25171 ||| 25172 ||| 8912 ||| 23308 ||| 1301 ||| 
2020 ||| turn-level recurrence self-attention for joint dialogue action prediction and response generation. ||| 25173 ||| 8466 ||| 25174 ||| 25175 ||| 8467 ||| 
2019 ||| transformer and multi-scale convolution for target-oriented sentiment analysis. ||| 25176 ||| 25177 ||| 25178 ||| 986 ||| 25179 ||| 
2019 ||| medical treatment migration prediction in healthcare via attention-based bidirectional gru. ||| 7087 ||| 1559 ||| 1558 ||| 1557 ||| 
2021 ||| self-supervised learning for semantic sentence matching with dense transformer inference network. ||| 25180 ||| 701 ||| 25181 ||| 704 ||| 705 ||| 
2020 ||| knowledge graph attention network enhanced sequential recommendation. ||| 25182 ||| 659 ||| 658 ||| 11122 ||| 660 ||| 24830 ||| 11116 ||| 6475 ||| 
2021 ||| multi-interest network based on double attention for click-through rate prediction. ||| 25183 ||| 25184 ||| 25185 ||| 
2020 ||| global and local attention embedding network for few-shot fine-grained image classification. ||| 25186 ||| 1004 ||| 25187 ||| 
2020 ||| graph attentive network for region recommendation with poi- and roi-level attention. ||| 25188 ||| 25189 ||| 1979 ||| 1224 ||| 
2019 ||| enhancing joint entity and relation extraction with language modeling and hierarchical attention. ||| 413 ||| 411 ||| 25190 ||| 25191 ||| 
2019 ||| boundary detector encoder and decoder with soft attention for video captioning. ||| 25192 ||| 25193 ||| 9576 ||| 
2018 ||| attention-based recurrent neural network for sequence labeling. ||| 25194 ||| 5463 ||| 22746 ||| 23842 ||| 
2021 ||| a graph attention network model for gmv forecast on online shopping festival. ||| 25195 ||| 18790 ||| 883 ||| 25196 ||| 11139 ||| 25197 ||| 1215 ||| 25198 ||| 1086 ||| 25199 ||| 
2020 ||| densely-connected transformer with co-attentive information for matching text sequences. ||| 25200 ||| 25201 ||| 18181 ||| 782 ||| 12333 ||| 
2020 ||| natural answer generation via graph transformer. ||| 5199 ||| 25202 ||| 25203 ||| 
2018 ||| neural indexes of attention extracted from eeg correlate with elderly reaction time in response to an attentional task. ||| 24211 ||| 24212 ||| 24215 ||| 10077 ||| 
2021 ||| research on transformer fault diagnosis method based on neighborhood rough set and grey wolf algorithm optimized support vector machine. ||| 25204 ||| 4784 ||| 25205 ||| 
2021 ||| multi-attention parallel cnn-gru fault diagnosis method for rolling bearing. ||| 25206 ||| 1323 ||| 6514 ||| 25207 ||| 25208 ||| 
2021 ||| remaining useful life prediction of tool with bigru-attention and improved particle filter. ||| 1329 ||| 7826 ||| 25209 ||| 25210 ||| 20781 ||| 
2021 ||| a bearing remaining useful life prediction method based on inception-resnet module and attention mechanism. ||| 25211 ||| 25212 ||| 6754 ||| 25213 ||| 25214 ||| 25215 ||| 
2021 ||| life prediction of rolling bearing using temporal convolution network and attention mechanism. ||| 25216 ||| 25217 ||| 25218 ||| 170 ||| 25219 ||| 
2019 ||| -harmonic-shorting four-way transformer and integrated thermal sensors. ||| 25220 ||| 25221 ||| 25222 ||| 25223 ||| 25224 ||| 25225 ||| 
2019 ||| a 0.1-to-0.2v transformer-based switched-mode folded dco in 22nm fdsoi with active step-down impedance achieving 197dbc/hz peak fom and 40mhz/v frequency pushing. ||| 25226 ||| 22390 ||| 
2018 ||| an 82-to-108ghz -181db-fomt adpll employing a dco with split-transformer and dual-path switched-capacitor ladder and a clock-skew-sampling delta-sigma tdc. ||| 25227 ||| 25228 ||| 
2022 ||| a 28nm 27.5tops/w approximate-computing-based transformer processor with asymptotic sparsity speculating and out-of-order computing. ||| 602 ||| 25229 ||| 25230 ||| 25231 ||| 6931 ||| 25232 ||| 25233 ||| 7725 ||| 7726 ||| 7728 ||| 7729 ||| 
2019 ||| a 60ghz cmos power amplifier with cascaded asymmetric distributed-active-transformer achieving watt-level peak output power with 20.8% pae and supporting 2gsym/s 64-qam modulation. ||| 25234 ||| 25235 ||| 300 ||| 
2018 ||| a compact dual-band digital doherty power amplifier using parallel-combining transformer for cellular nb-iot applications. ||| 25236 ||| 25237 ||| 25238 ||| 25239 ||| 25240 ||| 22422 ||| 
2021 ||| a 1.25w 46.5%-peak-efficiency transformer-in-package isolated dc-dc converter using glass-based fan-out wafer-level packaging achieving 50mw/mm2 power density. ||| 25241 ||| 25242 ||| 25243 ||| 25244 ||| 25245 ||| 25246 ||| 124 ||| 7087 ||| 
2022 ||| a 28ghz compact 3-way transformer-based parallel-series doherty power amplifier with 20.4%/14.2% pae at 6-/12-db power back-off and 25.5dbm psat in 55nm bulk cmos. ||| 25247 ||| 25248 ||| 5761 ||| 25249 ||| 
2021 ||| a 60ghz 186.5dbc/hz fom quad-core fundamental vco using circular triple-coupled transformer with no mode ambiguity in 65nm cmos. ||| 5772 ||| 5773 ||| 5771 ||| 5774 ||| 5775 ||| 
2021 ||| 270-to-300ghz double-balanced parametric upconverter using asymmetric mos varactors and a power-splitting- transformer hybrid in 65nm cmos. ||| 25250 ||| 25251 ||| 25252 ||| 
2018 ||| a 28ghz 41%-pae linear cmos power amplifier using a transformer-based am-pm distortion-correction technique for 5g phased arrays. ||| 25253 ||| 25254 ||| 25255 ||| 25256 ||| 25257 ||| 25258 ||| 
2019 |||  loop antenna and transformer-boost power oscillator. ||| 25259 ||| 585 ||| 25260 ||| 25261 ||| 25262 ||| 
2022 ||| j/token full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes. ||| 21682 ||| 25263 ||| 25264 ||| 25265 ||| 11307 ||| 18023 ||| 7726 ||| 7728 ||| 12120 ||| 7729 ||| 
2020 ||| 24.5 a 15b quadrature digital power amplifier with transformer-based complex-domain power-efficiency enhancement. ||| 25266 ||| 25236 ||| 25238 ||| 25237 ||| 25267 ||| 25268 ||| 22422 ||| 
2019 ||| a broadband switched-transformer digital power amplifier for deep back-off efficiency enhancement. ||| 25237 ||| 5935 ||| 25236 ||| 25240 ||| 25268 ||| 22422 ||| 
2018 ||| a 0.3ppm dual-resonance transformer-based drift-cancelling reference-free magnetic sensor for biosensing applications. ||| 25269 ||| 25270 ||| 25271 ||| 25272 ||| 
2019 ||| 1w isolated power transfer system using fully integrated magnetic-core transformer. ||| 5765 ||| 25273 ||| 5764 ||| 25274 ||| 12417 ||| 25275 ||| 5766 ||| 
2021 ||| 26.5 a watt-level quadrature switched/floated-capacitor power amplifier with back-off efficiency enhancement in complex domain using reconfigurable self-coupling canceling transformer. ||| 25276 ||| 25277 ||| 25278 ||| 
2021 |||  soc for iot devices with 18ms noise-robust speech-to-text latency via bayesian speech denoising and attention-based sequence-to-sequence dnn speech recognition in 16nm finfet. ||| 25279 ||| 25280 ||| 25281 ||| 25282 ||| 25283 ||| 25284 ||| 25285 ||| 4962 ||| 25286 ||| 25287 ||| 
2021 ||| a 1.75db-nf 25mw 5ghz transformer-based noise- cancelling cmos receiver front-end. ||| 25288 ||| 25289 ||| 10777 ||| 17772 ||| 6474 ||| 15177 ||| 25290 ||| 25291 ||| 25292 ||| 25293 ||| 
2020 ||| explainable and adaptable augmentation in knowledge attention network for multi-agent deep reinforcement learning systems. ||| 25294 ||| 25295 ||| 
2018 ||| emotion recognition from human behaviors using attention model. ||| 25296 ||| 25297 ||| 25298 ||| 25299 ||| 
2019 ||| poster abstract: a wearable diagnostic assessment system for attention deficit hyperactivity disorder. ||| 6161 ||| 25300 ||| 6163 ||| 25301 ||| 6162 ||| 13463 ||| 
2021 ||| sensor-based human activity recognition for elderly in-patients with a luong self-attention network. ||| 21526 ||| 25302 ||| 25303 ||| 25304 ||| 25305 ||| 
2021 ||| a transformer-based framework for multivariate time series representation learning. ||| 1654 ||| 25306 ||| 25307 ||| 25308 ||| 1653 ||| 
2019 ||| multiple relational attention network for multi-task learning. ||| 25309 ||| 25310 ||| 18143 ||| 3476 ||| 23306 ||| 9592 ||| 
2018 ||| r-vqa: learning visual relation facts with semantic attention for visual question answering. ||| 17712 ||| 18515 ||| 781 ||| 3706 ||| 3480 ||| 8907 ||| 
2020 ||| attentional multi-graph convolutional network for regional economy prediction with open migration data. ||| 9069 ||| 2969 ||| 25311 ||| 
2021 ||| accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts. ||| 9732 ||| 25312 ||| 25313 ||| 9733 ||| 
2018 ||| stamp: short-term attention/memory priority model for session-based recommendation. ||| 8922 ||| 8924 ||| 12516 ||| 8923 ||| 
2021 ||| intention-aware heterogeneous graph attention networks for fraud transactions detection. ||| 25314 ||| 21803 ||| 3199 ||| 25315 ||| 8880 ||| 2792 ||| 
2019 ||| conversion prediction using multi-task conditional attention networks to support the creation of effective ad creatives. ||| 25316 ||| 25317 ||| 25318 ||| 
2020 ||| attention realignment and pseudo-labelling for interpretable cross-lingual classification of crisis tweets. ||| 7733 ||| 15802 ||| 359 ||| 
2021 ||| rapt: pre-training of time-aware transformer for learning robust healthcare representation. ||| 25319 ||| 25320 ||| 3504 ||| 25321 ||| 
2021 ||| tuta: tree-based transformers for generally structured table pre-training. ||| 25322 ||| 18742 ||| 25323 ||| 2383 ||| 5985 ||| 25324 ||| 8893 ||| 
2019 ||| understanding consumer journey using attention based recurrent neural networks. ||| 9683 ||| 25325 ||| 25326 ||| 25327 ||| 25328 ||| 
2017 ||| a context-aware attention network for interactive question answering. ||| 15990 ||| 23938 ||| 25329 ||| 23935 ||| 
2018 ||| graph classification using structural attention. ||| 1193 ||| 1194 ||| 1195 ||| 
2019 ||| real-time attention based look-alike model for recommender system. ||| 25330 ||| 25331 ||| 181 ||| 9569 ||| 
2019 ||| daml: dual attention mutual learning between ratings and reviews for item recommendation. ||| 25332 ||| 4807 ||| 17757 ||| 25333 ||| 20871 ||| 
2020 ||| attention based multi-modal new product sales time-series forecasting. ||| 25334 ||| 25335 ||| 25336 ||| 25337 ||| 25338 ||| 25339 ||| 
2021 ||| hgamn: heterogeneous graph attention matching network for multilingual poi retrieval at baidu maps. ||| 25340 ||| 3417 ||| 13859 ||| 25341 ||| 25342 ||| 5294 ||| 25343 ||| 
2019 ||| akupm: attention-enhanced knowledge-aware user preference model for recommendation. ||| 25344 ||| 25345 ||| 25346 ||| 25347 ||| 
2021 ||| heterogeneous temporal graph transformer: an intelligent system for evolving android malware detection. ||| 25348 ||| 25349 ||| 25350 ||| 9022 ||| 25351 ||| 25352 ||| 25353 ||| 25354 ||| 
2020 ||| combo-attention network for baidu video advertising. ||| 1327 ||| 208 ||| 1329 ||| 1330 ||| 25355 ||| 977 ||| 
2017 ||| dipole: diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. ||| 1071 ||| 1298 ||| 1299 ||| 1296 ||| 23368 ||| 930 ||| 
2020 ||| hitanet: hierarchical time-aware attention networks for risk prediction on electronic health records. ||| 1069 ||| 1068 ||| 1070 ||| 1071 ||| 
2018 ||| multi-pointer co-attention networks for recommendation. ||| 1398 ||| 9261 ||| 9016 ||| 
2020 ||| deterrent: knowledge guided graph attention network for detecting healthcare misinformation. ||| 25356 ||| 25357 ||| 25358 ||| 1071 ||| 25359 ||| 25360 ||| 
2019 ||| npa: neural news recommendation with personalized attention. ||| 3754 ||| 3755 ||| 25361 ||| 2489 ||| 2795 ||| 9574 ||| 
2019 ||| social recommendation with optimal limited attention. ||| 398 ||| 1088 ||| 25362 ||| 
2019 ||| kgat: knowledge graph attention network for recommendation. ||| 1894 ||| 1063 ||| 3603 ||| 6736 ||| 3605 ||| 
2020 ||| hierarchical attention propagation for healthcare representation learning. ||| 25363 ||| 25364 ||| 25365 ||| 5348 ||| 
2018 ||| detection of paroxysmal atrial fibrillation using attention-based bidirectional recurrent neural networks. ||| 25366 ||| 25367 ||| 25368 ||| 25369 ||| 
2020 ||| recurrent networks for guided multi-attention classification. ||| 9767 ||| 1195 ||| 25370 ||| 1193 ||| 5787 ||| 9768 ||| 
2020 ||| graph attention networks over edge content-based channels. ||| 25371 ||| 8883 ||| 
2020 ||| preserving dynamic attention for long-term spatial-temporal prediction. ||| 25372 ||| 25373 ||| 1438 ||| 7854 ||| 25374 ||| 
2021 ||| m6: multi-modality-to-multi-modality multitask mega-transformer for unified pretraining. ||| 25375 ||| 25376 ||| 25377 ||| 2482 ||| 25378 ||| 5845 ||| 1245 ||| 7030 ||| 8916 ||| 
2019 ||| buying or browsing?: predicting real-time purchasing intent using attention-based deep network with multiple behavior. ||| 25379 ||| 25380 ||| 25381 ||| 25382 ||| 17868 ||| 12333 ||| 
2019 ||| alphastock: a buying-winners-and-selling-losers investment strategy using interpretable deep reinforcement attention networks. ||| 25320 ||| 1420 ||| 25383 ||| 4327 ||| 4214 ||| 
2017 ||| learning to generate rock descriptions from multivariate well logs with hierarchical attention. ||| 25384 ||| 25385 ||| 25386 ||| 25387 ||| 25388 ||| 25389 ||| 25390 ||| 
2021 ||| why attentions may not be interpretable? ||| 25391 ||| 25392 ||| 25393 ||| 1705 ||| 9598 ||| 2355 ||| 
2020 ||| a dual heterogeneous graph attention network to improve long-tail performance for shop search in e-commerce. ||| 25394 ||| 25194 ||| 1400 ||| 25395 ||| 25396 ||| 22923 ||| 9061 ||| 
2019 ||| hats: a hierarchical sequence-attention framework for inductive set-of-sets embeddings. ||| 15098 ||| 25397 ||| 25398 ||| 15101 ||| 
2018 ||| multi-cast attention networks. ||| 1398 ||| 9015 ||| 9016 ||| 
2017 ||| gram: graph-based attention model for healthcare representation learning. ||| 18102 ||| 25399 ||| 25400 ||| 25401 ||| 23323 ||| 
2020 ||| attention and memory-augmented networks for dual-view sequential learning. ||| 18589 ||| 3691 ||| 5192 ||| 25402 ||| 
2019 ||| graph representation learning via hard and channel-wise attention networks. ||| 25403 ||| 23491 ||| 
2017 ||| dynamic attention deep model for article recommendation by learning human editors' demonstration. ||| 21166 ||| 25404 ||| 1217 ||| 25405 ||| 1219 ||| 1223 ||| 1224 ||| 
2019 ||| multi-horizon time series forecasting with temporal attention learning. ||| 17904 ||| 25406 ||| 16855 ||| 25407 ||| 1460 ||| 25408 ||| 8976 ||| 19299 ||| 17851 ||| 9143 ||| 
2020 ||| rationale-based human-in-the-loop via supervised attention. ||| 25409 ||| 25410 ||| 25411 ||| 18032 ||| 
2020 ||| kronecker attention networks. ||| 25403 ||| 25412 ||| 23491 ||| 
2020 ||| taming pretrained transformers for extreme multi-label text classification. ||| 25413 ||| 22825 ||| 25414 ||| 2622 ||| 22826 ||| 
2018 ||| leveraging meta-path based context for top- n recommendation with a neural co-attention model. ||| 11139 ||| 1373 ||| 3504 ||| 1094 ||| 
2019 ||| mvan: multi-view attention networks for real money trading detection in online games. ||| 25415 ||| 25416 ||| 25417 ||| 25418 ||| 25419 ||| 25420 ||| 1084 ||| 
2021 ||| triplet attention: rethinking the similarity in transformers. ||| 18004 ||| 215 ||| 18006 ||| 3364 ||| 18005 ||| 
2020 ||| constgat: contextual spatial-temporal graph attention network for travel time estimation at baidu maps. ||| 25421 ||| 25340 ||| 1704 ||| 25422 ||| 25423 ||| 3417 ||| 
2021 ||| attention guidance technique using visual subliminal cues and its application on videos. ||| 25424 ||| 25425 ||| 
2020 ||| joint attention for automated video editing. ||| 7543 ||| 7544 ||| 7545 ||| 7546 ||| 7547 ||| 
2019 ||| understanding user attention in vr using gaze controlled games. ||| 25426 ||| 25427 ||| 
2017 ||| two-step joint attention network for visual question answering. ||| 18972 ||| 1151 ||| 16927 ||| 22380 ||| 25428 ||| 
2021 ||| hybrid attention cascaded u-net for building extraction from aerial images. ||| 6895 ||| 6896 ||| 6897 ||| 6898 ||| 6901 ||| 6900 ||| 6899 ||| 
2020 ||| recurrent factorization machine with self-attention for time-aware service recommendation. ||| 25429 ||| 2739 ||| 25430 ||| 
2019 ||| qos attributes prediction with attention-based lstm network for mobile services. ||| 13930 ||| 15240 ||| 25431 ||| 
2019 ||| mining aspects in online comments with attention and bi-lstm. ||| 25432 ||| 25433 ||| 25434 ||| 25435 ||| 
2020 ||| joint self-attention and multi-embeddings for chinese named entity recognition. ||| 25436 ||| 25437 ||| 25438 ||| 6073 ||| 
2018 ||| emotionx-area66: predicting emotions in dialogues using hierarchical attention network with sequence labeling. ||| 18567 ||| 18568 ||| 18569 ||| 
2021 ||| self-contextualized attention for abusive language identification. ||| 23164 ||| 23165 ||| 23166 ||| 23167 ||| 21540 ||| 11932 ||| 11933 ||| 
2018 ||| emotionx-jtml: detecting emotions with attention. ||| 25439 ||| 
2018 ||| auditory attention, implications for serious game design. ||| 25440 ||| 25441 ||| 25442 ||| 12581 ||| 25443 ||| 3882 ||| 25444 ||| 
2020 ||| how do students and researchers behave and feel while playing the "online attention game"?: an exploratory study on digital identity education. ||| 25445 ||| 25446 ||| 25447 ||| 25448 ||| 25449 ||| 25450 ||| 
2018 ||| cran: a hybrid cnn-rnn attention-based model for text classification. ||| 25379 ||| 3282 ||| 3279 ||| 12273 ||| 12333 ||| 
2021 ||| fake speech detection using residual network with transformer encoder. ||| 758 ||| 25451 ||| 25452 ||| 
2020 ||| left ventricular myocardium segmentation in coronary computed tomography angiography using 3d deep attention u-net. ||| 24693 ||| 24694 ||| 24695 ||| 25453 ||| 24696 ||| 24697 ||| 24699 ||| 24700 ||| 
2020 ||| classification of attention-deficit/hyperactivity disorder from resting-state functional mri with mutual connectivity analysis. ||| 25454 ||| 25455 ||| 25456 ||| 25457 ||| 3831 ||| 
2020 ||| the impact of a social robot public speaker on audience attention. ||| 25458 ||| 25459 ||| 8441 ||| 25460 ||| 25461 ||| 
2017 ||| proposal of a model to determine the attention target for an agent in group discussion with non-verbal features. ||| 25462 ||| 25463 ||| 336 ||| 25464 ||| 25465 ||| 25466 ||| 
2019 ||| a speech promotion system by using embodied entrainment objects of spoken words and a listener character for joint attention. ||| 25467 ||| 25468 ||| 25469 ||| 
2019 ||| acting as if being aware of visitors' attention strengthens a robotic salesperson's social presence. ||| 25470 ||| 25471 ||| 25472 ||| 25473 ||| 25474 ||| 25475 ||| 
2021 ||| enhancing sense of attention from a communication robot by drawing the user's face on its thought bubble in the video conferencing system. ||| 25476 ||| 21785 ||| 25477 ||| 20474 ||| 
2019 ||| gender differences in allocation of attention and read time in an educational history game. ||| 25478 ||| 25479 ||| 25480 ||| 25481 ||| 
2018 ||| attracting attention and changing behavior toward wall advertisements with a walking virtual agent. ||| 15937 ||| 25482 ||| 15938 ||| 
2017 ||| multi-level attention-based neural networks for distant supervised relation extraction. ||| 8872 ||| 8873 ||| 25483 ||| 8875 ||| 
2017 ||| deep semantic indexing using convolutional localization network with region-based visual attention for image database. ||| 18990 ||| 11466 ||| 2484 ||| 18992 ||| 13410 ||| 1040 ||| 
2017 ||| jointly learning attentions with semantic cross-modal correlation for visual question answering. ||| 25484 ||| 1039 ||| 9576 ||| 9579 ||| 1040 ||| 
2021 ||| contrastively learning visual attention as affordance cues from demonstrations for robotic grasping. ||| 25485 ||| 25486 ||| 25487 ||| 
2019 ||| ronet: real-time range-only indoor localization via stacked bidirectional lstm with residual attention. ||| 25488 ||| 25489 ||| 25490 ||| 
2020 ||| self-supervised attention learning for depth and ego-motion estimation. ||| 25491 ||| 25492 ||| 
2021 ||| ptt: point-track-transformer module for 3d single object tracking in point clouds. ||| 25493 ||| 25494 ||| 4236 ||| 25495 ||| 
2018 ||| attention-aware cross-modal cross-level fusion network for rgb-d salient object detection. ||| 2424 ||| 25496 ||| 4530 ||| 
2019 ||| goal-directed behavior under variational predictive coding: dynamic organization of visual attention and working memory. ||| 25497 ||| 25498 ||| 25499 ||| 
2020 ||| few-shot relation learning with attention for eeg-based motor imagery classification. ||| 25500 ||| 25501 ||| 25502 ||| 25503 ||| 
2021 ||| latent attention augmentation for robust autonomous driving policies. ||| 25504 ||| 25505 ||| 25506 ||| 25507 ||| 25508 ||| 
2017 ||| measurement and prediction of situation awareness in human-robot interaction based on a framework of probabilistic attention. ||| 17061 ||| 25509 ||| 25510 ||| 25511 ||| 25512 ||| 25513 ||| 17060 ||| 
2021 ||| transformer-based deep imitation learning for dual-arm robot manipulation. ||| 25514 ||| 4999 ||| 5000 ||| 
2019 ||| a deep learning approach for multi-view engagement estimation of children in a child-robot joint attention task. ||| 25515 ||| 8810 ||| 7929 ||| 25516 ||| 25517 ||| 7930 ||| 
2021 ||| joint intention and trajectory prediction based on transformer. ||| 25518 ||| 6599 ||| 7662 ||| 6598 ||| 25519 ||| 
2021 ||| trajectory-constrained deep latent visual attention for improved local planning in presence of heterogeneous terrain. ||| 25520 ||| 25521 ||| 25507 ||| 25508 ||| 
2018 ||| calibnet: geometrically supervised extrinsic calibration using 3d spatial transformer networks. ||| 25522 ||| 25523 ||| 25524 ||| 25525 ||| 
2021 ||| feanet: feature-enhanced attention network for rgb-thermal real-time semantic segmentation. ||| 25526 ||| 25527 ||| 25528 ||| 25529 ||| 25530 ||| 5519 ||| 25531 ||| 25532 ||| 25533 ||| 21847 ||| 
2021 ||| attention augmented convlstm for environment prediction. ||| 25534 ||| 25535 ||| 25536 ||| 
2020 ||| probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles. ||| 25537 ||| 917 ||| 25538 ||| 8660 ||| 
2021 ||| self attention guided depth completion using rgb and sparse lidar point clouds. ||| 7302 ||| 7287 ||| 
2021 ||| multi-scale aggregation with self-attention network for modeling electrical motor dynamics. ||| 25539 ||| 11292 ||| 25540 ||| 
2020 ||| learning accurate and human-like driving using semantic maps and attention. ||| 25541 ||| 25542 ||| 25543 ||| 25544 ||| 7814 ||| 
2021 ||| local memory attention for fast video semantic segmentation. ||| 25545 ||| 25546 ||| 7814 ||| 7815 ||| 
2019 ||| attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. ||| 19010 ||| 19011 ||| 19012 ||| 7869 ||| 19013 ||| 19014 ||| 
2020 ||| diagnose like a clinician: third-order attention guided lesion amplification network for wce image classification. ||| 16744 ||| 25547 ||| 21829 ||| 
2020 ||| dr-spaam: a spatial-attention and auto-regressive model for person detection in 2d range data. ||| 25548 ||| 25549 ||| 25550 ||| 
2021 ||| re-attention is all you need: memory-efficient scene text detection via re-attention on uncertain regions. ||| 25551 ||| 25552 ||| 25553 ||| 1819 ||| 1820 ||| 
2020 ||| hamlet: a hierarchical multimodal attention-based human activity recognition algorithm. ||| 25554 ||| 25555 ||| 
2021 ||| design of an ssvep-based bci stimuli system for attention-based robot navigation in robotic telepresence. ||| 25556 ||| 25557 ||| 16933 ||| 21844 ||| 21846 ||| 21848 ||| 
2018 ||| tssd: temporal single-shot detector based on attention and lstm. ||| 5832 ||| 25558 ||| 25559 ||| 
2020 ||| dynamic attention-based visual odometry. ||| 18782 ||| 18783 ||| 18784 ||| 25560 ||| 25561 ||| 18785 ||| 
2021 ||| marine autonomous navigation for biomimetic underwater robots based on deep stereo attention network. ||| 25562 ||| 25558 ||| 8349 ||| 25563 ||| 25559 ||| 
2017 ||| attentional masking for pre-trained deep networks. ||| 25564 ||| 25565 ||| 3882 ||| 
2019 ||| responsive joint attention in human-robot interaction. ||| 3369 ||| 25566 ||| 21672 ||| 25567 ||| 25568 ||| 21675 ||| 
2019 ||| local pose optimization with an attention-based neural network. ||| 25569 ||| 25570 ||| 6479 ||| 16446 ||| 19538 ||| 25571 ||| 
2020 ||| lane-attention: predicting vehicles' moving trajectories by learning their attention over lanes. ||| 25572 ||| 25573 ||| 25574 ||| 25575 ||| 25576 ||| 25577 ||| 25578 ||| 
2018 ||| policy shaping with supervisory attention driven exploration. ||| 3912 ||| 3914 ||| 3916 ||| 
2020 ||| towards understanding and inferring the crowd: guided second order attention networks and re-identification for multi-object tracking. ||| 25579 ||| 25580 ||| 25581 ||| 12273 ||| 
2021 ||| pctma-net: point cloud transformer with morphing atlas-based point generation network for dense point cloud completion. ||| 25582 ||| 25583 ||| 25584 ||| 14153 ||| 
2020 ||| planning on the fast lane: learning to interact using attention mechanisms in path integral inverse reinforcement learning. ||| 25585 ||| 1204 ||| 25586 ||| 25587 ||| 25588 ||| 23158 ||| 
2020 ||| spatio-temporal attention model for tactile texture recognition. ||| 25589 ||| 17822 ||| 14952 ||| 25590 ||| 
2021 ||| stereo waterdrop removal with row-wise dilated attention. ||| 25591 ||| 25592 ||| 23917 ||| 2284 ||| 
2021 ||| siamapn++: siamese attentional aggregation network for real-time uav tracking. ||| 2604 ||| 2605 ||| 2606 ||| 2607 ||| 2608 ||| 
2020 ||| end-to-end contextual perception and prediction with interaction transformer. ||| 25593 ||| 10875 ||| 21776 ||| 21775 ||| 9220 ||| 25594 ||| 9239 ||| 
2018 ||| i can see your aim: estimating user attention from gaze for handheld robot collaboration. ||| 25595 ||| 18921 ||| 
2020 ||| towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters. ||| 25596 ||| 2605 ||| 25597 ||| 2608 ||| 19508 ||| 
2020 ||| italian transformers under the linguistic lens. ||| 9834 ||| 25598 ||| 25599 ||| 9836 ||| 25600 ||| 
2020 ||| exploring attention in a multimodal corpus of guided tours. ||| 9835 ||| 25601 ||| 9836 ||| 
2019 ||| deep bidirectional transformers for italian question answering. ||| 10939 ||| 25602 ||| 10940 ||| 
2021 ||| tackling italian university assessment tests with transformer-based language models. ||| 25603 ||| 25604 ||| 25605 ||| 
2019 ||| the impact of self-interaction attention on the extraction of drug-drug interactions. ||| 9820 ||| 24435 ||| 9822 ||| 9823 ||| 
2018 ||| multi-source transformer for automatic post-editing. ||| 21398 ||| 21399 ||| 14627 ||| 14628 ||| 
2020 ||| cross-language transformer adaptation for frequently asked questions. ||| 25606 ||| 25607 ||| 3374 ||| 25608 ||| 25609 ||| 25610 ||| 
2017 ||| a reconfigurable analog baseband transformer for multistandard applications in 14nm finfet cmos. ||| 25611 ||| 25612 ||| 25613 ||| 25614 ||| 25615 ||| 25616 ||| 25617 ||| 25618 ||| 
2021 ||| a 196.2 dbc/hz fomt 16.8-to-21.6 ghz class-f23 vco with transformer-based optimal q-factor tank in 65-nm cmos. ||| 25619 ||| 25620 ||| 25621 ||| 
2021 ||| a 7.9-14.3ghz -243.3db fomt sub-sampling pll with transformer-based dual-mode vco in 40nm cmos. ||| 25622 ||| 25623 ||| 25239 ||| 25624 ||| 8862 ||| 25268 ||| 
2020 ||| attention models for pm2.5 prediction. ||| 25625 ||| 25626 ||| 7570 ||| 
2017 ||| feature engineering and classification models for partial discharge events in power transformers. ||| 25627 ||| 25628 ||| 25629 ||| 25630 ||| 
2019 ||| diagnosing and classifying the fault of transformer with deep belief network. ||| 25631 ||| 13946 ||| 25632 ||| 25633 ||| 
2017 ||| mining association rules from multidimensional transformer defect records. ||| 208 ||| 25634 ||| 25635 ||| 767 ||| 8220 ||| 
2018 ||| videogame-based case studies for improving communication and attention in children with asd. ||| 25636 ||| 25637 ||| 25638 ||| 25639 ||| 25640 ||| 25641 ||| 11005 ||| 
2020 ||| sentiment analysis of online reviews with a hierarchical attention network. ||| 1245 ||| 25642 ||| 
2020 ||| data-sparsity service discovery using enriched neural topic model and attentional bi-lstm. ||| 25643 ||| 8838 ||| 8349 ||| 
2020 ||| collaborative denoising graph attention autoencoders for social recommendation. ||| 6248 ||| 6249 ||| 1976 ||| 25644 ||| 
2019 ||| a convolutional neural network pruning method based on attention mechanism. ||| 25645 ||| 25646 ||| 7775 ||| 
2020 ||| deep graph attention neural network for click-through rate prediction. ||| 25647 ||| 218 ||| 
2020 ||| a combined model for extractive and abstractive summarization based on transformer model. ||| 189 ||| 19494 ||| 
2019 ||| acnet: attention-based convolution network with additional discriminative features for dcm classification (s). ||| 8281 ||| 398 ||| 8285 ||| 8286 ||| 25648 ||| 15556 ||| 14552 ||| 25649 ||| 15558 ||| 
2019 ||| multistep flow prediction on car-sharing systems: a multi-graph convolutional neural network with attention mechanism. ||| 25650 ||| 20085 ||| 25651 ||| 25652 ||| 25653 ||| 25654 ||| 25310 ||| 
2020 ||| a novel self-attention based automatic code completion neural network. ||| 25027 ||| 25655 ||| 25656 ||| 25657 ||| 
2021 ||| streaming transformer asr with blockwise synchronous beam search. ||| 12365 ||| 12364 ||| 3549 ||| 
2021 ||| streaming attention-based models with augmented memory for end-to-end speech recognition. ||| 11978 ||| 11973 ||| 11974 ||| 11976 ||| 11975 ||| 11977 ||| 12491 ||| 
2021 ||| a light transformer for speech-to-intent applications. ||| 25658 ||| 19568 ||| 
2021 ||| convolution-based attention model with positional encoding for streaming speech recognition on embedded devices. ||| 25659 ||| 13906 ||| 1613 ||| 
2018 ||| audio-visual speech recognition with a hybrid ctc/attention architecture. ||| 5717 ||| 25660 ||| 25661 ||| 25662 ||| 5719 ||| 
2018 ||| multi-scale alignment and contextual history for attention mechanism in sequence-to-sequence model. ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| multimodal attention fusion for target speaker extraction. ||| 12219 ||| 8251 ||| 1493 ||| 1491 ||| 1492 ||| 1494 ||| 
2021 ||| on the usefulness of self-attention for automatic speech recognition with transformers. ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2018 ||| exploring end-to-end attention-based neural networks for native language identification. ||| 25663 ||| 25664 ||| 25665 ||| 
2018 ||| exploring layer trajectory lstm with depth processing units and attention. ||| 12179 ||| 14310 ||| 12031 ||| 12033 ||| 
2021 ||| supervised attention for speaker recognition. ||| 25666 ||| 12731 ||| 14397 ||| 
2021 ||| self-supervised learning with cross-modal transformers for emotion recognition. ||| 3723 ||| 3722 ||| 3724 ||| 
2021 ||| transformer-based online speech recognition with decoder-end adaptive computation steps. ||| 12321 ||| 12322 ||| 12323 ||| 
2018 ||| attention mechanism in speaker recognition: what does it learn in deep speaker embedding? ||| 25667 ||| 25668 ||| 14562 ||| 25669 ||| 25670 ||| 
2021 ||| emotion recognition in public speaking scenarios utilising an lstm-rnn approach with attention. ||| 25671 ||| 4028 ||| 25672 ||| 648 ||| 649 ||| 
2018 ||| context-aware attention mechanism for speech emotion recognition. ||| 25673 ||| 14613 ||| 6978 ||| 14614 ||| 
2018 ||| improving attention-based end-to-end asr systems with sequence-based loss functions. ||| 12190 ||| 12586 ||| 14262 ||| 1224 ||| 12655 ||| 12189 ||| 4530 ||| 3808 ||| 
2021 ||| transformer based deliberation for two-pass speech recognition. ||| 1098 ||| 3336 ||| 12066 ||| 12099 ||| 
2021 ||| controllable emphatic speech synthesis based on forward attention for expressive speech synthesis. ||| 4457 ||| 25674 ||| 3138 ||| 2754 ||| 25675 ||| 4459 ||| 4460 ||| 
2021 ||| detecting expressions with multimodal transformers. ||| 3722 ||| 3724 ||| 
2021 ||| automated scoring of spontaneous speech from young learners of english using transformers. ||| 25676 ||| 25665 ||| 25664 ||| 25677 ||| 
2021 ||| simplified self-attention for transformer-based end-to-end speech recognition. ||| 14669 ||| 14494 ||| 14599 ||| 12384 ||| 
2021 ||| transformer-based direct speech-to-speech translation with transcoder. ||| 13944 ||| 13907 ||| 11757 ||| 
2018 ||| improving very deep time-delay neural network with vertical-attention for effectively training ctc-based asr systems. ||| 4417 ||| 14705 ||| 25678 ||| 14706 ||| 4418 ||| 12308 ||| 
2021 ||| operator-adapted evolutionary large-scale multiobjective optimization for voltage transformer ratio error estimation. ||| 25679 ||| 25680 ||| 25681 ||| 25504 ||| 25682 ||| 
2019 ||| plant identification based on multi-branch convolutional neural network with attention. ||| 25683 ||| 25684 ||| 25685 ||| 25686 ||| 25687 ||| 786 ||| 18197 ||| 25688 ||| 
2019 ||| pixel and channel attention network for person re-identification. ||| 25689 ||| 9390 ||| 25690 ||| 25691 ||| 25692 ||| 10076 ||| 
2019 ||| attention-based gan for single image super-resolution. ||| 25693 ||| 14797 ||| 25694 ||| 
2019 ||| multi-attention network for 2d face alignment in the wild. ||| 189 ||| 25695 ||| 16649 ||| 25696 ||| 25697 ||| 
2019 ||| sar ship detection under complex background based on attention mechanism. ||| 2230 ||| 25698 ||| 25699 ||| 25700 ||| 25701 ||| 23395 ||| 
2020 ||| eyebrow deserves attention: upper periocular biometrics. ||| 25702 ||| 25703 ||| 25704 ||| 
2019 ||| from material objects to social objects: researching the material-dialogic spaces of joint attention in a school-based makerspace. ||| 25705 ||| 25706 ||| 
2019 ||| a qualitative analysis of joint visual attention and collaboration with high- and low-achieving groups in computer-mediated learning. ||| 25707 ||| 25708 ||| 25709 ||| 
2018 ||| message-efficient self-stabilizing transformer using snap-stabilizing quiescence detection. ||| 25710 ||| 25711 ||| 25712 ||| 
2019 ||| keep attention: a personalized serious game for attention training. ||| 22335 ||| 25713 ||| 25714 ||| 
2021 ||| attention enhanced hierarchical feature representation for three-way decision boundary processing. ||| 1037 ||| 18416 ||| 6781 ||| 25715 ||| 25716 ||| 
2019 ||| design of a d-band transformer-based neutralized class-ab power amplifier in silicon technologies. ||| 25717 ||| 25718 ||| 25719 ||| 25720 ||| 648 ||| 25721 ||| 25722 ||| 
2017 ||| self-aware sensing and attention-based data collection in multi-processor system-on-chips. ||| 25723 ||| 25724 ||| 25725 ||| 
2021 ||| cloud detection method using convolutional neural network based on cascaded color and texture feature attention. ||| 875 ||| 1035 ||| 4061 ||| 1341 ||| 6701 ||| 
2021 ||| remaining useful life estimation for turbofan engine with transformer-based deep architecture. ||| 21631 ||| 1251 ||| 25726 ||| 21632 ||| 6514 ||| 
2021 ||| 6d object pose estimation with attention networks. ||| 1978 ||| 25727 ||| 
2021 ||| accurate visual tracking with attention feature fusion. ||| 13163 ||| 25728 ||| 13435 ||| 
2018 ||| powering up attentional focus: validating a school-based deep breathing intervention with mobile eeg - a pilot exploration. ||| 25729 ||| 25730 ||| 
2021 ||| effectiveness of decoder transformer network in breaking low-resource real-time text captcha system. ||| 25731 ||| 25732 ||| 25733 ||| 25734 ||| 
2020 ||| wellhead compressor failure prediction using attention-based bidirectional lstms with data reduction techniques. ||| 25735 ||| 25736 ||| 
2020 ||| robust speaker recognition using speech enhancement and attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2020 ||| speaker characterization using tdnn, tdnn-lstm, tdnn-lstm-attention based speaker embeddings for nist sre 2019. ||| 2992 ||| 
2020 ||| learning mixture representation for deep speaker embedding using attention. ||| 25737 ||| 25738 ||| 25739 ||| 
2019 ||| state evaluation of power transformer based on digital twin. ||| 25530 ||| 25740 ||| 25741 ||| 21478 ||| 1796 ||| 25742 ||| 25743 ||| 25744 ||| 
2020 ||| an approach for neural machine translation with graph attention network. ||| 25745 ||| 701 ||| 25746 ||| 705 ||| 
2019 ||| attention-based feature pyramid network for object detection. ||| 25747 ||| 10968 ||| 4550 ||| 
2020 ||| gaze estimation with multi-scale channel and spatial attention. ||| 2143 ||| 25748 ||| 23371 ||| 
2019 ||| a multi-scale network based on attention mechanism for hyperspectral image classification. ||| 9999 ||| 13528 ||| 25749 ||| 25750 ||| 13529 ||| 
2021 ||| lightcvt: audio forgery detection via fusion of light cnn and transformer. ||| 24674 ||| 2383 ||| 18594 ||| 7064 ||| 18593 ||| 
2019 ||| multigraph attention network for analyzing company relations. ||| 25751 ||| 25752 ||| 25753 ||| 
2021 ||| research on general text classification model integrating character-level attention and multi-scale features. ||| 23834 ||| 4002 ||| 25754 ||| 
2021 ||| amdet: an efficient infrared small object detection model based on visual attention and multi-dilation feature. ||| 25755 ||| 25756 ||| 25757 ||| 14165 ||| 20184 ||| 
2020 ||| weakly supervised fine-grained image recognition based on multi-channel attention and object localization. ||| 25758 ||| 5043 ||| 25759 ||| 
2020 ||| attention mechanism balances semantic representation and syntactic representation. ||| 385 ||| 386 ||| 2008 ||| 387 ||| 
2019 ||| fabric defect detection using fully convolutional network with attention mechanism. ||| 20184 ||| 21755 ||| 14165 ||| 20183 ||| 21756 ||| 
2021 ||| explicit attention network for face alignment. ||| 25760 ||| 3304 ||| 2098 ||| 
2019 ||| a graph-enhanced convolution network with attention gate for skeleton based action recognition. ||| 5492 ||| 5491 ||| 5400 ||| 
2020 ||| aminn: attention-based multi-information neural network for emotion recognition. ||| 3617 ||| 2304 ||| 12041 ||| 25761 ||| 25762 ||| 14675 ||| 
2019 ||| few-shot knowledge reasoning method based on attention mechanism. ||| 25763 ||| 10833 ||| 25764 ||| 
2019 ||| se-densenet: attention-based network for detecting pathological images of metastatic breast cancer. ||| 25765 ||| 25766 ||| 17295 ||| 
2020 ||| attention-based end-to-end keywords spotting. ||| 25767 ||| 25768 ||| 25769 ||| 25130 ||| 6415 ||| 
2019 ||| combining attentional cnn and gru networks for ocean current prediction based on hf radar observations. ||| 25770 ||| 25771 ||| 25772 ||| 25773 ||| 15786 ||| 
2019 ||| prediction of short-term precipitation in qinghai lake based on bilstm-attention method. ||| 25774 ||| 25775 ||| 25776 ||| 
2018 ||| assessing consequences of the final failure of a power transformer using fuzzy logic and expert criteria. ||| 25777 ||| 25778 ||| 3882 ||| 25779 ||| 25780 ||| 25781 ||| 
2018 ||| automated protection of power transformers at distribution grid connected to green energy sources. ||| 25782 ||| 25783 ||| 25784 ||| 25785 ||| 
2017 ||| optimization of miniaturized single- and multiband cpw-based matching transformers for rf circuitry on lcp substrates. ||| 25786 ||| 25787 ||| 25788 ||| 25789 ||| 25790 ||| 
2017 ||| application of ensemble classification method for power transformers condition assessment. ||| 25791 ||| 25792 ||| 25793 ||| 15900 ||| 
2017 ||| a three-phase ac-dc converter with transformer isolation and reduced number of switches. ||| 25794 ||| 25795 ||| 
2019 ||| reducing the loss of life of distribution transformers affected by plug-in electric vehicles using electric water heaters. ||| 25796 ||| 25797 ||| 
2021 ||| exploring how saliency affects attention in virtual reality. ||| 16125 ||| 25798 ||| 25799 ||| 25800 ||| 16127 ||| 16128 ||| 
2019 ||| gazelens: guiding attention to improve gaze interpretation in hub-satellite collaboration. ||| 25801 ||| 25802 ||| 3157 ||| 25803 ||| 25804 ||| 13618 ||| 
2019 ||| p(l)ay attention! co-designing for and with children with attention deficit hyperactivity disorder (adhd). ||| 23280 ||| 25805 ||| 3369 ||| 25806 ||| 
2021 ||| attngan: realistic text-to-image synthesis with attentional generative adversarial networks. ||| 25807 ||| 25808 ||| 25809 ||| 
2019 ||| you talkin' to me? a practical attention-aware embodied agent. ||| 25810 ||| 25811 ||| 25812 ||| 25813 ||| 25814 ||| 
2021 ||| effect of attention saturating and cognitive load on tactile texture recognition for mobile surface. ||| 25815 ||| 25816 ||| 25817 ||| 
2017 ||| feedback modulated attention within a predictive framework. ||| 25818 ||| 25819 ||| 
2021 ||| a study on email topic identification using latent dirichlet allocation integrated with visual attention. ||| 25820 ||| 25821 ||| 25822 ||| 25823 ||| 
2018 ||| visual attention in omnidirectional video for virtual reality applications. ||| 1595 ||| 1597 ||| 
2020 ||| a code-description representation learning model based on attention. ||| 25824 ||| 25825 ||| 25826 ||| 4715 ||| 
2021 ||| two-stage attention-based model for code search with textual and structural features. ||| 8200 ||| 25827 ||| 859 ||| 25828 ||| 8201 ||| 22698 ||| 14788 ||| 
2018 ||| machine learning of user attentions in sensor data visualization. ||| 25829 ||| 25830 ||| 25831 ||| 
2019 ||| transformer based memory network for sentiment analysis of chinese weibo texts. ||| 25832 ||| 1871 ||| 1254 ||| 
2018 ||| tweetit: analyzing topics for twitter users to garner maximum attention. ||| 1027 ||| 25833 ||| 25834 ||| 
2021 ||| mix-groups attention network for object detection. ||| 25835 ||| 25836 ||| 25837 ||| 25838 ||| 
2020 ||| multimodal sentiment analysis based on multi-head attention mechanism. ||| 25839 ||| 25840 ||| 25841 ||| 
2021 ||| channel attention module and weighted local feature person re-id network. ||| 25842 ||| 25843 ||| 5449 ||| 25844 ||| 2377 ||| 
2019 ||| the role of attention mechanism and multi-feature in image captioning. ||| 25845 ||| 25846 ||| 25847 ||| 2841 ||| 
2021 ||| transformer-based machine translation for low-resourced languages embedded with language identification. ||| 25848 ||| 25849 ||| 25850 ||| 25851 ||| 25852 ||| 25853 ||| 
2018 ||| experimental demonstration of a 5g network slice deployment through the 5g-transformer architecture. ||| 25854 ||| 25855 ||| 25856 ||| 25857 ||| 25858 ||| 25859 ||| 6101 ||| 6093 ||| 25860 ||| 
2021 ||| programmable integrated microwave photonic filter using a modulation transformer and a double-injection ring resonator. ||| 25861 ||| 25862 ||| 25863 ||| 25864 ||| 25865 ||| 25866 ||| 852 ||| 25867 ||| 25868 ||| 
2021 ||| transformer-based alarm context-vectorization representation for reliable alarm root cause identification in optical networks. ||| 15722 ||| 15721 ||| 15720 ||| 25869 ||| 15724 ||| 698 ||| 1254 ||| 
2021 ||| iterative se(3)-transformers. ||| 25870 ||| 25871 ||| 25872 ||| 22290 ||| 
2019 ||| enhanced air quality inference with mobile sensing attention mechanism: poster abstract. ||| 9027 ||| 7400 ||| 25873 ||| 25874 ||| 22573 ||| 
2018 ||| speech emotion recognition via attention-based dnn from multi-task learning. ||| 6733 ||| 25875 ||| 781 ||| 25876 ||| 25877 ||| 22573 ||| 
2018 ||| attention-based lstm-cnns for time-series classification. ||| 25878 ||| 25875 ||| 22573 ||| 25877 ||| 
2020 ||| bert4nilm: a bidirectional transformer model for non-intrusive load monitoring. ||| 25879 ||| 25880 ||| 25881 ||| 25882 ||| 
2021 ||| detecting online risks and supportive interaction in instant messenger conversations using czech transformers. ||| 25883 ||| 59 ||| 10173 ||| 25884 ||| 13478 ||| 25885 ||| 25886 ||| 12225 ||| 25887 ||| 
2019 ||| dynamic fusion: attentional language model for neural machine translation. ||| 25888 ||| 14211 ||| 
2020 ||| ram-net: a residual attention mobilenet to detect covid-19 cases from chest x-ray images. ||| 25889 ||| 25890 ||| 2496 ||| 25891 ||| 
2019 ||| using bidirectional long short term memory with attention layer to estimate driver behavior. ||| 25892 ||| 25893 ||| 
2021 ||| predicting real-time scientific experiments using transformer models and reinforcement learning. ||| 25894 ||| 
2018 ||| a novel neural sequence model with multiple attentions for word sense disambiguation. ||| 3223 ||| 3224 ||| 3225 ||| 
2021 ||| a transformer-based approach for translating natural language to bash commands. ||| 25895 ||| 25896 ||| 25897 ||| 25898 ||| 
2020 ||| gcn with clustering coefficients and attention module. ||| 25899 ||| 25900 ||| 25901 ||| 25902 ||| 
2018 ||| parallel attention mechanisms in neural machine translation. ||| 25903 ||| 20513 ||| 
2020 ||| on analyzing covid-19-related hate speech using bert attention. ||| 25904 ||| 25905 ||| 16637 ||| 25906 ||| 25907 ||| 25908 ||| 
2021 ||| super resolution with sparse gradient-guided attention for suppressing structural distortion. ||| 25909 ||| 25910 ||| 25911 ||| 25912 ||| 19120 ||| 25913 ||| 
2018 ||| inner attention based bi-lstms with indexing for non-factoid question answering. ||| 25914 ||| 25915 ||| 
2020 ||| dat-rnn: trajectory prediction with diverse attention. ||| 6679 ||| 25916 ||| 4979 ||| 
2021 ||| medical code prediction from discharge summary: document to sequence bert using sequence attention. ||| 25917 ||| 25918 ||| 25919 ||| 25920 ||| 25921 ||| 25922 ||| 
2020 ||| safl: a self-attention scene text recognizer with focal loss. ||| 25923 ||| 25924 ||| 25925 ||| 25926 ||| 25927 ||| 25928 ||| 
2018 ||| an attention-based air quality forecasting method. ||| 1748 ||| 25929 ||| 17433 ||| 25930 ||| 2969 ||| 25931 ||| 25932 ||| 
2021 ||| a study of the plausibility of attention between rnn encoders in natural language inference. ||| 25933 ||| 25934 ||| 25935 ||| 25936 ||| 
2021 ||| leveraging transformers for starcraft macromanagement prediction. ||| 25937 ||| 25938 ||| 25939 ||| 
2021 ||| sentiment analysis of stocktwits using transformer models. ||| 25940 ||| 25941 ||| 25942 ||| 25943 ||| 
2021 ||| semi-supervised graph instance transformer for mental health inference. ||| 25944 ||| 25945 ||| 25946 ||| 2111 ||| 25947 ||| 
2020 ||| estimating the effect of general health checkup using uncertainty aware attention of deep instrument variable 2-stage network. ||| 25948 ||| 25949 ||| 25950 ||| 
2021 ||| a physics-informed graph attention-based approach for power flow analysis. ||| 25951 ||| 25952 ||| 
2019 ||| gram: gradient rescaling attention model for data uncertainty estimation in single image super resolution. ||| 25953 ||| 25954 ||| 
2020 ||| a survey of biometric and machine learning methods for tracking students' attention and engagement. ||| 25955 ||| 25956 ||| 25957 ||| 25958 ||| 25959 ||| 25703 ||| 
2020 ||| covid-19 screening using residual attention network an artificial intelligence approach. ||| 25960 ||| 25961 ||| 
2020 ||| interpreting attention models: lstm vs. cnn : a case study on customer activation. ||| 13132 ||| 25962 ||| 13133 ||| 13134 ||| 
2018 ||| unsupervised anomaly detection in energy time series data using variational recurrent autoencoders with attention. ||| 1994 ||| 25963 ||| 25964 ||| 
2021 ||| decoder transformer for temporally-embedded health outcome predictions. ||| 25965 ||| 25966 ||| 25967 ||| 
2021 ||| transformer based bengali chatbot using general knowledge dataset. ||| 4656 ||| 4659 ||| 25968 ||| 25969 ||| 4658 ||| 
2019 ||| radar gesture recognition system in presence of interference using self-attention neural network. ||| 25970 ||| 25971 ||| 
2021 ||| active learning and machine teaching for online learning: a study of attention and labelling cost. ||| 25972 ||| 25973 ||| 25974 ||| 
2021 ||| self-attention mechanism in gans for molecule generation. ||| 25975 ||| 25976 ||| 25977 ||| 
2021 ||| improving next-application prediction with deep personalized-attention neural network. ||| 25978 ||| 25979 ||| 3157 ||| 25980 ||| 
2021 ||| attention on classification for fire segmentation. ||| 25981 ||| 5412 ||| 
2021 ||| continuous multi-modal emotion prediction in video based on recurrent neural network variants with attention. ||| 25982 ||| 25983 ||| 11257 ||| 
2021 ||| temporal bottleneck attention for video recognition. ||| 25984 ||| 25985 ||| 25986 ||| 
2020 ||| disease state prediction from single-cell data using graph attention networks. ||| 25987 ||| 25988 ||| 25989 ||| 25990 ||| 25991 ||| 
2020 ||| deidentification of free-text medical records using pre-trained bidirectional transformers. ||| 25992 ||| 25993 ||| 25994 ||| 
2020 ||| hhh: an online medical chatbot system based on knowledge graph and hierarchical bi-directional attention. ||| 25995 ||| 25996 ||| 25997 ||| 
2020 ||| attentional matrix factorization with document-context awareness and implicit api relationship for service recommendation. ||| 25998 ||| 25999 ||| 26000 ||| 26001 ||| 26002 ||| 
2017 ||| character-level intra attention network for natural language inference. ||| 26003 ||| 3466 ||| 852 ||| 3467 ||| 
2017 ||| refining raw sentence representations for textual entailment recognition via attention. ||| 20763 ||| 7447 ||| 21248 ||| 20764 ||| 
2017 ||| recurrent neural network-based sentence encoder with gated attention for natural language inference. ||| 12760 ||| 26004 ||| 4894 ||| 3644 ||| 26005 ||| 26006 ||| 
2017 ||| neural machine translation with phrasal attention. ||| 26007 ||| 3181 ||| 1254 ||| 
2020 ||| attention tracking for developers. ||| 26008 ||| 
2021 ||| empirical study of transformers for source code. ||| 26009 ||| 26010 ||| 
2021 ||| fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. ||| 26011 ||| 26012 ||| 
2020 ||| intellicode compose: code generation using transformer. ||| 15264 ||| 26013 ||| 26014 ||| 15265 ||| 
2018 ||| effects of e-games on the development of saudi children with attention deficit hyperactivity disorder cognitively, behaviourally and socially: an experimental study. ||| 26015 ||| 26016 ||| 26017 ||| 
2017 ||| modifications of driver attention post-distraction: a detection response task study. ||| 26018 ||| 26019 ||| 26020 ||| 26021 ||| 
2019 ||| automated behavioral modeling and pattern analysis of children with autism in a joint attention training application: a preliminary study. ||| 13868 ||| 13869 ||| 
2018 ||| automatic low-level overlays on presentations to support regaining an audience's attention. ||| 26022 ||| 26023 ||| 26024 ||| 26025 ||| 26026 ||| 
2021 ||| comparing eye tracking and head tracking during a visual attention task in immersive virtual reality. ||| 26027 ||| 26028 ||| 26029 ||| 26030 ||| 26031 ||| 26032 ||| 26033 ||| 5630 ||| 
2017 ||| attentional trade-offs under resource scarcity. ||| 26034 ||| 26035 ||| 
2017 ||| interpretable feature maps for robot attention. ||| 26036 ||| 26037 ||| 
2019 ||| hmi design for autonomous cars: investigating on driver's attention distribution. ||| 26038 ||| 683 ||| 
2017 ||| guiding visual attention based on visual saliency map with projector-camera system. ||| 26039 ||| 26040 ||| 26041 ||| 26042 ||| 26043 ||| 
2021 ||| towards a computerized approach to identify attentional states of online learners. ||| 26044 ||| 26045 ||| 
2020 ||| quantifying museum visitor attention using bluetooth proximity beacons. ||| 26046 ||| 26047 ||| 26048 ||| 26049 ||| 
2019 ||| eeg acquisition during the vr administration of resting state, attention, and image recognition tasks: a feasibility study. ||| 26050 ||| 26051 ||| 26052 ||| 26053 ||| 26054 ||| 26055 ||| 26056 ||| 26057 ||| 26058 ||| 
2019 ||| attention assessment: evaluation of facial expressions of children with autism spectrum disorder. ||| 4346 ||| 4347 ||| 26059 ||| 4348 ||| 26060 ||| 26061 ||| 26062 ||| 11828 ||| 
2020 ||| basic study on incidence of micro-error in visual attention-controlled environment. ||| 26063 ||| 26064 ||| 26065 ||| 26066 ||| 
2021 ||| comparison study of attention between training in a simulator vs. live-fire range. ||| 26067 ||| 26068 ||| 26069 ||| 26070 ||| 
2018 ||| cyber vulnerability: an attentional dilemma. ||| 26071 ||| 26072 ||| 26073 ||| 26074 ||| 
2017 ||| attention value of motion graphics on digital signages. ||| 26075 ||| 26076 ||| 
2017 ||| tactile acoustic devices: the effect on drowsiness during prolonged attentional tasks. ||| 26077 ||| 26078 ||| 
2017 ||| attention sharing in a virtual environment attracts others. ||| 26079 ||| 26080 ||| 26081 ||| 26082 ||| 
2021 ||| attention-based design and selective exposure amid covid-19 misinformation sharing. ||| 26083 ||| 26084 ||| 11483 ||| 
2018 ||| a robot-based cognitive assessment model based on visual working memory and attention level. ||| 26085 ||| 26086 ||| 26087 ||| 26088 ||| 26089 ||| 
2020 ||| visual attention of young and older drivers in takeover tasks of highly automated driving. ||| 26090 ||| 26091 ||| 
2021 ||| influence of a video game on children's attention to food: should games be served with a character during mealtime? ||| 26092 ||| 1748 ||| 26093 ||| 
2021 ||| monitoring attention of crane operators during load oscillations using gaze entropy measures. ||| 26094 ||| 26095 ||| 26096 ||| 
2020 ||| brain activation in virtual reality for attention guidance. ||| 26097 ||| 26098 ||| 26099 ||| 3831 ||| 
2018 ||| a novel way of estimating a user's focus of attention in a virtual environment. ||| 26100 ||| 26101 ||| 
2018 ||| a method of evaluating user visual attention to moving objects in head mounted virtual reality. ||| 26102 ||| 
2021 ||| a comparative analysis of attention to facial recognition payment between china and south korea: a news analysis using latent dirichlet allocation. ||| 26103 ||| 26104 ||| 26105 ||| 26106 ||| 
2020 ||| the relation between video game experience and children's attentional networks. ||| 4175 ||| 26107 ||| 26108 ||| 
2017 ||| a comparison of attention estimation techniques in a public display scenario. ||| 26109 ||| 
2018 ||| product web page design: a psychophysiological investigation of the influence of product similarity, visual proximity on attention and performance. ||| 26110 ||| 26111 ||| 26112 ||| 3882 ||| 26113 ||| 26114 ||| 6787 ||| 
2019 ||| semi-automatic aggregation of multiple models of visual attention for model-based user interface evaluation. ||| 26115 ||| 21002 ||| 26116 ||| 
2020 ||| information visualization design of nuclear power control system based on attention capture mechanism. ||| 26117 ||| 5929 ||| 
2017 ||| psychophysiological and intraoperative aeps and seps monitoring for perception, attention and cognition. ||| 26118 ||| 26119 ||| 26120 ||| 
2020 ||| age-related differences in takeover request modality preferences and attention allocation during semi-autonomous driving. ||| 26121 ||| 26122 ||| 
2018 ||| use bci to generate attention-based metadata for the assessment of effective learning duration. ||| 26123 ||| 26124 ||| 26125 ||| 26126 ||| 
2017 ||| using portable eeg to assess human visual attention. ||| 26127 ||| 26128 ||| 26129 ||| 
2020 ||| investigating the relation between sense of presence, attention and performance: virtual reality versus web. ||| 26130 ||| 26131 ||| 26132 ||| 26133 ||| 26134 ||| 
2020 ||| attention! designing a target group-oriented risk communication strategy. ||| 26135 ||| 26136 ||| 26137 ||| 
2017 ||| mentally imagined item captures attention during visual search. ||| 12801 ||| 26138 ||| 
2019 ||| attentional dynamics after take-over requests: the need for handover assistance systems in highly automated vehicles. ||| 26139 ||| 26140 ||| 
2017 ||| a comparison of an attention acknowledgement measure and eye tracking: application of the as low as reasonable assessment (alara) discount usability principle for control system studies. ||| 26141 ||| 26142 ||| 26143 ||| 26144 ||| 
2020 ||| the impact of advertisements on user attention during permission authorization. ||| 26145 ||| 26146 ||| 26147 ||| 
2017 ||| testing the specificity of eeg neurofeedback training on first- and second-order measures of attention. ||| 26148 ||| 
2019 ||| visual symbol attention and cross-cultural communication - a case study of catering commercial graphic advertising. ||| 26149 ||| 254 ||| 
2017 ||| driver's multi-attribute task battery performance and attentional switch cost are correlated with speeding behavior in simulated driving. ||| 1134 ||| 26150 ||| 26151 ||| 
2018 ||| cognitos: a student-centric working environment for an attention-aware intelligent classroom. ||| 26152 ||| 26153 ||| 26154 ||| 26155 ||| 26156 ||| 
2020 ||| nature at your service - nature inspired representations combined with eye-gaze features to infer user attention and provide contextualized support. ||| 26157 ||| 26158 ||| 26159 ||| 26160 ||| 
2019 ||| breaking down the "wall of text" - software tool to address complex assignments for students with attention disorders. ||| 26161 ||| 26162 ||| 26163 ||| 26164 ||| 
2018 ||| measuring focused attention using fixation inner-density. ||| 26165 ||| 26166 ||| 26167 ||| 26168 ||| 
2020 ||| software log anomaly detection through one class clustering of transformer encoder representation. ||| 1508 ||| 1553 ||| 1510 ||| 
2021 ||| attention to breathing in response to vibrational and verbal cues in mindfulness meditation mediated by wearable devices. ||| 26169 ||| 26170 ||| 26171 ||| 
2021 ||| body-part attention probability for measuring gaze during impression word evaluation. ||| 26172 ||| 26173 ||| 26174 ||| 26175 ||| 
2019 ||| effect of mental fatigue on visual selective attention. ||| 26176 ||| 26177 ||| 26178 ||| 
2020 ||| the storm project: using video game to promote completion of morning routine for children with attention deficit hyperactivity disorder and autism spectrum disorder. ||| 26179 ||| 26180 ||| 26181 ||| 26182 ||| 26183 ||| 26184 ||| 26185 ||| 26186 ||| 
2017 ||| patterns of attention: how data visualizations are read. ||| 26187 ||| 26188 ||| 26189 ||| 26190 ||| 
2020 ||| adapting interaction to address critical user states of high workload and incorrect attentional focus - an evaluation of five adaptation strategies. ||| 26191 ||| 26192 ||| 26193 ||| 26194 ||| 26195 ||| 
2017 ||| design and evaluation of an assistive window for soft keyboards of tablet pcs that reduces visual attention shifts. ||| 26196 ||| 26197 ||| 26198 ||| 26199 ||| 
2021 ||| preliminary findings from a single session of virtual reality attentional bias modification training in healthy women. ||| 26200 ||| 26201 ||| 26202 ||| 26203 ||| 3419 ||| 26204 ||| 26205 ||| 26206 ||| 852 ||| 26207 ||| 26208 ||| 
2020 ||| attentional autoencoder for course recommendation in mooc with course relevance. ||| 26209 ||| 10800 ||| 26210 ||| 26211 ||| 
2018 ||| self-information of the variation in spatial parameter based audio attention model. ||| 26212 ||| 12342 ||| 26213 ||| 307 ||| 16550 ||| 
2020 ||| lar: a user behavior prediction model in server log based on lstm-attention network and rsc algorithm. ||| 26214 ||| 
2020 ||| hierarchy spatial-temporal transformer for action recognition in short videos. ||| 9920 ||| 26215 ||| 
2018 ||| the application of big data to improve the users' attention to we-media. ||| 26216 ||| 26217 ||| 26218 ||| 26219 ||| 26220 ||| 
2019 ||| differential item functioning for boys and girls in a screening instrument for attention deficit hyperactivity disorder. ||| 26221 ||| 26222 ||| 16310 ||| 26223 ||| 26224 ||| 
2021 ||| transformer based spatial-temporal fusion network for metro passenger flow forecasting. ||| 26225 ||| 10922 ||| 26226 ||| 
2021 ||| a novel approach of fault diagnosis based on multi-source signals and attention mechanism. ||| 26227 ||| 26228 ||| 26229 ||| 26230 ||| 
2020 ||| a soft graph attention reinforcement learning for multi-agent cooperation. ||| 5106 ||| 5108 ||| 5107 ||| 5109 ||| 26231 ||| 
2021 ||| an attention transfer model for human-assisted failure avoidance in robot manipulations. ||| 26232 ||| 26233 ||| 26234 ||| 1840 ||| 
2021 ||| repairing human trust by promptly correcting robot mistakes with an attention transfer model. ||| 26234 ||| 1124 ||| 26233 ||| 26232 ||| 1840 ||| 
2020 ||| weak scratch detection of optical components using attention fusion network. ||| 26235 ||| 26236 ||| 26237 ||| 10962 ||| 13320 ||| 26238 ||| 
2021 ||| multi-zone indoor temperature prediction based on graph attention network and gated recurrent unit. ||| 26239 ||| 26240 ||| 18469 ||| 14036 ||| 26241 ||| 
2020 ||| attention based graph covolution networks for intelligent traffic flow analysis. ||| 26242 ||| 8308 ||| 24486 ||| 9543 ||| 
2020 ||| mobile agents' dynamic small-world network based on attention mechanism. ||| 18184 ||| 
2020 ||| how self-attention improves rare class performance in a question-answering dialogue agent. ||| 26243 ||| 14552 ||| 12409 ||| 
2021 ||| domain-independent user simulation with transformers for task-oriented dialogue systems. ||| 26244 ||| 26245 ||| 26246 ||| 26247 ||| 26248 ||| 26249 ||| 26250 ||| 26251 ||| 
2021 ||| a task-oriented dialogue architecture via transformer neural language models and symbolic injection. ||| 26252 ||| 26253 ||| 26254 ||| 26255 ||| 12926 ||| 
2020 ||| counseling-style reflection generation using generative pretrained transformers with augmented context. ||| 17600 ||| 26256 ||| 18140 ||| 16145 ||| 26257 ||| 26258 ||| 
2021 ||| weakly supervised extractive summarization with attention. ||| 26259 ||| 2576 ||| 26260 ||| 
2017 ||| it takes two to tango: modification of siamese long short term memory network with attention mechanism in recognizing argumentative relations in persuasive essay. ||| 22673 ||| 22674 ||| 22675 ||| 22672 ||| 26261 ||| 26262 ||| 
2020 ||| automatic content analysis of computer-supported collaborative inquiry-based learning using deep networks and attention mechanisms. ||| 26263 ||| 26264 ||| 4046 ||| 26265 ||| 26266 ||| 26267 ||| 26268 ||| 5335 ||| 2713 ||| 26269 ||| 26270 ||| 
2019 ||| how can a robot calculate the level of visual focus of human's attention. ||| 26271 ||| 26272 ||| 26273 ||| 26274 ||| 
2020 ||| an attention-based approach for traffic conditions forecasting considering spatial-temporal features. ||| 26275 ||| 26276 ||| 26277 ||| 26278 ||| 26279 ||| 23315 ||| 
2018 ||| adversarial transfer learning for chinese named entity recognition with self-attention mechanism. ||| 26280 ||| 3128 ||| 3129 ||| 1418 ||| 26281 ||| 
2021 ||| discodvt: generating long text with discourse-aware discrete variational transformer. ||| 26282 ||| 9008 ||| 
2020 ||| factorized transformer for multi-domain neural machine translation. ||| 26283 ||| 26284 ||| 17793 ||| 3468 ||| 3471 ||| 
2021 ||| gradient-based adversarial attacks against text transformers. ||| 26285 ||| 2119 ||| 1890 ||| 1891 ||| 1892 ||| 3826 ||| 
2019 ||| self-attention enhanced cnns and collaborative curriculum learning for distantly supervised relation extraction. ||| 26286 ||| 26287 ||| 
2020 ||| cascaded semantic and positional self-attention network for document classification. ||| 26288 ||| 1134 ||| 3433 ||| 
2019 ||| humor detection: a transformer gets the last laugh. ||| 26289 ||| 26290 ||| 
2017 ||| satirical news detection and analysis using attention mechanism and linguistic features. ||| 1856 ||| 10755 ||| 26291 ||| 
2018 ||| a genre-aware attention model to improve the likability prediction of books. ||| 26292 ||| 11932 ||| 11933 ||| 26293 ||| 8048 ||| 26294 ||| 
2018 ||| an analysis of encoder representations in transformer-based machine translation. ||| 23766 ||| 4194 ||| 23767 ||| 
2019 ||| event detection with multi-order graph convolution and aggregated attention. ||| 26295 ||| 3227 ||| 26296 ||| 3738 ||| 1445 ||| 
2017 ||| coarse-to-fine attention models for document summarization. ||| 22820 ||| 4962 ||| 
2017 ||| towards bidirectional hierarchical representations for attention-based neural machine translation. ||| 3037 ||| 3039 ||| 2333 ||| 3040 ||| 3306 ||| 
2021 ||| cross-attention is all you need: adapting pretrained transformers for machine translation. ||| 26297 ||| 1250 ||| 26298 ||| 
2019 ||| a gated self-attention memory network for answer selection. ||| 26299 ||| 3569 ||| 4954 ||| 26300 ||| 
2020 ||| blockwise self-attention for long document understanding. ||| 26301 ||| 25062 ||| 3348 ||| 4898 ||| 26302 ||| 7030 ||| 
2019 ||| jointly learning to align and translate with transformer models. ||| 26303 ||| 26304 ||| 26305 ||| 26306 ||| 
2021 ||| the devil is in the detail: simple tricks improve systematic generalization of transformers. ||| 59 ||| 26307 ||| 7111 ||| 12659 ||| 4194 ||| 11785 ||| 
2018 ||| a hierarchical neural attention-based text classifier. ||| 9416 ||| 3349 ||| 3354 ||| 26308 ||| 
2018 ||| integrating transformer and paraphrase rules for sentence simplification. ||| 26309 ||| 12634 ||| 26310 ||| 26311 ||| 26312 ||| 
2018 ||| exploiting attention to reveal shortcomings in memory models. ||| 26313 ||| 3718 ||| 26314 ||| 26315 ||| 26316 ||| 
2021 ||| pushing on text readability assessment: a transformer meets handcrafted linguistic features. ||| 26317 ||| 26318 ||| 26319 ||| 
2019 ||| the bottom-up evolution of representations in the transformer: a study with machine translation and language modeling objectives. ||| 3844 ||| 3847 ||| 3848 ||| 
2020 ||| losing heads in the lottery: pruning transformer attention in neural machine translation. ||| 26320 ||| 21416 ||| 
2021 ||| universal-kd: attention-based output-grounded intermediate layer knowledge distillation. ||| 17818 ||| 14630 ||| 26321 ||| 14629 ||| 26322 ||| 
2020 ||| social commonsense reasoning with multi-head knowledge attention. ||| 26323 ||| 14228 ||| 
2019 ||| making asynchronous stochastic gradient descent work for transformers. ||| 26324 ||| 21416 ||| 
2021 ||| veealign: multifaceted context representation using dual attention for ontology alignment. ||| 26325 ||| 26326 ||| 26327 ||| 
2018 ||| why self-attention? a targeted evaluation of neural machine translation architectures. ||| 21389 ||| 26328 ||| 3831 ||| 4860 ||| 3847 ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| 14083 ||| 24747 ||| 
2020 ||| transformers: state-of-the-art natural language processing. ||| 26329 ||| 26330 ||| 26331 ||| 26332 ||| 26333 ||| 26334 ||| 26335 ||| 26336 ||| 59 ||| 26337 ||| 26338 ||| 26339 ||| 26340 ||| 26341 ||| 26342 ||| 26343 ||| 26344 ||| 26345 ||| 26346 ||| 26347 ||| 26348 ||| 26349 ||| 4962 ||| 
2021 ||| contrastive document representation learning with graph attention networks. ||| 3676 ||| 23354 ||| 26350 ||| 12360 ||| 3251 ||| 
2018 ||| paragraph-level neural question generation with maxout pointer and gated self-attention networks. ||| 4400 ||| 26351 ||| 26352 ||| 26353 ||| 
2021 ||| transformer feed-forward layers are key-value memories. ||| 26354 ||| 26355 ||| 26356 ||| 3348 ||| 
2020 ||| calibration of pre-trained transformers. ||| 3178 ||| 26357 ||| 
2018 ||| learning universal sentence representations with mean-max attention autoencoder. ||| 18264 ||| 18233 ||| 26358 ||| 3337 ||| 
2021 ||| melt: message-level transformer with masked document representations as pre-training for stance detection. ||| 4903 ||| 26359 ||| 3299 ||| 3699 ||| 
2021 ||| when attention meets fast recurrence: training language models with reduced compute. ||| 19869 ||| 
2017 ||| cascaded attention based unsupervised information distillation for compressive summarization. ||| 23337 ||| 3015 ||| 3385 ||| 26360 ||| 6799 ||| 
2021 ||| inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks. ||| 4755 ||| 3810 ||| 
2017 ||| multi-task attention-based neural networks for implicit discourse relationship representation and identification. ||| 349 ||| 8145 ||| 350 ||| 26361 ||| 3417 ||| 
2019 ||| cloze-driven pretraining of self-attention networks. ||| 3823 ||| 26362 ||| 26363 ||| 24017 ||| 3825 ||| 
2020 ||| attention is all you need for chinese word segmentation. ||| 13593 ||| 3111 ||| 
2019 ||| neural news recommendation with multi-head self-attention. ||| 3754 ||| 3755 ||| 10621 ||| 3756 ||| 2795 ||| 9574 ||| 
2019 ||| syntax-enhanced self-attention-based semantic role labeling. ||| 3289 ||| 3049 ||| 3087 ||| 
2020 ||| investigating african-american vernacular english in transformer-based text generation. ||| 26364 ||| 26365 ||| 26366 ||| 26367 ||| 26368 ||| 26369 ||| 3802 ||| 
2021 ||| value-aware approximate attention. ||| 26370 ||| 26356 ||| 
2021 ||| what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers. ||| 26371 ||| 26372 ||| 26373 ||| 26374 ||| 26375 ||| 26376 ||| 3693 ||| 26377 ||| 26378 ||| 26379 ||| 26380 ||| 26381 ||| 26382 ||| 26383 ||| 26384 ||| 26385 ||| 26386 ||| 26387 ||| 26388 ||| 26389 ||| 26390 ||| 26391 ||| 26392 ||| 26393 ||| 26394 ||| 26395 ||| 26396 ||| 26397 ||| 26398 ||| 26399 ||| 26400 ||| 26401 ||| 26402 ||| 26403 ||| 9523 ||| 26404 ||| 26405 ||| 
2020 ||| towards reasonably-sized character-level transformer nmt by finetuning subword systems. ||| 3591 ||| 26406 ||| 
2018 ||| attention-based capsule network with dynamic routing for relation extraction. ||| 17987 ||| 17988 ||| 26407 ||| 5250 ||| 781 ||| 17990 ||| 
2020 ||| an empirical study of pre-trained transformers for arabic information extraction. ||| 26408 ||| 8335 ||| 7804 ||| 26409 ||| 
2018 ||| phrase-level self-attention networks for universal sentence encoding. ||| 293 ||| 11662 ||| 638 ||| 10200 ||| 
2021 ||| sentence bottleneck autoencoders from transformer language models. ||| 26410 ||| 14940 ||| 3277 ||| 
2021 ||| on pursuit of designing multi-modal transformer for video grounding. ||| 455 ||| 9570 ||| 26411 ||| 26412 ||| 4430 ||| 
2020 ||| learning to fuse sentences with transformers for summarization. ||| 26413 ||| 4952 ||| 4953 ||| 24787 ||| 4956 ||| 523 ||| 
2020 ||| from zero to hero: on the limitations of zero-shot language transfer with multilingual transformers. ||| 26414 ||| 24929 ||| 15106 ||| 15105 ||| 
2020 ||| transformer-gcrf: recovering chinese dropped pronouns with general conditional random fields. ||| 26415 ||| 26416 ||| 2013 ||| 449 ||| 10971 ||| 786 ||| 1378 ||| 26417 ||| 
2020 ||| adaptive attentional network for few-shot knowledge graph completion. ||| 26418 ||| 26419 ||| 26420 ||| 26421 ||| 26422 ||| 759 ||| 22492 ||| 
2018 ||| a visual attention grounding neural model for multimodal machine translation. ||| 26423 ||| 26424 ||| 8701 ||| 1753 ||| 
2021 ||| rumor detection on twitter with claim-guided hierarchical graph attention networks. ||| 15185 ||| 3646 ||| 26425 ||| 23330 ||| 26426 ||| 1382 ||| 
2021 ||| translation as cross-domain knowledge: attention augmentation for unsupervised cross-domain segmenting and labeling tasks. ||| 26427 ||| 340 ||| 26428 ||| 3751 ||| 
2019 ||| mixed multi-head self-attention for neural machine translation. ||| 3742 ||| 3740 ||| 3743 ||| 3744 ||| 3745 ||| 
2020 ||| a dual-attention network for joint named entity recognition and sentence classification of adverse drug events. ||| 26429 ||| 5289 ||| 26430 ||| 1195 ||| 3528 ||| 
2019 ||| heterogeneous graph attention networks for semi-supervised short text classification. ||| 25190 ||| 26431 ||| 1373 ||| 9021 ||| 3488 ||| 
2019 ||| context-aware interactive attention for multi-modal sentiment and emotion analysis. ||| 5137 ||| 3836 ||| 165 ||| 405 ||| 
2019 ||| contrastive attention mechanism for abstractive sentence summarization. ||| 3468 ||| 26284 ||| 3469 ||| 1254 ||| 3471 ||| 3289 ||| 
2019 ||| negative focus detection via contextual attention mechanism. ||| 26432 ||| 26433 ||| 11628 ||| 3088 ||| 222 ||| 26434 ||| 
2021 ||| synchronous dual network with cross-type attention for joint entity and relation extraction. ||| 1805 ||| 18156 ||| 
2020 ||| analyzing redundancy in pretrained transformer models. ||| 26435 ||| 26436 ||| 26437 ||| 20960 ||| 
2020 ||| improve transformer models with better relative position embeddings. ||| 12360 ||| 26438 ||| 3676 ||| 3251 ||| 
2020 ||| table fact verification with structure-aware transformer. ||| 26439 ||| 11704 ||| 26440 ||| 1059 ||| 1062 ||| 2146 ||| 
2021 ||| multi-vector attention models for deep re-ranking. ||| 26441 ||| 4778 ||| 
2020 ||| less is more: attention supervision with counterfactuals for text classification. ||| 18541 ||| 18542 ||| 26442 ||| 3716 ||| 
2018 ||| a wordnet-encoded collocation-attention network for homographic pun recognition. ||| 8973 ||| 8974 ||| 8976 ||| 8967 ||| 8978 ||| 16590 ||| 8349 ||| 21178 ||| 728 ||| 8977 ||| 
2020 ||| kermit: complementing transformer architectures with encoders of explicit syntactic interpretations. ||| 26443 ||| 26444 ||| 26445 ||| 26446 ||| 26447 ||| 26448 ||| 
2021 ||| sparse attention with linear units. ||| 3180 ||| 3848 ||| 3847 ||| 
2020 ||| be more with less: hypergraph attention networks for inductive text classification. ||| 9755 ||| 9754 ||| 26449 ||| 26450 ||| 5791 ||| 
2020 ||| luke: deep contextualized entity representations with entity-aware self-attention. ||| 26451 ||| 26452 ||| 4802 ||| 26453 ||| 4803 ||| 
2020 ||| x-lxmert: paint, caption and answer questions with multi-modal transformers. ||| 26454 ||| 8568 ||| 26455 ||| 4765 ||| 24052 ||| 
2019 ||| latent suicide risk detection on microblog via suicide-oriented word embeddings and layered attention. ||| 26456 ||| 26457 ||| 26458 ||| 26459 ||| 398 ||| 26460 ||| 26461 ||| 
2021 ||| bidirectional hierarchical attention networks based on document-level context for emotion cause extraction. ||| 26462 ||| 26463 ||| 26464 ||| 
2020 ||| multilevel text alignment with cross-document attention. ||| 26465 ||| 14940 ||| 3277 ||| 
2021 ||| structure-aware fine-tuning of sequence-to-sequence transformers for transition-based amr parsing. ||| 4818 ||| 3551 ||| 1633 ||| 3553 ||| 3686 ||| 4819 ||| 3688 ||| 
2021 ||| deep attention diffusion graph neural networks for text classification. ||| 26466 ||| 13692 ||| 26467 ||| 26468 ||| 13689 ||| 
2021 ||| stanker: stacking network based on level-grained attention-masked bert for rumor detection on social media. ||| 26469 ||| 21486 ||| 26470 ||| 21980 ||| 
2021 ||| what's in your head? emergent behaviour in multi-task transformer models. ||| 26354 ||| 26471 ||| 26472 ||| 26356 ||| 
2020 ||| scheduled drophead: a regularization method for transformer models. ||| 23947 ||| 26473 ||| 3174 ||| 3480 ||| 3617 ||| 
2021 ||| contrastive out-of-distribution detection for pretrained transformers. ||| 21161 ||| 24743 ||| 15099 ||| 
2021 ||| attention weights in transformer nmt fail aligning words between sequences but largely explain model predictions. ||| 26474 ||| 3466 ||| 
2019 ||| fine-tune bert with sparse self-attention mechanism. ||| 26475 ||| 22668 ||| 8009 ||| 17724 ||| 
2018 ||| interpretable structure induction via sparse attention. ||| 26476 ||| 9210 ||| 3369 ||| 3370 ||| 
2021 ||| gated transformer for robust de-noised sequence-to-sequence modelling. ||| 3833 ||| 2209 ||| 3834 ||| 26477 ||| 
2017 ||| no need to pay attention: simple recurrent neural networks work! ||| 26478 ||| 7350 ||| 26479 ||| 
2021 ||| effective convolutional attention network for multi-label clinical document classification. ||| 1305 ||| 26480 ||| 26481 ||| 26482 ||| 26483 ||| 
2020 ||| unsupervised extractive summarization by pre-training hierarchical transformers. ||| 25311 ||| 3479 ||| 2342 ||| 3174 ||| 3480 ||| 
2018 ||| supervised domain enablement attention for personalized domain classification. ||| 26484 ||| 3027 ||| 
2018 ||| co-stack residual affinity networks with multi-level attention refinement for matching text sequences. ||| 1398 ||| 9261 ||| 9016 ||| 
2018 ||| improving multi-label emotion classification via sentiment classification with dual attention transfer network. ||| 3827 ||| 8396 ||| 26485 ||| 800 ||| 26486 ||| 26487 ||| 
2018 ||| hierarchical relation extraction with coarse-to-fine grained attention. ||| 17798 ||| 26488 ||| 3232 ||| 3233 ||| 3675 ||| 
2020 ||| how can self-attention networks recognize dyck-n languages? ||| 1405 ||| 26489 ||| 781 ||| 
2019 ||| enhancing dialogue symptom diagnosis with global attention and symptom graph. ||| 26490 ||| 26491 ||| 9604 ||| 26492 ||| 4814 ||| 10333 ||| 
2017 ||| deeper attention to abusive user content moderation. ||| 24915 ||| 26493 ||| 26494 ||| 
2021 ||| enjoy the salience: towards better transformer-based faithful explanations with word salience. ||| 3735 ||| 3736 ||| 
2019 ||| improving relation extraction with knowledge-attention. ||| 481 ||| 740 ||| 17665 ||| 6721 ||| 
2018 ||| extracting syntactic trees from transformer encoder self-attentions. ||| 20958 ||| 20959 ||| 
2020 ||| pre-training transformers as energy-based cloze models. ||| 20965 ||| 22748 ||| 9372 ||| 20967 ||| 
2020 ||| on the ability and limitations of transformers to recognize formal languages. ||| 23083 ||| 26495 ||| 23085 ||| 
2021 ||| subformer: exploring weight sharing for parameter efficiency in generative transformers. ||| 26496 ||| 7447 ||| 20764 ||| 
2018 ||| training deeper neural machine translation models with transparent attention. ||| 26497 ||| 26498 ||| 26499 ||| 26500 ||| 12067 ||| 
2018 ||| deriving machine attention from human rationales. ||| 26501 ||| 3094 ||| 3093 ||| 26502 ||| 
2020 ||| transfer learning and distant supervision for multilingual transformer models: a study on african languages. ||| 20963 ||| 26503 ||| 26504 ||| 26505 ||| 26506 ||| 20964 ||| 
2020 ||| efficient transformer-based large scale language representations using hardware-friendly block structured pruning. ||| 9874 ||| 9872 ||| 25744 ||| 9747 ||| 6002 ||| 11023 ||| 11024 ||| 
2021 ||| vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers. ||| 26507 ||| 3089 ||| 26508 ||| 
2020 ||| extremely low bit transformer quantization for on-device neural machine translation. ||| 3495 ||| 26509 ||| 26510 ||| 26511 ||| 26512 ||| 26513 ||| 3496 ||| 26514 ||| 
2021 ||| self-attention graph residual convolutional networks for event detection with dependency relations. ||| 26515 ||| 7957 ||| 15184 ||| 
2021 ||| haconvgnn: hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks. ||| 26516 ||| 3095 ||| 26517 ||| 4832 ||| 18010 ||| 
2020 ||| vd-bert: a unified vision and dialog transformer with bert. ||| 7400 ||| 1313 ||| 597 ||| 598 ||| 3287 ||| 3303 ||| 
2021 ||| on the effects of transformer size on in- and out-of-domain calibration. ||| 26518 ||| 4830 ||| 
2020 ||| a compare aggregate transformer for understanding document-grounded dialogue. ||| 240 ||| 26519 ||| 26520 ||| 3311 ||| 
2020 ||| multi-unit transformers for neural machine translation. ||| 26521 ||| 3075 ||| 1921 ||| 
2021 ||| consistent accelerated inference via confident adaptive transformers. ||| 26522 ||| 26523 ||| 26524 ||| 26502 ||| 
2020 ||| a time-aware transformer based model for suicide ideation detection on social media. ||| 12548 ||| 26525 ||| 26526 ||| 12550 ||| 
2020 ||| discourse self-attention for discourse element identification in argumentative student essays. ||| 17827 ||| 26527 ||| 26528 ||| 26529 ||| 26530 ||| 3311 ||| 
2017 ||| stack-based multi-layer attention for transition-based dependency parsing. ||| 26531 ||| 12389 ||| 3046 ||| 3480 ||| 1301 ||| 
2021 ||| recurrent attention for neural machine translation. ||| 3043 ||| 3045 ||| 26532 ||| 26533 ||| 3046 ||| 
2021 ||| the stem cell hypothesis: dilemma behind multi-task learning with transformer encoders. ||| 1593 ||| 375 ||| 
2017 ||| interactive visualization and manipulation of attention-based neural machine translation. ||| 26534 ||| 26535 ||| 26536 ||| 
2018 ||| attentive gated lexicon reader with contrastive contextual co-attention for sentiment classification. ||| 1398 ||| 9261 ||| 9016 ||| 9262 ||| 
2021 ||| frequency effects on syntactic rule learning in transformers. ||| 26537 ||| 26538 ||| 26539 ||| 26540 ||| 
2019 ||| low-rank hoca: efficient high-order cross-modal attention for video captioning. ||| 14156 ||| 22669 ||| 22668 ||| 17724 ||| 
2020 ||| powertransformer: unsupervised controllable revision for biased language correction. ||| 26541 ||| 3537 ||| 3536 ||| 3355 ||| 
2018 ||| a co-attention neural network model for emotion cause analysis with emotional context awareness. ||| 1309 ||| 26542 ||| 1311 ||| 1312 ||| 8426 ||| 
2021 ||| finetuning pretrained transformers into rnns. ||| 22827 ||| 9407 ||| 1709 ||| 23970 ||| 26543 ||| 14940 ||| 17138 ||| 3175 ||| 3277 ||| 
2019 ||| prado: projection attention networks for document classification on-device. ||| 26544 ||| 24969 ||| 24970 ||| 
2020 ||| pymt5: multi-mode translation of natural language and python code with transformers. ||| 26545 ||| 15263 ||| 26546 ||| 15264 ||| 15265 ||| 
2018 ||| jointly multiple events extraction via attention-based graph information aggregation. ||| 2530 ||| 10612 ||| 688 ||| 
2020 ||| attnio: knowledge graph exploration with in-and-out attention flow for knowledge-grounded dialogue. ||| 26547 ||| 22822 ||| 26548 ||| 
2017 ||| a cognition based attention model for sentiment analysis. ||| 5532 ||| 26549 ||| 26550 ||| 26551 ||| 8956 ||| 
2018 ||| hard non-monotonic attention for character-level transduction. ||| 3484 ||| 26552 ||| 3485 ||| 
2020 |||  job description matching using context-aware transformer models. ||| 3489 ||| 26553 ||| 26554 ||| 26555 ||| 26556 ||| 375 ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| 20961 ||| 20962 ||| 20963 ||| 20964 ||| 
2020 ||| autoeter: automated entity type representation with relation-aware attention for knowledge graph embedding. ||| 26557 ||| 1717 ||| 26558 ||| 1937 ||| 26559 ||| 
2020 ||| point to the expression: solving algebraic word problems using the expression-pointer transformer model. ||| 26560 ||| 26561 ||| 26562 ||| 26563 ||| 
2019 ||| transformer-based model for single documents neural summarization. ||| 26564 ||| 6244 ||| 
2021 ||| hitrans: a hierarchical transformer network for nested named entity recognition. ||| 23330 ||| 3646 ||| 23331 ||| 19517 ||| 3410 ||| 
2020 ||| coupled hierarchical transformer for stance-aware rumor verification in social media conversations. ||| 3827 ||| 800 ||| 26565 ||| 26566 ||| 3828 ||| 
2020 ||| convert: efficient and accurate conversational representations from transformers. ||| 26567 ||| 6089 ||| 26568 ||| 26569 ||| 26570 ||| 26571 ||| 15106 ||| 
2020 ||| using pre-trained transformer for better lay summarization. ||| 26572 ||| 
2020 ||| text graph transformer for document classification. ||| 6716 ||| 538 ||| 
2019 ||| modeling graph structure in transformer for better amr-to-text generation. ||| 12282 ||| 3564 ||| 21137 ||| 21126 ||| 1254 ||| 3088 ||| 
2019 ||| improving deep transformer with depth-scaled initialization and merged attention. ||| 3180 ||| 3848 ||| 3847 ||| 
2019 ||| enhanced transformer model for data-to-text generation. ||| 26573 ||| 26574 ||| 26575 ||| 
2020 ||| pair: planning and iterative refinement in pre-trained transformers for long text generation. ||| 26576 ||| 4754 ||| 
2018 ||| collective event detection via a hierarchical and bias tagging networks with gated multi-level attention mechanisms. ||| 3128 ||| 1822 ||| 3129 ||| 1418 ||| 17639 ||| 
2020 ||| a concise model for multi-criteria chinese word segmentation with transformer encoder. ||| 3272 ||| 26577 ||| 3816 ||| 3273 ||| 
2020 ||| structured attention for unsupervised dialogue structure induction. ||| 26578 ||| 26579 ||| 26580 ||| 26581 ||| 26582 ||| 26583 ||| 1753 ||| 18982 ||| 
2018 ||| surprisingly easy hard-attention for sequence to sequence learning. ||| 23965 ||| 18017 ||| 3859 ||| 
2021 ||| hitter: hierarchical transformers for knowledge graph embeddings. ||| 26584 ||| 24050 ||| 1958 ||| 4816 ||| 4817 ||| 26585 ||| 
2020 ||| text segmentation by cross segment attention. ||| 26586 ||| 26587 ||| 26588 ||| 26589 ||| 26590 ||| 227 ||| 
2019 ||| hierarchically-refined label attention network for sequence labeling. ||| 26591 ||| 3289 ||| 
2020 ||| oie: multilingual open information extraction based on multi-head attention with bert. ||| 26592 ||| 26593 ||| 26594 ||| 
2020 ||| mmft-bert: multimodal fusion transformer with bert encodings for visual question answering. ||| 26595 ||| 1750 ||| 26596 ||| 1752 ||| 
2018 ||| abstractive text-image summarization using multi-modal attentional hierarchical rnn. ||| 26597 ||| 26598 ||| 
2018 ||| contextual inter-modal attention for multi-modal sentiment analysis. ||| 26599 ||| 3836 ||| 5137 ||| 892 ||| 165 ||| 405 ||| 
2019 ||| syntax-aware aspect level sentiment classification with graph attention networks. ||| 7738 ||| 7739 ||| 
2021 ||| are transformers a modern version of eliza? observations on french object verb agreement. ||| 26600 ||| 26601 ||| 3766 ||| 3767 ||| 
2019 ||| interpretable relevant emotion ranking with event-driven attention. ||| 11466 ||| 3663 ||| 3664 ||| 5475 ||| 
2020 ||| question directed graph attention network for numerical reasoning over text. ||| 23075 ||| 5201 ||| 23073 ||| 26602 ||| 26603 ||| 25400 ||| 14931 ||| 1087 ||| 12772 ||| 
2021 ||| contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems. ||| 26604 ||| 26605 ||| 6334 ||| 26606 ||| 26607 ||| 
2019 ||| attention optimization for abstractive document summarization. ||| 26608 ||| 10590 ||| 3049 ||| 1979 ||| 
2021 ||| tag: gradient attack on transformer-based language models. ||| 26609 ||| 26610 ||| 9747 ||| 26611 ||| 805 ||| 11023 ||| 807 ||| 11024 ||| 
2020 ||| etc: encoding long and structured inputs in transformers. ||| 3557 ||| 9233 ||| 3882 ||| 9232 ||| 26612 ||| 26613 ||| 9234 ||| 3555 ||| 26614 ||| 9235 ||| 2884 ||| 
2021 ||| narrative embedding: re-contextualization through attention. ||| 26615 ||| 26616 ||| 26617 ||| 
2018 ||| learning when to concentrate or divert attention: self-adaptive attention temperature for neural machine translation. ||| 25375 ||| 3751 ||| 9350 ||| 26618 ||| 16564 ||| 
2021 ||| t3-vis: visual analytic for training and fine-tuning transformers in nlp. ||| 26619 ||| 4793 ||| 26620 ||| 8273 ||| 4795 ||| 
2017 ||| efficient attention using a fixed-size memory representation. ||| 26621 ||| 26622 ||| 22748 ||| 
2018 ||| document-level neural machine translation with hierarchical attention networks. ||| 26623 ||| 26624 ||| 14940 ||| 3672 ||| 
2018 ||| linguistically-informed self-attention for semantic role labeling. ||| 9507 ||| 26625 ||| 26626 ||| 26627 ||| 24973 ||| 
2020 ||| transition-based parsing with stack-transformers. ||| 1633 ||| 3553 ||| 11712 ||| 3551 ||| 26628 ||| 4819 ||| 
2019 ||| adaptively sparse transformers. ||| 26589 ||| 26629 ||| 9210 ||| 3369 ||| 3370 ||| 
2021 ||| towards incremental transformers: an empirical analysis of transformer models for incremental nlu. ||| 26630 ||| 26631 ||| 3580 ||| 
2021 ||| mt6: multilingual pretrained text-to-text transformer with translation pairs. ||| 26632 ||| 3171 ||| 10200 ||| 3499 ||| 26633 ||| 26634 ||| 688 ||| 23931 ||| 3174 ||| 
2020 ||| compressing transformer-based semantic parsing models using compositional code embeddings. ||| 26635 ||| 26636 ||| 26637 ||| 26638 ||| 26639 ||| 26640 ||| 
2021 ||| progressive transformer-based generation of radiology reports. ||| 26641 ||| 26642 ||| 26643 ||| 26644 ||| 26645 ||| 
2020 ||| inserting information bottleneck for attribution in transformers. ||| 3007 ||| 26646 ||| 17741 ||| 3009 ||| 
2020 ||| focus-constrained attention mechanism for cvae-based response generation. ||| 26647 ||| 26648 ||| 26649 ||| 26650 ||| 26651 ||| 379 ||| 
2020 ||| on extractive and abstractive neural document summarization with transformer language models. ||| 26652 ||| 26619 ||| 26653 ||| 26654 ||| 
2021 ||| lamad: a linguistic attentional model for arabic text diacritization. ||| 26655 ||| 1129 ||| 
2021 ||| mate: multi-view attention for table transformer efficiency. ||| 3832 ||| 26656 ||| 3830 ||| 3831 ||| 3246 ||| 
2020 ||| on the weak link between importance and prunability of attention heads. ||| 18067 ||| 18066 ||| 3327 ||| 11768 ||| 3328 ||| 
2020 ||| friendly topic assistant for transformer based abstractive summarization. ||| 26657 ||| 26658 ||| 3386 ||| 26659 ||| 26660 ||| 9283 ||| 9284 ||| 
2020 ||| generating radiology reports via memory-driven transformer. ||| 6318 ||| 3198 ||| 26661 ||| 26662 ||| 
2020 ||| transformer based multi-source domain adaptation. ||| 26663 ||| 3586 ||| 
2021 ||| grouped-attention for content-selection and content-plan generation. ||| 17785 ||| 13160 ||| 17786 ||| 3248 ||| 26664 ||| 
2020 ||| assessing phrasal representation and composition in transformers. ||| 3274 ||| 3275 ||| 
2019 ||| video dialog via progressive inference and cross-transformer. ||| 9703 ||| 1306 ||| 9704 ||| 7652 ||| 3174 ||| 7654 ||| 
2020 ||| biomedical event extraction on graph edge-conditioned attention networks with hierarchical knowledge graphs. ||| 26665 ||| 26666 ||| 18020 ||| 
2020 ||| summarizing chinese medical answer with graph convolution networks and question-focused dual attention. ||| 17987 ||| 17988 ||| 1448 ||| 5250 ||| 781 ||| 17990 ||| 
2019 ||| capsule network with interactive attention for aspect-level sentiment classification. ||| 26667 ||| 6189 ||| 6187 ||| 6188 ||| 6191 ||| 17704 ||| 124 ||| 
2019 ||| weakly supervised attention networks for entity recognition. ||| 26668 ||| 26669 ||| 
2021 ||| arabictransformer: efficient large arabic language model with funnel transformer and electra objective. ||| 23753 ||| 23754 ||| 
2020 ||| cross-media keyphrase prediction: a unified framework with multi-modality multi-head attention and image wordings. ||| 7400 ||| 4807 ||| 597 ||| 598 ||| 
2021 ||| transformer-based lexically constrained headline generation. ||| 26670 ||| 26671 ||| 26672 ||| 26673 ||| 3090 ||| 26674 ||| 26675 ||| 
2021 ||| enlivening redundant heads in multi-head self-attention for machine translation. ||| 26676 ||| 688 ||| 717 ||| 18214 ||| 
2019 ||| transformer and seq2seq model for paraphrase generation. ||| 26564 ||| 6244 ||| 
2020 ||| repulsive attention: rethinking multi-head attention as bayesian inference. ||| 26677 ||| 26678 ||| 26679 ||| 26680 ||| 26681 ||| 26682 ||| 26683 ||| 26684 ||| 17231 ||| 
2020 ||| graph-to-graph transformer for transition-based dependency parsing. ||| 26685 ||| 3672 ||| 
2021 ||| disentangling representations of text by masking transformers. ||| 26686 ||| 26687 ||| 4950 ||| 
2020 ||| how effective is task-agnostic data augmentation for pretrained transformers? ||| 26688 ||| 3906 ||| 26689 ||| 
2021 ||| attention-based contrastive learning for winograd schemas. ||| 3572 ||| 3573 ||| 
2018 ||| improving large-scale fact-checking using decomposable attention models and lexical tagging. ||| 26690 ||| 26691 ||| 10654 ||| 
2019 ||| multi-granularity self-attention for neural machine translation. ||| 4796 ||| 3309 ||| 4882 ||| 4797 ||| 3041 ||| 
2019 ||| transformer dissection: an unified understanding for transformer's attention via the lens of kernel. ||| 3597 ||| 3598 ||| 26692 ||| 3601 ||| 3247 ||| 
2017 ||| position-aware attention and supervised data improve slot filling. ||| 24349 ||| 23928 ||| 26693 ||| 26694 ||| 20967 ||| 
2019 ||| original semantics-oriented attention and deep fusion network for sentence matching. ||| 20810 ||| 20779 ||| 18160 ||| 18159 ||| 
2021 ||| block pruning for faster transformers. ||| 1226 ||| 26695 ||| 26696 ||| 26331 ||| 4962 ||| 
2020 ||| query-key normalization for transformers. ||| 26697 ||| 26698 ||| 26699 ||| 26700 ||| 
2021 ||| artificial text detection via examining the topology of attention maps. ||| 26701 ||| 26702 ||| 26703 ||| 10339 ||| 26704 ||| 26705 ||| 18098 ||| 26706 ||| 2648 ||| 
2019 ||| self-attention with structural position representations. ||| 3309 ||| 3041 ||| 3038 ||| 4882 ||| 
2019 ||| lxmert: learning cross-modality encoder representations from transformers. ||| 26707 ||| 3810 ||| 
2018 ||| importance of self-attention for sentiment analysis. ||| 8063 ||| 26708 ||| 15325 ||| 15326 ||| 26709 ||| 26710 ||| 7350 ||| 1226 ||| 26711 ||| 
2018 ||| iterative recursive attention model for interpretable sequence classification. ||| 23760 ||| 23761 ||| 
2021 ||| do transformer modifications transfer across implementations and applications? ||| 13146 ||| 26712 ||| 1398 ||| 26713 ||| 26714 ||| 26715 ||| 26716 ||| 26717 ||| 26718 ||| 9132 ||| 26719 ||| 26720 ||| 3337 ||| 26721 ||| 26722 ||| 4822 ||| 3338 ||| 
2019 ||| auto-sizing the transformer network: improving speed, efficiency, and performance for low-resource machine translation. ||| 26723 ||| 26724 ||| 4738 ||| 26725 ||| 4846 ||| 
2021 ||| multimodal phased transformer for sentiment analysis. ||| 26726 ||| 26727 ||| 26728 ||| 26729 ||| 
2018 ||| visual interrogation of attention-based models for natural language inference and machine comprehension. ||| 26730 ||| 4003 ||| 21186 ||| 26731 ||| 26732 ||| 26733 ||| 
2020 ||| fixed encoder self-attention patterns in transformer-based machine translation. ||| 23766 ||| 26734 ||| 4194 ||| 23767 ||| 
2018 ||| multi-level structured self-attentions for distantly supervised relation extraction. ||| 26287 ||| 26735 ||| 16059 ||| 26736 ||| 
2019 ||| towards better modeling hierarchical structure for self-attention with ordered neurons. ||| 4796 ||| 3309 ||| 4882 ||| 4797 ||| 3041 ||| 
2021 ||| effects of parameter norm growth during transformer training: inductive bias from gradient descent. ||| 26737 ||| 26738 ||| 3441 ||| 23971 ||| 3277 ||| 
2021 ||| attentionrank: unsupervised keyphrase extraction using self and cross attentions. ||| 26739 ||| 26740 ||| 
2019 ||| can: constrained attention networks for multi-aspect sentiment analysis. ||| 26741 ||| 21574 ||| 254 ||| 26742 ||| 26743 ||| 26744 ||| 26745 ||| 
2021 ||| hetformer: heterogeneous transformer with sparse attention for long-text extractive summarization. ||| 23311 ||| 14896 ||| 3891 ||| 11630 ||| 9988 ||| 1094 ||| 
2020 ||| improving constituency parsing with span attention. ||| 3197 ||| 3198 ||| 3200 ||| 2814 ||| 
2020 ||| guiding attention for self-supervised learning with transformers. ||| 26746 ||| 3651 ||| 
2018 ||| leveraging gloss knowledge in neural word sense disambiguation by hierarchical co-attention. ||| 26747 ||| 638 ||| 1432 ||| 26748 ||| 26749 ||| 26750 ||| 
2021 ||| exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media. ||| 26751 ||| 26752 ||| 14233 ||| 26753 ||| 26754 ||| 8955 ||| 
2021 ||| bag of tricks for optimizing transformer efficiency. ||| 17717 ||| 17716 ||| 2333 ||| 3306 ||| 
2021 ||| adapterdrop: on the efficiency of adapters in transformers. ||| 3700 ||| 3701 ||| 26755 ||| 26756 ||| 26757 ||| 23157 ||| 26758 ||| 3702 ||| 
2019 ||| discourse-aware semantic self-attention for narrative reading comprehension. ||| 26759 ||| 14228 ||| 
2021 ||| mirtt: learning multimodal interaction representations from trilinear transformers for visual question answering. ||| 26760 ||| 26761 ||| 26762 ||| 3177 ||| 26763 ||| 
2019 ||| incorporating graph attention mechanism into knowledge graph reasoning based on deep reinforcement learning. ||| 22747 ||| 18024 ||| 2238 ||| 18025 ||| 
2021 ||| cross attention augmented transducer networks for simultaneous translation. ||| 26764 ||| 26765 ||| 26766 ||| 6159 ||| 1301 ||| 
2020 ||| slot attention with value normalization for multi-domain dialogue state tracking. ||| 26767 ||| 13701 ||| 26768 ||| 
2018 ||| neural related work summarization with a joint context-driven attention mechanism. ||| 26769 ||| 3086 ||| 26770 ||| 
2018 ||| multi-head attention with disagreement regularization. ||| 595 ||| 3041 ||| 3037 ||| 597 ||| 2814 ||| 
2019 ||| hierarchical attention prototypical networks for few-shot text classification. ||| 26771 ||| 26772 ||| 26773 ||| 26774 ||| 
2020 ||| relation-aware graph attention networks with relational position encodings for emotion recognition in conversations. ||| 26775 ||| 26776 ||| 16013 ||| 16017 ||| 
2021 ||| all bark and no bite: rogue dimensions in transformer language models obscure representational quality. ||| 26777 ||| 26778 ||| 
2019 ||| knowledge-enriched transformer for emotion detection in textual conversations. ||| 738 ||| 15790 ||| 16696 ||| 
2020 ||| understanding the difficulty of training transformers. ||| 4805 ||| 24050 ||| 1958 ||| 3175 ||| 1252 ||| 
2019 ||| multi-head attention with diversity for learning grounded multilingual multimodal representations. ||| 23838 ||| 1781 ||| 18704 ||| 
2020 ||| long document ranking with query-directed sparse transformer. ||| 9682 ||| 3322 ||| 26779 ||| 1160 ||| 
2020 ||| retrofitting structure-aware transformer language model for end tasks. ||| 26780 ||| 26781 ||| 3725 ||| 
2021 ||| tsdae: using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning. ||| 19389 ||| 26758 ||| 3702 ||| 
2021 ||| shape : shifted absolute position embedding for transformers. ||| 26782 ||| 26783 ||| 26784 ||| 26674 ||| 
2021 ||| learning hard retrieval decoder attention for transformers. ||| 8 ||| 3260 ||| 3207 ||| 3181 ||| 
2021 ||| neural attention-aware hierarchical topic model. ||| 26785 ||| 26786 ||| 124 ||| 26787 ||| 26788 ||| 
2018 ||| modeling localness for self-attention networks. ||| 3037 ||| 3041 ||| 3039 ||| 3075 ||| 3040 ||| 2814 ||| 
2017 ||| deep joint entity disambiguation with local neural attention. ||| 26789 ||| 26790 ||| 
2020 ||| turngpt: a transformer-based language model for predicting turn-taking in spoken dialog. ||| 26791 ||| 21674 ||| 
2019 ||| recognizing conflict opinions in aspect-level sentiment classification with dual attention networks. ||| 26792 ||| 11156 ||| 26793 ||| 
2019 ||| tree transformer: integrating tree structures into self-attention. ||| 26794 ||| 12644 ||| 4843 ||| 
2020 ||| rethinking self-attention: towards interpretability in neural parsing. ||| 3566 ||| 4952 ||| 3569 ||| 4954 ||| 4956 ||| 3568 ||| 
2019 ||| effective use of transformer networks for entity tracking. ||| 26795 ||| 26357 ||| 
2018 ||| attention-guided answer distillation for machine reading comprehension. ||| 970 ||| 972 ||| 3174 ||| 967 ||| 11105 ||| 9413 ||| 3480 ||| 
2019 ||| interrogating the explanatory power of attention in neural machine translation. ||| 14942 ||| 14943 ||| 14944 ||| 
2021 ||| gradts: a gradient-based automatic auxiliary task selection method based on transformer networks. ||| 3432 ||| 3434 ||| 3433 ||| 3435 ||| 3436 ||| 
2020 ||| h2kgat: hierarchical hyperbolic knowledge graph attention network. ||| 9642 ||| 26796 ||| 3157 ||| 3249 ||| 18122 ||| 18123 ||| 26797 ||| 3251 ||| 1094 ||| 
2021 ||| a simple and effective positional encoding for transformers. ||| 26798 ||| 26799 ||| 2567 ||| 26712 ||| 9156 ||| 26800 ||| 
2021 ||| sequence length is a domain: length-based overfitting in transformer models. ||| 26801 ||| 21421 ||| 
2021 ||| transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence. ||| 26802 ||| 26785 ||| 26803 ||| 124 ||| 26787 ||| 26788 ||| 
2021 ||| a label-aware bert attention network for zero-shot multi-intent detection in spoken language understanding. ||| 7456 ||| 26804 ||| 26805 ||| 
2021 ||| revisiting robust neural machine translation: a transformer case study. ||| 17817 ||| 26806 ||| 3443 ||| 
2020 ||| on the sub-layer functionalities of transformer decoder. ||| 26807 ||| 3038 ||| 4882 ||| 4916 ||| 26808 ||| 3041 ||| 
2021 ||| considering nested tree structure in sentence extractive summarization with pre-trained transformer. ||| 26809 ||| 26810 ||| 4982 ||| 14011 ||| 
2020 ||| a bilingual generative transformer for semantic sentence embedding. ||| 26811 ||| 3067 ||| 26812 ||| 
2018 ||| interpreting recurrent and attention-based neural models: a case study on natural language inference. ||| 4913 ||| 4914 ||| 4916 ||| 
2018 ||| multi-grained attention network for aspect-level sentiment classification. ||| 26813 ||| 10234 ||| 23540 ||| 
2017 ||| recurrent attention network on memory for aspect sentiment analysis. ||| 26814 ||| 18150 ||| 3385 ||| 2334 ||| 
2021 ||| #howyoutagtweets: learning user hashtagging preferences via personalized topic attention. ||| 26815 ||| 26816 ||| 11640 ||| 4807 ||| 13956 ||| 26817 ||| 
2017 ||| incorporating global visual features into attention-based neural machine translation. ||| 26818 ||| 3443 ||| 
2020 ||| pruning redundant mappings in transformer models via spectral-normalized identity prior. ||| 26819 ||| 26820 ||| 26821 ||| 26822 ||| 4830 ||| 
2021 ||| extracting fine-grained knowledge graphs of scientific claims: dataset and transformer-based results. ||| 26823 ||| 26824 ||| 
2020 ||| program enhanced fact verification with verbalization and graph attention network. ||| 1323 ||| 17804 ||| 26825 ||| 14059 ||| 11254 ||| 26004 ||| 
2020 ||| graph transformer networks with syntactic and semantic structures for event argument extraction. ||| 1404 ||| 26826 ||| 11744 ||| 
2020 ||| tnt: text normalization based pre-training of transformers for content moderation. ||| 26682 ||| 26684 ||| 26681 ||| 26827 ||| 26828 ||| 
2021 ||| modeling concentrated cross-attention for neural machine translation with gaussian mixture model. ||| 11809 ||| 3076 ||| 
2019 ||| attention is not not explanation. ||| 26829 ||| 26830 ||| 
2020 ||| adapterhub: a framework for adapting transformers. ||| 23157 ||| 3700 ||| 3701 ||| 26831 ||| 26832 ||| 15106 ||| 3671 ||| 3008 ||| 3702 ||| 
2021 ||| what's hidden in a one-layer randomly weighted transformer? ||| 3822 ||| 22812 ||| 3826 ||| 2596 ||| 22814 ||| 
2018 ||| improving the transformer translation model with document-level context. ||| 494 ||| 26833 ||| 3233 ||| 26834 ||| 17952 ||| 1254 ||| 1305 ||| 
2017 ||| towards neural machine translation with latent tree attention. ||| 26835 ||| 19267 ||| 
2021 ||| ctal: pre-training cross-modal transformer for audio-and-language representations. ||| 6799 ||| 8083 ||| 26836 ||| 8082 ||| 26837 ||| 8087 ||| 
2020 ||| persian ezafe recognition using transformers and its role in part-of-speech tagging. ||| 26838 ||| 26839 ||| 26840 ||| 
2021 ||| sparsity and sentence structure in encoder-decoder attention of summarization systems. ||| 3795 ||| 3796 ||| 
2020 ||| stepwise extractive summarization and planning with structured transformers. ||| 3404 ||| 3491 ||| 26841 ||| 20399 ||| 26842 ||| 26843 ||| 3493 ||| 
2021 ||| irene-viz: visualizing energy consumption of transformer models. ||| 3296 ||| 26844 ||| 3297 ||| 3295 ||| 3298 ||| 3299 ||| 
2021 ||| fbert: a neural transformer for identifying offensive content. ||| 10622 ||| 10623 ||| 3849 ||| 10624 ||| 
2021 ||| skim-attention: learning to focus via document layout. ||| 26845 ||| 3140 ||| 3142 ||| 3141 ||| 
2020 ||| long-short term masking transformer: a simple but effective baseline for document-level neural machine translation. ||| 3803 ||| 3470 ||| 3804 ||| 3805 ||| 
2020 ||| fully quantized transformer for machine translation. ||| 26846 ||| 26696 ||| 14630 ||| 
2020 ||| stl-cqa: structure-based transformers with localization and encoding for chart question answering. ||| 26847 ||| 26848 ||| 
2021 ||| explore better relative position embeddings from encoding perspective for transformer models. ||| 13492 ||| 2478 ||| 13493 ||| 
2019 ||| dual attention networks for visual reference resolution in visual dialog. ||| 26849 ||| 26850 ||| 8580 ||| 
2018 ||| hybrid neural attention for agreement/disagreement inference in online debates. ||| 26851 ||| 21175 ||| 3385 ||| 15906 ||| 
2018 ||| interpretable emoji prediction via label-wise attention lstms. ||| 11766 ||| 26852 ||| 852 ||| 24953 ||| 26853 ||| 10282 ||| 
2020 ||| isaaq - mastering textbook questions with pre-trained transformers and bottom-up and top-down attention. ||| 852 ||| 15095 ||| 15096 ||| 15097 ||| 2600 ||| 23764 ||| 26854 ||| 
2021 ||| understanding and overcoming the challenges of efficient transformer quantization. ||| 26855 ||| 26856 ||| 26857 ||| 
2020 ||| attention is not only a weight: analyzing transformers with vector norms. ||| 26858 ||| 26859 ||| 26860 ||| 26674 ||| 
2018 ||| hierarchical attention network for context-aware query suggestion. ||| 1430 ||| 1255 ||| 633 ||| 26861 ||| 1377 ||| 1254 ||| 1256 ||| 
2020 ||| x-aware: context-aware human-environment attention fusion for driver gaze prediction in the wild. ||| 12147 ||| 26862 ||| 648 ||| 649 ||| 
2019 ||| understanding the attention demand of touch and tangible interaction on a composite task. ||| 25816 ||| 26863 ||| 26864 ||| 
2020 ||| temporal attention and consistency measuring for video question answering. ||| 26865 ||| 17779 ||| 
2019 ||| attention-driven interaction systems for augmented reality. ||| 13860 ||| 
2019 ||| improved visual focus of attention estimation and prosodic features for analyzing group interactions. ||| 26865 ||| 26866 ||| 26867 ||| 26868 ||| 26869 ||| 26870 ||| 21879 ||| 17779 ||| 
2020 ||| model-based prediction of exogeneous and endogeneous attention shifts during an everyday activity. ||| 13862 ||| 26871 ||| 13860 ||| 24320 ||| 
2020 ||| mebal: a multimodal database for eye blink detection and attention level estimation. ||| 26872 ||| 26873 ||| 26874 ||| 26875 ||| 14327 ||| 26876 ||| 
2020 ||| multi-rate attention based gru model for engagement prediction. ||| 26877 ||| 26878 ||| 15859 ||| 26879 ||| 26880 ||| 
2019 ||| multi-attention fusion network for video-based emotion recognition. ||| 4537 ||| 4538 ||| 26881 ||| 
2019 ||| modeling emotion influence using attention-based graph convolutional recurrent network. ||| 6839 ||| 4459 ||| 3138 ||| 
2021 ||| what's fair is fair: detecting and mitigating encoded bias in multimodal models of museum visitor attention. ||| 26882 ||| 26883 ||| 26884 ||| 26885 ||| 26886 ||| 26887 ||| 
2020 ||| implicit knowledge injectable cross attention audiovisual model for group emotion recognition. ||| 4537 ||| 4538 ||| 26888 ||| 26889 ||| 26890 ||| 26891 ||| 
2018 ||| looking beyond a clever narrative: visual context and attention are primary drivers of affect in video advertisements. ||| 26892 ||| 26893 ||| 19019 ||| 26894 ||| 
2018 ||| group-level emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions. ||| 15859 ||| 26877 ||| 26895 ||| 3419 ||| 26880 ||| 26879 ||| 
2018 ||| an attention model for group-level emotion recognition. ||| 26896 ||| 26897 ||| 26898 ||| 26899 ||| 2351 ||| 
2018 ||| attention network for engagement prediction in the wild. ||| 26900 ||| 
2021 ||| attention-based multimodal feature fusion for dance motion generation. ||| 26901 ||| 26902 ||| 26903 ||| 26904 ||| 
2018 ||| cascade attention networks for group emotion recognition with face, body and image cues. ||| 333 ||| 26905 ||| 8771 ||| 11244 ||| 26906 ||| 8769 ||| 2149 ||| 
2020 ||| multimodal physiological synchrony as measure of attentional engagement. ||| 24572 ||| 
2018 ||| sensing arousal and focal attention during visual interaction. ||| 10 ||| 26907 ||| 26908 ||| 
2019 ||| real-time multimodal classification of internal and external attention. ||| 13860 ||| 13861 ||| 26909 ||| 26910 ||| 13862 ||| 
2017 ||| multimodal language grounding for improved human-robot collaboration: exploring spatial semantic representations in the shared space of attention. ||| 21668 ||| 
2020 ||| attention sensing through multimodal user modeling in an augmented reality guessing game. ||| 13862 ||| 26911 ||| 26912 ||| 24522 ||| 26913 ||| 26914 ||| 
2018 ||| using interlocutor-modulated attention blstm to predict personality traits in small group interaction. ||| 12346 ||| 12347 ||| 
2018 ||| dozing off or thinking hard?: classifying multi-dimensional attentional states in the classroom from video. ||| 13862 ||| 26911 ||| 26912 ||| 26915 ||| 26909 ||| 
2018 ||| attention-based audio-visual fusion for robust automatic speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2018 ||| estimating visual focus of attention in multiparty meetings using deep convolutional neural networks. ||| 26916 ||| 26917 ||| 26918 ||| 12529 ||| 
2020 ||| detection of micro-expression recognition based on spatio-temporal modelling and spatial attention. ||| 26919 ||| 
2020 ||| smarthelm: towards multimodal detection of attention in an outdoor augmented reality biking scenario. ||| 26920 ||| 26921 ||| 26911 ||| 26912 ||| 13862 ||| 26922 ||| 26923 ||| 26924 ||| 11933 ||| 24320 ||| 
2020 ||| graph self-attention network for image captioning. ||| 26925 ||| 26926 ||| 
2021 ||| social media adverse drug reaction detection based on bi-lstm with multi-head attention mechanism. ||| 26927 ||| 26928 ||| 26929 ||| 
2019 ||| hierarchical attention network for predicting dna-protein binding sites. ||| 26930 ||| 26931 ||| 5289 ||| 26932 ||| 26933 ||| 
2020 ||| real-time object detection based on convolutional block attention module. ||| 26934 ||| 145 ||| 10212 ||| 
2021 ||| efficient face detector using spatial attention module in real-time application on an edge device. ||| 22184 ||| 4357 ||| 4353 ||| 
2020 ||| hgalinker: drug-disease association prediction based on attention mechanism of heterogeneous graph. ||| 26935 ||| 1706 ||| 26936 ||| 13572 ||| 10599 ||| 
2021 ||| attention-based deep multi-scale network for plant leaf recognition. ||| 5289 ||| 11796 ||| 18085 ||| 26937 ||| 26938 ||| 26939 ||| 26940 ||| 
2020 ||| aggregated deep saliency prediction by self-attention network. ||| 4354 ||| 26941 ||| 4353 ||| 
2021 ||| detection of drug-drug interactions through knowledge graph integrating multi-attention with capsule network. ||| 26942 ||| 26943 ||| 26944 ||| 26945 ||| 
2021 ||| graph semantics based neighboring attentional entity alignment for knowledge graphs. ||| 26946 ||| 5355 ||| 6049 ||| 
2021 ||| a diabetic retinopathy classification method based on novel attention mechanism. ||| 26947 ||| 24583 ||| 26948 ||| 
2020 ||| inferring disease-associated piwi-interacting rnas via graph attention networks. ||| 11162 ||| 26943 ||| 3279 ||| 26949 ||| 26950 ||| 
2020 ||| position attention-guided learning for infrared-visible person re-identification. ||| 5782 ||| 26951 ||| 8976 ||| 1589 ||| 26939 ||| 5289 ||| 26952 ||| 26953 ||| 
2018 ||| mandarin prosody prediction based on attention mechanism and multi-model ensemble. ||| 26954 ||| 26955 ||| 
2021 ||| multi-class text classification model based on weighted word vector and bilstm-attention optimization. ||| 3890 ||| 26956 ||| 26957 ||| 26958 ||| 26959 ||| 26960 ||| 
2021 ||| a lightweight attention fusion module for multi-sensor 3-d object detection. ||| 26961 ||| 26962 ||| 4353 ||| 
2017 ||| recognizing text entailment via bidirectional lstm model with inner-attention. ||| 10544 ||| 1305 ||| 16569 ||| 10546 ||| 10545 ||| 
2019 ||| flower species recognition system combining object detection and attention mechanism. ||| 26963 ||| 26964 ||| 26931 ||| 5289 ||| 26933 ||| 26932 ||| 26951 ||| 
2019 ||| missing data imputation for operation data of transformer based on functional principal component analysis and wavelet transform. ||| 14183 ||| 208 ||| 14185 ||| 14186 ||| 
2021 ||| fine-grained recognition of crop pests based on capsule network with attention mechanism. ||| 12699 ||| 26927 ||| 26928 ||| 26929 ||| 
2020 ||| paying deep attention to both neighbors and multiple tasks. ||| 26965 ||| 26966 ||| 26967 ||| 26968 ||| 26969 ||| 
2017 ||| mining implicit intention using attention-based rnn encoder-decoder model. ||| 4521 ||| 26970 ||| 26971 ||| 
2020 ||| depth guided attention for person re-identification. ||| 26972 ||| 26973 ||| 26974 ||| 26975 ||| 26976 ||| 
2021 ||| mitt: musical instrument timbre transfer based on the multichannel attention-guided mechanism. ||| 26977 ||| 12787 ||| 
2019 ||| using attention-based bidirectional lstm to identify different categories of offensive language directed toward female celebrities. ||| 26978 ||| 26979 ||| 
2020 ||| sam: self-attention based deep learning method for online traffic classification. ||| 26980 ||| 3477 ||| 3614 ||| 7895 ||| 26981 ||| 8207 ||| 26982 ||| 2748 ||| 
2019 ||| conditional dilated convolution attention tracking model. ||| 4557 ||| 26983 ||| 26984 ||| 26985 ||| 
2018 ||| multi-robot scheduling and path-planning for non-overlapping operator attention. ||| 26986 ||| 26987 ||| 26988 ||| 26989 ||| 19092 ||| 26990 ||| 
2017 ||| multi-robot planning for non-overlapping operator attention allocation. ||| 26986 ||| 26987 ||| 26991 ||| 26988 ||| 26990 ||| 
2020 ||| dualanet: dual lesion attention network for thoracic disease classification in chest x-rays. ||| 26992 ||| 26993 ||| 26994 ||| 2712 ||| 26995 ||| 26996 ||| 
2021 ||| graph attention autoencoder for collaborative pair-wise ranking. ||| 6248 ||| 6249 ||| 6250 ||| 1976 ||| 
2021 ||| knowledge graph construction and decision support towards transformer fault maintenance. ||| 26997 ||| 22040 ||| 
2021 ||| research on the impacts of feedback in instructional videos on college students' attention and learning effects. ||| 18768 ||| 3477 ||| 9464 ||| 
2021 ||| incorporating background knowledge into dialogue generation using multi-task transformer learning. ||| 26998 ||| 26999 ||| 
2019 ||| multilingual transformer ensembles for portuguese natural language tasks. ||| 16106 ||| 4194 ||| 27000 ||| 27001 ||| 3882 ||| 27002 ||| 27003 ||| 27004 ||| 
2019 ||| multilingual transformer ensembles for portuguese natural language tasks. ||| 27005 ||| 1994 ||| 27006 ||| 
2022 ||| view-invariant 3d skeleton-based human activity recognition based on transformer and spatio-temporal features. ||| 27007 ||| 27008 ||| 27009 ||| 
2020 ||| dimensionality reduction and attention mechanisms for extracting affective state from sound spectrograms. ||| 27010 ||| 27011 ||| 27012 ||| 27013 ||| 27014 ||| 27015 ||| 
2021 ||| a blended attention-ctc network architecture for amharic text-image recognition. ||| 27016 ||| 27017 ||| 27018 ||| 27019 ||| 23196 ||| 
2021 ||| upgraded w-net with attention gates and its application in unsupervised 3d liver segmentation. ||| 27020 ||| 27021 ||| 27022 ||| 20729 ||| 20730 ||| 
2021 ||| neurofeedback video games in the rehabilitative treatment of hyperactivity and attention deficit disorder for attention enhancement and hyperactivity reduction. ||| 27023 ||| 27024 ||| 27025 ||| 
2021 ||| comparison between active and passive attention using eeg waves and deep neural network. ||| 27026 ||| 27027 ||| 8367 ||| 27028 ||| 27029 ||| 
2021 ||| an attention-based mood controlling framework for social media users. ||| 27030 ||| 27031 ||| 27032 ||| 27033 ||| 27034 ||| 27035 ||| 27036 ||| 
2021 ||| towards learning a joint representation from transformer in multimodal emotion recognition. ||| 27037 ||| 27038 ||| 
2018 ||| functional connectivity analysis using the oddball auditory paradigm for attention tasks. ||| 27039 ||| 3882 ||| 27040 ||| 27041 ||| 2712 ||| 27042 ||| 27043 ||| 27044 ||| 18717 ||| 
2021 ||| the influence of clutter on search-based learning, long-term memory, and memory-guided attention in real-world scenes: an eye-movement research protocol. ||| 27045 ||| 27046 ||| 
2019 ||| deep learning investigation for chess player attention prediction using eye-tracking and game data. ||| 27047 ||| 27048 ||| 27049 ||| 27050 ||| 
2020 ||| eye-tracking and virtual reality in 360-degrees: exploring two ways to assess attentional orienting in rear space. ||| 59 ||| 27051 ||| 27052 ||| 27053 ||| 27054 ||| 27055 ||| 27056 ||| 
2019 ||| attention towards privacy notifications on web pages. ||| 27057 ||| 27058 ||| 27059 ||| 15940 ||| 
2021 ||| what the eyes can tell: analyzing visual attention with an educational video game. ||| 27060 ||| 5810 ||| 27061 ||| 27062 ||| 
2018 ||| how many words is a picture worth?: attention allocation on thumbnails versus title text regions. ||| 27063 ||| 27064 ||| 27065 ||| 27066 ||| 
2019 ||| characterizing joint attention behavior during real world interactions using automated object and gaze detection. ||| 27067 ||| 27068 ||| 27069 ||| 27070 ||| 27071 ||| 27072 ||| 27073 ||| 
2020 ||| how shared visual attention patterns of pairs unfold over time when workload changes. ||| 27074 ||| 27075 ||| 27076 ||| 
2021 ||| gaze interactive and attention aware low vision aids as future smart glasses. ||| 27077 ||| 27078 ||| 27079 ||| 27080 ||| 27081 ||| 27082 ||| 27083 ||| 27084 ||| 27085 ||| 
2020 ||| attention-based cross-modal unification of visualized text and image features: understanding the influence of interface and user idiosyncrasies on unification for free-viewing. ||| 24559 ||| 24560 ||| 27086 ||| 24561 ||| 
2019 ||| quantitative visual attention prediction on webpage images using multiclass svm. ||| 24559 ||| 24560 ||| 24561 ||| 
2019 ||| the vision and interpretation of paintings: bottom-up visual processes, top-down culturally informed attention, and aesthetic experience. ||| 27087 ||| 27088 ||| 27089 ||| 
2020 ||| towards capturing focal/ambient attention during dynamic wayfinding. ||| 27090 ||| 27091 ||| 27092 ||| 
2021 ||| estimation of visual attention using microsaccades in response to vibrations in the peripheral field of vision. ||| 27093 ||| 27094 ||| 
2020 ||| giuplayer: a gaze immersive youtube player enabling eye control and attention analysis. ||| 27095 ||| 16200 ||| 27096 ||| 27097 ||| 
2019 ||| attentional orienting in real and virtual 360-degree environments: applications to aeronautics. ||| 59 ||| 27051 ||| 27052 ||| 27055 ||| 27056 ||| 
2020 ||| toward a pervasive gaze-contingent assistance system: attention and context-awareness in augmented reality. ||| 27098 ||| 
2021 ||| climate change overlooked. the role of attitudes and mood regulation in visual attention to global warming. ||| 27099 ||| 
2019 ||| attentional orienting in virtual reality using endogenous and exogenous cues in auditory and visual modalities. ||| 59 ||| 27051 ||| 27052 ||| 27053 ||| 27055 ||| 27056 ||| 
2021 ||| 55 rides: attention annotated head and gaze data during naturalistic driving. ||| 27100 ||| 27101 ||| 27102 ||| 27103 ||| 7218 ||| 
2021 ||| saccades, attentional orienting and disengagement: the effects of anodal tdcs over right posterior parietal cortex (ppc) and frontal eye field (fef). ||| 27104 ||| 27105 ||| 27106 ||| 27107 ||| 1881 ||| 24141 ||| 14195 ||| 27108 ||| 
2018 ||| virtual reality as a proxy for real-life social attention? ||| 27109 ||| 27110 ||| 
2020 ||| detecting ambient/focal visual attention in professional airline pilots with a modified coefficient k: a full flight simulator study. ||| 27111 ||| 27112 ||| 27113 ||| 27056 ||| 17370 ||| 27114 ||| 
2020 ||| attention-based graph neural network enabled method to predict short-term metro passenger flow. ||| 1556 ||| 2013 ||| 27115 ||| 27116 ||| 27117 ||| 27118 ||| 
2017 ||| attention retargeting in real space with projector camera system. ||| 26040 ||| 26039 ||| 26041 ||| 
2021 ||| attention based multi-unit spatial-temporal network for traffic flow forecasting. ||| 27119 ||| 27120 ||| 27121 ||| 10330 ||| 
2020 ||| research on load forecasting method of distribution transformer based on deep learning. ||| 1207 ||| 27122 ||| 13684 ||| 21690 ||| 27123 ||| 17209 ||| 27124 ||| 
2021 ||| electric transformer oil leakage visual detection as service based on lstm and genetic algorithm. ||| 27125 ||| 27126 ||| 27127 ||| 27128 ||| 27129 ||| 25741 ||| 18912 ||| 27130 ||| 27131 ||| 
2020 ||| deriving interpretable rules for iot discovery through attention. ||| 17169 ||| 7199 ||| 
2020 ||| attention-based robot learning of haptic interaction. ||| 4222 ||| 4223 ||| 27132 ||| 4224 ||| 
2021 ||| uzh onpoint at swisstext-2021: sentence end and punctuation prediction in nlg text through ensembling of different transformers (short paper). ||| 27133 ||| 27134 ||| 12534 ||| 27135 ||| 
2020 ||| evaluating german transformer language models with syntactic agreement tests. ||| 27136 ||| 27137 ||| 27138 ||| 27139 ||| 12198 ||| 3831 ||| 
2021 ||| crisisbert: a robust transformer for crisis classification and contextual crisis embedding. ||| 17228 ||| 27140 ||| 27141 ||| 27142 ||| 27143 ||| 15208 ||| 
2018 ||| intellieye: enhancing mooc learners' video watching experience through real-time attention tracking. ||| 13883 ||| 3226 ||| 13884 ||| 13885 ||| 
2019 ||| human attention span modeling using 2d visualization plots for gaze progression and gaze sustenance. ||| 22517 ||| 27144 ||| 27145 ||| 27146 ||| 27147 ||| 
2019 ||| understanding and modelling human attention for soft biometrics purposes. ||| 18626 ||| 27148 ||| 27149 ||| 27150 ||| 18630 ||| 
2018 ||| enhanced character segmentation for multi-language data plate in substation transformer based on connected component analysis. ||| 27151 ||| 27152 ||| 27153 ||| 8260 ||| 15823 ||| 
2018 ||| impact of harmonic distortion on the energization of energy distribution transformers integrated in virtual power plants. ||| 27154 ||| 27155 ||| 8382 ||| 27156 ||| 27157 ||| 
2020 ||| hybrid feature network driven by attention and graph features for multiple sclerosis lesion segmentation from mr images. ||| 27158 ||| 27159 ||| 27160 ||| 
2020 ||| 6d object pose estimation with color/geometry attention fusion. ||| 27161 ||| 27162 ||| 
2020 ||| self-intersection attention pooling based classification for rock recognition. ||| 12373 ||| 27163 ||| 27164 ||| 27165 ||| 27166 ||| 
2020 ||| a residual network for de novo peptide sequencing with attention mechanism. ||| 27167 ||| 27168 ||| 
2018 ||| visual dialog with multi-turn attentional memory network. ||| 27169 ||| 2258 ||| 
2018 ||| multi-decoder based co-attention for image captioning. ||| 27170 ||| 10061 ||| 11241 ||| 4180 ||| 4183 ||| 
2018 ||| attention to refine through multi scales for semantic segmentation. ||| 27171 ||| 27172 ||| 
2017 ||| attention window aware encoder-decoder model for spoken language understanding. ||| 12382 ||| 4211 ||| 4210 ||| 27173 ||| 4214 ||| 
2018 ||| spatial-temporal attention for action recognition. ||| 27174 ||| 27175 ||| 27176 ||| 382 ||| 5755 ||| 
2018 ||| val: visual-attention action localizer. ||| 27177 ||| 7648 ||| 
2018 ||| spatial attention network for head detection. ||| 5674 ||| 3180 ||| 967 ||| 27178 ||| 4127 ||| 4128 ||| 
2018 ||| intra-view and inter-view attention for multi-view network embedding. ||| 27179 ||| 1066 ||| 7654 ||| 2258 ||| 
2018 ||| latitude-based visual attention in 360-degree video display. ||| 2915 ||| 27180 ||| 2916 ||| 27181 ||| 2919 ||| 
2018 ||| temporal-contextual attention network for video-based person re-identification. ||| 26851 ||| 8710 ||| 9614 ||| 18071 ||| 17860 ||| 
2018 ||| hand pose estimation with attention-and-sequence network. ||| 27182 ||| 2006 ||| 173 ||| 
2018 ||| reading document and answering question via global attentional inference. ||| 27183 ||| 17723 ||| 27184 ||| 1088 ||| 2258 ||| 
2018 ||| context and temporal aware attention model for flood prediction. ||| 20336 ||| 4285 ||| 18655 ||| 4287 ||| 173 ||| 
2017 ||| multi-modal emotion recognition with temporal-band attention based on lstm-rnn. ||| 27185 ||| 27186 ||| 6438 ||| 
2018 ||| multi-modal sequence to sequence learning with content attention for hotspot traffic speed prediction. ||| 27187 ||| 17723 ||| 27188 ||| 1088 ||| 2258 ||| 
2018 ||| scan: spatial and channel attention network for vehicle re-identification. ||| 27189 ||| 27190 ||| 14494 ||| 13825 ||| 
2018 ||| text-guided dual-branch attention network for visual question answering. ||| 27191 ||| 20194 ||| 4180 ||| 4183 ||| 
2018 ||| contextual attention model for social recommendation. ||| 27192 ||| 15205 ||| 27193 ||| 
2020 ||| semantic segmentation of nuclei from breast histopathological images by incorporating attention in u-net. ||| 27194 ||| 27195 ||| 27196 ||| 
2020 ||| ecasr: efficient channel attention based super-resolution. ||| 27197 ||| 27198 ||| 
2019 ||| automatic report generation for chest x-ray images: a multilevel multi-attention approach. ||| 27199 ||| 27200 ||| 27201 ||| 
2019 ||| traffic sign recognition using color and spatial transformer network on gpu embedded development board. ||| 27202 ||| 27203 ||| 
2020 ||| spatial attention for autonomous decision-making in highway scene. ||| 27204 ||| 27205 ||| 27206 ||| 
2020 ||| joint learning with pre-trained transformer on named entity recognition and relation extraction tasks for clinical analytics. ||| 2660 ||| 2662 ||| 2661 ||| 2663 ||| 
2020 ||| dilated convolutional attention network for medical code assignment from clinical text. ||| 27207 ||| 893 ||| 9366 ||| 
2020 ||| information extraction from swedish medical prescriptions with sig-transformer encoder. ||| 27208 ||| 1241 ||| 27209 ||| 27210 ||| 
2020 ||| incorporating risk factor embeddings in pre-trained transformers improves sentiment prediction in psychiatric discharge summaries. ||| 27211 ||| 27212 ||| 27213 ||| 
2020 ||| 3d medical image registration based on spatial attention. ||| 10768 ||| 27214 ||| 27215 ||| 
2019 ||| attentional bi-directional lstm for semantic attribute prediction. ||| 27216 ||| 27217 ||| 209 ||| 
2019 ||| improving variational auto-encoder with self-attention and mutual information for image generation. ||| 27218 ||| 5787 ||| 27219 ||| 
2020 ||| attention enhanced multi-patch deformable network for image deblurring. ||| 2383 ||| 209 ||| 
2021 ||| unsupervised monocular depth estimation with attention based inception pipe and overlap regularized loss. ||| 27220 ||| 5544 ||| 5543 ||| 
2019 ||| dangerous driving behavior detection with attention mechanism. ||| 11361 ||| 27221 ||| 19504 ||| 
2018 ||| template attentional siamese network for object tracking. ||| 27222 ||| 4267 ||| 4270 ||| 
2021 ||| construction site safety detection based on object detection with channel-wise attention. ||| 27223 ||| 27224 ||| 27225 ||| 27226 ||| 27227 ||| 27228 ||| 27229 ||| 10429 ||| 
2021 ||| a fine-grained classification method based on self-attention siamese network. ||| 27230 ||| 27231 ||| 27232 ||| 
2019 ||| attention-based generative graph convolutional network for skeleton-based human action recognition. ||| 5492 ||| 5491 ||| 5400 ||| 
2021 ||| u-shaped network based on transformer for 3d point clouds semantic segmentation. ||| 27233 ||| 27234 ||| 27235 ||| 27236 ||| 1770 ||| 
2020 ||| image super-resolution using hybrid attention mechanism. ||| 1037 ||| 27237 ||| 27238 ||| 
2020 ||| siamese region proposal networks and attention module for real-time visual tracking. ||| 4775 ||| 27239 ||| 27240 ||| 
2019 ||| mdanet: multiple fusion network with double attention for visual question answering. ||| 27241 ||| 10968 ||| 27242 ||| 
2021 ||| transformer-based bidirectional encoder representations for emotion detection from text. ||| 27243 ||| 893 ||| 27244 ||| 
2020 ||| detecting proper mask usage with soft attention. ||| 27245 ||| 27246 ||| 27247 ||| 27248 ||| 
2021 ||| dibert: dependency injected bidirectional encoder representations from transformers. ||| 27249 ||| 27250 ||| 
2020 ||| low-rank temporal attention-augmented bilinear network for financial time-series forecasting. ||| 27251 ||| 926 ||| 
2021 ||| improving transformer model translation for low resource south african languages using bert. ||| 27252 ||| 27253 ||| 
2019 ||| weakly supervised attention inference generative adversarial network for text-to-image. ||| 27254 ||| 27255 ||| 27256 ||| 
2020 ||| action detection based on 3d convolution neural network with channel attention mechanism. ||| 1446 ||| 27257 ||| 27258 ||| 27259 ||| 
2018 ||| self-attention enhanced recurrent neural networks for sentence classification. ||| 27260 ||| 27261 ||| 
2020 ||| inpainting electrical logging images based on deep cnn with attention mechanisms. ||| 27262 ||| 27263 ||| 27264 ||| 1224 ||| 27258 ||| 27259 ||| 
2020 ||| improvement of mixture-of-experts-type model to construct dynamic saliency maps for predicting drivers' attention. ||| 27265 ||| 27266 ||| 
2021 ||| multistream graph attention networks for wind speed forecasting. ||| 27267 ||| 27268 ||| 
2021 ||| electrode selection and convolutional attention network for recognition of silently spoken words from eeg signals. ||| 27269 ||| 27270 ||| 27271 ||| 
2020 ||| interpretable multivariate time series forecasting with temporal attention convolutional neural networks. ||| 27272 ||| 27273 ||| 27274 ||| 
2021 ||| a daily tourism demand prediction framework based on multi-head attention cnn: the case of the foreign entrant in south korea. ||| 27275 ||| 27276 ||| 27277 ||| 27278 ||| 27279 ||| 
2017 ||| diversity-guided generalized extremal optimization for transformer design problem. ||| 27280 ||| 27281 ||| 27282 ||| 27283 ||| 27284 ||| 27285 ||| 27286 ||| 
2019 ||| on machine learning apporaches towards dissolved gases analyses of power transformer oil chromatography. ||| 27287 ||| 27288 ||| 27289 ||| 10961 ||| 27290 ||| 27291 ||| 
2018 ||| gaan: gated attention networks for learning on large and spatiotemporal graphs. ||| 1339 ||| 18293 ||| 27292 ||| 25062 ||| 598 ||| 23917 ||| 
2019 ||| compatibility study of paper and epoxy with natural ester- a substitute for transformer mineral oil. ||| 27293 ||| 27294 ||| 27295 ||| 27296 ||| 27297 ||| 27298 ||| 
2018 ||| analyzing the single-line to ground fault current contribution by the type of transformer and distributed generator. ||| 27299 ||| 27300 ||| 27301 ||| 
2019 ||| multilingual cyber abuse detection using advanced transformer architecture. ||| 27302 ||| 18532 ||| 
2019 ||| deep recurrent architecture with attention for remaining useful life estimation. ||| 27303 ||| 27304 ||| 2320 ||| 27305 ||| 27306 ||| 
2019 ||| fork and d/y/d connected transformer supplied 12-pulse uncontrolled converter fed retrofit vector controlled induction motor drive: a comparative study. ||| 27307 ||| 27308 ||| 27309 ||| 
2021 ||| convolutional neural network or vision transformer? benchmarking various machine learning models for distracted driver detection. ||| 27310 ||| 27311 ||| 27312 ||| 
2019 ||| isolated switched boost dc-dc converter with coupled inductor and transformer. ||| 27313 ||| 27314 ||| 27315 ||| 27316 ||| 11435 ||| 
2021 ||| hyperspectral image classification based on multi-stage vision transformer with stacked samples. ||| 27317 ||| 11256 ||| 12142 ||| 
2018 ||| a method of modeling tap-changing transformers for power-flow and short-circuit analysis studies. ||| 27318 ||| 
2021 ||| investigating the vision transformer model for image retrieval tasks. ||| 27319 ||| 27320 ||| 27321 ||| 
2020 ||| music genre classification with transformer classifier. ||| 26259 ||| 27322 ||| 6837 ||| 
2021 ||| a real-time inference method of graph attention network based on knowledge graph for lung cancer. ||| 27323 ||| 27324 ||| 
2020 ||| at-gan: attention transfer gan for image-to-image translation. ||| 27325 ||| 24335 ||| 11220 ||| 11221 ||| 
2021 ||| boundary-aware transformers for skin lesion segmentation. ||| 22108 ||| 27326 ||| 15563 ||| 27327 ||| 978 ||| 8636 ||| 
2021 ||| ccut-net: pixel-wise global context channel attention ut-net for head and neck tumor segmentation. ||| 27328 ||| 27329 ||| 27330 ||| 27331 ||| 27332 ||| 
2021 ||| joint segmentation and quantification of main coronary vessels using dual-branch multi-scale attention network. ||| 27333 ||| 1751 ||| 27334 ||| 27335 ||| 
2021 ||| mil-vt: multiple instance learning enhanced vision transformer for fundus image classification. ||| 27336 ||| 343 ||| 27337 ||| 27338 ||| 27339 ||| 6748 ||| 27340 ||| 19375 ||| 27341 ||| 
2021 ||| priori and posteriori attention for generalizing head and neck tumors segmentation. ||| 27342 ||| 15622 ||| 27343 ||| 15623 ||| 
2021 ||| convolutional nets versus vision transformers for diabetic foot ulcer classification. ||| 27344 ||| 18877 ||| 15453 ||| 16473 ||| 27345 ||| 
2021 ||| gt u-net: a u-net like group transformer network for tooth root segmentation. ||| 27346 ||| 11634 ||| 1224 ||| 27347 ||| 27348 ||| 17296 ||| 27349 ||| 17295 ||| 
2019 ||| comparing deep learning strategies and attention mechanisms of discrete registration for multimodal image-guided interventions. ||| 27350 ||| 27351 ||| 
2020 ||| anatomy prior based u-net for pathology segmentation with attention. ||| 27352 ||| 19919 ||| 27353 ||| 27354 ||| 20755 ||| 
2021 ||| u-net transformer: self and cross attention for medical image segmentation. ||| 27355 ||| 27356 ||| 27357 ||| 27358 ||| 27359 ||| 27360 ||| 
2021 ||| multi-frame attention network for left ventricle segmentation in 3d echocardiography. ||| 27361 ||| 27362 ||| 27363 ||| 27364 ||| 27365 ||| 15551 ||| 
2020 ||| paying per-label attention for multi-label extraction from radiology reports. ||| 13366 ||| 27366 ||| 13368 ||| 13364 ||| 2227 ||| 13363 ||| 13365 ||| 27367 ||| 27368 ||| 27369 ||| 
2020 ||| attention-guided deep domain adaptation for brain dementia identification with multi-site neuroimaging data. ||| 27370 ||| 27371 ||| 27372 ||| 18051 ||| 27373 ||| 
2020 ||| learning bronchiole-sensitive airway segmentation cnns by feature recalibration and attention distillation. ||| 27374 ||| 27375 ||| 27376 ||| 11266 ||| 1132 ||| 27377 ||| 27378 ||| 
2021 ||| soft attention improves skin cancer classification performance. ||| 27379 ||| 9788 ||| 9791 ||| 27380 ||| 
2019 ||| deep cascaded attention network for multi-task brain tumor segmentation. ||| 27381 ||| 18071 ||| 27382 ||| 27383 ||| 27384 ||| 17860 ||| 
2021 ||| stacked hourglass network with a multi-level attention mechanism: where to look for intervertebral disc labeling. ||| 8804 ||| 27385 ||| 27386 ||| 
2020 ||| inside: steering spatial attention with non-imaging information in cnns. ||| 27387 ||| 18148 ||| 27369 ||| 27388 ||| 13369 ||| 
2020 ||| pranet: parallel reverse attention network for polyp segmentation. ||| 1861 ||| 4054 ||| 614 ||| 4055 ||| 4056 ||| 2445 ||| 1932 ||| 
2021 ||| imaging biomarker knowledge transfer for attention-based diagnosis of covid-19 in lung ultrasound videos. ||| 27389 ||| 27390 ||| 27391 ||| 27392 ||| 27393 ||| 27394 ||| 27395 ||| 27396 ||| 27397 ||| 27398 ||| 
2020 ||| attention, suggestion and annotation: a deep active learning framework for biomedical image segmentation. ||| 27399 ||| 24982 ||| 
2020 ||| lightweight double attention-fused networks for intraoperative stent segmentation. ||| 276 ||| 5186 ||| 271 ||| 5185 ||| 5184 ||| 27400 ||| 
2019 ||| deep learning via fused bidirectional attention stacked long short-term memory for obsessive-compulsive disorder diagnosis and risk screening. ||| 27401 ||| 27402 ||| 27403 ||| 6578 ||| 5476 ||| 6582 ||| 27404 ||| 
2021 ||| attention based cnn-lstm network for pulmonary embolism prediction on chest computed tomography pulmonary angiograms. ||| 27405 ||| 27406 ||| 27407 ||| 27408 ||| 27409 ||| 27410 ||| 18884 ||| 27411 ||| 
2019 ||| dense-residual attention network for skin lesion segmentation. ||| 27412 ||| 27413 ||| 2188 ||| 19803 ||| 
2018 ||| lstm spatial co-transformer networks for registration of 3d fetal us and mr brain images. ||| 27414 ||| 27415 ||| 27416 ||| 11933 ||| 27417 ||| 27418 ||| 27419 ||| 27420 ||| 27421 ||| 
2021 ||| kidney and kidney tumor segmentation using spatial and channel attention enhanced u-net. ||| 27422 ||| 27423 ||| 
2021 ||| combining attention-based multiple instance learning and gaussian processes for ct hemorrhage detection. ||| 27424 ||| 27425 ||| 27426 ||| 27427 ||| 4252 ||| 27428 ||| 27429 ||| 
2021 ||| spatial attention-based deep learning system for breast cancer pathological complete response prediction with serial histopathology images in multiple stains. ||| 27430 ||| 27431 ||| 27432 ||| 27433 ||| 6033 ||| 27434 ||| 27435 ||| 27436 ||| 27437 ||| 27438 ||| 27439 ||| 
2019 ||| reinforced transformer for medical image captioning. ||| 27440 ||| 17757 ||| 20023 ||| 
2021 ||| spine-transformers: vertebra detection and localization in arbitrary field-of-view spine ct with transformers. ||| 27441 ||| 27442 ||| 
2019 ||| residual attention generative adversarial networks for nuclei detection on routine colon cancer histology images. ||| 27443 ||| 5279 ||| 23347 ||| 27444 ||| 15620 ||| 
2019 ||| deep angular embedding and feature correlation attention for breast mri cancer analysis. ||| 27445 ||| 2424 ||| 232 ||| 27446 ||| 27447 ||| 27448 ||| 27449 ||| 8637 ||| 
2018 ||| a framework for identifying diabetic retinopathy based on anti-noise detection and attention-based fusion. ||| 19458 ||| 27450 ||| 10778 ||| 27451 ||| 27452 ||| 6613 ||| 16782 ||| 1236 ||| 
2021 ||| less is more: contrast attention assisted u-net for kidney, tumor and cyst segmentations. ||| 27453 ||| 27454 ||| 
2021 ||| surgical instruction generation with transformers. ||| 27455 ||| 27456 ||| 27457 ||| 27458 ||| 
2019 ||| a cascade attention network for liver lesion classification in weakly-labeled multi-phase ct images. ||| 3889 ||| 1550 ||| 1551 ||| 11468 ||| 1526 ||| 1549 ||| 1528 ||| 11469 ||| 1236 ||| 
2019 ||| permutohedral attention module for efficient non-local neural networks. ||| 27459 ||| 27460 ||| 27461 ||| 7111 ||| 14874 ||| 27462 ||| 27463 ||| 
2021 ||| co-segmentation of multi-modality spinal image using channel and spatial attention. ||| 27464 ||| 27465 ||| 
2021 ||| ted-net: convolution-free t2t vision transformer-based encoder-decoder dilation network for low-dose ct denoising. ||| 27466 ||| 27467 ||| 27468 ||| 
2021 ||| teds-net: enforcing diffeomorphisms in spatial transformers to guarantee topology preservation in segmentations. ||| 27469 ||| 27470 ||| 27471 ||| 27472 ||| 
2021 ||| ultrasound video transformers for cardiac ejection fraction estimation. ||| 27473 ||| 27474 ||| 27475 ||| 27476 ||| 27477 ||| 27478 ||| 
2020 ||| automatic localization of landmarks in craniomaxillofacial cbct images using a local attention-based graph convolution network. ||| 27479 ||| 27480 ||| 27481 ||| 27482 ||| 3559 ||| 27483 ||| 27484 ||| 27485 ||| 27372 ||| 27486 ||| 18051 ||| 
2020 ||| weakly supervised organ localization with attention maps regularized by local area reconstruction. ||| 27487 ||| 27488 ||| 27489 ||| 241 ||| 2490 ||| 
2018 ||| iterative attention mining for weakly supervised thoracic disease pattern localization in chest x-rays. ||| 27490 ||| 27491 ||| 27492 ||| 17961 ||| 27493 ||| 17965 ||| 
2018 ||| deep attentional features for prostate segmentation in ultrasound. ||| 9149 ||| 8633 ||| 8634 ||| 978 ||| 7676 ||| 8635 ||| 8637 ||| 20535 ||| 
2021 ||| learning with noise: mask-guided attention model for weakly supervised nuclei segmentation. ||| 27494 ||| 27495 ||| 326 ||| 
2019 ||| feature transformers: privacy preserving lifelong learners for medical imaging. ||| 27496 ||| 27497 ||| 27498 ||| 27499 ||| 27500 ||| 
2021 ||| e-dssr: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception. ||| 27501 ||| 1986 ||| 27502 ||| 27503 ||| 1991 ||| 1992 ||| 27446 ||| 
2020 ||| error attention interactive segmentation of medical image through matting and fusion. ||| 27504 ||| 27505 ||| 27506 ||| 27507 ||| 27508 ||| 27509 ||| 1222 ||| 12302 ||| 
2020 ||| prediction of plantar shear stress distribution by conditional gan with attention mechanism. ||| 27510 ||| 27511 ||| 5089 ||| 7011 ||| 7162 ||| 27512 ||| 
2020 ||| learning rich attention for pediatric bone age assessment. ||| 18070 ||| 18071 ||| 27513 ||| 5963 ||| 17860 ||| 
2020 ||| model-driven deep attention network for ultra-fast compressive sensing mri guided by cross-contrast mr image. ||| 3666 ||| 27514 ||| 27515 ||| 4394 ||| 8852 ||| 
2021 ||| instance-based vision transformer for subtyping of papillary renal cell carcinoma in histopathological image. ||| 27516 ||| 27517 ||| 27518 ||| 438 ||| 27519 ||| 16603 ||| 16606 ||| 8850 ||| 399 ||| 
2021 ||| utnet: a hybrid transformer architecture for medical image segmentation. ||| 27520 ||| 27521 ||| 1749 ||| 
2021 ||| a hybrid attention ensemble framework for zonal prostate segmentation. ||| 27522 ||| 27523 ||| 27524 ||| 
2020 ||| self-supervised nuclei segmentation in histopathological images using attention. ||| 27525 ||| 19515 ||| 27526 ||| 27527 ||| 27528 ||| 27529 ||| 27530 ||| 18883 ||| 
2019 ||| patch transformer for multi-tagging whole slide histopathology images. ||| 27531 ||| 27532 ||| 2070 ||| 27533 ||| 27534 ||| 2166 ||| 
2021 ||| predicting symptoms from multiphasic mri via multi-instance attention learning for hepatocellular carcinoma grading. ||| 27535 ||| 27536 ||| 27537 ||| 27538 ||| 9199 ||| 18051 ||| 
2020 ||| dual attention u-net for multi-sequence cardiac mr images segmentation. ||| 304 ||| 27539 ||| 27540 ||| 2230 ||| 27541 ||| 27542 ||| 
2018 ||| multi-task sonoeyenet: detection of fetal standardized planes assisted by generated sonographer attention maps. ||| 10019 ||| 10020 ||| 10021 ||| 10024 ||| 
2019 ||| weakly supervised brain lesion segmentation via attentional representation learning. ||| 27543 ||| 25310 ||| 27544 ||| 27545 ||| 27546 ||| 18962 ||| 
2019 ||| an attention-guided deep regression model for landmark detection in cephalograms. ||| 27547 ||| 4634 ||| 27548 ||| 27549 ||| 6621 ||| 
2019 ||| interpretable multimodality embedding of cerebral cortex using attention graph network for identifying bipolar disorder. ||| 27550 ||| 8776 ||| 27551 ||| 27552 ||| 20494 ||| 15551 ||| 27553 ||| 27554 ||| 
2020 ||| a generalizable deep-learning approach for cardiac magnetic resonance image segmentation using image augmentation and attention u-net. ||| 27555 ||| 27556 ||| 
2020 ||| asymmetrical multi-task attention u-net for the segmentation of prostate bed in ct image. ||| 27557 ||| 27480 ||| 11634 ||| 27558 ||| 27559 ||| 27560 ||| 27561 ||| 18051 ||| 
2019 ||| multi-task attention-based semi-supervised learning for medical image segmentation. ||| 27562 ||| 27563 ||| 27564 ||| 27565 ||| 2600 ||| 27566 ||| 27567 ||| 
2021 ||| few-shot domain adaptation with polymorphic transformers. ||| 23339 ||| 23340 ||| 3362 ||| 4056 ||| 23341 ||| 27568 ||| 23342 ||| 4297 ||| 27569 ||| 3390 ||| 
2020 ||| attention-guided quality assessment for automated cryo-em grid screening. ||| 27570 ||| 27571 ||| 27572 ||| 
2021 ||| multi-view surgical video action detection via mixed global view attention. ||| 27573 ||| 27574 ||| 27575 ||| 27576 ||| 27577 ||| 
2020 ||| microscopic fine-grained instance classification through deep attention. ||| 15545 ||| 15546 ||| 15547 ||| 2377 ||| 15548 ||| 
2021 ||| ccbanet: cascading context and balancing attention for polyp segmentation. ||| 27578 ||| 27579 ||| 27580 ||| 27581 ||| 27582 ||| 4033 ||| 
2021 ||| convolution-free medical image segmentation using transformers. ||| 27583 ||| 27584 ||| 27585 ||| 
2020 ||| domain-invariant prior knowledge guided attention networks for robust skull stripping of developing macaque brains. ||| 27586 ||| 9472 ||| 27587 ||| 27588 ||| 17773 ||| 27589 ||| 1052 ||| 18051 ||| 858 ||| 
2021 ||| learning dual transformer network for diffeomorphic registration. ||| 27590 ||| 27591 ||| 5668 ||| 
2019 ||| mvp-net: multi-view fpn with position-aware attention for deep universal lesion detection. ||| 2552 ||| 6588 ||| 4258 ||| 27592 ||| 2142 ||| 1801 ||| 
2021 ||| self-guided multi-attention network for periventricular leukomalacia recognition. ||| 27593 ||| 27594 ||| 1956 ||| 15645 ||| 1270 ||| 27595 ||| 13335 ||| 8968 ||| 27596 ||| 27597 ||| 1825 ||| 577 ||| 
2021 ||| multi-head gagnn: a multi-head guided attention graph neural network for modeling spatio-temporal patterns of holistic brain functional networks. ||| 6461 ||| 2957 ||| 6590 ||| 6588 ||| 6462 ||| 6463 ||| 6589 ||| 3473 ||| 6592 ||| 6593 ||| 6466 ||| 6467 ||| 
2020 ||| nas-scam: neural architecture search-based spatial and channel joint attention module for nuclei semantic segmentation and classification. ||| 15621 ||| 2054 ||| 15555 ||| 15623 ||| 27598 ||| 
2021 ||| cross-modal attention for mri and ultrasound volume registration. ||| 27599 ||| 27600 ||| 27557 ||| 27601 ||| 27602 ||| 27603 ||| 27604 ||| 8534 ||| 20023 ||| 
2021 ||| transformer network for significant stenosis detection in ccta of coronary arteries. ||| 27605 ||| 27606 ||| 1160 ||| 20424 ||| 
2019 ||| multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures. ||| 27607 ||| 27608 ||| 27609 ||| 27610 ||| 247 ||| 8660 ||| 
2021 ||| aligntransformer: hierarchical alignment of visual regions and disease tags for medical report generation. ||| 1283 ||| 3746 ||| 3749 ||| 27611 ||| 875 ||| 3748 ||| 
2020 ||| end-to-end coordinate regression model with attention-guided mechanism for landmark localization in 3d medical images. ||| 27612 ||| 27613 ||| 27614 ||| 858 ||| 27615 ||| 
2019 ||| rca-u-net: residual channel attention u-net for fast tissue quantification in magnetic resonance fingerprinting. ||| 27616 ||| 20639 ||| 18047 ||| 27617 ||| 18051 ||| 
2020 ||| multimodality biomedical image registration using free point transformer networks. ||| 27618 ||| 27619 ||| 27620 ||| 
2021 ||| integration of patch features through self-supervised learning and transformer for survival analysis on whole slide images. ||| 16689 ||| 18199 ||| 16688 ||| 16690 ||| 16595 ||| 16691 ||| 
2019 ||| multi-task learning for neonatal brain segmentation using 3d dense-unet with dense attention guided by geodesic distance. ||| 27621 ||| 1052 ||| 9375 ||| 27617 ||| 858 ||| 18051 ||| 
2021 ||| multimodal spatial attention network for automatic head and neck tumor segmentation in fdg-pet and ct images. ||| 27622 ||| 27623 ||| 27624 ||| 27625 ||| 27626 ||| 27627 ||| 
2018 ||| multimodal recurrent model with attention for automated radiology report generation. ||| 27628 ||| 19025 ||| 27629 ||| 27630 ||| 27631 ||| 27632 ||| 19027 ||| 
2018 ||| multiview two-task recursive attention model for left atrium and atrial scars segmentation. ||| 1785 ||| 6005 ||| 27334 ||| 27633 ||| 27634 ||| 27635 ||| 27636 ||| 25716 ||| 27637 ||| 27335 ||| 27638 ||| 27639 ||| 
2020 ||| graph attention multi-instance learning for accurate colorectal cancer staging. ||| 27640 ||| 27641 ||| 27642 ||| 27643 ||| 1265 ||| 
2021 ||| co-graph attention reasoning based imaging and clinical features integration for lymph node metastasis prediction. ||| 421 ||| 24119 ||| 27644 ||| 27645 ||| 27646 ||| 27647 ||| 27648 ||| 27649 ||| 27650 ||| 27651 ||| 10536 ||| 14786 ||| 
2021 ||| sama: spatially-aware multimodal network with attention for early lung cancer diagnosis. ||| 27652 ||| 27653 ||| 10907 ||| 27654 ||| 27655 ||| 27656 ||| 
2019 ||| triple anet: adaptive abnormal-aware attention network for wce image classification. ||| 27657 ||| 25547 ||| 
2019 ||| rsanet: recurrent slice-wise attention network for multiple sclerosis lesion segmentation. ||| 17750 ||| 17751 ||| 17753 ||| 27658 ||| 2973 ||| 27659 ||| 17754 ||| 17755 ||| 9389 ||| 9149 ||| 
2021 ||| deep mri reconstruction with generative vision transformers. ||| 27660 ||| 27661 ||| 27662 ||| 27663 ||| 27664 ||| 27665 ||| 27666 ||| 
2021 ||| multimodal pet/ct tumour segmentation and prediction of progression-free survival using a full-scale unet with attention. ||| 27667 ||| 27668 ||| 27669 ||| 27670 ||| 
2021 ||| opera: attention-regularized transformers for surgical phase recognition. ||| 27671 ||| 27672 ||| 27673 ||| 20025 ||| 27674 ||| 13628 ||| 
2021 ||| efficient multi-model vision transformer based on feature fusion for classification of dfuc2021 challenge. ||| 27675 ||| 27676 ||| 27677 ||| 20364 ||| 20365 ||| 
2021 ||| focusing on clinically interpretable features: selective attention regularization for liver biopsy image classification. ||| 27678 ||| 17302 ||| 27679 ||| 27680 ||| 
2020 ||| detect and identify aneurysms based on adjusted 3d attention unet. ||| 27681 ||| 27682 ||| 27683 ||| 27684 ||| 27685 ||| 27686 ||| 27687 ||| 
2021 ||| end-to-end ugly duckling sign detection for melanoma identification with transformers. ||| 27688 ||| 27689 ||| 27690 ||| 27691 ||| 27692 ||| 241 ||| 27693 ||| 
2021 ||| transbridge: a lightweight transformer for left ventricle segmentation in echocardiography. ||| 27694 ||| 27695 ||| 27696 ||| 27697 ||| 27698 ||| 27699 ||| 24408 ||| 27700 ||| 
2021 ||| the head and neck tumor segmentation in pet/ct based on multi-channel attention network. ||| 27701 ||| 27702 ||| 10211 ||| 27703 ||| 
2021 ||| lesion segmentation and recist diameter prediction via click-driven attention and dual-path connection. ||| 14910 ||| 2379 ||| 27490 ||| 27704 ||| 3395 ||| 705 ||| 27705 ||| 27706 ||| 27491 ||| 
2020 ||| cerebrovascular segmentation in mra via reverse edge attention network. ||| 3386 ||| 27707 ||| 2953 ||| 24413 ||| 27708 ||| 5206 ||| 24408 ||| 
2019 ||| encoder-decoder attention network for lesion segmentation of diabetic retinopathy. ||| 24687 ||| 24688 ||| 27709 ||| 24692 ||| 27710 ||| 24686 ||| 
2019 ||| the channel attention based context encoder network for inner limiting membrane detection. ||| 27711 ||| 24411 ||| 27712 ||| 24407 ||| 27713 ||| 24408 ||| 5206 ||| 15003 ||| 
2021 ||| dual-branch attention network and atrous spatial pyramid pooling for diabetic retinopathy classification using ultra-widefield images. ||| 27714 ||| 27715 ||| 27716 ||| 27717 ||| 27718 ||| 27719 ||| 15670 ||| 6582 ||| 
2021 ||| 3d transformer-gan for high-quality pet reconstruction. ||| 27720 ||| 247 ||| 27721 ||| 27722 ||| 25649 ||| 25648 ||| 18051 ||| 27723 ||| 
2021 ||| attention-based multi-scale gated recurrent encoder with novel correlation loss for covid-19 progression prediction. ||| 27724 ||| 27725 ||| 27406 ||| 27408 ||| 27726 ||| 27409 ||| 27410 ||| 27411 ||| 
2021 ||| dllnet: an attention-based deep learning method for dental landmark localization on high-resolution 3d digital dental models. ||| 27479 ||| 27482 ||| 27481 ||| 27480 ||| 27727 ||| 27483 ||| 27372 ||| 27486 ||| 
2020 ||| automatic head and neck tumor segmentation in pet/ct with scale attention network. ||| 24665 ||| 
2019 ||| automated segmentation of skin lesion based on pyramid attention network. ||| 2054 ||| 15623 ||| 27728 ||| 15555 ||| 
2021 ||| transbts: multimodal brain tumor segmentation using transformer. ||| 11656 ||| 2230 ||| 27541 ||| 304 ||| 27539 ||| 27542 ||| 
2021 ||| multi-compound transformer for accurate biomedical image segmentation. ||| 9148 ||| 27729 ||| 27730 ||| 5189 ||| 27731 ||| 15555 ||| 2011 ||| 
2021 ||| task transformer network for joint mri reconstruction and super-resolution. ||| 27732 ||| 27733 ||| 4056 ||| 3036 ||| 1125 ||| 
2019 ||| volumetric attention for 3d medical image segmentation and detection. ||| 12824 ||| 27734 ||| 27735 ||| 18677 ||| 18678 ||| 
2021 ||| predicting esophageal fistula risks using a multimodal self-attention network. ||| 27736 ||| 421 ||| 27648 ||| 27644 ||| 14782 ||| 27737 ||| 24119 ||| 27650 ||| 10536 ||| 27738 ||| 
2019 ||| arrhythmia classification with attention-based res-bilstm-net. ||| 16788 ||| 27739 ||| 16789 ||| 27740 ||| 
2019 ||| novel iterative attention focusing strategy for joint pathology localization and prediction of mci progression. ||| 27741 ||| 27742 ||| 2903 ||| 1956 ||| 16567 ||| 27743 ||| 27744 ||| 27596 ||| 27745 ||| 27597 ||| 26582 ||| 
2020 ||| infant cognitive scores prediction with multi-stream attention-based temporal path signature features. ||| 1340 ||| 27746 ||| 27633 ||| 17772 ||| 4482 ||| 27747 ||| 1052 ||| 27617 ||| 18051 ||| 858 ||| 
2021 ||| symmetry-enhanced attention network for acute ischemic infarct segmentation with non-contrast ct images. ||| 27748 ||| 19744 ||| 27749 ||| 27750 ||| 2608 ||| 2142 ||| 1801 ||| 
2019 ||| dpanet: a novel network based on dense pyramid feature extractor and dual correlation analysis attention modules for colon glands segmentation. ||| 27751 ||| 7240 ||| 2259 ||| 27752 ||| 27753 ||| 27754 ||| 27755 ||| 
2021 ||| skip-scse multi-scale attention and co-learning method for oropharyngeal tumor segmentation on multi-modal pet-ct images. ||| 27756 ||| 27757 ||| 27758 ||| 27759 ||| 27760 ||| 27761 ||| 27762 ||| 
2021 ||| false positive suppression in cervical cell screening via attention-guided semi-supervised learning. ||| 27763 ||| 15645 ||| 27764 ||| 577 ||| 15648 ||| 
2020 ||| pay more attention to discontinuity for medical image segmentation. ||| 27765 ||| 27766 ||| 5067 ||| 27767 ||| 14171 ||| 27768 ||| 27769 ||| 27770 ||| 
2019 ||| end-to-end dementia status prediction from brain mri using multi-task weakly-supervised attention network. ||| 27480 ||| 27373 ||| 1052 ||| 18051 ||| 
2021 ||| multi-view analysis of unregistered medical images using cross-view transformers. ||| 27566 ||| 27771 ||| 27772 ||| 
2020 ||| high-order attention networks for medical image segmentation. ||| 23784 ||| 5908 ||| 1035 ||| 5896 ||| 27773 ||| 27774 ||| 5907 ||| 
2019 ||| improving deep lesion detection using 3d contextual and spatial attention. ||| 27775 ||| 27693 ||| 1691 ||| 17666 ||| 17667 ||| 
2021 ||| shallow attention network for polyp segmentation. ||| 27776 ||| 27777 ||| 27729 ||| 5189 ||| 27778 ||| 13677 ||| 
2019 ||| attention-guided decoder in dilated residual network for accurate aortic valve segmentation in 3d ct scans. ||| 27779 ||| 27780 ||| 27781 ||| 27782 ||| 27783 ||| 27784 ||| 27785 ||| 27786 ||| 27787 ||| 27788 ||| 
2021 ||| transformesh: a transformer network for longitudinal modeling of anatomical meshes. ||| 27789 ||| 27790 ||| 27791 ||| 27792 ||| 
2020 ||| max-fusion u-net for multi-modal pathology segmentation with attention and dynamic resampling. ||| 27793 ||| 27794 ||| 27795 ||| 13369 ||| 
2017 ||| automatic classification of proximal femur fractures based on attention models. ||| 15676 ||| 15680 ||| 27796 ||| 27797 ||| 4252 ||| 27798 ||| 27799 ||| 13628 ||| 27800 ||| 
2021 ||| transct: dual-path transformer for low dose computed tomography. ||| 27801 ||| 27802 ||| 27803 ||| 7700 ||| 27804 ||| 
2021 ||| medical transformer: gated axial-attention for medical image segmentation. ||| 27805 ||| 19134 ||| 27806 ||| 18609 ||| 
2019 ||| unified attentional generative adversarial network for brain tumor segmentation from multimodal unpaired images. ||| 27807 ||| 18807 ||| 27808 ||| 3219 ||| 27809 ||| 
2019 ||| multi-label thoracic disease image classification with cross-attention networks. ||| 27810 ||| 27811 ||| 3303 ||| 
2020 ||| joint left atrial segmentation and scar quantification based on a dnn with spatial encoding and shape attention. ||| 3034 ||| 27812 ||| 27421 ||| 20755 ||| 
2018 ||| asdnet: attention based semi-supervised deep networks for medical image segmentation. ||| 18047 ||| 27813 ||| 1052 ||| 18051 ||| 
2017 ||| attention-driven deep learning for pathological spine segmentation. ||| 27814 ||| 27815 ||| 27816 ||| 14848 ||| 27817 ||| 
2021 ||| hierarchical attention guided framework for multi-resolution collaborative whole slide image segmentation. ||| 27818 ||| 27819 ||| 27820 ||| 27821 ||| 27822 ||| 2067 ||| 27823 ||| 6756 ||| 1265 ||| 27824 ||| 2527 ||| 16746 ||| 
2021 ||| a topological-attention convlstm network and its application to em images. ||| 27825 ||| 27826 ||| 6915 ||| 27827 ||| 
2021 ||| improving pneumonia localization via cross-attention on medical images and reports. ||| 27828 ||| 7205 ||| 27829 ||| 2022 ||| 2021 ||| 27830 ||| 2024 ||| 
2021 ||| integrating multimodal mris for adult adhd identification with heterogeneous graph attention convolutional network. ||| 27831 ||| 27371 ||| 21803 ||| 15655 ||| 27373 ||| 
2019 ||| et-net: a generic edge-attention guidance network for medical image segmentation. ||| 6925 ||| 4056 ||| 8582 ||| 2445 ||| 2279 ||| 1932 ||| 
2019 ||| automatic detection of ecg abnormalities by using an ensemble of deep residual networks with attention. ||| 1305 ||| 9897 ||| 20424 ||| 20426 ||| 4103 ||| 20430 ||| 20429 ||| 
2020 ||| attention-guided deep graph neural network for longitudinal alzheimer's disease analysis. ||| 27832 ||| 13237 ||| 27833 ||| 27834 ||| 27835 ||| 
2021 ||| domain composition and attention for unseen-domain generalizable medical image segmentation. ||| 27343 ||| 14392 ||| 1858 ||| 15622 ||| 15623 ||| 15555 ||| 
2021 ||| u-net with hierarchical bottleneck attention for landmark detection in fundus images of the degenerated retina. ||| 27836 ||| 27837 ||| 27838 ||| 27839 ||| 
2020 ||| image-level harmonization of multi-site data using image-and-spatial transformer networks. ||| 27840 ||| 27446 ||| 27841 ||| 27842 ||| 27843 ||| 14912 ||| 27420 ||| 27844 ||| 
2019 ||| attention guided network for retinal image segmentation. ||| 27845 ||| 4056 ||| 27846 ||| 27847 ||| 1827 ||| 19774 ||| 6413 ||| 15560 ||| 
2019 ||| achieving accurate segmentation of nasopharyngeal carcinoma in mr images through recurrent attention. ||| 21451 ||| 27848 ||| 27849 ||| 27850 ||| 24363 ||| 27851 ||| 
2021 ||| mftrans-net: quantitative measurement of hepatocellular carcinoma via multi-function transformer regression network. ||| 27852 ||| 27853 ||| 17238 ||| 27854 ||| 27855 ||| 9283 ||| 27856 ||| 
2021 ||| transfuse: fusing transformers and cnns for medical image segmentation. ||| 7430 ||| 27857 ||| 27858 ||| 
2019 ||| automatic segmentation of vestibular schwannoma from t2-weighted mri by deep spatial attention with hardness-weighted loss. ||| 15623 ||| 27859 ||| 27860 ||| 27460 ||| 27861 ||| 27862 ||| 27863 ||| 27864 ||| 15555 ||| 7111 ||| 14874 ||| 27462 ||| 
2020 ||| generalizing spatial transformers to projective geometry with applications to 2d/3d registration. ||| 27865 ||| 1987 ||| 27866 ||| 27867 ||| 27868 ||| 1991 ||| 1992 ||| 
2021 ||| pay attention with focus: a novel learning scheme for classification of whole slide images. ||| 27869 ||| 27870 ||| 27871 ||| 27872 ||| 27873 ||| 27874 ||| 
2021 ||| multi-scale hybrid transformer networks: application to prostate disease classification. ||| 27875 ||| 27876 ||| 27877 ||| 27878 ||| 27844 ||| 27879 ||| 
2020 ||| multimodal priors guided segmentation of liver lesions in mri using mutual information based graph co-attention networks. ||| 27880 ||| 27881 ||| 1550 ||| 11469 ||| 27882 ||| 10836 ||| 1551 ||| 1526 ||| 6422 ||| 1528 ||| 
2021 ||| conditional gan with an attention-based generator and a 3d discriminator for 3d medical image generation. ||| 27883 ||| 27884 ||| 25503 ||| 
2021 ||| cotr: efficiently bridging cnn and transformer for 3d medical image segmentation. ||| 27885 ||| 27886 ||| 6335 ||| 9199 ||| 
2020 ||| 3d attention u-net with pretraining: a solution to cada-aneurysm segmentation challenge. ||| 27684 ||| 27681 ||| 27682 ||| 27683 ||| 27685 ||| 27686 ||| 27687 ||| 
2021 ||| effective pancreatic cancer screening on non-contrast ct scans via anatomy-aware transformers. ||| 27887 ||| 27641 ||| 27491 ||| 27704 ||| 3395 ||| 705 ||| 8660 ||| 27888 ||| 6133 ||| 
2020 ||| spatiotemporal attention autoencoder (staae) for adhd classification. ||| 15543 ||| 27889 ||| 15541 ||| 2008 ||| 6593 ||| 27890 ||| 
2020 ||| demographic-guided attention in recurrent neural networks for modeling neuropathophysiological heterogeneity. ||| 27891 ||| 8776 ||| 27892 ||| 27893 ||| 15551 ||| 
2017 ||| ssemnet: serial-section electron microscopy image registration using a spatial transformer network with learned features. ||| 27894 ||| 27895 ||| 27896 ||| 27897 ||| 27898 ||| 
2020 ||| attention based multiple instance learning for classification of blood cell disorders. ||| 27899 ||| 27900 ||| 27901 ||| 13628 ||| 27902 ||| 15680 ||| 27903 ||| 
2021 ||| 3d u-net applied to simple attention module for head and neck tumor segmentation in pet and ct images. ||| 5463 ||| 27904 ||| 27905 ||| 27906 ||| 27907 ||| 
2021 ||| covid-net us: a tailored, highly efficient, self-attention deep convolutional neural network design for detection of covid-19 patient cases from point-of-care ultrasound imaging. ||| 27908 ||| 27909 ||| 27910 ||| 27911 ||| 27912 ||| 27913 ||| 27914 ||| 27915 ||| 7865 ||| 
2021 ||| trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer. ||| 27916 ||| 27917 ||| 27501 ||| 27446 ||| 8637 ||| 
2018 ||| attention based hierarchical aggregation network for 3d left atrial segmentation. ||| 27918 ||| 27919 ||| 27920 ||| 27921 ||| 27922 ||| 13217 ||| 8637 ||| 
2018 ||| attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ||| 14911 ||| 2021 ||| 27492 ||| 27491 ||| 705 ||| 14912 ||| 
2017 ||| agnet: attention-guided network for surgical tool presence detection. ||| 8634 ||| 27802 ||| 2424 ||| 8636 ||| 8637 ||| 
2019 ||| msafusionnet: multiple subspace attention based deep multi-modal fusion network. ||| 10418 ||| 27923 ||| 26620 ||| 27924 ||| 27768 ||| 27925 ||| 19282 ||| 2166 ||| 
2019 ||| cross-modal attention-guided convolutional network for multi-modal cardiac segmentation. ||| 17974 ||| 27926 ||| 27927 ||| 27928 ||| 27723 ||| 3279 ||| 19774 ||| 
2021 ||| graph transformers for characterization and interpretation of surgical margins. ||| 27929 ||| 27930 ||| 27931 ||| 27932 ||| 27933 ||| 27934 ||| 27935 ||| 27936 ||| 27937 ||| 27938 ||| 27939 ||| 27940 ||| 27941 ||| 
2019 ||| cs-net: channel and spatial attention network for curvilinear structure segmentation. ||| 27712 ||| 24408 ||| 3036 ||| 15003 ||| 24411 ||| 27708 ||| 27942 ||| 27700 ||| 27943 ||| 5206 ||| 
2020 ||| cascaded attention guided network for retinal vessel segmentation. ||| 27944 ||| 2206 ||| 2207 ||| 8709 ||| 
2019 ||| image-and-spatial transformer networks for structure-guided image registration. ||| 27945 ||| 27946 ||| 27947 ||| 27948 ||| 27844 ||| 
2020 ||| collaborative learning of cross-channel clinical attention for radiotherapy-related esophageal fistula prediction from ct. ||| 421 ||| 27648 ||| 27650 ||| 10536 ||| 14786 ||| 
2021 ||| attention guided slit lamp image quality assessment. ||| 27949 ||| 27950 ||| 27951 ||| 25647 ||| 27952 ||| 
2020 ||| generalisable cardiac structure segmentation via attentional and stacked image adaptation. ||| 14846 ||| 14896 ||| 14848 ||| 
2021 ||| m-seam-nam: multi-instance self-supervised equivalent attention mechanism with neighborhood affinity module for double weakly supervised segmentation of covid-19. ||| 18566 ||| 27953 ||| 8539 ||| 27954 ||| 5752 ||| 14891 ||| 27955 ||| 
2019 ||| graph convolution based attention model for personalized disease prediction. ||| 15676 ||| 15678 ||| 15677 ||| 27956 ||| 27957 ||| 27958 ||| 27959 ||| 5335 ||| 27960 ||| 15680 ||| 13628 ||| 
2018 ||| predicting cancer with a recurrent visual attention model for histopathology images. ||| 3419 ||| 27961 ||| 7837 ||| 
2019 ||| biomedical image segmentation by retina-like sequential attention mechanism using only a few training images. ||| 27962 ||| 27963 ||| 27964 ||| 27965 ||| 
2021 ||| ratchet: medical transformer for chest x-ray diagnosis and reporting. ||| 27475 ||| 27966 ||| 14912 ||| 27478 ||| 
2018 ||| a diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism. ||| 27967 ||| 17546 ||| 27968 ||| 20713 ||| 
2021 ||| automated kidney tumor segmentation with convolution and transformer network. ||| 19634 ||| 1007 ||| 19749 ||| 13810 ||| 
2019 ||| brain segmentation from k-space with end-to-end recurrent attention network. ||| 15514 ||| 3889 ||| 1749 ||| 27969 ||| 
2021 ||| transpath: transformer-based self-supervised learning for histopathological image classification. ||| 27970 ||| 2095 ||| 1796 ||| 27971 ||| 875 ||| 1265 ||| 2334 ||| 5785 ||| 
2021 ||| 3dmet: 3d medical image transformer for knee cartilage defect assessment. ||| 1270 ||| 27972 ||| 15646 ||| 24239 ||| 27597 ||| 27973 ||| 5439 ||| 27974 ||| 15648 ||| 577 ||| 18051 ||| 
2021 ||| dt-mil: deformable transformer for multi-instance learning on histopathological image. ||| 6799 ||| 1856 ||| 3473 ||| 16744 ||| 1796 ||| 16745 ||| 1265 ||| 15563 ||| 16746 ||| 
2021 ||| a multi-branch hybrid transformer network for corneal endothelial cell segmentation. ||| 27975 ||| 5203 ||| 4056 ||| 15560 ||| 1420 ||| 27976 ||| 12196 ||| 5206 ||| 
2021 ||| progressively normalized self-attention network for video polyp segmentation. ||| 4054 ||| 27977 ||| 1861 ||| 4055 ||| 4056 ||| 20165 ||| 1932 ||| 
2019 ||| feature pyramid based attention for cervical image classification. ||| 27978 ||| 4095 ||| 254 ||| 6644 ||| 2884 ||| 27890 ||| 
2020 ||| learned deep radiomics for survival analysis with attention. ||| 27979 ||| 27980 ||| 27981 ||| 27982 ||| 8382 ||| 27983 ||| 27984 ||| 27985 ||| 27986 ||| 27987 ||| 27988 ||| 1226 ||| 27989 ||| 59 ||| 27990 ||| 27800 ||| 
2020 ||| multi-task dynamic transformer network for concurrent bone segmentation and large-scale landmark localization with dental cbct. ||| 27480 ||| 1704 ||| 27482 ||| 1052 ||| 27481 ||| 27727 ||| 27991 ||| 27483 ||| 27484 ||| 27372 ||| 27486 ||| 18051 ||| 
2017 ||| attention-based lstm-cnns for uncertainty identification on chinese social media texts. ||| 27992 ||| 27993 ||| 1310 ||| 17798 ||| 27994 ||| 
2017 ||| modeling anomalous attention over an online social network through read/post analytics. ||| 18275 ||| 25997 ||| 
2021 ||| design and research of transformer fault diagnosis method based on data-driven. ||| 27995 ||| 27996 ||| 27997 ||| 4149 ||| 11318 ||| 27998 ||| 5089 ||| 
2017 ||| a convolutional attentional neural network for sentiment classification. ||| 21175 ||| 3662 ||| 3664 ||| 15906 ||| 
2021 ||| math word problem solver based on text-to-text transformer model. ||| 27999 ||| 28000 ||| 28001 ||| 28002 ||| 
2019 ||| design of online learning mobile app for the elderly based on attention, relevance, confidence, and satisfaction (arcs) motivation model. ||| 6579 ||| 28003 ||| 5107 ||| 
2020 ||| interactive attention model explorer for natural language processing tasks with unbalanced data sizes. ||| 28004 ||| 28005 ||| 28006 ||| 28007 ||| 
2021 ||| keywordmap: attention-based visual exploration for keyword analysis. ||| 28008 ||| 28009 ||| 28010 ||| 
2019 ||| simulation of spatial memory for human navigation based on visual attention in floorplan review. ||| 28011 ||| 28012 ||| 
2020 ||| spotnet: self-attention multi-task network for object detection. ||| 28013 ||| 28014 ||| 28015 ||| 28016 ||| 28017 ||| 
2019 ||| traffic risk assessment: a two-stream approach using dynamic-attention. ||| 28018 ||| 28019 ||| 
2021 ||| 2lspe: 2d learnable sinusoidal positional encoding using transformer for scene text recognition. ||| 18995 ||| 18996 ||| 18997 ||| 18998 ||| 18999 ||| 
2018 ||| estimation of scenes contributing to score in tennis video using attention. ||| 28020 ||| 28021 ||| 28022 ||| 
2019 ||| translation of sign language glosses to text using sequence-to-sequence attention models. ||| 28023 ||| 28024 ||| 28025 ||| 
2019 ||| fine-grained action recognition in assembly work scenes by drawing attention to the hands. ||| 28026 ||| 28022 ||| 28027 ||| 28028 ||| 28029 ||| 
2021 ||| automatic text summarization using transformers. ||| 28030 ||| 28031 ||| 7111 ||| 28032 ||| 28033 ||| 
2019 ||| learning contextual features with multi-head self-attention for fake news detection. ||| 28034 ||| 28035 ||| 28036 ||| 28037 ||| 795 ||| 
2020 ||| end-to-end nested multi-attention network for 3d brain tumor segmentation. ||| 28038 ||| 3177 ||| 
2019 ||| a neural rumor detection framework by incorporating uncertainty attention on social media texts. ||| 1446 ||| 17798 ||| 27992 ||| 
2018 ||| depth in the visual attention modelling from the egocentric perspective of view. ||| 28039 ||| 28040 ||| 
2018 ||| visual attention for behavioral cloning in autonomous driving. ||| 28041 ||| 28042 ||| 28043 ||| 
2019 ||| performance of bottom-up visual attention models when compared in contextless and context awareness scenarios. ||| 28044 ||| 28045 ||| 28046 ||| 28047 ||| 28048 ||| 
2019 ||| a deep interactive segmentation method with user interaction-based attention module and polar transformation. ||| 7934 ||| 7936 ||| 7937 ||| 28049 ||| 7938 ||| 
2019 ||| phrase-guided attention web article recommendation for next clicks and views. ||| 28050 ||| 28051 ||| 28052 ||| 28053 ||| 
2020 ||| fine-tuning techniques and data augmentation on transformer-based models for conversational texts and noisy user-generated content. ||| 28054 ||| 28055 ||| 28056 ||| 28057 ||| 28058 ||| 28059 ||| 28060 ||| 28061 ||| 28062 ||| 28063 ||| 
2020 ||| vstreamdrls: dynamic graph representation learning with self-attention for enterprise distributed video streaming solutions. ||| 17105 ||| 17106 ||| 
2021 ||| scate: shared cross attention transformer encoders for multimodal fake news detection. ||| 28064 ||| 28065 ||| 1186 ||| 1185 ||| 
2021 ||| group-node attention for community evolution prediction. ||| 28066 ||| 9750 ||| 28067 ||| 
2021 ||| analyzing topic attention in online small groups. ||| 15803 ||| 15807 ||| 15805 ||| 15806 ||| 8500 ||| 28068 ||| 15808 ||| 15809 ||| 15810 ||| 
2021 ||| care: learning convolutional attentional recurrent embedding for sequential recommendation. ||| 28069 ||| 3712 ||| 
2021 ||| hgats: hierarchical graph attention networks for multiple comments integration. ||| 28070 ||| 1558 ||| 28071 ||| 6475 ||| 
2020 ||| co-refining user and item representations with feature-level self-attention for enhanced recommendation. ||| 28072 ||| 9632 ||| 28073 ||| 28074 ||| 9635 ||| 
2021 ||| ian: interpretable attention network for churn prediction in lbsns. ||| 28075 ||| 28076 ||| 28077 ||| 28078 ||| 28079 ||| 
2020 ||| language identification on massive datasets of short messages using an attention mechanism cnn. ||| 28080 ||| 28081 ||| 
2017 ||| multiplex media attention and disregard network among 129 countries. ||| 9044 ||| 9045 ||| 
2019 ||| attention and vigilance detection based on electroencephalography - a summary of a literature review. ||| 26008 ||| 28082 ||| 28083 ||| 28084 ||| 28085 ||| 
2021 ||| a multi-scale deep learning attention-based feature method for rolling elements bearing fault detection in industrial motor drives. ||| 28086 ||| 28087 ||| 28088 ||| 
2017 ||| a methodology for integrated transformer compact modeling. ||| 28089 ||| 28090 ||| 28091 ||| 
2021 ||| the role of diodes in the leakage current suppression mechanism of decoupling transformerless pv inverter topologies. ||| 28092 ||| 28093 ||| 28094 ||| 
2020 ||| accuracy improvement of instantaneous frequency estimation by finite order fir hilbert transformer using notch filter. ||| 4524 ||| 4525 ||| 4526 ||| 4527 ||| 
2017 ||| questions classification with attention machine. ||| 28095 ||| 3888 ||| 3313 ||| 
2019 ||| tagattention: mobile object tracing without object appearance information by vision-rfid fusion. ||| 14158 ||| 28096 ||| 8534 ||| 28097 ||| 28098 ||| 28099 ||| 18226 ||| 
2020 ||| chinese punctuation prediction with adaptive attention and dependency tree. ||| 28100 ||| 701 ||| 704 ||| 28101 ||| 705 ||| 
2021 ||| structural dependency self-attention based hierarchical event model for chinese financial event extraction. ||| 2740 ||| 2777 ||| 16690 ||| 28102 ||| 5528 ||| 28103 ||| 11701 ||| 2378 ||| 
2020 ||| obstetric diagnosis assistant via knowledge powered attention and information-enhanced strategy. ||| 28104 ||| 7662 ||| 28105 ||| 13573 ||| 10104 ||| 
2017 ||| attention-based event relevance model for stock price movement prediction. ||| 6307 ||| 3128 ||| 3129 ||| 1418 ||| 
2018 ||| an enhanced esim model for sentence pair matching with self-attention. ||| 28106 ||| 28107 ||| 28108 ||| 185 ||| 28109 ||| 340 ||| 28110 ||| 
2021 ||| a biaffine attention-based approach for event factor extraction. ||| 28111 ||| 28112 ||| 17947 ||| 
2021 ||| dependency to semantics: structure transformation and syntax-guided attention for neural semantic parsing. ||| 28113 ||| 9283 ||| 10428 ||| 3656 ||| 
2019 ||| adaptive multilingual representations for cross-lingual entity linking with attention on entity descriptions. ||| 28114 ||| 3128 ||| 3129 ||| 1418 ||| 
2019 ||| fast neural chinese named entity recognition with multi-head self-attention. ||| 3756 ||| 3754 ||| 3755 ||| 10621 ||| 4792 ||| 2795 ||| 9574 ||| 
2019 ||| reka: relation extraction with knowledge-aware attention. ||| 8198 ||| 4166 ||| 3755 ||| 28115 ||| 9599 ||| 696 ||| 
2021 ||| patentminer: patent vacancy mining via context-enhanced and knowledge-guided graph attention. ||| 28116 ||| 92 ||| 28117 ||| 28118 ||| 28119 ||| 28120 ||| 28121 ||| 
2018 ||| adversarial training for relation classification with attention based gate mechanism. ||| 26280 ||| 3128 ||| 3129 ||| 1418 ||| 
2019 ||| discriminative spectral-spatial attention-aware residual network for hyperspectral image classification. ||| 28122 ||| 28123 ||| 28124 ||| 28125 ||| 28126 ||| 
2019 ||| hyperspectral and multispectral image fusion based on deep attention network. ||| 17441 ||| 6781 ||| 17615 ||| 6782 ||| 
2021 ||| spectral-spatial-temporal attention network for hyperspectral tracking. ||| 11521 ||| 28127 ||| 11522 ||| 836 ||| 1086 ||| 11523 ||| 
2021 ||| convcatb: an attention-based cnn-catboost risk prediction model for driving safety. ||| 28128 ||| 3386 ||| 28129 ||| 28130 ||| 978 ||| 28131 ||| 
2021 ||| dual attention mechanism object tracking algorithm based on fully-convolutional siamese network. ||| 28132 ||| 28133 ||| 241 ||| 28134 ||| 28135 ||| 6546 ||| 6548 ||| 
2021 ||| attentionae: autoencoder for anomaly detection in attributed networks. ||| 28136 ||| 28137 ||| 28138 ||| 3049 ||| 
2019 ||| question generation for reading comprehension of language learning test : -a method using seq2seq approach with transformer model-. ||| 28139 ||| 28140 ||| 28141 ||| 28142 ||| 
2020 ||| extraction of question-related sentences for reading comprehension tests via attention mechanism. ||| 28139 ||| 28140 ||| 28142 ||| 28141 ||| 
2019 ||| deep residual attention reinforcement learning. ||| 28143 ||| 28144 ||| 
2020 ||| aspect-based sentiment analysis on convolution neural network and multi-hierarchical attention. ||| 28145 ||| 18556 ||| 23775 ||| 28146 ||| 
2019 ||| currency exchange rate prediction with long short-term memory networks based on attention and news sentiment analysis. ||| 28147 ||| 28148 ||| 28149 ||| 
2018 ||| interactive interface design for the evaluation of attention deficiencies in preschool children. ||| 28150 ||| 28151 ||| 28152 ||| 8047 ||| 8048 ||| 22999 ||| 28153 ||| 
2017 ||| entropic brain-computer interfaces - using fnirs and eeg to measure attentional states in a bayesian framework. ||| 28154 ||| 28155 ||| 28156 ||| 28157 ||| 28158 ||| 28159 ||| 28160 ||| 
2017 ||| a hardware/software platform to acquire bioelectrical signals. a case study: characterizing computer access through attention. ||| 28161 ||| 28162 ||| 11933 ||| 28163 ||| 28164 ||| 28165 ||| 3419 ||| 28166 ||| 28167 ||| 28168 ||| 10907 ||| 28169 ||| 
2021 ||| gates: using graph attention networks for entity summarization. ||| 28170 ||| 28171 ||| 28172 ||| 
2019 ||| contextual graph attention for answering logical queries over incomplete knowledge graphs. ||| 28173 ||| 28174 ||| 28175 ||| 8473 ||| 28176 ||| 28177 ||| 
2021 ||| application of transfer learning in field verification for children in attention deficit hyperactivity disorder. ||| 28178 ||| 25209 ||| 28179 ||| 16042 ||| 
2017 ||| detecting driver's visual attention area by using vehicle-mounted device. ||| 28180 ||| 28181 ||| 28182 ||| 28183 ||| 
2018 ||| visual cognitive attention based bag-of-words image representation for object discovery. ||| 6855 ||| 28184 ||| 
2019 ||| measuring and controlling cognitive process of visual attention in forest fire monitoring system. ||| 28185 ||| 28186 ||| 28187 ||| 
2018 ||| cognitive training modulates cognitive processes of the brain: the response inhibition improved by attention training. ||| 602 ||| 9149 ||| 22265 ||| 
2017 ||| question answering over knowledgebase with attention-based lstm networks and knowledge embeddings. ||| 5829 ||| 5830 ||| 5831 ||| 5832 ||| 28188 ||| 
2019 ||| cognitive attention in autism using virtual reality learning tool. ||| 28189 ||| 28190 ||| 28191 ||| 28192 ||| 28193 ||| 
2020 ||| sapcgan: self-attention based generative adversarial network for point clouds. ||| 28194 ||| 28195 ||| 
2017 ||| analysis of driver's visual attention using near-miss incidents. ||| 28181 ||| 28196 ||| 
2021 ||| scnet: a generalized attention-based model for crack fault segmentation. ||| 28197 ||| 28198 ||| 28199 ||| 
2021 ||| meranet: facial micro-expression recognition using 3d residual attention network. ||| 28200 ||| 28201 ||| 28202 ||| 28203 ||| 
2018 ||| a bottom-up and top-down approach for image captioning using transformer. ||| 5224 ||| 366 ||| 
2021 ||| attention guided complementary feature integration for latent image recovery from noisy/blurry pairs. ||| 28204 ||| 28205 ||| 28206 ||| 28207 ||| 
2020 ||| scaling language data import/export with a data transformer interface. ||| 28208 ||| 28209 ||| 
2017 ||| automatic assessment of engagement and attention of the student by means of facial expressions. ||| 28210 ||| 28211 ||| 6235 ||| 23764 ||| 28212 ||| 8648 ||| 
2021 ||| small geodetic datasets and deep networks: attention-based residual lstm autoencoder stacking for geodetic time series. ||| 6847 ||| 6848 ||| 
2021 ||| synthesizing object state transformers for dynamic software updates. ||| 28213 ||| 28214 ||| 3156 ||| 28215 ||| 28216 ||| 
2021 ||| pasta: synthesizing object state transformers for dynamic software updates. ||| 28213 ||| 28214 ||| 3156 ||| 28215 ||| 28216 ||| 
2021 ||| studying the usage of text-to-text transfer transformer to support code-related tasks. ||| 28217 ||| 28218 ||| 28219 ||| 28220 ||| 28221 ||| 28222 ||| 28223 ||| 
2021 ||| code prediction by feeding trees to transformers. ||| 28224 ||| 28225 ||| 28226 ||| 28227 ||| 
2019 ||| eeg based driver inattention identification via feature profiling and dimensionality reduction. ||| 28228 ||| 28229 ||| 
2019 ||| the attention pattern emerging from information technology: a structural perspective. ||| 13137 ||| 
2018 ||| attention models for motor coordination and resulting interface design. ||| 28230 ||| 28231 ||| 28232 ||| 28233 ||| 28234 ||| 
2022 ||| universal adversarial perturbation generated by using attention information. ||| 28235 ||| 11266 ||| 1132 ||| 28236 ||| 
2022 ||| 3d cnn architectures and attention mechanisms for deepfake detection. ||| 28237 ||| 28238 ||| 5679 ||| 5681 ||| 
2021 ||| theory of mind and joint attention. ||| 28239 ||| 28240 ||| 4995 ||| 10259 ||| 
2022 ||| graph attention lstm: a spatiotemporal approach for traffic flow forecasting. ||| 28241 ||| 28242 ||| 
2022 ||| vehicle trajectory prediction using lstms with spatial-temporal attention mechanisms. ||| 10545 ||| 28243 ||| 28244 ||| 28245 ||| 
2017 ||| connectivity preserving network transformers. ||| 28246 ||| 28247 ||| 
2021 ||| attention based multi-component spatiotemporal cross-domain neural network model for wireless cellular network traffic prediction. ||| 28248 ||| 4103 ||| 4055 ||| 28249 ||| 
2017 ||| effects of animation on attentional resources of online consumers. ||| 28250 ||| 28251 ||| 28252 ||| 
2019 ||| when risks need attention: adoption of green supply chain initiatives in the pharmaceutical industry. ||| 28253 ||| 28254 ||| 28255 ||| 28256 ||| 28257 ||| 28258 ||| 
2020 ||| chinese microblog sentiment detection based on cnn-bigru and multihead attention mechanism. ||| 28259 ||| 28260 ||| 28261 ||| 28262 ||| 16314 ||| 
2021 ||| network course recommendation system based on double-layer attention mechanism. ||| 28263 ||| 
2021 ||| neighborhood attentional memory networks for recommendation systems. ||| 28264 ||| 28265 ||| 10801 ||| 10800 ||| 5110 ||| 
2020 ||| adaptive residual channel attention network for single image super-resolution. ||| 28266 ||| 28267 ||| 28268 ||| 28269 ||| 
2021 ||| english machine translation model based on an improved self-attention technology. ||| 691 ||| 
2021 ||| facial expression recognition based on attention mechanism. ||| 28270 ||| 28271 ||| 28272 ||| 28273 ||| 
2021 ||| a multifeature complementary attention mechanism for image topic representation in social networks. ||| 6389 ||| 28274 ||| 28275 ||| 28276 ||| 28277 ||| 
2020 ||| face detection and recognition based on visual attention mechanism guidance model in unrestricted posture. ||| 28278 ||| 
2021 ||| multi-scale guided attention network for crowd counting. ||| 481 ||| 1254 ||| 28279 ||| 1871 ||| 
2021 ||| research on intelligent english translation method based on the improved attention mechanism model. ||| 14797 ||| 
2021 ||| performance analysis of hybrid deep learning models with attention mechanism positioning and focal loss for text classification. ||| 28280 ||| 28281 ||| 28282 ||| 
2021 ||| innovative research on the development of online education mode of internet thinking based on the discrimination of learning attention under the analysis of head posture. ||| 28283 ||| 28284 ||| 
2021 ||| voice keyword retrieval method using attention mechanism and multimodal information fusion. ||| 28285 ||| 
2021 ||| spatial transformer network-based automatic modulation recognition of blind signals. ||| 28286 ||| 
2021 ||| learning deep attention network from incremental and decremental features for evolving features. ||| 26968 ||| 26966 ||| 
2019 ||| software defect prediction via attention-based recurrent neural network. ||| 28287 ||| 28288 ||| 28289 ||| 9888 ||| 28290 ||| 
2021 ||| multi-instance deep learning based on attention mechanism for failure prediction of unlabeled hard disk drives. ||| 28291 ||| 3906 ||| 28292 ||| 
2019 ||| detection of hv winding radial deformation and pd in power transformer using stepped-frequency hyperboloid method. ||| 28293 ||| 28294 ||| 28295 ||| 
2021 ||| qscgan: an un-supervised quick self-attention convolutional gan for lre bearing fault diagnosis under limited label-lacked data. ||| 28296 ||| 28297 ||| 28298 ||| 24555 ||| 28299 ||| 
2017 ||| an ac power standard for loss measurement systems for testing power transformers. ||| 23237 ||| 23253 ||| 23254 ||| 23252 ||| 
2021 ||| high-frequency current transformer design and implementation considerations for wideband partial discharge applications. ||| 28300 ||| 28301 ||| 28302 ||| 28303 ||| 
2021 ||| attention recurrent autoencoder hybrid model for early fault diagnosis of rotating machinery. ||| 21918 ||| 21920 ||| 28304 ||| 21919 ||| 23392 ||| 
2022 ||| an auxiliary framework to mitigate measurement inaccuracies caused by capacitive voltage transformers. ||| 28305 ||| 28306 ||| 28307 ||| 28308 ||| 
2021 ||| transformer fault prognosis using deep recurrent neural network over vibration signals. ||| 28309 ||| 28310 ||| 28311 ||| 28312 ||| 
2020 ||| a comb-type capacitive 2-fal sensor for transformer oil with improved sensitivity. ||| 28313 ||| 28314 ||| 28315 ||| 28316 ||| 28317 ||| 
2021 ||| a miniature transformer-coupled low-noise preamplifier for low source resistance sensors at low frequency. ||| 295 ||| 28318 ||| 28319 ||| 
2019 ||| a low-cost generator for testing and calibrating current transformers. ||| 28320 ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2019 ||| transformer health management based on self-powered rfid sensor and multiple kernel rvm. ||| 128 ||| 22230 ||| 28321 ||| 28322 ||| 8838 ||| 
2022 ||| toward small sample challenge in intelligent fault diagnosis: attention-weighted multidepth feature fusion net with signals augmentation. ||| 28323 ||| 28297 ||| 28298 ||| 28324 ||| 28325 ||| 
2021 ||| feature analysis of oscillating wave signal for axial displacement in autotransformer. ||| 21767 ||| 28326 ||| 28327 ||| 28328 ||| 28329 ||| 28330 ||| 28331 ||| 28332 ||| 
2018 ||| characterization of voltage instrument transformers under nonsinusoidal conditions based on the best linear approximation. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2020 ||| performances evaluation of on-chip large-size-tapped transformer for mems applications. ||| 28333 ||| 28334 ||| 28335 ||| 28336 ||| 
2021 ||| vit-p: classification of genitourinary syndrome of menopause from oct images based on vision transformer models. ||| 2277 ||| 28337 ||| 28338 ||| 28339 ||| 28340 ||| 28341 ||| 
2020 ||| recovery of partial discharge signal and noise cancellation in power transformer using radial basis function. ||| 28342 ||| 28343 ||| 28295 ||| 
2021 ||| 3-d facial expression recognition via attention-based multichannel data fusion network. ||| 6824 ||| 28344 ||| 586 ||| 2740 ||| 5890 ||| 
2021 ||| retinanet with difference channel attention and adaptively spatial feature fusion for steel surface defect detection. ||| 28345 ||| 28346 ||| 
2021 ||| visual landmark learning via attention-based deep neural networks. ||| 28347 ||| 28348 ||| 28349 ||| 
2021 ||| lightweight attention module for deep learning on classification and segmentation of 3-d point clouds. ||| 28350 ||| 28351 ||| 4217 ||| 28352 ||| 28353 ||| 
2019 ||| harmonic distortion compensation in voltage transformers for improved power quality measurements. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2022 ||| effective fault diagnosis based on wavelet and convolutional attention neural network for induction motors. ||| 28354 ||| 28355 ||| 28356 ||| 28357 ||| 
2019 ||| mechanical fault diagnostics of power transformer on-load tap changers using dynamic time warping. ||| 28358 ||| 28359 ||| 28360 ||| 5492 ||| 28361 ||| 144 ||| 
2021 ||| 3-kv two-stage voltage transformer with high-voltage excitation. ||| 5170 ||| 6924 ||| 28362 ||| 28363 ||| 28364 ||| 9465 ||| 6307 ||| 
2021 ||| 1100-kv uhvdc all fiber current transformer. ||| 1418 ||| 6389 ||| 28365 ||| 
2021 ||| linking attention-based multiscale cnn with dynamical gcn for driving fatigue detection. ||| 28366 ||| 7781 ||| 28367 ||| 28368 ||| 28369 ||| 
2020 ||| a new testing method for the diagnosis of winding faults in transformer. ||| 21767 ||| 28326 ||| 6243 ||| 28370 ||| 28327 ||| 28371 ||| 28329 ||| 
2021 ||| fds measurement-based moisture estimation model for transformer oil-paper insulation including the aging effect. ||| 28372 ||| 28373 ||| 28374 ||| 18177 ||| 28375 ||| 
2021 ||| voltage dependence of the reference system in medium- and high-voltage current transformer calibrations. ||| 23221 ||| 28376 ||| 28377 ||| 23240 ||| 
2021 ||| dual attention-based temporal convolutional network for fault prognosis under time-varying operating conditions. ||| 13745 ||| 13747 ||| 13746 ||| 28378 ||| 
2017 ||| voltage ratio traceability of 10 kv low-voltage excited two-stage voltage transformer. ||| 28379 ||| 28380 ||| 28381 ||| 1160 ||| 28382 ||| 28383 ||| 7700 ||| 28384 ||| 28385 ||| 
2022 ||| performance improvement of transformer differential protection during cross-country fault using hyperbolic s-transform. ||| 28386 ||| 28387 ||| 28295 ||| 
2021 ||| attention and feature fusion ssd for remote sensing object detection. ||| 28388 ||| 28389 ||| 28390 ||| 13442 ||| 
2021 ||| multipath fusion mask r-cnn with double attention and its application into gear pitting detection. ||| 28391 ||| 28392 ||| 28393 ||| 28394 ||| 9103 ||| 
2019 ||| detection of winding faults based on a characterization of the nonlinear dynamics of transformers. ||| 3773 ||| 5158 ||| 28395 ||| 
2021 ||| self-attention convlstm and its application in rul prediction of rolling bearings. ||| 8481 ||| 28396 ||| 16602 ||| 28397 ||| 
2017 ||| frequency response of mv voltage transformer under actual waveforms. ||| 5599 ||| 5603 ||| 5600 ||| 5604 ||| 5605 ||| 5610 ||| 
2021 ||| gas volume fraction measurement of oil-gas-water three-phase flows in vertical pipe by combining ultrasonic sensor and deep attention network. ||| 28398 ||| 28399 ||| 28400 ||| 28401 ||| 28402 ||| 
2020 ||| detecting trees in street images via deep learning with attention module. ||| 28403 ||| 5724 ||| 28404 ||| 1086 ||| 1224 ||| 
2022 ||| -net: triple-attention semantic segmentation network for small surface defect detection. ||| 28405 ||| 28406 ||| 
2021 ||| plane-wave image reconstruction via generative adversarial network and attention mechanism. ||| 28407 ||| 28408 ||| 28409 ||| 28410 ||| 28411 ||| 
2022 ||| dual-aspect self-attention based on transformer for remaining useful life prediction. ||| 11585 ||| 18266 ||| 28412 ||| 
2022 ||| res2fusion: infrared and visible image fusion based on dense res2net and double nonlocal attention models. ||| 28413 ||| 28414 ||| 28415 ||| 17218 ||| 28416 ||| 
2021 ||| anchor-based spatio-temporal attention 3-d convolutional networks for dynamic 3-d point cloud sequences. ||| 28417 ||| 28418 ||| 28419 ||| 28420 ||| 6474 ||| 25570 ||| 
2021 ||| self-detecting the measurement error of electronic voltage transformer based on principal component analysis-wavelet packet decomposition. ||| 22566 ||| 20664 ||| 28421 ||| 
2021 ||| accurate fault diagnosis in transformers using an auxiliary current-compensation-based framework for differential relays. ||| 28305 ||| 28306 ||| 28422 ||| 28307 ||| 28308 ||| 
2019 ||| overcoming frequency response measurements of voltage transformers: an approach based on quasi-sinusoidal volterra models. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 5599 ||| 5600 ||| 5607 ||| 28423 ||| 5608 ||| 
2021 ||| transformer winding faults detection based on time series analysis. ||| 28424 ||| 28425 ||| 28426 ||| 
2017 ||| an ac current transformer standard measuring system for power frequencies. ||| 23237 ||| 28427 ||| 23241 ||| 28428 ||| 
2021 ||| classify and localize threat items in x-ray imagery with multiple attention mechanism and high-resolution and high-semantic features. ||| 28429 ||| 28430 ||| 184 ||| 28431 ||| 28432 ||| 
2020 ||| nestfuse: an infrared and visible image fusion architecture based on nest connection and spatial/channel attention models. ||| 4175 ||| 7853 ||| 28433 ||| 
2021 ||| a novel application of the cross-capacitive sensor in real-time condition monitoring of transformer oil. ||| 28434 ||| 28435 ||| 28316 ||| 28315 ||| 
2020 ||| a defect inspection for explosive cartridge using an improved visual attention and image-weighted eigenvalue. ||| 28436 ||| 28437 ||| 28438 ||| 28439 ||| 
2022 ||| dense attention-guided cascaded network for salient object detection of strip steel surface defects. ||| 17663 ||| 2402 ||| 2740 ||| 28440 ||| 28441 ||| 28442 ||| 17648 ||| 
2020 ||| an innovative approach to express uncertainty introduced by voltage transformers. ||| 5632 ||| 23244 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2021 ||| high-precision self-calibrating current transformer with stray capacitances control. ||| 5611 ||| 5616 ||| 5613 ||| 28443 ||| 
2021 ||| measuring harmonics with inductive voltage transformers in presence of subharmonics. ||| 5599 ||| 5601 ||| 5606 ||| 5605 ||| 
2020 ||| effects of multiple influence quantities on rogowski-coil-type current transformers. ||| 5590 ||| 5591 ||| 5627 ||| 
2017 ||| a new calibration transformer and measurement setup for bridge standard calibrations up to 5 khz. ||| 28444 ||| 28445 ||| 28446 ||| 
2019 ||| compensation of current transformers' nonlinearities by tensor linearization. ||| 23245 ||| 5602 ||| 5603 ||| 23246 ||| 5605 ||| 
2021 ||| eta-rppgnet: effective time-domain attention network for remote heart rate measurement. ||| 5888 ||| 28447 ||| 28448 ||| 28449 ||| 6879 ||| 5890 ||| 
2021 ||| a modified simulation model for predicting the fds of transformer oil-paper insulation under nonuniform aging. ||| 28373 ||| 28450 ||| 28372 ||| 18177 ||| 28451 ||| 
2017 ||| wiener filtering for real-time dsp compensation of current transformers over a wide frequency range. ||| 1625 ||| 28452 ||| 1627 ||| 
2021 ||| transportation monitoring of geo-location, speed, vibration, and shock acceleration for 110-kv vehicular mobile transformers. ||| 28453 ||| 28454 ||| 10079 ||| 28455 ||| 28456 ||| 28457 ||| 28458 ||| 
2021 ||| a new method for transformer fault prediction based on multifeature enhancement and refined long short-term memory. ||| 9442 ||| 22227 ||| 28459 ||| 
2020 ||| improved stepup method to determine the errors of voltage instrument transformer with high accuracy. ||| 6924 ||| 9465 ||| 28363 ||| 28460 ||| 
2021 ||| rcag-net: residual channelwise attention gate network for hot spot defect detection of photovoltaic farms. ||| 28461 ||| 28462 ||| 14036 ||| 28463 ||| 
2017 ||| a computer-controlled calibrator for instrument transformer test sets. ||| 28464 ||| 28465 ||| 
2021 ||| comparing the new improved rlc and cmtl models for measuring partial discharge in transformer winding. ||| 28466 ||| 28343 ||| 28467 ||| 
2019 ||| a high-precision current transformer for loss measurements of ehv shunt reactors. ||| 28468 ||| 28469 ||| 28470 ||| 28471 ||| 28472 ||| 
2019 ||| compensation of nonlinearity of voltage and current instrument transformers. ||| 5655 ||| 5656 ||| 5599 ||| 5602 ||| 5657 ||| 5603 ||| 5600 ||| 5604 ||| 5605 ||| 5610 ||| 5658 ||| 
2019 ||| comparison of reference setups for calibrating power transformer loss measurement systems. ||| 28377 ||| 23237 ||| 23240 ||| 23252 ||| 28473 ||| 
2021 ||| research on residual flux density measurement for single-phase transformer core based on energy changes. ||| 28474 ||| 28475 ||| 28476 ||| 28477 ||| 
2021 ||| dftnet: deep fish tracker with attention mechanism in unconstrained marine environments. ||| 28478 ||| 17364 ||| 12797 ||| 12796 ||| 28479 ||| 
2022 ||| a novel anti-dc bias energy meter based on magnetic-valve-type current transformer. ||| 21933 ||| 28480 ||| 28481 ||| 21932 ||| 21929 ||| 
2021 ||| joint attention network for finger vein authentication. ||| 28482 ||| 28483 ||| 28484 ||| 13422 ||| 
2021 ||| rotating machinery fault diagnosis through a transformer convolution network subjected to transfer learning. ||| 28485 ||| 28486 ||| 28487 ||| 
2022 ||| selective multibranch attention network with material constraint for baggage reidentification. ||| 4600 ||| 28488 ||| 399 ||| 28489 ||| 28490 ||| 28491 ||| 
2021 ||| harmonic synchrophasors measurement algorithms with embedded compensation of voltage transformer frequency response. ||| 5647 ||| 5633 ||| 5648 ||| 5649 ||| 5635 ||| 5636 ||| 
2021 ||| a sequence-to-sequence model with attention and monotonicity loss for tool wear monitoring and prediction. ||| 8608 ||| 729 ||| 
2022 ||| surface defect detection of steel strips based on anchor-free network with channel attention and bidirectional feature fusion. ||| 28346 ||| 28345 ||| 27741 ||| 
2020 ||| condition assessment of power transformer insulation using short-duration time-domain dielectric spectroscopy measurement data. ||| 12790 ||| 28492 ||| 28493 ||| 28494 ||| 
2019 ||| a variational mode decomposition approach for degradation assessment of power transformer windings. ||| 28495 ||| 10415 ||| 28496 ||| 
2020 ||| detrapped charge-affected depolarization-current estimation using short-duration dielectric response for diagnosis of transformer insulation. ||| 28497 ||| 28492 ||| 28494 ||| 
2022 ||| eanet: edge-attention 6d pose estimation network for texture-less objects. ||| 28498 ||| 28499 ||| 28500 ||| 1086 ||| 28501 ||| 1224 ||| 
2019 ||| a fundamental step-up method for standard voltage transformers based on an active capacitive high-voltage divider. ||| 23237 ||| 9465 ||| 23252 ||| 28502 ||| 
2021 ||| automatic attention learning using neural architecture search for detection of cardiac abnormality in 12-lead ecg. ||| 15621 ||| 2054 ||| 22355 ||| 28503 ||| 
2022 ||| clformer: a lightweight transformer based on convolutional embedding and linear self-attention with strong robustness for bearing fault diagnosis under limited sample conditions. ||| 28504 ||| 28505 ||| 28506 ||| 19977 ||| 4417 ||| 28507 ||| 28508 ||| 
2021 ||| sps-net: self-attention photometric stereo network. ||| 28509 ||| 28510 ||| 28511 ||| 15239 ||| 
2021 ||| rotating machine systems fault diagnosis using semisupervised conditional random field-based graph attention network. ||| 28512 ||| 906 ||| 28513 ||| 28514 ||| 28515 ||| 28516 ||| 28517 ||| 
2021 ||| defect detection method of aluminum profile surface using deep self-attention mechanism under hybrid noise conditions. ||| 28518 ||| 28519 ||| 18228 ||| 28520 ||| 11634 ||| 
2017 ||| a comparison of two current transformer calibration systems at nrc canada. ||| 28521 ||| 28522 ||| 28523 ||| 23555 ||| 28524 ||| 28525 ||| 28526 ||| 
2022 ||| a novel interpretable method based on dual-level attentional deep neural network for actual multilabel arrhythmia detection. ||| 28527 ||| 28528 ||| 28529 ||| 28530 ||| 7332 ||| 28531 ||| 28532 ||| 28533 ||| 
2021 ||| multigrained attention network for infrared and visible image fusion. ||| 4807 ||| 28534 ||| 28409 ||| 28535 ||| 28536 ||| 26093 ||| 
2020 ||| temperature measuring-based decision-making prognostic approach in electric power transformers winding failures. ||| 16210 ||| 28537 ||| 28538 ||| 28539 ||| 
2019 ||| impact of coreless current transformer position on current measurement. ||| 28540 ||| 13701 ||| 23235 ||| 23233 ||| 23236 ||| 
2021 ||| deep rational attention network with threshold strategy embedded for mechanical fault diagnosis. ||| 28541 ||| 28285 ||| 3127 ||| 28542 ||| 28543 ||| 
2021 ||| acr-net: attention integrated and cross-spatial feature fused rotation network for tubular solder joint detection. ||| 28544 ||| 28545 ||| 5845 ||| 28546 ||| 28547 ||| 28548 ||| 
2022 ||| improving the temperature and vibration robustness of fiber optic current transformer using fiber polarization rotator. ||| 28549 ||| 28550 ||| 19774 ||| 26583 ||| 28551 ||| 
2021 ||| unsupervised anomaly segmentation via multilevel image reconstruction and adaptive attention-level transition. ||| 28552 ||| 28553 ||| 28554 ||| 28555 ||| 
2020 ||| nonlinear behavioral modeling of voltage transformers in the frequency domain: comparing different approaches. ||| 5632 ||| 5633 ||| 5634 ||| 5635 ||| 5636 ||| 
2019 ||| a new vibration testing platform for electronic current transformers. ||| 28556 ||| 10819 ||| 28557 ||| 28558 ||| 4086 ||| 28559 ||| 28560 ||| 
2021 ||| integrated multiple directed attention-based deep learning for improved air pollution forecasting. ||| 2901 ||| 2902 ||| 28561 ||| 2903 ||| 
2020 ||| image captioning using facial expression and attention. ||| 28562 ||| 3258 ||| 3257 ||| 3157 ||| 3158 ||| 
2019 ||| interpretable charge prediction for criminal cases with dynamic rationale attention. ||| 4649 ||| 17840 ||| 10612 ||| 28563 ||| 28564 ||| 
2021 ||| multi-document summarization with determinantal point process attention. ||| 28565 ||| 3408 ||| 
2020 ||| bi-directional recurrent attentional topic model. ||| 18024 ||| 9472 ||| 2238 ||| 
2020 ||| a deep multi-task contextual attention framework for multi-modal affect analysis. ||| 3836 ||| 5137 ||| 165 ||| 
2022 ||| kran: knowledge refining attention network for recommendation. ||| 758 ||| 241 ||| 28566 ||| 1041 ||| 
2022 ||| dimbert: learning vision-language grounded representations with disentangled multimodal-attention. ||| 3746 ||| 3748 ||| 3749 ||| 9350 ||| 7417 ||| 3751 ||| 4430 ||| 
2022 ||| knowledge distillation with attention for deep transfer learning of convolutional networks. ||| 23983 ||| 23984 ||| 6214 ||| 23988 ||| 3478 ||| 6285 ||| 1406 ||| 
2022 ||| graph-based stock recommendation by time-aware relational attention network. ||| 1129 ||| 1127 ||| 1128 ||| 1130 ||| 28567 ||| 885 ||| 
2021 ||| harp: a novel hierarchical attention model for relation prediction. ||| 21183 ||| 6275 ||| 
2019 ||| attention models in graphs: a survey. ||| 1193 ||| 1194 ||| 1196 ||| 28568 ||| 1197 ||| 
2021 ||| ggatb-lstm: grouping and global attention-based time-aware bidirectional lstm medical treatment behavior prediction. ||| 7087 ||| 1557 ||| 1558 ||| 28569 ||| 17465 ||| 
2020 ||| nguard+: an attention-based game bot detection framework via player behavior sequences. ||| 28570 ||| 28571 ||| 25415 ||| 25420 ||| 1306 ||| 28572 ||| 
2018 ||| the effect of attention cueing on science text learning. ||| 
2021 ||| attention based vehicle trajectory prediction. ||| 15463 ||| 28573 ||| 28574 ||| 15465 ||| 
2021 ||| attention-based gated recurrent unit for gesture recognition. ||| 23655 ||| 28575 ||| 28576 ||| 28577 ||| 
2019 ||| salientdso: bringing attention to direct sparse odometry. ||| 28578 ||| 28579 ||| 28580 ||| 3831 ||| 28581 ||| 
2021 ||| needs-based product configurator design for mass customization using hierarchical attention network. ||| 7400 ||| 26637 ||| 28582 ||| 
2021 ||| fine-grained user location prediction using meta-path context with attention mechanism. ||| 28583 ||| 28584 ||| 8191 ||| 
2022 ||| hybrid ctc-attention network-based end-to-end speech recognition system for korean language. ||| 28585 ||| 28586 ||| 28587 ||| 14744 ||| 14746 ||| 
2020 ||| sara-gan: self-attention and relative average discriminator based generative adversarial networks for fast compressed sensing mri reconstruction. ||| 28588 ||| 28589 ||| 28590 ||| 14286 ||| 12639 ||| 28591 ||| 28592 ||| 28593 ||| 6005 ||| 
2019 ||| transcranial magnetic stimulation to the middle frontal gyrus during attention modes induced dynamic module reconfiguration in brain networks. ||| 28594 ||| 28595 ||| 28596 ||| 28597 ||| 28598 ||| 28599 ||| 3676 ||| 26926 ||| 
2020 ||| machine learning methods for diagnosing autism spectrum disorder and attention- deficit/hyperactivity disorder using functional and structural mri: a survey. ||| 28600 ||| 28601 ||| 28602 ||| 28603 ||| 
2020 ||| developmental designs and adult functions of cortical maps in multiple modalities: perception, attention, navigation, numbers, streaming, speech, and cognition. ||| 28604 ||| 
2021 ||| dr-iixrn : detection algorithm of diabetic retinopathy based on deep ensemble learning and attention mechanism. ||| 28605 ||| 28606 ||| 28607 ||| 16665 ||| 28608 ||| 28609 ||| 
2020 ||| effects of visual attentional load on the tactile sensory memory indexed by somatosensory mismatch negativity. ||| 10812 ||| 12196 ||| 28610 ||| 28611 ||| 28612 ||| 28613 ||| 28614 ||| 28615 ||| 
2019 ||| fuzzy sliding mode control with state estimation for velocity control system of hydraulic cylinder using a new hydraulic transformer. ||| 8906 ||| 28616 ||| 28617 ||| 28618 ||| 
2021 ||| short-term wind speed prediction with a two-layer attention-based lstm. ||| 28619 ||| 28620 ||| 28621 ||| 5123 ||| 
2022 ||| facial expression recognition using enhanced convolution neural network with attention mechanism. ||| 28622 ||| 28623 ||| 28624 ||| 28625 ||| 28626 ||| 
2021 ||| mixed attention densely residual network for single image super-resolution. ||| 5550 ||| 2058 ||| 5551 ||| 18632 ||| 28627 ||| 1528 ||| 28628 ||| 5553 ||| 
2020 ||| predicting mobile trading system discontinuance: the role of attention. ||| 28629 ||| 28630 ||| 28631 ||| 28632 ||| 
2019 ||| attention to online channels across the path to purchase: an eye-tracking study. ||| 5335 ||| 28633 ||| 8648 ||| 28634 ||| 28635 ||| 28636 ||| 
2020 ||| a multi-attention matching model for multiple-choice reading comprehension. ||| 10830 ||| 10832 ||| 10833 ||| 
2021 ||| transformer models for text-based emotion detection: a review of bert-based approaches. ||| 28637 ||| 28638 ||| 13782 ||| 
2020 ||| selective attention to historical comparison or social comparison in the evolutionary iterated prisoner's dilemma game. ||| 28639 ||| 28640 ||| 
2022 ||| attention-based neural joint source-channel coding of text for point to point and broadcast channel. ||| 3311 ||| 28641 ||| 
2021 ||| a novel hybrid network model based on attentional multi-feature fusion for deception detection. ||| 28642 ||| 28643 ||| 28644 ||| 28645 ||| 17104 ||| 
2021 ||| self-channel attention weighted part for person re-identification. ||| 16785 ||| 28646 ||| 28647 ||| 28648 ||| 28649 ||| 28650 ||| 28651 ||| 28652 ||| 
2020 ||| siamese attention-based lstm for speech emotion recognition. ||| 28653 ||| 17104 ||| 28645 ||| 28654 ||| 28655 ||| 
2017 ||| attentional control and other executive functions. ||| 28656 ||| 28657 ||| 
2017 ||| the role of perceived relevance and attention in teachers' attitude and intention to use educational video games. ||| 28658 ||| 28659 ||| 852 ||| 28660 ||| 28661 ||| 7033 ||| 21794 ||| 28662 ||| 28663 ||| 
2019 ||| attention and its role: theories and models. ||| 28656 ||| 28657 ||| 
2020 ||| joint model-based attention for spoken language understanding task. ||| 189 ||| 28664 ||| 7093 ||| 
2018 ||| a digital phase shift method for phase compensation of electronic transformer. ||| 5474 ||| 28665 ||| 2755 ||| 28666 ||| 28667 ||| 
2021 ||| experimental investigation on creep characteristic of the spacer between winding turns of power transformers. ||| 28668 ||| 28669 ||| 28670 ||| 28671 ||| 
2018 ||| driver inattention monitoring system based on multimodal fusion with visual cues to improve driving safety. ||| 28672 ||| 28673 ||| 
2019 ||| study of an accurate electronic power measurement technique using modified current transformer and potential transformer. ||| 28674 ||| 28675 ||| 28676 ||| 28677 ||| 28678 ||| 
2018 ||| high accuracy optical voltage transformer with digital output based on coaxial capacitor voltage divider. ||| 28679 ||| 28680 ||| 
2020 ||| a foot in two camps or your undivided attention? the impact of intra- and inter-community collaboration on firm innovation performance. ||| 28681 ||| 22267 ||| 28682 ||| 28683 ||| 
2021 ||| cross-scale global attention feature pyramid network for person search. ||| 438 ||| 28684 ||| 28685 ||| 28686 ||| 
2021 ||| dct-net: a deep co-interactive transformer network for video temporal grounding. ||| 8948 ||| 2343 ||| 6893 ||| 
2021 ||| unsupervised cross-domain person re-identification with self-attention and joint-flexible optimization. ||| 28687 ||| 6496 ||| 6497 ||| 1828 ||| 3503 ||| 28688 ||| 11592 ||| 
2021 ||| atcc: accurate tracking by criss-cross location attention. ||| 5782 ||| 2740 ||| 17663 ||| 18830 ||| 602 ||| 
2021 ||| attention-guided aggregation stereo matching network. ||| 28689 ||| 19921 ||| 25100 ||| 2304 ||| 
2021 ||| multi-tier attention network using term-weighted question features for visual question answering. ||| 28690 ||| 28691 ||| 
2021 ||| visual question answering model based on graph neural network and contextual attention. ||| 28692 ||| 28693 ||| 
2021 ||| a study on attention-based lstm for abnormal behavior recognition with variable pooling. ||| 5384 ||| 5840 ||| 6511 ||| 7910 ||| 28694 ||| 
2022 ||| facial expression recognition using densely connected convolutional neural network and hierarchical spatial attention. ||| 28695 ||| 28696 ||| 28697 ||| 28698 ||| 28699 ||| 
2020 ||| variance-guided attention-based twin deep network for cross-spectral periocular recognition. ||| 28700 ||| 28701 ||| 2651 ||| 16408 ||| 
2021 ||| roi tanh-polar transformer network for face parsing in the wild. ||| 28702 ||| 20051 ||| 28703 ||| 5719 ||| 
2020 ||| a calibration method of computer vision system based on dual attention mechanism. ||| 28704 ||| 
2022 ||| improving image captioning with pyramid attention and sc-gan. ||| 11456 ||| 264 ||| 28705 ||| 262 ||| 28706 ||| 
2020 ||| cross-correlated attention networks for person re-identification. ||| 2128 ||| 2129 ||| 2127 ||| 2131 ||| 2130 ||| 
2021 ||| point cloud completion using multiscale feature fusion and cross-regional attention. ||| 20226 ||| 20227 ||| 28707 ||| 
2021 ||| graph-based reasoning attention pooling with curriculum design for content-based image retrieval. ||| 22444 ||| 3091 ||| 22449 ||| 28708 ||| 22575 ||| 
2022 ||| ltst: long-term segmentation tracker with memory attention network. ||| 3274 ||| 28709 ||| 28710 ||| 28711 ||| 10812 ||| 
2020 ||| an attention-based deep learning model for multiple pedestrian attributes recognition. ||| 28712 ||| 28713 ||| 1994 ||| 28714 ||| 28715 ||| 28716 ||| 3419 ||| 
2021 ||| spatiotemporal module for video saliency prediction based on self-attention. ||| 28717 ||| 28718 ||| 28719 ||| 6908 ||| 6909 ||| 
2021 ||| attention-guided chained context aggregation for semantic segmentation. ||| 28720 ||| 28721 ||| 2814 ||| 2051 ||| 9472 ||| 
2020 ||| pcanet: pyramid convolutional attention network for semantic segmentation. ||| 28722 ||| 12678 ||| 11819 ||| 
2020 ||| non-local attention association scheme for online multi-object tracking. ||| 28723 ||| 28724 ||| 28725 ||| 28726 ||| 28727 ||| 
2021 ||| multi-information-based convolutional neural network with attention mechanism for pedestrian trajectory prediction. ||| 1787 ||| 28728 ||| 28729 ||| 472 ||| 28730 ||| 
2021 ||| multimodal assessment of apparent personality using feature attention and error consistency constraint. ||| 7111 ||| 28731 ||| 28732 ||| 15326 ||| 28733 ||| 28734 ||| 
2021 ||| short-term anchor linking and long-term self-guided attention for video object detection. ||| 28735 ||| 2253 ||| 28736 ||| 28737 ||| 
2022 ||| attention guided contextual feature fusion network for salient object detection. ||| 390 ||| 28738 ||| 6645 ||| 28739 ||| 3503 ||| 5337 ||| 
2021 ||| flow guided mutual attention for person re-identification. ||| 28740 ||| 28741 ||| 28742 ||| 28743 ||| 28744 ||| 5748 ||| 
2021 ||| da-sacot: domain adaptive-segmentation guided attention for correlation based object tracking. ||| 28745 ||| 12790 ||| 17364 ||| 
2020 ||| crossatnet - a novel cross-attention based framework for sketch-based image retrieval. ||| 6864 ||| 6865 ||| 6866 ||| 6867 ||| 
2021 ||| transformer models for enhancing attngan based text to image generation. ||| 28746 ||| 28747 ||| 28748 ||| 28749 ||| 28750 ||| 
2020 ||| a novel co-attention computation block for deep learning based image co-segmentation. ||| 28751 ||| 28752 ||| 28753 ||| 28754 ||| 
2020 ||| attention-guided rgbd saliency detection using appearance information. ||| 17663 ||| 28755 ||| 20235 ||| 2740 ||| 28442 ||| 
2020 ||| cam: a fine-grained vehicle model recognition method based on visual attention model. ||| 28756 ||| 28757 ||| 3437 ||| 28758 ||| 28759 ||| 5017 ||| 
2021 ||| exploring region relationships implicitly: image captioning with visual relationship attention. ||| 601 ||| 603 ||| 602 ||| 604 ||| 
2022 ||| pu-gacnet: graph attention convolution network for point cloud upsampling. ||| 28760 ||| 9863 ||| 28761 ||| 
2020 ||| spatial biases in crowdsourced data: social media content attention concentrates on populous areas in disasters. ||| 28762 ||| 28763 ||| 28764 ||| 28765 ||| 28766 ||| 28767 ||| 
2020 ||| capturing what human eyes perceive: a visual hierarchy generation approach to emulating saliency-based visual attention for grid-like urban street networks. ||| 28768 ||| 17965 ||| 28769 ||| 28770 ||| 28771 ||| 28772 ||| 28773 ||| 28774 ||| 28775 ||| 28776 ||| 
2019 ||| functional and contextual attention-based lstm for service recommendation in mashup creation. ||| 160 ||| 161 ||| 162 ||| 
2021 ||| multi-deformation aware attention learning for concrete structural defect classification. ||| 28777 ||| 2651 ||| 16408 ||| 
2022 ||| action-centric relation transformer network for video question answering. ||| 3280 ||| 155 ||| 28778 ||| 1039 ||| 9579 ||| 1040 ||| 
2021 ||| multi-level fusion and attention-guided cnn for image dehazing. ||| 28779 ||| 128 ||| 28780 ||| 17217 ||| 
2020 ||| recurrent prediction with spatio-temporal attention for crowd attribute recognition. ||| 28781 ||| 17645 ||| 2824 ||| 27592 ||| 
2020 ||| complementation-reinforced attention network for person re-identification. ||| 28782 ||| 28783 ||| 1899 ||| 1900 ||| 
2020 ||| video summarization with attention-based encoder-decoder networks. ||| 2276 ||| 28784 ||| 2279 ||| 6922 ||| 
2020 ||| porn streamer recognition in live video streaming via attention-gated multimodal deep features. ||| 12076 ||| 875 ||| 2398 ||| 28785 ||| 28786 ||| 
2022 ||| iid-net: image inpainting detection network via neural architecture search and attention. ||| 12427 ||| 19695 ||| 
2017 ||| attention-weighted texture and depth bit-allocation in general-geometry free-viewpoint television. ||| 28787 ||| 28788 ||| 
2020 ||| video dialog via multi-grained convolutional self-attention context multi-modal networks. ||| 9704 ||| 1306 ||| 9703 ||| 1115 ||| 2258 ||| 
2019 ||| two-stream collaborative learning with spatial-temporal attention for video classification. ||| 5954 ||| 28789 ||| 28790 ||| 
2021 ||| deep convolutional-neural-network-based channel attention for single image dynamic scene blind deblurring. ||| 28791 ||| 28792 ||| 28793 ||| 28794 ||| 28795 ||| 12203 ||| 28796 ||| 
2020 ||| driver drowsiness recognition via 3d conditional gan and two-level attention bi-lstm. ||| 28797 ||| 28798 ||| 28799 ||| 7659 ||| 
2021 ||| attention-aligned network for person re-identification. ||| 28800 ||| 28801 ||| 5746 ||| 
2020 ||| optical flow estimation using dual self-attention pyramid networks. ||| 11590 ||| 11589 ||| 11591 ||| 6927 ||| 11592 ||| 
2022 ||| transformer-based language-person search with multiple region slicing. ||| 4175 ||| 19558 ||| 28802 ||| 28803 ||| 4400 ||| 
2020 ||| multimodal transformer with multi-view visual representation for image captioning. ||| 1754 ||| 4807 ||| 1753 ||| 13825 ||| 
2021 ||| multi-view spatial attention embedding for vehicle re-identification. ||| 27189 ||| 14494 ||| 13825 ||| 437 ||| 
2021 ||| a novel just-noticeable-difference-based saliency-channel attention residual network for full-reference image quality predictions. ||| 28804 ||| 28805 ||| 7823 ||| 
2022 ||| dahp: deep attention-guided hashing with pairwise labels. ||| 9889 ||| 28806 ||| 11973 ||| 28807 ||| 28808 ||| 28809 ||| 
2021 ||| cross-view gait recognition using pairwise spatial transformer networks. ||| 28810 ||| 28811 ||| 2008 ||| 28812 ||| 836 ||| 
2021 ||| multi-turn video question generation via reinforced multi-choice attention network. ||| 28813 ||| 1306 ||| 9703 ||| 28814 ||| 1081 ||| 17707 ||| 28815 ||| 
2020 ||| probabilistic topic model for context-driven visual attention understanding. ||| 28816 ||| 10506 ||| 28817 ||| 28818 ||| 16464 ||| 8485 ||| 8486 ||| 3419 ||| 
2020 ||| fast and accurate action detection in videos with motion-centric attention model. ||| 19978 ||| 2200 ||| 7204 ||| 
2021 ||| dynamic attention guided multi-trajectory analysis for single object tracking. ||| 5957 ||| 5790 ||| 5755 ||| 382 ||| 6416 ||| 28819 ||| 8711 ||| 
2022 ||| tagnet: triplet-attention graph networks for hashtag recommendation. ||| 28820 ||| 28821 ||| 8709 ||| 24738 ||| 
2021 ||| multi-grained attention networks for single image super-resolution. ||| 28822 ||| 2185 ||| 28823 ||| 28824 ||| 12749 ||| 1796 ||| 9062 ||| 6782 ||| 
2022 ||| stacked multimodal attention network for context-aware video captioning. ||| 28688 ||| 9689 ||| 580 ||| 6514 ||| 17845 ||| 
2022 ||| hierarchical feature fusion with mixed convolution attention for single image dehazing. ||| 28779 ||| 6067 ||| 128 ||| 28825 ||| 
2021 ||| multimodal local-global attention network for affective video content analysis. ||| 28826 ||| 9061 ||| 8711 ||| 
2019 ||| sharp attention network via adaptive sampling for person re-identification. ||| 22328 ||| 7380 ||| 28827 ||| 28828 ||| 28829 ||| 28830 ||| 2490 ||| 
2022 ||| self-paced feature attention fusion network for concealed object detection in millimeter-wave image. ||| 28831 ||| 6914 ||| 28832 ||| 28833 ||| 5107 ||| 6917 ||| 28834 ||| 
2019 ||| multi-scale attention deep neural network for fast accurate object detection. ||| 28835 ||| 1007 ||| 28836 ||| 
2020 ||| three-dimension transmissible attention network for person re-identification. ||| 28837 ||| 28800 ||| 28838 ||| 5746 ||| 28839 ||| 6930 ||| 
2022 ||| probabilistic spatial distribution prior based attentional keypoints matching network. ||| 28840 ||| 28841 ||| 11594 ||| 11596 ||| 28842 ||| 28843 ||| 
2021 ||| decomposition makes better rain removal: an improved attention-guided deraining network. ||| 12061 ||| 2146 ||| 12062 ||| 2230 ||| 28844 ||| 19756 ||| 28845 ||| 12056 ||| 
2022 ||| learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion. ||| 20010 ||| 8482 ||| 28846 ||| 19435 ||| 19848 ||| 
2020 ||| aggregating attentional dilated features for salient object detection. ||| 978 ||| 24818 ||| 8634 ||| 1739 ||| 8635 ||| 8636 ||| 8637 ||| 
2022 ||| lightweight image super-resolution with expectation-maximization attention mechanism. ||| 28847 ||| 28848 ||| 28849 ||| 23377 ||| 5888 ||| 28850 ||| 
2021 ||| co-saliency detection with co-attention fully convolutional network. ||| 28851 ||| 2815 ||| 28852 ||| 5705 ||| 
2021 ||| learning dual semantic relations with graph attention for image-text matching. ||| 736 ||| 737 ||| 4260 ||| 
2022 ||| feature aggregation networks based on dual attention capsules for visual object tracking. ||| 1272 ||| 28853 ||| 9900 ||| 28854 ||| 
2019 ||| attention-based 3d-cnns for large-vocabulary sign language recognition. ||| 23312 ||| 1806 ||| 1807 ||| 28855 ||| 
2021 ||| attention transfer network for nature image matting. ||| 28856 ||| 28857 ||| 28858 ||| 
2022 ||| syntax-guided hierarchical attention network for video captioning. ||| 28859 ||| 13824 ||| 28860 ||| 13823 ||| 19820 ||| 13825 ||| 
2020 ||| task-aware attention model for clothing attribute prediction. ||| 28861 ||| 28862 ||| 8626 ||| 16546 ||| 1921 ||| 
2022 ||| tsan: synthesized view quality enhancement via two-stream attention network for 3d-hevc. ||| 28863 ||| 28864 ||| 19853 ||| 4449 ||| 19728 ||| 
2020 ||| fine-grained age estimation in the wild with attention lstm networks. ||| 19919 ||| 28865 ||| 28866 ||| 28867 ||| 28868 ||| 28869 ||| 1484 ||| 
2021 ||| where to look and how to describe: fashion image retrieval with an attentional heterogeneous bilinear network. ||| 28870 ||| 5845 ||| 6334 ||| 4175 ||| 5189 ||| 10075 ||| 
2021 ||| attentional kernel encoding networks for fine-grained visual categorization. ||| 7238 ||| 8588 ||| 1796 ||| 1931 ||| 1930 ||| 
2022 ||| task-adaptive attention for image captioning. ||| 17648 ||| 28871 ||| 13824 ||| 24823 ||| 26515 ||| 5963 ||| 26420 ||| 28872 ||| 
2021 ||| multiscale omnibearing attention networks for person re-identification. ||| 28837 ||| 28800 ||| 5746 ||| 28839 ||| 6930 ||| 
2022 ||| wide weighted attention multi-scale network for accurate mr image super-resolution. ||| 19803 ||| 19802 ||| 19805 ||| 1730 ||| 
2021 ||| transformer3d-det: improving 3d object detection by vote refinement. ||| 2564 ||| 28873 ||| 1696 ||| 2566 ||| 
2019 ||| action recognition with spatio-temporal visual attention on skeleton image sequences. ||| 2416 ||| 20094 ||| 20095 ||| 2166 ||| 
2017 ||| visual-attention-based background modeling for detecting infrequently moving objects. ||| 2617 ||| 7334 ||| 4979 ||| 28874 ||| 307 ||| 
2022 ||| pman: progressive multi-attention network for human pose transfer. ||| 28875 ||| 340 ||| 28876 ||| 621 ||| 28877 ||| 
2020 ||| attention-driven loss for anomaly detection in video surveillance. ||| 3389 ||| 17992 ||| 28878 ||| 28879 ||| 9187 ||| 8488 ||| 
2022 ||| ptpgc: pedestrian trajectory prediction by graph attention network with convlstm. ||| 932 ||| 3751 ||| 28880 ||| 28881 ||| 
2018 ||| visual attention and object naming in humanoid robots using a bio-inspired spiking neural network. ||| 28882 ||| 28883 ||| 3419 ||| 28884 ||| 28885 ||| 28886 ||| 28887 ||| 13070 ||| 
2020 ||| visual-spatial attention as a comfort measure in human-robot collaborative tasks. ||| 28888 ||| 28889 ||| 4046 ||| 28890 ||| 
2022 ||| joint disease classification and lesion segmentation via one-stage attention-based convolutional neural network in oct images. ||| 10405 ||| 24564 ||| 12570 ||| 28891 ||| 4646 ||| 4488 ||| 
2021 ||| attention graph convolutional nets for esophageal contraction pattern recognition in high-resolution manometries. ||| 369 ||| 28892 ||| 28893 ||| 28894 ||| 1134 ||| 28895 ||| 28896 ||| 
2021 ||| bascnet: bilateral adaptive spatial and channel attention network for breast density classification in the mammogram. ||| 28897 ||| 12431 ||| 3991 ||| 3992 ||| 12382 ||| 11466 ||| 28898 ||| 28899 ||| 
2021 ||| ffanet: feature fusion attention network to medical image segmentation. ||| 28900 ||| 28901 ||| 28902 ||| 
2021 ||| deep learning models for cuffless blood pressure monitoring from ppg signals using attention mechanism. ||| 28903 ||| 28904 ||| 
2021 ||| deep connected attention (dca) resnet for robust voice pathology detection and classification. ||| 28905 ||| 28906 ||| 2588 ||| 28907 ||| 4754 ||| 28908 ||| 
2022 ||| a novel sleep staging network based on multi-scale dual attention. ||| 28909 ||| 28910 ||| 336 ||| 28911 ||| 28912 ||| 28913 ||| 28914 ||| 
2022 ||| attention gated tensor neural network architectures for speech emotion recognition. ||| 28915 ||| 28916 ||| 28917 ||| 
2020 ||| identifying heart-brain interactions during internally and externally operative attention using conditional entropy. ||| 28918 ||| 28919 ||| 28920 ||| 
2021 ||| anterior chamber angle classification in anterior segment optical coherence tomography images using hybrid attention based pyramidal convolutional network. ||| 11215 ||| 28921 ||| 28922 ||| 28923 ||| 28924 ||| 28925 ||| 28926 ||| 
2021 ||| multi-scale attention-guided network for mammograms classification. ||| 28927 ||| 3992 ||| 3991 ||| 12382 ||| 28928 ||| 28899 ||| 
2022 ||| estimating finger joint angles on surface emg using manifold learning and long short-term memory with attention mechanism. ||| 28929 ||| 28930 ||| 28931 ||| 28932 ||| 
2022 ||| adjacent slices feature transformer network for single anisotropic 3d brain mri image super-resolution. ||| 609 ||| 16924 ||| 611 ||| 16925 ||| 16923 ||| 
2021 ||| a multiscale residual pyramid attention network for medical image fusion. ||| 11391 ||| 20432 ||| 28933 ||| 28934 ||| 
2022 ||| brain tumor segmentation with corner attention and high-dimensional perceptual loss. ||| 28935 ||| 447 ||| 28936 ||| 28937 ||| 28938 ||| 18225 ||| 
2021 ||| dense gan and multi-layer attention based lesion segmentation method for covid-19 ct images. ||| 28939 ||| 28940 ||| 28941 ||| 28942 ||| 28943 ||| 28944 ||| 28945 ||| 5786 ||| 28946 ||| 
2021 ||| identification of autism spectrum disorder using multi-regional resting-state data through an attention learning approach. ||| 28947 ||| 15189 ||| 11676 ||| 5536 ||| 28948 ||| 
2022 ||| a novel analytical method to measure intra-individual variability of steady-state evoked potentials; new insights into attention deficit. ||| 28949 ||| 28950 ||| 
2022 ||| saa-net: u-shaped network with scale-axis-attention for liver tumor segmentation. ||| 1460 ||| 28951 ||| 28952 ||| 28953 ||| 28954 ||| 
2022 ||| dual attention based network for skin lesion classification with auxiliary learning. ||| 28955 ||| 16895 ||| 16904 ||| 
2017 ||| efficient visual attention driven framework for key frames extraction from hysteroscopy videos. ||| 28956 ||| 28957 ||| 28958 ||| 28959 ||| 
2022 ||| attention deficit/hyperactivity disorder classification based on deep spatio-temporal features of functional magnetic resonance imaging. ||| 3854 ||| 6643 ||| 6644 ||| 8838 ||| 28960 ||| 
2022 ||| automatic detection of multiple types of pneumonia: open dataset and a multi-scale attention network. ||| 28961 ||| 28962 ||| 28963 ||| 28964 ||| 18565 ||| 438 ||| 28965 ||| 28966 ||| 
2022 ||| eff2net: an efficient channel attention-based convolutional neural network for skin disease classification. ||| 28967 ||| 28968 ||| 28969 ||| 28970 ||| 28971 ||| 
2022 ||| broad learning system stacking with multi-scale attention for the diagnosis of gastric intestinal metaplasia. ||| 28972 ||| 28973 ||| 28962 ||| 28974 ||| 28975 ||| 12385 ||| 
2021 ||| auditory attention decoding from electroencephalography based on long short-term memory networks. ||| 28976 ||| 28977 ||| 28978 ||| 28979 ||| 28980 ||| 14551 ||| 2037 ||| 28981 ||| 5170 ||| 28982 ||| 124 ||| 24266 ||| 
2021 ||| gcaunet: a group cross-channel attention residual unet for slice based brain tumor segmentation. ||| 11446 ||| 28983 ||| 28984 ||| 13221 ||| 
2019 ||| comparison of brain effective connectivity in different states of attention and consciousness based on eeg signals. ||| 28985 ||| 28986 ||| 28987 ||| 
2021 ||| a self-attention based faster r-cnn for polyp detection from colonoscopy images. ||| 28988 ||| 28989 ||| 28990 ||| 28991 ||| 28992 ||| 
2018 ||| a visual attention guided unsupervised feature learning for robust vessel delineation in retinal images. ||| 28993 ||| 28994 ||| 28995 ||| 
2022 ||| corrigendum to "identification of autism spectrum disorder using multi-regional resting-state data through an attention learning approach" [biomed. signal process. control 69 (2021) 102833]. ||| 28947 ||| 15189 ||| 11676 ||| 5536 ||| 28948 ||| 
2022 ||| attention res-unet with guided decoder for semantic segmentation of brain tumors. ||| 28996 ||| 28997 ||| 28998 ||| 
2018 ||| investigation of brain networks in children with attention deficit/hyperactivity disorder using a graph theoretical approach. ||| 28999 ||| 438 ||| 304 ||| 29000 ||| 29001 ||| 29002 ||| 
2021 ||| automated skin lesion segmentation using attention-based deep convolutional neural network. ||| 29003 ||| 29004 ||| 29005 ||| 29006 ||| 
2021 ||| an attention-based bi-lstm method for visual object classification via eeg. ||| 29007 ||| 29008 ||| 
2019 ||| chemical-protein interaction extraction via contextualized word representations and multihead attention. ||| 16566 ||| 8974 ||| 16590 ||| 8349 ||| 29009 ||| 
2018 ||| hierarchical bi-directional attention-based rnns for supporting document classification on protein-protein interactions affected by genetic mutations. ||| 29010 ||| 3728 ||| 29011 ||| 29012 ||| 3729 ||| 
2018 ||| extracting chemical-protein relations using attention-based neural networks. ||| 29013 ||| 29014 ||| 29015 ||| 29016 ||| 29017 ||| 29018 ||| 29019 ||| 
2018 ||| bio-inspired model learning visual goals and attention skills through contingencies and intrinsic motivations. ||| 29020 ||| 29021 ||| 
2019 ||| brain-inspired cognitive model with attention for self-driving cars. ||| 29022 ||| 29023 ||| 29024 ||| 29025 ||| 14779 ||| 
2020 ||| toward improving engagement in neural rehabilitation: attention enhancement based on brain-computer interface and audiovisual feedback. ||| 269 ||| 270 ||| 271 ||| 
2017 ||| flexible task execution and attentional regulations in human-robot interaction. ||| 5422 ||| 5427 ||| 
2017 ||| deep reinforcement learning with visual attention for vehicle classification. ||| 29026 ||| 1012 ||| 29027 ||| 
2020 ||| joint attention in hearing parent-deaf child and hearing parent-hearing child dyads. ||| 29028 ||| 29029 ||| 
2021 ||| color facial expression recognition by quaternion convolutional neural network with gabor attention. ||| 11689 ||| 29030 ||| 2519 ||| 29031 ||| 
2021 ||| attention-based video hashing for large-scale video retrieval. ||| 29032 ||| 13203 ||| 8611 ||| 29033 ||| 13205 ||| 
2021 ||| understanding the role of objects in joint attention task framework for children with autism. ||| 4679 ||| 4680 ||| 4681 ||| 
2018 ||| multibranch attention networks for action recognition in still images. ||| 13 ||| 11226 ||| 11227 ||| 11228 ||| 
2022 ||| attention-net: an ensemble sketch recognition approach using vector images. ||| 10207 ||| 10208 ||| 10209 ||| 10210 ||| 
2022 ||| trear: transformer-based rgb-d egocentric action recognition. ||| 5199 ||| 20002 ||| 1703 ||| 29034 ||| 2365 ||| 20005 ||| 
2018 ||| news attention in a mobile era. ||| 29035 ||| 29036 ||| 29037 ||| 29038 ||| 
2021 ||| method based on the cross-layer attention mechanism and multiscale perception for safety helmet-wearing detection. ||| 29039 ||| 29040 ||| 29041 ||| 29042 ||| 
2021 ||| intelligent real-time arabic sign language classification using attention-based inception and bilstm. ||| 5654 ||| 5653 ||| 5650 ||| 29043 ||| 5652 ||| 29044 ||| 29045 ||| 29046 ||| 
2021 ||| image captioning in hindi language using transformer networks. ||| 29047 ||| 29048 ||| 404 ||| 405 ||| 29049 ||| 
2020 ||| self-attention guided model for defect detection of aluminium alloy casting on x-ray image. ||| 29050 ||| 24498 ||| 472 ||| 29051 ||| 
2022 ||| attention meta-transfer learning approach for few-shot iris recognition. ||| 29052 ||| 29053 ||| 29054 ||| 29055 ||| 3997 ||| 3996 ||| 
2021 ||| self-attention and adversary learning deep hashing network for cross-modal retrieval. ||| 29056 ||| 5126 ||| 1052 ||| 29057 ||| 
2021 ||| pooling attention-based encoder-decoder network for semantic segmentation. ||| 16648 ||| 29058 ||| 13469 ||| 29059 ||| 29060 ||| 5067 ||| 
2018 ||| highly reliable inverter topology with a novel soft computing technique to eliminate leakage current in grid-connected transformerless photovoltaic systems. ||| 29061 ||| 29062 ||| 
2022 ||| predicate-attention neural model for chinese semantic role labeling. ||| 29063 ||| 9024 ||| 16550 ||| 29064 ||| 
2022 ||| laednet: a lightweight attention encoder-decoder network for ultrasound medical image segmentation. ||| 11215 ||| 21516 ||| 29065 ||| 29066 ||| 18802 ||| 19801 ||| 
2022 ||| deep hierarchical lstm networks with attention for video summarization. ||| 6311 ||| 6312 ||| 19754 ||| 
2022 ||| regional attention reinforcement learning for rapid object detection. ||| 29067 ||| 29068 ||| 29069 ||| 1754 ||| 
2022 ||| attention-inception-based u-net for retinal vessel segmentation with advanced residual. ||| 29070 ||| 29071 ||| 28938 ||| 2801 ||| 29072 ||| 29073 ||| 29074 ||| 
2019 ||| deep attention network for person re-identification with multi-loss. ||| 8207 ||| 29075 ||| 29076 ||| 18759 ||| 
2017 ||| commentary: interactivity - agency, pace and attention. ||| 29077 ||| 
2019 ||| a joint approach to detect malicious url based on attention mechanism. ||| 29078 ||| 29079 ||| 29080 ||| 29081 ||| 29082 ||| 
2020 ||| spatial relational attention using fully convolutional networks for image caption generation. ||| 11790 ||| 29083 ||| 11792 ||| 
2017 ||| software-defined networking model for smart transformers with iso/iec/ieee 21451 sensors. ||| 29084 ||| 1035 ||| 29085 ||| 29086 ||| 13795 ||| 
2022 ||| improving communication protocols in smart cities with transformers. ||| 29087 ||| 29088 ||| 29089 ||| 29090 ||| 10314 ||| 
2021 ||| intelligent short-term voltage stability assessment via spatial attention rectified rnn learning. ||| 29091 ||| 29092 ||| 15629 ||| 
2021 ||| remaining useful life prediction using a novel feature-attention-based end-to-end approach. ||| 17677 ||| 10978 ||| 29093 ||| 29094 ||| 
2019 ||| distribution transformer's loss of life considering residential prosumers owning solar shingles, high-power fast chargers and second-generation battery energy storage. ||| 29095 ||| 29096 ||| 
2022 ||| attention network for rail surface defect detection via consistency of intersection-over-union(iou)-guided center-point estimation. ||| 29097 ||| 29098 ||| 5310 ||| 29099 ||| 29100 ||| 
2021 ||| localization of partial discharge in electrical transformer considering multimedia refraction and diffraction. ||| 29101 ||| 29102 ||| 11145 ||| 29103 ||| 1241 ||| 29104 ||| 
2017 ||| hierarchical system design and control of an mmc-based power-electronic transformer. ||| 29105 ||| 29106 ||| 25352 ||| 29107 ||| 29108 ||| 
2017 ||| efficient control of active transformers for increasing the pv hosting capacity of lv grids. ||| 29109 ||| 29110 ||| 22051 ||| 29111 ||| 29112 ||| 29113 ||| 
2020 ||| understanding and learning discriminant features based on multiattention 1dcnn for wheelset bearing fault diagnosis. ||| 2054 ||| 29114 ||| 29115 ||| 14491 ||| 
2021 ||| gated dual attention unit neural networks for remaining useful life prediction of rolling bearings. ||| 28392 ||| 29116 ||| 18175 ||| 29117 ||| 
2020 ||| ar-net: adaptive attention and residual refinement network for copy-move forgery detection. ||| 29118 ||| 25012 ||| 21769 ||| 29119 ||| 29120 ||| 
2020 ||| visual attention assessment for expert-in-the-loop training in a maritime operation simulator. ||| 29121 ||| 29122 ||| 29123 ||| 29124 ||| 
2020 ||| interactive grid interfacing system by matrix-converter-based solid state transformer with model predictive control. ||| 29125 ||| 29126 ||| 29127 ||| 10934 ||| 
2021 ||| three-attention mechanisms for one-stage 3-d object detection based on lidar and camera. ||| 26961 ||| 4353 ||| 
2021 ||| a new diagnostic technique for reliable decision-making on transformer fra data in interturn short-circuit condition. ||| 29128 ||| 29129 ||| 28312 ||| 28309 ||| 28295 ||| 
2017 ||| self-repairable smart grids via online coordination of smart transformers. ||| 29130 ||| 29131 ||| 
2020 ||| integration of accelerated deep neural network into power transformer differential protection. ||| 29132 ||| 29133 ||| 29134 ||| 29135 ||| 
2022 ||| parallel operation of transformer-based improved z-source inverter with high boost and interleaved control. ||| 22150 ||| 29136 ||| 29137 ||| 29138 ||| 
2019 ||| self-triggered reduced-attention output feedback control for linear networked control systems. ||| 29139 ||| 29140 ||| 
2020 ||| a surface defect detection framework for glass bottle bottom using visual attention model and wavelet transform. ||| 29141 ||| 28490 ||| 21518 ||| 29142 ||| 29143 ||| 29144 ||| 4600 ||| 
2021 ||| wide-attention and deep-composite model for traffic flow prediction in transportation cyber-physical systems. ||| 29145 ||| 29146 ||| 1371 ||| 29147 ||| 
2022 ||| multilevel attention based u-shape graph neural network for point clouds learning. ||| 29148 ||| 29149 ||| 1242 ||| 
2022 ||| graph cardinality preserved attention network for fault diagnosis of induction motor under varying speed and load condition. ||| 28512 ||| 906 ||| 28514 ||| 28517 ||| 28516 ||| 28515 ||| 29150 ||| 
2021 ||| parallel deep learning algorithms with hybrid attention mechanism for image segmentation of lung tumors. ||| 29151 ||| 29152 ||| 29153 ||| 23018 ||| 
2020 ||| fault-attention generative probabilistic adversarial autoencoder for machine anomaly detection. ||| 29154 ||| 29155 ||| 29156 ||| 29157 ||| 29158 ||| 
2021 ||| locating inter-turn faults in transformer windings using isometric feature mapping of frequency response traces. ||| 22308 ||| 22309 ||| 22310 ||| 29159 ||| 
2017 ||| autonomous energy management strategy for solid-state transformer to integrate pv-assisted ev charging station participating in ancillary service. ||| 29160 ||| 2411 ||| 29161 ||| 4259 ||| 29162 ||| 
2021 ||| attention-aware encoder-decoder neural networks for heterogeneous graphs of things. ||| 29163 ||| 1242 ||| 29164 ||| 11514 ||| 29149 ||| 
2019 ||| fpga implementation of passivity-based control and output load algebraic estimation for transformerless multilevel active rectifier. ||| 29165 ||| 29166 ||| 12581 ||| 29167 ||| 852 ||| 29168 ||| 29169 ||| 29170 ||| 4252 ||| 29171 ||| 
2017 ||| novel calculation method of indices to improve classification of transformer winding fault type, location, and extent. ||| 29172 ||| 28295 ||| 
2021 ||| active object discovery and localization using sound-induced attention. ||| 22949 ||| 6832 ||| 29173 ||| 29174 ||| 3613 ||| 1168 ||| 
2021 ||| lightweight attention convolutional neural network for retinal vessel image segmentation. ||| 2008 ||| 29175 ||| 26551 ||| 29176 ||| 
2022 ||| novel transformer based on gated convolutional neural network for dynamic soft sensor modeling of industrial processes. ||| 469 ||| 29177 ||| 29178 ||| 470 ||| 
2021 ||| clothing sale forecasting by a composite gru-prophet model with an attention mechanism. ||| 29179 ||| 208 ||| 457 ||| 17720 ||| 
2017 ||| application of a recursive phasor estimation method for adaptive fault component based differential protection of power transformers. ||| 29180 ||| 29181 ||| 28558 ||| 
2021 ||| attentional residual network for necking predictions in hot strip mills. ||| 29182 ||| 29183 ||| 29184 ||| 
2020 ||| a temporally irreversible visual attention model inspired by motion sensitive neurons. ||| 17218 ||| 29185 ||| 28779 ||| 
2022 ||| effective meta-attention dehazing networks for vision-based outdoor industrial systems. ||| 29186 ||| 29187 ||| 28786 ||| 29188 ||| 
2022 ||| path enhanced bidirectional graph attention network for quality prediction in multistage manufacturing process. ||| 29189 ||| 10978 ||| 29093 ||| 17677 ||| 29190 ||| 
2017 ||| application of dip to detect power transformers axial displacement and disk space variation using fra polar plot signature. ||| 29191 ||| 28557 ||| 
2021 ||| nondestructive defect detection in castings by using spatial attention bilinear convolutional neural network. ||| 29192 ||| 24497 ||| 29050 ||| 29193 ||| 29194 ||| 
2020 ||| feasibility study on simultaneous detection of partial discharge and axial displacement of hv transformer winding using electromagnetic waves. ||| 29195 ||| 28293 ||| 28295 ||| 29196 ||| 28294 ||| 
2018 ||| classification and discrimination among winding mechanical defects, internal and external electrical faults, and inrush current of transformer. ||| 28387 ||| 29197 ||| 28295 ||| 
2021 ||| deephealth: a self-attention based method for instant intelligent predictive maintenance in industrial internet of things. ||| 29198 ||| 2019 ||| 29199 ||| 29200 ||| 1796 ||| 29201 ||| 
2021 ||| deep learning-based solar-cell manufacturing defect detection with complementary attention network. ||| 28461 ||| 28462 ||| 26814 ||| 5184 ||| 14036 ||| 28463 ||| 
2021 ||| softwarized attention-based context-aware group recommendation technology in event-based industrial cyber-physical systems. ||| 25167 ||| 25169 ||| 29202 ||| 25170 ||| 29203 ||| 
2020 ||| pga-net: pyramid feature fusion and global context attention network for automated surface defect detection. ||| 29204 ||| 28511 ||| 16295 ||| 4620 ||| 28510 ||| 29205 ||| 
2020 ||| analysis and experimental validation of current-fed switched capacitor-based modular dc transformer. ||| 29206 ||| 29207 ||| 29208 ||| 29209 ||| 19339 ||| 29210 ||| 29211 ||| 29212 ||| 
2022 ||| integrating multihub driven attention mechanism and big data analytics for virtual representation of visual scenes. ||| 19610 ||| 19898 ||| 29213 ||| 29214 ||| 23886 ||| 
2021 ||| moisture diagnosis of transformer oil-immersed insulation with intelligent technique and frequency-domain spectroscopy. ||| 28373 ||| 28372 ||| 28375 ||| 29215 ||| 18177 ||| 29216 ||| 29217 ||| 29218 ||| 
2021 ||| covtanet: a hybrid tri-level attention-based network for lesion segmentation, diagnosis, and severity prediction of covid-19 chest ct scans. ||| 29219 ||| 29220 ||| 29221 ||| 29222 ||| 29223 ||| 29224 ||| 29225 ||| 
2022 ||| special issue: operation and control of modular dc transformer applied to energy internet. ||| 29226 ||| 10974 ||| 29227 ||| 29228 ||| 
2020 ||| a skill-based approach to modeling the attentional blink. ||| 29229 ||| 29230 ||| 29231 ||| 29232 ||| 
2019 ||| visual search without selective attention: a cognitive architecture account. ||| 29233 ||| 
2020 ||| sarcasm detection in mash-up language using soft-attention based bi-directional lstm and feature-rich cnn. ||| 957 ||| 958 ||| 29234 ||| 
2021 ||| semantic boundary enhancement and position attention network with long-range dependency for semantic segmentation. ||| 5250 ||| 28844 ||| 29235 ||| 29236 ||| 13766 ||| 13767 ||| 7716 ||| 519 ||| 6796 ||| 19460 ||| 
2022 ||| multi-scale sparse network with cross-attention mechanism for image-based butterflies fine-grained classification. ||| 29237 ||| 29238 ||| 29239 ||| 29240 ||| 29241 ||| 29242 ||| 29243 ||| 29244 ||| 
2021 ||| sentiment classification using attention mechanism and bidirectional long short-term memory network. ||| 29245 ||| 29246 ||| 29247 ||| 29248 ||| 29249 ||| 
2021 ||| danhar: dual attention network for multimodal human activity recognition using wearable sensors. ||| 29250 ||| 241 ||| 29251 ||| 532 ||| 3890 ||| 
2019 ||| improved power transformer condition monitoring under uncertainty through soft computing and probabilistic health index. ||| 29252 ||| 29253 ||| 29254 ||| 29255 ||| 29256 ||| 29257 ||| 
2021 ||| cross-sean: a cross-stitch semi-supervised neural attention model for covid-19 fake news detection. ||| 15159 ||| 15158 ||| 29258 ||| 15161 ||| 3835 ||| 
2021 ||| a novel rule-based evolving fuzzy system applied to the thermal modeling of power transformers. ||| 29259 ||| 29260 ||| 
2021 ||| predictive intelligence powered attentional stacking matrix factorization algorithm for the computational drug repositioning. ||| 29261 ||| 20791 ||| 29262 ||| 29263 ||| 18575 ||| 
2021 ||| channel pruning guided by spatial and channel attention for dnns in intelligent edge computing. ||| 29264 ||| 29265 ||| 29266 ||| 29267 ||| 29202 ||| 24193 ||| 
2020 ||| designing a composite deep learning based differential protection scheme of power transformers. ||| 29132 ||| 29133 ||| 29134 ||| 29135 ||| 
2021 ||| application of particle swarm optimization for optimal setting of phase shifting transformers to minimize unscheduled active power flows. ||| 29268 ||| 29269 ||| 29270 ||| 
2020 ||| bg-sac: entity relationship classification model based on self-attention supported capsule networks. ||| 29271 ||| 5352 ||| 5010 ||| 811 ||| 
2021 ||| attention induced multi-head convolutional neural network for human activity recognition. ||| 29272 ||| 29273 ||| 
2022 ||| an attention based dual learning approach for video captioning. ||| 29274 ||| 19437 ||| 29275 ||| 12038 ||| 
2021 ||| video salient object detection using dual-stream spatiotemporal attention. ||| 29276 ||| 27334 ||| 27335 ||| 27856 ||| 29277 ||| 
2021 ||| hybrid attention-based long short-term memory network for sarcasm identification. ||| 29278 ||| 29279 ||| 29280 ||| 29281 ||| 
2021 ||| correlational graph attention-based long short-term memory network for multivariate time series prediction. ||| 29282 ||| 29283 ||| 29284 ||| 29285 ||| 5457 ||| 
2020 ||| human action recognition using two-stream attention based lstm networks. ||| 29286 ||| 8424 ||| 29287 ||| 
2020 ||| stockwell transform of time-series of fmri data for diagnoses of attention deficit hyperactive disorder. ||| 24321 ||| 29288 ||| 29289 ||| 29290 ||| 
2020 ||| a spatio-temporal attention-based spot-forecasting framework for urban traffic prediction. ||| 29291 ||| 852 ||| 29292 ||| 
2020 ||| visual fixation prediction with incomplete attention map based on brain storm optimization. ||| 1825 ||| 8831 ||| 20941 ||| 
2022 ||| multi-scale attention recalibration network for crowd counting. ||| 29293 ||| 29294 ||| 29295 ||| 13824 ||| 5025 ||| 5024 ||| 2519 ||| 
2019 ||| global optimization algorithms applied to solve a multi-variable inverse artificial neural network to improve the performance of an absorption heat transformer with energy recycling. ||| 12581 ||| 29296 ||| 29297 ||| 2600 ||| 852 ||| 29298 ||| 29299 ||| 29300 ||| 10314 ||| 29301 ||| 4046 ||| 29302 ||| 29303 ||| 26875 ||| 29304 ||| 
2021 ||| acomnn: attention enhanced compound neural network for financial time-series forecasting with cross-regional features. ||| 5937 ||| 5971 ||| 29305 ||| 5972 ||| 29306 ||| 2068 ||| 29307 ||| 
2022 ||| tcran: multivariate time series classification using residual channel attention networks with time correction. ||| 29308 ||| 29309 ||| 29310 ||| 333 ||| 29311 ||| 
2021 ||| att-net: enhanced emotion recognition system using lightweight self-attention module. ||| 29312 ||| 29313 ||| 
2021 ||| adversarial transfer network with bilinear attention for the detection of adverse drug reactions from social media. ||| 29314 ||| 8974 ||| 29315 ||| 16590 ||| 8349 ||| 21178 ||| 728 ||| 29316 ||| 
2020 ||| a novel deep learning method based on attention mechanism for bearing remaining useful life prediction. ||| 29317 ||| 29318 ||| 29319 ||| 29320 ||| 
2021 ||| biomedical cross-sentence relation extraction via multihead attention and graph convolutional networks. ||| 6481 ||| 8349 ||| 8974 ||| 398 ||| 16590 ||| 16566 ||| 
2021 ||| attention augmented convolutional neural network for acoustics based machine state estimation. ||| 29321 ||| 29322 ||| 
2020 ||| multi-scale channel importance sorting and spatial attention mechanism for retinal vessels segmentation. ||| 29323 ||| 29324 ||| 29325 ||| 29326 ||| 4634 ||| 
2020 ||| attention distribution guided information transfer networks for recommendation in practice. ||| 29327 ||| 11641 ||| 29328 ||| 14069 ||| 
2020 ||| efficient point-of-interest recommendation with hierarchical attention mechanism. ||| 8427 ||| 22375 ||| 29329 ||| 10429 ||| 13738 ||| 
2022 ||| computational intelligence for preventive maintenance of power transformers. ||| 29330 ||| 29331 ||| 29332 ||| 29333 ||| 
2018 ||| multi-objective ensemble forecasting with an application to power transformers. ||| 29334 ||| 29335 ||| 29336 ||| 29337 ||| 
2020 ||| music auto-tagging using scattering transform and convolutional neural network with self-attention. ||| 29338 ||| 29339 ||| 21362 ||| 29340 ||| 29341 ||| 
2020 ||| attention embedded residual cnn for disease detection in tomato leaves. ||| 29342 ||| 29343 ||| 29344 ||| 29345 ||| 29346 ||| 29347 ||| 
2021 ||| transformer-based identification of stochastic information cascades in social networks using text and image similarity. ||| 29348 ||| 29349 ||| 29350 ||| 29351 ||| 29352 ||| 29353 ||| 29354 ||| 
2021 ||| deep multi-scale attentional features for medical image segmentation. ||| 4026 ||| 2843 ||| 
2019 ||| visual question answering via attention-based syntactic structure tree-lstm. ||| 4477 ||| 5320 ||| 19722 ||| 29355 ||| 843 ||| 
2020 ||| denseattentionseg: segment hands from interacted objects using depth input. ||| 29356 ||| 3386 ||| 29357 ||| 29358 ||| 29359 ||| 
2020 ||| aglnet: towards real-time semantic segmentation of self-driving images via attention-guided lightweight network. ||| 11215 ||| 3906 ||| 11216 ||| 11217 ||| 29360 ||| 15881 ||| 7247 ||| 
2021 ||| attention-based c-bilstm for fake news detection. ||| 27244 ||| 29361 ||| 29362 ||| 29363 ||| 
2018 ||| optimal multivariable conditions in the operation of an absorption heat transformer with energy recycling solved by the genetic algorithm in artificial neural network inverse. ||| 29303 ||| 26875 ||| 29304 ||| 29364 ||| 29365 ||| 29300 ||| 10314 ||| 
2021 ||| attention-based dynamic user preference modeling and nonlinear feature interaction learning for collaborative filtering recommendation. ||| 29366 ||| 29367 ||| 29368 ||| 
2020 ||| a fast self-attention cascaded network for object detection in large scene remote sensing images. ||| 29369 ||| 29370 ||| 29371 ||| 19948 ||| 952 ||| 
2022 ||| gabor log-euclidean gaussian and its fusion with deep network based on self-attention for face recognition. ||| 29372 ||| 471 ||| 5085 ||| 
2022 ||| interpretable cognitive learning with spatial attention for high-volatility time series prediction. ||| 29373 ||| 8281 ||| 
2020 ||| ed-acnn: novel attention convolutional neural network based on encoder-decoder framework for human traffic prediction. ||| 29374 ||| 2487 ||| 29375 ||| 29149 ||| 29376 ||| 
2020 ||| interpreting network knowledge with attention mechanism for bearing fault diagnosis. ||| 19321 ||| 29377 ||| 29155 ||| 29378 ||| 29158 ||| 
2020 ||| dual path attention net for remote sensing semantic image segmentation. ||| 29379 ||| 29380 ||| 29381 ||| 4811 ||| 
2021 ||| a visual attention model based on eye tracking in 3d scene maps. ||| 29382 ||| 14846 ||| 
2019 ||| incorporating graph attention and recurrent architectures for city-wide taxi demand prediction. ||| 29383 ||| 11105 ||| 
2021 ||| cascaded attention denseunet (cadunet) for road extraction from very-high-resolution images. ||| 4807 ||| 4297 ||| 29384 ||| 1420 ||| 
2021 ||| detection of schools in remote sensing images based on attention-guided dense network. ||| 29385 ||| 29386 ||| 29387 ||| 29388 ||| 
2021 ||| a dynamic and static context-aware attention network for trajectory prediction. ||| 25999 ||| 28328 ||| 398 ||| 29389 ||| 29390 ||| 9283 ||| 
2022 ||| cascaded residual attention enhanced road extraction from remote sensing images. ||| 29391 ||| 29392 ||| 29393 ||| 1768 ||| 29394 ||| 29395 ||| 728 ||| 29396 ||| 29397 ||| 8976 ||| 
2021 ||| a3t-gcn: attention temporal graph convolutional network for traffic forecasting. ||| 29398 ||| 29399 ||| 29400 ||| 6643 ||| 29401 ||| 29402 ||| 12801 ||| 
2022 ||| end-to-end pedestrian trajectory forecasting with transformer network. ||| 29403 ||| 29404 ||| 2008 ||| 
2018 ||| reduction of map information regulates visual attention without affecting route recognition performance. ||| 29405 ||| 29406 ||| 29407 ||| 29408 ||| 29409 ||| 
2021 ||| residual multi-attention classification network for a forest dominated tropical landscape using high-resolution remote sensing imagery. ||| 23044 ||| 29410 ||| 20235 ||| 29411 ||| 
2021 ||| high-resolution remote sensing image segmentation framework based on attention mechanism and adaptive weighting. ||| 29412 ||| 29413 ||| 29414 ||| 29415 ||| 29416 ||| 
2019 ||| an attention-based spatiotemporal gated recurrent unit network for point-of-interest recommendation. ||| 24729 ||| 29417 ||| 8349 ||| 29418 ||| 29419 ||| 8335 ||| 
2020 ||| dem void filling based on context attention generation model. ||| 29420 ||| 29421 ||| 29422 ||| 29423 ||| 29424 ||| 
2020 ||| neur and smartphone zombie: smartphone users' altering visual attention and walking behavior in public space. ||| 29425 ||| 29426 ||| 29427 ||| 
2021 ||| neurogrid simulates cortical cell-types, active dendrites, and top-down attention. ||| 29428 ||| 29429 ||| 29430 ||| 29431 ||| 29432 ||| 
2021 ||| a scalable off-the-shelf framework for measuring patterns of attention in young children and its application in autism spectrum disorder. ||| 29433 ||| 29434 ||| 29435 ||| 20690 ||| 
2021 ||| improving attention model based on cognition grounded data for sentiment analysis. ||| 5532 ||| 26550 ||| 26549 ||| 8956 ||| 26551 ||| 
2021 ||| a computational model of focused attention meditation and its transfer to a sustained attention task. ||| 29436 ||| 29437 ||| 
2022 ||| attention-based skill translation models for expert finding. ||| 29438 ||| 29439 ||| 
2022 ||| multi-view graph attention network for travel recommendation. ||| 1207 ||| 4470 ||| 29440 ||| 29441 ||| 29442 ||| 
2021 ||| data augmentation for skin lesion using self-attention based progressive generative adversarial network. ||| 29443 ||| 29444 ||| 29445 ||| 
2021 ||| representation learning using attention network and cnn for heterogeneous networks. ||| 29446 ||| 24486 ||| 9283 ||| 29447 ||| 
2020 ||| multiple premises entailment recognition based on attention and gate mechanism. ||| 29448 ||| 29449 ||| 11215 ||| 29450 ||| 29451 ||| 29452 ||| 29453 ||| 18548 ||| 
2021 ||| dynamic network embedding via structural attention. ||| 10922 ||| 29454 ||| 29455 ||| 2073 ||| 29456 ||| 29457 ||| 
2021 ||| image super-resolution based on adaptive cascading attention network. ||| 29458 ||| 29459 ||| 29460 ||| 29461 ||| 
2021 ||| learning attention embeddings based on memory networks for neural collaborative recommendation. ||| 29462 ||| 29463 ||| 
2021 ||| an attention enhanced sentence feature network for subtitle extraction and summarization. ||| 29464 ||| 29465 ||| 29466 ||| 29467 ||| 29468 ||| 
2021 ||| an attention-based cnn-bilstm hybrid neural network enhanced with features of discrete wavelet transformation for fetal acidosis classification. ||| 29469 ||| 29470 ||| 29471 ||| 29472 ||| 29473 ||| 
2018 ||| curriculum learning based approach for noise robust language identification using dnn with attention. ||| 25033 ||| 12222 ||| 25032 ||| 
2020 ||| ilwaanet: an interactive lexicon-aware word-aspect attention network for aspect-level sentiment classification on social networking. ||| 7538 ||| 4735 ||| 
2020 ||| a two-step hybrid unsupervised model with attention mechanism for aspect extraction. ||| 29474 ||| 29475 ||| 29476 ||| 29477 ||| 
2022 ||| distinguishing between fake news and satire with transformers. ||| 29478 ||| 29479 ||| 29480 ||| 29481 ||| 
2019 ||| detecting user attention to video segments using interval eeg features. ||| 29482 ||| 29483 ||| 29484 ||| 29485 ||| 
2020 ||| rumor detection based on propagation graph neural network with attention mechanism. ||| 29486 ||| 29487 ||| 29488 ||| 29489 ||| 29490 ||| 
2021 ||| speech emotion recognition using recurrent neural networks with directional self-attention. ||| 29491 ||| 29492 ||| 5218 ||| 29493 ||| 7436 ||| 
2019 ||| word n-gram attention models for sentence similarity and inference. ||| 6089 ||| 29494 ||| 29495 ||| 3408 ||| 29496 ||| 
2020 ||| friend recommendation for cross marketing in online brand community based on intelligent attention allocation link prediction algorithm. ||| 29497 ||| 29498 ||| 29499 ||| 29500 ||| 29501 ||| 5743 ||| 
2021 ||| improved few-shot learning method for transformer fault diagnosis based on approximation space and belief functions. ||| 29502 ||| 8177 ||| 29503 ||| 29504 ||| 29505 ||| 
2021 ||| attention-aware metapath-based network embedding for hin based recommendation. ||| 29506 ||| 29507 ||| 29508 ||| 19254 ||| 29509 ||| 
2022 ||| noun-based attention mechanism for fine-grained named entity recognition. ||| 29510 ||| 29511 ||| 29512 ||| 6235 ||| 29513 ||| 29514 ||| 3419 ||| 
2020 ||| dstp-rnn: a dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction. ||| 29515 ||| 29516 ||| 29517 ||| 29518 ||| 
2021 ||| dsanet: dilated spatial attention for real-time semantic segmentation in urban street scenes. ||| 29519 ||| 17472 ||| 29520 ||| 29521 ||| 
2022 ||| la-hcn: label-based attention for hierarchical multi-label text classification neural network. ||| 29522 ||| 29523 ||| 29524 ||| 29525 ||| 
2021 ||| driver stress detection via multimodal fusion using attention-based cnn-lstm. ||| 29526 ||| 29527 ||| 29528 ||| 29529 ||| 29530 ||| 29531 ||| 7204 ||| 
2021 ||| identification of rice plant diseases using lightweight attention networks. ||| 29532 ||| 15562 ||| 29533 ||| 29534 ||| 
2021 ||| mffnet: multi-dimensional feature fusion network based on attention mechanism for semg analysis to detect muscle fatigue. ||| 13553 ||| 29535 ||| 29536 ||| 6286 ||| 29537 ||| 29538 ||| 25648 ||| 13179 ||| 
2021 ||| candidate point selection using a self-attention mechanism for generating a smooth volatility surface under the sabr model. ||| 29539 ||| 29540 ||| 29541 ||| 29542 ||| 29543 ||| 29544 ||| 8045 ||| 
2021 ||| amfb: attention based multimodal factorized bilinear pooling for multimodal fake news detection. ||| 29545 ||| 165 ||| 
2020 ||| path-based reasoning approach for knowledge graph completion using cnn-bilstm with attention mechanism. ||| 29546 ||| 29547 ||| 29548 ||| 29549 ||| 29550 ||| 
2022 ||| relation-aware heterogeneous graph transformer based drug repurposing. ||| 29551 ||| 29552 ||| 29553 ||| 29554 ||| 
2021 ||| rice diseases detection and classification using attention based neural network and bayesian optimization. ||| 29555 ||| 3417 ||| 29556 ||| 
2020 ||| learning competitive channel-wise attention in residual network with masked regularization and signal boosting. ||| 29557 ||| 29558 ||| 29559 ||| 29560 ||| 29561 ||| 
2021 ||| detection of tuberculosis from chest x-ray images: boosting the performance with vision transformer and transfer learning. ||| 29562 ||| 29563 ||| 29564 ||| 29565 ||| 29566 ||| 
2021 ||| deep multi-scale separable convolutional network with triple attention mechanism: a novel multi-task domain adaptation method for intelligent fault diagnosis. ||| 7873 ||| 29567 ||| 29568 ||| 29569 ||| 
2019 ||| distinguishing mental attention states of humans via an eeg-based passive bci using machine learning methods. ||| 63 ||| 62 ||| 64 ||| 
2022 ||| attention-based dynamic user modeling and deep collaborative filtering recommendation. ||| 29366 ||| 29570 ||| 29368 ||| 29367 ||| 
2021 ||| attentional matrix factorization with context and co-invocation for service recommendation. ||| 25998 ||| 25999 ||| 29571 ||| 26002 ||| 
2020 ||| attention-based deep neural network for internet platform group users' dynamic identification and recommendation. ||| 29572 ||| 29573 ||| 29574 ||| 
2020 ||| agcn: attention-based graph convolutional networks for drug-drug interaction extraction. ||| 29575 ||| 29576 ||| 29577 ||| 
2022 ||| attention based cnn model for fire detection and localization in real-world images. ||| 29578 ||| 29579 ||| 29580 ||| 29581 ||| 29582 ||| 29583 ||| 7442 ||| 29584 ||| 
2022 ||| deep multi-graph neural networks with attention fusion for recommendation. ||| 29585 ||| 29586 ||| 765 ||| 29587 ||| 
2021 ||| an end-to-end framework combining time-frequency expert knowledge and modified transformer networks for vibration signal classification. ||| 29588 ||| 5250 ||| 
2021 ||| an attention-driven convolutional neural network-based multi-level spectral-spatial feature learning for hyperspectral image classification. ||| 29589 ||| 15985 ||| 19940 ||| 
2021 ||| deep learning with multiple scale attention and direction regularization for asset price prediction. ||| 29590 ||| 29591 ||| 
2021 ||| dual-path attention network for single image super-resolution. ||| 29592 ||| 29460 ||| 29461 ||| 29458 ||| 
2021 ||| fast prediction of complicated temperature field using conditional multi-attention generative adversarial networks (cmagan). ||| 29593 ||| 29594 ||| 29595 ||| 2230 ||| 
2022 ||| weakly supervised attention-based models using activation maps for citrus mite and insect pest classification. ||| 29596 ||| 29597 ||| 2712 ||| 26995 ||| 5511 ||| 
2022 ||| identification method of vegetable diseases based on transfer learning and attention mechanism. ||| 15242 ||| 29598 ||| 24190 ||| 29599 ||| 29600 ||| 
2021 ||| reflectance images of effective wavelengths from hyperspectral imaging for identification of fusarium head blight-infected wheat kernels combined with a residual attention convolution neural network. ||| 29601 ||| 29602 ||| 29603 ||| 29604 ||| 29605 ||| 29606 ||| 29607 ||| 29608 ||| 29609 ||| 
2022 ||| ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism. ||| 29610 ||| 29611 ||| 9579 ||| 29612 ||| 
2021 ||| dual attention-guided feature pyramid network for instance segmentation of group pigs. ||| 19242 ||| 1007 ||| 29613 ||| 
2022 ||| an improved yolov5 model based on visual attention mechanism: application to recognition of tomato virus disease. ||| 29614 ||| 29615 ||| 19109 ||| 29616 ||| 3974 ||| 29617 ||| 29618 ||| 29619 ||| 438 ||| 
2021 ||| a dual attention network based on efficientnet-b2 for short-term fish school feeding behavior analysis in aquaculture. ||| 29517 ||| 29620 ||| 29621 ||| 29622 ||| 29623 ||| 29624 ||| 29518 ||| 
2020 ||| crop leaf disease recognition based on self-attention convolutional neural network. ||| 29625 ||| 29626 ||| 
2021 ||| a new attention-based cnn approach for crop mapping using time series sentinel-2 images. ||| 29627 ||| 29628 ||| 29629 ||| 29630 ||| 29631 ||| 
2019 ||| attention-based recurrent neural networks for accurate short-term and long-term dissolved oxygen prediction. ||| 29515 ||| 2251 ||| 29632 ||| 29518 ||| 
2022 ||| canopy-attention-yolov4-based immature/mature apple fruit detection on dense-foliage tree architectures for early crop load estimation. ||| 29633 ||| 29634 ||| 1340 ||| 29635 ||| 
2021 ||| a dual-head attention model for time series data imputation. ||| 29636 ||| 29637 ||| 
2021 ||| semantic segmentation model of cotton roots in-situ image based on attention mechanism. ||| 29638 ||| 29639 ||| 29640 ||| 22328 ||| 8292 ||| 29641 ||| 
2020 ||| chinese agricultural diseases and pests named entity recognition with multi-scale local context features and self-attention mechanism. ||| 29642 ||| 3474 ||| 29643 ||| 29644 ||| 29645 ||| 29646 ||| 1556 ||| 
2021 ||| feature detection method for hind leg segmentation of sheep carcass based on multi-scale dual attention u-net. ||| 29647 ||| 29648 ||| 29649 ||| 29650 ||| 2532 ||| 29651 ||| 29652 ||| 
2021 ||| visual classification of apple bud-types via attention-guided data enrichment network. ||| 29653 ||| 29654 ||| 3402 ||| 29655 ||| 
2021 ||| dual-branch, efficient, channel attention-based crop disease identification. ||| 29656 ||| 14797 ||| 2110 ||| 29657 ||| 29658 ||| 
2020 ||| grape disease image classification based on lightweight convolution neural networks and channelwise attention. ||| 29659 ||| 29660 ||| 9779 ||| 29661 ||| 
2021 ||| a three-dimensional prediction method of dissolved oxygen in pond culture based on attention-gru-gbrt. ||| 29662 ||| 29663 ||| 29664 ||| 29665 ||| 29666 ||| 
2020 ||| detection of unregistered electric distribution transformers in agricultural fields with the aid of sentinel-1 sar images by machine learning approaches. ||| 29667 ||| 
2020 ||| two-level attention and score consistency network for plant segmentation. ||| 29668 ||| 11494 ||| 29669 ||| 5248 ||| 
2022 ||| an efficient attention module for instance segmentation network in pest monitoring. ||| 29670 ||| 29671 ||| 29672 ||| 29673 ||| 
2021 ||| autonomous underwater robot for underwater image enhancement via multi-scale deformable convolution network with attention mechanism. ||| 16933 ||| 29674 ||| 8621 ||| 29675 ||| 
2018 ||| neurofeedback training for enhancement of the focused attention related to athletic performance in elite rifle shooters. ||| 29676 ||| 29677 ||| 29678 ||| 29679 ||| 29680 ||| 29681 ||| 
2020 ||| classification of visual attention level during target gazing using microsaccades. ||| 24594 ||| 24595 ||| 24596 ||| 
2017 ||| sensorimotor accounts of joint attention. ||| 29682 ||| 29683 ||| 29684 ||| 29685 ||| 
2019 ||| applying siamese hierarchical attention neural networks for multi-document summarization. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 29686 ||| 29687 ||| 29688 ||| 29689 ||| 29690 ||| 
2021 ||| consumer cynicism identification for spanish reviews using a spanish transformer model. ||| 29691 ||| 29692 ||| 11005 ||| 3357 ||| 29693 ||| 11927 ||| 11928 ||| 11929 ||| 
2018 ||| experimental research on encoder-decoder architectures with attention for chatbots. ||| 3466 ||| 29694 ||| 29695 ||| 
2021 ||| transformer-based extractive social media question answering on tweetqa. ||| 29696 ||| 29697 ||| 29698 ||| 10728 ||| 29699 ||| 
2017 ||| hybrid attention networks for chinese short text classification. ||| 5349 ||| 5350 ||| 4470 ||| 728 ||| 3307 ||| 728 ||| 
2019 ||| promoting the knowledge of source syntax in transformer nmt is not needed. ||| 29700 ||| 21419 ||| 7440 ||| 21421 ||| 
2019 ||| multi-head multi-layer attention to deep language representations for grammatical error detection. ||| 16989 ||| 14211 ||| 
2019 ||| a deep attention based framework for image caption generation in hindi language. ||| 29048 ||| 29047 ||| 404 ||| 405 ||| 
2020 ||| anomaly-based web attack detection: the application of deep neural network seq2seq with attention mechanism. ||| 29701 ||| 29702 ||| 
2018 ||| decoding covert somatosensory attention by a bci system calibrated with tactile sensation. ||| 5556 ||| 29703 ||| 5560 ||| 29704 ||| 5559 ||| 5112 ||| 
2021 ||| fast eeg-based decoding of the directional focus of auditory attention using common spatial patterns. ||| 8253 ||| 8254 ||| 8255 ||| 
2019 ||| sensory stimulation training for bci system based on somatosensory attentional orientation. ||| 5556 ||| 29703 ||| 5560 ||| 29704 ||| 5559 ||| 5112 ||| 
2022 ||| ma-net: cross-modal cross-attention network for acute ischemic stroke lesion segmentation based on ct perfusion scans. ||| 29705 ||| 29706 ||| 29707 ||| 
2021 ||| inference of the selective auditory attention using sequential lmmse estimation. ||| 24386 ||| 29708 ||| 24387 ||| 24388 ||| 
2018 ||| three-dimensional brain-computer interface control through simultaneous overt spatial attentional and motor imagery tasks. ||| 29709 ||| 29710 ||| 29711 ||| 29712 ||| 370 ||| 
2020 ||| analysis of miniaturization effects and channel selection strategies for eeg sensor networks with application to auditory attention detection. ||| 24414 ||| 8255 ||| 
2020 ||| toward decoding selective attention from single-trial eeg data in cochlear implant users. ||| 12732 ||| 29713 ||| 29714 ||| 29715 ||| 29716 ||| 29717 ||| 29718 ||| 
2022 ||| channel attention networks for robust mr fingerprint matching. ||| 29719 ||| 29720 ||| 29721 ||| 29722 ||| 29723 ||| 29724 ||| 29725 ||| 29726 ||| 29727 ||| 29728 ||| 29729 ||| 7442 ||| 
2020 ||| skin lesion classification using cnns with patch-based attention and diagnosis-guided loss weighting. ||| 29730 ||| 29731 ||| 29732 ||| 59 ||| 29733 ||| 29734 ||| 29735 ||| 1881 ||| 29736 ||| 14906 ||| 
2019 ||| foot inertial sensing for combined cognitive-motor exercise of the sustained attention domain. ||| 24268 ||| 24269 ||| 29737 ||| 24270 ||| 29738 ||| 24274 ||| 24273 ||| 29739 ||| 29740 ||| 29741 ||| 24271 ||| 24272 ||| 29742 ||| 24275 ||| 
2019 ||| ultrasound image segmentation: a deeply supervised network with attention to boundaries. ||| 12790 ||| 12797 ||| 29743 ||| 29744 ||| 
2020 ||| multi-attention mechanism medical image segmentation combined with word embedding technology. ||| 29745 ||| 29079 ||| 29080 ||| 29746 ||| 
2020 ||| drug adverse reaction discovery based on attention mechanism and fusion of emotional information. ||| 29747 ||| 29079 ||| 29080 ||| 
2020 ||| identification of local adverse drug reactions in xinjiang based on attention mechanism and bilstm-cnn hybrid network. ||| 29748 ||| 29079 ||| 29080 ||| 29749 ||| 
2020 ||| personal-bullying detection based on multi-attention and cognitive feature. ||| 29750 ||| 29751 ||| 29752 ||| 29753 ||| 29754 ||| 
2020 ||| coordination motives and competition for attention in information markets. ||| 29755 ||| 29756 ||| 
2021 ||| attentional role of quota implementation. ||| 29757 ||| 29758 ||| 
2017 ||| limited attention and status quo bias. ||| 29759 ||| 58 ||| 29760 ||| 29761 ||| 
2019 ||| inattention and belief polarization. ||| 29762 ||| 29763 ||| 
2021 ||| rational inattention and the monotone likelihood ratio property. ||| 29764 ||| 
2020 ||| estimating information cost functions in models of rational inattention. ||| 29765 ||| 29766 ||| 
2019 ||| directed attention and nonparametric learning. ||| 29767 ||| 29768 ||| 
2018 ||| dynamic rational inattention: analytical results. ||| 29769 ||| 29770 ||| 29771 ||| 
2018 ||| foundations for optimal inattention. ||| 29772 ||| 
2018 ||| limited attention, competition and welfare. ||| 29773 ||| 
2017 ||| rational inattention and the dynamics of consumption and wealth in general equilibrium. ||| 29774 ||| 29775 ||| 29776 ||| 29777 ||| 
2018 ||| the prevalence and gratification of nude self-presentation of men who have sex with men in online-dating environments: attracting attention, empowerment, and self-verification. ||| 29778 ||| 29779 ||| 
2021 ||| internet addiction and attention in adolescents: a systematic review. ||| 29780 ||| 29781 ||| 29782 ||| 10318 ||| 29783 ||| 
2020 ||| short-term prosocial video game exposure influences attentional bias toward prosocial stimuli. ||| 29784 ||| 29785 ||| 29786 ||| 29787 ||| 781 ||| 
2021 ||| whose tweets on covid-19 gain the most attention: celebrities, political, or scientific authorities? ||| 29788 ||| 29789 ||| 29790 ||| 
2022 ||| trends, limits, and challenges of computer technologies in attention deficit hyperactivity disorder diagnosis and treatment. ||| 29791 ||| 29792 ||| 5335 ||| 29793 ||| 29794 ||| 29795 ||| 
2019 ||| interactive avatar boosts the performances of children with attention deficit hyperactivity disorder in dynamic measures of intelligence. ||| 29796 ||| 29797 ||| 29798 ||| 29799 ||| 29800 ||| 
2019 ||| development of virtual reality continuous performance test utilizing social cues for children and adolescents with attention-deficit/hyperactivity disorder. ||| 29801 ||| 29802 ||| 29803 ||| 29804 ||| 29805 ||| 29806 ||| 29807 ||| 
2020 ||| generative attention learning: a "general" framework for high-performance multi-fingered grasping in clutter. ||| 29808 ||| 29809 ||| 29810 ||| 29359 ||| 29811 ||| 29812 ||| 29813 ||| 
2022 ||| heterogeneous graph attention networks for scalable multi-robot scheduling with temporospatial constraints. ||| 22678 ||| 4811 ||| 3929 ||| 
2019 ||| learning attentional regulations for structured tasks execution in robotic cognitive control. ||| 5422 ||| 5427 ||| 
2020 ||| attention-based active visual search for mobile robots. ||| 29814 ||| 29815 ||| 29816 ||| 29817 ||| 
2019 ||| kinesthetic teaching and attentional supervision of structured tasks in human-robot interaction. ||| 5422 ||| 5423 ||| 5427 ||| 5426 ||| 
2019 ||| a driving simulation study on visual cue presented in the peripheral visual field for prompting driver's attention. ||| 29818 ||| 24532 ||| 
2021 ||| design and evaluation of attention guidance through eye gazing of "namida" driving agent. ||| 4594 ||| 29819 ||| 29820 ||| 4597 ||| 
2021 ||| an attention-based deep learning model for traffic flow prediction using spatiotemporal features towards sustainable smart city. ||| 29821 ||| 29822 ||| 29823 ||| 29824 ||| 29825 ||| 29826 ||| 29827 ||| 29828 ||| 29829 ||| 
2017 ||| measurements and characterization of power transformer and low voltage access network for nb-plc. ||| 29830 ||| 29831 ||| 29832 ||| 29833 ||| 
2021 ||| the dynamic effect of public information on liquidity: from the perspective of limited attention. ||| 29834 ||| 29835 ||| 
2021 ||| gaitvision: real-time extraction of gait parameters using residual attention network. ||| 29836 ||| 29837 ||| 29838 ||| 29839 ||| 29840 ||| 29841 ||| 
2020 ||| adaptive attention with consumer sentinel for movie box office prediction. ||| 29842 ||| 23046 ||| 
2021 ||| aemf: an attention-based efficient and multifeature fast text detector. ||| 29843 ||| 29844 ||| 1132 ||| 1236 ||| 
2021 ||| multiscale efficient channel attention for fusion lane line segmentation. ||| 3129 ||| 13676 ||| 
2021 ||| multiscale receptive fields graph attention network for point cloud classification. ||| 29845 ||| 29846 ||| 12720 ||| 
2020 ||| a tri-attention neural network model-basedrecommendation. ||| 29554 ||| 29553 ||| 14268 ||| 29552 ||| 29551 ||| 8582 ||| 
2020 ||| fine-grained lung cancer classification from pet and ct images based on multidimensional attention mechanism. ||| 29847 ||| 29848 ||| 29849 ||| 29850 ||| 29851 ||| 9375 ||| 29852 ||| 29853 ||| 1697 ||| 
2021 ||| time- and quantile-varying causality between investor attention and bitcoin returns: a rolling-window causality-in-quantiles approach. ||| 29854 ||| 181 ||| 
2021 ||| improving transformer-based neural machine translation with prior alignments. ||| 29855 ||| 29856 ||| 29857 ||| 29858 ||| 
2019 ||| decoding attentional state to faces and scenes using eeg brainwaves. ||| 29859 ||| 29860 ||| 29861 ||| 29862 ||| 
2020 ||| multitask learning with local attention for tibetan speech recognition. ||| 1341 ||| 4176 ||| 3226 ||| 2884 ||| 29863 ||| 29864 ||| 
2020 ||| attention with long-term interval-based deep sequential learning for recommendation. ||| 885 ||| 4864 ||| 9002 ||| 29865 ||| 1129 ||| 1423 ||| 
2020 ||| dtfa-net: dynamic and texture features fusion attention network for face antispoofing. ||| 5228 ||| 29866 ||| 29867 ||| 29868 ||| 29869 ||| 29870 ||| 
2018 ||| weibo attention and stock market performance: some empirical evidence. ||| 29871 ||| 29872 ||| 6821 ||| 29873 ||| 
2021 ||| application of multiattention mechanism in power system branch parameter identification. ||| 8084 ||| 29874 ||| 29875 ||| 1235 ||| 29876 ||| 
2020 ||| a hierarchical attention recommender system based on cross-domain social networks. ||| 29877 ||| 29878 ||| 29879 ||| 29880 ||| 29881 ||| 29882 ||| 
2020 ||| complexity to forecast flood: problem definition and spatiotemporal attention lstm solution. ||| 4285 ||| 18655 ||| 18656 ||| 4287 ||| 29883 ||| 
2020 ||| a bichannel transformer with context encoding for document-driven conversation generation in social media. ||| 29884 ||| 29885 ||| 5831 ||| 29886 ||| 1424 ||| 
2021 ||| a spatial-temporal self-attention network (stsan) for location prediction. ||| 6827 ||| 29887 ||| 29888 ||| 29889 ||| 29890 ||| 29891 ||| 29892 ||| 
2022 ||| river segmentation of remote sensing images based on composite attention network. ||| 29893 ||| 29894 ||| 29895 ||| 29896 ||| 19822 ||| 
2020 ||| multichannel deep attention neural networks for the classification of autism spectrum disorder using neuroimaging and personal characteristic data. ||| 8205 ||| 29897 ||| 29898 ||| 13676 ||| 2732 ||| 7015 ||| 29899 ||| 
2020 ||| identification and classification of atmospheric particles based on sem images using convolutional neural network with attention mechanism. ||| 3747 ||| 29900 ||| 29901 ||| 29902 ||| 
2020 ||| articulatory-to-acoustic conversion using bilstm-cnn word-attention-based method. ||| 29903 ||| 29904 ||| 29905 ||| 
2021 ||| comparative efficacy and acceptability of nonpharmacotherapy in the treatment of inattention for adhd: a network meta-analysis. ||| 29906 ||| 29907 ||| 29908 ||| 
2020 ||| phonetics and ambiguity comprehension gated attention network for humor recognition. ||| 8975 ||| 8974 ||| 8967 ||| 8973 ||| 22328 ||| 29909 ||| 29314 ||| 
2021 ||| multi-indices quantification for left ventricle via densenet and gru-based encoder-decoder with attention. ||| 2740 ||| 29910 ||| 29911 ||| 1300 ||| 27856 ||| 9283 ||| 
2021 ||| arabic fake news detection: comparative study of neural networks and transformer-based approaches. ||| 29912 ||| 29913 ||| 29914 ||| 29915 ||| 29916 ||| 
2019 ||| dynamic cross-correlations between participants' attentions to p2p lending and offline loan in the private lending market. ||| 29917 ||| 781 ||| 29918 ||| 
2021 ||| person reidentification model based on multiattention modules and multiscale residuals. ||| 29919 ||| 19391 ||| 29920 ||| 29921 ||| 29922 ||| 29923 ||| 
2021 ||| erratum to "a hierarchical attention recommender system based on cross-domain social networks". ||| 29877 ||| 29878 ||| 29879 ||| 29880 ||| 29881 ||| 29882 ||| 
2020 ||| load estimation of complex power networks from transformer measurements and forecasted loads. ||| 29924 ||| 29925 ||| 
2020 ||| recommendation algorithm in double-layer network based on vector dynamic evolution clustering and attention mechanism. ||| 29926 ||| 19956 ||| 29927 ||| 29928 ||| 
2019 ||| dynamic transmission of correlation between investor attention and stock price: evidence from china's energy industry typical stocks. ||| 29929 ||| 29930 ||| 29931 ||| 29932 ||| 
2020 ||| a deep learning approach for a source code detection model using self-attention. ||| 29933 ||| 7122 ||| 
2021 ||| cross-model transformer method for medical image synthesis. ||| 6646 ||| 5170 ||| 6647 ||| 6648 ||| 
2021 ||| influencing factors of social service satisfaction of the elderly under the background of internet attention. ||| 29934 ||| 
2021 ||| collaborative big data management and analytics in complex systems with edge 2021 eacamera: a case study on ai-based complex attention analysis with edge system. ||| 29935 ||| 29936 ||| 4398 ||| 16954 ||| 
2020 ||| an easy to implement and robust design control method dedicated to multi-cell converters using inter cell transformers. ||| 29937 ||| 29938 ||| 29939 ||| 
2017 ||| design and fault-operation analysis of a modular cyclic cascade inter-cell transformer (ict) for parallel multicell converters. ||| 7111 ||| 29940 ||| 15325 ||| 15326 ||| 29941 ||| 29942 ||| 
2020 ||| realization of extremely high and low impedance transforming ratios using cross-shaped impedance transformer. ||| 29943 ||| 29944 ||| 29945 ||| 29946 ||| 29947 ||| 
2017 ||| a novel ch5 inverter for single-phase transformerless photovoltaic system applications. ||| 21702 ||| 
2018 ||| a -8 mv/+15 mv double polarity piezoelectric transformer-based step-up oscillator for energy harvesting applications. ||| 29948 ||| 29949 ||| 29950 ||| 
2018 ||| a k-ka-band concurrent dual-band single-ended input to differential output low-noise amplifier employing a novel transformer feedback dual-band load. ||| 29951 ||| 29952 ||| 
2020 ||| a digitally programmable wide tuning-range active transformer for inductorless bpf and dcos. ||| 29953 ||| 29954 ||| 
2018 ||| transformer-based input integrated matching in cascode amplifiers: analytical proofs. ||| 29955 ||| 29956 ||| 29957 ||| 
2018 ||| an on-chip transformer-based self-startup hybrid sidito converter for thermoelectric energy harvesting. ||| 25664 ||| 29958 ||| 8430 ||| 10411 ||| 
2020 ||| high-selectivity single-ended/balanced dc-block filtering impedance transformer and its application on power amplifier. ||| 29959 ||| 29960 ||| 29961 ||| 14713 ||| 
2017 ||| tuning range extension of a transformer-based oscillator through common-mode colpitts resonance. ||| 29962 ||| 29963 ||| 29964 ||| 
2017 ||| design and analysis of cmos lnas with transformer feedback for wideband input matching and noise cancellation. ||| 29965 ||| 29966 ||| 29967 ||| 
2020 ||| transformerless grid-connected pv inverter without common mode leakage current and shoot-through problems. ||| 10919 ||| 26816 ||| 8662 ||| 
2019 ||| a v-band power amplifier with integrated wilkinson power dividers-combiners and transformers in 0.18- $\mu$ m sige bicmos. ||| 29968 ||| 29952 ||| 
2020 ||| galvanically isolated dc-dc converter using a single isolation transformer for multi-channel communication. ||| 29969 ||| 6443 ||| 29970 ||| 6444 ||| 
2020 ||| a 0.2-v three-winding transformer-based dco in 16-nm finfet cmos. ||| 29971 ||| 29972 ||| 29973 ||| 29974 ||| 29975 ||| 29964 ||| 
2018 ||| cmos switched-capacitor dsb-ssb converter using a hilbert transformer. ||| 29976 ||| 29977 ||| 852 ||| 29978 ||| 29979 ||| 
2018 ||| synthesis of low-pass real-to-real impedance transformer with coupled-inductors. ||| 29980 ||| 29981 ||| 29982 ||| 29983 ||| 29984 ||| 
2017 ||| transformer-feedback dual-band neutralization technique. ||| 29985 ||| 29986 ||| 
2020 ||| a multiport power electronic transformer based on modular multilevel converter and mixed-frequency modulation. ||| 22002 ||| 22003 ||| 22004 ||| 29987 ||| 29988 ||| 29989 ||| 
2018 ||| a transformer-based current-reuse qvco with an fom up to -200.5 dbc/hz. ||| 29990 ||| 29991 ||| 29992 ||| 29993 ||| 29994 ||| 
2018 ||| second-order equivalent circuits for the design of doubly-tuned transformer matching networks. ||| 29995 ||| 29996 ||| 
2020 ||| -boosted 10-ghz vco with center-tap transformer and stacked transistor. ||| 29997 ||| 29998 ||| 29999 ||| 30000 ||| 30001 ||| 
2019 ||| a 2-ghz fbar-based transformer coupled oscillator design with phase noise reduction. ||| 5760 ||| 5761 ||| 5762 ||| 5763 ||| 
2020 ||| design of compact coupled-line complex impedance transformers with the series susceptance component. ||| 30002 ||| 21825 ||| 30003 ||| 30004 ||| 
2020 ||| an improved pwm technique to achieve continuous input current in common-ground transformerless boost inverter. ||| 30005 ||| 30006 ||| 30007 ||| 30008 ||| 
2020 ||| a 65-81 ghz cmos dual-mode vco using high quality factor transformer-based inductors. ||| 30009 ||| 30010 ||| 4279 ||| 30011 ||| 
2018 ||| a transformer-based 3-db differential coupler. ||| 11973 ||| 25248 ||| 30012 ||| 
2018 ||| a low-voltage low-phase-noise 25-ghz two-tank transformer-feedback vco. ||| 30013 ||| 30014 ||| 30015 ||| 6514 ||| 30016 ||| 30017 ||| 30018 ||| 30019 ||| 
2017 ||| an efficient digital background control for hybrid transformer-based receivers. ||| 30020 ||| 30021 ||| 30022 ||| 30023 ||| 30024 ||| 
2020 ||| design of d-band transformer-based gain-boosting class-ab power amplifiers in silicon technologies. ||| 25717 ||| 25719 ||| 25718 ||| 25720 ||| 30025 ||| 648 ||| 25721 ||| 25722 ||| 
2018 ||| adaptive spatiotemporal feature extraction and dynamic combining methods for selective visual attention system. ||| 30026 ||| 30027 ||| 
2018 ||| a neural attention based model for morphological segmentation. ||| 30028 ||| 
2018 ||| evaluation of the forecast models of chinese tourists to thailand based on search engine attention: a case study of baidu. ||| 30029 ||| 
2020 ||| science map of cochrane systematic reviews receiving the most altmetric attention score: a network analysis. ||| 30030 ||| 30031 ||| 30032 ||| 30033 ||| 30034 ||| 30035 ||| 30036 ||| 30037 ||| 
2020 ||| the balance of attention: the challenges of creating locative cultural storytelling experiences. ||| 30038 ||| 30039 ||| 30040 ||| 30041 ||| 
2019 ||| attention-based sentiment reasoner for aspect-based sentiment analysis. ||| 9027 ||| 30042 ||| 30043 ||| 30044 ||| 30045 ||| 
2019 ||| an audio attention computational model based on information entropy of two channels and exponential moving average. ||| 16550 ||| 12342 ||| 26213 ||| 307 ||| 10003 ||| 
2020 ||| facial uv map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual unets. ||| 25847 ||| 30046 ||| 30047 ||| 30048 ||| 
2020 ||| information cascades prediction with attention neural network. ||| 4477 ||| 30049 ||| 30043 ||| 30050 ||| 30051 ||| 
2022 ||| blind image separation based on attentional generative adversarial network. ||| 443 ||| 30052 ||| 30053 ||| 14598 ||| 30054 ||| 30055 ||| 
2021 ||| attention-based hierarchical recurrent neural networks for mooc forum posts analysis. ||| 30056 ||| 30057 ||| 30058 ||| 13475 ||| 
2021 ||| sentiment analysis of student feedback using multi-head attention fusion model of word and context embedding for lstm. ||| 30059 ||| 30060 ||| 
2021 ||| composite deep neural network with gated-attention mechanism for diabetic retinopathy severity classification. ||| 30061 ||| 30062 ||| 30063 ||| 
2022 ||| improving time series forecasting using lstm and attention models. ||| 30064 ||| 30065 ||| 
2022 ||| serial attention network for skin lesion segmentation. ||| 30066 ||| 29080 ||| 29079 ||| 29745 ||| 30067 ||| 30068 ||| 
2021 ||| textspamdetector: textual content based deep learning framework for social spam detection using conjoint attention mechanism. ||| 30069 ||| 30070 ||| 30071 ||| 
2022 ||| single-channel blind source separation based on attentional generative adversarial network. ||| 443 ||| 30052 ||| 30053 ||| 14598 ||| 30054 ||| 
2020 ||| development of virtual reality rehabilitation games for children with attention-deficit hyperactivity disorder. ||| 30072 ||| 30073 ||| 30074 ||| 30075 ||| 30076 ||| 30077 ||| 
2022 ||| three-stream spatio-temporal attention network for first-person action and interaction recognition. ||| 30078 ||| 29004 ||| 
2020 ||| migrating a software factory to design thinking: paying attention to people and mind-sets. ||| 30079 ||| 30080 ||| 30081 ||| 30082 ||| 30083 ||| 
2022 ||| an attention-aided deep learning framework for massive mimo channel estimation. ||| 15868 ||| 15869 ||| 15870 ||| 15871 ||| 2049 ||| 
2018 ||| scalable lumped models of integrated transformers for galvanically isolated power transfer systems. ||| 30084 ||| 29969 ||| 29970 ||| 6443 ||| 6444 ||| 
2021 ||| a transformer with high coupling coefficient and small area based on tsv. ||| 30085 ||| 30086 ||| 30087 ||| 30088 ||| 30089 ||| 
2017 ||| design of low-power wideband frequency quadruplers based on transformer-coupled resonators for e-band backhaul applications. ||| 30090 ||| 30091 ||| 29995 ||| 30092 ||| 
2018 ||| made in academia: the effect of institutional origin on inventors' attention to science. ||| 30093 ||| 30094 ||| 
2018 ||| tasks interrupted: how anticipating time pressure on resumption of an interrupted task causes attention residue and low performance on interrupting tasks and how a "ready-to-resume" plan mitigates the effects. ||| 30095 ||| 30096 ||| 
2019 ||| visual focus of attention and spontaneous smile recognition based on continuous head pose estimation by cascaded multi-task learning. ||| 30097 ||| 30098 ||| 11318 ||| 30099 ||| 30100 ||| 30101 ||| 
2021 ||| caffnet: channel attention and feature fusion network for multi-target traffic sign detection. ||| 968 ||| 30102 ||| 19843 ||| 11973 ||| 3386 ||| 
2021 ||| baggage image retrieval with attention-based network for security checks. ||| 30103 ||| 2884 ||| 30104 ||| 30105 ||| 30106 ||| 
2018 ||| decentralized robust dynamic state estimation in power systems using instrument transformers. ||| 30107 ||| 30108 ||| 
2022 ||| toast: automated testing of object transformers in dynamic software updates. ||| 28213 ||| 5704 ||| 28216 ||| 
2021 ||| watuning: a workload-aware tuning system with attention-based deep reinforcement learning. ||| 30109 ||| 30110 ||| 30111 ||| 
2019 ||| adversarial heterogeneous network embedding with metapath attention mechanism. ||| 16831 ||| 6718 ||| 30112 ||| 301 ||| 30113 ||| 
2017 ||| deep multimodal reinforcement network with contextually guided recurrent attention for image question answering. ||| 30114 ||| 1748 ||| 30115 ||| 
2020 ||| atlrec: an attentional adversarial transfer learning network for cross-domain recommendation. ||| 949 ||| 30116 ||| 659 ||| 11122 ||| 5110 ||| 660 ||| 
2017 ||| type-aware question answering over knowledge base with attention-based tree-structured neural networks. ||| 8471 ||| 3504 ||| 12334 ||| 
2020 ||| word-pair relevance modeling with multi-view neural attention mechanism for sentence alignment. ||| 23315 ||| 3564 ||| 30117 ||| 3088 ||| 
2020 ||| reference image guided super-resolution via progressive channel attention networks. ||| 30118 ||| 3822 ||| 30119 ||| 30120 ||| 30121 ||| 
2021 ||| attend to chords: improving harmonic analysis of symbolic music using transformer-based models. ||| 11899 ||| 11900 ||| 
2021 ||| impact of mainstream classroom setting on attention of children with autism spectrum disorder: an eye-tracking study. ||| 4346 ||| 4347 ||| 4348 ||| 11828 ||| 26059 ||| 
2022 ||| effects of a cognitive stimulation software on attention, memory, and activities of daily living in mexican older adults. ||| 30122 ||| 30123 ||| 21794 ||| 30124 ||| 8048 ||| 30125 ||| 3419 ||| 10907 ||| 30126 ||| 6234 ||| 30127 ||| 30128 ||| 
2019 ||| enhancement of english learning performance by using an attention-based diagnosing and review mechanism in paper-based learning context with digital pen support. ||| 8009 ||| 30129 ||| 21103 ||| 
2022 ||| ms-transformer: introduce multiple structural priors into a unified transformer for encoding sentences. ||| 30130 ||| 9472 ||| 11682 ||| 3311 ||| 
2020 ||| investigating topics, audio representations and attention for multimodal scene-aware dialog. ||| 9293 ||| 9294 ||| 9295 ||| 9296 ||| 9297 ||| 
2020 ||| rap-net: recurrent attention pooling networks for dialogue response selection. ||| 13952 ||| 30131 ||| 4841 ||| 4843 ||| 
2021 ||| attention-based bilstm fused cnn with gating mechanism model for chinese long text classification. ||| 30132 ||| 5479 ||| 30133 ||| 
2022 ||| talking-heads attention-based knowledge representation for link prediction. ||| 30134 ||| 30135 ||| 7965 ||| 
2020 ||| learning multi-level information for dialogue response selection by highway recurrent transformer. ||| 30131 ||| 13952 ||| 4841 ||| 4843 ||| 
2020 ||| hierarchical multimodal attention for end-to-end audio-visual scene-aware dialogue response generation. ||| 3300 ||| 3301 ||| 3302 ||| 3303 ||| 
2020 ||| speaker-informed time-and-content-aware attention for spoken language understanding. ||| 4940 ||| 30136 ||| 4941 ||| 
2019 ||| joint dialog act segmentation and recognition in human conversations using attention to dialog context. ||| 14598 ||| 4418 ||| 
2022 ||| combining context-relevant features with multi-stage attention network for short text classification. ||| 30137 ||| 30138 ||| 30139 ||| 
2020 ||| knowledge-grounded response generation with deep attentional latent-variable model. ||| 30140 ||| 30141 ||| 4841 ||| 4843 ||| 
2021 ||| a korean named entity recognition method using bi-lstm-crf and masked self-attention. ||| 30142 ||| 6277 ||| 
2017 ||| focusstack: orchestrating edge clouds using focus of attention. ||| 30143 ||| 30144 ||| 30145 ||| 30146 ||| 
2020 ||| residual dense network based on channel-spatial attention for the scene classification of a high-resolution remote sensing image. ||| 2770 ||| 875 ||| 30147 ||| 28786 ||| 1134 ||| 
2021 ||| integrating weighted feature fusion and the spatial attention module with convolutional neural networks for automatic aircraft detection from sar images. ||| 30148 ||| 30149 ||| 30150 ||| 30151 ||| 30152 ||| 30153 ||| 30154 ||| 
2021 ||| erratum: liu et al. nightlight as a proxy of economic indicators: fine-grained gdp inference around chinese mainland via attention-augmented cnn from daytime satellite imagery. remote sens. 2021, 13, 2067. ||| 24810 ||| 30155 ||| 30156 ||| 5902 ||| 30157 ||| 30158 ||| 30159 ||| 
2019 ||| semantic segmentation on remotely sensed images using an enhanced global convolutional network with channel attention and domain specific transfer learning. ||| 30160 ||| 25771 ||| 25772 ||| 25773 ||| 15786 ||| 
2021 ||| knowledge and spatial pyramid distance-based gated graph attention network for remote sensing semantic segmentation. ||| 17991 ||| 10812 ||| 30161 ||| 30162 ||| 30163 ||| 4634 ||| 30164 ||| 30165 ||| 30166 ||| 17694 ||| 30167 ||| 
2021 ||| trs: transformers for remote sensing scene classification. ||| 30168 ||| 30169 ||| 9600 ||| 
2021 ||| target detection network for sar images based on semi-supervised learning and attention mechanism. ||| 30170 ||| 30171 ||| 26787 ||| 3501 ||| 
2022 ||| transformer for tree counting in aerial images. ||| 1382 ||| 669 ||| 
2021 ||| domain-adversarial training of self-attention-based networks for land cover classification using multi-temporal sentinel-2 satellite imagery. ||| 30172 ||| 30173 ||| 30174 ||| 30175 ||| 
2020 ||| multi-temporal unmanned aerial vehicle remote sensing for vegetable mapping using an attention-based recurrent convolutional neural network. ||| 30176 ||| 5233 ||| 30177 ||| 30178 ||| 30179 ||| 30180 ||| 30181 ||| 30182 ||| 
2021 ||| split-attention networks with self-calibrated convolution for moon impact crater detection from multi-source data. ||| 30183 ||| 30184 ||| 10330 ||| 7284 ||| 30185 ||| 30186 ||| 7830 ||| 30187 ||| 
2022 ||| a spatiotemporal fusion method based on multiscale feature extraction and spatial channel attention mechanism. ||| 30188 ||| 30189 ||| 30190 ||| 20432 ||| 
2022 ||| swin transformer and deep convolutional neural networks for coastal wetland classification using sentinel-1, sentinel-2, and lidar data. ||| 30191 ||| 30192 ||| 
2021 ||| semantic segmentation of aerial imagery via split-attention networks with disentangled nonlocal and edge supervision. ||| 3761 ||| 30193 ||| 30194 ||| 
2021 ||| attention multi-scale network for automatic layer extraction of ice radar topological sequences. ||| 30195 ||| 26764 ||| 1824 ||| 30196 ||| 30197 ||| 30198 ||| 
2021 ||| a 3d cascaded spectral-spatial element attention network for hyperspectral image classification. ||| 30199 ||| 1224 ||| 20669 ||| 30200 ||| 6506 ||| 3151 ||| 30201 ||| 
2020 ||| a multiscale self-adaptive attention network for remote sensing scene classification. ||| 3535 ||| 30202 ||| 30203 ||| 400 ||| 30204 ||| 5743 ||| 2094 ||| 
2022 ||| uatnet: u-shape attention-based transformer net for meteorological satellite cloud recognition. ||| 16873 ||| 30205 ||| 30206 ||| 6679 ||| 30207 ||| 30208 ||| 
2020 ||| super-resolution for hyperspectral remote sensing images based on the 3d attention-srgan network. ||| 30209 ||| 30210 ||| 6919 ||| 6918 ||| 
2021 ||| multi-modality and multi-scale attention fusion network for land cover classification from vhr remote sensing images. ||| 19869 ||| 30211 ||| 30212 ||| 30213 ||| 19870 ||| 30214 ||| 
2021 ||| memory-augmented transformer for remote sensing image semantic segmentation. ||| 17645 ||| 30215 ||| 30216 ||| 21133 ||| 
2021 ||| ship object detection of remote sensing image based on visual attention. ||| 30217 ||| 30218 ||| 29282 ||| 5170 ||| 
2019 ||| a mutiscale residual attention network for multitask learning of human activity using radar micro-doppler signatures. ||| 3001 ||| 2419 ||| 30219 ||| 
2019 ||| hdranet: hybrid dilated residual attention network for sar image despeckling. ||| 14653 ||| 949 ||| 30220 ||| 30221 ||| 
2021 ||| cross-dimension attention guided self-supervised remote sensing single-image super-resolution. ||| 30222 ||| 30223 ||| 27259 ||| 11493 ||| 27258 ||| 
2021 ||| self-attention-based conditional variational auto-encoder generative adversarial networks for hyperspectral classification. ||| 30224 ||| 30225 ||| 30226 ||| 8160 ||| 30227 ||| 
2020 ||| sample generation with self-attention generative adversarial adaptation network (sagaan) for hyperspectral image classification. ||| 30228 ||| 5250 ||| 30229 ||| 30230 ||| 
2021 ||| sga-net: self-constructing graph attention neural network for semantic segmentation of remote sensing images. ||| 3678 ||| 6579 ||| 2424 ||| 5536 ||| 30231 ||| 
2022 ||| a dual attention convolutional neural network for crop classification using time-series sentinel-2 imagery. ||| 30232 ||| 30233 ||| 30234 ||| 
2021 ||| hybrid attention based residual network for pansharpening. ||| 20085 ||| 30235 ||| 22906 ||| 25652 ||| 6610 ||| 25651 ||| 25310 ||| 30236 ||| 
2019 ||| building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms. ||| 30237 ||| 1856 ||| 30238 ||| 30239 ||| 30240 ||| 30241 ||| 10070 ||| 
2020 ||| generating anchor boxes based on attention mechanism for object detection in remote sensing images. ||| 30242 ||| 30243 ||| 30244 ||| 1160 ||| 23687 ||| 30245 ||| 
2022 ||| pan-sharpening based on cnn+ pyramid transformer by using no-reference loss. ||| 30246 ||| 18280 ||| 30247 ||| 
2022 ||| an investigation of a multidimensional cnn combined with an attention mechanism model to resolve small-sample problems in hyperspectral image classification. ||| 30248 ||| 30249 ||| 30250 ||| 30251 ||| 30252 ||| 30253 ||| 30254 ||| 30255 ||| 
2021 ||| improving yolov5 with attention mechanism for detecting boulders from planetary images. ||| 30256 ||| 30257 ||| 6679 ||| 30258 ||| 
2021 ||| land use and land cover mapping using rapideye imagery based on a novel band attention deep learning method in the three gorges reservoir area. ||| 1340 ||| 4208 ||| 30259 ||| 30260 ||| 30261 ||| 27239 ||| 30262 ||| 
2022 ||| superpixel-based attention graph neural network for semantic segmentation in aerial images. ||| 30263 ||| 23181 ||| 13362 ||| 15016 ||| 30264 ||| 1153 ||| 
2021 ||| remote sensing image defogging networks based on dual self-attention boost residual octave convolution. ||| 30265 ||| 30266 ||| 30267 ||| 16872 ||| 2969 ||| 30268 ||| 
2021 ||| saffnet: self-attention-based feature fusion network for remote sensing few-shot scene classification. ||| 30269 ||| 30270 ||| 
2019 ||| an improved grabcut method based on a visual attention model for rare-earth ore mining area recognition with high-resolution remote sensing images. ||| 30271 ||| 30272 ||| 30273 ||| 30274 ||| 
2019 ||| spectral-spatial attention networks for hyperspectral image classification. ||| 6859 ||| 6857 ||| 6858 ||| 6860 ||| 1244 ||| 6861 ||| 30275 ||| 30276 ||| 6863 ||| 
2021 ||| effect of attention mechanism in deep learning-based remote sensing image processing: a systematic literature review. ||| 30277 ||| 1994 ||| 30278 ||| 30279 ||| 30280 ||| 
2021 ||| attention-based spatial and spectral network with pca-guided self-supervised feature extraction for change detection in hyperspectral images. ||| 30281 ||| 30282 ||| 30283 ||| 7909 ||| 3675 ||| 
2020 ||| a hybrid attention-aware fusion network (hafnet) for building extraction from high-resolution imagery and lidar data. ||| 989 ||| 30284 ||| 30285 ||| 398 ||| 30286 ||| 30287 ||| 30288 ||| 
2020 ||| a slimmer network with polymorphic and group attention modules for more efficient object detection in aerial images. ||| 5819 ||| 5820 ||| 19928 ||| 5821 ||| 30289 ||| 30290 ||| 
2019 ||| attentionbased deep feature fusion for the scene classification of highresolution remote sensing images. ||| 
2022 ||| fusion classification of hsi and msi using a spatial-spectral vision transformer for wetland biodiversity estimation. ||| 30291 ||| 30292 ||| 3337 ||| 30293 ||| 30294 ||| 30295 ||| 30296 ||| 
2021 ||| deanet: dual encoder with attention network for semantic segmentation of remote sensing imagery. ||| 2548 ||| 30297 ||| 30298 ||| 30299 ||| 23181 ||| 
2022 ||| gansformer: a detection network for aerial images with high performance combining convolutional network and transformer. ||| 2349 ||| 19209 ||| 30300 ||| 30301 ||| 30302 ||| 
2022 ||| reflective noise filtering of large-scale point cloud using transformer. ||| 19504 ||| 30303 ||| 30304 ||| 30305 ||| 
2022 ||| multiscale feature fusion network incorporating 3d self-attention for hyperspectral image classification. ||| 30306 ||| 30307 ||| 30308 ||| 30309 ||| 30310 ||| 
2019 ||| deep feature fusion with integration of residual connection and attention model for classification of vhr remote sensing images. ||| 30311 ||| 30312 ||| 30313 ||| 30314 ||| 30315 ||| 
2020 ||| generative adversarial networks based on collaborative learning and attention mechanism for hyperspectral image classification. ||| 6738 ||| 30316 ||| 30317 ||| 30318 ||| 397 ||| 400 ||| 18653 ||| 
2022 ||| a multi-domain collaborative transfer learning method with multi-scale repeated attention mechanism for underwater side-scan sonar image classification. ||| 30319 ||| 30320 ||| 30321 ||| 
2021 ||| u2-onet: a two-level nested octave u-structure network with a multi-scale attention mechanism for moving object segmentation. ||| 30322 ||| 30323 ||| 1235 ||| 382 ||| 1647 ||| 30324 ||| 1446 ||| 
2021 ||| improved transformer net for hyperspectral image classification. ||| 30306 ||| 30310 ||| 30308 ||| 30325 ||| 
2019 ||| multi-scale semantic segmentation and spatial relationship recognition of remote sensing images based on an attention model. ||| 17991 ||| 2355 ||| 10812 ||| 30326 ||| 30327 ||| 30161 ||| 30162 ||| 30328 ||| 
2021 ||| mare: self-supervised multi-attention resu-net for semantic segmentation in remote sensing. ||| 30329 ||| 30330 ||| 23997 ||| 
2021 ||| an attention-guided multilayer feature aggregation network for remote sensing image scene classification. ||| 765 ||| 6820 ||| 30331 ||| 6822 ||| 30332 ||| 
2021 ||| double-branch network with pyramidal convolution and iterative attention for hyperspectral image classification. ||| 14650 ||| 30333 ||| 30334 ||| 30335 ||| 6891 ||| 
2022 ||| a transformer-based coarse-to-fine wide-swath sar image registration method under weak texture conditions. ||| 5217 ||| 6832 ||| 30336 ||| 
2022 ||| agnet: an attention-based graph network for point cloud classification and segmentation. ||| 30337 ||| 8802 ||| 30338 ||| 30339 ||| 30340 ||| 8349 ||| 
2021 ||| hcnet: a point cloud object detection network based on height and channel attention. ||| 875 ||| 20531 ||| 9419 ||| 6701 ||| 
2021 ||| deep residual dual-attention network for super-resolution reconstruction of remote sensing images. ||| 7828 ||| 30341 ||| 30342 ||| 30343 ||| 
2021 ||| revise-net: exploiting reverse attention mechanism for salient object detection. ||| 30344 ||| 30345 ||| 30346 ||| 954 ||| 30347 ||| 20354 ||| 
2020 ||| a new framework for automatic airports extraction from sar images using multi-level dual attention mechanism. ||| 30150 ||| 30348 ||| 30152 ||| 30151 ||| 30349 ||| 30350 ||| 989 ||| 
2021 ||| gsap: a global structure attention pooling method for graph-based visual place recognition. ||| 30351 ||| 15854 ||| 30352 ||| 2932 ||| 30353 ||| 
2021 ||| multiscale and multitemporal road detection from high resolution sar images using attention mechanism. ||| 30354 ||| 30355 ||| 30356 ||| 30357 ||| 
2021 ||| pag-yolo: a portable attention-guided yolo network for small ship detection. ||| 15447 ||| 30358 ||| 30359 ||| 781 ||| 30360 ||| 30361 ||| 
2021 ||| improved yolov3 based on attention mechanism for fast and accurate ship detection in optical remote sensing images. ||| 28290 ||| 3117 ||| 30362 ||| 
2021 ||| combinational fusion and global attention of the single-shot method for synthetic aperture radar ship detection. ||| 30363 ||| 30364 ||| 30365 ||| 30366 ||| 
2022 ||| remote sensing image denoising based on deep and shallow feature fusion and attention mechanism. ||| 30367 ||| 30368 ||| 30369 ||| 30370 ||| 30371 ||| 30372 ||| 
2021 ||| vision transformers for remote sensing image classification. ||| 6726 ||| 6725 ||| 6727 ||| 30373 ||| 30374 ||| 
2022 ||| hyperspectral image classification based on 3d coordination attention mechanism network. ||| 30375 ||| 30376 ||| 28341 ||| 30377 ||| 
2021 ||| attention-guided siamese fusion network for change detection of remote sensing images. ||| 30378 ||| 15544 ||| 397 ||| 30379 ||| 17877 ||| 400 ||| 
2021 ||| transformer-based decoder designs for semantic segmentation on remotely sensed images. ||| 30160 ||| 25771 ||| 25772 ||| 25773 ||| 15786 ||| 
2021 ||| looking for change? roll the dice and demand attention. ||| 30380 ||| 1226 ||| 30381 ||| 30382 ||| 
2021 ||| hybrid dense network with dual attention for hyperspectral image classification. ||| 30383 ||| 24482 ||| 30384 ||| 29609 ||| 
2022 ||| da-imrn: dual-attention-guided interactive multi-scale residual network for hyperspectral image classification. ||| 2982 ||| 30385 ||| 30386 ||| 30387 ||| 30388 ||| 2188 ||| 
2021 ||| a spatial-channel collaborative attention network for enhancement of multiresolution classification. ||| 30389 ||| 30390 ||| 9335 ||| 30391 ||| 400 ||| 4104 ||| 6829 ||| 
2021 ||| efficient transformer for remote sensing image segmentation. ||| 30392 ||| 30393 ||| 30394 ||| 30395 ||| 27542 ||| 
2019 ||| building extraction from very high resolution aerial imagery using joint attention deep neural network. ||| 30396 ||| 30397 ||| 30398 ||| 30399 ||| 30400 ||| 25099 ||| 
2020 ||| transboundary basins need more attention: anthropogenic impacts on land cover changes in aras river basin, monitoring and prediction. ||| 30401 ||| 30402 ||| 30403 ||| 30404 ||| 30405 ||| 30406 ||| 30407 ||| 30408 ||| 
2022 ||| voids filling of dem with multiattention generative adversarial network model. ||| 30409 ||| 1274 ||| 30410 ||| 10923 ||| 30411 ||| 
2021 ||| real-time underwater maritime object detection in side-scan sonar images based on transformer-yolov5. ||| 30412 ||| 30413 ||| 30414 ||| 1124 ||| 30415 ||| 30416 ||| 
2021 ||| wildfire segmentation using deep vision transformers. ||| 30417 ||| 24563 ||| 30418 ||| 30419 ||| 30420 ||| 30421 ||| 
2020 ||| da-capsunet: a dual-attention capsule u-net for road extraction from remote sensing imagery. ||| 30422 ||| 30423 ||| 30424 ||| 
2021 ||| building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets. ||| 30425 ||| 729 ||| 30426 ||| 30427 ||| 30428 ||| 30429 ||| 30430 ||| 30431 ||| 
2021 ||| building extraction from remote sensing images with sparse token transformers. ||| 30432 ||| 2185 ||| 6654 ||| 
2021 ||| light-weight cloud detection network for optical remote sensing images with attention-based deeplabv3+ architecture. ||| 30433 ||| 18280 ||| 30247 ||| 
2019 ||| description generation for remote sensing images using attribute attention mechanism. ||| 397 ||| 398 ||| 6617 ||| 30434 ||| 399 ||| 
2020 ||| a new deep learning network for automatic bridge detection from sar images based on balanced and attention mechanism. ||| 30150 ||| 30435 ||| 30151 ||| 30152 ||| 30349 ||| 30350 ||| 989 ||| 
2021 ||| residual augmented attentional u-shaped network for spectral reconstruction from rgb images. ||| 6699 ||| 6698 ||| 6700 ||| 6701 ||| 6843 ||| 
2021 ||| converting optical videos to infrared videos using attention gan and its impact on target detection and classification performance. ||| 30436 ||| 30437 ||| 30438 ||| 30439 ||| 30440 ||| 16761 ||| 
2021 ||| hyperspectral image classification based on multi-scale residual network with attention mechanism. ||| 30306 ||| 30310 ||| 
2020 ||| radet: refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images. ||| 6852 ||| 21507 ||| 30441 ||| 400 ||| 30442 ||| 
2021 ||| compound multiscale weak dense network with hybrid attention for hyperspectral image classification. ||| 30334 ||| 30333 ||| 14650 ||| 30335 ||| 27687 ||| 6891 ||| 
2021 ||| eaau-net: enhanced asymmetric attention u-net for infrared small target detection. ||| 30443 ||| 30444 ||| 30445 ||| 11212 ||| 30446 ||| 
2019 ||| lam: remote sensing image captioning with label-attention mechanism. ||| 30447 ||| 30448 ||| 19850 ||| 30449 ||| 13676 ||| 10525 ||| 
2019 ||| smokenet: satellite smoke scene detection using convolutional neural network with spatial and channel-wise attention. ||| 30450 ||| 2230 ||| 30451 ||| 30452 ||| 30453 ||| 
2020 ||| classification of hyperspectral image based on double-branch dual-attention mechanism network. ||| 8207 ||| 30454 ||| 30455 ||| 11466 ||| 30456 ||| 
2020 ||| kda3d: key-point densification and multi-attention guidance for 3d object detection. ||| 30457 ||| 8952 ||| 1241 ||| 30458 ||| 17676 ||| 30459 ||| 30460 ||| 
2020 ||| compact cloud detection with bidirectional self-attention knowledge distillation. ||| 30461 ||| 12746 ||| 10525 ||| 30448 ||| 30462 ||| 30463 ||| 3279 ||| 
2021 ||| lightweight underwater object detection based on yolo v4 and multi-scale attentional feature fusion. ||| 18264 ||| 30464 ||| 17827 ||| 30465 ||| 30466 ||| 
2020 ||| deep discriminative representation learning with attention map for scene classification. ||| 5536 ||| 30467 ||| 602 ||| 21132 ||| 30468 ||| 30469 ||| 30470 ||| 
2021 ||| subtask attention based object detection in remote sensing images. ||| 30471 ||| 21754 ||| 6707 ||| 30472 ||| 30473 ||| 
2019 ||| attention graph convolution network for image segmentation in big sar imagery data. ||| 6733 ||| 4176 ||| 30474 ||| 30434 ||| 30475 ||| 
2021 ||| semantic-guided attention refinement network for salient object detection in optical remote sensing images. ||| 30476 ||| 30477 ||| 30478 ||| 30479 ||| 
2021 ||| ship detection via dilated rate search and attention-guided feature representation. ||| 15447 ||| 30358 ||| 30359 ||| 30480 ||| 781 ||| 
2021 ||| a novel lstm model with interaction dual attention for radar echo extrapolation. ||| 30481 ||| 25166 ||| 30482 ||| 7466 ||| 30483 ||| 
2021 ||| a fast aircraft detection method for sar images based on efficient bidirectional path aggregated attention network. ||| 30153 ||| 30150 ||| 30151 ||| 30349 ||| 30348 ||| 30154 ||| 30148 ||| 
2019 ||| double-branch multi-attention mechanism network for hyperspectral image classification. ||| 30389 ||| 23366 ||| 4104 ||| 7700 ||| 397 ||| 
2022 ||| an adaptive attention fusion mechanism convolutional network for object detection in remote sensing images. ||| 30484 ||| 30485 ||| 30486 ||| 30487 ||| 30488 ||| 30489 ||| 30490 ||| 
2021 ||| ams-net: an attention-based multi-scale network for classification of 3d terracotta warrior fragments. ||| 3114 ||| 23033 ||| 30491 ||| 30492 ||| 30493 ||| 30494 ||| 30495 ||| 30496 ||| 30497 ||| 
2021 ||| a multi-scale spatial attention region proposal network for high-resolution optical remote sensing imagery. ||| 30498 ||| 400 ||| 2349 ||| 30499 ||| 30500 ||| 
2021 ||| mra-snet: siamese networks of multiscale residual and attention for change detection in high-resolution remote sensing images. ||| 7676 ||| 24482 ||| 30501 ||| 30502 ||| 
2021 ||| attention enhanced u-net for building extraction from farmland based on google and worldview-2 remote sensing images. ||| 30503 ||| 30504 ||| 21518 ||| 25978 ||| 4236 ||| 30505 ||| 30506 ||| 30507 ||| 
2018 ||| attention-mechanism-containing neural networks for high-resolution remote sensing image classification. ||| 30508 ||| 30509 ||| 30510 ||| 30511 ||| 
2020 ||| a multi-level attention model for remote sensing image captions. ||| 6852 ||| 30512 ||| 400 ||| 30513 ||| 30442 ||| 
2021 ||| a spectral spatial attention fusion with deformable convolutional residual network for hyperspectral image classification. ||| 28341 ||| 30375 ||| 30376 ||| 30377 ||| 
2020 ||| csa-mso3dcnn: multiscale octave 3d cnn with channel and spatial attention for hyperspectral image classification. ||| 30514 ||| 30515 ||| 30516 ||| 382 ||| 
2021 ||| ecap-yolo: efficient channel attention pyramid yolo for small object detection in aerial image. ||| 30517 ||| 30518 ||| 30519 ||| 
2020 ||| residual group channel and space attention network for hyperspectral image classification. ||| 30520 ||| 30521 ||| 30522 ||| 968 ||| 
2021 ||| pcan - part-based context attention network for thermal power plant detection in remote sensing imagery. ||| 30523 ||| 30448 ||| 30524 ||| 13676 ||| 6159 ||| 10525 ||| 
2021 ||| dual attention feature fusion and adaptive context for accurate segmentation of very high-resolution remote sensing images. ||| 14650 ||| 30525 ||| 30526 ||| 12373 ||| 
2019 ||| hyperspectral images classification based on dense convolutional networks with spectral-wise attention mechanism. ||| 30527 ||| 949 ||| 30528 ||| 30529 ||| 
2022 ||| a lightweight convolutional neural network based on group-wise hybrid attention for remote sensing scene classification. ||| 30375 ||| 30530 ||| 30531 ||| 30377 ||| 
2021 ||| hyperspectral image classification based on two-branch spectral-spatial-feature attention network. ||| 19149 ||| 22306 ||| 30532 ||| 6011 ||| 30533 ||| 3304 ||| 
2021 ||| a residual attention and local context-aware network for road extraction from high-resolution remote sensing imagery. ||| 2498 ||| 30534 ||| 30535 ||| 30536 ||| 
2021 ||| nightlight as a proxy of economic indicators: fine-grained gdp inference around mainland china via attention-augmented cnn from daytime satellite imagery. ||| 24810 ||| 30155 ||| 30156 ||| 5902 ||| 30157 ||| 30158 ||| 30159 ||| 
2022 ||| multiscale object detection in remote sensing images combined with multi-receptive-field features and relation-connected attention. ||| 30537 ||| 30538 ||| 11960 ||| 
2021 ||| a novel ensemble architecture of residual attention-based deep metric learning for remote sensing image retrieval. ||| 30539 ||| 30540 ||| 6891 ||| 30541 ||| 30542 ||| 
2020 ||| attention-guided multi-scale segmentation neural network for interactive extraction of region objects from high-resolution satellite imagery. ||| 8293 ||| 30543 ||| 30544 ||| 30545 ||| 22842 ||| 
2021 ||| remote sensing image scene classification based on global self-attention module. ||| 30546 ||| 30547 ||| 30548 ||| 
2021 ||| gacm: a graph attention capsule model for the registration of tls point clouds in the urban scene. ||| 30549 ||| 30550 ||| 6604 ||| 30551 ||| 30552 ||| 30553 ||| 14271 ||| 30554 ||| 
2022 ||| a bidirectional deep-learning-based spectral attention mechanism for hyperspectral data classification. ||| 30555 ||| 30556 ||| 
2020 ||| object tracking in unmanned aerial vehicle videos via multifeature discrimination and instance-aware attention network. ||| 8441 ||| 28786 ||| 4600 ||| 29187 ||| 
2021 ||| transformer meets convolution: a bilateral awareness network for semantic segmentation of very fine resolution urban scene images. ||| 30557 ||| 8207 ||| 30558 ||| 30455 ||| 30559 ||| 30560 ||| 
2021 ||| crop type mapping from optical and radar time series using attention-based deep learning. ||| 30561 ||| 30562 ||| 30563 ||| 
2021 ||| a dual-model architecture with grouping-attention-fusion for remote sensing scene classification. ||| 30564 ||| 2814 ||| 30565 ||| 20027 ||| 6627 ||| 30566 ||| 
2018 ||| building extraction in very high resolution imagery by dense-attention networks. ||| 25869 ||| 30567 ||| 30568 ||| 30569 ||| 890 ||| 30570 ||| 
2021 ||| dganet: a dilated graph attention-based network for local feature extraction on 3d point clouds. ||| 30571 ||| 30572 ||| 30570 ||| 30573 ||| 30574 ||| 30575 ||| 
2019 ||| mask obb: a semantic attention-based mask oriented bounding box representation for multi-category object detection in aerial images. ||| 30576 ||| 18906 ||| 30577 ||| 30578 ||| 30579 ||| 30580 ||| 
2021 ||| triple-attention-based parallel network for hyperspectral image classification. ||| 30581 ||| 30582 ||| 2983 ||| 2982 ||| 
2021 ||| semantic segmentation of urban buildings using a high-resolution network (hrnet) with channel and spatial attention gates. ||| 30583 ||| 30584 ||| 
2021 ||| flood discharge prediction based on remote-sensed spatiotemporal features fusion and graph attention. ||| 2230 ||| 30585 ||| 30586 ||| 30587 ||| 6931 ||| 30588 ||| 30589 ||| 
2020 ||| a cloud detection method using convolutional neural network based on gabor transform and attention mechanism with dark channel subnet for remote sensing image. ||| 875 ||| 6851 ||| 1035 ||| 4061 ||| 1341 ||| 6701 ||| 30590 ||| 1305 ||| 
2021 ||| msnet: a multi-stream fusion network for remote sensing spatiotemporal fusion based on transformer and convolution. ||| 20432 ||| 30591 ||| 30592 ||| 497 ||| 
2021 ||| ha-net: a lake water body extraction network based on hybrid-scale attention and transfer learning. ||| 30593 ||| 30594 ||| 30595 ||| 
2020 ||| retraction: zhu r. et al. attention-based deep feature fusion for the scene classification of high-resolution remote sensing images. remote sensing. 2019 11(17), 1996. ||| 
2018 ||| remote sensing scene classification based on convolutional neural networks pre-trained using attention-guided sparse filters. ||| 30596 ||| 14693 ||| 6855 ||| 17547 ||| 30597 ||| 30598 ||| 
2020 ||| an efficient method for infrared and visual images fusion based on visual attention technique. ||| 30599 ||| 30600 ||| 8335 ||| 30601 ||| 
2020 ||| multi-image super resolution of remotely sensed images using residual attention deep neural networks. ||| 30602 ||| 30173 ||| 30174 ||| 30175 ||| 
2022 ||| transformer neural network for weed and crop classification of high resolution uav images. ||| 30603 ||| 30604 ||| 13128 ||| 30605 ||| 30606 ||| 
2020 ||| attention-based residual network with scattering transform features for hyperspectral unmixing with limited training samples. ||| 30607 ||| 30608 ||| 30609 ||| 30610 ||| 
2020 ||| attention-based pyramid network for segmentation and classification of high-resolution and hyperspectral remote sensing images. ||| 30611 ||| 5958 ||| 30612 ||| 30613 ||| 
2022 ||| transformer with transfer cnn for remote-sensing-image object detection. ||| 30614 ||| 30615 ||| 30616 ||| 
2021 ||| few-shot object detection on remote sensing images via shared attention module and balanced fine-tuning strategy. ||| 30617 ||| 30618 ||| 30619 ||| 30620 ||| 30621 ||| 
2021 ||| ebarec-bs: effective band attention reconstruction network for hyperspectral imagery band selection. ||| 12167 ||| 30622 ||| 30623 ||| 30624 ||| 
2019 ||| hyperspectral image super-resolution with 1d-2d attentional convolutional neural network. ||| 6699 ||| 30625 ||| 1717 ||| 6700 ||| 6701 ||| 6720 ||| 
2021 ||| improved singan integrated with an attentional mechanism for remote sensing image classification. ||| 30626 ||| 3248 ||| 30627 ||| 30628 ||| 30629 ||| 30630 ||| 
2022 ||| structural attention enhanced continual meta-learning for graph edge labeling based few-shot remote sensing scene classification. ||| 30631 ||| 30632 ||| 901 ||| 30633 ||| 30634 ||| 
2021 ||| self-attention in reconstruction bias u-net for semantic segmentation of building rooftops in optical remote sensing images. ||| 14270 ||| 30635 ||| 9555 ||| 30424 ||| 3691 ||| 19197 ||| 
2021 ||| msst-net: a multi-scale adaptive network for building extraction from remote sensing images based on swin transformer. ||| 30636 ||| 30637 ||| 
2020 ||| a spatial-temporal attention-based method and a new dataset for remote sensing image change detection. ||| 2424 ||| 6654 ||| 
2021 ||| spectral and spatial global context attention for hyperspectral image classification. ||| 30638 ||| 30639 ||| 18638 ||| 3386 ||| 30640 ||| 30641 ||| 
2021 ||| a multi-branch feature fusion strategy based on an attention mechanism for remote sensing image scene classification. ||| 30375 ||| 17645 ||| 30377 ||| 
2021 ||| an improved swin transformer-based model for remote sensing object detection and instance segmentation. ||| 30642 ||| 30643 ||| 30644 ||| 30645 ||| 30646 ||| 30647 ||| 30648 ||| 30649 ||| 
2021 ||| synergistic attention for ship instance segmentation in sar images. ||| 6909 ||| 6908 ||| 30650 ||| 30651 ||| 30652 ||| 6654 ||| 
2020 ||| building extraction based on u-net with an attention block and multiple losses. ||| 30653 ||| 13335 ||| 30570 ||| 30654 ||| 
2021 ||| aau-net: attention-based asymmetric u-net for subject-sensitive hashing of remote sensing images. ||| 30655 ||| 8116 ||| 3906 ||| 30656 ||| 30613 ||| 30657 ||| 
2021 ||| remote sensing time series classification based on self-attention mechanism and time sequence enhancement. ||| 30658 ||| 30659 ||| 6703 ||| 14420 ||| 30660 ||| 2519 ||| 
2020 ||| pga-siamnet: pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection. ||| 30544 ||| 30543 ||| 8293 ||| 30661 ||| 30662 ||| 22842 ||| 
2021 ||| attention-guided multispectral and panchromatic image classification. ||| 23392 ||| 30663 ||| 30664 ||| 30212 ||| 30665 ||| 
2020 ||| mffa-sarnet: deep transferred multi-level feature fusion attention network with dual optimized loss for small-sample sar atr. ||| 30666 ||| 30667 ||| 7955 ||| 30668 ||| 30669 ||| 30670 ||| 30671 ||| 30672 ||| 30673 ||| 10846 ||| 10848 ||| 
2021 ||| adt-det: adaptive dynamic refined single-stage transformer detector for arbitrary-oriented object detection in satellite optical imagery. ||| 30674 ||| 15877 ||| 30675 ||| 30676 ||| 30677 ||| 
2020 ||| anchor-free convolutional network with dense attention feature aggregation for ship detection in sar images. ||| 4176 ||| 30678 ||| 1224 ||| 30475 ||| 30434 ||| 
2019 ||| transferred multi-perception attention networks for remote sensing image super-resolution. ||| 4011 ||| 30679 ||| 3751 ||| 30238 ||| 
2020 ||| gsca-unet: towards automatic shadow detection in urban aerial imagery with global-spatial-context attention module. ||| 30680 ||| 30637 ||| 30681 ||| 30682 ||| 9999 ||| 30683 ||| 
2021 ||| pyramid information distillation attention network for super-resolution reconstruction of remote sensing images. ||| 7828 ||| 30343 ||| 30342 ||| 30341 ||| 30684 ||| 30685 ||| 
2020 ||| amn: attention metric network for one-shot remote sensing image scene classification. ||| 5907 ||| 30686 ||| 11453 ||| 30687 ||| 5858 ||| 
2021 ||| remote sensing image super-resolution based on dense channel attention network. ||| 30688 ||| 30689 ||| 5170 ||| 30690 ||| 30511 ||| 
2021 ||| spatial-spectral transformer for hyperspectral image classification. ||| 10812 ||| 30615 ||| 30691 ||| 
2021 ||| a deformable convolutional neural network with spatial-channel attention for remote sensing scene classification. ||| 15790 ||| 30610 ||| 
2019 ||| attention matters: an exploration of relationship between google search behaviors and crude oil prices. ||| 633 ||| 30692 ||| 30693 ||| 24048 ||| 
2021 ||| motor fault diagnosis algorithm based on wavelet and attention mechanism. ||| 30694 ||| 1073 ||| 30695 ||| 
2021 ||| pedestrian reidentification algorithm based on deconvolution network feature extraction-multilayer attention mechanism convolutional neural network. ||| 30696 ||| 30697 ||| 2388 ||| 
2019 ||| reverse engineering for the design patterns extraction of android mobile applications for attention deficit disorder. ||| 30698 ||| 3369 ||| 30699 ||| 3157 ||| 10373 ||| 10318 ||| 
2022 ||| flicker phase-noise reduction using gate-drain phase shift in transformer-based oscillators. ||| 5250 ||| 30700 ||| 30701 ||| 30702 ||| 29964 ||| 30703 ||| 
2021 ||| frequency selective impedance transformer with high-impedance transforming ratio and extremely high/low termination impedances. ||| 30704 ||| 30705 ||| 30706 ||| 
2021 ||| a compact transformer-based fractional-n adpll in 10-nm finfet cmos. ||| 29971 ||| 29972 ||| 29974 ||| 29975 ||| 29973 ||| 30707 ||| 30708 ||| 30709 ||| 30710 ||| 30711 ||| 30712 ||| 29964 ||| 
2021 ||| evaluation of static synchronous compensator and rail power conditioner in electrified railway systems using v/v and scott power transformers. ||| 13094 ||| 13095 ||| 2871 ||| 13096 ||| 1994 ||| 13097 ||| 13098 ||| 
2020 ||| graph attention network for detecting license plates in crowded street scenes. ||| 30713 ||| 30714 ||| 30715 ||| 30716 ||| 30717 ||| 173 ||| 30718 ||| 
2018 ||| explicit ensemble attention learning for improving visual question answering. ||| 22418 ||| 22419 ||| 13153 ||| 
2020 ||| spatio-temporal fall event detection in complex scenes using attention guided lstm. ||| 11225 ||| 11223 ||| 11222 ||| 3226 ||| 11224 ||| 16895 ||| 
2020 ||| partial attention and multi-attribute learning for vehicle re-identification. ||| 30719 ||| 11183 ||| 30720 ||| 155 ||| 
2020 ||| video captioning with text-based dynamic attention and step-by-step learning. ||| 30721 ||| 30722 ||| 
2021 ||| attention fusion network for multi-spectral semantic segmentation. ||| 30723 ||| 30724 ||| 12273 ||| 
2019 ||| dual-supervised attention network for deep cross-modal hashing. ||| 30725 ||| 8783 ||| 30726 ||| 2148 ||| 2149 ||| 
2018 ||| boosting image classification through semantic attention filtering strategies. ||| 30727 ||| 30728 ||| 2253 ||| 30729 ||| 30730 ||| 30731 ||| 30732 ||| 
2021 ||| image captioning with transformer and knowledge graph. ||| 9472 ||| 30733 ||| 30734 ||| 5157 ||| 
2019 ||| learning part-aware attention networks for kinship verification. ||| 30735 ||| 30736 ||| 
2021 ||| msar-net: multi-scale attention based light-weight image super-resolution. ||| 30737 ||| 30738 ||| 
2022 ||| hmfca-net: hierarchical multi-frequency based channel attention net for mobile phone surface defect detection. ||| 30739 ||| 10253 ||| 26217 ||| 30740 ||| 500 ||| 16446 ||| 
2021 ||| snipedet: attention-guided pyramidal prediction kernels for generic object detection. ||| 30741 ||| 30742 ||| 30743 ||| 30744 ||| 
2018 ||| fusion-attention network for person search with free-form natural language. ||| 2276 ||| 30745 ||| 2279 ||| 
2021 ||| self-attention binary neural tree for video summarization. ||| 5033 ||| 20383 ||| 
2021 ||| clothes image caption generation with attribute detection and visual attention model. ||| 30746 ||| 30747 ||| 5543 ||| 30748 ||| 
2020 ||| attention-aware invertible hashing network with skip connections. ||| 10768 ||| 17583 ||| 17584 ||| 17585 ||| 17586 ||| 24989 ||| 
2021 ||| trseg: transformer for semantic segmentation. ||| 30749 ||| 8559 ||| 8560 ||| 
2020 ||| can-gan: conditioned-attention normalized gan for face age synthesis. ||| 30750 ||| 30751 ||| 498 ||| 13415 ||| 30752 ||| 30753 ||| 
2021 ||| self-attention-based conditional random fields latent variables model for sequence labeling. ||| 30754 ||| 677 ||| 678 ||| 30755 ||| 30756 ||| 7010 ||| 
2021 ||| laryngoscope8: laryngeal image dataset and classification of laryngeal disease based on attention mechanism. ||| 30757 ||| 1305 ||| 30758 ||| 30759 ||| 30760 ||| 16925 ||| 
2018 ||| multimodal architecture for video captioning with memory networks and an attention mechanism. ||| 3337 ||| 19890 ||| 8726 ||| 
2020 ||| attention guided neural network models for occluded pedestrian detection. ||| 30761 ||| 30762 ||| 6553 ||| 19075 ||| 
2020 ||| object detection with class aware region proposal network and focused attention objective. ||| 30763 ||| 11549 ||| 30764 ||| 30765 ||| 
2020 ||| learning cross-modal correlations by exploring inter-word semantics and stacked co-attention. ||| 8160 ||| 8159 ||| 1273 ||| 7095 ||| 4190 ||| 689 ||| 
2020 ||| enhanced factorization machine via neural pairwise ranking and attention networks. ||| 255 ||| 30766 ||| 30767 ||| 254 ||| 1203 ||| 
2020 ||| single-image raindrop removal using concurrent channel-spatial attention and long-short skip connections. ||| 30768 ||| 1125 ||| 30769 ||| 4648 ||| 
2021 ||| document-level relation extraction via graph transformer networks and temporal convolutional networks. ||| 30770 ||| 8488 ||| 30771 ||| 30772 ||| 8489 ||| 
2020 ||| sahan: scale-aware hierarchical attention network for scene text recognition. ||| 23358 ||| 17418 ||| 6559 ||| 17680 ||| 30773 ||| 17420 ||| 
2017 ||| human attribute recognition by refining attention heat map. ||| 8358 ||| 9669 ||| 307 ||| 
2020 ||| semantically consistent text to fashion image synthesis with an enhanced attentional generative adversarial network. ||| 30774 ||| 7904 ||| 7905 ||| 7906 ||| 
2020 ||| diablo: dictionary-based attention block for deep metric learning. ||| 30775 ||| 30776 ||| 30777 ||| 30778 ||| 
2021 ||| triplet interactive attention network for cross-modality person re-identification. ||| 837 ||| 30779 ||| 19869 ||| 30780 ||| 
2021 ||| landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition. ||| 30781 ||| 30782 ||| 
2020 ||| hierarchical attention network for action segmentation. ||| 30783 ||| 11374 ||| 11330 ||| 11331 ||| 
2021 ||| scanet: a spatial and channel attention based network for partial-to-partial point cloud registration. ||| 30784 ||| 30785 ||| 30193 ||| 
2020 ||| adversarial learning based attentional scene text recognizer. ||| 30786 ||| 30787 ||| 30788 ||| 30789 ||| 30790 ||| 30791 ||| 
2019 ||| video-based person re-identification via spatio-temporal attentional and two-stream fusion convolutional networks. ||| 19796 ||| 30792 ||| 155 ||| 
2021 ||| smaat-unet: precipitation nowcasting using a small attention-unet architecture. ||| 30793 ||| 30794 ||| 27268 ||| 
2021 ||| human motion reconstruction using deep transformer networks. ||| 13395 ||| 13396 ||| 30795 ||| 13397 ||| 
2022 ||| deep relational self-attention networks for scene graph generation. ||| 977 ||| 1753 ||| 6850 ||| 
2021 ||| midcan: a multiple input deep convolutional attention network for covid-19 diagnosis based on chest ct and chest x-ray. ||| 30796 ||| 1770 ||| 1340 ||| 28960 ||| 
2019 ||| quantifying patterns of joint attention during human-robot interactions: an application for autism spectrum disorder assessment. ||| 9818 ||| 30797 ||| 30798 ||| 30799 ||| 30800 ||| 30801 ||| 9817 ||| 9815 ||| 
2021 ||| adversarial robustness via attention transfer. ||| 30802 ||| 9537 ||| 7707 ||| 30803 ||| 30804 ||| 30805 ||| 
2018 ||| move, attend and predict: an attention-based neural model for people's movement prediction. ||| 30806 ||| 30807 ||| 30808 ||| 30809 ||| 
2020 ||| an attention-based row-column encoder-decoder model for text recognition in japanese historical documents. ||| 9792 ||| 9793 ||| 9794 ||| 
2019 ||| image-attribute reciprocally guided attention network for pedestrian attribute recognition. ||| 2276 ||| 30810 ||| 2277 ||| 30811 ||| 
2019 ||| multi-level attention model for person re-identification. ||| 23472 ||| 8832 ||| 19342 ||| 8532 ||| 
2020 ||| constraint saliency based intelligent camera for enhancing viewers attention towards intended face. ||| 19213 ||| 19214 ||| 19215 ||| 19216 ||| 
2020 ||| movie fill in the blank by joint learning from video and text with adaptive temporal attention. ||| 1037 ||| 155 ||| 154 ||| 
2020 ||| pedestrian attribute recognition based on multiple time steps attention. ||| 2276 ||| 30812 ||| 30810 ||| 2278 ||| 2279 ||| 
2022 ||| attention guided deep features for accurate body mass index estimation. ||| 1390 ||| 30813 ||| 30814 ||| 30815 ||| 30816 ||| 30817 ||| 
2022 ||| sam-net: semantic probabilistic and attention mechanisms of dynamic objects for self-supervised depth and camera pose estimation in visual odometry applications. ||| 30818 ||| 30819 ||| 10070 ||| 30820 ||| 15544 ||| 2855 ||| 
2022 ||| edge detection with attention: from global view to local focus. ||| 1998 ||| 30821 ||| 835 ||| 30822 ||| 
2019 ||| assessment of feature fusion strategies in visual attention mechanism for saliency detection. ||| 6554 ||| 11215 ||| 30823 ||| 13203 ||| 30824 ||| 22343 ||| 13205 ||| 
2022 ||| category attention transfer for efficient fine-grained visual categorization. ||| 30825 ||| 30826 ||| 11333 ||| 
2020 ||| thorax disease classification with attention guided convolutional neural network. ||| 30827 ||| 2918 ||| 30828 ||| 30829 ||| 8571 ||| 208 ||| 
2020 ||| visual question answering with attention transfer and a cross-modal gating mechanism. ||| 3337 ||| 17230 ||| 19889 ||| 19888 ||| 8726 ||| 
2021 ||| adaptive hybrid attention network for hyperspectral image classification. ||| 6920 ||| 6865 ||| 
2020 ||| feature fusion network based on attention mechanism for 3d semantic segmentation of point clouds. ||| 30830 ||| 30831 ||| 30832 ||| 7828 ||| 30833 ||| 30834 ||| 
2021 ||| mixed pooling and richer attention feature fusion for crack detection. ||| 7965 ||| 30835 ||| 30836 ||| 
2018 ||| joint spatial-temporal attention for action recognition. ||| 30837 ||| 2248 ||| 4259 ||| 30838 ||| 2252 ||| 2255 ||| 
2020 ||| multi-label chest x-ray image classification via category-wise residual attention learning. ||| 30827 ||| 2918 ||| 
2021 ||| 3d dental model segmentation with graph attentional convolution network. ||| 3226 ||| 30839 ||| 30840 ||| 30841 ||| 1305 ||| 14647 ||| 30842 ||| 11223 ||| 
2020 ||| refineu-net: improved u-net with progressive global feedbacks and residual attention guided local refinement for medical image segmentation. ||| 30843 ||| 30844 ||| 30845 ||| 30846 ||| 30847 ||| 
2022 ||| bert-kgly: a bidirectional encoder representations from transformers (bert)-based model for predicting lysine glycation site for homo sapiens. ||| 30848 ||| 5530 ||| 30849 ||| 30850 ||| 30851 ||| 30852 ||| 
2019 ||| pyramid predictive attention network for medical image segmentation. ||| 30853 ||| 30854 ||| 30855 ||| 30856 ||| 24118 ||| 
2018 ||| deep attention residual hashing. ||| 438 ||| 30857 ||| 23309 ||| 30858 ||| 6799 ||| 
2019 ||| attention-guided spatial transformer networks for fine-grained visual recognition. ||| 11249 ||| 3906 ||| 11251 ||| 
2019 ||| multi-level attention based blstm neural network for biomedical event extraction. ||| 15301 ||| 16631 ||| 12584 ||| 1417 ||| 5890 ||| 
2020 ||| neural machine translation with target-attention model. ||| 30859 ||| 1254 ||| 3112 ||| 3049 ||| 880 ||| 
2017 ||| an attention-based hybrid neural network for document modeling. ||| 30860 ||| 30861 ||| 30862 ||| 3248 ||| 30863 ||| 
2018 ||| ultra-low field mri food inspection system using hts-squid with flux transformer. ||| 30864 ||| 30865 ||| 30866 ||| 30867 ||| 
2018 ||| winding ratio design of transformer in equivalent circuit of circular patch array absorber. ||| 30868 ||| 30869 ||| 30870 ||| 30871 ||| 30872 ||| 
2018 ||| fully integrated cmos pas with two-winding and single-winding combined transformer for wlan applications. ||| 30873 ||| 30874 ||| 30875 ||| 30876 ||| 30877 ||| 
2019 ||| attention-guided region proposal network for pedestrian detection. ||| 19250 ||| 30878 ||| 1796 ||| 7237 ||| 
2019 ||| channel and frequency attention module for diverse animal sound classification. ||| 30879 ||| 30880 ||| 8559 ||| 8560 ||| 
2019 ||| attention-based dense lstm for speech emotion recognition. ||| 28654 ||| 28645 ||| 30881 ||| 17104 ||| 
2019 ||| spectra restoration of bone-conducted speech via attention-based contextual information and spectro-temporal structure constraint. ||| 30882 ||| 30883 ||| 30884 ||| 4494 ||| 4493 ||| 
2017 ||| image modification based on spatial frequency components for visual attention retargeting. ||| 26039 ||| 30885 ||| 30886 ||| 26042 ||| 26043 ||| 
2021 ||| dissolved gas analysis for transformer fault based on learning spiking neural p system with belief adaboost. ||| 30887 ||| 30888 ||| 30889 ||| 30890 ||| 30891 ||| 30892 ||| 30893 ||| 
2020 ||| cab u-net: an end-to-end category attention boosting algorithm for segmentation. ||| 7783 ||| 7786 ||| 7784 ||| 7785 ||| 
2021 ||| a category attention instance segmentation network for four cardiac chambers segmentation in fetal echocardiography. ||| 30894 ||| 30895 ||| 30896 ||| 30897 ||| 30898 ||| 5157 ||| 17979 ||| 30899 ||| 27549 ||| 30900 ||| 
2020 ||| fusion based on attention mechanism and context constraint for multi-modal brain tumor segmentation. ||| 15607 ||| 2693 ||| 15610 ||| 15608 ||| 
2020 ||| pyramid attention recurrent networks for real-time guidewire segmentation and tracking in intraoperative x-ray fluoroscopy. ||| 276 ||| 5186 ||| 5185 ||| 27400 ||| 5184 ||| 271 ||| 
2021 ||| three-dimensional breast tumor segmentation on dce-mri with a multilabel attention-guided joint-phase-learning network. ||| 30901 ||| 30902 ||| 30903 ||| 30904 ||| 30905 ||| 13701 ||| 30906 ||| 30907 ||| 
2020 ||| mri image synthesis with dual discriminator adversarial learning and difficulty-aware attention mechanism for hippocampal subfields segmentation. ||| 27759 ||| 10646 ||| 30908 ||| 5450 ||| 30909 ||| 30910 ||| 30911 ||| 30912 ||| 
2020 ||| multiple sclerosis lesion activity segmentation with attention-guided two-path cnns. ||| 29730 ||| 30913 ||| 6787 ||| 30914 ||| 30915 ||| 30916 ||| 30917 ||| 30918 ||| 14906 ||| 
2021 ||| human papilloma virus detection in oropharyngeal carcinomas with in situ hybridisation using hand crafted morphological features and deep central attention residual networks. ||| 15530 ||| 15529 ||| 30919 ||| 15528 ||| 15531 ||| 
2021 ||| sentiment classification with adversarial learning and attention mechanism. ||| 30920 ||| 3034 ||| 10817 ||| 30921 ||| 8207 ||| 30922 ||| 
2019 ||| a hierarchical recurrent approach to predict scene graphs from a visual-attention-oriented perspective. ||| 30923 ||| 9090 ||| 8802 ||| 19919 ||| 10817 ||| 
2018 ||| attention recognition in eeg-based affective learning research using cfs+knn algorithm. ||| 23377 ||| 15710 ||| 30924 ||| 30925 ||| 
2022 ||| sacall: a neural network basecaller for oxford nanopore sequencing data based on self-attention mechanism. ||| 16634 ||| 16635 ||| 16636 ||| 16637 ||| 1130 ||| 
2021 ||| multi-view mammographic density classification by dilated and attention-guided residual learning. ||| 130 ||| 30926 ||| 30927 ||| 30928 ||| 30929 ||| 30930 ||| 9199 ||| 30931 ||| 15597 ||| 
2021 ||| a deep segmentation network of multi-scale feature fusion based on attention mechanism for ivoct lumen contour. ||| 17472 ||| 30932 ||| 30933 ||| 30934 ||| 30935 ||| 22961 ||| 30936 ||| 30937 ||| 30938 ||| 30939 ||| 7084 ||| 16821 ||| 
2021 ||| high-risk prediction of cardiovascular diseases via attention-based deep neural networks. ||| 13558 ||| 30940 ||| 16792 ||| 16687 ||| 1130 ||| 
2022 ||| novel transformer networks for improved sequence labeling in genomics. ||| 30941 ||| 30942 ||| 
2021 ||| attention based simplified deep residual network for citywide crowd flows prediction. ||| 30943 ||| 30944 ||| 30945 ||| 30946 ||| 30947 ||| 
2022 ||| distant supervised relation extraction based on residual attention. ||| 30948 ||| 4477 ||| 30949 ||| 30950 ||| 
2022 ||| referring image segmentation with attention guided cross modal fusion for semantic oriented languages. ||| 30951 ||| 14797 ||| 13819 ||| 30952 ||| 30953 ||| 
2021 ||| using bilstm with attention mechanism to automatically detect self-admitted technical debt. ||| 1565 ||| 5744 ||| 19066 ||| 1037 ||| 
2022 ||| polynomial stacked-attention network for nationality classification. ||| 30954 ||| 1134 ||| 1916 ||| 
2019 ||| codeattention: translating source code to comments by exploiting the code constructs. ||| 30955 ||| 7913 ||| 765 ||| 1863 ||| 
2022 ||| speech-driven facial animation with spectral gathering and temporal attention. ||| 30956 ||| 30957 ||| 30958 ||| 30959 ||| 
2022 ||| visual saliency prediction using multi-scale attention gated network. ||| 30960 ||| 30961 ||| 16853 ||| 19098 ||| 
2022 ||| attention based video captioning framework for hindi. ||| 30962 ||| 30963 ||| 30964 ||| 
2022 ||| scale-aware attention-based multi-resolution representation for multi-person pose estimation. ||| 30965 ||| 30966 ||| 6524 ||| 30967 ||| 
2022 ||| nasmamsr: a fast image super-resolution network based on neural architecture search and multiple attention mechanism. ||| 7676 ||| 30968 ||| 8813 ||| 30969 ||| 4003 ||| 
2022 ||| code generation from a graphical user interface via attention-based encoder-decoder model. ||| 30970 ||| 30971 ||| 1820 ||| 30972 ||| 11299 ||| 
2019 ||| panorama based on multi-channel-attention cnn for 3d model recognition. ||| 19742 ||| 11361 ||| 6076 ||| 30973 ||| 
2020 ||| evaluation of uhf transfer function in a power transformer for real-time partial discharge detection. ||| 30974 ||| 30975 ||| 
2020 ||| library and information science papers discussed on twitter: a new network-based approach for measuring public attention. ||| 18401 ||| 18402 ||| 18403 ||| 
2017 ||| understanding the correlations between social attention and topic trends of scientific publications. ||| 30976 ||| 922 ||| 23315 ||| 17183 ||| 9012 ||| 30977 ||| 
2021 ||| a topic detection method based on word-attention networks. ||| 30978 ||| 
2021 ||| modeling temporal patterns of cyberbullying detection with hierarchical attention networks. ||| 9728 ||| 9729 ||| 9730 ||| 9731 ||| 5791 ||| 
2018 ||| multi-dgas: a pattern based educational framework design for power transformers faults interpretation and comparative performance analysis. ||| 30979 ||| 30980 ||| 
2018 ||| gaze-based cursor control impairs performance in divided attention. ||| 59 ||| 30981 ||| 30982 ||| 
2019 ||| learners' attention preferences of information in online learning. ||| 30983 ||| 30984 ||| 30985 ||| 30986 ||| 30987 ||| 
2022 ||| fetal ecg extraction from maternal ecg using attention-based cyclegan. ||| 30988 ||| 30989 ||| 30990 ||| 30991 ||| 30992 ||| 30993 ||| 
2021 ||| multiscale attention guided network for covid-19 diagnosis using chest x-ray images. ||| 30994 ||| 17295 ||| 11634 ||| 1224 ||| 1235 ||| 27349 ||| 25766 ||| 
2022 ||| deep attention and graphical neural network for multiple sclerosis lesion segmentation from mr imaging sequences. ||| 27158 ||| 27159 ||| 14316 ||| 744 ||| 27160 ||| 
2022 ||| task-induced pyramid and attention gan for multimodal brain image imputation and classification in alzheimer's disease. ||| 28872 ||| 26582 ||| 18051 ||| 13426 ||| 
2021 ||| unsupervised self-adaptive auditory attention decoding. ||| 8253 ||| 8254 ||| 8255 ||| 
2021 ||| graph convolutional autoencoder and fully-connected autoencoder with attention mechanism based method for predicting drug-disease associations. ||| 24119 ||| 30995 ||| 30996 ||| 24117 ||| 24118 ||| 
2020 ||| hard attention net for automatic retinal vessel segmentation. ||| 30997 ||| 30998 ||| 30999 ||| 31000 ||| 31001 ||| 
2022 ||| aprnet: a 3d anisotropic pyramidal reversible network with multi-modal cross-dimension attention for brain tissue segmentation in mr images. ||| 31002 ||| 2519 ||| 29031 ||| 31003 ||| 30297 ||| 31004 ||| 
2020 ||| prediction of reaction time and vigilance variability from spatio-spectral features of resting-state eeg in a long sustained attention task. ||| 24205 ||| 24206 ||| 24208 ||| 5335 ||| 24209 ||| 24210 ||| 
2022 ||| cross-model attention-guided tumor segmentation for 3d automated breast ultrasound (abus) images. ||| 6599 ||| 31005 ||| 31006 ||| 31007 ||| 31008 ||| 18051 ||| 
2021 ||| bidirectional representation learning from transformers using multimodal electronic health record data to predict depression. ||| 31009 ||| 31010 ||| 31011 ||| 31012 ||| 
2021 ||| attention-aware residual network based manifold learning for white blood cells classification. ||| 17239 ||| 4550 ||| 12196 ||| 31013 ||| 5010 ||| 31014 ||| 31015 ||| 31016 ||| 31017 ||| 17238 ||| 
2021 ||| multi-scale self-guided attention for medical image segmentation. ||| 31018 ||| 26899 ||| 
2019 ||| photoplethysmographic waveform versus heart rate variability to identify low-stress states: attention test. ||| 31019 ||| 31020 ||| 10907 ||| 31021 ||| 31022 ||| 31023 ||| 31024 ||| 
2022 ||| attention-guided discriminative region localization and label distribution learning for bone age assessment. ||| 6915 ||| 6318 ||| 31025 ||| 31026 ||| 31010 ||| 31012 ||| 
2021 ||| an attention based cnn-lstm approach for sleep-wake detection with heterogeneous sensors. ||| 17993 ||| 16597 ||| 17991 ||| 31027 ||| 3488 ||| 
2021 ||| accurate retinal vessel segmentation in color fundus images via fully attention-based networks. ||| 5498 ||| 5501 ||| 5500 ||| 5499 ||| 20216 ||| 16673 ||| 
2022 ||| multi-level attention network for retinal vessel segmentation. ||| 8835 ||| 241 ||| 4644 ||| 31028 ||| 
2020 ||| a residual based attention model for eeg based sleep staging. ||| 31029 ||| 5018 ||| 31030 ||| 20532 ||| 31031 ||| 31032 ||| 31033 ||| 
2021 ||| attention-guided multi-branch convolutional neural network for mitosis detection from histopathological images. ||| 27715 ||| 31034 ||| 440 ||| 31035 ||| 6582 ||| 
2020 ||| heart sound segmentation using bidirectional lstms with attention. ||| 12567 ||| 31036 ||| 11374 ||| 11330 ||| 31037 ||| 11331 ||| 
2020 ||| pulmonary textures classification via a multi-scale attention network. ||| 345 ||| 31038 ||| 19935 ||| 31039 ||| 31040 ||| 31041 ||| 31042 ||| 31043 ||| 31044 ||| 
2022 ||| exploiting icd hierarchy for classification of ehrs in spanish through multi-task transformers. ||| 13979 ||| 13981 ||| 2600 ||| 13983 ||| 
2020 ||| lesion location attention guided network for multi-label thoracic disease classification in chest x-rays. ||| 31045 ||| 31046 ||| 26463 ||| 31047 ||| 
2021 ||| gcsba-net: gabor-based and cascade squeeze bi-attention network for gland segmentation. ||| 31048 ||| 31049 ||| 15600 ||| 949 ||| 31050 ||| 
2021 ||| discriminative feature network based on a hierarchical attention mechanism for semantic hippocampus segmentation. ||| 31051 ||| 6465 ||| 31052 ||| 31053 ||| 262 ||| 10876 ||| 
2021 ||| attention-guided deep neural network with multi-scale feature fusion for liver vessel segmentation. ||| 19015 ||| 1241 ||| 781 ||| 8884 ||| 7804 ||| 31054 ||| 10075 ||| 19017 ||| 1166 ||| 31055 ||| 
2021 ||| attention-refnet: interactive attention refinement network for infected area segmentation of covid-19. ||| 31056 ||| 27882 ||| 31057 ||| 1526 ||| 1551 ||| 31058 ||| 6915 ||| 31059 ||| 6496 ||| 1550 ||| 11469 ||| 31060 ||| 1528 ||| 
2021 ||| automatic acetowhite lesion segmentation via specular reflection removal and deep attention network. ||| 31061 ||| 31062 ||| 31063 ||| 31064 ||| 31065 ||| 
2021 ||| an attention-based mechanism to combine images and metadata in deep learning models applied to skin cancer classification. ||| 3369 ||| 31066 ||| 31067 ||| 
2020 ||| using a multi-task recurrent neural network with attention mechanisms to predict hospital mortality of patients. ||| 31068 ||| 31069 ||| 31070 ||| 31071 ||| 31072 ||| 
2021 ||| limitations of transformers on clinical text classification. ||| 23769 ||| 31073 ||| 31074 ||| 31075 ||| 31076 ||| 31077 ||| 31078 ||| 31079 ||| 31080 ||| 31081 ||| 31082 ||| 23771 ||| 
2021 ||| attention-based lstm for non-contact sleep stage classification using ir-uwb radar. ||| 31083 ||| 31084 ||| 31085 ||| 31086 ||| 31087 ||| 31088 ||| 31089 ||| 31090 ||| 
2020 ||| thorax-net: an attention regularized deep neural network for classification of thoracic diseases on chest radiography. ||| 31091 ||| 31092 ||| 27491 ||| 9199 ||| 
2020 ||| attention-guided 3d-cnn framework for glaucoma detection and structural-functional association using volumetric images. ||| 31093 ||| 31094 ||| 20046 ||| 31095 ||| 31096 ||| 31097 ||| 
2021 ||| multi-res-attention unet: a cnn model for the segmentation of focal cortical dysplasia lesions from magnetic resonance images. ||| 31098 ||| 31099 ||| 31100 ||| 31101 ||| 31102 ||| 31103 ||| 31104 ||| 28995 ||| 
2021 ||| a spatio-temporal attention-based model for infant movement assessment from videos. ||| 31105 ||| 31106 ||| 31107 ||| 31108 ||| 31109 ||| 31110 ||| 
2021 ||| attention-based parallel multiscale convolutional neural network for visual evoked potentials eeg classification. ||| 31111 ||| 31112 ||| 31113 ||| 31114 ||| 5264 ||| 31115 ||| 
2020 ||| motor imagery classification via temporal attention cues of graph embedded eeg signals. ||| 773 ||| 770 ||| 18539 ||| 771 ||| 
2021 ||| covid-19 automatic diagnosis with radiographic imaging: explainable attention transfer deep neural networks. ||| 13683 ||| 13684 ||| 13686 ||| 13687 ||| 
2022 ||| self-attention-based deep learning network for regional influenza forecasting. ||| 31116 ||| 31117 ||| 31118 ||| 31119 ||| 
2021 ||| learning a deep cnn denoising approach using anatomical prior information implemented with attention mechanism for low-dose ct imaging on clinical patient data from multiple anatomical sites. ||| 31120 ||| 31121 ||| 31122 ||| 31123 ||| 31124 ||| 189 ||| 30931 ||| 11467 ||| 27703 ||| 
2021 ||| medication combination prediction using temporal attention mechanism and simple graph convolution. ||| 8149 ||| 31125 ||| 8154 ||| 31126 ||| 2532 ||| 2977 ||| 5472 ||| 
2022 ||| an o-shape neural network with attention modules to detect junctions in biomedical images without segmentation. ||| 31127 ||| 6796 ||| 31128 ||| 7785 ||| 28490 ||| 
2020 ||| computer-aided diagnosis in histopathological images of the endometrium using a convolutional neural network and attention mechanisms. ||| 7725 ||| 31129 ||| 19025 ||| 27172 ||| 31130 ||| 
2021 ||| multimodal spatial attention module for targeting multimodal pet-ct lung tumor segmentation. ||| 31131 ||| 14038 ||| 31132 ||| 31133 ||| 14039 ||| 
2021 ||| long-term prediction for temporal propagation of seasonal influenza using transformer-based model. ||| 13824 ||| 31134 ||| 31135 ||| 
2021 ||| adversarial neural network with sentiment-aware attention for detecting adverse drug reactions. ||| 29314 ||| 8974 ||| 728 ||| 8967 ||| 8349 ||| 29316 ||| 
2021 ||| drug-drug interaction extraction using a position and similarity fusion-based attention mechanism. ||| 31136 ||| 31137 ||| 
2021 ||| traditional chinese medicine symptom normalization approach leveraging hierarchical semantic information and text matching with attention mechanism. ||| 19432 ||| 31138 ||| 31139 ||| 5739 ||| 31140 ||| 31141 ||| 1128 ||| 31142 ||| 31143 ||| 31144 ||| 
2021 ||| attention-based bidirectional long short-term memory networks for extracting temporal relationships from clinical discharge summaries. ||| 31145 ||| 31146 ||| 31147 ||| 
2021 ||| nd transformers for evidence-based medicine and argument mining in medical literature. ||| 31148 ||| 13155 ||| 
2021 ||| explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation. ||| 4775 ||| 2253 ||| 31149 ||| 31150 ||| 31151 ||| 1476 ||| 
2021 ||| extracting chemical-induced disease relation by integrating a hierarchical concentrative attention and a hybrid graph-based neural network. ||| 16633 ||| 16631 ||| 31152 ||| 31153 ||| 
2019 ||| associative attention networks for temporal relation extraction from electronic health records. ||| 31153 ||| 16631 ||| 16633 ||| 20809 ||| 20808 ||| 
2022 ||| ammu: a survey of transformer-based biomedical pretrained language models. ||| 31154 ||| 31155 ||| 31156 ||| 
2018 ||| using neural attention networks to detect adverse medical events from electronic health records. ||| 31157 ||| 8305 ||| 31158 ||| 31159 ||| 31160 ||| 
2021 ||| improved biomedical word embeddings in the transformer era. ||| 31161 ||| 31162 ||| 
2022 ||| an attentive joint model with transformer-based weighted graph convolutional network for extracting adverse drug event relation. ||| 31163 ||| 31164 ||| 31165 ||| 31166 ||| 
2019 ||| chinese clinical named entity recognition with radical-level feature and self-attention mechanism. ||| 31167 ||| 31168 ||| 31169 ||| 521 ||| 
2021 ||| a deep attention model to forecast the length of stay and the in-hospital mortality right on admission from icd codes and demographic data. ||| 31170 ||| 31171 ||| 31172 ||| 
2019 ||| automatic icd code assignment of chinese clinical notes based on multilayer attention birnn. ||| 5253 ||| 12646 ||| 16854 ||| 31173 ||| 16687 ||| 1130 ||| 
2020 ||| s from texts with biobert and multiple entity-aware attentions. ||| 1107 ||| 16631 ||| 16633 ||| 20809 ||| 31174 ||| 
2021 ||| supervised line attention for tumor attribute classification from pathology reports: higher performance with less data. ||| 31175 ||| 31176 ||| 31177 ||| 31178 ||| 31179 ||| 2073 ||| 
2019 ||| knowledge-aware attention network for protein-protein interaction extraction. ||| 13576 ||| 1415 ||| 13577 ||| 13578 ||| 31180 ||| 31181 ||| 
2020 ||| multi-view self-attention for interpretable drug-target interaction prediction. ||| 31182 ||| 31183 ||| 31184 ||| 31185 ||| 31186 ||| 1207 ||| 
2021 ||| enhancing biomedical relation extraction with transformer models using shortest dependency path features and triplet information. ||| 15087 ||| 31187 ||| 
2021 ||| gla-net: a global-local attention network for automatic cataract classification. ||| 31188 ||| 17433 ||| 19593 ||| 31189 ||| 30194 ||| 254 ||| 144 ||| 
2020 ||| an attention-based multi-task model for named entity recognition and intent analysis of chinese online medical questions. ||| 31190 ||| 15247 ||| 31191 ||| 31192 ||| 31193 ||| 1372 ||| 
2020 ||| attention guided capsule networks for chemical-protein interaction extraction. ||| 31194 ||| 16590 ||| 3279 ||| 16591 ||| 8974 ||| 8349 ||| 
2021 ||| incorporating multi-level cnn and attention mechanism for chinese clinical named entity recognition. ||| 27439 ||| 31195 ||| 24565 ||| 20238 ||| 
2021 ||| dynamic deformable attention network (ddanet) for covid-19 lesions semantic segmentation. ||| 4050 ||| 31196 ||| 27351 ||| 
2021 ||| graph convolutional and attention models for entity classification in multilayer networks. ||| 31197 ||| 31198 ||| 31199 ||| 31200 ||| 
2020 ||| automated diagnosis of bone metastasis based on multi-view bone scans using attention-augmented deep neural networks. ||| 31201 ||| 31202 ||| 31203 ||| 11176 ||| 31204 ||| 31205 ||| 
2021 ||| cross-attention multi-branch network for fundus diseases classification using slo images. ||| 27716 ||| 27717 ||| 27715 ||| 31206 ||| 31207 ||| 15670 ||| 22436 ||| 5476 ||| 6582 ||| 
2021 ||| end-to-end prostate cancer detection in bpmri via 3d cnns: effects of attention mechanisms, clinical priori and decoupled false positive reduction. ||| 31208 ||| 31209 ||| 31210 ||| 
2020 ||| whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. ||| 27641 ||| 31211 ||| 27643 ||| 31212 ||| 1265 ||| 
2022 ||| global-local attention network with multi-task uncertainty loss for abnormal lymph node detection in mr images. ||| 11634 ||| 19768 ||| 31213 ||| 31214 ||| 31215 ||| 14910 ||| 21585 ||| 21587 ||| 14912 ||| 
2021 ||| deep pyramid local attention neural network for cardiac structure segmentation in two-dimensional echocardiography. ||| 523 ||| 11361 ||| 26764 ||| 7676 ||| 344 ||| 
2022 ||| resganet: residual group attention network for medical image classification and segmentation. ||| 29745 ||| 29079 ||| 29080 ||| 31216 ||| 31217 ||| 31218 ||| 31219 ||| 31220 ||| 31221 ||| 
2021 ||| a novel attention-guided convolutional network for the detection of abnormal cervical cells in cervical cancer screening. ||| 26456 ||| 31222 ||| 31223 ||| 31224 ||| 31225 ||| 31226 ||| 31227 ||| 479 ||| 31228 ||| 31229 ||| 9673 ||| 444 ||| 30496 ||| 31230 ||| 
2019 ||| learning to detect chest radiographs containing pulmonary lesions using visual attention networks. ||| 31231 ||| 31232 ||| 31233 ||| 31234 ||| 31235 ||| 31236 ||| 
2022 ||| spatio-temporal directed acyclic graph learning with attention mechanisms on brain functional time series and connectivity. ||| 31237 ||| 31238 ||| 31239 ||| 31240 ||| 
2022 ||| fat-net: feature adaptive transformers for automated skin lesion segmentation. ||| 22260 ||| 31241 ||| 31242 ||| 1160 ||| 6582 ||| 22263 ||| 
2019 ||| a collaborative computer aided diagnosis (c-cad) system with eye-tracking, sparse attentional model, and deep learning. ||| 19319 ||| 31243 ||| 27603 ||| 31244 ||| 27604 ||| 31245 ||| 
2020 ||| prediction of in-plane organ deformation during free-breathing radiotherapy via discriminative spatial transformer networks. ||| 31246 ||| 31247 ||| 31248 ||| 31249 ||| 31250 ||| 31251 ||| 1226 ||| 31252 ||| 31253 ||| 31254 ||| 31255 ||| 31256 ||| 
2022 ||| multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. ||| 31257 ||| 31258 ||| 31259 ||| 31260 ||| 31261 ||| 31262 ||| 31263 ||| 31264 ||| 2048 ||| 
2020 ||| spatio-temporal visual attention modelling of standard biometry plane-finding navigation. ||| 10019 ||| 10018 ||| 10020 ||| 10021 ||| 10022 ||| 10023 ||| 10024 ||| 
2020 ||| automatic ischemic stroke lesion segmentation from computed tomography perfusion images by image synthesis and attention-based deep neural networks. ||| 15623 ||| 20159 ||| 31265 ||| 31266 ||| 31267 ||| 15555 ||| 
2021 ||| asymmetric multi-task attention network for prostate bed segmentation in computed tomography images. ||| 27557 ||| 27480 ||| 11634 ||| 7757 ||| 27560 ||| 27558 ||| 27559 ||| 27372 ||| 18051 ||| 27561 ||| 
2020 ||| fully automatic brain tumor segmentation with deep learning-based selective attention using overlapping patches and multi-class weighted cross-entropy. ||| 31268 ||| 31269 ||| 31270 ||| 31271 ||| 
2022 ||| fully transformer network for skin lesion analysis. ||| 31272 ||| 31273 ||| 31274 ||| 31275 ||| 15542 ||| 6582 ||| 
2020 ||| semi-supervised wce image classification with adaptive aggregated attention. ||| 27657 ||| 25547 ||| 
2021 ||| anatomical-guided attention enhances unsupervised pet image denoising performance. ||| 25473 ||| 31276 ||| 31277 ||| 31278 ||| 31279 ||| 31280 ||| 31281 ||| 
2019 ||| attention gated networks: learning to leverage salient regions in medical images. ||| 31282 ||| 27946 ||| 27948 ||| 27351 ||| 27478 ||| 27844 ||| 27420 ||| 
2022 ||| awsnet: an auto-weighted supervision attention network for myocardial scar and edema segmentation in multi-sequence cardiac magnetic resonance images. ||| 31283 ||| 7676 ||| 31284 ||| 3034 ||| 8847 ||| 31285 ||| 20534 ||| 31286 ||| 20755 ||| 20535 ||| 
2021 ||| dual attention multiple instance learning with unsupervised complementary loss for covid-19 screening. ||| 25502 ||| 27884 ||| 31287 ||| 31288 ||| 31289 ||| 25503 ||| 
2022 ||| uncertainty-guided graph attention network for parapneumonic effusion diagnosis. ||| 31290 ||| 5206 ||| 31291 ||| 31292 ||| 31293 ||| 31294 ||| 6506 ||| 31295 ||| 31296 ||| 19676 ||| 7263 ||| 24408 ||| 
2021 ||| multi-site mri harmonization via attention-guided deep domain adaptation for brain disorder identification. ||| 27370 ||| 31297 ||| 27371 ||| 27372 ||| 18051 ||| 27373 ||| 
2022 ||| m3net: a multi-scale multi-view framework for multi-phase pancreas segmentation based on cross-phase non-local attention. ||| 31298 ||| 31299 ||| 15508 ||| 31300 ||| 1448 ||| 977 ||| 31301 ||| 27749 ||| 31302 ||| 1801 ||| 31303 ||| 
2021 ||| rigid and non-rigid motion artifact reduction in x-ray ct using attention module. ||| 31304 ||| 31305 ||| 31306 ||| 18694 ||| 
2022 ||| spine-transformers: vertebra labeling and segmentation in arbitrary field-of-view spine cts via 3d transformers. ||| 27441 ||| 31307 ||| 27442 ||| 
2021 ||| automated cardiac segmentation of cross-modal medical images using unsupervised multi-domain adaptation and spatial neural attention structure. ||| 31308 ||| 17677 ||| 31309 ||| 31310 ||| 31311 ||| 31312 ||| 31313 ||| 
2021 ||| triple attention learning for classification of 14 thoracic diseases using chest radiography. ||| 31091 ||| 15597 ||| 31314 ||| 10075 ||| 31315 ||| 9199 ||| 
2020 ||| self-co-attention neural network for anatomy segmentation in whole breast ultrasound. ||| 6582 ||| 15534 ||| 6799 ||| 21980 ||| 27338 ||| 15533 ||| 31206 ||| 24204 ||| 31035 ||| 15532 ||| 
2019 ||| abdominal multi-organ segmentation with organ-attention networks and statistical fusion. ||| 247 ||| 27607 ||| 8906 ||| 31316 ||| 31317 ||| 8660 ||| 
2022 ||| surginet: pyramid attention aggregation and class-wise self-distillation for surgical instrument segmentation. ||| 5183 ||| 5185 ||| 18290 ||| 31318 ||| 5189 ||| 5184 ||| 271 ||| 
2021 ||| multi-channel attention-fusion neural network for brain age estimation: accuracy, generality, and interpretation with 16, 705 healthy mris across lifespan. ||| 31319 ||| 31320 ||| 31321 ||| 31322 ||| 31323 ||| 31324 ||| 31325 ||| 31326 ||| 31327 ||| 27851 ||| 
2020 ||| attention convolutional neural network for accurate segmentation and quantification of lesions in ischemic stroke disease. ||| 16854 ||| 31328 ||| 16687 ||| 1130 ||| 
2021 ||| dual attention enhancement feature fusion network for segmentation and quantitative analysis of paediatric echocardiography. ||| 6575 ||| 6582 ||| 13808 ||| 31206 ||| 27943 ||| 8636 ||| 6574 ||| 31329 ||| 31330 ||| 5476 ||| 
2022 ||| prostattention-net: a deep attention model for prostate cancer segmentation by aggressiveness in mri scans. ||| 14858 ||| 31331 ||| 31332 ||| 7350 ||| 31333 ||| 14859 ||| 14860 ||| 
2020 ||| attentional reinforcement learning in the brain. ||| 31334 ||| 
2017 ||| erratum to: visualizing collective attention using association networks. ||| 31335 ||| 
2019 ||| development and application research of smart distribution district based on idtt-b new-type transformer terminal unit. ||| 31336 ||| 31337 ||| 31338 ||| 31339 ||| 18653 ||| 31340 ||| 5999 ||| 
2022 ||| improving wireless indoor non-intrusive load disaggregation using attention-based deep learning networks. ||| 1302 ||| 875 ||| 24050 ||| 31341 ||| 21206 ||| 31342 ||| 31343 ||| 
2021 ||| effcdnet: transfer learning with deep attention network for change detection in high spatial resolution satellite images. ||| 31344 ||| 31345 ||| 31346 ||| 
2021 ||| dual attention residual group networks for single image deraining. ||| 31347 ||| 31348 ||| 31349 ||| 31350 ||| 
2022 ||| the multi-level classification and regression network for visual tracking via residual channel attention. ||| 28711 ||| 31351 ||| 31352 ||| 28710 ||| 10812 ||| 
2021 ||| dual attention per-pixel filter network for spatially varying image deblurring. ||| 31353 ||| 5820 ||| 19928 ||| 31354 ||| 
2020 ||| cross-modal feature alignment based hybrid attentional generative adversarial networks for text-to-image synthesis. ||| 4260 ||| 737 ||| 
2022 ||| a novel multi-scale convolution model based on multi-dilation rates and multi-attention mechanism for mechanical fault diagnosis. ||| 31355 ||| 6166 ||| 31356 ||| 31357 ||| 8718 ||| 
2022 ||| pedestrian detection algorithm based on multi-scale feature extraction and attention feature fusion. ||| 31358 ||| 9547 ||| 31359 ||| 31360 ||| 31361 ||| 
2022 ||| multi-scale residual attention network for single image dehazing. ||| 31362 ||| 31363 ||| 31364 ||| 31365 ||| 31366 ||| 
2021 ||| structural pixel-wise target attention for robust object tracking. ||| 28710 ||| 31367 ||| 18083 ||| 31368 ||| 31369 ||| 28711 ||| 
2021 ||| boundary-aware pyramid attention network for detecting salient objects in rgb-d images. ||| 31370 ||| 31371 ||| 31372 ||| 17120 ||| 31373 ||| 31374 ||| 
2021 ||| spatial-temporal attention network for multistep-ahead forecasting of chlorophyll. ||| 31375 ||| 31376 ||| 31377 ||| 15189 ||| 10140 ||| 
2021 ||| infrared image super-resolution reconstruction by using generative adversarial network with an attention mechanism. ||| 31378 ||| 31379 ||| 31380 ||| 31381 ||| 31382 ||| 31383 ||| 
2022 ||| triplet attention multiple spacetime-semantic graph convolutional network for skeleton-based action recognition. ||| 31384 ||| 31385 ||| 31386 ||| 10875 ||| 31387 ||| 
2020 ||| multi-branch cross attention model for prediction of kras mutation in rectal cancer with t2-weighted mri. ||| 9694 ||| 31388 ||| 31389 ||| 31390 ||| 31391 ||| 4059 ||| 31392 ||| 21897 ||| 31393 ||| 
2020 ||| two-dimensional discrete feature based spatial attention capsnet for semg signal recognition. ||| 31394 ||| 31395 ||| 369 ||| 12819 ||| 31396 ||| 31397 ||| 
2021 ||| crowd counting method based on the self-attention residual network. ||| 31380 ||| 31379 ||| 31378 ||| 31398 ||| 31383 ||| 
2022 ||| self attention mechanism of bidirectional information enhancement. ||| 31399 ||| 31400 ||| 4095 ||| 31401 ||| 
2018 ||| recurrent networks with attention and convolutional networks for sentence representation and classification. ||| 618 ||| 24792 ||| 24790 ||| 24793 ||| 
2022 ||| multi-stage attention and center triplet loss for person re-identication. ||| 15304 ||| 31402 ||| 31403 ||| 
2022 ||| a temporal attention based appearance model for video object segmentation. ||| 1341 ||| 31404 ||| 31405 ||| 
2021 ||| residual attention network using multi-channel dense connections for image super-resolution. ||| 1428 ||| 31406 ||| 31407 ||| 31408 ||| 31409 ||| 
2022 ||| adaptive spatial-temporal graph attention networks for traffic flow forecasting. ||| 31410 ||| 12196 ||| 31411 ||| 31405 ||| 3131 ||| 
2021 ||| collaborative attention neural network for multi-domain sentiment classification. ||| 31412 ||| 15294 ||| 15293 ||| 31413 ||| 
2022 ||| egsanet: edge-guided sparse attention network for improving license plate detection in the wild. ||| 31414 ||| 31415 ||| 247 ||| 31416 ||| 
2021 ||| subtler mixed attention network on fine-grained image classification. ||| 859 ||| 5278 ||| 31417 ||| 13750 ||| 
2021 ||| an efficient attention module for 3d convolutional neural networks in action recognition. ||| 31418 ||| 5061 ||| 30831 ||| 31419 ||| 
2022 ||| self-attention enhanced cnns with average margin loss for chinese zero pronoun resolution. ||| 31420 ||| 
2017 ||| improving human-robot interaction based on joint attention. ||| 31421 ||| 31422 ||| 2101 ||| 31423 ||| 31424 ||| 31425 ||| 31426 ||| 31427 ||| 
2021 ||| mixed attention dense network for sketch classification. ||| 8952 ||| 31428 ||| 31429 ||| 31430 ||| 18731 ||| 
2021 ||| vehicle theft recognition from surveillance video based on spatiotemporal attention. ||| 23028 ||| 31431 ||| 4719 ||| 2755 ||| 
2021 ||| attention-based vgg-16 model for covid-19 chest x-ray image classification. ||| 31432 ||| 31433 ||| 
2022 ||| mc-net: multi-scale context-attention network for medical ct image segmentation. ||| 31434 ||| 31435 ||| 17585 ||| 31436 ||| 
2021 ||| unsupervised medical images denoising via graph attention dual adversarial network. ||| 16856 ||| 13195 ||| 16756 ||| 16757 ||| 
2021 ||| multi-attention based semantic deep hashing for cross-modal retrieval. ||| 12214 ||| 31437 ||| 31438 ||| 9673 ||| 31439 ||| 12213 ||| 
2021 ||| structural attention network for graph. ||| 31440 ||| 31441 ||| 
2020 ||| elective future: the influence factor mining of students' graduation development based on hierarchical attention neural network model with graph. ||| 9479 ||| 9480 ||| 20871 ||| 255 ||| 9475 ||| 
2022 ||| image super-resolution via channel attention and spatial attention. ||| 31442 ||| 31443 ||| 
2021 ||| memory network with hierarchical multi-head attention for aspect-based sentiment analysis. ||| 2957 ||| 31444 ||| 2954 ||| 
2022 ||| efficient residual attention network for single image super-resolution. ||| 31445 ||| 11314 ||| 31446 ||| 13713 ||| 
2021 ||| attention augmented multi-scale network for single image super-resolution. ||| 31447 ||| 31448 ||| 31449 ||| 8534 ||| 
2022 ||| convolutional neural network based on attention mechanism and bi-lstm for bearing remaining life prediction. ||| 31450 ||| 181 ||| 
2021 ||| correction to: non-parallel text style transfer with domain adaptation and an attention model. ||| 31451 ||| 15144 ||| 
2022 ||| an attention network via pronunciation, lexicon and syntax for humor recognition. ||| 31452 ||| 728 ||| 8974 ||| 31453 ||| 8967 ||| 
2022 ||| neural tv program recommendation with label and user dual attention. ||| 31454 ||| 31455 ||| 31456 ||| 22589 ||| 
2021 ||| exploiting multi-attention network with contextual influence for point-of-interest recommendation. ||| 10800 ||| 5110 ||| 31457 ||| 10801 ||| 10799 ||| 
2022 ||| focus on temporal graph convolutional networks with unified attention for skeleton-based action recognition. ||| 31458 ||| 31459 ||| 31460 ||| 31461 ||| 
2021 ||| co-attention fusion based deep neural network for chinese medical answer selection. ||| 31462 ||| 30821 ||| 31463 ||| 31464 ||| 31465 ||| 
2022 ||| knowledge-aware recommendation model with dynamic co-attention and attribute regularize. ||| 31466 ||| 30218 ||| 30217 ||| 31467 ||| 
2021 ||| modeling low- and high-order feature interactions with fm and self-attention network. ||| 5878 ||| 31468 ||| 5877 ||| 28954 ||| 
2021 ||| link traffic speed forecasting using convolutional attention-based gated recurrent unit. ||| 23655 ||| 23658 ||| 23659 ||| 
2022 ||| piecewise convolutional neural networks with position attention and similar bag attention for distant supervision relation extraction. ||| 31469 ||| 13930 ||| 31470 ||| 2950 ||| 
2022 ||| a multi-focus image fusion method based on attention mechanism and supervised learning. ||| 31471 ||| 31472 ||| 31473 ||| 
2020 ||| hierarchical graph attention networks for semi-supervised node classification. ||| 31474 ||| 31475 ||| 31476 ||| 31477 ||| 
2021 ||| spatio-temporal attention on manifold space for 3d human action recognition. ||| 31478 ||| 19109 ||| 31479 ||| 31480 ||| 
2021 ||| an enhanced siamese angular softmax network with dual joint-attention for person re-identification. ||| 29643 ||| 31481 ||| 31482 ||| 31483 ||| 30939 ||| 
2020 ||| deep spatial-temporal networks for crowd flows prediction by dilated convolutions and region-shifting attention mechanism. ||| 31484 ||| 1150 ||| 31485 ||| 24048 ||| 
2022 ||| rman: relational multi-head attention neural network for joint extraction of entities and relations. ||| 31486 ||| 5479 ||| 31487 ||| 31488 ||| 13773 ||| 
2022 ||| a novel deep pixel restoration video prediction algorithm integrating attention mechanism. ||| 31489 ||| 31490 ||| 
2022 ||| pyramid-attention based multi-scale feature fusion network for multispectral pan-sharpening. ||| 31491 ||| 31473 ||| 31472 ||| 
2022 ||| deca: a novel multi-scale efficient channel attention module for object detection in real-life fire images. ||| 26760 ||| 28806 ||| 31492 ||| 
2021 ||| non-parallel text style transfer with domain adaptation and an attention model. ||| 31451 ||| 15144 ||| 
2022 ||| residual deep attention mechanism and adaptive reconstruction network for single image super-resolution. ||| 31493 ||| 31494 ||| 31495 ||| 31496 ||| 31497 ||| 
2021 ||| saliency prediction on omnidirectional images with attention-aware feature fusion network. ||| 19902 ||| 19903 ||| 19905 ||| 19907 ||| 8532 ||| 
2021 ||| neural attention model for recommendation based on factorization machines. ||| 31498 ||| 31499 ||| 31500 ||| 31501 ||| 31502 ||| 
2021 ||| sta-net: spatial-temporal attention network for video salient object detection. ||| 31460 ||| 3399 ||| 31503 ||| 13538 ||| 31504 ||| 
2022 ||| joint pyramid attention network for real-time semantic segmentation of urban scenes. ||| 30139 ||| 31505 ||| 31506 ||| 
2022 ||| a multi-task dual attention deep recommendation model using ratings and review helpfulness. ||| 5107 ||| 31507 ||| 31508 ||| 
2022 ||| dynamic-boosting attention for self-supervised video representation learning. ||| 8219 ||| 31509 ||| 31510 ||| 31511 ||| 
2022 ||| civil airline fare prediction with a multi-attribute dual-stage attention mechanism. ||| 31512 ||| 31513 ||| 31514 ||| 31515 ||| 31516 ||| 
2020 ||| a ctr prediction model based on user interest via attention mechanism. ||| 1705 ||| 31517 ||| 31518 ||| 31519 ||| 3906 ||| 
2021 ||| image super-resolution reconstruction based on feature map attention mechanism. ||| 31520 ||| 31521 ||| 31522 ||| 31523 ||| 31524 ||| 31525 ||| 2251 ||| 5492 ||| 
2020 ||| gama: graph attention multi-agent reinforcement learning algorithm for cooperation. ||| 31526 ||| 31527 ||| 30675 ||| 31528 ||| 1251 ||| 
2021 ||| attributed network representation learning via improved graph attention with robust negative sampling. ||| 31529 ||| 31530 ||| 31531 ||| 31532 ||| 
2018 ||| a human-like visual-attention-based artificial vision system for wildland firefighting assistance. ||| 31533 ||| 31534 ||| 31535 ||| 2253 ||| 31536 ||| 31537 ||| 31538 ||| 
2020 ||| end-to-end multitask siamese network with residual hierarchical attention for real-time object tracking. ||| 31539 ||| 31540 ||| 9442 ||| 31541 ||| 
2021 ||| sat-net: a side attention network for retinal image segmentation. ||| 31542 ||| 30831 ||| 31543 ||| 31544 ||| 30832 ||| 
2022 ||| reference-guided deep deblurring via a selective attention network. ||| 20152 ||| 5298 ||| 5300 ||| 
2019 ||| hierarchical attention based long short-term memory for chinese lyric generation. ||| 610 ||| 31545 ||| 18548 ||| 31546 ||| 
2022 ||| sf-ann: leveraging structural features with an attention neural network for candidate fact ranking. ||| 31401 ||| 21134 ||| 31547 ||| 18575 ||| 31548 ||| 31549 ||| 
2021 ||| cropping and attention based approach for masked face recognition. ||| 31550 ||| 2954 ||| 31551 ||| 2014 ||| 
2022 ||| reinforcement-learning-guided source code summarization using hierarchical attention. ||| 31552 ||| 31553 ||| 3893 ||| 3891 ||| 1306 ||| 1236 ||| 1094 ||| 3894 ||| 
2020 ||| an end-to-end framework for biomedical event trigger identification with hierarchical attention and adaptive cost learning. ||| 16887 ||| 16888 ||| 16889 ||| 16890 ||| 16891 ||| 16892 ||| 16893 ||| 
2020 ||| drug target interaction prediction via multi-task co-attention. ||| 16807 ||| 31554 ||| 4175 ||| 2389 ||| 16809 ||| 
2018 ||| pcb-planar transformers equivalent circuit model identification using genetic algorithm. ||| 31555 ||| 31556 ||| 31557 ||| 
2022 ||| identification of encrypted traffic through attention mechanism based long short term memory. ||| 31558 ||| 31559 ||| 31560 ||| 31561 ||| 31562 ||| 31563 ||| 
2021 ||| universal transformer hawkes process with adaptive recursive iteration. ||| 414 ||| 415 ||| 416 ||| 417 ||| 
2020 ||| an attention long short-term memory based system for automatic classification of speech intelligibility. ||| 31564 ||| 31565 ||| 16464 ||| 14389 ||| 14390 ||| 3882 ||| 
2021 ||| a novel dual attention-based blstm with hybrid features in speech emotion recognition. ||| 31566 ||| 31567 ||| 
2020 ||| purifying real images with an attention-guided style transfer network for gaze estimation. ||| 20653 ||| 31568 ||| 17129 ||| 31569 ||| 31570 ||| 
2021 ||| transformer based network for open information extraction. ||| 31571 ||| 31572 ||| 
2022 ||| sequential transformer via an outside-in attention for image captioning. ||| 18634 ||| 18635 ||| 31573 ||| 31574 ||| 
2021 ||| computer vision detection of foreign objects in coal processing using attention cnn. ||| 31575 ||| 31576 ||| 31577 ||| 31578 ||| 326 ||| 
2021 ||| attention-based learning of self-media data for marketing intention detection. ||| 31579 ||| 10445 ||| 31580 ||| 31581 ||| 31582 ||| 31583 ||| 31584 ||| 
2021 ||| matching images and texts with multi-head attention network for cross-media hashing retrieval. ||| 264 ||| 31585 ||| 20357 ||| 262 ||| 31586 ||| 
2021 ||| ptanet: triple attention network for point cloud semantic segmentation. ||| 31587 ||| 12720 ||| 31588 ||| 683 ||| 31589 ||| 
2020 ||| automatic epileptic eeg classification based on differential entropy and attention model. ||| 12196 ||| 31590 ||| 31591 ||| 5033 ||| 
2021 ||| transformers-based information extraction with limited data for domain-specific business documents. ||| 7521 ||| 16000 ||| 16004 ||| 
2017 ||| the impact of a model-based clinical regional registry for attention-deficit hyperactivity disorder. ||| 31592 ||| 31593 ||| 31594 ||| 31595 ||| 31596 ||| 31597 ||| 31598 ||| 
2020 ||| predicting substance use disorder using long-term attention deficit hyperactivity disorder medication records in truven. ||| 31599 ||| 31600 ||| 31601 ||| 13706 ||| 31602 ||| 31603 ||| 31604 ||| 31605 ||| 31606 ||| 31607 ||| 31608 ||| 
2019 ||| perceptual presence: an attentional account. ||| 31609 ||| 
2021 ||| joint attention and perceptual experience. ||| 31610 ||| 31611 ||| 
2017 ||| perceptual content is indexed to attention. ||| 31612 ||| 
2021 ||| the transparency of experience and the neuroscience of attention. ||| 31613 ||| 31614 ||| 31615 ||| 
2021 ||| joint attention to mental content and the social origin of reasoning. ||| 31616 ||| 31617 ||| 
2022 ||| multi-scale graph capsule with influence attention for information cascades prediction. ||| 31618 ||| 9014 ||| 9010 ||| 31619 ||| 
2022 ||| dadcnet: dual attention densely connected network for more accurate real iris region segmentation. ||| 3503 ||| 31620 ||| 31621 ||| 23019 ||| 
2021 ||| an attention-based category-aware gru model for the next poi recommendation. ||| 10190 ||| 31622 ||| 1704 ||| 10191 ||| 31623 ||| 1371 ||| 31624 ||| 31625 ||| 25874 ||| 
2021 ||| a dual-stage attention-based conv-lstm network for spatio-temporal correlation and multivariate time series prediction. ||| 31626 ||| 31627 ||| 31628 ||| 17730 ||| 7430 ||| 20336 ||| 
2021 ||| an effective framework for semistructured document classification via hierarchical attention model. ||| 16889 ||| 16888 ||| 16887 ||| 4400 ||| 6278 ||| 16892 ||| 16710 ||| 16893 ||| 
2021 ||| topology and channel affinity reinforced global attention for person re-identification. ||| 31629 ||| 31630 ||| 31631 ||| 31632 ||| 31633 ||| 
2019 ||| an experimental study of the attention-based view of idea integration: the need for a multi-level dependent variable. ||| 31634 ||| 31635 ||| 
2020 ||| knowledge graph completion for the chinese text of cultural relics based on bidirectional encoder representations from transformers with entity-type information. ||| 1254 ||| 30494 ||| 31636 ||| 31637 ||| 
2021 ||| hierarchical classification of event-related potentials for the recognition of gender differences in the attention task. ||| 31638 ||| 31639 ||| 
2019 ||| convolutional recurrent neural networks with a self-attention mechanism for personnel performance prediction. ||| 31640 ||| 4287 ||| 5194 ||| 6736 ||| 19548 ||| 31641 ||| 31642 ||| 31643 ||| 
2021 ||| a lightweight yolov4-based forestry pest detection method using coordinate attention and feature fusion. ||| 31644 ||| 31645 ||| 31646 ||| 17475 ||| 
2021 ||| a novel feature extraction method for power transformer vibration signal based on ceemdan and multi-scale dispersion entropy. ||| 31647 ||| 31648 ||| 31649 ||| 19464 ||| 31650 ||| 
2021 ||| personal interest attention graph neural networks for session-based recommendation. ||| 8622 ||| 31651 ||| 22980 ||| 31652 ||| 
2019 ||| attention to the variation of probabilistic events: information processing with message importance measure. ||| 31653 ||| 31654 ||| 31655 ||| 
2021 ||| attention-based fault-tolerant approach for multi-agent reinforcement learning systems. ||| 31656 ||| 31657 ||| 588 ||| 
2019 ||| quantum information remote carnot engines and voltage transformers. ||| 31658 ||| 31659 ||| 
2021 ||| diabetic retinal grading using attention-based bilinear convolutional neural network and complement cross entropy. ||| 31660 ||| 8532 ||| 31661 ||| 31662 ||| 
2019 ||| joint deep model with multi-level attention and hybrid-prediction for recommendation. ||| 5081 ||| 5084 ||| 5082 ||| 
2021 ||| benchmarking attention-based interpretability of deep learning in multivariate time series predictions. ||| 31663 ||| 31664 ||| 31665 ||| 31666 ||| 
2019 ||| supervisors' visual attention allocation modeling using hybrid entropy. ||| 31667 ||| 31668 ||| 31669 ||| 5845 ||| 
2021 ||| interpretable multi-head self-attention architecture for sarcasm detection in social media. ||| 31670 ||| 31671 ||| 
2019 ||| learning to cooperate via an attention-based communication neural network in decentralized multi-robot exploration. ||| 31657 ||| 362 ||| 31672 ||| 363 ||| 19547 ||| 241 ||| 
2022 ||| mfan: multi-level features attention network for fake certificate image detection. ||| 673 ||| 6656 ||| 4400 ||| 
2021 ||| an information gain-based model and an attention-based rnn for wearable human activity recognition. ||| 31673 ||| 31674 ||| 31675 ||| 31676 ||| 31677 ||| 8875 ||| 
2022 ||| regularity normalization: neuroscience-inspired unsupervised attention across neural network layers. ||| 31678 ||| 
2021 ||| s2a: scale-attention-aware networks for video super-resolution. ||| 31679 ||| 7895 ||| 13453 ||| 19716 ||| 7897 ||| 
2021 ||| learning numerosity representations with transformers: number generation tasks and out-of-distribution generalization. ||| 31680 ||| 31681 ||| 31682 ||| 
2020 ||| optic disc segmentation using attention-based u-net and the improved cross-entropy convolutional neural network. ||| 31661 ||| 31660 ||| 5845 ||| 31683 ||| 764 ||| 
2020 ||| separated channel attention convolutional neural network (sc-cnn-attention) to identify adhd in multi-site rs-fmri dataset. ||| 6514 ||| 31684 ||| 19773 ||| 31685 ||| 31686 ||| 31687 ||| 31688 ||| 31689 ||| 31690 ||| 31691 ||| 3676 ||| 
2021 ||| multi-level fusion temporal-spatial co-attention for video-based person re-identification. ||| 31692 ||| 31693 ||| 
2018 ||| identification of auditory object-specific attention from single-trial electroencephalogram signals via entropy measures and machine learning. ||| 28976 ||| 28977 ||| 14551 ||| 31694 ||| 
2021 ||| part-aware mask-guided attention for thorax disease classification. ||| 31695 ||| 1856 ||| 11359 ||| 31696 ||| 31697 ||| 778 ||| 
2021 ||| a transformer-based hierarchical variational autoencoder combined hidden markov model for long text generation. ||| 692 ||| 31698 ||| 3993 ||| 31699 ||| 
2021 ||| attention mechanisms and their applications to complex systems. ||| 11927 ||| 21533 ||| 10314 ||| 852 ||| 31700 ||| 
2021 ||| a pi+passivity-based control of a wind energy conversion system enabled with a solid-state transformer. ||| 31701 ||| 19504 ||| 31702 ||| 31703 ||| 
2019 ||| consumer choice under limited attention when alternatives have different information costs. ||| 31704 ||| 31705 ||| 31706 ||| 31707 ||| 31708 ||| 
2017 ||| smart testing environment for the evaluation of students' attention. ||| 8502 ||| 16240 ||| 31709 ||| 31710 ||| 31711 ||| 
2020 ||| multi-level context extraction and attention-based contextual inter-modal fusion for multimodal sentiment analysis and emotion classification. ||| 31712 ||| 31713 ||| 31714 ||| 
2020 ||| a semisupervised recurrent convolutional attention model for human activity recognition. ||| 770 ||| 771 ||| 773 ||| 772 ||| 1781 ||| 31715 ||| 
2021 ||| global and local knowledge-aware attention network for action recognition. ||| 31716 ||| 17561 ||| 31717 ||| 17562 ||| 
2019 ||| attention inspiring receptive-fields network for learning invariant representations. ||| 12169 ||| 31718 ||| 31719 ||| 31720 ||| 
2021 ||| casnet: a cross-attention siamese network for video salient object detection. ||| 31721 ||| 31722 ||| 31723 ||| 19727 ||| 28491 ||| 
2022 ||| multitask attention network for lane detection and fitting. ||| 6627 ||| 20105 ||| 2256 ||| 31724 ||| 6922 ||| 
2021 ||| deep coattention-based comparator for relative representation learning in person re-identification. ||| 17580 ||| 602 ||| 619 ||| 444 ||| 8710 ||| 1756 ||| 
2020 ||| neural machine translation with gru-gated attention model. ||| 3180 ||| 3181 ||| 11745 ||| 3182 ||| 
2017 ||| quantized attention-gated kernel reinforcement learning for brain-machine interface decoding. ||| 10836 ||| 31725 ||| 5723 ||| 31726 ||| 31727 ||| 31728 ||| 16952 ||| 31729 ||| 852 ||| 853 ||| 854 ||| 
2022 |||  clnn: spatial, spectral and multiscale attention convlstm neural network for multisource remote sensing data classification. ||| 31730 ||| 31731 ||| 3337 ||| 5536 ||| 6720 ||| 6750 ||| 
2021 ||| automated social text annotation with joint multilabel attention networks. ||| 4775 ||| 1160 ||| 4776 ||| 4777 ||| 
2021 ||| scene segmentation with dual relation-aware attention network. ||| 11391 ||| 2058 ||| 11390 ||| 2969 ||| 1171 ||| 2080 ||| 
2021 ||| neighborhood attention networks with adversarial learning for link prediction. ||| 1176 ||| 1379 ||| 1178 ||| 
2021 ||| dual attention-based encoder-decoder: a customized sequence-to-sequence learning for soft sensor development. ||| 31732 ||| 27168 ||| 31733 ||| 
2020 ||| pay attention to them: deep reinforcement learning-based cascade object detection. ||| 31734 ||| 5704 ||| 5705 ||| 
2020 ||| cross-modal attention with semantic consistence for image-text matching. ||| 9579 ||| 2481 ||| 11466 ||| 31735 ||| 1038 ||| 1040 ||| 
2020 ||| robust deep co-saliency detection with group semantic and pyramid attention. ||| 8710 ||| 31736 ||| 8709 ||| 18071 ||| 17860 ||| 
2019 ||| temporal attention-augmented bilinear network for financial time-series data analysis. ||| 31737 ||| 926 ||| 31738 ||| 31739 ||| 
2021 ||| aanet: adaptive attention network for covid-19 detection from chest x-ray images. ||| 19440 ||| 28406 ||| 31740 ||| 8012 ||| 31741 ||| 31742 ||| 31743 ||| 
2021 ||| adversarial learning with multi-modal attention for visual question answering. ||| 4477 ||| 5320 ||| 19722 ||| 31744 ||| 843 ||| 
2020 ||| gramme: semisupervised learning using multilayered graph attention models. ||| 12004 ||| 12005 ||| 14421 ||| 12006 ||| 
2021 ||| attention-based road registration for gps-denied uas navigation. ||| 30559 ||| 13833 ||| 6473 ||| 31745 ||| 31746 ||| 
2021 ||| attention in natural language processing. ||| 31747 ||| 31748 ||| 31749 ||| 
2020 ||| an efficient group recommendation model with multiattention-based neural networks. ||| 8979 ||| 5858 ||| 31750 ||| 24629 ||| 
2020 ||| redundancy and attention in convolutional lstm for gesture recognition. ||| 9351 ||| 1166 ||| 12169 ||| 9352 ||| 9354 ||| 5328 ||| 9353 ||| 
2019 ||| development of a multi-user system to identify the level of attention in people. ||| 10375 ||| 10376 ||| 8048 ||| 10378 ||| 
2020 ||| frequency of mind-wandering in a sustained attention to response task: a cognitive model of distraction. ||| 31751 ||| 
2020 ||| extraction of body posture characteristics as a correlation variable with the level of attention. ||| 10375 ||| 10376 ||| 8048 ||| 10378 ||| 
2017 ||| bubbleview: an interface for crowdsourcing image importance maps and tracking visual attention. ||| 15340 ||| 8781 ||| 31752 ||| 31753 ||| 2372 ||| 15325 ||| 31754 ||| 1735 ||| 
2019 ||| attention regulation framework: designing self-regulated mindfulness technologies. ||| 15364 ||| 15365 ||| 15367 ||| 15368 ||| 
2017 ||| gaze-contingent auditory displays for improved spatial attention in virtual reality. ||| 31755 ||| 31756 ||| 31757 ||| 
2021 ||| a new perspective on online malicious comments: effects of attention and neutralization. ||| 22324 ||| 22325 ||| 31758 ||| 
2019 ||| transfer hierarchical attention network for generative dialog system. ||| 586 ||| 11145 ||| 
2018 ||| a selective attention guided initiative semantic cognition algorithm for service robot. ||| 31759 ||| 31760 ||| 31761 ||| 
2021 ||| fault classification for on-board equipment of high-speed railway based on attention capsule network. ||| 31762 ||| 31763 ||| 31764 ||| 
2021 ||| encoding-decoding network with pyramid self-attention module for retinal vessel segmentation. ||| 31765 ||| 3879 ||| 4550 ||| 31766 ||| 6030 ||| 
2021 ||| correction to: transfer hierarchical attention network for generative dialog system. ||| 586 ||| 11145 ||| 
2021 ||| attention-based deep recurrent model for survival prediction. ||| 31767 ||| 8305 ||| 23430 ||| 31158 ||| 31160 ||| 
2021 ||| attention-gated graph convolutions for extracting drug interaction information from drug labels. ||| 31768 ||| 31162 ||| 31769 ||| 
2022 ||| attention-based unsupervised keyphrase extraction and phrase graph for covid-19 medical literature retrieval. ||| 26739 ||| 26740 ||| 
2018 ||| intelligent virtual security system using attention mechanism. ||| 31770 ||| 31771 ||| 31772 ||| 31773 ||| 31774 ||| 31775 ||| 31776 ||| 
2022 ||| hybrid neural network model based on multi-head attention for english text emotion analysis. ||| 977 ||| 
2022 ||| a spatio-temporal attention fusion model for students behaviour recognition. ||| 31777 ||| 
2020 ||| classification of fake news by fine-tuning deep bidirectional transformers based language model. ||| 31778 ||| 31779 ||| 31780 ||| 31781 ||| 31782 ||| 
2019 ||| understanding and improving deep learning-based rolling bearing fault diagnosis with attention mechanism. ||| 2008 ||| 781 ||| 31783 ||| 
2018 ||| recurrent attention network using spatial-temporal relations for action recognition. ||| 18990 ||| 11466 ||| 18992 ||| 13410 ||| 1038 ||| 
2020 ||| rafnet: recurrent attention fusion network of hyperspectral and multispectral images. ||| 31784 ||| 9283 ||| 31785 ||| 31786 ||| 
2019 ||| target-aware recurrent attentional network for radar hrrp target recognition. ||| 92 ||| 9283 ||| 31787 ||| 31788 ||| 31789 ||| 
2021 ||| multi-semantic crf-based attention model for image forgery detection and localization. ||| 3265 ||| 31790 ||| 31791 ||| 
2020 ||| image captioning via hierarchical attention mechanism and policy gradient optimization. ||| 13 ||| 12120 ||| 31792 ||| 11226 ||| 11227 ||| 11228 ||| 
2021 ||| region-factorized recurrent attentional network with deep clustering for radar hrrp target recognition. ||| 31793 ||| 26660 ||| 9283 ||| 241 ||| 31794 ||| 31788 ||| 
2019 ||| monitoring driver attention distraction with binocular vision. ||| 31795 ||| 31796 ||| 
2018 ||| effect of visual attention guidance by camera work in visualization using dome display. ||| 31797 ||| 31798 ||| 
2017 ||| look before you authorize: using eye-tracking to enforce user attention towards application permissions. ||| 26145 ||| 26147 ||| 
2019 ||| fault prediction of a transformer bushing based on entropy weight topsis and gray theory. ||| 31799 ||| 
2020 ||| super resolution with kernel estimation and dual attention mechanism. ||| 31800 ||| 31801 ||| 2355 ||| 31802 ||| 25428 ||| 
2020 ||| attention-based seriesnet: an attention-based hybrid neural network model for conditional time series forecasting. ||| 31803 ||| 31804 ||| 10154 ||| 
2020 ||| emotion-semantic-enhanced bidirectional lstm with multi-head attention mechanism for microblog sentiment analysis. ||| 31805 ||| 9090 ||| 30923 ||| 455 ||| 30628 ||| 
2019 ||| interactional and informational attention on twitter. ||| 31806 ||| 5335 ||| 31807 ||| 31808 ||| 
2021 ||| improving amharic speech recognition system using connectionist temporal classification with attention model and phoneme-based byte-pair-encodings. ||| 31809 ||| 12692 ||| 12688 ||| 31810 ||| 31811 ||| 
2021 ||| combating fake news with transformers: a comparative analysis of stance detection and subjectivity analysis. ||| 29348 ||| 29351 ||| 29353 ||| 
2020 ||| semantic enhanced distantly supervised relation extraction via graph attention network. ||| 31812 ||| 31813 ||| 14797 ||| 
2021 ||| attention paid to privacy policy statements. ||| 20220 ||| 31814 ||| 
2020 ||| attentional colorization networks with adaptive group-instance normalization. ||| 31802 ||| 31801 ||| 2355 ||| 31800 ||| 
2018 ||| chinese knowledge base question answering by attention-based multi-granularity model. ||| 31815 ||| 31816 ||| 13230 ||| 4398 ||| 12746 ||| 
2022 ||| dual co-attention-based multi-feature fusion method for rumor detection. ||| 31817 ||| 21133 ||| 31818 ||| 31819 ||| 31820 ||| 31821 ||| 
2020 ||| outpatient text classification using attention-based bidirectional lstm for robot-assisted servicing in hospital. ||| 31822 ||| 31823 ||| 31824 ||| 31825 ||| 
2021 ||| canet: a combined attention network for remote sensing image change detection. ||| 3399 ||| 4719 ||| 12637 ||| 12639 ||| 12638 ||| 
2020 ||| crowd counting guided by attention network. ||| 31826 ||| 31827 ||| 31828 ||| 28290 ||| 15072 ||| 
2020 ||| an attention-based model using character composition of entities in chinese relation extraction. ||| 31829 ||| 3289 ||| 19850 ||| 31816 ||| 
2020 ||| adversarial hard attention adaptation. ||| 31830 ||| 532 ||| 31831 ||| 241 ||| 
2019 ||| attention and signal detection. ||| 31832 ||| 
2020 ||| cwpc_biatt: character-word-position combined bilstm-attention for chinese named entity recognition. ||| 31833 ||| 31834 ||| 31835 ||| 
2020 ||| information needs and visual attention during urban, highly automated driving - an investigation of potential influencing factors. ||| 31836 ||| 31837 ||| 31838 ||| 26021 ||| 
2021 ||| financial volatility forecasting: a sparse multi-head attention neural network. ||| 31839 ||| 31840 ||| 
2020 ||| triadic automata and machines as information transformers. ||| 31841 ||| 
2020 ||| a novel method for twitter sentiment analysis based on attentional-graph neural network. ||| 31842 ||| 31843 ||| 
2021 ||| feature extraction network with attention mechanism for data enhancement and recombination fusion for multimodal sentiment analysis. ||| 31844 ||| 31845 ||| 3248 ||| 
2021 ||| deep hash with improved dual attention for image retrieval. ||| 4552 ||| 4719 ||| 12637 ||| 12639 ||| 12638 ||| 
2020 ||| vehicle pedestrian detection method based on spatial pyramid pooling and attention mechanism. ||| 31846 ||| 31847 ||| 3675 ||| 31848 ||| 
2022 ||| a bidirectional context embedding transformer for automatic speech recognition. ||| 31849 ||| 31850 ||| 12069 ||| 31851 ||| 11973 ||| 31852 ||| 31853 ||| 
2021 ||| does salience of neighbor-comparison information attract attention and conserve energy? eye-tracking experiment and interview with korean local apartment residents. ||| 31854 ||| 
2020 ||| antonyms: a computer game to improve inhibitory control of impulsivity in children with attention deficit/hyperactivity disorder (adhd). ||| 31855 ||| 31856 ||| 31857 ||| 31858 ||| 31859 ||| 31860 ||| 31861 ||| 
2022 ||| object detection of road assets using transformer-based yolox with feature pyramid decoder on thai highway panorama. ||| 30160 ||| 31862 ||| 31863 ||| 31864 ||| 31865 ||| 31866 ||| 
2020 ||| multilingual transformer-based personality traits estimation. ||| 31867 ||| 31868 ||| 31869 ||| 31870 ||| 
2019 ||| attention-based joint entity linking with entity embedding. ||| 4811 ||| 4398 ||| 10525 ||| 31871 ||| 
2021 ||| cyberbullying detection in social networks using bi-gru with self-attention mechanism. ||| 31872 ||| 31873 ||| 22078 ||| 31874 ||| 
2021 ||| short-term load forecasting based on the transformer model. ||| 31875 ||| 31876 ||| 31877 ||| 31878 ||| 3337 ||| 9057 ||| 31879 ||| 
2020 ||| spatiotemporal convolutional neural network with convolutional block attention module for micro-expression recognition. ||| 2384 ||| 31880 ||| 2411 ||| 31881 ||| 20231 ||| 5664 ||| 
2021 ||| a transformer-based framework for neutralizing and reversing the political polarity of news articles. ||| 31882 ||| 31883 ||| 3436 ||| 
2017 ||| credibility and the dynamics of collective attention. ||| 31884 ||| 31885 ||| 31886 ||| 
2020 ||| gestatten: estimation of user's attention in mobile moocs from eye gaze and gaze gesture tracking. ||| 31887 ||| 31888 ||| 31889 ||| 
2020 ||| looking for a deal?: visual social attention during negotiations via mixed media videoconferencing. ||| 15342 ||| 15343 ||| 31890 ||| 31891 ||| 31892 ||| 
2019 ||| estimating attention flow in online video networks. ||| 22916 ||| 31893 ||| 14077 ||| 
2020 ||| quality of and attention to instructions in telementoring. ||| 31894 ||| 31895 ||| 31896 ||| 31897 ||| 31898 ||| 31899 ||| 31900 ||| 
2017 ||| types of motivation affect study selection, attention, and dropouts in online experiments. ||| 31901 ||| 31902 ||| 15314 ||| 
2019 ||| smac: a simplified model of attention and capture in multi-device desk-centric environments. ||| 5189 ||| 31903 ||| 31904 ||| 31905 ||| 
2019 ||| ba-pnn-based methods for power transformer fault diagnosis. ||| 31906 ||| 31907 ||| 31908 ||| 31909 ||| 31910 ||| 31911 ||| 
2022 ||| heterogeneous star graph attention network for product attributes prediction. ||| 31912 ||| 4297 ||| 16693 ||| 31913 ||| 31914 ||| 16696 ||| 
2020 ||| a cnn-based personalized system for attention detection in wayfinding tasks. ||| 31915 ||| 28011 ||| 28012 ||| 31916 ||| 6627 ||| 
2020 ||| aicf: attention-based item collaborative filtering. ||| 31917 ||| 25208 ||| 31918 ||| 778 ||| 31919 ||| 
2021 ||| driver behavior detection via adaptive spatial attention mechanism. ||| 660 ||| 20341 ||| 31920 ||| 31921 ||| 31922 ||| 31923 ||| 
2021 ||| a newly-designed fault diagnostic method for transformers via improved empirical wavelet transform and kernel extreme learning machine. ||| 31924 ||| 1310 ||| 31925 ||| 31926 ||| 
2021 ||| research on hybrid feature selection method of power transformer based on fuzzy information entropy. ||| 31927 ||| 31928 ||| 31929 ||| 31930 ||| 31931 ||| 31932 ||| 
2019 ||| recognizing people's identity in construction sites with computer vision: a spatial and temporal attention pooling network. ||| 31933 ||| 31934 ||| 31935 ||| 31936 ||| 31937 ||| 
2020 ||| review visual attention and spatial memory in building inspection: toward a cognition-driven information system. ||| 28011 ||| 28012 ||| 21025 ||| 
2021 ||| a novel deep learning prediction model for concrete dam displacements using interpretable mixed attention mechanism. ||| 31938 ||| 27949 ||| 31939 ||| 8831 ||| 
2021 ||| an autoencoder wavelet based deep neural network with attention mechanism for multi-step prediction of plant growth. ||| 31940 ||| 31941 ||| 31942 ||| 31943 ||| 31944 ||| 31945 ||| 
2018 ||| deep attention network for joint hand gesture localization and recognition using static rgb-d images. ||| 8177 ||| 2219 ||| 2222 ||| 31946 ||| 
2019 ||| self-attention convolutional neural network for improved mr image reconstruction. ||| 15016 ||| 31947 ||| 2058 ||| 31948 ||| 27804 ||| 
2020 ||| dispatched attention with multi-task learning for nested mention recognition. ||| 26780 ||| 26781 ||| 3725 ||| 
2022 ||| knowledge graph embedding by logical-default attention graph convolution neural network for link prediction. ||| 719 ||| 12758 ||| 31949 ||| 31950 ||| 15238 ||| 
2021 ||| attention guided for partial domain adaptation. ||| 31951 ||| 31952 ||| 
2021 ||| collaborative filtering with a deep adversarial and attention network for cross-domain recommendation. ||| 31953 ||| 31954 ||| 1053 ||| 17444 ||| 15166 ||| 
2021 ||| cross-domain sentiment classification via parameter transferring and attention sharing mechanism. ||| 31955 ||| 31956 ||| 31957 ||| 31958 ||| 31959 ||| 31960 ||| 
2021 ||| dual attention guided multi-scale cnn for fine-grained image classification. ||| 31961 ||| 5991 ||| 4003 ||| 31962 ||| 31963 ||| 
2020 ||| semantic relation extraction using sequential and tree-structured lstm with attention. ||| 469 ||| 31964 ||| 470 ||| 29416 ||| 706 ||| 
2021 ||| facial expression recognition with grid-wise attention and visual transformer. ||| 31965 ||| 8094 ||| 31966 ||| 4878 ||| 
2022 ||| a pattern-aware self-attention network for distant supervised relation extraction. ||| 31967 ||| 688 ||| 6288 ||| 5474 ||| 26634 ||| 
2021 ||| fusion of heterogeneous attention mechanisms in multi-view convolutional neural network for text classification. ||| 18505 ||| 31968 ||| 16806 ||| 775 ||| 3002 ||| 31969 ||| 20663 ||| 
2017 ||| visual attention analysis and prediction on human faces. ||| 11343 ||| 6516 ||| 31523 ||| 2058 ||| 19391 ||| 19540 ||| 8532 ||| 
2020 ||| learning reinforced attentional representation for end-to-end visual tracking. ||| 2170 ||| 14551 ||| 2355 ||| 31970 ||| 31546 ||| 2349 ||| 
2021 ||| attention enhanced long short-term memory network with multi-source heterogeneous information fusion: an application to bgi genomics. ||| 31971 ||| 31972 ||| 6924 ||| 
2021 ||| dig users' intentions via attention flow network for personalized recommendation. ||| 17090 ||| 31973 ||| 31974 ||| 31975 ||| 31976 ||| 977 ||| 
2022 ||| hybrid attention network based on progressive embedding scale-context for crowd counting. ||| 31977 ||| 19909 ||| 19908 ||| 1302 ||| 1900 ||| 
2020 ||| a spatiotemporal attention mechanism-based model for multi-step citywide passenger demand prediction. ||| 18550 ||| 5536 ||| 2424 ||| 18551 ||| 18552 ||| 18553 ||| 
2020 ||| stmag: a spatial-temporal mixed attention graph-based convolution model for multi-data flow safety prediction. ||| 31978 ||| 31979 ||| 31980 ||| 
2021 ||| claver: an integrated framework of convolutional layer, bidirectional lstm with attention mechanism based scholarly venue recommendation. ||| 31981 ||| 31982 ||| 31983 ||| 
2020 ||| attention-aware perceptual enhancement nets for low-resolution image classification. ||| 17416 ||| 17584 ||| 31984 ||| 10768 ||| 31985 ||| 
2020 ||| transferable attention networks for adversarial domain adaptation. ||| 31951 ||| 31952 ||| 3906 ||| 
2020 ||| attention-based bidirectional gru networks for efficient https traffic classification. ||| 2659 ||| 31986 ||| 31987 ||| 5935 ||| 31988 ||| 31989 ||| 31990 ||| 
2022 ||| global and local attention-based multi-label learning with missing labels. ||| 31991 ||| 13520 ||| 31992 ||| 
2021 ||| attention-based word embeddings using artificial bee colony algorithm for aspect-level sentiment classification. ||| 1251 ||| 31993 ||| 247 ||| 31994 ||| 
2021 ||| taert: triple-attentional explainable recommendation with temporal convolutional network. ||| 31995 ||| 7830 ||| 6928 ||| 31996 ||| 31997 ||| 398 ||| 
2020 ||| attention-based context-aware sequential recommendation model. ||| 31499 ||| 2614 ||| 31998 ||| 31999 ||| 19928 ||| 
2019 ||| attention-based spatio-temporal dependence learning network. ||| 3219 ||| 32000 ||| 18807 ||| 27808 ||| 32001 ||| 
2021 ||| a spatiotemporal hierarchical attention mechanism-based model for multi-step station-level crowd flow prediction. ||| 18550 ||| 5536 ||| 2424 ||| 18551 ||| 18552 ||| 18553 ||| 
2021 ||| pgra: projected graph relation-feature attention network for heterogeneous information network embedding. ||| 32002 ||| 189 ||| 18080 ||| 
2021 ||| document-level relation extraction with entity-selection attention. ||| 32003 ||| 688 ||| 717 ||| 32004 ||| 32005 ||| 
2021 ||| attention-adaptive and deformable convolutional modules for dynamic scene deblurring. ||| 1207 ||| 6637 ||| 32006 ||| 
2021 ||| integrating object proposal with attention networks for video saliency detection. ||| 6554 ||| 32007 ||| 13435 ||| 32008 ||| 
2021 ||| attention based consistent semantic learning for micro-video scene recognition. ||| 8718 ||| 13203 ||| 32009 ||| 32010 ||| 32011 ||| 13205 ||| 
2021 ||| hybrid-attention guided network with multiple resolution features for person re-identification. ||| 32012 ||| 32013 ||| 32014 ||| 6718 ||| 2342 ||| 13196 ||| 
2021 ||| tagcn: station-level demand prediction for bike-sharing system via a temporal attention graph convolution network. ||| 3678 ||| 6579 ||| 2424 ||| 18553 ||| 
2021 ||| interpretable duplicate question detection models based on attention mechanism. ||| 32015 ||| 19332 ||| 13930 ||| 
2021 ||| dsagan: a generative adversarial network based on dual-stream attention mechanism for anatomical and functional image fusion. ||| 11391 ||| 20432 ||| 28933 ||| 32016 ||| 
2021 ||| joint extraction of entities and relations via an entity correlated attention neural model. ||| 9023 ||| 6552 ||| 14130 ||| 32017 ||| 28965 ||| 14131 ||| 32018 ||| 
2021 ||| hierarchical segment-channel attention network for explainable multichannel signal classification. ||| 32019 ||| 32020 ||| 32021 ||| 32022 ||| 32023 ||| 
2019 ||| learning peer recommendation using attention-driven cnn with interaction tripartite graph. ||| 32024 ||| 32025 ||| 32026 ||| 31965 ||| 32027 ||| 
2018 ||| attention driven multi-modal similarity learning. ||| 32028 ||| 32029 ||| 32030 ||| 444 ||| 
2021 ||| pa-net: learning local features using by pose attention for short-term person re-identification. ||| 333 ||| 32031 ||| 2411 ||| 32032 ||| 4003 ||| 5077 ||| 
2021 ||| uda: a user-difference attention for group recommendation. ||| 32033 ||| 20779 ||| 32034 ||| 32035 ||| 32036 ||| 
2021 ||| fine-grained learning performance prediction via adaptive sparse self-attention networks. ||| 31966 ||| 32037 ||| 31965 ||| 32025 ||| 8094 ||| 
2022 ||| mgat-esm: multi-channel graph attention neural network with event-sharing module for rumor detection. ||| 32038 ||| 5970 ||| 32039 ||| 17091 ||| 
2021 ||| learning sentiment sentence representation with multiview attention model. ||| 3312 ||| 3313 ||| 3315 ||| 
2020 ||| cross-attentional bracket-shaped convolutional network for semantic image segmentation. ||| 13348 ||| 13349 ||| 8747 ||| 13351 ||| 
2022 ||| deep multi-scale attention network for rna-binding proteins prediction. ||| 17757 ||| 32040 ||| 32041 ||| 
2022 ||| self-attention-based multi-agent continuous control method in cooperative environments. ||| 19109 ||| 32042 ||| 8608 ||| 32043 ||| 
2021 ||| exploiting dynamic spatio-temporal correlations for citywide traffic flow prediction using attention based neural networks. ||| 32044 ||| 11190 ||| 32045 ||| 
2022 ||| convolutional attention neural network over graph structures for improving the performance of aspect-level sentiment analysis. ||| 32046 ||| 32047 ||| 32048 ||| 
2021 ||| dfiam: deep factorization integrated attention mechanism for smart tv recommendation. ||| 32049 ||| 32050 ||| 32051 ||| 32052 ||| 3894 ||| 
2021 ||| open-world knowledge graph completion with multiple interaction attention. ||| 32053 ||| 24816 ||| 11145 ||| 654 ||| 11254 ||| 32054 ||| 11162 ||| 
2020 ||| incorporating word attention with convolutional neural networks for abstractive summarization. ||| 32055 ||| 32056 ||| 1142 ||| 4080 ||| 
2019 ||| attention based hierarchical lstm network for context-aware microblog sentiment classification. ||| 1311 ||| 602 ||| 32057 ||| 1312 ||| 4067 ||| 
2021 ||| emochannel-sa: exploring emotional dependency towards classification task with self-attention mechanism. ||| 32058 ||| 24898 ||| 24819 ||| 3477 ||| 20372 ||| 32059 ||| 
2021 ||| modalnet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network. ||| 2855 ||| 299 ||| 32060 ||| 32061 ||| 16806 ||| 775 ||| 
2020 ||| hybrid graph convolutional networks with multi-head attention for location recommendation. ||| 9013 ||| 32062 ||| 9010 ||| 9012 ||| 9011 ||| 30646 ||| 
2019 ||| residual attention-based lstm for video captioning. ||| 17738 ||| 32063 ||| 32064 ||| 1039 ||| 
2021 ||| aspect-based sentiment analysis for online reviews with hybrid attention networks. ||| 32065 ||| 32066 ||| 32067 ||| 9920 ||| 11154 ||| 
2021 ||| a multi-view attention-based deep learning system for online deviant content detection. ||| 18505 ||| 16806 ||| 775 ||| 3002 ||| 299 ||| 20669 ||| 
2021 ||| an approximation theorist's view on solving operator equations - with special attention to trefftz, mfs, mps, and drm methods. ||| 32068 ||| 
2021 ||| power electronic transformer design with dual-pwm based on matlab/simulink. ||| 18452 ||| 32069 ||| 18451 ||| 18453 ||| 18454 ||| 
2021 ||| ethics, rules of engagement, and ai: neural narrative mapping using large transformer language models. ||| 32070 ||| 32071 ||| 32072 ||| 
2020 ||| an approach on lifetime estimation of distribution transformers based on degree of polymerization. ||| 32073 ||| 32074 ||| 16220 ||| 
2022 ||| attention-based deep survival model for time series data. ||| 32075 ||| 32076 ||| 32077 ||| 
2021 ||| hierarchical attention graph convolutional network to fuse multi-sensor signals for remaining useful life prediction. ||| 32078 ||| 29155 ||| 29156 ||| 29157 ||| 29158 ||| 
2021 ||| a novel temporal convolutional network with residual self-attention mechanism for remaining useful life prediction of rolling bearings. ||| 32079 ||| 32080 ||| 32081 ||| 32082 ||| 
2019 ||| an improved polynomial-based nonlinear variable importance measure and its application to degradation assessment for high-voltage transformer under imbalance data. ||| 32083 ||| 8349 ||| 32084 ||| 2039 ||| 
2018 ||| top-down neural attention by excitation backprop. ||| 5365 ||| 32085 ||| 18766 ||| 32086 ||| 7142 ||| 7958 ||| 
2021 ||| deepvs2.0: a saliency-structured deep learning method for predicting dynamic visual attention. ||| 19374 ||| 18741 ||| 18740 ||| 2301 ||| 
2021 ||| talk2nav: long-range vision-and-language navigation with dual attention and spatial memory. ||| 32087 ||| 25542 ||| 7814 ||| 
2022 ||| dual-attention-guided network for ghost-free high dynamic range imaging. ||| 19015 ||| 19016 ||| 32088 ||| 5177 ||| 6335 ||| 9216 ||| 10075 ||| 
2021 ||| a-net: joint facial action unit detection and face alignment via adaptive attention. ||| 8795 ||| 5247 ||| 1691 ||| 5141 ||| 
2021 ||| attention guided low-light image enhancement with a large scale low-light simulation dataset. ||| 32089 ||| 11641 ||| 13639 ||| 
2019 ||| zoom out-and-in network with map attention decision for region proposal and object detection. ||| 5949 ||| 16550 ||| 2303 ||| 1846 ||| 
2019 ||| hierarchical attention for part-aware face detection. ||| 32090 ||| 1915 ||| 1916 ||| 1788 ||| 
2021 ||| enhanced 3d human pose estimation from videos by using attention-based neural network with dilated convolutions. ||| 19283 ||| 19284 ||| 19285 ||| 2230 ||| 32091 ||| 19287 ||| 
2020 ||| a simple and light-weight attention module for convolutional neural networks. ||| 5369 ||| 7916 ||| 7917 ||| 5372 ||| 
2020 ||| robust attentional aggregation of deep feature sets for multi-view 3d reconstruction. ||| 2760 ||| 1300 ||| 17812 ||| 17813 ||| 
2022 ||| codon: on orchestrating cross-domain attentions for depth super-resolution. ||| 32092 ||| 32093 ||| 875 ||| 1756 ||| 
2020 ||| scalable person re-identification by harmonious attention. ||| 3337 ||| 2196 ||| 19101 ||| 
2020 ||| simultaneous deep stereo matching and dehazing with feature attention. ||| 32094 ||| 32095 ||| 32096 ||| 32097 ||| 32098 ||| 1515 ||| 
2021 ||| guided attention in cnns for occluded pedestrian detection and re-identification. ||| 19144 ||| 26851 ||| 1825 ||| 19145 ||| 
2020 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in cnns. ||| 23143 ||| 23144 ||| 23145 ||| 32099 ||| 23147 ||| 
2022 ||| perspectives and prospects on transformer architecture for cross-modal tasks with language and vision. ||| 3229 ||| 32100 ||| 3230 ||| 
2021 ||| selective wavelet attention learning for single image deraining. ||| 18593 ||| 32101 ||| 6660 ||| 2824 ||| 17803 ||| 
2021 ||| continuous 3d multi-channel sign language production via progressive transformers and mixture density networks. ||| 8528 ||| 8529 ||| 7442 ||| 8530 ||| 
2021 ||| multi-level motion attention for human motion prediction. ||| 8503 ||| 8504 ||| 8505 ||| 7449 ||| 
2018 ||| traffic signal detection and classification in street views using an attention model. ||| 32102 ||| 32103 ||| 32104 ||| 32105 ||| 
2021 ||| pct: point cloud transformer. ||| 24014 ||| 32106 ||| 32107 ||| 32108 ||| 32109 ||| 32110 ||| 
2021 ||| can attention enable mlps to catch up with cnns? ||| 24014 ||| 32107 ||| 32108 ||| 32111 ||| 32109 ||| 32110 ||| 
2022 ||| transformers in computational visual media: a survey. ||| 1813 ||| 32112 ||| 32113 ||| 32114 ||| 19414 ||| 32115 ||| 32116 ||| 19415 ||| 2382 ||| 1175 ||| 
2019 ||| recurrent 3d attentional networks for end-to-end active object recognition. ||| 6796 ||| 32117 ||| 32118 ||| 5723 ||| 6238 ||| 7315 ||| 
2020 ||| high frequency transformer in electric traction with bidirectional dc-dc converter using customized embedded system. ||| 32119 ||| 32120 ||| 
2018 ||| efficient one-dimensional forward and inverse discrete wavelet transformers. ||| 22250 ||| 22251 ||| 22252 ||| 22253 ||| 
2020 ||| a cnn-lstm network with attention approach for learning universal sentence representation in embedded system. ||| 32121 ||| 778 ||| 17798 ||| 
2021 ||| fine grained sentiment polarity classification using augmented knowledge sequence-attention mechanism. ||| 32122 ||| 32123 ||| 
2021 ||| design and optimization for current transformer core based on magnetic field analysis. ||| 32124 ||| 32125 ||| 32126 ||| 32127 ||| 32128 ||| 
2019 ||| an lstm-cnn attention approach for aspect-level sentiment classification. ||| 1871 ||| 13735 ||| 1254 ||| 32129 ||| 32130 ||| 
2021 ||| multiway dynamic mask attention networks for natural language inference. ||| 32131 ||| 32132 ||| 1254 ||| 32133 ||| 1871 ||| 
2021 ||| self-attention networks for code search. ||| 32134 ||| 32135 ||| 6514 ||| 32136 ||| 
2021 ||| improving requirements specification use by transferring attention with eye tracking data. ||| 23714 ||| 23715 ||| 
2018 ||| semantic parsing natural language into sparql: improving target language representation with neural attention. ||| 32137 ||| 32138 ||| 
2018 ||| channel attention and multi-level features fusion for single image super-resolution. ||| 17342 ||| 17898 ||| 11441 ||| 21702 ||| 21703 ||| 
2021 ||| cltr: an end-to-end, transformer-based system for cell level table retrieval and table question answering. ||| 4772 ||| 4767 ||| 4766 ||| 3689 ||| 32139 ||| 
2020 ||| self-attention-based bigru and capsule network for named entity recognition. ||| 30132 ||| 5479 ||| 30133 ||| 
2017 ||| identity-aware textual-visual matching with latent co-attention. ||| 2332 ||| 2333 ||| 1848 ||| 2334 ||| 1846 ||| 
2021 ||| ammu - a survey of transformer-based biomedical pretrained language models. ||| 31154 ||| 31155 ||| 31156 ||| 
2020 ||| attention-guided discriminative region localization for bone age assessment. ||| 6915 ||| 6318 ||| 31025 ||| 31026 ||| 31010 ||| 31012 ||| 
2021 ||| tcl: transformer-based dynamic graph modelling via contrastive learning. ||| 4754 ||| 32140 ||| 2332 ||| 32141 ||| 4175 ||| 781 ||| 11185 ||| 25400 ||| 1245 ||| 8916 ||| 
2020 ||| end-to-end video instance segmentation with transformers. ||| 10192 ||| 18944 ||| 18945 ||| 6335 ||| 18946 ||| 10211 ||| 5155 ||| 
2018 ||| summarizing videos with attention. ||| 6401 ||| 6402 ||| 6403 ||| 6404 ||| 6405 ||| 
2019 ||| computer-aided diagnosis in histopathological images of the endometrium using a convolutional neural network and attention mechanisms. ||| 7725 ||| 31129 ||| 19025 ||| 27172 ||| 31130 ||| 
2019 ||| attention-based modeling for emotion detection and classification in textual conversations. ||| 32142 ||| 4194 ||| 59 ||| 32143 ||| 32144 ||| 32145 ||| 
2020 ||| image-based vehicle re-identification model with adaptive attention modules and metadata re-ranking. ||| 32146 ||| 32147 ||| 32148 ||| 9940 ||| 32149 ||| 
2021 ||| aaformer: auto-aligned transformer for person re-identification. ||| 32150 ||| 11288 ||| 14494 ||| 6416 ||| 32151 ||| 32152 ||| 2058 ||| 2077 ||| 2074 ||| 
2022 ||| a text attention network for spatial deformation robust scene text image super-resolution. ||| 32153 ||| 32154 ||| 241 ||| 
2021 ||| encoder fusion network with co-attention embedding for referring image segmentation. ||| 19241 ||| 19242 ||| 19243 ||| 1700 ||| 
2019 ||| constructive type-logical supertagging with self-attention networks. ||| 23757 ||| 23758 ||| 23759 ||| 
2021 ||| situation-aware environment perception using a multi-layer attention map. ||| 32155 ||| 32156 ||| 3831 ||| 32157 ||| 32158 ||| 23624 ||| 
2021 ||| pose discrepancy spatial transformer based feature disentangling for partial aspect angles sar target recognition. ||| 32159 ||| 10583 ||| 32160 ||| 32161 ||| 
2021 ||| vidface: a full-transformer solver for video facehallucination with unaligned tiny snapshots. ||| 20139 ||| 18978 ||| 23905 ||| 7788 ||| 208 ||| 
2021 ||| using self-supervised feature extractors with attention for automatic covid-19 detection from speech. ||| 32162 ||| 3419 ||| 14327 ||| 32163 ||| 3419 ||| 32164 ||| 32165 ||| 
2021 ||| pama-tts: progression-aware monotonic attention for stable seq2seq tts with accurate phoneme duration control. ||| 32166 ||| 32167 ||| 4551 ||| 
2020 ||| optimizing deeper transformers on small datasets: an application on text-to-sql semantic parsing. ||| 3676 ||| 2334 ||| 3678 ||| 3679 ||| 32168 ||| 3354 ||| 3682 ||| 
2021 ||| adversarial token attacks on vision transformers. ||| 32169 ||| 32170 ||| 32171 ||| 
2021 ||| sparse spatial attention network for semantic segmentation. ||| 11232 ||| 11233 ||| 
2020 ||| tado: time-varying attention with dual-optimizer model. ||| 18556 ||| 17797 ||| 18557 ||| 18558 ||| 
2019 ||| an empirical study of spatial attention mechanisms in deep networks. ||| 2638 ||| 2639 ||| 1770 ||| 1771 ||| 1847 ||| 
2021 ||| combining cnns with transformer for multimodal 3d mri brain tumor segmentation with self-supervised pretraining. ||| 32172 ||| 32173 ||| 32174 ||| 32175 ||| 
2021 ||| trouspi-net: spatio-temporal attention on parallel atrous convolutions and u-grus for skeletal pedestrian crossing prediction. ||| 5697 ||| 5698 ||| 32176 ||| 5700 ||| 
2021 ||| doodleformer: creative sketch drawing with transformers. ||| 1968 ||| 32177 ||| 1970 ||| 1971 ||| 1972 ||| 1855 ||| 32178 ||| 
2018 ||| development of spatial suppression surrounding the focus of visual attention. ||| 32179 ||| 29817 ||| 32180 ||| 
2019 ||| compressive transformers for long-range sequence modelling. ||| 3709 ||| 23906 ||| 22772 ||| 23908 ||| 
2017 ||| attentional factorization machines: learning the weight of feature interactions via attention networks. ||| 7652 ||| 6270 ||| 1063 ||| 2484 ||| 2258 ||| 3605 ||| 
2018 ||| show, attend and translate: unpaired multi-domain image-to-image translation with visual attention. ||| 32181 ||| 32182 ||| 32183 ||| 32184 ||| 32185 ||| 
2020 ||| answer-checking in context: a multi-modal fullyattention network for visual question answering. ||| 20104 ||| 20105 ||| 14543 ||| 20106 ||| 20107 ||| 
2021 ||| cat: cross-attention transformer for one-shot object detection. ||| 32186 ||| 32187 ||| 5089 ||| 6003 ||| 32188 ||| 6334 ||| 241 ||| 5845 ||| 
2020 ||| chroma intra prediction with attention-based cnn architectures. ||| 11480 ||| 14913 ||| 11482 ||| 11483 ||| 1675 ||| 11484 ||| 
2018 ||| self-attention: a better building block for sentiment analysis neural network classifiers. ||| 20773 ||| 20774 ||| 
2020 ||| catching attention with automatic pull quote selection. ||| 11657 ||| 11658 ||| 
2020 ||| complaint identification in social media with transformer networks. ||| 11655 ||| 3736 ||| 
2022 ||| kformer: knowledge injection in transformer feed-forward layers. ||| 32189 ||| 3499 ||| 17987 ||| 3171 ||| 3174 ||| 17990 ||| 
2022 ||| scorenet: learning non-uniform attention and augmentation for transformer-based histopathological image classification. ||| 32190 ||| 3831 ||| 32191 ||| 32192 ||| 32193 ||| 
2021 ||| clta: contents and length-based temporal attention for few-shot action recognition. ||| 32194 ||| 32195 ||| 32196 ||| 
2021 ||| predicting opioid use disorder from longitudinal healthcare data using multi-stream transformer. ||| 31599 ||| 31606 ||| 31602 ||| 31601 ||| 32197 ||| 32198 ||| 31162 ||| 13706 ||| 
2021 ||| compositional generalization in semantic parsing with pretrained transformers. ||| 32199 ||| 
2018 ||| sarn: relational reasoning through sequential attention. ||| 32200 ||| 26548 ||| 32201 ||| 
2018 ||| conditional transfer with dense residual attention: synthesizing traffic signs from street-view imagery. ||| 20121 ||| 20120 ||| 20122 ||| 20123 ||| 20124 ||| 
2021 ||| subformer: exploring weight sharing for parameter efficiency in generative transformers. ||| 26496 ||| 7447 ||| 20764 ||| 
2022 ||| the dual form of neural networks revisited: connecting test time predictions to training patterns via spotlights of attention. ||| 12659 ||| 59 ||| 26307 ||| 7111 ||| 4194 ||| 11785 ||| 
2021 ||| 3d medical point transformer: introducing convolution to attention networks for medical point cloud analysis. ||| 32202 ||| 32203 ||| 22747 ||| 32204 ||| 326 ||| 32205 ||| 32206 ||| 32207 ||| 
2021 ||| do time constraints re-prioritize attention to shapes during visual photo inspection? ||| 1209 ||| 32208 ||| 32209 ||| 32210 ||| 
2020 ||| attentional biased stochastic gradient for imbalanced classification. ||| 6188 ||| 15538 ||| 32211 ||| 32212 ||| 32213 ||| 
2021 ||| mutualformer: multi-modality representation learning via mutual transformer. ||| 32214 ||| 2632 ||| 5957 ||| 382 ||| 
2021 ||| karl-trans-ner: knowledge aware representation learning for named entity recognition using transformers. ||| 20836 ||| 20837 ||| 20838 ||| 20839 ||| 
2021 ||| stage conscious attention network (scan) : a demonstration-conditioned policy for few-shot imitation. ||| 32215 ||| 24330 ||| 1385 ||| 32216 ||| 1387 ||| 
2019 ||| conversational emotion analysis via attention mechanisms. ||| 6227 ||| 12041 ||| 2304 ||| 12758 ||| 
2021 ||| an exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers. ||| 3849 ||| 3850 ||| 3851 ||| 
2020 ||| linear attention mechanism: an efficient attention for semantic segmentation. ||| 8207 ||| 32217 ||| 30455 ||| 30454 ||| 
2022 ||| multimodal pre-training based on graph attention network for document understanding. ||| 32218 ||| 32219 ||| 1010 ||| 29193 ||| 4489 ||| 
2017 ||| human action recognition: pose-based attention draws focus to hands. ||| 7968 ||| 7969 ||| 7970 ||| 
2021 ||| residual attention based network for automatic classification of phonation modes. ||| 11982 ||| 20019 ||| 3337 ||| 
2021 ||| transformer-based deep image matching for generalizable person re-identification. ||| 32220 ||| 1932 ||| 
2020 ||| delight: very deep and light-weight transformer. ||| 24016 ||| 22829 ||| 4895 ||| 24017 ||| 4765 ||| 
2019 ||| spatio-temporal attention pooling for audio scene classification. ||| 12352 ||| 12354 ||| 3882 ||| 14474 ||| 12355 ||| 14475 ||| 12357 ||| 12358 ||| 
2020 ||| progressive transformers for end-to-end sign language production. ||| 8528 ||| 8529 ||| 7442 ||| 8530 ||| 
2020 ||| encoding syntactic knowledge in transformer encoder for intent detection and slot filling. ||| 17932 ||| 17933 ||| 12484 ||| 16905 ||| 14646 ||| 
2020 ||| the monte carlo transformer: a stochastic self-attention model for sequence prediction. ||| 32221 ||| 32222 ||| 32223 ||| 32224 ||| 23466 ||| 
2021 ||| transformer-based spatial-temporal feature learning for eeg decoding. ||| 14577 ||| 32225 ||| 32226 ||| 14578 ||| 
2017 ||| atrank: an attention-based user behavior modeling framework for recommendation. ||| 2482 ||| 17780 ||| 17781 ||| 17782 ||| 17783 ||| 17784 ||| 1423 ||| 
2018 ||| band selection from hyperspectral images using attention-based convolutional neural networks. ||| 32227 ||| 32228 ||| 32229 ||| 13472 ||| 
2018 ||| generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization. ||| 3327 ||| 4917 ||| 4918 ||| 3329 ||| 4919 ||| 3328 ||| 
2021 ||| localtrans: a multiscale local transformer network for cross-resolution homography estimation. ||| 1711 ||| 1712 ||| 1713 ||| 1714 ||| 1716 ||| 
2020 ||| fine-grained visual textual alignment for cross-modal retrieval using transformer encoders. ||| 2689 ||| 2690 ||| 20143 ||| 2691 ||| 2692 ||| 2693 ||| 2694 ||| 
2020 ||| privileged pooling: supervised attention-based pooling for compensating dataset bias. ||| 3369 ||| 32230 ||| 6235 ||| 32231 ||| 23147 ||| 23146 ||| 
2021 ||| lmms reloaded: transformer-based sense embeddings for disambiguation and beyond. ||| 9976 ||| 9977 ||| 32232 ||| 32233 ||| 852 ||| 24953 ||| 
2021 ||| tcct: tightly-coupled convolutional transformer on time series forecasting. ||| 30312 ||| 32234 ||| 
2021 ||| searching for efficient multi-stage vision transformers. ||| 32235 ||| 21806 ||| 32236 ||| 
2017 ||| attention is all you need. ||| 2466 ||| 9132 ||| 9133 ||| 4960 ||| 8069 ||| 9134 ||| 9135 ||| 9136 ||| 
2021 ||| billion-scale pretraining with vision transformers for multi-task visual representations. ||| 7183 ||| 7184 ||| 7185 ||| 7186 ||| 7187 ||| 
2021 ||| dprost: 6-dof object pose estimation using space carving and dynamic projective spatial transformer. ||| 32237 ||| 32238 ||| 
2021 ||| a pressure ulcer care system for remote medical assistance: residual u-net with an attention model based for wound area segmentation. ||| 8363 ||| 32239 ||| 8366 ||| 
2020 ||| marathi to english neural machine translation with near perfect corpus and transformers. ||| 32240 ||| 
2021 ||| stformer: a noise-aware efficient spatio-temporal transformer architecture for traffic forecasting. ||| 32241 ||| 1218 ||| 32242 ||| 32243 ||| 18829 ||| 32244 ||| 
2022 ||| a transformer-based siamese network for change detection. ||| 32245 ||| 18609 ||| 
2021 ||| transformers: "the end of history" for nlp? ||| 7047 ||| 7048 ||| 7049 ||| 
2021 ||| e-dssr: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception. ||| 27501 ||| 1986 ||| 27502 ||| 27503 ||| 1991 ||| 1992 ||| 27446 ||| 
2021 ||| a new rotating machinery fault diagnosis method based on the time series transformer. ||| 32246 ||| 20813 ||| 32247 ||| 
2019 ||| learning to dynamically coordinate multi-robot teams in graph attention networks. ||| 22678 ||| 3929 ||| 
2021 ||| hydra - hyper dependency representation attentions. ||| 32248 ||| 32249 ||| 32250 ||| 32251 ||| 32252 ||| 4735 ||| 
2021 ||| dual aspect self-attention based on transformer for remaining useful life prediction. ||| 11585 ||| 18266 ||| 28412 ||| 
2019 ||| herding effect based attention for personalized time-sync video recommendation. ||| 1437 ||| 1439 ||| 1440 ||| 1438 ||| 19873 ||| 1441 ||| 
2020 ||| controllable time-delay transformer for real-time punctuation prediction and disfluency detection. ||| 12760 ||| 12761 ||| 1717 ||| 8948 ||| 
2021 ||| transformer-based conditional variational autoencoder for controllable story generation. ||| 32253 ||| 32254 ||| 32255 ||| 9575 ||| 32256 ||| 17231 ||| 
2021 ||| do vision transformers see like convolutional neural networks? ||| 32257 ||| 2571 ||| 9371 ||| 32258 ||| 9291 ||| 
2021 ||| transunet: transformers make strong encoders for medical image segmentation. ||| 32259 ||| 21460 ||| 32260 ||| 23341 ||| 18050 ||| 247 ||| 27491 ||| 8660 ||| 27607 ||| 
2020 ||| pair: planning and iterative refinement in pre-trained transformers for long text generation. ||| 26576 ||| 4754 ||| 
2021 ||| ad text classification with transformer-based natural language processing methods. ||| 32261 ||| 32262 ||| 13310 ||| 32263 ||| 32264 ||| 2101 ||| 13478 ||| 32265 ||| 7111 ||| 32266 ||| 32267 ||| 
2019 ||| attribute-aware attention model for fine-grained representation learning. ||| 19744 ||| 19745 ||| 8862 ||| 19746 ||| 
2019 ||| tanet: robust 3d object detection from point clouds with triple attention. ||| 6474 ||| 17645 ||| 17971 ||| 17972 ||| 11689 ||| 17429 ||| 
2017 ||| segmentation-aware convolutional networks using local attention masks. ||| 1849 ||| 1850 ||| 1851 ||| 
2021 ||| maast: map attention with semantic transformersfor efficient visual navigation. ||| 21838 ||| 21839 ||| 21840 ||| 21841 ||| 21842 ||| 21843 ||| 
2021 ||| spatial graph attention and curiosity-driven policy for antiviral drug discovery. ||| 32268 ||| 32269 ||| 32270 ||| 32271 ||| 32272 ||| 32273 ||| 16145 ||| 32274 ||| 32275 ||| 32276 ||| 32277 ||| 29214 ||| 32278 ||| 32279 ||| 32280 ||| 32281 ||| 32282 ||| 
2020 ||| space: unsupervised object-oriented scene representation via spatial attention and decomposition. ||| 24006 ||| 22862 ||| 24007 ||| 24008 ||| 24009 ||| 24010 ||| 24011 ||| 22864 ||| 
2021 ||| groupformer: group activity recognition with clustered spatial-temporal transformer. ||| 1938 ||| 1939 ||| 1940 ||| 1941 ||| 1942 ||| 1943 ||| 1944 ||| 
2020 ||| joint self-attention and scale-aggregation for self-calibrated deraining network. ||| 778 ||| 19646 ||| 19647 ||| 19648 ||| 
2018 ||| couplenet: paying attention to couples with coupled attention for relationship recommendation. ||| 1398 ||| 9261 ||| 9016 ||| 
2021 ||| ms-tct: multi-scale temporal convtransformer for action detection. ||| 7316 ||| 5680 ||| 32283 ||| 8594 ||| 1226 ||| 7320 ||| 7321 ||| 
2021 ||| end-to-end human object interaction detection with hoi transformer. ||| 19217 ||| 19218 ||| 689 ||| 19219 ||| 5993 ||| 3473 ||| 19220 ||| 19221 ||| 1460 ||| 19222 ||| 4394 ||| 
2018 ||| a neural attention model for speech command recognition. ||| 32284 ||| 32285 ||| 32286 ||| 32287 ||| 
2021 ||| temporal transformer networks with self-supervision for action recognition. ||| 32288 ||| 5536 ||| 32289 ||| 14048 ||| 32290 ||| 32291 ||| 32292 ||| 
2020 ||| structured multimodal attentions for textvqa. ||| 32293 ||| 4631 ||| 5845 ||| 4175 ||| 32294 ||| 5177 ||| 6417 ||| 
2021 ||| mm-vit: multi-modal video transformer for compressed video action recognition. ||| 1060 ||| 7163 ||| 
2022 ||| tgfuse: an infrared and visible image fusion approach based on transformer and generative adversarial network. ||| 32295 ||| 7853 ||| 32296 ||| 
2018 ||| decoupled spatial neural attention for weakly supervised semantic segmentation. ||| 32297 ||| 1677 ||| 1691 ||| 32298 ||| 6335 ||| 11620 ||| 
2021 ||| traffic flow forecasting with maintenance downtime via multi-channel attention-based spatio-temporal graph convolutional networks. ||| 32299 ||| 32300 ||| 32301 ||| 32302 ||| 
2020 ||| multi-modal automated speech scoring using attention fusion. ||| 32303 ||| 32304 ||| 32305 ||| 32306 ||| 32307 ||| 12550 ||| 
2021 ||| agstn: learning attention-adjusted graph spatio-temporal networks for short-term urban sensor value forecasting. ||| 3711 ||| 3712 ||| 
2020 ||| improving multimodal accuracy through modality pre-training and attention. ||| 9127 ||| 32308 ||| 32309 ||| 
2019 ||| behrt: transformer for electronic health records. ||| 32310 ||| 32311 ||| 852 ||| 32312 ||| 32313 ||| 16324 ||| 32314 ||| 32315 ||| 32316 ||| 32317 ||| 
2021 ||| prototransformer: a meta-learning approach to providing student feedback. ||| 32318 ||| 32319 ||| 32320 ||| 22794 ||| 
2022 ||| transformer-based sar image despeckling. ||| 32321 ||| 32245 ||| 27805 ||| 18609 ||| 
2019 ||| ognet: salient object detection with output-guided attention module. ||| 32322 ||| 32323 ||| 
2019 ||| bert and pals: projected attention layers for efficient adaptation in multi-task learning. ||| 9391 ||| 22815 ||| 
2021 ||| extracting fine-grained knowledge graphs of scientific claims: dataset and transformer-based results. ||| 26823 ||| 26824 ||| 
2021 ||| transhash: transformer-based hamming hashing for efficient image retrieval. ||| 9886 ||| 13776 ||| 9881 ||| 7708 ||| 17926 ||| 32324 ||| 
2019 ||| contrastive attention mechanism for abstractive sentence summarization. ||| 3468 ||| 26284 ||| 3469 ||| 1254 ||| 3471 ||| 3289 ||| 
2020 ||| a financial service chatbot based on deep bidirectional transformers. ||| 32325 ||| 32326 ||| 32327 ||| 
2021 ||| trocr: transformer-based optical character recognition with pre-trained models. ||| 569 ||| 26774 ||| 9837 ||| 32328 ||| 32329 ||| 32330 ||| 12599 ||| 843 ||| 3174 ||| 
2020 ||| unsupervised pansharpening based on self-attention mechanism. ||| 32331 ||| 32332 ||| 7421 ||| 30439 ||| 
2021 ||| adavit: adaptive vision transformers for efficient image recognition. ||| 32333 ||| 32334 ||| 32335 ||| 7312 ||| 7314 ||| 17842 ||| 32336 ||| 
2019 ||| improving transformer-based speech recognition using unsupervised pre-training. ||| 12257 ||| 32337 ||| 12258 ||| 3882 ||| 32338 ||| 32339 ||| 12263 ||| 11688 ||| 
2017 ||| latent attention networks. ||| 32340 ||| 32341 ||| 32342 ||| 32343 ||| 32344 ||| 32345 ||| 
2020 ||| deberta: decoding-enhanced bert with disentangled attention. ||| 24049 ||| 24050 ||| 1958 ||| 3175 ||| 
2021 ||| attention-based clinical note summarization. ||| 32346 ||| 31869 ||| 
2021 ||| interaction detection between vehicles and vulnerable road users: a deep generative approach with attention. ||| 341 ||| 25769 ||| 30371 ||| 23647 ||| 23649 ||| 32347 ||| 
2020 ||| cosea: convolutional code search with layer-wise attention. ||| 1371 ||| 720 ||| 4787 ||| 2989 ||| 8862 ||| 4791 ||| 
2020 ||| selective attention encoders by syntactic graph convolutional networks for document summarization. ||| 12510 ||| 12511 ||| 7134 ||| 12512 ||| 12513 ||| 11688 ||| 
2018 ||| focusing on what is relevant: time-series learning and understanding using attention. ||| 20090 ||| 7210 ||| 7212 ||| 20091 ||| 20092 ||| 7209 ||| 7213 ||| 
2020 ||| cluster-former: clustering-based sparse transformer for long-range dependency encoding. ||| 3363 ||| 3574 ||| 2044 ||| 3575 ||| 3576 ||| 3577 ||| 2045 ||| 2046 ||| 
2021 ||| on isotropy calibration of transformers. ||| 32348 ||| 32349 ||| 23999 ||| 32350 ||| 24002 ||| 
2021 ||| a fine-grained visual attention approach for fingerspelling recognition in the wild. ||| 24592 ||| 24593 ||| 392 ||| 
2021 ||| interflow: aggregating multi-layer feature mappings with attention mechanism. ||| 32351 ||| 
2021 ||| token pooling in vision transformers. ||| 32352 ||| 32353 ||| 32354 ||| 32355 ||| 32356 ||| 32357 ||| 
2020 ||| a generalization of transformer networks to graphs. ||| 32358 ||| 11322 ||| 
2021 ||| laughing heads: can transformers detect what makes a sentence funny? ||| 13297 ||| 23365 ||| 13296 ||| 13300 ||| 
2020 ||| scheduled drophead: a regularization method for transformer models. ||| 23947 ||| 26473 ||| 3617 ||| 3174 ||| 3480 ||| 
2021 ||| thank you for attention: a survey on attention-based artificial neural networks for automatic speech recognition. ||| 32359 ||| 32360 ||| 20314 ||| 
2019 ||| focus your attention: a bidirectional focal attention network for image-text matching. ||| 2290 ||| 5963 ||| 19713 ||| 18733 ||| 379 ||| 17860 ||| 
2021 ||| an attention-driven hierarchical multi-scale representation for visual recognition. ||| 6384 ||| 6383 ||| 18075 ||| 
2017 ||| star-rt: visual attention for real-time video game playing. ||| 32361 ||| 29817 ||| 
2019 ||| action recognition in untrimmed videos with composite self-attention two-stream framework. ||| 2829 ||| 2830 ||| 2831 ||| 
2021 ||| predictive maintenance for general aviation using convolutional transformers. ||| 9483 ||| 32362 ||| 32363 ||| 
2021 ||| recursive fusion and deformable spatiotemporal attention for video compression artifact reduction. ||| 19483 ||| 15538 ||| 2613 ||| 
2020 ||| combining predicate transformer semantics for effects: a case study in parsing regular languages. ||| 32364 ||| 32365 ||| 
2022 ||| attention based memory video portrait matting. ||| 32366 ||| 
2020 ||| parallel machine translation with disentangled context transformer. ||| 22827 ||| 22828 ||| 22829 ||| 9318 ||| 
2020 ||| depth potentiality-aware gated attention network for rgb-d salient object detection. ||| 32367 ||| 13825 ||| 
2022 ||| towards unbiased multi-label zero-shot learning with pyramid and semantic attention. ||| 6910 ||| 19684 ||| 19681 ||| 32368 ||| 32369 ||| 
2021 ||| : interpretability-aware redundancy reduction for vision transformers. ||| 32370 ||| 32371 ||| 2521 ||| 7195 ||| 16198 ||| 32372 ||| 2372 ||| 
2021 ||| attention toward neighbors: a context aware framework for high resolution image segmentation. ||| 11207 ||| 7376 ||| 7375 ||| 7377 ||| 
2020 ||| zero-shot reinforcement learning with deep attention convolutional neural networks. ||| 24225 ||| 32373 ||| 32374 ||| 4150 ||| 24224 ||| 
2020 ||| a hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention. ||| 18371 ||| 22872 ||| 14072 ||| 22873 ||| 
2021 ||| supermeshing: a new deep learning architecture for increasing the mesh density of metal forming stress field with attention mechanism and perceptual features. ||| 32375 ||| 32376 ||| 32377 ||| 32378 ||| 32379 ||| 
2022 ||| tisat: time series anomaly transformer. ||| 32380 ||| 32381 ||| 32382 ||| 
2022 ||| cnn attention guidance for improved orthopedics radiographic fracture classification. ||| 15386 ||| 15385 ||| 7064 ||| 32383 ||| 32384 ||| 32385 ||| 15387 ||| 5177 ||| 15389 ||| 
2020 ||| sahdl: sparse attention hypergraph regularized dictionary learning. ||| 21994 ||| 345 ||| 27259 ||| 11493 ||| 27258 ||| 
2021 ||| swinir: image restoration using swin transformer. ||| 7811 ||| 7812 ||| 7813 ||| 3433 ||| 7814 ||| 7815 ||| 
2021 ||| scnet: a generalized attention-based model for crack fault segmentation. ||| 28197 ||| 28198 ||| 32386 ||| 
2021 ||| weakly supervised attention model for rv strainclassification from volumetric ctpa scans. ||| 32387 ||| 32388 ||| 32389 ||| 32390 ||| 32391 ||| 32392 ||| 32393 ||| 
2022 ||| multi-class token transformer for weakly supervised semantic segmentation. ||| 32394 ||| 2303 ||| 5328 ||| 32395 ||| 15326 ||| 436 ||| 
2021 ||| deepfake detection scheme based on vision transformer and distillation. ||| 32396 ||| 20117 ||| 20118 ||| 20119 ||| 
2018 ||| attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions. ||| 8740 ||| 5902 ||| 8741 ||| 
2021 ||| real-time speaker counting in a cocktail party scenario using attention-guided convolutional neural network. ||| 14711 ||| 14712 ||| 
2021 ||| soft: softmax-free transformer with linear complexity. ||| 18959 ||| 32397 ||| 4258 ||| 2196 ||| 1687 ||| 32398 ||| 1688 ||| 2198 ||| 254 ||| 
2021 ||| attention based end to end speech recognition for voice search in hindi and english. ||| 15045 ||| 15046 ||| 
2021 ||| introduce the result into self-attention. ||| 32399 ||| 
2021 ||| lattention: lattice-attention in asr rescoring. ||| 32400 ||| 32401 ||| 32402 ||| 3843 ||| 32403 ||| 
2021 ||| sketching as a tool for understanding and accelerating self-attention for long sequences. ||| 23691 ||| 32404 ||| 32405 ||| 8971 ||| 3403 ||| 21288 ||| 
2020 ||| public sentiment toward solar energy: opinion mining of twitter using a transformer-based language model. ||| 32406 ||| 32407 ||| 32408 ||| 32409 ||| 
2020 ||| mqtransformer: multi-horizon forecasts with context dependent and feedback-aware attention. ||| 32410 ||| 32411 ||| 32412 ||| 
2020 ||| directed graph attention neural network utilizing 3d coordinates for molecular property prediction. ||| 18226 ||| 32413 ||| 6007 ||| 
2018 ||| two-level attention with two-stage multi-task learning for facial emotion recognition. ||| 5885 ||| 5886 ||| 5887 ||| 5888 ||| 5889 ||| 5890 ||| 
2020 ||| attentionanatomy: a unified framework for whole-body organs at risk segmentation using multiple partially annotated datasets. ||| 7138 ||| 1305 ||| 7139 ||| 435 ||| 7136 ||| 7137 ||| 4297 ||| 7135 ||| 
2021 ||| a study of latent monotonic attention variants. ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| gdca: gan-based single image super resolution with dual discriminators and channel attention. ||| 32414 ||| 32415 ||| 11231 ||| 
2021 ||| learning of frequency-time attention mechanism for automatic modulation recognition. ||| 32416 ||| 27239 ||| 27240 ||| 
2021 ||| vtnet: visual transformer network for object goal navigation. ||| 23904 ||| 23905 ||| 8571 ||| 
2018 ||| training tips for the transformer model. ||| 21388 ||| 21421 ||| 
2019 ||| deep discriminative representation learning with attention map for scene classification. ||| 5536 ||| 30467 ||| 602 ||| 21132 ||| 30469 ||| 
2020 ||| interpretable detail-fidelity attention network for single image super-resolution. ||| 32417 ||| 4634 ||| 6621 ||| 32418 ||| 32419 ||| 
2020 ||| uld@nuig at semeval-2020 task 9: generative morphemes with an attention model for sentiment analysis in code-mixed text. ||| 10632 ||| 10633 ||| 10634 ||| 10635 ||| 10636 ||| 
2021 ||| can transformer language models predict psychometric properties? ||| 3941 ||| 14229 ||| 14230 ||| 3942 ||| 
2019 ||| deep contextual attention for human-object interaction detection. ||| 2546 ||| 1971 ||| 2308 ||| 1972 ||| 2279 ||| 1932 ||| 1855 ||| 
2020 ||| learning to pay attention to mistakes. ||| 32420 ||| 21492 ||| 21493 ||| 21494 ||| 
2019 ||| data augmentation for skin lesion using self-attention based progressive generative adversarial network. ||| 32421 ||| 29444 ||| 29445 ||| 
2020 |||  job description matching using context-aware transformer models. ||| 3489 ||| 26553 ||| 26554 ||| 26555 ||| 26556 ||| 375 ||| 
2020 ||| the benefit of distraction: denoising remote vitals measurements using inverse attention. ||| 1762 ||| 5732 ||| 1764 ||| 
2021 ||| fusionpainting: multimodal fusion with adaptive attention for 3d object detection. ||| 23615 ||| 18707 ||| 23616 ||| 18705 ||| 23617 ||| 23618 ||| 
2018 ||| unsupervised attention-guided image to image translation. ||| 8777 ||| 9152 ||| 9153 ||| 9154 ||| 8779 ||| 
2020 ||| from graph low-rank global attention to 2-fwl approximation. ||| 32422 ||| 32423 ||| 32424 ||| 
2018 ||| structured triplet learning with pos-tag guided attention for visual question answering. ||| 7436 ||| 7437 ||| 32425 ||| 2216 ||| 2149 ||| 7135 ||| 2035 ||| 
2018 ||| attention-based encoder-decoder networks for spelling and grammatical error correction. ||| 32426 ||| 
2020 ||| leveraging bottom-up and top-down attention for few-shot object detection. ||| 17672 ||| 1871 ||| 1872 ||| 
2019 ||| residual non-local attention networks for image restoration. ||| 1730 ||| 2232 ||| 2233 ||| 8562 ||| 1734 ||| 
2019 ||| cognitive functions of the brain: perception, attention and memory. ||| 538 ||| 
2020 ||| normalized and geometry-aware self-attention network for image captioning. ||| 19005 ||| 2058 ||| 11392 ||| 19006 ||| 19007 ||| 2080 ||| 
2022 ||| graph self-attention for learning graph representation with transformer. ||| 32427 ||| 32428 ||| 26562 ||| 32429 ||| 
2022 ||| pointattn: you only need attention for point cloud completion. ||| 1224 ||| 13194 ||| 13193 ||| 32430 ||| 6625 ||| 6335 ||| 
2018 ||| an attention-based bi-gru-capsnet model for hypernymy detection between compound entities. ||| 6627 ||| 16884 ||| 16883 ||| 16882 ||| 16885 ||| 16886 ||| 
2020 ||| residual spatial attention network for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 5337 ||| 5067 ||| 5338 ||| 
2021 ||| from augmented microscopy to the topological transformer: a new approach in cell image analysis for alzheimer's research. ||| 32431 ||| 
2020 ||| predicting goal-directed attention control using inverse-reinforcement learning. ||| 18710 ||| 19323 ||| 19325 ||| 18709 ||| 19321 ||| 19322 ||| 18884 ||| 7365 ||| 
2021 ||| transrefer3d: entity-and-relation aware transformer for fine-grained 3d visual grounding. ||| 19734 ||| 19735 ||| 1069 ||| 19736 ||| 19737 ||| 19738 ||| 17854 ||| 
2020 ||| attend and segment: attention guided active semantic segmentation. ||| 2179 ||| 2181 ||| 
2019 ||| robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts. ||| 14540 ||| 14541 ||| 6879 ||| 
2021 ||| convdysat: deep neural representation learning on dynamic graphs via self-attention and convolutional neural networks. ||| 32432 ||| 32433 ||| 32434 ||| 32435 ||| 
2021 ||| bdanet: multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images. ||| 32436 ||| 2226 ||| 2229 ||| 2230 ||| 32437 ||| 32438 ||| 17199 ||| 6720 ||| 
2021 ||| sparse attention guided dynamic value estimation for single-task multi-scene reinforcement learning. ||| 32439 ||| 8571 ||| 
2020 ||| learning to encode position for transformer with continuous dynamical model. ||| 22824 ||| 22825 ||| 22826 ||| 21252 ||| 
2020 ||| transformer networks for trajectory forecasting. ||| 20111 ||| 7272 ||| 7276 ||| 7277 ||| 
2017 ||| cham: action recognition using convolutional hierarchical attention model. ||| 13 ||| 11226 ||| 11227 ||| 11228 ||| 
2019 ||| contextual attention for hand detection in the wild. ||| 1808 ||| 1809 ||| 602 ||| 1810 ||| 7365 ||| 
2019 ||| frequency domain transformer networks for video prediction. ||| 407 ||| 409 ||| 
2022 ||| vista: boosting 3d object detection via dual cross-view spatial attention. ||| 32440 ||| 32441 ||| 917 ||| 32442 ||| 
2021 ||| 3d human pose estimation with spatial and temporal transformers. ||| 2225 ||| 2226 ||| 2227 ||| 2228 ||| 2229 ||| 2230 ||| 2231 ||| 
2021 ||| generative chemical transformer: attention makes neural machine learn molecular geometric structures via text. ||| 32443 ||| 32444 ||| 32445 ||| 
2021 ||| benchmarking detection transfer learning with vision transformers. ||| 2028 ||| 1741 ||| 1740 ||| 32446 ||| 59 ||| 1742 ||| 32447 ||| 
2021 ||| bornon: bengali image captioning with transformer-based deep learning approach. ||| 32448 ||| 32449 ||| 32450 ||| 32451 ||| 32452 ||| 
2020 ||| multitask learning and joint optimization for transformer-rnn-transducer speech recognition. ||| 12443 ||| 12444 ||| 
2020 ||| length-adaptive transformer: train once with length drop, use anytime with search. ||| 3060 ||| 3008 ||| 
2022 ||| a transformer-based network for deformable medical image registration. ||| 32453 ||| 32454 ||| 28926 ||| 
2017 ||| lyrics-based music genre classification using a hierarchical attention network. ||| 11890 ||| 
2018 ||| bidirectional attention for sql generation. ||| 23390 ||| 32455 ||| 
2020 ||| channel recurrent attention networks for video pedestrian retrieval. ||| 2127 ||| 6336 ||| 2128 ||| 2130 ||| 2131 ||| 
2020 ||| cross-media keyphrase prediction: a unified framework with multi-modality multi-head attention and image wordings. ||| 7400 ||| 4807 ||| 597 ||| 598 ||| 
2021 ||| short-term electricity price forecasting based on graph convolution network and attention mechanism. ||| 32456 ||| 32457 ||| 32458 ||| 32459 ||| 32460 ||| 
2019 ||| unraveling the origin of social bursts in collective attention. ||| 32461 ||| 32462 ||| 
2021 ||| autoformer: decomposition transformers with auto-correlation for long-term series forecasting. ||| 32463 ||| 32464 ||| 17636 ||| 17635 ||| 
2022 ||| the dark side of the language: pre-trained transformers in the darknet. ||| 26445 ||| 32465 ||| 32466 ||| 32467 ||| 26446 ||| 26448 ||| 26443 ||| 
2021 ||| an end-to-end khmer optical character recognition using sequence-to-sequence with attention. ||| 32468 ||| 32469 ||| 32470 ||| 
2018 ||| a shared attention mechanism for interpretation of neural automatic post-editing systems. ||| 17538 ||| 17539 ||| 17540 ||| 
2017 ||| stream attention for far-field multi-microphone asr. ||| 12608 ||| 8298 ||| 12611 ||| 
2021 ||| roformer: enhanced transformer with rotary position embedding. ||| 32217 ||| 3042 ||| 12437 ||| 32471 ||| 12438 ||| 
2021 ||| trade when opportunity comes: price movement forecasting via locality-aware attention and adaptive refined labeling. ||| 32243 ||| 3279 ||| 32472 ||| 595 ||| 32473 ||| 32474 ||| 32475 ||| 10415 ||| 
2021 ||| psg@hasoc-dravidian codemixfire2021: pretrained transformers for offensive language identification in tanglish. ||| 32476 ||| 32477 ||| 
2021 ||| thundr: transformer-based 3d human reconstruction with markers. ||| 1789 ||| 1790 ||| 1791 ||| 1792 ||| 1793 ||| 1794 ||| 
2021 ||| hit: hierarchical transformer with momentum contrast for video-text retrieval. ||| 2143 ||| 2025 ||| 1174 ||| 2144 ||| 2145 ||| 2146 ||| 
2020 ||| feature based sequential classifier with attention mechanism. ||| 32478 ||| 32479 ||| 32480 ||| 27629 ||| 27630 ||| 32481 ||| 32482 ||| 27631 ||| 
2017 ||| advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnn-lm. ||| 2508 ||| 3549 ||| 9472 ||| 14276 ||| 
2022 ||| a spatial-temporal attention multi-graph convolution network for ride-hailing demand prediction based on periodicity with offset. ||| 32483 ||| 32484 ||| 8608 ||| 
2021 ||| an attention ensemble approach for efficient text classification of indian languages. ||| 20769 ||| 32485 ||| 32486 ||| 
2018 ||| bidirectional attentional encoder-decoder model and bidirectional beam search for abstractive summarization. ||| 32487 ||| 32488 ||| 9888 ||| 
2018 ||| sfa: small faces attention face detector. ||| 32489 ||| 32490 ||| 8473 ||| 24526 ||| 
2018 ||| stream attention-based multi-array end-to-end speech recognition. ||| 12608 ||| 12609 ||| 12610 ||| 2508 ||| 3549 ||| 12611 ||| 
2018 ||| discriminative feature learning with foreground attention for person re-identification. ||| 2354 ||| 2357 ||| 8850 ||| 32491 ||| 11549 ||| 14779 ||| 
2021 ||| contextual hate speech detection in code mixed text using transformer based approaches. ||| 32492 ||| 15045 ||| 
2021 ||| transformerfusion: monocular rgb scene reconstruction using transformers. ||| 32493 ||| 32494 ||| 32495 ||| 32496 ||| 13617 ||| 12149 ||| 
2021 ||| probing for bridging inference in transformer language models. ||| 4831 ||| 4832 ||| 
2019 ||| dagcn: dual attention graph convolutional networks. ||| 798 ||| 799 ||| 800 ||| 801 ||| 802 ||| 
2020 ||| relation transformer network. ||| 32497 ||| 32498 ||| 15822 ||| 
2020 ||| deep entwined learning head pose and face alignment inside an attentional cascade with doubly-conditional fusion. ||| 5733 ||| 13478 ||| 32499 ||| 2118 ||| 
2017 ||| automatic music highlight extraction using convolutional recurrent attention networks. ||| 9523 ||| 32500 ||| 32501 ||| 32502 ||| 32503 ||| 
2022 ||| transformer-based models of text normalization for speech applications. ||| 32504 ||| 32505 ||| 10992 ||| 12617 ||| 
2020 ||| adaptive graph diffusion networks with hop-wise attention. ||| 24524 ||| 32506 ||| 
2021 ||| bert-gt: cross-sentence n-ary relation extraction with bert and graph transformer. ||| 32507 ||| 21587 ||| 
2022 ||| agent-temporal attention for reward redistribution in episodic multi-agent reinforcement learning. ||| 32508 ||| 32509 ||| 32510 ||| 
2021 ||| enriching non-autoregressive transformer with syntactic and semanticstructures for neural machine translation. ||| 23311 ||| 3891 ||| 14896 ||| 2815 ||| 1094 ||| 
2017 ||| towards bidirectional hierarchical representations for attention-based neural machine translation. ||| 3037 ||| 3039 ||| 2333 ||| 3040 ||| 3306 ||| 
2021 ||| hrformer: high-resolution transformer for dense prediction. ||| 32511 ||| 32512 ||| 32513 ||| 32514 ||| 8862 ||| 1788 ||| 18793 ||| 
2021 ||| attention-guided black-box adversarial attacks with large-scale multiobjective evolutionary optimization. ||| 5894 ||| 13570 ||| 800 ||| 8836 ||| 
2022 ||| dct-former: efficient self-attention with discrete cosine transform. ||| 18914 ||| 18916 ||| 32515 ||| 18918 ||| 
2021 ||| vsec: transformer-based model for vietnamese spelling correction. ||| 22578 ||| 22579 ||| 22580 ||| 22581 ||| 
2019 ||| m3d-gan: multi-modal multi-domain translation with universal attention. ||| 18947 ||| 5732 ||| 20350 ||| 
2021 ||| uction via attention-guided deep fusion of hybrid lenses. ||| 10406 ||| 17677 ||| 19426 ||| 1906 ||| 
2020 ||| how can self-attention networks recognize dyck-n languages? ||| 1405 ||| 26489 ||| 781 ||| 
2021 ||| semi-autoregressive transformer for image captioning. ||| 7825 ||| 7826 ||| 7827 ||| 444 ||| 
2019 ||| self-attention and ingredient-attention based model for recipe retrieval from image queries. ||| 19514 ||| 19515 ||| 19516 ||| 
2020 ||| pedestrian trajectory prediction using context-augmented transformer networks. ||| 32516 ||| 
2020 ||| improving constituency parsing with span attention. ||| 3197 ||| 3198 ||| 3200 ||| 2814 ||| 
2022 ||| brain cancer survival prediction on treatment-na ive mri using deep anchor attention learning with vision transformer. ||| 18777 ||| 27411 ||| 
2021 ||| differentiable spatial planning using transformers. ||| 17768 ||| 22858 ||| 2030 ||| 
2021 ||| towards emotion recognition in hindi-english code-mixed data: a transformer based approach. ||| 20765 ||| 20766 ||| 
2021 ||| fine-tuned transformers show clusters of similar representations across layers. ||| 32517 ||| 32518 ||| 3720 ||| 
2020 ||| when do drivers concentrate? attention-based driver behavior modeling with deep reinforcement learning. ||| 18468 ||| 32519 ||| 32520 ||| 
2017 ||| distance-based self-attention network for natural language inference. ||| 32521 ||| 32201 ||| 
2019 ||| atloc: attention guided camera localization. ||| 1780 ||| 17808 ||| 17810 ||| 17985 ||| 17813 ||| 17812 ||| 
2021 ||| ptnet: a high-resolution infant mri synthesizer based on transformer. ||| 31275 ||| 31272 ||| 5932 ||| 32522 ||| 32523 ||| 32524 ||| 32525 ||| 32526 ||| 12511 ||| 
2019 ||| a simple and robust convolutional-attention network for irregular text recognition. ||| 5845 ||| 12169 ||| 4175 ||| 32187 ||| 6335 ||| 10075 ||| 
2020 ||| max-fusion u-net for multi-modal pathology segmentation with attention and dynamic resampling. ||| 27793 ||| 27794 ||| 27795 ||| 13369 ||| 
2021 ||| dynamic clone transformer for efficient convolutional neural netwoks. ||| 32527 ||| 
2021 ||| deepvit: towards deeper vision transformer. ||| 19090 ||| 32528 ||| 7141 ||| 19358 ||| 19357 ||| 1902 ||| 1685 ||| 
2021 |||  cognition: the role of attention. ||| 20228 ||| 32529 ||| 32530 ||| 32531 ||| 32532 ||| 11880 ||| 
2020 ||| unsupervised extraction of market moving events with neural attention. ||| 32533 ||| 32534 ||| 
2021 ||| a non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. ||| 32535 ||| 32536 ||| 3616 ||| 32537 ||| 3618 ||| 793 ||| 
2021 ||| high-resolution depth maps imaging via attention-based hierarchical multi-modal fusion. ||| 12055 ||| 12058 ||| 12056 ||| 32538 ||| 32539 ||| 32540 ||| 
2020 ||| learning accurate integer transformer machine-translation models. ||| 32541 ||| 
2021 ||| recurrent vision transformer for solving visual reasoning problems. ||| 2689 ||| 2690 ||| 32542 ||| 2692 ||| 2691 ||| 
2021 ||| adaattn: revisit attention mechanism in arbitrary neural style transfer. ||| 1757 ||| 1758 ||| 1759 ||| 136 ||| 1907 ||| 633 ||| 1718 ||| 1719 ||| 1761 ||| 
2018 ||| diversity regularized spatiotemporal attention for video-based person re-identification. ||| 2332 ||| 18935 ||| 18936 ||| 1846 ||| 
2021 ||| how does a pre-trained transformer integrate contextual keywords? application to humanitarian computing. ||| 32543 ||| 32544 ||| 
2020 ||| short text classification via knowledge powered attention with similarity matrix based cnn. ||| 
2018 ||| quantum statistics-inspired neural attention. ||| 32545 ||| 1963 ||| 
2020 ||| deepbrain: towards personalized eeg interaction through attentional and embedded lstm learning. ||| 8976 ||| 32546 ||| 32547 ||| 32548 ||| 32549 ||| 3095 ||| 
2020 ||| attention-based self-supervised feature learning for security data. ||| 32550 ||| 32551 ||| 32552 ||| 
2021 ||| vision transformer for small-size datasets. ||| 32553 ||| 21488 ||| 21489 ||| 
2022 ||| memvit: memory-augmented multiscale vision transformer for efficient long-term video recognition. ||| 32554 ||| 2028 ||| 2027 ||| 2025 ||| 2026 ||| 2030 ||| 2031 ||| 
2021 ||| learning the physics of particle transport via transformers. ||| 32555 ||| 12413 ||| 32556 ||| 
2021 ||| dfcanet: dense feature calibration-attention guided network for cross domain iris presentation attack detection. ||| 20298 ||| 32557 ||| 20300 ||| 32558 ||| 
2021 ||| attention-based domain adaptation for single stage detectors. ||| 32559 ||| 8505 ||| 
2020 ||| a better use of audio-visual cues: dense video captioning with bi-modal transformer. ||| 21474 ||| 6321 ||| 
2019 ||| syntax-aware aspect level sentiment classification with graph attention networks. ||| 7738 ||| 7739 ||| 
2019 ||| multilingual named entity recognition using pretrained embeddings, attention mechanism and ncrf. ||| 10338 ||| 10339 ||| 
2020 ||| distant supervision for e-commerce query segmentation via attention network. ||| 885 ||| 11147 ||| 11146 ||| 1108 ||| 5250 ||| 1422 ||| 1129 ||| 32560 ||| 32561 ||| 
2017 ||| local monotonic attention mechanism for end-to-end speech recognition. ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| gpt2mvs: generative pre-trained transformer-2 for multi-modal video summarization. ||| 7455 ||| 23849 ||| 11484 ||| 7461 ||| 
2021 ||| going beyond linear transformers with recurrent fast weight programmers. ||| 12659 ||| 22751 ||| 59 ||| 26307 ||| 7111 ||| 4194 ||| 11785 ||| 
2018 ||| improving distantly supervised relation extraction using word and entity based attention. ||| 9769 ||| 2506 ||| 9770 ||| 
2021 ||| accelerated multi-modal mr imaging with transformers. ||| 27732 ||| 27733 ||| 4055 ||| 4056 ||| 1125 ||| 1932 ||| 
2021 ||| pvtv2: improved baselines with pyramid vision transformer. ||| 2006 ||| 2007 ||| 2008 ||| 1861 ||| 2009 ||| 2010 ||| 173 ||| 2011 ||| 1932 ||| 
2017 ||| attention allocation aid for visual search. ||| 15315 ||| 15316 ||| 15317 ||| 15318 ||| 15319 ||| 
2020 ||| synthesizer: rethinking self-attention in transformer models. ||| 1398 ||| 3292 ||| 3294 ||| 22745 ||| 22746 ||| 9580 ||| 
2021 ||| rap-net: region attention predictive network for precipitation nowcasting. ||| 30481 ||| 32562 ||| 32563 ||| 25166 ||| 7466 ||| 
2020 ||| efficient transformer-based large scale language representations using hardware-friendly block structured pruning. ||| 9874 ||| 9872 ||| 25744 ||| 9747 ||| 6002 ||| 11023 ||| 11024 ||| 
2022 ||| global matching with overlapping attention for optical flow estimation. ||| 32564 ||| 21442 ||| 21642 ||| 32565 ||| 1749 ||| 
2021 ||| ultrasound video transformers for cardiac ejection fraction estimation. ||| 27473 ||| 27474 ||| 27475 ||| 27476 ||| 27477 ||| 27478 ||| 
2021 ||| fully transformer networks for semantic image segmentation. ||| 32566 ||| 19088 ||| 32567 ||| 29079 ||| 2323 ||| 
2019 ||| a hybrid text normalization system using multi-head self-attention for mandarin. ||| 12522 ||| 12523 ||| 12524 ||| 399 ||| 12525 ||| 1420 ||| 4089 ||| 12526 ||| 
2021 ||| attention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario. ||| 22479 ||| 22480 ||| 11121 ||| 22481 ||| 22482 ||| 16276 ||| 
2017 ||| fusionnet: fusing via fully-aware attention with application to machine comprehension. ||| 23969 ||| 6064 ||| 3172 ||| 3175 ||| 
2018 ||| automatic graphics program generation using attention-based hierarchical decoder. ||| 2958 ||| 6350 ||| 6351 ||| 
2021 ||| knowledge-enhanced session-based recommendation with temporal transformer. ||| 32568 ||| 1143 ||| 3876 ||| 25814 ||| 
2020 ||| successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. ||| 3452 ||| 3453 ||| 3454 ||| 
2022 ||| a survey of controllable text generation using transformer-based pre-trained language models. ||| 32569 ||| 32570 ||| 32571 ||| 3480 ||| 3764 ||| 
2018 ||| optimal tap setting of voltage regulation transformers using batch reinforcement learning. ||| 32572 ||| 32573 ||| 32574 ||| 3419 ||| 32575 ||| 
2018 ||| ms-uedin submission to the wmt2018 ape shared task: dual-source transformer for automatic post-editing. ||| 21417 ||| 21418 ||| 
2021 ||| scaled relu matters for training vision transformers. ||| 1703 ||| 9464 ||| 1702 ||| 32576 ||| 32577 ||| 1704 ||| 1705 ||| 32211 ||| 
2021 ||| span: subgraph prediction attention network for dynamic graphs. ||| 8177 ||| 22569 ||| 22570 ||| 13541 ||| 
2022 ||| contextformer: a transformer with spatio-channel attention for context modeling in learned image compression. ||| 32578 ||| 8861 ||| 32579 ||| 
2020 ||| deep learning with attention mechanism for predicting driver intention at intersection. ||| 15439 ||| 15440 ||| 15441 ||| 15442 ||| 15443 ||| 
2019 ||| pairwise interactive graph attention network for context-aware recommendation. ||| 32580 ||| 32581 ||| 4095 ||| 
2022 ||| video instance segmentation via multi-scale spatio-temporal split attention transformer. ||| 32582 ||| 32583 ||| 32584 ||| 1970 ||| 1971 ||| 2308 ||| 32177 ||| 32178 ||| 1972 ||| 
2021 ||| not all attention is all you need. ||| 3739 ||| 3111 ||| 1254 ||| 
2020 ||| memory transformer. ||| 32585 ||| 32586 ||| 
2020 ||| parsbert: transformer-based model for persian language understanding. ||| 32587 ||| 32588 ||| 32589 ||| 32590 ||| 
2021 ||| git: graph interactive transformer for vehicle re-identification. ||| 32591 ||| 11173 ||| 32592 ||| 17416 ||| 32593 ||| 
2020 ||| multispeech: multi-speaker text to speech with transformer. ||| 14506 ||| 14507 ||| 14508 ||| 14509 ||| 7725 ||| 12137 ||| 4789 ||| 
2020 ||| learning accurate and human-like driving using semantic maps and attention. ||| 25541 ||| 25542 ||| 25543 ||| 7814 ||| 
2021 ||| hawk: rapid android malware detection through heterogeneous graph attention networks. ||| 32594 ||| 32595 ||| 9407 ||| 26422 ||| 32596 ||| 5310 ||| 2519 ||| 24731 ||| 11629 ||| 
2021 ||| disentangled attention as intrinsic regularization for bimanual multi-object manipulation. ||| 32597 ||| 32598 ||| 2342 ||| 32599 ||| 1117 ||| 
2020 ||| how far does bert look at: distance-based clustering and analysis of bert's attention. ||| 11683 ||| 11684 ||| 242 ||| 11685 ||| 11686 ||| 
2020 ||| modality-agnostic attention fusion for visual search with text feedback. ||| 32600 ||| 32601 ||| 32602 ||| 1420 ||| 32603 ||| 
2021 ||| attention-gated convolutional neural networks for off-resonance correction of spiral real-time mri. ||| 32604 ||| 14462 ||| 32605 ||| 
2021 ||| switch transformers: scaling to trillion parameter models with simple and efficient sparsity. ||| 32606 ||| 2464 ||| 9132 ||| 
2018 ||| what increases (social) media attention: research impact, author prominence or title attractiveness? ||| 15797 ||| 32607 ||| 32608 ||| 15796 ||| 32609 ||| 
2022 ||| the nlp task effectiveness of long-range transformers. ||| 32610 ||| 32611 ||| 32612 ||| 
2022 ||| when shift operation meets vision transformer: an extremely simple alternative to attention mechanism. ||| 7911 ||| 23303 ||| 32613 ||| 23304 ||| 7912 ||| 
2022 ||| attention over self-attention: intention-aware re-ranking with dynamic transformer encoders for recommendation. ||| 32614 ||| 32615 ||| 32616 ||| 1133 ||| 28810 ||| 16599 ||| 
2020 ||| improving long-tail relation extraction with collaborating relation-augmented attention. ||| 438 ||| 4871 ||| 802 ||| 800 ||| 4872 ||| 4873 ||| 
2021 ||| triple attention network architecture for movieqa. ||| 32617 ||| 32618 ||| 3484 ||| 
2021 ||| understanding attention in machine reading comprehension. ||| 3642 ||| 26519 ||| 3707 ||| 3311 ||| 11254 ||| 
2017 ||| hashgan: attention-aware deep adversarial hashing for cross modal retrieval. ||| 1325 ||| 32619 ||| 1685 ||| 8589 ||| 1717 ||| 32620 ||| 24823 ||| 1728 ||| 
2019 ||| conditionally learn to pay attention for sequential visual task. ||| 532 ||| 31831 ||| 241 ||| 
2019 ||| a symmetric equilibrium generative adversarial network with attention refine block for retinal vessel segmentation. ||| 32621 ||| 32622 ||| 32623 ||| 22485 ||| 32624 ||| 32625 ||| 
2020 ||| volumetric transformer networks. ||| 8797 ||| 8798 ||| 8799 ||| 8505 ||| 
2019 ||| language identification on massive datasets of short message using an attention mechanism cnn. ||| 28080 ||| 28081 ||| 
2021 ||| conversational question answering over knowledge graphs with transformer and graph attention networks. ||| 14084 ||| 14083 ||| 14085 ||| 14086 ||| 3581 ||| 24939 ||| 
2020 ||| resnest: split-attention networks. ||| 17750 ||| 7943 ||| 32626 ||| 2422 ||| 32627 ||| 32628 ||| 24979 ||| 2448 ||| 32629 ||| 2644 ||| 3046 ||| 18298 ||| 
2020 ||| sparse and continuous attention mechanisms. ||| 3369 ||| 3370 ||| 9367 ||| 2871 ||| 7901 ||| 9210 ||| 5335 ||| 9368 ||| 7902 ||| 
2021 ||| parallel scale-wise attention network for effective scene text recognition. ||| 388 ||| 389 ||| 390 ||| 391 ||| 392 ||| 
2022 ||| mutual attention-based hybrid dimensional network for multimodal imaging computer-aided diagnosis. ||| 32630 ||| 32631 ||| 32632 ||| 11391 ||| 
2018 ||| visual attention driven by convolutional features. ||| 896 ||| 898 ||| 
2021 ||| sparsebert: rethinking the importance analysis in self-attention. ||| 22739 ||| 22740 ||| 22741 ||| 1687 ||| 1686 ||| 22742 ||| 32633 ||| 
2021 ||| tensor-to-image: image-to-image translation with vision transformers. ||| 32634 ||| 29583 ||| 
2021 ||| attention models for point clouds in deep learning: a survey. ||| 8012 ||| 14114 ||| 17571 ||| 128 ||| 14115 ||| 
2021 ||| semi-supervised music tagging transformer. ||| 11915 ||| 11916 ||| 11917 ||| 
2019 ||| learning attention-based embeddings for relation prediction in knowledge graphs. ||| 3056 ||| 3057 ||| 3058 ||| 3059 ||| 
2020 ||| talking-heads attention. ||| 9132 ||| 26719 ||| 32635 ||| 26721 ||| 32636 ||| 
2020 ||| from zero to hero: on the limitations of zero-shot cross-lingual transfer with multilingual transformers. ||| 26414 ||| 24929 ||| 15106 ||| 15105 ||| 
2021 ||| trackformer: multi-object tracking with transformers. ||| 32637 ||| 8684 ||| 2575 ||| 2031 ||| 
2018 ||| learning context-sensitive time-decay attention for role-based dialogue modeling. ||| 4841 ||| 4842 ||| 4843 ||| 
2019 ||| the evolved transformer. ||| 22752 ||| 17950 ||| 9372 ||| 
2021 ||| point cloud segmentation using sparse temporal local attention. ||| 32638 ||| 32639 ||| 11331 ||| 11330 ||| 
2021 ||| multimodal end-to-end group emotion recognition using cross-modal attention. ||| 32640 ||| 
2019 ||| sequential recommendation with relation-aware kernelized self-attention. ||| 17789 ||| 17966 ||| 17967 ||| 17968 ||| 17969 ||| 
2020 ||| attention-based multi-modal fusion network for semantic scene completion. ||| 18040 ||| 18041 ||| 18042 ||| 18043 ||| 820 ||| 
2021 ||| tent: tensorized encoder transformer for temperature forecasting. ||| 32641 ||| 32642 ||| 32643 ||| 27268 ||| 
2020 ||| crisisbert: a robust transformer for crisis classification and contextual crisis embedding. ||| 17228 ||| 27140 ||| 27141 ||| 27142 ||| 27143 ||| 15208 ||| 
2018 ||| self-attention equipped graph convolutions for disease prediction. ||| 15676 ||| 15677 ||| 15678 ||| 15679 ||| 15680 ||| 13628 ||| 
2021 ||| trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer. ||| 27916 ||| 27917 ||| 32644 ||| 27446 ||| 8637 ||| 
2022 ||| correcting diacritics and typos with byt5 transformer model. ||| 23285 ||| 23286 ||| 32645 ||| 32646 ||| 16289 ||| 
2019 ||| voice transformer network: sequence-to-sequence voice conversion using transformer with text-to-speech pretraining. ||| 14450 ||| 8223 ||| 14451 ||| 12538 ||| 12130 ||| 
2021 ||| cmsaone@dravidian-codemix-fire2020: a meta embedding and transformer model for code-mixed sentiment analysis on social media text. ||| 14016 ||| 10510 ||| 
2019 ||| attention enriched deep learning model for breast tumor segmentation in ultrasound images. ||| 32647 ||| 32648 ||| 32649 ||| 
2021 ||| mergebert: program merge conflict resolution via neural transformers. ||| 15264 ||| 32650 ||| 32651 ||| 32652 ||| 32653 ||| 32654 ||| 15265 ||| 32655 ||| 
2018 ||| more specificity, more attention to social context: reframing how we address "bad actors". ||| 13287 ||| 
2018 ||| convolutional spatial attention model for reading comprehension with multiple-choice questions. ||| 3643 ||| 3642 ||| 17877 ||| 1308 ||| 3645 ||| 
2020 ||| sentence level human translation quality estimation with attention-based neural networks. ||| 21676 ||| 15080 ||| 
2018 ||| factorized attention: self-attention with linear complexities. ||| 7265 ||| 7266 ||| 1944 ||| 2391 ||| 7267 ||| 
2019 ||| fingerspelling recognition in the wild with iterative visual attention. ||| 2150 ||| 2151 ||| 2152 ||| 2153 ||| 2154 ||| 2155 ||| 
2022 ||| quantum expectation transformers for cost analysis. ||| 32656 ||| 32657 ||| 32658 ||| 32659 ||| 32660 ||| 32661 ||| 
2018 ||| end-to-end audio visual scene-aware dialog using multimodal attention-based video features. ||| 2507 ||| 12020 ||| 7284 ||| 12021 ||| 2508 ||| 7283 ||| 2512 ||| 12022 ||| 12023 ||| 12024 ||| 12025 ||| 8564 ||| 8567 ||| 
2021 ||| a new gastric histopathology subsize image database (gashissdb) for classification algorithm test: from linear regression to visual transformer. ||| 8841 ||| 399 ||| 5378 ||| 13789 ||| 423 ||| 32662 ||| 32663 ||| 13788 ||| 15956 ||| 
2022 ||| global2local: a joint-hierarchical attention for video captioning. ||| 32664 ||| 17673 ||| 2504 ||| 2367 ||| 2364 ||| 6831 ||| 
2021 ||| multi-modal perception attention network with self-supervised learning for audio-visual speaker tracking. ||| 32665 ||| 2519 ||| 435 ||| 
2020 ||| purifying real images with an attention-guided style transfer network for gaze estimation. ||| 31568 ||| 17129 ||| 31569 ||| 31570 ||| 20653 ||| 
2021 ||| ssan: separable self-attention network for video representation learning. ||| 5079 ||| 19061 ||| 19062 ||| 
2021 ||| a multi-modal transformer-based code summarization approach for smart contracts. ||| 5937 ||| 5971 ||| 5972 ||| 737 ||| 5973 ||| 5974 ||| 2068 ||| 
2022 ||| multi-head temporal attention-augmented bilinear network for financial time series prediction. ||| 27251 ||| 31737 ||| 32666 ||| 31738 ||| 926 ||| 
2021 ||| ow-detr: open-world detection transformer. ||| 32667 ||| 32583 ||| 32668 ||| 1969 ||| 1972 ||| 1752 ||| 
2022 ||| dit: self-supervised pre-training for document image transformer. ||| 32669 ||| 32670 ||| 26774 ||| 9837 ||| 12599 ||| 3174 ||| 
2021 ||| faketransformer: exposing face forgery from spatial-temporal representation modeled by facial pixel variations. ||| 32671 ||| 27228 ||| 27224 ||| 10429 ||| 32672 ||| 
2018 ||| modality attention for end-to-end audio-visual speech recognition. ||| 12300 ||| 12301 ||| 5110 ||| 12302 ||| 4459 ||| 
2021 ||| skeletal graph self-attention: embedding a skeleton inductive bias into sign language production. ||| 8528 ||| 8529 ||| 7442 ||| 8530 ||| 
2021 ||| topological attention for time series forecasting. ||| 32673 ||| 32674 ||| 32675 ||| 32676 ||| 
2019 ||| image captioning with weakly-supervised attention penalty. ||| 7289 ||| 7288 ||| 7292 ||| 7290 ||| 
2019 ||| promoting the knowledge of source syntax in transformer nmt is not needed. ||| 29700 ||| 21419 ||| 7440 ||| 21421 ||| 
2021 ||| the cat set on the mat: cross attention for set matching in bipartite hypergraphs. ||| 32677 ||| 32678 ||| 32679 ||| 7043 ||| 
2022 ||| dual-flattening transformers through decomposed row and column queries for semantic segmentation. ||| 7830 ||| 32680 ||| 19595 ||| 32681 ||| 12569 ||| 7380 ||| 
2022 ||| multi-dimensional model compression of vision transformer. ||| 12151 ||| 12153 ||| 
2021 ||| hformer: hybrid cnn-transformer for fringe order prediction in phase unwrapping of fringe projection. ||| 32682 ||| 32683 ||| 32684 ||| 32685 ||| 32686 ||| 
2022 ||| cp-vit: cascade vision transformer pruning via progressive sparsity prediction. ||| 32687 ||| 32688 ||| 9883 ||| 2336 ||| 25118 ||| 32689 ||| 
2021 ||| elsa: enhanced local self-attention for vision transformer. ||| 32576 ||| 1703 ||| 1704 ||| 6658 ||| 1705 ||| 32211 ||| 
2021 ||| dpt-fsnet: dual-path transformer based full-band and sub-band fusion network for speech enhancement. ||| 32690 ||| 32691 ||| 12104 ||| 
2022 ||| swinunet3d - a hierarchical architecture for deep traffic prediction using shifted window transformers. ||| 1401 ||| 1402 ||| 1403 ||| 
2019 ||| 3dviewgraph: learning global features for 3d shapes from a graph of unordered views with attention. ||| 2563 ||| 23402 ||| 23403 ||| 2559 ||| 18230 ||| 15206 ||| 
2018 ||| hierarchical lstms with adaptive attention for visual captioning. ||| 9576 ||| 17738 ||| 1039 ||| 1040 ||| 
2021 ||| super-resolution-based change detection network with stacked attention module for images with different resolutions. ||| 6918 ||| 6919 ||| 32692 ||| 32693 ||| 29235 ||| 17760 ||| 
2020 ||| insertion-deletion transformer. ||| 32694 ||| 22755 ||| 32695 ||| 14276 ||| 
2020 ||| transmodality: an end2end fusion method with transformer for multimodal sentiment analysis. ||| 8950 ||| 8951 ||| 3121 ||| 
2021 ||| systematic generalization with edge transformers. ||| 32696 ||| 32697 ||| 32698 ||| 
2021 ||| pica: a pixel correlation-based attentional black-box adversarial attack. ||| 5894 ||| 13570 ||| 5755 ||| 800 ||| 382 ||| 
2020 ||| tatl at w-nut 2020 task 2: a transformer-based baseline system for identification of informative covid-19 english tweets. ||| 24748 ||| 
2021 ||| transformer in transformer. ||| 19744 ||| 32699 ||| 32700 ||| 19745 ||| 1688 ||| 19362 ||| 
2021 ||| multi-modal trajectory prediction for autonomous driving with semantic map and dynamic graph attention network. ||| 17092 ||| 5170 ||| 32701 ||| 32702 ||| 32703 ||| 32704 ||| 32705 ||| 
2018 ||| attend and rectify: a gated attention mechanism for fine-grained recovery. ||| 8668 ||| 6235 ||| 8669 ||| 8670 ||| 8671 ||| 8672 ||| 8048 ||| 
2021 ||| abusive language detection in heterogeneous contexts: dataset collection and the role of supervised attention. ||| 17911 ||| 17912 ||| 17913 ||| 17914 ||| 17915 ||| 17916 ||| 
2020 ||| unit test case generation with transformers. ||| 32706 ||| 15263 ||| 15264 ||| 26013 ||| 15265 ||| 
2021 ||| tiled squeeze-and-excite: channel attention with local spatial context. ||| 7881 ||| 7882 ||| 7883 ||| 
2021 ||| passive attention in artificial neural networks predicts human visual selectivity. ||| 32707 ||| 32708 ||| 26314 ||| 32709 ||| 26316 ||| 32710 ||| 
2020 ||| efficient image super-resolution using pixel attention. ||| 8550 ||| 8551 ||| 8552 ||| 2149 ||| 6964 ||| 
2021 ||| raga: relation-aware graph attention networks for global entity alignment. ||| 15231 ||| 2882 ||| 2885 ||| 
2019 ||| muse: parallel multi-scale attention for sequence to sequence learning. ||| 32711 ||| 3751 ||| 32712 ||| 30044 ||| 32713 ||| 
2019 ||| mrnn: a multi-resolution neural network with duplex attention for document retrieval in the context of question answering. ||| 32714 ||| 6278 ||| 
2018 ||| attentional road safety networks. ||| 11602 ||| 11603 ||| 11604 ||| 11605 ||| 
2021 ||| self-supervised video transformer. ||| 32715 ||| 32716 ||| 32177 ||| 1972 ||| 32717 ||| 
2021 ||| information bottleneck approach to spatial attention learning. ||| 23332 ||| 11641 ||| 23333 ||| 23334 ||| 23335 ||| 23336 ||| 
2020 ||| long range arena: a benchmark for efficient transformers. ||| 1398 ||| 2293 ||| 3189 ||| 24044 ||| 3292 ||| 9234 ||| 4840 ||| 1041 ||| 3671 ||| 3294 ||| 
2021 ||| learning hierarchical attention for weakly-supervised chest x-ray abnormality localization and diagnosis. ||| 32718 ||| 1745 ||| 1744 ||| 2441 ||| 15645 ||| 27596 ||| 577 ||| 15532 ||| 
2022 ||| entropy-based attention regularization frees unintended bias mitigation from lists. ||| 32719 ||| 32720 ||| 24766 ||| 32721 ||| 
2020 ||| attend to the beginning: a study on using bidirectional attention for extractive summarization. ||| 3954 ||| 3955 ||| 
2020 ||| unsupervised sparse-view backprojection via convolutional and spatial transformer networks. ||| 32722 ||| 32723 ||| 
2021 ||| sequential attention module for natural language processing. ||| 32724 ||| 24048 ||| 32725 ||| 10459 ||| 10460 ||| 
2021 ||| masked-attention mask transformer for universal image segmentation. ||| 32726 ||| 2055 ||| 8566 ||| 8684 ||| 1663 ||| 
2021 ||| delayed propagation transformer: a universal computation engine towards practical control in cyber-physical systems. ||| 32727 ||| 32728 ||| 2792 ||| 32729 ||| 7195 ||| 
2020 ||| a survey on visual transformer. ||| 19744 ||| 19362 ||| 19361 ||| 8687 ||| 19745 ||| 19365 ||| 32730 ||| 32699 ||| 1688 ||| 32731 ||| 32732 ||| 32733 ||| 1756 ||| 
2019 ||| attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems. ||| 11994 ||| 10650 ||| 10652 ||| 3676 ||| 10654 ||| 
2022 ||| denseunets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. ||| 32734 ||| 3882 ||| 32735 ||| 32736 ||| 32737 ||| 32738 ||| 32739 ||| 32740 ||| 
2021 ||| "killing me" is not a spoiler: spoiler detection model using graph neural networks with dependency relation-aware attention mechanism. ||| 15228 ||| 24957 ||| 9752 ||| 9250 ||| 
2021 ||| trans2seg: transparent object segmentation with transformer. ||| 2007 ||| 23411 ||| 2006 ||| 23412 ||| 1687 ||| 2010 ||| 2011 ||| 
2018 ||| long short-term attention. ||| 2817 ||| 10061 ||| 10062 ||| 
2021 ||| trtr: visual tracking with transformer. ||| 32741 ||| 32742 ||| 32743 ||| 
2018 ||| edge attention-based multi-relational graph convolutional networks. ||| 
2017 ||| video fill in the blank using lr/rl lstms with spatial-temporal attentions. ||| 1750 ||| 1751 ||| 1752 ||| 
2019 ||| deep bidirectional transformers for relation extraction without supervision. ||| 85 ||| 86 ||| 87 ||| 
2020 ||| human versus machine attention in deep reinforcement learning tasks. ||| 8824 ||| 1748 ||| 32744 ||| 32745 ||| 8829 ||| 8830 ||| 32746 ||| 
2021 ||| improving super-resolution performance using meta-attention layers. ||| 32747 ||| 32748 ||| 32749 ||| 32750 ||| 32751 ||| 
2021 ||| m3detr: multi-representation, multi-scale, mutual-relation 3d object detection with transformers. ||| 7311 ||| 1224 ||| 7312 ||| 7313 ||| 7314 ||| 1948 ||| 7315 ||| 
2020 ||| attention-gating for improved radio galaxy classification. ||| 32752 ||| 32753 ||| 32754 ||| 32755 ||| 32756 ||| 
2018 ||| attention-based walking gait and direction recognition in wi-fi networks. ||| 
2021 ||| shaq: single headed attention with quasi-recurrence. ||| 32757 ||| 32758 ||| 32759 ||| 32760 ||| 
2021 ||| hard-attention for scalable image classification. ||| 32761 ||| 32762 ||| 32763 ||| 
2019 ||| can attention masks improve adversarial robustness? ||| 32764 ||| 32765 ||| 32766 ||| 32767 ||| 9532 ||| 
2019 ||| hierarchically-refined label attention network for sequence labeling. ||| 26591 ||| 3289 ||| 
2021 ||| efficient-capsnet: capsule network with self-attention routing. ||| 30173 ||| 30602 ||| 30175 ||| 
2021 ||| cpt: a pre-trained unbalanced transformer for both chinese language understanding and generation. ||| 4969 ||| 32768 ||| 32769 ||| 32770 ||| 20341 ||| 32771 ||| 19081 ||| 3272 ||| 
2022 ||| convolutional self-attention-based multi-user mimo demapper. ||| 32772 ||| 32773 ||| 32774 ||| 32775 ||| 
2019 ||| one-stage inpainting with bilateral attention and pyramid filling block. ||| 2618 ||| 170 ||| 471 ||| 497 ||| 
2021 ||| non-invasive self-attention for side information fusion in sequential recommendation. ||| 748 ||| 13455 ||| 13454 ||| 13456 ||| 6053 ||| 13458 ||| 
2022 ||| boat: bilateral local attention vision transformer. ||| 1327 ||| 2140 ||| 977 ||| 1801 ||| 
2020 ||| uncertainty-aware attention graph neural network for defending adversarial attacks. ||| 18021 ||| 18022 ||| 369 ||| 18023 ||| 
2021 ||| listening to the city, attentively: a spatio-temporal attention boosted autoencoder for the short-term flow prediction problem. ||| 32776 ||| 32777 ||| 32778 ||| 
2018 ||| triplet network with attention for speaker diarization. ||| 14421 ||| 14422 ||| 12005 ||| 12252 ||| 12006 ||| 
2018 ||| an analysis of attention mechanisms: the case of word sense disambiguation in neural machine translation. ||| 21389 ||| 3847 ||| 21390 ||| 
2021 ||| multi-modal transformers excel at class-agnostic object detection. ||| 32779 ||| 32780 ||| 1969 ||| 1972 ||| 1971 ||| 7143 ||| 
2021 ||| learning to shift attention for motion generation. ||| 32781 ||| 1958 ||| 32782 ||| 
2021 ||| nlp-iis@ut at semeval-2021 task 4: machine reading comprehension using the long document transformer. ||| 10436 ||| 10437 ||| 10438 ||| 10439 ||| 10440 ||| 
2021 ||| streaming transformer transducer based speech recognition using non-causal convolution. ||| 11974 ||| 11976 ||| 32783 ||| 11979 ||| 12447 ||| 5450 ||| 12304 ||| 1424 ||| 32784 ||| 32785 ||| 14661 ||| 12622 ||| 
2020 ||| attention model enhanced network for classification of breast cancer image. ||| 32786 ||| 32787 ||| 13203 ||| 16741 ||| 13205 ||| 
2021 ||| class semantics-based attention for action detection. ||| 2584 ||| 2585 ||| 2586 ||| 2587 ||| 2588 ||| 2589 ||| 
2021 ||| scenes and surroundings: scene graph generation using relation transformer. ||| 32497 ||| 32498 ||| 15822 ||| 
2020 ||| adaptive attention span in computer vision. ||| 32788 ||| 32789 ||| 32790 ||| 
2021 ||| a volumetric transformer for accurate 3d tumor segmentation. ||| 32791 ||| 32792 ||| 32793 ||| 32794 ||| 2131 ||| 
2021 ||| attend2pack: bin packing through deep reinforcement learning with attention. ||| 18878 ||| 32795 ||| 32796 ||| 
2021 ||| feature pyramid network with multi-head attention for semantic segmentation of fine-resolution remotely sensed images. ||| 8207 ||| 30454 ||| 30455 ||| 
2021 ||| duality temporal-channel-frequency attention enhanced speaker representation learning. ||| 254 ||| 13930 ||| 12384 ||| 
2021 ||| transmef: a transformer-based multi-exposure image fusion framework using self-supervised multi-task learning. ||| 32797 ||| 32798 ||| 32799 ||| 27524 ||| 
2017 ||| task-driven visual saliency and attention-based visual question answering. ||| 23481 ||| 23482 ||| 23483 ||| 7654 ||| 
2021 ||| hift: hierarchical feature transformer for aerial tracking. ||| 2604 ||| 2605 ||| 2606 ||| 2607 ||| 2608 ||| 
2019 ||| extracting multiple-relations in one-pass with pre-trained transformers. ||| 3091 ||| 3092 ||| 3093 ||| 3094 ||| 3095 ||| 3096 ||| 3097 ||| 3098 ||| 
2019 ||| positional attention-based frame identification with bert: a deep learning approach to target disambiguation and semantic frame selection. ||| 32800 ||| 32801 ||| 
2021 ||| end-to-end prostate cancer detection in bpmri via 3d cnns: effect of attention mechanisms, clinical priori and decoupled false positive reduction. ||| 31208 ||| 31209 ||| 31210 ||| 
2021 ||| no-reference image quality assessment via transformers, relative ranking, and self-consistency. ||| 7443 ||| 7444 ||| 7445 ||| 
2021 ||| inducing meaningful units from character sequences with slot attention. ||| 32802 ||| 3672 ||| 
2021 ||| swin-unet: unet-like pure transformer for medical image segmentation. ||| 32803 ||| 32804 ||| 32805 ||| 32806 ||| 15595 ||| 2398 ||| 32799 ||| 
2020 ||| multi-scale transformer language models. ||| 26653 ||| 32807 ||| 32808 ||| 32809 ||| 
2018 ||| paccmann: prediction of anticancer compound sensitivity with multi-modal attention-based neural networks. ||| 32810 ||| 32811 ||| 17954 ||| 32812 ||| 32813 ||| 32814 ||| 6235 ||| 10907 ||| 32815 ||| 32816 ||| 4046 ||| 
2020 ||| hierarchical transformer for task oriented dialog systems. ||| 4971 ||| 4972 ||| 4973 ||| 
2017 ||| mining significant microblogs for misinformation identification: an attention-based approach. ||| 1073 ||| 32817 ||| 1075 ||| 10429 ||| 
2021 ||| trear: transformer-based rgb-d egocentric action recognition. ||| 5199 ||| 20002 ||| 1703 ||| 29034 ||| 2365 ||| 20005 ||| 
2021 ||| using transformer based ensemble learning to classify scientific articles. ||| 15186 ||| 15187 ||| 
2021 ||| attention-guided image compression by deep reconstruction of compressive sensed saliency skeleton. ||| 1325 ||| 19165 ||| 
2021 ||| a latent transformer for disentangled and identity-preserving face editing. ||| 1668 ||| 1669 ||| 1670 ||| 1671 ||| 
2020 ||| neural arbitrary style transfer for portrait images using the attention mechanism. ||| 32818 ||| 32819 ||| 
2020 ||| transformer reasoning network for image-text matching and retrieval. ||| 2689 ||| 2691 ||| 20143 ||| 2690 ||| 
2021 ||| lightweight transformer in federated setting for human activity recognition. ||| 32820 ||| 32821 ||| 32822 ||| 6709 ||| 32823 ||| 32824 ||| 
2019 ||| fan: focused attention networks. ||| 32825 ||| 32826 ||| 32827 ||| 32828 ||| 32829 ||| 
2021 ||| transmask: a compact and fast speech separation model based on transformer. ||| 12684 ||| 12685 ||| 12686 ||| 
2021 ||| parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. ||| 3670 ||| 3671 ||| 2293 ||| 3672 ||| 
2021 ||| sscan: a spatial-spectral cross attention network for hyperspectral image denoising. ||| 5514 ||| 32830 ||| 18085 ||| 20135 ||| 19756 ||| 32831 ||| 
2018 ||| residential transformer overloading risk assessment using clustering analysis. ||| 15278 ||| 32832 ||| 32833 ||| 
2020 ||| difference attention based error correction lstm model for time series prediction. ||| 32834 ||| 32835 ||| 32836 ||| 
2020 ||| higher order linear transformer. ||| 21860 ||| 
2020 ||| multimodal spatial attention module for targeting multimodal pet-ct lung tumor segmentation. ||| 31131 ||| 14038 ||| 31132 ||| 31133 ||| 14039 ||| 
2022 ||| parallel spatio-temporal attention-based tcn for multivariate time series prediction. ||| 32837 ||| 19919 ||| 32838 ||| 32839 ||| 32840 ||| 
2021 ||| pale transformer: a general vision transformer backbone with pale-shaped attention. ||| 32566 ||| 19088 ||| 32841 ||| 2323 ||| 
2021 ||| cap-gan: towards adversarial robustness with cycle-consistent attentional purification. ||| 751 ||| 752 ||| 753 ||| 754 ||| 
2021 ||| disenkgat: knowledge graph embedding with disentangled graph attention network. ||| 1057 ||| 1058 ||| 1059 ||| 1060 ||| 1061 ||| 1062 ||| 293 ||| 1063 ||| 
2019 ||| attention-based structural-plasticity. ||| 32842 ||| 32843 ||| 32844 ||| 32845 ||| 32846 ||| 
2018 ||| to find where you talk: temporal sentence localization in video with attention based location regression. ||| 17894 ||| 2165 ||| 1088 ||| 
2020 ||| human-to-robot attention transfer for robot execution failure avoidance using stacked neural networks. ||| 26232 ||| 26233 ||| 26234 ||| 1840 ||| 
2019 ||| question classification with deep contextualized transformer. ||| 32847 ||| 32848 ||| 32849 ||| 
2020 ||| stacked debert: all attention in incomplete data for text classification. ||| 32850 ||| 488 ||| 
2022 ||| indication as prior knowledge for multimodal disease classification in chest radiographs with transformers. ||| 27387 ||| 18148 ||| 27369 ||| 13369 ||| 
2020 ||| a transformer-based embedding model for personalized product search. ||| 9571 ||| 1276 ||| 1140 ||| 
2020 ||| pedestrian tracking with gated recurrent units and attention mechanisms. ||| 22443 ||| 2393 ||| 
2021 ||| dual causal/non-causal self-attention for streaming end-to-end speech recognition. ||| 11980 ||| 2508 ||| 11981 ||| 
2021 ||| on the rate of convergence of a classifier based on a transformer encoder. ||| 3702 ||| 32851 ||| 2101 ||| 32852 ||| 32853 ||| 
2021 ||| mixed transformer u-net for medical image segmentation. ||| 32686 ||| 32854 ||| 1550 ||| 1526 ||| 6422 ||| 1528 ||| 11469 ||| 
2020 ||| towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters. ||| 25596 ||| 2605 ||| 25597 ||| 2608 ||| 19508 ||| 
2020 ||| meranet: facial micro-expression recognition using 3d residual attention network. ||| 28200 ||| 28201 ||| 28202 ||| 28203 ||| 
2021 ||| bevt: bert pretraining of video transformers. ||| 3049 ||| 2494 ||| 7314 ||| 1959 ||| 1954 ||| 2430 ||| 17842 ||| 3574 ||| 1957 ||| 
2019 ||| generating natural language explanations for visual question answering using scene graphs and visual attention. ||| 19471 ||| 32855 ||| 32856 ||| 32857 ||| 
2020 ||| unsupervised attention based instance discriminative learning for person re-identification. ||| 7219 ||| 7220 ||| 
2022 ||| fullsubnet+: channel attention fullsubnet with complex spectrograms for speech enhancement. ||| 1785 ||| 32858 ||| 14517 ||| 3138 ||| 4529 ||| 4460 ||| 
2020 ||| adaptive compact attention for few-shot video-to-video translation. ||| 32859 ||| 30312 ||| 28037 ||| 32860 ||| 32861 ||| 
2021 ||| finding strong gravitational lenses through self-attention. ||| 32862 ||| 32863 ||| 32864 ||| 
2021 ||| end-to-end attention-based image captioning. ||| 32865 ||| 32866 ||| 32867 ||| 32868 ||| 
2020 ||| hierarchical attention learning of scene flow in 3d point clouds. ||| 28417 ||| 32869 ||| 6474 ||| 25570 ||| 
2017 ||| a gru-gated attention model for neural machine translation. ||| 3180 ||| 3181 ||| 3182 ||| 
2021 ||| channel-temporal attention for first-person video domain adaptation. ||| 32870 ||| 32871 ||| 19869 ||| 32872 ||| 
2021 ||| multilingual pre-trained transformers and convolutional nn classification models for technical domain identification. ||| 14016 ||| 10510 ||| 
2020 ||| one-shot text field labeling using attention and belief propagation for structure information extraction. ||| 19462 ||| 1240 ||| 19463 ||| 1244 ||| 19464 ||| 
2017 ||| look-ahead attention for generation in neural machine translation. ||| 21182 ||| 3044 ||| 11690 ||| 
2018 ||| attention-based recurrent neural network for urban vehicle trajectory prediction. ||| 15124 ||| 15125 ||| 15126 ||| 
2020 ||| does presence of social media plugins in a journal website result in higher social media attention of its research publications. ||| 32873 ||| 32874 ||| 32875 ||| 
2018 ||| attention branch network: learning of attention mechanism for visual explanation. ||| 15475 ||| 723 ||| 724 ||| 725 ||| 
2022 ||| rice diseases detection and classification using attention based neural network and bayesian optimization. ||| 29555 ||| 3417 ||| 29556 ||| 
2021 ||| explainable health risk predictor with transformer-based medicare claim encoder. ||| 32876 ||| 32877 ||| 32878 ||| 32879 ||| 
2017 ||| attngan: fine-grained text to image generation with attentional generative adversarial networks. ||| 19025 ||| 1953 ||| 19026 ||| 14048 ||| 2044 ||| 19027 ||| 3561 ||| 
2020 ||| social commonsense reasoning with multi-head knowledge attention. ||| 26323 ||| 14228 ||| 
2019 ||| fully quantizing a simplified transformer for end-to-end speech recognition. ||| 32880 ||| 32881 ||| 32882 ||| 14629 ||| 14630 ||| 
2019 ||| character-based nmt with transformer. ||| 32883 ||| 3510 ||| 32884 ||| 32885 ||| 
2018 ||| connecting gaze, scene, and attention: generalized attention estimation via joint modeling of gaze and scene saliency. ||| 22 ||| 8690 ||| 4945 ||| 6553 ||| 8691 ||| 8692 ||| 
2020 ||| utilizing bidirectional encoder representations from transformers for answer selection. ||| 16980 ||| 10289 ||| 9606 ||| 
2021 ||| low-latency auditory spatial attention detection based on spectro-spatial features from eeg. ||| 14575 ||| 24319 ||| 24320 ||| 12494 ||| 
2021 ||| transfer: learning relation-aware facial expression representations with transformers. ||| 2321 ||| 2322 ||| 2323 ||| 
2019 ||| towards explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders. ||| 17954 ||| 32810 ||| 32811 ||| 32812 ||| 32813 ||| 32814 ||| 6235 ||| 10907 ||| 32815 ||| 32816 ||| 4046 ||| 
2020 ||| epipolar transformers. ||| 18887 ||| 10233 ||| 18888 ||| 18889 ||| 
2018 ||| attention mechanism in speaker recognition: what does it learn in deep speaker embedding? ||| 25667 ||| 25668 ||| 14562 ||| 25669 ||| 25670 ||| 
2018 ||| toward extractive summarization of online forum discussions via hierarchical attention networks. ||| 3960 ||| 523 ||| 3961 ||| 
2021 ||| wsgat: weighted and signed graph attention networks for link prediction. ||| 32886 ||| 32887 ||| 
2021 ||| civil rephrases of toxic texts with self-supervised transformers. ||| 24914 ||| 24915 ||| 24916 ||| 24917 ||| 
2019 ||| sequence to sequence with attention for influenza prevalence prediction using google trends. ||| 32888 ||| 32889 ||| 32890 ||| 
2019 ||| contextualized sparse representation with rectified n-gram attention for open-domain question answering. ||| 32891 ||| 24051 ||| 4765 ||| 9250 ||| 
2020 ||| blur-attention: a boosting mechanism for non-uniform blurred image restoration. ||| 13455 ||| 18087 ||| 19991 ||| 28786 ||| 29187 ||| 
2020 ||| compressing transformer-based semantic parsing models using compositional code embeddings. ||| 26635 ||| 26636 ||| 26637 ||| 26638 ||| 26639 ||| 26640 ||| 
2020 ||| texture transform attention for realistic image inpainting. ||| 32892 ||| 18834 ||| 18837 ||| 
2021 ||| self-attention based anchor proposal for skeleton-based action recognition. ||| 32893 ||| 30281 ||| 
2020 ||| gobo: quantizing attention-based nlp models for low latency and energy efficient inference. ||| 21086 ||| 21089 ||| 
2021 ||| sotr: segmenting objects with transformers. ||| 1964 ||| 1965 ||| 1966 ||| 1967 ||| 
2021 ||| graph kernel attention transformers. ||| 22791 ||| 32894 ||| 32895 ||| 32896 ||| 
2020 ||| context-transformer: tackling object confusion for few-shot detection. ||| 4117 ||| 2148 ||| 17672 ||| 2363 ||| 2149 ||| 
2022 ||| ssha: video violence recognition and localization using a semi-supervised hard attention model. ||| 32897 ||| 32898 ||| 
2018 ||| graph convolutional neural networks via motif-based attention. ||| 9407 ||| 215 ||| 18200 ||| 18201 ||| 26422 ||| 
2018 ||| skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention. ||| 20020 ||| 20021 ||| 17293 ||| 
2021 ||| exploring separable attention for multi-contrast mr image super-resolution. ||| 27732 ||| 27733 ||| 28533 ||| 4056 ||| 1125 ||| 1932 ||| 
2021 ||| context-aware attentional pooling (cap) for fine-grained visual classification. ||| 6383 ||| 6384 ||| 6385 ||| 18075 ||| 
2022 ||| learning semantics for visual place recognition through multi-scale attention. ||| 32899 ||| 32900 ||| 32901 ||| 32902 ||| 32903 ||| 
2018 ||| pay more attention - neural architectures for question-answering. ||| 32904 ||| 32905 ||| 
2021 ||| radams: resilient and adaptive alert and attention management strategy against informational denial-of-service (idos) attacks. ||| 9714 ||| 9715 ||| 
2021 ||| context transformer with stacked pointer networks for conversational question answering over knowledge graphs. ||| 14083 ||| 14084 ||| 14085 ||| 14086 ||| 3581 ||| 
2022 ||| scaling laws under the microscope: predicting transformer performance from small scale experiments. ||| 32906 ||| 32907 ||| 26356 ||| 
2020 ||| contour transformer network for one-shot segmentation of anatomical structures. ||| 8159 ||| 19070 ||| 27531 ||| 32908 ||| 27492 ||| 32909 ||| 307 ||| 705 ||| 27491 ||| 32910 ||| 32911 ||| 
2021 ||| cma-net: a cascaded mutual attention network for light field salient object detection. ||| 340 ||| 2037 ||| 21726 ||| 32912 ||| 32913 ||| 
2021 ||| spatio-temporal attention mechanism and knowledge distillation for lip reading. ||| 21068 ||| 21069 ||| 21070 ||| 21071 ||| 21072 ||| 21073 ||| 21074 ||| 
2020 ||| da-transformer: distance-aware transformer. ||| 3754 ||| 3755 ||| 2795 ||| 
2021 ||| widecaps: a wide attention based capsule network for image classification. ||| 31099 ||| 32914 ||| 32915 ||| 32916 ||| 28995 ||| 
2021 ||| oriented object detection with transformer. ||| 32917 ||| 32918 ||| 32919 ||| 2170 ||| 32920 ||| 32921 ||| 1761 ||| 7240 ||| 2442 ||| 
2021 ||| bayesian attention belief networks. ||| 9282 ||| 9281 ||| 9283 ||| 9284 ||| 
2021 ||| global-local attention for emotion recognition. ||| 32922 ||| 32923 ||| 19443 ||| 32924 ||| 
2020 ||| looking for change? roll the dice and demand attention. ||| 30380 ||| 1226 ||| 30381 ||| 30382 ||| 
2020 ||| on the computational power of transformers and its implications in sequence modeling. ||| 23083 ||| 23084 ||| 23085 ||| 
2019 ||| improving generation quality of pointer networks via guided attention. ||| 32925 ||| 4856 ||| 3515 ||| 
2019 ||| sid4vam: a benchmark dataset with synthetic images for visual attention modeling. ||| 1864 ||| 1865 ||| 1866 ||| 32926 ||| 1869 ||| 1865 ||| 1870 ||| 
2018 ||| aspect term extraction with history attention and selective transformation. ||| 633 ||| 3385 ||| 23337 ||| 3015 ||| 23338 ||| 
2021 ||| neural attention-aware hierarchical topic model. ||| 26785 ||| 26786 ||| 124 ||| 26787 ||| 26788 ||| 
2020 ||| pruning redundant mappings in transformer models via spectral-normalized identity prior. ||| 26819 ||| 32927 ||| 26821 ||| 26822 ||| 4830 ||| 
2019 ||| uan: unified attention network for convolutional neural networks. ||| 21452 ||| 1850 ||| 21453 ||| 
2021 ||| rethinking spatial dimensions of vision transformers. ||| 2086 ||| 2087 ||| 2088 ||| 2089 ||| 2090 ||| 2091 ||| 
2020 ||| transformers for modeling physical systems. ||| 32928 ||| 32929 ||| 
2021 ||| analysis of graphsum's attention weights to improve the explainability of multi-document summarization. ||| 11036 ||| 11037 ||| 11038 ||| 11039 ||| 11040 ||| 11041 ||| 11042 ||| 
2021 ||| real-time instance segmentation of surgical instruments using attention and multi-scale feature fusion. ||| 32930 ||| 32931 ||| 32932 ||| 20166 ||| 
2019 ||| a transformer-based approach to irony and sarcasm detection. ||| 32933 ||| 32934 ||| 32935 ||| 
2021 ||| vision transformer for fast and efficient scene text recognition. ||| 17405 ||| 
2021 ||| giid-net: generalizable image inpainting detection via neural architecture search and attention. ||| 12427 ||| 19695 ||| 
2021 ||| demystifying the better performance of position encoding variants for transformer. ||| 26798 ||| 26799 ||| 2567 ||| 26712 ||| 9156 ||| 26800 ||| 
2021 ||| graph attention multi-layer perceptron. ||| 32936 ||| 32937 ||| 32938 ||| 32939 ||| 32940 ||| 32941 ||| 16733 ||| 12333 ||| 
2020 ||| exploring global diverse attention via pairwise temporal relation for video summarization. ||| 977 ||| 32942 ||| 32943 ||| 1722 ||| 32944 ||| 1932 ||| 
2022 ||| blue at memotion 2.0 2022: you have my image, my text and my transformer. ||| 24759 ||| 24760 ||| 32945 ||| 
2018 ||| attention driven person re-identification. ||| 1856 ||| 2379 ||| 7406 ||| 11404 ||| 11405 ||| 7204 ||| 
2019 ||| neural-attention-based deep learning architectures for modeling traffic dynamics on lane graphs. ||| 23619 ||| 23620 ||| 23621 ||| 
2018 ||| on attention modules for audio-visual synchronization. ||| 19319 ||| 19320 ||| 8728 ||| 
2017 ||| pre-training attention mechanisms. ||| 32946 ||| 
2018 ||| area attention. ||| 438 ||| 9135 ||| 22843 ||| 22844 ||| 
2022 ||| septr: separable transformer for audio spectrogram processing. ||| 32947 ||| 7356 ||| 1972 ||| 
2022 ||| training-free transformer architecture search. ||| 32948 ||| 19414 ||| 32949 ||| 1424 ||| 32950 ||| 28819 ||| 1037 ||| 2367 ||| 
2018 ||| visual attention and its intimate links to spatial cognition. ||| 29817 ||| 32361 ||| 29814 ||| 32951 ||| 
2020 ||| human-centric spatio-temporal video grounding with visual transformers. ||| 32952 ||| 32953 ||| 17854 ||| 1800 ||| 7141 ||| 32954 ||| 1695 ||| 1696 ||| 
2019 ||| comparison of neuronal attention models. ||| 32955 ||| 
2021 ||| exploiting multi-scale fusion, spatial attention and patch interaction techniques for text-independent writer identification. ||| 32956 ||| 32957 ||| 30717 ||| 
2020 ||| iscas at semeval-2020 task 5: pre-trained transformers for counterfactual statement modeling. ||| 10425 ||| 10426 ||| 10427 ||| 10428 ||| 3656 ||| 
2019 ||| attention, please! a critical review of neural attention models in natural language processing. ||| 31747 ||| 31748 ||| 31749 ||| 
2022 ||| a context-integrated transformer-based neural network for auction design. ||| 32958 ||| 32959 ||| 32960 ||| 3269 ||| 25696 ||| 9229 ||| 32961 ||| 
2021 ||| episodic transformer for vision-and-language navigation. ||| 2092 ||| 2093 ||| 2094 ||| 
2018 ||| attention-aware generalized mean pooling for image retrieval. ||| 32962 ||| 32963 ||| 32964 ||| 
2020 ||| attention cube network for image restoration. ||| 19511 ||| 6117 ||| 6116 ||| 19512 ||| 1921 ||| 
2019 ||| adaptive attention span in transformers. ||| 3797 ||| 3798 ||| 2265 ||| 1889 ||| 
2018 ||| improving the transformer translation model with document-level context. ||| 494 ||| 26833 ||| 3233 ||| 26834 ||| 17952 ||| 1254 ||| 1305 ||| 
2018 ||| attention-guided answer distillation for machine reading comprehension. ||| 970 ||| 972 ||| 3174 ||| 967 ||| 11105 ||| 9413 ||| 3480 ||| 
2020 ||| automatic detection of cardiac chambers using an attention-based yolov4 framework from four-chamber view of fetal echocardiography. ||| 32965 ||| 32966 ||| 32967 ||| 32968 ||| 12038 ||| 214 ||| 32969 ||| 32970 ||| 
2020 ||| augmenting transformers with knn-based composite memory for dialogue. ||| 23898 ||| 32971 ||| 9772 ||| 32972 ||| 32973 ||| 
2021 ||| block-skim: efficient question answering for transformer. ||| 11683 ||| 32974 ||| 11684 ||| 30691 ||| 11686 ||| 32975 ||| 
2021 ||| group-free 3d object detection via transformers. ||| 1765 ||| 1770 ||| 1767 ||| 1768 ||| 1693 ||| 
2021 ||| continuous-time sequential recommendation with temporal graph collaborative transformer. ||| 1427 ||| 1428 ||| 538 ||| 1090 ||| 1429 ||| 1094 ||| 
2017 ||| two-stream flow-guided convolutional attention networks for action recognition. ||| 7990 ||| 7991 ||| 
2021 ||| baller2vec: a multi-entity transformer for multi-agent spatiotemporal modeling. ||| 32976 ||| 19443 ||| 
2019 ||| haxmlnet: hierarchical attention network for extreme multi-label text classification. ||| 9174 ||| 9175 ||| 9177 ||| 9179 ||| 
2019 ||| spa-gan: spatial attention gan for image-to-image translation. ||| 15277 ||| 32977 ||| 15278 ||| 32978 ||| 
2020 ||| cross-lingual zero- and few-shot hate speech detection utilising frozen transformer language models and axel. ||| 12147 ||| 32979 ||| 648 ||| 649 ||| 
2019 ||| attention control with metric learning alignment for image set-based recognition. ||| 8525 ||| 19896 ||| 5379 ||| 8526 ||| 
2017 ||| multi-task coupled attentions for category-specific aspect and opinion terms co-extraction. ||| 17725 ||| 2247 ||| 3196 ||| 
2021 ||| transformer based trajectory prediction. ||| 32980 ||| 32981 ||| 32982 ||| 
2020 ||| explicitly modeling adaptive depths for transformer. ||| 18158 ||| 3075 ||| 1921 ||| 18159 ||| 18160 ||| 
2022 ||| graph attention retrospective. ||| 32983 ||| 32984 ||| 32985 ||| 32986 ||| 32987 ||| 
2020 ||| retinotopicnet: an iterative attention mechanism using local descriptors with global context. ||| 32988 ||| 32989 ||| 
2020 ||| self-supervised attention learning for depth and ego-motion estimation. ||| 25491 ||| 25492 ||| 
2020 ||| toward improving the evaluation of visual attention models: a crowdsourcing approach. ||| 896 ||| 897 ||| 898 ||| 
2021 ||| spatial attention-based non-reference perceptual quality prediction network for omnidirectional images. ||| 2884 ||| 18741 ||| 19976 ||| 19977 ||| 
2018 ||| denssiam: end-to-end densely-siamese network with self-attention model for object tracking. ||| 7846 ||| 7847 ||| 13847 ||| 
2021 ||| unsupervised out-of-domain detection via pre-trained transformers. ||| 3283 ||| 3284 ||| 3285 ||| 3286 ||| 3287 ||| 
2021 ||| kvt: k-nn attention for boosting vision transformers. ||| 1703 ||| 9464 ||| 1704 ||| 32990 ||| 32991 ||| 32992 ||| 1705 ||| 32211 ||| 
2021 ||| automated essay scoring using transformer models. ||| 32993 ||| 32994 ||| 32995 ||| 32996 ||| 32997 ||| 
2022 ||| incremental transformer structure enhanced image inpainting with masking positional encoding. ||| 32998 ||| 32999 ||| 6365 ||| 
2022 ||| vitranspad: video transformer using convolution and self-attention for face presentation attack detection. ||| 33000 ||| 33001 ||| 33002 ||| 33003 ||| 33004 ||| 17359 ||| 
2022 ||| arbitrary shape text detection using transformers. ||| 18995 ||| 18997 ||| 18999 ||| 
2019 ||| attention: a big surprise for cross-domain person re-identification. ||| 33005 ||| 2343 ||| 33006 ||| 8948 ||| 
2020 ||| moltrans: molecular interaction transformer for drug target interaction prediction. ||| 33007 ||| 1070 ||| 33008 ||| 23323 ||| 
2021 ||| co-scale conv-attentional image transformers. ||| 1812 ||| 1813 ||| 1814 ||| 1815 ||| 
2020 ||| perceive, attend, and drive: learning spatial attention for safe self-driving. ||| 21774 ||| 9220 ||| 21775 ||| 21776 ||| 10875 ||| 9239 ||| 
2021 ||| spatial-temporal transformer for dynamic scene graph generation. ||| 2406 ||| 2407 ||| 2408 ||| 2410 ||| 2409 ||| 
2021 ||| a new state-of-the-art transformers-based load forecaster on the smart grid domain. ||| 3369 ||| 33009 ||| 33010 ||| 1994 ||| 33011 ||| 33012 ||| 
2020 ||| problemconquero at semeval-2020 task 12: transformer and soft label-based approaches. ||| 10641 ||| 10642 ||| 10643 ||| 4930 ||| 
2022 ||| pretr: spatio-temporal non-autoregressive trajectory prediction transformer. ||| 33013 ||| 33014 ||| 33015 ||| 33016 ||| 1226 ||| 33017 ||| 1226 ||| 33018 ||| 
2020 ||| compressing large-scale transformer-based models: a case study on bert. ||| 33019 ||| 24503 ||| 33020 ||| 33021 ||| 25908 ||| 7500 ||| 33022 ||| 26436 ||| 7049 ||| 
2020 ||| pruning and sparsemax methods for hierarchical attention networks. ||| 1994 ||| 33023 ||| 33024 ||| 33025 ||| 
2022 ||| uvcgan: unet vision transformer cycle-consistent gan for unpaired image-to-image translation. ||| 33026 ||| 22602 ||| 33027 ||| 33028 ||| 2616 ||| 33029 ||| 33030 ||| 33031 ||| 
2019 ||| porous lattice-based transformer encoder for chinese ner. ||| 756 ||| 3326 ||| 759 ||| 379 ||| 11626 ||| 23404 ||| 
2022 ||| your "attention" deserves attention: a self-diversified multi-channel attention for facial action analysis. ||| 5710 ||| 5711 ||| 5712 ||| 5713 ||| 5714 ||| 
2022 ||| mdan: multi-level dependent attention network for visual emotion analysis. ||| 33032 ||| 33033 ||| 411 ||| 33034 ||| 
2021 ||| instance-level image retrieval using reranking transformers. ||| 2003 ||| 2004 ||| 2005 ||| 
2019 ||| spatio-temporal crop classification of low-resolution satellite imagery with capsule layers and distributed attention. ||| 33035 ||| 
2021 ||| generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. ||| 1665 ||| 1666 ||| 1667 ||| 
2021 ||| racebert - a transformer-based model for predicting race and ethnicity from names. ||| 33036 ||| 
2020 ||| learning efficient representations of mouse movements to predict user attention. ||| 9659 ||| 9660 ||| 
2022 ||| pear: personalized re-ranking with contextualized transformer for recommendation. ||| 1329 ||| 33037 ||| 33038 ||| 8443 ||| 13454 ||| 336 ||| 9065 ||| 2744 ||| 9066 ||| 
2020 ||| applying the transformer to character-level transduction. ||| 3484 ||| 3485 ||| 13266 ||| 
2021 ||| dispensed transformer network for unsupervised domain adaptation. ||| 27346 ||| 30994 ||| 33039 ||| 11634 ||| 17473 ||| 27347 ||| 1224 ||| 33040 ||| 17296 ||| 30434 ||| 27349 ||| 1052 ||| 17295 ||| 
2021 ||| transzero++: cross attribute-guided transformer for zero-shot learning. ||| 33041 ||| 33042 ||| 33043 ||| 4095 ||| 1705 ||| 33044 ||| 1728 ||| 1932 ||| 
2021 ||| sparsity and sentence structure in encoder-decoder attention of summarization systems. ||| 3795 ||| 3796 ||| 
2019 ||| knowledge-enriched transformer for emotion detection in textual conversations. ||| 738 ||| 15790 ||| 16696 ||| 
2021 ||| what's hidden in a one-layer randomly weighted transformer? ||| 3822 ||| 22812 ||| 3826 ||| 2596 ||| 22814 ||| 
2019 ||| temporal transformer networks: joint learning of invariant and discriminative time warping. ||| 18670 ||| 18671 ||| 18672 ||| 
2020 ||| a co-interactive transformer for joint slot filling and intent detection. ||| 12348 ||| 12349 ||| 3707 ||| 12350 ||| 12351 ||| 3311 ||| 
2022 ||| when transformer meets robotic grasping: exploits context for efficient grasp detection. ||| 33045 ||| 33046 ||| 33047 ||| 
2021 ||| visualsparta: sparse transformer fragment-level matching for large-scale text-to-image search. ||| 4849 ||| 4848 ||| 4850 ||| 
2021 ||| automatic detection of rail components via a deep convolutional transformer network. ||| 33048 ||| 33049 ||| 33050 ||| 33051 ||| 
2020 ||| moving target defense for robust monitoring of electric grid transformers in adversarial environments. ||| 9716 ||| 9717 ||| 9718 ||| 9719 ||| 
2019 ||| hats: a hierarchical graph attention network for stock movement prediction. ||| 9249 ||| 33052 ||| 9248 ||| 4506 ||| 2041 ||| 9250 ||| 
2018 ||| jointly multiple events extraction via attention-based graph information aggregation. ||| 2530 ||| 10612 ||| 688 ||| 
2021 ||| cross attentional audio-visual fusion for dimensional emotion recognition. ||| 33053 ||| 5748 ||| 5749 ||| 
2021 ||| can the transformer be used as a drop-in replacement for rnns in text-generating gans? ||| 14001 ||| 14002 ||| 
2019 ||| attention-based deep reinforcement learning for multi-view environments. ||| 3924 ||| 3925 ||| 3926 ||| 
2018 ||| dual recurrent attention units for visual question answering. ||| 33054 ||| 33055 ||| 
2019 ||| deraincyclegan: an attention-guided unsupervised benchmark for single image deraining and rainmaking. ||| 33056 ||| 5543 ||| 33057 ||| 602 ||| 1728 ||| 444 ||| 
2021 ||| learning dynamic graph representation of brain connectome with spatio-temporal attention. ||| 8712 ||| 2048 ||| 29806 ||| 
2018 ||| a question-focused multi-factor attention network for question answering. ||| 7327 ||| 3195 ||| 
2021 ||| decoupled spatial-temporal transformer for video inpainting. ||| 1840 ||| 1841 ||| 1842 ||| 1843 ||| 1844 ||| 1845 ||| 1846 ||| 1847 ||| 1848 ||| 
2021 ||| swin transformer v2: scaling up capacity and resolution. ||| 1765 ||| 1768 ||| 1766 ||| 8017 ||| 33058 ||| 1769 ||| 33059 ||| 1767 ||| 1770 ||| 3171 ||| 3174 ||| 1772 ||| 
2019 ||| weakly labelled audioset classification with attention neural networks. ||| 12618 ||| 13518 ||| 33060 ||| 1125 ||| 11418 ||| 12619 ||| 
2020 ||| han-ecg: an interpretable atrial fibrillation detection model using hierarchical attention networks. ||| 6124 ||| 6125 ||| 6127 ||| 
2021 ||| attention mechanisms and deep learning for machine vision: a survey of the state of the art. ||| 33061 ||| 33062 ||| 33063 ||| 
2018 ||| bert: pre-training of deep bidirectional transformers for language understanding. ||| 4778 ||| 4779 ||| 4780 ||| 3185 ||| 
2020 ||| probabilistic transformers. ||| 33064 ||| 
2021 ||| visual keyword spotting with attention. ||| 33065 ||| 33066 ||| 33067 ||| 1997 ||| 
2019 ||| an end-to-end approach for lexical stress detection based on transformer. ||| 33068 ||| 33069 ||| 2519 ||| 33070 ||| 116 ||| 33071 ||| 33072 ||| 
2019 ||| learning to deceive with attention-based explanations. ||| 3064 ||| 3065 ||| 3066 ||| 3067 ||| 3068 ||| 
2020 ||| distilling knowledge from ensembles of acoustic models for joint ctc-attention end-to-end speech recognition. ||| 1446 ||| 13954 ||| 13955 ||| 
2018 ||| adaptive edge features guided graph attention networks. ||| 33073 ||| 33074 ||| 
2021 ||| tdan: top-down attention networks for enhanced feature selectivity in cnns. ||| 33075 ||| 33076 ||| 33077 ||| 
2021 ||| single-shot motion completion with transformer. ||| 33078 ||| 2186 ||| 2185 ||| 33079 ||| 33080 ||| 33081 ||| 19846 ||| 
2021 ||| cross-domain generalization and knowledge transfer in transformers trained on legal data. ||| 10173 ||| 10174 ||| 10175 ||| 10176 ||| 
2018 ||| improved training of end-to-end attention models for speech recognition. ||| 12532 ||| 12659 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| scarlet: explainable attention based graph neural network for fake news spreader prediction. ||| 15244 ||| 15245 ||| 1096 ||| 
2019 ||| non-autoregressive transformer by position learning. ||| 3427 ||| 2736 ||| 17824 ||| 3428 ||| 4845 ||| 4847 ||| 3034 ||| 
2019 ||| weakly-supervised completion moment detection using temporal attention. ||| 7792 ||| 7793 ||| 7794 ||| 
2018 ||| a discourse-aware attention model for abstractive summarization of long documents. ||| 3122 ||| 4952 ||| 4953 ||| 4954 ||| 4955 ||| 4956 ||| 4957 ||| 
2021 ||| decision transformer: reinforcement learning via sequence modeling. ||| 33082 ||| 33083 ||| 33084 ||| 33085 ||| 33086 ||| 33087 ||| 18850 ||| 18848 ||| 33088 ||| 
2021 ||| handwriting transformers. ||| 1968 ||| 1969 ||| 1970 ||| 1971 ||| 1972 ||| 1752 ||| 
2021 ||| unifying multimodal transformer for bi-directional image and text generation. ||| 19678 ||| 19679 ||| 19680 ||| 16594 ||| 
2020 ||| vstreamdrls: dynamic graph representation learning with self-attention for enterprise distributed video streaming solutions. ||| 17105 ||| 17106 ||| 
2018 ||| discovery and usage of joint attention in images. ||| 33089 ||| 33090 ||| 33091 ||| 
2021 ||| distantly supervised transformers for e-commerce product qa. ||| 4985 ||| 4986 ||| 4987 ||| 4988 ||| 4989 ||| 
2021 ||| co-training transformer with videos and images improves action recognition. ||| 7468 ||| 12814 ||| 33092 ||| 14543 ||| 18106 ||| 3336 ||| 17735 ||| 
2021 ||| turn-to-diarize: online speaker diarization constrained by transformer transducer speaker turn detection. ||| 33093 ||| 12612 ||| 12652 ||| 12614 ||| 12653 ||| 12613 ||| 
2020 ||| linguistically-aware attention for reducing the semantic-gap in vision-language tasks. ||| 33094 ||| 1909 ||| 33095 ||| 1910 ||| 
2021 ||| readnet: a hierarchical transformer framework for web article readability analysis. ||| 15098 ||| 15099 ||| 15100 ||| 15101 ||| 
2021 ||| object based attention through internal gating. ||| 33096 ||| 33097 ||| 33098 ||| 
2020 ||| xd at semeval-2020 task 12: ensemble approach to offensive language identification in social media using transformer encoders. ||| 10453 ||| 375 ||| 
2021 ||| lotr: face landmark localization using localization transformer. ||| 33099 ||| 33100 ||| 33101 ||| 33102 ||| 33103 ||| 33104 ||| 33105 ||| 33106 ||| 33107 ||| 
2020 ||| permutationless many-jet event reconstruction with symmetry preserving attention networks. ||| 33108 ||| 33109 ||| 33110 ||| 33111 ||| 33112 ||| 4311 ||| 
2020 ||| attention over parameters for dialogue systems. ||| 10651 ||| 10652 ||| 26691 ||| 10653 ||| 10654 ||| 
2022 ||| mhsnet: multi-head and spatial attention network with false-positive reduction for pulmonary nodules detection. ||| 16868 ||| 33113 ||| 33114 ||| 16867 ||| 33115 ||| 33116 ||| 14965 ||| 33117 ||| 33118 ||| 16869 ||| 11466 ||| 33119 ||| 33120 ||| 1241 ||| 33121 ||| 
2022 ||| spatio-temporal tuples transformer for skeleton-based action recognition. ||| 33122 ||| 6829 ||| 33123 ||| 6495 ||| 
2020 ||| attviz: online exploration of self-attention for transparent neural language modeling. ||| 10213 ||| 24950 ||| 24949 ||| 24951 ||| 24948 ||| 24947 ||| 
2017 ||| delineation of skin strata in reflectance confocal microscopy images using recurrent convolutional networks with toeplitz attention. ||| 33124 ||| 96 ||| 33125 ||| 31427 ||| 33126 ||| 33127 ||| 33128 ||| 14091 ||| 33129 ||| 
2021 ||| tsn-ca: a two-stage network with channel attention for low-light image enhancement. ||| 33130 ||| 33131 ||| 33132 ||| 33133 ||| 33134 ||| 
2018 ||| multi-pointer co-attention networks for recommendation. ||| 1398 ||| 9015 ||| 9016 ||| 
2019 ||| improving the harmony of the composite image by spatial-separated attention module. ||| 17856 ||| 17857 ||| 
2018 ||| convolutional self-attention network. ||| 
2021 ||| fast multi-resolution transformer fine-tuning for extreme multi-label text classification. ||| 31293 ||| 25413 ||| 22825 ||| 22826 ||| 
2020 ||| sequential weakly labeled multi-activity recognition and location on wearable sensors using recurrent attention network. ||| 11361 ||| 532 ||| 241 ||| 
2019 ||| improved attention models for memory augmented neural network adaptive controllers. ||| 23172 ||| 23173 ||| 23174 ||| 
2021 ||| progressively normalized self-attention network for video polyp segmentation. ||| 4054 ||| 27977 ||| 1861 ||| 4055 ||| 4056 ||| 20165 ||| 1932 ||| 
2019 ||| assessing the ability of self-attention networks to learn word order. ||| 3037 ||| 3038 ||| 3039 ||| 3040 ||| 3041 ||| 
2020 ||| cross-global attention graph kernel network prediction of drug prescription. ||| 13678 ||| 13679 ||| 9640 ||| 13680 ||| 13681 ||| 13682 ||| 
2021 ||| multi-granularity network with modal attention for dense affective understanding. ||| 33135 ||| 5744 ||| 33136 ||| 5834 ||| 2530 ||| 33137 ||| 33138 ||| 33139 ||| 
2020 ||| targeted attention attack on deep learning models in road sign recognition. ||| 33140 ||| 11493 ||| 33141 ||| 683 ||| 1756 ||| 
2019 ||| resilient combination of complementary cnn and rnn features for text classification through attention and ensembling. ||| 6975 ||| 6976 ||| 6977 ||| 6978 ||| 6979 ||| 
2020 ||| attention, please: a spatio-temporal transformer for 3d human motion prediction. ||| 13636 ||| 7482 ||| 13637 ||| 2123 ||| 
2021 ||| transformer with a mixture of gaussian keys. ||| 33142 ||| 33143 ||| 33144 ||| 33145 ||| 33146 ||| 10168 ||| 33147 ||| 33148 ||| 
2017 ||| video question answering via attribute-augmented attention network learning. ||| 7651 ||| 1306 ||| 7650 ||| 9570 ||| 7652 ||| 7654 ||| 
2022 ||| solving dynamic graph problems with multi-attention deep reinforcement learning. ||| 33149 ||| 33150 ||| 33151 ||| 33152 ||| 
2020 ||| syntactically look-ahead attention network for sentence compression. ||| 4982 ||| 14011 ||| 
2021 ||| transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence. ||| 26802 ||| 26785 ||| 26803 ||| 124 ||| 26787 ||| 26788 ||| 
2019 ||| image inpainting with learnable bidirectional attention maps. ||| 2528 ||| 2529 ||| 242 ||| 1904 ||| 2018 ||| 2530 ||| 2531 ||| 1761 ||| 
2020 ||| point transformer. ||| 23622 ||| 23623 ||| 23624 ||| 
2021 ||| heat: holistic edge attention transformer for structured reconstruction. ||| 33153 ||| 33154 ||| 33155 ||| 
2022 ||| hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism. ||| 33156 ||| 33157 ||| 33158 ||| 14797 ||| 2527 ||| 13460 ||| 
2019 ||| convolutional quantum-like language model with mutual-attention for product rating prediction. ||| 33159 ||| 33160 ||| 
2021 ||| unidrop: a simple yet effective technique to improve transformer without extra cost. ||| 4784 ||| 4785 ||| 4786 ||| 4787 ||| 4788 ||| 4789 ||| 4790 ||| 4791 ||| 
2020 ||| structured attention graphs for understanding deep image classifications. ||| 33161 ||| 33162 ||| 33163 ||| 4916 ||| 33164 ||| 
2021 ||| linear algebra with transformers. ||| 1226 ||| 33165 ||| 
2022 ||| lgt-net: indoor panoramic room layout estimation with geometry-aware transformer network. ||| 33166 ||| 33167 ||| 33168 ||| 15191 ||| 
2021 ||| word2pix: word to pixel cross attention transformer in visual grounding. ||| 33169 ||| 3389 ||| 33170 ||| 
2021 ||| long-range transformers for dynamic spatiotemporal forecasting. ||| 33171 ||| 7436 ||| 9140 ||| 
2019 ||| improving textual network embedding with global attention via optimal transport. ||| 3132 ||| 1029 ||| 3133 ||| 1031 ||| 3134 ||| 3135 ||| 3136 ||| 1709 ||| 1032 ||| 
2022 ||| give me your attention: dot-product attention considered harmful for adversarial patch robustness. ||| 33172 ||| 33173 ||| 33174 ||| 33175 ||| 33176 ||| 
2022 ||| meta-attention for vit-backed continual learning. ||| 23431 ||| 33177 ||| 16954 ||| 23433 ||| 
2021 ||| describing and localizing multiple changes with transformers. ||| 2324 ||| 2325 ||| 2326 ||| 2327 ||| 2328 ||| 2329 ||| 2330 ||| 
2021 ||| end-to-end video object detection with spatial-temporal transformers. ||| 19604 ||| 19605 ||| 19606 ||| 5948 ||| 19607 ||| 6821 ||| 8374 ||| 18501 ||| 5141 ||| 5088 ||| 
2018 ||| skeleton transformer networks: 3d human pose and skinned mesh from single rgb image. ||| 6323 ||| 6324 ||| 6325 ||| 6326 ||| 
2021 ||| attention-based multi-scale gated recurrent encoder with novel correlation loss for covid-19 progression prediction. ||| 27724 ||| 27725 ||| 27406 ||| 27408 ||| 27726 ||| 27409 ||| 27410 ||| 27411 ||| 
2019 ||| age progression and regression with spatial attention modules. ||| 6721 ||| 17944 ||| 17945 ||| 
2022 ||| multimodal depression classification using articulatory coordination features and hierarchical attention based text embeddings. ||| 33178 ||| 33179 ||| 
2020 ||| task-adaptive feature transformer for few-shot segmentation. ||| 33180 ||| 33181 ||| 33182 ||| 33183 ||| 
2021 ||| attentionlite: towards efficient self-attention models for vision. ||| 7327 ||| 12166 ||| 
2020 ||| language modelling for source code with transformer-xl. ||| 33184 ||| 8888 ||| 
2021 ||| joint aec and beamforming with double-talk detection using rnn-transformer. ||| 33185 ||| 1125 ||| 12188 ||| 14635 ||| 3808 ||| 
2021 ||| discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer. ||| 33186 ||| 17654 ||| 3836 ||| 3835 ||| 
2021 ||| siamtrans: zero-shot multi-frame image restoration with pre-trained siamese transformers. ||| 5071 ||| 19031 ||| 2363 ||| 15859 ||| 33187 ||| 2398 ||| 
2021 ||| targeted aspect based multimodal sentiment analysis: an attention capsule extraction and multi-head fusion network. ||| 33188 ||| 526 ||| 33189 ||| 527 ||| 33190 ||| 33191 ||| 525 ||| 
2020 ||| end-to-end lane shape prediction with transformers. ||| 7343 ||| 6351 ||| 7344 ||| 7345 ||| 
2018 ||| an attention-based word-level interaction model: relation detection for knowledge base question answering. ||| 26439 ||| 3894 ||| 13230 ||| 31816 ||| 12746 ||| 
2021 ||| muslcat: multi-scale multi-level convolutional attention transformer for discriminative music modeling on raw waveforms. ||| 33192 ||| 33193 ||| 33194 ||| 
2020 ||| afn: attentional feedback network based 3d terrain super-resolution. ||| 6407 ||| 6408 ||| 6409 ||| 6410 ||| 
2017 ||| vain: attentional multi-agent predictive modeling. ||| 9183 ||| 
2021 ||| don't sweep your learning rate under the rug: a closer look at cross-modal transfer of pretrained transformers. ||| 33195 ||| 33196 ||| 23900 ||| 23901 ||| 33197 ||| 
2021 ||| transformer guided geometry model for flow-based unsupervised visual odometry. ||| 5199 ||| 20002 ||| 1703 ||| 29034 ||| 2365 ||| 20005 ||| 
2017 ||| multichannel attention network for analyzing visual behavior in public speaking. ||| 7285 ||| 7286 ||| 7287 ||| 
2019 ||| relation-aware graph attention network for visual question answering. ||| 2043 ||| 2044 ||| 2045 ||| 2046 ||| 
2022 ||| do transformers encode a foundational ontology? probing abstract classes in natural language. ||| 33198 ||| 33199 ||| 3369 ||| 33200 ||| 
2019 ||| sample efficient text summarization using a single pre-trained transformer. ||| 20966 ||| 20965 ||| 3583 ||| 9135 ||| 
2021 ||| dual attention suppression attack: generate adversarial camouflage in physical world. ||| 18857 ||| 18858 ||| 18859 ||| 18860 ||| 18861 ||| 17695 ||| 
2021 ||| handwritten mathematical expression recognition via attention aggregation based bi-directional mutual learning. ||| 33201 ||| 33202 ||| 33203 ||| 33204 ||| 33205 ||| 12302 ||| 
2021 ||| hi-behrt: hierarchical transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. ||| 32310 ||| 33206 ||| 32317 ||| 32311 ||| 32313 ||| 16324 ||| 32314 ||| 33207 ||| 32316 ||| 
2021 ||| detection of winding axial deformation in power transformers by uwb radar imaging. ||| 33208 ||| 33209 ||| 33210 ||| 33211 ||| 
2020 ||| application of transformer impedance correction tables in power flow studies. ||| 33212 ||| 33213 ||| 33214 ||| 33215 ||| 33216 ||| 33217 ||| 
2019 ||| topic sensitive attention on generic corpora corrects sense bias in pretrained embeddings. ||| 3858 ||| 3859 ||| 3860 ||| 
2020 ||| fastidious attention network for navel orange segmentation. ||| 33218 ||| 33219 ||| 33220 ||| 
2021 ||| can attention enable mlps to catch up with cnns? ||| 24014 ||| 32107 ||| 32108 ||| 32111 ||| 32109 ||| 32110 ||| 
2018 ||| improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. ||| 19292 ||| 8741 ||| 
2019 ||| deep built-structure counting in satellite imagery using attention based re-weighting. ||| 33221 ||| 33222 ||| 33223 ||| 
2019 ||| region tracking in an image sequence: preventing driver inattention. ||| 33224 ||| 33225 ||| 33226 ||| 33227 ||| 
2021 ||| on the regularity of attention. ||| 33228 ||| 33229 ||| 33230 ||| 
2021 ||| stransgan: an empirical study on transformer in gans. ||| 345 ||| 2331 ||| 472 ||| 2373 ||| 2291 ||| 
2018 ||| end-to-end models with auditory attention in multi-channel keyword spotting. ||| 33231 ||| 12757 ||| 4551 ||| 
2022 ||| gcn-transformer for short-term passenger flow prediction on holidays in urban rail transit systems. ||| 33232 ||| 33233 ||| 33234 ||| 33235 ||| 
2021 ||| fine-grained visual classification of plant species in the wild: object detection as a reinforced means of attention. ||| 33236 ||| 33237 ||| 33238 ||| 33239 ||| 33240 ||| 33241 ||| 
2021 ||| multi-attention multiple instance learning. ||| 33242 ||| 33243 ||| 
2019 ||| multi-stream network with temporal attention for environmental sound classification. ||| 2419 ||| 14399 ||| 12359 ||| 
2020 ||| attention2angiogan: synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks. ||| 7798 ||| 7799 ||| 7800 ||| 7801 ||| 
2022 ||| a high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network. ||| 33244 ||| 33245 ||| 33246 ||| 33247 ||| 
2022 ||| et-bert: a contextualized datagram representation with pre-training transformers for encrypted traffic classification. ||| 33248 ||| 9543 ||| 745 ||| 5189 ||| 33249 ||| 8160 ||| 
2020 ||| beyond chemical 1d knowledge using transformers. ||| 33250 ||| 4125 ||| 4124 ||| 
2021 ||| plate: visually-grounded planning with transformers in procedural tasks. ||| 33251 ||| 33252 ||| 33253 ||| 33254 ||| 2373 ||| 33255 ||| 
2021 ||| a cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. ||| 33256 ||| 968 ||| 33257 ||| 33258 ||| 5255 ||| 22615 ||| 33259 ||| 
2022 ||| protecting celebrities with identity consistency transformer. ||| 18967 ||| 23783 ||| 2494 ||| 1349 ||| 18972 ||| 2305 ||| 6604 ||| 5909 ||| 1772 ||| 
2018 ||| describe and attend to track: learning natural language guided structural representation and visual attention for object tracking. ||| 5957 ||| 11455 ||| 11453 ||| 18733 ||| 5755 ||| 382 ||| 
2021 ||| mist-net: multi-domain integrative swin transformer network for sparse-view ct reconstruction. ||| 33260 ||| 33261 ||| 27334 ||| 27335 ||| 
2020 ||| neural retrieval for question answering with cross-attention supervised data augmentation. ||| 3786 ||| 3787 ||| 3788 ||| 3789 ||| 3790 ||| 
2021 ||| tuber: tube-transformer for action detection. ||| 33262 ||| 2419 ||| 2420 ||| 2421 ||| 2424 ||| 18223 ||| 2426 ||| 
2021 ||| transformer language models with lstm-based cross-utterance information representation. ||| 12011 ||| 8862 ||| 12012 ||| 
2022 ||| active phase-encode selection for slice-specific fast mr scanning using a transformer-based deep reinforcement learning framework. ||| 30177 ||| 2279 ||| 33263 ||| 33264 ||| 
2022 ||| tts-gan: a transformer-based time-series generative adversarial network. ||| 33265 ||| 8361 ||| 33266 ||| 33267 ||| 
2020 ||| empirical study of transformers for source code. ||| 26009 ||| 26010 ||| 
2021 ||| anticipative video transformer. ||| 1663 ||| 1664 ||| 
2021 ||| pocformer: a lightweight transformer architecture for detection of covid-19 using point of care ultrasound. ||| 11518 ||| 11519 ||| 11520 ||| 
2020 ||| spotnet: self-attention multi-task network for object detection. ||| 28013 ||| 28014 ||| 28015 ||| 28016 ||| 28017 ||| 
2021 ||| 6d-vit: category-level 6d object pose estimation via transformer-based instance representation learning. ||| 17556 ||| 17557 ||| 17558 ||| 33268 ||| 
2019 ||| an attention-based recurrent convolutional network for vehicle taillight recognition. ||| 15434 ||| 15435 ||| 15436 ||| 15437 ||| 15438 ||| 
2018 ||| document-level neural machine translation with hierarchical attention networks. ||| 26623 ||| 26624 ||| 14940 ||| 3672 ||| 
2020 ||| attention based writer independent handwriting verification. ||| 9788 ||| 9789 ||| 9790 ||| 9791 ||| 
2021 ||| reinforced attention for few-shot learning and beyond. ||| 18932 ||| 2127 ||| 18933 ||| 2814 ||| 18934 ||| 2131 ||| 2130 ||| 
2021 ||| exploring low-cost transformer model compression for large-scale commercial reply suggestions. ||| 33269 ||| 33270 ||| 33271 ||| 2180 ||| 
2021 ||| label-attention transformer with geometrically coherent objects for image captioning. ||| 33272 ||| 33273 ||| 33274 ||| 33275 ||| 33276 ||| 
2021 ||| lerna: transformer architectures for configuring error correction tools for short- and long-read genome sequencing. ||| 33277 ||| 33278 ||| 33279 ||| 23545 ||| 33280 ||| 33281 ||| 
2020 ||| multi-step joint-modality attention network for scene-aware dialogue system. ||| 33282 ||| 33283 ||| 33284 ||| 28053 ||| 
2021 ||| discrete-continuous action space policy gradient-based attention for image-text matching. ||| 13 ||| 5999 ||| 12120 ||| 
2020 ||| universal vector neural machine translation with effective attention. ||| 33285 ||| 33286 ||| 33287 ||| 33288 ||| 
2020 ||| very deep transformers for neural machine translation. ||| 24050 ||| 33289 ||| 4805 ||| 1958 ||| 
2020 ||| pyramid attention networks for image restoration. ||| 19270 ||| 19271 ||| 1730 ||| 12814 ||| 19272 ||| 3937 ||| 1734 ||| 17653 ||| 33290 ||| 
2021 ||| isegformer: interactive image segmentation with transformers. ||| 20085 ||| 
2020 ||| disease state prediction from single-cell data using graph attention networks. ||| 25987 ||| 25988 ||| 25989 ||| 25990 ||| 25991 ||| 
2020 ||| interpreting attention models with human visual attention in machine reading comprehension. ||| 9269 ||| 9270 ||| 23096 ||| 8348 ||| 20979 ||| 
2022 ||| compact bidirectional transformer for image captioning. ||| 7825 ||| 7827 ||| 33291 ||| 33292 ||| 444 ||| 
2019 ||| autodiscern: rating the quality of online health information with hierarchical encoder attention-based neural networks. ||| 33293 ||| 33294 ||| 26645 ||| 
2020 ||| prediction diversity and selective attention in the wisdom of crowds. ||| 33295 ||| 852 ||| 33296 ||| 
2020 ||| towards transparent and explainable attention models. ||| 3513 ||| 3327 ||| 3514 ||| 3328 ||| 3515 ||| 3330 ||| 
2022 ||| learning operators with coupled attention. ||| 33297 ||| 33298 ||| 33299 ||| 33300 ||| 33301 ||| 33302 ||| 
2021 ||| the nlp cookbook: modern recipes for transformer based deep learning architectures. ||| 33303 ||| 33304 ||| 
2020 ||| ma-unet: an improved version of unet based on multi-scale and attention mechanism for medical image segmentation. ||| 33305 ||| 16446 ||| 
2021 ||| cs60075_team2 at semeval-2021 task 1 : lexical complexity prediction using transformer-based language models pre-trained on various text corpora. ||| 10556 ||| 10557 ||| 10558 ||| 10559 ||| 
2021 ||| embedding calibration for music semantic similarity using auto-regressive transformer. ||| 33306 ||| 3233 ||| 33307 ||| 33308 ||| 
2020 ||| improving audio anomalies recognition using temporal convolutional attention network. ||| 12087 ||| 8233 ||| 
2021 ||| robust gps-vision localization via integrity-driven landmark attention. ||| 33309 ||| 33310 ||| 
2021 ||| dynamics of cross-platform attention to retracted papers: pervasiveness, audience skepticism, and timing of retractions. ||| 9407 ||| 9001 ||| 33311 ||| 33312 ||| 6785 ||| 
2017 ||| learning social image embedding with deep multimodal attention networks. ||| 19722 ||| 5320 ||| 843 ||| 2165 ||| 19723 ||| 19724 ||| 
2017 ||| multi-focus attention network for efficient deep reinforcement learning. ||| 18242 ||| 18243 ||| 8580 ||| 
2021 ||| traisformer-a generative transformer for ais trajectory prediction. ||| 33313 ||| 33314 ||| 
2020 ||| fetal ecg extraction from maternal ecg using attention-based asymmetric cyclegan. ||| 30988 ||| 30989 ||| 30990 ||| 30991 ||| 30992 ||| 
2022 ||| vinter: image narrative generation with emotion-arc-aware transformer. ||| 33315 ||| 33316 ||| 33317 ||| 19502 ||| 
2021 ||| picaso: permutation-invariant cascaded attentional set operator. ||| 33318 ||| 6112 ||| 
2018 ||| parameter-free spatial attention network for person re-identification. ||| 2277 ||| 33319 ||| 7805 ||| 400 ||| 19145 ||| 
2022 ||| deformable vistr: spatio temporal deformable attention for video instance segmentation. ||| 33320 ||| 33321 ||| 6336 ||| 15538 ||| 8546 ||| 
2019 ||| understanding attention in graph neural networks. ||| 2579 ||| 2582 ||| 9333 ||| 
2018 ||| larnn: linear attention recurrent neural network. ||| 33322 ||| 
2020 ||| spatio-temporal relation and attention learning for facial action unit detection. ||| 8795 ||| 1146 ||| 1691 ||| 33323 ||| 5141 ||| 
2020 ||| multi-label image recognition with multi-class attentional regions. ||| 33324 ||| 7913 ||| 
2020 ||| planning on the fast lane: learning to interact using attention mechanisms in path integral inverse reinforcement learning. ||| 25585 ||| 1204 ||| 25586 ||| 25587 ||| 25588 ||| 23158 ||| 
2022 ||| arm3d: attention-based relation module for indoor 3d object detection. ||| 33325 ||| 33326 ||| 33327 ||| 33328 ||| 33329 ||| 6238 ||| 5723 ||| 
2019 ||| component attention guided face super-resolution network: cagface. ||| 7407 ||| 4003 ||| 7408 ||| 
2021 ||| lambdanetworks: modeling long-range interactions without attention. ||| 2463 ||| 
2022 ||| masked transformer for neighhourhood-aware click-through rate prediction. ||| 33330 ||| 1261 ||| 1262 ||| 9189 ||| 9346 ||| 1265 ||| 650 ||| 33331 ||| 33332 ||| 
2021 ||| changing the mind of transformers for topically-controllable language generation. ||| 24971 ||| 24972 ||| 3325 ||| 24973 ||| 
2021 ||| nam: normalization-based attention module. ||| 33333 ||| 14333 ||| 33334 ||| 33335 ||| 
2020 ||| computational efficient deep neural network with difference attention maps for facial action unit detection. ||| 8260 ||| 6531 ||| 6534 ||| 33336 ||| 
2021 ||| attdlnet: attention-based dl network for 3d lidar place recognition. ||| 33337 ||| 8396 ||| 33338 ||| 33339 ||| 33340 ||| 33341 ||| 
2021 ||| iot: instance-wise layer reordering for transformer structures. ||| 23930 ||| 4785 ||| 4787 ||| 4788 ||| 4789 ||| 1806 ||| 1807 ||| 4791 ||| 
2021 ||| visual-semantic transformer for scene text recognition. ||| 33342 ||| 33343 ||| 5439 ||| 33344 ||| 33345 ||| 
2021 ||| sstvos: sparse spatiotemporal transformers for video object segmentation. ||| 19063 ||| 19064 ||| 7969 ||| 19065 ||| 2582 ||| 
2019 ||| basn - learning steganography with binary attention mechanism. ||| 11466 ||| 
2022 ||| patch similarity aware data-free quantization for vision transformers. ||| 33346 ||| 33347 ||| 33348 ||| 33349 ||| 33350 ||| 
2021 ||| diagonal attention and style-based gan for content-style disentanglement in image generation and translation. ||| 2047 ||| 2048 ||| 
2021 ||| anchor-free 3d single stage detector with mask-guided attention for point cloud. ||| 19644 ||| 8582 ||| 1932 ||| 8583 ||| 
2021 ||| medical code prediction from discharge summary: document to sequence bert using sequence attention. ||| 25917 ||| 25918 ||| 25919 ||| 25920 ||| 
2021 ||| transcmd: cross-modal decoder equipped with transformer for rgb-d salient object detection. ||| 33351 ||| 33352 ||| 19243 ||| 1700 ||| 
2020 ||| training transformers for information security tasks: a case study on malicious url prediction. ||| 33353 ||| 33354 ||| 
2019 ||| how does bert answer questions? a layer-wise analysis of transformer representations. ||| 1331 ||| 1332 ||| 1333 ||| 1334 ||| 1335 ||| 
2021 ||| hotr: end-to-end human-object interaction detection with transformers. ||| 19094 ||| 19095 ||| 9250 ||| 8574 ||| 9251 ||| 
2018 ||| hierarchical attention: what really counts in various nlp tasks. ||| 33355 ||| 3173 ||| 
2021 ||| calliope - a polyphonic music transformer. ||| 33356 ||| 33357 ||| 33358 ||| 
2021 ||| shift-and-balance attention. ||| 33359 ||| 33360 ||| 33361 ||| 3279 ||| 33362 ||| 
2020 ||| sequence-to-sequence learning via attention transfer for incremental speech recognition. ||| 14652 ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| an automatic detection method of cerebral aneurysms in time-of-flight magnetic resonance angiography images based on attention 3d u-net. ||| 33363 ||| 17466 ||| 33364 ||| 33365 ||| 33366 ||| 33093 ||| 33367 ||| 33368 ||| 
2021 ||| deep visual attention-based transfer clustering. ||| 8039 ||| 8038 ||| 33369 ||| 33370 ||| 8040 ||| 8041 ||| 33371 ||| 
2021 ||| beamtransformer: microphone array-based overlapping speech detection. ||| 33372 ||| 14494 ||| 14432 ||| 12760 ||| 33373 ||| 14599 ||| 14434 ||| 14600 ||| 
2017 ||| attention-based extraction of structured information from street view imagery. ||| 17353 ||| 6339 ||| 17354 ||| 17355 ||| 1695 ||| 6338 ||| 6341 ||| 
2019 ||| convolutional self-attention networks. ||| 3037 ||| 3038 ||| 3039 ||| 3040 ||| 3041 ||| 
2018 ||| mri reconstruction via cascaded channel-wise attention network. ||| 15514 ||| 2019 ||| 15515 ||| 15516 ||| 15517 ||| 1749 ||| 
2018 ||| attention, please! adversarial defense via attention rectification and preservation. ||| 33374 ||| 19393 ||| 33375 ||| 7856 ||| 620 ||| 33376 ||| 25999 ||| 
2020 ||| transformer-transducers for code-switched speech recognition. ||| 12405 ||| 12406 ||| 12407 ||| 12359 ||| 
2019 ||| sharing attention weights for fast transformer. ||| 2333 ||| 23520 ||| 3306 ||| 2950 ||| 23453 ||| 
2021 ||| audiomer: a convolutional transformer for keyword spotting. ||| 33377 ||| 33378 ||| 33379 ||| 33380 ||| 
2020 ||| pretrained transformers improve out-of-distribution robustness. ||| 3459 ||| 3460 ||| 3461 ||| 3462 ||| 3463 ||| 3464 ||| 
2021 ||| oh-former: omni-relational high-order transformer for person re-identification. ||| 33381 ||| 33382 ||| 33383 ||| 19827 ||| 
2021 ||| updet: universal multi-agent reinforcement learning via policy decoupling with transformers. ||| 24019 ||| 24020 ||| 1781 ||| 1686 ||| 
2021 ||| transformer-encoder-gru (t-e-gru) for chinese sentiment analysis on chinese comment text. ||| 33384 ||| 5067 ||| 
2020 ||| wavetransformer: a novel architecture for audio captioning based on learning temporal and time-frequency information. ||| 7990 ||| 8240 ||| 8241 ||| 
2021 ||| tracer: extreme attention guided salient object tracing network. ||| 33385 ||| 33386 ||| 33387 ||| 
2020 ||| robust speaker recognition using speech enhancement and attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2022 ||| predicting physics in mesh-reduced space with temporal attention. ||| 17798 ||| 8861 ||| 33388 ||| 33389 ||| 33390 ||| 
2020 ||| edinburghnlp at wnut-2020 task 2: leveraging transformers with generalized augmentation for identifying informativeness in covid-19 tweets. ||| 24769 ||| 
2020 ||| video super-resolution with temporal group attention. ||| 19029 ||| 5665 ||| 19030 ||| 19031 ||| 18874 ||| 1688 ||| 19032 ||| 11221 ||| 2398 ||| 
2021 ||| explaining the attention mechanism of end-to-end speech recognition using decision trees. ||| 33391 ||| 33392 ||| 33393 ||| 640 ||| 
2021 ||| ab-mapper: attention and bicnet based multi-agent path finding for dynamic crowded environment. ||| 33394 ||| 5519 ||| 15649 ||| 25530 ||| 25526 ||| 21847 ||| 
2022 ||| claret: pre-training a correlation-aware context-to-event transformer for event-centric generation and classification. ||| 33395 ||| 4871 ||| 33396 ||| 802 ||| 3708 ||| 
2018 ||| detail preserving depth estimation from a single image using attention guided networks. ||| 13638 ||| 11641 ||| 8678 ||| 13639 ||| 
2021 ||| learned token pruning for transformers. ||| 33397 ||| 3822 ||| 33398 ||| 22813 ||| 33399 ||| 2596 ||| 
2019 ||| attention deep model with multi-scale deep supervision for person re-identification. ||| 8976 ||| 1589 ||| 5782 ||| 33400 ||| 
2019 ||| stgrat: a spatio-temporal graph attention network for traffic forecasting. ||| 1408 ||| 1409 ||| 1410 ||| 33401 ||| 1413 ||| 1412 ||| 1414 ||| 1183 ||| 
2019 ||| attention convolutional binary neural tree for fine-grained visual categorization. ||| 18730 ||| 8751 ||| 8750 ||| 8749 ||| 8753 ||| 18731 ||| 17695 ||| 2382 ||| 
2019 ||| mvp-net: multi-view fpn with position-aware attention for deep universal lesion detection. ||| 2552 ||| 6588 ||| 4258 ||| 27592 ||| 2142 ||| 1801 ||| 
2020 ||| robustness verification for transformers. ||| 23951 ||| 16915 ||| 3033 ||| 9008 ||| 21252 ||| 
2018 ||| attentional multi-reading sarcasm detection. ||| 4913 ||| 4914 ||| 4916 ||| 
2019 ||| not all attention is needed: gated attention network for sequence data. ||| 3589 ||| 15072 ||| 3590 ||| 
2020 ||| adapterhub: a framework for adapting transformers. ||| 23157 ||| 3700 ||| 3701 ||| 26831 ||| 26832 ||| 15106 ||| 3671 ||| 3008 ||| 3702 ||| 
2020 ||| history repeats itself: human motion prediction via motion attention. ||| 8503 ||| 8504 ||| 8505 ||| 
2021 ||| tfpose: direct human pose estimation with transformers. ||| 33402 ||| 33403 ||| 6335 ||| 15886 ||| 18945 ||| 33404 ||| 
2020 ||| academic performance estimation with attention-based graph convolutional networks. ||| 6797 ||| 359 ||| 
2022 ||| design optimization of a three-phase transformer using finite element analysis. ||| 33405 ||| 33406 ||| 33407 ||| 
2021 ||| spvit: enabling faster vision transformers via soft token pruning. ||| 9872 ||| 33408 ||| 33409 ||| 33410 ||| 33411 ||| 33412 ||| 33413 ||| 33414 ||| 435 ||| 15791 ||| 
2020 ||| how the world's collective attention is being paid to a pandemic: covid-19 related 1-gram time series for 24 languages on twitter. ||| 33415 ||| 33416 ||| 33417 ||| 33418 ||| 33419 ||| 33420 ||| 33421 ||| 33422 ||| 33423 ||| 
2021 ||| pre-trained transformer-based approach for arabic question answering : a comparative study. ||| 33424 ||| 33425 ||| 33426 ||| 
2022 ||| multi-modal multi-label facial action unit detection with transformer. ||| 4259 ||| 33132 ||| 27598 ||| 
2021 ||| t3-vis: a visual analytic framework for training and fine-tuning transformers in nlp. ||| 26619 ||| 4793 ||| 26620 ||| 8273 ||| 4795 ||| 
2020 ||| energy transmission control for a grid-connected modern power system non-linear loads with a series multi-stage transformer voltage reinjection with controlled converters. ||| 33427 ||| 33428 ||| 33429 ||| 33430 ||| 
2020 ||| distributed control of charging for electric vehicle fleets under dynamic transformer ratings. ||| 33431 ||| 33432 ||| 33433 ||| 33434 ||| 33435 ||| 33436 ||| 
2021 ||| iacn: influence-aware and attention-based co-evolutionary network for recommendation. ||| 1095 ||| 15169 ||| 1096 ||| 
2019 ||| an analysis of deep neural networks with attention for action recognition from a neurophysiological perspective. ||| 9832 ||| 9833 ||| 
2019 ||| conversion prediction using multi-task conditional attention networks to support the creation of effective ad creative. ||| 25316 ||| 25317 ||| 25318 ||| 
2020 ||| character-level translation with self-attention. ||| 3593 ||| 3594 ||| 3595 ||| 3596 ||| 
2020 ||| finding experts in transformer models. ||| 33437 ||| 33438 ||| 33439 ||| 
2021 ||| on the interplay between fine-tuning and composition in transformers. ||| 3274 ||| 3275 ||| 
2022 ||| auto-scaling vision transformers without training. ||| 33440 ||| 471 ||| 33441 ||| 33442 ||| 7195 ||| 33443 ||| 
2021 ||| refiner: refining self-attention for vision transformers. ||| 19090 ||| 1725 ||| 32528 ||| 1724 ||| 1726 ||| 8177 ||| 7141 ||| 1902 ||| 1685 ||| 
2021 ||| the transformer network for the traveling salesman problem. ||| 11322 ||| 33444 ||| 
2021 ||| msht: multi-stage hybrid transformer for the rose image analysis of pancreatic cancer. ||| 32297 ||| 33445 ||| 3473 ||| 33446 ||| 33447 ||| 33448 ||| 989 ||| 33449 ||| 33450 ||| 33451 ||| 33452 ||| 33453 ||| 
2020 ||| spatial context-aware self-attention model for multi-organ segmentation. ||| 435 ||| 7133 ||| 7134 ||| 7138 ||| 7139 ||| 7136 ||| 7137 ||| 4297 ||| 7135 ||| 
2022 ||| pedestrian detection: domain generalization, cnns, transformers and beyond. ||| 7272 ||| 32220 ||| 984 ||| 33454 ||| 1932 ||| 
2021 ||| semantic reinforced attention learning for visual place recognition. ||| 1795 ||| 21766 ||| 1796 ||| 21767 ||| 21768 ||| 1798 ||| 
2018 ||| deep attention-guided fusion network for lesion segmentation. ||| 33455 ||| 33456 ||| 5141 ||| 33457 ||| 300 ||| 
2021 ||| methodology and feasibility of neurofeedback to improve visual attention to letters in mild alzheimer's disease. ||| 33458 ||| 33459 ||| 33460 ||| 33461 ||| 33462 ||| 33463 ||| 33464 ||| 
2022 ||| disentangled latent transformer for interpretable monocular height estimation. ||| 6708 ||| 33465 ||| 33466 ||| 16131 ||| 
2022 ||| equivariant graph attention networks for molecular property prediction. ||| 33467 ||| 33468 ||| 33469 ||| 33470 ||| 
2020 ||| u2-onet: a two-level nested octave u-structure with multiscale attention mechanism for moving instances segmentation. ||| 30322 ||| 30323 ||| 382 ||| 
2021 ||| stable, fast and accurate: kernelized attention with relative positional encoding. ||| 33471 ||| 33472 ||| 33473 ||| 18012 ||| 33474 ||| 22737 ||| 33475 ||| 3807 ||| 4791 ||| 
2021 ||| trig: transformer-based text recognizer with initial embedding guidance. ||| 33476 ||| 33477 ||| 33478 ||| 6564 ||| 
2019 ||| neural related work summarization with a joint context-driven attention mechanism. ||| 26769 ||| 3086 ||| 26770 ||| 
2021 ||| -former: infinite memory transformer. ||| 11242 ||| 11243 ||| 3369 ||| 3370 ||| 
2020 ||| question directed graph attention network for numerical reasoning over text. ||| 23075 ||| 5201 ||| 23073 ||| 26602 ||| 26603 ||| 25400 ||| 14931 ||| 1087 ||| 12772 ||| 
2021 ||| show why the answer is correct! towards explainable ai using compositional temporal attention. ||| 24589 ||| 24590 ||| 24591 ||| 
2021 ||| agmi: attention-guided multi-omics integration for drug response prediction with graph neural networks. ||| 16779 ||| 16780 ||| 16781 ||| 16782 ||| 16783 ||| 1236 ||| 
2021 ||| kdexplainer: a task-oriented attention model for explaining knowledge distillation. ||| 23431 ||| 16954 ||| 18726 ||| 3503 ||| 23432 ||| 23433 ||| 
2020 ||| evaluating the role of language typology in transformer-based multilingual text classification. ||| 26364 ||| 26367 ||| 26365 ||| 26366 ||| 26368 ||| 26369 ||| 3802 ||| 
2021 ||| transrppg: remote photoplethysmography transformer for 3d mask face presentation attack detection. ||| 33001 ||| 33479 ||| 1703 ||| 33480 ||| 
2017 ||| joint modeling of event sequence and time series with attentional twin recurrent neural networks. ||| 33481 ||| 2307 ||| 33482 ||| 25400 ||| 8532 ||| 8949 ||| 
2020 ||| improving auto-encoder novelty detection using channel attention and entropy minimization. ||| 13193 ||| 13192 ||| 13194 ||| 13195 ||| 13196 ||| 
2020 ||| xlm-t: scaling up multilingual machine translation with pretrained cross-lingual transformer encoders. ||| 10200 ||| 1825 ||| 33483 ||| 26632 ||| 3171 ||| 5352 ||| 33484 ||| 33485 ||| 33486 ||| 26633 ||| 23931 ||| 33487 ||| 3174 ||| 
2021 ||| enhancing transformer for video understanding using gated multi-level attention and temporal adversarial training. ||| 19469 ||| 19470 ||| 
2019 ||| satellite image time series classification with pixel-set encoders and temporal self-attention. ||| 2309 ||| 2310 ||| 2311 ||| 7111 ||| 19153 ||| 19154 ||| 
2018 ||| deep imbalanced attribute classification using visual attention aggregation. ||| 8703 ||| 8704 ||| 8705 ||| 
2020 ||| streaming automatic speech recognition with the transformer model. ||| 11980 ||| 2508 ||| 11981 ||| 
2020 ||| sbat: video captioning with sparse boundary-aware transformer. ||| 14156 ||| 22669 ||| 8009 ||| 22668 ||| 17724 ||| 
2018 ||| pay attention! - robustifying a deep visuomotor policy through task-focused attention. ||| 19022 ||| 1750 ||| 1752 ||| 19023 ||| 2713 ||| 19024 ||| 
2020 ||| mlnet: an adaptive multiple receptive-field attention neural network for voice activity detection. ||| 14463 ||| 701 ||| 704 ||| 12509 ||| 705 ||| 
2021 ||| fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. ||| 26011 ||| 26012 ||| 
2021 ||| code structure guided transformer for source code summarization. ||| 33488 ||| 33489 ||| 3664 ||| 33490 ||| 33491 ||| 5984 ||| 
2018 ||| weakly supervised one-shot detection with attention siamese networks. ||| 33492 ||| 33493 ||| 33494 ||| 648 ||| 649 ||| 
2021 ||| on the adversarial robustness of visual transformers. ||| 33495 ||| 23951 ||| 22647 ||| 12671 ||| 21252 ||| 
2020 ||| vocoder-free end-to-end voice conversion with transformer network. ||| 486 ||| 487 ||| 488 ||| 
2022 ||| v2x-vit: vehicle-to-everything cooperative perception with vision transformer. ||| 33496 ||| 33497 ||| 33498 ||| 5984 ||| 7143 ||| 33499 ||| 
2019 ||| scale-aware attention network for crowd counting. ||| 33500 ||| 2421 ||| 33501 ||| 33502 ||| 
2021 ||| key-sparse transformer with cascaded cross-attention block for multimodal speech emotion recognition. ||| 19538 ||| 33503 ||| 4482 ||| 4481 ||| 33504 ||| 
2020 ||| sac: accelerating and structuring self-attention via sparse adaptive connection. ||| 9200 ||| 9201 ||| 9203 ||| 2258 ||| 9204 ||| 
2022 ||| ct-sat: contextual transformer for sequential audio tagging. ||| 14737 ||| 33505 ||| 33506 ||| 12511 ||| 14742 ||| 
2020 ||| microscopic fine-grained instance classification through deep attention. ||| 15545 ||| 15546 ||| 15547 ||| 2377 ||| 15548 ||| 
2019 ||| self-attention network for skeleton-based human action recognition. ||| 7304 ||| 7305 ||| 523 ||| 7306 ||| 
2019 ||| crowd transformer network. ||| 33507 ||| 1752 ||| 1811 ||| 
2020 ||| residual channel attention generative adversarial network for image super-resolution and noise reduction. ||| 7333 ||| 19176 ||| 7163 ||| 
2019 ||| auto-sizing the transformer network: improving speed, efficiency, and performance for low-resource machine translation. ||| 26723 ||| 26724 ||| 4738 ||| 26725 ||| 4846 ||| 
2021 ||| a note on learning rare events in molecular dynamics using lstm and transformer. ||| 33508 ||| 33509 ||| 590 ||| 18909 ||| 
2021 ||| multi-view stereo with transformer. ||| 12282 ||| 19855 ||| 20005 ||| 7064 ||| 2855 ||| 19853 ||| 
2021 ||| next generation multitarget trackers: random finite set methods vs transformer-based deep learning. ||| 20842 ||| 20843 ||| 20844 ||| 20845 ||| 20846 ||| 20847 ||| 
2021 ||| video instance segmentation using inter-frame communication transformers. ||| 33510 ||| 33511 ||| 33512 ||| 19306 ||| 
2020 ||| cafe-gan: arbitrary face attribute editing with complementary attention feature. ||| 8558 ||| 8559 ||| 8560 ||| 
2020 ||| saadb: a self-attention guided adb network for person re-identification. ||| 2632 ||| 1270 ||| 5957 ||| 17624 ||| 
2021 ||| frequency-temporal attention network for singing melody extraction. ||| 4520 ||| 11982 ||| 4112 ||| 3337 ||| 
2018 ||| i can see your aim: estimating user attention from gaze for handheld robot collaboration. ||| 25595 ||| 18921 ||| 
2019 ||| small object detection using context and attention. ||| 17277 ||| 17278 ||| 17279 ||| 17280 ||| 
2022 ||| codedvtr: codebook-based sparse voxel transformer with geometric guidance. ||| 33513 ||| 33514 ||| 33515 ||| 19285 ||| 33516 ||| 3906 ||| 
2020 ||| visual transformers: token-based image representation and processing for computer vision. ||| 2590 ||| 2591 ||| 2592 ||| 2593 ||| 2594 ||| 2272 ||| 2596 ||| 2597 ||| 
2022 ||| s3t: self-supervised pre-training with swin transformer for music classification. ||| 18496 ||| 10922 ||| 33517 ||| 12526 ||| 33518 ||| 
2018 ||| multi-scale attention with dense encoder for handwritten mathematical expression recognition. ||| 4489 ||| 1010 ||| 4463 ||| 
2018 ||| amnet: memorability estimation with attention. ||| 6401 ||| 6403 ||| 6404 ||| 6405 ||| 
2021 ||| retrieval-augmented transformer-xl for close-domain dialog generation. ||| 3935 ||| 3936 ||| 3937 ||| 3938 ||| 
2021 ||| efficient conformer with prob-sparse attention mechanism for end-to-endspeech recognition. ||| 12467 ||| 12395 ||| 12384 ||| 13917 ||| 
2022 ||| structure-aware transformer for graph representation learning. ||| 23967 ||| 33519 ||| 33520 ||| 
2021 ||| vision transformer based video hashing retrieval for tracing the source of fake videos. ||| 33521 ||| 25452 ||| 33522 ||| 33523 ||| 25451 ||| 
2021 ||| cae-transformer: transformer-based model to predict invasiveness of lung adenocarcinoma subsolid nodules from non-thin section 3d ct scans. ||| 33524 ||| 33525 ||| 33526 ||| 33527 ||| 33528 ||| 
2021 ||| channel-based attention for lcc using sentinel-2 time series. ||| 6783 ||| 33529 ||| 6785 ||| 6786 ||| 6787 ||| 6788 ||| 6789 ||| 
2019 ||| spatiotemporal attention networks for wind power forecasting. ||| 18468 ||| 6810 ||| 18469 ||| 18470 ||| 18471 ||| 
2021 ||| enriching transformers with structured tensor-product representations for abstractive summarization. ||| 4755 ||| 3539 ||| 4756 ||| 4757 ||| 4758 ||| 4759 ||| 4760 ||| 4761 ||| 3810 ||| 1958 ||| 
2020 ||| attention-based saliency hashing for ophthalmic image retrieval. ||| 15296 ||| 15560 ||| 5330 ||| 5331 ||| 5206 ||| 
2020 ||| neural architecture search with reinforce and masked attention autoregressive density estimators. ||| 17096 ||| 17097 ||| 2577 ||| 17098 ||| 
2021 ||| cubetr: learning to solve the rubiks cube using transformers. ||| 33530 ||| 
2021 ||| vision transformer for learning driving policies in complex multi-agent environments. ||| 33531 ||| 33532 ||| 
2019 ||| attention guided metal artifact correction in mri using deep neural networks. ||| 33533 ||| 33534 ||| 33535 ||| 33536 ||| 
2021 ||| fp-age: leveraging face parsing attention for facial age estimation in the wild. ||| 28702 ||| 20051 ||| 28703 ||| 5719 ||| 
2021 ||| eeg-based classification of drivers attention using convolutional neural network. ||| 24609 ||| 24610 ||| 
2019 ||| reinforcement learning with attention that works: a self-supervised approach. ||| 5175 ||| 5176 ||| 5177 ||| 
2021 ||| pruning attention heads of transformer models using a* search: a novel approach to compress big nlp architectures. ||| 33537 ||| 33538 ||| 33539 ||| 
2020 ||| brain atlas guided attention u-net for white matter hyperintensity segmentation. ||| 33540 ||| 33541 ||| 3747 ||| 1661 ||| 33542 ||| 33543 ||| 3750 ||| 
2020 ||| multi-task temporal shift attention networks for on-device contactless vitals measurement. ||| 189 ||| 9309 ||| 9310 ||| 5732 ||| 
2020 ||| attention augmented convlstm forenvironment prediction. ||| 25534 ||| 25535 ||| 25536 ||| 
2022 ||| a novel perspective to look at attention: bi-level attention-based explainable topic modeling for news classification. ||| 33544 ||| 33545 ||| 8875 ||| 
2019 ||| multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation. ||| 435 ||| 436 ||| 437 ||| 15791 ||| 7342 ||| 127 ||| 
2021 ||| ernie-tiny : a progressive distillation framework for pretrained transformer compression. ||| 33546 ||| 33547 ||| 10585 ||| 10583 ||| 33548 ||| 673 ||| 3415 ||| 3416 ||| 3417 ||| 
2020 ||| person image generation with semantic attention network for person re-identification. ||| 33336 ||| 6534 ||| 33549 ||| 33550 ||| 
2019 ||| dialogue act classification with context-aware self-attention. ||| 4869 ||| 4870 ||| 
2019 ||| big mood: relating transformers to explicit commonsense knowledge. ||| 33551 ||| 
2020 ||| toward transformer-based object detection. ||| 7183 ||| 33552 ||| 33553 ||| 7185 ||| 7186 ||| 7187 ||| 
2020 ||| deformable detr: deformable transformers for end-to-end object detection. ||| 2638 ||| 23994 ||| 1844 ||| 6502 ||| 1846 ||| 1847 ||| 
2020 ||| paying more attention to snapshots of iterative pruning: improving model compression via ensemble distillation. ||| 13716 ||| 21468 ||| 13719 ||| 
2021 ||| exploring text-to-text transformers for english to hinglish machine translation with synthetic code-mixing. ||| 33554 ||| 3154 ||| 3152 ||| 33555 ||| 
2021 ||| going full-tilt boogie on document understanding with text-image-layout transformer. ||| 17334 ||| 17335 ||| 17336 ||| 17337 ||| 17338 ||| 17339 ||| 
2021 ||| causal transformers perform below chance on recursive nested constructions, unlike humans. ||| 33556 ||| 33557 ||| 3341 ||| 8119 ||| 
2020 ||| knowledge-aware attention network for protein-protein interaction extraction. ||| 13576 ||| 1415 ||| 13577 ||| 13578 ||| 31180 ||| 31181 ||| 
2017 ||| attention based convolutional neural network for predicting rna-protein binding sites. ||| 33558 ||| 2307 ||| 
2018 ||| mist: multiple instance spatial transformer network. ||| 19371 ||| 33559 ||| 2555 ||| 2556 ||| 
2021 ||| is image size important? a robustness comparison of deep learning methods for multi-scale cell image classification tasks: from convolutional neural networks to visual transformers. ||| 32663 ||| 399 ||| 33560 ||| 33561 ||| 8841 ||| 32662 ||| 13788 ||| 15956 ||| 
2021 ||| emotion classification in a resource constrained language using transformer-based approach. ||| 4883 ||| 4884 ||| 4885 ||| 4886 ||| 
2021 ||| faceformer: speech-driven 3d facial animation with transformers. ||| 33562 ||| 10652 ||| 33563 ||| 19461 ||| 33564 ||| 
2018 ||| latent alignment and variational attention. ||| 3944 ||| 9410 ||| 9411 ||| 9412 ||| 4962 ||| 
2020 ||| multi-head attention: collaborate instead of concatenate. ||| 22850 ||| 22851 ||| 23895 ||| 
2021 ||| convolutional neural network (cnn) vs visual transformer (vit) for digital holography. ||| 2693 ||| 33565 ||| 13128 ||| 33566 ||| 
2020 ||| a universal representation transformer layer for few-shot image classification. ||| 6174 ||| 9237 ||| 802 ||| 800 ||| 9359 ||| 
2018 ||| a novel focal tversky loss function with improved attention u-net for lesion segmentation. ||| 15589 ||| 15590 ||| 
2021 ||| so-vit: mind visual tokens for vision transformer. ||| 33567 ||| 33568 ||| 19228 ||| 17974 ||| 19230 ||| 
2021 ||| knowledge distillation from bert transformer to speech transformer for intent classification. ||| 14377 ||| 14378 ||| 13580 ||| 12494 ||| 
2020 ||| improving bert with syntax-aware local attention. ||| 3615 ||| 3616 ||| 242 ||| 3617 ||| 3618 ||| 
2019 ||| a sparse annotation strategy based on attention-guided active learning for 3d medical image segmentation. ||| 27548 ||| 4634 ||| 27547 ||| 27549 ||| 6621 ||| 
2019 ||| alphastock: a buying-winners-and-selling-losers investment strategy using interpretable deep reinforcement attention networks. ||| 25320 ||| 1420 ||| 25383 ||| 4327 ||| 4214 ||| 
2020 ||| arabert: transformer-based model for arabic language understanding. ||| 33569 ||| 33570 ||| 33571 ||| 
2017 ||| attend and diagnose: clinical time series analysis using attention models. ||| 14421 ||| 14878 ||| 12005 ||| 12006 ||| 
2020 ||| rotate to attend: convolutional triplet attention module. ||| 7176 ||| 7177 ||| 7178 ||| 1902 ||| 
2021 ||| homogeneous learning: self-attention decentralized deep learning. ||| 33572 ||| 33573 ||| 
2019 ||| nat: neural architecture transformer for accurate and compact architectures. ||| 9373 ||| 9374 ||| 6413 ||| 6415 ||| 9375 ||| 9346 ||| 1265 ||| 
2019 ||| attention network robustification for person reid. ||| 33574 ||| 33575 ||| 33576 ||| 33577 ||| 33578 ||| 
2020 ||| g-darts-a: groups of channel parallel sampling with attention. ||| 8762 ||| 781 ||| 12007 ||| 
2018 ||| attention-based audio-visual fusion for robust automatic speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2018 ||| yuanfudao at semeval-2018 task 11: three-way attention and relational knowledge for commonsense machine comprehension. ||| 10429 ||| 4493 ||| 7700 ||| 10430 ||| 10431 ||| 
2020 ||| comparing transformers and rnns on predicting human sentence processing data. ||| 20736 ||| 20737 ||| 
2019 ||| enhancing the transformer with explicit relational encoding for math problem solving. ||| 22751 ||| 4756 ||| 4760 ||| 33579 ||| 4194 ||| 11785 ||| 1958 ||| 
2021 ||| convit: improving vision transformers with soft convolutional inductive biases. ||| 2693 ||| 22714 ||| 1887 ||| 22715 ||| 3824 ||| 22716 ||| 22717 ||| 
2021 ||| score transformer: generating musical score from note-level representation. ||| 13214 ||| 
2018 ||| reciprocal attention fusion for visual question answering. ||| 20206 ||| 1969 ||| 
2021 ||| teasel: a transformer-based speech-prefixed language model. ||| 33580 ||| 12089 ||| 33581 ||| 
2021 ||| multi-task prediction of clinical outcomes in the intensive care unit using flexible multimodal transformers. ||| 33582 ||| 33583 ||| 33584 ||| 33585 ||| 
2022 ||| how do vision transformers work? ||| 33586 ||| 33587 ||| 
2021 ||| mate: multi-view attention for table transformer efficiency. ||| 33588 ||| 26656 ||| 3830 ||| 3831 ||| 3246 ||| 
2021 ||| improving vision transformers for incremental learning. ||| 33589 ||| 1959 ||| 14795 ||| 8573 ||| 
2021 ||| epsanet: an efficient pyramid split attention block on convolutional neural network. ||| 33590 ||| 33591 ||| 12720 ||| 33592 ||| 8850 ||| 
2018 ||| an affect-rich neural conversational model with biased attention and weighted cross-entropy loss. ||| 738 ||| 15790 ||| 16696 ||| 
2022 ||| multi-level attention for unsupervised person re-identification. ||| 28688 ||| 
2021 ||| lstm-sakt: lstm-encoded sakt-like transformer for knowledge tracing. ||| 33593 ||| 33594 ||| 
2022 ||| transvod: end-to-end video object detection with spatial-temporal transformers. ||| 19605 ||| 19606 ||| 19604 ||| 2517 ||| 19607 ||| 18501 ||| 5141 ||| 1756 ||| 
2019 ||| supervised multimodal bitransformers for classifying images and text. ||| 3826 ||| 9240 ||| 9241 ||| 9242 ||| 
2019 ||| self-attention for raw optical satellite time series classification. ||| 33595 ||| 33596 ||| 33597 ||| 33598 ||| 
2020 ||| contextualised graph attention for improved relation extraction. ||| 33599 ||| 14952 ||| 4777 ||| 
2020 ||| visbert: hidden-state visualizations for transformers. ||| 1331 ||| 1332 ||| 1333 ||| 1334 ||| 1335 ||| 
2020 ||| polarization-driven semantic segmentation via efficient attention-bridged fusion. ||| 33600 ||| 7857 ||| 5906 ||| 
2021 ||| smac-seg: lidar panoptic segmentation via sparse multi-directional attention clustering. ||| 33601 ||| 33602 ||| 33603 ||| 21816 ||| 
2018 ||| scene parsing via dense recurrent neural networks with attentional selection. ||| 7245 ||| 7246 ||| 7247 ||| 2163 ||| 
2021 ||| mvs2d: efficient multi-view stereo via attention-driven 2d convolutions. ||| 33604 ||| 2033 ||| 33605 ||| 19520 ||| 
2021 ||| trading with the momentum transformer: an intelligent and interpretable architecture. ||| 33606 ||| 33607 ||| 33608 ||| 33609 ||| 
2021 ||| a-esrgan: training real-world blind super-resolution with attention u-net discriminators. ||| 33610 ||| 33611 ||| 33612 ||| 33613 ||| 33614 ||| 
2020 ||| deriving contextualised semantic features from bert (and other transformer model) embeddings. ||| 33615 ||| 33616 ||| 33617 ||| 
2021 ||| musical speech: a transformer-based composition tool. ||| 9400 ||| 9401 ||| 9402 ||| 33618 ||| 9404 ||| 
2018 ||| crowd-robot interaction: crowd-aware robot navigation with attention-based deep reinforcement learning. ||| 21830 ||| 21831 ||| 21832 ||| 21833 ||| 
2021 ||| cloth interactive transformer for virtual try-on. ||| 33413 ||| 435 ||| 11558 ||| 10253 ||| 1932 ||| 2160 ||| 437 ||| 
2021 ||| focal self-attention for local-global interactions in vision transformers. ||| 1955 ||| 26680 ||| 1953 ||| 1954 ||| 1956 ||| 1957 ||| 1958 ||| 
2019 ||| cross-modal image fusion theory guided by subjective visual attention. ||| 33619 ||| 10058 ||| 10075 ||| 
2020 ||| attention-based 3d object reconstruction from a single image. ||| 265 ||| 266 ||| 267 ||| 268 ||| 229 ||| 
2021 ||| transformers can do bayesian inference. ||| 33620 ||| 3831 ||| 33621 ||| 33622 ||| 33623 ||| 33624 ||| 
2020 ||| transquest: translation quality estimation with cross-lingual transformers. ||| 3849 ||| 3850 ||| 3851 ||| 
2021 ||| interpretable visual understanding with cognitive attention network. ||| 4110 ||| 4111 ||| 4112 ||| 4113 ||| 4114 ||| 4115 ||| 4116 ||| 
2019 ||| fcem: a novel fast correlation extract model for real time steganalysis of voip stream via multi-head attention. ||| 2792 ||| 2793 ||| 2791 ||| 2794 ||| 2795 ||| 
2021 ||| adaadepth: adapting data augmentation and attention for self-supervised monocular depth estimation. ||| 33625 ||| 33626 ||| 12796 ||| 
2018 ||| image super-resolution using very deep residual channel attention networks. ||| 1730 ||| 2232 ||| 2233 ||| 8561 ||| 8562 ||| 1734 ||| 
2018 ||| multi-scale alignment and contextual history for attention mechanism in sequence-to-sequence model. ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| visual transformer with statistical test for covid-19 classification. ||| 33627 ||| 33628 ||| 33629 ||| 
2018 ||| joint attention in driver-pedestrian interaction: from theory to practice. ||| 29814 ||| 29817 ||| 
2020 ||| how to train your robust human pose estimator: pay attention to the constraint cue. ||| 4120 ||| 18688 ||| 18689 ||| 18690 ||| 
2021 ||| attention-based sensor fusion for human activity recognition using imu signals. ||| 33630 ||| 33631 ||| 24981 ||| 33632 ||| 33633 ||| 24983 ||| 
2021 ||| attention-guided temporal coherent video object matting. ||| 19517 ||| 4171 ||| 19518 ||| 19519 ||| 18911 ||| 2490 ||| 19081 ||| 19520 ||| 19521 ||| 
2021 ||| oscar-net: object-centric scene graph attention for image attribution. ||| 2285 ||| 2286 ||| 33634 ||| 2288 ||| 
2021 ||| searching the search space of vision transformer. ||| 1774 ||| 1773 ||| 33635 ||| 1698 ||| 19680 ||| 1699 ||| 1775 ||| 2163 ||| 
2020 ||| gmat: global memory augmentation for transformers. ||| 26370 ||| 26356 ||| 
2021 ||| salypath: a deep-based architecture for visual attention prediction. ||| 11574 ||| 11575 ||| 11576 ||| 11577 ||| 
2020 ||| volumetric attention for 3d medical image segmentation and detection. ||| 12824 ||| 27734 ||| 27735 ||| 18677 ||| 18678 ||| 
2020 ||| coarse to fine: multi-label image classification with global/local attention. ||| 10069 ||| 10072 ||| 6475 ||| 15833 ||| 10071 ||| 15834 ||| 
2020 ||| transformers are rnns: fast autoregressive transformers with linear attention. ||| 9348 ||| 9347 ||| 14940 ||| 1226 ||| 9349 ||| 
2020 ||| multi-scale receptive fields graph attention network for point cloud classification. ||| 29845 ||| 241 ||| 29846 ||| 12720 ||| 
2021 ||| semi-supervised segmentation of radiation-induced pulmonary fibrosis from lung ct scans with multi-scale guided dense attention. ||| 15623 ||| 33636 ||| 33637 ||| 33638 ||| 33639 ||| 30726 ||| 33640 ||| 1749 ||| 32188 ||| 15555 ||| 
2017 ||| attend and predict: understanding gene regulation by selective attention on chromatin. ||| 9137 ||| 9138 ||| 9139 ||| 9140 ||| 
2018 ||| attention-based active visual search for mobile robots. ||| 29814 ||| 29815 ||| 29816 ||| 29817 ||| 
2020 ||| single headed attention based sequence-to-sequence model for state-of-the-art results on switchboard-300. ||| 12413 ||| 12414 ||| 12415 ||| 14765 ||| 14662 ||| 12416 ||| 
2021 ||| ice hockey player identification via transformers. ||| 33641 ||| 33642 ||| 33643 ||| 33644 ||| 18999 ||| 
2022 ||| boosting crowd counting via multifaceted attention. ||| 3508 ||| 33645 ||| 2367 ||| 6416 ||| 11548 ||| 
2020 ||| transformer-encoder detector module: using context to improve robustness to adversarial attacks on object detection. ||| 20284 ||| 20285 ||| 2603 ||| 
2020 ||| conv-transformer transducer: low latency, low frame rate, streamable end-to-end speech recognition. ||| 14756 ||| 14757 ||| 14758 ||| 3889 ||| 
2020 ||| conditional self-attention for query-based summarization. ||| 33646 ||| 4872 ||| 17138 ||| 3175 ||| 
2020 ||| sliceout: training transformers and cnns faster while using less memory. ||| 33647 ||| 9134 ||| 33648 ||| 33649 ||| 
2019 ||| the channel attention based context encoder network for inner limiting membrane detection. ||| 27711 ||| 24411 ||| 27712 ||| 24407 ||| 27713 ||| 24408 ||| 5206 ||| 15003 ||| 
2022 ||| dsrrtracker: dynamic search region refinement for attention-based siamese multi-object tracking. ||| 33650 ||| 6735 ||| 390 ||| 33651 ||| 27118 ||| 185 ||| 33652 ||| 
2017 ||| where to focus: deep attention-based spatially recurrent bilinear networks for fine-grained visual recognition. ||| 17580 ||| 602 ||| 
2021 ||| unified questioner transformer for descriptive question generation in goal-oriented visual dialogue. ||| 1981 ||| 1982 ||| 1983 ||| 1984 ||| 726 ||| 1985 ||| 
2019 ||| multi-task attention-based semi-supervised learning for medical image segmentation. ||| 27562 ||| 27563 ||| 33653 ||| 27566 ||| 27567 ||| 
2019 ||| document-level neural machine translation with inter-sentence attention. ||| 33654 ||| 3049 ||| 33655 ||| 4907 ||| 3112 ||| 4908 ||| 3111 ||| 684 ||| 
2021 ||| deep neural network loses attention to adversarial images. ||| 23320 ||| 23321 ||| 
2021 ||| is space-time attention all you need for video understanding? ||| 7362 ||| 22747 ||| 7366 ||| 
2018 ||| attention fusion networks: combining behavior and e-mail content to improve customer support. ||| 33656 ||| 33657 ||| 33658 ||| 33659 ||| 33660 ||| 
2022 ||| detail-preserving transformer for light field image super-resolution. ||| 6459 ||| 33661 ||| 4151 ||| 22572 ||| 
2021 ||| pre-training and fine-tuning transformers for fmri prediction tasks. ||| 1368 ||| 33662 ||| 1667 ||| 33663 ||| 
2018 ||| iterative transformer network for 3d point cloud. ||| 33664 ||| 33665 ||| 2000 ||| 33666 ||| 
2021 ||| bayesian transformer language models for speech recognition. ||| 12292 ||| 12293 ||| 12294 ||| 12295 ||| 12296 ||| 12297 ||| 12298 ||| 12299 ||| 4460 ||| 
2020 ||| guiding attention in sequence-to-sequence models for dialogue act prediction. ||| 18127 ||| 18128 ||| 17954 ||| 18129 ||| 18130 ||| 9772 ||| 9773 ||| 
2021 ||| quantifying and maximizing the benefits of back-end noise adaption on attention-based speech recognition models. ||| 25283 ||| 25279 ||| 25287 ||| 
2019 ||| cross attention network for semantic segmentation. ||| 11232 ||| 11233 ||| 
2020 ||| lossless attention in convolutional networks for facial expression recognition in the wild. ||| 812 ||| 813 ||| 5888 ||| 5206 ||| 33667 ||| 33668 ||| 1871 ||| 33669 ||| 
2022 ||| aggregating global features into local vision transformer. ||| 33670 ||| 33671 ||| 33672 ||| 392 ||| 
2021 ||| capturing multi-resolution context by dilated self-attention. ||| 11980 ||| 2508 ||| 11981 ||| 
2021 ||| on the difficulty of segmenting words with attention. ||| 33673 ||| 435 ||| 33674 ||| 
2021 ||| vidt: an efficient and effective fully transformer-based object detector. ||| 33675 ||| 33676 ||| 2089 ||| 33677 ||| 2088 ||| 2086 ||| 9355 ||| 7143 ||| 
2019 ||| classification of hand movements from eeg using a deep attention-based lstm network. ||| 33678 ||| 33679 ||| 12091 ||| 33680 ||| 12092 ||| 
2017 ||| end-to-end flow correlation tracking with spatial-temporal attention. ||| 18688 ||| 293 ||| 12263 ||| 2391 ||| 
2021 ||| self-attention meta-learner for continual learning. ||| 3921 ||| 3922 ||| 3923 ||| 
2021 ||| adaptable gan encoders for image reconstruction via multi-type latent vectors with two-scale attentions. ||| 11121 ||| 2200 ||| 
2021 ||| wake word detection with streaming transformers. ||| 12382 ||| 12383 ||| 12191 ||| 12384 ||| 12194 ||| 
2021 ||| exploring text-transformers in aaai 2021 shared task: covid-19 fake news detection in english. ||| 18018 ||| 18019 ||| 18016 ||| 6679 ||| 11660 ||| 
2018 ||| inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks. ||| 23143 ||| 23144 ||| 23145 ||| 23146 ||| 23147 ||| 
2020 ||| information extraction from swedish medical prescriptions with sig-transformer encoder. ||| 27208 ||| 1241 ||| 27209 ||| 27210 ||| 
2022 ||| drtam: dual rank-1 tensor attention module. ||| 33681 ||| 33682 ||| 844 ||| 10429 ||| 
2021 ||| deep attention-guided graph clustering with dual self-supervision. ||| 19424 ||| 17677 ||| 19425 ||| 19426 ||| 
2021 ||| infusing future information into monotonic attention through language models. ||| 33683 ||| 3494 ||| 33684 ||| 33685 ||| 3496 ||| 
2020 ||| exploring self-attention for image recognition. ||| 2335 ||| 2204 ||| 1884 ||| 
2020 ||| improving image captioning by leveraging intra- and inter-layer global representation in transformer network. ||| 17659 ||| 17658 ||| 2504 ||| 17673 ||| 17674 ||| 6831 ||| 820 ||| 2367 ||| 
2020 ||| a dc-autotransformer based multilevel inverter for automotive applications. ||| 33686 ||| 33687 ||| 33688 ||| 
2021 ||| deep reinforced attention regression for partial sketch based image retrieval. ||| 18488 ||| 18489 ||| 18490 ||| 18491 ||| 
2017 ||| attention networks for image-to-text. ||| 33689 ||| 33690 ||| 
2018 ||| coarse-to-fine: a rnn-based hierarchical attention model for vehicle re-identification. ||| 6332 ||| 6333 ||| 6334 ||| 6335 ||| 1863 ||| 
2019 ||| attention guided anomaly detection and localization in images. ||| 8732 ||| 1746 ||| 1747 ||| 8733 ||| 
2021 ||| impact of attention on adversarial robustness of image classification models. ||| 17111 ||| 12821 ||| 12822 ||| 12823 ||| 
2019 ||| conditioning lstm decoder and bi-directional attention based question answering system. ||| 33691 ||| 
2017 ||| data-driven approach to measuring the level of press freedom using media attention diversity from unfiltered news. ||| 9045 ||| 9044 ||| 
2021 ||| attention-guided nir image colorization via adaptive fusion of semantic and texture clues. ||| 33692 ||| 1037 ||| 33693 ||| 17993 ||| 
2020 ||| glu variants improve transformer. ||| 9132 ||| 
2021 ||| vision transformer architecture search. ||| 33694 ||| 33695 ||| 1483 ||| 33696 ||| 2355 ||| 18226 ||| 12748 ||| 1846 ||| 3156 ||| 
2021 ||| using large pre-trained models with cross-modal attention for multi-modal emotion recognition. ||| 14754 ||| 
2021 ||| speech summarization using restricted self-attention. ||| 33697 ||| 33698 ||| 4890 ||| 14721 ||| 
2020 ||| on task-level dialogue composition of generative transformer model. ||| 13145 ||| 13147 ||| 13146 ||| 
2019 ||| learning target-oriented dual attention for robust rgb-t tracking. ||| 11453 ||| 11454 ||| 5957 ||| 11455 ||| 5755 ||| 
2021 ||| lymph node detection in t2 mri with transformers. ||| 33699 ||| 31213 ||| 31214 ||| 31215 ||| 21585 ||| 21587 ||| 14912 ||| 
2021 ||| feature combination meets attention: baidu soccer embeddings and transformer based temporal detection. ||| 29033 ||| 33700 ||| 33701 ||| 6186 ||| 33702 ||| 
2017 ||| a nested attention neural hybrid model for grammatical error correction. ||| 3183 ||| 3184 ||| 3185 ||| 3186 ||| 3187 ||| 1958 ||| 
2019 ||| a two-stream end-to-end deep learning network for recognizing atypical visual attention in autism spectrum disorder. ||| 1824 ||| 33703 ||| 33704 ||| 19610 ||| 20589 ||| 2039 ||| 20590 ||| 
2020 ||| code completion using neural attention and byte pair encoding. ||| 33705 ||| 33706 ||| 33707 ||| 
2021 ||| acnet: mask-aware attention with dynamic context enhancement for robust acne detection. ||| 24508 ||| 24509 ||| 24502 ||| 
2022 ||| audio-visual generalised zero-shot learning with cross-modal attention and language. ||| 33708 ||| 33709 ||| 33710 ||| 33711 ||| 
2019 ||| comet: commonsense transformers for automatic knowledge graph construction. ||| 3353 ||| 3536 ||| 3537 ||| 3538 ||| 3539 ||| 3355 ||| 
2022 ||| transformer for graphs: an overview from architecture perspective. ||| 33330 ||| 33712 ||| 9189 ||| 1262 ||| 33713 ||| 1263 ||| 9346 ||| 1265 ||| 33332 ||| 1261 ||| 
2021 ||| edgeconv with attention module for monocular depth estimation. ||| 7225 ||| 7226 ||| 7227 ||| 7228 ||| 
2020 ||| exploiting typed syntactic dependencies for targeted sentiment classification using graph attention neural network. ||| 6273 ||| 33714 ||| 3289 ||| 
2020 ||| hamlet: a hierarchical multimodal attention-based human activity recognition algorithm. ||| 25554 ||| 25555 ||| 
2021 ||| predicting discourse trees from transformer-based neural summarizers. ||| 4793 ||| 4794 ||| 4795 ||| 
2021 ||| attention-enhanced cross-task network for analysing multiple attributes of lung nodules in ct. ||| 31131 ||| 14038 ||| 31132 ||| 31133 ||| 14039 ||| 
2021 ||| attention-based stylisation for exemplar image colourisation. ||| 11480 ||| 11481 ||| 24894 ||| 11483 ||| 1675 ||| 11484 ||| 
2018 ||| convolutional attention networks for multimodal emotion recognition from speech and text data. ||| 
2018 ||| hierarchical attention networks for knowledge base completion via joint adversarial training. ||| 399 ||| 33715 ||| 18005 ||| 215 ||| 26422 ||| 
2022 ||| shapeformer: transformer-based shape completion via sparse representation. ||| 33716 ||| 33717 ||| 33718 ||| 33719 ||| 33720 ||| 6238 ||| 
2020 ||| multi-head linear attention generative adversarial network for thin cloud removal. ||| 30455 ||| 8207 ||| 
2018 ||| comparing attention-based convolutional and recurrent neural networks: success and limitations in machine reading comprehension. ||| 23093 ||| 23094 ||| 9269 ||| 7193 ||| 20979 ||| 
2021 ||| mixed precision of quantization of transformer language models for speech recognition. ||| 12294 ||| 12296 ||| 12293 ||| 12299 ||| 4460 ||| 
2022 ||| transformer compressed sensing via global image tokens. ||| 33721 ||| 33722 ||| 33723 ||| 
2018 ||| improving robustness of attention models on graphs. ||| 12004 ||| 12005 ||| 12006 ||| 
2019 ||| graph representation learning via hard and channel-wise attention networks. ||| 25403 ||| 23491 ||| 
2019 ||| audiovisual transformer architectures for large-scale classification and synchronization of weakly labeled audio events. ||| 19567 ||| 19568 ||| 
2021 ||| consistent accelerated inference via confident adaptive transformers. ||| 26522 ||| 26523 ||| 26524 ||| 26502 ||| 
2021 ||| geometric transformers for protein interface contact prediction. ||| 33724 ||| 2230 ||| 33725 ||| 
2021 ||| real-time attention span tracking in online education. ||| 33726 ||| 33727 ||| 33728 ||| 33729 ||| 
2020 ||| svga-net: sparse voxel-graph attention network for 3d object detection from point clouds. ||| 33730 ||| 33731 ||| 33732 ||| 331 ||| 19236 ||| 33733 ||| 
2019 ||| attention optimization for abstractive document summarization. ||| 26608 ||| 10590 ||| 3049 ||| 1979 ||| 
2018 ||| generative model for material experiments based on prior knowledge and attention mechanism. ||| 33734 ||| 33735 ||| 2014 ||| 
2021 ||| graph attention networks for anti-spoofing. ||| 14571 ||| 12728 ||| 14572 ||| 14573 ||| 14574 ||| 
2021 ||| video joint modelling based on hierarchical transformer for co-summarization. ||| 33736 ||| 7270 ||| 17834 ||| 3248 ||| 
2019 ||| differentiating features for scene segmentation based on dedicated attention mechanisms. ||| 33737 ||| 18779 ||| 33738 ||| 33739 ||| 
2020 ||| attention guided semantic relationship parsing for visual question answering. ||| 20206 ||| 1969 ||| 2475 ||| 
2021 ||| transformer-unet: raw image processing with unet. ||| 33740 ||| 31341 ||| 33741 ||| 24482 ||| 
2020 ||| track-assignment detailed routing using attention-based policy model with supervision. ||| 17483 ||| 17484 ||| 17485 ||| 17486 ||| 17487 ||| 
2020 ||| luke: deep contextualized entity representations with entity-aware self-attention. ||| 26451 ||| 26452 ||| 4802 ||| 26453 ||| 4803 ||| 
2021 ||| chasing sparsity in vision transformers: an end-to-end exploration. ||| 19181 ||| 2045 ||| 2044 ||| 1957 ||| 241 ||| 7195 ||| 
2021 ||| attentional graph neural network for parking-slot detection. ||| 33742 ||| 33743 ||| 17199 ||| 33744 ||| 33745 ||| 33746 ||| 
2019 ||| stnreid : deep convolutional networks with pairwise spatial transformer networks for partial person re-identification. ||| 1702 ||| 33747 ||| 1460 ||| 1706 ||| 
2021 ||| transfg: a transformer architecture for fine-grained recognition. ||| 33748 ||| 32259 ||| 9455 ||| 33749 ||| 1372 ||| 33750 ||| 8727 ||| 8660 ||| 
2021 ||| language modeling using lmus: 10x better data efficiency or improved scaling compared to transformers. ||| 33751 ||| 33752 ||| 33753 ||| 33754 ||| 33755 ||| 33756 ||| 
2022 ||| dual-decoder transformer for end-to-end mandarin chinese speech recognition with pinyin and character. ||| 1381 ||| 33757 ||| 3049 ||| 18443 ||| 33758 ||| 
2020 ||| sparsifying transformer models with differentiable representation pooling. ||| 17338 ||| 17335 ||| 33759 ||| 
2021 ||| translational equivariance in kernelizable attention. ||| 33760 ||| 33761 ||| 33762 ||| 33763 ||| 
2019 ||| latent suicide risk detection on microblog via suicide-oriented word embeddings and layered attention. ||| 26456 ||| 26457 ||| 26458 ||| 26459 ||| 398 ||| 26460 ||| 26461 ||| 
2020 ||| quantifying attention flow in transformers. ||| 3189 ||| 3190 ||| 
2020 ||| emptransfo: a multi-head transformer architecture for creating empathetic dialog systems. ||| 3939 ||| 3940 ||| 
2021 ||| bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification. ||| 2715 ||| 2716 ||| 2717 ||| 
2021 ||| variational transformer networks for layout generation. ||| 19257 ||| 19258 ||| 19259 ||| 19260 ||| 
2020 ||| graphspeech: syntax-aware graph attention network for neural speech synthesis. ||| 1840 ||| 12493 ||| 12494 ||| 
2021 ||| robust lane detection via expanded self attention. ||| 7225 ||| 7280 ||| 7281 ||| 7282 ||| 33764 ||| 7228 ||| 
2021 ||| twins: revisiting spatial attention design in vision transformers. ||| 6433 ||| 15886 ||| 10192 ||| 1099 ||| 7972 ||| 6935 ||| 5155 ||| 6335 ||| 
2022 ||| transformer-based approaches for legal text processing. ||| 32248 ||| 32252 ||| 33765 ||| 32251 ||| 33766 ||| 32250 ||| 32249 ||| 4735 ||| 33767 ||| 
2018 ||| deep ordinal hashing with spatial attention. ||| 13226 ||| 30753 ||| 2233 ||| 13227 ||| 7380 ||| 8536 ||| 
2021 ||| attention-based feature decomposition-reconstruction network for scene text detection. ||| 1872 ||| 2852 ||| 8011 ||| 32064 ||| 
2020 ||| multi-task network for noise-robust keyword spotting and speaker verification using ctc-based soft vad and global query attention. ||| 14584 ||| 14396 ||| 14585 ||| 14397 ||| 
2021 ||| transvos: video object segmentation with transformers. ||| 33768 ||| 33769 ||| 33770 ||| 4297 ||| 
2022 ||| towards data-efficient detection transformers. ||| 8948 ||| 875 ||| 1903 ||| 33771 ||| 1756 ||| 
2021 ||| dodrio: exploring transformer models with interactive visualization. ||| 33772 ||| 33773 ||| 33774 ||| 
2019 ||| xlsor: a robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. ||| 14910 ||| 14911 ||| 705 ||| 14912 ||| 
2021 ||| facial attribute transformers for precise and robust makeup transfer. ||| 7422 ||| 7423 ||| 33775 ||| 7425 ||| 7426 ||| 2166 ||| 
2021 ||| stochastic layers in vision transformers. ||| 33776 ||| 33777 ||| 33778 ||| 7814 ||| 
2021 ||| do transformer modifications transfer across implementations and applications? ||| 13146 ||| 26712 ||| 1398 ||| 32606 ||| 26714 ||| 26715 ||| 26716 ||| 26717 ||| 26718 ||| 9132 ||| 26719 ||| 26720 ||| 3337 ||| 26721 ||| 26722 ||| 4822 ||| 3338 ||| 
2021 ||| conditional generative data-free knowledge distillation based on attention transfer. ||| 33779 ||| 33780 ||| 33781 ||| 
2022 ||| similarity and content-based phonetic self attention for speech recognition. ||| 1611 ||| 1613 ||| 
2019 ||| rasnet: segmentation for tracking surgical instruments in surgical videos using refined attention segmentation network. ||| 5183 ||| 5184 ||| 24364 ||| 271 ||| 5185 ||| 276 ||| 
2022 ||| chitransformer: towards reliable stereo from cues. ||| 33782 ||| 33783 ||| 
2019 ||| multimodal unified attention networks for vision-and-language interactions. ||| 1753 ||| 19336 ||| 1754 ||| 1756 ||| 2398 ||| 
2021 ||| development and testing of an image transformer for explainable autonomous driving systems. ||| 23637 ||| 23638 ||| 23639 ||| 23640 ||| 33784 ||| 23641 ||| 
2020 ||| transformer vq-vae for unsupervised unit discovery and speech synthesis: zerospeech 2020 challenge. ||| 12303 ||| 13907 ||| 11757 ||| 
2022 ||| translog: a unified transformer-based framework for log anomaly detection. ||| 33785 ||| 33786 ||| 1825 ||| 33787 ||| 33788 ||| 33789 ||| 1099 ||| 843 ||| 
2019 ||| self-attention networks for connectionist temporal classification in speech recognition. ||| 4739 ||| 12359 ||| 12360 ||| 
2022 ||| keypoints tracking via transformer networks. ||| 33790 ||| 1226 ||| 33791 ||| 
2020 ||| keyphrase generation with cross-document attention. ||| 3631 ||| 3198 ||| 2814 ||| 
2020 ||| augmented transformer achieves 97% and 85% for top5 prediction of direct and classical retro-synthesis. ||| 4125 ||| 4123 ||| 33250 ||| 4124 ||| 
2020 ||| transformer in action: a comparative study of transformer-based acoustic models for large scale speech recognition applications. ||| 11973 ||| 11974 ||| 11975 ||| 11976 ||| 11977 ||| 11978 ||| 11979 ||| 
2021 ||| dualformer: local-global stratified transformer for efficient video recognition. ||| 23497 ||| 12300 ||| 8364 ||| 1728 ||| 
2017 ||| residual attention network for image classification. ||| 2355 ||| 18789 ||| 18226 ||| 18790 ||| 130 ||| 675 ||| 1846 ||| 18791 ||| 
2019 ||| improving attention mechanism in graph neural networks via cardinality preservation. ||| 1315 ||| 12384 ||| 
2019 ||| effective use of transformer networks for entity tracking. ||| 26795 ||| 26357 ||| 
2017 ||| iterative multi-document neural attention for multiple answer prediction. ||| 9810 ||| 33792 ||| 22334 ||| 33793 ||| 10683 ||| 
2021 ||| spatial attention improves iterative 6d object pose estimation. ||| 13605 ||| 2123 ||| 
2022 ||| efficient classification of long documents using transformers. ||| 33794 ||| 33795 ||| 33796 ||| 
2021 ||| incorporating transformer and lstm to kalman filter with em algorithm for state estimation. ||| 33797 ||| 
2022 ||| graph decipher: a transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification. ||| 33798 ||| 859 ||| 
2021 ||| bolt-dumbo transformer: asynchronous consensus as fast as pipelined bft. ||| 33799 ||| 33800 ||| 33801 ||| 
2020 ||| an image is worth 16x16 words: transformers for image recognition at scale. ||| 9291 ||| 23909 ||| 7979 ||| 9289 ||| 23910 ||| 2571 ||| 2293 ||| 23911 ||| 2294 ||| 23912 ||| 4960 ||| 23913 ||| 
2022 ||| swin-pose: swin transformer based human pose estimation. ||| 33802 ||| 33803 ||| 949 ||| 11359 ||| 4979 ||| 
2021 ||| han: higher-order attention network for spoken language understanding. ||| 28508 ||| 1229 ||| 4430 ||| 
2017 ||| saliency-based sequential image attention with multiset prediction. ||| 9342 ||| 9343 ||| 3008 ||| 1770 ||| 
2021 ||| g-transformer for document-level machine translation. ||| 3606 ||| 3289 ||| 3607 ||| 3470 ||| 3471 ||| 
2021 ||| m2gan: a multi-stage self-attention network for image rain removal on autonomous vehicles. ||| 33804 ||| 2843 ||| 
2020 ||| patenttransformer-2: controlling patent text generation by structural metadata. ||| 8059 ||| 8060 ||| 
2020 ||| weakly supervised attention pyramid convolutional neural network for fine-grained visual classification. ||| 33805 ||| 33806 ||| 1483 ||| 1482 ||| 1484 ||| 33807 ||| 2163 ||| 
2018 ||| formal verification of spacecraft control programs using a metalanguage for state transformers. ||| 33808 ||| 33809 ||| 33810 ||| 
2022 ||| attention mechanism meets with hybrid dense network for hyperspectral image classification. ||| 33811 ||| 33812 ||| 33813 ||| 33814 ||| 33815 ||| 24821 ||| 
2019 ||| tf-attention-net: an end to end neural network for singing voice separation. ||| 14621 ||| 1060 ||| 21341 ||| 765 ||| 
2021 ||| tunet: a block-online bandwidth extension model based on transformers and self-supervised pretraining. ||| 33816 ||| 33817 ||| 33818 ||| 
2019 ||| modeling recurrence for transformer. ||| 4796 ||| 3309 ||| 3037 ||| 3038 ||| 4797 ||| 3041 ||| 
2018 ||| an initial attempt of combining visual selective attention with deep reinforcement learning. ||| 33819 ||| 8824 ||| 8830 ||| 
2019 ||| whatcha lookin' at? deeplifting bert's attention in question answering. ||| 33820 ||| 33821 ||| 
2021 ||| more identifiable yet equally performant transformers for text classification. ||| 3657 ||| 3658 ||| 892 ||| 3659 ||| 
2021 ||| self-attention networks can process bounded hierarchical languages. ||| 3648 ||| 3649 ||| 3650 ||| 3651 ||| 
2019 ||| residual attention graph convolutional network for geometric 3d scene classification. ||| 7907 ||| 7908 ||| 
2022 ||| efficient long-range attention network for image super-resolution. ||| 33822 ||| 33823 ||| 33824 ||| 241 ||| 
2021 ||| bioie: biomedical information extraction with multi-head attention enhanced graph convolutional network. ||| 16603 ||| 1305 ||| 27516 ||| 16755 ||| 16606 ||| 399 ||| 
2019 ||| multi-vision attention networks for on-line red jujube grading. ||| 33218 ||| 11420 ||| 33219 ||| 
2021 ||| nast: non-autoregressive spatial-temporal transformer for time series forecasting. ||| 472 ||| 1382 ||| 436 ||| 22154 ||| 23597 ||| 14153 ||| 
2020 ||| focus longer to see better: recursively refined attention for fine-grained image classification. ||| 19180 ||| 19181 ||| 1905 ||| 7195 ||| 
2017 ||| mining fine-grained opinions on closed captions of youtube videos with an attention-rnn. ||| 7447 ||| 20763 ||| 20764 ||| 
2021 ||| fully attentional network for semantic segmentation. ||| 14552 ||| 4634 ||| 33825 ||| 8358 ||| 1858 ||| 
2022 ||| an attention-based convlstm autoencoder with dynamic thresholding for unsupervised anomaly detection in multivariate time series. ||| 33826 ||| 33827 ||| 33828 ||| 33829 ||| 
2021 ||| knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. ||| 1126 ||| 1124 ||| 1125 ||| 2588 ||| 1123 ||| 17850 ||| 17851 ||| 9575 ||| 
2020 ||| image captioning with attention for smart local tourism using efficientnet. ||| 33830 ||| 33831 ||| 33832 ||| 33833 ||| 33834 ||| 33835 ||| 33836 ||| 
2020 ||| generalisable cardiac structure segmentation via attentional and stacked image adaptation. ||| 14846 ||| 14896 ||| 14848 ||| 
2021 ||| attention to warp: deep metric learning for multivariate time series. ||| 17391 ||| 17392 ||| 17393 ||| 12269 ||| 12271 ||| 6933 ||| 6934 ||| 
2019 ||| an attention-guided deep regression model for landmark detection in cephalograms. ||| 27547 ||| 4634 ||| 27548 ||| 27549 ||| 6621 ||| 
2019 ||| transmission lines positive sequence parameters estimation and instrument transformers calibration based on pmu measurement error model. ||| 5187 ||| 33837 ||| 33838 ||| 33839 ||| 
2021 ||| transformer-based models for question answering on covid19. ||| 33840 ||| 33841 ||| 33842 ||| 33843 ||| 
2022 ||| a prospective approach for human-to-human interaction recognition from wi-fi channel data using attention bidirectional gated recurrent neural network with gui application implementation. ||| 33844 ||| 33845 ||| 33846 ||| 
2018 ||| attention based natural language grounding by navigating virtual environment. ||| 7390 ||| 7391 ||| 7254 ||| 3237 ||| 
2022 ||| attention-based deep neural networks for battery discharge capacity forecasting. ||| 33847 ||| 33848 ||| 19066 ||| 
2021 ||| learning fair face representation with progressive cross transformer. ||| 2969 ||| 33849 ||| 2816 ||| 1916 ||| 1825 ||| 
2021 ||| counterfactual attention learning for fine-grained visual categorization and re-identification. ||| 2115 ||| 1917 ||| 1920 ||| 1921 ||| 
2019 ||| attention-based fusion for outfit recommendation. ||| 33850 ||| 10702 ||| 
2021 ||| contrastive out-of-distribution detection for pretrained transformers. ||| 21161 ||| 15099 ||| 
2019 ||| self-attention aligner: a latency-control end-to-end model for asr using self-attention network and chunk-hopping. ||| 5269 ||| 6832 ||| 728 ||| 
2020 ||| investigating the true performance of transformers in low-resource languages: a case study in automatic corpus creation. ||| 22559 ||| 33851 ||| 33852 ||| 33853 ||| 22560 ||| 
2022 ||| fedformer: frequency enhanced decomposed transformer for long-term series forecasting. ||| 26279 ||| 33854 ||| 33855 ||| 9464 ||| 19819 ||| 32211 ||| 
2022 ||| spherical transformer. ||| 7336 ||| 33856 ||| 7338 ||| 
2020 ||| longformer: the long-document transformer. ||| 3124 ||| 33857 ||| 3122 ||| 
2021 ||| relational self-attention: what's missing in attention for video understanding. ||| 33858 ||| 33859 ||| 7910 ||| 11601 ||| 17737 ||| 
2021 ||| on the expressive power of self-attention matrices. ||| 24034 ||| 22791 ||| 24042 ||| 
2020 ||| translating natural language instructions for behavioral robot navigation with a multi-head attention mechanism. ||| 33860 ||| 21680 ||| 7432 ||| 
2020 ||| end-to-end domain adaptive attention network for cross-domain person re-identification. ||| 33861 ||| 11374 ||| 11330 ||| 11331 ||| 
2020 ||| stronger transformers for neural multi-hop question generation. ||| 21387 ||| 18010 ||| 33862 ||| 9237 ||| 
2019 ||| comic: towards a compact image captioning model with attention. ||| 33863 ||| 7852 ||| 27311 ||| 
2020 ||| attention and misinformation sharing on social media. ||| 26083 ||| 26084 ||| 11483 ||| 
2022 ||| stdan: deformable attention network for space-time video super-resolution. ||| 33864 ||| 33865 ||| 33866 ||| 6116 ||| 6117 ||| 
2021 ||| decoupling the role of data, attention, and losses in multimodal transformers. ||| 3717 ||| 33867 ||| 33868 ||| 1993 ||| 3718 ||| 
2019 ||| neural review rating prediction with hierarchical attentions and latent factors. ||| 4165 ||| 4166 ||| 8198 ||| 3755 ||| 9599 ||| 696 ||| 9574 ||| 
2021 ||| d2a u-net: automatic segmentation of covid-19 lesions from ct slices with dilated convolution and dual attention mechanism. ||| 9055 ||| 989 ||| 33449 ||| 33446 ||| 33451 ||| 14908 ||| 33869 ||| 33870 ||| 33453 ||| 
2021 ||| cpt: convolutional point transformer for 3d point cloud processing. ||| 15636 ||| 33871 ||| 8582 ||| 33872 ||| 
2021 ||| dla-net: learning dual local attention features for semantic segmentation of large-scale building facade point clouds. ||| 17597 ||| 17595 ||| 17599 ||| 5287 ||| 33873 ||| 33874 ||| 3691 ||| 
2021 ||| iip-transformer: intra-inter-part transformer for skeleton-based action recognition. ||| 33875 ||| 33876 ||| 17570 ||| 33877 ||| 33878 ||| 33879 ||| 
2022 ||| ai can evolve without labels: self-evolving vision transformer for chest x-ray diagnosis through knowledge distillation. ||| 31257 ||| 31258 ||| 31259 ||| 31260 ||| 31261 ||| 31262 ||| 31263 ||| 31264 ||| 33880 ||| 2048 ||| 
2021 ||| multi-attention-based soft partition network for vehicle re-identification. ||| 33881 ||| 33882 ||| 33883 ||| 
2017 ||| single shot text detector with regional attention. ||| 2446 ||| 2447 ||| 2448 ||| 2449 ||| 2149 ||| 2450 ||| 
2021 ||| toward accurate and realistic outfits visualization with attention to details. ||| 18817 ||| 18818 ||| 18819 ||| 18820 ||| 
2019 ||| multi-modal simultaneous forecasting of vehicle position sequences using social attention. ||| 21860 ||| 21861 ||| 21862 ||| 21863 ||| 21864 ||| 21865 ||| 
2020 ||| can transformers reason about effects of actions? ||| 33884 ||| 33885 ||| 27544 ||| 33886 ||| 33887 ||| 33888 ||| 33889 ||| 
2022 ||| caft: clustering and filter on tokens of transformer for weakly supervised object localization. ||| 765 ||| 
2019 ||| improving relation extraction with knowledge-attention. ||| 481 ||| 740 ||| 17665 ||| 6721 ||| 
2021 ||| early lane change prediction for automated driving systems using multi-task attention-based convolutional neural networks. ||| 33890 ||| 33891 ||| 33892 ||| 33893 ||| 
2017 |||  attention-driven neural distant supervision. ||| 33894 ||| 9769 ||| 9770 ||| 
2021 ||| ai-upv at iberlef-2021 detoxis task: toxicity detection in immigration-related web news comments using transformers and statistical models. ||| 16483 ||| 16484 ||| 16485 ||| 
2018 ||| next item recommendation with self-attention. ||| 3364 ||| 1398 ||| 771 ||| 1397 ||| 
2021 ||| task adaptive pretraining of transformers for hostility detection. ||| 10508 ||| 15035 ||| 15036 ||| 15037 ||| 1185 ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| 2170 ||| 2171 ||| 1846 ||| 1847 ||| 1848 ||| 
2021 ||| deep transformers for fast small intestine grounding in capsule endoscope video. ||| 15507 ||| 15508 ||| 6810 ||| 15509 ||| 15510 ||| 1800 ||| 
2020 ||| iitk at semeval-2020 task 10: transformers for emphasis selection. ||| 10422 ||| 10423 ||| 10424 ||| 4930 ||| 
2018 ||| parallel attention mechanisms in neural machine translation. ||| 33895 ||| 20513 ||| 
2017 ||| brain inspired cognitive model with attention for self-driving cars. ||| 29022 ||| 29023 ||| 29024 ||| 29025 ||| 14779 ||| 
2021 ||| regionvit: regional-to-local attention for vision transformers. ||| 33896 ||| 2521 ||| 2369 ||| 
2020 ||| ctc-synchronous training for monotonic attention model. ||| 12682 ||| 12683 ||| 4418 ||| 
2020 ||| leugan: low-light image enhancement by unsupervised generative attentional networks. ||| 33897 ||| 859 ||| 33898 ||| 
2020 ||| cross-modal self-attention distillation for prostate cancer segmentation. ||| 16821 ||| 16822 ||| 5298 ||| 16823 ||| 16824 ||| 16825 ||| 16826 ||| 5300 ||| 
2021 ||| attention-based distributed speech enhancement for unconstrained microphone arrays with varying number of nodes. ||| 8228 ||| 8229 ||| 8062 ||| 8230 ||| 
2022 ||| training vision transformers with only 2040 images. ||| 33899 ||| 5447 ||| 1863 ||| 
2021 ||| audio transformers: transformer architectures for large scale audio understanding. adieu convolutions. ||| 33900 ||| 33901 ||| 
2021 ||| spatial-temporal transformer for 3d point cloud sequences. ||| 7268 ||| 5170 ||| 7269 ||| 7270 ||| 7271 ||| 
2022 ||| visual attention analysis of pathologists examining whole slide images of prostate cancer. ||| 33902 ||| 18879 ||| 18881 ||| 33903 ||| 18710 ||| 18882 ||| 18884 ||| 
2021 ||| voice quality and pitch features in transformer-based speech recognition. ||| 33904 ||| 33905 ||| 33906 ||| 23563 ||| 7111 ||| 
2021 ||| loglab: attention-based labeling of log data anomalies via weak supervision. ||| 8327 ||| 8328 ||| 8329 ||| 8330 ||| 
2020 ||| mmm : exploring conditional multi-track music generation with the transformer. ||| 33907 ||| 102 ||| 
2021 ||| methodology to assess quality, presence, empathy, attitude, and attention in social vr: international experiences use case. ||| 33908 ||| 33909 ||| 2600 ||| 12581 ||| 33910 ||| 26875 ||| 33911 ||| 3419 ||| 
2021 ||| svt-net: a super light-weight network for large scale place recognition using sparse voxel transformers. ||| 23841 ||| 33912 ||| 23840 ||| 532 ||| 23842 ||| 
2020 ||| t-recs: a transformer-based recommender generating textual explanations and integrating unsupervised language-based critiquing. ||| 18467 ||| 6979 ||| 33913 ||| 
2019 ||| cloze-driven pretraining of self-attention networks. ||| 3823 ||| 26362 ||| 26363 ||| 24017 ||| 3825 ||| 
2019 ||| multi-task bidirectional transformer representations for irony detection. ||| 10441 ||| 3152 ||| 
2021 ||| trans4trans: efficient transformer for transparent object and semantic scene segmentation in real-world navigation assistance. ||| 7856 ||| 7857 ||| 7858 ||| 7859 ||| 7860 ||| 3831 ||| 7861 ||| 
2019 ||| syntax-enhanced self-attention-based semantic role labeling. ||| 3289 ||| 3049 ||| 3087 ||| 
2021 ||| improved transformer for high-resolution gans. ||| 21442 ||| 17964 ||| 10333 ||| 1749 ||| 14048 ||| 
2022 ||| the principle of diversity: training stronger vision transformers calls for reducing all levels of redundancy. ||| 19181 ||| 758 ||| 2045 ||| 33914 ||| 7195 ||| 
2020 ||| fcanet: frequency channel attention networks. ||| 2256 ||| 2257 ||| 2258 ||| 2259 ||| 
2021 ||| graph attention network for microwave imaging of brain anomaly. ||| 33915 ||| 15544 ||| 33916 ||| 
2022 ||| table structure recognition with conditional attention. ||| 1956 ||| 33917 ||| 33918 ||| 33919 ||| 
2017 ||| what does attention in neural machine translation pay attention to? ||| 14936 ||| 11693 ||| 
2021 ||| ppt fusion: pyramid patch transformerfor a case study in image fusion. ||| 32066 ||| 32296 ||| 6524 ||| 6525 ||| 
2020 ||| fat albert: finding answers in large texts using semantic similarity attention layer based on bert. ||| 33920 ||| 33921 ||| 33922 ||| 33923 ||| 33924 ||| 
2020 ||| audio-visual event localization via recursive fusion by joint co-attention. ||| 7130 ||| 435 ||| 1160 ||| 7131 ||| 7132 ||| 127 ||| 
2021 ||| a probabilistic hard attention model for sequentially observed scenes. ||| 33925 ||| 18856 ||| 
2019 ||| squeeze-and-attention networks for semantic segmentation. ||| 19191 ||| 19192 ||| 19193 ||| 19194 ||| 19195 ||| 19197 ||| 7865 ||| 
2020 ||| vmrfanet: view-specific multi-receptive field attention network for person re-identification. ||| 19041 ||| 20719 ||| 19042 ||| 20720 ||| 19043 ||| 
2022 ||| attention-based proposals refinement for 3d object detection. ||| 33926 ||| 33927 ||| 33928 ||| 33929 ||| 33930 ||| 
2018 ||| attention-gan for object transfiguration in wild images. ||| 8531 ||| 3156 ||| 8532 ||| 1756 ||| 
2021 ||| exploring multi-task multi-lingual learning of transformer models for hate speech and offensive speech identification in social media. ||| 8410 ||| 8411 ||| 8412 ||| 
2019 ||| capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation. ||| 33678 ||| 12092 ||| 
2018 ||| interpretable visual question answering by visual grounding from attention supervision mining. ||| 7430 ||| 7431 ||| 7432 ||| 
2020 ||| scouter: slot attention-based classifier for explainable image recognition. ||| 1873 ||| 1874 ||| 1875 ||| 1876 ||| 1877 ||| 1878 ||| 
2019 ||| interpretation of feature space using multi-channel attentional sub-networks. ||| 19136 ||| 19137 ||| 
2021 ||| multistream graph attention networks for wind speed forecasting. ||| 27267 ||| 27268 ||| 
2019 ||| self-attention based bilstm-cnn classifier for the prediction of ischemic and non-ischemic cardiomyopathy. ||| 33931 ||| 33932 ||| 33933 ||| 33934 ||| 33935 ||| 
2019 ||| humor detection: a transformer gets the last laugh. ||| 26289 ||| 26290 ||| 
2020 ||| scalar coupling constant prediction using graph embedding local attention encoder. ||| 33936 ||| 33937 ||| 12196 ||| 27377 ||| 
2020 ||| one model to pronounce them all: multilingual grapheme-to-phoneme conversion with a transformer ensemble. ||| 13269 ||| 3152 ||| 13270 ||| 
2020 ||| how to teach dnns to pay attention to the visual modality in speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2020 ||| where is the model looking at?-concentrate and explain the network attention. ||| 19479 ||| 19478 ||| 602 ||| 21132 ||| 7697 ||| 21133 ||| 
2019 ||| knowledge enhanced attention for robust natural language inference. ||| 33938 ||| 33939 ||| 
2021 ||| attention-based neural re-ranking approach for next city in trip recommendations. ||| 22933 ||| 22934 ||| 
2021 ||| probing image-language transformers for verb understanding. ||| 3717 ||| 3718 ||| 
2021 ||| attention weights in transformer nmt fail aligning words between sequences but largely explain model predictions. ||| 26474 ||| 3466 ||| 
2020 ||| communities of attention networks: introducing qualitative and conversational perspectives for altmetrics. ||| 33940 ||| 
2019 ||| a sensitivity analysis of attention-gated convolutional neural networks for sentence classification. ||| 1305 ||| 27886 ||| 8154 ||| 33941 ||| 33942 ||| 
2021 ||| iris presentation attack detection by attention-based and deep pixel-wise binary supervision network. ||| 7170 ||| 7171 ||| 18595 ||| 7172 ||| 7173 ||| 
2021 ||| easy and efficient transformer : scalable inference solution for large nlp mode. ||| 33943 ||| 33944 ||| 33945 ||| 33946 ||| 33947 ||| 25420 ||| 33948 ||| 33949 ||| 
2021 ||| a dynamic residual self-attention network for lightweight single image super-resolution. ||| 33950 ||| 33951 ||| 32238 ||| 
2021 ||| prostformer: pre-trained progressive space-time self-attention model for traffic flow forecasting. ||| 33952 ||| 33953 ||| 33954 ||| 3049 ||| 
2018 ||| character-level language modeling with deeper self-attention. ||| 4824 ||| 17878 ||| 4821 ||| 3789 ||| 8069 ||| 
2021 ||| crossatnet - a novel cross-attention based framework for sketch-based image retrieval. ||| 6864 ||| 6865 ||| 6866 ||| 6867 ||| 
2019 ||| tanda: transfer and adapt pre-trained transformer models for answer sentence selection. ||| 18017 ||| 9680 ||| 3374 ||| 
2019 ||| siamese attentional keypoint network for high performance visual tracking. ||| 2170 ||| 33955 ||| 33956 ||| 31970 ||| 2355 ||| 
2022 ||| extreme precipitation forecasting using attention augmented convolutions. ||| 33957 ||| 
2021 ||| efficient vision transformers via fine-grained manifold distillation. ||| 33958 ||| 19744 ||| 19362 ||| 32730 ||| 19745 ||| 8862 ||| 1756 ||| 
2021 ||| inpainting transformer for anomaly detection. ||| 33959 ||| 33960 ||| 
2021 ||| mdmmt: multidomain multimodal transformer for video retrieval. ||| 19276 ||| 19277 ||| 19278 ||| 19279 ||| 
2017 ||| modality-specific cross-modal similarity measurement with recurrent attention network. ||| 5954 ||| 23380 ||| 5953 ||| 
2021 ||| incorporating convolution designs into visual transformers. ||| 2496 ||| 2497 ||| 2498 ||| 2499 ||| 2500 ||| 293 ||| 
2020 ||| using a bi-directional lstm model with attention mechanism trained on midi data for generating unique music. ||| 33961 ||| 33962 ||| 33963 ||| 
2022 ||| hts-at: a hierarchical token-semantic audio transformer for sound classification and detection. ||| 33964 ||| 14740 ||| 14741 ||| 12526 ||| 26812 ||| 33965 ||| 
2022 ||| transdarc: transformer-based driver activity recognition with latent space feature calibration. ||| 7859 ||| 23614 ||| 7857 ||| 7856 ||| 7861 ||| 
2021 ||| an emd-based method for the detection of power transformer faults with a hierarchical ensemble classifier. ||| 33966 ||| 33967 ||| 
2021 ||| each attribute matters: contrastive attention for sentence-based image editing. ||| 33968 ||| 10069 ||| 10072 ||| 4776 ||| 33969 ||| 33970 ||| 
2021 ||| vidtr: video transformer without convolutions. ||| 2419 ||| 2418 ||| 2420 ||| 2421 ||| 2422 ||| 2423 ||| 2424 ||| 2425 ||| 2426 ||| 
2022 ||| cnns and transformers perceive hybrid images similar to humans. ||| 1854 ||| 
2020 ||| attention dynamics on the chinese social media sina weibo during the covid-19 pandemic. ||| 29310 ||| 4194 ||| 33971 ||| 11944 ||| 
2019 ||| selective attention based graph convolutional networks for aspect-level sentiment classification. ||| 33972 ||| 14316 ||| 23361 ||| 33973 ||| 3561 ||| 3562 ||| 
2020 ||| entity linking via dual and cross-attention encoders. ||| 33974 ||| 33975 ||| 
2018 ||| multi-level structured self-attentions for distantly supervised relation extraction. ||| 26287 ||| 26735 ||| 16059 ||| 26736 ||| 
2021 ||| hierarchical transformer-based large-context end-to-end asr with large-context knowledge distillation. ||| 4408 ||| 10275 ||| 10274 ||| 10276 ||| 4407 ||| 10277 ||| 
2021 ||| where and when: space-time attention for audio-visual explanations. ||| 19100 ||| 33976 ||| 33710 ||| 33711 ||| 
2021 ||| tecanet: temporal-contextual attention network for environment-aware speech dereverberation. ||| 12636 ||| 11346 ||| 14634 ||| 12188 ||| 12293 ||| 1125 ||| 14635 ||| 12586 ||| 4530 ||| 3808 ||| 
2019 ||| transformer-xl: attentive language models beyond a fixed-length context. ||| 3780 ||| 3245 ||| 2622 ||| 3781 ||| 9372 ||| 3247 ||| 
2018 ||| uni-due student team: tackling fact checking through decomposable attention neural network. ||| 33977 ||| 33978 ||| 
2021 ||| pisltrc: position-informed sign language transformer with content-aware convolution. ||| 33979 ||| 33980 ||| 528 ||| 
2020 ||| face hallucination using split-attention in split-attention network. ||| 19757 ||| 19756 ||| 3906 ||| 19758 ||| 
2021 ||| medusa: multi-scale encoder-decoder self-attention deep neural network architecture for medical image analysis. ||| 33981 ||| 27912 ||| 27913 ||| 33982 ||| 33983 ||| 33984 ||| 7865 ||| 
2020 ||| m3d-cam: a pytorch library to generate 3d data attention maps for medical deep learning. ||| 13271 ||| 13272 ||| 8048 ||| 13273 ||| 13274 ||| 
2022 ||| look closer: bridging egocentric and third-person views with transformers for robotic manipulation. ||| 33985 ||| 33986 ||| 33987 ||| 14750 ||| 1117 ||| 
2021 ||| subdimensional expansion using attention-based learning for multi-agent path finding. ||| 33988 ||| 33989 ||| 33990 ||| 33991 ||| 
2020 ||| decoupled spatial-temporal attention network for skeleton-based action recognition. ||| 6389 ||| 2341 ||| 2343 ||| 2080 ||| 
2021 ||| synthesizing abstract transformers. ||| 33992 ||| 33993 ||| 33994 ||| 13191 ||| 33995 ||| 
2021 ||| distract your attention: multi-head cross attention network for facial expression recognition. ||| 33996 ||| 33997 ||| 128 ||| 33998 ||| 
2019 ||| graph transformer for graph-to-sequence learning. ||| 1115 ||| 3015 ||| 
2022 ||| investigating expressiveness of transformer in spectral domain for graphs. ||| 33999 ||| 34000 ||| 14085 ||| 34001 ||| 8139 ||| 34002 ||| 
2021 ||| attanet: attention-augmented network for fast and accurate scene parsing. ||| 14552 ||| 17943 ||| 1858 ||| 
2019 ||| eaten: entity-aware attention for single shot visual text extraction. ||| 17412 ||| 17413 ||| 17414 ||| 2538 ||| 2536 ||| 1761 ||| 
2020 ||| patient cohort retrieval using transformer language models. ||| 1638 ||| 1639 ||| 
2020 ||| multi-attention-network for semantic segmentation of high-resolution remote sensing images. ||| 8207 ||| 30454 ||| 30455 ||| 32217 ||| 
2017 ||| why do men get more attention? exploring factors behind success in an online design community. ||| 13307 ||| 34003 ||| 34004 ||| 13478 ||| 3369 ||| 13309 ||| 59 ||| 7111 ||| 13310 ||| 13311 ||| 13312 ||| 
2019 ||| the resale price prediction of secondhand jewelry items using a multi-modal deep model with iterative co-attention. ||| 34005 ||| 34006 ||| 34007 ||| 
2021 ||| hybrid local-global transformer for image dehazing. ||| 5803 ||| 2383 ||| 16570 ||| 7763 ||| 
2020 ||| modeling long-term and short-term interests with parallel attentions for session-based recommendation. ||| 11188 ||| 11189 ||| 11190 ||| 
2022 ||| gradvit: gradient inversion of vision transformers. ||| 7205 ||| 34008 ||| 34009 ||| 27860 ||| 24023 ||| 2024 ||| 34010 ||| 
2021 ||| u2-former: a nested u-shaped transformer for image restoration. ||| 34011 ||| 34012 ||| 1131 ||| 31046 ||| 26463 ||| 
2021 ||| multi-scale attention neural network for acoustic echo cancellation. ||| 6073 ||| 2754 ||| 34013 ||| 26837 ||| 
2021 ||| dancenet3d: music based dance generation with parametric motion transformer. ||| 34014 ||| 1573 ||| 2566 ||| 
2020 ||| economical visual attention test for elderly drivers. ||| 34015 ||| 
2018 ||| cross-relation cross-bag attention for distantly-supervised relation extraction. ||| 17722 ||| 4805 ||| 17723 ||| 17724 ||| 7654 ||| 1937 ||| 2258 ||| 1250 ||| 
2020 ||| perm2vec: graph permutation selection for decoding of error correction codes using self-attention. ||| 34016 ||| 1366 ||| 34017 ||| 1487 ||| 34018 ||| 
2017 ||| cross-domain image retrieval with attention modeling. ||| 19486 ||| 1160 ||| 149 ||| 11466 ||| 
2020 ||| up-detr: unsupervised pre-training for object detection with transformers. ||| 18841 ||| 18842 ||| 18843 ||| 18844 ||| 
2020 ||| where to look and how to describe: fashion image retrieval with an attentional heterogeneous bilinear network. ||| 28870 ||| 5845 ||| 6334 ||| 4175 ||| 5189 ||| 10075 ||| 
2021 ||| structure-regularized attention for deformable object representation. ||| 34019 ||| 30312 ||| 19196 ||| 683 ||| 
2022 ||| ensemble transformer for efficient and accurate ranking tasks: an application to question answering systems. ||| 9679 ||| 3373 ||| 34020 ||| 3374 ||| 
2021 ||| boosting transformers for job expression extraction and classification in a low-resource setting. ||| 16488 ||| 16489 ||| 15085 ||| 15086 ||| 
2021 ||| a survey of visual transformers. ||| 1305 ||| 5893 ||| 34021 ||| 34022 ||| 5332 ||| 27967 ||| 1420 ||| 27968 ||| 1755 ||| 23687 ||| 
2021 ||| sleep staging based on serialized dual attention network. ||| 28909 ||| 28910 ||| 336 ||| 28911 ||| 28912 ||| 28913 ||| 28914 ||| 
2021 ||| vitgan: training gans with vision transformers. ||| 34023 ||| 34024 ||| 18703 ||| 14048 ||| 1815 ||| 34025 ||| 
2020 ||| investigating african-american vernacular english in transformer-based text generation. ||| 26364 ||| 26365 ||| 26366 ||| 26367 ||| 26368 ||| 26369 ||| 3802 ||| 
2019 ||| interactive variance attention based online spoiler detection for time-sync comments. ||| 1437 ||| 1438 ||| 1439 ||| 1440 ||| 1441 ||| 
2021 ||| description-based label attention classifier for explainable icd-9 classification. ||| 24767 ||| 24768 ||| 15103 ||| 15822 ||| 
2020 ||| attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view. ||| 9642 ||| 9641 ||| 9643 ||| 9644 ||| 9407 ||| 7030 ||| 1094 ||| 
2021 ||| mcl@iitk at semeval-2021 task 2: multilingual and cross-lingual word-in-context disambiguation using augmented data, signals, and transformers. ||| 10607 ||| 10608 ||| 10609 ||| 4930 ||| 
2021 ||| a graph attention learning approach to antenna tilt optimization. ||| 34026 ||| 34027 ||| 34028 ||| 34029 ||| 34030 ||| 
2020 ||| concatenated attention neural network for image restoration. ||| 28857 ||| 25264 ||| 34031 ||| 28858 ||| 
2021 ||| conditional attention networks for distilling knowledge graphs in recommendation. ||| 1083 ||| 1084 ||| 1085 ||| 883 ||| 1086 ||| 1087 ||| 1088 ||| 
2017 ||| skeleton based human action recognition with global context-aware attention lstm networks. ||| 1235 ||| 8608 ||| 18938 ||| 18937 ||| 11620 ||| 
2017 ||| temporal attention augmented bilinear network for financial time-series data analysis. ||| 31737 ||| 926 ||| 31738 ||| 31739 ||| 
2021 ||| replica: enhanced feature pyramid network by local image translation and conjunct attention for high-resolution breast tumor detection. ||| 2341 ||| 18742 ||| 34032 ||| 34033 ||| 34034 ||| 
2020 ||| spike-triggered non-autoregressive transformer for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 12243 ||| 3364 ||| 12244 ||| 
2021 ||| estimating articulatory movements in speech production with transformer networks. ||| 14536 ||| 14537 ||| 14338 ||| 14339 ||| 14340 ||| 
2021 ||| opinion extraction as a structured sentiment analysis using transformers. ||| 34035 ||| 34036 ||| 
2021 ||| ptt: point-track-transformer module for 3d single object tracking in point clouds. ||| 25493 ||| 25494 ||| 4236 ||| 25495 ||| 
2019 ||| guided attention network for object detection and counting on drones. ||| 19505 ||| 8749 ||| 8750 ||| 8751 ||| 19506 ||| 8753 ||| 15557 ||| 
2020 ||| bridging text and video: a universal multimodal transformer for video-audio scene-aware dialog. ||| 3073 ||| 34037 ||| 3442 ||| 3076 ||| 3074 ||| 1921 ||| 
2020 ||| inside: steering spatial attention with non-imaging information in cnns. ||| 27387 ||| 18148 ||| 27369 ||| 27388 ||| 13369 ||| 
2020 ||| transformers are better than humans at identifying generated text. ||| 34038 ||| 34039 ||| 11716 ||| 11717 ||| 
2020 ||| hyperspectral image classification with attention aided cnns. ||| 34040 ||| 34041 ||| 6625 ||| 34042 ||| 34043 ||| 
2020 ||| on the dynamics of training attention models. ||| 23995 ||| 18115 ||| 23996 ||| 
2021 ||| visual composite set detection using part-and-sum transformers. ||| 2069 ||| 1815 ||| 2070 ||| 2071 ||| 1949 ||| 2072 ||| 
2020 ||| guided transformer: leveraging multiple external sources for representation learning in conversational search. ||| 9648 ||| 9585 ||| 1140 ||| 
2019 ||| stack-vs: stacked visual-semantic attention for image caption generation. ||| 5474 ||| 34044 ||| 34045 ||| 11710 ||| 34046 ||| 
2020 ||| cognitive-driven convolutional beamforming using eeg-based auditory attention decoding. ||| 1490 ||| 1491 ||| 1492 ||| 1493 ||| 1494 ||| 1495 ||| 
2020 ||| end-to-end neural transformer based spoken language understanding. ||| 12484 ||| 12485 ||| 12487 ||| 
2021 ||| infrared small-dim target detection with transformer under complex backgrounds. ||| 34047 ||| 11223 ||| 604 ||| 8850 ||| 2018 ||| 6621 ||| 
2020 ||| t-vse: transformer-based visual semantic embedding. ||| 34048 ||| 34049 ||| 34050 ||| 
2021 ||| mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction. ||| 19804 ||| 21242 ||| 19802 ||| 19803 ||| 5958 ||| 1730 ||| 7815 ||| 7814 ||| 
2020 ||| co-saliency detection with co-attention fully convolutional network. ||| 28851 ||| 2815 ||| 28852 ||| 5705 ||| 
2021 ||| case relation transformer: a crossmodal language generation model for fetching instructions. ||| 34051 ||| 726 ||| 
2019 ||| attention based pruning for shift networks. ||| 20222 ||| 34052 ||| 34053 ||| 20224 ||| 20225 ||| 9196 ||| 
2021 ||| attention based video summaries of live online zoom classes. ||| 34054 ||| 34055 ||| 34056 ||| 34057 ||| 34058 ||| 11483 ||| 
2019 ||| bertqa - attention on steroids. ||| 34059 ||| 34060 ||| 
2017 ||| attention clusters: purely attention based local feature integration for video classification. ||| 18016 ||| 2190 ||| 3774 ||| 18957 ||| 2530 ||| 2531 ||| 
2021 ||| indt5: a text-to-text transformer for 10 indigenous languages. ||| 3154 ||| 34061 ||| 3152 ||| 34062 ||| 
2021 ||| attention based semantic segmentation on uav dataset for natural disaster damage assessment. ||| 6770 ||| 6771 ||| 
2021 ||| transformer-based behavioral representation learning enables transfer learning for mobile sensing in small datasets. ||| 34063 ||| 34064 ||| 
2022 ||| raytran: 3d pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers. ||| 34065 ||| 34066 ||| 34067 ||| 7982 ||| 
2022 ||| persformer: 3d lane detection via perspective transformer and the openlane benchmark. ||| 3036 ||| 34068 ||| 438 ||| 34069 ||| 658 ||| 34070 ||| 5949 ||| 34071 ||| 8655 ||| 2149 ||| 2307 ||| 
2020 ||| visual-semantic graph attention network for human-object interaction detection. ||| 21852 ||| 21853 ||| 21850 ||| 
2021 ||| exercise? i thought you said 'extra fries': leveraging sentence demarcations and multi-hop attention for meme affect analysis. ||| 13313 ||| 3836 ||| 3835 ||| 
2021 ||| attnmove: history enhanced trajectory recovery via attentional network. ||| 17934 ||| 17935 ||| 6738 ||| 9069 ||| 8863 ||| 17936 ||| 2969 ||| 
2021 ||| staf: a spatio-temporal attention fusion network for few-shot video classification. ||| 34072 ||| 34073 ||| 9162 ||| 189 ||| 
2021 ||| redesigning the transformer architecture with insights from multi-particle dynamical systems. ||| 34074 ||| 34075 ||| 3860 ||| 3835 ||| 
2021 ||| multi-branch with attention network for hand-based person recognition. ||| 34076 ||| 34077 ||| 34078 ||| 34079 ||| 34080 ||| 
2021 ||| a study of social and behavioral determinants of health in lung cancer patients using transformers-based natural language processing models. ||| 21565 ||| 13408 ||| 34081 ||| 34082 ||| 34083 ||| 34084 ||| 34085 ||| 34086 ||| 13701 ||| 2989 ||| 12067 ||| 
2021 ||| packet routing with graph attention multi-agent reinforcement learning. ||| 15888 ||| 15889 ||| 15890 ||| 
2021 ||| parameter selection: why we should pay more attention to it. ||| 3375 ||| 3376 ||| 3377 ||| 3378 ||| 
2017 ||| ruminating reader: reasoning with gated multi-hop attention. ||| 3719 ||| 3720 ||| 
2018 ||| stacked cross attention for image-text matching. ||| 8673 ||| 5250 ||| 8674 ||| 8675 ||| 3561 ||| 
2022 ||| dxm-transfuse u-net: dual cross-modal transformer fusion u-net for automated nerve identification. ||| 34087 ||| 34088 ||| 34089 ||| 34090 ||| 34091 ||| 
2021 ||| medical image segmentation using squeeze-and-expansion transformers. ||| 23339 ||| 23340 ||| 23341 ||| 23342 ||| 4297 ||| 3390 ||| 
2021 ||| detection of transformer winding axial displacement by kirchhoff and delay and sum radar imaging algorithms. ||| 33208 ||| 34092 ||| 28294 ||| 34093 ||| 34094 ||| 
2020 ||| attention-based graph resnet for motor intent detection from raw eeg signals. ||| 34095 ||| 34096 ||| 34097 ||| 438 ||| 
2021 ||| bam: a lightweight and efficient balanced attention mechanism for single image super resolution. ||| 34098 ||| 34099 ||| 34100 ||| 
2020 ||| manipulated face detector: joint spatial and frequency domain attention network. ||| 34101 ||| 1007 ||| 
2021 ||| on pursuit of designing multi-modal transformer for video grounding. ||| 455 ||| 9570 ||| 26411 ||| 26412 ||| 4430 ||| 
2020 ||| segattngan: text to image generation with segmentation attention. ||| 34102 ||| 34103 ||| 569 ||| 34104 ||| 2995 ||| 
2018 ||| interpreting recurrent and attention-based neural models: a case study on natural language inference. ||| 4913 ||| 4914 ||| 4916 ||| 
2021 ||| towards reinforcement learning for pivot-based neural machine translation with non-autoregressive transformer. ||| 4727 ||| 4728 ||| 3732 ||| 4729 ||| 4730 ||| 4731 ||| 3454 ||| 
2021 ||| a comparative study of transformers on word sense disambiguation. ||| 20836 ||| 20837 ||| 20838 ||| 20839 ||| 10306 ||| 
2018 ||| combining pyramid pooling and attention mechanism for pelvic mr image semantic segmentaion. ||| 34105 ||| 34106 ||| 17436 ||| 34107 ||| 34108 ||| 
2022 ||| cmkd: cnn/transformer-based cross-model knowledge distillation for audio classification. ||| 1708 ||| 34109 ||| 34110 ||| 12254 ||| 
2021 ||| relaxed transformer decoders for direct action proposal generation. ||| 2214 ||| 2215 ||| 2216 ||| 2217 ||| 
2019 ||| collaborative self-attention for recommender systems. ||| 
2022 ||| botnets breaking transformers: localization of power botnet attacks against the distribution grid. ||| 34111 ||| 34112 ||| 34113 ||| 34114 ||| 34115 ||| 34116 ||| 989 ||| 34117 ||| 
2021 ||| exploring transformer based models to identify hate speech and offensive content in english and indo-aryan languages. ||| 34118 ||| 34119 ||| 34120 ||| 34121 ||| 34122 ||| 
2021 ||| agkd-bml: defense against adversarial attack by attention guided knowledge distillation and bi-directional metric learning. ||| 2614 ||| 2615 ||| 2616 ||| 2163 ||| 2617 ||| 
2020 ||| end to end dialogue transformer. ||| 34123 ||| 34124 ||| 34125 ||| 34126 ||| 
2020 ||| neutral theory for competing attention in social networks. ||| 34127 ||| 34128 ||| 34129 ||| 34130 ||| 10907 ||| 34131 ||| 34132 ||| 34133 ||| 34134 ||| 34135 ||| 34136 ||| 
2021 ||| something old, something new: grammar-based ccg parsing with transformer models. ||| 34137 ||| 
2017 ||| face attention network: an effective face detector for the occluded faces. ||| 292 ||| 2296 ||| 18222 ||| 
2019 ||| region attention networks for pose and occlusion robust facial expression recognition. ||| 333 ||| 8769 ||| 8771 ||| 11244 ||| 2149 ||| 
2021 ||| haconvgnn: hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks. ||| 26516 ||| 3095 ||| 26517 ||| 18010 ||| 
2019 ||| attention-based method for categorizing different types of online harassment language. ||| 7052 ||| 7053 ||| 
2019 ||| drug-drug adverse effect prediction with graph co-attention. ||| 34138 ||| 17094 ||| 23952 ||| 23955 ||| 1248 ||| 
2022 ||| pmp-net++: point cloud completion by transformer-enhanced multi-step point moving paths. ||| 2558 ||| 2557 ||| 2563 ||| 2560 ||| 2561 ||| 2562 ||| 2559 ||| 
2021 ||| ufo-vit: high performance linear vision transformer without softmax. ||| 34139 ||| 
2021 ||| multi-channel transformer transducer for speech recognition. ||| 12483 ||| 12484 ||| 12485 ||| 13949 ||| 
2021 ||| complexity-based partitioning of csfi problem instances with transformers. ||| 13974 ||| 34140 ||| 34141 ||| 
2020 ||| concentrated multi-grained multi-attention network for video based person re-identification. ||| 34142 ||| 34143 ||| 1858 ||| 
2020 ||| unsupervised foveal vision neural networks with top-down attention. ||| 850 ||| 34144 ||| 34145 ||| 852 ||| 853 ||| 854 ||| 
2021 ||| brain tumors classification for mr images based on attention guided deep learning model. ||| 24349 ||| 24350 ||| 24351 ||| 24352 ||| 24353 ||| 
2020 ||| multiresolution and multimodal speech recognition with transformers. ||| 3721 ||| 3722 ||| 3723 ||| 3724 ||| 
2020 ||| conformer: convolution-augmented transformer for speech recognition. ||| 14542 ||| 14472 ||| 3334 ||| 9133 ||| 9472 ||| 12814 ||| 14543 ||| 14544 ||| 11356 ||| 12067 ||| 3336 ||| 
2017 ||| a regularized framework for sparse and structured neural attention. ||| 9210 ||| 9211 ||| 
2021 ||| global-local transformer for brain age estimation. ||| 31319 ||| 31327 ||| 27851 ||| 
2020 ||| diablo: dictionary-based attention block for deep metric learning. ||| 30775 ||| 30776 ||| 30777 ||| 30778 ||| 
2019 ||| inter and intra document attention for depression risk assessment. ||| 16977 ||| 16978 ||| 16979 ||| 
2020 ||| attention-guided generative adversarial network to address atypical anatomy in modality transfer. ||| 15277 ||| 15278 ||| 15279 ||| 
2019 ||| denseattentionseg: segment hands from interacted objects using depth input. ||| 29356 ||| 17750 ||| 29357 ||| 29359 ||| 
2021 ||| stochastic transformer networks with linear competing units: application to end-to-end sl translation. ||| 1960 ||| 1961 ||| 1962 ||| 1749 ||| 1963 ||| 
2020 ||| learning dual semantic relations with graph attention for image-text matching. ||| 736 ||| 737 ||| 4260 ||| 
2021 ||| sam: a self-adaptive attention module for context-aware recommendation system. ||| 34146 ||| 34147 ||| 34148 ||| 34149 ||| 8349 ||| 4814 ||| 336 ||| 
2019 ||| text steganalysis with attentional lstm-cnn. ||| 2791 ||| 2792 ||| 2793 ||| 2794 ||| 2795 ||| 
2021 ||| gn-transformer: fusing sequence and graph representation for improved code summarization. ||| 26726 ||| 26727 ||| 26728 ||| 
2021 ||| grounding dialogue systems via knowledge graph aware decoding with pre-trained transformers. ||| 14073 ||| 14074 ||| 3581 ||| 
2020 ||| non-autoregressive transformer asr with ctc-enhanced decoder input. ||| 12584 ||| 3138 ||| 12585 ||| 12586 ||| 4530 ||| 4460 ||| 
2019 ||| hierarchical transformers for long document classification. ||| 13950 ||| 12580 ||| 12581 ||| 12582 ||| 13951 ||| 12583 ||| 
2019 ||| da-refinenet:a dual input whole slide image segmentation algorithm based on attention. ||| 20358 ||| 20359 ||| 20360 ||| 6502 ||| 
2020 ||| self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. ||| 19225 ||| 1134 ||| 1915 ||| 1916 ||| 1788 ||| 
2020 ||| denoising pre-training and data augmentation strategies for enhanced rdf verbalization with transformers. ||| 34150 ||| 34151 ||| 34152 ||| 16061 ||| 34153 ||| 
2020 ||| paying per-label attention for multi-label extraction from radiology reports. ||| 13366 ||| 27366 ||| 13368 ||| 13364 ||| 2227 ||| 13363 ||| 13365 ||| 27367 ||| 27368 ||| 27369 ||| 
2019 ||| how much research output from india gets social media attention? ||| 32874 ||| 32875 ||| 34154 ||| 34155 ||| 
2020 ||| multi-pass transformer for machine translation. ||| 2170 ||| 2507 ||| 17909 ||| 2508 ||| 11981 ||| 
2020 ||| tensorcoder: dimension-wise attention via tensor representation for natural language modeling. ||| 3364 ||| 989 ||| 5076 ||| 17839 ||| 34156 ||| 3443 ||| 
2017 ||| neobility at semeval-2017 task 1: an attention-based sentence similarity model. ||| 10493 ||| 10494 ||| 
2020 ||| a novel global spatial attention mechanism in convolutional neural network for medical image classification. ||| 34157 ||| 1244 ||| 34158 ||| 34159 ||| 34160 ||| 
2019 ||| stabilizing transformers for reinforcement learning. ||| 18744 ||| 22768 ||| 3709 ||| 22769 ||| 22770 ||| 2713 ||| 22771 ||| 22772 ||| 22773 ||| 34161 ||| 22775 ||| 22776 ||| 34162 ||| 22778 ||| 22289 ||| 
2020 ||| detecting expressions with multimodal transformers. ||| 3722 ||| 3724 ||| 
2020 ||| unpaired image enhancement with quality-attention generative adversarial network. ||| 19725 ||| 19726 ||| 19391 ||| 19727 ||| 19728 ||| 
2021 ||| speech emotion recognition with multiscale area attention and data augmentation. ||| 6206 ||| 2532 ||| 12171 ||| 781 ||| 
2019 ||| class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment. ||| 34163 ||| 
2018 ||| self-attentional acoustic models. ||| 3516 ||| 11470 ||| 3067 ||| 14607 ||| 69 ||| 3518 ||| 
2021 ||| combining efficientnet and vision transformers for video deepfake detection. ||| 34164 ||| 2689 ||| 2692 ||| 2691 ||| 
2022 ||| d^2etr: decoder-only detr with computationally efficient cross-scale attention. ||| 34165 ||| 34166 ||| 34167 ||| 4060 ||| 3001 ||| 12377 ||| 
2020 ||| flat: chinese ner using flat-lattice transformer. ||| 3815 ||| 3816 ||| 3272 ||| 3273 ||| 
2021 ||| mltr: multi-label classification with transformer. ||| 34168 ||| 34169 ||| 34170 ||| 1856 ||| 34171 ||| 2146 ||| 34172 ||| 34173 ||| 
2021 ||| thg: transformer with hyperbolic geometry. ||| 6474 ||| 34174 ||| 
2022 ||| end-to-end human-gaze-target detection with transformers. ||| 34175 ||| 11343 ||| 34176 ||| 2323 ||| 6516 ||| 8906 ||| 
2020 ||| multivariate time-series anomaly detection via graph attention network. ||| 18496 ||| 18497 ||| 18498 ||| 18499 ||| 18500 ||| 18501 ||| 18502 ||| 18503 ||| 18504 ||| 336 ||| 
2020 ||| human action performance using deep neuro-fuzzy recurrent attention model. ||| 24589 ||| 13077 ||| 13079 ||| 
2021 ||| relative molecule self-attention transformer. ||| 955 ||| 34177 ||| 956 ||| 34178 ||| 7369 ||| 34179 ||| 34180 ||| 34181 ||| 
2021 ||| ecg-based heart arrhythmia diagnosis through attentional convolutional neural networks. ||| 5401 ||| 586 ||| 
2021 ||| classification of breast cancer lesions in ultrasound images by using attention layer and loss ensembles in deep convolutional neural networks. ||| 34182 ||| 34183 ||| 34184 ||| 34185 ||| 34186 ||| 34187 ||| 34188 ||| 
2018 ||| linguistically-informed self-attention for semantic role labeling. ||| 9507 ||| 26625 ||| 26626 ||| 26627 ||| 24973 ||| 
2020 ||| query-key normalization for transformers. ||| 26697 ||| 26698 ||| 26699 ||| 26700 ||| 
2022 ||| do transformers use variable binding? ||| 34189 ||| 34190 ||| 34191 ||| 
2018 ||| bi-directional block self-attention for fast and memory-efficient sequence modeling. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 4873 ||| 
2021 ||| have attention heads in bert learned constituency grammar? ||| 24956 ||| 
2021 ||| an attention score based attacker for black-box nlp classifier. ||| 34192 ||| 34193 ||| 34194 ||| 
2021 ||| gated transformer networks for multivariate time series classification. ||| 34195 ||| 34196 ||| 34197 ||| 34198 ||| 31468 ||| 34199 ||| 17827 ||| 
2019 ||| l2g auto-encoder: understanding point clouds by local-to-global reconstruction with hierarchical self-attention. ||| 18229 ||| 2563 ||| 2558 ||| 2559 ||| 18230 ||| 
2021 ||| investigating the limitations of the transformers with simple arithmetic tasks. ||| 3006 ||| 3007 ||| 3009 ||| 
2022 ||| aprnet: attention-based pixel-wise rendering network for photo-realistic text image generation. ||| 28011 ||| 17445 ||| 472 ||| 7191 ||| 
2018 ||| iterative attention mining for weakly supervised thoracic disease pattern localization in chest x-rays. ||| 27490 ||| 27491 ||| 27492 ||| 17961 ||| 27493 ||| 17965 ||| 
2022 ||| from unstructured text to causal knowledge graphs: a transformer-based approach. ||| 26824 ||| 26823 ||| 34200 ||| 34201 ||| 
2019 ||| models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. ||| 34202 ||| 12109 ||| 3510 ||| 
2021 ||| text compression-aided transformer encoding. ||| 33655 ||| 34203 ||| 3111 ||| 3049 ||| 3112 ||| 4907 ||| 4908 ||| 
2021 ||| semantic communication with adaptive universal transformer. ||| 34204 ||| 6967 ||| 6968 ||| 34205 ||| 675 ||| 
2018 ||| self-attention with relative position representations. ||| 4959 ||| 4960 ||| 2466 ||| 
2021 ||| a generative model for raw audio using transformer architectures. ||| 33900 ||| 34206 ||| 
2022 ||| attention-based vandalism detection in openstreetmap. ||| 34207 ||| 34208 ||| 
2022 ||| spatio-temporal vision transformer for super-resolution microscopy. ||| 34209 ||| 34210 ||| 34211 ||| 23955 ||| 34212 ||| 
2020 ||| anchor-based spatial-temporal attention convolutional networks for dynamic 3d point cloud sequences. ||| 28417 ||| 28418 ||| 28419 ||| 28420 ||| 6474 ||| 25570 ||| 
2021 ||| explainable student performance prediction with personalized attention for explaining why a student fails. ||| 34213 ||| 34214 ||| 34215 ||| 
2021 ||| language-based video editing via multi-modal multi-level transformer. ||| 18272 ||| 34216 ||| 34217 ||| 15319 ||| 3802 ||| 
2019 ||| analyzing multi-head self-attention: specialized heads do the heavy lifting, the rest can be pruned. ||| 3844 ||| 3845 ||| 3846 ||| 3847 ||| 3848 ||| 
2017 ||| visual attention models for scene text recognition. ||| 17356 ||| 17357 ||| 9332 ||| 
2019 ||| dialogue transformers. ||| 34218 ||| 34219 ||| 34220 ||| 
2021 ||| attention-guided generative models for extractive question answering. ||| 3676 ||| 26438 ||| 12360 ||| 3251 ||| 
2020 ||| augmented equivariant attention networks for electron microscopy image super-resolution. ||| 34221 ||| 24987 ||| 23491 ||| 
2020 ||| spatial-angular attention network for light field reconstruction. ||| 1712 ||| 1716 ||| 1715 ||| 34222 ||| 
2021 ||| tntc: two-stream network with transformer-based complementarity for gait-based emotion recognition. ||| 24498 ||| 24984 ||| 17092 ||| 20835 ||| 
2020 ||| a simple yet effective method for video temporal grounding with cross-modality attention. ||| 34223 ||| 11641 ||| 1280 ||| 19739 ||| 34224 ||| 23393 ||| 
2020 ||| rotation averaging with attention graph neural networks. ||| 34225 ||| 34226 ||| 34227 ||| 
2021 ||| a million tweets are worth a few points: tuning transformers for customer service tasks. ||| 4935 ||| 4936 ||| 2253 ||| 4937 ||| 4938 ||| 4939 ||| 
2021 ||| focal attention networks: optimising attention for biomedical image segmentation. ||| 34228 ||| 34229 ||| 34230 ||| 34231 ||| 34232 ||| 6005 ||| 
2018 ||| r-vqa: learning visual relation facts with semantic attention for visual question answering. ||| 17712 ||| 18515 ||| 781 ||| 3706 ||| 3480 ||| 8907 ||| 
2018 ||| knowing where to look? analysis on attention of visual question answering system. ||| 3337 ||| 8725 ||| 8726 ||| 8727 ||| 
2022 ||| regression transformer: concurrent conditional generation and regression by blending numerical and textual tokens. ||| 32811 ||| 17954 ||| 
2021 ||| attention-based convolutional autoencoders for 3d-variational data assimilation. ||| 34233 ||| 34234 ||| 34235 ||| 34236 ||| 
2021 ||| scalable transformers for neural machine translation. ||| 2170 ||| 17909 ||| 2149 ||| 1846 ||| 1847 ||| 1848 ||| 
2021 ||| transformer for polyp detection. ||| 34237 ||| 34238 ||| 34239 ||| 11544 ||| 
2020 ||| tabtransformer: tabular data modeling using contextual embeddings. ||| 17578 ||| 34240 ||| 34241 ||| 34242 ||| 
2021 ||| a transformer-based cross-modal fusion model with adversarial training for vqa challenge 2021. ||| 34243 ||| 34244 ||| 17201 ||| 
2021 ||| towards transferable adversarial attacks on vision transformers. ||| 34245 ||| 14648 ||| 34246 ||| 7314 ||| 34247 ||| 17842 ||| 
2019 ||| attentional policies for cross-context multi-agent reinforcement learning. ||| 23619 ||| 23621 ||| 
2020 ||| advancing multiple instance learning with attention modeling for categorical speech emotion recognition. ||| 14519 ||| 14520 ||| 14521 ||| 14522 ||| 
2019 ||| attention-wrapped hierarchical blstms for ddi extraction. ||| 34248 ||| 34249 ||| 
2022 ||| cosformer: rethinking softmax in attention. ||| 3293 ||| 34250 ||| 34251 ||| 34252 ||| 34253 ||| 34254 ||| 2391 ||| 3139 ||| 34255 ||| 
2021 ||| attention bottlenecks for multimodal fusion. ||| 34256 ||| 12571 ||| 2292 ||| 34257 ||| 2093 ||| 2094 ||| 
2021 ||| regional attention network (ran) for head pose and fine-grained gesture recognition. ||| 6383 ||| 6384 ||| 34258 ||| 4611 ||| 7264 ||| 
2020 ||| finnish language modeling with deep transformer models. ||| 14722 ||| 12311 ||| 14723 ||| 14724 ||| 11941 ||| 
2020 ||| finding fast transformers: one-shot neural architecture search by component composition. ||| 26799 ||| 34259 ||| 26800 ||| 26712 ||| 34260 ||| 
2018 ||| exploring a unified attention-based pooling framework for speaker verification. ||| 1199 ||| 329 ||| 21340 ||| 10107 ||| 
2020 ||| ratt: recurrent attention to transient tasks for continual image captioning. ||| 9330 ||| 9331 ||| 9332 ||| 7105 ||| 
2021 ||| identifying ards using the hierarchical attention network with sentence objectives framework. ||| 34261 ||| 34262 ||| 34263 ||| 34264 ||| 34265 ||| 1636 ||| 
2021 ||| context-aware transformer transducer for speech recognition. ||| 12483 ||| 2058 ||| 12484 ||| 12485 ||| 13949 ||| 3841 ||| 12487 ||| 
2021 ||| rethinking lifelong sequential recommendation with incremental multi-interest attention. ||| 8911 ||| 8914 ||| 8912 ||| 8915 ||| 8913 ||| 1245 ||| 8916 ||| 
2020 ||| rethinking the value of transformer components. ||| 11656 ||| 3041 ||| 
2020 ||| looking glamorous: vehicle re-id in heterogeneous cameras networks with global and local attention. ||| 34266 ||| 34267 ||| 
2021 ||| softermax: hardware/software co-design of an efficient softmax for transformers. ||| 20514 ||| 20515 ||| 20516 ||| 20517 ||| 20518 ||| 
2019 ||| patent citation dynamics modeling via multi-attention recurrent networks. ||| 17189 ||| 17191 ||| 17190 ||| 17188 ||| 17192 ||| 8985 ||| 
2021 ||| hierarchical attention fusion for geo-localization. ||| 12439 ||| 3642 ||| 12440 ||| 12441 ||| 
2021 ||| fine-tuning transformers: vocabulary transfer. ||| 34268 ||| 3430 ||| 34269 ||| 13374 ||| 
2021 ||| multi-modal attention network for stock movements prediction. ||| 34270 ||| 27554 ||| 
2018 ||| volatility in the issue attention economy. ||| 34271 ||| 34272 ||| 34273 ||| 34274 ||| 
2021 ||| adversarial robustness comparison of vision transformer and mlp-mixer to cnns. ||| 34275 ||| 34276 ||| 34277 ||| 34278 ||| 5372 ||| 
2021 ||| cdtrans: cross-domain transformer for unsupervised domain adaptation. ||| 34279 ||| 34280 ||| 1703 ||| 1704 ||| 1705 ||| 32211 ||| 
2020 ||| camta: casual attention model for multi-touch attribution. ||| 18578 ||| 18579 ||| 18580 ||| 18581 ||| 607 ||| 18582 ||| 
2018 ||| close to human quality tts with transformer. ||| 17799 ||| 12389 ||| 14323 ||| 12137 ||| 124 ||| 3480 ||| 
2019 ||| attentional encoder network for targeted sentiment classification. ||| 4098 ||| 4099 ||| 4100 ||| 4101 ||| 4102 ||| 
2021 ||| geometry attention transformer with position-aware lstms for image captioning. ||| 4171 ||| 2909 ||| 34281 ||| 
2020 ||| multi-head attention with joint agent-map representation for trajectory prediction in autonomous driving. ||| 15463 ||| 15464 ||| 7888 ||| 15465 ||| 
2019 ||| near-optimal glimpse sequences for improved hard attention neural network training. ||| 34282 ||| 34283 ||| 34284 ||| 
2017 ||| online and linear-time attention by enforcing monotonic alignments. ||| 3338 ||| 22748 ||| 22749 ||| 12097 ||| 22750 ||| 
2020 ||| attention aware cost volume pyramid based multi-view stereo network for 3d reconstruction. ||| 34285 ||| 34286 ||| 3072 ||| 19066 ||| 398 ||| 34287 ||| 34288 ||| 
2020 ||| know what you don't need: single-shot meta-pruning for attention heads. ||| 34289 ||| 34290 ||| 3232 ||| 3443 ||| 3233 ||| 
2019 ||| co-attention based neural network for source-dependent essay scoring. ||| 789 ||| 3188 ||| 
2021 ||| coarse temporal attention network (cta-net) for driver's activity recognition. ||| 6384 ||| 6383 ||| 7263 ||| 7264 ||| 
2021 ||| transalnet: visual saliency prediction using transformers. ||| 34291 ||| 34292 ||| 34293 ||| 34294 ||| 34295 ||| 
2019 ||| effective attention modeling for neural relation extraction. ||| 23090 ||| 3195 ||| 
2021 ||| high-resolution complex scene synthesis with transformers. ||| 34296 ||| 1802 ||| 648 ||| 1804 ||| 
2021 ||| rethinking skip connection with layer normalization in transformers and resnets. ||| 3746 ||| 9350 ||| 30044 ||| 3751 ||| 4430 ||| 
2021 ||| lung cancer diagnosis using deep attention based on multiple instance learning and radiomics. ||| 34297 ||| 34298 ||| 20212 ||| 6654 ||| 34299 ||| 34300 ||| 6089 ||| 34301 ||| 
2019 ||| proposal-free temporal moment localization of a natural-language query in video using guided attention. ||| 7446 ||| 7447 ||| 7448 ||| 7449 ||| 7450 ||| 
2018 ||| modelling the dynamic joint policy of teammates with attention multi-agent ddpg. ||| 3917 ||| 3918 ||| 3919 ||| 3920 ||| 
2018 ||| integrating transformer and paraphrase rules for sentence simplification. ||| 26309 ||| 12634 ||| 26310 ||| 26311 ||| 26312 ||| 
2020 ||| whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks. ||| 27641 ||| 31211 ||| 27643 ||| 31212 ||| 1265 ||| 
2021 ||| position information in transformers: an overview. ||| 11714 ||| 11715 ||| 11716 ||| 11717 ||| 
2020 ||| learning selective mutual attention and contrast for rgb-d saliency detection. ||| 2411 ||| 2412 ||| 1932 ||| 2414 ||| 
2022 ||| numhtml: numeric-oriented hierarchical transformer model for multi-task financial forecasting. ||| 8872 ||| 34302 ||| 8875 ||| 3289 ||| 8874 ||| 
2020 ||| unsupervised deep metric learning with transformed attention consistency and contrastive clustering loss. ||| 438 ||| 8853 ||| 8854 ||| 
2018 ||| facial action unit detection using attention and relation learning. ||| 8795 ||| 5247 ||| 1691 ||| 33323 ||| 5141 ||| 
2021 ||| dga-net dynamic gaussian attention network for sentence semantic matching. ||| 1558 ||| 15204 ||| 444 ||| 1301 ||| 
2021 ||| sequence-to-sequence piano transcription with transformers. ||| 11910 ||| 11911 ||| 11912 ||| 11913 ||| 11914 ||| 
2021 ||| hit: a hierarchically fused deep attention network for robust code-mixed language representation. ||| 3833 ||| 3834 ||| 3835 ||| 3836 ||| 
2020 ||| frequency-based multi task learning with attention mechanism for fault detection in power systems. ||| 16253 ||| 16254 ||| 
2021 ||| towards interpretable attention networks for cervical cancer analysis. ||| 24162 ||| 24163 ||| 11374 ||| 2130 ||| 24164 ||| 
2020 ||| sparse sinkhorn attention. ||| 1398 ||| 3292 ||| 1041 ||| 3294 ||| 22745 ||| 
2021 ||| raw produce quality detection with shifted window self-attention. ||| 34303 ||| 34304 ||| 34305 ||| 
2021 ||| empirical evaluation of pre-trained transformers for human-level nlp: the role of sample size and dimensionality. ||| 4902 ||| 4903 ||| 4904 ||| 4905 ||| 3699 ||| 
2021 ||| random feature attention. ||| 9407 ||| 14940 ||| 23970 ||| 23971 ||| 3277 ||| 3139 ||| 
2018 ||| improved hybrid ctc-attention model for speech recognition. ||| 34306 ||| 34307 ||| 9204 ||| 31373 ||| 
2021 ||| anatomy-guided parallel bottleneck transformer network for automated evaluation of root canal therapy. ||| 27346 ||| 27347 ||| 2341 ||| 1224 ||| 17296 ||| 27349 ||| 25766 ||| 34308 ||| 34309 ||| 34310 ||| 34311 ||| 17295 ||| 11634 ||| 
2019 ||| attention model for articulatory features detection. ||| 14409 ||| 14410 ||| 
2019 ||| gram: scalable generative models for graphs with graph attention mechanism. ||| 34312 ||| 33317 ||| 19502 ||| 
2020 ||| adversarial watermarking transformer: towards tracing text provenance with data hiding. ||| 107 ||| 108 ||| 
2020 ||| attention-based planning with active perception. ||| 21811 ||| 3362 ||| 
2021 ||| effective attention sheds light on interpretability. ||| 3021 ||| 3022 ||| 
2020 ||| transformer based multilingual document embedding model. ||| 3337 ||| 21354 ||| 
2020 ||| transformer-based online ctc/attention end-to-end speech recognition architecture. ||| 12401 ||| 12101 ||| 12100 ||| 12104 ||| 8298 ||| 
2018 ||| self-attention generative adversarial networks. ||| 14048 ||| 22830 ||| 1749 ||| 9181 ||| 
2017 ||| paying more attention to saliency: image captioning with saliency and context attention. ||| 19001 ||| 19003 ||| 4700 ||| 13611 ||| 
2020 ||| da2: deep attention adapter for memory-efficienton-device multi-domain learning. ||| 2884 ||| 34313 ||| 34314 ||| 
2019 ||| joint source-target self attention with locality constraints. ||| 852 ||| 3467 ||| 3465 ||| 3466 ||| 
2022 ||| transfollower: long-sequence car-following trajectory prediction through transformer. ||| 34315 ||| 34316 ||| 34317 ||| 2792 ||| 34318 ||| 15832 ||| 
2020 ||| gradient-based adversarial training on transformer networks for detecting check-worthy factual claims. ||| 34319 ||| 34320 ||| 34321 ||| 34322 ||| 34323 ||| 34324 ||| 
2022 ||| representing long-range context for graph neural networks with global attention. ||| 3162 ||| 34325 ||| 23619 ||| 34326 ||| 34327 ||| 34328 ||| 
2021 ||| multi-factors aware dual-attentional knowledge tracing. ||| 1149 ||| 1150 ||| 1151 ||| 1152 ||| 1153 ||| 1154 ||| 
2019 ||| utterance-level end-to-end language identification using attention-based cnn-blstm. ||| 12366 ||| 12367 ||| 12368 ||| 765 ||| 
2021 ||| towards a unified foundation model: jointly pre-training transformers on unpaired images and text. ||| 3477 ||| 2193 ||| 34329 ||| 34330 ||| 33441 ||| 7143 ||| 34331 ||| 
2022 ||| rich cnn-transformer feature aggregation networks for super-resolution. ||| 34332 ||| 34333 ||| 17885 ||| 34334 ||| 7429 ||| 8755 ||| 
2017 ||| tracking gaze and visual focus of attention of people involved in social interaction. ||| 34335 ||| 34336 ||| 34337 ||| 34338 ||| 
2021 ||| joint intent detection and slot filling with wheel-graph attention networks. ||| 34339 ||| 34340 ||| 34341 ||| 
2020 ||| self-supervised visual attention learning for vehicle re-identification. ||| 765 ||| 34342 ||| 34343 ||| 2394 ||| 
2019 ||| synaptic partner assignment using attentional voxel association networks. ||| 15502 ||| 15503 ||| 15504 ||| 10834 ||| 15505 ||| 15506 ||| 
2021 ||| inadvert: an interactive and adaptive counterdeception platform for attention enhancement and phishing prevention. ||| 9714 ||| 9715 ||| 
2021 ||| oodformer: out-of-distribution detection transformer. ||| 32497 ||| 32498 ||| 34344 ||| 34345 ||| 34346 ||| 15822 ||| 
2020 ||| landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition. ||| 30781 ||| 30782 ||| 
2021 ||| transformer uncertainty estimation with hierarchical stochastic attention. ||| 34347 ||| 3691 ||| 23280 ||| 34348 ||| 
2020 ||| mebal: a multimodal database for eye blink detection and attention level estimation. ||| 26872 ||| 26873 ||| 26874 ||| 26875 ||| 14327 ||| 26876 ||| 
2021 ||| dr-tanet: dynamic receptive temporal attention network for street scene change detection. ||| 9996 ||| 7857 ||| 7861 ||| 
2021 ||| cisco at semeval-2021 task 5: what's toxic?: leveraging transformers for multiple toxic span extraction from online comments. ||| 34349 ||| 34350 ||| 
2021 ||| graph attention networks for channel estimation in ris-assisted satellite iot communications. ||| 13478 ||| 13479 ||| 2101 ||| 13480 ||| 13482 ||| 13483 ||| 
2018 ||| aspect level sentiment classification with attention-over-attention neural networks. ||| 7738 ||| 2298 ||| 7739 ||| 
2020 ||| temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition. ||| 925 ||| 926 ||| 
2021 ||| spagan: shortest path graph attention network. ||| 23434 ||| 18726 ||| 23433 ||| 8546 ||| 1756 ||| 
2018 ||| actor conditioned attention maps for video action detection. ||| 7197 ||| 7198 ||| 7199 ||| 7201 ||| 
2020 ||| deep transformers with latent depth. ||| 9390 ||| 9391 ||| 9392 ||| 9393 ||| 
2021 ||| human parity on commonsenseqa: augmenting self-attention with external attention. ||| 34351 ||| 6064 ||| 3363 ||| 3577 ||| 341 ||| 24050 ||| 1958 ||| 24049 ||| 34352 ||| 34353 ||| 
2020 ||| subjective question answering: deciphering the inner workings of transformers in the realm of subjectivity. ||| 20956 ||| 
2019 ||| speech emotion recognition using multi-hop attention mechanism. ||| 12462 ||| 12463 ||| 12464 ||| 10565 ||| 
2021 ||| rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos. ||| 34354 ||| 23044 ||| 34355 ||| 34356 ||| 34357 ||| 34358 ||| 34359 ||| 34360 ||| 
2021 ||| vision transformers for weeds and crops classification of high resolution uav images. ||| 30603 ||| 30604 ||| 13128 ||| 30605 ||| 30606 ||| 
2021 ||| transformers solve the limited receptive field for monocular depth prediction. ||| 2522 ||| 435 ||| 2523 ||| 437 ||| 2524 ||| 
2020 ||| local self-attention over long text for efficient document retrieval. ||| 9583 ||| 9584 ||| 9585 ||| 9582 ||| 9586 ||| 9615 ||| 
2020 ||| text classification based on multi-granularity attention hybrid neural network. ||| 10978 ||| 34361 ||| 34362 ||| 34363 ||| 34364 ||| 
2022 ||| surds: self-supervised attention-guided reconstruction and dual triplet loss for writer independent offline signature verification. ||| 34365 ||| 34366 ||| 34367 ||| 30717 ||| 
2022 ||| tranad: deep transformer networks for anomaly detection in multivariate time series data. ||| 34368 ||| 34369 ||| 34370 ||| 
2019 ||| enhancing pre-trained chinese character representation with word-aligned attention. ||| 757 ||| 3326 ||| 756 ||| 759 ||| 
2019 ||| savehr: self attention vector representations for ehr based personalized chronic disease onset prediction and interpretability. ||| 32373 ||| 34371 ||| 32374 ||| 34372 ||| 24225 ||| 
2021 ||| multi-level attention pooling for graph neural networks: unifying graph representations with multiple localities. ||| 4420 ||| 4421 ||| 4427 ||| 
2020 ||| recurrent attention model with log-polar mapping is robust against adversarial attacks. ||| 34373 ||| 34374 ||| 
2020 ||| autotrans: automating transformer design via reinforced architecture search. ||| 3394 ||| 11136 ||| 3272 ||| 3392 ||| 3395 ||| 
2018 ||| transformer for emotion recognition. ||| 9280 ||| 
2017 ||| multi-modal conditional attention fusion for dimensional emotion prediction. ||| 19642 ||| 19643 ||| 
2020 ||| studying attention models in sentiment attitude extraction task. ||| 7567 ||| 7568 ||| 
2022 ||| an attention-based method for action unit detection at the 3rd abaw competition. ||| 34375 ||| 34376 ||| 34377 ||| 34378 ||| 34379 ||| 2839 ||| 2841 ||| 2838 ||| 
2019 ||| an attention enhanced graph convolutional lstm network for skeleton-based action recognition. ||| 18100 ||| 19130 ||| 1160 ||| 10429 ||| 17803 ||| 
2019 ||| toward interpretable music tagging with self-attention. ||| 11915 ||| 2089 ||| 11917 ||| 
2020 ||| probabilistic crowd gan: multimodal pedestrian trajectory prediction using a graph vehicle-pedestrian attention network. ||| 21778 ||| 21777 ||| 21779 ||| 21781 ||| 34380 ||| 21782 ||| 
2020 ||| automatic ischemic stroke lesion segmentation from computed tomography perfusion images by image synthesis and attention-based deep neural networks. ||| 15623 ||| 20159 ||| 31265 ||| 31266 ||| 31267 ||| 15555 ||| 
2017 ||| a hierarchical contextual attention-based gru network for sequential recommendation. ||| 
2018 ||| aaane: attention-based adversarial autoencoder for multi-scale network embedding. ||| 15165 ||| 11333 ||| 1174 ||| 15166 ||| 
2020 ||| future-guided incremental transformer for simultaneous translation. ||| 11809 ||| 3076 ||| 3635 ||| 
2021 ||| car-net: unsupervised co-attention guided registration network for joint registration and structure learning. ||| 6007 ||| 18749 ||| 34381 ||| 27943 ||| 
2020 ||| a deep neural network for audio classification with a classifier attention mechanism. ||| 23995 ||| 34382 ||| 34383 ||| 
2020 ||| predicting rigid body dynamics using dual quaternion recurrent neural networks with quaternion attention. ||| 34384 ||| 34385 ||| 34386 ||| 
2020 ||| a price-per-attention auction scheme using mouse cursor information. ||| 9659 ||| 34387 ||| 34388 ||| 9660 ||| 
2022 ||| a differential attention fusion model based on transformer for time series forecasting. ||| 34389 ||| 980 ||| 979 ||| 
2018 ||| uncertainty-aware attention for reliable interpretation and prediction. ||| 9311 ||| 9312 ||| 9313 ||| 9314 ||| 9315 ||| 9316 ||| 9317 ||| 
2021 ||| diverse part discovery: occluded person re-identification with part-aware transformer. ||| 5780 ||| 19331 ||| 18733 ||| 19332 ||| 17860 ||| 8711 ||| 
2020 ||| lightweight temporal self-attention for classifying satellite image time series. ||| 2309 ||| 2310 ||| 2311 ||| 
2021 ||| zero-shot certified defense against adversarial patches with vision transformers. ||| 34390 ||| 34391 ||| 
2021 ||| exploiting attention-based sequence-to-sequence architectures for sound event localization. ||| 8250 ||| 8251 ||| 1491 ||| 1493 ||| 1492 ||| 1494 ||| 8252 ||| 
2020 ||| revisiting robust neural machine translation: a transformer case study. ||| 17817 ||| 26806 ||| 3443 ||| 
2021 ||| relation-aware hierarchical attention framework for video question answering. ||| 6614 ||| 23854 ||| 23855 ||| 6615 ||| 23856 ||| 411 ||| 
2021 ||| visual grounding with transformers. ||| 8213 ||| 34392 ||| 28852 ||| 5705 ||| 
2021 ||| video crowd localization with multi-focus gaussian neighbor attention and a large-scale benchmark. ||| 33736 ||| 1940 ||| 1941 ||| 1942 ||| 31724 ||| 22078 ||| 3248 ||| 1943 ||| 
2021 ||| discrete auto-regressive variational attention models for text modeling. ||| 593 ||| 594 ||| 595 ||| 596 ||| 597 ||| 598 ||| 
2021 ||| pilot: introducing transformers for probabilistic sound event localization. ||| 8250 ||| 14311 ||| 14312 ||| 8251 ||| 1491 ||| 1493 ||| 1492 ||| 1494 ||| 8252 ||| 
2019 ||| iterative answer prediction with pointer-augmented multimodal transformers for textvqa. ||| 1879 ||| 1880 ||| 19048 ||| 19131 ||| 
2020 ||| atss-net: target speaker separation via attention-based neural network. ||| 14621 ||| 14622 ||| 14623 ||| 765 ||| 
2020 ||| focus on the present: a regularization method for the asr source-target attention layer. ||| 12579 ||| 12580 ||| 12581 ||| 12582 ||| 12583 ||| 
2020 ||| pay attention to what you read: non-recurrent handwritten text-line recognition. ||| 23150 ||| 23152 ||| 10907 ||| 23154 ||| 23155 ||| 9787 ||| 7111 ||| 23153 ||| 
2022 ||| can pre-trained transformers be used in detecting complex sensitive sentences? - a monsanto case study. ||| 34393 ||| 34394 ||| 3159 ||| 34395 ||| 
2017 ||| high-order attention models for visual question answering. ||| 9305 ||| 8566 ||| 9306 ||| 
2021 ||| depth infused binaural audio generation using hierarchical cross-modal attention. ||| 7301 ||| 7302 ||| 34396 ||| 7287 ||| 
2019 ||| personalizing graph neural networks with attention mechanism for session-based recommendation. ||| 1075 ||| 34397 ||| 17840 ||| 3617 ||| 10429 ||| 
2021 ||| building interpretable models for business process prediction using shared and specialised attention mechanisms. ||| 34398 ||| 34399 ||| 34400 ||| 34401 ||| 34402 ||| 34403 ||| 
2022 ||| uniformer: unified transformer for efficient spatiotemporal representation learning. ||| 34404 ||| 2148 ||| 2170 ||| 34405 ||| 16550 ||| 1848 ||| 2149 ||| 
2021 ||| semi-supervised wide-angle portraits correction by multi-scale transformer. ||| 34406 ||| 18133 ||| 5845 ||| 1371 ||| 34407 ||| 19236 ||| 
2019 ||| acnet: attention based network to exploit complementary features for rgbd semantic segmentation. ||| 11524 ||| 7857 ||| 11525 ||| 5906 ||| 
2019 ||| prime sample attention in object detection. ||| 18870 ||| 472 ||| 2291 ||| 2162 ||| 
2018 ||| agil: learning attention from human for visuomotor tasks. ||| 8824 ||| 8825 ||| 8826 ||| 8827 ||| 8828 ||| 8829 ||| 8830 ||| 
2021 ||| smart bird: learnable sparse attention for efficient and effective transformer. ||| 3754 ||| 3755 ||| 3756 ||| 34408 ||| 3708 ||| 2795 ||| 
2021 ||| segformer: simple and efficient design for semantic segmentation with transformers. ||| 2007 ||| 2006 ||| 34409 ||| 34410 ||| 34411 ||| 2011 ||| 
2022 ||| bending reality: distortion-aware transformers for adapting to panoramic semantic segmentation. ||| 7856 ||| 7857 ||| 23613 ||| 34412 ||| 7859 ||| 7861 ||| 
2021 ||| mask attention networks: rethinking and strengthen transformer. ||| 4812 ||| 4813 ||| 2921 ||| 4814 ||| 4815 ||| 4816 ||| 3706 ||| 4817 ||| 3273 ||| 
2022 ||| sats: self-attention transfer for continual semantic segmentation. ||| 34413 ||| 34414 ||| 34415 ||| 34416 ||| 34417 ||| 14897 ||| 14895 ||| 
2021 ||| reconstruction student with attention for student-teacher pyramid matching. ||| 34418 ||| 8731 ||| 
2018 ||| attend before you act: leveraging human visual attention for continual learning. ||| 17764 ||| 17765 ||| 
2019 ||| fast and accurate capitalization and punctuation for automatic speech recognition using transformer and chunk merging. ||| 21430 ||| 7526 ||| 21431 ||| 21432 ||| 21433 ||| 7527 ||| 7528 ||| 
2019 ||| mapping social media attention in microbiology: identifying main topics and actors. ||| 34419 ||| 34420 ||| 3419 ||| 34421 ||| 34422 ||| 
2019 ||| semantic relation classification via bidirectional lstm networks with entity-aware attention using latent entity typing. ||| 34423 ||| 34424 ||| 34425 ||| 
2021 ||| spatio-temporal weather forecasting and attention mechanism on convolutional lstms. ||| 34426 ||| 34427 ||| 34428 ||| 34429 ||| 34430 ||| 
2021 ||| pointr: diverse point cloud completion with geometry-aware transformers. ||| 2114 ||| 2115 ||| 2116 ||| 2117 ||| 1920 ||| 1921 ||| 
2021 ||| task transformer network for joint mri reconstruction and super-resolution. ||| 27732 ||| 27733 ||| 4056 ||| 3036 ||| 1125 ||| 
2022 ||| style transformer for image inversion and editing. ||| 34431 ||| 34432 ||| 34433 ||| 19881 ||| 1899 ||| 21803 ||| 7716 ||| 
2020 ||| vd-bert: a unified vision and dialog transformer with bert. ||| 7400 ||| 1313 ||| 597 ||| 598 ||| 3287 ||| 3303 ||| 
2018 ||| deep co-attention based comparators for relative representation learning in person re-identification. ||| 17580 ||| 602 ||| 619 ||| 1756 ||| 
2021 ||| psa-gan: progressive self attention gans for synthetic time series. ||| 34434 ||| 34435 ||| 34436 ||| 34437 ||| 34438 ||| 34439 ||| 34440 ||| 34441 ||| 
2022 ||| learning confidence for transformer-based neural machine translation. ||| 3042 ||| 3043 ||| 3044 ||| 3045 ||| 3046 ||| 
2019 ||| a coarse-to-fine framework for learned color enhancement with non-local attention. ||| 11584 ||| 11585 ||| 11586 ||| 
2021 ||| mvit: mask vision transformer for facial expression recognition in the wild. ||| 34442 ||| 34443 ||| 34444 ||| 19820 ||| 8711 ||| 
2021 ||| transformer-based lexically constrained headline generation. ||| 26670 ||| 26671 ||| 26672 ||| 26673 ||| 3090 ||| 26674 ||| 26675 ||| 
2021 ||| coordinate attention for efficient mobile network design. ||| 1902 ||| 19090 ||| 1685 ||| 
2021 ||| structure information is the key: self-attention roi feature extractor in 3d object detection. ||| 34445 ||| 16401 ||| 21639 ||| 34446 ||| 
2019 ||| : meshed-memory transformer for image captioning. ||| 19001 ||| 19002 ||| 19003 ||| 13611 ||| 
2021 ||| multi-scale high-resolution vision transformer for semantic segmentation. ||| 34447 ||| 34448 ||| 32783 ||| 4245 ||| 5860 ||| 34449 ||| 34450 ||| 34451 ||| 34452 ||| 
2021 ||| tenet: temporal cnn with attention for anomaly detection in automotive cyber-physical systems. ||| 7489 ||| 7490 ||| 7491 ||| 
2019 ||| dada: a large-scale benchmark and model for driver attention prediction in accidental scenarios. ||| 34453 ||| 34454 ||| 34455 ||| 34456 ||| 
2021 ||| attention-based reinforcement learning for real-time uav semantic communication. ||| 11866 ||| 11867 ||| 11868 ||| 11869 ||| 11870 ||| 11871 ||| 11872 ||| 
2021 ||| massformer: tandem mass spectrum prediction with graph transformers. ||| 34457 ||| 1241 ||| 34458 ||| 2693 ||| 
2020 ||| orientation-aware vehicle re-identification with semantics-guided part attention network. ||| 8814 ||| 8815 ||| 8816 ||| 8817 ||| 
2022 ||| fine- and coarse-granularity hybrid self-attention for efficient bert. ||| 764 ||| 1266 ||| 4271 ||| 3560 ||| 3561 ||| 
2019 ||| learning parallax attention for stereo image super-resolution. ||| 19138 ||| 19139 ||| 19140 ||| 19141 ||| 19308 ||| 19143 ||| 7271 ||| 
2021 ||| heterogeneous graph attention network for multi-hop machine reading comprehension. ||| 6810 ||| 34459 ||| 2170 ||| 34460 ||| 34461 ||| 31546 ||| 
2021 ||| salient object ranking with position-preserved attention. ||| 2402 ||| 2403 ||| 340 ||| 1774 ||| 2404 ||| 2405 ||| 1115 ||| 2359 ||| 
2021 ||| long short-term transformer for online action detection. ||| 34462 ||| 34463 ||| 2424 ||| 2419 ||| 33093 ||| 1815 ||| 2072 ||| 
2022 ||| semantic-aligned fusion transformer for one-shot object detection. ||| 26579 ||| 19061 ||| 19062 ||| 
2020 ||| pre-training transformers as energy-based cloze models. ||| 20965 ||| 22748 ||| 9372 ||| 20967 ||| 
2022 ||| grounding commands for autonomous vehicles via layer fusion with region-specific dynamic layer attention. ||| 34464 ||| 34465 ||| 6285 ||| 
2021 ||| colorization transformer. ||| 24026 ||| 9289 ||| 24027 ||| 
2021 ||| cascaded head-colliding attention. ||| 3137 ||| 3138 ||| 3139 ||| 
2021 ||| t6d-direct: transformers for multi-object 6d pose direct regression. ||| 23135 ||| 23136 ||| 409 ||| 
2019 ||| kernel graph attention network for fact verification. ||| 3321 ||| 3322 ||| 3233 ||| 
2020 ||| locating cephalometric x-ray landmarks with foveated pyramid attention. ||| 14842 ||| 14843 ||| 
2017 ||| improving visually grounded sentence representations with self-attention. ||| 26390 ||| 34466 ||| 7340 ||| 
2018 ||| group-attention single-shot detector (ga-ssd): finding pulmonary nodules in large-scale ct images. ||| 14889 ||| 2008 ||| 14846 ||| 14848 ||| 14890 ||| 14891 ||| 14892 ||| 
2021 ||| transcamp: graph transformer for 6-dof camera pose estimation. ||| 5125 ||| 2163 ||| 
2021 ||| global interaction modelling in vision transformer via super tokens. ||| 34467 ||| 8008 ||| 24207 ||| 6525 ||| 
2022 ||| rgb-d slam using attention guided frame association. ||| 34468 ||| 34469 ||| 34470 ||| 34471 ||| 14713 ||| 34472 ||| 34473 ||| 
2020 ||| parallel rescoring with transformer for streaming on-device speech recognition. ||| 3337 ||| 14472 ||| 3334 ||| 3336 ||| 12098 ||| 
2020 ||| forecasting photovoltaic power production using a deep learning sequence to sequence model with attention. ||| 661 ||| 662 ||| 663 ||| 664 ||| 
2020 ||| alp-kd: attention-based layer projection for knowledge distillation. ||| 17817 ||| 17818 ||| 14630 ||| 3443 ||| 
2021 ||| planetr: structure-guided transformers for 3d plane recovery. ||| 2081 ||| 2082 ||| 2083 ||| 2084 ||| 2085 ||| 
2021 ||| do pedestrians pay attention? eye contact detection in the wild. ||| 34474 ||| 34475 ||| 34476 ||| 34477 ||| 21833 ||| 
2020 ||| gated recurrent context: softmax-free attention for online encoder-decoder speech recognition. ||| 34478 ||| 14603 ||| 34479 ||| 34480 ||| 10966 ||| 
2020 ||| adaptive attentional network for few-shot knowledge graph completion. ||| 26418 ||| 26419 ||| 26420 ||| 26421 ||| 26422 ||| 759 ||| 22492 ||| 
2021 ||| deep co-supervision and attention fusion strategy for automatic covid-19 lung infection segmentation on ct images. ||| 34481 ||| 34482 ||| 34483 ||| 34484 ||| 34485 ||| 15608 ||| 
2021 ||| transformer-based korean pretrained language models: a survey on three years of progress. ||| 34486 ||| 
2019 ||| a short virtual reality mindfulness meditation training for regaining sustained attention. ||| 34487 ||| 34488 ||| 
2021 ||| sequence-to-sequence lexical normalization with multilingual transformers. ||| 24759 ||| 24760 ||| 24761 ||| 
2021 ||| guiding query position and performing similar attention for transformer-based detection heads. ||| 34489 ||| 6430 ||| 18779 ||| 34490 ||| 1280 ||| 
2021 ||| learning transformer features for image quality assessment. ||| 34491 ||| 19728 ||| 
2020 ||| assessing phrasal representation and composition in transformers. ||| 3274 ||| 3275 ||| 
2021 ||| transforming fake news: robust generalisable news classification using transformers. ||| 17181 ||| 17182 ||| 
2020 ||| baksa at semeval-2020 task 9: bolstering cnn with self-attention for sentiment analysis of code mixed text. ||| 10575 ||| 10576 ||| 10577 ||| 4930 ||| 
2019 ||| knee menisci segmentation and relaxometry of 3d ultrashort echo time (ute) cones mr imaging using attention u-net with transfer learning. ||| 34492 ||| 34493 ||| 14935 ||| 34494 ||| 31947 ||| 34495 ||| 34496 ||| 31948 ||| 
2019 ||| attention-guided generative adversarial networks for unsupervised image-to-image translation. ||| 435 ||| 436 ||| 437 ||| 127 ||| 
2017 ||| person re-identification using visual attention. ||| 11306 ||| 11307 ||| 11308 ||| 326 ||| 7421 ||| 
2019 ||| neutron: an implementation of the transformer translation model and its variants. ||| 8 ||| 3260 ||| 
2020 ||| efficient transformers: a survey. ||| 1398 ||| 2293 ||| 3292 ||| 3294 ||| 
2021 ||| cotext: multi-task learning with code-text transformer. ||| 34497 ||| 16138 ||| 34498 ||| 34499 ||| 34500 ||| 34501 ||| 9022 ||| 
2021 ||| high-order tensor pooling with attention for action recognition. ||| 8694 ||| 3279 ||| 34502 ||| 
2019 ||| explanation vs attention: a two-player game to obtain attention for vqa. ||| 7149 ||| 18267 ||| 7152 ||| 
2019 ||| non-intrusive load monitoring with an attention-based deep neural network. ||| 34503 ||| 34504 ||| 
2020 ||| powertransformer: unsupervised controllable revision for biased language correction. ||| 26541 ||| 3537 ||| 3536 ||| 3355 ||| 
2019 ||| multi-modal attention-based fusion model for semantic segmentation of rgb-depth images. ||| 34505 ||| 34506 ||| 
2020 ||| hierachical delta-attention method for multimodal fusion. ||| 34507 ||| 
2021 ||| a state-of-the-art survey of artificial neural networks for whole-slide image analysis: from popular convolutional neural networks to potential visual transformers. ||| 399 ||| 3047 ||| 5378 ||| 13789 ||| 12689 ||| 1236 ||| 34508 ||| 15956 ||| 
2019 ||| manipulation-skill assessment from videos with spatial attention network. ||| 7892 ||| 6419 ||| 7893 ||| 7894 ||| 
2021 ||| covtanet: a hybrid tri-level attention based network for lesion segmentation, diagnosis, and severity prediction of covid-19 chest ct scans. ||| 29219 ||| 29220 ||| 29221 ||| 29222 ||| 29223 ||| 29224 ||| 29225 ||| 
2021 ||| u-shape transformer for underwater image enhancement. ||| 34509 ||| 34510 ||| 34511 ||| 
2021 ||| an efficient transformer decoder with compressed sub-layers. ||| 17716 ||| 17717 ||| 2333 ||| 3306 ||| 
2021 ||| melody structure transfer network: generating music with separable self-attention. ||| 3402 ||| 2307 ||| 
2021 ||| ear-u-net: efficientnet and attention-based residual u-net for automatic liver segmentation in ct. ||| 34512 ||| 34513 ||| 34514 ||| 34515 ||| 11294 ||| 
2019 ||| self-supervised attention model for weakly labeled audio event classification. ||| 8221 ||| 8222 ||| 
2021 ||| mhattnsurv: multi-head attention for survival prediction using whole-slide pathology images. ||| 11376 ||| 34516 ||| 34517 ||| 
2020 ||| self-attention generative adversarial network for speech enhancement. ||| 12352 ||| 12353 ||| 12354 ||| 3882 ||| 12355 ||| 12356 ||| 12357 ||| 12358 ||| 
2021 ||| deep gaussian denoiser epistemic uncertainty and decoupled dual-attention fusion. ||| 11510 ||| 11511 ||| 11512 ||| 8798 ||| 8799 ||| 
2020 ||| liver segmentation in abdominal ct images via auto-context neural network and self-supervised contour attention. ||| 34518 ||| 34519 ||| 34520 ||| 34521 ||| 
2021 ||| attribution mask: filtering out irrelevant features by recursively focusing attention on inputs of dnns. ||| 34522 ||| 10985 ||| 
2019 ||| logic and the 2-simplicial transformer. ||| 23976 ||| 23977 ||| 23978 ||| 23979 ||| 
2022 ||| gascn: graph attention shape completion network. ||| 803 ||| 13634 ||| 13635 ||| 
2020 ||| regularized forward-backward decoder for attention models. ||| 23274 ||| 23270 ||| 23271 ||| 23273 ||| 5695 ||| 
2021 ||| rca-iunet: a residual cross-spatial attention guided inception u-net model for tumor segmentation in breast ultrasound imaging. ||| 12821 ||| 12823 ||| 
2022 ||| a lightweight and accurate spatial-temporal transformer for traffic forecasting. ||| 34523 ||| 34524 ||| 34525 ||| 34526 ||| 34527 ||| 34528 ||| 24727 ||| 
2021 ||| repairing human trust by promptly correcting robot mistakes with an attention transfer model. ||| 26234 ||| 1124 ||| 26233 ||| 26232 ||| 1840 ||| 
2021 ||| 3d object tracking with transformer. ||| 25495 ||| 4236 ||| 25493 ||| 34529 ||| 25494 ||| 
2021 ||| bossnas: exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. ||| 1776 ||| 1777 ||| 1778 ||| 1779 ||| 1780 ||| 1686 ||| 1781 ||| 
2019 ||| improving referring expression grounding with cross-modal attention-guided erasing. ||| 18803 ||| 18804 ||| 18805 ||| 1846 ||| 1848 ||| 
2021 ||| scaled-time-attention robust edge network. ||| 34530 ||| 34531 ||| 34532 ||| 34533 ||| 34534 ||| 34535 ||| 34536 ||| 34537 ||| 34538 ||| 
2020 ||| attributes-guided and pure-visual attention alignment for few-shot recognition. ||| 1161 ||| 1254 ||| 18001 ||| 1162 ||| 
2020 ||| hat: hardware-aware transformers for efficient natural language processing. ||| 3161 ||| 3162 ||| 3163 ||| 3164 ||| 3165 ||| 2190 ||| 3166 ||| 
2021 ||| invertible attention. ||| 34539 ||| 34255 ||| 875 ||| 34540 ||| 8571 ||| 
2018 ||| attending to mathematical language with transformers. ||| 34541 ||| 
2020 ||| strokecoder: path-based image generation from single examples using transformers. ||| 34542 ||| 20353 ||| 
2018 ||| same representation, different attentions: shareable sentence representation learning from multiple tasks. ||| 23518 ||| 23519 ||| 3272 ||| 
2020 ||| mapping the space of chemical reactions using attention-based neural networks. ||| 34543 ||| 34544 ||| 34545 ||| 34546 ||| 34547 ||| 34548 ||| 34549 ||| 
2021 ||| anatomical-guided attention enhances unsupervised pet image denoising performance. ||| 25473 ||| 31276 ||| 31277 ||| 31278 ||| 31279 ||| 31280 ||| 31281 ||| 
2018 ||| ram: residual attention module for single image super-resolution. ||| 24552 ||| 24551 ||| 18834 ||| 24553 ||| 
2018 ||| stacked dense u-nets with dual transformers for robust face alignment. ||| 5932 ||| 21445 ||| 21446 ||| 18875 ||| 
2018 ||| learning how to listen: a temporal-frequential attention model for sound event detection. ||| 11777 ||| 11778 ||| 11779 ||| 
2020 ||| adaptiveweighted attention network with camera spectral sensitivity prior for spectral reconstruction from rgb images. ||| 6699 ||| 6698 ||| 6700 ||| 6701 ||| 523 ||| 
2019 ||| mixed pooling multi-view attention autoencoder for representation learning in healthcare. ||| 34550 ||| 17183 ||| 1094 ||| 34551 ||| 
2019 ||| eyenet: attention based convolutional encoder-decoder network for eye region segmentation. ||| 7839 ||| 4035 ||| 
2021 ||| trunet: transformer-recurrent-u network for multi-channel reverberant sound source separation. ||| 1490 ||| 34552 ||| 34553 ||| 
2021 ||| implicit transformer network for screen content image continuous super-resolution. ||| 2820 ||| 3822 ||| 30118 ||| 8293 ||| 
2021 ||| trilateral attention network for real-time medical image segmentation. ||| 34554 ||| 34555 ||| 27631 ||| 
2022 ||| a unified framework for attention-based few-shot object detection. ||| 34556 ||| 34557 ||| 
2021 ||| unsupervised mri reconstruction via zero-shot learned adversarial transformers. ||| 27660 ||| 27662 ||| 27661 ||| 27663 ||| 27664 ||| 27665 ||| 27666 ||| 
2019 ||| attention-guided low-light image enhancement. ||| 32089 ||| 13639 ||| 
2022 ||| ctformer: convolution-free token2token dilated vision transformer for low-dose ct denoising. ||| 27466 ||| 34558 ||| 27467 ||| 1840 ||| 2355 ||| 27468 ||| 
2020 ||| exploiting temporal attention features for effective denoising in videos. ||| 34559 ||| 34560 ||| 34561 ||| 34562 ||| 
2018 ||| lstms with attention for aggression detection. ||| 11663 ||| 11664 ||| 11665 ||| 11666 ||| 
2021 ||| transcouplet: transformer based chinese couplet generation. ||| 34563 ||| 34564 ||| 34565 ||| 34566 ||| 34567 ||| 
2019 ||| coherent semantic attention for image inpainting. ||| 2618 ||| 170 ||| 2619 ||| 497 ||| 
2019 ||| great ape detection in challenging jungle camera trap footage via attention-based spatial and temporal feature blending. ||| 7854 ||| 7793 ||| 7855 ||| 
2021 ||| learning vision transformer with squeeze and excitation for facial expression recognition. ||| 34568 ||| 21726 ||| 34569 ||| 34570 ||| 34571 ||| 34572 ||| 
2019 ||| video-based convolutional attention for person re-identification. ||| 4697 ||| 4698 ||| 4699 ||| 4700 ||| 4701 ||| 4702 ||| 4703 ||| 4704 ||| 
2019 ||| dual path multi-scale fusion networks with attention for crowd counting. ||| 30261 ||| 34573 ||| 15629 ||| 34574 ||| 9042 ||| 9041 ||| 
2020 ||| automatic composition of guitar tabs by transformers and groove modeling. ||| 11918 ||| 17094 ||| 11919 ||| 4374 ||| 
2021 ||| attend and guide (ag-net): a keypoints-driven attention-based deep network for image recognition. ||| 18075 ||| 6384 ||| 7263 ||| 7264 ||| 6383 ||| 
2021 ||| sea: graph shell attention in graph neural networks. ||| 34575 ||| 34576 ||| 34577 ||| 
2021 ||| dropout regularization for self-supervised learning of transformer encoder speech representation. ||| 12509 ||| 701 ||| 704 ||| 705 ||| 
2021 ||| multi-modal land cover mapping of remote sensing images using pyramid attention and gated fusion networks. ||| 34578 ||| 34579 ||| 34580 ||| 34581 ||| 34582 ||| 
2018 ||| attention-based neural text segmentation. ||| 7251 ||| 15056 ||| 1186 ||| 1185 ||| 
2021 ||| covid-net us: a tailored, highly efficient, self-attention deep convolutional neural network design for detection of covid-19 patient cases from point-of-care ultrasound imaging. ||| 27908 ||| 27909 ||| 27910 ||| 27911 ||| 27912 ||| 27913 ||| 27914 ||| 27915 ||| 7865 ||| 
2021 ||| eigen analysis of self-attention and its reconstruction from partial computation. ||| 2567 ||| 2568 ||| 34583 ||| 9159 ||| 26586 ||| 2572 ||| 
2020 ||| feathertts: robust and efficient attention based neural tts. ||| 34584 ||| 34585 ||| 859 ||| 12572 ||| 34586 ||| 34587 ||| 34588 ||| 2063 ||| 
2022 ||| swin transformer for fast mri. ||| 34589 ||| 34590 ||| 34591 ||| 34592 ||| 27334 ||| 438 ||| 34593 ||| 34594 ||| 6005 ||| 
2021 ||| mvt: multi-view vision transformer for 3d object recognition. ||| 9996 ||| 1327 ||| 977 ||| 
2022 ||| geometric transformer for fast and robust point cloud registration. ||| 5146 ||| 5447 ||| 969 ||| 7271 ||| 972 ||| 5723 ||| 
2018 ||| predicting blood pressure response to fluid bolus therapy using attention-based neural networks for clinical interpretability. ||| 34595 ||| 34596 ||| 34597 ||| 34598 ||| 34599 ||| 2433 ||| 
2020 ||| normalized attention without probability cage. ||| 24000 ||| 24002 ||| 
2018 ||| compositional attention networks for machine reasoning. ||| 22836 ||| 20967 ||| 
2019 ||| medical time series classification with hierarchical attention-based temporal convolutional networks: a case study of myotonic dystrophy diagnosis. ||| 10545 ||| 19172 ||| 19173 ||| 19174 ||| 19175 ||| 
2022 ||| transformers and the representation of biomedical background knowledge. ||| 34600 ||| 34460 ||| 34601 ||| 34602 ||| 34603 ||| 34604 ||| 3369 ||| 33200 ||| 
2019 ||| event-based attention and tracking on neuromorphic hardware. ||| 10093 ||| 10094 ||| 10096 ||| 
2019 ||| attentionrnn: a structured spatial attention mechanism. ||| 2506 ||| 2301 ||| 
2020 ||| towards fully 8-bit integer inference for the transformer model. ||| 17717 ||| 17716 ||| 23452 ||| 2333 ||| 23453 ||| 3306 ||| 
2020 ||| ma-dst: multi-attention based scalable dialog state tracking. ||| 17938 ||| 17939 ||| 17940 ||| 17941 ||| 17942 ||| 59 ||| 
2021 ||| iiitt@lt-edi-eacl2021-hope speech detection: there is always hope in transformers. ||| 34605 ||| 34606 ||| 34607 ||| 34608 ||| 10634 ||| 
2021 ||| face transformer for recognition. ||| 34609 ||| 2400 ||| 
2021 ||| spectnt: a time-frequency transformer for music audio. ||| 11920 ||| 11921 ||| 11915 ||| 11916 ||| 11922 ||| 
2018 ||| attention-based deep multiple instance learning. ||| 22787 ||| 22788 ||| 9255 ||| 
2019 ||| self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction. ||| 438 ||| 802 ||| 4871 ||| 4872 ||| 771 ||| 801 ||| 800 ||| 
2021 ||| single-read reconstruction for dna data storage using transformers. ||| 34610 ||| 34611 ||| 34612 ||| 
2019 ||| one-pass multi-task networks with cross-task guided attention for brain tumor segmentation. ||| 34613 ||| 18611 ||| 18726 ||| 34614 ||| 1756 ||| 
2020 ||| upb at semeval-2020 task 9: identifying sentiment in code-mixed social media texts using transformers and multi-task learning. ||| 10488 ||| 10489 ||| 10490 ||| 10491 ||| 10492 ||| 
2021 ||| pay attention with focus: a novel learning scheme for classification of whole slide images. ||| 27869 ||| 27870 ||| 27871 ||| 27872 ||| 27873 ||| 27874 ||| 
2021 ||| transvg: end-to-end visual grounding with transformers. ||| 2415 ||| 2416 ||| 2417 ||| 1806 ||| 1807 ||| 
2020 ||| attention-guided context feature pyramid network for object detection. ||| 34615 ||| 6415 ||| 786 ||| 34616 ||| 
2020 ||| capsules with inverted dot-product attention routing. ||| 3597 ||| 23974 ||| 23975 ||| 3247 ||| 
2018 ||| pixel-wise attentional gating for parsimonious pixel labeling. ||| 7324 ||| 2035 ||| 
2021 ||| melons: generating melody with long-term structure using transformers and structure graph. ||| 34617 ||| 34618 ||| 26464 ||| 34619 ||| 30206 ||| 4523 ||| 
2021 ||| bitr-unet: a cnn-transformer combined network for mri brain tumor segmentation. ||| 34620 ||| 24682 ||| 
2021 ||| attention, please! a survey of neural attention models in deep learning. ||| 9074 ||| 5421 ||| 
2021 ||| survtrace: transformers for survival analysis with competing events. ||| 34621 ||| 23323 ||| 
2019 ||| enhancing salient object segmentation through attention. ||| 19168 ||| 19169 ||| 19170 ||| 19171 ||| 
2020 ||| multi-modal attention for speech emotion recognition. ||| 14247 ||| 14248 ||| 4481 ||| 12494 ||| 
2020 ||| a novel fusion of attention and sequence to sequence autoencoders to predict sleepiness from speech. ||| 4028 ||| 19564 ||| 19565 ||| 4029 ||| 4027 ||| 648 ||| 649 ||| 
2019 ||| granular multimodal attention networks for visual dialog. ||| 7149 ||| 34622 ||| 7152 ||| 
2018 ||| kernel transformer networks for compact spherical convolution. ||| 18927 ||| 1664 ||| 
2021 ||| proposal-free one-stage referring expression via grid-word cross-attention. ||| 23470 ||| 23471 ||| 5845 ||| 6417 ||| 
2021 ||| docformer: end-to-end transformer for document understanding. ||| 2640 ||| 2641 ||| 2642 ||| 2643 ||| 2644 ||| 
2022 ||| webformer: the web-page transformer for structure information extraction. ||| 9235 ||| 13614 ||| 3555 ||| 34623 ||| 3101 ||| 12441 ||| 
2019 ||| coulgat: an experiment on interpretability of graph attention networks. ||| 34624 ||| 
2021 ||| end-to-end speaker height and age estimation using attention mechanism with lstm-rnn. ||| 4466 ||| 4467 ||| 4469 ||| 
2020 ||| hybrid attention-based transformer block model for distant supervision relation extraction. ||| 34625 ||| 34626 ||| 25504 ||| 34627 ||| 
2019 ||| towards generation of visual attention map for source code. ||| 4420 ||| 4421 ||| 4422 ||| 4423 ||| 4424 ||| 4425 ||| 4426 ||| 4427 ||| 
2020 ||| multi-task learning from videos via efficient inter-frame attention. ||| 7954 ||| 7955 ||| 7956 ||| 7957 ||| 7224 ||| 7958 ||| 7959 ||| 2101 ||| 7960 ||| 
2020 ||| etc: encoding long and structured data in transformers. ||| 3557 ||| 9233 ||| 3882 ||| 9232 ||| 9234 ||| 3555 ||| 26614 ||| 
2020 ||| modulated fusion using transformer for linguistic-acoustic emotion recognition. ||| 9280 ||| 4862 ||| 34628 ||| 2693 ||| 34629 ||| 
2021 ||| vivit: a video vision transformer. ||| 2292 ||| 2293 ||| 2294 ||| 2094 ||| 2295 ||| 2093 ||| 
2022 ||| rethinking attention-model explainability through faithfulness violation test. ||| 34630 ||| 34631 ||| 34632 ||| 34633 ||| 4807 ||| 19391 ||| 
2021 ||| an end-to-end trainable video panoptic segmentation method usingtransformers. ||| 34634 ||| 34635 ||| 
2021 ||| sit: self-supervised vision transformer. ||| 24207 ||| 8008 ||| 6525 ||| 
2020 ||| minilmv2: multi-head self-attention relation distillation for compressing pretrained transformers. ||| 3497 ||| 3498 ||| 3499 ||| 3171 ||| 3174 ||| 
2019 ||| discourse-aware semantic self-attention for narrative reading comprehension. ||| 26759 ||| 14228 ||| 
2018 ||| actor-attention-critic for multi-agent reinforcement learning. ||| 22821 ||| 17735 ||| 
2021 ||| vad-free streaming hybrid ctc/attention asr for unsegmented recording. ||| 12682 ||| 4418 ||| 
2020 ||| an implicit attention mechanism for deep learning pedestrian re-identification frameworks. ||| 28712 ||| 28713 ||| 28715 ||| 28716 ||| 3419 ||| 
2019 ||| lane attention: predicting vehicles' moving trajectories by learning their attention over lanes. ||| 25572 ||| 25573 ||| 25574 ||| 25575 ||| 25576 ||| 25577 ||| 25578 ||| 
2019 ||| adaptive embedding gate for attention-based scene text recognition. ||| 17682 ||| 17680 ||| 17681 ||| 6559 ||| 17418 ||| 
2019 ||| why adam beats sgd for attention models. ||| 9256 ||| 9257 ||| 2572 ||| 9258 ||| 9158 ||| 9159 ||| 9259 ||| 
2020 ||| dual attention model for citation recommendation. ||| 1420 ||| 11691 ||| 
2020 ||| attention as activation. ||| 7144 ||| 7146 ||| 7147 ||| 7148 ||| 
2021 ||| sleeptransformer: automatic sleep staging with interpretability and uncertainty quantification. ||| 12352 ||| 34636 ||| 12354 ||| 3882 ||| 12355 ||| 12358 ||| 14475 ||| 
2022 ||| transformer memory as a differentiable search index. ||| 1398 ||| 34637 ||| 2293 ||| 34638 ||| 3292 ||| 34639 ||| 3293 ||| 34640 ||| 22746 ||| 3290 ||| 26522 ||| 3246 ||| 3294 ||| 
2020 ||| untangling tradeoffs between recurrence and self-attention in neural networks. ||| 9192 ||| 9193 ||| 9194 ||| 9195 ||| 9196 ||| 9197 ||| 
2017 ||| visual-textual attention driven fine-grained representation learning. ||| 6618 ||| 5954 ||| 
2021 ||| pq-transformer: jointly parsing 3d objects and layouts from point clouds. ||| 17682 ||| 34641 ||| 34642 ||| 34643 ||| 
2018 ||| universal transformers. ||| 2293 ||| 24003 ||| 22760 ||| 4960 ||| 9135 ||| 
2021 ||| tapl: dynamic part-based visual tracking via attention-guided part localization. ||| 14543 ||| 20104 ||| 34644 ||| 
2020 ||| spatial-temporal dynamic graph attention networks for ride-hailing demand prediction. ||| 34645 ||| 34646 ||| 
2021 ||| transreid: transformer-based object re-identification. ||| 1701 ||| 1702 ||| 1703 ||| 1704 ||| 1705 ||| 1706 ||| 
2021 ||| streamult: streaming multimodal transformer for heterogeneous and arbitrary long sequential data. ||| 34647 ||| 34648 ||| 34649 ||| 3157 ||| 25980 ||| 
2022 ||| short range correlation transformer for occluded person re-identification. ||| 34650 ||| 34651 ||| 15057 ||| 34652 ||| 
2021 ||| edge-augmented graph transformers: global self-attention is enough for graphs. ||| 34653 ||| 34654 ||| 34655 ||| 
2019 ||| very deep self-attention networks for end-to-end speech recognition. ||| 3517 ||| 14549 ||| 11470 ||| 14550 ||| 3831 ||| 3518 ||| 
2020 ||| machine translation of novels in the age of transformer. ||| 14214 ||| 34656 ||| 34657 ||| 3882 ||| 
2020 ||| bilinear fusion of commonsense knowledge with attention-based nli models. ||| 4240 ||| 4241 ||| 914 ||| 4242 ||| 
2020 ||| safe: self-attention based unsupervised road safety classification in hazardous environments. ||| 34658 ||| 7313 ||| 7315 ||| 
2021 ||| a unified transformer-based framework for duplex text normalization. ||| 26299 ||| 1420 ||| 34659 ||| 12698 ||| 3403 ||| 
2022 ||| on guiding visual attention with language specification. ||| 34660 ||| 34661 ||| 34662 ||| 2595 ||| 19048 ||| 19047 ||| 
2019 ||| pre-training of graph augmented transformers for medication recommendation. ||| 3413 ||| 23322 ||| 1070 ||| 23323 ||| 
2018 ||| learning deep structured multi-scale features using attention-gated crfs for contour prediction. ||| 436 ||| 2303 ||| 9285 ||| 2524 ||| 1846 ||| 437 ||| 
2020 ||| multi-modal feature fusion with feature attention for vatex captioning challenge 2020. ||| 34663 ||| 34664 ||| 3807 ||| 
2019 ||| crowd counting using scale-aware attention networks. ||| 7167 ||| 7168 ||| 7169 ||| 602 ||| 
2018 ||| an introductory survey on attention mechanisms in nlp problems. ||| 8355 ||| 
2019 ||| multi-scale body-part mask guided attention for person re-identification. ||| 19041 ||| 19042 ||| 19043 ||| 
2022 ||| wasserstein adversarial transformer for cloud workload prediction. ||| 34665 ||| 34666 ||| 34667 ||| 1160 ||| 34668 ||| 
2022 ||| xai for transformers: better explanations through conservative propagation. ||| 34669 ||| 34670 ||| 34671 ||| 17321 ||| 34672 ||| 34673 ||| 3831 ||| 1667 ||| 
2022 ||| compare learning: bi-attention network for few-shot learning. ||| 12325 ||| 12326 ||| 12327 ||| 6552 ||| 
2021 ||| dynamic allocation of visual attention for vision-based autonomous navigation under data rate constraints. ||| 22404 ||| 22405 ||| 22406 ||| 
2021 ||| a single-target license plate detection with attention. ||| 34674 ||| 17857 ||| 
2019 ||| polytransform: deep polygon transformer for instance segmentation. ||| 18845 ||| 18727 ||| 18728 ||| 18846 ||| 18847 ||| 9239 ||| 
2021 ||| generalizing rnn-transducer to out-domain audio via sparse self-attention layers. ||| 32429 ||| 34675 ||| 34676 ||| 
2022 ||| global tracking transformers. ||| 34677 ||| 34678 ||| 1884 ||| 34679 ||| 34680 ||| 34681 ||| 
2017 ||| classification of radiology reports using neural attention models. ||| 372 ||| 373 ||| 374 ||| 375 ||| 
2021 ||| ctrl-c: camera calibration transformer with line-classification. ||| 2468 ||| 2469 ||| 2470 ||| 2471 ||| 2472 ||| 2473 ||| 
2018 ||| lsta: long short-term attention for egocentric action recognition. ||| 9832 ||| 8035 ||| 9833 ||| 
2019 ||| example-guided scene image synthesis using masked spatial-channel attention and patch-based self-supervision. ||| 8519 ||| 2070 ||| 8520 ||| 6579 ||| 2417 ||| 2166 ||| 
2022 ||| formula graph self-attention network for representation-domain independent materials discovery. ||| 34682 ||| 34683 ||| 
2021 ||| motion guided attention fusion to recognize interactions from videos. ||| 2337 ||| 2338 ||| 2339 ||| 
2022 ||| internal language model estimation through explicit context vector learning for attention-based encoder-decoder asr. ||| 12167 ||| 12755 ||| 21343 ||| 2944 ||| 12526 ||| 34684 ||| 
2021 ||| a unified generative adversarial network training via self-labeling and self-attention. ||| 22753 ||| 22754 ||| 
2021 ||| region attention and graph embedding network for occlusion objective class-based micro-expression recognition. ||| 14649 ||| 19996 ||| 540 ||| 34685 ||| 34686 ||| 
2019 ||| state-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions. ||| 13963 ||| 13964 ||| 34687 ||| 10187 ||| 
2021 ||| musiq: multi-scale image quality transformer. ||| 2316 ||| 2317 ||| 2318 ||| 2319 ||| 2320 ||| 
2021 ||| omninet: omnidirectional representations from transformers. ||| 1398 ||| 2293 ||| 3291 ||| 3290 ||| 9234 ||| 3293 ||| 3292 ||| 22745 ||| 3294 ||| 
2021 ||| paint transformer: feed forward neural painting with stroke prediction. ||| 1757 ||| 1758 ||| 1759 ||| 136 ||| 1760 ||| 633 ||| 1761 ||| 1371 ||| 
2020 ||| a survey on principles, models and methods for learning from irregularly sampled time series: from discretization to attention and invariance. ||| 24012 ||| 6023 ||| 
2021 ||| extracting qualitative causal structure with transformer-based nlp. ||| 26824 ||| 26823 ||| 34688 ||| 
2020 ||| tsam: temporal link prediction in directed networks based on self-attention mechanism. ||| 20820 ||| 20819 ||| 34689 ||| 20818 ||| 17546 ||| 
2022 ||| eeg to fmri synthesis benefits from attentional graphs of electrode relationships. ||| 34690 ||| 34691 ||| 
2021 ||| you only sample (almost) once: linear cost self-attention via bernoulli sampling. ||| 18029 ||| 18028 ||| 22765 ||| 22766 ||| 18032 ||| 18033 ||| 
2021 ||| a comparison for anti-noise robustness of deep learning classification methods on a tiny object image dataset: from convolutional neural network to visual transformer and performer. ||| 6598 ||| 399 ||| 32662 ||| 34692 ||| 17444 ||| 8841 ||| 32663 ||| 34693 ||| 15956 ||| 
2021 ||| swinbert: end-to-end transformers with sparse attention for video captioning. ||| 18746 ||| 2043 ||| 34694 ||| 34695 ||| 2044 ||| 8573 ||| 34696 ||| 17976 ||| 
2020 ||| off-policy self-critical training for transformer in visual paragraph generation. ||| 13 ||| 18061 ||| 18064 ||| 
2019 ||| learning where to see: a novel attention model for automated immunohistochemical scoring. ||| 15 ||| 34697 ||| 
2019 ||| transformer dissection: an unified understanding for transformer's attention via the lens of kernel. ||| 3597 ||| 3598 ||| 26692 ||| 3601 ||| 3247 ||| 
2021 ||| pedestrian attribute recognition in video surveillance scenarios based on view-attribute attention localization. ||| 34698 ||| 33779 ||| 33781 ||| 
2020 ||| transformer query-target knowledge discovery (tend): drug discovery from cord-19. ||| 27829 ||| 2021 ||| 2024 ||| 
2020 ||| la furca: iterative context-aware end-to-end monaural speech separation based on dual-path deep parallel inter-intra bi-lstm with attention. ||| 14487 ||| 14489 ||| 12107 ||| 
2021 ||| exploring vision transformers for fine-grained classification. ||| 34699 ||| 34700 ||| 
2019 ||| a bilingual generative transformer for semantic sentence embedding. ||| 26811 ||| 3067 ||| 26812 ||| 
2020 ||| attention u-net based adversarial architectures for chest x-ray lung segmentation. ||| 10193 ||| 10194 ||| 2713 ||| 10195 ||| 10196 ||| 3369 ||| 10197 ||| 10198 ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| 20096 ||| 7307 ||| 7310 ||| 
2018 ||| a self-attention network for hierarchical data structures with an application to claims management. ||| 34701 ||| 18148 ||| 34702 ||| 34703 ||| 
2022 ||| transformers in self-supervised monocular depth estimation with unknown camera intrinsics. ||| 16394 ||| 16395 ||| 16396 ||| 16397 ||| 
2018 ||| attention models with random features for multi-layered graph embeddings. ||| 12004 ||| 12005 ||| 14421 ||| 12006 ||| 
2021 ||| spherical transformer: adapting spherical signal to cnns. ||| 34704 ||| 24771 ||| 34705 ||| 2307 ||| 34706 ||| 
2021 ||| hierarchical self attention based autoencoder for open-set human activity recognition. ||| 10203 ||| 10202 ||| 7377 ||| 7376 ||| 7375 ||| 
2021 ||| soft-sensing conformer: a curriculum learning-based convolutional transformer. ||| 17208 ||| 8862 ||| 17211 ||| 17209 ||| 17210 ||| 17223 ||| 17213 ||| 
2018 ||| senti-attend: image captioning using sentiment and attention. ||| 28562 ||| 3258 ||| 3257 ||| 3157 ||| 3158 ||| 
2021 ||| attention on global-local embedding spaces in recommender systems. ||| 34707 ||| 34708 ||| 34709 ||| 
2021 ||| multi-stream attention learning for monocular vehicle velocity and inter-vehicle distance estimation. ||| 25539 ||| 7808 ||| 1387 ||| 
2018 ||| troy: give attention to saliency and for saliency. ||| 
2020 ||| on optimal transformer depth for low-resource language translation. ||| 34710 ||| 34711 ||| 34712 ||| 
2020 ||| confidence-aware non-repetitive multimodal transformers for textcaps. ||| 17852 ||| 17853 ||| 6417 ||| 17854 ||| 
2021 ||| power transformer fault diagnosis with intrinsic time-scale decomposition and xgboost classifier. ||| 33966 ||| 33967 ||| 
2020 ||| focus of attention improves information transfer in visual features. ||| 9397 ||| 897 ||| 9398 ||| 9399 ||| 898 ||| 
2019 ||| on the relationship between self-attention and convolutional layers. ||| 22850 ||| 22851 ||| 23895 ||| 
2020 ||| contextual pyramid attention network for building segmentation in aerial imagery. ||| 20121 ||| 34713 ||| 34714 ||| 20124 ||| 
2020 ||| the perceptual boost of visual attention is task-dependent in naturalistic settings. ||| 34715 ||| 34716 ||| 34717 ||| 34718 ||| 
2021 ||| statistically meaningful approximation: a case study on approximating turing machines with transformers. ||| 34719 ||| 34720 ||| 34721 ||| 
2019 ||| learning where to look: semantic-guided multi-attention localization for zero-shot learning. ||| 9184 ||| 9185 ||| 9186 ||| 9187 ||| 9188 ||| 
2018 ||| psychophysical evaluation of individual low-level feature influences on visual attention. ||| 1864 ||| 1865 ||| 1866 ||| 32926 ||| 1869 ||| 2253 ||| 34722 ||| 3882 ||| 1865 ||| 1870 ||| 
2021 ||| an attention free transformer. ||| 34723 ||| 34724 ||| 23974 ||| 34725 ||| 23975 ||| 34726 ||| 34727 ||| 
2021 ||| dual attention-in-attention model for joint rain streak and raindrop removal. ||| 8625 ||| 34252 ||| 28780 ||| 8621 ||| 19727 ||| 7449 ||| 
2017 ||| human trajectory prediction using spatially aware deep attention models. ||| 34728 ||| 20464 ||| 
2019 ||| link prediction with mutual attention for text-attributed networks. ||| 8939 ||| 8940 ||| 8941 ||| 
2021 ||| cetransformer: casual effect estimation via transformer based representation learning. ||| 6503 ||| 6504 ||| 6505 ||| 6506 ||| 6507 ||| 
2021 ||| is attention to bounding boxes all you need for pedestrian action prediction? ||| 33013 ||| 33016 ||| 33015 ||| 1226 ||| 33017 ||| 1226 ||| 33018 ||| 
2021 ||| enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning. ||| 3182 ||| 3652 ||| 26005 ||| 3653 ||| 3654 ||| 3655 ||| 3181 ||| 3656 ||| 2166 ||| 
2021 ||| convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. ||| 19579 ||| 9738 ||| 19580 ||| 9740 ||| 9735 ||| 1159 ||| 
2021 ||| siamese transformer pyramid networks for real-time uav tracking. ||| 7155 ||| 7156 ||| 7157 ||| 7158 ||| 
2022 ||| aggregated pyramid vision transformer: split-transform-merge strategy for image recognition without convolutions. ||| 34729 ||| 34730 ||| 34731 ||| 34732 ||| 34733 ||| 34734 ||| 
2021 ||| transmil: transformer based correlated multiple instance learning for whole slide image classication. ||| 34735 ||| 34736 ||| 8335 ||| 22358 ||| 12196 ||| 32540 ||| 18994 ||| 
2020 ||| probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles. ||| 25537 ||| 917 ||| 25538 ||| 8660 ||| 
2019 ||| federated multi-task hierarchical attention model for sensor analytics. ||| 356 ||| 357 ||| 358 ||| 359 ||| 
2018 ||| hyperbolic attention networks. ||| 22770 ||| 2713 ||| 22771 ||| 23923 ||| 8785 ||| 3710 ||| 22769 ||| 23924 ||| 8788 ||| 23925 ||| 23926 ||| 8787 ||| 23927 ||| 
2020 ||| dual attention gans for semantic image synthesis. ||| 435 ||| 2083 ||| 437 ||| 
2020 ||| e2eet: from pipeline to end-to-end entity typing via transformer-based embeddings. ||| 34737 ||| 683 ||| 
2022 ||| ensembles of vision transformers as a new paradigm for automated classification in ecology. ||| 34738 ||| 34739 ||| 34740 ||| 34741 ||| 34742 ||| 34743 ||| 34744 ||| 
2022 ||| es-drnn with dynamic attention for short-term load forecasting. ||| 34745 ||| 34746 ||| 34747 ||| 
2021 ||| charformer: fast character transformers via gradient-based subword tokenization. ||| 1398 ||| 34637 ||| 3671 ||| 3290 ||| 26712 ||| 3292 ||| 3293 ||| 34748 ||| 34749 ||| 3294 ||| 
2021 ||| motion planning transformers: one model to plan them all. ||| 34750 ||| 34751 ||| 21783 ||| 34752 ||| 
2019 ||| prediction of reaction time and vigilance variability from spatiospectral features of resting-state eeg in a long sustained attention task. ||| 24205 ||| 24206 ||| 24208 ||| 5335 ||| 24209 ||| 24210 ||| 
2022 ||| does entity abstraction help generative transformers reason? ||| 9415 ||| 9417 ||| 9418 ||| 
2021 ||| how attentive are graph attention networks? ||| 34753 ||| 34754 ||| 22713 ||| 
2021 ||| decoding high-level imagined speech using attention-based deep neural networks. ||| 16092 ||| 16102 ||| 16103 ||| 
2017 ||| predicting the driver's focus of attention: the dr(eye)ve project. ||| 34755 ||| 34756 ||| 34757 ||| 34758 ||| 13611 ||| 
2020 ||| code prediction by feeding trees to transformers. ||| 28224 ||| 28225 ||| 28226 ||| 28227 ||| 
2021 ||| the brownian motion in the transformer model. ||| 34759 ||| 
2021 ||| lightxml: transformer with dynamic negative sampling for high-performance extreme multi-label text classification. ||| 18141 ||| 18142 ||| 18143 ||| 18144 ||| 18145 ||| 3476 ||| 
2020 ||| vertical-horizontal structured attention for generating music with chords. ||| 26579 ||| 26578 ||| 34760 ||| 26582 ||| 18982 ||| 
2021 ||| multi-rate attention architecture for fast streamable text-to-speech spectrum modeling. ||| 8880 ||| 12527 ||| 12528 ||| 12529 ||| 12530 ||| 
2019 ||| et-usb: transformer-based sequential behavior modeling for inbound customer service. ||| 9979 ||| 34761 ||| 
2021 ||| transformer-based approach towards music emotion recognition from lyrics. ||| 15107 ||| 15108 ||| 15109 ||| 
2019 ||| bag: bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. ||| 4979 ||| 4980 ||| 1756 ||| 
2020 ||| pum at semeval-2020 task 12: aggregation of transformer-based models' features for offensive language recognition. ||| 10647 ||| 10648 ||| 10649 ||| 
2021 ||| a unified efficient pyramid transformer for semantic segmentation. ||| 7942 ||| 2422 ||| 254 ||| 7943 ||| 6365 ||| 3046 ||| 
2020 ||| bangla text classification using transformers. ||| 24754 ||| 24755 ||| 24756 ||| 
2019 ||| tree-transformer: a transformer-based method for correction of tree-structured data. ||| 34762 ||| 34763 ||| 34764 ||| 
2019 ||| what would elsa do? freezing layers during transformer fine-tuning. ||| 34765 ||| 26646 ||| 3009 ||| 
2019 ||| attention monitoring and hazard assessment with bio-sensing and vision: empirical analysis utilizing cnns on the kitti dataset. ||| 15469 ||| 7888 ||| 
2017 ||| attention-based vocabulary selection for nmt decoding. ||| 34766 ||| 34767 ||| 34768 ||| 
2021 ||| aei: actors-environment interaction with adaptive attention for temporal action proposals generation. ||| 34769 ||| 34770 ||| 34771 ||| 34772 ||| 2299 ||| 4033 ||| 2137 ||| 
2021 ||| sparse fuzzy attention for structured sentiment analysis. ||| 34773 ||| 33655 ||| 3111 ||| 
2019 ||| pcan: 3d attention map learning using contextual information for point cloud based retrieval. ||| 19226 ||| 19227 ||| 
2021 ||| temgnet: deep transformer-based decoding of upperlimb semg for hand gestures recognition. ||| 34774 ||| 34775 ||| 34776 ||| 5559 ||| 34777 ||| 33528 ||| 
2020 ||| superbloom: bloom filter meets transformer. ||| 34778 ||| 34779 ||| 34780 ||| 34781 ||| 254 ||| 
2019 ||| attention-based multi-instance neural network for medical diagnosis from incomplete and low quality data. ||| 846 ||| 847 ||| 848 ||| 849 ||| 
2018 ||| attention based visual analysis for fast grasp planning with multi-fingered robotic hand. ||| 34782 ||| 2236 ||| 5736 ||| 18083 ||| 
2021 ||| transclaw u-net: claw u-net with transformers for medical image segmentation. ||| 34783 ||| 7715 ||| 6516 ||| 12647 ||| 
2020 ||| autoregressive reasoning over chains of facts with transformers. ||| 11727 ||| 11728 ||| 10702 ||| 
2020 ||| learning to solve vehicle routing problems with time windows through joint attention. ||| 34784 ||| 34785 ||| 
2018 ||| unsupervised word segmentation from speech with attention. ||| 4736 ||| 19 ||| 14601 ||| 13942 ||| 1226 ||| 4737 ||| 13943 ||| 3510 ||| 
2019 ||| singing synthesis: with a little help from my attention. ||| 14699 ||| 14700 ||| 14701 ||| 14370 ||| 
2022 ||| stacked hybrid-attention and group collaborative learning for unbiased scene graph generation. ||| 34786 ||| 34787 ||| 9674 ||| 2516 ||| 7385 ||| 9631 ||| 
2021 ||| deep learning transformer architecture for named entity recognition on low resourced languages: state of the art results. ||| 10294 ||| 
2019 ||| video question generation via cross-modal self-attention networks learning. ||| 12336 ||| 1385 ||| 12337 ||| 1387 ||| 
2019 ||| smart: training shallow memory-aware transformers for robotic explainability. ||| 19001 ||| 19003 ||| 13611 ||| 
2021 ||| fastformer: additive attention can be all you need. ||| 3754 ||| 3755 ||| 3756 ||| 2795 ||| 9574 ||| 
2020 ||| spatial-spectral ffpnet: attention-based pyramid network for segmentation and classification of remote sensing images. ||| 30611 ||| 5958 ||| 30612 ||| 30613 ||| 
2021 ||| exploiting both domain-specific and invariant knowledge via a win-win transformer for unsupervised domain adaptation. ||| 34788 ||| 30661 ||| 2332 ||| 19349 ||| 34789 ||| 3337 ||| 
2019 ||| cross attention network for few-shot classification. ||| 9376 ||| 9377 ||| 9378 ||| 1916 ||| 1788 ||| 
2019 ||| complex transformer: a framework for modeling complex-valued sequence. ||| 12558 ||| 12559 ||| 12560 ||| 3597 ||| 3247 ||| 
2020 ||| fast graph attention networks using effective resistance based graph sparsification. ||| 34790 ||| 1070 ||| 33008 ||| 34791 ||| 23323 ||| 
2021 ||| stjla: a multi-context aware spatio-temporal joint linear attention network for traffic forecasting. ||| 1218 ||| 32241 ||| 32242 ||| 18829 ||| 32244 ||| 
2021 ||| fdgatii : fast dynamic graph attention with initial residual and identity mapping. ||| 34792 ||| 34793 ||| 34794 ||| 33723 ||| 
2021 ||| an attention-based weakly supervised framework for spitzoid melanocytic lesion diagnosis in wsi. ||| 16460 ||| 34795 ||| 34796 ||| 34797 ||| 11927 ||| 34798 ||| 25710 ||| 34799 ||| 3369 ||| 34800 ||| 34801 ||| 34802 ||| 
2019 ||| attention-guided network for ghost-free high dynamic range imaging. ||| 19015 ||| 19016 ||| 19017 ||| 5177 ||| 6335 ||| 9216 ||| 10075 ||| 
2021 ||| physics driven domain specific transporter framework with attention mechanism for ultrasound imaging. ||| 34803 ||| 34804 ||| 24158 ||| 34805 ||| 34806 ||| 34807 ||| 34808 ||| 34809 ||| 34810 ||| 34811 ||| 
2017 ||| time series forecasting using rnns: an extended attention mechanism to model periods and handle missing values. ||| 5127 ||| 5128 ||| 5129 ||| 5130 ||| 5131 ||| 5132 ||| 5133 ||| 
2021 ||| pare: part attention regressor for 3d human body estimation. ||| 2121 ||| 2122 ||| 2123 ||| 2100 ||| 
2020 ||| adapterdrop: on the efficiency of adapters in transformers. ||| 3700 ||| 3701 ||| 26755 ||| 26756 ||| 26757 ||| 23157 ||| 26758 ||| 3702 ||| 
2021 ||| diagnosing transformers in task-oriented semantic parsing. ||| 3178 ||| 3179 ||| 
2019 ||| a neural topic-attention model for medical term abbreviation disambiguation. ||| 34812 ||| 3250 ||| 34813 ||| 34814 ||| 34815 ||| 34816 ||| 3252 ||| 
2022 ||| hierarchical point cloud encoding and decoding with lightweight self-attention based model. ||| 34817 ||| 3386 ||| 11321 ||| 3387 ||| 
2021 ||| self-supervised pre-training for transformer-based person re-identification. ||| 1702 ||| 1703 ||| 15538 ||| 34818 ||| 34819 ||| 1704 ||| 1705 ||| 32211 ||| 
2020 ||| mast: multimodal abstractive summarization with trimodal hierarchical attention. ||| 34820 ||| 34821 ||| 
2019 ||| distilling transformers into simple neural networks with unlabeled transfer data. ||| 34822 ||| 33914 ||| 
2017 ||| syntax-directed attention for neural machine translation. ||| 
2019 ||| pag-net: progressive attention guided depth super-resolution network. ||| 34823 ||| 34824 ||| 34825 ||| 
2020 ||| multiscale mesh deformation component analysis with attention-based autoencoders. ||| 1132 ||| 34826 ||| 34827 ||| 22897 ||| 34828 ||| 1833 ||| 
2021 ||| nxmtransformer: semi-structured sparsification for natural language understanding via admm. ||| 34829 ||| 9224 ||| 9225 ||| 11346 ||| 
2018 ||| focal visual-text attention for visual question answering. ||| 18702 ||| 18703 ||| 12717 ||| 14441 ||| 18704 ||| 
2021 ||| generic event boundary detection challenge at cvpr 2021 technical report: cascaded temporal attention network (castanet). ||| 34830 ||| 8748 ||| 8751 ||| 34831 ||| 8750 ||| 
2018 ||| bilinear attention networks. ||| 8644 ||| 9260 ||| 8580 ||| 
2018 ||| attention gated networks: learning to leverage salient regions in medical images. ||| 31282 ||| 27946 ||| 27948 ||| 27351 ||| 27478 ||| 27844 ||| 27420 ||| 
2021 ||| deep neural networks evolve human-like attention distribution during reading comprehension. ||| 34832 ||| 16953 ||| 
2021 ||| attention w-net: improved skip connections for better representations. ||| 34833 ||| 34367 ||| 34834 ||| 
2020 ||| eeg based continuous speech recognition using transformers. ||| 34835 ||| 34836 ||| 34837 ||| 34838 ||| 
2021 ||| cvt-assd: convolutional vision-transformer based attentive single shot multibox detector. ||| 6257 ||| 6258 ||| 6258 ||| 
2017 ||| what gets media attention and how media attention evolves over time - large-scale empirical evidence from 196 countries. ||| 9045 ||| 9044 ||| 
2021 ||| do multilingual neural machine translation models contain language pair specific attention heads? ||| 3509 ||| 3510 ||| 3511 ||| 3512 ||| 
2018 ||| a fully attention-based information retriever. ||| 501 ||| 502 ||| 503 ||| 504 ||| 505 ||| 
2019 ||| transfer learning from transformers to fake news challenge stance detection (fnc-1) task. ||| 21667 ||| 
2022 ||| team yao at factify 2022: utilizing pre-trained models and co-attention networks for multi-modal fact verification. ||| 34839 ||| 24727 ||| 
2021 ||| bch-nlp at biocreative vii track 3: medications detection in tweets using transformer networks and multi-task learning. ||| 34840 ||| 6059 ||| 27213 ||| 
2021 ||| gashis-transformer: a multi-scale visual transformer approach for gastric histopathology image classification. ||| 32662 ||| 399 ||| 5378 ||| 8841 ||| 13786 ||| 32663 ||| 13788 ||| 34508 ||| 15956 ||| 
2020 ||| deep generative model for image inpainting with local binary pattern learning and spatial attention. ||| 12427 ||| 19695 ||| 19694 ||| 
2021 ||| corgi: content-rich graph neural networks with attention. ||| 34841 ||| 34842 ||| 34843 ||| 34844 ||| 9681 ||| 34845 ||| 
2021 ||| predicting attention sparsity in transformers. ||| 9367 ||| 2871 ||| 34846 ||| 34847 ||| 3547 ||| 34848 ||| 3369 ||| 3370 ||| 
2021 ||| prose2poem: the blessing of transformers in translating prose to persian poetry. ||| 34849 ||| 34850 ||| 34851 ||| 34852 ||| 
2019 ||| divided we stand: a novel residual group attention mechanism for medical image segmentation. ||| 15636 ||| 15638 ||| 15637 ||| 
2021 ||| soft sensing transformer: hundreds of sensors are worth a single word. ||| 8862 ||| 17208 ||| 17209 ||| 17210 ||| 17211 ||| 17212 ||| 17213 ||| 
2020 ||| pranet: parallel reverse attention network for polyp segmentation. ||| 1861 ||| 4054 ||| 614 ||| 4055 ||| 4056 ||| 2445 ||| 1932 ||| 
2020 ||| editor: an edit-based transformer with repositioning for neural machine translation with soft lexical constraints. ||| 34853 ||| 34854 ||| 
2020 ||| a hierarchical transformer for unsupervised parsing. ||| 34855 ||| 
2022 ||| panformer: a transformer based model for pan-sharpening. ||| 34856 ||| 28852 ||| 5705 ||| 
2018 ||| abdominal multi-organ segmentation with organ-attention networks and statistical fusion. ||| 247 ||| 27607 ||| 8906 ||| 31316 ||| 31317 ||| 8660 ||| 
2022 ||| an audio-visual attention based multimodal network for fake talking face videos detection. ||| 34857 ||| 989 ||| 12384 ||| 471 ||| 6488 ||| 10075 ||| 
2021 ||| high-resolution pelvic mri reconstruction using a generative adversarial network with attention and cyclic loss. ||| 21160 ||| 34858 ||| 6522 ||| 34859 ||| 6005 ||| 
2021 ||| transferring knowledge with attention distillation for multi-domain image-to-image translation. ||| 34860 ||| 34861 ||| 34862 ||| 34863 ||| 19187 ||| 
2021 ||| multi-query multi-head attention pooling and inter-topk penalty for speaker verification. ||| 34864 ||| 34865 ||| 34866 ||| 14268 ||| 6796 ||| 34867 ||| 
2020 ||| sign language transformers: joint end-to-end sign language recognition and translation. ||| 8529 ||| 7442 ||| 8767 ||| 8768 ||| 8530 ||| 
2020 ||| the elephant in the interpretability room: why use attention as explanation when we have saliency methods? ||| 20952 ||| 20953 ||| 
2021 ||| recurrent glimpse-based decoder for detection with transformer. ||| 5790 ||| 875 ||| 1756 ||| 
2021 ||| on biasing transformer attention towards monotonicity. ||| 4860 ||| 4861 ||| 4862 ||| 4863 ||| 3847 ||| 
2021 ||| frequency effects on syntactic rule learning in transformers. ||| 26537 ||| 26538 ||| 26539 ||| 26540 ||| 
2021 ||| beyond mono to binaural: generating binaural audio from mono audio with depth and cross modal attention. ||| 7301 ||| 7302 ||| 7287 ||| 
2021 ||| what helps transformers recognize conversational structure? importance of context, punctuation, and labels in dialog act recognition. ||| 12580 ||| 13950 ||| 12583 ||| 
2021 ||| are vision transformers robust to patch perturbations? ||| 34868 ||| 15822 ||| 23521 ||| 
2020 ||| attention-guided chained context aggregation for semantic segmentation. ||| 28720 ||| 28721 ||| 2051 ||| 9472 ||| 
2021 ||| transformers for eeg emotion recognition. ||| 4439 ||| 254 ||| 3890 ||| 15201 ||| 
2019 ||| response of selective attention in middle temporal area. ||| 34869 ||| 
2019 ||| a bi-directional transformer for musical chord recognition. ||| 11902 ||| 11903 ||| 11904 ||| 11905 ||| 11906 ||| 
2020 ||| the jazz transformer on the front line: exploring the shortcomings of ai-composed music through quantitative measures. ||| 11901 ||| 4374 ||| 
2018 ||| multi-task learning with multi-view attention for answer selection and knowledge base question answering. ||| 18178 ||| 18179 ||| 1080 ||| 1081 ||| 18180 ||| 7417 ||| 18181 ||| 1082 ||| 
2021 ||| comparison of convexificated sqcqp and pso for the optimal transmission system operation based on incremental in-phase and quadrature voltage controlled transformers. ||| 34870 ||| 34871 ||| 34872 ||| 34873 ||| 
2021 ||| heterogeneity-aware twitter bot detection with relational graph transformers. ||| 34874 ||| 34875 ||| 8207 ||| 34876 ||| 
2020 ||| hierarchical bi-directional self-attention networks for paper review rating recommendation. ||| 11644 ||| 9407 ||| 11630 ||| 215 ||| 9988 ||| 1094 ||| 
2022 ||| awsnet: an auto-weighted supervision attention network for myocardial scar and edema segmentation in multi-sequence cardiac magnetic resonance images. ||| 31283 ||| 7676 ||| 31284 ||| 3034 ||| 8847 ||| 31285 ||| 20534 ||| 31286 ||| 20755 ||| 20535 ||| 
2020 ||| efficient document re-ranking for transformers by precomputing term representations. ||| 9636 ||| 9637 ||| 9638 ||| 9639 ||| 4957 ||| 9640 ||| 
2020 ||| ventral-dorsal neural networks: object detection via selective attention. ||| 7288 ||| 7289 ||| 7290 ||| 34877 ||| 7292 ||| 7143 ||| 7293 ||| 
2021 ||| quantum mechanics and machine learning synergies: graph attention neural networks to predict chemical reactivity. ||| 34878 ||| 34879 ||| 34880 ||| 4311 ||| 
2021 ||| voxel transformer for 3d object detection. ||| 1681 ||| 1682 ||| 1683 ||| 1684 ||| 1685 ||| 1686 ||| 1687 ||| 1688 ||| 
2021 ||| simullr: simultaneous lip reading transducer with attention-guided adaptive memory. ||| 19440 ||| 1306 ||| 19441 ||| 19442 ||| 5475 ||| 3634 ||| 2359 ||| 
2019 ||| sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization. ||| 12618 ||| 1125 ||| 11418 ||| 12619 ||| 
2020 ||| modality attention and sampling enables deep learning with heterogeneous marker combinations in fluorescence microscopy. ||| 34881 ||| 34882 ||| 34883 ||| 34884 ||| 34885 ||| 3157 ||| 34886 ||| 34887 ||| 34888 ||| 13997 ||| 
2018 ||| speaking style adaptation in text-to-speech synthesis using sequence-to-sequence models with attention. ||| 34889 ||| 34890 ||| 34891 ||| 
2020 ||| knowing what, where and when to look: efficient video action modeling with attention. ||| 34892 ||| 34893 ||| 4046 ||| 2196 ||| 34894 ||| 34895 ||| 2198 ||| 
2021 ||| transsc: transformer-based shape completion for grasp evaluation. ||| 31907 ||| 34896 ||| 34897 ||| 1168 ||| 18083 ||| 
2020 ||| attention-aware noisy label learning for image classification. ||| 29848 ||| 11246 ||| 34898 ||| 8546 ||| 
2019 ||| speaker adaptation for attention-based end-to-end speech recognition. ||| 13892 ||| 13893 ||| 12179 ||| 12033 ||| 
2022 ||| omvp: a transformer-based time and team reinforcement learning scheme for observation-constrained multi-vehicle pursuit in urban area. ||| 34899 ||| 34900 ||| 34901 ||| 34902 ||| 3034 ||| 22573 ||| 
2021 ||| automated identification of cell populations in flow cytometry data with transformers. ||| 34903 ||| 34904 ||| 34905 ||| 34906 ||| 34907 ||| 34908 ||| 34909 ||| 
2020 ||| method and dataset entity mining in scientific literature: a cnn + bi-lstm model with self-attention. ||| 34910 ||| 1422 ||| 15249 ||| 34911 ||| 1419 ||| 885 ||| 1129 ||| 34912 ||| 34913 ||| 
2022 ||| multi-modal brain tumor segmentation via missing modality synthesis and modality-level attention fusion. ||| 8925 ||| 24342 ||| 34914 ||| 34915 ||| 24344 ||| 
2021 ||| end-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection. ||| 14571 ||| 12728 ||| 14572 ||| 34916 ||| 14573 ||| 14574 ||| 
2021 ||| do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet. ||| 34917 ||| 
2021 ||| dual hierarchical attention networks for bi-typed heterogeneous graph learning. ||| 3473 ||| 34918 ||| 34919 ||| 34920 ||| 3477 ||| 3476 ||| 3478 ||| 34921 ||| 
2020 ||| self-attention gazetteer embeddings for named-entity recognition. ||| 14020 ||| 34922 ||| 34923 ||| 
2021 ||| multimodal motion prediction with stacked transformers. ||| 18890 ||| 18891 ||| 18892 ||| 18893 ||| 2373 ||| 
2021 ||| disenhan: disentangled heterogeneous graph attention network for recommendation. ||| 1266 ||| 1267 ||| 1268 ||| 1269 ||| 1270 ||| 1251 ||| 
2018 ||| fading of collective attention shapes the evolution of linguistic variants. ||| 34924 ||| 34925 ||| 34926 ||| 
2021 ||| exploiting global and local attentions for heavy rain removal on single images. ||| 34927 ||| 34928 ||| 7823 ||| 
2019 ||| skin lesion classification using cnns with patch-based attention and diagnosis-guided loss weighting. ||| 29730 ||| 29731 ||| 29732 ||| 59 ||| 29733 ||| 29734 ||| 29735 ||| 1881 ||| 29736 ||| 14906 ||| 
2021 ||| fibro-cosanet: pulmonary fibrosis prognosis prediction using a convolutional self attention network. ||| 34929 ||| 34930 ||| 21814 ||| 34931 ||| 
2020 ||| deep transformer based data augmentation with subword units for morphologically rich online asr. ||| 10195 ||| 34932 ||| 3882 ||| 23280 ||| 34933 ||| 13478 ||| 34934 ||| 16413 ||| 34935 ||| 
2021 ||| on the strengths of cross-attention in pretrained transformers for machine translation. ||| 26297 ||| 1250 ||| 26298 ||| 
2020 ||| chart-to-text: generating natural language descriptions for charts by adapting the transformer model. ||| 10288 ||| 10289 ||| 
2021 ||| boosting the speed of entity alignment 10*: dual attention matching network with normalized hard sample mining. ||| 9025 ||| 9026 ||| 350 ||| 349 ||| 
2020 ||| mmft-bert: multimodal fusion transformer with bert encodings for visual question answering. ||| 26595 ||| 1750 ||| 26596 ||| 1752 ||| 
2020 ||| multimodality biomedical image registration using free point transformer networks. ||| 27618 ||| 27619 ||| 27620 ||| 
2021 ||| action-conditioned 3d human motion synthesis with transformer vae. ||| 2099 ||| 2100 ||| 2101 ||| 2102 ||| 
2017 ||| multi-label image recognition by recurrently discovering attentional regions. ||| 2312 ||| 2313 ||| 1800 ||| 2314 ||| 2315 ||| 
2020 ||| gaussian constrained attention network for scene text recognition. ||| 20339 ||| 20340 ||| 11689 ||| 20341 ||| 4201 ||| 
2020 ||| channel attention networks for robust mr fingerprinting matching. ||| 29719 ||| 29720 ||| 29721 ||| 29722 ||| 29723 ||| 29724 ||| 29725 ||| 29726 ||| 29727 ||| 29728 ||| 29729 ||| 7442 ||| 
2021 ||| can vision transformers learn without natural images? ||| 2326 ||| 2329 ||| 34936 ||| 2328 ||| 11971 ||| 
2022 ||| reltr: relation transformer for scene graph generation. ||| 2406 ||| 2410 ||| 2409 ||| 
2021 ||| graph attention collaborative similarity embedding for recommender system. ||| 11164 ||| 11165 ||| 1234 ||| 11166 ||| 11167 ||| 1239 ||| 
2019 ||| learning lightweight lane detection cnns by self attention distillation. ||| 2289 ||| 312 ||| 2290 ||| 2291 ||| 
2019 ||| multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures. ||| 27607 ||| 27608 ||| 27609 ||| 27610 ||| 247 ||| 8660 ||| 
2020 ||| deep reinforced self-attention masks for abstractive summarization (dr.sas). ||| 34059 ||| 34937 ||| 
2018 ||| defactonlp: fact verification using entity recognition, tfidf vector comparison and decomposable attention. ||| 34938 ||| 34939 ||| 34940 ||| 
2022 ||| multi-label transformer for action unit detection. ||| 34941 ||| 34942 ||| 5733 ||| 5734 ||| 
2020 ||| salience estimation with multi-attention learning for abstractive text summarization. ||| 23337 ||| 3385 ||| 4814 ||| 3015 ||| 
2018 ||| medical code prediction with multi-view convolution and description-regularized label-dependent attention. ||| 34943 ||| 34944 ||| 34945 ||| 34946 ||| 34947 ||| 34948 ||| 34949 ||| 34950 ||| 34951 ||| 
2019 ||| star-transformer. ||| 4967 ||| 3272 ||| 4968 ||| 4969 ||| 4970 ||| 1770 ||| 
2020 ||| distance-aware molecule graph attention network for drug-target binding affinity prediction. ||| 34952 ||| 34953 ||| 14420 ||| 23984 ||| 1704 ||| 17704 ||| 9592 ||| 1406 ||| 
2021 ||| april: finding the achilles' heel on privacy for vision transformers. ||| 2914 ||| 34954 ||| 34955 ||| 34956 ||| 2343 ||| 
2022 ||| artemis: attention-based retrieval with text-explicit matching and implicit similarity. ||| 34957 ||| 34958 ||| 34959 ||| 34960 ||| 
2021 ||| f3snet: a four-step strategy for qim steganalysis of compressed speech based on hierarchical attention network. ||| 34961 ||| 2334 ||| 34962 ||| 
2019 ||| predicting retrosynthetic reaction using self-corrected transformer neural networks. ||| 16736 ||| 23503 ||| 32626 ||| 2013 ||| 16595 ||| 
2020 ||| heads-up! unsupervised constituency parsing via self-attention heads. ||| 2607 ||| 14947 ||| 3713 ||| 13068 ||| 
2019 ||| cdsa: cross-dimensional self-attention for multivariate, geo-tagged time series imputation. ||| 8680 ||| 34963 ||| 34964 ||| 34965 ||| 34966 ||| 18926 ||| 
2021 ||| large scale audio understanding without transformers/ convolutions/ berts/ mixers/ attention/ rnns or .... ||| 33900 ||| 
2021 ||| normformer: improved transformer pretraining with extra normalization. ||| 26340 ||| 34967 ||| 34968 ||| 
2021 ||| levit: a vision transformer in convnet's clothing for faster inference. ||| 1885 ||| 1886 ||| 1887 ||| 1888 ||| 1889 ||| 1890 ||| 1891 ||| 1892 ||| 1893 ||| 
2020 ||| caa-net: conditional atrous cnns with attention for explainable device-robust acoustic scene classification. ||| 12762 ||| 12618 ||| 12763 ||| 12619 ||| 648 ||| 649 ||| 
2022 ||| glassoformer: a query-sparse transformer for post-fault power grid voltage prediction. ||| 34969 ||| 34970 ||| 34971 ||| 34972 ||| 34973 ||| 34974 ||| 
2021 ||| glit: neural architecture search for global and local image transformer. ||| 2384 ||| 2385 ||| 2386 ||| 2387 ||| 2388 ||| 2389 ||| 2390 ||| 2391 ||| 2303 ||| 
2020 ||| whaletrans: e2e whisper to natural speech conversion using modified transformer network. ||| 34975 ||| 34976 ||| 9857 ||| 9858 ||| 
2020 ||| gcan: graph-aware co-attention networks for explainable fake news detection on social media. ||| 3711 ||| 3712 ||| 
2019 ||| question generation by transformers. ||| 34977 ||| 34541 ||| 
2020 ||| see, attend and brake: an attention-based saliency map prediction model for end-to-end driving. ||| 65 ||| 66 ||| 34978 ||| 
2019 ||| attention is all you need for videos: self-attention based video summarization using universal transformers. ||| 34979 ||| 34980 ||| 27068 ||| 
2020 ||| attention-based neural bag-of-features learning for sequence data. ||| 31737 ||| 22419 ||| 13153 ||| 31739 ||| 926 ||| 
2019 ||| dynamic convolution: attention over convolution kernels. ||| 1959 ||| 1954 ||| 2430 ||| 2494 ||| 1957 ||| 8573 ||| 
2018 ||| multimodal affective analysis using hierarchical attention strategy with word-level alignment. ||| 3627 ||| 3628 ||| 3629 ||| 3630 ||| 2419 ||| 2425 ||| 
2020 ||| attention routing: track-assignment detailed routing using attention-based reinforcement learning. ||| 17483 ||| 17484 ||| 34981 ||| 34982 ||| 34983 ||| 17485 ||| 17486 ||| 17487 ||| 
2020 ||| fastformers: highly efficient transformer models for natural language understanding. ||| 34984 ||| 33484 ||| 
2019 ||| caire_hkust at semeval-2019 task 3: hierarchical attention for dialogue emotion classification. ||| 10650 ||| 10651 ||| 10652 ||| 10653 ||| 2377 ||| 3676 ||| 10654 ||| 
2021 ||| dynamic attention-based communication-efficient federated learning. ||| 34985 ||| 34986 ||| 12160 ||| 
2021 ||| normal learning in videos with attention prototype network. ||| 381 ||| 8968 ||| 30164 ||| 34987 ||| 34988 ||| 
2017 ||| multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory. ||| 34989 ||| 34990 ||| 34991 ||| 
2018 ||| watch, listen, and describe: globally and locally aligned cross-modal attentions for video captioning. ||| 398 ||| 4958 ||| 3802 ||| 
2020 ||| end-to-end multi-speaker speech recognition with transformer. ||| 12245 ||| 12246 ||| 12247 ||| 11981 ||| 3549 ||| 
2021 ||| hierarchical multimodal transformer to summarize videos. ||| 22078 ||| 34992 ||| 6922 ||| 
2021 ||| probabilistic attention for interactive segmentation. ||| 34993 ||| 34979 ||| 33064 ||| 
2021 ||| thermal image super-resolution using second-order channel attention with varying receptive fields. ||| 16326 ||| 16327 ||| 
2020 ||| user attention and behaviour in virtual reality art encounter. ||| 25427 ||| 25426 ||| 34994 ||| 34995 ||| 34996 ||| 34997 ||| 
2021 ||| a battle of network structures: an empirical study of cnn, transformer, and mlp. ||| 23303 ||| 7911 ||| 32613 ||| 23304 ||| 7912 ||| 8710 ||| 
2021 ||| end-to-end speaker diarization with transformer. ||| 33343 ||| 33342 ||| 33344 ||| 33345 ||| 
2021 ||| self-attention generative adversarial network for iterative reconstruction of ct images. ||| 34998 ||| 34999 ||| 21591 ||| 
2019 ||| understanding spatial correlation in eye-fixation maps for visual attention in videos. ||| 23 ||| 12604 ||| 35000 ||| 
2019 ||| rap-net: recurrent attention pooling networks for dialogue response selection. ||| 13952 ||| 30131 ||| 4841 ||| 4843 ||| 
2021 ||| multi-time attention networks for irregularly sampled time series. ||| 24012 ||| 6023 ||| 
2021 ||| group-based distinctive image captioning with memory attention. ||| 19478 ||| 19479 ||| 6370 ||| 6371 ||| 
2022 ||| simulation-driven training of vision transformers enabling metal segmentation in x-ray images. ||| 35001 ||| 35002 ||| 35003 ||| 35004 ||| 648 ||| 35005 ||| 35006 ||| 35007 ||| 6818 ||| 
2018 ||| attention boosted sequential inference model. ||| 35008 ||| 32039 ||| 5970 ||| 
2019 ||| drone-based joint density map estimation, localization and tracking with space-time multi-scale attention network. ||| 8751 ||| 8749 ||| 8754 ||| 5077 ||| 19228 ||| 9575 ||| 15557 ||| 
2021 ||| mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. ||| 24016 ||| 32356 ||| 
2019 ||| a tensorized transformer for language modeling. ||| 5076 ||| 989 ||| 3364 ||| 3706 ||| 5078 ||| 3764 ||| 3480 ||| 
2019 ||| integrating source-channel and attention-based sequence-to-sequence models for speech recognition. ||| 12715 ||| 8862 ||| 12012 ||| 
2020 ||| few-shot action recognition via improved attention with self-supervision. ||| 8693 ||| 254 ||| 2203 ||| 7449 ||| 2160 ||| 8694 ||| 
2019 ||| sanvis: visual analytics for understanding self-attention networks. ||| 1408 ||| 24846 ||| 24847 ||| 24848 ||| 24849 ||| 24850 ||| 4095 ||| 24851 ||| 24852 ||| 1183 ||| 
2019 ||| robust invisible video watermarking with attention. ||| 35009 ||| 4060 ||| 35010 ||| 35011 ||| 
2020 ||| mt-bioner: multi-task learning for biomedical named entity recognition using deep bidirectional transformers. ||| 35012 ||| 35013 ||| 35014 ||| 
2021 ||| tvt: transferable vision transformer for unsupervised domain adaptation. ||| 35015 ||| 2046 ||| 7957 ||| 1265 ||| 
2022 ||| measuring the mixing of contextual information in the transformer. ||| 26474 ||| 35016 ||| 35017 ||| 3466 ||| 
2022 ||| attention aided csi wireless localization. ||| 35018 ||| 35019 ||| 35020 ||| 
2021 ||| fusion of medical imaging and electronic health records with attention and multi-head machanisms. ||| 35021 ||| 35022 ||| 35023 ||| 19666 ||| 35024 ||| 16746 ||| 
2021 ||| vision transformer hashing for image retrieval. ||| 28203 ||| 35025 ||| 35026 ||| 
2018 ||| sequence-based person attribute recognition with joint ctc-attention model. ||| 5170 ||| 35027 ||| 860 ||| 35028 ||| 33123 ||| 
2019 ||| self-attention based end-to-end hindi-english neural machine translation. ||| 35029 ||| 35030 ||| 
2019 ||| real image denoising with feature attention. ||| 2474 ||| 2475 ||| 
2018 ||| multilingual nmt with a language-independent attention bridge. ||| 23764 ||| 23765 ||| 2698 ||| 23766 ||| 4194 ||| 23767 ||| 23768 ||| 
2021 ||| multimodal pet/ct tumour segmentation and prediction of progression-free survival using a full-scale unet with attention. ||| 27667 ||| 27668 ||| 27669 ||| 27670 ||| 
2020 ||| transformer based language models for similar text retrieval and ranking. ||| 35031 ||| 35032 ||| 35033 ||| 35034 ||| 35035 ||| 35036 ||| 35037 ||| 
2020 ||| character-aware attention-based end-to-end speech recognition. ||| 13892 ||| 13893 ||| 12179 ||| 12033 ||| 
2021 ||| visual parser: representing part-whole hierarchies with transformers. ||| 2157 ||| 2156 ||| 2083 ||| 2160 ||| 
2022 ||| acort: a compact object relation transformer for parameter efficient image captioning. ||| 33863 ||| 35038 ||| 7852 ||| 27311 ||| 
2021 ||| heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning. ||| 30672 ||| 18265 ||| 18245 ||| 35039 ||| 18266 ||| 1134 ||| 
2021 ||| predicting vehicles trajectories in urban scenarios with transformer networks and augmented information. ||| 15448 ||| 15449 ||| 15450 ||| 15451 ||| 14327 ||| 15452 ||| 15453 ||| 15454 ||| 
2018 ||| forward attention in sequence-to-sequence acoustic modelling for speech synthesis. ||| 12661 ||| 4894 ||| 12372 ||| 
2018 ||| classification based grasp detection using spatial transformer network. ||| 35040 ||| 35041 ||| 
2020 ||| transformer-based end-to-end speech recognition with local dense synthesizer attention. ||| 4484 ||| 4483 ||| 4381 ||| 
2020 ||| heart sound segmentation using bidirectional lstms with attention. ||| 12567 ||| 31036 ||| 11374 ||| 11330 ||| 31037 ||| 11331 ||| 
2021 ||| content-augmented feature pyramid network with light linear transformers. ||| 35042 ||| 35043 ||| 35044 ||| 3501 ||| 
2020 ||| lite transformer with long-short range attention. ||| 3162 ||| 3163 ||| 24024 ||| 24025 ||| 3166 ||| 
2020 ||| exploration of audio quality assessment and anomaly localisation using attention models. ||| 12087 ||| 8233 ||| 
2021 ||| augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. ||| 1428 ||| 1427 ||| 3906 ||| 1094 ||| 
2020 ||| keep your eyes on the lane: attention-guided lane detection. ||| 19123 ||| 19124 ||| 19125 ||| 7033 ||| 19126 ||| 19127 ||| 19128 ||| 
2021 ||| offroadtranseg: semi-supervised segmentation using transformers on offroad environments. ||| 35045 ||| 35046 ||| 35047 ||| 
2021 ||| not all images are worth 16x16 words: dynamic vision transformers with adaptive sequence length. ||| 34789 ||| 1858 ||| 35048 ||| 2356 ||| 7875 ||| 
2020 ||| contextualized graph attention network for recommendation with item knowledge graph. ||| 19671 ||| 4297 ||| 16693 ||| 16696 ||| 16597 ||| 1692 ||| 
2020 ||| semi-supervised learning of galaxy morphology using equivariant transformer variational autoencoders. ||| 35049 ||| 35050 ||| 33649 ||| 
2018 ||| deep neural net with attention for multi-channel multi-touch attribution. ||| 7015 ||| 35051 ||| 35052 ||| 35053 ||| 35054 ||| 
2021 ||| poshan: cardinal pos pattern guided attention for news headline incongruence. ||| 1314 ||| 1315 ||| 
2019 ||| deep learning investigation for chess player attention prediction using eye-tracking and game data. ||| 27047 ||| 27048 ||| 27049 ||| 27050 ||| 
2021 ||| revisiting linformer with a modified self-attention with linear complexity. ||| 35055 ||| 
2021 ||| non-autoregressive transformer with unified bidirectional decoder for automatic speech recognition. ||| 35056 ||| 9337 ||| 35057 ||| 17394 ||| 17396 ||| 5051 ||| 
2021 ||| unetr: transformers for 3d medical image segmentation. ||| 7205 ||| 2019 ||| 34009 ||| 2024 ||| 
2021 ||| where to look at the movies : analyzing visual attention to understand movie editing. ||| 35058 ||| 35059 ||| 35060 ||| 
2021 ||| local multi-head channel self-attention for facial expression recognition. ||| 35061 ||| 35062 ||| 35063 ||| 35064 ||| 
2020 ||| attention based on-device streaming speech recognition with large speech corpus. ||| 13904 ||| 13935 ||| 13902 ||| 13936 ||| 13937 ||| 13938 ||| 13939 ||| 13940 ||| 13941 ||| 13933 ||| 12048 ||| 13934 ||| 13906 ||| 
2021 ||| lmr-cbt: learning modality-fused representations with cb-transformer for multimodal emotion recognition from unaligned multimodal sequences. ||| 33256 ||| 968 ||| 33257 ||| 35065 ||| 22614 ||| 33258 ||| 5255 ||| 22615 ||| 
2021 ||| realtrans: end-to-end simultaneous speech translation with convolutional weighted-shrinking transformer. ||| 3634 ||| 3635 ||| 3443 ||| 
2022 ||| using multi-scale swintransformer-htc with data augmentation in conic challenge. ||| 35066 ||| 35067 ||| 35068 ||| 35069 ||| 35070 ||| 35071 ||| 
2021 ||| leveraging human selective attention for medical image analysis with limited training data. ||| 6419 ||| 8776 ||| 35072 ||| 35073 ||| 19768 ||| 35074 ||| 35075 ||| 19502 ||| 7894 ||| 
2022 ||| under the hood of transformer networks for trajectory forecasting. ||| 35076 ||| 35077 ||| 20111 ||| 7272 ||| 7276 ||| 7277 ||| 
2020 ||| attention-aware inference for neural abstractive summarization. ||| 19576 ||| 35078 ||| 
2021 ||| eegdnet: fusing non-local and local self-similarity for 1-d eeg signal denoising with 2-d transformer. ||| 12062 ||| 35079 ||| 35080 ||| 6481 ||| 35081 ||| 35082 ||| 
2021 ||| longt5: efficient text-to-text transformer for long sequences. ||| 3789 ||| 3557 ||| 35083 ||| 9233 ||| 3882 ||| 34638 ||| 35084 ||| 3786 ||| 
2021 ||| geometric algebra attention networks for small point clouds. ||| 35085 ||| 
2021 ||| guided interactive video object segmentation using reliability-based attention maps. ||| 19268 ||| 19269 ||| 8631 ||| 
2021 ||| self-supervision and spatial-sequential attention based loss for multi-person pose estimation. ||| 35086 ||| 35087 ||| 35088 ||| 20086 ||| 
2021 ||| natural language inference with a human touch: using human explanations to guide model attention. ||| 35089 ||| 20960 ||| 11639 ||| 
2021 ||| multi-mode transformer transducer with stochastic future context. ||| 13904 ||| 14702 ||| 14703 ||| 13963 ||| 3549 ||| 
2019 ||| modelling bahdanau attention using election methods aided by q-learning. ||| 35090 ||| 35091 ||| 
2019 ||| social attention for autonomous decision-making in dense traffic. ||| 35092 ||| 21860 ||| 
2022 ||| linearizing transformer with key-value memory bank. ||| 1709 ||| 1115 ||| 
2020 ||| message-aware graph attention networks for large-scale multi-robot path planning. ||| 35093 ||| 35094 ||| 6474 ||| 35095 ||| 
2018 ||| phrase-based attentions. ||| 35096 ||| 1313 ||| 
2018 ||| weakly supervised attention learning for textual phrases grounding. ||| 35097 ||| 7324 ||| 23892 ||| 35098 ||| 
2017 ||| dipole: diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. ||| 1071 ||| 1298 ||| 1299 ||| 1296 ||| 23368 ||| 930 ||| 
2021 ||| tri-transformer hawkes process: three heads are better than one. ||| 416 ||| 5310 ||| 414 ||| 5345 ||| 
2020 ||| automated topical component extraction using neural network attention scores from source-based essay scoring. ||| 789 ||| 3188 ||| 
2019 ||| multisource region attention network for fine-grained object recognition in remote sensing imagery. ||| 6902 ||| 35099 ||| 35100 ||| 
2022 ||| zero-shot action recognition with transformer-based video semantic embedding. ||| 32380 ||| 32382 ||| 
2021 ||| oracle linguistic graphs complement a pretrained transformer language model: a cross-formalism comparison. ||| 35101 ||| 35102 ||| 3139 ||| 
2019 ||| enhancing neural sequence labeling with position-aware self-attention. ||| 5474 ||| 35103 ||| 34045 ||| 11710 ||| 12300 ||| 35104 ||| 
2021 ||| did the cat drink the coffee? challenging transformers with generalized event knowledge. ||| 14231 ||| 14232 ||| 8955 ||| 14233 ||| 10699 ||| 14234 ||| 
2021 ||| where to look: a unified attention model for visual recognition with reinforcement learning. ||| 1216 ||| 
2021 ||| on evolving attention towards domain adaptation. ||| 19414 ||| 1424 ||| 32949 ||| 25392 ||| 19415 ||| 2382 ||| 2367 ||| 32950 ||| 
2021 ||| pretrained transformers as universal computation engines. ||| 33083 ||| 33086 ||| 18850 ||| 33088 ||| 
2020 |||  net: augmented parallel-pyramid net for attention guided pose estimation. ||| 20355 ||| 4470 ||| 20356 ||| 7064 ||| 1248 ||| 2824 ||| 
2018 ||| deriving machine attention from human rationales. ||| 26501 ||| 3094 ||| 3093 ||| 26502 ||| 
2018 ||| subjective annotations for vision-based attention level estimation. ||| 16412 ||| 16413 ||| 16414 ||| 16415 ||| 16416 ||| 16417 ||| 
2022 ||| efficient-dyn: dynamic graph representation learning via event-based temporal sparse attention network. ||| 33798 ||| 859 ||| 
2022 ||| interacting attention graph for single image two-hand reconstruction. ||| 35105 ||| 35106 ||| 35107 ||| 35108 ||| 17396 ||| 18653 ||| 1716 ||| 
2021 ||| local frequency domain transformer networks for video prediction. ||| 407 ||| 408 ||| 409 ||| 
2019 ||| hyper-sagnn: a self-attention based graph neural network for hypergraphs. ||| 24046 ||| 24047 ||| 24048 ||| 
2019 ||| signed graph attention networks. ||| 4120 ||| 4121 ||| 4122 ||| 1445 ||| 
2022 ||| spatiotemporal transformer attention network for 3d voxel level joint segmentation and motion prediction in point cloud. ||| 35109 ||| 35110 ||| 35111 ||| 35112 ||| 35113 ||| 35114 ||| 35115 ||| 28106 ||| 35116 ||| 
2022 ||| hindi/bengali sentiment analysis using transfer learning and joint dual input learning with self attention. ||| 35117 ||| 35118 ||| 
2017 ||| structured attention networks. ||| 9410 ||| 23956 ||| 23957 ||| 4962 ||| 
2022 ||| multi-tailed vision transformer for efficient inference. ||| 35119 ||| 17757 ||| 3156 ||| 
2021 ||| full transformer framework for robust point cloud registration with deep information interaction. ||| 35120 ||| 1907 ||| 21766 ||| 35121 ||| 1722 ||| 
2021 ||| sub-word level lip reading with visual attention. ||| 35122 ||| 33067 ||| 1997 ||| 
2021 ||| improving visual quality of image synthesis by a token-based generator with transformers. ||| 35123 ||| 18950 ||| 1775 ||| 35124 ||| 1699 ||| 
2018 ||| deep attentional structured representation learning for visual recognition. ||| 21490 ||| 8505 ||| 
2021 ||| irene: interpretable energy prediction for transformers. ||| 3295 ||| 3296 ||| 3297 ||| 3298 ||| 3299 ||| 
2020 ||| bet: a backtranslation approach for easy data augmentation in transformer-based paraphrase identification context. ||| 35125 ||| 35126 ||| 
2021 ||| attentionflow: visualising influence in networks of time series. ||| 22914 ||| 22915 ||| 22916 ||| 22917 ||| 14797 ||| 22918 ||| 14077 ||| 
2021 ||| uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer. ||| 35127 ||| 7482 ||| 7079 ||| 14023 ||| 14024 ||| 
2020 ||| channel attention with embedding gaussian process: a probabilistic methodology. ||| 1483 ||| 1482 ||| 1484 ||| 35128 ||| 786 ||| 
2021 ||| spatial-channel transformer network for trajectory prediction on the traffic scenes. ||| 35129 ||| 28672 ||| 35130 ||| 35131 ||| 
2021 ||| pa-resseg: a phase attention residual network for liver tumor segmentation from multi-phase ct images. ||| 28628 ||| 27881 ||| 1550 ||| 3289 ||| 1551 ||| 35132 ||| 11468 ||| 27882 ||| 35133 ||| 1526 ||| 6422 ||| 1528 ||| 11469 ||| 
2021 ||| improving customer service chatbots with attention-based transfer learning. ||| 35134 ||| 
2019 ||| relation-aware global attention. ||| 11585 ||| 18278 ||| 7912 ||| 18802 ||| 11586 ||| 
2018 ||| self attention grid for person re-identification. ||| 13741 ||| 13742 ||| 13743 ||| 
2021 ||| a tri-attention fusion guided multi-modal segmentation network. ||| 15607 ||| 15608 ||| 35135 ||| 2693 ||| 15610 ||| 
2020 ||| group equivariant stand-alone self-attention for vision. ||| 23897 ||| 22850 ||| 
2022 ||| grapheye: a novel solution for detecting vulnerable functions based on graph attention network. ||| 11358 ||| 35136 ||| 35137 ||| 5064 ||| 17694 ||| 35138 ||| 
2021 ||| convolution-free waveform transformers for multi-lead ecg classification. ||| 20414 ||| 20418 ||| 20415 ||| 20435 ||| 20420 ||| 
2021 ||| understanding robustness of transformers for image classification. ||| 2567 ||| 2568 ||| 2569 ||| 2570 ||| 2571 ||| 2572 ||| 
2021 ||| full attention bidirectional deep learning structure for single channel speech enhancement. ||| 35139 ||| 11779 ||| 35140 ||| 
2021 ||| a graph vae and graph transformer approach to generating molecular graphs. ||| 33871 ||| 35141 ||| 35142 ||| 33872 ||| 
2018 ||| attention-mechanism-based tracking method for intelligent internet of vehicles. ||| 35143 ||| 9705 ||| 8718 ||| 35144 ||| 35145 ||| 
2022 ||| deep learning assisted end-to-end synthesis of mm-wave passive networks with 3d em structures: a study on a transformer-based matching network. ||| 35146 ||| 35147 ||| 35148 ||| 185 ||| 35149 ||| 22811 ||| 300 ||| 
2019 ||| session-based social recommendation via dynamic graph attention networks. ||| 1269 ||| 22912 ||| 1266 ||| 22913 ||| 1251 ||| 1248 ||| 
2021 ||| investigating transfer learning capabilities of vision transformers and cnns by fine-tuning a single trainable block. ||| 35150 ||| 35151 ||| 35152 ||| 
2020 ||| super resolution using segmentation-prior self-attention generative adversarial network. ||| 35153 ||| 35154 ||| 35155 ||| 
2021 ||| nommer: nominate synergistic context in vision transformer for visual recognition. ||| 5170 ||| 19628 ||| 633 ||| 35156 ||| 35157 ||| 33123 ||| 
2018 ||| advancing connectionist temporal classification with attention modeling. ||| 12178 ||| 12179 ||| 8164 ||| 12033 ||| 
2020 ||| multi-scale attention u-net (msaunet): a modified u-net architecture for scene segmentation. ||| 35158 ||| 7000 ||| 
2021 ||| relating transformers to models and neural representations of the hippocampal formation. ||| 35159 ||| 35160 ||| 35161 ||| 
2021 ||| style transfer with target feature palette and attention coloring. ||| 35162 ||| 35163 ||| 7338 ||| 
2021 ||| transformer based automatic covid-19 fake news detection system. ||| 35164 ||| 10510 ||| 
2021 ||| adaptive fourier neural operators: efficient token mixers for transformers. ||| 35165 ||| 35166 ||| 35167 ||| 2458 ||| 34410 ||| 2459 ||| 
2018 ||| attention-based temporal weighted convolutional neural network for action recognition. ||| 14777 ||| 6436 ||| 14778 ||| 6437 ||| 35168 ||| 8674 ||| 14779 ||| 
2022 ||| overcoming a theoretical limitation of self-attention. ||| 4846 ||| 35169 ||| 
2020 ||| step: sequence-to-sequence transformer pre-training for document summarization. ||| 35170 ||| 3479 ||| 3131 ||| 3174 ||| 3480 ||| 
2019 ||| transfer nas: knowledge transfer between search spaces with transformer agents. ||| 35171 ||| 35172 ||| 35173 ||| 35174 ||| 
2021 ||| transformers and transfer learning for improving portuguese semantic role labeling. ||| 9975 ||| 9976 ||| 9977 ||| 9978 ||| 
2021 ||| learned queries for efficient local attention. ||| 35175 ||| 1980 ||| 35176 ||| 
2021 ||| mrat-sql+gap: a portuguese text-to-sql transformer. ||| 16110 ||| 504 ||| 505 ||| 
2019 ||| rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation. ||| 14696 ||| 14697 ||| 14670 ||| 12659 ||| 14698 ||| 14320 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2019 ||| a radio signal modulation recognition algorithm based on residual networks and attention mechanisms. ||| 35177 ||| 17729 ||| 35178 ||| 5187 ||| 35179 ||| 35180 ||| 
2021 ||| boosting neural machine translation with dependency-scaled self-attention network. ||| 35181 ||| 35182 ||| 13614 ||| 6938 ||| 9320 ||| 
2018 ||| dynamic self-attention : computing attention over words dynamically for sentence embedding. ||| 35183 ||| 35184 ||| 35185 ||| 
2020 ||| improve transformer models with better relative position embeddings. ||| 12360 ||| 26438 ||| 3676 ||| 3251 ||| 
2017 ||| structured attentions for visual question answering. ||| 1835 ||| 1836 ||| 1837 ||| 1838 ||| 1839 ||| 
2020 ||| multi-unit transformers for neural machine translation. ||| 26521 ||| 3075 ||| 1921 ||| 
2021 ||| the center of attention: center-keypoint grouping via attention for multi-person pose estimation. ||| 2573 ||| 2574 ||| 2575 ||| 
2020 ||| geometric scattering attention networks. ||| 12656 ||| 12657 ||| 12658 ||| 
2021 ||| videogpt: video generation using vq-vae and transformers. ||| 35186 ||| 35187 ||| 18850 ||| 18848 ||| 
2022 ||| input-specific attention subnetworks for adversarial detection. ||| 10935 ||| 11767 ||| 11768 ||| 3328 ||| 
2020 ||| attention-guided network for iris presentation attack detection. ||| 7153 ||| 7154 ||| 
2020 ||| identify speakers in cocktail parties with end-to-end attention. ||| 14545 ||| 14546 ||| 12396 ||| 
2021 ||| thinking like transformers. ||| 22712 ||| 3441 ||| 22713 ||| 
2019 ||| dependency-aware named entity recognition with relative and global attentions. ||| 35188 ||| 26294 ||| 
2018 ||| image denoising and restoration with cnn-lstm encoder decoder with direct attention. ||| 35189 ||| 26272 ||| 35190 ||| 
2020 ||| wat zei je? detecting out-of-distribution translations with variational transformers. ||| 35191 ||| 9134 ||| 33649 ||| 
2021 ||| nested multiple instance learning with attention mechanisms. ||| 35192 ||| 35193 ||| 2713 ||| 35194 ||| 
2021 ||| x-volution: on the unification of convolution and self-attention. ||| 35195 ||| 35196 ||| 8832 ||| 
2021 ||| vqa-mhug: a gaze dataset to study multimodal neural attention in visual question answering. ||| 9269 ||| 23109 ||| 23110 ||| 23111 ||| 23112 ||| 8348 ||| 
2020 ||| audio adversarial examples for robust hybrid ctc/attention speech recognition. ||| 23270 ||| 23271 ||| 23272 ||| 23273 ||| 23274 ||| 5695 ||| 
2022 ||| deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation. ||| 35197 ||| 35198 ||| 2693 ||| 34629 ||| 
2021 ||| fully adaptive self-stabilizing transformer for lcl problems. ||| 35199 ||| 35200 ||| 35201 ||| 25712 ||| 
2022 ||| viewformer: nerf-free neural rendering from few images using transformers. ||| 35202 ||| 35203 ||| 35204 ||| 35205 ||| 35206 ||| 35207 ||| 
2021 ||| the next 700 program transformers. ||| 35208 ||| 
2020 ||| telling bert's full story: from local attention to global aggregation. ||| 23999 ||| 23998 ||| 24002 ||| 
2022 ||| planet: dynamic content planning in autoregressive transformers for long-form text generation. ||| 35209 ||| 34464 ||| 35210 ||| 3439 ||| 3416 ||| 3604 ||| 
2021 ||| domain transformer: predicting samples of unseen, future domains. ||| 35211 ||| 
2018 ||| multilingual end-to-end speech recognition with a single transformer on low-resource languages. ||| 5268 ||| 5270 ||| 728 ||| 
2019 ||| taking a stance on fake news: towards automatic disinformation assessment via deep bidirectional transformer language models for stance detection. ||| 35212 ||| 35213 ||| 19195 ||| 7865 ||| 
2020 ||| learning light-weight translation models from deep transformer. ||| 3305 ||| 11355 ||| 17677 ||| 17678 ||| 2333 ||| 17679 ||| 3306 ||| 
2017 ||| attention-based end-to-end speech recognition in mandarin. ||| 12394 ||| 12757 ||| 4551 ||| 12384 ||| 
2021 ||| global structure-aware drum transcription based on self-attention mechanisms. ||| 35214 ||| 8071 ||| 8074 ||| 
2022 ||| smoothing matters: momentum transformer for domain adaptive semantic segmentation. ||| 33712 ||| 1261 ||| 35215 ||| 35216 ||| 1168 ||| 1262 ||| 1263 ||| 
2018 ||| diagnose like a radiologist: attention guided convolutional neural network for thorax disease classification. ||| 30827 ||| 2918 ||| 30828 ||| 30829 ||| 8571 ||| 208 ||| 
2018 ||| social media attention increases article visits: an investigation on article-level referral data of peerj. ||| 35217 ||| 35218 ||| 35219 ||| 35220 ||| 
2021 ||| eeg-transformer: self-attention from transformer architecture for decoding eeg of imagined speech. ||| 16100 ||| 16101 ||| 
2017 ||| deep semantic role labeling with self-attention. ||| 11746 ||| 3428 ||| 11745 ||| 18155 ||| 18156 ||| 
2021 ||| va-gcn: a vector attention graph convolution network for learning on point clouds. ||| 34099 ||| 34098 ||| 35221 ||| 
2021 ||| differentiable subset pruning of transformer heads. ||| 35222 ||| 3485 ||| 33862 ||| 
2021 ||| orthogonal attention: a cloze-style approach to negation scope resolution. ||| 13371 ||| 35223 ||| 
2021 ||| generating abstractive summaries of lithuanian news articles using a transformer model. ||| 23285 ||| 23286 ||| 
2021 ||| a text autoencoder from transformer for fast encoding language representation. ||| 35224 ||| 
2020 ||| incorporating effective global information via adaptive gate attention for text classification. ||| 35225 ||| 32058 ||| 35226 ||| 24819 ||| 3477 ||| 
2019 ||| maanet: multi-view aware attention networks for image super-resolution. ||| 19681 ||| 19682 ||| 19684 ||| 
2021 ||| starnet: joint action-space prediction with star graphs and implicit global frame self-attention. ||| 35227 ||| 35228 ||| 35229 ||| 17425 ||| 
2020 ||| fma-eta: estimating travel time entirely based on ffn with attention. ||| 12744 ||| 12745 ||| 12746 ||| 369 ||| 12747 ||| 12748 ||| 12749 ||| 
2020 ||| berters: multimodal representation learning for expert recommendation system with transformer. ||| 35230 ||| 35231 ||| 35232 ||| 35233 ||| 
2021 ||| boxer: box-attention for 2d and 3d transformers. ||| 19292 ||| 35234 ||| 35235 ||| 35236 ||| 18223 ||| 
2019 ||| less memory, faster speed: refining self-attention module for image reconstruction. ||| 369 ||| 33204 ||| 35237 ||| 35238 ||| 
2019 ||| temporal self-attention network for medical concept embedding. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 2730 ||| 
2020 ||| topological planning with transformers for vision-and-language navigation. ||| 19104 ||| 19105 ||| 19106 ||| 19107 ||| 2698 ||| 9218 ||| 
2018 ||| augmenting neural response generation with context-aware topical attention. ||| 35239 ||| 35240 ||| 35241 ||| 14023 ||| 14024 ||| 
2021 ||| cswin transformer: a general vision transformer backbone with cross-shaped windows. ||| 18967 ||| 23783 ||| 2494 ||| 18972 ||| 2305 ||| 1957 ||| 6604 ||| 1772 ||| 
2022 ||| deep bidirectional transformers for soc flow specification mining. ||| 35242 ||| 27375 ||| 
2019 ||| aspect specific opinion expression extraction using attention based lstm-crf network. ||| 35243 ||| 10755 ||| 
2019 ||| progressive self-supervised attention learning for aspect-level sentiment analysis. ||| 3652 ||| 3653 ||| 3182 ||| 3654 ||| 3655 ||| 3656 ||| 2166 ||| 
2021 ||| few-shot temporal action localization with query adaptive transformer. ||| 35244 ||| 2196 ||| 2198 ||| 
2021 ||| inversemv: composing piano scores with a convolutional video-music transformer. ||| 35245 ||| 26666 ||| 
2020 ||| weak supervision and referring attention for temporal-textual association learning. ||| 35097 ||| 7324 ||| 7436 ||| 2035 ||| 35098 ||| 
2022 ||| transfusion: robust lidar-camera fusion for 3d object detection with transformers. ||| 35246 ||| 35247 ||| 35248 ||| 35249 ||| 19010 ||| 35250 ||| 35251 ||| 
2019 ||| cross-modal self-attention network for referring image segmentation. ||| 18830 ||| 18831 ||| 2740 ||| 602 ||| 
2021 ||| are pre-trained convolutions better than pre-trained transformers? ||| 1398 ||| 2293 ||| 3290 ||| 3292 ||| 3291 ||| 3293 ||| 3294 ||| 
2018 ||| sequential attention gan for interactive image editing via dialogue. ||| 2045 ||| 2044 ||| 19570 ||| 2046 ||| 1958 ||| 
2021 ||| attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. ||| 35252 ||| 35253 ||| 8405 ||| 
2018 ||| harmonious attention network for person re-identification. ||| 3337 ||| 2196 ||| 19101 ||| 
2020 ||| electricity theft detection with self-attention. ||| 35254 ||| 35255 ||| 35256 ||| 35257 ||| 3006 ||| 35258 ||| 35259 ||| 
2017 ||| interpretable learning for self-driving cars by visualizing causal attention. ||| 2041 ||| 2042 ||| 
2020 ||| stress test evaluation of transformer-based models in natural language understanding tasks. ||| 21678 ||| 3369 ||| 21679 ||| 21680 ||| 
2020 ||| see more, know more: unsupervised video object segmentation with co-attention siamese networks. ||| 19316 ||| 2444 ||| 5264 ||| 2445 ||| 1932 ||| 7408 ||| 
2021 ||| m-based algorithm for approximating self-attention. ||| 18028 ||| 18029 ||| 18030 ||| 18031 ||| 18032 ||| 8766 ||| 18033 ||| 
2021 ||| distantly supervised relation extraction via recursive hierarchy-interactive attention and entity-order perception. ||| 35260 ||| 11124 ||| 35261 ||| 11095 ||| 35262 ||| 6174 ||| 
2019 ||| program classification using gated graph attention neural network for online programming service. ||| 35263 ||| 35264 ||| 29202 ||| 32622 ||| 12801 ||| 
2017 ||| a neural attention model for categorizing patient safety events. ||| 3122 ||| 15090 ||| 4957 ||| 15091 ||| 
2018 ||| video object segmentation with joint re-identification and attention-aware mask propagation. ||| 8776 ||| 2291 ||| 
2021 ||| transferring bert-like transformers' knowledge for authorship verification. ||| 4974 ||| 4975 ||| 4976 ||| 35265 ||| 7356 ||| 35266 ||| 
2018 ||| exgate: externally controlled gating for feature-based attention in artificial neural networks. ||| 35267 ||| 35268 ||| 
2019 ||| self-attention with functional time representation learning. ||| 9419 ||| 9420 ||| 9423 ||| 9421 ||| 9422 ||| 9424 ||| 
2021 ||| efficient conformer-based speech recognition with linear attention. ||| 4483 ||| 4484 ||| 4381 ||| 
2018 ||| video-based person re-identification via 3d convolutional networks and non-local attention. ||| 6386 ||| 6387 ||| 6388 ||| 
2021 ||| tsnat: two-step non-autoregressvie transformer models for speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 12243 ||| 3364 ||| 12244 ||| 14380 ||| 
2022 ||| transfuse: a unified transformer-based image fusion framework using self-supervised learning. ||| 32797 ||| 32798 ||| 32799 ||| 35269 ||| 35270 ||| 35271 ||| 27524 ||| 
2020 ||| age and gender prediction from face images using attentional convolutional network. ||| 35272 ||| 35273 ||| 35274 ||| 17204 ||| 
2020 ||| rethinking batch normalization in transformers. ||| 3822 ||| 22812 ||| 22813 ||| 22814 ||| 2596 ||| 
2022 ||| improving fraud detection via hierarchical attention-based graph neural network. ||| 35275 ||| 35276 ||| 35277 ||| 
2019 ||| spatiotemporal tile-based attention-guided lstms for traffic video prediction. ||| 35278 ||| 
2019 ||| image super-resolution via attention based back projection networks. ||| 1542 ||| 1539 ||| 7834 ||| 1540 ||| 7835 ||| 
2022 ||| x-trans2cap: cross-modal knowledge transfer using transformer for 3d dense captioning. ||| 35279 ||| 35280 ||| 35281 ||| 20179 ||| 1800 ||| 5189 ||| 13677 ||| 
2020 ||| sceneformer: indoor scene generation with transformers. ||| 13615 ||| 13616 ||| 13617 ||| 12149 ||| 
2018 ||| learning when to concentrate or divert attention: self-adaptive attention temperature for neural machine translation. ||| 25375 ||| 3751 ||| 9350 ||| 26618 ||| 16564 ||| 
2019 ||| forced spatial attention for driver foot activity classification. ||| 7887 ||| 7888 ||| 
2019 ||| image captioning using facial expression and attention. ||| 28562 ||| 3258 ||| 3257 ||| 3157 ||| 3158 ||| 
2021 ||| study of positional encoding approaches for audio spectrogram transformers. ||| 35282 ||| 35283 ||| 35284 ||| 
2022 ||| estimation of speaker age and height from speech signal using bi-encoder transformer mixture model. ||| 35285 ||| 35286 ||| 4468 ||| 35287 ||| 
2019 ||| progressive face super-resolution via attention to facial landmark. ||| 21495 ||| 21496 ||| 2047 ||| 7383 ||| 
2020 ||| loss-analysis via attention-scale for physiologic time series. ||| 35288 ||| 35289 ||| 35290 ||| 35291 ||| 
2022 ||| robustness verification for attention networks using mixed integer programming. ||| 35292 ||| 35293 ||| 35294 ||| 2713 ||| 14153 ||| 
2021 ||| federated learning with dynamic transformer for text to speech. ||| 14348 ||| 701 ||| 703 ||| 3114 ||| 14349 ||| 705 ||| 
2022 ||| uofa-truth at factify 2022 : transformer and transfer learning based multi-modal fact-checking. ||| 35295 ||| 14023 ||| 14024 ||| 1226 ||| 35296 ||| 
2018 ||| mattnet: modular attention network for referring expression comprehension. ||| 18765 ||| 18766 ||| 7142 ||| 18767 ||| 18768 ||| 3810 ||| 3809 ||| 
2020 ||| coral: code representation learning with weakly-supervised transformers for analyzing data analysis. ||| 35297 ||| 34063 ||| 1305 ||| 35298 ||| 34064 ||| 
2021 ||| learning graph structures with transformer for multivariate time series anomaly detection in iot. ||| 1187 ||| 35299 ||| 9587 ||| 1192 ||| 1190 ||| 
2020 ||| pinet: attention pooling for graph classification. ||| 35300 ||| 35301 ||| 35302 ||| 
2019 ||| multi-graph transformer for free-hand sketch recognition. ||| 3676 ||| 35303 ||| 11322 ||| 
2020 ||| why is attention not so attentive? ||| 25391 ||| 25392 ||| 25393 ||| 1705 ||| 9598 ||| 2355 ||| 
2020 ||| sequential neural rendering with transformer. ||| 6319 ||| 6320 ||| 6321 ||| 6322 ||| 
2022 ||| tableformer: table structure understanding with transformers. ||| 35304 ||| 35305 ||| 35306 ||| 35307 ||| 
2021 ||| class token and knowledge distillation for multi-head self-attention speaker verification systems. ||| 12378 ||| 12379 ||| 12380 ||| 4046 ||| 12381 ||| 
2021 ||| mutually-constrained monotonic multihead attention for online asr. ||| 12145 ||| 12146 ||| 9316 ||| 
2021 ||| gene transformer: transformers for the gene expression-based classification of cancer subtypes. ||| 35308 ||| 35309 ||| 
2021 ||| u-gat: multimodal graph attention network for covid-19 outcome prediction. ||| 35310 ||| 27956 ||| 35311 ||| 27672 ||| 27671 ||| 35312 ||| 35313 ||| 35314 ||| 13628 ||| 35315 ||| 
2020 ||| cnrl at semeval-2020 task 5: modelling causal reasoning in language with multi-head self-attention weights based counterfactual detection. ||| 10591 ||| 4053 ||| 
2020 ||| airborne lidar point cloud classification with graph attention convolution neural network. ||| 35316 ||| 2008 ||| 35317 ||| 6747 ||| 35318 ||| 
2021 ||| hi-transformer: hierarchical interactive transformer for efficient and effective long document modeling. ||| 3754 ||| 3755 ||| 3756 ||| 2795 ||| 
2020 ||| weakly supervised segmentation with multi-scale adversarial attention gates. ||| 35319 ||| 35320 ||| 13369 ||| 
2021 ||| attention based communication and control for multi-uav path planning. ||| 35321 ||| 35322 ||| 11870 ||| 11872 ||| 
2021 ||| anchor detr: query design for transformer-based detector. ||| 35323 ||| 35324 ||| 35325 ||| 4394 ||| 
2017 ||| diversity driven attention model for query-based abstractive summarization. ||| 3327 ||| 3328 ||| 3329 ||| 3330 ||| 
2018 ||| end-to-end neural relation extraction using deep biaffine attention. ||| 15064 ||| 15065 ||| 
2020 ||| attention-based network for low-light image enhancement. ||| 3761 ||| 19015 ||| 1107 ||| 19865 ||| 19866 ||| 10075 ||| 
2019 ||| learning dynamics of attention: human prior for interpretable machine reasoning. ||| 9355 ||| 9356 ||| 
2020 ||| fairs - soft focus generator and attention for robust object segmentation from extreme points. ||| 35326 ||| 35327 ||| 1932 ||| 35328 ||| 
2017 ||| bottom-up and top-down attention for image captioning and vqa. ||| 8565 ||| 3561 ||| 18973 ||| 18974 ||| 3259 ||| 7450 ||| 241 ||| 
2020 ||| convolutional neural network optimization via channel reassessment attention module. ||| 35329 ||| 35330 ||| 
2020 ||| spatten: efficient sparse attention architecture with cascade token and head pruning. ||| 3161 ||| 8326 ||| 3166 ||| 
2019 ||| point clouds learning with attention-based graph convolution networks. ||| 35331 ||| 35332 ||| 19855 ||| 
2019 ||| improving object detection with inverted attention. ||| 2356 ||| 7392 ||| 7393 ||| 
2022 ||| signal-aware direction-of-arrival estimation using attention mechanisms. ||| 12553 ||| 35333 ||| 12556 ||| 35334 ||| 
2021 ||| transformer-based dual relation graph for multi-label image recognition. ||| 2378 ||| 2379 ||| 2380 ||| 2381 ||| 2382 ||| 2383 ||| 
2022 ||| 3d multi-object tracking using graph neural networks with cross-edge modality attention. ||| 35335 ||| 35336 ||| 
2020 ||| graph-based universal dependency parsing in the age of the transformer: what works, and what doesn't. ||| 35337 ||| 35338 ||| 15084 ||| 20980 ||| 
2021 ||| revitalizing cnn attentions via transformers in self-supervised visual representation learning. ||| 35339 ||| 35340 ||| 8540 ||| 8538 ||| 7284 ||| 2011 ||| 
2020 ||| cars can't fly up in the sky: improving urban-scene segmentation via height-driven attention networks. ||| 18851 ||| 18852 ||| 1183 ||| 
2021 ||| query2label: a simple transformer way to multi-label classification. ||| 35341 ||| 241 ||| 2985 ||| 5041 ||| 25978 ||| 
2020 ||| spatial transformer point convolution. ||| 35342 ||| 11246 ||| 2816 ||| 541 ||| 1825 ||| 
2018 ||| statistical transformer networks: learning shape and appearance models via self supervision. ||| 8005 ||| 8007 ||| 
2018 ||| automated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks. ||| 35343 ||| 35344 ||| 35345 ||| 
2021 ||| investigating attention mechanism in 3d point cloud object detection. ||| 13619 ||| 13620 ||| 2474 ||| 13621 ||| 
2021 ||| lattegan: visually guided language attention for multi-turn text-conditioned image manipulation. ||| 1981 ||| 1983 ||| 1982 ||| 726 ||| 1985 ||| 
2019 ||| boosted attention: leveraging human attention for image captioning. ||| 2235 ||| 1872 ||| 
2022 ||| multiscale convolutional transformer with center mask pretraining for hyperspectral image classification. ||| 35346 ||| 1266 ||| 
2022 ||| datr: domain-adaptive transformer for multi-domain landmark detection. ||| 35347 ||| 35348 ||| 27778 ||| 
2020 ||| alphanet: an attention guided deep network for automatic image matting. ||| 11858 ||| 11859 ||| 11860 ||| 
2020 ||| semg gesture recognition with a simple model of attention. ||| 9338 ||| 9339 ||| 35349 ||| 9341 ||| 
2020 |||  english fasttext embeddings with the transformer. ||| 35350 ||| 35351 ||| 27018 ||| 
2022 ||| crossformer: cross spatio-temporal transformer for 3d human pose estimation. ||| 35352 ||| 35353 ||| 5328 ||| 32395 ||| 15326 ||| 35354 ||| 
2020 ||| deformer: decomposing pre-trained transformers for faster question answering. ||| 3295 ||| 3297 ||| 3298 ||| 3299 ||| 
2022 ||| dnnfuser: generative pre-trained transformer as a generalized mapper for layer fusion in dnn accelerators. ||| 35355 ||| 35356 ||| 35357 ||| 
2020 ||| s-vectors: speaker embeddings based on transformer's encoder for text-independent speaker verification. ||| 12073 ||| 35358 ||| 12074 ||| 
2021 ||| rethinking and improving relative position encoding for vision transformer. ||| 1773 ||| 1698 ||| 1774 ||| 1699 ||| 1775 ||| 
2021 ||| the entire network structure of crossmodal transformer. ||| 5860 ||| 17297 ||| 17298 ||| 17299 ||| 15890 ||| 5480 ||| 17300 ||| 17301 ||| 
2019 ||| cross-attention end-to-end asr for two-party conversations. ||| 12177 ||| 12405 ||| 14721 ||| 
2019 ||| location attention for extrapolation to longer sequences. ||| 3339 ||| 3340 ||| 3341 ||| 3342 ||| 
2021 ||| unifying global-local representations in salient object detection with transformer. ||| 19151 ||| 35359 ||| 19150 ||| 35360 ||| 19152 ||| 
2020 ||| sea-net: squeeze-and-excitation attention net for diabetic retinopathy grading. ||| 11281 ||| 11513 ||| 11514 ||| 3488 ||| 
2020 ||| knowledge fusion transformers for video action recognition. ||| 35361 ||| 35362 ||| 35363 ||| 
2020 ||| brums at semeval-2020 task 12 : transformer based multilingual offensive language identification in social media. ||| 3849 ||| 10586 ||| 
2020 ||| cost-effective interactive attention learning with neural attention processes. ||| 9311 ||| 22781 ||| 22782 ||| 9315 ||| 9314 ||| 9316 ||| 9317 ||| 
2021 ||| neural attention distillation: erasing backdoor triggers from deep neural networks. ||| 24028 ||| 24029 ||| 24030 ||| 24031 ||| 1717 ||| 24032 ||| 
2020 ||| guiding monocular depth estimation using depth-attention volume. ||| 6320 ||| 6319 ||| 8855 ||| 6321 ||| 6322 ||| 
2021 ||| an ensemble of pre-trained transformer models for imbalanced multiclass malware classification. ||| 35364 ||| 35365 ||| 35366 ||| 35367 ||| 35368 ||| 35369 ||| 
2020 ||| apan: asynchronous propagate attention network for real-time temporal graph embedding. ||| 15150 ||| 15151 ||| 15152 ||| 15153 ||| 5101 ||| 15154 ||| 15155 ||| 15156 ||| 11792 ||| 15157 ||| 6503 ||| 
2021 ||| caltext: contextual attention localization for offline handwritten text. ||| 9795 ||| 9796 ||| 
2021 ||| ask2transformers: zero-shot domain labelling with pre-trained language models. ||| 11950 ||| 11951 ||| 
2020 ||| hierarchical multi-scale attention for semantic segmentation. ||| 2458 ||| 35370 ||| 2459 ||| 
2021 ||| new approaches to long document summarization: fourier transform based attention in a transformer model. ||| 35371 ||| 35372 ||| 35373 ||| 
2021 ||| hierarchical rnns-based transformers maddpg for mixed cooperative-competitive environments. ||| 35374 ||| 35375 ||| 35376 ||| 35377 ||| 35378 ||| 35379 ||| 35380 ||| 
2022 ||| multi-view subspace adaptive learning via autoencoder and attention. ||| 415 ||| 5311 ||| 5312 ||| 35381 ||| 
2022 ||| uniformer: unifying convolution and self-attention for visual recognition. ||| 34404 ||| 2148 ||| 6213 ||| 2170 ||| 34405 ||| 16550 ||| 1848 ||| 2149 ||| 
2020 ||| supervised attention for speaker recognition. ||| 25666 ||| 12731 ||| 14397 ||| 
2019 ||| camal: context-aware multi-scale attention framework for lightweight visual place recognition. ||| 35382 ||| 35383 ||| 35384 ||| 35385 ||| 
2020 ||| epgat: gene essentiality prediction with graph attention networks. ||| 1994 ||| 35386 ||| 35387 ||| 35388 ||| 
2018 ||| attentioned convolutional lstm inpaintingnetwork for anomaly detection in videos. ||| 35389 ||| 35390 ||| 
2019 ||| an attention mechanism for musical instrument recognition. ||| 11907 ||| 11908 ||| 11909 ||| 
2021 ||| sgtr: end-to-end scene graph generation with transformer. ||| 35391 ||| 14513 ||| 17714 ||| 
2019 ||| directing dnns attention for facial attribution classification using gradient-weighted class activation mapping. ||| 13408 ||| 19114 ||| 19115 ||| 17507 ||| 
2020 ||| attention-slam: a visual monocular slam learning from human gaze. ||| 35392 ||| 21599 ||| 35393 ||| 35394 ||| 6417 ||| 4003 ||| 27170 ||| 35395 ||| 
2021 ||| human attention in fine-grained classification. ||| 35396 ||| 19479 ||| 33711 ||| 7218 ||| 
2020 ||| global self-attention networks for image recognition. ||| 7265 ||| 2463 ||| 35397 ||| 35398 ||| 35399 ||| 
2020 ||| cross-correlated attention networks for person re-identification. ||| 2128 ||| 2129 ||| 2127 ||| 2131 ||| 2130 ||| 
2021 ||| multi-modal fusion transformer for end-to-end autonomous driving. ||| 2125 ||| 2124 ||| 2126 ||| 
2021 ||| multi-task learning with cross attention for keyword spotting. ||| 13960 ||| 13961 ||| 13962 ||| 
2020 ||| on the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition. ||| 925 ||| 926 ||| 
2019 ||| dynamic graph attention for referring expression comprehension. ||| 1799 ||| 1800 ||| 1801 ||| 
2018 ||| compositional attention networks for interpretability in natural language question answering. ||| 35400 ||| 23745 ||| 23746 ||| 18375 ||| 
2020 ||| radial deformation emplacement in power transformers using long short-term memory networks. ||| 22308 ||| 22309 ||| 22310 ||| 22311 ||| 22312 ||| 
2019 ||| mina: multilevel knowledge-guided attention for modeling electrocardiography signals. ||| 23442 ||| 1070 ||| 23322 ||| 23443 ||| 23323 ||| 
2021 ||| hopper: multi-hop transformer for spatiotemporal reasoning. ||| 23934 ||| 23935 ||| 23936 ||| 23937 ||| 23938 ||| 4933 ||| 23939 ||| 
2019 ||| accelerating transformer decoding via a hybrid of self-attention and recurrent neural network. ||| 14693 ||| 3045 ||| 12389 ||| 
2022 ||| mdmmt-2: multidomain multimodal transformer for video retrieval, one more step towards generalization. ||| 35401 ||| 19277 ||| 19276 ||| 35402 ||| 
2019 ||| semantically conditioned dialog response generation via hierarchical disentangled self-attention. ||| 3799 ||| 3800 ||| 784 ||| 3801 ||| 3802 ||| 
2017 ||| su-rug at the conll-sigmorphon 2017 shared task: morphological inflection with attentional sequence-to-sequence models. ||| 23080 ||| 23081 ||| 20957 ||| 
2021 ||| graph relation transformer: incorporating pairwise object features into the transformer architecture. ||| 35403 ||| 35404 ||| 35405 ||| 35406 ||| 
2021 ||| dual attention-based federated learning for wireless traffic prediction. ||| 20499 ||| 20500 ||| 20501 ||| 20502 ||| 
2021 ||| a weakly-supervised depth estimation network using attention mechanism. ||| 19776 ||| 28648 ||| 1754 ||| 35407 ||| 19777 ||| 
2021 ||| attributing fair decisions with attention interventions. ||| 35408 ||| 35409 ||| 3909 ||| 35410 ||| 3911 ||| 
2021 ||| on the robustness of vision transformers to adversarial examples. ||| 2167 ||| 2168 ||| 2169 ||| 
2018 ||| attention-aware generative adversarial networks (ata-gans). ||| 3930 ||| 3931 ||| 3932 ||| 3933 ||| 3934 ||| 
2020 ||| multistage attention resu-net for semantic segmentation of fine-resolution remote sensing images. ||| 8207 ||| 32217 ||| 30455 ||| 30454 ||| 
2019 ||| is attention all what you need? - an empirical investigation on convolution-based active memory and self-attention. ||| 33184 ||| 8888 ||| 
2021 ||| variational attention: propagating domain-specific knowledge for multi-domain learning in crowd counting. ||| 2015 ||| 2016 ||| 1424 ||| 2017 ||| 890 ||| 2018 ||| 241 ||| 
2019 ||| attentiongan: unpaired image-to-image translation using attention-guided generative adversarial networks. ||| 435 ||| 2519 ||| 436 ||| 2160 ||| 437 ||| 
2021 ||| understanding top-down attention using task-oriented ablation design. ||| 34715 ||| 34717 ||| 34716 ||| 34718 ||| 
2021 ||| adversarially regularized graph attention networks for inductive learning on partially labeled graphs. ||| 35411 ||| 1167 ||| 35412 ||| 35413 ||| 35414 ||| 
2018 ||| multi-cast attention networks for retrieval-based question answering and response prediction. ||| 1398 ||| 9015 ||| 9016 ||| 
2020 ||| do syntax trees help pre-trained transformers extract information? ||| 21387 ||| 24349 ||| 112 ||| 9237 ||| 
2020 ||| $o(n)$ connections are expressive enough: universal approximability of sparse transformers. ||| 9155 ||| 9156 ||| 2567 ||| 9157 ||| 9158 ||| 9159 ||| 
2021 ||| open set domain recognition via attention-based gcn and semantic matching optimization. ||| 20153 ||| 6650 ||| 6769 ||| 
2022 ||| knowledge-enriched attention network with group-wise semantic for visual storytelling. ||| 19238 ||| 7656 ||| 370 ||| 13811 ||| 
2019 ||| explainable authorship verification in social media via attention-based similarity learning. ||| 17153 ||| 17154 ||| 8252 ||| 17155 ||| 
2021 ||| learning to recognize actions on objects in egocentric video with attention dictionaries. ||| 9832 ||| 8035 ||| 9833 ||| 
2020 ||| detection of lexical stress errors in non-native (l2) english with data augmentation and attention. ||| 14364 ||| 14365 ||| 14366 ||| 14367 ||| 14368 ||| 14369 ||| 12016 ||| 14370 ||| 14371 ||| 
2021 ||| teach me how to label: labeling functions from natural language with text-to-text transformers. ||| 85 ||| 
2020 ||| geometric attention for prediction of differential properties in 3d point clouds. ||| 2645 ||| 2646 ||| 2647 ||| 2648 ||| 
2018 ||| pcas: pruning channels with attention statistics. ||| 21461 ||| 21462 ||| 
2021 ||| dual-camera super-resolution with aligned attention modules. ||| 2281 ||| 2282 ||| 1845 ||| 2283 ||| 2284 ||| 
2021 ||| image inpainting with edge-guided learnable bidirectional attention maps. ||| 15057 ||| 2528 ||| 2529 ||| 35168 ||| 2018 ||| 
2021 ||| ast-transformer: encoding abstract syntax trees efficiently for code summarization. ||| 3873 ||| 3874 ||| 3875 ||| 3876 ||| 3877 ||| 382 ||| 
2022 ||| tactis: transformer-attentional copulas for time series. ||| 35415 ||| 35416 ||| 35417 ||| 
2020 ||| multi-interactive attention network for fine-grained feature learning in ctr prediction. ||| 3433 ||| 22907 ||| 22930 ||| 1302 ||| 22931 ||| 1086 ||| 22932 ||| 1301 ||| 
2020 ||| label enhanced event detection with heterogeneous graph attention networks. ||| 35418 ||| 3326 ||| 35419 ||| 759 ||| 23404 ||| 16421 ||| 
2021 ||| an iterative contextualization algorithm with second-order attention. ||| 16977 ||| 16979 ||| 
2019 ||| an attentional neural network architecture for folk song classification. ||| 35420 ||| 35421 ||| 35422 ||| 
2021 ||| all the attention you need: global-local, spatial-channel attention for image retrieval. ||| 7433 ||| 7434 ||| 7435 ||| 
2021 ||| el-attention: memory efficient lossless attention for generation. ||| 7666 ||| 22779 ||| 17691 ||| 22780 ||| 4813 ||| 3706 ||| 4817 ||| 
2021 ||| dnn quantization with attention. ||| 20222 ||| 35423 ||| 34552 ||| 35424 ||| 
2020 ||| selective particle attention: visual feature-based attention in deep reinforcement learning. ||| 35425 ||| 35426 ||| 
2021 ||| multi-task learning with attention for end-to-end autonomous driving. ||| 18812 ||| 18813 ||| 18814 ||| 18815 ||| 18816 ||| 
2019 ||| improving graph attention networks with large margin-based constraints. ||| 23361 ||| 23362 ||| 14316 ||| 23363 ||| 
2021 ||| detecting log anomalies with multi-head attention (lama). ||| 35427 ||| 15164 ||| 35428 ||| 35429 ||| 15162 ||| 
2020 ||| attention driven fusion for multi-modal emotion recognition. ||| 12566 ||| 12567 ||| 11374 ||| 11331 ||| 11330 ||| 
2020 ||| la-hcn: label-based attention for hierarchical multi-label textclassification neural network. ||| 29522 ||| 29523 ||| 29524 ||| 29525 ||| 
2022 ||| hyperbolic vision transformers: combining improvements in metric learning. ||| 35430 ||| 35431 ||| 35432 ||| 437 ||| 35433 ||| 
2018 ||| exploring the use of attention within an neural machine translation decoder states to translate idioms. ||| 13994 ||| 35434 ||| 13995 ||| 
2017 ||| character-level intra attention network for natural language inference. ||| 26003 ||| 3466 ||| 852 ||| 3467 ||| 
2021 ||| multivariate and propagation graph attention network for spatial-temporal prediction with outdoor cellular traffic. ||| 1384 ||| 1385 ||| 1386 ||| 1387 ||| 
2021 ||| date: detecting anomalies in text via self-supervision of transformers. ||| 4974 ||| 4975 ||| 4976 ||| 
2020 ||| news-driven stock prediction with attention-based noisy recurrent state transition. ||| 2530 ||| 688 ||| 3289 ||| 32003 ||| 
2020 ||| transformer-based context-aware sarcasm detection in conversation threads from social media. ||| 10453 ||| 3489 ||| 375 ||| 
2021 ||| information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition. ||| 12403 ||| 1010 ||| 1008 ||| 13930 ||| 35435 ||| 12404 ||| 
2020 ||| attendaffectnet: self-attention based networks for predicting affective responses from movies. ||| 20374 ||| 20375 ||| 936 ||| 20376 ||| 
2019 ||| multi-granularity self-attention for neural machine translation. ||| 4796 ||| 3309 ||| 4882 ||| 4797 ||| 3041 ||| 
2022 ||| external attention assisted multi-phase splenic vascular injury segmentation with limited data. ||| 27607 ||| 27608 ||| 247 ||| 35436 ||| 8906 ||| 8660 ||| 
2019 ||| query-based interactive recommendation by meta-path and adapted attention-gru. ||| 1107 ||| 1108 ||| 1109 ||| 1110 ||| 1111 ||| 1112 ||| 1113 ||| 1114 ||| 1115 ||| 
2022 ||| trasetr: track-to-segment transformer with contrastive query for instance-level instrument segmentation in robotic surgery. ||| 35437 ||| 27917 ||| 8637 ||| 
2020 ||| feature pyramid transformer. ||| 1751 ||| 2484 ||| 8536 ||| 444 ||| 8537 ||| 2483 ||| 
2021 ||| cca-mdd: a coupled cross-attention based framework for streaming mispronunciation detection and diagnosis. ||| 35438 ||| 35439 ||| 14756 ||| 14758 ||| 35440 ||| 35441 ||| 35442 ||| 17840 ||| 3443 ||| 
2021 ||| dynamicvit: efficient vision transformers with dynamic token sparsification. ||| 2115 ||| 35443 ||| 35444 ||| 1920 ||| 1921 ||| 21252 ||| 
2020 ||| transformer based multi-source domain adaptation. ||| 26663 ||| 3586 ||| 
2021 ||| efficient training of visual transformers with small-size datasets. ||| 32580 ||| 7351 ||| 35445 ||| 437 ||| 35446 ||| 35447 ||| 
2020 ||| kronecker attention networks. ||| 25403 ||| 25412 ||| 23491 ||| 
2017 ||| reliability assessment of distribution system using fuzzy logic for modelling of transformer and line uncertainties. ||| 35448 ||| 35449 ||| 35450 ||| 35451 ||| 35452 ||| 35453 ||| 
2021 ||| offensive language detection with bert-based models, by customizing attention probabilities. ||| 35454 ||| 35455 ||| 35456 ||| 
2021 ||| 3d-transformer: molecular representation with transformer in 3d space. ||| 35457 ||| 817 ||| 35458 ||| 35459 ||| 13735 ||| 35460 ||| 17987 ||| 17990 ||| 
2020 ||| san-m: memory equipped self-attention for end-to-end speech recognition. ||| 14668 ||| 14494 ||| 14599 ||| 12357 ||| 
2021 ||| evaluating transformer based semantic segmentation networks for pathological image segmentation. ||| 29952 ||| 35461 ||| 24706 ||| 
2020 ||| digital twin of distribution power transformer for real-time monitoring of medium voltage from low voltage measurements. ||| 35462 ||| 35463 ||| 
2021 ||| dual contrastive loss and attention for gans. ||| 2455 ||| 2456 ||| 2457 ||| 2458 ||| 2459 ||| 1948 ||| 108 ||| 
2021 ||| main: multihead-attention imputation networks. ||| 320 ||| 321 ||| 322 ||| 323 ||| 
2020 ||| predicting chemical properties using self-attention multi-task learning based on smiles representation. ||| 20317 ||| 20318 ||| 
2021 ||| attention-based contextual language model adaptation for speech recognition. ||| 3838 ||| 3839 ||| 3840 ||| 3841 ||| 3842 ||| 3843 ||| 
2018 ||| a mixed hierarchical attention based encoder-decoder approach for standard table summarization. ||| 4918 ||| 3329 ||| 4919 ||| 3327 ||| 3328 ||| 4917 ||| 
2021 ||| improving hybrid ctc/attention end-to-end speech recognition with pretrained acoustic and language model. ||| 12400 ||| 13915 ||| 13916 ||| 13917 ||| 
2021 ||| low-rank temporal attention-augmented bilinear network for financial time-series forecasting. ||| 27251 ||| 926 ||| 
2022 ||| can transformers be strong treatment effect estimators? ||| 29636 ||| 35464 ||| 3068 ||| 12015 ||| 35465 ||| 
2021 ||| enhancing content preservation in text style transfer using reverse attention and conditional layer normalization. ||| 3587 ||| 3588 ||| 3589 ||| 3590 ||| 
2022 ||| end-to-end video text spotting with transformer. ||| 35466 ||| 35467 ||| 1714 ||| 6335 ||| 35468 ||| 19505 ||| 2011 ||| 
2021 ||| efficient large-scale image retrieval with deep feature orthogonality and hybrid-swin-transformers. ||| 35469 ||| 
2022 ||| bioformers: embedding transformers for ultra-low power semg-based gesture recognition. ||| 11861 ||| 35470 ||| 11862 ||| 35471 ||| 11865 ||| 11027 ||| 11028 ||| 11025 ||| 
2018 ||| destnet: densely fused spatial transformer networks. ||| 21465 ||| 21466 ||| 21467 ||| 
2021 ||| meetsum: transforming meeting transcript summarization using transformers! ||| 35472 ||| 33081 ||| 35473 ||| 
2021 ||| dyadformer: a multi-modal transformer for long-range modeling of dyadic interactions. ||| 8026 ||| 8027 ||| 7111 ||| 8028 ||| 8029 ||| 4194 ||| 8030 ||| 7591 ||| 8031 ||| 8032 ||| 8033 ||| 8034 ||| 8035 ||| 8036 ||| 
2022 ||| self-supervised and interpretable anomaly detection using network transformers. ||| 35474 ||| 35475 ||| 35476 ||| 35477 ||| 
2018 ||| an unsupervised model with attention autoencoders for question retrieval. ||| 18264 ||| 18233 ||| 
2019 ||| reducing transformer depth on demand with structured dropout. ||| 23898 ||| 3798 ||| 1889 ||| 
2020 ||| ernie-doc: the retrospective long-document modeling transformer. ||| 3412 ||| 3413 ||| 3414 ||| 673 ||| 3415 ||| 3416 ||| 3417 ||| 
2020 ||| egad: evolving graph representation learning with self-attention and knowledge distillation for live video streaming events. ||| 17105 ||| 17106 ||| 17107 ||| 
2021 ||| multi-view analysis of unregistered medical images using cross-view transformers. ||| 27566 ||| 27771 ||| 27772 ||| 
2019 ||| who did they respond to? conversation structure modeling using masked hierarchical transformer. ||| 18120 ||| 18121 ||| 18122 ||| 18123 ||| 3251 ||| 
2022 ||| towards weakly-supervised text spotting using a multi-task transformer. ||| 35478 ||| 35479 ||| 35480 ||| 35481 ||| 2644 ||| 35482 ||| 
2019 ||| controlling utterance length in nmt-based word segmentation with attention. ||| 4736 ||| 3510 ||| 1226 ||| 4737 ||| 
2019 ||| multi-agent attentional activity recognition. ||| 770 ||| 771 ||| 773 ||| 16806 ||| 775 ||| 
2020 ||| dr-spaam: a spatial-attention and auto-regressive model for person detection in 2d range data. ||| 25548 ||| 25549 ||| 25550 ||| 
2022 ||| factored attention and embedding for unstructured-view topic-related ultrasound report generation. ||| 17673 ||| 2367 ||| 32664 ||| 35483 ||| 35484 ||| 35485 ||| 820 ||| 
2021 ||| dct: dynamic compressive transformer for modeling unbounded sequence. ||| 35486 ||| 18273 ||| 
2021 ||| gcn-se: attention as explainability for node classification in dynamic graphs. ||| 18518 ||| 18519 ||| 18520 ||| 
2020 ||| meantime: mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. ||| 21245 ||| 21246 ||| 21247 ||| 
2020 ||| "when they say weed causes depression, but it's your fav antidepressant": knowledge-aware attention framework for relationship extraction. ||| 3062 ||| 35487 ||| 35488 ||| 35489 ||| 35490 ||| 35491 ||| 
2020 ||| attention-based joint detection of object and semantic part. ||| 35492 ||| 35493 ||| 35494 ||| 
2021 ||| fusformer: a transformer-based fusion approach for hyperspectral image super-resolution. ||| 35495 ||| 35496 ||| 35497 ||| 
2020 ||| pneumoxttention: a cnn compensating for human fallibility when detecting pneumonia through cxr images with attention. ||| 3018 ||| 
2018 ||| aggregated sparse attention for steering angle prediction. ||| 2195 ||| 20342 ||| 20343 ||| 2603 ||| 
2022 ||| direcformer: a directed attention in transformer approach to robust action recognition. ||| 2132 ||| 35498 ||| 2133 ||| 35499 ||| 20208 ||| 633 ||| 2138 ||| 
2021 ||| transformer-based image compression. ||| 35500 ||| 35501 ||| 35502 ||| 35503 ||| 19057 ||| 
2021 ||| automated news summarization using transformers. ||| 35504 ||| 35505 ||| 35506 ||| 35507 ||| 
2022 ||| satr: slice attention with transformer for universal lesion detection. ||| 3694 ||| 9570 ||| 5752 ||| 27778 ||| 
2019 ||| text generation from knowledge graphs with graph transformers. ||| 4762 ||| 4763 ||| 4764 ||| 3408 ||| 4765 ||| 
2020 ||| streaming attention-based models with augmented memory for end-to-end speech recognition. ||| 11978 ||| 11973 ||| 11974 ||| 11976 ||| 11975 ||| 11977 ||| 12491 ||| 
2022 ||| pathsage: spatial graph attention neural networks with random path sampling. ||| 5105 ||| 1221 ||| 209 ||| 210 ||| 
2019 ||| bira-net: bilinear attention net for diabetic retinopathy grading. ||| 11281 ||| 11282 ||| 11283 ||| 11284 ||| 11285 ||| 3036 ||| 5858 ||| 
2021 ||| wassa@iitk at wassa 2021: multi-task learning and transformer finetuning for emotion classification and empathy prediction. ||| 10608 ||| 10607 ||| 20762 ||| 
2021 ||| zero-shot cross-lingual transfer in legal domain using transformer models. ||| 35508 ||| 35509 ||| 35510 ||| 
2017 ||| dense transformer networks. ||| 5536 ||| 23488 ||| 23489 ||| 23490 ||| 23491 ||| 
2021 ||| learning to iteratively solve routing problems with dual-aspect collaborative transformer. ||| 35511 ||| 30672 ||| 18245 ||| 18266 ||| 17992 ||| 17993 ||| 35512 ||| 
2017 ||| refining raw sentence representations for textual entailment recognition via attention. ||| 20763 ||| 7447 ||| 21248 ||| 20764 ||| 
2017 ||| code completion with neural attention and pointer networks. ||| 595 ||| 7400 ||| 598 ||| 597 ||| 
2020 ||| attention-based residual speech portrait model for speech to face generation. ||| 14759 ||| 35513 ||| 2014 ||| 683 ||| 5950 ||| 35514 ||| 
2021 ||| single-layer vision transformers for more accurate early exits with less overhead. ||| 35515 ||| 336 ||| 926 ||| 
2020 ||| hybrid dynamic-static context-aware attention network for action assessment in long videos. ||| 19571 ||| 19572 ||| 14892 ||| 19573 ||| 19482 ||| 19574 ||| 19575 ||| 
2020 ||| improved biomedical word embeddings in the transformer era. ||| 31161 ||| 31162 ||| 
2022 ||| monocular robot navigation with self-supervised pretrained vision transformers. ||| 35516 ||| 35517 ||| 35518 ||| 
2021 ||| transformers generalize deepsets and can be extended to graphs and hypergraphs. ||| 4505 ||| 35519 ||| 35520 ||| 
2020 ||| how to track your dragon: a multi-attentional framework for real-time rgb-d 6-dof object pose tracking. ||| 8807 ||| 7929 ||| 8808 ||| 8809 ||| 8810 ||| 7930 ||| 
2019 ||| spatial attention for far-field speech recognition with deep beamforming neural networks. ||| 12445 ||| 218 ||| 12446 ||| 12447 ||| 12448 ||| 12449 ||| 
2020 ||| stable style transformer: delete and generate approach with encoder-decoder for text style transfer. ||| 10273 ||| 
2021 ||| gtnet: guided transformer network for detecting human-object interactions. ||| 19185 ||| 35521 ||| 35522 ||| 35523 ||| 7201 ||| 
2019 ||| pdanet: polarity-consistent deep attention network for fine-grained visual emotion regression. ||| 1831 ||| 19450 ||| 10064 ||| 7760 ||| 10065 ||| 2596 ||| 
2018 ||| multimodal dual attention memory for video story question answering. ||| 8643 ||| 8578 ||| 8644 ||| 8580 ||| 
2020 ||| free-form image inpainting via contrastive attention network. ||| 9442 ||| 20137 ||| 18593 ||| 6660 ||| 6935 ||| 2824 ||| 
2021 ||| an attention-based unsupervised adversarial model for movie review spam detection. ||| 5519 ||| 34992 ||| 29455 ||| 19537 ||| 
2020 ||| gatcluster: self-supervised gaussian-attention network for image clustering. ||| 8533 ||| 1796 ||| 8534 ||| 8535 ||| 
2021 ||| regional attention with architecture-rebuilt 3d network for rgb-d gesture recognition. ||| 18065 ||| 13441 ||| 13439 ||| 
2018 ||| attention to refine through multi-scales for semantic segmentation. ||| 27171 ||| 27172 ||| 
2018 ||| attention-based graph neural network for semi-supervised learning. ||| 35524 ||| 31736 ||| 35525 ||| 14441 ||| 
2020 ||| wessa at semeval-2020 task 9: code-mixed sentiment analysis using transformers. ||| 10539 ||| 10540 ||| 10541 ||| 10542 ||| 
2022 ||| o-vit: orthogonal vision transformer. ||| 35526 ||| 35527 ||| 35528 ||| 35529 ||| 
2020 ||| unsupervised extractive summarization by pre-training hierarchical transformers. ||| 25311 ||| 3479 ||| 2342 ||| 3174 ||| 3480 ||| 
2019 ||| integrating temporal and spatial attentions for vatex video captioning challenge 2019. ||| 19642 ||| 35530 ||| 6476 ||| 19643 ||| 6417 ||| 
2021 ||| progressive attention on multi-level dense difference maps for generic event boundary detection. ||| 2215 ||| 20336 ||| 18226 ||| 35531 ||| 2216 ||| 
2018 ||| region proposal networks with contextual selective attention for real-time organ detection. ||| 15673 ||| 15674 ||| 15675 ||| 
2022 ||| the devil is in the details: window-based attention for image compression. ||| 35532 ||| 6147 ||| 18260 ||| 
2019 ||| single headed attention rnn: stop thinking with your head. ||| 35533 ||| 
2021 ||| transformers with competitive ensembles of independent mechanisms. ||| 22719 ||| 18012 ||| 9194 ||| 33475 ||| 35534 ||| 12285 ||| 9196 ||| 
2019 ||| progress notes classification and keyword extraction using attention-based deep learning models with bert. ||| 35535 ||| 35536 ||| 35537 ||| 35538 ||| 35539 ||| 26740 ||| 
2019 ||| learning good representation via continuous attention. ||| 2932 ||| 7804 ||| 
2022 ||| learning wave propagation with attention-based convolutional recurrent autoencoder net. ||| 35540 ||| 35541 ||| 
2021 ||| armour: generalizable compact self-attention for vision transformers. ||| 35542 ||| 
2021 ||| an empirical study of training self-supervised vision transformers. ||| 1740 ||| 1741 ||| 1742 ||| 
2020 ||| data mining in clinical trial text: transformers for classification and question answering tasks. ||| 20572 ||| 20573 ||| 20574 ||| 
2021 ||| spoiler in a textstack: how much can transformers help? ||| 35543 ||| 35544 ||| 35545 ||| 35546 ||| 
2021 ||| adela: automatic dense labeling with attention for viewpoint adaptation in semantic segmentation. ||| 35547 ||| 35548 ||| 19285 ||| 35549 ||| 35550 ||| 35551 ||| 35552 ||| 35553 ||| 
2021 ||| p2t: pyramid pooling transformer for scene understanding. ||| 35554 ||| 4477 ||| 29988 ||| 1904 ||| 
2019 ||| topic spotting using hierarchical networks with self attention. ||| 4929 ||| 4930 ||| 4931 ||| 4932 ||| 4933 ||| 
2021 ||| transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. ||| 21229 ||| 21230 ||| 21232 ||| 35555 ||| 21233 ||| 
2018 ||| -nets: double attention networks. ||| 1723 ||| 9303 ||| 9304 ||| 1728 ||| 1685 ||| 
2021 ||| sega: semantic guided attention on visual prototype for few-shot learning. ||| 7229 ||| 1787 ||| 1788 ||| 
2021 ||| spatio-temporal self-attention network for video saliency prediction. ||| 35556 ||| 2740 ||| 28755 ||| 35557 ||| 35558 ||| 7088 ||| 
2020 ||| visual attention: deep rare features. ||| 35559 ||| 35560 ||| 11565 ||| 
2018 ||| mars: memory attention-aware recommender system. ||| 1429 ||| 9987 ||| 9988 ||| 9989 ||| 9992 ||| 9990 ||| 1094 ||| 
2021 ||| when vision transformers outperform resnets without pretraining or strong data augmentations. ||| 35561 ||| 21252 ||| 2193 ||| 
2019 ||| a deep patent landscaping model using transformer and graph convolutional network. ||| 
2018 ||| attention-based few-shot person re-identification using meta learning. ||| 11306 ||| 7421 ||| 
2020 ||| hybrid attention for automatic segmentation of whole fetal head in prenatal ultrasound volumes. ||| 7676 ||| 8012 ||| 9149 ||| 35562 ||| 35563 ||| 35564 ||| 16933 ||| 8637 ||| 20535 ||| 
2018 ||| wider channel attention network for remote sensing image super-resolution. ||| 
2021 ||| full-attention based neural architecture search using context auto-regression. ||| 31651 ||| 35565 ||| 35566 ||| 25304 ||| 
2021 ||| pynet-ca: enhanced pynet with channel attention for end-to-end mobile image signal processing. ||| 8712 ||| 8713 ||| 2048 ||| 8714 ||| 
2021 ||| multi-modal fusion using fine-tuned self-attention and transfer learning for veracity analysis of web information. ||| 35567 ||| 13827 ||| 
2021 ||| point-bert: pre-training 3d point cloud transformers with masked point modeling. ||| 2114 ||| 35568 ||| 2115 ||| 8556 ||| 1921 ||| 1920 ||| 
2020 ||| recursive non-autoregressive graph-to-graph transformer for dependency parsing with iterative refinement. ||| 26685 ||| 3672 ||| 
2020 ||| few-shot object detection with self-adaptive attention network for remote sensing images. ||| 35569 ||| 35570 ||| 35571 ||| 
2021 ||| predicting mood disorder symptoms with remotely collected videos using an interpretable multimodal dynamic attention fusion network. ||| 35572 ||| 35573 ||| 35574 ||| 35575 ||| 35576 ||| 35577 ||| 35578 ||| 
2022 ||| a comprehensive study of vision transformers on dense prediction tasks. ||| 16409 ||| 16410 ||| 16394 ||| 16411 ||| 16396 ||| 16397 ||| 
2020 ||| heterogeneous graph attention networks for early detection of rumors on twitter. ||| 376 ||| 377 ||| 378 ||| 379 ||| 
2020 ||| research on modeling units of transformer transducer for mandarin speech recognition. ||| 35579 ||| 8776 ||| 35580 ||| 
2022 ||| natural language to code using transformers. ||| 35581 ||| 35582 ||| 
2019 ||| emotion recognition with spatial attention and temporal softmax pooling. ||| 20861 ||| 2351 ||| 5749 ||| 5748 ||| 
2020 ||| facial uv map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual unets. ||| 25847 ||| 30046 ||| 30047 ||| 30048 ||| 
2021 ||| self-adversarial training incorporating forgery attention for image forgery localization. ||| 35583 ||| 35584 ||| 6502 ||| 35585 ||| 
2021 ||| multi-head or single-head? an empirical comparison for transformer training. ||| 4805 ||| 35586 ||| 1252 ||| 
2022 ||| spectral compressive imaging reconstruction using convolution and spectral contextual transformer. ||| 35587 ||| 35588 ||| 35589 ||| 5958 ||| 
2021 ||| beyond self-attention: external attention using two linear layers for visual tasks. ||| 24014 ||| 32107 ||| 32108 ||| 32110 ||| 
2019 ||| multi-attention networks for temporal localization of video-level labels. ||| 22154 ||| 35590 ||| 35591 ||| 34699 ||| 
2018 ||| neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering. ||| 3248 ||| 3157 ||| 3249 ||| 3250 ||| 3251 ||| 3252 ||| 
2019 ||| what i see is what you see: joint attention learning for first and third person video co-analysis. ||| 19692 ||| 7893 ||| 9994 ||| 13639 ||| 
2020 ||| classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. ||| 7368 ||| 7367 ||| 35592 ||| 35593 ||| 7370 ||| 
2022 ||| multi-view fusion transformer for sensor-based human activity recognition. ||| 35594 ||| 35595 ||| 247 ||| 12377 ||| 
2020 ||| multi-channel attention selection gans for guided image-to-image translation. ||| 435 ||| 436 ||| 127 ||| 7342 ||| 2160 ||| 437 ||| 
2021 ||| exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. ||| 35596 ||| 35597 ||| 35598 ||| 35599 ||| 
2021 ||| glance-and-gaze vision transformer. ||| 32260 ||| 27887 ||| 33750 ||| 21460 ||| 8660 ||| 8906 ||| 
2021 ||| 3d-anas v2: grafting transformer module on automatically designed convnet for hyperspectral image classification. ||| 35600 ||| 30528 ||| 948 ||| 949 ||| 
2021 ||| investigating the vision transformer model for image retrieval tasks. ||| 27319 ||| 27320 ||| 27321 ||| 
2017 ||| code attention: translating code to comments by exploiting domain features. ||| 30955 ||| 7913 ||| 765 ||| 1863 ||| 
2021 ||| tracking by joint local and global search: a target-aware attention based approach. ||| 5957 ||| 5755 ||| 382 ||| 6416 ||| 28819 ||| 8711 ||| 
2022 ||| heat: hyperedge attention networks. ||| 35601 ||| 35602 ||| 34845 ||| 
2020 ||| symmetric parallax attention for stereo image super-resolution. ||| 19139 ||| 19307 ||| 19138 ||| 19308 ||| 19143 ||| 7271 ||| 
2017 ||| a gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. ||| 4489 ||| 1010 ||| 12372 ||| 
2020 ||| graph transformer networks with syntactic and semantic structures for event argument extraction. ||| 1404 ||| 26826 ||| 11744 ||| 
2022 ||| image search with text feedback by additive attention compositional learning. ||| 35603 ||| 35604 ||| 32603 ||| 
2018 ||| learning attentional communication for multi-agent cooperation. ||| 9205 ||| 9206 ||| 
2020 ||| multiple sclerosis lesion activity segmentation with attention-guided two-path cnns. ||| 29730 ||| 30913 ||| 6787 ||| 30914 ||| 30915 ||| 30916 ||| 30917 ||| 30918 ||| 14906 ||| 
2021 ||| mm-pyramid: multimodal pyramid attentional network for audio-visual event localization and video parsing. ||| 35605 ||| 19704 ||| 35606 ||| 580 ||| 9689 ||| 
2021 ||| cctrans: simplifying and improving crowd counting with transformer. ||| 11478 ||| 6433 ||| 21636 ||| 
2022 ||| fast online video super-resolution with deformable attention pyramid. ||| 35607 ||| 25546 ||| 7815 ||| 7814 ||| 
2021 ||| attention-based aspect reasoning for knowledge base question answering on clinical notes. ||| 2885 ||| 35608 ||| 35609 ||| 35610 ||| 8954 ||| 
2021 ||| covid-19 tweets analysis through transformer language models. ||| 35611 ||| 35612 ||| 
2018 ||| image captioning based on a hierarchical attention mechanism and policy gradient optimization. ||| 13 ||| 31792 ||| 11226 ||| 11227 ||| 11228 ||| 
2021 ||| multi-stream transformers. ||| 32585 ||| 3458 ||| 
2021 ||| question-aware transformer models for consumer health question summarization. ||| 3062 ||| 4018 ||| 35613 ||| 11647 ||| 
2019 ||| dada-2000: can driving accident be predicted by driver attention? analyzed by a benchmark. ||| 34453 ||| 34454 ||| 34455 ||| 34456 ||| 19285 ||| 11504 ||| 
2022 ||| remaining useful life prediction using temporal deep degradation network for complex machinery with attention-based feature extraction. ||| 35614 ||| 35615 ||| 17109 ||| 33847 ||| 35616 ||| 19066 ||| 
2021 ||| a transformer-based model to detect phishing urls. ||| 35617 ||| 
2021 ||| scaling vision transformers. ||| 23910 ||| 7979 ||| 23913 ||| 23909 ||| 
2020 ||| an automatic covid-19 ct segmentation network using spatial and channel attention mechanism. ||| 15607 ||| 2693 ||| 15610 ||| 15608 ||| 
2020 ||| a knowledge-enhanced recommendation model with attribute-level co-attention. ||| 9632 ||| 9633 ||| 9634 ||| 9635 ||| 
2022 ||| staged training for transformer language models. ||| 3822 ||| 35618 ||| 2596 ||| 35619 ||| 33857 ||| 3124 ||| 
2019 ||| generating diverse translation by manipulating multi-head attention. ||| 17973 ||| 4845 ||| 17792 ||| 18284 ||| 4847 ||| 
2021 ||| resvit: residual vision transformers for multi-modal medical image synthesis. ||| 35620 ||| 27661 ||| 27665 ||| 27666 ||| 
2020 ||| superpixel image classification with graph attention networks. ||| 8392 ||| 8393 ||| 8394 ||| 8382 ||| 8395 ||| 8396 ||| 8397 ||| 
2021 ||| a dual-questioning attention network for emotion-cause pair extraction with context awareness. ||| 302 ||| 303 ||| 304 ||| 
2017 ||| japanese sentiment classification using a tree-structured long short-term memory with attention. ||| 16024 ||| 14211 ||| 
2021 ||| transfuse: fusing transformers and cnns for medical image segmentation. ||| 7430 ||| 27857 ||| 27858 ||| 
2018 ||| attention-based hierarchical neural query suggestion. ||| 1045 ||| 1046 ||| 1047 ||| 1048 ||| 
2022 ||| dual-tasks siamese transformer framework for building damage assessment. ||| 35621 ||| 35622 ||| 35623 ||| 2259 ||| 3668 ||| 35624 ||| 
2019 ||| learning coupled spatial-temporal attention for skeleton-based action recognition. ||| 35625 ||| 
2021 ||| attentive contractive flow: improved contractive flows with lipschitz-constrained self-attention. ||| 35626 ||| 35627 ||| 35628 ||| 35629 ||| 7152 ||| 
2021 ||| session-based recommendation with self-attention networks. ||| 35630 ||| 
2019 ||| multi-frame content integration with a spatio-temporal attention mechanism for person video motion transfer. ||| 35631 ||| 32861 ||| 1280 ||| 35632 ||| 683 ||| 
2019 ||| theme-matters: fashion compatibility learning via theme attention. ||| 35633 ||| 11346 ||| 398 ||| 13816 ||| 2165 ||| 18820 ||| 
2020 ||| students need more attention: bert-based attentionmodel for small data with application to automaticpatient message triage. ||| 1030 ||| 3049 ||| 20695 ||| 3386 ||| 20696 ||| 1029 ||| 4859 ||| 1032 ||| 
2021 ||| tune-in: training under negative environments with interference for attention networks simulating cocktail party effect. ||| 1224 ||| 18078 ||| 4530 ||| 3808 ||| 
2018 ||| da-gan: instance-level image translation by deep attention generative adversarial networks (with supplementary materials). ||| 18947 ||| 1699 ||| 13811 ||| 2165 ||| 
2019 ||| agglomerative attention. ||| 35085 ||| 
2020 ||| attention module is not only a weight: analyzing transformers with vector norms. ||| 26858 ||| 26859 ||| 26860 ||| 26674 ||| 
2020 ||| blind deinterleaving of signals in time series with self-attention based soft min-cost flow learning. ||| 12227 ||| 12228 ||| 12229 ||| 7442 ||| 12230 ||| 12231 ||| 
2021 ||| vision transformers are robust learners. ||| 35634 ||| 12671 ||| 
2021 ||| neural rule-execution tracking machine for transformer-based text generation. ||| 2852 ||| 35635 ||| 35636 ||| 23537 ||| 3257 ||| 3258 ||| 3259 ||| 3708 ||| 
2018 ||| adversarial tableqa: attention supervision for question answering on tables. ||| 22670 ||| 3713 ||| 3716 ||| 22671 ||| 
2020 ||| lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. ||| 18705 ||| 2445 ||| 18706 ||| 18707 ||| 18708 ||| 
2021 ||| abnormal occupancy grid map recognition using attention network. ||| 25526 ||| 25527 ||| 25528 ||| 11225 ||| 35637 ||| 25530 ||| 5519 ||| 25531 ||| 21847 ||| 
2020 ||| rose: real one-stage effort to detect the fingerprint singular point based on multi-scale spatial attention. ||| 35638 ||| 35639 ||| 35640 ||| 35641 ||| 33169 ||| 
2022 ||| gtrans: spatiotemporal autoregressive transformer with graph embeddings for nowcasting extreme events. ||| 19977 ||| 35642 ||| 
2019 ||| collaborative attention network for person re-identification. ||| 7235 ||| 7236 ||| 2357 ||| 7234 ||| 35643 ||| 35644 ||| 
2021 ||| hierarchical transformer for multilingual machine translation. ||| 24084 ||| 24085 ||| 24086 ||| 24087 ||| 24088 ||| 24089 ||| 
2021 ||| phraseformer: multimodal key-phrase extraction using transformer and graph embedding. ||| 35230 ||| 35232 ||| 35645 ||| 35231 ||| 35646 ||| 35647 ||| 35648 ||| 35649 ||| 35650 ||| 35651 ||| 
2021 ||| 3d reconstruction with transformer. ||| 2182 ||| 2183 ||| 2184 ||| 2185 ||| 2186 ||| 2187 ||| 2188 ||| 2189 ||| 
2022 ||| anti-oversmoothing in deep vision transformers via the fourier domain analysis: from theory to practice. ||| 32729 ||| 32727 ||| 19181 ||| 7195 ||| 
2022 ||| pre-trained language transformers are universal image classifiers. ||| 35652 ||| 35653 ||| 35654 ||| 35655 ||| 35656 ||| 35657 ||| 35658 ||| 
2021 ||| towards understanding the effectiveness of attention mechanism. ||| 35659 ||| 35660 ||| 22747 ||| 2969 ||| 
2021 ||| a transformer-based model for default prediction in mid-cap corporate markets. ||| 35661 ||| 35662 ||| 35663 ||| 35664 ||| 
2021 ||| mounting video metadata on transformer-based language model for open-ended video question answering. ||| 26562 ||| 35665 ||| 35666 ||| 8580 ||| 
2020 ||| agatha: automatic graph-mining and transformer based hypothesis generation approach. ||| 1360 ||| 1361 ||| 1362 ||| 1363 ||| 
2018 ||| deep attention-based classification network for robust depth prediction. ||| 6352 ||| 6353 ||| 6335 ||| 6354 ||| 6355 ||| 6356 ||| 
2021 ||| multi-person 3d motion prediction with multi-range transformers. ||| 35667 ||| 32599 ||| 35668 ||| 1117 ||| 
2020 ||| manet: multimodal attention network based point- view fusion for 3d shape recognition. ||| 20076 ||| 20077 ||| 35669 ||| 
2020 ||| divergent modes of online collective attention to the covid-19 pandemic are associated with future caseload variance. ||| 33419 ||| 33415 ||| 33417 ||| 33416 ||| 33422 ||| 33423 ||| 
2022 ||| temporal relation extraction with a graph-based deep biaffine attention model. ||| 35670 ||| 35671 ||| 35672 ||| 35673 ||| 
2021 ||| advanced long-context end-to-end speech recognition using context-expanded transformers. ||| 2508 ||| 11980 ||| 2507 ||| 11981 ||| 
2020 ||| bidirectional encoder representations from transformers (bert): a sentiment analysis odyssey. ||| 35674 ||| 35675 ||| 
2022 ||| unleashing the power of transformer for graphs. ||| 35676 ||| 817 ||| 17990 ||| 
2020 ||| attention-driven dynamic graph convolutional network for multi-label image recognition. ||| 8782 ||| 8783 ||| 8769 ||| 8784 ||| 2149 ||| 
2021 ||| assessing the syntactic capabilities of transformer-based multilingual language models. ||| 3519 ||| 3520 ||| 3521 ||| 3522 ||| 3419 ||| 3523 ||| 3524 ||| 
2018 ||| generative image inpainting with contextual attention. ||| 12814 ||| 18766 ||| 18767 ||| 7142 ||| 18768 ||| 17653 ||| 
2019 ||| delta: deep learning transfer using feature map with attention for convolutional networks. ||| 23983 ||| 23984 ||| 23985 ||| 23986 ||| 23987 ||| 23988 ||| 
2021 ||| knowledge graph embedding using graph convolutional networks with relation-aware attention. ||| 18008 ||| 5289 ||| 18009 ||| 35677 ||| 35678 ||| 35679 ||| 
2019 ||| encoding musical style with transformer autoencoders. ||| 22838 ||| 11910 ||| 11911 ||| 22839 ||| 11914 ||| 
2020 ||| tdaf: top-down attention framework for vision tasks. ||| 4900 ||| 17957 ||| 17958 ||| 17959 ||| 17960 ||| 5136 ||| 
2019 ||| inverse attention guided deep crowd counting network. ||| 19135 ||| 18609 ||| 
2018 ||| detection of paroxysmal atrial fibrillation using attention-based bidirectional recurrent neural networks. ||| 25366 ||| 25367 ||| 25368 ||| 25369 ||| 
2020 ||| pre-trained and attention-based neural networks for building noetic task-oriented dialogue systems. ||| 35680 ||| 35681 ||| 14059 ||| 26004 ||| 4894 ||| 35682 ||| 
2019 ||| style transformer: unpaired text style transfer without disentangled latent representation. ||| 3270 ||| 3271 ||| 3272 ||| 3273 ||| 
2021 ||| avatr: one-shot speaker extraction with transformers. ||| 14372 ||| 14373 ||| 14374 ||| 4051 ||| 14375 ||| 14376 ||| 
2019 ||| forecasting human object interaction: joint prediction of motor attention and egocentric activity. ||| 8764 ||| 8765 ||| 8766 ||| 8692 ||| 
2021 ||| rams-trans: recurrent attention multi-scale transformer forfine-grained image recognition. ||| 19673 ||| 19674 ||| 16591 ||| 19675 ||| 19676 ||| 3001 ||| 12377 ||| 
2017 ||| query-by-example spoken term detection using attention-based multi-hop networks. ||| 12740 ||| 12644 ||| 
2019 ||| coinnet: deep ancient roman republican coin classification via feature fusion and attention. ||| 35683 ||| 2474 ||| 35684 ||| 7408 ||| 
2020 ||| neural abstractive summarization with structural attention. ||| 23493 ||| 18578 ||| 3835 ||| 
2020 ||| lexically constrained neural machine translation with levenshtein transformer. ||| 3167 ||| 3168 ||| 3169 ||| 
2020 ||| i-bert: inductive generalization of transformer to arbitrary context lengths. ||| 35685 ||| 35686 ||| 35687 ||| 35688 ||| 35689 ||| 
2019 ||| saliency learning: teaching the model where to pay attention. ||| 4913 ||| 4914 ||| 4915 ||| 4916 ||| 
2019 ||| sequence-to-sequence singing synthesis using the feed-forward transformer. ||| 12551 ||| 12552 ||| 
2019 ||| cascade attention guided residue learning gan for cross-modal translation. ||| 7130 ||| 1160 ||| 435 ||| 20228 ||| 127 ||| 
2020 ||| transformer based unsupervised pre-training for acoustic representation learning. ||| 12259 ||| 12427 ||| 12258 ||| 12257 ||| 12263 ||| 11688 ||| 
2020 ||| spatiotemporal attention for multivariate time series prediction and interpretation. ||| 12631 ||| 12632 ||| 12633 ||| 12634 ||| 12635 ||| 
2021 ||| towards a question answering assistant for software development using a transformer-based language model. ||| 10298 ||| 10299 ||| 
2021 ||| attention-guided generative adversarial network for whisper to normal speech conversion. ||| 35690 ||| 25471 ||| 25695 ||| 25697 ||| 35691 ||| 
2019 ||| transformer asr with contextual block processing. ||| 12365 ||| 12364 ||| 13953 ||| 3549 ||| 
2020 ||| funnel-transformer: filtering out sequential redundancy for efficient language processing. ||| 3780 ||| 9369 ||| 2622 ||| 9372 ||| 
2021 ||| paanet: progressive alternating attention for automatic medical image segmentation. ||| 32956 ||| 32957 ||| 20165 ||| 20169 ||| 16413 ||| 20170 ||| 20168 ||| 30717 ||| 
2020 ||| deep attention spatio-temporal point processes. ||| 35692 ||| 35693 ||| 35694 ||| 35695 ||| 
2021 ||| shuffle transformer: rethinking spatial shuffle for vision transformer. ||| 2218 ||| 22648 ||| 35696 ||| 35697 ||| 18222 ||| 35698 ||| 
2019 ||| theoretical limitations of self-attention in neural sequence models. ||| 35699 ||| 
2020 ||| parameter efficient multimodal transformers for video representation learning. ||| 24021 ||| 18984 ||| 18989 ||| 24022 ||| 24023 ||| 20350 ||| 
2018 ||| deep snp: an end-to-end deep neural network with attention-based localization for break-point detection in snp array genomic data. ||| 35700 ||| 35701 ||| 35702 ||| 35703 ||| 35704 ||| 35705 ||| 35706 ||| 35707 ||| 35708 ||| 35709 ||| 11898 ||| 35710 ||| 
2021 ||| graphit: encoding graph structure in transformers. ||| 17321 ||| 23966 ||| 23967 ||| 35711 ||| 2264 ||| 
2019 ||| adaptation of deep bidirectional multilingual transformers for russian language. ||| 10336 ||| 10334 ||| 
2022 ||| multi-view and multi-modal event detection utilizing transformer-based multi-sensor fusion. ||| 12574 ||| 12268 ||| 12576 ||| 12578 ||| 
2021 ||| context matters: self-attention for sign language recognition. ||| 20308 ||| 20309 ||| 
2021 ||| looking twice for partial clues: weakly-supervised part-mentored attention network for vehicle re-identification. ||| 35712 ||| 9149 ||| 35713 ||| 
2022 ||| mfa: tdnn with multi-scale frequency-channel attention for text-independent speaker verification with short utterances. ||| 14464 ||| 13581 ||| 14562 ||| 12494 ||| 
2021 ||| lea-net: layer-wise external attention network for efficient color anomaly detection. ||| 35714 ||| 35715 ||| 
2020 ||| axial-deeplab: stand-alone axial-attention for panoptic segmentation. ||| 8656 ||| 8657 ||| 8658 ||| 8659 ||| 8660 ||| 8661 ||| 
2021 ||| predicting pedestrian crossing intention with feature fusion and spatio-temporal attention. ||| 35716 ||| 35717 ||| 35718 ||| 35719 ||| 35720 ||| 58 ||| 12149 ||| 
2021 ||| are pretrained transformers robust in intent classification? a missing ingredient in evaluation of out-of-scope intent detection. ||| 14896 ||| 4888 ||| 3891 ||| 23311 ||| 3287 ||| 1094 ||| 
2020 ||| turkeyes: a web-based toolbox for crowdsourcing attention data. ||| 15334 ||| 15335 ||| 15336 ||| 15337 ||| 15338 ||| 15339 ||| 15340 ||| 8781 ||| 
2019 ||| e2-capsule neural networks for facial expression recognition using au-aware attention. ||| 6567 ||| 35721 ||| 17561 ||| 
2021 ||| detection of deepfake videos using long distance attention. ||| 3131 ||| 35722 ||| 35723 ||| 25452 ||| 35724 ||| 35585 ||| 
2021 ||| domain attention consistency for multi-source domain adaptation. ||| 35725 ||| 35726 ||| 35727 ||| 2198 ||| 
2019 ||| recosa: detecting the relevant contexts with self-attention for multi-turn dialogue generation. ||| 3737 ||| 1444 ||| 1443 ||| 3738 ||| 1445 ||| 
2021 ||| conam: confidence attention module for convolutional neural networks. ||| 35728 ||| 35729 ||| 35730 ||| 
2021 ||| fine-grained style control in transformer-based text-to-speech synthesis. ||| 35731 ||| 35732 ||| 
2019 ||| language models with transformers. ||| 35733 ||| 3046 ||| 18298 ||| 
2019 ||| insertion transformer: flexible sequence generation via insertion operations. ||| 22755 ||| 14276 ||| 22756 ||| 4960 ||| 
2020 ||| stereo endoscopic image super-resolution using disparity-constrained parallel attention. ||| 32297 ||| 27376 ||| 11266 ||| 20234 ||| 1132 ||| 
2021 ||| probabilistic spatial distribution prior based attentional keypoints matching network. ||| 28840 ||| 28841 ||| 11594 ||| 11596 ||| 28842 ||| 28843 ||| 
2017 ||| path-based attention neural model for fine-grained entity typing. ||| 9589 ||| 17638 ||| 17639 ||| 17637 ||| 3228 ||| 
2021 ||| scale efficiently: insights from pre-training and fine-tuning transformers. ||| 1398 ||| 2293 ||| 4840 ||| 32606 ||| 3189 ||| 26712 ||| 13146 ||| 23970 ||| 2466 ||| 3294 ||| 
2021 ||| restormer: efficient transformer for high-resolution image restoration. ||| 35734 ||| 35735 ||| 1969 ||| 32792 ||| 1972 ||| 7143 ||| 
2019 ||| coarse-grain fine-grain coattention network for multi-evidence question answering. ||| 23928 ||| 3287 ||| 23929 ||| 19267 ||| 
2022 ||| local-global context aware transformer for language-guided video segmentation. ||| 17950 ||| 2444 ||| 33661 ||| 35736 ||| 18978 ||| 208 ||| 
2019 ||| lsanet: feature learning on point sets by local spatial attention. ||| 18871 ||| 35737 ||| 1861 ||| 1904 ||| 333 ||| 1977 ||| 
2021 ||| transformers are deep infinite-dimensional non-mercer binary kernel machines. ||| 23619 ||| 34327 ||| 
2021 ||| towards end-to-end image compression and analysis with transformers. ||| 18251 ||| 5157 ||| 12058 ||| 12056 ||| 6416 ||| 32540 ||| 7204 ||| 
2020 ||| upgraded w-net with attention gates and its application in unsupervised 3d liver segmentation. ||| 27020 ||| 27021 ||| 27022 ||| 20729 ||| 20730 ||| 
2020 ||| regularizing attention networks for anomaly detection in visual question answering. ||| 18117 ||| 18118 ||| 18119 ||| 
2022 ||| aa-transunet: attention augmented transunet for nowcasting tasks. ||| 35738 ||| 27268 ||| 
2019 ||| question-agnostic attention for visual question answering. ||| 20206 ||| 1969 ||| 2475 ||| 
2018 ||| scan: self-and-collaborative attention network for video person re-identification. ||| 27729 ||| 22662 ||| 14653 ||| 35739 ||| 2315 ||| 2011 ||| 1846 ||| 
2019 ||| processing megapixel images with deep attention-sampling models. ||| 9348 ||| 1226 ||| 9349 ||| 
2021 ||| can transformers jump around right in natural language? assessing performance transfer from scan. ||| 35740 ||| 15025 ||| 35741 ||| 
2021 ||| neural transfer learning with transformers for social science text analysis. ||| 35742 ||| 3831 ||| 
2021 ||| stabilizing deep q-learning with convnets and vision transformers under data augmentation. ||| 33986 ||| 35743 ||| 1117 ||| 
2019 ||| a self-attention joint model for spoken language understanding in situational dialog applications. ||| 35744 ||| 8721 ||| 35745 ||| 
2021 ||| pttr: relational 3d point cloud object tracking with transformer. ||| 35746 ||| 35747 ||| 35748 ||| 35749 ||| 35750 ||| 35751 ||| 7267 ||| 7406 ||| 
2021 ||| loftr: detector-free local feature matching with transformers. ||| 841 ||| 19079 ||| 19080 ||| 19081 ||| 19082 ||| 
2019 ||| image captioning with integrated bottom-up and multi-level residual top-down attention for game scene understanding. ||| 35752 ||| 35753 ||| 35754 ||| 12125 ||| 35755 ||| 20382 ||| 
2021 ||| lctr: on awakening the local continuity of transformer for weakly supervised object localization. ||| 29177 ||| 35756 ||| 18961 ||| 35757 ||| 35758 ||| 19077 ||| 35759 ||| 781 ||| 17660 ||| 
2020 ||| devising malware characterstics using transformers. ||| 35760 ||| 35761 ||| 35762 ||| 35763 ||| 
2020 ||| query twice: dual mixture attention meta learning for video summarization. ||| 19591 ||| 1281 ||| 18907 ||| 19592 ||| 6660 ||| 19593 ||| 6935 ||| 
2018 ||| who is killed by police: introducing supervised attention for hierarchical lstms. ||| 9940 ||| 11744 ||| 
2022 ||| improving across-dataset brain tissue segmentation using transformer. ||| 35764 ||| 35765 ||| 35766 ||| 35767 ||| 11478 ||| 35768 ||| 5932 ||| 
2020 ||| improving attention mechanism with query-value interaction. ||| 3754 ||| 3755 ||| 3756 ||| 2795 ||| 
2021 ||| pyramid medical transformer for medical image segmentation. ||| 35769 ||| 35770 ||| 35771 ||| 
2021 ||| multi-attentional deepfake detection. ||| 19368 ||| 11556 ||| 2494 ||| 19369 ||| 18972 ||| 2305 ||| 
2018 ||| attention-based end-to-end models for small-footprint keyword spotting. ||| 12394 ||| 12757 ||| 4551 ||| 12384 ||| 
2021 ||| operationalizing a national digital library: the case for a norwegian transformer model. ||| 10824 ||| 10825 ||| 10826 ||| 10827 ||| 
2021 ||| leveraging redundancy in attention with reuse transformers. ||| 2567 ||| 2568 ||| 2572 ||| 26586 ||| 34583 ||| 35772 ||| 9156 ||| 9159 ||| 
2020 ||| self-attention networks for intent detection. ||| 13988 ||| 2101 ||| 13989 ||| 13990 ||| 13310 ||| 13991 ||| 13992 ||| 
2020 ||| masked language modeling for proteins via linearly scalable long-context transformers. ||| 22791 ||| 24034 ||| 23941 ||| 24035 ||| 35773 ||| 11942 ||| 24037 ||| 7111 ||| 35774 ||| 24041 ||| 24042 ||| 
2021 ||| explaining a neural attention model for aspect-based sentiment classification using diagnostic classification. ||| 18370 ||| 14072 ||| 18371 ||| 
2021 ||| starformer: transformer with state-action-reward representations. ||| 35775 ||| 8594 ||| 
2022 ||| short-term passenger flow prediction for multi-traffic modes: a residual network and transformer based multi-task learning method. ||| 35776 ||| 33233 ||| 33234 ||| 33235 ||| 
2019 ||| transformer based reinforcement learning for games. ||| 993 ||| 35777 ||| 35778 ||| 35779 ||| 
2018 ||| image-level attentional context modeling using nested-graph neural networks. ||| 35780 ||| 32192 ||| 35781 ||| 32193 ||| 35782 ||| 
2020 ||| dilated convolutional attention network for medical code assignment from clinical text. ||| 27207 ||| 893 ||| 9366 ||| 
2021 ||| groupbert: enhanced transformer architecture with efficient grouped structures. ||| 35783 ||| 35784 ||| 35785 ||| 35786 ||| 35787 ||| 35788 ||| 35789 ||| 
2021 ||| dpt: deformable patch-based transformer for visual recognition. ||| 19480 ||| 19481 ||| 11290 ||| 18063 ||| 19482 ||| 2077 ||| 2074 ||| 
2022 ||| algorithm selection for software verification using graph attention networks. ||| 23680 ||| 23681 ||| 
2022 ||| automatic audio captioning using attention weighted event based embeddings. ||| 12161 ||| 12162 ||| 12163 ||| 
2021 ||| gigapixel histopathological image analysis using attention-based neural networks. ||| 35790 ||| 35791 ||| 35792 ||| 35793 ||| 
2018 ||| craft: complementary recommendations using adversarial feature transformer. ||| 35794 ||| 35795 ||| 8618 ||| 8617 ||| 
2021 ||| attention-based neural networks for chroma intra prediction in video coding. ||| 11480 ||| 14913 ||| 11482 ||| 11483 ||| 1675 ||| 11484 ||| 
2020 ||| deep exposure fusion with deghosting via homography estimation and attention learning. ||| 12064 ||| 2345 ||| 
2020 ||| origin-aware next destination recommendation with personalized preference attention. ||| 1342 ||| 1343 ||| 1344 ||| 1345 ||| 1346 ||| 1347 ||| 22906 ||| 
2022 ||| overlaptransformer: an efficient and rotation-invariant transformer network for lidar-based place recognition. ||| 35796 ||| 1796 ||| 35797 ||| 35798 ||| 35799 ||| 35800 ||| 35801 ||| 
2021 ||| transformer-based network for rgb-d saliency detection. ||| 7400 ||| 19030 ||| 2037 ||| 35802 ||| 35803 ||| 1700 ||| 
2021 ||| grapheme-to-phoneme transformer model for transfer learning dialects. ||| 35804 ||| 35805 ||| 35806 ||| 
2021 ||| self-attentive ensemble transformer: representing ensemble interactions in neural networks for earth system models. ||| 35807 ||| 
2021 ||| deep attention-based representation learning for heart sound classification. ||| 12762 ||| 13520 ||| 35808 ||| 35809 ||| 35810 ||| 648 ||| 649 ||| 
2021 ||| sctn: sparse convolution-transformer network for scene flow estimation. ||| 8838 ||| 9681 ||| 35811 ||| 35812 ||| 
2022 ||| deepnet: scaling transformers to 1, 000 layers. ||| 31091 ||| 10200 ||| 3171 ||| 3499 ||| 5352 ||| 3174 ||| 
2020 ||| mara-net: single image deraining network with multi-level connection and adaptive regional attention. ||| 35813 ||| 35814 ||| 35815 ||| 8045 ||| 
2017 ||| integrating three mechanisms of visual attention for active visual search. ||| 29814 ||| 29817 ||| 
2020 ||| an improved attention for visual question answering. ||| 18747 ||| 18748 ||| 2301 ||| 4795 ||| 
2020 ||| object 6d pose estimation with non-local attention. ||| 35816 ||| 2266 ||| 2268 ||| 
2020 ||| extracting angina symptoms from clinical notes using pre-trained transformer architectures. ||| 1651 ||| 1652 ||| 1653 ||| 1654 ||| 1655 ||| 1656 ||| 1657 ||| 
2021 ||| rrnet: relational reasoning network with parallel multi-scale attention for salient object detection in optical remote sensing images. ||| 35817 ||| 35818 ||| 6749 ||| 5536 ||| 35819 ||| 4400 ||| 19728 ||| 
2019 ||| a self-attentional neural architecture for code completion with multi-task learning. ||| 
2020 ||| attention meets perturbations: robust and interpretable attention with adversarial training. ||| 25316 ||| 25317 ||| 
2018 ||| what we read, what we search: media attention and public attention among 193 countries. ||| 9044 ||| 9045 ||| 9046 ||| 9047 ||| 9048 ||| 
2019 ||| neural simile recognition with cyclic multitask learning and local attention. ||| 3043 ||| 3655 ||| 3182 ||| 11745 ||| 17827 ||| 2166 ||| 
2021 ||| drug-target interaction prediction with graph attention networks. ||| 35565 ||| 35820 ||| 17302 ||| 9682 ||| 1160 ||| 
2019 ||| graph attention memory for visual navigation. ||| 6552 ||| 29026 ||| 1014 ||| 35821 ||| 379 ||| 
2021 ||| mt-transunet: mediating multi-task tokens in transformers for skin lesion segmentation and classification. ||| 35822 ||| 32259 ||| 35823 ||| 6502 ||| 8660 ||| 21460 ||| 
2022 ||| spatio-temporal outdoor lighting aggregation on image sequences using transformer networks. ||| 35824 ||| 35825 ||| 35826 ||| 35827 ||| 35828 ||| 
2019 ||| learning high-order structural and attribute information by knowledge graph attention networks for enhancing knowledge graph embedding. ||| 35829 ||| 895 ||| 6560 ||| 35830 ||| 35831 ||| 35832 ||| 
2017 ||| an attention-based collaboration framework for multi-view network representation learning. ||| 1247 ||| 1248 ||| 1249 ||| 1250 ||| 1251 ||| 1252 ||| 
2021 ||| are the multilingual models better? improving czech sentiment with transformers. ||| 13977 ||| 3882 ||| 13978 ||| 
2019 ||| improving n-gram language models with pre-trained deep transformer. ||| 35833 ||| 12490 ||| 6474 ||| 35834 ||| 11973 ||| 17103 ||| 12727 ||| 
2021 ||| wifimod: transformer-based indoor human mobility modeling using passive sensing. ||| 9505 ||| 9506 ||| 9507 ||| 9508 ||| 
2021 ||| da-fdftnet: dual attention fake detection fine-tuning network to detect various ai-generated fake images. ||| 35835 ||| 35836 ||| 
2018 ||| reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 1300 ||| 4873 ||| 
2020 ||| jdi-t: jointly trained duration informed transformer for text-to-speech without explicit alignment. ||| 14681 ||| 14682 ||| 14683 ||| 35837 ||| 14685 ||| 14686 ||| 
2020 ||| dual-path transformer network: direct context-aware modeling for end-to-end monaural speech separation. ||| 14648 ||| 14649 ||| 8709 ||| 
2020 ||| dissecting lottery ticket transformers: structural and behavioral study of sparse neural machine translation. ||| 20972 ||| 20973 ||| 
2020 ||| adaptive bi-directional attention: exploring multi-granularity representations for machine reading comprehension. ||| 12623 ||| 3746 ||| 7412 ||| 12624 ||| 4430 ||| 
2019 ||| spectral graph transformer networks for brain surface parcellation. ||| 2824 ||| 15511 ||| 15512 ||| 15513 ||| 
2022 ||| problem-dependent attention and effort in neural networks with an application to image resolution. ||| 35838 ||| 
2019 ||| training optimus prime, m.d.: generating medical certification items by fine-tuning openai's gpt2 transformer model. ||| 35839 ||| 
2021 ||| sparse fusion for multimodal transformers. ||| 24193 ||| 35840 ||| 35841 ||| 13600 ||| 35842 ||| 13602 ||| 13603 ||| 13604 ||| 
2021 ||| densepass: dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange. ||| 23613 ||| 7856 ||| 7857 ||| 23614 ||| 7861 ||| 
2021 ||| lazyformer: self attention with lazy update. ||| 35843 ||| 33475 ||| 18012 ||| 4791 ||| 
2022 ||| octattention: octree-based large-scale contexts model for point cloud compression. ||| 35844 ||| 2064 ||| 6700 ||| 1310 ||| 2063 ||| 
2018 ||| differential attention for visual question answering. ||| 7149 ||| 7152 ||| 
2019 ||| improving generalization of transformer for speech recognition with parallel schedule sampling and relative positional embedding. ||| 12300 ||| 12771 ||| 5110 ||| 4459 ||| 
2019 ||| progressive sparse local attention for video object detection. ||| 2248 ||| 2249 ||| 2250 ||| 2251 ||| 2252 ||| 2253 ||| 2254 ||| 2255 ||| 
2021 ||| full-reference speech quality estimation with attentional siamese neural networks. ||| 12197 ||| 12198 ||| 3831 ||| 
2017 ||| supervising neural attention models for video captioning by human gaze data. ||| 18984 ||| 18985 ||| 18986 ||| 18987 ||| 18988 ||| 18989 ||| 
2020 ||| explicitly modeled attention maps for image classification. ||| 17761 ||| 17762 ||| 17763 ||| 13617 ||| 12149 ||| 9163 ||| 
2020 ||| multi-view self-attention for interpretable drug-target interaction prediction. ||| 31182 ||| 31183 ||| 31184 ||| 31185 ||| 31186 ||| 1207 ||| 
2021 ||| temporal-wise attention spiking neural networks for event streams classification. ||| 2539 ||| 2540 ||| 2541 ||| 2542 ||| 2543 ||| 2544 ||| 2545 ||| 
2019 ||| do attention heads in bert track syntactic dependencies? ||| 35845 ||| 32517 ||| 35846 ||| 3720 ||| 
2020 ||| graph-aware transformer: is attention all graphs need? ||| 35847 ||| 35848 ||| 35849 ||| 35850 ||| 35851 ||| 12256 ||| 12054 ||| 
2018 ||| scaling notifications beyond alerts: from subtly drawing attention up to forcing the user to take action. ||| 17503 ||| 17504 ||| 17505 ||| 
2018 ||| modeling attention flow on graphs. ||| 35852 ||| 35853 ||| 624 ||| 1503 ||| 12081 ||| 
2021 ||| asl to pet translation by a semi-supervised residual-based attention-guided convolutional neural network. ||| 35854 ||| 35855 ||| 35856 ||| 35857 ||| 35858 ||| 35859 ||| 35860 ||| 
2019 ||| spatial-temporal self-attention network for flow prediction. ||| 25372 ||| 1438 ||| 5209 ||| 25374 ||| 
2021 ||| self-supervised learning with swin transformers. ||| 33058 ||| 1766 ||| 8017 ||| 1770 ||| 6545 ||| 1767 ||| 1768 ||| 
2020 ||| se(3)-transformers: 3d roto-translation equivariant attention networks. ||| 25870 ||| 9253 ||| 9254 ||| 9255 ||| 
2021 ||| adaptive sparse transformer for multilingual translation. ||| 17911 ||| 9390 ||| 35861 ||| 
2020 ||| structured attention for unsupervised dialogue structure induction. ||| 26578 ||| 26579 ||| 26580 ||| 26581 ||| 26582 ||| 26583 ||| 1753 ||| 18982 ||| 
2020 ||| temporal-channel transformer for 3d lidar-based video object detection in autonomous driving. ||| 35862 ||| 28729 ||| 2388 ||| 1806 ||| 7436 ||| 2303 ||| 
2019 ||| how far are we from quantifying visual attention in mobile hci? ||| 15349 ||| 15350 ||| 15351 ||| 8348 ||| 
2021 ||| cyclegan-based non-parallel speech enhancement with an adaptive attention-in-attention mechanism. ||| 4382 ||| 4383 ||| 4384 ||| 1341 ||| 4385 ||| 
2018 ||| recurrent neural network attention mechanisms for interpretable system log anomaly detection. ||| 35863 ||| 35864 ||| 35865 ||| 35866 ||| 
2020 ||| pixel-bert: aligning image pixels with text by deep multi-modal transformers. ||| 35867 ||| 8770 ||| 19680 ||| 17609 ||| 1699 ||| 
2019 ||| transformer-cnn: fast and reliable tool for qsar. ||| 4123 ||| 4124 ||| 4125 ||| 
2019 ||| message passing attention networks for document understanding. ||| 18287 ||| 18288 ||| 18289 ||| 
2018 ||| a brand-level ranking system with the customized attention-gru model. ||| 1107 ||| 1112 ||| 23455 ||| 23456 ||| 1113 ||| 1114 ||| 1115 ||| 
2021 ||| fine-tuning of pre-trained transformers for hate, offensive, and profane content detection in english and marathi. ||| 35868 ||| 35869 ||| 35870 ||| 
2020 ||| on the global self-attention mechanism for graph convolutional networks. ||| 5187 ||| 20345 ||| 
2021 ||| generalized decision transformer for offline hindsight information matching. ||| 35871 ||| 20764 ||| 35872 ||| 
2021 ||| memory-augmented non-local attention for video super-resolution. ||| 35873 ||| 18820 ||| 9575 ||| 2165 ||| 
2021 ||| learning bounded context-free-grammar via lstm and the transformer: difference and explanations. ||| 15300 ||| 35874 ||| 35875 ||| 19046 ||| 35876 ||| 
2021 ||| dynstgat: dynamic spatial-temporal graph attention network for traffic signal control. ||| 213 ||| 214 ||| 216 ||| 378 ||| 
2020 ||| multi-image super resolution of remotely sensed images using residual feature attention deep neural networks. ||| 30602 ||| 30173 ||| 30174 ||| 30175 ||| 
2019 ||| perceptual attention-based predictive control. ||| 22277 ||| 22278 ||| 22279 ||| 22280 ||| 
2021 ||| bumblebee: a transformer for music. ||| 35877 ||| 35878 ||| 
2020 ||| learning frame level attention for environmental sound classification. ||| 6563 ||| 6564 ||| 6566 ||| 6565 ||| 6567 ||| 
2021 ||| memory efficient adaptive attention for multiple domain learning. ||| 35879 ||| 35880 ||| 35881 ||| 35882 ||| 
2019 ||| scene memory transformer for embodied agents in long-horizon tasks. ||| 19177 ||| 19178 ||| 19179 ||| 9218 ||| 
2020 ||| human brain activity for machine attention. ||| 20956 ||| 13358 ||| 23082 ||| 
2021 ||| wlv-rit at germeval 2021: multitask learning with transformers to detect toxic, engaging, and fact-claiming comments. ||| 35883 ||| 3849 ||| 10623 ||| 
2021 ||| investigating transformers in the decomposition of polygonal shapes as point collections. ||| 7983 ||| 7984 ||| 7985 ||| 
2021 ||| transmix: attend to mix for vision transformers. ||| 32259 ||| 2157 ||| 33748 ||| 2160 ||| 8660 ||| 2083 ||| 
2021 ||| stereo waterdrop removal with row-wise dilated attention. ||| 25591 ||| 25592 ||| 23917 ||| 2284 ||| 
2022 ||| tempera: spatial transformer feature pyramid network for cardiac mri segmentation. ||| 20745 ||| 20746 ||| 20747 ||| 20748 ||| 20749 ||| 20750 ||| 
2020 ||| global voxel transformer networks for augmented microscopy. ||| 25412 ||| 34221 ||| 23491 ||| 
2022 ||| actformer: a gan transformer framework towards general action-conditioned 3d human motion generation. ||| 20210 ||| 35884 ||| 35885 ||| 35886 ||| 35887 ||| 35888 ||| 293 ||| 
2020 ||| rra-u-net: a residual encoder to attention decoder by residual connections framework for spine segmentation under noisy labels. ||| 11355 ||| 11356 ||| 11357 ||| 
2021 ||| syntax-bert: improving pre-trained transformers with syntax trees. ||| 22726 ||| 18497 ||| 24962 ||| 7703 ||| 18503 ||| 8160 ||| 18501 ||| 
2021 ||| ds-transunet: dual swin transformer u-net for medical image segmentation. ||| 35889 ||| 31045 ||| 35890 ||| 1770 ||| 26463 ||| 
2020 ||| spatio-temporal graph transformer networks for pedestrian trajectory prediction. ||| 8696 ||| 8697 ||| 8698 ||| 7267 ||| 1944 ||| 
2021 ||| motr: end-to-end multiple-object tracking with transformer. ||| 35891 ||| 5707 ||| 2546 ||| 10980 ||| 35324 ||| 19222 ||| 
2021 ||| representation learning for neural population activity with neural data transformers. ||| 35892 ||| 35893 ||| 
2021 ||| noise classification aided attention-based neural network for monaural speech enhancement. ||| 6073 ||| 2754 ||| 34013 ||| 26837 ||| 
2020 ||| weak-attention suppression for transformer based speech recognition. ||| 11974 ||| 11973 ||| 11976 ||| 12449 ||| 11975 ||| 12489 ||| 11978 ||| 12491 ||| 
2021 ||| transformer based bengali chatbot using general knowledge dataset. ||| 4656 ||| 4659 ||| 25968 ||| 25969 ||| 4658 ||| 
2020 ||| inner attention modeling for flexible teaming of heterogeneous multi robots using multi-agent reinforcement learning. ||| 1124 ||| 1840 ||| 
2020 ||| multi-level head-wise match and aggregation in transformer for textual sequence matching. ||| 3363 ||| 18212 ||| 1398 ||| 800 ||| 2046 ||| 
2021 ||| glimpse-attend-and-explore: self-attention for active visual exploration. ||| 2179 ||| 2180 ||| 2181 ||| 
2020 ||| mono vs multilingual transformer-based models: a comparison across several language tasks. ||| 35894 ||| 35895 ||| 
2020 ||| specter: document-level representation learning using citation-informed transformers. ||| 3122 ||| 3123 ||| 3124 ||| 3125 ||| 3126 ||| 
2020 ||| attentionddi: siamese attention-based deep learning method for drug-drug interaction predictions. ||| 35896 ||| 33294 ||| 35897 ||| 26645 ||| 
2019 ||| attention-gated graph convolution for extracting drugs and their interactions from drug labels. ||| 31768 ||| 31162 ||| 31769 ||| 
2019 ||| hybrid-attention based decoupled metric learning for zero-shot image retrieval. ||| 2015 ||| 2400 ||| 
2022 ||| task-adaptive feature transformer with semantic enrichment for few-shot segmentation. ||| 33180 ||| 33181 ||| 33182 ||| 33183 ||| 
2021 ||| power law graph transformer for machine translation and representation learning. ||| 34624 ||| 
2022 ||| vitbis: vision transformer for biomedical image segmentation. ||| 7335 ||| 
2021 ||| transformer for image quality assessment. ||| 11247 ||| 11248 ||| 
2018 ||| can: constrained attention networks for multi-aspect sentiment analysis. ||| 26741 ||| 21574 ||| 254 ||| 26742 ||| 26743 ||| 26744 ||| 26745 ||| 
2022 ||| edgeformer: a parameter-efficient transformer for on-device seq2seq generation. ||| 26473 ||| 3174 ||| 
2019 ||| attention guided graph convolutional networks for relation extraction. ||| 3820 ||| 2349 ||| 3131 ||| 
2019 ||| english-czech systems in wmt19: document-level transformer. ||| 21388 ||| 21419 ||| 7440 ||| 21420 ||| 21421 ||| 21422 ||| 
2019 ||| blockwise self-attention for long document understanding. ||| 26301 ||| 25062 ||| 3348 ||| 35898 ||| 26302 ||| 7030 ||| 
2021 ||| aasist: audio anti-spoofing using integrated spectro-temporal graph attention networks. ||| 12728 ||| 12729 ||| 14571 ||| 13497 ||| 12731 ||| 35899 ||| 12730 ||| 14574 ||| 
2020 ||| unsupervised evaluation for question answering with transformers. ||| 20956 ||| 3586 ||| 20957 ||| 
2021 ||| multimodal graph-based transformer framework for biomedical relation extraction. ||| 3061 ||| 3062 ||| 3063 ||| 404 ||| 
2019 ||| semantic adversarial network with multi-scale pyramid attention for video classification. ||| 17787 ||| 2306 ||| 1371 ||| 242 ||| 17788 ||| 
2017 ||| clothing retrieval with visual attention model. ||| 21742 ||| 21743 ||| 1222 ||| 1086 ||| 19085 ||| 
2018 ||| attention-aware compositional network for person re-identification. ||| 4620 ||| 8164 ||| 7758 ||| 19103 ||| 2303 ||| 
2021 ||| energon: towards efficient acceleration of transformers using dynamic sparse attention. ||| 35900 ||| 35901 ||| 35902 ||| 35903 ||| 
2019 ||| attention routing between capsules. ||| 8042 ||| 8043 ||| 8044 ||| 35904 ||| 
2020 ||| guider l'attention dans les modeles de sequence a sequence pour la prediction des actes de dialogue. ||| 18127 ||| 18128 ||| 17954 ||| 18129 ||| 18130 ||| 9772 ||| 9773 ||| 
2019 ||| david: dual-attentional video deblurring. ||| 7192 ||| 7193 ||| 3937 ||| 7194 ||| 7195 ||| 
2021 ||| attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams. ||| 14737 ||| 14738 ||| 14739 ||| 14740 ||| 14741 ||| 12526 ||| 14742 ||| 
2021 ||| automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation. ||| 15580 ||| 15581 ||| 15582 ||| 15583 ||| 15584 ||| 15585 ||| 15586 ||| 15587 ||| 15588 ||| 
2021 ||| interactively generating explanations for transformer language models. ||| 35905 ||| 35906 ||| 35907 ||| 35908 ||| 
2021 ||| transformer based deliberation for two-pass speech recognition. ||| 1098 ||| 3336 ||| 12066 ||| 12099 ||| 
2021 ||| learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. ||| 35909 ||| 32597 ||| 33986 ||| 32599 ||| 1117 ||| 
2020 ||| da-hgt: domain adaptive heterogeneous graph transformer. ||| 35910 ||| 3617 ||| 1162 ||| 
2022 ||| stock movement prediction based on bi-typed hybrid-relational market knowledge graph via dual attention networks. ||| 3473 ||| 34919 ||| 5439 ||| 34918 ||| 34920 ||| 3476 ||| 3477 ||| 3478 ||| 34921 ||| 
2021 ||| bio-inspired audio-visual cues integration for visual attention prediction. ||| 6650 ||| 35911 ||| 22078 ||| 
2021 ||| hierarchical transformers are more efficient language models. ||| 35912 ||| 35913 ||| 35914 ||| 9135 ||| 35915 ||| 35916 ||| 35917 ||| 
2020 ||| few-shot classification via adaptive attention. ||| 1726 ||| 32528 ||| 35918 ||| 1685 ||| 
2021 ||| tiny transformers for environmental sound classification at the edge. ||| 22635 ||| 22637 ||| 22634 ||| 35919 ||| 
2022 ||| hyperprompt: prompt-based task-conditioning of transformers. ||| 327 ||| 35920 ||| 1398 ||| 3290 ||| 484 ||| 3291 ||| 22746 ||| 17488 ||| 35921 ||| 3294 ||| 22878 ||| 35922 ||| 
2020 ||| deepremaster: temporal source-reference attention networks for comprehensive video enhancement. ||| 20045 ||| 15920 ||| 
2020 ||| efficient attention network: accelerate attention by searching where to plug. ||| 17921 ||| 17922 ||| 17923 ||| 16694 ||| 17924 ||| 
2021 ||| heuristic search planning with deep neural networks using imitation, attention and curriculum learning. ||| 35923 ||| 20220 ||| 35924 ||| 35925 ||| 35926 ||| 35927 ||| 
2021 ||| fingat: financial graph attention networks for recommending top-k profitable stocks. ||| 35928 ||| 28069 ||| 3712 ||| 
2022 ||| disentangling patterns and transformations from one sequence of images with shape-invariant lie group transformer. ||| 4998 ||| 35929 ||| 4999 ||| 5000 ||| 
2021 ||| fast point transformer. ||| 35930 ||| 35931 ||| 17737 ||| 35932 ||| 
2022 ||| paying u-attention to textures: multi-stage hourglass vision transformer for universal texture synthesis. ||| 35933 ||| 35934 ||| 35935 ||| 35936 ||| 
2018 ||| a comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese. ||| 5268 ||| 5269 ||| 5270 ||| 728 ||| 
2021 ||| wlv-rit at semeval-2021 task 5: a neural transformer framework for detecting toxic spans. ||| 3849 ||| 10622 ||| 10623 ||| 10624 ||| 
2019 ||| situation-aware pedestrian trajectory prediction with spatio-temporal attention model. ||| 35937 ||| 35938 ||| 35939 ||| 35940 ||| 
2021 ||| transicd: transformer based code-wise attention model for explainable icd coding. ||| 24433 ||| 24434 ||| 3750 ||| 
2020 ||| voice and accompaniment separation in music using self-attention convolutional neural network. ||| 35941 ||| 35942 ||| 35943 ||| 35944 ||| 
2021 ||| swin transformer: hierarchical vision transformer using shifted windows. ||| 1765 ||| 1766 ||| 1767 ||| 1768 ||| 1769 ||| 1770 ||| 1771 ||| 1772 ||| 
2019 ||| efficient adaptation of pretrained transformers for abstractive summarization. ||| 35945 ||| 3353 ||| 3539 ||| 3355 ||| 
2021 ||| adding quaternion representations to attention networks for classification. ||| 35946 ||| 8248 ||| 
2020 ||| hgat: hierarchical graph attention network for fake news detection. ||| 537 ||| 538 ||| 
2018 ||| iterative recursive attention model for interpretable sequence classification. ||| 23760 ||| 23761 ||| 
2021 ||| chinese sentences similarity via cross-attention based siamese network. ||| 1419 ||| 35947 ||| 35948 ||| 
2021 ||| attention can reflect syntactic structure (if you let it). ||| 24929 ||| 24930 ||| 24931 ||| 3079 ||| 3080 ||| 21390 ||| 
2021 ||| caspianet++: a multidimensional channel-spatial asymmetric attention network with noisy student curriculum learning paradigm for brain tumor segmentation. ||| 35949 ||| 35950 ||| 35951 ||| 35952 ||| 
2021 ||| affinity attention graph neural network for weakly supervised semantic segmentation. ||| 35953 ||| 19558 ||| 8538 ||| 1905 ||| 4400 ||| 
2019 ||| smart transformer modelling in optimal power flow analysis. ||| 21979 ||| 21980 ||| 21981 ||| 16214 ||| 21982 ||| 21983 ||| 
2020 ||| retrofitting structure-aware transformer language model for end tasks. ||| 26780 ||| 26781 ||| 3725 ||| 
2018 ||| self-attention linguistic-acoustic decoder. ||| 11844 ||| 11845 ||| 11846 ||| 
2021 ||| a channel attention based mlp-mixer network for motor imagery decoding with eeg. ||| 35954 ||| 35955 ||| 1224 ||| 15657 ||| 
2021 ||| finetuning pretrained transformers into variational autoencoders. ||| 35956 ||| 35957 ||| 
2020 ||| ampa-net: optimization-inspired attention neural network for deep compressed sensing. ||| 15706 ||| 15707 ||| 
2019 ||| improving semantic segmentation of aerial images using patch-based attention. ||| 35958 ||| 435 ||| 35959 ||| 
2019 ||| a cnn-rnn framework with a novel patch-based multi-attention mechanism for multi-label image classification in remote sensing. ||| 6902 ||| 6903 ||| 6904 ||| 
2018 ||| learning to remember, forget and ignore using attention control in memory. ||| 9298 ||| 13041 ||| 13042 ||| 9300 ||| 13043 ||| 35960 ||| 9302 ||| 
2021 ||| sru++: pioneering fast recurrence with attention for speech recognition. ||| 24359 ||| 19869 ||| 13904 ||| 13963 ||| 3549 ||| 
2021 ||| socialbert - transformers for online socialnetwork language modelling. ||| 35961 ||| 35962 ||| 
2018 ||| agile amulet: real-time salient object detection with contextual attention. ||| 19691 ||| 16642 ||| 952 ||| 1700 ||| 6335 ||| 
2022 ||| acvnet: attention concatenation volume for accurate and efficient stereo matching. ||| 35963 ||| 35964 ||| 6286 ||| 7676 ||| 
2020 ||| deep interleaved network for image super-resolution with asymmetric co-attention. ||| 4398 ||| 23549 ||| 4399 ||| 23550 ||| 
2019 ||| robot navigation in crowds by graph convolutional networks with attention learned from human gaze. ||| 1303 ||| 21804 ||| 124 ||| 5407 ||| 
2021 ||| improving patent mining and relevance classification using transformers. ||| 13992 ||| 35965 ||| 35966 ||| 35967 ||| 35968 ||| 
2020 ||| mhsan: multi-head self-attention network for visual semantic embedding. ||| 7381 ||| 7382 ||| 7384 ||| 7383 ||| 
2020 ||| alleviating the inequality of attention heads for neural machine translation. ||| 17973 ||| 4845 ||| 4790 ||| 4847 ||| 
2017 ||| multiple range-restricted bidirectional gated recurrent units with attention for relation classification. ||| 4940 ||| 4941 ||| 
2021 ||| compound word transformer: learning to compose full-song music over dynamic directed hypergraphs. ||| 11919 ||| 18269 ||| 18270 ||| 4374 ||| 
2021 ||| asformer: transformer for action segmentation. ||| 35969 ||| 35970 ||| 35971 ||| 
2018 ||| 3d feature pyramid attention module for robust visual speech recognition. ||| 35972 ||| 35973 ||| 35974 ||| 1916 ||| 1788 ||| 
2019 ||| self-attention transducers for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 12243 ||| 12244 ||| 
2019 ||| a seq-to-seq transformer premised temporal convolutional network for chinese word segmentation. ||| 1706 ||| 4716 ||| 
2020 ||| attngrounder: talking to cars with attention. ||| 8811 ||| 
2020 ||| multi-head monotonic chunkwise attention for online speech recognition. ||| 35975 ||| 13915 ||| 12395 ||| 34684 ||| 13917 ||| 
2021 ||| xrjl-hkust at semeval-2021 task 4: wordnet-enhanced dual multi-head co-attention for reading comprehension of abstract meaning. ||| 10625 ||| 10626 ||| 10627 ||| 3890 ||| 10628 ||| 
2021 ||| caranet: context axial reverse attention network for segmentation of small medical objects. ||| 35976 ||| 35977 ||| 35978 ||| 
2019 ||| decoupled attention network for text recognition. ||| 17680 ||| 17681 ||| 6559 ||| 17418 ||| 17682 ||| 17683 ||| 17684 ||| 17685 ||| 
2021 |||  adaptive attention convolutional neural network. ||| 15439 ||| 15443 ||| 24510 ||| 24511 ||| 24512 ||| 
2021 ||| trajectory-constrained deep latent visual attention for improved local planning in presence of heterogeneous terrain. ||| 25520 ||| 25521 ||| 25507 ||| 25508 ||| 
2020 ||| s2a: wasserstein gan with spatio-spectral laplacian attention for multi-spectral band synthesis. ||| 18866 ||| 18867 ||| 18868 ||| 18869 ||| 
2021 ||| tsdae: using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning. ||| 19389 ||| 26758 ||| 3702 ||| 
2018 ||| attention on attention: architectures for visual question answering (vqa). ||| 35979 ||| 35980 ||| 35981 ||| 
2021 ||| improving streaming transformer based asr under a framework of self-supervised learning. ||| 13915 ||| 14768 ||| 14769 ||| 14770 ||| 12395 ||| 13916 ||| 13917 ||| 
2021 ||| embodied bert: a transformer model for embodied, language-guided visual task completion. ||| 33792 ||| 35982 ||| 35983 ||| 35984 ||| 35985 ||| 
2021 ||| clip2tv: an empirical study on transformer-based methods for video-text retrieval. ||| 364 ||| 2139 ||| 35986 ||| 35987 ||| 3386 ||| 35988 ||| 
2021 ||| video frame interpolation transformer. ||| 1784 ||| 2331 ||| 1782 ||| 1785 ||| 7143 ||| 
2019 ||| rthn: a rnn-transformer hierarchical network for emotion cause extraction. ||| 3828 ||| 23494 ||| 21188 ||| 
2020 ||| object-based attention for spatio-temporal reasoning: outperforming neuro-symbolic models with flexible distributed architectures. ||| 35989 ||| 35990 ||| 8787 ||| 35991 ||| 
2021 ||| sum-product-attention networks: leveraging self-attention in probabilistic circuits. ||| 35992 ||| 35993 ||| 35908 ||| 
2021 ||| exploring and improving mobile level vision transformers. ||| 35994 ||| 5348 ||| 8654 ||| 35995 ||| 2204 ||| 
2019 ||| levenshtein transformer. ||| 9318 ||| 9319 ||| 35996 ||| 
2021 ||| structext: structured text understanding with multi-modal transformers. ||| 5780 ||| 19717 ||| 35997 ||| 17413 ||| 19718 ||| 9337 ||| 19719 ||| 2538 ||| 2536 ||| 1761 ||| 
2021 ||| attend and select: a segment attention based selection mechanism for microblog hashtag generation. ||| 35998 ||| 2259 ||| 9407 ||| 1350 ||| 26419 ||| 215 ||| 26422 ||| 1094 ||| 
2022 ||| vision transformer with deformable attention. ||| 35999 ||| 30237 ||| 35048 ||| 12015 ||| 7875 ||| 
2019 ||| two-headed monster and crossed co-attention networks. ||| 14937 ||| 800 ||| 
2021 ||| unidirectional memory-self-attention transducer for online speech recognition. ||| 12509 ||| 701 ||| 704 ||| 705 ||| 
2020 ||| traffic agent trajectory prediction using social convolution and attention mechanism. ||| 15490 ||| 15491 ||| 6828 ||| 15492 ||| 14779 ||| 
2020 ||| on the sub-layer functionalities of transformer decoder. ||| 26807 ||| 3038 ||| 4882 ||| 4916 ||| 26808 ||| 3041 ||| 
2022 ||| temporal attention for language models. ||| 36000 ||| 36001 ||| 
2021 ||| robust facial expression recognition with convolutional visual transformers. ||| 36002 ||| 36003 ||| 36004 ||| 
2019 ||| multi-modal attention network learning for semantic source code retrieval. ||| 3891 ||| 3892 ||| 3893 ||| 3894 ||| 1306 ||| 1236 ||| 1094 ||| 
2021 ||| separable temporal convolution plus temporally pooled attention for lightweight high-performance keyword spotting. ||| 4549 ||| 4550 ||| 4551 ||| 4552 ||| 
2022 ||| decepticons: corrupted transformers breach privacy in federated learning for language models. ||| 36005 ||| 36006 ||| 36007 ||| 36008 ||| 36009 ||| 34246 ||| 34247 ||| 
2022 ||| transition relation aware self-attention for session-based recommendation. ||| 36010 ||| 36011 ||| 36012 ||| 8837 ||| 22897 ||| 
2020 ||| experiments with lvt and fre for transformer model. ||| 36013 ||| 36014 ||| 
2019 ||| wikipedia and digital currencies: interplay between collective attention and market performance. ||| 36015 ||| 36016 ||| 36017 ||| 
2021 ||| mindless attractor: a false-positive resistant intervention for drawing attention using auditory perturbation. ||| 15392 ||| 15119 ||| 
2021 ||| attention gate in traffic forecasting. ||| 36018 ||| 19443 ||| 32924 ||| 
2022 ||| laneformer: object-aware row-column transformers for lane detection. ||| 36019 ||| 36020 ||| 36021 ||| 5937 ||| 1687 ||| 1688 ||| 1686 ||| 
2021 ||| leveraging transformers for hate speech detection in conversational code-mixed tweets. ||| 36022 ||| 34349 ||| 12550 ||| 
2019 ||| transcoding compositionally: using attention to find more generalizable solutions. ||| 20954 ||| 3341 ||| 20955 ||| 3342 ||| 
2020 ||| turngpt: a transformer-based language model for predicting turn-taking in spoken dialog. ||| 26791 ||| 21674 ||| 
2022 ||| joint rotational invariance and adversarial training of a dual-stream transformer yields state of the art brain-score for area v4. ||| 36023 ||| 15315 ||| 
2017 ||| reading scene text with attention convolutional sequence modeling. ||| 11578 ||| 11579 ||| 2077 ||| 2080 ||| 
2021 ||| transformer-based deep imitation learning for dual-arm robot manipulation. ||| 25514 ||| 4999 ||| 5000 ||| 
2021 ||| legoformer: transformers for block-by-block multi-view 3d reconstruction. ||| 36024 ||| 36025 ||| 19260 ||| 
2022 ||| facial expression recognition with swin transformer. ||| 36026 ||| 36027 ||| 36028 ||| 
2021 ||| attention-based multi-hypothesis fusion for speech summarization. ||| 13944 ||| 8235 ||| 1491 ||| 3549 ||| 
2021 ||| cross-view geo-localization with evolving transformer. ||| 36029 ||| 36030 ||| 19768 ||| 
2022 ||| continuous-time audiovisual fusion with recurrence vs. attention for in-the-wild affect recognition. ||| 19565 ||| 36031 ||| 12680 ||| 36032 ||| 648 ||| 649 ||| 
2018 ||| understanding visual ads by aligning symbols and objects using co-attention. ||| 36033 ||| 36034 ||| 36035 ||| 36036 ||| 
2021 ||| audio-visual scene-aware dialog and reasoning using audio-visual transformers with joint student-teacher learning. ||| 36037 ||| 17909 ||| 2170 ||| 7283 ||| 2508 ||| 2512 ||| 11981 ||| 2507 ||| 
2020 ||| sag-gan: semi-supervised attention-guided gans for data augmentation on medical images. ||| 16727 ||| 19648 ||| 16728 ||| 16726 ||| 33207 ||| 1305 ||| 
2020 ||| preserving dynamic attention for long-term spatial-temporal prediction. ||| 25372 ||| 25373 ||| 1438 ||| 7854 ||| 25374 ||| 
2020 ||| a label attention model for icd coding from clinical text. ||| 23495 ||| 15064 ||| 23496 ||| 
2019 ||| position focused attention network for image-text matching. ||| 23435 ||| 2792 ||| 23436 ||| 19727 ||| 811 ||| 8481 ||| 8482 ||| 
2020 ||| robust brain magnetic resonance image segmentation for hydrocephalus patients: hard and soft attention. ||| 15644 ||| 15645 ||| 15646 ||| 15647 ||| 15648 ||| 577 ||| 
2020 ||| spatio-temporal attention model for tactile texture recognition. ||| 25589 ||| 17822 ||| 14952 ||| 25590 ||| 
2020 ||| net2: a graph attention network method customized for pre-placement net length estimation. ||| 7483 ||| 7484 ||| 7485 ||| 7486 ||| 7487 ||| 7488 ||| 
2021 ||| hierarchical transformer networks for longitudinal clinical document classification. ||| 36038 ||| 1639 ||| 
2021 ||| attention map-guided two-stage anomaly detection using hard augmentation. ||| 22469 ||| 7295 ||| 22470 ||| 7298 ||| 
2021 ||| persformer: a transformer architecture for topological machine learning. ||| 36039 ||| 36040 ||| 36041 ||| 
2022 ||| a multi-scale transformer for medical image segmentation: architectures, model efficiency, and benchmarks. ||| 27520 ||| 27521 ||| 16941 ||| 1749 ||| 
2020 ||| aragpt2: pre-trained transformer for arabic language generation. ||| 33569 ||| 33570 ||| 33571 ||| 
2020 ||| seq2seq ai chatbot with attention mechanism. ||| 36042 ||| 
2021 ||| clvsa: a convolutional lstm based variational sequence-to-sequence model with attention for predicting trends of financial markets. ||| 23367 ||| 23368 ||| 23369 ||| 4979 ||| 23370 ||| 
2021 ||| diverse image inpainting with bidirectional and autoregressive transformers. ||| 19382 ||| 19383 ||| 19384 ||| 19385 ||| 19386 ||| 7406 ||| 19387 ||| 18911 ||| 16696 ||| 
2017 ||| efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. ||| 12110 ||| 12111 ||| 12112 ||| 
2021 ||| ormer with enhanced self-attention. ||| 36043 ||| 2318 ||| 5365 ||| 6828 ||| 19324 ||| 18766 ||| 8660 ||| 
2020 ||| attention mechanism for multivariate time series recurrent model interpretability applied to the ironmaking industry. ||| 36044 ||| 36045 ||| 36046 ||| 
2021 ||| curriculum pre-training heterogeneous subgraph transformer for top-n recommendation. ||| 1341 ||| 30959 ||| 3504 ||| 25320 ||| 1378 ||| 
2021 ||| dual graph convolutional networks with transformer and curriculum learning for image captioning. ||| 19594 ||| 7175 ||| 19595 ||| 19227 ||| 
2022 ||| latentformer: multi-agent transformer-based interaction modeling and trajectory prediction. ||| 36047 ||| 29814 ||| 36048 ||| 36049 ||| 28393 ||| 
2022 ||| visualizing and understanding patch interactions in vision transformer. ||| 11423 ||| 8856 ||| 8562 ||| 781 ||| 19117 ||| 2165 ||| 
2021 ||| improving the faithfulness of attention-based explanations with task-specific information for text classification. ||| 3735 ||| 3736 ||| 
2022 ||| dpst: de novo peptide sequencing with amino-acid-aware transformers. ||| 3666 ||| 5324 ||| 36050 ||| 36051 ||| 36052 ||| 36053 ||| 
2022 ||| dan: a segmentation-free document attention network for handwritten document recognition. ||| 36054 ||| 8382 ||| 36055 ||| 36056 ||| 
2018 ||| end-to-end dense video captioning with masked transformer. ||| 3574 ||| 4889 ||| 7342 ||| 19267 ||| 3287 ||| 
2021 ||| a comparative study of transformer-based language models on extractive question answering. ||| 36057 ||| 36058 ||| 36059 ||| 36060 ||| 
2021 ||| contrastive document representation learning with graph attention networks. ||| 3676 ||| 23354 ||| 26350 ||| 12360 ||| 3251 ||| 
2021 ||| action unit detection with joint adaptive attention and graph relation. ||| 36061 ||| 36062 ||| 36063 ||| 36064 ||| 36065 ||| 5247 ||| 
2022 ||| sunet: swin transformer unet for image denoising. ||| 36066 ||| 22477 ||| 22476 ||| 
2022 ||| task specific attention is one more thing you need for object detection. ||| 36067 ||| 
2018 ||| image transformer. ||| 9133 ||| 2466 ||| 4960 ||| 9135 ||| 9132 ||| 22783 ||| 
2018 ||| fine-grained attention mechanism for neural machine translation. ||| 36068 ||| 3008 ||| 9196 ||| 
2019 ||| quantifying the impact of user attention on fair group representation in ranked lists. ||| 9034 ||| 9035 ||| 9036 ||| 9037 ||| 9038 ||| 
2022 ||| improving sample efficiency of value based models using attention and vision transformers. ||| 36069 ||| 36070 ||| 36071 ||| 17765 ||| 
2019 ||| dynamic evaluation of transformer language models. ||| 36072 ||| 36073 ||| 22815 ||| 11997 ||| 
2021 ||| gumbel-attention for multi-modal machine translation. ||| 33714 ||| 36074 ||| 880 ||| 
2021 ||| improving automated visual fault detection by combining a biologically plausible model of visual attention with deep learning. ||| 1 ||| 22073 ||| 22074 ||| 22075 ||| 
2018 ||| dual attention matching network for context-aware feature sequence based person re-identification. ||| 19205 ||| 675 ||| 19206 ||| 19207 ||| 19208 ||| 11620 ||| 8608 ||| 
2021 ||| transformer-based end-to-end speech recognition with residual gaussian-based self-attention. ||| 4378 ||| 4484 ||| 4381 ||| 
2018 ||| attentional aggregation of deep feature sets for multi-view 3d reconstruction. ||| 2760 ||| 1300 ||| 17812 ||| 17813 ||| 
2021 ||| deep learning based ofdm channel estimation using frequency-time division and attention mechanism. ||| 15876 ||| 15877 ||| 15878 ||| 15879 ||| 14174 ||| 
2019 ||| recurrent attention walk for semi-supervised classification. ||| 22902 ||| 22903 ||| 22904 ||| 9751 ||| 
2021 ||| cotr: efficiently bridging cnn and transformer for 3d medical image segmentation. ||| 27885 ||| 27886 ||| 6335 ||| 9199 ||| 
2022 ||| metric hypertransformers are universal adapted maps. ||| 36075 ||| 36076 ||| 36077 ||| 
2020 ||| streaming transformer-based acoustic models using self-attention with augmented memory. ||| 11976 ||| 11973 ||| 11974 ||| 11978 ||| 11975 ||| 
2022 ||| partially fake audio detection by self-attention-based fake span discovery. ||| 36078 ||| 36079 ||| 36080 ||| 36081 ||| 12644 ||| 22482 ||| 4433 ||| 4460 ||| 
2021 ||| blt: bidirectional layout transformer for controllable layout generation. ||| 9393 ||| 18703 ||| 34024 ||| 14048 ||| 36082 ||| 36083 ||| 12025 ||| 
2017 ||| data fusion and machine learning integration for transformer loss of life estimation. ||| 36084 ||| 36085 ||| 
2022 ||| rformer: transformer-based generative adversarial network for real fundus image restoration on a new clinical benchmark. ||| 36086 ||| 19804 ||| 3147 ||| 36087 ||| 36088 ||| 36089 ||| 36090 ||| 36091 ||| 36092 ||| 
2020 ||| meta-embeddings based on self-attention. ||| 36093 ||| 36094 ||| 34594 ||| 595 ||| 
2019 ||| input-cell attention reduces vanishing saliency of recurrent neural networks. ||| 9127 ||| 9128 ||| 9129 ||| 2712 ||| 9130 ||| 9131 ||| 
2021 ||| medical sansformers: training self-supervised transformers without attention for electronic medical records. ||| 9361 ||| 36095 ||| 9362 ||| 9365 ||| 36096 ||| 9366 ||| 
2019 ||| multimodal transformer networks for end-to-end video-grounded dialogue systems. ||| 3300 ||| 3301 ||| 3302 ||| 3303 ||| 
2019 ||| explicit sparse transformer: concentrated attention through explicit selection. ||| 32711 ||| 25375 ||| 30044 ||| 9350 ||| 16564 ||| 3751 ||| 
2021 ||| hypertenet: hypergraph and transformer-based neural network for personalized list continuation. ||| 7050 ||| 18534 ||| 7042 ||| 
2020 ||| learning to execute programs with instruction pointer attention graph neural networks. ||| 9357 ||| 9358 ||| 9359 ||| 9360 ||| 
2021 ||| attention-based keyword localisation in speech using visual grounding. ||| 14294 ||| 14295 ||| 
2021 ||| transformer is all you need: multimodal multitask learning with a unified transformer. ||| 1879 ||| 1880 ||| 
2021 ||| tea: program repair using neural network based on program information attention matrix. ||| 36097 ||| 3668 ||| 36098 ||| 1420 ||| 
2020 ||| temporal embeddings and transformer models for narrative text understanding. ||| 15087 ||| 15088 ||| 15089 ||| 
2021 ||| training vision transformers for image retrieval. ||| 1886 ||| 36099 ||| 2174 ||| 1890 ||| 1891 ||| 1892 ||| 
2019 ||| path ranking with attention to type hierarchies. ||| 18091 ||| 18092 ||| 18093 ||| 18094 ||| 
2020 ||| calibration of pre-trained transformers. ||| 3178 ||| 26357 ||| 
2019 ||| explicit pairwise word interaction modeling improves pretrained transformers for english semantic similarity tasks. ||| 36100 ||| 26646 ||| 3009 ||| 
2021 ||| towards training stronger video vision transformers for epic-kitchens-100 action recognition. ||| 36101 ||| 1896 ||| 1894 ||| 36102 ||| 1895 ||| 18261 ||| 36103 ||| 36104 ||| 1900 ||| 36105 ||| 
2021 ||| pgganet: pose guided graph attention network for person re-identification. ||| 36106 ||| 22113 ||| 36107 ||| 
2022 ||| uncovering more shallow heuristics: probing the natural language inference capacities of transformer-based pre-trained language models using syllogistic patterns. ||| 36108 ||| 2924 ||| 
2022 ||| tsa-net: tube self-attention network for action quality assessment. ||| 19419 ||| 19420 ||| 19421 ||| 19422 ||| 19423 ||| 
2019 ||| hirenet: a hierarchical attention model for the automatic analysis of asynchronous video job interviews. ||| 2713 ||| 18136 ||| 18137 ||| 18138 ||| 18139 ||| 9772 ||| 9773 ||| 
2019 ||| context-aware graph attention networks. ||| 2632 ||| 36109 ||| 5755 ||| 382 ||| 
2021 ||| auditory attention decoding from eeg using convolutional recurrent neural network. ||| 8258 ||| 1241 ||| 8259 ||| 8260 ||| 
2021 ||| escaping the gradient vanishing: periodic alternatives of softmax in attention mechanism. ||| 36110 ||| 2304 ||| 968 ||| 
2021 ||| exploring transformers in natural language generation: gpt, bert, and xlnet. ||| 36111 ||| 8005 ||| 36112 ||| 
2022 ||| a new generation of perspective api: efficient multilingual character-level transformers. ||| 36113 ||| 34637 ||| 1398 ||| 24916 ||| 3290 ||| 3294 ||| 36114 ||| 
2020 ||| a compare aggregate transformer for understanding document-grounded dialogue. ||| 240 ||| 1219 ||| 26520 ||| 3311 ||| 
2021 ||| feedback pyramid attention networks for single image super-resolution. ||| 28822 ||| 28823 ||| 1796 ||| 32633 ||| 6782 ||| 
2019 ||| one-shot object detection with co-attention and co-excitation. ||| 9307 ||| 9308 ||| 6362 ||| 6363 ||| 
2020 ||| infinite attention: nngp and ntk for deep attention networks. ||| 22761 ||| 22762 ||| 22763 ||| 22764 ||| 
2021 ||| cross-modality fusion transformer for multispectral object detection. ||| 36115 ||| 36116 ||| 36117 ||| 
2021 ||| i2c2w: image-to-character-to-word transformers for accurate scene text recognition. ||| 36118 ||| 7406 ||| 2083 ||| 36119 ||| 8727 ||| 
2021 ||| cross-level cross-scale cross-attention network for point cloud representation. ||| 36120 ||| 36121 ||| 11126 ||| 36122 ||| 
2022 ||| transformer module networks for systematic generalization in visual question answering. ||| 36123 ||| 36124 ||| 36125 ||| 36126 ||| 36127 ||| 
2019 ||| assigning medical codes at the encounter level by paying attention to documents. ||| 36128 ||| 17759 ||| 36129 ||| 
2019 ||| selective attention for context-aware neural machine translation. ||| 4934 ||| 3369 ||| 3370 ||| 3570 ||| 
2021 ||| improving transformer-kernel ranking model using conformer and query term independence. ||| 9582 ||| 9583 ||| 9584 ||| 9585 ||| 9586 ||| 
2021 ||| discrete representations strengthen vision transformer robustness. ||| 36130 ||| 18703 ||| 2293 ||| 19351 ||| 1793 ||| 12025 ||| 
2017 ||| top-down flow transformer networks. ||| 33477 ||| 36131 ||| 34980 ||| 1815 ||| 
2019 ||| adaptively sparse transformers. ||| 26589 ||| 26629 ||| 9210 ||| 3369 ||| 3370 ||| 
2020 ||| learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. ||| 22718 ||| 22719 ||| 9194 ||| 22720 ||| 22721 ||| 9197 ||| 22722 ||| 9196 ||| 
2021 ||| paying attention to astronomical transients: photometric classification with the time-series transformer. ||| 36132 ||| 36133 ||| 
2020 ||| not all parameters are born equal: attention is mostly what you need. ||| 36134 ||| 
2018 ||| a robust deep attention network to noisy labels in semi-supervised biomedical segmentation. ||| 17858 ||| 17859 ||| 
2021 ||| are transformers more robust than cnns? ||| 33750 ||| 36135 ||| 8660 ||| 36136 ||| 
2021 ||| cotr: convolution in transformer network for end to end polyp detection. ||| 19634 ||| 36137 ||| 13810 ||| 
2022 ||| efficacy of transformer networks for classification of raw eeg data. ||| 36138 ||| 13961 ||| 36139 ||| 36140 ||| 
2021 ||| air-nets: an attention-based framework for locally conditioned implicit representations. ||| 13622 ||| 13623 ||| 13624 ||| 
2019 ||| polarimetric thermal to visible face verification via self-attention guided synthesis. ||| 18606 ||| 7220 ||| 18607 ||| 18608 ||| 18609 ||| 
2021 ||| augmented shortcuts for vision transformers. ||| 32730 ||| 19744 ||| 3156 ||| 32699 ||| 19364 ||| 19367 ||| 19362 ||| 
2019 ||| attention-passing models for robust and data-efficient end-to-end speech translation. ||| 3516 ||| 3067 ||| 11470 ||| 3518 ||| 
2021 ||| livestock monitoring with transformer. ||| 36141 ||| 36142 ||| 15326 ||| 36143 ||| 23255 ||| 36144 ||| 36145 ||| 36146 ||| 
2021 ||| lightseq: accelerated training for transformer-based models on gpus. ||| 4781 ||| 4782 ||| 36147 ||| 4783 ||| 3034 ||| 3428 ||| 
2021 ||| modality fusion network and personalized attention in momentary stress detection in the wild. ||| 15239 ||| 22504 ||| 22505 ||| 22506 ||| 
2020 ||| implicit kernel attention. ||| 17967 ||| 18038 ||| 18039 ||| 17969 ||| 
2020 ||| learning to generate diverse dance motions with transformer. ||| 36148 ||| 36149 ||| 36150 ||| 17822 ||| 36151 ||| 36152 ||| 1705 ||| 
2020 ||| minilm: deep self-attention distillation for task-agnostic compression of pre-trained transformers. ||| 3497 ||| 3174 ||| 3171 ||| 3498 ||| 9413 ||| 3480 ||| 
2021 ||| it's all in the heads: using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. ||| 3430 ||| 3431 ||| 
2018 ||| improving review representations with user attention and product attention for sentiment classification. ||| 4784 ||| 18284 ||| 18285 ||| 4845 ||| 4847 ||| 
2022 ||| path-aware graph attention for hd maps in motion prediction. ||| 36153 ||| 9472 ||| 
2021 ||| shuffle transformer with feature alignment for video face parsing. ||| 3248 ||| 12262 ||| 2218 ||| 35697 ||| 35696 ||| 18222 ||| 35698 ||| 
2018 ||| an attention-gated convolutional neural network for sentence classification. ||| 1305 ||| 33942 ||| 36154 ||| 36155 ||| 8154 ||| 
2021 ||| fast-slow transformer for visually grounding speech. ||| 36156 ||| 12272 ||| 
2020 ||| cross-view image synthesis with deformable convolution and attention mechanism. ||| 6630 ||| 6631 ||| 435 ||| 2258 ||| 6632 ||| 6633 ||| 
2020 ||| ganbert: generative adversarial networks with bidirectional encoder representations from transformers for mri to pet synthesis. ||| 36157 ||| 36158 ||| 36159 ||| 36160 ||| 36161 ||| 36162 ||| 36163 ||| 
2022 ||| monodtr: monocular 3d object detection with depth-aware transformer. ||| 25539 ||| 7809 ||| 1385 ||| 1387 ||| 
2018 ||| a deep learning model with hierarchical lstms and supervised attention for anti-phishing. ||| 9940 ||| 36164 ||| 11744 ||| 
2020 ||| automatic brain tumor segmentation with scale attention network. ||| 24665 ||| 
2020 ||| transformer with depth-wise lstm. ||| 8 ||| 3260 ||| 3181 ||| 3207 ||| 
2022 ||| the quarks of attention. ||| 4311 ||| 36165 ||| 
2021 ||| attention aware wavelet-based detection of morphed face images. ||| 18598 ||| 18599 ||| 18600 ||| 18601 ||| 18602 ||| 
2019 ||| can-ner: convolutional attention network for chinese named entity recognition. ||| 4880 ||| 4881 ||| 13310 ||| 36166 ||| 
2021 ||| attention flows are shapley value explanations. ||| 3582 ||| 3583 ||| 
2021 ||| cerberus transformer: joint semantic, affordance and attribute parsing. ||| 17682 ||| 638 ||| 34641 ||| 34642 ||| 34643 ||| 
2022 ||| simcrosstrans: a simple cross-modality transfer learning for object detection with convnets or vision transformers. ||| 36167 ||| 36168 ||| 
2021 ||| universal transformer hawkes process with adaptive recursive iteration. ||| 414 ||| 5310 ||| 416 ||| 417 ||| 
2017 ||| amc: attention guided multi-modal correlation learning for image search. ||| 18808 ||| 4954 ||| 604 ||| 8762 ||| 8667 ||| 
2021 ||| is it time to replace cnns with transformers for medical images? ||| 36169 ||| 36170 ||| 36171 ||| 36172 ||| 36173 ||| 
2019 ||| dual attention networks for visual reference resolution in visual dialog. ||| 26849 ||| 26850 ||| 8580 ||| 
2020 ||| assemblenet++: assembling modality representations via attention connections. ||| 8594 ||| 8593 ||| 8638 ||| 8595 ||| 
2021 ||| physformer: facial video-based physiological measurement with temporal difference transformer. ||| 33001 ||| 36174 ||| 36175 ||| 2335 ||| 2160 ||| 33480 ||| 
2019 ||| pointwise attention-based atrous convolutional neural networks. ||| 27390 ||| 34505 ||| 34506 ||| 
2018 ||| a sequential guiding network with attention for image captioning. ||| 12210 ||| 7095 ||| 12211 ||| 7094 ||| 
2021 ||| answer questions with right image regions: a visual attention regularization approach. ||| 34630 ||| 34632 ||| 36176 ||| 9674 ||| 11493 ||| 9631 ||| 
2021 ||| panoptic segmentation of satellite image time series with convolutional temporal attention networks. ||| 2309 ||| 2310 ||| 2311 ||| 
2020 ||| multi^2oie: multilingual open information extraction based on multi-head attention with bert. ||| 26592 ||| 26593 ||| 26594 ||| 
2019 ||| realistic image generation using region-phrase attention. ||| 22650 ||| 36177 ||| 22651 ||| 
2021 ||| lessons on parameter sharing across layers in transformers. ||| 36178 ||| 26782 ||| 
2019 ||| dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems. ||| 8857 ||| 8858 ||| 1578 ||| 8859 ||| 8860 ||| 8861 ||| 1579 ||| 
2022 ||| label dependent attention model for disease risk prediction using multimodal electronic health records. ||| 18546 ||| 15246 ||| 18547 ||| 18548 ||| 18549 ||| 
2020 ||| automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study. ||| 14877 ||| 14878 ||| 14879 ||| 14880 ||| 14881 ||| 14882 ||| 
2022 ||| on using transformers for speech-separation. ||| 12284 ||| 12285 ||| 12286 ||| 1226 ||| 36179 ||| 12287 ||| 
2020 ||| low-rank bottleneck in multi-head attention models. ||| 2567 ||| 9155 ||| 9157 ||| 9158 ||| 9159 ||| 
2020 ||| cross-lingual relation extraction with transformers. ||| 36180 ||| 36181 ||| 36182 ||| 4819 ||| 
2022 ||| block-sparse adversarial attack to fool transformer-based text classifiers. ||| 36183 ||| 36184 ||| 36185 ||| 
2021 ||| coatnet: marrying convolution and attention for all data sizes. ||| 3780 ||| 3244 ||| 9372 ||| 18031 ||| 
2020 ||| learning spatial attention for face super-resolution. ||| 36186 ||| 36187 ||| 1371 ||| 19196 ||| 36188 ||| 
2020 ||| sparta: efficient open-domain question answering via sparse transformer matching retrieval. ||| 4848 ||| 4849 ||| 4850 ||| 
2019 ||| dynamic attention networks for task oriented grounding. ||| 36189 ||| 7149 ||| 7152 ||| 
2019 ||| skeleton based activity recognition by fusing part-wise spatio-temporal and attention driven residues. ||| 36190 ||| 13827 ||| 36191 ||| 
2022 ||| deepchorus: a hybrid model of multi-scale convolution and self-attention for chorus detection. ||| 36192 ||| 11982 ||| 4112 ||| 3337 ||| 
2020 ||| hierarchical transformer network for utterance-level emotion recognition. ||| 35093 ||| 36193 ||| 36194 ||| 7436 ||| 
2021 ||| co-segmentation inspired attention module for video-based computer vision tasks. ||| 1908 ||| 36195 ||| 36196 ||| 1909 ||| 1910 ||| 
2021 ||| comparison of czech transformers on text classification tasks. ||| 11935 ||| 11934 ||| 
2021 ||| moving towards centers: re-ranking with attention and memory for re-identification. ||| 36197 ||| 9149 ||| 35713 ||| 
2021 ||| kat: a knowledge augmented transformer for vision-and-language. ||| 19650 ||| 8334 ||| 19026 ||| 36198 ||| 36199 ||| 1958 ||| 
2021 ||| skeletor: skeletal transformers for robust body-pose estimation. ||| 4100 ||| 8529 ||| 7442 ||| 8530 ||| 
2019 ||| progressive attention memory network for movie story question answering. ||| 11230 ||| 18673 ||| 18675 ||| 1606 ||| 11231 ||| 
2021 ||| flight demand forecasting with transformers. ||| 36200 ||| 36201 ||| 36202 ||| 36203 ||| 
2019 ||| self-attention capsule networks for image classification. ||| 36204 ||| 36205 ||| 36206 ||| 36207 ||| 
2020 ||| anatomy prior based u-net for pathology segmentation with attention. ||| 27352 ||| 19919 ||| 27353 ||| 27354 ||| 20755 ||| 
2020 ||| multi-task learning with multi-head attention for multi-choice reading comprehension. ||| 36208 ||| 
2021 ||| mformer - approximation of self-attention by spectral shifting. ||| 35055 ||| 
2021 ||| muse: multi-faceted attention for signed network embedding. ||| 36209 ||| 36210 ||| 3337 ||| 23708 ||| 
2021 ||| transformer ensembles for sexism detection. ||| 16515 ||| 16516 ||| 16517 ||| 16518 ||| 
2021 ||| mt6: multilingual pretrained text-to-text transformer with translation pairs. ||| 26632 ||| 3171 ||| 10200 ||| 3499 ||| 26634 ||| 688 ||| 3174 ||| 
2021 ||| deep personalized glucose level forecasting using attention-based recurrent neural networks. ||| 482 ||| 483 ||| 484 ||| 485 ||| 
2021 ||| pipetransformer: automated elastic pipelining for distributed training of transformers. ||| 22859 ||| 12469 ||| 22860 ||| 22861 ||| 
2019 ||| event recognition with automatic album detection based on sequential processing, neural attention and image captioning. ||| 962 ||| 
2021 ||| agsfcos: based on attention mechanism and scale-equalizing pyramid network of object detection. ||| 1052 ||| 36211 ||| 36212 ||| 36213 ||| 36214 ||| 
2022 ||| aspect-based api review classification: how far can pre-trained transformer model go? ||| 36215 ||| 13148 ||| 36216 ||| 36217 ||| 36218 ||| 36219 ||| 13151 ||| 
2019 ||| scene-based factored attention for image captioning. ||| 22328 ||| 2367 ||| 17673 ||| 2504 ||| 36220 ||| 
2021 ||| sequence length is a domain: length-based overfitting in transformer models. ||| 26801 ||| 21421 ||| 
2020 ||| understanding the difficulty of training transformers. ||| 4805 ||| 24050 ||| 1958 ||| 3175 ||| 1252 ||| 
2019 ||| fashion editing with multi-scale attention normalization. ||| 36221 ||| 1686 ||| 36222 ||| 36223 ||| 36224 ||| 36225 ||| 36226 ||| 7142 ||| 24823 ||| 
2020 ||| understanding attention: in minds and machines. ||| 36227 ||| 36228 ||| 
2020 ||| trans-blstm: transformer with bidirectional lstm for language understanding. ||| 12360 ||| 3676 ||| 26438 ||| 36229 ||| 3251 ||| 
2019 ||| hypergraph convolution and hypergraph attention. ||| 2083 ||| 36230 ||| 2160 ||| 
2020 ||| split then refine: stacked attention-guided resunets for blind single image visible watermark removal. ||| 17856 ||| 17857 ||| 
2022 ||| flowformer: linearizing transformers with conservation flows. ||| 32463 ||| 36231 ||| 32464 ||| 17636 ||| 17635 ||| 
2020 ||| infominer at wnut-2020 task 2: transformer-based covid-19 informative tweet extraction. ||| 10586 ||| 3849 ||| 
2022 ||| cross-channel attention-based target speaker voice activity detection: experimental results for m2met challenge. ||| 36232 ||| 36233 ||| 765 ||| 
2021 ||| nlp-cuet@dravidianlangtech-eacl2021: offensive language detection from multilingual code-mixed text using transformers. ||| 4884 ||| 36234 ||| 4885 ||| 
2021 ||| 3d-man: 3d multi-frame attention network for object detection. ||| 18824 ||| 18825 ||| 12069 ||| 18826 ||| 
2018 ||| an attention model for group-level emotion recognition. ||| 26896 ||| 26897 ||| 26898 ||| 26899 ||| 2351 ||| 
2021 ||| the sensory neuron as a transformer: permutation-invariant neural networks for reinforcement learning. ||| 36235 ||| 36236 ||| 
2020 ||| earl: speedup transformer-based rankers with pre-computed representation. ||| 36237 ||| 15113 ||| 15114 ||| 
2021 ||| unlocking pixels for reinforcement learning via implicit attention. ||| 22791 ||| 36238 ||| 32896 ||| 24035 ||| 24034 ||| 36239 ||| 36240 ||| 36241 ||| 24042 ||| 
2021 ||| relaxed attention: a simple method to boost performance of end-to-end automatic speech recognition. ||| 13888 ||| 13889 ||| 13890 ||| 13891 ||| 
2017 ||| sequential attention. ||| 23762 ||| 23763 ||| 3720 ||| 
2021 ||| mfevit: a robust lightweight transformer-based network for multimodal 2d+3d facial expression recognition. ||| 34442 ||| 34443 ||| 36242 ||| 34444 ||| 
2020 ||| efficient scene text detection with textual attention tower. ||| 1166 ||| 12167 ||| 12168 ||| 12169 ||| 9351 ||| 9354 ||| 5328 ||| 9353 ||| 
2021 ||| mutformer: a context-dependent transformer-based model to predict pathogenic missense mutations. ||| 36243 ||| 30664 ||| 333 ||| 
2021 ||| gsa-forecaster: forecasting graph-based time-dependent data with graph sequence attention. ||| 438 ||| 15790 ||| 852 ||| 10217 ||| 
2021 ||| learning pruned structure and weights simultaneously from scratch: an attention based approach. ||| 36244 ||| 15278 ||| 36245 ||| 36246 ||| 
2022 ||| integrating dependency tree into self-attention for sentence representation. ||| 5105 ||| 1221 ||| 32834 ||| 36247 ||| 9889 ||| 
2022 ||| sparse cross-scale attention network for efficient lidar panoptic segmentation. ||| 31937 ||| 36248 ||| 36249 ||| 36250 ||| 36251 ||| 
2021 ||| vision transformer based covid-19 detection using chest x-rays. ||| 36252 ||| 36253 ||| 
2021 ||| a multi-level attention model for evidence-based fact checking. ||| 3316 ||| 3317 ||| 398 ||| 
2021 ||| h-transformer-1d: fast one-dimensional hierarchical attention for sequences. ||| 3730 ||| 3731 ||| 
2020 ||| attention neural network for trash detection on water channels. ||| 36254 ||| 36255 ||| 36256 ||| 36257 ||| 
2021 ||| sp-sedt: self-supervised pre-training for sound event detection transformer. ||| 36258 ||| 33069 ||| 2519 ||| 33072 ||| 36259 ||| 36260 ||| 810 ||| 
2021 ||| structure-aware fine-tuning of sequence-to-sequence transformers for transition-based amr parsing. ||| 4818 ||| 3551 ||| 1633 ||| 3553 ||| 3686 ||| 4819 ||| 3688 ||| 
2017 ||| minimum word error rate training for attention-based sequence-to-sequence models. ||| 12065 ||| 12066 ||| 12067 ||| 12068 ||| 12069 ||| 3334 ||| 12070 ||| 
2020 |||  2020: identifying check-worthy tweets with transformer models. ||| 10729 ||| 10730 ||| 10604 ||| 7049 ||| 
2022 ||| block-recurrent transformers. ||| 36261 ||| 22751 ||| 35915 ||| 36262 ||| 36263 ||| 
2020 ||| attention-guided quality assessment for automated cryo-em grid screening. ||| 27570 ||| 27571 ||| 27572 ||| 
2022 ||| feat: face editing with attention. ||| 15601 ||| 13429 ||| 36264 ||| 36265 ||| 6238 ||| 
2018 ||| simple attention-based representation learning for ranking short social media posts. ||| 4839 ||| 4840 ||| 3009 ||| 
2021 ||| using keypoint matching and interactive self attention network to verify retail posms. ||| 36266 ||| 36267 ||| 36268 ||| 
2018 ||| recurrently exploring class-wise attention in a hybrid convolutional and bidirectional lstm network for multi-label aerial image classification. ||| 16129 ||| 16130 ||| 16131 ||| 
2021 ||| integrated training for sequence-to-sequence models using non-autoregressive transformer. ||| 4727 ||| 4728 ||| 3732 ||| 4729 ||| 4730 ||| 4731 ||| 3454 ||| 
2020 ||| dmd: a large-scale multi-modal driver monitoring dataset for attention and alertness analysis. ||| 8645 ||| 8646 ||| 8647 ||| 8648 ||| 8649 ||| 8650 ||| 8651 ||| 8652 ||| 8653 ||| 
2022 ||| lmn at semeval-2022 task 11: a transformer-based system for english named entity recognition. ||| 36269 ||| 
2021 ||| towards more effective prm-based crowd counting via a multi-resolution fusion and attention network. ||| 388 ||| 392 ||| 
2020 ||| hypergrid: efficient multi-task transformers with grid-wise decomposable hyper projections. ||| 1398 ||| 22746 ||| 3292 ||| 3294 ||| 22745 ||| 
2021 ||| attention meets geometry: geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation. ||| 13625 ||| 13626 ||| 13627 ||| 13628 ||| 27674 ||| 
2022 ||| repre: improving self-supervised vision transformer with reconstructive pre-training. ||| 36270 ||| 36271 ||| 36272 ||| 675 ||| 2303 ||| 18805 ||| 
2020 ||| lava nat: a non-autoregressive translation model with look-around decoding and vocabulary attention. ||| 9200 ||| 9201 ||| 36273 ||| 2258 ||| 9204 ||| 
2021 ||| self-supervised transformer for multivariate clinical time-series with missing values. ||| 36274 ||| 8954 ||| 
2020 ||| relational graph attention network for aspect-based sentiment analysis. ||| 333 ||| 3099 ||| 3100 ||| 3101 ||| 3049 ||| 
2021 ||| cagan: text-to-image generation with combined attention gans. ||| 23132 ||| 23133 ||| 23134 ||| 
2021 ||| deeppseudo: deep pseudo-code generation via transformer and code feature extraction. ||| 6005 ||| 6007 ||| 5942 ||| 6008 ||| 
2021 ||| using transformers to provide teachers with personalized feedback on their classroom discourse: the talkmoves application. ||| 36275 ||| 36276 ||| 36277 ||| 36278 ||| 36279 ||| 36280 ||| 36281 ||| 
2022 ||| deep attention-based supernovae classification of multi-band light-curves. ||| 36282 ||| 36283 ||| 9801 ||| 36284 ||| 36285 ||| 
2021 ||| relation-aware graph attention model with adaptive self-adversarial training. ||| 5289 ||| 18008 ||| 18009 ||| 18010 ||| 
2022 ||| towards exemplar-free continual learning in vision transformers: an account of attention, functional and weight regularization. ||| 36286 ||| 36287 ||| 36288 ||| 36289 ||| 7105 ||| 
2021 ||| how to train your vit? data, augmentation, and regularization in vision transformers. ||| 36290 ||| 7979 ||| 23910 ||| 36291 ||| 4960 ||| 23909 ||| 
2022 ||| technical report for iccv 2021 challenge sslad-track3b: transformers are better continual learners. ||| 8706 ||| 36292 ||| 2611 ||| 2609 ||| 17807 ||| 
2019 ||| extreme low resolution activity recognition with spatial-temporal attention transfer. ||| 36293 ||| 36294 ||| 9570 ||| 
2020 ||| we learn better road pothole detection: from attention aggregation to adversarial domain adaptation. ||| 8844 ||| 8845 ||| 8846 ||| 124 ||| 
2019 ||| diversified co-attention towards informative live video commenting. ||| 8663 ||| 21143 ||| 21144 ||| 21145 ||| 21146 ||| 
2020 ||| reservoir transformer. ||| 3822 ||| 3823 ||| 3824 ||| 2596 ||| 3825 ||| 3826 ||| 
2021 ||| makeup216: logo recognition with adversarial attention representations. ||| 36295 ||| 36296 ||| 7873 ||| 36297 ||| 36298 ||| 13413 ||| 36299 ||| 36300 ||| 
2022 ||| etsformer: exponential smoothing transformers for time-series forecasting. ||| 36301 ||| 25362 ||| 3301 ||| 36302 ||| 3303 ||| 
2022 ||| lap: an attention-based module for faithful interpretation and knowledge injection in convolutional neural networks. ||| 36303 ||| 36304 ||| 36305 ||| 
2021 ||| sml: a new semantic embedding alignment transformer for efficient cross-lingual natural language inference. ||| 10743 ||| 10744 ||| 3882 ||| 10745 ||| 
2021 ||| a transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain mri images. ||| 36306 ||| 36307 ||| 36308 ||| 36309 ||| 36310 ||| 36311 ||| 36312 ||| 36313 ||| 36314 ||| 36315 ||| 36316 ||| 36317 ||| 36318 ||| 36319 ||| 36320 ||| 36321 ||| 36322 ||| 781 ||| 8534 ||| 36323 ||| 
2017 ||| object-part attention driven discriminative localization for fine-grained image classification. ||| 5954 ||| 6618 ||| 36324 ||| 
2021 ||| arat5: text-to-text transformers for arabic language understanding and generation. ||| 3154 ||| 3153 ||| 3152 ||| 
2021 ||| localizing objects with self-supervised transformers and no labels. ||| 36325 ||| 36326 ||| 1923 ||| 36327 ||| 36328 ||| 36329 ||| 36330 ||| 36331 ||| 2600 ||| 1925 ||| 36332 ||| 
2020 ||| hybrid attentional memory network for computational drug repositioning. ||| 16680 ||| 36333 ||| 36334 ||| 36335 ||| 
2021 ||| bag of tricks for optimizing transformer efficiency. ||| 17717 ||| 17716 ||| 2333 ||| 3306 ||| 
2021 ||| gaze estimation using transformer. ||| 36336 ||| 13639 ||| 
2019 ||| sanet: superpixel attention network for skin lesion attributes detection. ||| 31272 ||| 6582 ||| 5476 ||| 
2020 ||| guiding symbolic natural language grammar induction via transformer-based sequence probabilities. ||| 11875 ||| 36337 ||| 11878 ||| 
2021 ||| ctal: pre-training cross-modal transformer for audio-and-language representations. ||| 6799 ||| 26836 ||| 8082 ||| 8083 ||| 8087 ||| 
2021 ||| observable and attention-directing bdi agents for human-autonomy teaming. ||| 36338 ||| 36339 ||| 36340 ||| 36341 ||| 
2021 ||| referring segmentation in images and videos with cross-modal self-attention network. ||| 18830 ||| 18831 ||| 2740 ||| 28779 ||| 602 ||| 
2022 ||| vision transformer slimming: multi-dimension searching in continuous optimization space. ||| 36342 ||| 19634 ||| 1415 ||| 36343 ||| 36344 ||| 35465 ||| 
2019 ||| follow the attention: combining partial pose and object motion for fine-grained action detection. ||| 36345 ||| 5176 ||| 36346 ||| 
2021 ||| variational structured attention networks for deep visual representation learning. ||| 2522 ||| 36347 ||| 9285 ||| 436 ||| 2523 ||| 2524 ||| 
2020 ||| a transformer-based audio captioning model with keyword estimation. ||| 8065 ||| 4408 ||| 14535 ||| 12574 ||| 12576 ||| 
2017 ||| incorporating global visual features into attention-based neural machine translation. ||| 26818 ||| 3443 ||| 36348 ||| 
2021 ||| the case for translation-invariant self-attention in transformer-based language models. ||| 3202 ||| 3203 ||| 
2019 ||| fine-tuning pre-trained transformer language models to distantly supervised relation extraction. ||| 3448 ||| 3449 ||| 3450 ||| 3451 ||| 
2019 ||| self multi-head attention for speaker recognition. ||| 12474 ||| 12475 ||| 12476 ||| 
2021 ||| investigating methods to improve language model integration for attention-based encoder-decoder asr models. ||| 14318 ||| 14319 ||| 14320 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2020 ||| undivided attention: are intermediate layers necessary for bert? ||| 36349 ||| 19312 ||| 
2019 ||| real-time inference in multi-sentence tasks with deep pretrained transformers. ||| 36350 ||| 36351 ||| 36352 ||| 34967 ||| 
2020 ||| imram: iterative matching with recurrent attention memory for cross-modal image-text retrieval. ||| 10064 ||| 10065 ||| 12569 ||| 10066 ||| 3478 ||| 2278 ||| 
2022 ||| attention option-critic. ||| 36353 ||| 17765 ||| 
2021 ||| person re-identification via attention pyramid. ||| 1917 ||| 36354 ||| 1920 ||| 36355 ||| 1921 ||| 
2020 ||| junk news bubbles: modelling the rise and fall of attention in online arenas. ||| 13986 ||| 13987 ||| 2980 ||| 
2019 ||| augmenting self-attention with persistent memory. ||| 3797 ||| 3798 ||| 36356 ||| 1890 ||| 1891 ||| 1892 ||| 1889 ||| 
2017 ||| interactive attention networks for aspect-level sentiment classification. ||| 14934 ||| 11660 ||| 14935 ||| 11662 ||| 
2020 ||| generating accurate assert statements for unit test cases using pretrained transformers. ||| 32706 ||| 15263 ||| 15264 ||| 15265 ||| 
2021 ||| generative pre-trained transformer for design concept generation: an exploration. ||| 18209 ||| 36357 ||| 
2021 ||| multi-view trgru: transformer based spatiotemporal model for short-term metro origin-destination matrix prediction. ||| 36358 ||| 36359 ||| 31390 ||| 6284 ||| 2001 ||| 
2021 ||| accelerating covid-19 research with graph mining and transformer-based learning. ||| 1361 ||| 36360 ||| 1360 ||| 36361 ||| 1362 ||| 1363 ||| 
2021 ||| a unified pruning framework for vision transformers. ||| 5447 ||| 1863 ||| 
2021 ||| personalized transformer for explainable recommendation. ||| 3034 ||| 3035 ||| 3036 ||| 
2021 ||| a feature consistency driven attention erasing network for fine-grained image retrieval. ||| 1872 ||| 8012 ||| 8011 ||| 36362 ||| 27118 ||| 
2021 ||| interpretable multi-head self-attention model for sarcasm detection in social media. ||| 31670 ||| 31671 ||| 
2022 ||| cats++: boosting cost aggregation with convolutions and transformers. ||| 36363 ||| 36364 ||| 8797 ||| 
2018 ||| gated hierarchical attention for image captioning. ||| 6370 ||| 6371 ||| 
2021 ||| learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction. ||| 247 ||| 36365 ||| 27607 ||| 8906 ||| 31317 ||| 8660 ||| 
2019 ||| mixed high-order attention network for person re-identification. ||| 2015 ||| 2400 ||| 2401 ||| 
2021 ||| high-fidelity pluralistic image completion with transformers. ||| 2492 ||| 2493 ||| 2494 ||| 2495 ||| 
2022 ||| multiview transformers for video recognition. ||| 22840 ||| 8591 ||| 2292 ||| 36366 ||| 22842 ||| 2094 ||| 2093 ||| 
2021 ||| gmair: unsupervised object detection based on spatial attention and gaussian mixture. ||| 36367 ||| 36368 ||| 36369 ||| 36370 ||| 
2019 ||| single-modal and multi-modal false arrhythmia alarm reduction using attention-based convolutional and recurrent neural networks. ||| 6124 ||| 36371 ||| 6125 ||| 
2021 ||| a daily tourism demand prediction framework based on multi-head attention cnn: the case of the foreign entrant in south korea. ||| 27275 ||| 27276 ||| 27277 ||| 27278 ||| 27279 ||| 
2019 ||| interactive attention for semantic text matching. ||| 12351 ||| 18572 ||| 4639 ||| 18573 ||| 2355 ||| 
2020 ||| graph attentional autoencoder for anticancer hyperfood prediction. ||| 36372 ||| 36373 ||| 36374 ||| 36375 ||| 36376 ||| 
2021 ||| complex spectral mapping with attention based convolution recurrent neural network for speech enhancement. ||| 36377 ||| 36378 ||| 36379 ||| 9204 ||| 4111 ||| 
2022 ||| -scaled-attention: a novel fast attention mechanism for efficient modeling of protein sequences. ||| 33961 ||| 36380 ||| 36381 ||| 
2021 ||| cosformer: detecting co-salient object with transformers. ||| 6434 ||| 
2022 ||| supervised visual attention for simultaneous multimodal machine translation. ||| 36382 ||| 4744 ||| 4746 ||| 
2020 ||| dual-path self-attention rnn for real-time speech enhancement. ||| 36383 ||| 14725 ||| 
2021 ||| agd-autoencoder: attention gated deep convolutional autoencoder for brain tumor segmentation. ||| 36384 ||| 
2021 ||| detecting dementia from speech and transcripts using transformers. ||| 36385 ||| 36386 ||| 36387 ||| 
2021 ||| a hybrid attention mechanism for weakly-supervised temporal action localization. ||| 17778 ||| 7175 ||| 17779 ||| 
2020 ||| learning hard retrieval cross attention for transformer. ||| 8 ||| 3260 ||| 
2018 ||| exploring correlations in multiple facial attributes through graph attention network. ||| 2349 ||| 21803 ||| 
2020 ||| an experimental evaluation of transformer-based language models in the biomedical domain. ||| 36388 ||| 36389 ||| 36390 ||| 36391 ||| 36392 ||| 36393 ||| 33840 ||| 36394 ||| 36395 ||| 36396 ||| 36397 ||| 
2017 ||| tweetit- analyzing topics for twitter users to garner maximum attention. ||| 1027 ||| 25833 ||| 25834 ||| 
2021 ||| siamese network with interactive transformer for video object segmentation. ||| 36398 ||| 875 ||| 19635 ||| 17758 ||| 
2021 ||| evolving attention with residual convolutions. ||| 18497 ||| 7703 ||| 22726 ||| 19936 ||| 18503 ||| 8160 ||| 13362 ||| 7875 ||| 18501 ||| 
2021 ||| da-detr: domain adaptive detection transformer by hybrid attention. ||| 3206 ||| 36399 ||| 35747 ||| 36400 ||| 7406 ||| 
2019 ||| characterizing attention cascades in whatsapp groups. ||| 15803 ||| 15804 ||| 15805 ||| 15806 ||| 8500 ||| 15807 ||| 15808 ||| 15809 ||| 15810 ||| 
2020 ||| nlnde: enhancing neural sequence taggers with attention and noisy channel for robust pharmacological entity detection. ||| 16488 ||| 16489 ||| 15085 ||| 15086 ||| 
2022 ||| transfusion: multi-view divergent fusion for medical image segmentation with transformers. ||| 16941 ||| 27520 ||| 36401 ||| 36402 ||| 27521 ||| 1749 ||| 
2021 ||| kernel deformed exponential families for sparse continuous attention. ||| 36403 ||| 36404 ||| 36405 ||| 36406 ||| 8692 ||| 
2020 ||| streaming simultaneous speech translation with augmented memory transformer. ||| 12088 ||| 11973 ||| 12089 ||| 12090 ||| 11637 ||| 
2021 ||| temporal-relational hypergraph tri-attention networks for stock trend prediction. ||| 30823 ||| 8285 ||| 2853 ||| 4152 ||| 13203 ||| 444 ||| 13205 ||| 
2021 ||| domain adaptation with category attention network for deep sentiment analysis. ||| 8876 ||| 3476 ||| 8877 ||| 8878 ||| 8879 ||| 8880 ||| 
2021 ||| atiss: autoregressive transformers for indoor scene synthesis. ||| 36407 ||| 36408 ||| 36409 ||| 36410 ||| 2126 ||| 36152 ||| 
2020 ||| rethinking transformer-based set prediction for object detection. ||| 2620 ||| 2621 ||| 2622 ||| 2299 ||| 
2019 ||| self-attention with structural position representations. ||| 3309 ||| 3041 ||| 3038 ||| 4882 ||| 
2020 ||| reads: a rectified attentional double supervised network for scene text recognition. ||| 14552 ||| 20175 ||| 5192 ||| 3248 ||| 6935 ||| 
2021 ||| multiplicative position-aware transformer models for language understanding. ||| 12360 ||| 26438 ||| 3676 ||| 3251 ||| 
2020 ||| transpose: towards explainable human pose estimation by transformer. ||| 2095 ||| 2096 ||| 2097 ||| 2098 ||| 
2021 ||| early convolutions help transformers see better. ||| 2368 ||| 36411 ||| 36412 ||| 19048 ||| 32446 ||| 59 ||| 32447 ||| 
2021 ||| empirical analysis of training strategies of transformer-based japanese chit-chat systems. ||| 36413 ||| 36414 ||| 36415 ||| 36416 ||| 14476 ||| 36417 ||| 36418 ||| 
2019 ||| transformer-based automatic post-editing with a context-aware encoding approach for multi-source inputs. ||| 21391 ||| 36419 ||| 36420 ||| 4941 ||| 
2020 ||| gast-net: graph attention spatio-temporal convolutional networks for 3d human pose estimation in video. ||| 21849 ||| 36421 ||| 21850 ||| 
2021 ||| fnr: a similarity and transformer-based approach to detect multi-modal fake news in social media. ||| 36422 ||| 36423 ||| 36424 ||| 36305 ||| 
2020 ||| transformer with bidirectional decoder for speech recognition. ||| 5250 ||| 14513 ||| 14514 ||| 14515 ||| 7729 ||| 
2021 ||| dcf-asn: coarse-to-fine real-time visual tracking via discriminative correlation filter and attentional siamese network. ||| 35600 ||| 949 ||| 36425 ||| 36426 ||| 
2019 ||| knowledge-grounded response generation with deep attentional latent-variable model. ||| 30140 ||| 36427 ||| 4841 ||| 4843 ||| 
2022 ||| academic resource text level multi-label classification based on attention. ||| 7400 ||| 25343 ||| 13659 ||| 
2020 ||| choppy: cut transformer for ranked list truncation. ||| 3292 ||| 1398 ||| 9580 ||| 3294 ||| 9581 ||| 
2020 ||| a recursive network with dynamic attention for monaural speech enhancement. ||| 14674 ||| 4384 ||| 14675 ||| 14676 ||| 14677 ||| 
2022 ||| learning affective meanings that derives the social behavior using bidirectional encoder representations from transformers. ||| 36428 ||| 36429 ||| 36430 ||| 
2020 ||| daf-net: a saliency based weakly supervised method of dual attention fusion for fine-grained image classification. ||| 36431 ||| 12530 ||| 36432 ||| 7400 ||| 36433 ||| 
2021 ||| exploring sequence feature alignment for domain adaptive detection transformers. ||| 8948 ||| 1903 ||| 875 ||| 19635 ||| 8710 ||| 19636 ||| 1756 ||| 
2021 ||| analyzing the nuances of transformers' polynomial simplification abilities. ||| 36434 ||| 36435 ||| 23085 ||| 
2018 ||| guided attention for large scale scene text verification. ||| 6337 ||| 6338 ||| 6339 ||| 6340 ||| 6341 ||| 1695 ||| 6342 ||| 6343 ||| 
2021 ||| enhancing transformers with gradient boosted decision trees for nli fine-tuning. ||| 3102 ||| 3103 ||| 3104 ||| 
2021 ||| localization uncertainty-based attention for object detection. ||| 11471 ||| 11472 ||| 11473 ||| 2178 ||| 
2018 ||| forecasting user attention during everyday mobile interactions using device-integrated and wearable sensors. ||| 8345 ||| 8346 ||| 3831 ||| 8347 ||| 8348 ||| 
2021 ||| neural attention models in deep learning: survey and taxonomy. ||| 9074 ||| 36436 ||| 
2021 ||| vatt: transformers for multimodal self-supervised learning from raw video, audio and text. ||| 36437 ||| 36438 ||| 36439 ||| 36440 ||| 18926 ||| 34329 ||| 2193 ||| 
2020 ||| multi-view attention networks for visual dialog. ||| 36441 ||| 36442 ||| 36443 ||| 36444 ||| 
2020 ||| trace: early detection of chronic kidney disease onset with transformer-enhanced feature embedding. ||| 3906 ||| 6031 ||| 6032 ||| 6033 ||| 
2018 ||| video-based person re-identification using spatial-temporal attention networks. ||| 36445 ||| 18747 ||| 18831 ||| 602 ||| 
2021 ||| enriched attention for robust relation extraction. ||| 16489 ||| 15085 ||| 15086 ||| 
2021 ||| revisiting the onsets and frames model with additive attention. ||| 933 ||| 934 ||| 935 ||| 936 ||| 
2020 ||| streaming chunk-aware multihead attention for online end-to-end speech recognition. ||| 14494 ||| 14668 ||| 14669 ||| 14599 ||| 8479 ||| 14600 ||| 12384 ||| 
2021 ||| covid-vit: classification of covid-19 from ct chest images based on vision transformer models. ||| 36446 ||| 36447 ||| 36448 ||| 
2021 ||| attention-based contrastive learning for winograd schemas. ||| 3572 ||| 3573 ||| 
2020 ||| air: attention with reasoning capability. ||| 2235 ||| 1871 ||| 8695 ||| 1872 ||| 
2020 ||| weakly-supervised action localization and action recognition using global-local attention of 3d cnn. ||| 4318 ||| 36449 ||| 36450 ||| 
2020 ||| learning efficient gans using differentiable masks and co-attention distillation. ||| 36451 ||| 36452 ||| 247 ||| 2365 ||| 2382 ||| 6831 ||| 1932 ||| 2367 ||| 
2020 ||| improving target-driven visual navigation with attention on 3d spatial relationships. ||| 36453 ||| 13410 ||| 36454 ||| 36455 ||| 1040 ||| 
2020 ||| high tissue contrast mri synthesis using multi-stage attention-gan for glioma segmentation. ||| 18068 ||| 6582 ||| 5476 ||| 
2021 ||| transfornn: capturing the sequential information in self-attention representations for language modeling. ||| 36456 ||| 36457 ||| 17965 ||| 26760 ||| 
2021 ||| attention-based model and deep reinforcement learning for distribution of event processing tasks. ||| 36458 ||| 36459 ||| 36460 ||| 
2021 ||| fmmformer: efficient and flexible transformer via decomposed near-field and far-field attention. ||| 33143 ||| 36461 ||| 33148 ||| 9570 ||| 34973 ||| 
2018 ||| being curious about the answers to questions: novelty search with learned attention. ||| 36462 ||| 36463 ||| 36464 ||| 36465 ||| 
2019 ||| automatic detection of ecg abnormalities by using an ensemble of deep residual networks with attention. ||| 1305 ||| 9897 ||| 20424 ||| 20426 ||| 4103 ||| 20430 ||| 20429 ||| 
2021 ||| automatic sexism detection with multilingual transformer models. ||| 16505 ||| 11303 ||| 16506 ||| 16507 ||| 16508 ||| 16509 ||| 16510 ||| 16511 ||| 16512 ||| 16513 ||| 16514 ||| 
2021 ||| an empirical study on the usage of transformer models for code completion. ||| 36466 ||| 28219 ||| 36467 ||| 28217 ||| 36468 ||| 28221 ||| 36469 ||| 28223 ||| 
2019 ||| an attention-based graph neural network for heterogeneous structural learning. ||| 17879 ||| 10232 ||| 17880 ||| 17881 ||| 17882 ||| 12749 ||| 
2021 ||| pre-training transformers for domain adaptation. ||| 36470 ||| 36471 ||| 
2017 ||| video summarization with attention-based encoder-decoder networks. ||| 2276 ||| 28784 ||| 2279 ||| 6922 ||| 
2020 ||| ai-qmix: attention and imagination for dynamic multi-agent reinforcement learning. ||| 22821 ||| 36472 ||| 36473 ||| 32043 ||| 36474 ||| 22543 ||| 36475 ||| 17735 ||| 
2022 ||| not all patches are what you need: expediting vision transformers via token reorganizations. ||| 35340 ||| 35339 ||| 36476 ||| 8540 ||| 7284 ||| 36477 ||| 
2021 ||| are convolutional neural networks or transformers more like human vision? ||| 36478 ||| 32709 ||| 26314 ||| 26316 ||| 
2019 ||| reasoning about human-object interactions through dual attention networks. ||| 2368 ||| 2369 ||| 36479 ||| 2371 ||| 2372 ||| 2373 ||| 
2020 ||| manifold-driven attention maps for weakly supervised segmentation. ||| 36480 ||| 26899 ||| 15513 ||| 
2021 ||| transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. ||| 8277 ||| 8278 ||| 8279 ||| 8280 ||| 
2020 ||| multi-label thoracic disease image classification with cross-attention networks. ||| 27810 ||| 27811 ||| 3303 ||| 
2020 ||| end-to-end handwritten paragraph text recognition using a vertical attention network. ||| 36054 ||| 8382 ||| 36055 ||| 36056 ||| 
2019 ||| contextual graph attention for answering logical queries over incomplete knowledge graphs. ||| 28173 ||| 28174 ||| 28175 ||| 8473 ||| 28176 ||| 28177 ||| 
2021 ||| precise learning of source code contextual semantics via hierarchical dependence structure and graph attention networks. ||| 36481 ||| 2760 ||| 2064 ||| 36482 ||| 1390 ||| 
2021 ||| auto-tagging of short conversational sentences using transformer methods. ||| 32264 ||| 7111 ||| 32266 ||| 32267 ||| 32261 ||| 32262 ||| 36483 ||| 36484 ||| 36485 ||| 36486 ||| 2713 ||| 5335 ||| 31427 ||| 36487 ||| 36488 ||| 
2022 ||| a lightweight dual-domain attention framework for sparse-view ct reconstruction. ||| 24200 ||| 36489 ||| 36490 ||| 36491 ||| 
2021 ||| fine-tuning transformers for identifying self-reporting potential cases and symptoms of covid-19 in tweets. ||| 36492 ||| 36493 ||| 36494 ||| 36495 ||| 
2017 ||| structural attention neural networks for improved sentiment analysis. ||| 24961 ||| 3729 ||| 
2020 ||| one point is all you need: directional attention point for feature learning. ||| 33717 ||| 36496 ||| 1739 ||| 5723 ||| 3386 ||| 6238 ||| 
2021 ||| an attention-aided deep learning framework for massive mimo channel estimation. ||| 15868 ||| 15869 ||| 15870 ||| 15871 ||| 2049 ||| 
2021 ||| sra-lstm: social relationship attention lstm for human trajectory prediction. ||| 5197 ||| 5198 ||| 15657 ||| 6508 ||| 5200 ||| 
2019 ||| graph message passing with cross-location attentions for long-term ili prediction. ||| 1257 ||| 1258 ||| 359 ||| 1259 ||| 357 ||| 
2021 ||| multi-head self-attention via vision transformer for zero-shot learning. ||| 20284 ||| 23149 ||| 
2019 ||| cnns, lstms, and attention networks for pathology detection in medical data. ||| 36497 ||| 
2017 ||| multilingual hierarchical attention networks for document classification. ||| 14940 ||| 14941 ||| 
2022 ||| qs-attn: query-selected attention for contrastive learning in i2i translation. ||| 34431 ||| 36498 ||| 34432 ||| 34433 ||| 21803 ||| 7716 ||| 
2019 ||| tdapnet: prototype network with recurrent top-down attention for robust object classification under partial occlusion. ||| 36499 ||| 33749 ||| 36500 ||| 36501 ||| 8906 ||| 8660 ||| 
2020 ||| a deep reinforcement learning algorithm using dynamic attention model for vehicle routing problems. ||| 19855 ||| 4099 ||| 36502 ||| 
2021 ||| a reinforcement learning approach for sequential spatial transformer networks. ||| 4192 ||| 4193 ||| 4194 ||| 4195 ||| 4196 ||| 
2021 ||| dynamic transformer for efficient machine translation on embedded devices. ||| 17494 ||| 17495 ||| 17496 ||| 17497 ||| 17498 ||| 17499 ||| 
2021 ||| intformer: predicting pedestrian intention with the aid of the transformer architecture. ||| 36503 ||| 15451 ||| 15453 ||| 15454 ||| 
2020 ||| a mathematical theory of attention. ||| 33228 ||| 33229 ||| 33230 ||| 
2019 ||| overt visual attention on rendered 3d objects. ||| 36504 ||| 36505 ||| 36506 ||| 36507 ||| 
2021 ||| a hierarchical conditional random field-based attention mechanism approach for gastric histopathology image classification. ||| 13786 ||| 13787 ||| 399 ||| 13788 ||| 13789 ||| 34508 ||| 5378 ||| 7826 ||| 4100 ||| 
2022 ||| on the expressive power of message-passing neural networks as global feature map transformers. ||| 36508 ||| 36509 ||| 36510 ||| 
2020 ||| ttpp: temporal transformer with progressive prediction for efficient action anticipation. ||| 8948 ||| 8769 ||| 6830 ||| 2149 ||| 2343 ||| 
2021 |||  - the transformer language model for bosnian, croatian, montenegrin and serbian. ||| 36511 ||| 36512 ||| 
2021 ||| a simple single-scale vision transformer for object localization and instance segmentation. ||| 33440 ||| 33441 ||| 1856 ||| 23909 ||| 23910 ||| 18849 ||| 36513 ||| 4807 ||| 33442 ||| 7195 ||| 33443 ||| 
2020 ||| spatial-temporal transformer networks for traffic flow forecasting. ||| 4547 ||| 36514 ||| 36515 ||| 21376 ||| 36516 ||| 7380 ||| 1906 ||| 
2020 ||| testing pre-trained transformer models for lithuanian news clustering. ||| 23285 ||| 23286 ||| 
2021 ||| serialized multi-layer multi-head attention for neural speaker embedding. ||| 14561 ||| 14562 ||| 12494 ||| 
2020 ||| understanding self-attention of self-supervised audio transformers. ||| 12723 ||| 12722 ||| 12644 ||| 
2021 ||| context-aware heterogeneous graph attention network for user behavior prediction in local consumer service platform. ||| 36517 ||| 30105 ||| 36518 ||| 36519 ||| 36520 ||| 
2022 ||| gransformer: transformer-based graph generation. ||| 36521 ||| 36522 ||| 36523 ||| 29439 ||| 
2020 ||| covid-19 detection using residual attention network an artificial intelligence approach. ||| 25960 ||| 25961 ||| 
2018 ||| on attention models for human activity recognition. ||| 23021 ||| 36524 ||| 
2020 ||| sentence constituent-aware aspect-category sentiment analysis with graph attention networks. ||| 21129 ||| 21130 ||| 6312 ||| 
2021 ||| shallow attention network for polyp segmentation. ||| 27776 ||| 27777 ||| 27729 ||| 5189 ||| 27778 ||| 13677 ||| 
2022 ||| aspect-based sentiment analysis through edu-level attentions. ||| 3014 ||| 1397 ||| 36525 ||| 
2022 ||| adaptive transformers for robust few-shot cross-domain face anti-spoofing. ||| 36526 ||| 33676 ||| 36527 ||| 36528 ||| 36529 ||| 35988 ||| 8659 ||| 7143 ||| 
2017 ||| exploring human-like attention supervision in visual question answering. ||| 18282 ||| 12376 ||| 18283 ||| 
2021 ||| improving 3d object detection with channel-wise transformer. ||| 2485 ||| 2486 ||| 2487 ||| 2488 ||| 2489 ||| 2490 ||| 2491 ||| 
2020 ||| hurricanes and hashtags: characterizing online collective attention for natural disasters. ||| 33417 ||| 33419 ||| 33415 ||| 33416 ||| 33418 ||| 33422 ||| 33423 ||| 
2021 ||| gcst: graph convolutional skeleton transformer for action recognition. ||| 22597 ||| 12646 ||| 22599 ||| 36530 ||| 22601 ||| 22600 ||| 855 ||| 
2018 ||| how to become instagram famous: post popularity prediction with dual-attention. ||| 8760 ||| 2417 ||| 17179 ||| 17180 ||| 2166 ||| 
2019 ||| saliency-guided attention network for image-sentence matching. ||| 2276 ||| 2277 ||| 2278 ||| 2279 ||| 
2021 ||| bert got a date: introducing transformers to temporal tagging. ||| 36531 ||| 36532 ||| 36533 ||| 
2021 ||| grad-cam guided channel-spatial attention module for fine-grained visual classification. ||| 1481 ||| 1482 ||| 1483 ||| 1484 ||| 
2020 ||| advances of transformer-based models for news headline generation. ||| 36534 ||| 36535 ||| 
2021 ||| attentional multi-layer feature fusion convolution network for audio-visual speech enhancement. ||| 36536 ||| 602 ||| 36537 ||| 12342 ||| 36538 ||| 36539 ||| 36540 ||| 
2022 ||| unified fake news detection using transfer learning of bidirectional encoder representation from transformers model. ||| 36541 ||| 36542 ||| 36543 ||| 
2020 ||| contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems. ||| 26604 ||| 26605 ||| 6334 ||| 
2020 ||| deeper or wider networks of point clouds with self-attention? ||| 36544 ||| 12003 ||| 
2022 ||| efficient visual tracking via hierarchical cross-attention transformer. ||| 19066 ||| 952 ||| 29491 ||| 1700 ||| 
2021 ||| learnable compression network with transformer for approximate nearest neighbor search. ||| 30528 ||| 36545 ||| 2735 ||| 5231 ||| 
2018 ||| attention-based pyramid aggregation network for visual place recognition. ||| 19768 ||| 19769 ||| 2477 ||| 8571 ||| 
2017 ||| gated-attention architectures for task-oriented language grounding. ||| 17768 ||| 17769 ||| 17770 ||| 17771 ||| 3247 ||| 
2017 ||| phase conductor on multi-layered attentions for machine comprehension. ||| 1840 ||| 5474 ||| 36546 ||| 36547 ||| 
2021 ||| offline pre-trained multi-agent decision transformer: one big sequence model tackles all smac tasks. ||| 36548 ||| 36549 ||| 16798 ||| 36550 ||| 36551 ||| 1219 ||| 35330 ||| 36552 ||| 1224 ||| 728 ||| 
2019 ||| temporal fusion transformers for interpretable multi-horizon time series forecasting. ||| 36553 ||| 36554 ||| 36555 ||| 36556 ||| 36557 ||| 
2022 ||| cell segmentation from telecentric bright-field transmitted light microscopic images using a residual attention u-net: a case study on hela line. ||| 36558 ||| 36559 ||| 36560 ||| 36561 ||| 36562 ||| 
2021 ||| embracing single stride 3d object detector with sparse transformer. ||| 36563 ||| 36564 ||| 36565 ||| 36566 ||| 18496 ||| 6832 ||| 36567 ||| 18260 ||| 
2021 ||| monaural speech enhancement with complex convolutional block attention module and joint time frequency losses. ||| 12201 ||| 12202 ||| 12203 ||| 
2021 ||| visqa: x-raying vision and language reasoning in transformers. ||| 36568 ||| 36569 ||| 36570 ||| 36571 ||| 36572 ||| 7969 ||| 
2021 ||| nn-lut: neural approximation of non-linear operations for efficient transformer inference. ||| 36573 ||| 7617 ||| 35956 ||| 36574 ||| 36575 ||| 36576 ||| 1614 ||| 
2021 ||| 3d-retr: end-to-end single and multi-view 3d reconstruction with transformers. ||| 36577 ||| 1388 ||| 36578 ||| 34576 ||| 24002 ||| 
2021 ||| residual attention: a simple but effective method for multi-label recognition. ||| 1862 ||| 1863 ||| 
2021 ||| transformers predicting the future. applying attention in next-frame and time series forecasting. ||| 36579 ||| 36580 ||| 
2021 ||| claws: contrastive learning with hard attention and weak supervision. ||| 36581 ||| 36582 ||| 36583 ||| 36584 ||| 36585 ||| 
2021 ||| dynamic token normalization improves vision transformer. ||| 36586 ||| 36587 ||| 2049 ||| 36588 ||| 1846 ||| 23393 ||| 2011 ||| 
2018 ||| "factual" or "emotional": stylized image captioning with adaptive learning and attention. ||| 2417 ||| 8760 ||| 1296 ||| 8761 ||| 8762 ||| 8763 ||| 2166 ||| 
2021 ||| demystify optimization challenges in multilingual transformers. ||| 9390 ||| 17911 ||| 
2019 ||| span-based joint entity and relation extraction with transformer pre-training. ||| 10237 ||| 224 ||| 
2021 ||| deep rgb-d saliency detection with depth-sensitive attention and automatic multi-modal fusion. ||| 15877 ||| 19182 ||| 19183 ||| 19184 ||| 2259 ||| 
2021 ||| tph-yolov5: improved yolov5 based on transformer prediction head for object detection on drone-captured scenarios. ||| 8010 ||| 8011 ||| 8012 ||| 1872 ||| 
2021 ||| positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition. ||| 4439 ||| 4440 ||| 3890 ||| 4441 ||| 
2021 ||| vtgan: semi-supervised retinal image synthesis and disease prediction using vision transformers. ||| 7798 ||| 7799 ||| 7800 ||| 7801 ||| 36589 ||| 7802 ||| 
2020 ||| flow-guided attention networks for video-based person re-identification. ||| 28740 ||| 28741 ||| 28743 ||| 19304 ||| 28744 ||| 5748 ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| 13373 ||| 3430 ||| 13374 ||| 
2019 ||| to the attention of mobile software developers: guess what, test your app! ||| 36590 ||| 33560 ||| 36591 ||| 13151 ||| 
2019 ||| connection sensitive attention u-net for accurate retinal vessel segmentation. ||| 6596 ||| 36592 ||| 5292 ||| 
2018 ||| learning 6-dof grasping and pick-place using attention focus. ||| 22281 ||| 22282 ||| 
2020 ||| modern hopfield networks and attention for immune repertoire classification. ||| 9164 ||| 8758 ||| 277 ||| 9166 ||| 9165 ||| 9167 ||| 9168 ||| 9169 ||| 9170 ||| 9171 ||| 9172 ||| 2101 ||| 9173 ||| 
2021 ||| short and long range relation based spatio-temporal transformer for micro-expression recognition. ||| 36593 ||| 11548 ||| 36594 ||| 33480 ||| 
2020 ||| optimizing transformers with approximate computing for faster, smaller and more accurate nlp models. ||| 36595 ||| 36596 ||| 20514 ||| 20518 ||| 
2019 ||| attention augmented convolutional networks. ||| 2463 ||| 2464 ||| 2466 ||| 2467 ||| 9372 ||| 
2020 ||| reconsider: re-ranking using span-focused cross-attention for open domain question answering. ||| 4895 ||| 4896 ||| 4897 ||| 4898 ||| 
2020 ||| experts and authorities receive disproportionate attention on twitter during the covid-19 crisis. ||| 13296 ||| 13295 ||| 36597 ||| 3831 ||| 36598 ||| 13297 ||| 36599 ||| 36600 ||| 13300 ||| 
2022 ||| hibrids: attention with hierarchical biases for structure-aware long document summarization. ||| 4752 ||| 4754 ||| 
2020 ||| deepsumm - deep code summaries using neural transformer architecture. ||| 36601 ||| 
2020 ||| convolutional hierarchical attention network for query-focused video summarization. ||| 18274 ||| 1306 ||| 18275 ||| 18276 ||| 1081 ||| 
2020 ||| learning oracle attention for high-fidelity face completion. ||| 18724 ||| 18611 ||| 18725 ||| 18726 ||| 1756 ||| 
2022 ||| fourier disentangled space-time attention for aerial video recognition. ||| 34658 ||| 7311 ||| 36602 ||| 36603 ||| 32990 ||| 7315 ||| 
2020 ||| x-lxmert: paint, caption and answer questions with multi-modal transformers. ||| 26454 ||| 8568 ||| 26455 ||| 4765 ||| 24052 ||| 
2021 ||| few-shot segmentation via cycle-consistent transformer. ||| 25216 ||| 8570 ||| 1905 ||| 208 ||| 
2021 ||| the image local autoregressive transformer. ||| 32999 ||| 36604 ||| 2008 ||| 36605 ||| 36606 ||| 4970 ||| 6365 ||| 
2021 ||| covid-19 detection in chest x-ray images using swin-transformer and transformer in transformer. ||| 36607 ||| 36608 ||| 
2020 ||| sudden attention shifts on wikipedia following covid-19 mobility restrictions. ||| 13295 ||| 13296 ||| 13297 ||| 13298 ||| 13299 ||| 13300 ||| 
2021 ||| cvt: introducing convolutions to vision transformers. ||| 2428 ||| 1956 ||| 2429 ||| 2430 ||| 1954 ||| 1957 ||| 241 ||| 
2020 ||| agvnet: attention guided velocity learning for 3d human motion prediction. ||| 14027 ||| 36609 ||| 1235 ||| 
2021 ||| vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers. ||| 26507 ||| 3089 ||| 26508 ||| 
2018 ||| deep attention model for triage of emergency department patients. ||| 9721 ||| 9722 ||| 9723 ||| 9724 ||| 9725 ||| 9726 ||| 9727 ||| 
2021 ||| a sample-based training method for distantly supervised relation extraction with pre-trained transformers. ||| 4012 ||| 4013 ||| 4014 ||| 
2021 ||| attention-based neural beamforming layers for multi-channel speech recognition. ||| 36610 ||| 5089 ||| 12486 ||| 36611 ||| 12610 ||| 36612 ||| 12016 ||| 36613 ||| 
2021 ||| an empirical study of training end-to-end vision-and-language transformers. ||| 4981 ||| 34351 ||| 2044 ||| 292 ||| 3363 ||| 17976 ||| 6064 ||| 1953 ||| 1957 ||| 18020 ||| 8573 ||| 34352 ||| 
2021 ||| pu-transformer: point cloud upsampling transformer. ||| 13619 ||| 2474 ||| 2475 ||| 
2021 ||| instance-based vision transformer for subtyping of papillary renal cell carcinoma in histopathological image. ||| 27516 ||| 27517 ||| 27518 ||| 438 ||| 27519 ||| 16603 ||| 16606 ||| 8850 ||| 399 ||| 
2021 ||| cogview: mastering text-to-image generation via transformers. ||| 36614 ||| 36615 ||| 36616 ||| 36617 ||| 2482 ||| 3031 ||| 25375 ||| 36618 ||| 25154 ||| 8916 ||| 7030 ||| 
2017 ||| product function need recognition via semi-supervised attention network. ||| 17130 ||| 9989 ||| 17131 ||| 1094 ||| 
2021 ||| cotr: correspondence transformer for matching across images. ||| 1706 ||| 2553 ||| 2554 ||| 2555 ||| 2556 ||| 
2020 ||| end-to-end object detection with transformers. ||| 8681 ||| 8682 ||| 2120 ||| 8683 ||| 8684 ||| 8685 ||| 
2020 ||| attention with multiple sources knowledges for covid-19 from ct images. ||| 24797 ||| 36619 ||| 7517 ||| 5003 ||| 5004 ||| 
2019 ||| can neural image captioning be controlled via forced attention? ||| 10283 ||| 10284 ||| 3580 ||| 
2018 ||| st-gan: spatial transformer generative adversarial networks for image compositing. ||| 18720 ||| 18721 ||| 18722 ||| 8780 ||| 18723 ||| 
2020 ||| transformer++. ||| 36620 ||| 36621 ||| 
2021 ||| alignment attention by matching key and query distributions. ||| 9282 ||| 9281 ||| 36622 ||| 36623 ||| 9284 ||| 
2021 ||| revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning. ||| 18928 ||| 18929 ||| 18930 ||| 18931 ||| 
2022 ||| towards practical certifiable patch defense with vision transformer. ||| 36624 ||| 1717 ||| 36625 ||| 3608 ||| 36626 ||| 12503 ||| 
2018 ||| generating attention from classifier activations for fine-grained recognition. ||| 8906 ||| 14489 ||| 
2020 ||| hhh: an online medical chatbot system based on knowledge graph and hierarchical bi-directional attention. ||| 25995 ||| 25996 ||| 25997 ||| 
2020 ||| attentional speech recognition models misbehave on out-of-domain utterances. ||| 36627 ||| 33411 ||| 2576 ||| 4739 ||| 36628 ||| 
2020 ||| relationnet++: bridging visual representations for object detection via transformer decoder. ||| 9278 ||| 9279 ||| 1768 ||| 
2018 ||| multi-granularity hierarchical attention fusion networks for reading comprehension and question answering. ||| 1160 ||| 3669 ||| 3668 ||| 
2021 ||| paenet: a progressive attention-enhanced network for 3d to 2d retinal vessel segmentation. ||| 16668 ||| 16673 ||| 
2020 ||| attention-based transducer for online speech recognition. ||| 379 ||| 36629 ||| 3508 ||| 
2022 ||| local feature matching with transformers for low-end devices. ||| 36630 ||| 
2018 ||| facial landmarks detection by self-iterative regression based landmarks-attention network. ||| 17729 ||| 17730 ||| 17731 ||| 13825 ||| 
2021 ||| psvit: better vision transformer via token pooling and attention sharing. ||| 2384 ||| 2385 ||| 2387 ||| 2386 ||| 2388 ||| 2389 ||| 2390 ||| 2391 ||| 2303 ||| 
2022 ||| contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection. ||| 36631 ||| 36632 ||| 36633 ||| 36634 ||| 31736 ||| 36635 ||| 15389 ||| 18877 ||| 
2018 ||| recurrent attention unit. ||| 2817 ||| 2818 ||| 17149 ||| 
2019 ||| ranet: ranking attention network for fast video object segmentation. ||| 2012 ||| 2013 ||| 2014 ||| 1929 ||| 1932 ||| 
2021 ||| scene-adaptive attention network for crowd counting. ||| 8217 ||| 36636 ||| 36637 ||| 36638 ||| 36639 ||| 36640 ||| 11549 ||| 
2022 ||| transformers meet visual learning understanding: a comprehensive review. ||| 8004 ||| 400 ||| 3534 ||| 5743 ||| 36641 ||| 36642 ||| 6617 ||| 
2022 ||| reshaping robot trajectories using natural language commands: a study of multi-modal data alignment using transformers. ||| 36643 ||| 36644 ||| 36645 ||| 36646 ||| 18947 ||| 36647 ||| 
2021 ||| point cloud learning with transformer. ||| 36120 ||| 36648 ||| 36122 ||| 
2021 ||| hopeful_men@lt-edi-eacl2021: hope speech detection using indic transliteration and transformers. ||| 10509 ||| 36649 ||| 20765 ||| 10510 ||| 
2019 ||| password-conditioned anonymization and deanonymization with face identity transformers. ||| 8699 ||| 8700 ||| 8594 ||| 8701 ||| 
2018 ||| attentionxml: extreme multi-label text classification with multi-label attention based recurrent neural networks. ||| 9174 ||| 9177 ||| 9175 ||| 9178 ||| 9179 ||| 
2019 ||| multi-level attention network using text, audio and video for depression prediction. ||| 19446 ||| 19447 ||| 19448 ||| 17364 ||| 19449 ||| 
2021 ||| grassmannian graph-attentional landmark selection for domain adaptation. ||| 36003 ||| 36650 ||| 36651 ||| 36652 ||| 621 ||| 
2022 ||| cmx: cross-modal fusion for rgb-x semantic segmentation with transformers. ||| 36653 ||| 7856 ||| 7857 ||| 11524 ||| 7861 ||| 
2019 ||| retrosynthesis with attention-based nmt model and chemical analysis of the "wrong" predictions. ||| 36654 ||| 10415 ||| 36655 ||| 19958 ||| 
2018 ||| deepphys: video-based physiological measurement using convolutional attention networks. ||| 8803 ||| 5732 ||| 
2021 ||| transforensics: image forgery localization with dense self-attention. ||| 1933 ||| 1934 ||| 1935 ||| 1936 ||| 1937 ||| 
2021 ||| add: frequency attention and multi-view based knowledge distillation to detect low-quality compressed deepfake images. ||| 36656 ||| 35836 ||| 
2019 ||| in conclusion not repetition: comprehensive abstractive summarization with diversified attention based on determinantal point processes. ||| 3034 ||| 683 ||| 23104 ||| 23105 ||| 23106 ||| 
2019 ||| frequency and temporal convolutional attention for text-independent speaker recognition. ||| 12135 ||| 12136 ||| 
2021 ||| staircase attention for recurrent processing of sequences. ||| 36657 ||| 36658 ||| 3797 ||| 34967 ||| 
2021 ||| transwic at semeval-2021 task 2: transformer-based multilingual and cross-lingual word-in-context disambiguation. ||| 10586 ||| 3849 ||| 
2022 ||| the paradox of choice: using attention in hierarchical reinforcement learning. ||| 36659 ||| 17764 ||| 17765 ||| 
2022 ||| wegformer: transformers for weakly supervised semantic segmentation. ||| 36660 ||| 2007 ||| 23411 ||| 2006 ||| 6535 ||| 2011 ||| 
2018 ||| interpretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling. ||| 770 ||| 771 ||| 772 ||| 773 ||| 774 ||| 775 ||| 776 ||| 
2021 ||| agentformer: agent-aware transformers for socio-temporal multi-agent forecasting. ||| 2296 ||| 2297 ||| 2298 ||| 2299 ||| 
2019 ||| gapnet: graph attention based point neural network for exploiting local feature of point cloud. ||| 36661 ||| 36662 ||| 36663 ||| 
2020 ||| investigation of speaker-adaptation methods in transformer based asr. ||| 12072 ||| 12073 ||| 12074 ||| 
2021 ||| contributions of transformer attention heads in multi- and cross-lingual tasks. ||| 3432 ||| 3433 ||| 3434 ||| 3435 ||| 3436 ||| 
2021 ||| multi-view self-attention based transformer for speaker recognition. ||| 3049 ||| 21351 ||| 21182 ||| 12389 ||| 10844 ||| 21352 ||| 3477 ||| 9472 ||| 
2018 ||| a dual-attention hierarchical recurrent neural network for dialogue act classification. ||| 23086 ||| 23087 ||| 23088 ||| 6821 ||| 23089 ||| 
2020 ||| dual-sampling attention network for diagnosis of covid-19 from community acquired pneumonia. ||| 32718 ||| 15645 ||| 36664 ||| 36665 ||| 1235 ||| 36666 ||| 36667 ||| 36668 ||| 5101 ||| 9705 ||| 26582 ||| 36669 ||| 9191 ||| 36670 ||| 27813 ||| 27538 ||| 577 ||| 18051 ||| 
2021 ||| cnn-based search model underestimates attention guidance by simple visual features. ||| 36671 ||| 22541 ||| 
2020 ||| differentiable window for dynamic local attention. ||| 3486 ||| 3487 ||| 1313 ||| 3488 ||| 
2020 ||| transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. ||| 3489 ||| 375 ||| 
2020 ||| the lipschitz constant of self-attention. ||| 22835 ||| 22845 ||| 22846 ||| 
2020 ||| lip-reading with hierarchical pyramidal convolution and self-attention. ||| 14383 ||| 1010 ||| 14384 ||| 12372 ||| 12404 ||| 14385 ||| 
2021 ||| video swin transformer. ||| 1765 ||| 33059 ||| 1767 ||| 1769 ||| 1770 ||| 1771 ||| 1768 ||| 
2021 ||| strategic mitigation of agent inattention in drivers with open-quantum cognition models. ||| 36672 ||| 36673 ||| 36674 ||| 36675 ||| 
2021 ||| attention-driven graph clustering network. ||| 19424 ||| 17677 ||| 19425 ||| 19426 ||| 
2020 ||| 3d axial-attention for lung nodule classification. ||| 36676 ||| 36677 ||| 35952 ||| 
2021 ||| domain-adversarial training of self-attention based networks for land cover classification using multi-temporal sentinel-2 satellite imagery. ||| 30172 ||| 30173 ||| 30174 ||| 30175 ||| 
2021 ||| dynamic graph representation learning via graph transformer networks. ||| 36678 ||| 22891 ||| 35875 ||| 36679 ||| 36680 ||| 36681 ||| 36682 ||| 
2020 ||| arelu: attention-based rectified linear unit. ||| 36683 ||| 5723 ||| 
2021 ||| tctn: a 3d-temporal convolutional transformer network for spatiotemporal predictive learning. ||| 36684 ||| 36685 ||| 36686 ||| 
2021 ||| spectr: spectral transformer for hyperspectral pathology image segmentation. ||| 36687 ||| 247 ||| 32259 ||| 8656 ||| 8906 ||| 7716 ||| 
2022 ||| vit-p: rethinking data-efficient vision transformers from locality. ||| 12719 ||| 36688 ||| 36689 ||| 34012 ||| 
2022 ||| optimal-er auctions through attention. ||| 36690 ||| 36691 ||| 36692 ||| 36693 ||| 
2019 ||| fine-grained attention and feature-sharing generative adversarial networks for single image super-resolution. ||| 36694 ||| 36695 ||| 17231 ||| 36696 ||| 36697 ||| 36698 ||| 
2019 ||| attention-based fusion for multi-source human image generation. ||| 2693 ||| 7349 ||| 7350 ||| 7351 ||| 7352 ||| 437 ||| 
2022 ||| transbtsv2: wider instead of deeper transformer for medical image segmentation. ||| 27542 ||| 11656 ||| 2230 ||| 30394 ||| 27539 ||| 304 ||| 4550 ||| 
2021 ||| neuromorphic camera denoising using graph neural network-driven transformers. ||| 36699 ||| 36700 ||| 36701 ||| 36702 ||| 36703 ||| 36704 ||| 
2019 ||| modeling graph structure in transformer for better amr-to-text generation. ||| 12282 ||| 3564 ||| 21137 ||| 21126 ||| 1254 ||| 3088 ||| 
2018 ||| a survey of attention management systems in ubiquitous computing environments. ||| 36705 ||| 36706 ||| 36707 ||| 36708 ||| 36709 ||| 36710 ||| 22996 ||| 
2021 ||| patchformer: a versatile 3d transformer based on patch attention. ||| 36711 ||| 36712 ||| 36713 ||| 36714 ||| 
2018 ||| cross-topic argument mining from heterogeneous sources using attention-based neural networks. ||| 36715 ||| 36716 ||| 3702 ||| 
2021 ||| transformer meets tracker: exploiting temporal context for robust visual tracking. ||| 6003 ||| 1806 ||| 5894 ||| 1807 ||| 
2019 ||| an actor-critic-attention mechanism for deep reinforcement learning in multi-view environments. ||| 3924 ||| 3925 ||| 
2021 ||| spatiotemporal transformer for video-based person re-identification. ||| 28341 ||| 2480 ||| 2477 ||| 11426 ||| 26558 ||| 1717 ||| 2398 ||| 
2021 ||| transformer tracking. ||| 19066 ||| 1697 ||| 19067 ||| 952 ||| 19068 ||| 1700 ||| 
2021 ||| r2d2: recursive transformer based on differentiable tree for interpretable hierarchical language modeling. ||| 3768 ||| 3769 ||| 3770 ||| 3771 ||| 3772 ||| 3773 ||| 3774 ||| 
2021 ||| vtamiq: transformers for attention modulated image quality assessment. ||| 36717 ||| 28019 ||| 
2021 ||| learning defense transformers for counterattacking adversarial examples. ||| 36718 ||| 7812 ||| 2341 ||| 9375 ||| 6413 ||| 
2020 ||| tabular transformers for modeling multivariate time series. ||| 12450 ||| 12451 ||| 12452 ||| 12453 ||| 12454 ||| 12455 ||| 12456 ||| 12457 ||| 12458 ||| 
2018 ||| deephttp: semantics-structure model with attention for anomalous http traffic detection and pattern mining. ||| 36719 ||| 36720 ||| 36721 ||| 2736 ||| 
2020 ||| document modeling with graph attention networks for multi-grained machine reading comprehension. ||| 3703 ||| 3704 ||| 3705 ||| 3706 ||| 3707 ||| 3708 ||| 3480 ||| 3311 ||| 
2020 ||| residual network based direct synthesis of em structures: a study on one-to-one transformers. ||| 36722 ||| 35146 ||| 35148 ||| 185 ||| 36723 ||| 22811 ||| 300 ||| 
2021 ||| s2s-ft: fine-tuning pretrained transformer encoders for sequence-to-sequence learning. ||| 3498 ||| 3171 ||| 3497 ||| 9413 ||| 3174 ||| 
2018 ||| reducing visual confusion with discriminative attention. ||| 1743 ||| 1744 ||| 1745 ||| 1746 ||| 1747 ||| 
2022 ||| isda: position-aware instance segmentation with deformable attention. ||| 36724 ||| 18902 ||| 5030 ||| 36725 ||| 
2019 ||| unsupervised community detection with modularity-based attention model. ||| 36726 ||| 36727 ||| 
2021 ||| csformer: bridging convolution and transformer for compressive sensing. ||| 36728 ||| 19725 ||| 7656 ||| 12196 ||| 19391 ||| 19728 ||| 
2020 ||| : accelerating attention mechanisms in neural networks with approximation. ||| 2718 ||| 8317 ||| 8318 ||| 8319 ||| 8320 ||| 8321 ||| 8322 ||| 8323 ||| 8324 ||| 2724 ||| 8325 ||| 
2019 ||| music theme recognition using cnn and self-attention. ||| 4037 ||| 4038 ||| 
2017 ||| attention-based multimodal fusion for video description. ||| 2507 ||| 2508 ||| 2509 ||| 2513 ||| 2511 ||| 2512 ||| 
2019 ||| attention-based multi-input deep learning architecture for biological activity prediction: an application in egfr inhibitors. ||| 7504 ||| 7505 ||| 
2019 ||| outcome-driven clustering of acute coronary syndrome patients using multi-task neural network with attention. ||| 11064 ||| 11065 ||| 11066 ||| 11067 ||| 11068 ||| 11069 ||| 11070 ||| 595 ||| 11071 ||| 11072 ||| 11073 ||| 
2021 ||| transformer-based map matching model with limited ground-truth data using transfer-learning approach. ||| 36729 ||| 15124 ||| 15126 ||| 
2021 ||| transformers in vision: a survey. ||| 1969 ||| 32716 ||| 32792 ||| 35734 ||| 1972 ||| 1752 ||| 
2021 ||| transformer-based language models for factoid question answering at bioasq9b. ||| 10716 ||| 10717 ||| 
2021 ||| bio-inspired representation learning for visual attention prediction. ||| 6650 ||| 35911 ||| 8002 ||| 
2020 ||| cascaded semantic and positional self-attention network for document classification. ||| 26288 ||| 1134 ||| 3433 ||| 
2021 ||| encoder-decoder with multi-level attention for 3d human shape and pose estimation. ||| 2451 ||| 2452 ||| 2453 ||| 2454 ||| 1944 ||| 1848 ||| 
2022 ||| inverted pyramid multi-task transformer for dense scene understanding. ||| 36730 ||| 436 ||| 
2021 ||| multi-scale vision longformer: a new vision transformer for high-resolution image encoding. ||| 1953 ||| 1954 ||| 1955 ||| 1956 ||| 1957 ||| 241 ||| 1958 ||| 
2021 ||| deeprare: generic unsupervised visual attention models. ||| 35560 ||| 11564 ||| 11565 ||| 36731 ||| 
2020 ||| an effective automatic image annotation model via attention model and data equilibrium. ||| 36732 ||| 36733 ||| 36734 ||| 
2022 ||| a transformer-based feature segmentation and region alignment method for uav-view geo-localization. ||| 36735 ||| 36736 ||| 36737 ||| 36738 ||| 
2022 ||| an end-to-end transformer model for crowd localization. ||| 36739 ||| 7804 ||| 17429 ||| 
2018 ||| few-shot learning with attention-based sequence-to-sequence models. ||| 36740 ||| 11996 ||| 
2021 ||| ode transformer: an ordinary differential equation-inspired model for neural machine translation. ||| 3305 ||| 17678 ||| 614 ||| 36741 ||| 8423 ||| 2333 ||| 3306 ||| 
2017 ||| polar transformer networks. ||| 23961 ||| 23962 ||| 19082 ||| 23963 ||| 
2018 ||| tri-axial self-attention for concurrent activity recognition. ||| 2418 ||| 2419 ||| 36742 ||| 36743 ||| 3630 ||| 2425 ||| 
2020 ||| sact: self-aware multi-space feature composition transformer for multinomial attention for video captioning. ||| 36744 ||| 
2021 ||| connecting what to say with where to look by modeling human attention traces. ||| 19261 ||| 18765 ||| 3402 ||| 3809 ||| 19262 ||| 18033 ||| 19263 ||| 
2020 ||| the devil is in the details: evaluating limitations of transformer-based methods for granular tasks. ||| 11764 ||| 11765 ||| 11766 ||| 3400 ||| 
2019 ||| bp-transformer: modelling long-range context via binary partitioning. ||| 36745 ||| 4967 ||| 36746 ||| 3272 ||| 1770 ||| 
2021 ||| small in-distribution changes in 3d perspective and lighting fool both cnns and transformers. ||| 36747 ||| 36127 ||| 36748 ||| 36126 ||| 1735 ||| 
2022 ||| transformer based ensemble for emotion detection. ||| 36749 ||| 36750 ||| 36751 ||| 36752 ||| 
2017 ||| from attention to participation: reviewing and modelling engagement with computers. ||| 36753 ||| 
2021 ||| audio-visual transformer based crowd counting. ||| 388 ||| 7952 ||| 7953 ||| 391 ||| 392 ||| 
2019 ||| probing representations learned by multimodal recurrent and transformer models. ||| 3591 ||| 36754 ||| 
2019 ||| smiles transformer: pre-trained molecular fingerprint for low data drug discovery. ||| 36755 ||| 36756 ||| 36757 ||| 
2021 ||| dual-branch attention-in-attention transformer for single-channel speech enhancement. ||| 4382 ||| 14674 ||| 4383 ||| 36758 ||| 1341 ||| 4384 ||| 
2021 ||| transformer models for text coherence assessment. ||| 36759 ||| 36760 ||| 1186 ||| 1185 ||| 
2019 ||| a weakly-supervised attention-based visualization tool for assessing political affiliation. ||| 36761 ||| 36762 ||| 36763 ||| 
2021 ||| towards accurate and compact architectures via neural architecture transformer. ||| 9373 ||| 9374 ||| 6413 ||| 6415 ||| 5145 ||| 9375 ||| 9346 ||| 1265 ||| 
2017 ||| unwritten languages demand attention too! word discovery with encoder-decoder models. ||| 19 ||| 13942 ||| 13943 ||| 3510 ||| 
2020 ||| speech enhancement using self-adaptation and multi-head self-attention. ||| 8065 ||| 12083 ||| 1491 ||| 12084 ||| 12085 ||| 
2021 ||| pose transformers (potr): human motion prediction with non-autoregressive transformers. ||| 8046 ||| 8047 ||| 8048 ||| 8049 ||| 8050 ||| 
2021 ||| segmentation of lungs covid infected regions by attention mechanism and synthetic data. ||| 36764 ||| 36765 ||| 36766 ||| 36767 ||| 36768 ||| 36769 ||| 
2018 ||| cross-target stance classification with self-attention networks. ||| 3156 ||| 3157 ||| 3158 ||| 3159 ||| 3160 ||| 
2018 ||| decoding-history-based adaptive control of attention for neural machine translation. ||| 25375 ||| 10200 ||| 16564 ||| 3751 ||| 
2021 ||| blending anti-aliasing into vision transformer. ||| 36770 ||| 36771 ||| 2422 ||| 3046 ||| 2204 ||| 
2021 ||| semask: semantically masked transformers for semantic segmentation. ||| 36772 ||| 35045 ||| 36773 ||| 2218 ||| 2269 ||| 36774 ||| 33290 ||| 
2021 ||| hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. ||| 19356 ||| 19357 ||| 19358 ||| 5845 ||| 7141 ||| 18823 ||| 2011 ||| 
2021 ||| pointer over attention: an improved bangla text summarization approach using hybrid pointer generator network. ||| 36775 ||| 36776 ||| 36777 ||| 36778 ||| 36779 ||| 
2020 ||| robust hierarchical graph classification with subgraph attention. ||| 15179 ||| 15180 ||| 7043 ||| 
2021 ||| primer: searching for efficient transformers for language modeling. ||| 22752 ||| 36780 ||| 3244 ||| 3780 ||| 9132 ||| 9372 ||| 
2022 ||| singing-tacotron: global duration control attention and dynamic filter for end-to-end singing voice synthesis. ||| 128 ||| 12492 ||| 12242 ||| 12041 ||| 12244 ||| 
2020 ||| molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. ||| 36781 ||| 36782 ||| 36783 ||| 36784 ||| 34181 ||| 
2022 ||| transformers in medical image analysis: a review. ||| 8773 ||| 36785 ||| 36786 ||| 36787 ||| 36788 ||| 8772 ||| 5089 ||| 577 ||| 5745 ||| 18051 ||| 
2020 ||| evaluating german transformer language models with syntactic agreement tests. ||| 27136 ||| 27137 ||| 27138 ||| 27139 ||| 12198 ||| 3831 ||| 
2020 ||| ttvos: lightweight video object segmentation with adaptive template attention module and temporal consistency loss. ||| 36789 ||| 36790 ||| 36791 ||| 
2021 ||| efficient speech emotion recognition using multi-scale cnn and attention. ||| 12436 ||| 3042 ||| 12437 ||| 12438 ||| 
2022 ||| rxn hypergraph: a hypergraph attention model for chemical reaction representation. ||| 34878 ||| 33109 ||| 36792 ||| 4311 ||| 
2017 ||| focusing attention: towards accurate text recognition in natural images. ||| 2609 ||| 2610 ||| 2611 ||| 2612 ||| 1937 ||| 2613 ||| 
2020 ||| hitter: hierarchical transformers for knowledge graph embeddings. ||| 26584 ||| 24050 ||| 1958 ||| 4816 ||| 4817 ||| 26585 ||| 
2019 ||| dual attention mobdensenet(damdnet) for robust 3d face alignment. ||| 479 ||| 7853 ||| 6525 ||| 
2019 ||| attention-aware answers of the crowd. ||| 9748 ||| 9749 ||| 1224 ||| 9750 ||| 9751 ||| 
2021 ||| multi-airport delay prediction with transformers. ||| 36200 ||| 36793 ||| 36794 ||| 
2021 ||| dependency learning for legal judgment prediction with a unified text-to-text transformer. ||| 36795 ||| 3876 ||| 3874 ||| 3875 ||| 382 ||| 
2020 ||| accenture at checkthat! 2020: if you say so: post-hoc fact-checking of claims using transformer-based models. ||| 10665 ||| 10666 ||| 10667 ||| 
2019 ||| self-attentional models for lattice inputs. ||| 3516 ||| 3067 ||| 3517 ||| 3518 ||| 
2022 ||| uni4eye: unified 2d and 3d self-supervised pre-training via masked image modeling transformer for ophthalmic image classification. ||| 36796 ||| 24342 ||| 36797 ||| 24344 ||| 
2020 ||| temporal event segmentation using attention-based perceptual prediction model for continual learning. ||| 36798 ||| 36799 ||| 4194 ||| 36800 ||| 36801 ||| 
2019 ||| improving adversarial robustness via attention and adversarial logit pairing. ||| 36802 ||| 23983 ||| 23988 ||| 36803 ||| 
2018 ||| a3net: adversarial-and-attention network for machine reading comprehension. ||| 19478 ||| 21131 ||| 21132 ||| 21133 ||| 945 ||| 4783 ||| 21134 ||| 
2020 ||| the cascade transformer: an application for efficient answer sentence selection. ||| 3373 ||| 3374 ||| 
2021 ||| dynamic head: unifying object detection heads with attentions. ||| 1954 ||| 1959 ||| 1956 ||| 2494 ||| 2430 ||| 1957 ||| 241 ||| 
2022 ||| transformer quality in linear time. ||| 36804 ||| 3780 ||| 3244 ||| 9372 ||| 
2020 ||| kalman filtering attention for user behavior modeling in ctr prediction. ||| 5525 ||| 811 ||| 9405 ||| 9406 ||| 9407 ||| 9408 ||| 9383 ||| 595 ||| 9409 ||| 1171 ||| 1170 ||| 
2017 ||| efficient attention using a fixed-size memory representation. ||| 26621 ||| 26622 ||| 22748 ||| 
2020 ||| modality shifting attention network for multi-modal video question answering. ||| 11230 ||| 18673 ||| 18674 ||| 18675 ||| 11231 ||| 
2021 ||| feanet: feature-enhanced attention network for rgb-thermal real-time semantic segmentation. ||| 25526 ||| 25527 ||| 25528 ||| 25529 ||| 25530 ||| 5519 ||| 25531 ||| 25532 ||| 25533 ||| 21847 ||| 
2019 ||| fully quantized transformer for improved translation. ||| 26846 ||| 26696 ||| 14630 ||| 
2019 ||| bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer. ||| 14547 ||| 14548 ||| 
2022 ||| swin unetr: swin transformers for semantic segmentation of brain tumors in mri images. ||| 7205 ||| 7207 ||| 7206 ||| 2019 ||| 34009 ||| 2024 ||| 
2020 ||| text-to-image generation with attention based recurrent neural networks. ||| 36805 ||| 36806 ||| 36807 ||| 36808 ||| 
2020 ||| dapnet: a double self-attention convolutional network for segmentation of point clouds. ||| 3036 ||| 36809 ||| 36810 ||| 36811 ||| 36812 ||| 12801 ||| 
2021 ||| classifying tweet sentiment using the hidden state and attention matrix of a fine-tuned bertweet model. ||| 36813 ||| 36814 ||| 36815 ||| 36816 ||| 
2021 ||| video transformer for deepfake detection with incremental learning. ||| 19712 ||| 8582 ||| 
2022 ||| plumeria at semeval-2022 task 6: robust approaches for sarcasm detection for english and arabic using transformers and data augmentation. ||| 36817 ||| 36818 ||| 
2019 ||| why attention? analyzing and remedying bilstm deficiency in modeling cross-context for ner. ||| 
2019 ||| linestofacephoto: face photo generation from lines with conditional self-attention generative adversarial network. ||| 19730 ||| 17859 ||| 8711 ||| 8710 ||| 
2021 ||| semi-supervised medical image segmentation via cross teaching between cnn and transformer. ||| 23341 ||| 36819 ||| 20159 ||| 15623 ||| 15555 ||| 
2021 ||| geometric transformer for end-to-end molecule properties prediction. ||| 36820 ||| 1667 ||| 
2019 ||| multi-grained attention networks for single image super-resolution. ||| 28822 ||| 2185 ||| 28823 ||| 28824 ||| 12749 ||| 1796 ||| 9062 ||| 6782 ||| 
2019 ||| the bottom-up evolution of representations in the transformer: a study with machine translation and language modeling objectives. ||| 3844 ||| 3847 ||| 3848 ||| 
2020 ||| dilated-scale-aware attention convnet for multi-class object counting. ||| 7804 ||| 36739 ||| 36821 ||| 1484 ||| 
2022 ||| actionformer: localizing moments of actions with transformers. ||| 6333 ||| 1863 ||| 8766 ||| 
2017 ||| "attention" for detecting unreliable news in the information age. ||| 17742 ||| 
2021 ||| learning a 3d-cnn and transformer prior for hyperspectral image super-resolution. ||| 36822 ||| 12056 ||| 12058 ||| 6863 ||| 
2020 ||| sign language translation with transformers. ||| 3546 ||| 
2017 ||| setting an attention region for convolutional neural networks using region selective features, for recognition of materials within glass vessels. ||| 34163 ||| 
2021 ||| demystifying local vision transformer: sparse connectivity, weight sharing, and dynamic weight. ||| 36823 ||| 36824 ||| 6545 ||| 7190 ||| 1904 ||| 18279 ||| 18793 ||| 
2018 ||| visual attention model for cross-sectional stock return prediction and end-to-end multimodal market representation learning. ||| 3943 ||| 3944 ||| 3945 ||| 3946 ||| 3947 ||| 3948 ||| 
2020 ||| transformer-based models for automatic identification of argument relations: a cross-domain evaluation. ||| 36825 ||| 36826 ||| 852 ||| 36827 ||| 36828 ||| 36829 ||| 
2019 ||| stand-alone self-attention in vision models. ||| 9433 ||| 9133 ||| 2466 ||| 2463 ||| 9434 ||| 2467 ||| 
2020 ||| segmentation with residual attention u-net and an edge-enhancement approach preserves cell shape features. ||| 36830 ||| 4811 ||| 36831 ||| 36832 ||| 35768 ||| 5932 ||| 
2022 ||| self-supervised video-centralised transformer for video face clustering. ||| 28703 ||| 36833 ||| 20051 ||| 36834 ||| 28702 ||| 25661 ||| 5717 ||| 5719 ||| 
2021 ||| robust and precise facial landmark detection by self-calibrated pose attention network. ||| 13439 ||| 36835 ||| 1921 ||| 36836 ||| 36837 ||| 8012 ||| 36838 ||| 
2020 ||| hierarchical residual attention network for single image super-resolution. ||| 36839 ||| 8668 ||| 6235 ||| 36840 ||| 36841 ||| 36842 ||| 36843 ||| 8672 ||| 8048 ||| 
2019 ||| attention is not not explanation. ||| 26829 ||| 26830 ||| 
2020 ||| exploring self-attention for visual odometry. ||| 36844 ||| 36845 ||| 36846 ||| 
2021 ||| u-shaped transformer with frequency-band aware attention for speech enhancement. ||| 1329 ||| 9678 ||| 36847 ||| 
2021 ||| memory-efficient differentiable transformer architecture search. ||| 3170 ||| 3171 ||| 3172 ||| 3173 ||| 3174 ||| 3175 ||| 
2021 ||| transformer-s2a: robust and efficient speech-to-animation. ||| 36848 ||| 3138 ||| 36849 ||| 4458 ||| 14507 ||| 12137 ||| 
2019 ||| acfnet: attentional class feature network for semantic segmentation. ||| 2532 ||| 2533 ||| 2534 ||| 2535 ||| 2536 ||| 2537 ||| 2538 ||| 1761 ||| 
2017 ||| multi-modal factorized bilinear pooling with co-attention learning for visual question answering. ||| 1753 ||| 1754 ||| 1755 ||| 1756 ||| 
2020 ||| self-attention enhanced patient journey understanding in healthcare system. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 
2019 ||| the unreasonable effectiveness of transformer language models in grammatical error correction. ||| 16981 ||| 4869 ||| 
2020 ||| simultaneous paraphrasing and translation by fine-tuning transformer models. ||| 3052 ||| 
2021 ||| cross attention-guided dense network for images fusion. ||| 36850 ||| 1224 ||| 36851 ||| 36852 ||| 36853 ||| 
2022 ||| poseur: direct human pose regression with transformers. ||| 33402 ||| 33403 ||| 6335 ||| 15886 ||| 18945 ||| 33404 ||| 5177 ||| 
2021 ||| waveglove: transformer-based hand gesture recognition using multiple inertial sensors. ||| 8267 ||| 8268 ||| 8269 ||| 
2021 ||| transfusion: cross-view fusion with transformer for 3d human pose estimation. ||| 7410 ||| 32425 ||| 7411 ||| 7436 ||| 7133 ||| 435 ||| 7409 ||| 2643 ||| 36854 ||| 7135 ||| 
2021 ||| sparse is enough in scaling transformers. ||| 36855 ||| 36856 ||| 24039 ||| 9135 ||| 36857 ||| 35917 ||| 36858 ||| 
2019 ||| audio-attention discriminative language model for asr rescoring. ||| 3843 ||| 3841 ||| 
2021 ||| video transformer network. ||| 7841 ||| 7842 ||| 7843 ||| 7844 ||| 
2020 ||| a graph attention based approach for trajectory prediction in multi-agent sports games. ||| 36859 ||| 36860 ||| 
2020 ||| hard-coded gaussian attention for neural machine translation. ||| 3424 ||| 3425 ||| 3325 ||| 
2021 ||| pe-former: pose estimation transformer. ||| 36861 ||| 36862 ||| 
2017 ||| monotonic chunkwise attention. ||| 3334 ||| 3338 ||| 
2018 ||| modeling task effects in human reading with neural attention. ||| 35699 ||| 13068 ||| 
2019 ||| synchronous transformers for end-to-end speech recognition. ||| 12241 ||| 12242 ||| 12243 ||| 12041 ||| 3364 ||| 12244 ||| 
2022 ||| interpretable and generalizable graph learning via stochastic attention mechanism. ||| 36863 ||| 36864 ||| 4725 ||| 
2022 ||| minicons: enabling flexible behavioral and representational analyses of transformer language models. ||| 36865 ||| 
2017 ||| 3d morphable models as spatial transformer networks. ||| 8005 ||| 8006 ||| 8007 ||| 8008 ||| 6525 ||| 
2020 ||| guiding attention for self-supervised learning with transformers. ||| 26746 ||| 3651 ||| 
2021 ||| legal transformer models may not always help. ||| 36866 ||| 59 ||| 36867 ||| 36868 ||| 
2021 ||| atp-net: an attention-based ternary projection network for compressed sensing. ||| 36869 ||| 36870 ||| 
2020 ||| sg-net: syntax guided transformer for language representation. ||| 34203 ||| 36871 ||| 13594 ||| 13593 ||| 3111 ||| 3049 ||| 
2021 ||| attention-guided progressive mapping for profile face recognition. ||| 18610 ||| 18611 ||| 
2021 ||| abd-net: attention based decomposition network for 3d point cloud decomposition. ||| 8037 ||| 8038 ||| 8039 ||| 8040 ||| 8041 ||| 
2021 ||| multiscale vision transformers. ||| 2025 ||| 2026 ||| 2027 ||| 2028 ||| 2029 ||| 2030 ||| 2031 ||| 
2021 ||| evo-vit: slow-fast token evolution for dynamic vision transformer. ||| 1813 ||| 6925 ||| 32115 ||| 19414 ||| 1424 ||| 19415 ||| 5088 ||| 1175 ||| 32950 ||| 
2019 ||| kgat: knowledge graph attention network for recommendation. ||| 1894 ||| 1063 ||| 3603 ||| 6736 ||| 3605 ||| 
2021 ||| classification of multivariate weakly-labelled time-series with attention. ||| 36872 ||| 36873 ||| 
2021 ||| multimodal personality recognition using cross-attention transformer and behaviour encoding. ||| 16385 ||| 16386 ||| 16387 ||| 16388 ||| 1226 ||| 7320 ||| 7321 ||| 
2021 ||| attention guided dialogue state tracking with sparse supervision. ||| 36874 ||| 36875 ||| 36876 ||| 
2020 ||| neural machine translation system of indic languages - an attention based approach. ||| 36877 ||| 36878 ||| 
2018 ||| multilingual neural machine translation with task-specific attention. ||| 11711 ||| 11712 ||| 11713 ||| 
2021 ||| bilateral cross-modality graph matching attention for feature fusion in visual question answering. ||| 36879 ||| 17413 ||| 6456 ||| 2445 ||| 
2020 ||| attention-based image upsampling. ||| 7327 ||| 36880 ||| 36349 ||| 12166 ||| 
2021 ||| sparse attention with linear units. ||| 3180 ||| 3848 ||| 3847 ||| 
2021 ||| attention-based domain adaptation for time series forecasting. ||| 9265 ||| 36881 ||| 36882 ||| 19932 ||| 3801 ||| 
2021 ||| attention augmented convolutional transformer for tabular time-series. ||| 21227 ||| 21228 ||| 
2019 ||| attention-based lane change prediction. ||| 21869 ||| 21870 ||| 21871 ||| 13628 ||| 19260 ||| 
2021 ||| simpler is better: few-shot semantic segmentation with classifier weight transformer. ||| 2194 ||| 2195 ||| 2196 ||| 254 ||| 2197 ||| 2198 ||| 
2022 ||| attention-based contextual multi-view graph convolutional networks for short-term population prediction. ||| 36883 ||| 36884 ||| 36885 ||| 
2020 ||| inductive document network embedding with topic-word attention. ||| 8939 ||| 8940 ||| 8941 ||| 
2021 ||| sequential recommendation with bidirectional chronological augmentation of transformer. ||| 26288 ||| 8966 ||| 36886 ||| 3433 ||| 32503 ||| 
2021 ||| bottleneck transformers for visual recognition. ||| 18848 ||| 18849 ||| 9133 ||| 2467 ||| 18850 ||| 2466 ||| 
2022 ||| attention enables zero approximation error. ||| 36887 ||| 36888 ||| 36889 ||| 36890 ||| 
2018 ||| overcoming catastrophic forgetting with hard attention to the task. ||| 11846 ||| 15326 ||| 36891 ||| 7111 ||| 22724 ||| 22725 ||| 
2021 ||| a bilingual, openworld video text dataset and end-to-end video text spotter with transformer. ||| 35466 ||| 19505 ||| 35467 ||| 36892 ||| 36893 ||| 36894 ||| 36895 ||| 35468 ||| 
2019 ||| attributed graph clustering: a deep attentional embedding approach. ||| 23474 ||| 799 ||| 23475 ||| 802 ||| 800 ||| 4873 ||| 
2021 ||| structured co-reference graph attention for video-grounded dialogue. ||| 11230 ||| 18185 ||| 18186 ||| 11231 ||| 
2019 ||| solving math word problems with double-decoder transformer. ||| 36896 ||| 3458 ||| 
2020 ||| covid-transformer: detecting covid-19 trending topics on twitter using universal sentence encoder. ||| 35645 ||| 35230 ||| 17204 ||| 
2021 ||| attention-driven read-aloud technology increases reading comprehension in children with reading disabilities. ||| 36897 ||| 36898 ||| 36899 ||| 9808 ||| 36900 ||| 
2019 ||| graph-based knowledge distillation by multi-head attention network. ||| 36901 ||| 21489 ||| 
2021 ||| reltransformer: balancing the visual relationship detection from local context, scene and memory. ||| 1785 ||| 36902 ||| 36903 ||| 36904 ||| 7361 ||| 
2019 ||| r-transformer: recurrent neural network enhanced transformer. ||| 8084 ||| 36905 ||| 8087 ||| 8085 ||| 
2021 ||| deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. ||| 36906 ||| 36907 ||| 35065 ||| 16602 ||| 17677 ||| 
2020 ||| do we really need that many parameters in transformer for extractive summarization? discourse can help ! ||| 4793 ||| 4794 ||| 4795 ||| 
2018 ||| leveraging financial news for stock trend prediction with attention-based recurrent neural network. ||| 36908 ||| 
2019 ||| enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. ||| 9264 ||| 9265 ||| 9266 ||| 9267 ||| 3799 ||| 9268 ||| 3801 ||| 
2020 ||| $-i relation of single phase power transformer core by using on-line measurable parameters. ||| 36909 ||| 
2021 ||| few-shot fine-grained action recognition via bidirectional attention and contrastive meta-learning. ||| 19553 ||| 5705 ||| 2794 ||| 10426 ||| 
2017 ||| hierarchical multi-scale attention networks for action recognition. ||| 13 ||| 11226 ||| 11227 ||| 11228 ||| 
2020 ||| pre-trained image processing transformer. ||| 19361 ||| 19362 ||| 19363 ||| 3156 ||| 19364 ||| 19365 ||| 19366 ||| 1688 ||| 19367 ||| 7204 ||| 
2022 ||| vision transformer compression with structured pruning and low rank approximation. ||| 13903 ||| 
2018 ||| learning visual question answering by bootstrapping hard attention. ||| 8785 ||| 8786 ||| 8787 ||| 8788 ||| 
2019 ||| faclstm: convlstm with focused attention for scene text recognition. ||| 17340 ||| 17341 ||| 5123 ||| 17342 ||| 2730 ||| 17343 ||| 
2019 ||| hierarchical attentional hybrid neural networks for document classification. ||| 4145 ||| 4146 ||| 280 ||| 281 ||| 282 ||| 
2022 ||| sequential recommendation via stochastic self-attention. ||| 1427 ||| 1428 ||| 3906 ||| 36910 ||| 36911 ||| 1429 ||| 9407 ||| 1094 ||| 
2017 ||| recurrent neural network-based sentence encoder with gated attention for natural language inference. ||| 12760 ||| 26004 ||| 4894 ||| 3644 ||| 26005 ||| 26006 ||| 
2020 ||| attention! a lightweight 2d hand pose estimation approach. ||| 36912 ||| 36913 ||| 36914 ||| 36915 ||| 36916 ||| 
2020 ||| tinyspeech: attention condensers for deep speech recognition neural networks on edge devices. ||| 7865 ||| 36917 ||| 27912 ||| 36918 ||| 
2021 ||| from multimodal to unimodal attention in transformers using knowledge distillation. ||| 16386 ||| 16385 ||| 24064 ||| 1226 ||| 7320 ||| 7321 ||| 
2021 ||| latte: lstm self-attention based anomaly detection in embedded automotive platforms. ||| 7490 ||| 7489 ||| 7491 ||| 
2022 ||| locate: end-to-end localization of actions in 3d with transformers. ||| 33251 ||| 2373 ||| 2100 ||| 36919 ||| 
2018 ||| dynamic graph representation learning via self-attention networks. ||| 22890 ||| 22891 ||| 22892 ||| 781 ||| 2792 ||| 
2019 ||| are transformers universal approximators of sequence-to-sequence functions? ||| 9155 ||| 2567 ||| 9157 ||| 9158 ||| 9159 ||| 
2021 ||| the layout generation algorithm of graphic design based on transformer-cvae. ||| 36920 ||| 36921 ||| 11405 ||| 
2021 ||| a dynamic spatial-temporal attention network for early anticipation of traffic accidents. ||| 36922 ||| 11641 ||| 24983 ||| 24982 ||| 
2018 ||| supersaliency: predicting smooth pursuit-based attention with slicing cnns improves fixation prediction for naturalistic videos. ||| 36923 ||| 36924 ||| 
2021 ||| when attention meets fast recurrence: training language models with reduced compute. ||| 19869 ||| 
2021 ||| hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition. ||| 36925 ||| 36926 ||| 36927 ||| 
2019 ||| formula transformers and combinatorial test generators for propositional intuitionistic theorem provers. ||| 36928 ||| 
2020 ||| multi-modal, multi-task, multi-attention (m3) deep learning detection of reticular pseudodrusen: towards automated and accessible classification of age-related macular degeneration. ||| 36929 ||| 36930 ||| 36931 ||| 21585 ||| 36932 ||| 3882 ||| 36933 ||| 36934 ||| 36935 ||| 36936 ||| 36937 ||| 36938 ||| 36939 ||| 36940 ||| 36941 ||| 19768 ||| 36942 ||| 21587 ||| 
2017 ||| attention-based wav2text with feature transfer learning. ||| 12303 ||| 13907 ||| 11757 ||| 
2021 ||| malware analysis with artificial intelligence and a particular attention on results interpretability. ||| 15420 ||| 15421 ||| 15422 ||| 
2021 ||| a multi-size neural network with attention mechanism for answer selection. ||| 23312 ||| 
2020 ||| self-supervised learning with cross-modal transformers for emotion recognition. ||| 3723 ||| 3722 ||| 3724 ||| 
2020 ||| sentiment analysis with contextual embeddings and self-attention. ||| 11923 ||| 11924 ||| 11925 ||| 
2021 ||| latr: layout-aware transformer for scene-text vqa. ||| 36943 ||| 19160 ||| 2643 ||| 2640 ||| 2644 ||| 
2018 ||| pay attention to virality: understanding popularity of social media videos with the attention mechanism. ||| 19334 ||| 19335 ||| 
2021 ||| learning to ignore: rethinking attention in cnns. ||| 36944 ||| 36945 ||| 36946 ||| 926 ||| 31739 ||| 
2018 ||| fast directional self-attention mechanism. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 4873 ||| 
2021 ||| gtae: graph-transformer based auto-encoders for linguistic-constrained text style transfer. ||| 19224 ||| 10418 ||| 36947 ||| 1686 ||| 36948 ||| 2315 ||| 
2021 ||| tedge-caching: transformer-based edge caching towards 6g networks. ||| 36949 ||| 33528 ||| 34774 ||| 33524 ||| 36950 ||| 33527 ||| 
2020 ||| exploring fluent query reformulations with text-to-text transformers and reinforcement learning. ||| 36951 ||| 32325 ||| 2277 ||| 
2018 ||| structured attention guided convolutional neural fields for monocular depth estimation. ||| 436 ||| 1160 ||| 435 ||| 2519 ||| 437 ||| 2524 ||| 
2021 ||| perceiver: general perception with iterative attention. ||| 22757 ||| 22758 ||| 36952 ||| 1997 ||| 22760 ||| 1994 ||| 1995 ||| 
2020 ||| single image super-resolution via a holistic attention network. ||| 8619 ||| 8620 ||| 8621 ||| 8622 ||| 8623 ||| 8624 ||| 8625 ||| 8626 ||| 7064 ||| 
2020 ||| extractive opinion summarization in quantized transformer spaces. ||| 36953 ||| 3713 ||| 36954 ||| 36955 ||| 3408 ||| 
2021 ||| detectornet: transformer-enhanced spatial temporal graph neural network for traffic prediction. ||| 8440 ||| 8441 ||| 8442 ||| 8443 ||| 8444 ||| 8445 ||| 8446 ||| 8447 ||| 8448 ||| 
2018 ||| investigation of enhanced tacotron text-to-speech synthesis systems with self-attention for pitch accent language. ||| 12495 ||| 398 ||| 12496 ||| 3317 ||| 
2021 ||| realtime global attention network for semantic segmentation. ||| 36956 ||| 7952 ||| 
2021 ||| point cloud transformers applied to collider physics. ||| 36957 ||| 36958 ||| 
2021 ||| cross modification attention based deliberation model for image captioning. ||| 6227 ||| 31401 ||| 6228 ||| 3049 ||| 528 ||| 
2020 ||| red dragon ai at textgraphs 2020 shared task: lit : lstm-interleaved transformer for multi-hop explanation ranking. ||| 36959 ||| 36960 ||| 36961 ||| 
2017 ||| efficiently applying attention to sequential data with the recurrent discounted attention unit. ||| 36962 ||| 36963 ||| 
2021 ||| fanet: a feedback attention network for improved biomedical image segmentation. ||| 20164 ||| 20165 ||| 20169 ||| 2712 ||| 20167 ||| 20168 ||| 15548 ||| 16413 ||| 20170 ||| 20166 ||| 
2020 ||| cloud transformers. ||| 1951 ||| 1952 ||| 
2020 ||| affect expression behaviour analysis in the wild using spatio-channel attention and complementary context information. ||| 30781 ||| 30782 ||| 
2021 ||| ats: adaptive token sampling for efficient vision transformers. ||| 19314 ||| 36964 ||| 36965 ||| 36966 ||| 36967 ||| 9162 ||| 36968 ||| 
2021 ||| grounding spatio-temporal language with transformers. ||| 36969 ||| 36970 ||| 36971 ||| 8382 ||| 36972 ||| 36973 ||| 
2021 ||| multi-modal temporal attention models for crop mapping from satellite time series. ||| 2309 ||| 2310 ||| 2311 ||| 19154 ||| 
2021 ||| audio captioning transformer. ||| 13521 ||| 13522 ||| 13523 ||| 12619 ||| 11418 ||| 
2019 ||| learning manipulation skills via hierarchical spatial attention. ||| 22281 ||| 22282 ||| 
2021 ||| cross-modal self-attention with multi-task pre-training for medical visual question answering. ||| 23868 ||| 2113 ||| 23869 ||| 1801 ||| 1800 ||| 
2021 ||| turkish text classification: from lexicon analysis to bidirectional transformer. ||| 36974 ||| 
2020 ||| object-centric learning with slot attention. ||| 9288 ||| 9289 ||| 2571 ||| 9290 ||| 2294 ||| 4960 ||| 9291 ||| 9292 ||| 
2021 ||| ga-net: global attention network for point cloud semantic segmentation. ||| 36975 ||| 36976 ||| 
2018 ||| fast efficient object detection using selective attention. ||| 
2018 ||| attention models in graphs: a survey. ||| 1193 ||| 1194 ||| 1196 ||| 28568 ||| 1197 ||| 
2019 ||| speech-xlnet: unsupervised acoustic model pretraining for self-attention networks. ||| 36977 ||| 14262 ||| 3138 ||| 12585 ||| 4530 ||| 3808 ||| 4460 ||| 
2018 ||| attention is all we need: nailing down object-centric attention for egocentric activity recognition. ||| 9832 ||| 9833 ||| 
2021 ||| graph attention networks with positional embeddings. ||| 15173 ||| 15174 ||| 15175 ||| 
2021 ||| transmvsnet: global context-aware multi-view stereo network with transformers. ||| 36978 ||| 33664 ||| 36979 ||| 36980 ||| 36981 ||| 36982 ||| 2530 ||| 
2021 ||| generating bug-fixes using pretrained transformers. ||| 15263 ||| 3668 ||| 15264 ||| 15265 ||| 
2021 ||| videolightformer: lightweight action recognition using transformers. ||| 36983 ||| 32872 ||| 
2019 ||| attention is (not) all you need for commonsense reasoning. ||| 3572 ||| 3573 ||| 
2020 ||| how effective is task-agnostic data augmentation for pretrained transformers? ||| 26688 ||| 3906 ||| 36984 ||| 
2019 ||| attention-driven tree-structured convolutional lstm for high dimensional data understanding. ||| 15553 ||| 398 ||| 15554 ||| 9891 ||| 6810 ||| 15556 ||| 14552 ||| 15555 ||| 15557 ||| 15558 ||| 
2020 ||| comparison of attention-based deep learning models for eeg classification. ||| 20558 ||| 20557 ||| 36985 ||| 9826 ||| 9827 ||| 36986 ||| 
2020 ||| semantic layout manipulation with high-resolution sparse attention. ||| 8519 ||| 18766 ||| 36987 ||| 21485 ||| 5365 ||| 7957 ||| 2166 ||| 
2018 ||| ntua-slp at semeval-2018 task 2: predicting emojis using rnns with context-aware attention. ||| 3728 ||| 36988 ||| 3721 ||| 10533 ||| 10532 ||| 3729 ||| 
2022 ||| self-attention fusion for audiovisual emotion recognition with incomplete data. ||| 36945 ||| 926 ||| 31739 ||| 
2021 ||| miti-detr: object detection based on transformers with mitigatory self-attention convergence. ||| 36989 ||| 36990 ||| 392 ||| 
2017 ||| attention-based information fusion using multi-encoder-decoder recurrent neural networks. ||| 15820 ||| 15821 ||| 15822 ||| 
2021 ||| utnet: a hybrid transformer architecture for medical image segmentation. ||| 27520 ||| 27521 ||| 1749 ||| 
2020 ||| persian ezafe recognition using transformers and its role in part-of-speech tagging. ||| 26838 ||| 26839 ||| 26840 ||| 
2017 ||| attention-based natural language person retrieval. ||| 614 ||| 15099 ||| 11676 ||| 19078 ||| 
2021 ||| transmot: spatial-temporal graph transformer for multiple object tracking. ||| 7246 ||| 18111 ||| 1296 ||| 2163 ||| 8573 ||| 
2020 ||| multi-modal transformer for video retrieval. ||| 8542 ||| 2094 ||| 8543 ||| 2093 ||| 
2022 ||| attention-based region of interest (roi) detection for speech emotion recognition. ||| 36991 ||| 36992 ||| 36993 ||| 
2022 ||| multi-direction and multi-scale pyramid in transformer for video-based pedestrian retrieval. ||| 36994 ||| 2064 ||| 1310 ||| 
2020 ||| latent video transformer. ||| 16383 ||| 16384 ||| 2646 ||| 2647 ||| 2648 ||| 
2019 ||| a hierarchical attention based seq2seq model for chinese lyrics generation. ||| 22582 ||| 5894 ||| 22583 ||| 2994 ||| 705 ||| 
2021 ||| ts-cam: token semantic coupled attention map for weakly supervised object localization. ||| 1310 ||| 2395 ||| 2396 ||| 2397 ||| 2398 ||| 2399 ||| 2373 ||| 2364 ||| 
2019 ||| two computational models for analyzing political attention in social media. ||| 13287 ||| 13288 ||| 13289 ||| 
2020 ||| solid-state inrush current limiter controller based on inrush prediction for large transformers. ||| 36995 ||| 36996 ||| 36997 ||| 36998 ||| 36999 ||| 
2022 ||| characterizing renal structures with 3d block aggregate transformers. ||| 23905 ||| 7206 ||| 37000 ||| 37001 ||| 5101 ||| 37002 ||| 37003 ||| 24710 ||| 24706 ||| 37004 ||| 37005 ||| 37006 ||| 7208 ||| 
2021 ||| adaptation and attention for neural video coding. ||| 9924 ||| 9925 ||| 9926 ||| 9927 ||| 1852 ||| 9928 ||| 8024 ||| 9929 ||| 6321 ||| 
2018 ||| deeply learning molecular structure-property relationships using graph attention neural network. ||| 37007 ||| 37008 ||| 37009 ||| 
2021 ||| unsupervised domain-specific deblurring using scale-specific attention. ||| 37010 ||| 11486 ||| 
2021 ||| efficient attention branch network with combined loss function for automatic speaker verification spoof detection. ||| 37011 ||| 37012 ||| 37013 ||| 
2017 ||| dataset construction via attention for aspect term extraction with distant supervision. ||| 6975 ||| 18467 ||| 6979 ||| 6977 ||| 6978 ||| 
2020 ||| an autoencoder wavelet based deep neural network with attention mechanism for multistep prediction of plant growth. ||| 31940 ||| 31941 ||| 31942 ||| 31943 ||| 31944 ||| 31945 ||| 
2020 ||| covariance self-attention dual path unet for rectal tumor segmentation. ||| 21817 ||| 21820 ||| 21819 ||| 21818 ||| 
2021 ||| transcenter: transformers with dense queries for multiple-object tracking. ||| 32688 ||| 37014 ||| 37015 ||| 2190 ||| 37016 ||| 9285 ||| 
2020 ||| spatially aware multimodal transformers for textvqa. ||| 8563 ||| 8564 ||| 8565 ||| 8566 ||| 8567 ||| 8568 ||| 8569 ||| 
2019 ||| image-and-spatial transformer networks for structure-guided image registration. ||| 27945 ||| 27946 ||| 27947 ||| 27948 ||| 27844 ||| 
2021 ||| isomer: transfer enhanced dual-channel heterogeneous dependency attention network for aspect-based sentiment classification. ||| 14171 ||| 37017 ||| 37018 ||| 37019 ||| 37020 ||| 37021 ||| 14188 ||| 37022 ||| 
2021 ||| visual navigation with spatial attention. ||| 18797 ||| 9306 ||| 18798 ||| 
2021 ||| enhancing cross-sectional currency strategies by ranking refinement with transformer-based architectures. ||| 37023 ||| 36553 ||| 33609 ||| 33608 ||| 
2021 ||| deep co-attention network for multi-view subspace learning. ||| 8933 ||| 2045 ||| 8916 ||| 8947 ||| 1295 ||| 
2019 ||| cawa: an attention-network for credit attribution. ||| 17919 ||| 15169 ||| 
2018 ||| topical stance detection for twitter: a two-phase lstm model using attention. ||| 10435 ||| 15110 ||| 15111 ||| 
2021 ||| t-automl: automated machine learning for lesion segmentation using transformers in 3d medical imaging. ||| 2019 ||| 2020 ||| 2021 ||| 2022 ||| 2023 ||| 2024 ||| 
2018 ||| self-erasing network for integral object attention. ||| 1902 ||| 1901 ||| 1905 ||| 1904 ||| 
2021 ||| attention-free keyword spotting. ||| 37024 ||| 37025 ||| 
2019 ||| lightweight and efficient end-to-end speech recognition using low-rank transformer. ||| 10650 ||| 11993 ||| 10652 ||| 11994 ||| 10654 ||| 
2021 ||| continuous speech separation with recurrent selective attention network. ||| 36222 ||| 12029 ||| 1236 ||| 12030 ||| 12655 ||| 13892 ||| 12179 ||| 
2018 ||| cbam: convolutional block attention module. ||| 7916 ||| 5369 ||| 7917 ||| 5372 ||| 
2019 ||| multimodal attention branch network for perspective-free sentence generation. ||| 22285 ||| 726 ||| 12308 ||| 
2021 ||| end-to-end trainable multi-instance pose estimation with transformers. ||| 37026 ||| 37027 ||| 37028 ||| 
2018 ||| incremental few-shot learning with attention attractor networks. ||| 9220 ||| 9221 ||| 9222 ||| 9223 ||| 
2021 ||| sinkformers: transformers with doubly stochastic attention. ||| 37029 ||| 37030 ||| 9211 ||| 37031 ||| 
2021 ||| an attention self-supervised contrastive learning based three-stage model for hand shape feature representation in cued speech. ||| 14759 ||| 14760 ||| 5950 ||| 14761 ||| 14762 ||| 2014 ||| 
2021 ||| augmenting convolutional networks with attention-based aggregation. ||| 1887 ||| 2118 ||| 1886 ||| 2265 ||| 1889 ||| 2120 ||| 1890 ||| 1891 ||| 1892 ||| 
2021 ||| improving on-screen sound separation for open domain videos with audio-visual self-attention. ||| 37032 ||| 8067 ||| 37033 ||| 2511 ||| 
2021 ||| msg-transformer: exchanging local spatial information by manipulating messenger tokens. ||| 37034 ||| 2477 ||| 2219 ||| 15595 ||| 2222 ||| 2398 ||| 
2020 ||| ddanet: dual decoder attention network for automatic polyp segmentation. ||| 20164 ||| 20165 ||| 20166 ||| 2712 ||| 20167 ||| 20168 ||| 20169 ||| 16413 ||| 20170 ||| 
2021 ||| tb-net: a tailored, self-attention deep convolutional neural network design for detection of tuberculosis cases from chest x-ray images. ||| 7865 ||| 37035 ||| 37036 ||| 33983 ||| 33984 ||| 
2021 ||| fine-tuning pretrained language models with label attention for explainable biomedical text classification. ||| 37037 ||| 27207 ||| 
2020 ||| dynamically extracting outcome-specific problem lists from clinical notes with guided multi-headed attention. ||| 20680 ||| 20681 ||| 20682 ||| 20683 ||| 
2020 ||| paragraph-level commonsense transformers with recurrent memory. ||| 18165 ||| 3350 ||| 18166 ||| 18167 ||| 18168 ||| 3355 ||| 
2022 ||| vitaev2: vision transformer advanced by exploring inductive bias for image recognition and beyond. ||| 37038 ||| 37039 ||| 875 ||| 1756 ||| 
2019 ||| lattice-based transformer encoder for neural machine translation. ||| 3109 ||| 3110 ||| 3111 ||| 3049 ||| 3112 ||| 
2021 ||| tanet: a new paradigm for global face super-resolution via transformer-cnn aggregation network. ||| 19757 ||| 19756 ||| 19758 ||| 12056 ||| 20135 ||| 2146 ||| 6863 ||| 
2021 ||| bangla image caption generation through cnn-transformer based encoder-decoder network. ||| 37040 ||| 37041 ||| 37042 ||| 37043 ||| 37044 ||| 37045 ||| 
2021 ||| baanet: learning bi-directional adaptive attention gates for multispectral pedestrian detection. ||| 37046 ||| 37047 ||| 37048 ||| 23576 ||| 19774 ||| 
2020 ||| common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. ||| 26414 ||| 37049 ||| 37050 ||| 3702 ||| 37051 ||| 15105 ||| 
2021 ||| mega-cda: memory guided attention for category-aware unsupervised domain adaptive object detection. ||| 19132 ||| 19133 ||| 19134 ||| 19135 ||| 18609 ||| 
2019 ||| predicting multiple demographic attributes with task specific embedding transformation and attention network. ||| 9249 ||| 9752 ||| 9753 ||| 9250 ||| 
2021 ||| bloomnet: a robust transformer based model for bloom's learning outcome classification. ||| 4015 ||| 4016 ||| 4017 ||| 4018 ||| 4019 ||| 4020 ||| 
2022 ||| dawn of the transformer era in speech emotion recognition: closing the valence gap. ||| 37052 ||| 37053 ||| 37054 ||| 37055 ||| 33493 ||| 37056 ||| 37057 ||| 648 ||| 649 ||| 
2019 ||| paying more attention to motion: attention distillation for learning video representations. ||| 8764 ||| 19066 ||| 6553 ||| 8766 ||| 8692 ||| 
2022 ||| automatic segmentation of head and neck tumor: how powerful transformers are? ||| 37058 ||| 37059 ||| 37060 ||| 37061 ||| 
2020 ||| focus-constrained attention mechanism for cvae-based response generation. ||| 26647 ||| 26648 ||| 26649 ||| 26650 ||| 26651 ||| 379 ||| 
2020 ||| isia food-500: a dataset for large-scale food recognition via stacked global-local attention network. ||| 19601 ||| 19602 ||| 19729 ||| 19603 ||| 6659 ||| 6935 ||| 19491 ||| 
2020 ||| qsan: a quantum-probability based signed attention network for explainable false information detection. ||| 1321 ||| 1322 ||| 1323 ||| 1324 ||| 1325 ||| 1326 ||| 
2018 ||| modeling localness for self-attention networks. ||| 3037 ||| 3041 ||| 3039 ||| 3075 ||| 3040 ||| 2814 ||| 
2021 ||| attention for image registration (air): an unsupervised transformer approach. ||| 18804 ||| 1890 ||| 37062 ||| 
2020 ||| augmented parallel-pyramid net for attention guided pose-estimation. ||| 20355 ||| 4470 ||| 20356 ||| 7064 ||| 37063 ||| 2824 ||| 12749 ||| 
2021 ||| hybrid encoder: towards efficient and precise native adsrecommendation via hybrid transformer encoding networks. ||| 37064 ||| 921 ||| 37065 ||| 9707 ||| 8912 ||| 37066 ||| 37067 ||| 30324 ||| 16298 ||| 9574 ||| 
2020 ||| multiple structural priors guided self attention network for language understanding. ||| 30130 ||| 9472 ||| 11682 ||| 3311 ||| 
2021 ||| donut: document understanding transformer without ocr. ||| 11758 ||| 37068 ||| 37069 ||| 37070 ||| 37071 ||| 37072 ||| 2087 ||| 2088 ||| 9518 ||| 
2021 ||| mixup training leads to reduced overfitting and improved calibration for the transformer architecture. ||| 37073 ||| 37074 ||| 
2017 ||| zoom out-and-in network with map attention decision for region proposal and object detection. ||| 5949 ||| 16550 ||| 2303 ||| 1846 ||| 
2019 ||| behavior sequence transformer for e-commerce recommendation in alibaba. ||| 37075 ||| 15201 ||| 3337 ||| 37076 ||| 1111 ||| 
2021 ||| attention-based neural load forecasting: a dynamic feature selection approach. ||| 37077 ||| 37078 ||| 37079 ||| 9472 ||| 
2020 ||| extremely low bit transformer quantization for on-device neural machine translation. ||| 3495 ||| 26509 ||| 26510 ||| 26511 ||| 26512 ||| 26513 ||| 3496 ||| 26514 ||| 
2020 ||| sketchformer: transformer-based representation for sketched structure. ||| 37080 ||| 2286 ||| 2288 ||| 19301 ||| 
2021 ||| accounting for agreement phenomena in sentence comprehension with transformer language models: effects of similarity-based interference on surprisal and attention. ||| 20738 ||| 20739 ||| 
2022 ||| vovit: low latency graph-based audio-visual voice separation transformer. ||| 37081 ||| 37082 ||| 37083 ||| 
2022 ||| self-supervised vision transformers learn visual concepts in histopathology. ||| 2431 ||| 37084 ||| 
2021 ||| the channel-spatial attention-based vision transformer network for automated, accurate prediction of crop nitrogen status from uav imagery. ||| 1340 ||| 37085 ||| 37086 ||| 37087 ||| 37088 ||| 37089 ||| 37090 ||| 
2020 ||| weakly supervised few-shot object segmentation using co-attention with visual and semantic inputs. ||| 23444 ||| 23445 ||| 23446 ||| 23447 ||| 23448 ||| 23449 ||| 
2019 ||| part-guided attention learning for vehicle re-identification. ||| 3613 ||| 37091 ||| 19333 ||| 19016 ||| 28347 ||| 6335 ||| 
2020 ||| repulsive attention: rethinking multi-head attention as bayesian inference. ||| 26677 ||| 26678 ||| 26679 ||| 26680 ||| 26681 ||| 26682 ||| 26683 ||| 26684 ||| 17231 ||| 
2019 ||| jointly learning to align and translate with transformer models. ||| 26303 ||| 26304 ||| 26305 ||| 26306 ||| 
2020 ||| grover: self-supervised message passing transformer on large-scale molecular data. ||| 1261 ||| 9189 ||| 1262 ||| 9190 ||| 9191 ||| 1263 ||| 1265 ||| 
2020 ||| long document ranking with query-directed sparse transformer. ||| 9682 ||| 3322 ||| 26779 ||| 1160 ||| 
2021 ||| efficient attentions for long document summarization. ||| 4751 ||| 4752 ||| 4753 ||| 3403 ||| 4754 ||| 
2021 ||| transmed: transformers advance multi-modal medical image classification. ||| 32630 ||| 32631 ||| 
2019 ||| attention-aware age-agnostic visual place recognition. ||| 7961 ||| 7962 ||| 7963 ||| 7964 ||| 
2019 ||| attention based neural architecture for rumor detection with author context awareness. ||| 3960 ||| 3961 ||| 
2021 ||| end-to-end temporal action detection with transformer. ||| 1001 ||| 37092 ||| 2405 ||| 6617 ||| 2083 ||| 17429 ||| 
2019 ||| distant supervision relation extraction with intra-bag and inter-bag attentions. ||| 4893 ||| 4894 ||| 
2021 ||| transformer-f: a transformer network with effective methods for learning universal sentence representation. ||| 11796 ||| 
2019 ||| attention-based fault-tolerant approach for multi-agent reinforcement learning systems. ||| 31657 ||| 362 ||| 5779 ||| 37093 ||| 363 ||| 19547 ||| 
2022 ||| instantaneous physiological estimation using video transformers. ||| 37094 ||| 37095 ||| 37096 ||| 2713 ||| 23255 ||| 37097 ||| 
2020 ||| improving sentiment analysis over non-english tweets using multilingual transformers and automatic translation for data-augmentation. ||| 11730 ||| 7350 ||| 11731 ||| 
2020 ||| an attention-based system for damage assessment using satellite imagery. ||| 6880 ||| 6881 ||| 6882 ||| 6883 ||| 6884 ||| 6885 ||| 6886 ||| 6888 ||| 6887 ||| 
2021 ||| fine-grained image generation from bangla text description using attentional generative adversarial network. ||| 37040 ||| 37098 ||| 37099 ||| 37043 ||| 
2020 ||| dualnet: locate then detect effective payload with deep attention network. ||| 3972 ||| 3973 ||| 3974 ||| 
2021 ||| attend to who you are: supervising self-attention for keypoint detection and instance-aware association. ||| 2095 ||| 18779 ||| 6430 ||| 14994 ||| 37100 ||| 2096 ||| 7897 ||| 37101 ||| 34490 ||| 2098 ||| 
2021 ||| application of deep self-attention in knowledge tracing. ||| 37102 ||| 37103 ||| 13410 ||| 37104 ||| 
2020 ||| attention that does not explain away. ||| 26721 ||| 9281 ||| 26719 ||| 37105 ||| 3731 ||| 
2021 ||| timbre classification of musical instruments with a deep learning multi-head attention-based model. ||| 37106 ||| 852 ||| 1866 ||| 37107 ||| 3882 ||| 
2019 ||| improving deep lesion detection using 3d contextual and spatial attention. ||| 27775 ||| 27693 ||| 1691 ||| 17666 ||| 17667 ||| 
2021 ||| diverse single image generation with controllable global structure through self-attention. ||| 37108 ||| 37109 ||| 37110 ||| 
2021 ||| visual selective attention system to intervene user attention in sharing covid-19 misinformation. ||| 26083 ||| 26084 ||| 11483 ||| 
2021 ||| edge-featured graph attention network. ||| 1785 ||| 4097 ||| 
2021 ||| boosting few-shot semantic segmentation with transformers. ||| 7813 ||| 4477 ||| 7811 ||| 7814 ||| 
2020 |||  distillation through attention. ||| 1887 ||| 2118 ||| 1893 ||| 8682 ||| 2119 ||| 1890 ||| 1891 ||| 1892 ||| 
2018 ||| a visual attention grounding neural model for multimodal machine translation. ||| 26423 ||| 26424 ||| 8701 ||| 1753 ||| 
2018 ||| double supervised network with attention mechanism for scene text recognition. ||| 21714 ||| 11446 ||| 21715 ||| 
2020 ||| pay attention to evolution: time series forecasting with deep graph-evolution learning. ||| 37111 ||| 23442 ||| 37112 ||| 26979 ||| 852 ||| 15768 ||| 23323 ||| 
2020 ||| a self-attention network based node embedding model. ||| 7035 ||| 7036 ||| 7037 ||| 
2020 ||| tnt-kid: transformer-based neural tagger for keyword identification. ||| 37113 ||| 10213 ||| 24947 ||| 
2020 ||| neural encoding with visual attention. ||| 9385 ||| 9386 ||| 9387 ||| 9388 ||| 9389 ||| 
2022 ||| vrt: a video restoration transformer. ||| 7811 ||| 7812 ||| 19271 ||| 3433 ||| 37114 ||| 37115 ||| 7815 ||| 7814 ||| 
2022 ||| rtnet: relation transformer network for diabetic retinopathy multi-lesion segmentation. ||| 37116 ||| 7833 ||| 37117 ||| 37118 ||| 7829 ||| 
2018 ||| paying attention to attention: highlighting influential samples in sequential analysis. ||| 37119 ||| 37120 ||| 37121 ||| 37122 ||| 37123 ||| 
2022 ||| ernie-sparse: learning hierarchical efficient transformer through regularized self-attention. ||| 1305 ||| 10583 ||| 3036 ||| 37124 ||| 10585 ||| 10581 ||| 673 ||| 3415 ||| 3416 ||| 3417 ||| 
2022 ||| multiple sclerosis lesions segmentation using attention-based cnns in flair images. ||| 37125 ||| 37126 ||| 37127 ||| 
2021 ||| regularizing transformers with deep probabilistic layers. ||| 37128 ||| 37129 ||| 37130 ||| 37131 ||| 6235 ||| 37132 ||| 21536 ||| 
2021 ||| gradient-based adversarial attacks against text transformers. ||| 26285 ||| 2119 ||| 1890 ||| 1891 ||| 1892 ||| 3826 ||| 
2021 ||| attention! stay focus! ||| 19050 ||| 
2019 ||| a better way to attend: attention with trees for video question answering. ||| 37133 ||| 37134 ||| 1306 ||| 1115 ||| 
2018 ||| attention please: consider mockito when evaluating newly released automated program repair techniques. ||| 14190 ||| 14191 ||| 14193 ||| 14192 ||| 
2021 ||| decoding 3d representation of visual imagery eeg using attention-based dual-stream convolutional neural network. ||| 16091 ||| 16092 ||| 
2021 ||| transformers for prompt-level ema non-response prediction. ||| 36404 ||| 36403 ||| 37135 ||| 37136 ||| 37137 ||| 37138 ||| 37139 ||| 37140 ||| 37141 ||| 37142 ||| 37143 ||| 8692 ||| 
2021 ||| generating fake cyber threat intelligence using transformer-based models. ||| 460 ||| 461 ||| 462 ||| 463 ||| 464 ||| 
2017 ||| audio set classification with attention model: a probabilistic perspective. ||| 12618 ||| 1125 ||| 11418 ||| 12619 ||| 
2021 ||| a simple approach to image tilt correction with self-attention mobilenet for smartphones. ||| 18017 ||| 37144 ||| 37145 ||| 37146 ||| 
2021 ||| ptq4vit: post-training quantization framework for vision transformers. ||| 37147 ||| 37148 ||| 37149 ||| 603 ||| 35903 ||| 
2021 ||| deepprog: a transformer-based framework for predicting disease prognosis. ||| 37150 ||| 37151 ||| 37152 ||| 37153 ||| 
2021 ||| end-to-end speaker-attributed asr with transformer. ||| 13897 ||| 14324 ||| 13893 ||| 12608 ||| 13892 ||| 12029 ||| 12030 ||| 
2019 ||| dnanet: de-normalized attention based multi-resolution network for human pose estimation. ||| 1558 ||| 8859 ||| 12601 ||| 37154 ||| 37155 ||| 37156 ||| 35579 ||| 37157 ||| 
2020 ||| a accuracy with transformer-based models on large complex documents. ||| 37158 ||| 37159 ||| 37160 ||| 37161 ||| 
2018 ||| attention-based capsule networks with dynamic routing for relation extraction. ||| 17987 ||| 17988 ||| 37162 ||| 5250 ||| 781 ||| 17990 ||| 
2020 ||| inner attention supported adaptive cooperation for heterogeneous multi robots teaming based on multi-agent reinforcement learning. ||| 1124 ||| 1840 ||| 
2022 ||| segtransvae: hybrid cnn - transformer with regularization for medical image segmentation. ||| 37163 ||| 37164 ||| 37165 ||| 37166 ||| 
2021 ||| gattanet: global attention agreement for convolutional neural networks. ||| 4130 ||| 4131 ||| 
2020 ||| gret: global representation enhanced transformer. ||| 17791 ||| 17792 ||| 4845 ||| 17793 ||| 3385 ||| 3471 ||| 4847 ||| 
2021 ||| cate: computation-aware neural architecture encoding with transformers. ||| 22840 ||| 22841 ||| 3269 ||| 22842 ||| 
2021 ||| group-node attention for community evolution prediction. ||| 28066 ||| 9750 ||| 28067 ||| 
2020 ||| hierarchical attention network for action segmentation. ||| 30783 ||| 11374 ||| 11330 ||| 11331 ||| 
2022 ||| clustering text using attention. ||| 4670 ||| 
2020 ||| learning to decouple relations: few-shot relation classification with entity-guided attention and confusion-aware training. ||| 11704 ||| 4271 ||| 11705 ||| 3560 ||| 3561 ||| 3562 ||| 880 ||| 
2021 ||| spanet: generalized permutationless set assignment for particle physics using symmetry preserving attention. ||| 33109 ||| 33108 ||| 33110 ||| 33111 ||| 33112 ||| 4311 ||| 
2021 ||| augmented transformer with adaptive graph for temporal action proposal generation. ||| 32991 ||| 1703 ||| 1704 ||| 1705 ||| 1685 ||| 
2018 ||| cascade attention network for person search: both image and text-image similarity selection. ||| 18099 ||| 18100 ||| 18101 ||| 1160 ||| 10429 ||| 17803 ||| 
2021 ||| dual transformer for point cloud analysis. ||| 36120 ||| 37167 ||| 37168 ||| 36122 ||| 
2020 ||| relational pretrained transformers towards democratizing data preparation [vision]. ||| 37169 ||| 37170 ||| 37171 ||| 37172 ||| 23842 ||| 4094 ||| 37173 ||| 37174 ||| 
2020 ||| point cloud completion by skip-attention network with hierarchical folding. ||| 2558 ||| 19051 ||| 2563 ||| 2559 ||| 
2019 ||| hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification. ||| 9407 ||| 215 ||| 18200 ||| 18202 ||| 9988 ||| 1717 ||| 26422 ||| 1094 ||| 
2021 ||| continuous 3d multi-channel sign language production via progressive transformers and mixture density networks. ||| 8528 ||| 8529 ||| 7442 ||| 8530 ||| 
2020 ||| character-level transformer-based neural machine translation. ||| 13387 ||| 13388 ||| 13389 ||| 
2021 ||| joint attention for multi-agent coordination and social learning. ||| 37175 ||| 37176 ||| 37177 ||| 22750 ||| 37105 ||| 37178 ||| 
2020 ||| feedback graph attention convolutional network for medical image enhancement. ||| 14844 ||| 14845 ||| 8621 ||| 14846 ||| 3473 ||| 14847 ||| 14848 ||| 
2021 ||| attention-based partial face recognition. ||| 5691 ||| 5692 ||| 11458 ||| 5694 ||| 11459 ||| 5695 ||| 
2021 ||| efficient hybrid transformer: learning global-local context for urban sence segmentation. ||| 30557 ||| 37179 ||| 13362 ||| 8207 ||| 30455 ||| 
2021 ||| graph attention network-based multi-agent reinforcement learning for slicing resource management in dense cellular network. ||| 6966 ||| 6967 ||| 16875 ||| 37180 ||| 6968 ||| 675 ||| 
2021 ||| triple m: a practical neural text-to-speech system with multi-guidance attention and multi-band multi-time lpcnet. ||| 14752 ||| 12002 ||| 12001 ||| 12003 ||| 
2020 ||| transformer-based language modeling and decoding for conversational speech recognition. ||| 37181 ||| 
2021 ||| translation transformers rediscover inherent data domains. ||| 21401 ||| 21402 ||| 21403 ||| 
2019 ||| sequential attention-based network for noetic end-to-end response selection. ||| 12760 ||| 8948 ||| 
2018 ||| personalized attention-aware exposure control using reinforcement learning. ||| 18950 ||| 37182 ||| 37183 ||| 11686 ||| 37184 ||| 
2021 ||| improved multiscale vision transformers for classification and detection. ||| 2028 ||| 32554 ||| 2025 ||| 2027 ||| 2026 ||| 2030 ||| 2031 ||| 
2021 ||| learning slice-aware representations with mixture of attentions. ||| 3691 ||| 3692 ||| 3693 ||| 3694 ||| 3027 ||| 3030 ||| 
2020 ||| simultaneous left atrium anatomy and scar segmentations via deep learning in multiview information with attention. ||| 6005 ||| 1785 ||| 27334 ||| 27856 ||| 27633 ||| 27634 ||| 27636 ||| 27635 ||| 37185 ||| 37186 ||| 4060 ||| 25716 ||| 27637 ||| 27335 ||| 27639 ||| 27638 ||| 
2020 ||| ecapa-tdnn: emphasized channel attention, propagation and aggregation in tdnn based speaker verification. ||| 14313 ||| 14314 ||| 14315 ||| 
2021 ||| memory-efficient transformers via top-k attention. ||| 26370 ||| 37187 ||| 37188 ||| 37189 ||| 26356 ||| 
2021 ||| h-net: unsupervised attention-based stereo depth estimation leveraging epipolar geometry. ||| 37190 ||| 37191 ||| 37192 ||| 37193 ||| 
2021 ||| hetformer: heterogeneous transformer with sparse attention for long-text extractive summarization. ||| 23311 ||| 37194 ||| 3891 ||| 11630 ||| 9988 ||| 1094 ||| 
2021 ||| heterogeneous transformer: a scale adaptable neural network architecture for device activity detection. ||| 438 ||| 37195 ||| 37196 ||| 37197 ||| 37198 ||| 
2022 ||| joint cnn and transformer network via weakly supervised learning for efficient crowd counting. ||| 31977 ||| 19109 ||| 37199 ||| 1900 ||| 37200 ||| 19909 ||| 
2021 ||| attention-based bidirectional lstm for deceptive opinion spam classification. ||| 37201 ||| 
2021 ||| st-detr: spatio-temporal object traces attention detection transformer. ||| 23632 ||| 23633 ||| 
2021 ||| prototypical cross-attention networks for multiple object tracking and segmentation. ||| 37202 ||| 2514 ||| 25546 ||| 8549 ||| 19246 ||| 37203 ||| 
2022 ||| effective urban region representation learning using heterogeneous urban graph attention network (hugat). ||| 37204 ||| 37205 ||| 
2021 ||| transzero: attribute-guided transformer for zero-shot learning. ||| 33041 ||| 33042 ||| 1305 ||| 33043 ||| 37206 ||| 1705 ||| 37207 ||| 13200 ||| 33044 ||| 
2020 ||| large scale multimodal classification using an ensemble of transformer models and co-attention. ||| 37208 ||| 37209 ||| 
2020 ||| self-and-mixed attention decoder with deep acoustic structure for transformer-based lvcsr. ||| 14361 ||| 14527 ||| 14362 ||| 13947 ||| 13948 ||| 12494 ||| 
2021 ||| segmenter: transformer for semantic segmentation. ||| 2172 ||| 2173 ||| 2174 ||| 2093 ||| 
2020 ||| decoupled self attention for accurate one stage object detection. ||| 37210 ||| 37211 ||| 37212 ||| 37213 ||| 3337 ||| 
2018 ||| audio-visual speech recognition with a hybrid ctc/attention architecture. ||| 5717 ||| 25660 ||| 25661 ||| 25662 ||| 5719 ||| 
2021 ||| transfer learning with causal counterfactual reasoning in decision transformers. ||| 37214 ||| 37215 ||| 37216 ||| 
2020 ||| solar: second-order loss and attention for image retrieval. ||| 8584 ||| 8585 ||| 8586 ||| 8587 ||| 
2021 ||| mia-former: efficient and robust vision transformers via multi-grained input-adaptation. ||| 37217 ||| 37218 ||| 37219 ||| 37220 ||| 37221 ||| 
2019 ||| contrastive bidirectional transformer for temporal representation learning. ||| 2094 ||| 7968 ||| 17355 ||| 2093 ||| 
2021 ||| pose recognition with cascade transformers. ||| 1424 ||| 19354 ||| 586 ||| 1813 ||| 1812 ||| 1815 ||| 
2021 ||| optimising knee injury detection with spatial attention and validating localisation ability. ||| 22981 ||| 22982 ||| 22983 ||| 22984 ||| 22985 ||| 22986 ||| 22987 ||| 
2017 ||| a recurrent neural model with attention for the recognition of chinese implicit discourse relations. ||| 3541 ||| 3542 ||| 3543 ||| 3544 ||| 
2021 ||| attention-based vehicle self-localization with hd feature maps. ||| 23622 ||| 23623 ||| 23624 ||| 
2020 ||| pct: point cloud transformer. ||| 24014 ||| 32106 ||| 32107 ||| 32108 ||| 32109 ||| 32110 ||| 
2017 ||| hierarchical lstm with adjusted temporal attention for video captioning. ||| 9576 ||| 3391 ||| 1039 ||| 17651 ||| 3282 ||| 1040 ||| 
2018 ||| bam: bottleneck attention module. ||| 5369 ||| 7916 ||| 7917 ||| 5372 ||| 
2021 ||| self-attention for audio super-resolution. ||| 1479 ||| 1480 ||| 
2019 ||| encoding database schemas with relation-aware self-attention for text-to-sql parsers. ||| 37222 ||| 
2018 ||| attentionmask: attentive, efficient object proposal generation focusing on small objects. ||| 6439 ||| 5736 ||| 
2021 ||| deepra: predicting joint damage from radiographs using cnn with attention. ||| 37223 ||| 
2020 ||| improving auditory attention decoding performance of linear and non-linear methods using state-space model. ||| 1490 ||| 12082 ||| 1495 ||| 
2018 ||| attention-based lstm for psychological stress detection from spoken language using distant supervision. ||| 10650 ||| 12236 ||| 10654 ||| 
2021 ||| mask guided attention for fine-grained patchy image classification. ||| 1224 ||| 11311 ||| 11312 ||| 
2021 ||| hierarchical transformer encoders for vietnamese spelling correction. ||| 16138 ||| 16139 ||| 34497 ||| 16141 ||| 
2019 ||| detecting alzheimer's disease by estimating attention and elicitation path through the alignment of spoken picture descriptions with the picture prompt. ||| 14302 ||| 14301 ||| 37224 ||| 14303 ||| 14304 ||| 14305 ||| 14306 ||| 
2021 ||| disentangling representations of text by masking transformers. ||| 26686 ||| 26687 ||| 4950 ||| 
2021 ||| r2d2: relational text decoding with transformers. ||| 37225 ||| 37226 ||| 37227 ||| 18180 ||| 37228 ||| 
2019 ||| multi-scale guided attention for medical image segmentation. ||| 31018 ||| 26899 ||| 
2021 ||| keeping your eye on the ball: trajectory attention in video transformers. ||| 37229 ||| 37230 ||| 37231 ||| 2055 ||| 14721 ||| 2031 ||| 37232 ||| 1994 ||| 37233 ||| 
2021 ||| session-based recommendation with hypergraph attention networks. ||| 9754 ||| 9755 ||| 6452 ||| 9756 ||| 
2021 ||| sound event detection transformer: an event-based end-to-end model for sound event detection. ||| 36258 ||| 33069 ||| 2519 ||| 33072 ||| 36259 ||| 36260 ||| 810 ||| 
2020 ||| adaptive transformers in rl. ||| 32789 ||| 32788 ||| 37234 ||| 
2020 ||| an explainable 3d residual self-attention deep neural network for joint atrophy localization and alzheimer's disease diagnosis using structural mri. ||| 1340 ||| 37085 ||| 37235 ||| 19819 ||| 15620 ||| 
2022 ||| sotitle: a transformer-based post title generation approach for stack overflow. ||| 5942 ||| 6005 ||| 6007 ||| 6008 ||| 
2018 ||| neighbourhood watch: referring expression comprehension via language-guided graph attention networks. ||| 5845 ||| 6417 ||| 19333 ||| 6335 ||| 1039 ||| 5177 ||| 
2020 ||| gate: graph attention transformer encoder for cross-lingual relation and event extraction. ||| 3238 ||| 18020 ||| 3033 ||| 
2022 ||| dp-kb: data programming with knowledge bases improves transformer fine tuning for answer sentence selection. ||| 37236 ||| 9680 ||| 1186 ||| 3374 ||| 
2021 ||| on-device spatial attention based sequence learning approach for scene text script identification. ||| 37237 ||| 37238 ||| 37146 ||| 37239 ||| 37240 ||| 
2020 ||| revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. ||| 1986 ||| 1987 ||| 1990 ||| 1991 ||| 1992 ||| 
2022 ||| guided generative protein design using regularized transformers. ||| 37241 ||| 37242 ||| 37243 ||| 37244 ||| 37245 ||| 37246 ||| 
2019 ||| convolutional temporal attention model for video-based person re-identification. ||| 18747 ||| 18831 ||| 602 ||| 
2019 ||| a zero attention model for personalized product search. ||| 1276 ||| 1277 ||| 1278 ||| 1140 ||| 
2019 ||| speeding up reinforcement learning by combining attention and agency features. ||| 22242 ||| 13501 ||| 22243 ||| 22244 ||| 
2021 ||| df-conformer: integrated architecture of conv-tasnet and conformer using linear complexity self-attention for speech enhancement. ||| 8065 ||| 8066 ||| 8067 ||| 8068 ||| 2511 ||| 8069 ||| 8070 ||| 
2021 ||| somps-net : attention based social graph framework for early detection of fake health news. ||| 37247 ||| 37248 ||| 37249 ||| 37250 ||| 37251 ||| 21528 ||| 
2019 ||| eyecar: modeling the visual attention allocation of drivers in semi-autonomous vehicles. ||| 2107 ||| 2108 ||| 2005 ||| 2109 ||| 2110 ||| 2111 ||| 
2021 ||| swintrack: a simple and strong baseline for transformer tracking. ||| 37252 ||| 7245 ||| 1125 ||| 2163 ||| 
2021 ||| pyramid vision transformer: a versatile backbone for dense prediction without convolutions. ||| 2006 ||| 2007 ||| 2008 ||| 1861 ||| 2009 ||| 2010 ||| 173 ||| 2011 ||| 1932 ||| 
2021 ||| attention-based transformation from latent features to point clouds. ||| 37253 ||| 37254 ||| 37255 ||| 9688 ||| 
2021 ||| cuab: convolutional uncertainty attention block enhanced the chest x-ray image analysis. ||| 13328 ||| 37256 ||| 37257 ||| 37258 ||| 13329 ||| 
2020 ||| chatbot interaction with artificial intelligence: human data augmentation with t5 and language transformer ensemble for text classification. ||| 35134 ||| 34003 ||| 37259 ||| 37260 ||| 37261 ||| 
2020 ||| transtrack: multiple-object tracking with transformer. ||| 23412 ||| 37262 ||| 37091 ||| 2007 ||| 8548 ||| 27120 ||| 12433 ||| 8725 ||| 8727 ||| 2011 ||| 
2020 ||| phyaat: physiology of auditory attention to speech dataset. ||| 25440 ||| 12581 ||| 25443 ||| 3882 ||| 25441 ||| 
2021 ||| development of deep transformer-based models for long-term prediction of transient production of oil wells. ||| 37263 ||| 37264 ||| 37265 ||| 2648 ||| 37266 ||| 
2021 ||| improving predictions of tail-end labels using concatenated biomed-transformers for long medical documents. ||| 24437 ||| 24440 ||| 24439 ||| 24438 ||| 
2021 ||| multiplex behavioral relation learning for recommendation via memory augmented transformer network. ||| 1126 ||| 1124 ||| 1125 ||| 2588 ||| 1099 ||| 9575 ||| 
2020 ||| a novel transferability attention neural network model for eeg emotion recognition. ||| 438 ||| 37267 ||| 136 ||| 21748 ||| 540 ||| 
2019 ||| a multimodal target-source classifier with attention branches to understand ambiguous instructions for fetching daily objects. ||| 22285 ||| 726 ||| 12308 ||| 
2020 ||| transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis. ||| 4784 ||| 21128 ||| 4790 ||| 4845 ||| 4847 ||| 
2020 ||| brain tumor segmentation network using attention-based fusion and spatial relationship constraint. ||| 24674 ||| 24675 ||| 3034 ||| 19749 ||| 24676 ||| 24677 ||| 20755 ||| 
2021 ||| visual-aware attention dual-stream decoder for video captioning. ||| 37268 ||| 8372 ||| 8373 ||| 1556 ||| 7768 ||| 
2021 ||| buildformer: automatic building extraction with vision transformer. ||| 30557 ||| 37269 ||| 8207 ||| 
2017 ||| emoatt at emoint-2017: inner attention sentence embedding for emotion intensity. ||| 7447 ||| 20764 ||| 
2017 ||| picanet: learning pixel-wise contextual attention in convnets and its application in saliency detection. ||| 2411 ||| 2414 ||| 
2019 ||| a time attention based fraud transaction detection framework. ||| 22931 ||| 25197 ||| 23450 ||| 25196 ||| 1086 ||| 12009 ||| 
2022 ||| wind park power prediction: attention-based graph networks and deep learning to capture wake losses. ||| 37270 ||| 37271 ||| 37272 ||| 37273 ||| 37274 ||| 
2019 ||| mc-ista-net: adaptive measurement and initialization and channel attention optimization inspired neural network for compressive sensing. ||| 
2019 ||| eca-net: efficient channel attention for deep convolutional neural networks. ||| 19228 ||| 19229 ||| 8754 ||| 19230 ||| 2018 ||| 5077 ||| 
2021 ||| self-supervised video retrieval transformer network. ||| 6618 ||| 37275 ||| 36104 ||| 37276 ||| 
2017 ||| soft + hardwired attention: an lstm framework for human trajectory prediction and abnormal event detection. ||| 12567 ||| 11374 ||| 11330 ||| 11331 ||| 
2017 ||| confidence through attention. ||| 37277 ||| 21403 ||| 
2020 ||| fast transformers with clustered attention. ||| 9347 ||| 9348 ||| 1226 ||| 9349 ||| 
2019 ||| exploring the limits of transfer learning with a unified text-to-text transformer. ||| 3338 ||| 9132 ||| 4822 ||| 37278 ||| 13146 ||| 26716 ||| 26720 ||| 3337 ||| 22749 ||| 
2021 ||| can transformer models measure coherence in text? re-thinking the shuffle test. ||| 3105 ||| 3106 ||| 3107 ||| 3108 ||| 
2021 ||| locally shifted attention with early global integration. ||| 37279 ||| 37280 ||| 12541 ||| 1667 ||| 
2022 ||| joint liver and hepatic lesion segmentation using a hybrid cnn with transformer layers. ||| 37281 ||| 37282 ||| 37283 ||| 37284 ||| 37285 ||| 37286 ||| 
2021 ||| pairconnect: a compute-efficient mlp alternative to attention. ||| 37287 ||| 37288 ||| 37289 ||| 37290 ||| 
2017 ||| watch your step: learning graph embeddings through attention. ||| 9207 ||| 9208 ||| 4824 ||| 37291 ||| 
2019 ||| investigating self-attention network for chinese word segmentation. ||| 37292 ||| 3289 ||| 
2021 ||| what makes for hierarchical vision transformer? ||| 37293 ||| 2219 ||| 37294 ||| 2478 ||| 2222 ||| 
2019 ||| scar: spatial-/channel-wise attention regression networks for crowd counting. ||| 31724 ||| 6627 ||| 6650 ||| 
2019 ||| perceive, transform, and act: multi-modal attention networks for vision-and-language navigation. ||| 37295 ||| 19003 ||| 19001 ||| 37296 ||| 13611 ||| 
2018 ||| atts2s-vc: sequence-to-sequence voice conversion with attention and context preservation mechanisms. ||| 12537 ||| 12538 ||| 12539 ||| 12540 ||| 
2020 ||| msaf: multimodal split attention fusion. ||| 37297 ||| 37298 ||| 37299 ||| 37300 ||| 
2021 ||| multimodal attention fusion for target speaker extraction. ||| 12219 ||| 8251 ||| 1493 ||| 1491 ||| 1492 ||| 1494 ||| 
2021 ||| detecting gender bias in transformer-based models: a case study on bert. ||| 9874 ||| 9870 ||| 37301 ||| 37302 ||| 14066 ||| 37303 ||| 13660 ||| 37304 ||| 11023 ||| 11024 ||| 
2020 ||| don't shoot butterfly with rifles: multi-channel continuous speech separation with early exit transformer. ||| 12561 ||| 2280 ||| 12029 ||| 12030 ||| 12389 ||| 12179 ||| 
2022 ||| truetype transformer: character and font style recognition in outline format. ||| 37305 ||| 37306 ||| 37307 ||| 6934 ||| 
2017 ||| visual reference resolution using attention memory for visual dialog. ||| 9275 ||| 9276 ||| 9277 ||| 2301 ||| 
2019 ||| adversarial cross-domain action recognition with co-attention. ||| 18107 ||| 18108 ||| 18050 ||| 7431 ||| 
2020 ||| a contextual hierarchical attention network with adaptive objective for dialogue state tracking. ||| 3529 ||| 3073 ||| 3442 ||| 3075 ||| 3076 ||| 3074 ||| 1921 ||| 
2019 ||| joint language identification of code-switching speech using attention based e2e network. ||| 37308 ||| 9854 ||| 9855 ||| 9856 ||| 
2017 ||| order-free rnn with visual attention for multi-label classification. ||| 11324 ||| 18187 ||| 18188 ||| 6328 ||| 
2021 ||| tree decomposition attention for amr-to-text generation. ||| 37309 ||| 37310 ||| 
2022 ||| continual transformers: redundancy-free attention for online inference. ||| 37311 ||| 35515 ||| 926 ||| 
2020 ||| classification of tactile perception and attention on natural textures from eeg signals. ||| 16097 ||| 16098 ||| 16099 ||| 
2018 ||| visual attention network for low dose ct. ||| 37312 ||| 37313 ||| 37314 ||| 37315 ||| 8534 ||| 340 ||| 
2021 ||| larger-scale transformers for multilingual masked language modeling. ||| 37316 ||| 37317 ||| 34968 ||| 37318 ||| 37319 ||| 
2021 ||| on the control of attentional processes in vision. ||| 29817 ||| 37320 ||| 32361 ||| 32951 ||| 
2021 ||| multi-domain transformer-based counterfactual augmentation for earnings call analysis. ||| 9587 ||| 8934 ||| 781 ||| 37321 ||| 37322 ||| 9592 ||| 
2021 ||| boosting salient object detection with transformer-based asymmetric bilateral u-net. ||| 37323 ||| 4477 ||| 17992 ||| 4620 ||| 
2021 ||| tbn-vit: temporal bilateral network with vision transformer for video scene parsing. ||| 28175 ||| 37324 ||| 37325 ||| 
2022 ||| self-attention multi-view representation learning with diversity-promoting complementarity. ||| 415 ||| 37326 ||| 5312 ||| 35381 ||| 
2018 ||| attention-guided unified network for panoptic segmentation. ||| 18686 ||| 18687 ||| 18688 ||| 2477 ||| 18689 ||| 18690 ||| 11787 ||| 
2021 ||| revisiting mahalanobis distance for transformer-based out-of-domain detection. ||| 18095 ||| 18096 ||| 18097 ||| 10339 ||| 18098 ||| 
2022 ||| bert weaver: using weight averaging to enable lifelong learning for transformer-based models. ||| 37327 ||| 37328 ||| 37329 ||| 37330 ||| 
2021 ||| target-dependent uniter: a transformer-based multimodal language comprehension model for domestic service robots. ||| 37331 ||| 726 ||| 
2022 ||| transformer embeddings of irregularly spaced events and their participants. ||| 23773 ||| 17836 ||| 37332 ||| 
2020 ||| attack on multi-node attention for object detection. ||| 37333 ||| 37334 ||| 11266 ||| 1558 ||| 
2021 ||| attention-based model for predicting question relatedness on stack overflow. ||| 20594 ||| 20595 ||| 20596 ||| 20597 ||| 20598 ||| 
2022 ||| coarse-to-fine vision transformer. ||| 37335 ||| 36452 ||| 1424 ||| 35758 ||| 6831 ||| 37336 ||| 2367 ||| 
2019 ||| patch transformer for multi-tagging whole slide histopathology images. ||| 27531 ||| 27532 ||| 2070 ||| 27533 ||| 27534 ||| 2166 ||| 
2021 ||| vision transformers for femur fracture classification. ||| 37337 ||| 37338 ||| 37339 ||| 37340 ||| 37341 ||| 
2021 ||| space-time mixing attention for video transformer. ||| 37342 ||| 34892 ||| 9832 ||| 34893 ||| 4046 ||| 25662 ||| 
2019 ||| attention-based face antispoofing of rgb images, using a minimal end-2-end neural network. ||| 12172 ||| 12173 ||| 12174 ||| 
2021 ||| efficient two-stage detection of human-object interactions with a novel unary-pairwise transformer. ||| 37343 ||| 37230 ||| 7450 ||| 
2020 ||| trailer: transformer-based time-wise long term relation modeling for citywide traffic flow prediction. ||| 7322 ||| 37344 ||| 
2020 ||| multi-scale adaptive task attention network for few-shot learning. ||| 37345 ||| 24636 ||| 37346 ||| 37347 ||| 
2018 ||| anmm: ranking short answer texts with attention-based neural matching model. ||| 1041 ||| 1276 ||| 3738 ||| 1140 ||| 
2021 ||| muvam: a multi-view attention-based model for medical visual question answering. ||| 9906 ||| 37348 ||| 9907 ||| 37349 ||| 9910 ||| 37350 ||| 
2021 ||| enconter: entity constrained progressive sequence generation via insertion-based transformer. ||| 24965 ||| 24966 ||| 24967 ||| 
2018 ||| multilingual constituency parsing with self-attention and pre-training. ||| 3143 ||| 3145 ||| 
2021 ||| assistive tele-op: leveraging transformers to collect robotic task demonstrations. ||| 37351 ||| 37352 ||| 37353 ||| 37354 ||| 37355 ||| 23701 ||| 37356 ||| 29809 ||| 37357 ||| 37358 ||| 
2021 ||| vis-top: visual transformer overlay processor. ||| 6595 ||| 37359 ||| 37360 ||| 5743 ||| 37361 ||| 
2020 ||| understanding image captioning models beyond visualizing attention. ||| 37362 ||| 37363 ||| 33055 ||| 37364 ||| 
2020 ||| image super-resolution reconstruction based on attention mechanism and feature fusion. ||| 21223 ||| 21224 ||| 
2020 ||| rsanet: recurrent slice-wise attention network for multiple sclerosis lesion segmentation. ||| 17750 ||| 17751 ||| 17753 ||| 27658 ||| 2973 ||| 27659 ||| 17754 ||| 17755 ||| 9389 ||| 9149 ||| 
2017 ||| towards neural machine translation with latent tree attention. ||| 26835 ||| 19267 ||| 
2020 ||| deep interest with hierarchical attention network for click-through rate prediction. ||| 9616 ||| 9617 ||| 9618 ||| 9619 ||| 9620 ||| 9621 ||| 
2018 ||| pointgrow: autoregressively learned point cloud generation with self-attention. ||| 7399 ||| 7400 ||| 2498 ||| 7401 ||| 7402 ||| 
2022 ||| no parameters left behind: sensitivity guided adaptive learning rate for training large transformer models. ||| 17950 ||| 22809 ||| 22808 ||| 24049 ||| 24050 ||| 1958 ||| 3175 ||| 22811 ||| 
2021 ||| dual-attention residual network for automatic diagnosis of covid-19. ||| 15657 ||| 15656 ||| 16705 ||| 15660 ||| 5474 ||| 
2021 ||| spatial attention point network for deep-learning-based robust autonomous robot motion generation. ||| 37365 ||| 4570 ||| 4571 ||| 4572 ||| 4574 ||| 
2021 ||| what all do audio transformer models hear? probing acoustic representations for language delivery and its structure. ||| 37366 ||| 37367 ||| 17231 ||| 12550 ||| 
2021 ||| couplformer: rethinking vision transformer with coupling attention map. ||| 37368 ||| 37369 ||| 35528 ||| 
2022 ||| pay attention to relations: multi-embeddings for attributed multiplex networks. ||| 37370 ||| 37371 ||| 37372 ||| 
2021 ||| multi-modal motion prediction with transformer-based neural network for autonomous driving. ||| 37373 ||| 37374 ||| 37375 ||| 
2018 ||| on the alignment problem in multi-head attention-based neural machine translation. ||| 21424 ||| 21425 ||| 3454 ||| 
2020 ||| constructing a highlight classifier with an attention based lstm neural network. ||| 37376 ||| 37377 ||| 
2019 ||| fine-grained attention-based video face recognition. ||| 37378 ||| 37379 ||| 37380 ||| 23339 ||| 37381 ||| 
2022 ||| cnn self-attention voice activity detector. ||| 37382 ||| 13080 ||| 
2021 ||| session-aware item-combination recommendation with transformer network. ||| 17108 ||| 17109 ||| 
2021 ||| a global-local attention framework for weakly labelled audio tagging. ||| 12636 ||| 4430 ||| 11418 ||| 
2021 ||| towards efficient cross-modal visual textual retrieval using transformer-encoder deep features. ||| 2689 ||| 2690 ||| 2691 ||| 2692 ||| 2693 ||| 2694 ||| 
2020 ||| learning to fuse sentences with transformers for summarization. ||| 26413 ||| 4952 ||| 4953 ||| 24787 ||| 4956 ||| 523 ||| 
2020 ||| multimodal transformer with pointer network for the dstc8 avsd challenge. ||| 3300 ||| 3302 ||| 
2021 ||| a nir-to-vis face recognition via part adaptive and relation attention module. ||| 11309 ||| 11310 ||| 7228 ||| 
2021 ||| centeratt: fast 2-stage center attention network. ||| 37383 ||| 33342 ||| 37384 ||| 37385 ||| 37386 ||| 
2020 ||| bidirectional representation learning from transformers using multimodal electronic health record data for chronic to predict depression. ||| 31009 ||| 31010 ||| 31011 ||| 31012 ||| 
2018 ||| multistep speed prediction on traffic networks: a graph convolutional sequence-to-sequence learning approach with attention mechanism. ||| 3918 ||| 5860 ||| 37387 ||| 15832 ||| 37388 ||| 
2020 ||| sparse, dense, and attentional representations for text retrieval. ||| 4764 ||| 13316 ||| 3185 ||| 37389 ||| 
2021 ||| weaving attention u-net: a novel hybrid cnn and attention-based method for organs-at-risk segmentation in head and neck ct images. ||| 35769 ||| 14598 ||| 37390 ||| 35771 ||| 35770 ||| 
2020 ||| context-aware cross-attention for non-autoregressive translation. ||| 3794 ||| 3038 ||| 8976 ||| 1756 ||| 3041 ||| 
2020 ||| on estimating gaze by self-attention augmented convolutions. ||| 35253 ||| 8405 ||| 
2018 ||| logic attention based neighborhood aggregation for inductive knowledge graph embedding. ||| 17698 ||| 1399 ||| 1400 ||| 2238 ||| 
2019 ||| gated group self-attention for answer selection. ||| 1696 ||| 22922 ||| 37391 ||| 22923 ||| 37392 ||| 
2018 ||| multi-level attention model for weakly supervised audio classification. ||| 13518 ||| 13519 ||| 12618 ||| 10875 ||| 
2020 ||| attention beam: an image captioning approach. ||| 17654 ||| 3835 ||| 
2021 ||| on the power of saturated transformers: a view from circuit complexity. ||| 26737 ||| 3441 ||| 23971 ||| 3277 ||| 
2020 ||| correction of faulty background knowledge based on condition aware and revise transformer for question answering. ||| 8994 ||| 37393 ||| 8995 ||| 8996 ||| 8997 ||| 
2021 ||| solving arithmetic word problems with transformers and preprocessing of problem text. ||| 20512 ||| 20513 ||| 
2018 ||| placement delivery array design via attention-based deep neural network. ||| 37394 ||| 37395 ||| 28953 ||| 12186 ||| 37396 ||| 
2021 ||| discovering spatial relationships by transformers for domain generalization. ||| 37397 ||| 37398 ||| 
2019 ||| understanding and improving transformer from a multi-particle dynamic system point of view. ||| 37399 ||| 22852 ||| 18012 ||| 2620 ||| 5707 ||| 4789 ||| 3807 ||| 4791 ||| 
2020 ||| hand-crafted attention is all you need? a study of attention on self-supervised audio transformer. ||| 7809 ||| 37400 ||| 37401 ||| 12724 ||| 12644 ||| 
2019 ||| deja-vu: double feature presentation in deep transformer networks. ||| 12303 ||| 12304 ||| 11975 ||| 5450 ||| 11973 ||| 2120 ||| 11757 ||| 12305 ||| 
2021 ||| person re-identification with a locally aware transformer. ||| 3058 ||| 37402 ||| 37403 ||| 
2021 ||| attentional prototype inference for few-shot semantic segmentation. ||| 24993 ||| 19316 ||| 7239 ||| 13205 ||| 1930 ||| 18223 ||| 1932 ||| 
2021 ||| what's in your head? emergent behaviour in multi-task transformer models. ||| 26354 ||| 26471 ||| 26472 ||| 26356 ||| 
2021 ||| multimodal continuous visual attention mechanisms. ||| 2871 ||| 7901 ||| 3369 ||| 3370 ||| 7902 ||| 
2021 ||| causal attention for unbiased visual recognition. ||| 2481 ||| 2482 ||| 2483 ||| 2484 ||| 
2020 ||| bertology meets biology: interpreting attention in protein language models. ||| 3447 ||| 24004 ||| 15983 ||| 3287 ||| 19267 ||| 24005 ||| 
2020 ||| cloud removal for remote sensing imagery via spatial attention generative adversarial network. ||| 37404 ||| 
2022 ||| dbt-net: dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement. ||| 4382 ||| 14674 ||| 1341 ||| 4383 ||| 37405 ||| 4384 ||| 
2018 ||| show, attend and translate: unsupervised image translation with self-regularization and attention. ||| 497 ||| 37406 ||| 37407 ||| 9407 ||| 14521 ||| 
2017 ||| attention-aware face hallucination via deep reinforcement learning. ||| 19223 ||| 2315 ||| 19224 ||| 1686 ||| 1800 ||| 
2020 ||| spatial attention pyramid network for unsupervised domain adaptation. ||| 8748 ||| 8749 ||| 8750 ||| 8751 ||| 8752 ||| 8753 ||| 8754 ||| 
2021 ||| transformer transforms salient object detection and camouflaged object detection. ||| 37408 ||| 875 ||| 37409 ||| 2240 ||| 37410 ||| 37411 ||| 37412 ||| 1861 ||| 2475 ||| 
2019 ||| pay attention to the activations: a modular attention mechanism for fine-grained image recognition. ||| 8668 ||| 37413 ||| 11005 ||| 37414 ||| 8670 ||| 8669 ||| 8671 ||| 8672 ||| 37415 ||| 
2020 ||| natural image matting via guided contextual attention. ||| 5276 ||| 5277 ||| 
2022 ||| zero-shot long-form voice cloning with dynamic convolution attention. ||| 37416 ||| 37417 ||| 
2018 ||| attention-gated networks for improving ultrasound scan plane detection. ||| 31282 ||| 27946 ||| 12373 ||| 27418 ||| 37418 ||| 27478 ||| 27844 ||| 27420 ||| 
2022 ||| switch trajectory transformer with distributional value approximation for multi-task reinforcement learning. ||| 37419 ||| 37420 ||| 37421 ||| 
2021 ||| hybrid attention network based on progressive embedding scale-context for crowd counting. ||| 31977 ||| 19909 ||| 19908 ||| 1302 ||| 1900 ||| 
2020 ||| on the effectiveness of vision transformers for zero-shot face anti-spoofing. ||| 18617 ||| 7111 ||| 18618 ||| 
2021 ||| efficient pre-training objectives for transformers. ||| 25606 ||| 37422 ||| 3374 ||| 
2021 ||| gtn-ed: event detection using graph transformer networks. ||| 37423 ||| 37424 ||| 37425 ||| 3399 ||| 4870 ||| 37426 ||| 
2021 ||| lavt: language-aware vision transformer for referring image segmentation. ||| 1381 ||| 7079 ||| 19771 ||| 472 ||| 2335 ||| 2160 ||| 
2019 ||| factor graph attention. ||| 9305 ||| 18900 ||| 9306 ||| 8566 ||| 
2021 ||| floorlevel-net: recognizing floor-level lines with height-attention-guided multi-task learning. ||| 37427 ||| 19482 ||| 1739 ||| 
2018 ||| variational self-attention model for sentence representation. ||| 817 ||| 37428 ||| 37429 ||| 
2022 ||| wavelet-attention cnn for image classification. ||| 9055 ||| 
2019 ||| characterizing collective attention via descriptor context in public discussions of crisis events. ||| 13314 ||| 13315 ||| 13316 ||| 
2021 ||| covid-19 fake news detection using bidirectional encoder representations from transformers based models. ||| 37430 ||| 37431 ||| 37432 ||| 37433 ||| 
2021 ||| exploring corruption robustness: inductive biases in vision transformers and mlp-mixers. ||| 37434 ||| 37435 ||| 37436 ||| 37437 ||| 37438 ||| 
2019 ||| your local gan: designing two dimensional local attention mechanisms for generative models. ||| 9180 ||| 9181 ||| 14048 ||| 9182 ||| 
2019 ||| attention-based convolutional neural network for weakly labeled human activities recognition with wearable sensors. ||| 11361 ||| 532 ||| 241 ||| 
2021 ||| discodvt: generating long text with discourse-aware discrete variational transformer. ||| 26282 ||| 9008 ||| 
2021 ||| clickbait headline detection in indonesian news sites using multilingual bidirectional encoder representations from transformers (m-bert). ||| 37439 ||| 37440 ||| 37441 ||| 37442 ||| 
2018 ||| action recognition with visual attention on skeleton images. ||| 2416 ||| 20094 ||| 20095 ||| 2166 ||| 
2021 ||| image denoising using attention-residual convolutional neural networks. ||| 8380 ||| 8381 ||| 8384 ||| 8382 ||| 8383 ||| 1994 ||| 37443 ||| 
2022 ||| attention-based dual supervised decoder for rgbd semantic segmentation. ||| 1420 ||| 11466 ||| 37444 ||| 37445 ||| 8722 ||| 
2020 ||| limits to depth efficiencies of self-attention. ||| 9321 ||| 9322 ||| 9323 ||| 9324 ||| 9325 ||| 
2017 ||| dynamic computational time for visual attention. ||| 7803 ||| 208 ||| 2530 ||| 2531 ||| 7804 ||| 
2021 ||| multimodal-boost: multimodal medical image super-resolution using multi-attention network with wavelet transform. ||| 37446 ||| 37447 ||| 37448 ||| 37449 ||| 37450 ||| 37451 ||| 37452 ||| 
2020 ||| better distractions: transformer-based distractor generation and multiple choice question filtering. ||| 37453 ||| 37454 ||| 37455 ||| 
2020 ||| multiscale attention guided network for covid-19 detection using chest x-ray images. ||| 30994 ||| 17295 ||| 11634 ||| 1224 ||| 1235 ||| 27349 ||| 25766 ||| 
2021 ||| cross-sean: a cross-stitch semi-supervised neural attention model for covid-19 fake news detection. ||| 15159 ||| 15158 ||| 29258 ||| 15161 ||| 3835 ||| 
2021 ||| efficient transformers in reinforcement learning using actor-learner distillation. ||| 18744 ||| 3247 ||| 
2021 ||| semantic segmentation on vspw dataset through aggregation of transformer models. ||| 37456 ||| 37457 ||| 37458 ||| 
2021 ||| sa-matd3: self-attention-based multi-agent continuous control method in cooperative environments. ||| 19109 ||| 32042 ||| 8608 ||| 32043 ||| 
2020 ||| attention-based quantum tomography. ||| 37459 ||| 37460 ||| 14702 ||| 37461 ||| 37462 ||| 37463 ||| 
2019 ||| deep modular co-attention networks for visual question answering. ||| 1753 ||| 1754 ||| 19336 ||| 1756 ||| 2398 ||| 
2021 ||| stableemit: selection probability discount for reducing emission latency of streaming monotonic attention asr. ||| 12682 ||| 4418 ||| 
2018 ||| sdnet: contextualized attention-based deep network for conversational question answering. ||| 6064 ||| 34352 ||| 34353 ||| 
2018 ||| learning to match transient sound events using attentional similarity for few-shot sound recognition. ||| 4373 ||| 12465 ||| 12466 ||| 4374 ||| 
2019 ||| transformer to cnn: label-scarce distillation for efficient text classification. ||| 36959 ||| 36960 ||| 36961 ||| 
2022 ||| a squeeze-and-excitation and transformer based cross-task system for environmental sound recognition. ||| 4375 ||| 4377 ||| 4376 ||| 37464 ||| 
2018 ||| seq2graph: discovering dynamic dependencies from multivariate time series with multi-level attention. ||| 37465 ||| 37466 ||| 37467 ||| 
2021 ||| progressive transformer-based generation of radiology reports. ||| 26641 ||| 26642 ||| 26643 ||| 26644 ||| 26645 ||| 
2021 ||| m2a: motion aware attention for accurate video action recognition. ||| 37468 ||| 7865 ||| 33644 ||| 
2022 ||| pami-ad: an activity detector exploiting part-attention and motion information in surveillance videos. ||| 37469 ||| 37470 ||| 37471 ||| 37472 ||| 30158 ||| 
2021 ||| (asna) an attention-based siamese-difference neural network with surrogate ranking loss function for perceptual image quality assessment. ||| 18773 ||| 18774 ||| 
2018 ||| attention u-net: learning where to look for the pancreas. ||| 27946 ||| 31282 ||| 2310 ||| 37473 ||| 27945 ||| 27351 ||| 37474 ||| 22333 ||| 37475 ||| 37476 ||| 27478 ||| 27844 ||| 27420 ||| 
2020 ||| attentional bottleneck: towards an interpretable deep driving network. ||| 2041 ||| 19298 ||| 
2019 ||| structure tree-lstm: structure-aware attentional document encoders. ||| 3566 ||| 6979 ||| 6978 ||| 23895 ||| 
2021 ||| multi-view stereo network with attention thin volume. ||| 37477 ||| 
2018 ||| deep attention-guided hashing. ||| 37478 ||| 37479 ||| 37480 ||| 37481 ||| 
2020 ||| on the importance of local information in transformer based models. ||| 18066 ||| 18067 ||| 3327 ||| 11768 ||| 3328 ||| 
2017 ||| predicting human interaction via relative attention model. ||| 23472 ||| 8832 ||| 8532 ||| 
2020 ||| safcar: structured attention fusion for compositional action recognition. ||| 2337 ||| 2339 ||| 
2020 ||| hyperspectral image classification based on multi-scale residual network with attention mechanism. ||| 21369 ||| 37482 ||| 21288 ||| 
2019 ||| scram: spatially coherent randomized attention maps. ||| 37483 ||| 37484 ||| 21467 ||| 37485 ||| 37486 ||| 37487 ||| 37488 ||| 
2021 ||| pay more attention to history: a context modeling strategy for conversational text-to-sql. ||| 15236 ||| 37489 ||| 37490 ||| 26440 ||| 293 ||| 2349 ||| 
2021 ||| jointly learning truth-conditional denotations and groundings using parallel attention. ||| 32696 ||| 32698 ||| 32697 ||| 
2019 ||| interpretable icd code embeddings with self- and mutual-attention mechanisms. ||| 37491 ||| 9647 ||| 1032 ||| 
2021 ||| ia-gcn: interpretable attention based graph convolutional network for disease prediction. ||| 15676 ||| 37492 ||| 13628 ||| 
2022 ||| learning patch-to-cluster attention in vision transformer. ||| 37493 ||| 37494 ||| 37495 ||| 2084 ||| 
2019 ||| from balustrades to pierre vinken: looking for syntax in transformer self-attentions. ||| 20958 ||| 20959 ||| 
2021 ||| lesion segmentation and recist diameter prediction via click-driven attention and dual-path connection. ||| 14910 ||| 2379 ||| 27490 ||| 27704 ||| 3395 ||| 705 ||| 27705 ||| 27706 ||| 27491 ||| 
2022 ||| dkma-uld: domain knowledge augmented multi-head attention based robust universal lesion detection. ||| 37496 ||| 37497 ||| 605 ||| 607 ||| 
2020 ||| alanet: adaptive latent attention network forjoint video deblurring and interpolation. ||| 19614 ||| 19615 ||| 19616 ||| 
2019 ||| attention for inference compilation. ||| 34282 ||| 37498 ||| 37499 ||| 37500 ||| 37501 ||| 34284 ||| 
2021 ||| megan: memory enhanced graph attention network for space-time video super-resolution. ||| 7412 ||| 7413 ||| 7414 ||| 7415 ||| 7416 ||| 7417 ||| 
2020 ||| towards character-level transformer nmt by finetuning subword systems. ||| 3591 ||| 26406 ||| 
2021 ||| efficient training of audio transformers with patchout. ||| 35705 ||| 37502 ||| 12534 ||| 35700 ||| 11898 ||| 
2019 ||| intentional attention mask transformation for robust cnn classification. ||| 19136 ||| 19137 ||| 
2020 ||| glancing transformer for non-autoregressive neural machine translation. ||| 3426 ||| 2736 ||| 3427 ||| 3428 ||| 3429 ||| 1219 ||| 1223 ||| 3034 ||| 
2022 ||| efficient non-local contrastive attention for image super-resolution. ||| 923 ||| 19511 ||| 33866 ||| 6116 ||| 6117 ||| 1921 ||| 
2017 ||| attention-based mixture density recurrent networks for history-based recommendation. ||| 29147 ||| 3008 ||| 
2021 ||| dot: an efficient double transformer for nlp tasks with tables. ||| 3829 ||| 3830 ||| 3831 ||| 33588 ||| 
2019 ||| efficient attention mechanism for handling all the interactions between many inputs with application to visual dialog. ||| 8739 ||| 8740 ||| 8741 ||| 
2021 ||| a video is worth three views: trigeminal transformers for video-based person re-identification. ||| 37503 ||| 19691 ||| 37504 ||| 1700 ||| 37505 ||| 19068 ||| 
2021 ||| semantics-aware attention improves neural machine translation. ||| 37506 ||| 37507 ||| 37508 ||| 
2021 ||| automated tabulation of clinical trial results: a joint entity and relation extraction approach with transformer-based language representations. ||| 37509 ||| 37510 ||| 
2020 ||| peak detection on data independent acquisition mass spectrometry data with semisupervised convolutional transformers. ||| 37511 ||| 37512 ||| 2693 ||| 
2020 ||| trade: transformers for density estimation. ||| 37513 ||| 37514 ||| 32629 ||| 18298 ||| 
2020 ||| multimodal matching transformer for live commenting. ||| 10199 ||| 9837 ||| 10200 ||| 3174 ||| 10201 ||| 880 ||| 
2018 ||| deep learning with attention to predict gestational age of the fetal brain. ||| 37515 ||| 37516 ||| 37517 ||| 37518 ||| 37519 ||| 37520 ||| 37521 ||| 37522 ||| 37523 ||| 
2021 ||| a self-distillation embedded supervised affinity attention model for few-shot segmentation. ||| 1872 ||| 36362 ||| 8011 ||| 8012 ||| 27118 ||| 
2021 ||| a dual-attention neural network for pun location and using pun-gloss pairs for interpretation. ||| 21169 ||| 21170 ||| 6928 ||| 37524 ||| 350 ||| 349 ||| 
2019 ||| transformers with convolutional context for asr. ||| 12488 ||| 37525 ||| 24017 ||| 
2021 ||| hierarchical task learning from language instructions with unified transformers and self-monitoring. ||| 3757 ||| 3758 ||| 
2020 ||| memory controlled sequential self attention for sound recognition. ||| 14263 ||| 14264 ||| 14265 ||| 935 ||| 
2021 ||| sparse spatial transformers for few-shot learning. ||| 37345 ||| 24636 ||| 37346 ||| 37347 ||| 
2021 ||| ndt-transformer: large-scale 3d point cloud localisation using the normal distribution transform representation. ||| 21799 ||| 6574 ||| 21800 ||| 21801 ||| 5089 ||| 21802 ||| 21803 ||| 
2021 ||| an end-to-end transformer model for 3d object detection. ||| 2055 ||| 1663 ||| 1889 ||| 
2021 ||| ssat: a symmetric semantic-aware transformer network for makeup transfer and removal. ||| 37526 ||| 37527 ||| 12692 ||| 
2021 ||| osteoporosis prescreening using panoramic radiographs through a deep convolutional neural network with attention mechanism. ||| 7245 ||| 37528 ||| 1132 ||| 37529 ||| 2163 ||| 
2022 ||| multi-modal learning for au detection based on multi-head fused transformers. ||| 586 ||| 5714 ||| 
2020 ||| deep feature mining via attention-based bilstm-gcn for human motor imagery recognition. ||| 34096 ||| 34095 ||| 6588 ||| 37530 ||| 34097 ||| 438 ||| 37531 ||| 37532 ||| 15541 ||| 
2020 ||| crosstransformers: spatially-aware few-shot transfer. ||| 8786 ||| 9414 ||| 1997 ||| 
2021 ||| iterative se(3)-transformers. ||| 25870 ||| 25871 ||| 25872 ||| 22290 ||| 
2021 ||| brain tumor segmentation and survival prediction using 3d attention unet. ||| 21821 ||| 19132 ||| 24671 ||| 24672 ||| 24673 ||| 21822 ||| 
2018 ||| arbitrary style transfer with style-attentional networks. ||| 19376 ||| 19377 ||| 
2018 ||| improving distant supervision with maxpooled attention and sentence-level supervision. ||| 3124 ||| 37533 ||| 37534 ||| 
2021 ||| scalable visual transformers with hierarchical pooling. ||| 2056 ||| 2057 ||| 2058 ||| 2059 ||| 1691 ||| 
2018 ||| self-attention recurrent network for saliency detection. ||| 37535 ||| 6281 ||| 6279 ||| 
2020 ||| the costs and benefits of goal-directed attention in deep convolutional neural networks. ||| 34716 ||| 34717 ||| 34718 ||| 
2021 ||| multilingual speech translation with unified transformer: huawei noah's ark lab at iwslt 2021. ||| 3634 ||| 3635 ||| 3443 ||| 
2021 ||| transweather: transformer-based restoration of images degraded by adverse weather conditions. ||| 27805 ||| 37536 ||| 18609 ||| 
2017 ||| an attention-based deep net for learning to rank. ||| 37537 ||| 37538 ||| 
2020 ||| contrastive triple extraction with generative transformer. ||| 17986 ||| 17987 ||| 17988 ||| 17989 ||| 12432 ||| 9686 ||| 17990 ||| 
2021 ||| sparse mlp for image recognition: is self-attention really necessary? ||| 32613 ||| 23303 ||| 7911 ||| 23304 ||| 37539 ||| 7912 ||| 
2020 ||| inno at semeval-2020 task 11: leveraging pure transformer for multi-class propaganda detection. ||| 37540 ||| 37541 ||| 
2021 ||| making attention mechanisms more robust and interpretable with virtual adversarial training for semi-supervised text classification. ||| 25316 ||| 25317 ||| 
2020 ||| transformer hawkes process. ||| 22808 ||| 22809 ||| 22810 ||| 22811 ||| 8949 ||| 
2021 ||| learning synergistic attention for light field salient object detection. ||| 340 ||| 4055 ||| 12760 ||| 37542 ||| 32912 ||| 32913 ||| 21726 ||| 2037 ||| 
2022 ||| self-attention for incomplete utterance rewriting. ||| 7826 ||| 37543 ||| 701 ||| 704 ||| 705 ||| 
2020 ||| improving natural language processing tasks with human gaze-guided neural attention. ||| 9269 ||| 9270 ||| 8346 ||| 3831 ||| 8348 ||| 
2020 ||| attention-based transformers for instance segmentation of cells in microstructures. ||| 16697 ||| 16698 ||| 16699 ||| 
2022 ||| manner: multi-view attention network for noise erasure. ||| 37544 ||| 37545 ||| 33386 ||| 37546 ||| 33387 ||| 
2021 ||| spelling correction with denoising transformer. ||| 37547 ||| 37548 ||| 
2018 ||| set transformer. ||| 9314 ||| 9356 ||| 22800 ||| 22801 ||| 22802 ||| 22803 ||| 
2019 ||| ouroboros: on accelerating training of transformer-based language models. ||| 9141 ||| 9142 ||| 3136 ||| 9143 ||| 1032 ||| 
2021 ||| an attention model to analyse the risk of agitation and urinary tract infections in people with dementia. ||| 37549 ||| 37550 ||| 37551 ||| 37552 ||| 37553 ||| 37554 ||| 37555 ||| 37556 ||| 
2019 ||| repurposing decoder-transformer language models for abstractive summarization. ||| 37557 ||| 37558 ||| 37559 ||| 
2022 ||| hypertransformer: a textural and spectral feature fusion transformer for pansharpening. ||| 32245 ||| 18609 ||| 
2021 ||| studying the effects of self-attention for medical image analysis. ||| 7915 ||| 5369 ||| 7916 ||| 7917 ||| 7918 ||| 
2020 ||| dual-attention gan for large-pose face frontalization. ||| 5669 ||| 5670 ||| 5671 ||| 1734 ||| 
2021 ||| a lightweight graph transformer network for human mesh reconstruction from 2d human pose. ||| 2225 ||| 2227 ||| 2228 ||| 25658 ||| 37560 ||| 2230 ||| 
2021 ||| e(2) equivariant self-attention for radio astronomy. ||| 32752 ||| 37561 ||| 37562 ||| 37563 ||| 
2018 ||| deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization. ||| 8570 ||| 8571 ||| 127 ||| 208 ||| 
2019 ||| griddehazenet: attention-based multi-scale network for image dehazing. ||| 1782 ||| 1783 ||| 1784 ||| 1785 ||| 
2019 ||| context-aware self-attention networks. ||| 3037 ||| 595 ||| 3039 ||| 3040 ||| 3309 ||| 3041 ||| 
2021 ||| content-aware directed propagation network with pixel adaptive kernel attention. ||| 37564 ||| 37565 ||| 37566 ||| 7938 ||| 
2022 ||| attentional feature refinement and alignment network for aircraft detection in sar imagery. ||| 10646 ||| 80 ||| 29989 ||| 31528 ||| 30332 ||| 2014 ||| 
2021 ||| certified patch robustness via smoothed vision transformers. ||| 37567 ||| 37568 ||| 37569 ||| 37570 ||| 
2020 ||| a further study of unsupervised pre-training for transformer based speech recognition. ||| 12257 ||| 12258 ||| 12259 ||| 12260 ||| 12261 ||| 12262 ||| 12263 ||| 11688 ||| 
2022 ||| an intellectual property entity recognition method based on transformer and technological word information. ||| 20110 ||| 7688 ||| 25201 ||| 
2017 ||| learning what to look in chest x-rays with a recurrent visual attention model. ||| 31233 ||| 31236 ||| 
2018 ||| sequential image-based attention network for inferring force estimation without haptic sensor. ||| 37571 ||| 37572 ||| 37573 ||| 37574 ||| 37575 ||| 
2018 ||| attention solves your tsp. ||| 23991 ||| 9255 ||| 
2021 ||| stepping back to smiles transformers for fast molecular representation inference. ||| 37576 ||| 37577 ||| 37578 ||| 37579 ||| 
2020 ||| modeling document interactions for learning to rank with regularized self-attention. ||| 37580 ||| 33289 ||| 
2020 ||| modeling limited attention in opinion dynamics by topological interactions. ||| 2979 ||| 2980 ||| 2981 ||| 
2021 ||| festa: flow estimation via spatial-temporal attention for scene point clouds. ||| 18952 ||| 18953 ||| 18954 ||| 18955 ||| 18956 ||| 
2018 ||| deep metric learning by online soft mining and class-aware attention. ||| 18060 ||| 18061 ||| 18062 ||| 18063 ||| 18064 ||| 
2019 ||| an attention-based multi-resolution model for prostate whole slide imageclassification and localization. ||| 7289 ||| 6653 ||| 37581 ||| 37582 ||| 31010 ||| 31012 ||| 
2021 ||| han: an efficient hierarchical self-attention network for skeleton-based gesture recognition. ||| 2454 ||| 7830 ||| 2252 ||| 2255 ||| 
2021 ||| tsformer: time series transformer for tourism demand forecasting. ||| 37583 ||| 585 ||| 37584 ||| 
2019 ||| image super-resolution via residual blended attention generative adversarial network with dual discriminators. ||| 
2019 ||| har-net: joint learning of hybrid attention for single-stage object detection. ||| 19032 ||| 11221 ||| 
2022 ||| handcrafted histological transformer (h2t): unsupervised representation of whole slide images. ||| 37585 ||| 37586 ||| 37587 ||| 34697 ||| 
2020 ||| self-attention attribution: interpreting information interactions inside transformer. ||| 18234 ||| 3171 ||| 3174 ||| 3617 ||| 
2020 ||| exploring transformers for large-scale speech recognition. ||| 14310 ||| 12031 ||| 12179 ||| 12033 ||| 
2021 ||| how knowledge graph and attention help? a quantitative analysis into bag-level relation extraction. ||| 3602 ||| 3603 ||| 3604 ||| 3605 ||| 
2019 ||| aanet: attribute attention network for person re-identifications. ||| 19293 ||| 19294 ||| 19295 ||| 
2020 ||| coot: cooperative hierarchical transformer for video-text representation learning. ||| 9160 ||| 9161 ||| 9162 ||| 9163 ||| 
2021 ||| progressive co-attention network for fine-grained visual classification. ||| 16627 ||| 1482 ||| 1484 ||| 786 ||| 
2021 ||| attention-based multi-channel speaker verification with ad-hoc microphone arrays. ||| 4378 ||| 4379 ||| 4380 ||| 4381 ||| 
2019 ||| co-attention hierarchical network: generating coherent long distractors for reading comprehension. ||| 18231 ||| 18232 ||| 18233 ||| 
2020 ||| knowing what to listen to: early attention for deep speech representation learning. ||| 37588 ||| 20089 ||| 
2020 ||| attention transfer network for aspect-level sentiment classification. ||| 11633 ||| 4784 ||| 4790 ||| 
2022 ||| neuroplastic graph attention networks for nuclei segmentation in histopathology images. ||| 37589 ||| 30434 ||| 
2020 ||| s-transformer: segment-transformer for robust neural speech synthesis. ||| 232 ||| 37590 ||| 6879 ||| 37591 ||| 
2021 ||| what context features can transformer language models use? ||| 3204 ||| 3205 ||| 
2021 ||| mesa: a memory-saving training framework for transformers. ||| 2056 ||| 26814 ||| 2059 ||| 2058 ||| 1691 ||| 2057 ||| 
2020 ||| dual-stream maximum self-attention multi-instance learning. ||| 6502 ||| 20524 ||| 
2021 ||| eformer: edge enhancement based transformer for medical image denoising. ||| 7978 ||| 37592 ||| 37593 ||| 37594 ||| 37595 ||| 
2020 ||| structure-aware pre-training for table understanding with tree-based transformers. ||| 25322 ||| 18742 ||| 25323 ||| 2383 ||| 5985 ||| 25324 ||| 8893 ||| 
2021 ||| accurate prediction of free solvation energy of organic molecules via graph attention network and message passing neural network from pairwise atomistic interactions. ||| 37596 ||| 37597 ||| 
2020 ||| motion segmentation using frequency domain transformer networks. ||| 407 ||| 409 ||| 
2021 ||| ssast: self-supervised audio spectrogram transformer. ||| 1708 ||| 37598 ||| 14430 ||| 12254 ||| 
2021 ||| levi graph amr parser using heterogeneous attention. ||| 1593 ||| 375 ||| 
2018 ||| adcrowdnet: an attention-injective deformable convolutional network for crowd understanding. ||| 9027 ||| 18711 ||| 18041 ||| 18712 ||| 6044 ||| 18713 ||| 
2020 ||| tourism demand forecasting with tourist attention: an ensemble deep learning approach. ||| 37599 ||| 37600 ||| 30693 ||| 37601 ||| 
2021 ||| gophormer: ego-graph transformer for node classification. ||| 37602 ||| 9991 ||| 37603 ||| 25264 ||| 37604 ||| 7725 ||| 9574 ||| 9022 ||| 
2020 ||| large scale legal text classification using transformer models. ||| 35508 ||| 35509 ||| 37605 ||| 
2018 ||| question type guided attention in visual question answering. ||| 8611 ||| 8612 ||| 8613 ||| 8614 ||| 
2019 ||| interrogating the explanatory power of attention in neural machine translation. ||| 14942 ||| 37606 ||| 37607 ||| 14944 ||| 
2022 ||| interpreting arabic transformer models. ||| 37608 ||| 26437 ||| 26435 ||| 26436 ||| 
2018 ||| an attention-based approach for single image super resolution. ||| 2487 ||| 20312 ||| 5192 ||| 6560 ||| 20313 ||| 12186 ||| 20314 ||| 
2021 ||| learning delaunay triangulation using self-attention and domain knowledge. ||| 37609 ||| 37610 ||| 37611 ||| 
2018 ||| cold-start aware user and product attention for sentiment classification. ||| 3713 ||| 3714 ||| 3715 ||| 3716 ||| 
2019 ||| learning deep transformer models for machine translation. ||| 3304 ||| 3305 ||| 2333 ||| 3306 ||| 3307 ||| 3039 ||| 3040 ||| 
2018 ||| glac net: glocal attention cascading networks for multi-image cued story generation. ||| 37612 ||| 37613 ||| 37614 ||| 37615 ||| 8580 ||| 
2019 ||| pseudo random number generators: attention for a newly proposed generator. ||| 37616 ||| 37617 ||| 37618 ||| 
2022 ||| contrastive embedding distribution refinement and entropy-aware attention for 3d point cloud classification. ||| 2320 ||| 14181 ||| 35130 ||| 37619 ||| 28672 ||| 35131 ||| 
2020 ||| context-based transformer models for answer sentence selection. ||| 15070 ||| 3374 ||| 
2020 ||| predicting goal-directed human attention using inverse reinforcement learning. ||| 19321 ||| 19322 ||| 19323 ||| 19324 ||| 19325 ||| 18710 ||| 18884 ||| 7365 ||| 
2021 ||| ted-net: convolution-free t2t vision transformer-based encoder-decoder dilation network for low-dose ct denoising. ||| 27466 ||| 27467 ||| 27468 ||| 
2021 ||| pushing the limits of rule reasoning in transformers through natural language satisfiability. ||| 23388 ||| 3319 ||| 
2021 ||| learning tracking representations via dual-branch fully transformer networks. ||| 7909 ||| 7910 ||| 7911 ||| 2098 ||| 7912 ||| 
2017 ||| variational attention for sequence-to-sequence models. ||| 11679 ||| 1389 ||| 11680 ||| 11681 ||| 
2020 ||| anchor attention for hybrid crowd forecasts aggregation. ||| 3907 ||| 3369 ||| 3908 ||| 3909 ||| 3910 ||| 3911 ||| 
2021 ||| 4d attention-based neural network for eeg emotion recognition. ||| 37620 ||| 37621 ||| 13148 ||| 37622 ||| 37623 ||| 
2022 ||| game-on: graph attention network based multimodal fusion for fake news detection. ||| 37624 ||| 37625 ||| 37626 ||| 35656 ||| 11605 ||| 
2021 ||| attention actor-critic algorithm for multi-agent constrained co-operative reinforcement learning. ||| 3902 ||| 3903 ||| 3904 ||| 3905 ||| 
2019 ||| orthogonality constrained multi-head attention for keyword spotting. ||| 13918 ||| 13919 ||| 13920 ||| 13921 ||| 13922 ||| 13923 ||| 
2022 ||| self-attention neural bag-of-features. ||| 36945 ||| 926 ||| 31739 ||| 
2021 ||| mect: multi-metadata embedding based cross-transformer for chinese named entity recognition. ||| 3608 ||| 3609 ||| 3610 ||| 
2020 ||| occluded prohibited items detection: an x-ray security inspection benchmark and de-occlusion attention module. ||| 19784 ||| 19785 ||| 19786 ||| 19787 ||| 8750 ||| 17695 ||| 
2022 ||| coarse-to-fine sparse transformer for hyperspectral image reconstruction. ||| 21242 ||| 19804 ||| 19802 ||| 19803 ||| 5958 ||| 1730 ||| 7815 ||| 7814 ||| 
2019 ||| analysing coreference in transformer outputs. ||| 4358 ||| 4359 ||| 4360 ||| 3207 ||| 
2021 ||| geometry-contrastive transformer for generalized 3d pose transfer. ||| 37627 ||| 435 ||| 33001 ||| 437 ||| 33480 ||| 
2020 ||| should we hard-code the recurrence concept or learn it instead ? exploring the transformer architecture for audio-visual speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2022 ||| attention cannot be an explanation. ||| 37628 ||| 18982 ||| 
2022 ||| attribute surrogates learning and spectral tokens pooling in transformers for few-shot learning. ||| 37629 ||| 37630 ||| 37631 ||| 7913 ||| 37632 ||| 1801 ||| 12503 ||| 
2018 ||| fpan: fine-grained and progressive attention localization network for data retrieval. ||| 37633 ||| 9705 ||| 8718 ||| 35144 ||| 35145 ||| 
2021 ||| temporal convolutional networks and transformers for classifying the sleep stage in awake or asleep using pulse oximetry signals. ||| 37634 ||| 37635 ||| 29722 ||| 37636 ||| 
2020 ||| defraudnet: end2end fingerprint spoof detection using patch level attention. ||| 7346 ||| 7347 ||| 7348 ||| 
2020 ||| on layer normalization in the transformer architecture. ||| 22735 ||| 22736 ||| 18012 ||| 11162 ||| 22737 ||| 17849 ||| 22738 ||| 1444 ||| 3807 ||| 4791 ||| 
2019 ||| u-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation. ||| 2473 ||| 23920 ||| 23921 ||| 23922 ||| 
2018 ||| modeling and predicting popularity dynamics via deep learning attention mechanism. ||| 25153 ||| 9472 ||| 7030 ||| 4121 ||| 37637 ||| 
2021 ||| interpreting deep learning based cerebral palsy prediction with channel attention. ||| 6103 ||| 6104 ||| 6105 ||| 6106 ||| 6107 ||| 
2018 ||| why self-attention? a targeted evaluation of neural machine translation architectures. ||| 21389 ||| 26328 ||| 3831 ||| 4860 ||| 3847 ||| 
2020 ||| enhancing the interpretability of deep models in heathcare through attention: application to glucose forecasting for diabetic people. ||| 37638 ||| 37639 ||| 6177 ||| 
2020 ||| discrete variational attention models for language generation. ||| 593 ||| 594 ||| 596 ||| 597 ||| 598 ||| 
2020 ||| an optimal transport kernel for feature aggregation and its relationship to attention. ||| 17321 ||| 23966 ||| 23967 ||| 23968 ||| 2264 ||| 
2021 ||| localvit: bringing locality to vision transformers. ||| 37115 ||| 3433 ||| 7812 ||| 7815 ||| 7814 ||| 
2020 ||| multi-encoder-decoder transformer for code-switching speech recognition. ||| 14361 ||| 14362 ||| 13947 ||| 14363 ||| 12494 ||| 
2021 ||| resolution-invariant person reid based on feature transformation and self-weighted attention. ||| 11375 ||| 11376 ||| 11377 ||| 11378 ||| 
2021 ||| long-term series forecasting with query selector - efficient model of sparse attention. ||| 37640 ||| 20399 ||| 37641 ||| 37642 ||| 37643 ||| 
2021 |||  beyond: transformer language models in information systems research. ||| 37644 ||| 37645 ||| 
2020 ||| beyond 512 tokens: siamese multi-depth transformer-based hierarchical encoder for document matching. ||| 1041 ||| 1042 ||| 130 ||| 1043 ||| 1044 ||| 
2022 ||| hdam: heuristic difference attention module for convolutional neural networks. ||| 35728 ||| 35729 ||| 
2021 ||| ratchet: medical transformer for chest x-ray diagnosis and reporting. ||| 27475 ||| 27966 ||| 14912 ||| 27478 ||| 
2017 ||| attentional network for visual object detection. ||| 37646 ||| 37647 ||| 32357 ||| 37648 ||| 
2021 ||| attention approximates sparse distributed memory. ||| 37649 ||| 37650 ||| 
2021 ||| nisqa: a deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets. ||| 12197 ||| 14538 ||| 14539 ||| 12198 ||| 3831 ||| 
2018 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| 9832 ||| 9833 ||| 
2021 ||| adaptively multi-view and temporal fusing transformer for 3d human pose estimation. ||| 6624 ||| 37651 ||| 6625 ||| 
2020 ||| stepwise extractive summarization and planning with structured transformers. ||| 3404 ||| 3491 ||| 26841 ||| 20399 ||| 26842 ||| 26843 ||| 3493 ||| 
2020 ||| sa-unet: spatial attention u-net for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 5337 ||| 12344 ||| 20229 ||| 20230 ||| 
2021 ||| point-voxel transformer: an efficient approach to 3d deep learning. ||| 3761 ||| 36712 ||| 37652 ||| 36713 ||| 36714 ||| 
2022 ||| leveraging smooth attention prior for multi-agent trajectory prediction. ||| 18108 ||| 37653 ||| 37654 ||| 37655 ||| 
2021 ||| temporal-relational crosstransformers for few-shot action recognition. ||| 18963 ||| 18964 ||| 7855 ||| 7793 ||| 7794 ||| 
2020 ||| sct: set constrained temporal transformer for set supervised action segmentation. ||| 19314 ||| 36968 ||| 
2021 ||| melt: message-level transformer with masked document representations as pre-training for stance detection. ||| 4903 ||| 26359 ||| 3299 ||| 3699 ||| 
2020 ||| attention-driven body pose encoding for human activity recognition. ||| 20257 ||| 20258 ||| 4611 ||| 6383 ||| 
2020 ||| analyzing word translation of transformer layers. ||| 8 ||| 3207 ||| 3181 ||| 3260 ||| 
2021 ||| towards interpreting zoonotic potential of betacoronavirus sequences with attention. ||| 37656 ||| 37657 ||| 37658 ||| 37659 ||| 37660 ||| 37661 ||| 37662 ||| 
2019 ||| a gated self-attention memory network for answer selection. ||| 26299 ||| 3569 ||| 4954 ||| 26300 ||| 
2018 ||| fine-grained age estimation in the wild with attention lstm networks. ||| 19919 ||| 28865 ||| 28866 ||| 28867 ||| 28868 ||| 28869 ||| 
2017 ||| attention strategies for multi-source sequence-to-sequence learning. ||| 3591 ||| 3592 ||| 
2019 ||| hibert: document level pre-training of hierarchical bidirectional transformers for document summarization. ||| 3479 ||| 3174 ||| 3480 ||| 
2021 ||| intriguing properties of vision transformers. ||| 32716 ||| 32715 ||| 1969 ||| 32792 ||| 1972 ||| 7143 ||| 
2017 ||| parallel attention: a unified framework for visual object discovery through dialogs and queries. ||| 2057 ||| 6417 ||| 6335 ||| 9216 ||| 5177 ||| 
2021 ||| spatio-temporal multi-task learning transformer for joint moving object detection and segmentation. ||| 23632 ||| 23633 ||| 
2019 ||| attentional feature-pair relation networks for accurate face recognition. ||| 2175 ||| 2176 ||| 2177 ||| 2178 ||| 
2020 ||| feedback attention for cell image segmentation. ||| 8729 ||| 8730 ||| 8731 ||| 
2021 ||| phylotransformer: a discriminative model for mutation prediction based on a multi-head self-attention mechanism. ||| 37663 ||| 25311 ||| 37664 ||| 2342 ||| 
2021 ||| deep transformer networks for time series classification: the npp safety case. ||| 37665 ||| 37666 ||| 37667 ||| 37668 ||| 11520 ||| 
2021 ||| vara-tts: non-autoregressive text-to-speech synthesis based on very deep vae with residual attention. ||| 10572 ||| 37669 ||| 37670 ||| 14516 ||| 37671 ||| 12586 ||| 4530 ||| 
2018 ||| pervasive attention: 2d convolutional neural networks for sequence-to-sequence prediction. ||| 23095 ||| 3510 ||| 2353 ||| 
2021 ||| transformer in convolutional neural networks. ||| 4477 ||| 7813 ||| 37323 ||| 17992 ||| 37672 ||| 7814 ||| 
2021 ||| skim-attention: learning to focus via document layout. ||| 26845 ||| 3140 ||| 3142 ||| 3141 ||| 
2021 ||| learning to guide human attention on mobile telepresence robots with 360 degree vision. ||| 37673 ||| 37674 ||| 37675 ||| 37676 ||| 4397 ||| 14678 ||| 
2021 ||| scale-aware network with regional and semantic attentions for crowd counting under cluttered background. ||| 37677 ||| 37678 ||| 30114 ||| 37679 ||| 17943 ||| 5044 ||| 
2019 ||| graph-to-graph transformer for transition-based dependency parsing. ||| 26685 ||| 3672 ||| 
2021 ||| value-aware approximate attention. ||| 26370 ||| 26356 ||| 
2018 ||| attention-based guided structured sparsity of deep neural networks. ||| 37680 ||| 37681 ||| 
2021 ||| missformer: (in-)attention-based handling of missing observations for trajectory filtering and prediction. ||| 13838 ||| 13839 ||| 13840 ||| 3450 ||| 13841 ||| 13842 ||| 
2020 ||| attention patterns detection using brain computer interfaces. ||| 16235 ||| 16236 ||| 16237 ||| 16238 ||| 16239 ||| 16240 ||| 
2021 ||| heterogeneous edge-enhanced graph attention network for multi-agent trajectory prediction. ||| 37374 ||| 37682 ||| 37375 ||| 
2017 ||| sentiment classification with word attention based on weakly supervised learning with a convolutional neural network. ||| 26374 ||| 37683 ||| 37684 ||| 37685 ||| 26594 ||| 
2017 ||| mindid: person identification from brain waves through attention-based recurrent neural network. ||| 586 ||| 771 ||| 34395 ||| 37686 ||| 774 ||| 770 ||| 
2021 ||| make a long image short: adaptive token length for vision transformers. ||| 37687 ||| 37688 ||| 31206 ||| 9149 ||| 37689 ||| 37690 ||| 1248 ||| 
2021 ||| n-best asr transformer: enhancing slu performance using multiple asr hypotheses. ||| 3775 ||| 3776 ||| 3777 ||| 3778 ||| 3779 ||| 
2020 ||| few-shot sequence learning with transformers. ||| 37691 ||| 37692 ||| 34968 ||| 7429 ||| 32808 ||| 37693 ||| 
2020 ||| parallax attention for unsupervised stereo correspondence learning. ||| 19138 ||| 7271 ||| 19139 ||| 19140 ||| 19141 ||| 19308 ||| 19143 ||| 
2020 ||| attention prior for real image restoration. ||| 2474 ||| 2475 ||| 2130 ||| 
2021 ||| attention-like feature explanation for tabular data. ||| 33242 ||| 33243 ||| 
2021 ||| a free lunch from vit: adaptive attention multi-scale fusion transformer for fine-grained visual recognition. ||| 1503 ||| 17587 ||| 6133 ||| 37694 ||| 37695 ||| 20357 ||| 37696 ||| 
2020 ||| attentive fusion enhanced audio-visual encoding for transformer based robust speech recognition. ||| 4461 ||| 1134 ||| 4462 ||| 4463 ||| 
2021 ||| pam: pose attention module for pose-invariant face recognition. ||| 37697 ||| 37698 ||| 
2021 ||| semi-supervised vision transformers. ||| 37699 ||| 37700 ||| 13659 ||| 7314 ||| 17842 ||| 
2021 ||| lightner: a lightweight generative framework with prompt-guided attention for low-resource ner. ||| 6007 ||| 17987 ||| 3034 ||| 37701 ||| 17988 ||| 12432 ||| 9686 ||| 3087 ||| 17990 ||| 
2020 ||| hopgat: hop-aware supervision graph attention networks for sparsely labeled graphs. ||| 37702 ||| 20027 ||| 37703 ||| 37704 ||| 37705 ||| 
2019 ||| a self-attention based deep learning method for lesion attribute detection from ct reports. ||| 21585 ||| 2379 ||| 21586 ||| 14912 ||| 21587 ||| 
2021 ||| multi-modal sarcasm detection based on contrastive attention mechanism. ||| 21159 ||| 3503 ||| 21160 ||| 
2021 ||| simvit: exploring a simple vision transformer with sliding windows. ||| 858 ||| 37706 ||| 34168 ||| 2780 ||| 2781 ||| 
2020 ||| attendnets: tiny deep image recognition neural networks for the edge via visual attention condensers. ||| 7865 ||| 36917 ||| 33982 ||| 
2019 ||| generating synthetic audio data for attention-based speech recognition systems. ||| 12531 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2018 ||| attentional multilabel learning over graphs: a message passing approach. ||| 37707 ||| 31109 ||| 37708 ||| 31110 ||| 
2021 ||| ishne: influence self-attention for heterogeneous network embedding. ||| 17129 ||| 37709 ||| 
2019 ||| focusnet: an attention-based fully convolutional network for medical image segmentation. ||| 15636 ||| 15637 ||| 15638 ||| 
2021 ||| attention is indeed all you need: semantically attention-guided decoding for data-to-text nlg. ||| 10285 ||| 10286 ||| 
2021 ||| on the analysis and design of high-frequency transformers for dual and triple active bridge converters in more electric aircraft. ||| 37710 ||| 37711 ||| 37712 ||| 
2021 ||| vision-language transformer and query generation for referring segmentation. ||| 2266 ||| 748 ||| 2267 ||| 2268 ||| 
2021 ||| bendr: using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data. ||| 37713 ||| 37714 ||| 37715 ||| 
2021 ||| opera: attention-regularized transformers for surgical phase recognition. ||| 27671 ||| 27672 ||| 27673 ||| 37716 ||| 27674 ||| 13628 ||| 
2017 ||| recurrent attentional reinforcement learning for multi-label image recognition. ||| 2313 ||| 2312 ||| 1800 ||| 2315 ||| 
2019 ||| time-guided high-order attention model of longitudinal heterogeneous healthcare data. ||| 22602 ||| 19721 ||| 1175 ||| 
2021 ||| grounded situation recognition with transformers. ||| 37717 ||| 37718 ||| 37719 ||| 11601 ||| 
2020 ||| modrl/d-am: multiobjective deep reinforcement learning algorithm using decomposition and attention model for multiobjective optimization. ||| 11114 ||| 4099 ||| 36502 ||| 
2020 ||| memformer: the memory-augmented transformer. ||| 37720 ||| 26719 ||| 37721 ||| 1753 ||| 
2022 ||| self-promoted supervision for few-shot transformer. ||| 9993 ||| 12300 ||| 1728 ||| 2018 ||| 
2020 ||| pretrained transformers for text ranking: bert and beyond. ||| 3009 ||| 3006 ||| 9664 ||| 
2021 ||| theme transformer: symbolic music generation with theme-conditioned transformer. ||| 37722 ||| 11901 ||| 37723 ||| 37724 ||| 3831 ||| 4374 ||| 
2021 ||| portfolio optimization with 2d relative-attentional gated transformer. ||| 37725 ||| 402 ||| 
2017 ||| satirical news detection and analysis using attention mechanism and linguistic features. ||| 1856 ||| 10755 ||| 26291 ||| 
2019 ||| dropattention: a regularization method for fully-connected self-attention networks. ||| 37726 ||| 4968 ||| 37727 ||| 23519 ||| 3272 ||| 3273 ||| 
2021 ||| mismatch: learning to change predictive confidences with attention for consistency-based, semi-supervised medical image segmentation. ||| 32420 ||| 32621 ||| 37728 ||| 37729 ||| 37730 ||| 27843 ||| 37731 ||| 21493 ||| 21494 ||| 
2020 ||| actor-transformers for group activity recognition. ||| 19302 ||| 19303 ||| 19304 ||| 18223 ||| 
2021 ||| match-ignition: plugging pagerank into transformer for long-form text matching. ||| 1443 ||| 1444 ||| 1445 ||| 
2021 ||| fast convergence of detr with spatially modulated co-attention. ||| 2170 ||| 2171 ||| 1846 ||| 1847 ||| 1848 ||| 
2021 ||| cat: cross attention in vision transformer. ||| 34169 ||| 34168 ||| 34170 ||| 1856 ||| 34171 ||| 2146 ||| 31718 ||| 30636 ||| 
2021 ||| block pruning for faster transformers. ||| 1226 ||| 26695 ||| 26696 ||| 26331 ||| 4962 ||| 
2018 ||| hybrid self-attention network for machine translation. ||| 2009 ||| 37732 ||| 37733 ||| 836 ||| 
2021 ||| operation-wise attention network for tampering localization fusion. ||| 2704 ||| 2705 ||| 2706 ||| 2707 ||| 
2021 ||| vldeformer: learning visual-semantic embeddings by vision-language transformer decomposing. ||| 37734 ||| 37735 ||| 12391 ||| 37736 ||| 37737 ||| 27169 ||| 37738 ||| 37739 ||| 37740 ||| 
2021 ||| pushing on text readability assessment: a transformer meets handcrafted linguistic features. ||| 26317 ||| 26318 ||| 26319 ||| 
2022 ||| unsupervised learning of temporal abstractions with slot-based transformers. ||| 37741 ||| 12659 ||| 4194 ||| 11785 ||| 37742 ||| 
2019 ||| real-time emotion recognition via attention gated hierarchical memory network. ||| 18126 ||| 597 ||| 598 ||| 
2019 ||| ga-gan: ct reconstruction from biplanar drrs using gan with guided attention. ||| 31018 ||| 37743 ||| 37744 ||| 
2022 ||| crat-pred: vehicle trajectory prediction with crystal graph convolutional neural networks and multi-head self-attention. ||| 37745 ||| 37746 ||| 37747 ||| 23624 ||| 
2021 ||| sagan: adversarial spatial-asymmetric attention for noisy nona-bayer reconstruction. ||| 37748 ||| 37749 ||| 37750 ||| 
2020 ||| ca-net: comprehensive attention convolutional neural networks for explainable medical image segmentation. ||| 27343 ||| 15623 ||| 20159 ||| 1858 ||| 37751 ||| 37752 ||| 7111 ||| 14874 ||| 27462 ||| 15555 ||| 
2021 ||| an attention and prediction guided visual system for small target motion detection in complex natural environments. ||| 37753 ||| 37754 ||| 37755 ||| 37756 ||| 37757 ||| 
2018 ||| global-and-local attention networks for visual recognition. ||| 37758 ||| 37759 ||| 37760 ||| 37761 ||| 
2020 ||| end-to-end human pose and mesh reconstruction with transformers. ||| 18746 ||| 17976 ||| 8573 ||| 
2021 ||| a comparison for patch-level classification of deep learning methods on transparent images: from convolutional neural networks to visual transformers. ||| 34692 ||| 399 ||| 17444 ||| 6598 ||| 17645 ||| 15956 ||| 
2021 ||| hamilton-jacobi-bellman-isaacs equation for rational inattention in the long-run management of river environments under uncertainty. ||| 37762 ||| 37763 ||| 
2019 ||| conflict as an inverse of attention in sequence relationship. ||| 1433 ||| 
2020 ||| generating descriptions for sequential images with local-object attention and global semantic context modelling. ||| 37764 ||| 23087 ||| 37765 ||| 37766 ||| 37767 ||| 
2019 ||| generating long sequences with sparse transformers. ||| 37768 ||| 37769 ||| 37770 ||| 37771 ||| 
2020 ||| on the usefulness of self-attention for automatic speech recognition with transformers. ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2020 ||| gcn for hin via implicit utilization of attention and meta-paths. ||| 8971 ||| 37772 ||| 37773 ||| 17102 ||| 1094 ||| 1252 ||| 
2021 ||| morphmlp: a self-attention free, mlp-like backbone for image and video. ||| 37774 ||| 34404 ||| 1723 ||| 2148 ||| 37775 ||| 2149 ||| 37776 ||| 26411 ||| 
2018 ||| attention cropping: a novel data augmentation method for real-world plant species identification. ||| 37777 ||| 6535 ||| 37778 ||| 37779 ||| 2007 ||| 
2022 ||| diffusion tensor estimation with transformer neural networks. ||| 27583 ||| 27585 ||| 
2020 ||| adaptable multi-domain language model for transformer asr. ||| 12042 ||| 12043 ||| 12044 ||| 13933 ||| 12046 ||| 12047 ||| 12048 ||| 12049 ||| 12050 ||| 12051 ||| 12052 ||| 12053 ||| 12054 ||| 
2019 ||| beyond similarity: relation embedding with dual attentions for item-based recommendation. ||| 1166 ||| 37780 ||| 4327 ||| 
2018 ||| semantic aware attention based deep object co-segmentation. ||| 6418 ||| 6419 ||| 6420 ||| 
2021 ||| optimizing latency for online video captioningusing audio-visual transformers. ||| 2507 ||| 2508 ||| 11981 ||| 
2020 ||| stabilizing transformer-based action sequence generation for q-learning. ||| 37781 ||| 37782 ||| 37783 ||| 
2022 ||| fast mri reconstruction: how powerful transformers are? ||| 34589 ||| 34591 ||| 34592 ||| 6005 ||| 
2021 ||| universal approximation under constraints is possible with transformers. ||| 36076 ||| 37784 ||| 37785 ||| 37786 ||| 
2020 ||| inter-series attention model for covid-19 forecasting. ||| 9265 ||| 9268 ||| 3801 ||| 
2021 ||| towards incremental transformers: an empirical analysis of transformer models for incremental nlu. ||| 26630 ||| 26631 ||| 3580 ||| 
2021 ||| an explainable transformer-based deep learning model for the prediction of incident heart failure. ||| 32311 ||| 32310 ||| 37787 ||| 32313 ||| 16324 ||| 32314 ||| 37788 ||| 33207 ||| 32317 ||| 32316 ||| 
2021 ||| using prior knowledge to guide bert's attention in semantic textual matching tasks. ||| 8988 ||| 7400 ||| 5904 ||| 3410 ||| 
2021 ||| musemorphose: full-song and fine-grained music style transfer with just one transformer vae. ||| 11901 ||| 4374 ||| 
2018 ||| efficient super resolution for large-scale images using attentional gan. ||| 17202 ||| 17203 ||| 17204 ||| 17205 ||| 
2017 ||| vqs: linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation. ||| 2190 ||| 2191 ||| 2192 ||| 2094 ||| 2193 ||| 
2019 ||| huggingface's transformers: state-of-the-art natural language processing. ||| 26329 ||| 26330 ||| 26331 ||| 26332 ||| 26333 ||| 26334 ||| 26335 ||| 26336 ||| 59 ||| 26337 ||| 26338 ||| 37789 ||| 
2021 ||| transformer networks for data augmentation of human physical activity recognition. ||| 37790 ||| 37791 ||| 37792 ||| 37793 ||| 
2021 ||| visual transformer pruning. ||| 19746 ||| 19744 ||| 32730 ||| 19362 ||| 
2019 ||| transformer-based cascaded multimodal speech translation. ||| 4743 ||| 4744 ||| 72 ||| 4745 ||| 4746 ||| 
2020 ||| self-attention encoding and pooling for speaker recognition. ||| 12475 ||| 12474 ||| 12476 ||| 
2021 ||| self-attention between datapoints: going beyond individual input-output pairs in deep learning. ||| 37794 ||| 37795 ||| 37796 ||| 9134 ||| 37797 ||| 33649 ||| 
2019 ||| attention is all you need for chinese word segmentation. ||| 13593 ||| 3111 ||| 
2021 ||| scaling local self-attention for parameter efficient visual backbones. ||| 2466 ||| 9433 ||| 18848 ||| 9133 ||| 19355 ||| 2467 ||| 
2017 ||| fullie and wiselie: a dual-stream recurrent convolutional attention model for activity recognition. ||| 770 ||| 771 ||| 774 ||| 775 ||| 772 ||| 773 ||| 
2021 ||| temporal action proposal generation with transformers. ||| 37798 ||| 37799 ||| 8784 ||| 37800 ||| 37801 ||| 
2020 ||| relevance transformer: generating concise code snippets with relevance feedback. ||| 9649 ||| 9650 ||| 9651 ||| 
2021 ||| adaptive multi-resolution attention with linear complexity. ||| 5893 ||| 34576 ||| 37802 ||| 15822 ||| 
2020 ||| topicbert: a transformer transfer learning based memory-graph approach for multimodal streaming social media topic detection. ||| 35645 ||| 35232 ||| 37803 ||| 35231 ||| 35233 ||| 
2018 ||| recurrent transformer networks for semantic correspondence. ||| 8797 ||| 1771 ||| 9286 ||| 9287 ||| 1515 ||| 
2021 ||| inconsistent few-shot relation classification via cross-attentional prototype networks with contrastive learning. ||| 37804 ||| 37805 ||| 37806 ||| 8095 ||| 3647 ||| 
2020 ||| aist: an interpretable attention-based deep learning model for crime prediction. ||| 37807 ||| 37808 ||| 
2020 ||| multifaceted context representation using dual attention for ontology alignment. ||| 26325 ||| 26326 ||| 26327 ||| 
2021 ||| understanding transformers for bot detection in twitter. ||| 3369 ||| 15093 ||| 15094 ||| 37809 ||| 852 ||| 15095 ||| 15096 ||| 15097 ||| 2600 ||| 
2022 ||| docentr: an end-to-end document image enhancement transformer. ||| 17401 ||| 37810 ||| 37811 ||| 37812 ||| 9787 ||| 7111 ||| 17349 ||| 7111 ||| 30717 ||| 
2021 ||| learning disentangled representation implicitly via transformer for occluded person re-identification. ||| 37813 ||| 11413 ||| 7406 ||| 12196 ||| 
2018 ||| persistence pays off: paying attention to what the lstm gating mechanism persists. ||| 13994 ||| 13995 ||| 
2021 ||| finetuning pretrained transformers into rnns. ||| 22827 ||| 9407 ||| 1709 ||| 23970 ||| 26543 ||| 14940 ||| 17138 ||| 3175 ||| 3277 ||| 
2022 ||| transppg: two-stream transformer for remote heart rate estimate. ||| 37814 ||| 7117 ||| 18637 ||| 
2021 ||| automated question generation and question answering from turkish texts using text-to-text transformers. ||| 37815 ||| 37816 ||| 37817 ||| 37818 ||| 37819 ||| 
2020 ||| self-supervised nuclei segmentation in histopathological images using attention. ||| 27525 ||| 19515 ||| 27526 ||| 27527 ||| 27528 ||| 27529 ||| 27530 ||| 18883 ||| 
2021 ||| optimizing graph transformer networks with graph-based techniques. ||| 37820 ||| 37821 ||| 37822 ||| 37823 ||| 37824 ||| 37825 ||| 37826 ||| 
2020 ||| self-supervised gait encoding with locality-aware attention for person re-identification. ||| 23374 ||| 23375 ||| 16724 ||| 6413 ||| 23376 ||| 15003 ||| 23377 ||| 
2021 ||| a practical survey on faster and lighter transformers. ||| 37827 ||| 8063 ||| 37828 ||| 37829 ||| 
2021 ||| capturing row and column semantics in transformer based question answering over tables. ||| 4766 ||| 4767 ||| 3689 ||| 4768 ||| 4769 ||| 4770 ||| 4771 ||| 4772 ||| 4773 ||| 4774 ||| 
2021 ||| anomaly detection in dynamic graphs via transformer. ||| 37830 ||| 799 ||| 37831 ||| 30051 ||| 10429 ||| 15212 ||| 
2019 ||| attention is not explanation. ||| 4949 ||| 4950 ||| 
2021 ||| uncertainty, edge, and reverse-attention guided generative adversarial network for automatic building detection in remotely sensed images. ||| 37832 ||| 37833 ||| 
2021 ||| cola-net: collaborative attention network for image restoration. ||| 12195 ||| 12196 ||| 37834 ||| 23192 ||| 37835 ||| 
2021 ||| distilling transformers for neural cross-domain search. ||| 26545 ||| 3668 ||| 15263 ||| 15265 ||| 
2020 ||| masked cross self-attention encoding for deep speaker embedding. ||| 14744 ||| 37836 ||| 37837 ||| 14746 ||| 
2018 ||| finefool: fine object contour attack via attention. ||| 21005 ||| 21007 ||| 9592 ||| 37838 ||| 
2021 ||| crossformer: a versatile vision transformer based on cross-scale attention. ||| 37839 ||| 37840 ||| 9570 ||| 1115 ||| 2359 ||| 683 ||| 
2019 ||| an attentive survey of attention models. ||| 37841 ||| 37842 ||| 37843 ||| 9765 ||| 
2021 ||| predicting the factuality of reporting of news media using observations about user attention in their youtube channels. ||| 13984 ||| 13985 ||| 10604 ||| 13986 ||| 13987 ||| 7049 ||| 
2020 ||| how does selective mechanism improve self-attention networks? ||| 3308 ||| 3038 ||| 3309 ||| 3310 ||| 3311 ||| 3041 ||| 
2020 ||| fine-grained iterative attention network for temporallanguage localization in videos. ||| 19399 ||| 19400 ||| 37844 ||| 2045 ||| 12376 ||| 12300 ||| 
2021 ||| cmt: convolutional neural networks meet vision transformers. ||| 19745 ||| 19744 ||| 37845 ||| 3156 ||| 32730 ||| 1688 ||| 19362 ||| 
2021 ||| are transformers a modern version of eliza? observations on french object verb agreement. ||| 26600 ||| 26601 ||| 3766 ||| 3767 ||| 
2021 ||| 3d human texture estimation from a single image with transformers. ||| 2331 ||| 2291 ||| 
2020 ||| end-to-end spoken language understanding using transformer networks and self-supervised pre-trained features. ||| 12410 ||| 12411 ||| 12412 ||| 12413 ||| 12414 ||| 12415 ||| 12416 ||| 
2021 ||| the deformer: an order-agnostic distribution estimating transformer. ||| 32976 ||| 19443 ||| 
2021 ||| semantic maps and metrics for science semantic maps and metrics for science using deep transformer encoders. ||| 37846 ||| 37847 ||| 
2021 ||| jurassic mark: inattentional blindness for a datasaurus reveals that visualizations are explored, not seen. ||| 24843 ||| 24844 ||| 24845 ||| 
2020 |||  lt2 at semeval-2020 task 12: fine-tuning of pre-trained transformer networks for offensive language detection. ||| 10526 ||| 10527 ||| 10528 ||| 
2022 ||| transformer-based knowledge distillation for efficient semantic segmentation of road-driving scenes. ||| 37848 ||| 7857 ||| 36653 ||| 7856 ||| 7859 ||| 7861 ||| 
2020 ||| spatial temporal transformer network for skeleton-based action recognition. ||| 20096 ||| 7307 ||| 7310 ||| 
2020 ||| multichannel cnn with attention for text classification. ||| 10978 ||| 34362 ||| 34361 ||| 34363 ||| 
2018 ||| hybrid ctc-attention based end-to-end speech recognition using subword units. ||| 21347 ||| 21348 ||| 12772 ||| 3508 ||| 
2020 ||| character-level japanese text generation with attention mechanism for chest radiography diagnosis. ||| 37849 ||| 23946 ||| 37850 ||| 37851 ||| 23945 ||| 37852 ||| 37853 ||| 37854 ||| 20764 ||| 
2021 ||| a novel disaster image dataset and characteristics analysis using attention model. ||| 11207 ||| 20112 ||| 20113 ||| 20114 ||| 20115 ||| 7376 ||| 7375 ||| 20116 ||| 7377 ||| 
2018 ||| syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese. ||| 5268 ||| 5269 ||| 5270 ||| 728 ||| 
2021 ||| error detection in large-scale natural language understanding systems using transformer models. ||| 3052 ||| 3053 ||| 3054 ||| 3055 ||| 
2022 ||| a novel attention model for salient structure detection in seismic volumes. ||| 12603 ||| 12604 ||| 12605 ||| 35000 ||| 
2020 ||| universal adversarial attack on attention and the resulting dataset damagenet. ||| 37333 ||| 37855 ||| 37856 ||| 11266 ||| 
2020 ||| pretrained transformers for simple question answering over knowledge graphs. ||| 13130 ||| 13131 ||| 3581 ||| 
2022 ||| image-to-graph transformers for chemical structure recognition. ||| 35847 ||| 37857 ||| 12256 ||| 
2019 ||| analyzing the structure of attention in a transformer language model. ||| 3447 ||| 20960 ||| 
2021 ||| less is more: pay less attention in vision transformers. ||| 2056 ||| 2057 ||| 2059 ||| 2058 ||| 1691 ||| 
2019 ||| inattentional blindness for redirected walking using dynamic foveated rendering. ||| 37858 ||| 37859 ||| 
2021 ||| on the validity of pre-trained transformers for natural language processing in the software engineering domain. ||| 37860 ||| 37861 ||| 37862 ||| 
2021 ||| neural attention for image captioning: review of outstanding methods. ||| 37863 ||| 37864 ||| 
2021 ||| vision transformer for classification of breast ultrasound images. ||| 37865 ||| 37866 ||| 
2021 ||| see, hear, read: leveraging multimodality with guided attention for abstractive text summarization. ||| 37867 ||| 13313 ||| 37868 ||| 3835 ||| 
2020 ||| spotfast networks with memory augmented lateral transformers for lipreading. ||| 5236 ||| 
2020 ||| e.t.: entity-transformers. coreference augmented neural language model for richer mention representations via entity-transformer blocks. ||| 31148 ||| 13155 ||| 
2019 ||| making asynchronous stochastic gradient descent work for transformers. ||| 26324 ||| 21416 ||| 
2021 ||| flying guide dog: walkable path discovery for the visually impaired utilizing drones and transformer-based semantic segmentation. ||| 37869 ||| 8708 ||| 37870 ||| 7856 ||| 37871 ||| 7857 ||| 7861 ||| 
2019 ||| 2d attentional irregular scene text recognizer. ||| 37872 ||| 37873 ||| 37874 ||| 6524 ||| 37875 ||| 37876 ||| 
2021 ||| fbert: a neural transformer for identifying offensive content. ||| 10622 ||| 10623 ||| 3849 ||| 10624 ||| 
2019 ||| biomedical image segmentation by retina-like sequential attention mechanism using only a few training images. ||| 27962 ||| 27963 ||| 27964 ||| 27965 ||| 
2019 ||| understanding the teaching styles by an attention based multi-task cross-media dimensional modelling. ||| 19608 ||| 4459 ||| 19609 ||| 2008 ||| 19610 ||| 4646 ||| 19611 ||| 19612 ||| 4648 ||| 19613 ||| 
2021 ||| memory and attention in deep learning. ||| 3300 ||| 
2021 ||| construction material classification on imbalanced datasets for construction monitoring automation using vision transformer (vit) architecture. ||| 37877 ||| 37878 ||| 37879 ||| 37880 ||| 
2021 ||| unsupervised-learning-based method for chest mri-ct transformation using structure constrained unsupervised generative attention networks. ||| 37881 ||| 37882 ||| 37883 ||| 37884 ||| 37885 ||| 37886 ||| 37887 ||| 37888 ||| 37889 ||| 
2020 ||| barnet: bilinear attention network with adaptive receptive field for surgical instrument segmentation. ||| 5183 ||| 5184 ||| 18290 ||| 5185 ||| 271 ||| 5186 ||| 5189 ||| 23473 ||| 
2018 ||| weakly supervised local attention network for fine-grained visual classification. ||| 17729 ||| 17730 ||| 37890 ||| 13825 ||| 19062 ||| 17731 ||| 
2021 ||| concad: contrastive learning-based cross attention for sleep apnea detection. ||| 7028 ||| 1071 ||| 
2021 ||| visual saliency transformer. ||| 2411 ||| 2412 ||| 2413 ||| 2414 ||| 1932 ||| 
2020 ||| attention or memory? neurointerpretable agents in space and time. ||| 37891 ||| 37892 ||| 
2020 ||| attention-based neural networks for sentiment attitude extraction using distant supervision. ||| 7567 ||| 7568 ||| 
2019 ||| deep neural network for fast and accurate single image super-resolution via channel-attention-based fusion of orientation-aware features. ||| 37893 ||| 37894 ||| 37895 ||| 37896 ||| 37897 ||| 2410 ||| 17723 ||| 7654 ||| 
2021 ||| scout: socially-consistent and understandable graph attention network for trajectory prediction of vehicles and vrus. ||| 15468 ||| 15449 ||| 15450 ||| 15453 ||| 15454 ||| 
2019 ||| regularized adversarial sampling and deep time-aware attention for click-through rate prediction. ||| 1165 ||| 1166 ||| 1167 ||| 1168 ||| 1099 ||| 1169 ||| 1170 ||| 1171 ||| 
2019 ||| single image super-resolution via dense blended attention generative adversarial network for clinical diagnosis. ||| 
2018 ||| parameter sharing methods for multilingual self-attentional translation models. ||| 21387 ||| 3067 ||| 
2020 ||| ap-mtl: attention pruned multi-task learning model for real-time instrument detection and segmentation in robot-assisted surgery. ||| 21821 ||| 19132 ||| 21822 ||| 
2021 ||| transgan: two transformers can make one strong gan. ||| 32371 ||| 3094 ||| 7195 ||| 
2019 ||| temporal collaborative ranking via personalized transformer. ||| 21250 ||| 21251 ||| 21252 ||| 21253 ||| 
2020 ||| pay attention to the cough: early diagnosis of covid-19 using interpretable symptoms embeddings with cough sound signal processing. ||| 18374 ||| 18375 ||| 
2018 ||| ican: instance-centric attention network for human-object interaction detection. ||| 17109 ||| 21450 ||| 21451 ||| 
2020 ||| staying true to your word: (how) can attention become explanation? ||| 23760 ||| 23761 ||| 
2020 ||| pona: pose-guided non-local attention for human pose transfer. ||| 8293 ||| 14692 ||| 1716 ||| 1833 ||| 37898 ||| 
2021 ||| instance-aware remote sensing image captioning with cross-hierarchy attention. ||| 6768 ||| 6769 ||| 6650 ||| 
2020 |||  classification of transients in two-core symmetric phase angle regulating transformers. ||| 11774 ||| 11776 ||| 
2021 ||| exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media. ||| 26751 ||| 26752 ||| 14233 ||| 26753 ||| 26754 ||| 8955 ||| 
2021 ||| rest: an efficient transformer for visual recognition. ||| 12508 ||| 37899 ||| 
2018 ||| improving speech emotion recognition via transformer-based predictive coding through transfer learning. ||| 
2021 ||| handwritten mathematical expression recognition with bidirectionally trained transformer. ||| 17435 ||| 17436 ||| 17437 ||| 17438 ||| 16785 ||| 17439 ||| 
2020 ||| multi-head self-attention with role-guided masks. ||| 15057 ||| 15058 ||| 15059 ||| 15060 ||| 15061 ||| 15062 ||| 15063 ||| 
2021 ||| accelerating framework of transformer by hardware design and model compression co-optimization. ||| 9867 ||| 9868 ||| 9869 ||| 9870 ||| 9871 ||| 9872 ||| 9873 ||| 9874 ||| 
2021 ||| zero-shot controlled generation with encoder-decoder transformers. ||| 18481 ||| 37900 ||| 17942 ||| 59 ||| 
2022 ||| segmenting medical instruments in minimally invasive surgeries using attentionmask. ||| 6439 ||| 37901 ||| 59 ||| 29733 ||| 5736 ||| 
2021 ||| target speaker verification with selective auditory attention for single and multi-talker speech. ||| 12565 ||| 13946 ||| 37902 ||| 12494 ||| 
2020 ||| single image deraining via scale-space invariant attention neural network. ||| 4900 ||| 12057 ||| 12056 ||| 12058 ||| 
2020 ||| visual attention for musical instrument recognition. ||| 37903 ||| 11907 ||| 11909 ||| 
2021 ||| process outcome prediction: cnn vs. lstm (with attention). ||| 9712 ||| 9713 ||| 
2021 ||| attentive fine-tuning of transformers for translation of low-resourced languages @loresmt 2021. ||| 34605 ||| 34606 ||| 34607 ||| 10752 ||| 37904 ||| 37905 ||| 10634 ||| 
2019 ||| sequence-to-set semantic tagging: end-to-end multi-label prediction using neural attention for complex query reformulation and automated text categorization. ||| 23732 ||| 23733 ||| 12409 ||| 13382 ||| 13380 ||| 23734 ||| 23735 ||| 23736 ||| 
2019 ||| heterogeneous graph attention network. ||| 5957 ||| 9021 ||| 1373 ||| 412 ||| 1084 ||| 1094 ||| 9022 ||| 
2021 ||| transformer acceleration with dynamic sparse attention. ||| 11307 ||| 21681 ||| 21683 ||| 18023 ||| 12120 ||| 
2020 ||| detecting lane and road markings at a distance with perspective transformer layers. ||| 23595 ||| 23596 ||| 23597 ||| 23598 ||| 23599 ||| 
2020 ||| heterogeneous graph transformer. ||| 8958 ||| 8959 ||| 8960 ||| 8961 ||| 
2020 ||| egocentric action recognition by video attention and temporal context. ||| 34892 ||| 34894 ||| 34893 ||| 4046 ||| 34895 ||| 254 ||| 2196 ||| 2198 ||| 
2022 ||| the gatedtabtransformer. an enhanced deep learning architecture for tabular modeling. ||| 36579 ||| 36580 ||| 
2018 ||| learning roi transformer for detecting oriented objects in aerial images. ||| 18906 ||| 2082 ||| 18907 ||| 2085 ||| 18908 ||| 
2020 ||| gta: global temporal attention for video action understanding. ||| 6186 ||| 37700 ||| 7314 ||| 2424 ||| 32336 ||| 1950 ||| 
2020 ||| pa-gan: progressive attention generative adversarial network for facial attribute editing. ||| 37906 ||| 1915 ||| 37907 ||| 1916 ||| 
2021 ||| tweac: transformer with extendable qa agent classifiers. ||| 26755 ||| 26758 ||| 3700 ||| 3701 ||| 3702 ||| 
2021 ||| ssagcn: social soft attention graph convolution network for pedestrian trajectory prediction. ||| 18734 ||| 37908 ||| 37909 ||| 169 ||| 2365 ||| 1175 ||| 
2020 ||| multilevel text alignment with cross-document attention. ||| 26465 ||| 14940 ||| 3277 ||| 
2022 ||| particle transformer for jet tagging. ||| 37910 ||| 37911 ||| 37912 ||| 
2019 ||| structure-attentioned memory network for monocular depth estimation. ||| 11188 ||| 13612 ||| 37913 ||| 13614 ||| 37914 ||| 37915 ||| 
2020 ||| accelerating training of transformer-based language models with progressive layer dropping. ||| 9224 ||| 9225 ||| 
2018 ||| amobee at semeval-2018 task 1: gru neural network with a cnn attention mechanism for sentiment classification. ||| 10463 ||| 10464 ||| 
2021 ||| modeling concentrated cross-attention for neural machine translation with gaussian mixture model. ||| 11809 ||| 3076 ||| 
2021 ||| approximating how single head attention learns. ||| 37916 ||| 37917 ||| 3145 ||| 37918 ||| 
2021 ||| learning dynamic and hierarchical traffic spatiotemporal features with transformer. ||| 37919 ||| 37920 ||| 
2019 ||| efficient multi-robot exploration via multi-head attention-based cooperation strategy. ||| 37093 ||| 37921 ||| 
2019 ||| periphery-fovea multi-resolution driving model guided by human attention. ||| 6423 ||| 2041 ||| 2042 ||| 6426 ||| 6427 ||| 
2021 ||| towards coherent visual storytelling with ordered image attention. ||| 37922 ||| 9305 ||| 8566 ||| 1980 ||| 
2021 ||| artseg: employing attention for thermal images semantic segmentation. ||| 37923 ||| 37924 ||| 37925 ||| 33276 ||| 
2019 ||| improving video compression with deep visual-attention models. ||| 37926 ||| 37927 ||| 37928 ||| 37929 ||| 
2022 ||| half wavelet attention on m-net+ for low-light image enhancement. ||| 36066 ||| 22477 ||| 22476 ||| 
2021 ||| combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of covid-19 fake tweets. ||| 15158 ||| 15159 ||| 15160 ||| 15161 ||| 3835 ||| 
2017 ||| recurrent soft attention model for common object recognition. ||| 37930 ||| 
2020 ||| delaying interaction layers in transformer-based encoders for efficient open domain question answering. ||| 37931 ||| 37932 ||| 37933 ||| 
2021 ||| time-frequency attention for monaural speech enhancement. ||| 14551 ||| 14552 ||| 37934 ||| 14553 ||| 12494 ||| 
2017 ||| hierarchical recurrent attention network for response generation. ||| 17849 ||| 293 ||| 2280 ||| 3480 ||| 3118 ||| 37935 ||| 
2019 ||| interactional and informational attention on twitter. ||| 31806 ||| 5335 ||| 31807 ||| 31808 ||| 
2022 ||| multi-scale attention guided pose transfer. ||| 37936 ||| 34367 ||| 37937 ||| 30717 ||| 
2021 ||| aggregating nested transformers. ||| 17964 ||| 14048 ||| 21442 ||| 10333 ||| 36557 ||| 
2020 ||| real-time semantic segmentation with fast attention. ||| 18937 ||| 37938 ||| 37939 ||| 18722 ||| 18766 ||| 7223 ||| 7958 ||| 
2020 ||| deep reinforcement learning with stacked hierarchical attention for text-based games. ||| 9226 ||| 4980 ||| 9227 ||| 9228 ||| 3389 ||| 4873 ||| 
2018 ||| attention mechanisms for object recognition with event-based cameras. ||| 7307 ||| 7308 ||| 7309 ||| 7310 ||| 
2021 ||| cocatt: a cognitive-conditioned driver attention dataset. ||| 37940 ||| 37941 ||| 37942 ||| 37943 ||| 37944 ||| 37945 ||| 
2018 ||| twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm. ||| 8946 ||| 1036 ||| 1296 ||| 2166 ||| 
2019 ||| equivariant transformer networks. ||| 22847 ||| 9953 ||| 22848 ||| 
2021 ||| boosting graph search with attention network for solving the general orienteering problem. ||| 37946 ||| 4620 ||| 37947 ||| 37948 ||| 11466 ||| 
2020 ||| bengali abstractive news summarization(bans): a neural attention approach. ||| 36777 ||| 36778 ||| 36779 ||| 37949 ||| 
2021 ||| federated split vision transformer for covid-19 cxr diagnosis using task-agnostic training. ||| 31257 ||| 31258 ||| 37950 ||| 37951 ||| 2048 ||| 
2022 ||| umt: unified multi-modal transformers for joint video moment retrieval and highlight detection. ||| 23311 ||| 19881 ||| 3208 ||| 13811 ||| 23393 ||| 37952 ||| 
2022 ||| social computational design method for generating product shapes with gan and transformer models. ||| 37953 ||| 37954 ||| 
2021 ||| pvg at wassa 2021: a multi-input, multi-task, transformer-based architecture for empathy and distress prediction. ||| 20769 ||| 20770 ||| 20771 ||| 20772 ||| 
2019 ||| an augmented transformer architecture for natural language generation tasks. ||| 18472 ||| 18473 ||| 1305 ||| 18474 ||| 18475 ||| 18476 ||| 
2021 ||| contextual transformer networks for visual recognition. ||| 19118 ||| 19117 ||| 19116 ||| 2165 ||| 
2020 ||| confidence estimation for attention-based sequence-to-sequence models for speech recognition. ||| 12715 ||| 12716 ||| 9472 ||| 1717 ||| 12098 ||| 12012 ||| 12717 ||| 12099 ||| 
2021 ||| dual-attention enhanced bdense-unet for liver lesion segmentation. ||| 16750 ||| 37955 ||| 37956 ||| 37957 ||| 37958 ||| 37959 ||| 37960 ||| 37961 ||| 
2022 ||| matchformer: interleaving attention in transformers for feature matching. ||| 13930 ||| 7856 ||| 7857 ||| 7859 ||| 7861 ||| 
2022 ||| audio-visual speech separation based on joint feature representation with cross-modal attention. ||| 37962 ||| 989 ||| 12384 ||| 471 ||| 6488 ||| 10075 ||| 
2021 ||| image fusion transformer. ||| 19132 ||| 27805 ||| 19134 ||| 18609 ||| 
2019 ||| segmentation guided attention network for crowd counting via curriculum learning. ||| 577 ||| 11257 ||| 
2021 ||| searching for trionet: combining convolution with local and global self-attention. ||| 37963 ||| 8656 ||| 27609 ||| 37964 ||| 8660 ||| 
2021 ||| foveater: foveated transformer for image classification. ||| 37965 ||| 37966 ||| 15319 ||| 
2021 ||| attention-augmented spatio-temporal segmentation for land cover mapping. ||| 9763 ||| 17156 ||| 9764 ||| 17157 ||| 17158 ||| 9766 ||| 
2019 ||| personalizing search results using hierarchical rnn with query-aware attention. ||| 1374 ||| 1375 ||| 1376 ||| 1377 ||| 1378 ||| 
2019 ||| a data-driven dynamic rating forecast method and application for power transformer long-term planning. ||| 15278 ||| 
2021 ||| agpcnet: attention-guided pyramid context networks for infrared small target detection. ||| 37967 ||| 37968 ||| 37969 ||| 37970 ||| 
2022 ||| associating objects with scalable transformers for video object segmentation. ||| 37971 ||| 35736 ||| 18157 ||| 1905 ||| 208 ||| 
2019 ||| attention-based pairwise multi-perspective convolutional neural network for answer selection in question answering. ||| 37972 ||| 37973 ||| 37974 ||| 
2022 ||| rngdet: road network graph detection by transformer in aerial images. ||| 37975 ||| 32834 ||| 37976 ||| 37977 ||| 124 ||| 37978 ||| 
2019 ||| multi-agent game abstraction via graph attention neural network. ||| 4297 ||| 18205 ||| 18206 ||| 16800 ||| 18207 ||| 5089 ||| 
2021 ||| ba^2m: a batch aware attention module for image classification. ||| 7777 ||| 7778 ||| 7779 ||| 7782 ||| 
2022 ||| security evaluation of block-based image encryption for vision transformer against jigsaw puzzle solver attack. ||| 37979 ||| 6139 ||| 
2020 ||| capsule-transformer for neural machine translation. ||| 13593 ||| 21203 ||| 3111 ||| 
2021 ||| transformesh: a transformer network for longitudinal modeling of anatomical meshes. ||| 27789 ||| 27790 ||| 27791 ||| 27792 ||| 
2021 ||| geogat: graph model based on attention mechanism for geographic text classification. ||| 30337 ||| 37980 ||| 30339 ||| 37981 ||| 
2019 ||| et-net: a generic edge-attention guidance network for medical image segmentation. ||| 6925 ||| 4056 ||| 8582 ||| 2445 ||| 2279 ||| 1932 ||| 
2019 ||| language modeling with deep transformers. ||| 12659 ||| 12532 ||| 12533 ||| 12534 ||| 3454 ||| 
2017 ||| attentional pooling for action recognition. ||| 1663 ||| 9263 ||| 
2019 ||| bert4rec: sequential recommendation with bidirectional encoder representations from transformer. ||| 1234 ||| 1235 ||| 1236 ||| 1237 ||| 1238 ||| 1111 ||| 1239 ||| 
2021 ||| daema: denoising autoencoder with mask attention. ||| 4137 ||| 4138 ||| 4139 ||| 4140 ||| 4141 ||| 
2021 ||| lightweight attentional feature fusion for video retrieval by text. ||| 16812 ||| 37982 ||| 13513 ||| 37983 ||| 5907 ||| 
2020 ||| tmt: a transformer-based modal translator for improving multimodal sequence representations in audio visual scene-aware dialog. ||| 12258 ||| 12257 ||| 12263 ||| 11688 ||| 
2019 ||| source dependency-aware transformer with supervised self-attention. ||| 14693 ||| 3045 ||| 12389 ||| 
2020 ||| salient object detection combining a self-attention module and a feature pyramid network. ||| 37984 ||| 37985 ||| 37986 ||| 37987 ||| 
2018 ||| identifying protein-protein interaction using tree lstm and structured attention. ||| 3223 ||| 20455 ||| 3224 ||| 3225 ||| 
2022 ||| synthesizing tensor transformations for visual self-attention. ||| 35528 ||| 37369 ||| 37368 ||| 37988 ||| 37989 ||| 13435 ||| 1825 ||| 
2022 ||| facial expression recognition based on multi-head cross attention network. ||| 37990 ||| 37991 ||| 37992 ||| 37993 ||| 37994 ||| 
2020 ||| spatial and spectral deep attention fusion for multi-channel speech separation using deep embedding features. ||| 14675 ||| 2304 ||| 12041 ||| 12242 ||| 12244 ||| 
2021 ||| emerging properties in self-supervised vision transformers. ||| 2263 ||| 1887 ||| 2055 ||| 1890 ||| 1891 ||| 1892 ||| 2264 ||| 2265 ||| 1889 ||| 
2021 ||| signal transformer: complex-valued attention and meta-learning for signal recognition. ||| 37995 ||| 4638 ||| 12558 ||| 37996 ||| 37997 ||| 
2021 ||| multi-encoder learning and stream fusion for transformer-based end-to-end automatic speech recognition. ||| 13888 ||| 13890 ||| 13891 ||| 
2021 ||| unleashing transformers: parallel token prediction with discrete absorbing diffusion for fast high-resolution image generation from vector-quantized codes. ||| 37998 ||| 37999 ||| 38000 ||| 11257 ||| 38001 ||| 
2021 ||| fine-tuning vision transformers for the prediction of state variables in ising models. ||| 38002 ||| 25988 ||| 38003 ||| 
2019 ||| attention-based prototypical learning towards interpretable, confident and robust deep neural networks. ||| 36554 ||| 36555 ||| 36557 ||| 
2018 ||| learning universal sentence representations with mean-max attention autoencoder. ||| 18264 ||| 18233 ||| 26358 ||| 3337 ||| 
2018 ||| stacked semantic-guided attention model for fine-grained zero-shot learning. ||| 9394 ||| 2276 ||| 6365 ||| 9395 ||| 2279 ||| 17724 ||| 
2021 ||| transformer-based source-free domain adaptation. ||| 2522 ||| 435 ||| 30828 ||| 2523 ||| 1932 ||| 437 ||| 2524 ||| 
2019 ||| hyper vision net: kidney tumor segmentation using coordinate convolutional layer and attention unit. ||| 38004 ||| 38005 ||| 38006 ||| 
2022 ||| scalablevit: rethinking the context-oriented generalization of vision transformer. ||| 11453 ||| 6432 ||| 13795 ||| 19771 ||| 38007 ||| 19256 ||| 2527 ||| 
2021 ||| hierarchical attention-based age estimation and bias estimation. ||| 38008 ||| 2262 ||| 
2021 ||| transformers generalize linearly. ||| 38009 ||| 38010 ||| 
2020 ||| spatial attention as an interface for image captioning models. ||| 10283 ||| 
2022 ||| radiotransformer: a cascaded global-focal transformer for visual attention-guided disease classification. ||| 38011 ||| 38012 ||| 27411 ||| 
2020 ||| residual attention net for superior cross-domain time sequence modeling. ||| 38013 ||| 38014 ||| 35428 ||| 
2021 ||| line segment detection using transformers without edges. ||| 1813 ||| 1812 ||| 18754 ||| 1815 ||| 
2022 ||| interactive attention ai to translate low light photos to captions for night scene understanding in women safety. ||| 38015 ||| 38016 ||| 38017 ||| 
2020 ||| transformer on a diet. ||| 35733 ||| 36745 ||| 3360 ||| 1770 ||| 18298 ||| 
2019 ||| marginalized average attentional network for weakly-supervised learning. ||| 6650 ||| 23914 ||| 23915 ||| 23916 ||| 23917 ||| 
2020 ||| on the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers. ||| 20961 ||| 20962 ||| 20963 ||| 20964 ||| 
2019 ||| learning bodily and temporal attention in protective movement behavior detection. ||| 5663 ||| 3626 ||| 22520 ||| 13955 ||| 22521 ||| 22522 ||| 
2020 ||| attention-guided version of 2d unet for automatic brain tumor segmentation. ||| 38018 ||| 38019 ||| 38020 ||| 
2021 ||| attention forcing for machine translation. ||| 14579 ||| 38021 ||| 3795 ||| 38022 ||| 3796 ||| 
2021 ||| generative residual attention network for disease detection. ||| 38023 ||| 38024 ||| 18219 ||| 
2017 ||| rra: recurrent residual attention for sequence learning. ||| 3691 ||| 
2021 ||| efficient visual tracking with exemplar transformers. ||| 38025 ||| 38026 ||| 25546 ||| 7814 ||| 
2021 ||| all you can embed: natural language based vehicle retrieval with spatio-temporal transformers. ||| 18914 ||| 18915 ||| 18916 ||| 18917 ||| 18918 ||| 
2019 ||| adding interpretable attention to neural translation models improves word alignment. ||| 38027 ||| 38028 ||| 31178 ||| 
2022 ||| visual representation learning with self-supervised attention for low-label high-data regime. ||| 7889 ||| 38029 ||| 38030 ||| 38031 ||| 38032 ||| 38033 ||| 14195 ||| 38034 ||| 
2020 ||| enriching the transformer with linguistic and semantic factors for low-resource machine translation. ||| 13993 ||| 3466 ||| 11843 ||| 
2020 ||| channel-attention dense u-net for multichannel speech enhancement. ||| 12127 ||| 8075 ||| 12128 ||| 8076 ||| 8077 ||| 
2018 ||| recursive visual attention in visual dialog. ||| 18821 ||| 2484 ||| 5212 ||| 18822 ||| 18823 ||| 1378 ||| 
2020 ||| the chess transformer: mastering play using generative language models. ||| 8415 ||| 8413 ||| 8414 ||| 
2021 ||| is sparse attention more interpretable? ||| 3584 ||| 3585 ||| 3586 ||| 3485 ||| 
2021 ||| dan: decentralized attention-based neural network to solve the minmax multiple traveling salesman problem. ||| 38035 ||| 38036 ||| 38037 ||| 
2021 ||| routing with self-attention for multimodal capsule networks. ||| 38038 ||| 38039 ||| 38040 ||| 34110 ||| 12412 ||| 38041 ||| 12272 ||| 12254 ||| 38042 ||| 1752 ||| 
2021 ||| end-to-end neural diarization: from transformer to conformer. ||| 12123 ||| 14605 ||| 14606 ||| 3842 ||| 
2018 ||| predicting gaze in egocentric video by learning task-dependent attention transition. ||| 6419 ||| 7893 ||| 7892 ||| 7894 ||| 
2021 ||| the multimodal driver monitoring database: a naturalistic corpus to study driver attention. ||| 38043 ||| 38044 ||| 38045 ||| 38046 ||| 38047 ||| 38048 ||| 
2018 ||| phonetic-attention scoring for deep speaker features in speaker verification. ||| 4369 ||| 4370 ||| 4371 ||| 952 ||| 
2021 ||| token labeling: training a 85.4% top-1 accuracy vision transformer with 56m parameters on imagenet. ||| 1726 ||| 1902 ||| 1722 ||| 19090 ||| 7141 ||| 19597 ||| 1685 ||| 
2021 ||| choose a transformer: fourier or galerkin. ||| 38049 ||| 
2019 ||| forecaster: a graph transformer for forecasting spatial and time-dependent data. ||| 438 ||| 852 ||| 10217 ||| 
2021 ||| going deeper with image transformers. ||| 1887 ||| 2118 ||| 2119 ||| 2120 ||| 1890 ||| 1891 ||| 1892 ||| 
2021 ||| action forecasting with feature-wise self-attention. ||| 38050 ||| 33076 ||| 
2022 ||| voxel set transformer: a set-to-set approach to 3d object detection from point clouds. ||| 38051 ||| 38052 ||| 16764 ||| 241 ||| 
2019 ||| wildmix dataset and spectro-temporal transformer model for monoaural audio source separation. ||| 4948 ||| 38053 ||| 892 ||| 3601 ||| 
2018 ||| allegation of scientific misconduct increases twitter attention. ||| 18403 ||| 18401 ||| 
2021 ||| pixeltransformer: sample conditioned signal generation. ||| 22786 ||| 7364 ||| 
2022 ||| memorizing transformers. ||| 35915 ||| 38054 ||| 36261 ||| 35916 ||| 
2021 ||| (m)slae-net: multi-scale multi-level attention embedded network for retinal vessel segmentation. ||| 21571 ||| 21572 ||| 
2021 ||| polyp-pvt: polyp segmentation with pyramid vision transformers. ||| 17092 ||| 2006 ||| 1861 ||| 984 ||| 4056 ||| 1932 ||| 
2022 ||| mask usage recognition using vision transformer with transfer learning and data augmentation. ||| 38055 ||| 4318 ||| 38056 ||| 
2022 ||| zero-shot sketch based image retrieval using graph transformer. ||| 38057 ||| 6864 ||| 6865 ||| 
2021 ||| upanets: learning from the universal pixel attention networks. ||| 38058 ||| 38059 ||| 38060 ||| 38061 ||| 38062 ||| 38063 ||| 38064 ||| 38065 ||| 
2020 ||| transformer transducer: one model unifying streaming and non-streaming speech recognition. ||| 12614 ||| 12096 ||| 2251 ||| 12612 ||| 12613 ||| 
2020 ||| accessing higher-level representations in sequential transformers with feedback memory. ||| 23898 ||| 38066 ||| 3798 ||| 1889 ||| 3797 ||| 
2020 ||| liftformer: 3d human pose estimation using attention models. ||| 38067 ||| 
2019 ||| multi-head attention with diversity for learning grounded multilingual multimodal representations. ||| 23838 ||| 1781 ||| 18704 ||| 
2022 ||| 3dctn: 3d convolution-transformer network for point cloud classification. ||| 38068 ||| 28403 ||| 38069 ||| 19197 ||| 
2019 ||| multi-criteria chinese word segmentation with transformer. ||| 3272 ||| 26577 ||| 3816 ||| 3273 ||| 
2021 ||| levit-unet: make faster encoders with transformer for medical image segmentation. ||| 15293 ||| 38070 ||| 13514 ||| 38071 ||| 
2021 ||| soit: segmenting objects with instance-aware transformers. ||| 38072 ||| 36639 ||| 8217 ||| 38073 ||| 38074 ||| 36640 ||| 
2020 ||| transformer-xl based music generation with multiple sequences of time-valued notes. ||| 38075 ||| 38076 ||| 38077 ||| 
2020 ||| deep attention fusion feature for speech separation with end-to-end post-filter method. ||| 14675 ||| 12041 ||| 2304 ||| 12242 ||| 12244 ||| 14380 ||| 
2021 ||| emoji-based co-attention network for microblog sentiment analysis. ||| 38078 ||| 38079 ||| 38080 ||| 38081 ||| 5170 ||| 
2019 ||| multimodal transformer with multi-view visual representation for image captioning. ||| 1754 ||| 4807 ||| 1753 ||| 13825 ||| 
2019 ||| frame attention networks for facial expression recognition in videos. ||| 11244 ||| 8769 ||| 333 ||| 2149 ||| 
2021 ||| human interaction recognition framework based on interacting body part attention. ||| 38082 ||| 24502 ||| 
2021 ||| tokens-to-token vit: training vision transformers from scratch on imagenet. ||| 1722 ||| 1723 ||| 128 ||| 1724 ||| 1725 ||| 1727 ||| 1685 ||| 1728 ||| 
2021 ||| implicit and explicit attention for zero-shot learning. ||| 20284 ||| 23149 ||| 
2021 ||| attention-based adversarial appearance learning of augmented pedestrians. ||| 38083 ||| 38084 ||| 19260 ||| 
2019 ||| interpretable self-attention temporal reasoning for driving behavior understanding. ||| 12123 ||| 12124 ||| 12125 ||| 7457 ||| 7460 ||| 59 ||| 12126 ||| 
2022 ||| vision transformer with convolutions architecture search. ||| 38085 ||| 34627 ||| 36837 ||| 38086 ||| 38087 ||| 38088 ||| 
2020 ||| investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets. ||| 38089 ||| 38090 ||| 
2021 ||| ft-tdr: frequency-guided transformer and top-down refinement network for blind face inpainting. ||| 38091 ||| 17841 ||| 7314 ||| 17842 ||| 
2021 ||| stan: spatio-temporal attention network for next location recommendation. ||| 8966 ||| 1073 ||| 1074 ||| 
2021 ||| had-net: hybrid attention-based diffusion network for glucose level forecast. ||| 38092 ||| 38093 ||| 
2021 ||| aa3dnet: attention augmented real time 3d object detection. ||| 7335 ||| 
2021 ||| a transformer-based math language model for handwritten math expression recognition. ||| 17376 ||| 9793 ||| 13731 ||| 17377 ||| 9794 ||| 
2018 ||| vmav-c: a deep attention-based reinforcement learning algorithm for model-based control. ||| 38094 ||| 6627 ||| 38095 ||| 29989 ||| 38096 ||| 
2019 ||| attention-aware multi-stroke style transfer. ||| 18909 ||| 18910 ||| 18911 ||| 18912 ||| 18913 ||| 1224 ||| 
2021 ||| dudotrans: dual-domain transformer provides more attention for sinogram restoration in sparse-view ct reconstruction. ||| 19931 ||| 38097 ||| 38098 ||| 1719 ||| 38099 ||| 27778 ||| 
2020 ||| attentional feature fusion. ||| 7144 ||| 7145 ||| 7146 ||| 7147 ||| 7148 ||| 
2021 ||| interpretable attention guided network for fine-grained visual classification. ||| 20267 ||| 20268 ||| 7873 ||| 20269 ||| 7240 ||| 
2018 ||| cross-modal attentional context learning for rgb-d object detection. ||| 1800 ||| 38100 ||| 16691 ||| 38101 ||| 2315 ||| 
2019 ||| deep-emotion: facial expression recognition using attentional convolutional network. ||| 17204 ||| 35272 ||| 
2021 ||| stacked temporal attention: improving first-person action recognition by emphasizing discriminative clips. ||| 35072 ||| 6419 ||| 8347 ||| 7894 ||| 
2020 ||| super-sam: using the supervision signal from a pose estimator to train a spatial attention module for personal protective equipment recognition. ||| 7353 ||| 7354 ||| 7355 ||| 7356 ||| 
2019 ||| a free lunch in generating datasets: building a vqg and vqa system with attention and humans in the loop. ||| 38102 ||| 38103 ||| 
2020 ||| transformer interpretability beyond attention visualization. ||| 1665 ||| 1666 ||| 1667 ||| 
2019 ||| bionlp-ost 2019 rdoc tasks: multi-grain neural relevance ranking using topics and attention based query-document-sentence interactions. ||| 23752 ||| 23751 ||| 11716 ||| 11717 ||| 
2021 ||| multi-compound transformer for accurate biomedical image segmentation. ||| 9148 ||| 27729 ||| 27730 ||| 5189 ||| 27731 ||| 15555 ||| 2011 ||| 
2018 ||| learning multi-touch conversion attribution with dual-attention mechanisms for online advertising. ||| 1217 ||| 1218 ||| 1219 ||| 1220 ||| 1221 ||| 1222 ||| 1223 ||| 1224 ||| 
2017 ||| pay attention to those sets! learning quantification from images. ||| 38104 ||| 38105 ||| 17372 ||| 38106 ||| 38107 ||| 9812 ||| 
2020 ||| dual-decoder transformer for joint automatic speech recognition and multilingual speech translation. ||| 11636 ||| 11637 ||| 9319 ||| 9318 ||| 3512 ||| 3510 ||| 
2020 ||| data movement is all you need: a case study on optimizing transformers. ||| 38108 ||| 38109 ||| 38110 ||| 5356 ||| 38111 ||| 
2021 ||| video super-resolution transformer. ||| 7812 ||| 37115 ||| 3433 ||| 7814 ||| 
2021 ||| graph-based tri-attention network for answer ranking in cqa. ||| 781 ||| 17855 ||| 6964 ||| 8948 ||| 8949 ||| 8907 ||| 
2020 ||| transformers as soft reasoners over language. ||| 23386 ||| 23387 ||| 23388 ||| 
2021 ||| a transformer architecture for stress detection from ecg. ||| 25021 ||| 25022 ||| 25023 ||| 25024 ||| 20089 ||| 
2021 ||| neat: neural attention fields for end-to-end autonomous driving. ||| 2124 ||| 2125 ||| 2126 ||| 
2020 ||| tree-structured attention with hierarchical accumulation. ||| 3487 ||| 1313 ||| 3303 ||| 19267 ||| 
2020 ||| support-bert: predicting quality of question-answer pairs in msdn using deep bidirectional transformer. ||| 38112 ||| 38113 ||| 19436 ||| 
2019 ||| axial attention in multidimensional transformers. ||| 38114 ||| 24027 ||| 9289 ||| 38115 ||| 
2019 ||| deep floor plan recognition using a multi-task network with room-boundary-guided attention. ||| 1736 ||| 1737 ||| 1738 ||| 1739 ||| 
2020 ||| hybrid transformer/ctc networks for hardware efficient voice triggering. ||| 14554 ||| 14555 ||| 14556 ||| 14557 ||| 13962 ||| 
2021 ||| everything at once - multi-modal fusion transformer for video retrieval. ||| 38040 ||| 38039 ||| 34110 ||| 12412 ||| 12416 ||| 16198 ||| 32372 ||| 12272 ||| 12254 ||| 38042 ||| 
2021 ||| soft-attention improves skin cancer classification performance. ||| 27379 ||| 9788 ||| 9791 ||| 27380 ||| 
2018 ||| neural machine translation with key-value memory-augmented attention. ||| 3075 ||| 3041 ||| 9685 ||| 23371 ||| 23372 ||| 23373 ||| 15790 ||| 
2021 ||| more than just attention: learning cross-modal attentions with contrastive constraints. ||| 8946 ||| 1036 ||| 21442 ||| 27925 ||| 1948 ||| 1749 ||| 
2020 ||| limited view tomographic reconstruction using a deep recurrent framework with residual dense spatial-channel attention network and sinogram consistency. ||| 21551 ||| 27778 ||| 15551 ||| 38116 ||| 
2020 ||| taming transformers for high-resolution image synthesis. ||| 1803 ||| 1802 ||| 648 ||| 1804 ||| 
2022 ||| transformer-based htr for historical documents. ||| 38117 ||| 24419 ||| 32350 ||| 38118 ||| 38119 ||| 
2022 ||| anovit: unsupervised anomaly detection and localization with vision transformer-based encoder-decoder. ||| 38120 ||| 26594 ||| 
2020 ||| self-segregating and coordinated-segregating transformer for focused deep multi-modular network for visual question answering. ||| 36744 ||| 
2021 ||| efficient transformer for direct speech translation. ||| 38121 ||| 35016 ||| 35017 ||| 3466 ||| 
2019 ||| earlier attention? aspect-aware lstm for aspect sentiment analysis. ||| 23437 ||| 23438 ||| 14514 ||| 23439 ||| 1062 ||| 2146 ||| 688 ||| 
2021 ||| referring transformer: a one-step approach to multi-task visual grounding. ||| 17959 ||| 2301 ||| 
2019 ||| domain adaptive attention model for unsupervised cross-domain person re-identification. ||| 18002 ||| 18003 ||| 14114 ||| 11269 ||| 17572 ||| 38122 ||| 
2022 ||| a topology-attention convlstm network and its application to em images. ||| 27825 ||| 27826 ||| 6915 ||| 27827 ||| 
2020 ||| jointly cross- and self-modal graph attention network for query-based moment localization. ||| 19587 ||| 19399 ||| 19588 ||| 12376 ||| 12300 ||| 19402 ||| 
2022 ||| dagam: a domain adversarial graph attention model for subject independent eeg-based emotion recognition. ||| 19025 ||| 38123 ||| 28648 ||| 17898 ||| 
2018 ||| calibnet: self-supervised extrinsic calibration using 3d spatial transformer networks. ||| 25522 ||| 25523 ||| 25524 ||| 25525 ||| 
2021 ||| spatial transformer networks for curriculum learning. ||| 4192 ||| 38124 ||| 38125 ||| 4193 ||| 4194 ||| 4195 ||| 4196 ||| 
2021 ||| gradts: a gradient-based automatic auxiliary task selection method based on transformer networks. ||| 3432 ||| 3434 ||| 3433 ||| 3435 ||| 3436 ||| 
2020 ||| bilateral attention network for rgb-d salient object detection. ||| 5543 ||| 16443 ||| 2013 ||| 38126 ||| 1977 ||| 1861 ||| 
2020 ||| reformer: the efficient transformer. ||| 3143 ||| 9135 ||| 9434 ||| 
2020 ||| domain adaptive transfer learning on visual attention aware data augmentation for fine-grained visual categorization. ||| 13843 ||| 13844 ||| 
2019 ||| towards interpretable reinforcement learning using attention augmented agents. ||| 18864 ||| 2103 ||| 9272 ||| 9273 ||| 2106 ||| 
2018 ||| attention to head locations for crowd counting. ||| 17566 ||| 17567 ||| 17568 ||| 11620 ||| 
2022 ||| self-supervised transformer for deepfake detection. ||| 19368 ||| 11556 ||| 2494 ||| 18972 ||| 2305 ||| 
2021 ||| bounded logit attention: learning to explain image classifiers. ||| 38127 ||| 16508 ||| 16514 ||| 
2021 ||| graph attention networks with lstm-based path reweighting. ||| 38128 ||| 18497 ||| 9964 ||| 38129 ||| 35082 ||| 
2022 ||| enhancing local feature learning for 3d point cloud processing using unary-pairwise attention. ||| 38130 ||| 189 ||| 38131 ||| 38132 ||| 38133 ||| 38134 ||| 38135 ||| 
2019 ||| time-weighted attentional session-aware recommender system. ||| 6145 ||| 38136 ||| 127 ||| 
2020 ||| fragmentvc: any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention. ||| 12641 ||| 12642 ||| 12643 ||| 12644 ||| 12645 ||| 
2021 ||| moefication: conditional computation of transformer models for efficient inference. ||| 34289 ||| 3231 ||| 3232 ||| 3675 ||| 3233 ||| 1921 ||| 
2021 ||| dual-level collaborative transformer for image captioning. ||| 17658 ||| 17659 ||| 2504 ||| 17660 ||| 6831 ||| 2382 ||| 17661 ||| 2367 ||| 
2021 ||| snowflakenet: point cloud completion by snowflake point deconvolution with skip-transformer. ||| 2557 ||| 2558 ||| 2559 ||| 2560 ||| 2561 ||| 2562 ||| 2563 ||| 
2020 ||| atsal: an attention based architecture for saliency prediction in 360 videos. ||| 1672 ||| 11575 ||| 1674 ||| 1675 ||| 
2021 ||| an empirical evaluation of attention-based multi-head models for improved turbofan engine remaining useful life prediction. ||| 38137 ||| 2006 ||| 38138 ||| 38139 ||| 38140 ||| 
2020 ||| hybrid-attention guided network with multiple resolution features for person re-identification. ||| 32012 ||| 32013 ||| 32014 ||| 2342 ||| 13196 ||| 
2021 ||| sliced recursive transformer. ||| 19634 ||| 36343 ||| 35465 ||| 
2022 ||| clinical-longformer and clinical-bigbird: transformers for long clinical sequences. ||| 32310 ||| 38141 ||| 38142 ||| 38143 ||| 34551 ||| 
2021 ||| contrastive attention network with dense field estimation for face completion. ||| 9442 ||| 20137 ||| 18593 ||| 18592 ||| 6660 ||| 6935 ||| 
2021 ||| self-attention based context-aware 3d object detection. ||| 7889 ||| 7890 ||| 7891 ||| 
2022 ||| a pre-trained audio-visual transformer for emotion recognition. ||| 38144 ||| 26729 ||| 
2020 ||| trans^3: a transformer-based framework for unifying code summarization and code search. ||| 31552 ||| 31553 ||| 38145 ||| 3894 ||| 
2020 ||| be more with less: hypergraph attention networks for inductive text classification. ||| 9755 ||| 9754 ||| 26449 ||| 26450 ||| 5791 ||| 
2021 ||| spectrum attention mechanism for time series classification. ||| 38146 ||| 38147 ||| 
2021 ||| pgt: pseudo relevance feedback using a graph-based transformer. ||| 15112 ||| 15113 ||| 15114 ||| 
2019 ||| scheduled sampling for transformers. ||| 3368 ||| 3369 ||| 3370 ||| 
2020 ||| towards accurate pixel-wise object tracking by attention retrieval. ||| 5396 ||| 8838 ||| 8841 ||| 1698 ||| 
2022 ||| semi-supervised new event type induction and description via contrastive loss-enforced batch attention. ||| 38148 ||| 3403 ||| 
2019 ||| distraction-aware feature learning for human attribute recognition via coarse-to-fine attention mechanism. ||| 18224 ||| 5704 ||| 8972 ||| 5705 ||| 
2021 ||| coarse-to-fine q-attention: efficient learning for visual robotic manipulation via discretisation. ||| 38149 ||| 38150 ||| 38151 ||| 18801 ||| 
2020 ||| liquid warping gan with attention: a unified framework for human image synthesis. ||| 26165 ||| 38152 ||| 38153 ||| 28780 ||| 19727 ||| 19827 ||| 
2020 ||| isaaq - mastering textbook questions with pre-trained transformers and bottom-up and top-down attention. ||| 852 ||| 15095 ||| 15096 ||| 15097 ||| 2600 ||| 23764 ||| 26854 ||| 
2020 ||| light weight residual dense attention net for spectral reconstruction from rgb images. ||| 38004 ||| 38154 ||| 38155 ||| 38156 ||| 38157 ||| 
2021 ||| transformer-based methods for recognizing ultra fine-grained entities (rufes). ||| 8898 ||| 8900 ||| 
2021 ||| a transformer based approach for fighting covid-19 fake news. ||| 38158 ||| 38159 ||| 38160 ||| 
2022 ||| mirror-yolo: an attention-based instance segmentation and detection model for mirrors. ||| 38161 ||| 38162 ||| 38163 ||| 38164 ||| 10116 ||| 38165 ||| 38166 ||| 
2021 ||| learning spatio-temporal transformer for visual tracking. ||| 1697 ||| 1698 ||| 1699 ||| 952 ||| 1700 ||| 
2019 ||| convert: efficient and accurate conversational representations from transformers. ||| 26567 ||| 6089 ||| 26568 ||| 26569 ||| 26570 ||| 26571 ||| 15106 ||| 
2018 ||| graph2seq: graph to sequence learning with attention-based neural networks. ||| 3096 ||| 18010 ||| 18122 ||| 10234 ||| 38167 ||| 
2020 ||| association rules enhanced knowledge graph attention network. ||| 2778 ||| 8447 ||| 38168 ||| 
2019 ||| stochastic region pooling: make attention more expressive. ||| 29557 ||| 29558 ||| 29559 ||| 29560 ||| 38169 ||| 
2019 ||| automatic spelling correction with transformer for ctc-based end-to-end speech recognition. ||| 14494 ||| 14599 ||| 14600 ||| 
2017 ||| convolutional attention-based seq2seq neural network for end-to-end asr. ||| 14681 ||| 
2019 ||| factorized multimodal transformer for multimodal sequential learning. ||| 4948 ||| 3640 ||| 38170 ||| 8902 ||| 3599 ||| 892 ||| 3601 ||| 
2018 ||| attention incorporate network: a network can adapt various data size. ||| 38171 ||| 7725 ||| 
2021 ||| hand gesture recognition using temporal convolutions and attention mechanism. ||| 34774 ||| 34775 ||| 34776 ||| 5559 ||| 34777 ||| 33528 ||| 
2021 ||| ammasurv: asymmetrical multi-modal attention for accurate survival analysis with whole slide images and gene expression data. ||| 16688 ||| 16689 ||| 16690 ||| 16691 ||| 
2021 ||| towards robust vision transformer. ||| 34166 ||| 38172 ||| 34167 ||| 26220 ||| 38173 ||| 38174 ||| 3001 ||| 12377 ||| 
2019 ||| treegen: a tree-based transformer architecture for code generation. ||| 18208 ||| 18209 ||| 18210 ||| 18211 ||| 1389 ||| 2037 ||| 
2021 ||| visual transformers with primal object queries for multi-label image classification. ||| 38175 ||| 7105 ||| 38176 ||| 
2020 ||| multiresolution attention extractor for small object detection. ||| 2532 ||| 400 ||| 3535 ||| 5743 ||| 3534 ||| 
2020 ||| spectral graph attention network. ||| 1260 ||| 1261 ||| 1262 ||| 1263 ||| 1264 ||| 1265 ||| 1088 ||| 
2019 ||| depth-adaptive transformer. ||| 23095 ||| 9318 ||| 3798 ||| 3825 ||| 
2021 ||| video relation detection via tracklet based visual transformer. ||| 19439 ||| 9570 ||| 17744 ||| 7652 ||| 
2021 ||| visual attention in imaginative agents. ||| 33925 ||| 18856 ||| 
2021 ||| efficient transformer for single image super-resolution. ||| 12478 ||| 2519 ||| 37679 ||| 38177 ||| 
2019 ||| stage: spatio-temporal attention on graph entities for video action detection. ||| 38178 ||| 19003 ||| 34757 ||| 38179 ||| 13611 ||| 
2021 ||| visual explanation using attention mechanism in actor-critic-based deep reinforcement learning. ||| 722 ||| 723 ||| 724 ||| 725 ||| 726 ||| 
2021 ||| convolution-free medical image segmentation using transformers. ||| 27583 ||| 27584 ||| 27585 ||| 
2019 ||| computational attention system for children, adults and elderly. ||| 20386 ||| 20388 ||| 20387 ||| 
2019 ||| what would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention. ||| 2223 ||| 2224 ||| 
2020 ||| transformer-gcrf: recovering chinese dropped pronouns with general conditional random fields. ||| 26415 ||| 26416 ||| 2013 ||| 449 ||| 10971 ||| 786 ||| 1378 ||| 26417 ||| 
2020 ||| an attention-based deep learning model for multiple pedestrian attributes recognition. ||| 28712 ||| 28713 ||| 1994 ||| 28714 ||| 28715 ||| 28716 ||| 3419 ||| 
2020 ||| attention-based query expansion learning. ||| 8627 ||| 8628 ||| 8629 ||| 
2021 ||| do context-aware translation models pay the right attention? ||| 3546 ||| 3547 ||| 3064 ||| 3548 ||| 3369 ||| 3370 ||| 3067 ||| 
2020 ||| few-shot few-shot learning and the role of spatial attention. ||| 20187 ||| 7435 ||| 20188 ||| 
2020 ||| a hierarchical transformer with speaker modeling for emotion recognition in conversation. ||| 38180 ||| 16443 ||| 6891 ||| 38181 ||| 4201 ||| 
2018 ||| sam-gcnn: a gated convolutional neural network with segment-level attention mechanism for home activity monitoring. ||| 11777 ||| 11778 ||| 11779 ||| 
2021 ||| polarized self-attention: towards high-quality pixel-wise regression. ||| 1998 ||| 38182 ||| 38183 ||| 7393 ||| 
2021 ||| activity graph transformer for temporal action localization. ||| 38184 ||| 7871 ||| 
2021 ||| a state-of-the-art survey of object detection techniques in microorganism image analysis: from traditional image processing and classical machine learning to current deep convolutional neural networks and potential visual transformers. ||| 399 ||| 38185 ||| 13789 ||| 34508 ||| 538 ||| 34693 ||| 17645 ||| 15956 ||| 
2021 ||| matching with transformers in melt. ||| 13120 ||| 13121 ||| 13122 ||| 
2017 ||| pose-conditioned spatio-temporal attention for human action recognition. ||| 7968 ||| 7969 ||| 7970 ||| 
2019 ||| multimodal transformer for unaligned multimodal language sequences. ||| 3597 ||| 3598 ||| 3599 ||| 3600 ||| 3601 ||| 3247 ||| 
2020 ||| wavelet channel attention module with a fusion network for single image deraining. ||| 11292 ||| 7457 ||| 6328 ||| 
2018 ||| end-to-end segmentation with recurrent attention neural network. ||| 15514 ||| 3889 ||| 27969 ||| 
2022 ||| towards efficient and elastic visual question answering with doubly slimmable transformer. ||| 1753 ||| 38186 ||| 1754 ||| 2365 ||| 1755 ||| 
2021 ||| emoji-aware co-attention network with emograph2vec model for sentiment anaylsis. ||| 38078 ||| 38079 ||| 38080 ||| 38081 ||| 5170 ||| 
2022 ||| adaptively re-weighting multi-loss untrained transformer for sparse-view cone-beam ct reconstruction. ||| 7707 ||| 38187 ||| 28628 ||| 38188 ||| 27882 ||| 38189 ||| 
2020 ||| channel estimation for full-duplex ris-assisted haps backhauling with graph attention networks. ||| 13478 ||| 13479 ||| 2101 ||| 13480 ||| 13481 ||| 13482 ||| 13483 ||| 
2021 ||| shatter: an efficient transformer encoder with single-headed self-attention and relative sequence partitioning. ||| 38190 ||| 3491 ||| 38191 ||| 
2021 ||| cascade network with guided loss and hybrid attention for finding good correspondences. ||| 3148 ||| 1856 ||| 17668 ||| 
2021 ||| an ecologically valid examination of event-based and time-based prospective memory using immersive virtual reality: the influence of attention, memory, and executive function processes on real-world prospective memory. ||| 38192 ||| 38193 ||| 
2018 ||| context, attention and audio feature explorations for audio visual scene-aware dialog. ||| 9293 ||| 9294 ||| 9295 ||| 38194 ||| 9296 ||| 9297 ||| 
2020 ||| temporal convolutional attention-based network for sequence modeling. ||| 38195 ||| 247 ||| 38196 ||| 4095 ||| 32581 ||| 
2019 ||| feratt: facial expression recognition with attention net. ||| 18695 ||| 10314 ||| 18696 ||| 3419 ||| 18697 ||| 18698 ||| 
2019 ||| syntax-infused transformer and bert models for machine translation and natural language understanding. ||| 1027 ||| 1028 ||| 1029 ||| 1030 ||| 1031 ||| 952 ||| 1032 ||| 
2017 ||| sensor transformation attention networks. ||| 826 ||| 827 ||| 829 ||| 828 ||| 830 ||| 
2021 ||| semantic attention and scale complementary network for instance segmentation in remote sensing images. ||| 15298 ||| 397 ||| 6489 ||| 6617 ||| 399 ||| 400 ||| 30434 ||| 
2021 ||| rethinking graph transformers with spectral attention. ||| 38197 ||| 38198 ||| 9237 ||| 38199 ||| 38200 ||| 38201 ||| 
2020 ||| attention: to better stand on the shoulders of giants. ||| 25153 ||| 25154 ||| 9472 ||| 37637 ||| 2333 ||| 1266 ||| 7030 ||| 
2020 ||| n-ode transformer: a depth-adaptive variant of the transformer using neural ordinary differential equations. ||| 38202 ||| 38203 ||| 
2020 ||| graph attention tracking. ||| 13193 ||| 18901 ||| 13194 ||| 18902 ||| 18903 ||| 6335 ||| 
2019 ||| graph attention auto-encoders. ||| 6296 ||| 6297 ||| 
2019 ||| saccader: improving accuracy of hard attention models for vision. ||| 9370 ||| 9371 ||| 9372 ||| 
2019 ||| fine-grained sentiment analysis with faithful attention. ||| 37917 ||| 38204 ||| 38205 ||| 
2022 ||| motion-aware transformer for occluded person re-identification. ||| 28457 ||| 38206 ||| 38207 ||| 38208 ||| 38209 ||| 
2018 ||| self-attention-based message-relevant response generation for neural conversation model. ||| 4940 ||| 38210 ||| 4941 ||| 
2021 ||| tritransnet: rgb-d salient object detection with a triplet transformer embedding network. ||| 19662 ||| 4715 ||| 19663 ||| 9670 ||| 19664 ||| 
2018 ||| attention-based sequence-to-sequence model for speech recognition: development of state-of-the-art system on librispeech and its application to non-native english. ||| 36629 ||| 13964 ||| 379 ||| 38211 ||| 38212 ||| 1305 ||| 3508 ||| 
2021 ||| reveal of vision transformers robustness against adversarial attacks. ||| 38213 ||| 21726 ||| 32912 ||| 32913 ||| 
2020 ||| rethinking attention with performers. ||| 22791 ||| 24034 ||| 23941 ||| 24035 ||| 24036 ||| 11942 ||| 24037 ||| 7111 ||| 24038 ||| 35773 ||| 24039 ||| 9135 ||| 35774 ||| 24041 ||| 24042 ||| 
2019 ||| syntactically supervised transformers for faster neural machine translation. ||| 3323 ||| 3324 ||| 3325 ||| 
2020 ||| wavelet denoising and attention-based rnn-arima model to predict forex price. ||| 401 ||| 402 ||| 
2019 ||| t-gsa: transformer with gaussian-weighted self-attention for speech enhancement. ||| 12096 ||| 11315 ||| 11317 ||| 
2021 ||| vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. ||| 31257 ||| 31258 ||| 31259 ||| 31260 ||| 31261 ||| 31262 ||| 31263 ||| 31264 ||| 2048 ||| 
2022 ||| lilt: a simple yet effective language-independent layout transformer for structured document understanding. ||| 38214 ||| 6559 ||| 38215 ||| 
2022 ||| source code summarization with structural relative position guided transformer. ||| 38216 ||| 33489 ||| 35442 ||| 38217 ||| 38218 ||| 596 ||| 
2020 ||| the go transformer: natural language modeling for game play. ||| 8413 ||| 8415 ||| 8414 ||| 
2019 ||| sparse graph attention networks. ||| 38219 ||| 33783 ||| 
2021 ||| multi-scale speaker embedding-based graph attention networks for speaker diarisation. ||| 38220 ||| 12729 ||| 12728 ||| 9519 ||| 35899 ||| 12731 ||| 
2021 ||| mobile-former: bridging mobilenet and transformer. ||| 1959 ||| 1954 ||| 2494 ||| 2430 ||| 18967 ||| 1957 ||| 8573 ||| 
2020 ||| stp-udgat: spatial-temporal-preference user dimensional graph attention network for next poi recommendation. ||| 1342 ||| 1343 ||| 1344 ||| 1345 ||| 1346 ||| 1347 ||| 1348 ||| 
2021 ||| on learning the transformer kernel. ||| 38221 ||| 38222 ||| 38223 ||| 33862 ||| 
2020 ||| meta-context transformers for domain-specific response generation. ||| 15232 ||| 15233 ||| 15234 ||| 
2020 ||| context-aware group captioning via self-attention and contrastive features. ||| 18939 ||| 18940 ||| 18941 ||| 18766 ||| 8660 ||| 
2020 ||| permutohedral-gcn: graph convolutional networks with global attention. ||| 36880 ||| 38224 ||| 
2021 ||| paying attention to multiscale feature maps in multimodal image matching. ||| 38225 ||| 2262 ||| 
2022 ||| gaussian multi-head attention for simultaneous machine translation. ||| 11809 ||| 3076 ||| 
2019 ||| a hvs-inspired attention map to improve cnn-based perceptual losses for image restoration. ||| 7821 ||| 34928 ||| 7823 ||| 
2021 ||| a pseudo label-wise attention network for automatic icd coding. ||| 27551 ||| 38226 ||| 5253 ||| 12646 ||| 
2021 ||| urltran: improving phishing url detection using transformers. ||| 6016 ||| 6017 ||| 6018 ||| 6019 ||| 6020 ||| 6021 ||| 
2019 ||| spatiotemporal co-attention recurrent neural networks for human-skeleton motion prediction. ||| 30753 ||| 18903 ||| 7380 ||| 683 ||| 8536 ||| 
2019 ||| weakly supervised attention networks for fine-grained opinion mining and public health. ||| 24762 ||| 24763 ||| 24764 ||| 
2018 ||| prior attention for style-aware sequence-to-sequence models. ||| 38227 ||| 38228 ||| 4938 ||| 4939 ||| 
2019 ||| transfer learning with edge attention for prostate mri segmentation. ||| 38229 ||| 
2020 ||| explaining autonomous driving by learning end-to-end visual attention. ||| 19036 ||| 19037 ||| 19038 ||| 19039 ||| 19040 ||| 
2021 ||| domain-independent user simulation with transformers for task-oriented dialogue systems. ||| 26244 ||| 26245 ||| 26246 ||| 26247 ||| 26248 ||| 26249 ||| 26250 ||| 26251 ||| 
2021 ||| supervised video summarization via multiple feature sets with parallel attention. ||| 19872 ||| 10688 ||| 10689 ||| 
2021 ||| looking outside the window: wider-context transformer for the semantic segmentation of high-resolution remote sensing images. ||| 35958 ||| 38230 ||| 2978 ||| 875 ||| 38231 ||| 38232 ||| 435 ||| 35959 ||| 
2021 ||| weakly supervised attention-based models using activation maps for citrus mite and insect pest classification. ||| 29596 ||| 29597 ||| 2712 ||| 26995 ||| 5511 ||| 
2020 ||| few-shot relation learning with attention for eeg-based motor imagery classification. ||| 25500 ||| 25501 ||| 25502 ||| 25503 ||| 
2021 ||| token shift transformer for video classification. ||| 3386 ||| 19497 ||| 19498 ||| 
2021 ||| can a transformer pass the wug test? tuning copying bias in neural morphological inflection models. ||| 13453 ||| 13266 ||| 
2021 ||| neurosymbolic transformers for multi-agent communication. ||| 9425 ||| 9426 ||| 9427 ||| 9428 ||| 9429 ||| 9430 ||| 9431 ||| 9432 ||| 
2019 ||| neuro-dram: a 3d recurrent visual attention model for interpretable neuroimaging classification. ||| 38233 ||| 14875 ||| 14876 ||| 
2019 ||| attention mechanism enhanced kernel prediction networks for denoising of burst images. ||| 2747 ||| 12184 ||| 12185 ||| 12186 ||| 12187 ||| 
2021 ||| unsupervised training data generation of handwritten formulas using generative adversarial networks with self-attention. ||| 23851 ||| 23852 ||| 23853 ||| 10689 ||| 
2019 ||| visualizing attention in transformer-based language representation models. ||| 3447 ||| 
2021 ||| end-to-end multi-channel transformer for speech recognition. ||| 12483 ||| 12484 ||| 12485 ||| 12486 ||| 12487 ||| 
2020 ||| durian-sc: duration informed attention network based singing voice conversion system. ||| 14271 ||| 12189 ||| 12572 ||| 12586 ||| 14272 ||| 13512 ||| 14273 ||| 14274 ||| 3808 ||| 
2022 ||| tableformer: robust transformer modeling for table-text encoding. ||| 38234 ||| 26795 ||| 38235 ||| 38236 ||| 35652 ||| 38237 ||| 
2022 ||| beyond fixation: dynamic window visual transformer. ||| 38238 ||| 1776 ||| 1778 ||| 9670 ||| 38239 ||| 1686 ||| 1781 ||| 
2020 ||| w-net: dual supervised medical image segmentation model with multi-dimensional attention and cascade multi-scale convolution. ||| 1241 ||| 3279 ||| 19648 ||| 16726 ||| 33207 ||| 38240 ||| 
2019 ||| patchwork: a patch-wise attention network for efficient object detection and segmentation in video streams. ||| 2634 ||| 
2018 ||| picanet: pixel-wise contextual attention learning for accurate saliency detection. ||| 2411 ||| 2414 ||| 7143 ||| 
2017 ||| attention-set based metric learning for video face recognition. ||| 
2021 ||| processtransformer: predictive business process monitoring with transformer network. ||| 38241 ||| 38242 ||| 38243 ||| 
2019 ||| photometric transformer networks and label adjustment for breast density prediction. ||| 7939 ||| 38244 ||| 38245 ||| 7941 ||| 
2021 ||| epistemic planning with attention as a bounded resource. ||| 20506 ||| 20507 ||| 
2021 ||| inductive biases and variable creation in self-attention mechanisms. ||| 38246 ||| 38247 ||| 38248 ||| 38249 ||| 
2020 ||| multi-channel transformers for multi-articulatory sign language translation. ||| 8529 ||| 7442 ||| 8767 ||| 8768 ||| 8530 ||| 
2021 ||| textcnn with attention for text classification. ||| 38250 ||| 
2021 ||| s-at gcn: spatial-attention graph convolution network based feature enhancement for 3d object detection. ||| 1052 ||| 38251 ||| 3613 ||| 38252 ||| 5536 ||| 
2021 ||| d-net: siamese based network with mutual attention for volume alignment. ||| 37191 ||| 38253 ||| 27670 ||| 
2020 ||| scalable multi-agent inverse reinforcement learning via actor-attention-critic. ||| 38254 ||| 38255 ||| 38256 ||| 38257 ||| 
2020 ||| data-informed global sparseness in attention mechanisms for deep neural networks. ||| 38258 ||| 38259 ||| 38260 ||| 7049 ||| 38261 ||| 
2019 ||| human vs machine attention in neural networks: a comparative study. ||| 23332 ||| 2444 ||| 1969 ||| 2445 ||| 23335 ||| 1932 ||| 
2020 ||| single image super-resolution via residual neuron attention networks. ||| 11496 ||| 11497 ||| 11498 ||| 11499 ||| 
2021 ||| sentence bottleneck autoencoders from transformer language models. ||| 26410 ||| 14940 ||| 3277 ||| 
2018 ||| an online attention-based model for speech recognition. ||| 12771 ||| 12300 ||| 5110 ||| 4459 ||| 14570 ||| 
2022 ||| lightn: light-weight transformer network for performance-overhead tradeoff in point cloud downsampling. ||| 8012 ||| 14114 ||| 17571 ||| 128 ||| 38262 ||| 14115 ||| 
2022 ||| sparse local patch transformer for robust face alignment and landmarks inherent relation learning. ||| 38263 ||| 38264 ||| 38265 ||| 14896 ||| 232 ||| 11333 ||| 
2020 ||| cascaded text generation with markov transformers. ||| 3944 ||| 4962 ||| 
2021 ||| classifying scientific publications with bert - is self-attention a feature selection method? ||| 3369 ||| 15093 ||| 15094 ||| 852 ||| 15095 ||| 15096 ||| 15097 ||| 2600 ||| 
2019 ||| tdam: a topic-dependent attention model for sentiment analysis. ||| 3661 ||| 3662 ||| 3664 ||| 
2019 ||| decay-function-free time-aware attention to context and speaker indicator for spoken language understanding. ||| 4940 ||| 4941 ||| 
2017 ||| a spatiotemporal model with visual attention for video classification. ||| 38266 ||| 38267 ||| 
2020 ||| a frequency and phase attention based deep learning framework for partial discharge detection on insulated overhead conductors. ||| 38268 ||| 38269 ||| 38270 ||| 
2020 ||| dynamic attention based generative adversarial network with phase post-processing for speech enhancement. ||| 14674 ||| 4384 ||| 14676 ||| 14675 ||| 14677 ||| 
2019 ||| transformers without tears: improving the normalization of self-attention. ||| 4738 ||| 4739 ||| 
2020 ||| co-attentional transformers for story-based video understanding. ||| 648 ||| 12233 ||| 8580 ||| 
2018 ||| weakly supervised domain-specific color naming based on attention. ||| 17120 ||| 20305 ||| 7105 ||| 
2021 ||| centroid transformers: learning to abstract with attention. ||| 38271 ||| 38272 ||| 1073 ||| 
2020 ||| a response retrieval approach for dialogue using a multi-attentive transformer. ||| 38273 ||| 38274 ||| 32903 ||| 31869 ||| 
2021 ||| materialized knowledge bases from commonsense transformers. ||| 38275 ||| 38276 ||| 
2022 ||| transformer-based multimodal information fusion for facial expression analysis. ||| 781 ||| 38277 ||| 38278 ||| 38279 ||| 38280 ||| 33732 ||| 38281 ||| 24987 ||| 
2018 ||| tssd: temporal single-shot object detection based on attention-aware lstm. ||| 5832 ||| 25558 ||| 25559 ||| 
2019 ||| open-ended long-form video question answering via hierarchical convolutional self-attention networks. ||| 20664 ||| 1306 ||| 19440 ||| 9576 ||| 2359 ||| 
2021 ||| spectral transform forms scalable transformer. ||| 38282 ||| 427 ||| 38283 ||| 38284 ||| 23955 ||| 38285 ||| 
2021 ||| a survey of transformers. ||| 38286 ||| 18644 ||| 38287 ||| 3272 ||| 
2021 ||| focused attention improves document-grounded generation. ||| 4887 ||| 4888 ||| 4889 ||| 4890 ||| 3247 ||| 
2020 ||| channel attention residual u-net for retinal vessel segmentation. ||| 5334 ||| 5335 ||| 5336 ||| 5337 ||| 5067 ||| 
2021 ||| mention memory: incorporating textual knowledge into transformers through entity mention attention. ||| 38288 ||| 38289 ||| 38290 ||| 17735 ||| 38291 ||| 
2020 ||| generating radiology reports via memory-driven transformer. ||| 6318 ||| 3198 ||| 26661 ||| 26662 ||| 
2021 ||| streaming transformer for hardware efficient voice trigger detection and false trigger mitigation. ||| 14555 ||| 13922 ||| 14556 ||| 14554 ||| 14557 ||| 14583 ||| 13962 ||| 
2020 ||| explainable rumor detection using inter and intra-feature attention networks. ||| 38292 ||| 6003 ||| 14334 ||| 
2021 ||| unsupervised brain anomaly detection and segmentation with transformers. ||| 14883 ||| 14884 ||| 14885 ||| 14886 ||| 14887 ||| 7111 ||| 14874 ||| 14888 ||| 
2021 ||| graphical models with attention for context-specific independence and an application to perceptual grouping. ||| 38293 ||| 38294 ||| 38295 ||| 38296 ||| 38297 ||| 38298 ||| 
2020 ||| ultra lightweight image super-resolution with multi-attention layers. ||| 8742 ||| 8743 ||| 8744 ||| 8745 ||| 8746 ||| 8747 ||| 
2020 ||| adaptive graph convolutional network with attention graph clustering for co-saliency detection. ||| 19237 ||| 19238 ||| 19239 ||| 1748 ||| 13706 ||| 6625 ||| 
2020 ||| developing real-time streaming transformer transducer for speech recognition on large-scale dataset. ||| 12387 ||| 2280 ||| 12388 ||| 12389 ||| 12179 ||| 
2020 ||| a review-based transformer model for personalized product search. ||| 9571 ||| 1276 ||| 1140 ||| 
2020 ||| attention sequence to sequence model for machine remaining useful life prediction. ||| 38299 ||| 17993 ||| 16597 ||| 38300 ||| 29157 ||| 3488 ||| 
2021 ||| self-attention channel combinator frontend for end-to-end multichannel far-field speech recognition. ||| 14423 ||| 14424 ||| 14425 ||| 14426 ||| 852 ||| 14427 ||| 4046 ||| 14428 ||| 
2019 ||| h-vectors: utterance-level speaker embedding using a hierarchical attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2021 ||| attendseg: a tiny attention condenser neural network for semantic segmentation on the edge. ||| 38301 ||| 36917 ||| 38302 ||| 7865 ||| 
2020 ||| x-linear attention networks for image captioning. ||| 19116 ||| 19117 ||| 19118 ||| 2165 ||| 
2021 ||| leveraging transformers for starcraft macromanagement prediction. ||| 25937 ||| 25938 ||| 25939 ||| 
2018 ||| input combination strategies for multi-source transformer decoder. ||| 3591 ||| 3592 ||| 20958 ||| 
2020 ||| 3d attention mechanism for fine-grained classification of table tennis strokes using a twin spatio-temporal convolutional neural networks. ||| 20278 ||| 2701 ||| 20279 ||| 20280 ||| 20281 ||| 
2020 ||| fine-tuning a transformer-based language model to avoid generating non-normative text. ||| 38303 ||| 38304 ||| 38305 ||| 38306 ||| 
2021 ||| classifying long clinical documents with pre-trained transformers. ||| 1647 ||| 27213 ||| 27211 ||| 1649 ||| 1650 ||| 
2019 ||| a transformer with interleaved self-attention and convolution for hybrid acoustic models. ||| 14310 ||| 
2022 ||| q-vit: fully differentiable quantization for vision transformer. ||| 20916 ||| 35325 ||| 38307 ||| 2343 ||| 
2019 ||| attention based glaucoma detection: a large-scale database and cnn model. ||| 19373 ||| 18741 ||| 12608 ||| 19374 ||| 19375 ||| 
2022 ||| attention-based random forest and contamination model. ||| 33243 ||| 33242 ||| 
2021 ||| cross-lingual hate speech detection using transformer models. ||| 38308 ||| 38309 ||| 
2021 ||| vision transformer for covid-19 cxr diagnosis using chest x-ray feature corpus. ||| 31257 ||| 31258 ||| 31259 ||| 31260 ||| 31261 ||| 31262 ||| 31263 ||| 31264 ||| 2048 ||| 
2019 ||| multi-layer depth and epipolar feature transformers for 3d scene reconstruction. ||| 2032 ||| 2033 ||| 2034 ||| 2035 ||| 
2019 ||| solving arithmetic word problems automatically using transformer and unambiguous representations. ||| 20512 ||| 20513 ||| 
2021 ||| automatic segmentation of gross target volume of nasopharynx cancer using ensemble of multiscale deep neural networks with spatial attention. ||| 38310 ||| 15622 ||| 27343 ||| 38311 ||| 38312 ||| 38313 ||| 15623 ||| 
2021 ||| attention based broadly self-guided network for low light image enhancement. ||| 38314 ||| 38315 ||| 38316 ||| 
2019 ||| nasnet: a neuron attention stage-by-stage net for single image deraining. ||| 
2021 ||| weakly supervised instance attention for multisource fine-grained object recognition with an application to tree species classification. ||| 38317 ||| 38318 ||| 38319 ||| 38320 ||| 35100 ||| 
2020 ||| cass-nat: ctc alignment-based single step non-autoregressive transformer for speech recognition. ||| 12771 ||| 12772 ||| 12773 ||| 705 ||| 
2021 ||| behavioral research and practical models of drivers' attention. ||| 32361 ||| 29817 ||| 
2019 ||| deeply supervised multimodal attentional translation embeddings for visual relationship detection. ||| 7927 ||| 7928 ||| 7929 ||| 11612 ||| 7930 ||| 
2020 ||| leveraging affective bidirectional transformers for offensive language detection. ||| 3153 ||| 10441 ||| 3152 ||| 38321 ||| 
2021 ||| extracting temporal event relation with syntactic-guided temporal graph transformer. ||| 38322 ||| 3604 ||| 4828 ||| 
2022 ||| proformer: learning data-efficient representations of body movement with prototype-based feature augmentation and visual transformers. ||| 7859 ||| 23614 ||| 7857 ||| 7856 ||| 7861 ||| 
2021 ||| cova: context-aware visual attention for webpage information extraction. ||| 38323 ||| 35492 ||| 38324 ||| 9033 ||| 8566 ||| 
2021 ||| handsformer: keypoint transformer for monocular 3d pose estimation ofhands and object in interaction. ||| 38325 ||| 38326 ||| 38327 ||| 38328 ||| 
2021 ||| multi-level motion attention for human motion prediction. ||| 8503 ||| 8504 ||| 8505 ||| 7449 ||| 
2022 ||| cascade transformers for end-to-end person search. ||| 38329 ||| 8749 ||| 38330 ||| 38331 ||| 38332 ||| 38333 ||| 38334 ||| 
2020 ||| dynamically adjusting transformer batch size by monitoring gradient direction change. ||| 8 ||| 3207 ||| 3181 ||| 3260 ||| 
2018 ||| a deep learning approach with an attention mechanism for automatic sleep stage classification. ||| 38335 ||| 38336 ||| 38337 ||| 
2019 ||| video captioning with text-based dynamic attention and step-by-step learning. ||| 30721 ||| 30722 ||| 
2021 ||| consistent depth prediction under various illuminations using dilated cross attention. ||| 38338 ||| 38339 ||| 
2022 ||| transformers in time series: a survey. ||| 33855 ||| 26279 ||| 38340 ||| 38341 ||| 33854 ||| 2307 ||| 19819 ||| 
2020 ||| multi-decoder attention model with embedding glimpse for solving vehicle routing problems. ||| 18265 ||| 18266 ||| 18245 ||| 1134 ||| 
2021 ||| learning regional attention over multi-resolution deep convolutional features for trademark retrieval. ||| 11373 ||| 11374 ||| 11330 ||| 11331 ||| 
2021 ||| sensor-augmented egocentric-video captioning with dynamic modal attention. ||| 19625 ||| 13245 ||| 19626 ||| 
2021 ||| autoformer: searching transformers for visual recognition. ||| 1774 ||| 1698 ||| 1699 ||| 2163 ||| 
2021 ||| long-short temporal contrastive learning of video transformers. ||| 7284 ||| 7362 ||| 38342 ||| 7366 ||| 
2021 ||| ganav: group-wise attention network for classifying navigable regions in unstructured outdoor environments. ||| 7311 ||| 34658 ||| 7313 ||| 7315 ||| 
2020 ||| hardware accelerator for multi-head attention and position-wise feed-forward in the transformer. ||| 16348 ||| 16349 ||| 11182 ||| 10888 ||| 16350 ||| 
2018 ||| visual attention is beyond one single saliency map. ||| 595 ||| 
2022 ||| hyper attention recurrent neural network: tackling temporal covariate shift in time series analysis. ||| 38343 ||| 38344 ||| 38345 ||| 38346 ||| 38347 ||| 38348 ||| 
2019 ||| point attention network for semantic segmentation of 3d point clouds. ||| 38349 ||| 1166 ||| 38350 ||| 38351 ||| 38352 ||| 
2020 ||| text information aggregation with centrality attention. ||| 38353 ||| 3816 ||| 38354 ||| 3272 ||| 3273 ||| 
2020 ||| graph attention networks for speaker verification. ||| 12728 ||| 12729 ||| 12730 ||| 12731 ||| 
2021 ||| object propagation via inter-frame attentions for temporally stable video instance segmentation. ||| 19359 ||| 1732 ||| 1733 ||| 1731 ||| 2083 ||| 1735 ||| 
2021 ||| from known to unknown: knowledge-guided transformer for time-series sales forecasting in alibaba. ||| 38355 ||| 38356 ||| 15198 ||| 38357 ||| 38358 ||| 1111 ||| 
2021 ||| attention back-end for automatic speaker verification with multiple enrollment utterances. ||| 38359 ||| 398 ||| 38360 ||| 3317 ||| 
2021 ||| end-to-end referring video object segmentation with multimodal transformers. ||| 38361 ||| 38362 ||| 38363 ||| 
2021 ||| generative flows with invertible attentions. ||| 38364 ||| 38365 ||| 38366 ||| 7815 ||| 7814 ||| 
2017 ||| faster than real-time facial alignment: a 3d spatial transformer network approach in unconstrained poses. ||| 2460 ||| 2461 ||| 2138 ||| 2462 ||| 
2019 ||| multiresolution graph attention networks for relevance matching. ||| 1349 ||| 1350 ||| 1351 ||| 1352 ||| 1353 ||| 
2021 ||| dense nested attention network for infrared small target detection. ||| 38367 ||| 38368 ||| 19138 ||| 19139 ||| 19141 ||| 29626 ||| 19143 ||| 7271 ||| 
2020 ||| pay attention when required. ||| 36159 ||| 38369 ||| 38370 ||| 
2020 ||| speaker-aware speech-transformer. ||| 13914 ||| 4634 ||| 5268 ||| 728 ||| 
2021 ||| analyzing covid-19 tweets with transformer-based language models. ||| 32070 ||| 38371 ||| 38372 ||| 38373 ||| 38374 ||| 
2019 ||| on the validity of self-attention as explanation in transformer models. ||| 23998 ||| 1305 ||| 38375 ||| 38376 ||| 24000 ||| 24002 ||| 
2021 ||| multimodal breast lesion classification using cross-attention deep networks. ||| 6108 ||| 6109 ||| 6110 ||| 6111 ||| 6112 ||| 
2020 ||| pam: point-wise attention module for 6d object pose estimation. ||| 38377 ||| 38378 ||| 38379 ||| 
2018 ||| interaction-aware spatio-temporal pyramid attention networks for action classification. ||| 8836 ||| 8837 ||| 8838 ||| 8839 ||| 8840 ||| 8841 ||| 
2017 ||| weighted transformer network for machine translation. ||| 38380 ||| 23929 ||| 19267 ||| 
2021 ||| abc: attention with bounded-memory control. ||| 9407 ||| 22827 ||| 14940 ||| 23970 ||| 38381 ||| 3139 ||| 23971 ||| 3277 ||| 
2021 ||| self-supervised learning with local attention-aware feature. ||| 18674 ||| 38382 ||| 38383 ||| 11231 ||| 
2018 ||| noise-tolerant audio-visual online person verification using an attention-based neural network fusion. ||| 12253 ||| 7363 ||| 12254 ||| 
2018 ||| context-aware cascade attention-based rnn for video emotion recognition. ||| 38384 ||| 38385 ||| 38386 ||| 38387 ||| 
2020 ||| sparse and structured visual attention. ||| 11242 ||| 9210 ||| 11243 ||| 3369 ||| 3370 ||| 
2019 ||| two-stream video classification with cross-modality attention. ||| 8014 ||| 8015 ||| 8016 ||| 2398 ||| 
2020 ||| readonce transformers: reusable representations of text for transformers. ||| 3318 ||| 3319 ||| 3320 ||| 
2020 ||| attention-based fully gated cnn-bgru for russian handwritten text. ||| 38388 ||| 38389 ||| 38390 ||| 
2021 ||| transattunet: multi-level attention-guided u-net with transformer for medical image segmentation. ||| 31045 ||| 38391 ||| 1770 ||| 26463 ||| 31047 ||| 
2019 ||| look, investigate, and classify: a deep hybrid attention method for breast cancer classification. ||| 15599 ||| 15600 ||| 15601 ||| 15602 ||| 15603 ||| 15604 ||| 15605 ||| 13429 ||| 15606 ||| 
2020 ||| bi-directional attention for joint instance and semantic segmentation in point clouds. ||| 6357 ||| 6358 ||| 1239 ||| 6359 ||| 
2021 ||| synchronized audio-visual frames with fractional positional encoding for transformers in video-to-text translation. ||| 7256 ||| 38392 ||| 7257 ||| 
2020 ||| transformers for one-shot visual imitation. ||| 22284 ||| 7364 ||| 
2021 ||| speech enhancement using separable polling attention and global layer normalization followed with prelu. ||| 641 ||| 14692 ||| 14691 ||| 640 ||| 12075 ||| 
2021 ||| mitigating the position bias of transformer models in passage re-ranking. ||| 9583 ||| 9584 ||| 15102 ||| 15103 ||| 15104 ||| 9615 ||| 
2022 ||| ethics, rules of engagement, and ai: neural narrative mapping using large transformer language models. ||| 32070 ||| 32071 ||| 32072 ||| 
2021 ||| poolingformer: long document modeling with pooling attention. ||| 17750 ||| 4813 ||| 3172 ||| 20432 ||| 907 ||| 3706 ||| 3175 ||| 
2019 ||| embedding human knowledge in deep neural network via attention map. ||| 16377 ||| 15475 ||| 16378 ||| 16379 ||| 723 ||| 724 ||| 725 ||| 
2021 ||| transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. ||| 13378 ||| 13382 ||| 748 ||| 
2020 ||| attention and encoder-decoder based models for transforming articulatory movements at different speaking rates. ||| 14338 ||| 14339 ||| 14340 ||| 
2020 ||| hatnet: an end-to-end holistic attention network for diagnosis of breast biopsy images. ||| 24016 ||| 3351 ||| 38393 ||| 38394 ||| 4765 ||| 38395 ||| 
2019 ||| deep progressive multi-scale attention for acoustic event classification. ||| 14705 ||| 14706 ||| 4417 ||| 22482 ||| 12308 ||| 
2020 ||| stan: spatio-temporal attention network for pandemic prediction using real world evidence. ||| 18564 ||| 34790 ||| 38396 ||| 38397 ||| 38398 ||| 34791 ||| 23323 ||| 1070 ||| 
2017 ||| machine learning applications in estimating transformer loss of life. ||| 38399 ||| 36084 ||| 36085 ||| 
2019 ||| attention based image compression post-processing convolutional neural network. ||| 19129 ||| 38400 ||| 
2017 ||| deep graph attention model. ||| 1193 ||| 1194 ||| 1195 ||| 
2021 ||| vortx: volumetric 3d reconstruction with transformers for voxelwise view selection and fusion. ||| 13600 ||| 13601 ||| 13602 ||| 13603 ||| 13604 ||| 
2021 ||| an lstm-based plagiarism detection via attention mechanism and a population-based approach for pre-training parameters with imbalanced classes. ||| 5341 ||| 5342 ||| 5343 ||| 5344 ||| 
2022 ||| think global, act local: dual-scale graph transformer for vision-and-language navigation. ||| 19642 ||| 38401 ||| 38402 ||| 2093 ||| 2174 ||| 
2019 ||| medical image super-resolution method based on dense blended attention network. ||| 38403 ||| 38404 ||| 38405 ||| 38406 ||| 38407 ||| 38408 ||| 38409 ||| 
2021 ||| scatterbrain: unifying sparse and low-rank attention approximation. ||| 38410 ||| 22792 ||| 38411 ||| 38412 ||| 38413 ||| 22793 ||| 
2020 ||| attn-hybridnet: improving discriminability of hybrid features with attention fusion. ||| 38414 ||| 5187 ||| 38415 ||| 683 ||| 
2021 ||| vt-adl: a vision transformer network for image anomaly detection and localization. ||| 25078 ||| 25079 ||| 25080 ||| 25081 ||| 4704 ||| 
2021 ||| eeg-convtransformer for single-trial eeg based visual stimuli classification. ||| 38416 ||| 38417 ||| 
2021 ||| long-span dependencies in transformer-based summarization systems. ||| 3795 ||| 3796 ||| 
2019 ||| label-aware document representation via hybrid attention for extreme multi-label text classification. ||| 17578 ||| 38418 ||| 38419 ||| 33376 ||| 
2020 ||| classification by attention: scene graph classification with prior knowledge. ||| 18109 ||| 18110 ||| 15822 ||| 
2019 ||| instance-level microtubule segmentation using recurrent attention. ||| 38420 ||| 38421 ||| 38422 ||| 38423 ||| 31245 ||| 
2018 ||| gaan: gated attention networks for learning on large and spatiotemporal graphs. ||| 1339 ||| 18293 ||| 27292 ||| 25062 ||| 598 ||| 23917 ||| 
2020 ||| modifying memories in transformer models. ||| 1835 ||| 9157 ||| 9229 ||| 2567 ||| 2570 ||| 38424 ||| 9159 ||| 
2021 ||| an attention module for convolutional neural networks. ||| 4226 ||| 4227 ||| 4228 ||| 4229 ||| 
2020 ||| wave propagation of visual stimuli in focus of attention. ||| 38425 ||| 9398 ||| 896 ||| 897 ||| 898 ||| 
2021 ||| cross-modal attention for mri and ultrasound volume registration. ||| 27599 ||| 27600 ||| 27557 ||| 27601 ||| 27602 ||| 27603 ||| 27604 ||| 8534 ||| 20023 ||| 
2019 ||| aspect and opinion terms extraction using double embeddings and attention mechanism for indonesian hotel reviews. ||| 38426 ||| 38427 ||| 38428 ||| 
2020 ||| generating plausible counterfactual explanations for deep transformers in financial text classification. ||| 8872 ||| 11672 ||| 8873 ||| 208 ||| 8874 ||| 8875 ||| 
2022 ||| from discrimination to generation: knowledge graph completion with generative transformer. ||| 37701 ||| 17987 ||| 38429 ||| 17988 ||| 10064 ||| 38430 ||| 17989 ||| 17990 ||| 
2019 ||| mockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders. ||| 12722 ||| 12723 ||| 12724 ||| 12725 ||| 12644 ||| 
2021 ||| peco: perceptual codebook for bert pre-training of vision transformers. ||| 18967 ||| 23783 ||| 1349 ||| 2494 ||| 18972 ||| 1957 ||| 6604 ||| 5909 ||| 2305 ||| 
2019 ||| scene graph parsing by attention graph. ||| 36961 ||| 36959 ||| 36960 ||| 
2018 ||| context-aware attention for understanding twitter abuse. ||| 38431 ||| 38432 ||| 
2020 ||| cross-attention in coupled unmixing nets for unsupervised hyperspectral super-resolution. ||| 8847 ||| 8848 ||| 8849 ||| 8850 ||| 8851 ||| 8852 ||| 
2018 ||| focus on what's important: self-attention model for human pose estimation. ||| 
2020 ||| unsupervised domain attention adaptation network for caricature attribute recognition. ||| 8772 ||| 8773 ||| 8774 ||| 8775 ||| 5089 ||| 
2020 ||| hybrid generative-retrieval transformers for dialogue domain adaptation. ||| 12593 ||| 12594 ||| 12595 ||| 12596 ||| 
2020 ||| the devil is in the details: self-supervised attention for vehicle re-identification. ||| 2208 ||| 2210 ||| 2212 ||| 2213 ||| 
2020 ||| transition-based parsing with stack-transformers. ||| 1633 ||| 3553 ||| 11712 ||| 3551 ||| 26628 ||| 4819 ||| 
2020 ||| end-to-end learning for video frame compression with self-attention. ||| 9924 ||| 9925 ||| 9926 ||| 1852 ||| 9928 ||| 8024 ||| 9929 ||| 6321 ||| 
2020 ||| iqiyi submission to activitynet challenge 2019 kinetics-700 challenge: hierarchical group-wise attention. ||| 13809 ||| 38433 ||| 3114 ||| 26721 ||| 128 ||| 
2022 ||| human attention detection using am-fm representations. ||| 38434 ||| 
2019 ||| a multiscale visualization of attention in the transformer model. ||| 3447 ||| 
2019 ||| bayesian optimized continual learning with attention mechanism. ||| 38435 ||| 38436 ||| 10236 ||| 
2021 ||| the spatial selective auditory attention of cochlear implant users in different conversational sound levels. ||| 14386 ||| 14387 ||| 14388 ||| 
2021 ||| transformer-based language model fine-tuning methods for covid-19 fake news detection. ||| 18299 ||| 12719 ||| 18300 ||| 18301 ||| 11142 ||| 18302 ||| 11143 ||| 6931 ||| 
2021 ||| mlpruning: a multilevel structured pruning framework for transformer-based models. ||| 22812 ||| 38437 ||| 3822 ||| 2596 ||| 22814 ||| 
2021 ||| xcit: cross-covariance image transformers. ||| 1886 ||| 1887 ||| 2263 ||| 2265 ||| 1893 ||| 1889 ||| 2174 ||| 36099 ||| 2120 ||| 2353 ||| 1890 ||| 1891 ||| 1892 ||| 
2018 ||| a novel neural sequence model with multiple attentions for word sense disambiguation. ||| 3223 ||| 3224 ||| 3225 ||| 
2021 ||| knowledge neurons in pretrained transformers. ||| 38438 ||| 3171 ||| 18234 ||| 26749 ||| 3174 ||| 
2020 ||| inducing taxonomic knowledge from pretrained transformers. ||| 38439 ||| 18746 ||| 3145 ||| 
2021 ||| human interpretation and exploitation of self-attention patterns in transformers: a case study in extractive summarization. ||| 26619 ||| 4793 ||| 26620 ||| 4795 ||| 
2019 ||| mask-guided attention network for occluded pedestrian detection. ||| 2279 ||| 1824 ||| 2308 ||| 1971 ||| 1972 ||| 1932 ||| 
2021 ||| evaluating transformers for lightweight action recognition. ||| 36983 ||| 38440 ||| 32872 ||| 
2021 ||| domain adaptation with pre-trained transformers for query focused abstractive text summarization. ||| 16980 ||| 10289 ||| 9606 ||| 
2020 ||| rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. ||| 18958 ||| 18959 ||| 2335 ||| 2196 ||| 18960 ||| 18961 ||| 6365 ||| 18962 ||| 2198 ||| 2160 ||| 254 ||| 
2020 ||| transfer learning and distant supervision for multilingual transformer models: a study on african languages. ||| 20963 ||| 26503 ||| 26504 ||| 26505 ||| 26506 ||| 20964 ||| 
2018 ||| pyramid attention network for semantic segmentation. ||| 21512 ||| 6397 ||| 7424 ||| 21513 ||| 
2018 ||| dynamic fusion with intra- and inter- modality attention flow for visual question answering. ||| 2170 ||| 1848 ||| 19297 ||| 19296 ||| 17712 ||| 3303 ||| 1846 ||| 
2022 ||| dynamic scene video deblurring using non-local attention. ||| 19004 ||| 11486 ||| 
2018 ||| a deep learning approach for multi-view engagement estimation of children in a child-robot joint attention task. ||| 25515 ||| 8810 ||| 7929 ||| 25516 ||| 25517 ||| 7930 ||| 
2017 ||| multi-context attention for human pose estimation. ||| 18227 ||| 2334 ||| 2303 ||| 19240 ||| 8660 ||| 1846 ||| 
2017 ||| enriched deep recurrent visual attention model for multiple object recognition. ||| 7405 ||| 7406 ||| 1691 ||| 
2019 ||| unsupervised universal self-attention network for graph classification. ||| 7035 ||| 7036 ||| 7037 ||| 
2019 ||| auditory separation of a conversation from background via attentional gating. ||| 38441 ||| 8279 ||| 
2021 ||| ufo: a unified transformer for vision-language representation learning. ||| 292 ||| 8634 ||| 2044 ||| 2416 ||| 1954 ||| 8573 ||| 34696 ||| 17976 ||| 
2022 ||| class-aware generative adversarial transformers for medical image segmentation. ||| 7412 ||| 7415 ||| 3746 ||| 38442 ||| 38443 ||| 38444 ||| 15551 ||| 
2022 ||| aligntransformer: hierarchical alignment of visual regions and disease tags for medical report generation. ||| 1283 ||| 3746 ||| 3749 ||| 27611 ||| 875 ||| 3748 ||| 
2022 ||| high-performance transformer tracking. ||| 19066 ||| 1697 ||| 19067 ||| 952 ||| 1700 ||| 
2018 ||| a comparison of lstms and attention mechanisms for forecasting financial time series. ||| 38445 ||| 38446 ||| 38447 ||| 
2020 ||| resgcn: attention-based deep residual modeling for anomaly detection on attributed networks. ||| 9984 ||| 9985 ||| 9986 ||| 3923 ||| 
2021 ||| attention module improves both performance and interpretability of 4d fmri decoding neural network. ||| 38448 ||| 38449 ||| 38450 ||| 38451 ||| 38452 ||| 38453 ||| 17748 ||| 20618 ||| 38454 ||| 
2018 ||| reproduction report on "learn to pay attention". ||| 38455 ||| 38456 ||| 38457 ||| 38458 ||| 
2021 ||| between post-flaneur and smartphone zombie smartphone users altering visual attention and walking behavior in public space. ||| 29425 ||| 29426 ||| 29427 ||| 
2018 ||| knowledge-enriched two-layered attention network for sentiment analysis. ||| 4977 ||| 4978 ||| 3483 ||| 
2021 ||| bcfnet: a balanced collaborative filtering network with attention mechanism. ||| 38459 ||| 33028 ||| 38460 ||| 334 ||| 335 ||| 19575 ||| 1094 ||| 
2021 ||| pedestrian trajectory prediction via spatial interaction transformer network. ||| 15481 ||| 15482 ||| 2377 ||| 
2018 ||| chinese herbal recognition based on competitive attentional fusion of multi-hierarchies pyramid features. ||| 38169 ||| 29558 ||| 29559 ||| 29557 ||| 29560 ||| 38461 ||| 
2018 ||| teaching machines to code: neural markup generation with visual attention. ||| 38462 ||| 
2020 ||| sketch-bert: learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt. ||| 19370 ||| 6365 ||| 17842 ||| 4970 ||| 
2020 ||| dense attention fluid network for salient object detection in optical remote sensing images. ||| 38463 ||| 35817 ||| 13621 ||| 1904 ||| 19941 ||| 8626 ||| 4400 ||| 19728 ||| 
2021 ||| physics-informed attention-based neural network for solving non-linear partial differential equations. ||| 38464 ||| 38465 ||| 38466 ||| 38467 ||| 38468 ||| 38469 ||| 38470 ||| 
2020 ||| transformer based grapheme-to-phoneme conversion. ||| 13988 ||| 2101 ||| 13989 ||| 13990 ||| 13310 ||| 13991 ||| 13992 ||| 
2021 ||| dbia: data-free backdoor injection attack against transformer networks. ||| 38471 ||| 38472 ||| 38473 ||| 38474 ||| 472 ||| 38475 ||| 38476 ||| 
2019 ||| fine-grained information status classification using discourse context-aware self-attention. ||| 4832 ||| 
2017 ||| highrisk prediction from electronic medical records via deep attention networks. ||| 9519 ||| 38477 ||| 9520 ||| 9521 ||| 9522 ||| 9523 ||| 
2022 ||| attention-based cross-layer domain alignment for unsupervised domain adaptation. ||| 19807 ||| 38478 ||| 1528 ||| 11469 ||| 1550 ||| 
2022 ||| hat5: hate language identification using text-to-text transfer transformer. ||| 38479 ||| 35350 ||| 38480 ||| 23280 ||| 23281 ||| 10198 ||| 35351 ||| 27018 ||| 
2022 ||| mhatc: autism spectrum disorder identification utilizing multi-head attention encoder along with temporal consolidation modules. ||| 24286 ||| 38481 ||| 38482 ||| 6368 ||| 6369 ||| 
2021 ||| istr: end-to-end instance segmentation with transformers. ||| 38483 ||| 17660 ||| 4151 ||| 35484 ||| 247 ||| 1424 ||| 2382 ||| 1932 ||| 2367 ||| 
2019 ||| attention-based dropout layer for weakly supervised object localization. ||| 2090 ||| 18694 ||| 
2021 ||| iterative decoding for compositional generalization in transformers. ||| 38484 ||| 3557 ||| 9233 ||| 3882 ||| 
2021 ||| transformers for headline selection for russian news clusters. ||| 38485 ||| 38486 ||| 
2020 ||| dtca: decision tree-based co-attention networks for explainable claim verification. ||| 3264 ||| 3265 ||| 3266 ||| 3267 ||| 3268 ||| 
2021 ||| image captioning using multiple transformers for self-attention mechanism. ||| 33273 ||| 33272 ||| 38487 ||| 38488 ||| 33276 ||| 
2019 ||| incremental transformer with deliberation decoder for document grounded conversations. ||| 3073 ||| 3074 ||| 3075 ||| 3076 ||| 1719 ||| 1921 ||| 
2021 ||| q-attention: enabling efficient learning for vision-based robotic manipulation. ||| 38149 ||| 18801 ||| 
2021 ||| ripple attention for visual perception with sub-quadratic complexity. ||| 3137 ||| 38489 ||| 3139 ||| 
2020 ||| modeling dynamic heterogeneous network for link prediction using hierarchical attention with temporal rnn. ||| 7007 ||| 7008 ||| 6760 ||| 7009 ||| 7010 ||| 7011 ||| 
2020 ||| deep differentiable forest with sparse attention for the tabular data. ||| 34759 ||| 
2021 ||| pulmonary disease classification using globally correlated maximum likelihood: an auxiliary attention mechanism for convolutional neural networks. ||| 38490 ||| 38491 ||| 38492 ||| 9796 ||| 38493 ||| 
2021 ||| mhformer: multi-hypothesis transformer for 3d human pose estimation. ||| 11109 ||| 2519 ||| 435 ||| 1703 ||| 7814 ||| 
2018 ||| ccnet: criss-cross attention for semantic segmentation. ||| 2218 ||| 2219 ||| 1905 ||| 2220 ||| 33290 ||| 2222 ||| 17653 ||| 
2019 ||| do transformer attention heads provide transparency in abstractive summarization? ||| 38494 ||| 38495 ||| 38496 ||| 38497 ||| 1048 ||| 
2021 ||| geometry-aware transformer for molecular property prediction. ||| 38498 ||| 38499 ||| 38500 ||| 38501 ||| 
2019 ||| hyperbolic graph attention network. ||| 38502 ||| 5957 ||| 38503 ||| 1373 ||| 9022 ||| 
2019 ||| improving deep transformer with depth-scaled initialization and merged attention. ||| 3180 ||| 3848 ||| 3847 ||| 
2020 ||| image captioning through image transformer. ||| 2195 ||| 2407 ||| 1852 ||| 2410 ||| 2409 ||| 2603 ||| 
2021 ||| uot-uwf-partai at semeval-2021 task 5: self attention based bi-gru with multi-embedding representation for toxicity highlighter. ||| 10637 ||| 10638 ||| 10639 ||| 10640 ||| 
2021 ||| covid-19 pneumonia severity prediction using hybrid convolution-attention neural architectures. ||| 38504 ||| 38505 ||| 
2020 ||| jointly trained transformers models for spoken language translation. ||| 12222 ||| 12223 ||| 6785 ||| 12224 ||| 12225 ||| 10500 ||| 10501 ||| 12226 ||| 
2017 ||| multiplex media attention and disregard network among 129 countries. ||| 9044 ||| 9045 ||| 
2019 ||| line drawings of natural scenes guide visual attention. ||| 38506 ||| 38507 ||| 38508 ||| 38509 ||| 
2017 ||| the effect of collective attention on controversial debates on social media. ||| 8931 ||| 15812 ||| 15813 ||| 15814 ||| 
2020 ||| dialogue relation extraction with document-level heterogeneous graph attention networks. ||| 10064 ||| 38510 ||| 14543 ||| 3658 ||| 892 ||| 
2019 ||| multi-scale self-attention for text classification. ||| 4967 ||| 3272 ||| 4968 ||| 4970 ||| 1770 ||| 
2021 ||| inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks. ||| 4755 ||| 3810 ||| 
2021 ||| bridging between cognitive processing signals and linguistic features via a unified attentional network. ||| 29315 ||| 3181 ||| 
2020 ||| bayesian attention modules. ||| 9281 ||| 9282 ||| 9283 ||| 9284 ||| 
2018 ||| scan: sliding convolutional attention network for scene text recognition. ||| 38511 ||| 9783 ||| 17632 ||| 2014 ||| 779 ||| 
2021 ||| bidirectional lstm-crf attention-based model for chinese word segmentation. ||| 37728 ||| 33797 ||| 38512 ||| 38513 ||| 
2021 ||| decoupled transformer for scalable inference in open-domain question answering. ||| 38514 ||| 38515 ||| 
2020 ||| shallow feature based dense attention network for crowd counting. ||| 18146 ||| 10066 ||| 10065 ||| 2278 ||| 
2020 ||| dual multi-head co-attention for multi-choice reading comprehension. ||| 8754 ||| 3111 ||| 13455 ||| 
2021 ||| trankit: a light-weight transformer-based toolkit for multilingual natural language processing. ||| 24918 ||| 24919 ||| 1404 ||| 11744 ||| 
2020 ||| a transformer-based joint-encoding for emotion recognition and sentiment analysis. ||| 9280 ||| 4862 ||| 34628 ||| 38516 ||| 2693 ||| 34629 ||| 
2019 ||| local block-wise self attention for normal organ segmentation. ||| 38517 ||| 38518 ||| 38519 ||| 38520 ||| 38521 ||| 
2019 ||| canet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading. ||| 38522 ||| 8634 ||| 27802 ||| 978 ||| 1739 ||| 8637 ||| 
2020 ||| symbiotic attention with privileged information for egocentric action recognition. ||| 18157 ||| 2280 ||| 1974 ||| 208 ||| 
2021 ||| octree transformer: autoregressive 3d shape generation on hierarchically structured sequences. ||| 38523 ||| 38524 ||| 38525 ||| 
2018 ||| attention based fully convolutional network for speech emotion recognition. ||| 1008 ||| 1010 ||| 1009 ||| 4489 ||| 
2019 ||| end-to-end neural speaker diarization with self-attention. ||| 13896 ||| 13897 ||| 13898 ||| 13899 ||| 13900 ||| 3549 ||| 
2020 ||| inducing alignment structure with gated graph attention networks for sentence matching. ||| 1084 ||| 38526 ||| 38527 ||| 
2022 ||| pyramidtnt: improved transformer-in-transformer baselines with pyramid architecture. ||| 19744 ||| 19745 ||| 32730 ||| 19362 ||| 
2017 ||| semantic segmentation with reverse attention. ||| 21507 ||| 21509 ||| 21508 ||| 38528 ||| 6718 ||| 38529 ||| 14521 ||| 
2021 ||| transformer-based asr incorporating time-reduction layer and fine-tuning with self-knowledge distillation. ||| 14629 ||| 12133 ||| 14630 ||| 
2020 ||| tracking the twitter attention around the research efforts on the covid-19 pandemic. ||| 38530 ||| 18421 ||| 
2020 ||| smaat-unet: precipitation nowcasting using a small attention-unet architecture. ||| 30793 ||| 27268 ||| 
2018 ||| memory attention networks for skeleton-based action recognition. ||| 23319 ||| 10955 ||| 7240 ||| 2230 ||| 2278 ||| 18041 ||| 2363 ||| 
2021 ||| extreme precipitation seasonal forecast using a transformer neural network. ||| 38531 ||| 38532 ||| 38533 ||| 38534 ||| 
2021 ||| neural hmms are all you need (for high-quality attention-free tts). ||| 38535 ||| 38536 ||| 38537 ||| 21673 ||| 3203 ||| 
2021 ||| feature importance-aware graph attention network and dueling double deep q-network combined approach for critical node detection problems. ||| 38538 ||| 16883 ||| 38539 ||| 24629 ||| 
2021 ||| label-synchronous speech-to-text alignment for asr using forward and backward transformers. ||| 38540 ||| 4464 ||| 38541 ||| 
2021 ||| dpn-senet: a self-attention mechanism neural network for detection and diagnosis of covid-19 from chest x-ray images. ||| 17978 ||| 36212 ||| 1822 ||| 36214 ||| 36211 ||| 
2022 ||| edgeformer: improving light-weight convnets by learning from vision transformers. ||| 30528 ||| 36545 ||| 2735 ||| 
2018 ||| attention-based ensemble for deep metric learning. ||| 8734 ||| 8735 ||| 8736 ||| 8737 ||| 8738 ||| 
2021 ||| c5t5: controllable generation of organic molecules with transformers. ||| 38542 ||| 38543 ||| 38544 ||| 38545 ||| 2595 ||| 
2018 ||| triple attention mixed link network for single image super resolution. ||| 38546 ||| 2008 ||| 1825 ||| 
2019 ||| learning to caption images with two-stream attention and sentence auto-encoder. ||| 38547 ||| 33076 ||| 20975 ||| 38548 ||| 
2019 ||| attention-based curiosity-driven exploration in deep reinforcement learning. ||| 12095 ||| 5335 ||| 5336 ||| 
2021 ||| enhanced 3d human pose estimation from videos by using attention-based neural network with dilated convolutions. ||| 19283 ||| 19284 ||| 19285 ||| 2230 ||| 19286 ||| 19287 ||| 
2018 ||| bottom-up attention, models of. ||| 1854 ||| 1852 ||| 8781 ||| 
2021 ||| gca-net : utilizing gated context attention for improving image forgery localization and detection. ||| 38549 ||| 38160 ||| 38550 ||| 
2021 ||| attention-based neural network for driving environment complexity perception. ||| 13362 ||| 23662 ||| 23663 ||| 
2020 ||| transformer-based end-to-end question generation. ||| 22557 ||| 22558 ||| 22559 ||| 22560 ||| 
2021 ||| pyramidal dense attention networks for lightweight image super-resolution. ||| 28822 ||| 28823 ||| 1796 ||| 32633 ||| 6782 ||| 
2020 ||| multiple attentional pyramid networks for chinese herbal recognition. ||| 38169 ||| 29558 ||| 29559 ||| 29557 ||| 29560 ||| 38461 ||| 38551 ||| 
2021 ||| proto: program-guided transformer for program-guided tasks. ||| 28213 ||| 38552 ||| 38553 ||| 25400 ||| 
2021 ||| shoulder implant x-ray manufacturer classification: exploring with vision transformer. ||| 28328 ||| 38554 ||| 
2019 ||| non-local attention optimized deep image compression. ||| 19055 ||| 5664 ||| 35501 ||| 19056 ||| 38555 ||| 10409 ||| 19057 ||| 
2019 ||| take an emotion walk: perceiving emotions from gaits using hierarchical attention pooling and affective mapping. ||| 8818 ||| 8819 ||| 8820 ||| 7313 ||| 8823 ||| 7315 ||| 
2021 ||| hard attention control by mutual information maximization. ||| 38556 ||| 38557 ||| 
2018 ||| reverse attention for salient object detection. ||| 8601 ||| 8602 ||| 8603 ||| 8604 ||| 
2020 ||| program enhanced fact verification with verbalization and graph attention network. ||| 1323 ||| 17804 ||| 26825 ||| 14059 ||| 11254 ||| 26004 ||| 
2019 ||| ultrafast video attention prediction with coupled knowledge distillation. ||| 7762 ||| 17686 ||| 17687 ||| 17688 ||| 17689 ||| 2383 ||| 
2018 ||| co-stack residual affinity networks with multi-level attention refinement for matching text sequences. ||| 1398 ||| 9015 ||| 9016 ||| 
2021 ||| efficient self-supervised vision transformers for representation learning. ||| 26680 ||| 1955 ||| 1953 ||| 38558 ||| 1956 ||| 1954 ||| 1957 ||| 1958 ||| 
2019 ||| attention on abstract visual reasoning. ||| 38559 ||| 38560 ||| 38561 ||| 38562 ||| 38563 ||| 9584 ||| 38564 ||| 
2021 ||| presize: predicting size in e-commerce using transformers. ||| 9560 ||| 9561 ||| 9562 ||| 9563 ||| 
2021 ||| dcap: deep cross attentional product network for user response prediction. ||| 1187 ||| 1188 ||| 1189 ||| 1190 ||| 1191 ||| 1192 ||| 
2021 ||| tlsan: time-aware long- and short-term attention network for next-item recommendation. ||| 38565 ||| 38566 ||| 1565 ||| 
2022 ||| pyramid fusion transformer for semantic segmentation. ||| 38567 ||| 2454 ||| 10140 ||| 2453 ||| 2499 ||| 1944 ||| 1848 ||| 
2020 ||| general multi-label image classification with transformers. ||| 9138 ||| 18684 ||| 2005 ||| 9140 ||| 
2021 ||| r-gat: relational graph attention network for multi-relational graphs. ||| 38568 ||| 1503 ||| 38569 ||| 15236 ||| 2349 ||| 
2021 ||| baller2vec++: a look-ahead multi-entity transformer for modeling coordinated agents. ||| 32976 ||| 19443 ||| 
2017 ||| leveraging sensory data in estimating transformer lifetime. ||| 36084 ||| 38399 ||| 38570 ||| 36085 ||| 
2021 ||| u-net with hierarchical bottleneck attention for landmark detection in fundus images of the degenerated retina. ||| 27836 ||| 27837 ||| 27838 ||| 27839 ||| 
2020 ||| many-to-many voice transformer network. ||| 12538 ||| 14450 ||| 12537 ||| 12539 ||| 12540 ||| 12130 ||| 
2021 ||| vision transformers for dense prediction. ||| 1881 ||| 1882 ||| 1883 ||| 1884 ||| 
2020 ||| speaker-utterance dual attention for speaker and utterance verification. ||| 14464 ||| 13581 ||| 13580 ||| 14465 ||| 12494 ||| 
2019 ||| exact hard monotonic attention for character-level transduction. ||| 3484 ||| 3485 ||| 
2019 ||| leveraging topics and audio features with multimodal attention for audio visual scene-aware dialog. ||| 9293 ||| 9294 ||| 9295 ||| 9296 ||| 9297 ||| 
2021 ||| tanet++: triple attention network with filtered pointcloud on 3d detection. ||| 2827 ||| 
2022 ||| dall-eval: probing the reasoning skills and social biases of text-to-image generative transformers. ||| 26454 ||| 38571 ||| 3810 ||| 
2018 ||| efficient large-scale domain classification with personalized attention. ||| 3027 ||| 3028 ||| 3029 ||| 3030 ||| 
2021 ||| which transformer architecture fits my data? a vocabulary bottleneck in self-attention. ||| 9322 ||| 9321 ||| 22816 ||| 9325 ||| 
2021 ||| combating informational denial-of-service (idos) attacks: modeling and mitigation of attentional human vulnerability. ||| 9714 ||| 9715 ||| 
2020 ||| graph-bert: only attention is needed for learning graph representations. ||| 538 ||| 6716 ||| 11630 ||| 21803 ||| 
2020 ||| attention flow: end-to-end joint attention estimation. ||| 7214 ||| 7215 ||| 7216 ||| 7217 ||| 7218 ||| 
2021 ||| oadtr: online action detection with transformers. ||| 1894 ||| 1895 ||| 1896 ||| 1897 ||| 1898 ||| 1899 ||| 1900 ||| 
2018 ||| doubly attentive transformer machine translation. ||| 38572 ||| 21403 ||| 38573 ||| 
2021 ||| attacc the quadratic bottleneck of attention layers. ||| 35355 ||| 38574 ||| 38575 ||| 35357 ||| 
2020 ||| 3d facial geometry recovery from a depth view with attention guided generative adversarial network. ||| 38576 ||| 13435 ||| 38577 ||| 15882 ||| 38578 ||| 2628 ||| 
2021 ||| context-aware attention-based data augmentation for poi recommendation. ||| 438 ||| 24732 ||| 1770 ||| 24733 ||| 1084 ||| 
2021 ||| ba-net: bridge attention for deep convolutional neural networks. ||| 3226 ||| 35332 ||| 38579 ||| 38580 ||| 
2021 ||| learning multi-scene absolute pose regression with transformers. ||| 2260 ||| 2261 ||| 2262 ||| 
2021 ||| kernel identification through transformers. ||| 38581 ||| 38582 ||| 38583 ||| 38584 ||| 38585 ||| 38586 ||| 
2021 ||| fourier image transformer. ||| 38587 ||| 38588 ||| 
2020 ||| economic evaluation of transformer loss of life mitigation by energy storage and pv generation. ||| 16210 ||| 16212 ||| 
2019 ||| improving robustness in speaker identification using a two-stage attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2021 ||| attention vs non-attention for a shapley-based explanation method. ||| 8270 ||| 8271 ||| 8272 ||| 3341 ||| 
2019 ||| learning reinforced attentional representation for end-to-end visual tracking. ||| 2170 ||| 14551 ||| 31970 ||| 2349 ||| 2355 ||| 
2021 ||| improving online forums summarization via unifying hierarchical attention networks with convolutional neural networks. ||| 3960 ||| 38589 ||| 3961 ||| 
2021 ||| learnable multi-level frequency decomposition and hierarchical attention mechanism for generalized face presentation attack detection. ||| 7170 ||| 7171 ||| 7172 ||| 7173 ||| 
2021 ||| atrm: attention-based task-level relation module for gnn-based few-shot learning. ||| 38590 ||| 1484 ||| 539 ||| 21734 ||| 
2020 ||| rkt : relation-aware self-attention for knowledge tracing. ||| 1095 ||| 1096 ||| 
2018 ||| explainable social contextual image recommendation with hierarchical attention. ||| 15205 ||| 25329 ||| 1302 ||| 1301 ||| 5946 ||| 444 ||| 7688 ||| 
2020 ||| hierarchical gpt with congruent transformers for multi-sentence language models. ||| 38591 ||| 38592 ||| 38593 ||| 
2021 ||| modetr: moving object detection with transformers. ||| 23632 ||| 23633 ||| 
2019 ||| attention-augmented end-to-end multi-task learning for emotion prediction from speech. ||| 11983 ||| 11984 ||| 648 ||| 649 ||| 
2019 ||| paying attention to function words. ||| 38594 ||| 
2019 ||| but-fit at semeval-2019 task 7: determining the rumour stance with pre-trained deep bidirectional transformers. ||| 10498 ||| 10500 ||| 10501 ||| 10499 ||| 
2019 ||| semantic feature attention network for liver tumor segmentation in large-scale ct database. ||| 5893 ||| 38595 ||| 1420 ||| 27968 ||| 23687 ||| 
2019 ||| exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed. ||| 12420 ||| 12421 ||| 12422 ||| 12423 ||| 
2020 ||| soe-net: a self-attention and orientation encoding network for point cloud based place recognition. ||| 18749 ||| 18750 ||| 2332 ||| 3049 ||| 2853 ||| 18751 ||| 18752 ||| 
2021 ||| high-accuracy rgb-d face recognition via segmentation-aware face depth estimation and mask-guided attention network. ||| 5687 ||| 5688 ||| 5689 ||| 5690 ||| 
2021 ||| how bpe affects memorization in transformers. ||| 35741 ||| 38596 ||| 3341 ||| 
2021 ||| bert based transformers lead the way in extraction of health information from social media. ||| 38597 ||| 38598 ||| 38599 ||| 38600 ||| 36751 ||| 38601 ||| 38602 ||| 38603 ||| 
2019 ||| cccnet: an attention based deep learning framework for categorized crowd counting. ||| 635 ||| 636 ||| 637 ||| 
2021 ||| improving next-application prediction with deep personalized-attention neural network. ||| 25978 ||| 25979 ||| 3157 ||| 25980 ||| 
2021 ||| tag: transformer attack from gradient. ||| 26609 ||| 26610 ||| 9747 ||| 805 ||| 11023 ||| 807 ||| 11024 ||| 
2022 ||| preformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting. ||| 38604 ||| 38605 ||| 38606 ||| 
2021 ||| post-training quantization for vision transformer. ||| 19365 ||| 19362 ||| 19744 ||| 19366 ||| 7204 ||| 
2017 ||| modeling attention in panoramic video: a deep reinforcement learning approach. ||| 38529 ||| 18741 ||| 38607 ||| 38608 ||| 38609 ||| 18740 ||| 
2021 ||| greedy layer pruning: decreasing inference time of transformer models. ||| 38610 ||| 38611 ||| 38612 ||| 4250 ||| 4251 ||| 4252 ||| 
2019 ||| alcnn: attention-based model for fine-grained demand inference of dock-less shared bike in new cities. ||| 748 ||| 11189 ||| 11190 ||| 
2021 ||| leveraging multilingual transformers for hate speech detection. ||| 15035 ||| 15036 ||| 10508 ||| 15037 ||| 1185 ||| 
2022 ||| learning stochastic dynamics and predicting emergent behavior using transformers. ||| 38613 ||| 38614 ||| 38615 ||| 
2021 ||| shape registration in the time of transformers. ||| 38616 ||| 38617 ||| 38618 ||| 38619 ||| 38620 ||| 38621 ||| 
2021 ||| noisy text data: achilles' heel of popular transformer based nlp models. ||| 38622 ||| 27260 ||| 38623 ||| 38624 ||| 
2020 |||  vu: a contextualized temporal attention mechanism for sequential recommendation. ||| 8881 ||| 8882 ||| 8883 ||| 
2021 ||| transflower: probabilistic autoregressive dance generation with multimodal attention. ||| 38625 ||| 2600 ||| 3203 ||| 21673 ||| 38626 ||| 36973 ||| 38627 ||| 
2021 ||| a^2-fpn: attention aggregation based feature pyramid network for instance segmentation. ||| 19108 ||| 11220 ||| 1715 ||| 11221 ||| 
2017 ||| graph attention networks. ||| 23952 ||| 8670 ||| 23953 ||| 23954 ||| 23955 ||| 9196 ||| 
2020 ||| proformer: towards on-device lsh projection based transformers. ||| 24968 ||| 24969 ||| 24970 ||| 
2019 ||| pay attention: leveraging sequence models to predict the useful life of batteries. ||| 38628 ||| 38629 ||| 
2019 ||| visualizing and understanding self-attention based music tagging. ||| 11915 ||| 2089 ||| 11917 ||| 
2020 ||| multi-agent trajectory prediction with fuzzy query attention. ||| 9334 ||| 9335 ||| 9336 ||| 1251 ||| 9337 ||| 
2020 ||| dense cnn with self-attention for time-domain speech enhancement. ||| 36383 ||| 14725 ||| 
2018 ||| position-aware self-attention with relative positional encodings for slot filling. ||| 38630 ||| 38631 ||| 
2019 ||| improving transformer models by reordering their sublayers. ||| 3347 ||| 3277 ||| 3348 ||| 
2020 ||| social explorative attention based recommendation for content distribution platforms. ||| 38632 ||| 15201 ||| 38633 ||| 38634 ||| 894 ||| 11145 ||| 
2019 ||| deep angular embedding and feature correlation attention for breast mri cancer analysis. ||| 27445 ||| 2424 ||| 232 ||| 27446 ||| 27447 ||| 27448 ||| 27449 ||| 8637 ||| 
2020 ||| a transformer-based framework for multivariate time series representation learning. ||| 1654 ||| 25306 ||| 25307 ||| 25308 ||| 1653 ||| 
2019 ||| afs: an attention-based mechanism for supervised feature selection. ||| 17794 ||| 17795 ||| 17796 ||| 
2021 ||| towards natural language question answering over earth observation linked data using attention-based neural machine translation. ||| 6871 ||| 6872 ||| 6873 ||| 
2021 ||| attention based occlusion removal for hybrid telepresence systems. ||| 38635 ||| 38636 ||| 6409 ||| 
2021 ||| transcrowd: weakly-supervised crowd counting with transformer. ||| 36739 ||| 38637 ||| 7804 ||| 11689 ||| 17429 ||| 
2021 ||| rain: reinforced hybrid attention inference network for motion forecasting. ||| 2269 ||| 1856 ||| 2270 ||| 2271 ||| 2272 ||| 2273 ||| 
2020 ||| attention word embedding. ||| 11652 ||| 10166 ||| 10168 ||| 
2021 ||| aga-gan: attribute guided attention generative adversarial network with u-net for face hallucination. ||| 32956 ||| 32957 ||| 30717 ||| 
2019 ||| esa: entity summarization with attention. ||| 38638 ||| 38639 ||| 
2021 ||| topic-driven and knowledge-aware transformer for dialogue emotion detection. ||| 3660 ||| 3661 ||| 3662 ||| 3663 ||| 3664 ||| 
2020 ||| multi-head attention based probabilistic vehicle trajectory prediction. ||| 15455 ||| 3028 ||| 15456 ||| 15457 ||| 15458 ||| 
2020 ||| pre-training polish transformer-based language models at scale. ||| 3986 ||| 3987 ||| 3988 ||| 
2021 ||| memx: an attention-aware smart eyewear system for personalized moment auto-capture. ||| 38640 ||| 38641 ||| 36833 ||| 28703 ||| 38642 ||| 38643 ||| 38644 ||| 2961 ||| 2962 ||| 26933 ||| 
2020 ||| tensorized transformer for dynamical systems modeling. ||| 38645 ||| 35433 ||| 
2021 ||| domain composition and attention for unseen-domain generalizable medical image segmentation. ||| 27343 ||| 14392 ||| 1858 ||| 15622 ||| 15623 ||| 15555 ||| 
2020 ||| identifying similar movie characters quickly but effectively using non-exhaustive pair-wise attention. ||| 18250 ||| 35094 ||| 38646 ||| 
2021 ||| two heads are better than one: geometric-latent attention for point cloud classification and segmentation. ||| 38647 ||| 38648 ||| 38649 ||| 
2021 ||| perceptual image quality assessment with transformers. ||| 18834 ||| 18835 ||| 18836 ||| 18837 ||| 
2020 ||| cross-regional oil palm tree counting and detection via multi-level attention domain adaptation network. ||| 38650 ||| 38651 ||| 38652 ||| 38653 ||| 26464 ||| 38654 ||| 38655 ||| 
2020 ||| pointiso: point cloud based deep learning model for detecting arbitrary-precision peptide features in lc-ms map through attention based segmentation. ||| 38656 ||| 38657 ||| 38658 ||| 38659 ||| 38660 ||| 765 ||| 
2021 ||| roi tanh-polar transformer network for face parsing in the wild. ||| 28702 ||| 20051 ||| 28703 ||| 5719 ||| 
2020 ||| covid ct-net: predicting covid-19 from chest ct images using attentional convolutional network. ||| 38661 ||| 17204 ||| 38662 ||| 38663 ||| 38664 ||| 
2021 ||| attention head masking for inference time content selection in abstractive summarization. ||| 4752 ||| 4754 ||| 
2021 ||| linear transformers are secretly fast weight memory systems. ||| 22751 ||| 12659 ||| 4194 ||| 11785 ||| 
2021 ||| improving pneumonia localization via cross-attention on medical images and reports. ||| 27828 ||| 7205 ||| 27829 ||| 2022 ||| 2021 ||| 27830 ||| 2024 ||| 
2019 ||| attention-based deep tropical cyclone rapid intensification prediction. ||| 7038 ||| 7039 ||| 7040 ||| 
2022 ||| group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition. ||| 4968 ||| 8293 ||| 4460 ||| 
2021 ||| crossvit: cross-attention multi-scale vision transformer for image classification. ||| 33896 ||| 2369 ||| 2521 ||| 
2021 ||| ctin: robust contextual transformer network for inertial navigation. ||| 38665 ||| 38666 ||| 38667 ||| 38668 ||| 38669 ||| 38670 ||| 
2020 ||| scene text recognition via transformer. ||| 38671 ||| 37800 ||| 38672 ||| 1796 ||| 38673 ||| 
2020 ||| measuring systematic generalization in neural proof generation with transformers. ||| 9415 ||| 9416 ||| 9417 ||| 9418 ||| 
2020 ||| attentionnas: spatiotemporal attention cell search for video classification. ||| 8590 ||| 8591 ||| 8592 ||| 8593 ||| 8594 ||| 8595 ||| 7445 ||| 8596 ||| 
2020 ||| learning deep interleaved networks with asymmetric co-attention for image restoration. ||| 4398 ||| 35817 ||| 4399 ||| 23550 ||| 4400 ||| 24890 ||| 
2021 ||| banet: blur-aware attention networks for dynamic scene deblurring. ||| 38674 ||| 15992 ||| 2346 ||| 38675 ||| 17661 ||| 
2021 ||| survey: transformer based video-language pre-training. ||| 38676 ||| 19643 ||| 
2021 ||| train short, test long: attention with linear biases enables input length extrapolation. ||| 3347 ||| 3277 ||| 38677 ||| 
2020 ||| probabilistic spatial transformers for bayesian data augmentation. ||| 38678 ||| 24419 ||| 38679 ||| 23448 ||| 22118 ||| 38680 ||| 7111 ||| 18828 ||| 
2020 ||| stroke constrained attention network for online handwritten mathematical expression recognition. ||| 17434 ||| 1010 ||| 4489 ||| 
2020 ||| semantic segmentation with multi scale spatial attention for self driving cars. ||| 7335 ||| 7819 ||| 
2021 ||| causal attention for vision-language tasks. ||| 5157 ||| 2484 ||| 19282 ||| 1691 ||| 
2019 ||| iterative and adaptive sampling with spatial attention for black-box model explanations. ||| 7174 ||| 7175 ||| 
2017 ||| hierarchical rnn with static sentence-level attention for text-based speaker change detection. ||| 1388 ||| 1389 ||| 1390 ||| 
2020 ||| self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. ||| 18876 ||| 18877 ||| 
2021 ||| medical transformer: gated axial-attention for medical image segmentation. ||| 27805 ||| 19134 ||| 27806 ||| 18609 ||| 
2022 ||| where is my mind (looking at)? predicting visual attention from brain activity. ||| 6955 ||| 4862 ||| 34628 ||| 38681 ||| 38682 ||| 38683 ||| 6956 ||| 6959 ||| 6957 ||| 
2019 ||| spatially and temporally efficient non-local attention network for video-based person re-identification. ||| 8815 ||| 8816 ||| 6328 ||| 8817 ||| 
2022 ||| explore and match: end-to-end video grounding with transformer. ||| 38684 ||| 37070 ||| 38685 ||| 38686 ||| 38687 ||| 38688 ||| 
2021 ||| adnet: attention-guided deformable convolutional network for high dynamic range imaging. ||| 5107 ||| 19231 ||| 19232 ||| 19233 ||| 18141 ||| 19234 ||| 19235 ||| 4394 ||| 19236 ||| 
2020 ||| trat: tracking by attention using spatio-temporal features. ||| 38689 ||| 38690 ||| 20293 ||| 16413 ||| 20294 ||| 38691 ||| 
2022 ||| adversarial robustness of neural-statistical features in detection of generative transformers. ||| 38692 ||| 38693 ||| 965 ||| 38694 ||| 
2019 ||| efficient graph generation with graph recurrent attention networks. ||| 9221 ||| 231 ||| 326 ||| 9236 ||| 38695 ||| 9237 ||| 9238 ||| 9239 ||| 9223 ||| 
2019 ||| verifying asynchronous event-driven programs using partial abstract transformers (extended manuscript). ||| 13174 ||| 13175 ||| 13176 ||| 
2021 ||| towards joint intent detection and slot filling via higher-order attention. ||| 28508 ||| 1229 ||| 3748 ||| 3749 ||| 4430 ||| 
2020 ||| co-gat: a co-interactive graph attention network for joint dialog act recognition and sentiment classification. ||| 12348 ||| 18055 ||| 3707 ||| 18056 ||| 3311 ||| 
2021 ||| on efficient transformer and image pre-training for low-level vision. ||| 6429 ||| 18768 ||| 38696 ||| 35324 ||| 2204 ||| 
2021 ||| towards more efficient insertion transformer with fractional positional encoding. ||| 1011 ||| 1709 ||| 38697 ||| 
2022 ||| guided visual attention model based on interactions between top-down and bottom-up information for robot pose prediction. ||| 38698 ||| 4572 ||| 4574 ||| 
2021 ||| multi-attention generative adversarial network for remote sensing image super-resolution. ||| 15611 ||| 15222 ||| 38699 ||| 38700 ||| 35346 ||| 
2021 ||| dynamic attention guided multi-trajectory analysis for single object tracking. ||| 5957 ||| 5790 ||| 5755 ||| 382 ||| 6416 ||| 28819 ||| 8711 ||| 
2021 ||| multiview detection with shadow transformer (and view-coherent data augmentation). ||| 19513 ||| 8571 ||| 
2019 ||| feature-attention graph convolutional networks for noise resilient learning. ||| 160 ||| 161 ||| 23543 ||| 162 ||| 
2020 ||| channel attention based iterative residual learning for depth map super-resolution. ||| 18979 ||| 2240 ||| 18707 ||| 11307 ||| 3337 ||| 7449 ||| 18708 ||| 
2019 ||| attention-based transfer learning for brain-computer interface. ||| 12432 ||| 1168 ||| 12433 ||| 12434 ||| 12435 ||| 
2018 ||| semi-supervised confidence network aided gated attention based recurrent neural network for clickbait detection. ||| 38701 ||| 
2020 ||| deep reinforced attention learning for quality-aware visual recognition. ||| 8706 ||| 2284 ||| 
2021 ||| deep neural networks on eeg signals to predict auditory attention score using gramian angular difference field. ||| 38702 ||| 38703 ||| 38704 ||| 38705 ||| 4053 ||| 
2022 ||| transformer-based streaming asr with cumulative attention. ||| 12321 ||| 8232 ||| 12322 ||| 12323 ||| 
2021 ||| the animation transformer: visual correspondence via segment matching. ||| 2598 ||| 2253 ||| 2599 ||| 2600 ||| 2601 ||| 38706 ||| 38707 ||| 38708 ||| 38709 ||| 38710 ||| 
2018 ||| skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module. ||| 17772 ||| 1340 ||| 17773 ||| 6559 ||| 17774 ||| 
2021 ||| ast: audio spectrogram transformer. ||| 1708 ||| 14430 ||| 12254 ||| 
2022 ||| litetransformersearch: training-free on-device search for efficient autoregressive language models. ||| 38711 ||| 38712 ||| 34822 ||| 38713 ||| 38714 ||| 38715 ||| 7111 ||| 38716 ||| 38717 ||| 38718 ||| 
2020 ||| informer: beyond efficient transformer for long sequence time-series forecasting. ||| 18004 ||| 18005 ||| 18006 ||| 3364 ||| 215 ||| 9592 ||| 18007 ||| 
2021 ||| depth as attention for face representation learning. ||| 20087 ||| 12091 ||| 20088 ||| 12092 ||| 
2020 ||| on the transformer growth for progressive bert training. ||| 4804 ||| 4805 ||| 4806 ||| 4807 ||| 2230 ||| 1252 ||| 
2019 ||| acoustic scene analysis with multi-head attention networks. ||| 14713 ||| 14714 ||| 2390 ||| 1589 ||| 
2020 ||| efficient urdu caption generation using attention based lstms. ||| 38719 ||| 38720 ||| 38721 ||| 38722 ||| 38723 ||| 
2020 ||| attentional networks for music generation. ||| 38724 ||| 38725 ||| 17364 ||| 38726 ||| 38727 ||| 38728 ||| 
2019 ||| on extractive and abstractive neural document summarization with transformer language models. ||| 26653 ||| 26619 ||| 26652 ||| 38729 ||| 
2021 ||| show, attend and distill: knowledge distillation via attention-based feature matching. ||| 17789 ||| 2086 ||| 8511 ||| 
2020 ||| dual-attention guided dropblock module for weakly supervised object localization. ||| 20125 ||| 20126 ||| 1482 ||| 1484 ||| 786 ||| 
2021 ||| different kinds of cognitive plausibility: why are transformers better than rnns at predicting n400 amplitude? ||| 38730 ||| 38731 ||| 38732 ||| 38733 ||| 
2022 ||| pali-nlp at semeval-2022 task 4: discriminative fine-tuning of deep transformers for patronizing and condescending language detection. ||| 38734 ||| 32724 ||| 38735 ||| 38736 ||| 38737 ||| 10459 ||| 10460 ||| 14158 ||| 
2020 ||| attentional local contrast networks for infrared small target detection. ||| 7144 ||| 7147 ||| 6289 ||| 7148 ||| 
2021 ||| cytran: cycle-consistent transformers for non-contrast to contrast ct translation. ||| 32947 ||| 38738 ||| 38739 ||| 7355 ||| 38740 ||| 1972 ||| 7356 ||| 
2021 ||| caegcn: cross-attention fusion based enhanced graph convolutional network for clustering. ||| 38741 ||| 7826 ||| 619 ||| 38742 ||| 616 ||| 621 ||| 
2019 ||| improving tree-lstm with tree attention. ||| 3223 ||| 3224 ||| 3225 ||| 
2019 ||| pay less attention with lightweight and dynamic convolutions. ||| 14702 ||| 23898 ||| 3823 ||| 24043 ||| 3825 ||| 
2020 ||| lietransformer: equivariant self-attention for lie groups. ||| 22831 ||| 22832 ||| 22833 ||| 22834 ||| 22803 ||| 22835 ||| 
2020 ||| comprehensive attention self-distillation for weakly-supervised object detection. ||| 2356 ||| 9243 ||| 38743 ||| 7393 ||| 
2019 ||| listen and fill in the missing letters: non-autoregressive transformer for speech recognition. ||| 12579 ||| 3549 ||| 12581 ||| 12582 ||| 12583 ||| 
2021 ||| query-by-example keyword spotting system using multi-head attention and softtriple loss. ||| 12704 ||| 12705 ||| 12706 ||| 12707 ||| 
2021 ||| pose-guided inter- and intra-part relational transformer for occluded person re-identification. ||| 19753 ||| 2380 ||| 2383 ||| 
2021 ||| scifive: a text-to-text transformer model for biomedical literature. ||| 34497 ||| 34500 ||| 16138 ||| 38744 ||| 38745 ||| 34501 ||| 17321 ||| 38746 ||| 
2020 ||| semi-supervised classification using attention-based regularization on coarse-resolution data. ||| 9762 ||| 9763 ||| 9764 ||| 9765 ||| 9766 ||| 
2019 ||| improving multi-head attention with capsule networks. ||| 21202 ||| 3076 ||| 
2022 ||| fast monte-carlo approximation of the attention mechanism. ||| 38747 ||| 38748 ||| 
2021 ||| convolutions and self-attention: re-interpreting relative positions in pre-trained language models. ||| 1814 ||| 1813 ||| 1812 ||| 1815 ||| 
2022 ||| atek: augmenting transformers with expert knowledge for indoor layout synthesis. ||| 38749 ||| 38750 ||| 38751 ||| 38752 ||| 
2018 ||| left-center-right separated neural network for aspect-based sentiment analysis with rotatory attention. ||| 38753 ||| 3828 ||| 
2021 ||| aniformer: data-driven 3d animation with transformer. ||| 37627 ||| 435 ||| 437 ||| 33480 ||| 
2021 ||| star: sparse transformer-based action recognition. ||| 26582 ||| 38754 ||| 26578 ||| 26579 ||| 38755 ||| 38756 ||| 19904 ||| 18982 ||| 38757 ||| 
2019 ||| adaptively aligned image captioning via adaptive attention time. ||| 2199 ||| 2200 ||| 9246 ||| 1037 ||| 
2018 ||| where-and-when to look: deep siamese attention networks for video-based person re-identification. ||| 17580 ||| 602 ||| 619 ||| 9889 ||| 
2018 ||| salientdso: bringing attention to direct sparse odometry. ||| 28578 ||| 28579 ||| 28580 ||| 3831 ||| 28581 ||| 
2019 ||| gla-net: an attention network with guided loss for mismatch removal. ||| 3148 ||| 1856 ||| 17668 ||| 
2020 ||| molecule attention transformer. ||| 955 ||| 956 ||| 38758 ||| 38759 ||| 7369 ||| 34181 ||| 
2020 ||| mtgat: multimodal temporal graph attention networks for unaligned human multimodal language sequences. ||| 4944 ||| 4945 ||| 4946 ||| 4880 ||| 4947 ||| 4948 ||| 892 ||| 3601 ||| 
2019 ||| saliency guided self-attention network for weakly-supervised semantic segmentation. ||| 38760 ||| 38761 ||| 
2020 ||| fine-grained human evaluation of transformer and recurrent approaches to neural machine translation for english-to-chinese. ||| 14213 ||| 14214 ||| 
2019 ||| lxmert: learning cross-modality encoder representations from transformers. ||| 26707 ||| 3810 ||| 
2021 ||| filming multimodal sarcasm detection with attention. ||| 38762 ||| 38763 ||| 38764 ||| 38765 ||| 38766 ||| 
2021 ||| soft attention: does it actually help to learn social interactions in pedestrian trajectory prediction? ||| 38767 ||| 37829 ||| 28015 ||| 
2022 ||| saits: self-attention-based imputation for time series. ||| 38768 ||| 8243 ||| 6785 ||| 9337 ||| 
2021 ||| siamapn++: siamese attentional aggregation network for real-time uav tracking. ||| 2604 ||| 2605 ||| 2606 ||| 2607 ||| 2608 ||| 
2019 ||| real-time attention based look-alike model for recommender system. ||| 25330 ||| 25331 ||| 181 ||| 9569 ||| 
2020 ||| asap-net: attention and structure aware point cloud sequence segmentation. ||| 17960 ||| 21460 ||| 5136 ||| 4900 ||| 286 ||| 8660 ||| 
2019 ||| self-attentional models application in task-oriented dialogue generation systems. ||| 14021 ||| 14022 ||| 14023 ||| 14024 ||| 
2017 ||| deep joint entity disambiguation with local neural attention. ||| 26789 ||| 26790 ||| 
2018 ||| qanet: combining local convolution with global self-attention for reading comprehension. ||| 23940 ||| 23941 ||| 22748 ||| 8164 ||| 472 ||| 23942 ||| 9372 ||| 
2019 ||| dual-attention focused module for weakly supervised object localization. ||| 32621 ||| 32622 ||| 32623 ||| 31548 ||| 38769 ||| 38770 ||| 
2021 ||| let: linguistic knowledge enhanced graph transformer for chinese short text matching. ||| 18037 ||| 3147 ||| 3150 ||| 3151 ||| 
2019 ||| progressive pose attention transfer for person image generation. ||| 18897 ||| 17971 ||| 18898 ||| 17248 ||| 18899 ||| 17429 ||| 
2021 ||| nested-block self-attention for robust radiotherapy planning segmentation. ||| 38521 ||| 38517 ||| 38518 ||| 38771 ||| 38772 ||| 38773 ||| 38774 ||| 38775 ||| 
2021 ||| shunted self-attention via multi-scale token aggregation. ||| 19151 ||| 19090 ||| 19152 ||| 1685 ||| 18726 ||| 
2021 ||| more than encoder: introducing transformer decoder to upsample. ||| 38776 ||| 38777 ||| 16721 ||| 16724 ||| 
2021 ||| drug and disease interpretation learning with biomedical entity representation transformer. ||| 15075 ||| 15076 ||| 15077 ||| 15078 ||| 
2019 ||| social-bigat: multimodal trajectory forecasting using bicycle-gan and graph attention networks. ||| 9212 ||| 9213 ||| 9214 ||| 9215 ||| 3882 ||| 9216 ||| 38778 ||| 9218 ||| 
2021 ||| on the integration of self-attention and convolution. ||| 30237 ||| 38779 ||| 38780 ||| 35048 ||| 38781 ||| 2356 ||| 7875 ||| 
2019 ||| assessing knee oa severity with cnn attention-based end-to-end architectures. ||| 11480 ||| 14913 ||| 14914 ||| 1674 ||| 14915 ||| 14916 ||| 1675 ||| 
2018 ||| could interaction with social robots facilitate joint attention of children with autism spectrum disorder? ||| 38782 ||| 38783 ||| 38784 ||| 18958 ||| 35297 ||| 38785 ||| 38786 ||| 38787 ||| 38788 ||| 
2021 ||| an integrated attribute guided dense attention model for fine-grained generalized zero-shot learning. ||| 38789 ||| 32360 ||| 5325 ||| 38790 ||| 20314 ||| 
2020 ||| direct multi-hop attention based graph neural network. ||| 23361 ||| 23362 ||| 14316 ||| 23363 ||| 
2021 ||| nnformer: interleaved transformer for volumetric segmentation. ||| 7913 ||| 38791 ||| 38792 ||| 27802 ||| 15563 ||| 1801 ||| 
2019 ||| predicate transformer semantics for hybrid systems: verification components for isabelle/hol. ||| 38793 ||| 38794 ||| 38795 ||| 
2022 ||| transformers in action: weakly supervised action segmentation. ||| 38796 ||| 38797 ||| 38798 ||| 13628 ||| 19260 ||| 
2021 ||| dpnet: dual-path network for efficient object detectioj with lightweight self-attention. ||| 38799 ||| 11215 ||| 38800 ||| 11217 ||| 7247 ||| 
2021 ||| hot-vae: learning high-order label correlation for multi-label classification via attention-based variational autoencoders. ||| 2815 ||| 18161 ||| 18162 ||| 18163 ||| 18164 ||| 
2021 ||| how facial features convey attention in stationary environments. ||| 38801 ||| 
2021 ||| urban change detection by fully convolutional siamese concatenate network with attention. ||| 38802 ||| 38803 ||| 38804 ||| 38805 ||| 
2021 ||| efficient dialogue state tracking by masked hierarchical transformer. ||| 38806 ||| 38807 ||| 38808 ||| 38809 ||| 
2021 ||| graph transformer networks: learning meta-path graphs to improve gnns. ||| 9247 ||| 9248 ||| 38810 ||| 38811 ||| 38812 ||| 9249 ||| 9250 ||| 9251 ||| 
2021 ||| tfill: image completion via a transformer-based architecture. ||| 38813 ||| 38814 ||| 1691 ||| 
2022 ||| local information assisted attention-free decoder for audio captioning. ||| 38815 ||| 12317 ||| 38816 ||| 38817 ||| 11418 ||| 
2018 ||| where's your focus: personalized attention. ||| 38818 ||| 28079 ||| 
2019 ||| how transformer revitalizes character-based neural machine translation: an investigation on japanese-vietnamese translation systems. ||| 4732 ||| 4733 ||| 4734 ||| 4735 ||| 
2019 ||| gasl: guided attention for sparsity learning in deep neural networks. ||| 37680 ||| 37681 ||| 18600 ||| 18602 ||| 
2020 ||| ftrans: energy-efficient acceleration of transformers using fpga. ||| 9874 ||| 11017 ||| 11018 ||| 11019 ||| 9747 ||| 11020 ||| 11021 ||| 11022 ||| 11023 ||| 11024 ||| 
2022 ||| convolutional neural network with convolutional block attention module for finger vein recognition. ||| 38819 ||| 5044 ||| 
2021 ||| txt: crossmodal end-to-end learning with transformers. ||| 23156 ||| 23157 ||| 3702 ||| 23158 ||| 
2021 ||| ac-covidnet: attention guided contrastive cnn for recognition of covid-19 in chest x-ray images. ||| 38820 ||| 28203 ||| 
2021 ||| machine vision detection to daily facial fatigue with a nonlocal 3d attention network. ||| 6214 ||| 38821 ||| 1448 ||| 38822 ||| 1216 ||| 38823 ||| 38824 ||| 38825 ||| 38826 ||| 
2021 ||| co-grounding networks with semantic attention for referring expression comprehension in videos. ||| 18277 ||| 18924 ||| 18279 ||| 18925 ||| 18926 ||| 
2021 ||| sequential diagnosis prediction with transformer and ontological representation. ||| 2732 ||| 802 ||| 4871 ||| 1300 ||| 800 ||| 
2021 ||| do transformers really perform bad for graph representation? ||| 35843 ||| 33473 ||| 33471 ||| 22737 ||| 33475 ||| 18012 ||| 38827 ||| 4791 ||| 
2021 ||| a novel time-frequency transformer and its application in fault diagnosis of rolling bearings. ||| 32080 ||| 32081 ||| 38828 ||| 32079 ||| 
2018 ||| styling with attention to details. ||| 38829 ||| 38830 ||| 38831 ||| 25339 ||| 
2021 ||| improving transformer-based sequential recommenders through preference editing. ||| 38832 ||| 9545 ||| 1189 ||| 9546 ||| 38833 ||| 9547 ||| 1048 ||| 
2018 ||| learn to pay attention. ||| 23958 ||| 23959 ||| 23960 ||| 2160 ||| 
2021 ||| alebk: feasibility study of attention level estimation via blink detection applied to e-learning. ||| 26872 ||| 38834 ||| 26873 ||| 14327 ||| 26876 ||| 38835 ||| 26874 ||| 26875 ||| 
2021 ||| turning transformer attention weights into zero-shot sequence labelers. ||| 38836 ||| 38837 ||| 11639 ||| 
2018 ||| attention-based group recommendation. ||| 38838 ||| 9709 ||| 9710 ||| 38839 ||| 
2020 ||| deep transformer models for time series forecasting: the influenza prevalence case. ||| 38840 ||| 8658 ||| 38841 ||| 38842 ||| 
2018 ||| sequence-level knowledge distillation for model compression of attention-based sequence-to-sequence speech recognition. ||| 11970 ||| 11971 ||| 11972 ||| 
2020 ||| translating similar languages: role of mutual intelligibility in multilingual transformers. ||| 21395 ||| 3154 ||| 3152 ||| 
2021 ||| combining transformer generators with convolutional discriminators. ||| 5005 ||| 5006 ||| 4196 ||| 5008 ||| 
2018 ||| lcanet: end-to-end lipreading with cascaded attention-ctc. ||| 5723 ||| 5724 ||| 5725 ||| 1117 ||| 
2021 ||| attention-based multi-reference learning for image super-resolution. ||| 1911 ||| 1912 ||| 1913 ||| 
2021 ||| assessment of self-attention on learned features for sound event localization and detection. ||| 13494 ||| 13495 ||| 8240 ||| 
2022 ||| hypermixer: an mlp-based green ai alternative to transformers. ||| 38843 ||| 38844 ||| 38845 ||| 38846 ||| 1226 ||| 38847 ||| 1226 ||| 9349 ||| 3672 ||| 
2021 ||| compositional transformers for scene generation. ||| 22836 ||| 38848 ||| 
2018 ||| sta: spatial-temporal attention for large-scale video-based person re-identification. ||| 9107 ||| 17652 ||| 1905 ||| 17653 ||| 
2021 ||| graph decoupling attention markov networks for semi-supervised graph node classification. ||| 1037 ||| 38849 ||| 38850 ||| 38851 ||| 12666 ||| 619 ||| 
2017 ||| dcn+: mixed objective and deep residual coattention for question answering. ||| 3287 ||| 23928 ||| 19267 ||| 
2021 ||| transformer assisted convolutional network for cell instance segmentation. ||| 38852 ||| 38853 ||| 38854 ||| 38855 ||| 38856 ||| 
2022 ||| adaptive fine-tuning of transformer-based language models for named entity recognition. ||| 38857 ||| 
2022 ||| eventformer: au event transformer for facial action unit event detection. ||| 38858 ||| 719 ||| 7202 ||| 128 ||| 2142 ||| 16809 ||| 
2019 ||| durian: duration informed attention network for multimodal synthesis. ||| 12189 ||| 12572 ||| 14516 ||| 12188 ||| 12586 ||| 3096 ||| 10572 ||| 14517 ||| 4529 ||| 14518 ||| 4530 ||| 3808 ||| 
2020 ||| gated mechanism for attention based multimodal sentiment analysis. ||| 10575 ||| 12564 ||| 
2020 ||| layer-stacked attention for heterogeneous network embedding. ||| 38859 ||| 38860 ||| 
2019 ||| financial series prediction using attention lstm. ||| 13301 ||| 8045 ||| 
2019 ||| gman: a graph multi-attention network for traffic prediction. ||| 18151 ||| 18152 ||| 3691 ||| 17786 ||| 
2020 ||| extending equational monadic reasoning with monad transformers. ||| 10803 ||| 10804 ||| 
2021 ||| learning to cluster faces via transformer. ||| 38861 ||| 8769 ||| 37206 ||| 333 ||| 38862 ||| 1705 ||| 27175 ||| 
2019 ||| deep short text classification with knowledge powered attention. ||| 17861 ||| 17862 ||| 17863 ||| 9635 ||| 17864 ||| 
2018 ||| equity of attention: amortizing individual fairness in rankings. ||| 9549 ||| 9550 ||| 9551 ||| 
2022 ||| headposr: end-to-end trainable head pose estimation using transformer encoders. ||| 5715 ||| 
2021 ||| sdtp: semantic-aware decoupled transformer pyramid for dense image prediction. ||| 20034 ||| 38863 ||| 8838 ||| 8841 ||| 38864 ||| 32532 ||| 
2020 ||| variational transformers for diverse response generation. ||| 10652 ||| 10650 ||| 3676 ||| 11994 ||| 10654 ||| 
2022 ||| gaze-guided class activation mapping: leveraging human attention for network attention in chest x-rays classification. ||| 38865 ||| 2187 ||| 38866 ||| 
2019 ||| regularized context gates on transformer for machine translation. ||| 3047 ||| 3048 ||| 3049 ||| 3050 ||| 3051 ||| 
2020 ||| global attention based graph convolutional neural networks for improved materials property prediction. ||| 38867 ||| 11819 ||| 38868 ||| 38869 ||| 38870 ||| 523 ||| 38871 ||| 
2018 ||| an improved relative self-attention mechanism for transformer with application to music generation. ||| 23972 ||| 2466 ||| 4960 ||| 9132 ||| 11910 ||| 18106 ||| 23973 ||| 22750 ||| 
2017 ||| deep cropping via attention box prediction and aesthetics assessment. ||| 2444 ||| 2445 ||| 
2021 ||| teds-net: enforcing diffeomorphisms in spatial transformers to guarantee topology preservation in segmentations. ||| 27469 ||| 27470 ||| 27471 ||| 27472 ||| 
2020 ||| pop music transformer: generating music with rhythm and harmony. ||| 4372 ||| 4374 ||| 
2021 ||| malbert: using transformers for cybersecurity and malicious software detection. ||| 24562 ||| 24563 ||| 
2021 ||| boundary-aware transformers for skin lesion segmentation. ||| 22108 ||| 27326 ||| 15563 ||| 27327 ||| 978 ||| 8636 ||| 
2020 ||| attention mesh: high-fidelity face mesh prediction in real-time. ||| 38872 ||| 7405 ||| 38873 ||| 38874 ||| 38875 ||| 
2019 ||| interlaced sparse self-attention for semantic segmentation. ||| 32513 ||| 32511 ||| 19745 ||| 8862 ||| 1788 ||| 18793 ||| 
2021 ||| rumor detection on twitter with claim-guided hierarchical graph attention networks. ||| 15185 ||| 3646 ||| 26425 ||| 23330 ||| 26426 ||| 1382 ||| 
2021 ||| perceived and intended sarcasm detection with graph attention networks. ||| 14083 ||| 24747 ||| 
2021 ||| wangchanberta: pretraining transformer-based thai language models. ||| 38876 ||| 38877 ||| 38878 ||| 38879 ||| 
2021 ||| perspectives and prospects on transformer architecture for cross-modal tasks with language and vision. ||| 3229 ||| 32100 ||| 3230 ||| 
2021 ||| a review of bangla natural language processing tasks and the utility of transformer models. ||| 24756 ||| 38880 ||| 24754 ||| 24755 ||| 38881 ||| 38882 ||| 38883 ||| 
2021 ||| subject independent emotion recognition using eeg signals employing attention driven neural networks. ||| 24156 ||| 24157 ||| 24158 ||| 
2021 ||| graph conditioned sparse-attention for improved source code understanding. ||| 26726 ||| 26727 ||| 26728 ||| 
2020 ||| improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images. ||| 7798 ||| 7800 ||| 7801 ||| 
2021 ||| what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers. ||| 26371 ||| 26372 ||| 26373 ||| 26374 ||| 26375 ||| 26376 ||| 3693 ||| 26377 ||| 26378 ||| 26379 ||| 26380 ||| 26381 ||| 26382 ||| 26383 ||| 26384 ||| 26385 ||| 26386 ||| 26387 ||| 26388 ||| 26389 ||| 26390 ||| 26391 ||| 26392 ||| 26393 ||| 26394 ||| 26395 ||| 26396 ||| 26397 ||| 26398 ||| 26399 ||| 26400 ||| 26401 ||| 26402 ||| 26403 ||| 9523 ||| 26404 ||| 26405 ||| 
2021 ||| multi-scale context aggregation network with attention-guided for crowd counting. ||| 398 ||| 13249 ||| 38884 ||| 17562 ||| 
2021 ||| is attention always needed? a case study on language identification from speech. ||| 38885 ||| 21384 ||| 38886 ||| 38887 ||| 38888 ||| 
2019 ||| team papelo: transformer networks at fever. ||| 38889 ||| 
2021 ||| a comparison of deep learning classification methods on small-scale image data set: from convolutional neural networks to visual transformers. ||| 17444 ||| 399 ||| 13789 ||| 34692 ||| 4100 ||| 15956 ||| 
2020 ||| seeing both the forest and the trees: multi-head attention for joint classification on different compositional levels. ||| 11638 ||| 11639 ||| 
2021 ||| generative adversarial transformers. ||| 22836 ||| 38848 ||| 
2018 ||| combining deep and depth: deep learning and face depth maps for driver attention monitoring. ||| 13609 ||| 
2019 ||| attention filtering for multi-person spatiotemporal action detection on deep two-stream cnn architectures. ||| 1994 ||| 38890 ||| 38891 ||| 5412 ||| 38892 ||| 38893 ||| 
2021 ||| attention-guided progressive neural texture fusion for high dynamic range image restoration. ||| 1037 ||| 33693 ||| 38894 ||| 4175 ||| 19426 ||| 35713 ||| 
2022 ||| visual attention network. ||| 24014 ||| 38895 ||| 32107 ||| 1904 ||| 38896 ||| 
2019 ||| monotonic infinite lookback attention for simultaneous machine translation. ||| 3331 ||| 3332 ||| 3333 ||| 3334 ||| 3335 ||| 3336 ||| 3337 ||| 3338 ||| 
2018 ||| advancing acoustic-to-word ctc model with attention and mixed-units. ||| 12178 ||| 12179 ||| 14324 ||| 8164 ||| 12033 ||| 
2021 ||| sampling equivariant self-attention networks for object detection in aerial images. ||| 38897 ||| 38898 ||| 32109 ||| 32110 ||| 
2020 ||| linformer: self-attention with linear complexity. ||| 26302 ||| 38899 ||| 38900 ||| 38901 ||| 25062 ||| 
2017 ||| attention-based cnn matching net. ||| 38902 ||| 38903 ||| 12644 ||| 
2021 ||| polyvit: co-training vision transformers on images, videos and audio. ||| 24034 ||| 2292 ||| 22791 ||| 2295 ||| 1398 ||| 24042 ||| 2293 ||| 
2019 ||| deep fusion: an attention guided factorized bilinear pooling for audio-video emotion recognition. ||| 1008 ||| 1009 ||| 1010 ||| 
2019 ||| a neural attention model for adaptive learning of social friends' preferences. ||| 17106 ||| 38904 ||| 
2021 ||| action transformer: a self-attention model for short-time human action recognition. ||| 30173 ||| 38905 ||| 30602 ||| 38906 ||| 30175 ||| 
2022 ||| audio visual scene-aware dialog generation with transformer-based video representations. ||| 38907 ||| 10277 ||| 4408 ||| 38908 ||| 10276 ||| 
2021 ||| lipschitz normalization for self-attention layers with application to graph neural networks. ||| 22804 ||| 22805 ||| 22806 ||| 
2019 ||| injecting hierarchy with u-net transformers. ||| 38909 ||| 38910 ||| 3458 ||| 
2020 ||| attend and decode: 4d fmri task state decoding using attention models. ||| 9326 ||| 9327 ||| 9328 ||| 9329 ||| 
2019 ||| disambiguating speech intention via audio-text co-attention framework: a case of prosody-semantics interface. ||| 15995 ||| 38911 ||| 14603 ||| 10966 ||| 
2021 ||| dmsanet: dual multi scale attention network. ||| 7335 ||| 
2021 ||| dq-gat: towards safe and efficient autonomous driving with deep q-learning and graph attention networks. ||| 38912 ||| 8845 ||| 37977 ||| 124 ||| 
2020 ||| max-deeplab: end-to-end panoptic segmentation with mask transformers. ||| 8656 ||| 8657 ||| 8659 ||| 8660 ||| 8661 ||| 
2020 ||| would you like sashimi even if it's sliced too thin? selective neural attention for aspect targeted sentiment analysis (snat). ||| 2855 ||| 38913 ||| 38914 ||| 
2022 ||| graph attention transformer network for multi-label image classification. ||| 5332 ||| 38915 ||| 5893 ||| 27968 ||| 38916 ||| 1755 ||| 19345 ||| 
2021 ||| local citation recommendation with hierarchical-attention text encoder and scibert-based reranking. ||| 38917 ||| 3593 ||| 3596 ||| 
2021 ||| higher order recurrent space-time transformer. ||| 14358 ||| 38918 ||| 14360 ||| 9833 ||| 
2017 ||| frustratingly short attention spans in neural language modeling. ||| 23899 ||| 23900 ||| 23901 ||| 23902 ||| 23903 ||| 
2019 ||| show, price and negotiate: a hierarchical attention recurrent visual negotiator. ||| 38919 ||| 5176 ||| 6417 ||| 36346 ||| 
2021 ||| temporal memory attention for video semantic segmentation. ||| 1371 ||| 6390 ||| 2058 ||| 
2021 ||| on-the-fly attention modularization for neural generation. ||| 3349 ||| 3350 ||| 3351 ||| 3352 ||| 3353 ||| 3354 ||| 3355 ||| 
2018 ||| learning to exploit invariances in clinical time-series data using sequence transformer networks. ||| 20677 ||| 20678 ||| 20679 ||| 
2020 ||| towards boosting the channel attention in real image denoising : sub-band pyramid attention. ||| 15990 ||| 17550 ||| 17551 ||| 38920 ||| 6126 ||| 
2021 ||| yformer: u-net inspired transformer architecture for far horizon time series forecasting. ||| 38921 ||| 38922 ||| 38923 ||| 38924 ||| 34785 ||| 
2019 ||| pose-adaptive hierarchical attention network for facial expression recognition. ||| 30097 ||| 38925 ||| 38926 ||| 1916 ||| 
2021 ||| after-unet: axial fusion transformer unet for medical image segmentation. ||| 7409 ||| 435 ||| 7138 ||| 7410 ||| 7411 ||| 7135 ||| 
2021 ||| is attention better than matrix decomposition? ||| 24013 ||| 24014 ||| 18525 ||| 2514 ||| 24015 ||| 2518 ||| 
2021 ||| finetuning transformer models to build asag system. ||| 38927 ||| 
2020 ||| transforming multi-concept attention into video summarization. ||| 6327 ||| 2376 ||| 6328 ||| 
2018 ||| the pros and cons: rank-aware temporal attention for skill determination in long videos. ||| 18920 ||| 18921 ||| 7794 ||| 
2021 ||| thermal infrared image colorization for nighttime driving scenes with top-down guided attention. ||| 38928 ||| 38929 ||| 38930 ||| 22485 ||| 8608 ||| 33134 ||| 
2019 ||| is it worth the attention? a comparative evaluation of attention layers for argument unit segmentation. ||| 16144 ||| 16145 ||| 16146 ||| 16147 ||| 
2021 ||| from extreme multi-label to multi-class: a hierarchical approach for automated icd-10 coding using phrase-level attention. ||| 3525 ||| 38931 ||| 38932 ||| 38933 ||| 
2022 ||| mixformer: end-to-end tracking with iterative mixed attention. ||| 38934 ||| 35021 ||| 2216 ||| 2217 ||| 
2019 ||| exbert: a visual analysis tool to explore learned representations in transformers models. ||| 3261 ||| 3262 ||| 3263 ||| 
2018 ||| deep adaptive attention for joint facial action unit detection and face alignment. ||| 8795 ||| 5247 ||| 1691 ||| 5141 ||| 
2018 ||| hierarchical attention-based recurrent highway networks for time series prediction. ||| 24224 ||| 19727 ||| 38935 ||| 6307 ||| 683 ||| 38936 ||| 
2021 ||| the devil is in the detail: simple tricks improve systematic generalization of transformers. ||| 59 ||| 26307 ||| 7111 ||| 12659 ||| 4194 ||| 11785 ||| 
2021 ||| asper: attention-based approach to extract syntactic patterns denoting semantic relations in sentential context. ||| 35537 ||| 38937 ||| 26740 ||| 38938 ||| 
2021 ||| pnp-detr: towards efficient visual analysis with transformers. ||| 128 ||| 1722 ||| 1723 ||| 1685 ||| 1728 ||| 
2022 ||| a unified transformer framework for group-based segmentation: co-segmentation, co-saliency detection and video salient object detection. ||| 38939 ||| 38940 ||| 38941 ||| 1677 ||| 1827 ||| 
2021 ||| the right to talk: an audio-visual transformer approach. ||| 2132 ||| 2133 ||| 2134 ||| 2135 ||| 2136 ||| 2137 ||| 2138 ||| 
2018 ||| component-based attention for large-scale trademark retrieval. ||| 11373 ||| 11374 ||| 38942 ||| 11330 ||| 11331 ||| 38943 ||| 
2021 ||| mlma-net: multi-level multi-attentional learning for multi-label object detection in textile defect images. ||| 38088 ||| 34627 ||| 38086 ||| 
2019 ||| automatic segmentation of vestibular schwannoma from t2-weighted mri by deep spatial attention with hardness-weighted loss. ||| 15623 ||| 27859 ||| 27860 ||| 27460 ||| 27861 ||| 27862 ||| 27863 ||| 27864 ||| 7111 ||| 14874 ||| 27462 ||| 
2022 ||| bvit: broad attention based vision transformer. ||| 2062 ||| 1012 ||| 38944 ||| 21188 ||| 29026 ||| 
2020 ||| understood in translation, transformers for domain understanding. ||| 17953 ||| 17954 ||| 17955 ||| 17956 ||| 
2020 ||| contextualized attention-based knowledge transfer for spoken conversational question answering. ||| 7412 ||| 12623 ||| 4430 ||| 
2021 ||| improving the efficiency of transformers for resource-constrained devices. ||| 24114 ||| 24115 ||| 24116 ||| 16397 ||| 16396 ||| 
2022 ||| transformer grammars: augmenting transformer language models with syntactic inductive biases at scale. ||| 38945 ||| 38946 ||| 38947 ||| 38948 ||| 10518 ||| 24960 ||| 
2020 ||| big bird: transformers for longer sequences. ||| 9229 ||| 9230 ||| 38223 ||| 3557 ||| 9232 ||| 9233 ||| 3882 ||| 9234 ||| 3555 ||| 9235 ||| 2884 ||| 5776 ||| 
2021 ||| trans4trans: efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world. ||| 7856 ||| 7857 ||| 7858 ||| 7859 ||| 7860 ||| 3831 ||| 7861 ||| 
2020 ||| self-adaptive physics-informed neural networks using a soft attention mechanism. ||| 2664 ||| 2665 ||| 
2019 ||| ha-ccn: hierarchical attention-based crowd counting network. ||| 19135 ||| 18609 ||| 
2021 ||| solve routing problems with a residual edge-graph attention neural network. ||| 38949 ||| 6286 ||| 9149 ||| 19413 ||| 38950 ||| 
2022 ||| video transformers: a survey. ||| 8028 ||| 38951 ||| 8035 ||| 8379 ||| 8034 ||| 8027 ||| 7111 ||| 
2022 ||| domain adaptation via bidirectional cross-attention transformer. ||| 38952 ||| 38953 ||| 9472 ||| 
2022 ||| is cross-attention preferable to self-attention for multi-modal emotion recognition? ||| 38954 ||| 38955 ||| 38956 ||| 
2022 ||| spatial transformer k-means. ||| 38957 ||| 38958 ||| 38959 ||| 38960 ||| 10168 ||| 38961 ||| 
2021 ||| clinical relation extraction using transformer-based models. ||| 13408 ||| 21565 ||| 13701 ||| 2989 ||| 12067 ||| 
2022 ||| hypertransformer: model generation for supervised and semi-supervised few-shot learning. ||| 38962 ||| 38963 ||| 38964 ||| 
2020 ||| a multi-channel temporal attention convolutional neural network model for environmental sound classification. ||| 12459 ||| 12460 ||| 12461 ||| 
2019 ||| multi-scale time-frequency attention for rare sound event detection. ||| 14392 ||| 14393 ||| 14394 ||| 329 ||| 
2020 ||| a robust attentional framework for license plate recognition in the wild. ||| 6570 ||| 5845 ||| 4175 ||| 5189 ||| 6335 ||| 10075 ||| 
2020 ||| gravitational models explain shifts on human visual attention. ||| 896 ||| 898 ||| 897 ||| 38965 ||| 
2019 ||| dual-attention graph convolutional network. ||| 2813 ||| 2814 ||| 2815 ||| 2816 ||| 1825 ||| 
2021 ||| contrastively learning visual attention as affordance cues from demonstrations for robotic grasping. ||| 25485 ||| 25486 ||| 25487 ||| 
2018 ||| an end-to-end textspotter with explicit alignment and attention. ||| 2448 ||| 15886 ||| 2447 ||| 6335 ||| 2149 ||| 18753 ||| 
2020 ||| channel distillation: channel-wise attention for knowledge distillation. ||| 38966 ||| 38967 ||| 38968 ||| 26165 ||| 
2021 ||| ammus : a survey of transformer-based pretrained models in natural language processing. ||| 31154 ||| 31155 ||| 31156 ||| 
2019 ||| semantic mask for transformer based end-to-end speech recognition. ||| 14693 ||| 2280 ||| 14694 ||| 12179 ||| 12389 ||| 14310 ||| 14695 ||| 14324 ||| 12137 ||| 3480 ||| 
2019 ||| hybrid residual attention network for single image super resolution. ||| 8742 ||| 38969 ||| 8747 ||| 
2021 ||| semantic correspondence with transformers. ||| 36363 ||| 36364 ||| 9286 ||| 38970 ||| 1515 ||| 8797 ||| 
2022 ||| vaqf: fully automatic software-hardware co-design framework for low-bit vision transformer. ||| 33412 ||| 7410 ||| 8570 ||| 32371 ||| 19181 ||| 33409 ||| 7195 ||| 15791 ||| 
2020 ||| attendlight: universal attention-based reinforcement learning model for traffic signal control. ||| 9379 ||| 9380 ||| 9381 ||| 9382 ||| 
2020 ||| the effect of top-down attention in occluded object recognition. ||| 38971 ||| 
2021 ||| self-slimmed vision transformer. ||| 17696 ||| 34404 ||| 34405 ||| 2148 ||| 2149 ||| 38972 ||| 16550 ||| 
2020 ||| da4ad: end-to-end deep attention aware features aided visual localization for autonomous driving. ||| 8605 ||| 8606 ||| 8607 ||| 5999 ||| 8608 ||| 8609 ||| 8610 ||| 
2019 ||| tree transformer: integrating tree structures into self-attention. ||| 26794 ||| 12644 ||| 4843 ||| 
2020 ||| highway transformer: self-gating enhanced self-attentive networks. ||| 3241 ||| 3242 ||| 3243 ||| 
2020 ||| musicoder: a universal music-acoustic encoder based on transformers. ||| 5931 ||| 38973 ||| 38974 ||| 5932 ||| 33518 ||| 
2020 ||| do transformers need deep long-range memory. ||| 3709 ||| 3710 ||| 
2019 ||| attention-guided lightweight network for real-time segmentation of robotic surgical instruments. ||| 5183 ||| 5184 ||| 271 ||| 5185 ||| 5186 ||| 5189 ||| 
2022 ||| grouptransnet: group transformer network for rgb-d salient object detection. ||| 38975 ||| 38976 ||| 38977 ||| 21636 ||| 
2021 ||| attention in attention network for image super-resolution. ||| 37627 ||| 38978 ||| 32627 ||| 
2021 ||| caa : channelized axial attention for semantic segmentation. ||| 17343 ||| 17341 ||| 5123 ||| 11307 ||| 33367 ||| 1756 ||| 
2021 ||| visformer: the vision-friendly transformer. ||| 2476 ||| 2477 ||| 2478 ||| 2479 ||| 2480 ||| 2398 ||| 
2017 ||| call attention to rumors: deep attention based recurrent neural networks for early rumor detection. ||| 5664 ||| 17580 ||| 9889 ||| 1796 ||| 1203 ||| 602 ||| 
2020 ||| multi-domain dialogue state tracking - a purely transformer-based generative approach. ||| 38979 ||| 1377 ||| 
2018 ||| spatial transformer introspective neural network. ||| 38980 ||| 11478 ||| 8906 ||| 8660 ||| 
2022 ||| variational stacked local attention networks for diverse video captioning. ||| 7372 ||| 7373 ||| 7374 ||| 7375 ||| 7376 ||| 7377 ||| 
2021 ||| heterogeneous graph attention networks for learning diverse communication. ||| 38981 ||| 22678 ||| 3928 ||| 38982 ||| 38983 ||| 3929 ||| 
2019 ||| attention-based supply-demand prediction for autonomous vehicles. ||| 17456 ||| 14115 ||| 17457 ||| 17458 ||| 17459 ||| 
2021 ||| locating faulty methods with a mixed rnn and attention model. ||| 5975 ||| 5976 ||| 5977 ||| 5978 ||| 5979 ||| 
2020 ||| on the ability of self-attention networks to recognize counter languages. ||| 23083 ||| 26495 ||| 23085 ||| 
2020 ||| style example-guided text generation using generative adversarial transformers. ||| 38984 ||| 38985 ||| 37647 ||| 
2018 ||| understanding and improving recurrent networks for human activity recognition by continuous attention. ||| 9964 ||| 23043 ||| 23044 ||| 9963 ||| 23045 ||| 14548 ||| 23046 ||| 
2017 ||| dynamic time-aware attention to speaker roles and contexts for spoken language understanding. ||| 13912 ||| 13913 ||| 4841 ||| 4843 ||| 
2020 ||| mango: a mask attention guided one-stage scene text spotter. ||| 17806 ||| 3503 ||| 2609 ||| 2611 ||| 17807 ||| 1937 ||| 2258 ||| 
2021 ||| assessing the impact of attention and self-attention mechanisms on the classification of skin lesions. ||| 38986 ||| 38987 ||| 
2021 ||| a novel attention-based network for fast salient object detection. ||| 2747 ||| 3208 ||| 38988 ||| 38989 ||| 
2022 ||| gat-cadnet: graph attention network for panoptic symbol spotting in cad drawings. ||| 38990 ||| 38991 ||| 38992 ||| 38993 ||| 38994 ||| 38995 ||| 
2021 ||| javabert: training a transformer-based model for the java programming language. ||| 3886 ||| 3887 ||| 
2022 ||| open set recognition using vision transformer with an additional detection head. ||| 38996 ||| 38997 ||| 3114 ||| 38998 ||| 
2021 ||| spatio-temporal graph dual-attention network for multi-agent prediction and tracking. ||| 2269 ||| 2270 ||| 31880 ||| 38999 ||| 2272 ||| 
2019 ||| semi-interactive attention network for answer understanding in reverse-qa. ||| 15246 ||| 15247 ||| 15248 ||| 5077 ||| 15249 ||| 
2020 ||| optimizing transformer for low-resource neural machine translation. ||| 11692 ||| 11693 ||| 
2021 ||| demographic-guided attention in recurrent neural networks for modeling neuropathophysiological heterogeneity. ||| 27891 ||| 8776 ||| 27892 ||| 27893 ||| 15551 ||| 
2021 ||| lifting transformer for 3d human pose estimation in video. ||| 11109 ||| 2519 ||| 10253 ||| 11559 ||| 1703 ||| 
2017 ||| ssemnet: serial-section electron microscopy image registration using a spatial transformer network with learned features. ||| 27894 ||| 27895 ||| 27896 ||| 27897 ||| 27898 ||| 
2019 ||| dual adversarial learning with attention mechanism for fine-grained medical image synthesis. ||| 18047 ||| 18048 ||| 577 ||| 18051 ||| 
2021 ||| mst: masked self-supervised transformer for visual representation. ||| 39000 ||| 19480 ||| 1856 ||| 3337 ||| 19481 ||| 11290 ||| 39001 ||| 21250 ||| 8164 ||| 2074 ||| 2077 ||| 
2020 ||| point transformer. ||| 2335 ||| 2336 ||| 2204 ||| 2160 ||| 1884 ||| 
2021 ||| bootstrapping vits: towards liberating vision transformers from pre-training. ||| 33177 ||| 39002 ||| 23431 ||| 16954 ||| 21803 ||| 23433 ||| 
2021 ||| understanding and overcoming the challenges of efficient transformer quantization. ||| 26855 ||| 26856 ||| 26857 ||| 
2021 ||| efficient video transformers with spatial-temporal token selection. ||| 38091 ||| 37700 ||| 32334 ||| 7314 ||| 17842 ||| 
2020 ||| granet: global relation-aware attentional network for als point cloud classification. ||| 28795 ||| 18750 ||| 18752 ||| 
2020 ||| automatic lyrics transcription using dilated convolutional neural networks with self-attention. ||| 316 ||| 317 ||| 318 ||| 319 ||| 
2020 ||| exploring recurrent, memory and attention based architectures for scoring interactional aspects of human-machine text dialog. ||| 39003 ||| 25677 ||| 16986 ||| 
2021 ||| reason induced visual attention for explainable autonomous driving. ||| 23638 ||| 23637 ||| 39004 ||| 39005 ||| 23641 ||| 
2021 ||| the heads hypothesis: a unifying statistical approach towards understanding multi-headed attention in bert. ||| 18066 ||| 18067 ||| 3327 ||| 11768 ||| 3328 ||| 
2019 ||| permutohedral attention module for efficient non-local neural networks. ||| 27459 ||| 27460 ||| 27461 ||| 7111 ||| 14874 ||| 27462 ||| 27463 ||| 
2018 ||| video action transformer network. ||| 1663 ||| 1994 ||| 1995 ||| 8786 ||| 1997 ||| 
2020 ||| global attention for name tagging. ||| 23102 ||| 23103 ||| 3604 ||| 3403 ||| 
2021 ||| simple local attentions remain competitive for long-context tasks. ||| 39006 ||| 39007 ||| 39008 ||| 39009 ||| 39010 ||| 3348 ||| 4898 ||| 4897 ||| 
2020 ||| explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation. ||| 4775 ||| 2253 ||| 31149 ||| 31150 ||| 31151 ||| 1476 ||| 
2017 ||| paying attention to multi-word expressions in neural machine translation. ||| 37277 ||| 21421 ||| 
2021 ||| fq-vit: fully quantized vision transformer without retraining. ||| 996 ||| 28341 ||| 39011 ||| 6679 ||| 24938 ||| 
2021 ||| m2tr: multi-modal multi-scale transformers for deepfake detection. ||| 38091 ||| 7314 ||| 14648 ||| 17842 ||| 
2022 ||| transsleep: transitioning-aware attention-based deep neural network for sleep staging. ||| 39012 ||| 16081 ||| 16080 ||| 743 ||| 
2020 ||| bidirectional attention network for monocular depth estimation. ||| 21812 ||| 21813 ||| 21814 ||| 21815 ||| 21816 ||| 
2021 ||| mixed attention transformer for leveraging word-level knowledge to neural cross-lingual information retrieval. ||| 1229 ||| 1230 ||| 1231 ||| 1232 ||| 1233 ||| 
2020 ||| local context attention for salient object segmentation. ||| 2214 ||| 6397 ||| 6400 ||| 6399 ||| 6398 ||| 
2021 ||| attestnet - an attention and subword tokenization based approach for code-switched hindi-english hate speech detection. ||| 39013 ||| 39014 ||| 
2019 ||| low rank factorization for compact multi-head self-attention. ||| 8983 ||| 359 ||| 8985 ||| 
2020 ||| t-vectors: weakly supervised speaker identification using hierarchical transformer model. ||| 12086 ||| 39015 ||| 12087 ||| 8233 ||| 
2020 ||| convtransformer: a convolutional transformer network for video frame synthesis. ||| 39016 ||| 39017 ||| 39018 ||| 28951 ||| 39019 ||| 28953 ||| 37396 ||| 
2021 ||| evaluating pretrained transformer models for entity linking in task-oriented dialog. ||| 39020 ||| 39021 ||| 39022 ||| 
2021 ||| generative pre-trained transformer for cardiac abnormality detection. ||| 20402 ||| 20403 ||| 8382 ||| 20404 ||| 4194 ||| 59 ||| 20405 ||| 20406 ||| 20407 ||| 
2020 ||| dtgan: dual attention generative adversarial networks for text-to-image generation. ||| 862 ||| 863 ||| 
2021 ||| sequential vessel segmentation via deep channel attention network. ||| 39023 ||| 39024 ||| 39025 ||| 39026 ||| 39027 ||| 39028 ||| 39029 ||| 
2017 ||| a dual-stage attention-based recurrent neural network for time series prediction. ||| 23521 ||| 9738 ||| 1159 ||| 1156 ||| 23522 ||| 23523 ||| 
2020 ||| attentional separation-and-aggregation network for self-supervised depth-pose learning in dynamic scenes. ||| 6810 ||| 22283 ||| 10211 ||| 3906 ||| 8689 ||| 
2021 ||| relative positional encoding for transformers with linear complexity. ||| 22796 ||| 22797 ||| 22798 ||| 11901 ||| 22799 ||| 4374 ||| 8063 ||| 8064 ||| 
2021 ||| congested crowd instance localization with dilated convolutional swin transformer. ||| 31724 ||| 34992 ||| 6922 ||| 
2022 ||| recent advances in vision transformer: a survey and outlook of recent work. ||| 39030 ||| 
2022 ||| transcam: transformer attention-based cam refinement for weakly supervised semantic segmentation. ||| 39031 ||| 39032 ||| 39033 ||| 39034 ||| 39035 ||| 39036 ||| 
2021 ||| aaseg: attention aware network for real time semantic segmentation. ||| 7335 ||| 
2020 ||| a novel attention-based aggregation function to combine vision and language. ||| 19002 ||| 19001 ||| 19003 ||| 13611 ||| 
2018 ||| attnconvnet at semeval-2018 task 1: attention-based convolutional neural networks for multi-label emotion classification. ||| 10563 ||| 10564 ||| 10565 ||| 
2018 ||| medical concept embedding with time-aware attention. ||| 18015 ||| 23459 ||| 23460 ||| 23461 ||| 4646 ||| 15243 ||| 
2020 ||| palomino-ochoa at semeval-2020 task 9: robust system based on transformer for code-mixed sentiment classification. ||| 10534 ||| 852 ||| 9983 ||| 
2020 ||| understanding when spatial transformer networks do not support invariance, and what to do about it. ||| 20377 ||| 20378 ||| 20379 ||| 
2020 ||| attention based multiple instance learning for classification of blood cell disorders. ||| 27899 ||| 27900 ||| 27901 ||| 13628 ||| 27902 ||| 15680 ||| 27903 ||| 
2022 ||| semi-supervised graph attention networks for event representation learning. ||| 1994 ||| 18465 ||| 18466 ||| 
2020 ||| weakly-supervised action localization by generative attention modeling. ||| 18792 ||| 6545 ||| 8016 ||| 18793 ||| 
2021 ||| fast offline transformer-based end-to-end automatic speech recognition for real-world applications. ||| 14679 ||| 14680 ||| 39037 ||| 
2021 ||| saccadecam: adaptive visual attention for monocular depth sensing. ||| 2374 ||| 2375 ||| 
2021 ||| attention on classification for fire segmentation. ||| 25981 ||| 5412 ||| 
2020 ||| collaborative attention mechanism for multi-view action recognition. ||| 39038 ||| 39039 ||| 8561 ||| 4417 ||| 5669 ||| 1734 ||| 
2021 ||| alignment knowledge distillation for online streaming attention-based speech recognition. ||| 12682 ||| 4418 ||| 
2020 ||| utilising visual attention cues for vehicle detection and tracking. ||| 20331 ||| 20332 ||| 1675 ||| 11483 ||| 20333 ||| 
2022 ||| unified visual transformer compression. ||| 39040 ||| 19181 ||| 1928 ||| 36669 ||| 8545 ||| 2095 ||| 3478 ||| 7195 ||| 
2018 ||| point2sequence: learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. ||| 18229 ||| 2563 ||| 2559 ||| 18230 ||| 
2021 ||| sanmove: next location recommendation via self-attention network. ||| 39041 ||| 379 ||| 39042 ||| 640 ||| 
2021 ||| dytox: transformers for continual learning with dynamic token expansion. ||| 39043 ||| 39044 ||| 39045 ||| 2118 ||| 
2022 ||| learning affinity from attention: end-to-end weakly-supervised semantic segmentation with transformers. ||| 39046 ||| 6850 ||| 39047 ||| 17757 ||| 
2020 ||| document-level event-based extraction using generative template-filling transformers. ||| 4961 ||| 4962 ||| 4963 ||| 
2020 ||| emformer: efficient memory transformer based acoustic model for low latency streaming speech recognition. ||| 11974 ||| 11973 ||| 11976 ||| 11978 ||| 11977 ||| 11975 ||| 12489 ||| 12491 ||| 
2018 ||| cross-media multi-level alignment with relation attention network. ||| 23380 ||| 5954 ||| 5953 ||| 
2020 ||| attention is all you need in speech separation. ||| 12284 ||| 12285 ||| 12286 ||| 12287 ||| 12288 ||| 
2022 ||| generalised image outpainting with u-transformer. ||| 39048 ||| 13408 ||| 3248 ||| 4776 ||| 25634 ||| 
2022 ||| simplicial attention networks. ||| 39049 ||| 39050 ||| 39051 ||| 39052 ||| 39053 ||| 
2021 ||| probabilistic graph attention network with conditional kernels for pixel-wise prediction. ||| 436 ||| 9285 ||| 2303 ||| 2524 ||| 1846 ||| 437 ||| 
2021 ||| combining gcn and transformer for chinese grammatical error detection. ||| 39054 ||| 
2021 ||| towards low-latency energy-efficient deep snns via attention-guided compression. ||| 7327 ||| 7328 ||| 7329 ||| 7330 ||| 
2021 ||| on the distribution, sparsity, and inference-time quantization of attention values in transformers. ||| 3695 ||| 3696 ||| 3697 ||| 3698 ||| 3699 ||| 3299 ||| 
2022 ||| safl: a self-attention scene text recognizer with focal loss. ||| 25923 ||| 25924 ||| 25925 ||| 25926 ||| 25927 ||| 25928 ||| 
2021 ||| fine-grained attention for weakly supervised object localization. ||| 39055 ||| 16080 ||| 39056 ||| 39057 ||| 743 ||| 
2019 ||| goal-directed behavior under variational predictive coding: dynamic organization of visual attention and working memory. ||| 25497 ||| 25498 ||| 25499 ||| 
2020 ||| attention improves concentration when learning node embeddings. ||| 39058 ||| 39059 ||| 39060 ||| 39061 ||| 39062 ||| 39063 ||| 
2019 ||| a comparative study on transformer vs rnn in speech applications. ||| 8066 ||| 12579 ||| 8223 ||| 2508 ||| 12682 ||| 13956 ||| 13957 ||| 13958 ||| 13959 ||| 12608 ||| 3549 ||| 8224 ||| 12246 ||| 
2019 ||| attention guided network for retinal image segmentation. ||| 27845 ||| 4056 ||| 27846 ||| 27847 ||| 1827 ||| 19774 ||| 6413 ||| 15560 ||| 
2021 ||| bayesian attention networks for data compression. ||| 39064 ||| 
2017 ||| social attention: modeling attention in human crowds. ||| 21866 ||| 39065 ||| 15691 ||| 21867 ||| 
2018 ||| accelerating neural transformer via an average attention network. ||| 3180 ||| 3181 ||| 3182 ||| 
2021 ||| video sentiment analysis with bimodal information-augmented multi-head attention. ||| 39066 ||| 39067 ||| 12503 ||| 39068 ||| 39069 ||| 39070 ||| 
2021 ||| salient positions based attention network for image classification. ||| 39071 ||| 29598 ||| 9779 ||| 
2021 ||| cardiac segmentation on ct images through shape-aware contour attentions. ||| 39072 ||| 34518 ||| 
2019 ||| modeling point clouds with self-attention and gumbel subset sampling. ||| 15535 ||| 817 ||| 8832 ||| 19341 ||| 19342 ||| 19343 ||| 2398 ||| 
2020 ||| self-attention with cross-lingual position representation. ||| 3794 ||| 3038 ||| 1756 ||| 
2020 ||| context-aware learning to rank with self-attention. ||| 39073 ||| 39074 ||| 39075 ||| 39076 ||| 39077 ||| 
2019 ||| scattnet: semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images. ||| 12801 ||| 39078 ||| 3036 ||| 39079 ||| 39080 ||| 39081 ||| 
2021 ||| pureformer: do we even need attention? ||| 39082 ||| 39083 ||| 
2019 ||| deep features analysis with attention networks. ||| 39084 ||| 39085 ||| 6465 ||| 12377 ||| 
2020 ||| characterizing speech adversarial examples using self-attention u-net enhancement. ||| 7457 ||| 12670 ||| 12671 ||| 12672 ||| 12404 ||| 
2021 ||| 4d attention: comprehensive framework for spatio-temporal gaze mapping. ||| 39086 ||| 39087 ||| 39088 ||| 39089 ||| 
2021 ||| uninet: unified architecture search with convolution, transformer, and mlp. ||| 39090 ||| 1848 ||| 34405 ||| 17578 ||| 16550 ||| 
2020 ||| attention-oriented action recognition for real-time human-robot interaction. ||| 20210 ||| 20211 ||| 6351 ||| 20212 ||| 20213 ||| 20214 ||| 20215 ||| 
2019 ||| multi-step reasoning via recurrent dual attention for visual dialog. ||| 2044 ||| 2045 ||| 3472 ||| 2043 ||| 2046 ||| 1958 ||| 
2020 ||| ngat4rec: neighbor-aware graph attention network for recommendation. ||| 11164 ||| 11165 ||| 1234 ||| 39091 ||| 1239 ||| 
2021 ||| amr parsing with action-pointer transformer. ||| 4818 ||| 3551 ||| 1633 ||| 3553 ||| 4819 ||| 
2022 ||| swin transformers make strong contextual encoders for vhr image road extraction. ||| 1978 ||| 39092 ||| 6596 ||| 
2021 ||| locformer: enabling transformers to perform temporal moment localization on long untrimmed videos with a feature sampling approach. ||| 7446 ||| 7447 ||| 33076 ||| 14010 ||| 6417 ||| 
2021 ||| self-attention presents low-dimensional knowledge graph embeddings for link prediction. ||| 39093 ||| 39094 ||| 33581 ||| 
2019 ||| collective link prediction oriented network embedding with hierarchical graph attention. ||| 1336 ||| 1090 ||| 538 ||| 1093 ||| 
2020 ||| ulsam: ultra-lightweight subspace attention module for compact convolutional neural networks. ||| 7394 ||| 7395 ||| 7396 ||| 7397 ||| 7398 ||| 
2021 ||| on the prunability of attention heads in multilingual bert. ||| 18067 ||| 18066 ||| 11768 ||| 3328 ||| 
2020 ||| an exploratory study of argumentative writing by young students: a transformer-based approach. ||| 16986 ||| 16987 ||| 16988 ||| 
2020 ||| biomedical event extraction on graph edge-conditioned attention networks with hierarchical knowledge graphs. ||| 26665 ||| 26666 ||| 18020 ||| 
2022 ||| dsformer: a dual-domain self-supervised transformer for accelerated multi-contrast mri reconstruction. ||| 21551 ||| 31282 ||| 39095 ||| 39096 ||| 38116 ||| 15551 ||| 39097 ||| 
2018 ||| a comparison of transformer and recurrent neural networks on multilingual neural machine translation. ||| 11718 ||| 11719 ||| 11720 ||| 
2020 ||| dcanet: learning connected attentions for convolutional neural networks. ||| 19807 ||| 19806 ||| 19811 ||| 19916 ||| 6415 ||| 17441 ||| 19812 ||| 
2021 ||| transformer with peak suppression and knowledge guidance for fine-grained image recognition. ||| 39098 ||| 3435 ||| 39099 ||| 
2021 ||| artificial text detection via examining the topology of attention maps. ||| 26701 ||| 26702 ||| 26703 ||| 10339 ||| 26704 ||| 26705 ||| 18098 ||| 26706 ||| 2648 ||| 
2018 ||| multi-attention recurrent network for human communication comprehension. ||| 4948 ||| 3599 ||| 892 ||| 17970 ||| 893 ||| 3601 ||| 
2021 ||| crack semantic segmentation using the u-net with full attention strategy. ||| 39100 ||| 39101 ||| 39102 ||| 39103 ||| 
2021 ||| aweu-net: an attention-aware weight excitation u-net for lung nodule segmentation. ||| 39104 ||| 39105 ||| 39106 ||| 39107 ||| 39108 ||| 
2021 ||| improving face-based age estimation with attention-based dynamic patch fusion. ||| 39109 ||| 11621 ||| 39110 ||| 
2020 ||| improving bert with self-supervised attention. ||| 38569 ||| 7703 ||| 18497 ||| 13362 ||| 24962 ||| 18501 ||| 2349 ||| 18503 ||| 
2020 ||| weakly supervised training of hierarchical attention networks for speaker identification. ||| 12086 ||| 12087 ||| 8233 ||| 
2020 ||| careful analysis of xrd patterns with attention. ||| 39111 ||| 39112 ||| 39113 ||| 
2018 ||| cram: clued recurrent attention model. ||| 39114 ||| 32201 ||| 
2021 ||| generating symbolic reasoning problems with transformer gans. ||| 39115 ||| 39116 ||| 
2021 ||| generating coherent and diverse slogans with sequence-to-sequence transformer. ||| 39117 ||| 39118 ||| 39119 ||| 39120 ||| 
2021 ||| video salient object detection via contrastive features and attention modules. ||| 7140 ||| 7141 ||| 7142 ||| 7143 ||| 
2022 ||| dualsc: automatic generation and summarization of shellcode via transformer and dual learning. ||| 6005 ||| 6007 ||| 6006 ||| 6008 ||| 
2021 ||| e images combining self-attention multiple instance learning with a recurrent neural network. ||| 39121 ||| 39122 ||| 39123 ||| 39124 ||| 39125 ||| 39126 ||| 39127 ||| 39128 ||| 
2021 ||| improve the interpretability of attention: a fast, accurate, and interpretable high-resolution attention model. ||| 39129 ||| 39130 ||| 39131 ||| 39132 ||| 20302 ||| 7350 ||| 
2020 ||| multi-branch attentive transformer. ||| 39133 ||| 4788 ||| 4787 ||| 4785 ||| 4789 ||| 2742 ||| 4791 ||| 
2022 ||| gateformer: speeding up news feed recommendation with input gated transformers. ||| 39134 ||| 921 ||| 
2021 ||| discriminative and generative transformer-based models for situation entity classification. ||| 39135 ||| 39136 ||| 39137 ||| 39138 ||| 
2021 ||| graph attention recurrent neural networks for correlated time series forecasting - full version. ||| 39139 ||| 39140 ||| 10875 ||| 
2019 ||| unsupervised image-to-image translation with self-attention networks. ||| 20646 ||| 19377 ||| 
2020 ||| text segmentation by cross segment attention. ||| 26586 ||| 26587 ||| 26589 ||| 26590 ||| 227 ||| 26588 ||| 
2021 ||| non-autoregressive transformer-based end-to-end asr using bert. ||| 39141 ||| 17201 ||| 
2021 ||| combiner: full attention transformer with sparse computation cost. ||| 7203 ||| 39142 ||| 3780 ||| 39143 ||| 23363 ||| 37105 ||| 39144 ||| 
2019 ||| correction of automatic speech recognition with transformer sequence-to-sequence model. ||| 12696 ||| 12697 ||| 12698 ||| 
2017 ||| deep visual attention prediction. ||| 2444 ||| 2445 ||| 
2021 ||| accurate and clear precipitation nowcasting with consecutive attention and rain-map discrimination. ||| 39145 ||| 7039 ||| 39146 ||| 39147 ||| 39148 ||| 7040 ||| 
2021 ||| vision transformer with progressive sampling. ||| 2156 ||| 2157 ||| 2158 ||| 2159 ||| 2160 ||| 2161 ||| 2162 ||| 
2021 ||| indonesia's fake news detection using transformer network. ||| 4316 ||| 4315 ||| 4317 ||| 4318 ||| 
2021 ||| boosting crowd counting with transformers. ||| 7813 ||| 4477 ||| 33778 ||| 33777 ||| 33776 ||| 7814 ||| 
2019 ||| analysis of various transformer structures for high frequency isolation applications. ||| 39149 ||| 39150 ||| 22029 ||| 22028 ||| 
2021 ||| fda-gan: flow-based dual attention gan for human pose transfer. ||| 39151 ||| 17670 ||| 39152 ||| 39153 ||| 39154 ||| 
2021 ||| geometry-free view synthesis: transformers and no 3d priors. ||| 1802 ||| 1803 ||| 648 ||| 1804 ||| 
2021 ||| locally enhanced self-attention: rethinking self-attention as local and context terms. ||| 36043 ||| 36501 ||| 33749 ||| 8660 ||| 
2017 ||| a guided spatial transformer network for histology cell differentiation. ||| 16973 ||| 16974 ||| 16975 ||| 16976 ||| 6818 ||| 
2021 ||| greenformers: improving computation and memory efficiency in transformer models via low-rank approximation. ||| 11993 ||| 
2018 ||| mask r-cnn with pyramid attention network for scene text detection. ||| 7188 ||| 7189 ||| 7190 ||| 7191 ||| 
2022 ||| speech denoising in the waveform domain with self-attention. ||| 39155 ||| 39156 ||| 39157 ||| 2459 ||| 
2020 ||| gsanet: semantic segmentation with global and selective attention. ||| 4437 ||| 11315 ||| 11316 ||| 11317 ||| 
2018 ||| veram: view-enhanced recurrent attention model for 3d shape classification. ||| 39158 ||| 32118 ||| 2349 ||| 37268 ||| 5723 ||| 
2020 ||| check_square at checkthat! claim detection in social media via fusion of transformer and syntactic features. ||| 10687 ||| 10688 ||| 10689 ||| 
2019 ||| aspect category detection via topic-attention network. ||| 10437 ||| 39159 ||| 10440 ||| 10439 ||| 
2021 ||| doctr: document image transformer for geometric unwarping and illumination correction. ||| 2078 ||| 19778 ||| 1806 ||| 2415 ||| 1807 ||| 
2019 ||| graph transformer networks. ||| 9247 ||| 9248 ||| 9249 ||| 9250 ||| 9251 ||| 
2020 ||| attention augmented differentiable forest for tabular data. ||| 34759 ||| 
2021 ||| a spatio-temporal attention-based model for infant movement assessment from videos. ||| 31105 ||| 31106 ||| 31107 ||| 31108 ||| 31109 ||| 31110 ||| 
2020 ||| jaa-net: joint facial action unit detection and face alignment via adaptive attention. ||| 8795 ||| 5247 ||| 1691 ||| 5141 ||| 
2021 ||| multi-context attention fusion neural network for software vulnerability identification. ||| 39160 ||| 39161 ||| 39162 ||| 39163 ||| 39164 ||| 39165 ||| 
2020 ||| attention flows: analyzing and comparing attention mechanisms in language models. ||| 39166 ||| 39167 ||| 39168 ||| 
2022 ||| dftr: depth-supervised hierarchical feature fusion transformer for salient object detection. ||| 35347 ||| 3751 ||| 27340 ||| 343 ||| 27778 ||| 27341 ||| 
2019 ||| ctrl: a conditional transformer language model for controllable generation. ||| 23929 ||| 39169 ||| 15983 ||| 3287 ||| 19267 ||| 
2021 ||| uformer: a general u-shaped transformer for image restoration. ||| 39170 ||| 17856 ||| 23783 ||| 2363 ||| 
2021 ||| a multi-input multi-output transformer-based hybrid neural network for multi-class privacy disclosure detection. ||| 39171 ||| 39172 ||| 
2019 ||| learning similarity attention. ||| 39173 ||| 1745 ||| 2441 ||| 17779 ||| 1744 ||| 
2021 ||| attention temperature matters in abstractive summarization distillation. ||| 39174 ||| 3479 ||| 3498 ||| 3174 ||| 
2021 ||| associating objects with transformers for video object segmentation. ||| 37971 ||| 1905 ||| 208 ||| 
2019 ||| low-rank hoca: efficient high-order cross-modal attention for video captioning. ||| 14156 ||| 22669 ||| 22668 ||| 17724 ||| 
2020 ||| reproduction of lateral inhibition-inspired convolutional neural network for visual attention and saliency detection. ||| 39175 ||| 
2020 ||| dual attention on pyramid feature maps for image captioning. ||| 39176 ||| 12196 ||| 603 ||| 
2021 ||| vision xformers: efficient attention for image classification. ||| 39177 ||| 35882 ||| 
2021 ||| cma-clip: cross-modality attention clip for image-text classification. ||| 39178 ||| 39179 ||| 11213 ||| 1305 ||| 13410 ||| 39180 ||| 39181 ||| 5272 ||| 
2018 ||| sharp attention network via adaptive sampling for person re-identification. ||| 22328 ||| 7380 ||| 28827 ||| 28828 ||| 28829 ||| 28830 ||| 2490 ||| 
2020 ||| adaptive transformers for learning multimodal representations. ||| 3411 ||| 
2021 ||| mpvit: multi-path vision transformer for dense prediction. ||| 39182 ||| 39183 ||| 39184 ||| 9317 ||| 
2021 ||| reformer: the relational transformer for image captioning. ||| 39185 ||| 39186 ||| 398 ||| 
2021 ||| generative video transformer: can objects be the words? ||| 22862 ||| 22863 ||| 22864 ||| 
2019 ||| lattice transformer for speech translation. ||| 3803 ||| 3470 ||| 3804 ||| 3805 ||| 
2021 ||| adavit: adaptive tokens for efficient vision transformer. ||| 34008 ||| 39187 ||| 34411 ||| 39188 ||| 24023 ||| 34010 ||| 
2021 ||| transformer-based machine learning for fast sat solvers and logic synthesis. ||| 26582 ||| 38754 ||| 39189 ||| 39190 ||| 18982 ||| 38757 ||| 
2017 ||| online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. ||| 2302 ||| 2303 ||| 1848 ||| 1846 ||| 2304 ||| 2305 ||| 
2021 ||| the performance evaluation of attention-based neural asr under mixed speech input. ||| 39191 ||| 12484 ||| 
2018 ||| pairwise body-part attention for recognizing human-object interactions. ||| 8547 ||| 8548 ||| 8549 ||| 5136 ||| 
2022 ||| reconformer: accelerated mri reconstruction using recurrent transformer. ||| 39192 ||| 19270 ||| 39193 ||| 39194 ||| 18609 ||| 
2020 ||| hierarchical attention transformer architecture for syntactic spell correction. ||| 34975 ||| 9858 ||| 39195 ||| 
2020 ||| pdanet: pyramid density-aware attention net for accurate crowd counting. ||| 39196 ||| 5123 ||| 17341 ||| 30826 ||| 10330 ||| 
2020 ||| pre-training text-to-text transformers for concept-centric common sense. ||| 23947 ||| 23948 ||| 23949 ||| 23950 ||| 39197 ||| 1250 ||| 
2020 ||| human activity recognition from wearable sensor data using self-attention. ||| 10202 ||| 10203 ||| 7374 ||| 10204 ||| 7376 ||| 10205 ||| 10206 ||| 7375 ||| 
2022 ||| vit-fod: a vision transformer based fine-grained object discriminator. ||| 39198 ||| 23859 ||| 4945 ||| 9999 ||| 9675 ||| 
2019 ||| being the center of attention: a person-context cnn framework for personality recognition. ||| 39199 ||| 39200 ||| 11061 ||| 
2019 ||| hierarchical attention networks for medical image segmentation. ||| 23784 ||| 5908 ||| 39201 ||| 1035 ||| 5896 ||| 24731 ||| 27774 ||| 5907 ||| 
2021 ||| do we really need explicit position encodings for vision transformers? ||| 6433 ||| 1099 ||| 15886 ||| 6935 ||| 5155 ||| 
2019 ||| multi-agent actor-critic with hierarchical graph attention network. ||| 17980 ||| 17981 ||| 17982 ||| 
2020 ||| double multi-head attention for speaker verification. ||| 12474 ||| 12475 ||| 12476 ||| 
2021 ||| an improved single step non-autoregressive transformer for automatic speech recognition. ||| 12771 ||| 12772 ||| 12773 ||| 705 ||| 14442 ||| 
2022 ||| snowflake point deconvolution for point cloud completion and generation with skip-transformer. ||| 2557 ||| 2558 ||| 2559 ||| 2560 ||| 2561 ||| 2562 ||| 2563 ||| 
2020 ||| attention-aware fusion rgb-d face recognition. ||| 20087 ||| 12091 ||| 20088 ||| 12092 ||| 
2022 ||| transdreamer: reinforcement learning with transformer world models. ||| 8708 ||| 22862 ||| 22863 ||| 22864 ||| 
2018 ||| training deeper neural machine translation models with transparent attention. ||| 26497 ||| 26498 ||| 26499 ||| 26500 ||| 12067 ||| 
2018 ||| molecular transformer for chemical reaction prediction and uncertainty estimation. ||| 34543 ||| 34548 ||| 13992 ||| 39202 ||| 39203 ||| 39204 ||| 39205 ||| 
2020 ||| deliberate self-attention network with uncertainty estimation for multi-aspect review rating prediction. ||| 35608 ||| 2885 ||| 8954 ||| 
2017 ||| reinforcement learning with analogical similarity to guide schema induction and attention. ||| 39206 ||| 39207 ||| 
2021 ||| sentimentarcs: a novel method for self-supervised sentiment analysis of time series shows sota transformers can struggle finding narrative arcs. ||| 39208 ||| 
2017 ||| end-to-end attention based text-dependent speaker verification. ||| 14635 ||| 12029 ||| 11819 ||| 12179 ||| 12033 ||| 
2021 ||| beit: bert pre-training of image transformers. ||| 3498 ||| 3171 ||| 3174 ||| 
2022 ||| m2ts: multi-scale multi-modal approach based on transformer for source code summarization. ||| 39209 ||| 5025 ||| 
2020 ||| deeprhythm: exposing deepfakes with attentional visual heartbeat rhythms. ||| 19522 ||| 18280 ||| 19523 ||| 19524 ||| 6805 ||| 12081 ||| 1305 ||| 19525 ||| 
2018 ||| hierarchical spatial transformer network. ||| 5249 ||| 5250 ||| 39210 ||| 5252 ||| 
2020 ||| data augmentation using pre-trained transformer models. ||| 39211 ||| 39212 ||| 39213 ||| 
2021 ||| applications of artificial neural networks in microorganism image analysis: a comprehensive review from conventional multilayer perceptron to popular convolutional neural network and potential visual transformer. ||| 39214 ||| 399 ||| 15956 ||| 
2020 ||| bert-jam: boosting bert-enhanced neural machine translation with joint attention. ||| 7680 ||| 7681 ||| 7682 ||| 1216 ||| 
2021 ||| ladra-net: locally-aware dynamic re-read attention net for sentence semantic matching. ||| 1558 ||| 15204 ||| 15205 ||| 1301 ||| 1302 ||| 444 ||| 
2021 ||| history aware multimodal transformer for vision-and-language navigation. ||| 19642 ||| 38401 ||| 2093 ||| 2174 ||| 
2021 ||| learning generalizable vision-tactile robotic grasping strategy for deformable objects via transformer. ||| 39215 ||| 39216 ||| 39217 ||| 22811 ||| 39218 ||| 39219 ||| 13833 ||| 
2021 ||| tstnn: two-stage transformer based neural network for speech enhancement in the time domain. ||| 333 ||| 11998 ||| 11999 ||| 
2021 ||| a trained humanoid robot can perform human-like crossmodal social attention conflict resolution. ||| 39220 ||| 23359 ||| 39221 ||| 39222 ||| 39223 ||| 39224 ||| 2659 ||| 1017 ||| 
2021 ||| vut: versatile ui transformer for multi-modal multi-task user interface modeling. ||| 438 ||| 858 ||| 29033 ||| 2293 ||| 39225 ||| 
2020 ||| improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. ||| 9779 ||| 6559 ||| 9780 ||| 9781 ||| 
2017 ||| drug-drug interaction extraction via recurrent neural network with multiple attention layers. ||| 13177 ||| 11677 ||| 11676 ||| 7779 ||| 
2020 ||| mu-gan: facial attribute editing based on multi-attention mechanism. ||| 19919 ||| 38939 ||| 24484 ||| 24485 ||| 28869 ||| 
2021 ||| scene transformer: a unified multi-task model for behavior prediction and planning. ||| 18826 ||| 39226 ||| 39227 ||| 11356 ||| 39228 ||| 22820 ||| 39229 ||| 39230 ||| 17733 ||| 39231 ||| 26627 ||| 39232 ||| 12069 ||| 2467 ||| 
2021 ||| patentminer: patent vacancy mining via context-enhanced and knowledge-guided graph attention. ||| 28116 ||| 92 ||| 28117 ||| 28118 ||| 28119 ||| 28120 ||| 28121 ||| 
2021 ||| bitfit: simple parameter-efficient fine-tuning for transformer-based masked language-models. ||| 39233 ||| 39234 ||| 3441 ||| 
2019 ||| efficient 8-bit quantization of transformer neural machine language translation model. ||| 39235 ||| 39236 ||| 39237 ||| 39238 ||| 39239 ||| 39240 ||| 39241 ||| 
2022 ||| learning class prototypes from synthetic insar with vision transformers. ||| 39242 ||| 39243 ||| 39244 ||| 
2021 ||| syntax-aware graph-to-graph transformer for semantic role labelling. ||| 26685 ||| 3672 ||| 
2017 ||| textual entailment with structured attentions and composition. ||| 14419 ||| 14420 ||| 14418 ||| 
2020 ||| vsgnet: spatial attention network for detecting human object interactions using graph convolutions. ||| 7197 ||| 19185 ||| 39245 ||| 
2022 ||| dynamic n: m fine-grained structured sparse attention mechanism. ||| 21683 ||| 39246 ||| 21681 ||| 11307 ||| 18023 ||| 12120 ||| 
2019 ||| ultrasound image representation learning by modeling sonographer visual attention. ||| 10018 ||| 10019 ||| 10020 ||| 10021 ||| 10022 ||| 10023 ||| 10024 ||| 
2021 ||| attention mechanisms in computer vision: a survey. ||| 24014 ||| 39247 ||| 39248 ||| 32107 ||| 1901 ||| 32108 ||| 32104 ||| 32109 ||| 1904 ||| 32110 ||| 
2020 ||| automated labelling using an attention model for radiology reports of mri scans (alarm). ||| 14861 ||| 14862 ||| 14863 ||| 14864 ||| 14865 ||| 14866 ||| 14867 ||| 14868 ||| 14869 ||| 14870 ||| 14871 ||| 14872 ||| 14873 ||| 7111 ||| 14874 ||| 14875 ||| 14876 ||| 
2022 ||| abductionrules: training transformers to explain unexpected inputs. ||| 39249 ||| 25995 ||| 39250 ||| 39251 ||| 
2020 ||| simplified self-attention for transformer-based end-to-end speech recognition. ||| 14669 ||| 14494 ||| 14599 ||| 12384 ||| 
2019 ||| attentionboost: learning what to attend by boosting fully convolutional networks. ||| 39252 ||| 39253 ||| 39254 ||| 
2022 ||| sguie-net: semantic attention guided underwater image enhancement with multi-scale perception. ||| 6188 ||| 39255 ||| 2625 ||| 16201 ||| 39256 ||| 39257 ||| 
2019 ||| spatial transformer for 3d points. ||| 35625 ||| 18030 ||| 19021 ||| 
2019 ||| synthesising expressiveness in peking opera via duration informed attention network. ||| 13512 ||| 13516 ||| 12189 ||| 12572 ||| 12586 ||| 14271 ||| 3808 ||| 
2021 ||| unsupervised visual attention and invariance for reinforcement learning. ||| 12824 ||| 19020 ||| 19021 ||| 
2021 ||| attention-guided supervised contrastive learning for semantic segmentation. ||| 37002 ||| 7206 ||| 5101 ||| 23905 ||| 24710 ||| 7208 ||| 24706 ||| 
2021 ||| pay better attention to attention: head selection in multilingual and multi-domain sequence modeling. ||| 17911 ||| 14317 ||| 11637 ||| 9390 ||| 
2020 ||| faster transformer decoding: n-gram masked self-attention. ||| 39258 ||| 26498 ||| 26497 ||| 9132 ||| 
2019 ||| deep attention based semi-supervised 2d-pose estimation for surgical instruments. ||| 20292 ||| 20293 ||| 16413 ||| 20294 ||| 20295 ||| 20296 ||| 20297 ||| 5695 ||| 
2020 ||| linking social media posts to news with siamese transformers. ||| 39259 ||| 
2022 ||| adaptive cross-layer attention for image restoration. ||| 39260 ||| 7957 ||| 8957 ||| 39261 ||| 
2020 ||| low rank fusion based transformers for multimodal sequences. ||| 9295 ||| 9294 ||| 9293 ||| 9297 ||| 
2021 ||| temporal attention augmented transformer hawkes process. ||| 414 ||| 415 ||| 416 ||| 417 ||| 
2021 ||| dense dual-attention network for light field image super-resolution. ||| 39262 ||| 19139 ||| 38368 ||| 19308 ||| 19143 ||| 
2021 ||| improving users' mental model with attention-directed counterfactual edits. ||| 39263 ||| 32856 ||| 1238 ||| 39264 ||| 4194 ||| 39265 ||| 7594 ||| 39266 ||| 
2020 ||| spatio-temporal point processes with attention for traffic congestion event modeling. ||| 35692 ||| 35694 ||| 35693 ||| 39267 ||| 35695 ||| 
2018 ||| supervised domain enablement attention for personalized domain classification. ||| 26484 ||| 3027 ||| 
2019 ||| inverse visual question answering with multi-level attentions. ||| 22652 ||| 22653 ||| 
2019 ||| attention forcing for sequence-to-sequence model training. ||| 14579 ||| 38021 ||| 14580 ||| 3796 ||| 
2019 ||| multi-head multi-layer attention to deep language representations for grammatical error detection. ||| 16989 ||| 14211 ||| 
2021 ||| should i look at the head or the tail? dual-awareness attention for few-shot object detection. ||| 39268 ||| 7810 ||| 1385 ||| 39269 ||| 39270 ||| 32215 ||| 1387 ||| 
2020 ||| enhance multimodal transformer with external label and in-domain pretrain: hateful meme challenge winning solution. ||| 39271 ||| 
2021 ||| dsgpt: domain-specific generative pre-training of transformers for text generation in e-commerce title and review summarization. ||| 9665 ||| 9666 ||| 9667 ||| 9668 ||| 1460 ||| 9669 ||| 9670 ||| 9671 ||| 
2020 ||| end-to-end object detection with adaptive clustering transformer. ||| 2171 ||| 2170 ||| 1846 ||| 1848 ||| 39272 ||| 
2021 ||| medical transformer: universal brain encoder for 3d mri analysis. ||| 741 ||| 16079 ||| 39273 ||| 743 ||| 
2020 ||| sentibert: a transferable transformer-based architecture for compositional sentiment semantics. ||| 3031 ||| 3032 ||| 3033 ||| 
2019 ||| audio2face: generating speech/face animation from single audio with attention-based bidirectional lstm networks. ||| 19845 ||| 19846 ||| 4297 ||| 
2021 ||| shifted chunk transformer for spatio-temporal representational learning. ||| 39274 ||| 39275 ||| 39276 ||| 2095 ||| 3478 ||| 
2021 ||| surgical instruction generation with transformers. ||| 27455 ||| 27456 ||| 27457 ||| 27458 ||| 
2021 ||| a cnn-bilstm model with attention mechanism for earthquake prediction. ||| 39277 ||| 39278 ||| 39279 ||| 39280 ||| 
2021 ||| multimodal transformer with variable-length memory for vision-and-language navigation. ||| 39281 ||| 37262 ||| 1691 ||| 39282 ||| 3570 ||| 8725 ||| 
2021 ||| channel-wise attention-based network for self-supervised monocular depth estimation. ||| 13630 ||| 13631 ||| 13632 ||| 13633 ||| 
2020 ||| deep attention aware feature learning for person re-identification. ||| 23691 ||| 12273 ||| 39283 ||| 2249 ||| 39284 ||| 
2018 ||| ea-lstm: evolutionary attention-based lstm for time series prediction. ||| 39285 ||| 6507 ||| 39286 ||| 5252 ||| 4400 ||| 
2019 ||| infant brain mri segmentation with dilated convolution pyramid downsampling and self-attention. ||| 39287 ||| 11275 ||| 9191 ||| 39288 ||| 11276 ||| 
2021 ||| symmetry-enhanced attention network for acute ischemic infarct segmentation with non-contrast ct images. ||| 27748 ||| 19744 ||| 27749 ||| 27750 ||| 2608 ||| 2142 ||| 1801 ||| 
2019 ||| estimating attention flow in online video networks. ||| 22916 ||| 31893 ||| 14077 ||| 
2020 ||| avr: attention based salient visual relationship detection. ||| 4269 ||| 39289 ||| 39290 ||| 
2021 ||| bert transformer model for detecting arabic gpt2 auto-generated tweets. ||| 39291 ||| 39292 ||| 39293 ||| 37608 ||| 
2019 ||| path-augmented graph transformer network. ||| 39294 ||| 26502 ||| 26524 ||| 
2021 ||| moca: incorporating multi-stage domain pretraining and cross-guided multimodal attention for textbook question answering. ||| 39295 ||| 22886 ||| 1235 ||| 39296 ||| 39297 ||| 39298 ||| 22884 ||| 
2019 ||| giving attention to the unexpected: using prosody innovations in disfluency detection. ||| 4833 ||| 4834 ||| 
2022 ||| attentionhtr: handwritten text recognition based on attention encoder-decoder networks. ||| 39299 ||| 39300 ||| 
2018 ||| a collaborative computer aided diagnosis (c-cad) system with eye-tracking, sparse attentional model, and deep learning. ||| 19319 ||| 31243 ||| 27603 ||| 31244 ||| 27604 ||| 31245 ||| 
2021 ||| long-short transformer: efficient transformers for language and vision. ||| 1835 ||| 39156 ||| 39301 ||| 38985 ||| 34247 ||| 34410 ||| 2459 ||| 
2021 ||| luna: linear unified nested attention. ||| 39302 ||| 9393 ||| 26302 ||| 39303 ||| 26298 ||| 25062 ||| 24017 ||| 
2021 ||| topical language generation using transformers. ||| 3939 ||| 3940 ||| 
2021 ||| utnlp at semeval-2021 task 5: a comparative analysis of toxic span detection using attention-based, named entity recognition, and ensemble models. ||| 10547 ||| 10548 ||| 10549 ||| 10550 ||| 10439 ||| 
2019 ||| microsoft ai challenge india 2018: learning to rank passages for web question answering with deep attention networks. ||| 20508 ||| 
2017 ||| discrete choice and rational inattention: a general equivalence result. ||| 39304 ||| 39305 ||| 3369 ||| 39306 ||| 39307 ||| 
2019 ||| modulated self-attention convolutional network for vqa. ||| 9280 ||| 38683 ||| 38682 ||| 2693 ||| 34629 ||| 
2021 ||| transformer meets convolution: a bilateral awareness net-work for semantic segmentation of very fine resolution ur-ban scene images. ||| 30557 ||| 8207 ||| 30558 ||| 30455 ||| 30559 ||| 30560 ||| 
2020 ||| sit3: code summarization with structure-induced transformer. ||| 3739 ||| 3111 ||| 1254 ||| 
2019 ||| towards online end-to-end transformer automatic speech recognition. ||| 12365 ||| 12364 ||| 13953 ||| 3549 ||| 
2019 ||| i-mad: a novel interpretable malware detector using hierarchical transformer. ||| 39308 ||| 29479 ||| 39309 ||| 39310 ||| 
2018 ||| attention-guided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs. ||| 14911 ||| 2021 ||| 27492 ||| 27491 ||| 705 ||| 14912 ||| 
2021 ||| simple attention module based speaker verification with iterative noisy label detection. ||| 36233 ||| 10057 ||| 12586 ||| 4530 ||| 765 ||| 
2019 ||| attention privileged reinforcement learning for domain transfer. ||| 22286 ||| 22287 ||| 22288 ||| 22289 ||| 22290 ||| 
2021 ||| transformers to fight the covid-19 infodemic. ||| 14012 ||| 3849 ||| 10586 ||| 
2019 ||| hierarchical transformers for multi-document summarization. ||| 1305 ||| 3408 ||| 
2019 ||| vssa-net: vertical spatial sequence attention network for traffic sign detection. ||| 6650 ||| 6708 ||| 6627 ||| 
2021 ||| explainable identification of dementia from transcripts using transformer networks. ||| 36385 ||| 36386 ||| 
2020 ||| two stage transformer model for covid-19 fake news detection and fact checking. ||| 39311 ||| 39312 ||| 19447 ||| 39313 ||| 
2018 ||| dual attention network for scene segmentation. ||| 11391 ||| 2058 ||| 19086 ||| 19087 ||| 2080 ||| 
2021 ||| a separable temporal convolution neural network with attention for small-footprint keyword spotting. ||| 4549 ||| 4550 ||| 4551 ||| 39314 ||| 4552 ||| 
2021 ||| patch slimming for efficient vision transformers. ||| 32730 ||| 19744 ||| 19362 ||| 3156 ||| 19745 ||| 19367 ||| 1756 ||| 
2019 ||| ssa-cnn: semantic self-attention cnn for pedestrian detection. ||| 
2021 ||| scene representation transformer: geometry-free novel view synthesis through set-latent scene representations. ||| 8756 ||| 39315 ||| 39316 ||| 39317 ||| 39318 ||| 39319 ||| 39320 ||| 2295 ||| 39321 ||| 9291 ||| 4960 ||| 39322 ||| 2555 ||| 
2019 ||| self-attention graph pooling. ||| 19095 ||| 22744 ||| 9250 ||| 
2021 ||| linear-time self attention with codeword histogram for efficient recommendation. ||| 8911 ||| 8912 ||| 8913 ||| 8914 ||| 8915 ||| 1245 ||| 8916 ||| 
2018 ||| micro-attention for micro-expression recognition. ||| 5663 ||| 3626 ||| 39323 ||| 5664 ||| 
2019 ||| multimodal semantic attention network for video captioning. ||| 19819 ||| 8838 ||| 8837 ||| 19820 ||| 8841 ||| 
2020 ||| transformer-based neural text generation with syntactic guidance. ||| 39324 ||| 580 ||| 39325 ||| 8862 ||| 
2022 ||| lawin transformer: improving semantic segmentation transformer with multi-scale representations via large window attention. ||| 39326 ||| 985 ||| 10974 ||| 
2020 ||| mischief: a simple black-box attack against transformer architectures. ||| 39327 ||| 
2020 ||| long-short term masking transformer: a simple but effective baseline for document-level neural machine translation. ||| 3803 ||| 3470 ||| 3804 ||| 3805 ||| 
2021 ||| dsc-iitism at fincausal 2021: combining pos tagging with attention-based contextual representations for identifying causal relationships in financial documents. ||| 39328 ||| 39329 ||| 38853 ||| 
2019 ||| image super-resolution using attention based densenet with residual deconvolution. ||| 17584 ||| 
2021 ||| pt-vton: an image-based virtual try-on network with progressive pose attention transfer. ||| 39330 ||| 7955 ||| 39331 ||| 
2020 ||| self-attention aggregation network for video face representation and recognition. ||| 39332 ||| 39333 ||| 39334 ||| 39335 ||| 39336 ||| 39337 ||| 
2021 ||| improved context-based offline meta-rl with attention and contrastive learning. ||| 39338 ||| 39339 ||| 39340 ||| 
2021 ||| multi-density attention network for loop filtering in video compression. ||| 30281 ||| 39341 ||| 19815 ||| 
2019 ||| second-order non-local attention networks for person re-identification. ||| 17901 ||| 1708 ||| 1709 ||| 1710 ||| 
2019 ||| slices of attention in asynchronous video job interviews. ||| 2713 ||| 18136 ||| 18137 ||| 18139 ||| 9772 ||| 9773 ||| 
2019 ||| dependency-aware attention control for unconstrained face recognition with image sets. ||| 8525 ||| 8526 ||| 497 ||| 8527 ||| 5379 ||| 
2020 ||| unsupervised ct metal artifact learning using attention-guided beta-cyclegan. ||| 39342 ||| 39343 ||| 2048 ||| 
2021 ||| xeroalign: zero-shot cross-lingual transformer alignment. ||| 3103 ||| 3104 ||| 
2021 ||| peta: photo albums event recognition using transformers attention. ||| 39344 ||| 39345 ||| 39346 ||| 39347 ||| 39348 ||| 39349 ||| 33578 ||| 
2022 ||| knowledge amalgamation for object detection with transformers. ||| 33177 ||| 39350 ||| 23431 ||| 39351 ||| 39352 ||| 16954 ||| 23433 ||| 
2021 ||| distance and hop-wise structures encoding enhanced graph attention networks. ||| 39353 ||| 39354 ||| 39355 ||| 
2021 ||| graformer: graph convolution transformer for 3d pose estimation. ||| 39356 ||| 39357 ||| 2364 ||| 39358 ||| 19506 ||| 
2020 ||| enriched pre-trained transformers for joint slot filling and intent detection. ||| 10603 ||| 10604 ||| 7049 ||| 
2019 ||| multi-turn dialogue response generation with autoregressive transformer models. ||| 39359 ||| 39360 ||| 
2021 ||| atrous residual interconnected encoder to attention decoder framework for vertebrae segmentation via 3d volumetric ct images. ||| 39361 ||| 39362 ||| 11355 ||| 39363 ||| 39364 ||| 
2021 ||| cross-attention conformer for context modeling in speech enhancement for asr. ||| 13894 ||| 3334 ||| 13895 ||| 12652 ||| 12098 ||| 
2021 ||| multimodal integration of human-like attention in visual question answering. ||| 9269 ||| 23109 ||| 23110 ||| 8346 ||| 3831 ||| 39365 ||| 39366 ||| 8348 ||| 
2020 ||| magnet: multi-region attention-assisted grounding of natural language queries at phrase level. ||| 20144 ||| 20145 ||| 11018 ||| 20146 ||| 
2019 ||| automatic prostate zonal segmentation using fully convolutional network with feature pyramid attention. ||| 39367 ||| 6005 ||| 39368 ||| 39369 ||| 39370 ||| 39371 ||| 39372 ||| 39373 ||| 39374 ||| 39375 ||| 
2017 ||| disan: directional self-attention network for rnn/cnn-free language understanding. ||| 4871 ||| 4872 ||| 802 ||| 800 ||| 799 ||| 4873 ||| 
2021 ||| multimodal incremental transformer with visual grounding for visual dialogue generation. ||| 3673 ||| 3075 ||| 3674 ||| 3675 ||| 1921 ||| 
2020 ||| pymt5: multi-mode translation of natural language and python code with transformers. ||| 26545 ||| 15263 ||| 26546 ||| 15264 ||| 15265 ||| 
2021 ||| few-shot domain adaptation with polymorphic transformers. ||| 23339 ||| 23340 ||| 3362 ||| 4056 ||| 23341 ||| 27568 ||| 23342 ||| 4297 ||| 27569 ||| 3390 ||| 
2021 ||| attentional meta-learners are polythetic classifiers. ||| 39376 ||| 1633 ||| 39377 ||| 8648 ||| 39378 ||| 23955 ||| 
2020 ||| smyrf: efficient attention using asymmetric clustering. ||| 9180 ||| 3143 ||| 9181 ||| 9182 ||| 
2022 ||| visual attention prediction improves performance of autonomous drone racing agents. ||| 39379 ||| 39380 ||| 39381 ||| 39382 ||| 
2021 ||| pruning self-attentions into convolutional layers in single path. ||| 2059 ||| 2058 ||| 2056 ||| 1691 ||| 875 ||| 1756 ||| 2057 ||| 
2019 ||| controllable attention for structured layered video decomposition. ||| 1993 ||| 1994 ||| 1995 ||| 1996 ||| 1997 ||| 
2020 ||| structured self-attention weights encode semantics in sentiment analysis. ||| 20974 ||| 20975 ||| 20976 ||| 
2021 ||| sda-gan: unsupervised image translation using spectral domain attention-guided generative adversarial network. ||| 39383 ||| 39384 ||| 
2021 ||| improved drug-target interaction prediction with intermolecular graph transformer. ||| 39385 ||| 39386 ||| 6669 ||| 39387 ||| 329 ||| 39388 ||| 24823 ||| 14779 ||| 4791 ||| 
2019 ||| bsdar: beam search decoding with attention reward in neural keyphrase generation. ||| 39389 ||| 39390 ||| 3923 ||| 
2020 ||| transformer feed-forward layers are key-value memories. ||| 26354 ||| 26355 ||| 26356 ||| 3348 ||| 
2021 ||| multi-person extreme motion prediction with cross-interaction attention. ||| 39391 ||| 39392 ||| 9285 ||| 4605 ||| 
2021 ||| transformer network for significant stenosis detection in ccta of coronary arteries. ||| 27605 ||| 27606 ||| 1160 ||| 20424 ||| 
2021 ||| vilt: vision-and-language transformer without convolution or region supervision. ||| 9355 ||| 22822 ||| 22823 ||| 
2021 ||| ds-net++: dynamic weight slicing for efficient inference in cnns and transformers. ||| 1776 ||| 1778 ||| 1780 ||| 1686 ||| 39393 ||| 1781 ||| 
2022 ||| scala: accelerating adaptation of pre-trained transformer-based language models via efficient large-batch adversarial noise. ||| 9224 ||| 39394 ||| 9225 ||| 
2020 ||| when can self-attention be replaced by feed forward layers? ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2021 ||| local memory attention for fast video semantic segmentation. ||| 25545 ||| 25546 ||| 7814 ||| 7815 ||| 
2020 ||| efficient content-based sparse attention with routing transformers. ||| 39395 ||| 39396 ||| 2466 ||| 39397 ||| 
2021 ||| delving deep into the generalization of vision transformers under distribution shifts. ||| 39398 ||| 7266 ||| 18005 ||| 39399 ||| 7965 ||| 35751 ||| 7267 ||| 1944 ||| 17695 ||| 2498 ||| 
2019 ||| graph convolutional transformer: learning the graphical structure of electronic health records. ||| 18102 ||| 3212 ||| 231 ||| 39400 ||| 18104 ||| 27628 ||| 18106 ||| 
2021 ||| ear-net: error attention refining network for retinal vessel segmentation. ||| 1224 ||| 11311 ||| 11312 ||| 
2021 ||| test-time personalization with a transformer for human pose estimation. ||| 39401 ||| 17957 ||| 39402 ||| 39403 ||| 1117 ||| 
2021 ||| extracting the locus of attention at a cocktail party from single-trial eeg using a joint cnn-lstm model. ||| 24386 ||| 39404 ||| 24387 ||| 24388 ||| 
2020 ||| balancing cost and benefit with tied-multi transformers. ||| 14704 ||| 17542 ||| 17541 ||| 
2021 ||| recurrent attention models with object-centric capsule representation for multi-object recognition. ||| 18709 ||| 19325 ||| 18710 ||| 
2020 ||| tan-ntm: topic attention networks for neural topic modeling. ||| 3234 ||| 3235 ||| 3236 ||| 3237 ||| 
2020 ||| attention-based lstm network for covid-19 clinical trial parsing. ||| 17175 ||| 17176 ||| 17177 ||| 17178 ||| 
2021 ||| improved robustness of vision transformer via prelayernorm in patch embedding. ||| 39405 ||| 39406 ||| 39407 ||| 39408 ||| 39409 ||| 39410 ||| 
2019 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| 6563 ||| 6564 ||| 6565 ||| 6566 ||| 6567 ||| 
2021 ||| gacan: graph attention-convolution-attention networks for traffic forecasting based on multi-granularity time series. ||| 39411 ||| 30276 ||| 39412 ||| 28175 ||| 25997 ||| 2754 ||| 
2021 ||| hybrid self-attention neat: a novel evolutionary approach to improve the neat algorithm. ||| 39413 ||| 39414 ||| 
2017 ||| attention transfer from web images for video recognition. ||| 19472 ||| 19473 ||| 1872 ||| 19019 ||| 
2020 ||| interpretable crowd flow prediction with spatial-temporal self-attention. ||| 25372 ||| 1438 ||| 25374 ||| 5209 ||| 
2022 ||| vit-hgr: vision transformer-based hand gesture recognition from high density surface emg signals. ||| 39415 ||| 34775 ||| 34774 ||| 33528 ||| 39416 ||| 
2021 ||| attention based sequence to sequence learning for machine translation of low resourced indic languages - a case of sanskrit to hindi. ||| 39417 ||| 39418 ||| 
2021 ||| transformer-based multi-task learning for disaster tweet categorisation. ||| 39419 ||| 39420 ||| 39421 ||| 
2020 ||| multi-source attention for unsupervised domain adaptation. ||| 14951 ||| 14952 ||| 
2020 ||| the ioa system for deep noise suppression challenge using a framework combining dynamic attention and recursive learning. ||| 14674 ||| 4384 ||| 14676 ||| 39422 ||| 14677 ||| 
2018 ||| textually guided ranking network for attentional image retweet modeling. ||| 1306 ||| 39423 ||| 23454 ||| 7652 ||| 1754 ||| 1081 ||| 2258 ||| 1115 ||| 
2020 ||| mba-raingan: multi-branch attention generative adversarial network for mixture of rain removal from single images. ||| 39424 ||| 39425 ||| 39426 ||| 11467 ||| 8636 ||| 24819 ||| 39427 ||| 
2021 ||| crash report data analysis for creating scenario-wise, spatio-temporal attention guidance to support computer vision-based perception of fatal crash risks. ||| 11641 ||| 36922 ||| 24983 ||| 
2021 ||| contnet: why not use convolution and transformer at the same time? ||| 39326 ||| 9779 ||| 27531 ||| 8727 ||| 10974 ||| 985 ||| 
2022 ||| metamorph: learning universal controllers with transformers. ||| 39428 ||| 39429 ||| 39430 ||| 19179 ||| 
2021 ||| meningioma segmentation in t1-weighted mri leveraging global context and attention mechanisms. ||| 39431 ||| 3369 ||| 39432 ||| 39433 ||| 39434 ||| 39435 ||| 
2022 ||| a hybrid 2-stage vision transformer for ai-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies. ||| 31259 ||| 39436 ||| 39437 ||| 39438 ||| 2048 ||| 
2018 ||| sparse and constrained attention for neural machine translation. ||| 3538 ||| 3540 ||| 3369 ||| 3370 ||| 
2021 ||| grid partitioned attention: efficient transformerapproximation with inductive bias for high resolution detail generation. ||| 39439 ||| 2101 ||| 39440 ||| 39441 ||| 39442 ||| 
2021 ||| a2p-mann: adaptive attention inference hops pruned memory-augmented neural networks. ||| 39443 ||| 39444 ||| 39445 ||| 7329 ||| 
2019 ||| dianet: bert and hierarchical attention multi-task learning of fine-grained dialect. ||| 3152 ||| 10441 ||| 3153 ||| 10442 ||| 39446 ||| 
2022 ||| monodetr: depth-aware transformer for monocular 3d object detection. ||| 39447 ||| 39448 ||| 39449 ||| 39450 ||| 6042 ||| 2149 ||| 2170 ||| 1848 ||| 
2021 ||| nlpbk at vlsp-2020 shared task: compose transformer pretrained models for reliable intelligence identification on social network. ||| 39451 ||| 39452 ||| 
2019 ||| residual attention based network for hand bone age assessment. ||| 15552 ||| 15553 ||| 398 ||| 15554 ||| 9891 ||| 6810 ||| 15555 ||| 15556 ||| 14552 ||| 15557 ||| 15558 ||| 
2021 ||| the stem cell hypothesis: dilemma behind multi-task learning with transformer encoders. ||| 1593 ||| 375 ||| 
2021 ||| efficient softmax approximation for deep neural networks with attention mechanism. ||| 39453 ||| 39454 ||| 
2020 ||| improving sequence tagging for vietnamese text using transformer-based neural models. ||| 16018 ||| 22627 ||| 16020 ||| 
2021 ||| sea ice forecasting using attention-based ensemble lstm. ||| 39455 ||| 39456 ||| 17578 ||| 39457 ||| 
2022 ||| what to hide from your students: attention-guided masked image modeling. ||| 39458 ||| 36329 ||| 39459 ||| 7435 ||| 36330 ||| 39460 ||| 23997 ||| 
2021 ||| real-time prediction for mechanical ventilation in covid-19 patients using a multi-task gaussian process multi-objective self-attention network. ||| 3433 ||| 39461 ||| 39462 ||| 39463 ||| 39464 ||| 
2020 ||| mhsa-net: multi-head self-attention network for occluded person re-identification. ||| 28876 ||| 28877 ||| 39465 ||| 621 ||| 633 ||| 
2020 ||| text-to-image generation grounded by fine-grained user attention. ||| 7427 ||| 7428 ||| 7429 ||| 3786 ||| 
2021 ||| cycletransgan-evc: a cyclegan-based emotional voice conversion model with transformer. ||| 20470 ||| 39466 ||| 14590 ||| 20474 ||| 
2020 ||| end-to-end contextual perception and prediction with interaction transformer. ||| 25593 ||| 10875 ||| 21776 ||| 21775 ||| 9220 ||| 25594 ||| 9239 ||| 
2019 ||| what does bert look at? an analysis of bert's attention. ||| 20965 ||| 20966 ||| 3348 ||| 20967 ||| 
2019 ||| bidirectional context-aware hierarchical attention network for document understanding. ||| 39467 ||| 39468 ||| 18289 ||| 
2020 ||| reinforced medical report generation with x-linear attention and repetition penalty. ||| 39469 ||| 16727 ||| 16726 ||| 33207 ||| 
2020 ||| character region attention for text spotting. ||| 8508 ||| 8509 ||| 8510 ||| 8511 ||| 8512 ||| 8513 ||| 8514 ||| 
2020 ||| conditional set generation with transformers. ||| 22801 ||| 22835 ||| 2106 ||| 
2020 ||| image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. ||| 19270 ||| 19271 ||| 19272 ||| 2220 ||| 17653 ||| 33290 ||| 
2021 ||| keyword transformer: a self-attention model for keyword spotting. ||| 14249 ||| 14250 ||| 14251 ||| 
2020 ||| toward tag-free aspect based sentiment analysis: a multiple attention network approach. ||| 632 ||| 633 ||| 634 ||| 
2022 ||| maskgit: masked generative image transformer. ||| 34024 ||| 14048 ||| 18703 ||| 34025 ||| 1792 ||| 
2021 ||| gaussian kernelized self-attention for long sequence data and its application to ctc-based speech recognition. ||| 12364 ||| 12365 ||| 3549 ||| 
2019 ||| on recognizing texts of arbitrary shapes with 2d self-attention. ||| 8512 ||| 8511 ||| 8510 ||| 2091 ||| 19212 ||| 8514 ||| 
2022 ||| mutual generative transformer learning for cross-view geo-localization. ||| 39470 ||| 1857 ||| 1858 ||| 1860 ||| 
2021 ||| open-domain conversational search assistant with transformers. ||| 15115 ||| 15116 ||| 15117 ||| 1994 ||| 15118 ||| 227 ||| 
2020 ||| feature importance estimation with self-attention networks. ||| 10213 ||| 10214 ||| 10215 ||| 10216 ||| 
2021 ||| improve vision transformers training by suppressing over-smoothing. ||| 39471 ||| 32783 ||| 5860 ||| 34451 ||| 1073 ||| 
2020 ||| nestfuse: an infrared and visible image fusion architecture based on nest connection and spatial/channel attention models. ||| 4175 ||| 7853 ||| 28433 ||| 
2021 ||| biomedical data-to-text generation via fine-tuning transformers. ||| 10278 ||| 10279 ||| 10280 ||| 
2021 ||| thinking fast and slow: efficient text-to-visual retrieval with transformers. ||| 18775 ||| 1993 ||| 2174 ||| 18776 ||| 1997 ||| 
2020 ||| efficient folded attention for 3d medical image reconstruction and segmentation. ||| 17750 ||| 17751 ||| 17752 ||| 17753 ||| 17754 ||| 17755 ||| 9149 ||| 
2018 ||| ca3net: contextual-attentional attribute-appearance network for person re-identification. ||| 9614 ||| 8710 ||| 18071 ||| 2207 ||| 17860 ||| 
2021 ||| pose-guided feature disentangling for occluded person re-identification based on transformer. ||| 128 ||| 2519 ||| 39472 ||| 19363 ||| 24999 ||| 
2021 ||| focus attention: promoting faithfulness and diversity in summarization. ||| 3490 ||| 3404 ||| 3491 ||| 3492 ||| 3493 ||| 
2021 ||| local-to-global self-attention in vision transformers. ||| 984 ||| 23472 ||| 32220 ||| 8532 ||| 1932 ||| 
2021 ||| clustering and attention model based for intelligent trading. ||| 39473 ||| 39474 ||| 39475 ||| 39476 ||| 39477 ||| 402 ||| 
2022 ||| docsegtr: an instance-level end-to-end document image segmentation transformer. ||| 37810 ||| 39478 ||| 17349 ||| 7111 ||| 30717 ||| 
2019 ||| improving neural machine translation with parent-scaled self-attention. ||| 3089 ||| 3090 ||| 
2018 ||| on the importance of attention in meta-learning for few-shot text classification. ||| 5391 ||| 39479 ||| 39480 ||| 39481 ||| 39482 ||| 39483 ||| 35417 ||| 26979 ||| 
2019 ||| understanding multi-head attention in abstractive summarization. ||| 38494 ||| 38495 ||| 38496 ||| 38497 ||| 1048 ||| 
2021 ||| using knowledge-embedded attention to augment pre-trained language models for fine-grained emotion recognition. ||| 22507 ||| 20976 ||| 
2020 ||| example-guided image synthesis across arbitrary scenes using masked spatial-channel attention and self-supervision. ||| 8519 ||| 2070 ||| 8520 ||| 6579 ||| 2417 ||| 2166 ||| 
2022 ||| exploiting spatial sparsity for event cameras with visual transformers. ||| 39484 ||| 3595 ||| 830 ||| 
2021 ||| styleswin: transformer-based gan for high-resolution image generation. ||| 7468 ||| 39485 ||| 1099 ||| 23783 ||| 6604 ||| 5909 ||| 16446 ||| 1772 ||| 
2021 ||| paying attention to activation maps in camera pose regression. ||| 2260 ||| 2261 ||| 2262 ||| 
2018 ||| looking beyond a clever narrative: visual context and attention are primary drivers of affect in video advertisements. ||| 26892 ||| 26893 ||| 19019 ||| 26894 ||| 
2022 ||| torchmd-net: equivariant transformers for neural network based molecular potentials. ||| 39486 ||| 39487 ||| 39488 ||| 
2020 ||| 365 dots in 2019: quantifying attention of news sources. ||| 39489 ||| 39490 ||| 39491 ||| 
2021 ||| layer-wise pruning of transformer attention heads for efficient language modeling. ||| 1611 ||| 1612 ||| 1613 ||| 1614 ||| 
2019 ||| a self validation network for object-level human attention estimation. ||| 9383 ||| 4997 ||| 9384 ||| 
2019 ||| attention on attention for image captioning. ||| 2199 ||| 2200 ||| 1037 ||| 2201 ||| 
2020 ||| wireless image transmission using deep source channel coding with attention modules. ||| 39492 ||| 39493 ||| 5110 ||| 15876 ||| 15877 ||| 
2021 ||| learning attributed graph representations with communicative message passing transformer. ||| 16737 ||| 16736 ||| 23502 ||| 23503 ||| 16595 ||| 
2022 ||| towards automatic transcription of polyphonic electric guitar music: a new dataset and a multi-loss transformer model. ||| 11918 ||| 11919 ||| 39494 ||| 12466 ||| 4374 ||| 
2022 ||| unsupervised anomaly detection in medical images with a memory-augmented multi-level cross-attentional masked autoencoder. ||| 36631 ||| 36632 ||| 36634 ||| 31736 ||| 36635 ||| 36633 ||| 39495 ||| 15389 ||| 18877 ||| 
2021 ||| on automatic text extractive summarization based on graph and pre-trained language model attention. ||| 39496 ||| 19887 ||| 
2020 ||| danhar: dual attention network for multimodal human activity recognition using wearable sensors. ||| 29250 ||| 241 ||| 29251 ||| 3890 ||| 39497 ||| 532 ||| 
2021 ||| dual attention network for heart rate and respiratory rate estimation. ||| 24880 ||| 24881 ||| 24882 ||| 
2021 ||| an attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery. ||| 39498 ||| 10768 ||| 30239 ||| 8849 ||| 38700 ||| 30240 ||| 39499 ||| 39500 ||| 
2021 ||| vehicle trajectory prediction in city-scale road networks using a direction-based sequence-to-sequence model with spatiotemporal attention mechanisms. ||| 39501 ||| 39502 ||| 
2020 ||| cross-modal food retrieval: learning a joint embedding of food images and recipes with semantic consistency and attention mechanism. ||| 1371 ||| 3301 ||| 25362 ||| 39503 ||| 39504 ||| 24967 ||| 3303 ||| 
2022 ||| reward modeling for mitigating toxicity in transformer-based language models. ||| 679 ||| 681 ||| 680 ||| 
2021 ||| contextual similarity aggregation with self-attention for visual re-ranking. ||| 39505 ||| 1805 ||| 214 ||| 1806 ||| 1807 ||| 
2020 ||| streaming transformer asr with blockwise synchronous inference. ||| 12365 ||| 12364 ||| 3549 ||| 
2021 ||| combining attention with flow for person image synthesis. ||| 19529 ||| 19530 ||| 12218 ||| 2063 ||| 2064 ||| 
2020 ||| intellicode compose: code generation using transformer. ||| 15264 ||| 26013 ||| 26014 ||| 15265 ||| 
2018 ||| visual attention for behavioral cloning in autonomous driving. ||| 28042 ||| 28041 ||| 28043 ||| 
2021 ||| last query transformer rnn for knowledge tracing. ||| 39506 ||| 
2021 ||| sa-net: shuffle attention for deep convolutional neural networks. ||| 12508 ||| 5865 ||| 
2020 ||| realformer: transformer likes residual attention. ||| 3554 ||| 3555 ||| 3556 ||| 3557 ||| 
2022 ||| detecting offensive language on social networks: an end-to-end detection method based on graph attention networks. ||| 39507 ||| 39508 ||| 39509 ||| 39510 ||| 36219 ||| 39511 ||| 
2020 ||| scatter: selective context attentional scene text recognizer. ||| 19160 ||| 19161 ||| 19162 ||| 19163 ||| 19164 ||| 2644 ||| 
2020 ||| toward understanding the conditions that promote higher attention in software developments - a first step on music and standups. ||| 26008 ||| 39512 ||| 39513 ||| 39514 ||| 28083 ||| 
2019 ||| ffa-net: feature fusion attention network for single image dehazing. ||| 18249 ||| 18250 ||| 18251 ||| 11405 ||| 11404 ||| 
2022 ||| towards unsupervised domain adaptation via domain-transformer. ||| 39515 ||| 39516 ||| 39517 ||| 39518 ||| 
2021 ||| avgcn: trajectory prediction using graph convolutional networks guided by human attention. ||| 21804 ||| 1303 ||| 124 ||| 5407 ||| 
2018 ||| temporal pattern attention for multivariate time series forecasting. ||| 39519 ||| 39520 ||| 12644 ||| 
2020 ||| unsupervised speaker adaptation using attention-based speaker memory for end-to-end asr. ||| 12396 ||| 11980 ||| 2508 ||| 11981 ||| 
2020 ||| a novel multimodal music genre classifier using hierarchical attention and convolutional neural network. ||| 39521 ||| 10556 ||| 
2019 ||| pose guided attention for multi-label fashion image classification. ||| 7919 ||| 1994 ||| 7920 ||| 7921 ||| 7922 ||| 1994 ||| 7923 ||| 
2017 ||| modeling image virality with pairwise spatial transformer networks. ||| 19598 ||| 19599 ||| 
2020 ||| cascade network with guided loss and hybrid attention for two-view geometry. ||| 3148 ||| 1856 ||| 17668 ||| 
2020 ||| urban crowdsensing using social media: an empirical study on transformer and recurrent neural networks. ||| 17227 ||| 17228 ||| 15208 ||| 
2021 ||| bgt-net: bidirectional gru transformer network for scene graph generation. ||| 5715 ||| 19211 ||| 13618 ||| 
2021 ||| a multi-branch hybrid transformer networkfor corneal endothelial cell segmentation. ||| 27975 ||| 5203 ||| 4056 ||| 15560 ||| 1420 ||| 27976 ||| 12196 ||| 5206 ||| 
2019 ||| pyramnet: point cloud pyramid attention network and graph embedding module for classification and segmentation. ||| 39522 ||| 7015 ||| 
2020 ||| attentron: few-shot text-to-speech utilizing attention-based variable-length embedding. ||| 14586 ||| 14587 ||| 14588 ||| 14589 ||| 
2021 ||| text2gestures: a transformer-based network for generating emotive body gestures for virtual agents. ||| 8818 ||| 21046 ||| 21047 ||| 21048 ||| 8823 ||| 7315 ||| 
2021 ||| styleformer: transformer based generative adversarial networks with style vector. ||| 39523 ||| 39524 ||| 
2022 ||| hipa: hierarchical patch transformer for single image super resolution. ||| 39525 ||| 33154 ||| 31046 ||| 34858 ||| 39526 ||| 8711 ||| 31047 ||| 
2022 ||| an ensemble approach to acronym extraction using transformers. ||| 39527 ||| 39528 ||| 39529 ||| 39530 ||| 3850 ||| 
2020 ||| iart: intent-aware response ranking with transformers in information-seeking conversation systems. ||| 1041 ||| 1240 ||| 8986 ||| 1242 ||| 3738 ||| 3035 ||| 1140 ||| 8987 ||| 
2020 ||| hybrid attention networks for flow and pressure forecasting in water distribution systems. ||| 33854 ||| 39531 ||| 39532 ||| 39533 ||| 
2021 ||| escaping the big data paradigm with compact transformers. ||| 39534 ||| 36774 ||| 39535 ||| 39536 ||| 2269 ||| 33290 ||| 
2020 ||| channel pruning guided by spatial and channel attention for dnns in intelligent edge computing. ||| 29264 ||| 29265 ||| 29266 ||| 29267 ||| 29202 ||| 24193 ||| 
2020 ||| relational learning between multiple pulmonary nodules via deep set attention transformers. ||| 15535 ||| 15536 ||| 15537 ||| 8832 ||| 15538 ||| 
2018 ||| topic-guided attention for image captioning. ||| 2958 ||| 6350 ||| 6351 ||| 
2021 ||| cptr: full transformer network for image captioning. ||| 683 ||| 39537 ||| 19005 ||| 11392 ||| 2058 ||| 
2020 ||| two-level transformer and auxiliary coherence modeling for improved text segmentation. ||| 15105 ||| 18216 ||| 
2019 ||| global transformer u-nets for label-free prediction of fluorescence images. ||| 1199 ||| 6928 ||| 25412 ||| 23491 ||| 
2019 ||| free-lunch saliency via attention in atari agents. ||| 7877 ||| 7878 ||| 7879 ||| 7880 ||| 
2021 ||| dancing along battery: enabling transformer with run-time reconfigurability on mobile devices. ||| 9873 ||| 13660 ||| 9874 ||| 9867 ||| 9869 ||| 9868 ||| 20519 ||| 20520 ||| 11024 ||| 
2022 ||| attention-effective multiple instance learning on weakly stem cell colony segmentation. ||| 4318 ||| 36449 ||| 28995 ||| 36450 ||| 
2021 ||| symbolicgpt: a generative transformer model for symbolic regression. ||| 39538 ||| 39539 ||| 39540 ||| 26322 ||| 
2018 ||| fine-grained video categorization with redundancy reduction attention. ||| 1835 ||| 8723 ||| 6924 ||| 2530 ||| 8724 ||| 1761 ||| 1839 ||| 
2019 ||| why deep transformers are difficult to converge? from computation order to lipschitz restricted parameter initialization. ||| 8 ||| 3260 ||| 3207 ||| 3206 ||| 
2019 ||| towards universal object detection by domain attention. ||| 12824 ||| 18676 ||| 18677 ||| 18678 ||| 
2021 ||| deep attentional guided image filtering. ||| 12055 ||| 12058 ||| 12056 ||| 32538 ||| 32540 ||| 
2021 ||| compositional attention: disentangling search and retrieval. ||| 22718 ||| 39541 ||| 39542 ||| 9196 ||| 9197 ||| 
2019 ||| multi-layer attention mechanism for speech keyword recognition. ||| 35177 ||| 39543 ||| 5187 ||| 39544 ||| 35178 ||| 5384 ||| 35179 ||| 39545 ||| 
2021 ||| multi-pretext attention network for few-shot learning with self-supervision. ||| 19913 ||| 19785 ||| 5536 ||| 19914 ||| 19915 ||| 2039 ||| 17695 ||| 
2022 ||| on the efficacy of co-attention transformer layers in visual question answering. ||| 39546 ||| 39547 ||| 
2021 ||| improving ultrasound tongue image reconstruction from lip images using self-supervised learning and attention mechanism. ||| 35086 ||| 39548 ||| 
2019 ||| deep multi-kernel convolutional lstm networks and an attention-based mechanism for videos. ||| 39549 ||| 1387 ||| 
2021 ||| better pay attention whilst fuzzing. ||| 39550 ||| 1072 ||| 3879 ||| 1132 ||| 39551 ||| 39552 ||| 15367 ||| 
2022 ||| patch-based stochastic attention for image editing. ||| 39553 ||| 3369 ||| 39554 ||| 1670 ||| 1669 ||| 
2021 ||| graph joint attention networks. ||| 39555 ||| 13468 ||| 33170 ||| 
2019 ||| construct dynamic graphs for hand gesture recognition via spatial-temporal attention. ||| 8946 ||| 21442 ||| 9187 ||| 1036 ||| 1749 ||| 
2019 ||| motion guided attention for video salient object detection. ||| 2112 ||| 2113 ||| 1800 ||| 1801 ||| 
2020 ||| coarse- and fine-grained attention network with background-aware loss for crowd density map estimation. ||| 7325 ||| 7326 ||| 
2019 ||| learning multi-level information for dialogue response selection by highway recurrent transformer. ||| 30131 ||| 13952 ||| 4841 ||| 4843 ||| 
2019 ||| a synchronized multi-modal attention-caption dataset and analysis. ||| 2195 ||| 1852 ||| 1854 ||| 2603 ||| 
2020 ||| joint left atrial segmentation and scar quantification based on a dnn with spatial encoding and shape attention. ||| 3034 ||| 27812 ||| 27421 ||| 20755 ||| 
2021 ||| exploring transformers in emotion recognition: a comparison of bert, distillbert, roberta, xlnet and electra. ||| 39556 ||| 
2021 ||| power transformer faults diagnosis using undestructive methods (roger and iec) and artificial neural network for dissolved gas analysis applied on the functional transformer in the algerian north-eastern: a comparative study. ||| 39557 ||| 39558 ||| 39559 ||| 39560 ||| 
2021 ||| towards deep and efficient: a deep siamese self-attention fully efficient convolutional network for change detection in vhr images. ||| 35621 ||| 3668 ||| 17757 ||| 
2022 ||| transvpr: transformer-based place recognition with multi-level attention aggregation. ||| 39561 ||| 39562 ||| 24396 ||| 2354 ||| 14779 ||| 
2021 ||| ibert: idiom cloze-style reading comprehension with attention. ||| 39563 ||| 32847 ||| 39564 ||| 39565 ||| 
2020 ||| a two-stage cascade model with variational autoencoders and attention gates for mri brain tumor segmentation. ||| 24681 ||| 24682 ||| 
2021 ||| wakavt: a sequential variational transformer for waka generation. ||| 39566 ||| 39567 ||| 32125 ||| 2821 ||| 7854 ||| 
2020 ||| spectral pyramid graph attention network for hyperspectral image classification. ||| 8023 ||| 28417 ||| 39568 ||| 39569 ||| 
2020 ||| cort: complementary rankings from transformers. ||| 4927 ||| 4928 ||| 
2022 ||| iwin: human-object interaction detection via transformer with irregular windows. ||| 34175 ||| 11343 ||| 34176 ||| 2323 ||| 6516 ||| 8906 ||| 
2019 ||| fixed pattern noise reduction for infrared images based on cascade residual attention cnn. ||| 39570 ||| 39571 ||| 39572 ||| 474 ||| 35073 ||| 
2021 ||| enjoy the salience: towards better transformer-based faithful explanations with word salience. ||| 3735 ||| 3736 ||| 
2021 ||| looking beyond two frames: end-to-end multi-object tracking using spatial and temporal transformers. ||| 39573 ||| 39574 ||| 39575 ||| 39576 ||| 39577 ||| 9217 ||| 
2022 ||| contextual attention network: transformer meets u-net. ||| 8804 ||| 39578 ||| 39579 ||| 39580 ||| 
2021 ||| hat: hierarchical aggregation transformers for person re-identification. ||| 19690 ||| 19691 ||| 2038 ||| 1700 ||| 
2019 ||| relational graph attention networks. ||| 39581 ||| 39582 ||| 39583 ||| 37476 ||| 
2020 ||| integrating human gaze into attention for egocentric activity recognition. ||| 7341 ||| 7342 ||| 
2018 ||| detecting visual relationships using box attention. ||| 7979 ||| 39584 ||| 7982 ||| 
2021 ||| morph call: probing morphosyntactic content of multilingual transformers. ||| 26703 ||| 39585 ||| 10339 ||| 
2019 ||| nrpa: neural recommendation with personalized attention. ||| 4166 ||| 3755 ||| 696 ||| 4165 ||| 697 ||| 3754 ||| 9574 ||| 
2020 ||| fine-grained 3d shape classification with hierarchical part-view attentions. ||| 18229 ||| 2563 ||| 2559 ||| 18230 ||| 
2022 ||| learning to merge tokens in vision transformers. ||| 3157 ||| 39586 ||| 3369 ||| 39587 ||| 23913 ||| 39588 ||| 39589 ||| 39590 ||| 
2021 ||| pcam: product of cross-attention matrices for rigid registration of point clouds. ||| 1922 ||| 1923 ||| 1924 ||| 1925 ||| 
2021 ||| xtremedistiltransformers: task transfer for task-agnostic distillation. ||| 34822 ||| 33914 ||| 1958 ||| 
2020 ||| transformer transducer: a streamable speech recognition model with transformer encoders and rnn-t loss. ||| 2251 ||| 12612 ||| 12613 ||| 12614 ||| 12615 ||| 12616 ||| 12617 ||| 
2020 ||| multi-label text classification using attention-based graph neural network. ||| 18374 ||| 20707 ||| 18375 ||| 
2019 ||| indoor depth completion with boundary consistency and self-attention. ||| 7808 ||| 7809 ||| 7810 ||| 1387 ||| 
2020 ||| regression and learning with pixel-wise attention for retinal fundus glaucoma segmentation and detection. ||| 10572 ||| 21569 ||| 
2020 ||| a3t-gcn: attention temporal graph convolutional network for traffic forecasting. ||| 29399 ||| 29400 ||| 6643 ||| 12801 ||| 
2021 ||| poformer: a simple pooling transformer for speaker verification. ||| 34865 ||| 34866 ||| 34864 ||| 14268 ||| 6796 ||| 34867 ||| 
2018 ||| multi-head attention with disagreement regularization. ||| 595 ||| 3041 ||| 3037 ||| 597 ||| 2814 ||| 
2021 ||| hate-alert@dravidianlangtech-eacl2021: ensembling strategies for transformer-based offensive language detection. ||| 39591 ||| 39592 ||| 39593 ||| 34121 ||| 39594 ||| 
2020 ||| peking opera synthesis via duration informed attention network. ||| 13512 ||| 13516 ||| 12189 ||| 12572 ||| 12586 ||| 14271 ||| 3808 ||| 
2021 ||| categorical difference and related brain regions of the attentional blink effect. ||| 39595 ||| 39596 ||| 
2020 ||| a transformer-based approach for source code summarization. ||| 3238 ||| 3445 ||| 3446 ||| 3033 ||| 
2018 ||| attacks on state-of-the-art face recognition using attentional adversarial attack generative network. ||| 31718 ||| 31719 ||| 12169 ||| 
2022 ||| bba-net: a bi-branch attention network for crowd counting. ||| 12212 ||| 12213 ||| 1856 ||| 2827 ||| 12214 ||| 8177 ||| 11404 ||| 11405 ||| 
2021 ||| diformer: directional transformer for neural machine translation. ||| 39597 ||| 39598 ||| 39599 ||| 39600 ||| 39601 ||| 4639 ||| 39602 ||| 39603 ||| 1254 ||| 39604 ||| 2792 ||| 
2018 ||| finding a needle in the haystack: attention-based classification of high resolution microscopy images. ||| 39605 ||| 39606 ||| 26537 ||| 39607 ||| 34516 ||| 34517 ||| 
2018 ||| tell me where to look: guided attention inference network. ||| 2232 ||| 1744 ||| 1746 ||| 19330 ||| 1734 ||| 
2022 ||| holistic attention-fusion adversarial network for single image defogging. ||| 683 ||| 10980 ||| 18443 ||| 19756 ||| 12187 ||| 
2022 ||| tervit: an efficient ternary vision transformer. ||| 27602 ||| 39608 ||| 32917 ||| 39609 ||| 7240 ||| 2170 ||| 39610 ||| 
2022 ||| self-supervised transformers for unsupervised object discovery using normalized cut. ||| 39611 ||| 23915 ||| 14372 ||| 6650 ||| 27049 ||| 27050 ||| 
2020 ||| residual attention u-net for automated multi-class segmentation of covid-19 chest ct images. ||| 24840 ||| 771 ||| 9472 ||| 
2021 ||| attention guided cosine margin for overcoming class-imbalance in few-shot road object detection. ||| 7451 ||| 7452 ||| 7453 ||| 7454 ||| 
2020 ||| an empirical investigation of pre-trained transformer language models for open-domain dialogue generation. ||| 23337 ||| 
2022 ||| flow-guided sparse transformer for video deblurring. ||| 21242 ||| 19804 ||| 19802 ||| 19803 ||| 33187 ||| 39612 ||| 2266 ||| 1730 ||| 7815 ||| 7814 ||| 
2021 ||| global attention mechanism: retain information to enhance channel-spatial interactions. ||| 33333 ||| 14333 ||| 33335 ||| 
2021 ||| mag-net: mutli-task attention guided network for brain tumor segmentation and classification. ||| 12820 ||| 12821 ||| 12822 ||| 12823 ||| 
2021 ||| transformers in the loop: polarity in neural models of language. ||| 39613 ||| 3430 ||| 
2021 ||| automated essay scoring using efficient transformer-based language models. ||| 39614 ||| 39615 ||| 39616 ||| 
2021 ||| graph attention network based single-pixel compressive direction of arrival estimation. ||| 13478 ||| 13479 ||| 39617 ||| 2101 ||| 13480 ||| 
2021 ||| fuseformer: fusing fine-grained information in transformers for video inpainting. ||| 1840 ||| 1841 ||| 1842 ||| 1843 ||| 1844 ||| 1845 ||| 1846 ||| 1847 ||| 1848 ||| 
2019 ||| an attention-based speaker naming method for online adaptation in non-fixed scenarios. ||| 39618 ||| 39619 ||| 39620 ||| 39621 ||| 39622 ||| 
2019 ||| spatial-aware non-local attention for fashion landmark detection. ||| 13786 ||| 19885 ||| 19886 ||| 19887 ||| 
2021 ||| light field image super-resolution with transformers. ||| 39623 ||| 19139 ||| 19138 ||| 19308 ||| 23020 ||| 
2021 ||| end-to-end information extraction by character-level embedding and multi-stage attentional u-net. ||| 21501 ||| 39624 ||| 
2021 ||| dam-al: dilated attention mechanism with attention loss for 3d infant brain image segmentation. ||| 39625 ||| 27580 ||| 4033 ||| 39626 ||| 
2021 ||| multi-fold correlation attention network for predicting traffic speeds with heterogeneous frequency. ||| 9757 ||| 9758 ||| 9759 ||| 9761 ||| 39627 ||| 
2021 ||| graph pattern loss based diversified attention network for cross-modal retrieval. ||| 11239 ||| 6465 ||| 6850 ||| 
2019 ||| triplenet: triple attention network for multi-turn response selection in retrieval-based chatbots. ||| 17877 ||| 3642 ||| 23091 ||| 23092 ||| 1219 ||| 3311 ||| 1308 ||| 3645 ||| 
2019 ||| looking for the devil in the details: learning trilinear attention sampling network for fine-grained image recognition. ||| 2164 ||| 1699 ||| 8710 ||| 2166 ||| 
2021 ||| d-han: dynamic news recommendation with hierarchical attention network. ||| 39628 ||| 9645 ||| 4600 ||| 777 ||| 
2018 ||| ra-unet: a hybrid deep attention-aware network to extract liver and tumor in ct scans. ||| 27644 ||| 39629 ||| 18753 ||| 39630 ||| 39631 ||| 
2017 ||| detection and attention: diagnosing pulmonary lung cancer from ct by imitating physicians. ||| 7015 ||| 39632 ||| 39633 ||| 5819 ||| 39634 ||| 39635 ||| 8430 ||| 
2021 ||| transmorph: transformer for unsupervised medical image registration. ||| 39636 ||| 39637 ||| 39638 ||| 39639 ||| 11494 ||| 39640 ||| 
2022 ||| three things everyone should know about vision transformers. ||| 1887 ||| 2118 ||| 1886 ||| 2353 ||| 1890 ||| 1891 ||| 1892 ||| 
2021 ||| novelty detection and analysis of traffic scenario infrastructures in the latent space of a vision transformer-based triplet autoencoder. ||| 15470 ||| 15471 ||| 15472 ||| 15473 ||| 
2020 ||| graph attention network based pruning for reconstructing 3d liver vessel morphology from contrasted ct images. ||| 29189 ||| 17302 ||| 39641 ||| 39642 ||| 37004 ||| 39643 ||| 32207 ||| 39644 ||| 
2021 ||| anomaly transformer: time series anomaly detection with association discrepancy. ||| 32464 ||| 32463 ||| 17636 ||| 17635 ||| 
2019 ||| toward imitating visual attention of experts in software development tasks. ||| 4424 ||| 23717 ||| 4425 ||| 4421 ||| 4426 ||| 
2019 ||| information aggregation for multi-head attention with routing-by-agreement. ||| 595 ||| 3037 ||| 4981 ||| 3309 ||| 597 ||| 3041 ||| 
2020 ||| auto completion of user interface layout design using transformer-based tree decoders. ||| 438 ||| 39645 ||| 29033 ||| 22843 ||| 22844 ||| 
2021 ||| improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning. ||| 39646 ||| 39647 ||| 39648 ||| 
2021 ||| mass: multi-attentional semantic segmentation of lidar data for dense top-view understanding. ||| 7859 ||| 39649 ||| 7857 ||| 23614 ||| 7856 ||| 39650 ||| 39651 ||| 39652 ||| 7861 ||| 
2021 ||| axm-net: cross-modal context sharing attention network for person re-id. ||| 34467 ||| 8008 ||| 6525 ||| 39653 ||| 
2017 ||| attention-based models for text-dependent speaker verification. ||| 12651 ||| 12652 ||| 12653 ||| 12654 ||| 
2020 ||| meta graph attention on heterogeneous graph with node-edge co-evolution. ||| 17880 ||| 17879 ||| 17881 ||| 39654 ||| 39655 ||| 12749 ||| 
2019 ||| exploring reciprocal attention for salient object detection by cooperative learning. ||| 39656 ||| 2383 ||| 39657 ||| 28819 ||| 
2018 ||| collective attention towards scientists and research topics. ||| 15796 ||| 15797 ||| 15798 ||| 15799 ||| 
2021 ||| improved xception with dual attention mechanism and feature fusion for face forgery detection. ||| 39658 ||| 39659 ||| 39660 ||| 39661 ||| 
2020 ||| image-level harmonization of multi-site data using image-and-spatial transformer networks. ||| 27840 ||| 27446 ||| 27841 ||| 27842 ||| 27843 ||| 14912 ||| 27420 ||| 39662 ||| 
2022 ||| a deep neural framework for image caption generation using gru-based attention mechanism. ||| 39663 ||| 39664 ||| 39665 ||| 39666 ||| 39667 ||| 4438 ||| 
2021 ||| contrastive attention for automatic chest x-ray report generation. ||| 3746 ||| 3747 ||| 3748 ||| 3749 ||| 3750 ||| 3751 ||| 
2021 ||| pose-driven attention-guided image generation for person re-identification. ||| 33861 ||| 11374 ||| 11330 ||| 11331 ||| 
2017 ||| learning to detect chest radiographs containing lung nodules using visual attention networks. ||| 31231 ||| 31233 ||| 31232 ||| 31234 ||| 31235 ||| 31236 ||| 
2017 ||| learning an attention model in an artificial visual system. ||| 39668 ||| 39669 ||| 39670 ||| 
2019 ||| improving deep image clustering with spatial transformer layers. ||| 4173 ||| 282 ||| 
2021 ||| generalized wasserstein dice loss, test-time augmentation, and transformers for the brats 2021 challenge. ||| 39671 ||| 39672 ||| 39673 ||| 39674 ||| 7111 ||| 14874 ||| 27462 ||| 
2020 ||| text classification with lexicon from preattention mechanism. ||| 35093 ||| 36193 ||| 36194 ||| 
2019 ||| cross-modality attention with semantic graph embedding for multi-label classification. ||| 18192 ||| 18193 ||| 9837 ||| 18016 ||| 18194 ||| 2531 ||| 
2019 ||| entropy-enhanced multimodal attention model for scene-aware dialogue generation. ||| 33283 ||| 33284 ||| 4843 ||| 28053 ||| 
2021 ||| parallel attention network with sequence matching for video grounding. ||| 3386 ||| 1397 ||| 3387 ||| 3388 ||| 3389 ||| 3390 ||| 
2021 ||| video background music generation with controllable music transformer. ||| 19403 ||| 19404 ||| 17854 ||| 17852 ||| 19405 ||| 19406 ||| 19407 ||| 1728 ||| 
2020 ||| poor man's bert: smaller and faster transformer models. ||| 26436 ||| 26435 ||| 26437 ||| 7049 ||| 
2021 ||| understanding and improving robustness of vision transformers through patch-based negative augmentation. ||| 23521 ||| 32258 ||| 10333 ||| 39675 ||| 39676 ||| 30208 ||| 
2021 ||| crossmap transformer: a crossmodal masked path transformer using double back-translation for vision-and-language navigation. ||| 22285 ||| 726 ||| 12308 ||| 
2019 ||| self-attention based molecule representation for predicting drug-target interaction. ||| 372 ||| 20674 ||| 20675 ||| 20676 ||| 
2019 ||| transformer-based acoustic modeling for hybrid speech recognition. ||| 11973 ||| 12488 ||| 12489 ||| 12304 ||| 11979 ||| 12447 ||| 12490 ||| 12303 ||| 5450 ||| 11975 ||| 12449 ||| 12305 ||| 12491 ||| 
2019 ||| a dual path modelwith adaptive attention for vehicle re-identification. ||| 2208 ||| 2209 ||| 2210 ||| 2211 ||| 2212 ||| 2213 ||| 
2020 ||| enhancing monotonic multihead attention for streaming asr. ||| 12682 ||| 12683 ||| 4418 ||| 
2020 ||| res3atn - deep 3d residual attention network for hand gesture recognition in videos. ||| 5715 ||| 13618 ||| 
2021 ||| taming sparsely activated transformer with stochastic experts. ||| 22808 ||| 24050 ||| 4816 ||| 34984 ||| 39677 ||| 4817 ||| 22811 ||| 1958 ||| 
2018 ||| attention as a perspective for learning tempo-invariant audio queries. ||| 11895 ||| 39678 ||| 11898 ||| 
2021 ||| towards physically consistent data-driven weather forecasting: integrating data assimilation with equivariance-preserving deep spatial transformers. ||| 6192 ||| 6193 ||| 6194 ||| 39679 ||| 6195 ||| 
2021 ||| optimizing inference performance of transformers on cpus. ||| 39680 ||| 39681 ||| 
2021 ||| convnets vs. transformers: whose visual representations are more transferable? ||| 7913 ||| 7914 ||| 1799 ||| 1801 ||| 
2017 ||| two-stream collaborative learning with spatial-temporal attention for video classification. ||| 5954 ||| 28789 ||| 28790 ||| 
2020 ||| local contextual attention with hierarchical structure for dialogue act recognition. ||| 18841 ||| 39682 ||| 2449 ||| 25179 ||| 12009 ||| 1087 ||| 
2018 ||| coronary calcium detection using 3d attention identical dual deep network based on weakly supervised learning. ||| 24706 ||| 24707 ||| 24708 ||| 7207 ||| 24709 ||| 24710 ||| 24711 ||| 24712 ||| 7208 ||| 
2017 ||| leveraging the flow of collective attention for computational communication research. ||| 39683 ||| 39684 ||| 39685 ||| 39686 ||| 
2021 ||| gt u-net: a u-net like group transformer network for tooth root segmentation. ||| 27346 ||| 11634 ||| 1224 ||| 27347 ||| 27348 ||| 17296 ||| 27349 ||| 17295 ||| 
2021 ||| knowing when to quit: selective cascaded regression with patch attention for real-time face alignment. ||| 19698 ||| 19699 ||| 19700 ||| 19701 ||| 
2019 ||| water supply prediction based on initialized attention residual network. ||| 39687 ||| 39688 ||| 1072 ||| 
2019 ||| an empirical study of efficient asr rescoring with transformers. ||| 12490 ||| 12727 ||| 
2019 ||| learning soft-attention models for tempo-invariant audio-sheet music retrieval. ||| 11894 ||| 11895 ||| 11896 ||| 11897 ||| 11898 ||| 
2020 ||| vision-based fall event detection in complex background using attention guided bi-directional lstm. ||| 20639 ||| 4754 ||| 39689 ||| 39690 ||| 
2020 ||| memory attentive fusion: external language model integration for transformer-based sequence-to-sequence model. ||| 10274 ||| 4408 ||| 10275 ||| 4407 ||| 10276 ||| 10277 ||| 
2020 ||| generalizing spatial transformers to projective geometry with applications to 2d/3d registration. ||| 27865 ||| 1987 ||| 27866 ||| 27867 ||| 27868 ||| 1991 ||| 1992 ||| 
2021 ||| dynamic memory based attention network for sequential recommendation. ||| 18082 ||| 18083 ||| 18084 ||| 18085 ||| 8916 ||| 1245 ||| 18086 ||| 
2019 ||| band attention convolutional networks for hyperspectral image classification. ||| 39691 ||| 39692 ||| 39693 ||| 
2019 ||| link prediction via graph attention network. ||| 39694 ||| 4176 ||| 39695 ||| 39696 ||| 
2020 ||| social-wagdat: interaction-aware trajectory prediction via wasserstein graph double-attention network. ||| 2269 ||| 2270 ||| 31880 ||| 2272 ||| 
2017 ||| an attention mechanism for answer selection using a combined global and local view. ||| 6229 ||| 6230 ||| 6231 ||| 6232 ||| 6233 ||| 852 ||| 6234 ||| 6235 ||| 6236 ||| 
2021 ||| pathologies in priors and inference for bayesian transformers. ||| 39697 ||| 39698 ||| 33760 ||| 39699 ||| 
2020 ||| layout generation and completion with self-attention. ||| 1945 ||| 1947 ||| 1946 ||| 1948 ||| 1949 ||| 1950 ||| 
2017 ||| jointly trained sequential labeling and classification by sparse attention neural networks. ||| 14418 ||| 14419 ||| 14420 ||| 3251 ||| 3562 ||| 
2020 ||| adding recurrence to pretrained transformers for improved efficiency and context size. ||| 39700 ||| 3275 ||| 3793 ||| 
2021 ||| getam: gradient-weighted element-wise transformer attention map for weakly-supervised semantic segmentation. ||| 34250 ||| 875 ||| 39701 ||| 34255 ||| 2475 ||| 
2020 ||| attentional-gcnn: adaptive pedestrian trajectory prediction towards generic autonomous vehicle use cases. ||| 21777 ||| 21778 ||| 21779 ||| 21780 ||| 21781 ||| 21782 ||| 
2022 ||| attention-based lip audio-visual synthesis for talking face generation in the wild. ||| 34857 ||| 989 ||| 12384 ||| 471 ||| 6488 ||| 
2022 ||| transformers in medical imaging: a survey. ||| 39702 ||| 32177 ||| 35734 ||| 2308 ||| 32792 ||| 1972 ||| 4056 ||| 
2022 ||| lile: look in-depth before looking elsewhere - a dual attention network using transformers for cross-modal information retrieval in histopathology archives. ||| 39703 ||| 27874 ||| 
2020 ||| multi-lead ecg classification via an information-based attention convolutional neural network. ||| 39704 ||| 39705 ||| 39706 ||| 24239 ||| 
2022 ||| modelling the semantics of text in complex document layouts using graph transformer networks. ||| 39707 ||| 39708 ||| 39709 ||| 39710 ||| 39711 ||| 39712 ||| 39713 ||| 39714 ||| 
2021 ||| learning inception attention for image synthesis and image recognition. ||| 39715 ||| 2084 ||| 
2021 ||| detect the interactions that matter in matter: geometric attention for many-body systems. ||| 39716 ||| 39717 ||| 
2021 ||| looking at ctr prediction again: is attention all you need? ||| 7385 ||| 9661 ||| 
2021 ||| stacked hourglass network with a multi-level attention mechanism: where to look for intervertebral disc labeling. ||| 8804 ||| 27385 ||| 27386 ||| 
2019 ||| tener: adapting transformer encoder for named entity recognition. ||| 3816 ||| 39718 ||| 3815 ||| 3272 ||| 
2021 ||| a multi-view framework for bgp anomaly detection via graph attention network. ||| 39719 ||| 39720 ||| 39721 ||| 39722 ||| 3279 ||| 39723 ||| 22488 ||| 
2020 ||| parallel scheduling self-attention mechanism: generalization and optimization. ||| 39724 ||| 39725 ||| 
2020 ||| inserting information bottlenecks for attribution in transformers. ||| 3007 ||| 26646 ||| 17741 ||| 3009 ||| 
2021 ||| marl: multimodal attentional representation learning for disease prediction. ||| 16318 ||| 16319 ||| 16320 ||| 
2021 ||| transformed cnns: recasting pre-trained convolutional layers with self-attention. ||| 2693 ||| 22714 ||| 22717 ||| 22716 ||| 39726 ||| 
2021 ||| crisis domain adaptation using sequence-to-sequence transformers. ||| 39419 ||| 39420 ||| 39421 ||| 
2020 ||| span: spatial pyramid attention network forimage manipulation localization. ||| 8662 ||| 8663 ||| 8664 ||| 8665 ||| 8666 ||| 8667 ||| 
2020 ||| resolving the scope of speculation and negation using transformer-based architectures. ||| 13372 ||| 13371 ||| 
2020 ||| self-attention comparison module for boosting performance on retrieval-based open-domain dialog systems. ||| 7955 ||| 26634 ||| 39727 ||| 5474 ||| 688 ||| 
2020 ||| attributed multi-relational attention network for fact-checking url recommendation. ||| 1283 ||| 1284 ||| 1285 ||| 1073 ||| 
2019 ||| expectation-maximization attention networks for semantic segmentation. ||| 2514 ||| 2515 ||| 2516 ||| 2517 ||| 2518 ||| 2519 ||| 
2020 ||| self-supervised transformers for activity classification using ambient sensors. ||| 39728 ||| 39729 ||| 31993 ||| 39730 ||| 
2018 ||| plan-recognition-driven attention modeling for visual recognition. ||| 25485 ||| 39731 ||| 23892 ||| 9719 ||| 23894 ||| 
2018 ||| global pose estimation with an attention-based recurrent network. ||| 18744 ||| 17768 ||| 12196 ||| 3247 ||| 
2019 ||| hierarchical attention generative adversarial networks for cross-domain sentiment classification. ||| 39732 ||| 39733 ||| 7079 ||| 
2020 ||| self-attention dense depth estimation network for unrectified video sequences. ||| 11434 ||| 1002 ||| 11435 ||| 
2021 ||| shape: shifted absolute position embedding for transformers. ||| 26782 ||| 26783 ||| 26784 ||| 26674 ||| 
2022 ||| tsam: a two-stream attention model for causal emotion entailment. ||| 11740 ||| 5937 ||| 3075 ||| 3674 ||| 1921 ||| 
2019 ||| monotonic multihead attention. ||| 12088 ||| 11637 ||| 22828 ||| 23964 ||| 9318 ||| 
2017 ||| decidenet: counting varying density crowds through attention guided detection and density estimation. ||| 5206 ||| 11223 ||| 15326 ||| 39734 ||| 18704 ||| 
2022 ||| dcsau-net: a deeper and more compact split-attention u-net for medical image segmentation. ||| 22359 ||| 39735 ||| 39736 ||| 
2020 ||| alleviating the burden of labeling: sentence generation by attention branch encoder-decoder network. ||| 39737 ||| 22285 ||| 726 ||| 723 ||| 724 ||| 725 ||| 12308 ||| 
2021 ||| efficient re-parameterization residual attention network for nonhomogeneous image dehazing. ||| 39738 ||| 39739 ||| 39740 ||| 26814 ||| 
2021 ||| decision-based black-box attack against vision transformers via patch-wise adversarial removal. ||| 39741 ||| 7648 ||| 
2019 ||| transformer-transducer: end-to-end speech recognition with self-attention. ||| 11978 ||| 12447 ||| 12448 ||| 11973 ||| 12489 ||| 39742 ||| 39743 ||| 12449 ||| 12491 ||| 
2021 ||| learning multi-attention context graph for group-based re-identification. ||| 23472 ||| 12650 ||| 8832 ||| 18069 ||| 2014 ||| 1929 ||| 14892 ||| 8532 ||| 1932 ||| 
2021 ||| making transformers solve compositional tasks. ||| 9233 ||| 3882 ||| 3557 ||| 26612 ||| 26613 ||| 
2018 ||| multi-modality sensor data classification with selective attention. ||| 586 ||| 771 ||| 23416 ||| 1300 ||| 6413 ||| 802 ||| 13181 ||| 
2021 ||| on improving adversarial transferability of vision transformers. ||| 32716 ||| 32715 ||| 1969 ||| 1972 ||| 7408 ||| 
2021 ||| stytr^2: unbiased image style transfer with transformers. ||| 32114 ||| 32116 ||| 2396 ||| 19415 ||| 19416 ||| 1175 ||| 
2020 ||| accurate cell segmentation in digital pathology images via attention enforced networks. ||| 16673 ||| 5499 ||| 20217 ||| 
2021 ||| studying the usage of text-to-text transfer transformer to support code-related tasks. ||| 28217 ||| 28218 ||| 28219 ||| 28220 ||| 28221 ||| 28222 ||| 28223 ||| 
2021 ||| starvqa: space-time attention for video quality assessment. ||| 39744 ||| 39745 ||| 39746 ||| 7760 ||| 39747 ||| 
2021 ||| rethinking query, key, and value embedding in vision transformer under tiny model constraints. ||| 39748 ||| 39749 ||| 39750 ||| 39751 ||| 
2019 ||| unified attentional generative adversarial network for brain tumor segmentation from multimodal unpaired images. ||| 27807 ||| 18807 ||| 27808 ||| 3219 ||| 27809 ||| 
2021 ||| attention-based dual-stream vision transformer for radar gait recognition. ||| 39752 ||| 39753 ||| 39754 ||| 2268 ||| 
2018 ||| identification of internal faults in indirect symmetrical phase shift transformers using ensemble learning. ||| 11774 ||| 11775 ||| 11776 ||| 
2021 ||| uvce-iiitt@dravidianlangtech-eacl2021: tamil troll meme classification: you need to pay more attention. ||| 39755 ||| 34606 ||| 34607 ||| 34608 ||| 10634 ||| 
2021 ||| efficient human pose estimation by maximizing fusion and high-level spatial attention. ||| 5682 ||| 5683 ||| 5684 ||| 5685 ||| 5686 ||| 
2020 ||| turbotransformers: an efficient gpu serving system for transformer models. ||| 25019 ||| 882 ||| 25020 ||| 1921 ||| 
2020 ||| look, listen, and attend: co-attention network for self-supervised audio-visual representation learning. ||| 19704 ||| 19705 ||| 19706 ||| 580 ||| 9689 ||| 
2020 ||| stochastic attention head removal: a simple and effective method for improving automatic speech recognition with transformers. ||| 8232 ||| 11995 ||| 11996 ||| 11997 ||| 
2020 ||| achieving real-time execution of transformer-based large-scale models on mobile with compiler-aware neural architecture optimization. ||| 33411 ||| 9872 ||| 14065 ||| 13660 ||| 39756 ||| 11024 ||| 8885 ||| 29013 ||| 33413 ||| 15791 ||| 
2019 ||| an analysis of attention over clinical notes for predictive tasks. ||| 4949 ||| 39757 ||| 4950 ||| 
2020 ||| mt5: a massively multilingual pre-trained text-to-text transformer. ||| 4820 ||| 4821 ||| 4822 ||| 4823 ||| 4824 ||| 4825 ||| 4826 ||| 3338 ||| 
2019 ||| learning temporal attention in dynamic graphs with bilinear interactions. ||| 2579 ||| 39758 ||| 2582 ||| 
2019 ||| improved multi-stage training of online attention-based encoder-decoder models. ||| 13901 ||| 13902 ||| 13903 ||| 13904 ||| 13905 ||| 13906 ||| 
2020 ||| adapting pretrained transformer to lattices for spoken language understanding. ||| 13952 ||| 4843 ||| 
2021 ||| deepfake video detection using convolutional vision transformer. ||| 39759 ||| 39760 ||| 
2021 ||| learning robust scheduling with search and attention. ||| 39761 ||| 39762 ||| 39763 ||| 
2017 ||| unconstrained fashion landmark detection via hierarchical recurrent transformer networks. ||| 19740 ||| 2498 ||| 2011 ||| 13619 ||| 1846 ||| 18791 ||| 
2022 ||| building robust spoken language understanding by cross attention between phoneme sequence and asr hypothesis. ||| 39764 ||| 39765 ||| 2422 ||| 39766 ||| 39767 ||| 17466 ||| 3561 ||| 
2022 ||| dynamic group transformer: a general vision transformer backbone with dynamic group attention. ||| 19109 ||| 19088 ||| 5010 ||| 2323 ||| 
2020 ||| attention-based assisted excitation for salient object segmentation. ||| 12922 ||| 39768 ||| 12923 ||| 12925 ||| 
2019 ||| look globally, age locally: face aging with an attention mechanism. ||| 12663 ||| 12664 ||| 12665 ||| 12666 ||| 
2022 ||| edter: edge detection with transformer. ||| 39769 ||| 2918 ||| 37604 ||| 30827 ||| 2163 ||| 
2021 ||| saint: improved neural networks for tabular data via row attention and contrastive pre-training. ||| 39770 ||| 34246 ||| 39771 ||| 39772 ||| 34247 ||| 
2022 ||| end-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system. ||| 39773 ||| 12300 ||| 
2021 ||| visual transformer for task-aware active learning. ||| 39774 ||| 39775 ||| 39776 ||| 
2020 ||| few-shot object detection with feature attention highlight module in remote sensing images. ||| 35569 ||| 35571 ||| 39777 ||| 39778 ||| 35570 ||| 
2020 ||| probing for multilingual numerical understanding in transformer-based language models. ||| 20968 ||| 20969 ||| 39779 ||| 20971 ||| 
2021 ||| spectralformer: rethinking hyperspectral image classification with transformers. ||| 8848 ||| 39780 ||| 8847 ||| 30238 ||| 30240 ||| 6750 ||| 8849 ||| 
2021 ||| short text clustering with transformers. ||| 39781 ||| 32585 ||| 
2020 ||| train large, then compress: rethinking model size for efficient training and inference of transformers. ||| 22852 ||| 3461 ||| 3822 ||| 18746 ||| 2596 ||| 3145 ||| 34327 ||| 
2022 ||| retroformer: pushing the limits of interpretable end-to-end retrosynthesis transformer. ||| 39782 ||| 39783 ||| 39784 ||| 39785 ||| 
2020 ||| pointtransformer for shape classification and retrieval of 3d and als roof pointclouds. ||| 39786 ||| 39787 ||| 39788 ||| 
2019 ||| dstp-rnn: a dual-stage two-phase attention-based recurrent neural networks for long-term and multivariate time series prediction. ||| 29515 ||| 29516 ||| 29517 ||| 29518 ||| 
2021 ||| exploring the promises of transformer-based lms for the representation of normative claims in the legal domain. ||| 36108 ||| 39789 ||| 2924 ||| 
2021 ||| multi-task time series forecasting with shared attention. ||| 1187 ||| 18506 ||| 1190 ||| 18507 ||| 18508 ||| 
2022 ||| patch-fool: are vision transformers always robust against adversarial perturbations? ||| 37218 ||| 39790 ||| 6077 ||| 39791 ||| 37221 ||| 
2018 ||| densely connected attention propagation for reading comprehension. ||| 1398 ||| 9015 ||| 9016 ||| 9262 ||| 
2020 ||| epsnet: efficient panoptic segmentation network with cross-layer attention fusion. ||| 6372 ||| 6373 ||| 6374 ||| 6375 ||| 
2020 ||| discrimination of internal faults and other transients in an interconnected system with power transformers and phase angle regulators. ||| 11774 ||| 11776 ||| 39792 ||| 
2020 ||| suppressing mislabeled data via grouping and self-attention. ||| 8769 ||| 333 ||| 8770 ||| 3477 ||| 8771 ||| 2149 ||| 
2021 ||| learning generative vision transformer with energy-based latent space for saliency prediction. ||| 875 ||| 9185 ||| 2475 ||| 977 ||| 
2021 ||| missformer: an effective medical image segmentation transformer. ||| 39793 ||| 39794 ||| 39795 ||| 39796 ||| 
2021 ||| hepatic vessel segmentation based on 3d swin-transformer with inductive biased multi-head self-attention. ||| 39797 ||| 39798 ||| 27920 ||| 13217 ||| 8637 ||| 
2020 ||| parameter norm growth during training of transformers. ||| 26737 ||| 26738 ||| 3441 ||| 23971 ||| 3277 ||| 
2021 ||| you only look at one sequence: rethinking transformer in vision through object detection. ||| 37293 ||| 39799 ||| 2219 ||| 37034 ||| 6616 ||| 37294 ||| 2478 ||| 2222 ||| 
2020 ||| antidote: attention-based dynamic optimization for neural network runtime efficiency. ||| 15788 ||| 15789 ||| 15790 ||| 15791 ||| 6007 ||| 
2022 ||| transformer-based video front-ends for audio-visual speech recognition. ||| 39800 ||| 39801 ||| 13911 ||| 
2020 ||| selective segmentation networks using top-down attention. ||| 39802 ||| 29817 ||| 
2018 ||| multi-attention multi-class constraint for fine-grained image recognition. ||| 2390 ||| 8835 ||| 6924 ||| 1761 ||| 
2021 ||| cross-modal attention consistency for video-audio unsupervised learning. ||| 17858 ||| 6545 ||| 18071 ||| 2190 ||| 17860 ||| 18793 ||| 
2022 ||| ode transformer: an ordinary differential equation-inspired model for sequence generation. ||| 3305 ||| 17678 ||| 614 ||| 5149 ||| 36741 ||| 8423 ||| 2333 ||| 3306 ||| 39803 ||| 1254 ||| 
2021 ||| high-resolution optical flow from 1d attention and correlation. ||| 1689 ||| 1690 ||| 1691 ||| 1692 ||| 1693 ||| 
2019 ||| locality-constrained spatial transformer network for video crowd counting. ||| 19824 ||| 19825 ||| 19826 ||| 19827 ||| 19828 ||| 
2021 ||| herbert: efficiently pretrained transformer-based language model for polish. ||| 39804 ||| 39805 ||| 39806 ||| 35544 ||| 39807 ||| 
2019 ||| automatic short answer grading via multiway attention networks. ||| 8082 ||| 8083 ||| 8084 ||| 8085 ||| 8086 ||| 8087 ||| 
2021 ||| an interpretable framework for drug-target interaction with gated cross attention. ||| 39808 ||| 372 ||| 
2021 ||| towards dynamic feature selection with attention to assist banking customers in establishing a new business. ||| 39809 ||| 
2020 ||| verbal focus-of-attention system for learning-from-demonstration. ||| 21834 ||| 21835 ||| 21836 ||| 21837 ||| 
2021 ||| can't fool me: adversarially robust transformer for video understanding. ||| 39810 ||| 19470 ||| 19469 ||| 
2021 ||| transbts: multimodal brain tumor segmentation using transformer. ||| 11656 ||| 2230 ||| 27541 ||| 27542 ||| 304 ||| 27539 ||| 
2022 ||| aligning eyes between humans and deep neural network through interactive attention alignment. ||| 39811 ||| 23368 ||| 2932 ||| 39812 ||| 
2021 |||  marbert: deep bidirectional transformers for arabic. ||| 3152 ||| 3153 ||| 3154 ||| 
2020 ||| look here! a parametric learning based approach to redirect visual attention. ||| 9151 ||| 8778 ||| 8779 ||| 8780 ||| 8781 ||| 
2020 ||| svam: saliency-guided visual attention modeling by autonomous underwater robots. ||| 39813 ||| 39814 ||| 39815 ||| 39816 ||| 
2021 ||| expression snippet transformer for robust video-based facial expression recognition. ||| 30097 ||| 1786 ||| 39817 ||| 39818 ||| 5790 ||| 6850 ||| 
2021 ||| multi-exit vision transformer for dynamic inference. ||| 35515 ||| 336 ||| 926 ||| 
2019 ||| fast transformer decoding: one write-head is all you need. ||| 9132 ||| 
2020 ||| bison: bm25-weighted self-attention framework for multi-fields document search. ||| 17142 ||| 17143 ||| 17144 ||| 6415 ||| 17145 ||| 17146 ||| 17147 ||| 
2021 ||| adaptive channel encoding transformer for point cloud analysis. ||| 39819 ||| 39820 ||| 39821 ||| 3617 ||| 39822 ||| 12342 ||| 
2021 ||| cross-modal transformer-based neural correction models for automatic speech recognition. ||| 4407 ||| 4408 ||| 10274 ||| 10276 ||| 4409 ||| 12220 ||| 10277 ||| 10275 ||| 
2022 ||| entroformer: a transformer-based entropy model for learned image compression. ||| 39823 ||| 32990 ||| 38862 ||| 39824 ||| 32211 ||| 
2021 ||| self-supervised pre-training of swin transformers for 3d medical image analysis. ||| 7206 ||| 2019 ||| 27860 ||| 34009 ||| 7208 ||| 2024 ||| 7207 ||| 7205 ||| 
2020 ||| class: cross-level attention and supervision for salient objects detection. ||| 6434 ||| 1717 ||| 
2019 ||| back attention knowledge transfer for low-resource named entity recognition. ||| 39825 ||| 39826 ||| 8997 ||| 
2021 ||| raanet: range-aware attention network for lidar-based 3d object detection with auxiliary density level estimation. ||| 39827 ||| 13849 ||| 39828 ||| 39829 ||| 39830 ||| 39831 ||| 
2021 ||| u-net transformer: self and cross attention for medical image segmentation. ||| 27355 ||| 27356 ||| 8382 ||| 39832 ||| 27360 ||| 
2021 ||| query-graph with cross-gating attention model for text-to-audio grounding. ||| 13210 ||| 13211 ||| 39833 ||| 13213 ||| 
2021 ||| distributed attention for grounded image captioning. ||| 19456 ||| 2396 ||| 19457 ||| 14066 ||| 19458 ||| 19459 ||| 19460 ||| 2381 ||| 2382 ||| 19461 ||| 
2021 ||| comformer: code comment generation via transformer and fusion method-based hybrid code representation. ||| 6005 ||| 6007 ||| 21694 ||| 21695 ||| 21696 ||| 6008 ||| 5942 ||| 
2021 ||| transformer meets dcfam: a novel semantic segmentation scheme for fine-resolution remote sensing images. ||| 30557 ||| 8207 ||| 30455 ||| 37179 ||| 
2019 ||| what can computational models learn from human selective attention? a review from an audiovisual crossmodal perspective. ||| 39220 ||| 14416 ||| 2657 ||| 39222 ||| 12877 ||| 2656 ||| 887 ||| 2659 ||| 1017 ||| 
2019 ||| sesamebert: attention for anywhere. ||| 9979 ||| 9980 ||| 
2020 ||| tera: self-supervised learning of transformer encoder representation for speech. ||| 12722 ||| 39834 ||| 12644 ||| 
2020 ||| weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos. ||| 39835 ||| 39836 ||| 19727 ||| 1753 ||| 1754 ||| 
2021 ||| geometry-entangled visual semantic transformer for image captioning. ||| 34044 ||| 5474 ||| 34046 ||| 4297 ||| 16696 ||| 
2018 ||| r2cnn++: multi-dimensional attention based rotation invariant detector with robust anchor strategy. ||| 39837 ||| 12746 ||| 7725 ||| 39838 ||| 39839 ||| 30449 ||| 39840 ||| 10525 ||| 
2020 ||| monocular expressive body regression through body-driven attention. ||| 8597 ||| 8598 ||| 8599 ||| 8600 ||| 2100 ||| 
2020 ||| sofa-net: second-order and first-order attention network for crowd counting. ||| 21497 ||| 833 ||| 19593 ||| 
2019 ||| rethinking self-attention: an interpretable self-attentive encoder-decoder parser. ||| 3566 ||| 4952 ||| 4954 ||| 4956 ||| 3568 ||| 
2021 ||| attention is not all you need: pure attention loses rank doubly exponentially with depth. ||| 22849 ||| 22850 ||| 22851 ||| 
2019 ||| online multi-object tracking with dual matching attention networks. ||| 8800 ||| 1007 ||| 2411 ||| 8801 ||| 8802 ||| 7143 ||| 
2019 ||| vision-to-language tasks based on attributes and attention mechanism. ||| 6922 ||| 39841 ||| 8002 ||| 
2021 ||| mda-net: multi-dimensional attention-based neural network for 3d image segmentation. ||| 15671 ||| 15672 ||| 
2021 ||| say their names: resurgence in the collective attention toward black victims of fatal police violence following the death of george floyd. ||| 39842 ||| 39843 ||| 33415 ||| 33418 ||| 33416 ||| 33417 ||| 21879 ||| 39844 ||| 33423 ||| 33422 ||| 
2019 ||| dianet: dense-and-implicit attention network. ||| 17921 ||| 17922 ||| 17923 ||| 17924 ||| 
2020 ||| robust watermarking using inverse gradient attention. ||| 9925 ||| 27811 ||| 14115 ||| 39845 ||| 6335 ||| 
2022 ||| online decision transformer. ||| 39846 ||| 39847 ||| 33086 ||| 
2019 ||| attention-based conditioning methods for external knowledge integration. ||| 3727 ||| 3728 ||| 3729 ||| 
2021 ||| chunkformer: learning long time series with multi-stage chunked transformer. ||| 39848 ||| 39849 ||| 39850 ||| 
2020 ||| attention-based scaling adaptation for target speech extraction. ||| 13945 ||| 13947 ||| 13948 ||| 
2020 ||| breast mass segmentation based on ultrasonic entropy maps and attention gated u-net. ||| 34492 ||| 39851 ||| 39852 ||| 39853 ||| 39854 ||| 35544 ||| 39855 ||| 39856 ||| 
2017 ||| asap: automatic smoothing for attention prioritization in streaming time series visualization. ||| 9955 ||| 9953 ||| 
2021 ||| towards fine-grained visual representations by combining contrastive learning with image reconstruction and attention-weighted pooling. ||| 39857 ||| 39858 ||| 39859 ||| 39860 ||| 
2021 ||| business model canvas should pay more attention to the software startup team. ||| 15725 ||| 15726 ||| 15727 ||| 15728 ||| 15729 ||| 15730 ||| 15731 ||| 15732 ||| 15733 ||| 15734 ||| 15735 ||| 15736 ||| 15737 ||| 15738 ||| 15739 ||| 
2021 ||| pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. ||| 13378 ||| 13382 ||| 748 ||| 
2021 ||| structural guidance for transformer language models. ||| 3550 ||| 3551 ||| 3552 ||| 1633 ||| 3553 ||| 
2020 ||| spatio-temporal ranked-attention networks for video captioning. ||| 7283 ||| 7284 ||| 2507 ||| 2512 ||| 
2021 ||| bidirectional multi-scale attention networks for semantic segmentation of oblique uav imagery. ||| 39861 ||| 39862 ||| 39863 ||| 2410 ||| 
2018 ||| visual attention on the sun: what do existing models actually predict? ||| 2383 ||| 7761 ||| 7762 ||| 7763 ||| 
2021 ||| nvit: vision transformer compression and parameter redistribution. ||| 39864 ||| 34008 ||| 34010 ||| 9877 ||| 24023 ||| 
2018 ||| multiview two-task recursive attention model for left atrium and atrial scars segmentation. ||| 1785 ||| 6005 ||| 27334 ||| 27633 ||| 27634 ||| 27635 ||| 27636 ||| 25716 ||| 27637 ||| 27335 ||| 27638 ||| 27639 ||| 
2021 ||| vitae: vision transformer advanced by exploring intrinsic inductive bias. ||| 37039 ||| 37038 ||| 875 ||| 1756 ||| 
2021 ||| trees in transformers: a theoretical analysis of the transformer's ability to represent trees. ||| 30465 ||| 1994 ||| 39865 ||| 39866 ||| 
2021 ||| feature fusion vision transformer for fine-grained visual categorization. ||| 1224 ||| 11311 ||| 11312 ||| 
2021 ||| efficient sequence training of attention models using approximative recombination. ||| 39867 ||| 14320 ||| 4728 ||| 12533 ||| 12534 ||| 3454 ||| 
2021 ||| the neural data router: adaptive control flow in transformers improves systematic generalization. ||| 59 ||| 26307 ||| 7111 ||| 12659 ||| 4194 ||| 11785 ||| 
2021 ||| event camera simulator design for modeling attention-based inference architectures. ||| 14199 ||| 39868 ||| 14200 ||| 14201 ||| 
2021 ||| over-sampling de-occlusion attention network for prohibited items detection in noisy x-ray images. ||| 19785 ||| 19784 ||| 19913 ||| 18858 ||| 19915 ||| 19914 ||| 17695 ||| 
2019 ||| towards robust image classification using sequential attention models. ||| 2103 ||| 9272 ||| 18862 ||| 18863 ||| 18864 ||| 18865 ||| 
2020 ||| a transformer based pitch sequence autoencoder with midi augmentation. ||| 39869 ||| 39870 ||| 
2020 ||| attention-based deep learning framework for human activity recognition with user adaptation. ||| 39871 ||| 39872 ||| 
2021 ||| topic scene graph generation by attention distillation from caption. ||| 39873 ||| 39874 ||| 39875 ||| 
2021 ||| cmtr: cross-modality transformer for visible-infrared person re-identification. ||| 39876 ||| 14114 ||| 39877 ||| 17651 ||| 38122 ||| 128 ||| 14115 ||| 
2019 ||| raunet: residual attention u-net for semantic segmentation of cataract surgical instruments. ||| 5183 ||| 5184 ||| 5185 ||| 271 ||| 24364 ||| 5187 ||| 276 ||| 5188 ||| 5189 ||| 
2021 ||| progressive and aligned pose attention transfer for person image generation. ||| 18897 ||| 17971 ||| 39878 ||| 18898 ||| 4621 ||| 17429 ||| 
2021 ||| attention-oriented brain storm optimization for multimodal optimization problems. ||| 1825 ||| 20941 ||| 
2021 ||| pay attention to mlps. ||| 3244 ||| 3780 ||| 22752 ||| 9372 ||| 
2018 ||| end-to-end multi-task learning with attention. ||| 18799 ||| 18800 ||| 18801 ||| 
2020 ||| atlas-istn: joint segmentation, registration and atlas construction with image-and-spatial transformer networks. ||| 39879 ||| 27947 ||| 39880 ||| 39881 ||| 39882 ||| 39883 ||| 27948 ||| 27844 ||| 
2020 ||| learning texture transformer network for image super-resolution. ||| 18949 ||| 18950 ||| 1699 ||| 5277 ||| 1772 ||| 
2021 ||| multi-threshold attention u-net (mtau) based model for multimodal brain tumor segmentation in mri scans. ||| 24666 ||| 24667 ||| 24668 ||| 
2021 ||| vit-v-net: vision transformer for unsupervised volumetric medical image registration. ||| 39636 ||| 39638 ||| 39640 ||| 11494 ||| 39637 ||| 
2021 ||| graph attention layer evolves semantic segmentation for road pothole detection: a benchmark and algorithms. ||| 8844 ||| 8845 ||| 4715 ||| 124 ||| 39884 ||| 
2021 ||| multi-level attention fusion network for audio-visual event recognition. ||| 38516 ||| 39885 ||| 2693 ||| 34629 ||| 
2019 ||| finetext: text classification via attention-based language model fine-tuning. ||| 24224 ||| 39886 ||| 39887 ||| 39888 ||| 39889 ||| 39890 ||| 
2021 ||| uacanet: uncertainty augmented context attention for polyp segmentation. ||| 11462 ||| 19752 ||| 2178 ||| 
2021 ||| multi-scale graph convolutional networks with self-attention. ||| 39891 ||| 39892 ||| 
2022 ||| multi-image super-resolution via quality map associated temporal attention network. ||| 24501 ||| 38685 ||| 39893 ||| 38688 ||| 
2019 ||| npa: neural news recommendation with personalized attention. ||| 3754 ||| 3755 ||| 25361 ||| 2489 ||| 2795 ||| 9574 ||| 
2020 ||| deformable siamese attention networks for visual object tracking. ||| 18904 ||| 18905 ||| 2447 ||| 12430 ||| 
2019 ||| attention interpretability across nlp tasks. ||| 39894 ||| 38235 ||| 39895 ||| 39896 ||| 
2019 ||| heterogeneous memory enhanced multimodal attention model for video question answering. ||| 17904 ||| 7497 ||| 6588 ||| 19299 ||| 1460 ||| 9143 ||| 
2020 ||| improved automatic summarization of subroutines via attention to file context. ||| 20599 ||| 20600 ||| 18010 ||| 20601 ||| 
2021 ||| viesum: how robust are transformer-based models on vietnamese summarization? ||| 34499 ||| 16140 ||| 34500 ||| 34501 ||| 16138 ||| 
2020 ||| in-sample contrastive learning and consistent attention for weakly supervised object localization. ||| 2635 ||| 2636 ||| 6396 ||| 2637 ||| 
2021 ||| titan: t cell receptor specificity prediction with bimodal attention networks. ||| 39897 ||| 32811 ||| 10907 ||| 32815 ||| 32816 ||| 4046 ||| 
2022 ||| quadtree attention for vision transformers. ||| 39898 ||| 39899 ||| 39900 ||| 38995 ||| 
2020 ||| kernel self-attention in deep multiple instance learning. ||| 7367 ||| 7369 ||| 7370 ||| 
2021 ||| multimodal fusion with bert and attention mechanism for fake news detection. ||| 2671 ||| 2672 ||| 
2017 ||| dual attention network for product compatibility and function satisfiability analysis. ||| 17130 ||| 9989 ||| 17131 ||| 1094 ||| 
2021 ||| efficient transformer based method for remote sensing image change detection. ||| 2424 ||| 39901 ||| 6654 ||| 
2018 ||| attention based sentence extraction from scientific articles using pseudo-labeled data. ||| 39902 ||| 1453 ||| 39903 ||| 
2021 ||| glacier calving front segmentation using attention u-net. ||| 6814 ||| 6815 ||| 6816 ||| 6817 ||| 13279 ||| 6819 ||| 
2022 ||| mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models. ||| 21086 ||| 39904 ||| 39905 ||| 21089 ||| 
2021 ||| transformer-based approach for joint handwriting and named entity recognition in historical documents. ||| 39906 ||| 39907 ||| 37812 ||| 39908 ||| 
2020 ||| leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning. ||| 21342 ||| 4467 ||| 21343 ||| 21344 ||| 4469 ||| 14279 ||| 12203 ||| 
2018 ||| hard non-monotonic attention for character-level transduction. ||| 3484 ||| 26552 ||| 3485 ||| 
2021 ||| towards accurate rgb-d saliency detection with complementary attention and adaptive integration. ||| 31460 ||| 39909 ||| 27820 ||| 17092 ||| 4055 ||| 423 ||| 
2021 ||| uncertainty-based query strategies for active learning with transformers. ||| 39910 ||| 22541 ||| 39911 ||| 10674 ||| 
2017 ||| interacting attention-gated recurrent networks for recommendation. ||| 1131 ||| 1132 ||| 1133 ||| 1134 ||| 1135 ||| 1136 ||| 
2019 ||| detecting robotic affordances on novel objects with regional attention and attributes. ||| 39912 ||| 39913 ||| 39914 ||| 
2019 ||| pay attention to convolution filters: towards fast and accurate fine-grained transfer learning. ||| 39915 ||| 39916 ||| 39917 ||| 
2018 ||| the fine line between linguistic generalization and failure in seq2seq-attention models. ||| 39918 ||| 39919 ||| 3299 ||| 
2020 ||| deep convlstm with self-attention for human activity decoding using wearables. ||| 39920 ||| 39921 ||| 39922 ||| 39923 ||| 39924 ||| 39925 ||| 
2018 ||| representation based and attention augmented meta learning. ||| 39926 ||| 36298 ||| 39927 ||| 11269 ||| 13439 ||| 337 ||| 
2020 ||| dstc8-avsd: multimodal semantic transformer network with retrieval style word generator. ||| 10564 ||| 12462 ||| 4952 ||| 4953 ||| 4954 ||| 10565 ||| 
2017 ||| show, attend and interact: perceivable human-robot social interaction through neural attention q-network. ||| 21783 ||| 21784 ||| 21785 ||| 20474 ||| 
2018 ||| script identification in natural scene image and video frame using attention based convolutional-lstm network. ||| 1968 ||| 27724 ||| 39928 ||| 39929 ||| 36140 ||| 30717 ||| 
2019 ||| label aware graph convolutional network - not all edges deserve your attention. ||| 2424 ||| 4754 ||| 18202 ||| 39340 ||| 1263 ||| 843 ||| 
2021 ||| violet : end-to-end video-language transformers with masked visual-token modeling. ||| 18272 ||| 2043 ||| 2044 ||| 18746 ||| 3802 ||| 17976 ||| 8573 ||| 
2020 ||| mart: memory-augmented recurrent transformer for coherent video paragraph captioning. ||| 3806 ||| 3807 ||| 3172 ||| 3808 ||| 3809 ||| 3810 ||| 
2021 ||| robustness evaluation of transformer-based form field extractors via form attacks. ||| 39930 ||| 39931 ||| 17855 ||| 3287 ||| 39932 ||| 
2021 ||| how to help university students to manage their interruptions and improve their attention and time management. ||| 39933 ||| 4862 ||| 39934 ||| 32815 ||| 39935 ||| 3882 ||| 39936 ||| 504 ||| 39937 ||| 3419 ||| 852 ||| 39938 ||| 39939 ||| 39940 ||| 
2020 ||| attention-based clustering: learning a kernel from context. ||| 39941 ||| 39942 ||| 39943 ||| 
2018 ||| a multi-sentiment-resource enhanced attention network for sentiment classification. ||| 3176 ||| 3177 ||| 1081 ||| 1199 ||| 
2018 ||| patient risk assessment and warning symptom detection using deep attention-based neural networks. ||| 13355 ||| 13356 ||| 13357 ||| 13358 ||| 13359 ||| 13360 ||| 13361 ||| 13362 ||| 
2021 ||| brain dynamics via cumulative auto-regressive self-attention. ||| 39944 ||| 39945 ||| 15654 ||| 39946 ||| 
2019 ||| towards better modeling hierarchical structure for self-attention with ordered neurons. ||| 4796 ||| 3309 ||| 4882 ||| 4797 ||| 3041 ||| 
2019 ||| location-relative attention mechanisms for robust long-form speech synthesis. ||| 12764 ||| 12765 ||| 12766 ||| 12767 ||| 12768 ||| 12769 ||| 12770 ||| 
2021 ||| convolutional nets versus vision transformers for diabetic foot ulcer classification. ||| 27344 ||| 18877 ||| 15453 ||| 16473 ||| 27345 ||| 
2018 ||| differentiable dynamic programming for structured prediction and attention. ||| 22785 ||| 9211 ||| 
2020 ||| mcqa: multimodal co-attention based network for question answering. ||| 4977 ||| 8820 ||| 7315 ||| 
2020 ||| indic-transformers: an analysis of transformer language models for indian languages. ||| 39947 ||| 39948 ||| 33761 ||| 39949 ||| 39950 ||| 
2020 ||| open-domain frame semantic parsing using transformers. ||| 39951 ||| 39952 ||| 39953 ||| 39954 ||| 39955 ||| 39956 ||| 39957 ||| 
2021 ||| soat: a scene- and object-aware transformer for vision-and-language navigation. ||| 39958 ||| 39959 ||| 8569 ||| 26808 ||| 8564 ||| 
2017 ||| attention and localization based on a deep convolutional recurrent model for weakly supervised audio tagging. ||| 1125 ||| 12618 ||| 12087 ||| 11418 ||| 12619 ||| 
2021 ||| all bark and no bite: rogue dimensions in transformer language models obscure representational quality. ||| 26777 ||| 26778 ||| 
2018 ||| you may not need attention. ||| 3347 ||| 3277 ||| 
2020 ||| fixed encoder self-attention patterns in transformer-based machine translation. ||| 23766 ||| 26734 ||| 4194 ||| 23767 ||| 
2021 ||| inferring prototypes for multi-label few-shot image classification with word vector guided attention. ||| 6506 ||| 9595 ||| 1943 ||| 2885 ||| 39960 ||| 39961 ||| 26853 ||| 
2021 ||| can vision transformers perform convolution? ||| 33472 ||| 35561 ||| 18012 ||| 21252 ||| 
2021 ||| an investigation of enhancing ctc model for triggered attention-based streaming asr. ||| 4531 ||| 4532 ||| 4533 ||| 4534 ||| 
2019 ||| few-shot object detection with attention-rpn and multi-relation detector. ||| 19244 ||| 19245 ||| 8549 ||| 
2019 ||| is attention interpretable? ||| 3276 ||| 3277 ||| 
2019 ||| attention-based context aggregation network for monocular depth estimation. ||| 39962 ||| 39963 ||| 39964 ||| 
2021 ||| hyperspectral and lidar data classification based on linear self-attention. ||| 6809 ||| 6810 ||| 6811 ||| 2628 ||| 
2021 ||| attention based cnn-lstm network for pulmonary embolism prediction on chest computed tomography pulmonary angiograms. ||| 27405 ||| 27406 ||| 27407 ||| 27408 ||| 27409 ||| 27410 ||| 18884 ||| 27411 ||| 
2021 ||| auxiliary loss of transformer with residual connection for end-to-end speaker diarization. ||| 39965 ||| 39966 ||| 39967 ||| 
2020 ||| hmanet: hybrid multiple attention network for semantic segmentation in aerial images. ||| 39968 ||| 
2020 ||| an end-to-end visual-audio attention network for emotion recognition in user-generated videos. ||| 1831 ||| 18195 ||| 13464 ||| 1834 ||| 18196 ||| 18197 ||| 18198 ||| 18199 ||| 2596 ||| 
2021 ||| probing inter-modality: visual parsing with self-attention for vision-language pre-training. ||| 19679 ||| 19678 ||| 19680 ||| 1698 ||| 1699 ||| 1807 ||| 2166 ||| 
2020 ||| hierarchical recurrent attention networks for structured online maps. ||| 18727 ||| 18728 ||| 18729 ||| 9239 ||| 
2021 ||| efficient conformer: progressive downsampling and grouped attention for automatic speech recognition. ||| 13931 ||| 13932 ||| 
2021 ||| multi-glimpse network: a robust and efficient classification architecture based on recurrent downsampled attention. ||| 39969 ||| 39970 ||| 39971 ||| 
2021 ||| when fasttext pays attention: efficient estimation of word representations using constrained positional weighting. ||| 2253 ||| 39972 ||| 39973 ||| 39974 ||| 39975 ||| 39976 ||| 
2018 ||| where and when to look? spatio-temporal attention for action recognition in videos. ||| 7872 ||| 7873 ||| 7874 ||| 7875 ||| 7876 ||| 2301 ||| 
2020 ||| explainable cnn-attention networks (c-attention network) for automated detection of alzheimer's disease. ||| 6003 ||| 38292 ||| 14334 ||| 
2019 ||| dynamic fusion: attentional language model for neural machine translation. ||| 25888 ||| 14211 ||| 
2020 ||| adrn: attention-based deep residual network for hyperspectral image denoising. ||| 12278 ||| 12057 ||| 12056 ||| 12058 ||| 
2022 ||| d-former: a u-shaped dilated transformer for 3d medical image segmentation. ||| 39977 ||| 39978 ||| 39979 ||| 39980 ||| 16782 ||| 10817 ||| 1236 ||| 
2020 ||| gated convolutional bidirectional attention-based model for off-topic spoken response detection. ||| 3506 ||| 3507 ||| 3508 ||| 
2021 ||| deep autoregressive models with spectral attention. ||| 39981 ||| 39982 ||| 37130 ||| 37131 ||| 6235 ||| 
2022 ||| are vision transformers robust to spurious correlations? ||| 39983 ||| 39984 ||| 39985 ||| 
2021 ||| object-region video transformers. ||| 39986 ||| 39987 ||| 2027 ||| 39988 ||| 39989 ||| 19047 ||| 19048 ||| 39990 ||| 
2019 ||| multiresolution transformer networks: recurrence is not essential for modeling hierarchical structure. ||| 39991 ||| 22826 ||| 22825 ||| 
2021 ||| severity quantification and lesion localization of covid-19 on cxr using vision transformer. ||| 31258 ||| 31257 ||| 31259 ||| 31260 ||| 31261 ||| 31262 ||| 31263 ||| 31264 ||| 2048 ||| 
2020 ||| attention-based learning on molecular ensembles. ||| 39992 ||| 39993 ||| 
2019 ||| exploring context, attention and audio features for audio visual scene-aware dialog. ||| 9293 ||| 9294 ||| 9295 ||| 9296 ||| 9297 ||| 
2020 ||| development of learning media based on android games for children with attention deficit hyperactivity disorder. ||| 39994 ||| 39995 ||| 39996 ||| 
2017 ||| solitude or co-existence - or learning-together-apart with digital dialogic technologies for kids with developmental and attention difficulties. ||| 39997 ||| 39998 ||| 
2017 ||| inducing omnipotence or powerlessness in learners with developmental and attention difficulties through structuring technologies. ||| 39998 ||| 39997 ||| 
2021 ||| recipe recommendation with hierarchical graph attention network. ||| 39999 ||| 8917 ||| 40000 ||| 8918 ||| 
2021 ||| covid-19 as a research dynamic transformer: emerging cross-disciplinary and national characteristics. ||| 40001 ||| 40002 ||| 40003 ||| 40004 ||| 
2020 ||| dp3 signal as a neuro-indictor for attentional processing of stereoscopic contents in varied depths within the 'comfort zone'. ||| 40005 ||| 2823 ||| 40006 ||| 40007 ||| 40008 ||| 1785 ||| 
2021 ||| a metric-based meta-learning approach combined attention mechanism and ensemble learning for few-shot learning. ||| 40009 ||| 40010 ||| 23840 ||| 21361 ||| 40011 ||| 
2022 ||| cross attention redistribution with contrastive learning for few shot object detection. ||| 40012 ||| 40013 ||| 1207 ||| 
2022 ||| class center attention network with spatial adaption for enhancing hepatic segments classification with low-visibility vascular. ||| 40014 ||| 40015 ||| 40016 ||| 40017 ||| 40018 ||| 40019 ||| 40020 ||| 40021 ||| 40022 ||| 24570 ||| 40023 ||| 40024 ||| 
2021 ||| rafnet: rgb-d attention feature fusion network for indoor semantic segmentation. ||| 40025 ||| 40026 ||| 40027 ||| 40028 ||| 
2021 ||| quadratic polynomial guided fuzzy c-means and dual attention mechanism for medical image segmentation. ||| 29239 ||| 40029 ||| 4477 ||| 40030 ||| 40031 ||| 
2021 ||| an improved image enhancement framework based on multiple attention mechanism. ||| 40032 ||| 40033 ||| 6661 ||| 
2021 ||| t-gan: a deep learning framework for prediction of temporal complex networks with adaptive graph convolution and attention mechanism. ||| 40034 ||| 6805 ||| 40035 ||| 40036 ||| 
2022 ||| covid-19 ct image recognition algorithm based on transformer and cnn. ||| 40037 ||| 40038 ||| 40039 ||| 40040 ||| 
2017 ||| backlight dimming based on saliency map acquired by visual attention analysis. ||| 40041 ||| 7298 ||| 
2020 ||| 3-d visual discomfort assessment considering optical and neural attention models. ||| 40042 ||| 40043 ||| 40044 ||| 13249 ||| 32419 ||| 
2020 ||| modeling the screen content image quality via multiscale edge attention similarity. ||| 5101 ||| 19057 ||| 40045 ||| 23024 ||| 8802 ||| 3879 ||| 
2020 |||  (clinical electronic medical record named entity recognition incorporating language model and attention mechanism). ||| 40046 ||| 16885 ||| 16884 ||| 40047 ||| 6627 ||| 
2019 |||  (multi-view attentional approach to single-fact knowledge-based question answering). ||| 650 ||| 651 ||| 481 ||| 
2019 |||  (sentiment classification towards question-answering based on bidirectional attention mechanism). ||| 20817 ||| 2037 ||| 19849 ||| 3085 ||| 
2019 |||  (event coreference resolution method based on attention mechanism). ||| 13565 ||| 221 ||| 222 ||| 
2017 |||  (attention of bilinear function based bi-lstm model for machine reading comprehension). ||| 40048 ||| 30862 ||| 1216 ||| 40049 ||| 40050 ||| 
2019 |||  (asynchronous advantage actor-critic algorithm with visual attention mechanism). ||| 4634 ||| 40051 ||| 40052 ||| 14059 ||| 
2019 |||  (employing multi-attention mechanism to resolve event coreference). ||| 2904 ||| 221 ||| 222 ||| 
2019 |||  (distant supervision relation extraction model based on multi-level attention mechanism). ||| 1705 ||| 40053 ||| 8173 ||| 40054 ||| 
2019 |||  (deep neural network recommendation model based on user vectorization representation and attention mechanism). ||| 40055 ||| 406 ||| 
2019 |||  (image description model fusing word2vec and attention mechanism). ||| 40056 ||| 40057 ||| 40058 ||| 40059 ||| 
2019 |||  (video advertisement classification method based on shot segmentation and spatial attention model). ||| 40060 ||| 7779 ||| 7780 ||| 7781 ||| 
2019 |||  (study on named entity recognition model based on attention mechanism - - taking military text as example). ||| 40061 ||| 40062 ||| 9990 ||| 40063 ||| 
2019 |||  (model of music theme recommendation based on attention lstm). ||| 40064 ||| 40065 ||| 
2017 |||  (visual attention modeling based on multi-scale fusion of amplitude spectrum and phase spectrum). ||| 40066 ||| 40067 ||| 4172 ||| 27971 ||| 
2018 |||  (chinese part-of-speech tagging model using attention-based lstm). ||| 40068 ||| 40062 ||| 3337 ||| 40061 ||| 40069 ||| 
2019 |||  (attention mechanism based detection of malware call sequences). ||| 40070 ||| 40071 ||| 40072 ||| 
2019 |||  (prediction model of e-sports behavior pattern based on attention mechanism and lrua module). ||| 11121 ||| 40073 ||| 40074 ||| 40075 ||| 
2019 |||  (attention based acoustics model combining bottleneck feature long xing-yan qu dan zhang wen-lin). ||| 40076 ||| 40077 ||| 25768 ||| 
2020 |||  (software requirements clustering algorithm based on self-attention mechanism and multi- channel pyramid convolution). ||| 40078 ||| 40079 ||| 1705 ||| 40080 ||| 40081 ||| 40082 ||| 
2019 |||  (bilstm-based implicit discourse relation classification combining self-attentionmechanism and syntactic information). ||| 1427 ||| 1254 ||| 40083 ||| 
2020 |||  (chinese short text keyphrase extraction model based on attention). ||| 40084 ||| 18556 ||| 23775 ||| 
2020 |||  (comment sentiment analysis and sentiment words detection based on attention mechanism). ||| 8177 ||| 40085 ||| 40086 ||| 19103 ||| 1029 ||| 
2018 |||  (neural machine translation based on attention convolution). ||| 6627 ||| 3468 ||| 
2019 |||  (event temporal relation classification method based on self-attention mechanism). ||| 6838 ||| 221 ||| 222 ||| 
2021 ||| hierarchical attention learning of scene flow in 3d point clouds. ||| 28417 ||| 32869 ||| 6474 ||| 25570 ||| 
2021 ||| part-guided relational transformers for fine-grained visual recognition. ||| 2380 ||| 2383 ||| 40087 ||| 28819 ||| 
2021 ||| interleaved deep artifacts-aware attention mechanism for concrete structural defect classification. ||| 28777 ||| 2651 ||| 16408 ||| 
2018 ||| spatio-temporal attention-based lstm networks for 3d action recognition and detection. ||| 18277 ||| 18278 ||| 11269 ||| 7912 ||| 18279 ||| 
2022 ||| dmra: depth-induced multi-scale recurrent attention network for rgb-d saliency detection. ||| 2066 ||| 40088 ||| 2067 ||| 2065 ||| 3648 ||| 2068 ||| 40089 ||| 1700 ||| 
2021 ||| learning deep global multi-scale and local attention features for facial expression recognition in the wild. ||| 19554 ||| 6625 ||| 40090 ||| 
2021 ||| discriminative cross-modality attention network for temporal inconsistent audio-visual event localization. ||| 12386 ||| 28796 ||| 758 ||| 1825 ||| 127 ||| 
2020 ||| picanet: pixel-wise contextual attention learning for accurate saliency detection. ||| 2411 ||| 2414 ||| 7143 ||| 
2021 ||| interpretable detail-fidelity attention network for single image super-resolution. ||| 32417 ||| 4634 ||| 6621 ||| 32418 ||| 32419 ||| 
2019 ||| spatial-temporal attention-aware learning for video-based person re-identification. ||| 1917 ||| 1920 ||| 19774 ||| 1921 ||| 
2020 ||| dual-path attention network for compressed sensing image reconstruction. ||| 30960 ||| 40091 ||| 6625 ||| 1748 ||| 2323 ||| 
2021 ||| spatial-aware texture transformer for high-fidelity garment transfer. ||| 3311 ||| 40092 ||| 40093 ||| 1905 ||| 5392 ||| 4400 ||| 1685 ||| 
2019 ||| deep attention network for egocentric action recognition. ||| 7816 ||| 7818 ||| 40094 ||| 4172 ||| 
2020 ||| deep pyramidal pooling with attention for person re-identification. ||| 4699 ||| 4704 ||| 4702 ||| 
2021 ||| toward accurate pixelwise object tracking via attention retrieval. ||| 5396 ||| 38863 ||| 8838 ||| 8841 ||| 1698 ||| 
2019 ||| attention-based pedestrian attribute analysis. ||| 40095 ||| 11466 ||| 13439 ||| 40096 ||| 2323 ||| 338 ||| 
2021 ||| attention-based multi-source domain adaptation. ||| 40097 ||| 40098 ||| 1175 ||| 
2018 ||| recurrent spatial-temporal attention network for action recognition in videos. ||| 2147 ||| 2148 ||| 2149 ||| 
2021 ||| srgat: single image super-resolution with graph attention network. ||| 14845 ||| 8621 ||| 14844 ||| 8293 ||| 7064 ||| 8626 ||| 
2021 ||| layer-output guided complementary attention learning for image defocus blur detection. ||| 31046 ||| 40099 ||| 22854 ||| 40100 ||| 26463 ||| 1125 ||| 31047 ||| 
2022 ||| cross-attentional spatio-temporal semantic graph networks for video question answering. ||| 4477 ||| 5320 ||| 19722 ||| 1099 ||| 843 ||| 
2019 ||| saliency from growing neural gas: learning pre-attentional structures for a flexible attention system. ||| 16321 ||| 16322 ||| 40101 ||| 13310 ||| 16325 ||| 
2019 ||| deep ordinal hashing with spatial attention. ||| 13226 ||| 30753 ||| 2233 ||| 13227 ||| 7380 ||| 8536 ||| 
2022 ||| personality assessment based on multimodal attention network learning with category-based mean square error. ||| 443 ||| 23312 ||| 40102 ||| 40103 ||| 444 ||| 
2022 ||| universal adversarial patch attack for automatic checkout using perceptual and attentional bias. ||| 18857 ||| 18858 ||| 3239 ||| 17695 ||| 
2020 ||| textual-visual reference-aware attention network for visual dialog. ||| 19645 ||| 1341 ||| 13823 ||| 444 ||| 
2021 ||| loss-based attention for interpreting image-level prediction of convolutional neural networks. ||| 17961 ||| 17962 ||| 40104 ||| 27493 ||| 16809 ||| 21587 ||| 19896 ||| 
2022 ||| deep stereo matching with hysteresis attention and supervised cost volume construction. ||| 40105 ||| 28490 ||| 29142 ||| 40106 ||| 40107 ||| 25908 ||| 
2021 ||| multi-stream attention-aware graph convolution network for video salient object detection. ||| 11349 ||| 40108 ||| 3072 ||| 40109 ||| 
2019 ||| visual attention prediction for stereoscopic video by multi-module fully convolutional network. ||| 19941 ||| 1460 ||| 19942 ||| 19853 ||| 
2021 ||| predicting task-driven attention via integrating bottom-up stimulus and top-down guidance. ||| 15491 ||| 40110 ||| 1578 ||| 2354 ||| 24396 ||| 18980 ||| 14779 ||| 
2021 ||| dual attention-in-attention model for joint rain streak and raindrop removal. ||| 8625 ||| 34252 ||| 28780 ||| 8621 ||| 
2017 ||| end-to-end comparative attention networks for person re-identification. ||| 5170 ||| 1685 ||| 35028 ||| 860 ||| 1728 ||| 
2021 ||| learning spatial attention for face super-resolution. ||| 36186 ||| 36187 ||| 1371 ||| 19196 ||| 36188 ||| 
2021 ||| attention guided multiple source and target domain adaptation. ||| 40111 ||| 18260 ||| 40112 ||| 6147 ||| 
2020 ||| mvsnet++: learning depth-based attention pyramid features for multi-view stereo. ||| 40113 ||| 40114 ||| 40115 ||| 6331 ||| 
2019 ||| three-stream attention-aware network for rgb-d salient object detection. ||| 2424 ||| 25496 ||| 
2020 ||| har-net: joint learning of hybrid attention for single-stage object detection. ||| 19032 ||| 11221 ||| 
2019 ||| bi-directional spatial-semantic attention networks for image-text matching. ||| 19722 ||| 5320 ||| 19724 ||| 843 ||| 
2017 ||| visual attention saccadic models learn to emulate gaze patterns from childhood to adulthood. ||| 35060 ||| 11561 ||| 2740 ||| 40116 ||| 40117 ||| 12999 ||| 
2020 ||| bi-modal progressive mask attention for fine-grained recognition. ||| 2009 ||| 6332 ||| 30753 ||| 40118 ||| 836 ||| 
2021 ||| bilateral attention network for rgb-d salient object detection. ||| 5543 ||| 16443 ||| 2013 ||| 38126 ||| 1977 ||| 1861 ||| 
2019 ||| vssa-net: vertical spatial sequence attention network for traffic sign detection. ||| 6650 ||| 6708 ||| 6627 ||| 
2021 ||| i understand you: blind 3d human attention inference from the perspective of third-person. ||| 40119 ||| 17553 ||| 6658 ||| 
2021 ||| graph regularized flow attention network for video animal counting from drones. ||| 8754 ||| 11124 ||| 8749 ||| 40120 ||| 8750 ||| 5077 ||| 
2020 ||| learning rich part hierarchies with progressive attention networks for fine-grained image recognition. ||| 2164 ||| 1699 ||| 8710 ||| 2166 ||| 2165 ||| 
2020 ||| mava: multi-level adaptive visual-textual alignment by cross-media bi-attention mechanism. ||| 5954 ||| 23380 ||| 40121 ||| 
2018 ||| a better way to attend: attention with trees for video question answering. ||| 37133 ||| 37134 ||| 1306 ||| 1115 ||| 
2020 ||| sta-cnn: convolutional spatial-temporal attention learning for action recognition. ||| 2792 ||| 8837 ||| 254 ||| 40122 ||| 8841 ||| 18761 ||| 
2021 ||| person re-identification via attention pyramid. ||| 1917 ||| 36354 ||| 1920 ||| 36355 ||| 1921 ||| 
2020 ||| sequential dual attention network for rain streak removal in a single image. ||| 40123 ||| 40124 ||| 40125 ||| 40126 ||| 40127 ||| 
2021 ||| sloan: scale-adaptive orientation attention network for scene text recognition. ||| 40128 ||| 16546 ||| 8626 ||| 
2021 ||| learning to match anchor-target video pairs with dual attentional holographic networks. ||| 19497 ||| 19498 ||| 26877 ||| 
2019 ||| attention couplenet: fully convolutional attention coupling network for object detection. ||| 19481 ||| 11290 ||| 11288 ||| 2077 ||| 7662 ||| 2080 ||| 
2019 ||| scan: self-and-collaborative attention network for video person re-identification. ||| 27729 ||| 14653 ||| 22662 ||| 35739 ||| 2011 ||| 1846 ||| 2315 ||| 
2020 ||| image interpolation using multi-scale attention-aware inception network. ||| 40129 ||| 40130 ||| 40131 ||| 
2021 ||| graph attention layer evolves semantic segmentation for road pothole detection: a benchmark and algorithms. ||| 8844 ||| 8845 ||| 4715 ||| 124 ||| 39884 ||| 
2021 ||| fs-dsm: few-shot diagram-sentence matching via cross-modal attention graph model. ||| 22883 ||| 39296 ||| 1235 ||| 17087 ||| 40132 ||| 
2022 ||| improving face-based age estimation with attention-based dynamic patch fusion. ||| 39109 ||| 11621 ||| 39110 ||| 
2020 ||| spatio-temporal memory attention for image captioning. ||| 40133 ||| 21716 ||| 38080 ||| 38742 ||| 19488 ||| 
2021 ||| action anticipation using pairwise human-object interactions and transformers. ||| 40134 ||| 33076 ||| 
2022 ||| sst: spatial and semantic transformers for multi-label image recognition. ||| 40135 ||| 40136 ||| 40137 ||| 40118 ||| 28779 ||| 40138 ||| 
2019 ||| two-level attention network with multi-grain ranking loss for vehicle re-identification. ||| 11288 ||| 32150 ||| 2074 ||| 2077 ||| 
2020 ||| one-pass multi-task networks with cross-task guided attention for brain tumor segmentation. ||| 34613 ||| 18611 ||| 18726 ||| 34614 ||| 1756 ||| 
2021 ||| fine-grained 3d shape classification with hierarchical part-view attention. ||| 18229 ||| 2563 ||| 2559 ||| 18230 ||| 
2017 ||| unifying the video and question attentions for open-ended video question answering. ||| 37133 ||| 1306 ||| 1115 ||| 
2019 ||| learning semantics-preserving attention and contextual interaction for group activity recognition. ||| 19771 ||| 1920 ||| 19772 ||| 19774 ||| 1921 ||| 
2021 ||| ap-cnn: weakly supervised attention pyramid convolutional neural network for fine-grained visual classification. ||| 33805 ||| 1484 ||| 33806 ||| 1483 ||| 1482 ||| 33807 ||| 10974 ||| 2163 ||| 
2018 ||| object-part attention model for fine-grained image classification. ||| 5954 ||| 6618 ||| 36324 ||| 
2019 ||| 3d2seqviews: aggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation. ||| 2563 ||| 40139 ||| 40140 ||| 23403 ||| 2559 ||| 18230 ||| 2414 ||| 15206 ||| 
2019 ||| show, attend, and translate: unsupervised image translation with self-regularization and attention. ||| 497 ||| 37406 ||| 37407 ||| 9407 ||| 14521 ||| 
2020 ||| multi-modal recurrent attention networks for facial expression recognition. ||| 12629 ||| 12630 ||| 8797 ||| 1515 ||| 
2019 ||| seqviews2seqlabels: learning 3d global features via aggregating sequential views by rnn with attention. ||| 2563 ||| 40141 ||| 40140 ||| 23403 ||| 2559 ||| 18230 ||| 2414 ||| 15206 ||| 
2018 ||| modality-specific cross-modal similarity measurement with recurrent attention network. ||| 5954 ||| 23380 ||| 5953 ||| 
2022 ||| siamese implicit region proposal network with compound attention for visual tracking. ||| 40142 ||| 40143 ||| 40144 ||| 5030 ||| 28779 ||| 
2019 ||| cross-modal attentional context learning for rgb-d object detection. ||| 1800 ||| 38100 ||| 16691 ||| 38101 ||| 2315 ||| 
2020 ||| pona: pose-guided non-local attention for human pose transfer. ||| 8293 ||| 14692 ||| 1716 ||| 1833 ||| 37898 ||| 
2019 ||| multi-turn video question answering via hierarchical attention context reinforced networks. ||| 1306 ||| 20664 ||| 19628 ||| 1115 ||| 
2021 ||| relational reasoning for group activity recognition via self-attention augmented conditional random field. ||| 2242 ||| 2244 ||| 2243 ||| 
2021 ||| re-attention for visual question answering. ||| 18013 ||| 4646 ||| 1834 ||| 15243 ||| 
2021 ||| floorlevel-net: recognizing floor-level lines with height-attention-guided multi-task learning. ||| 37427 ||| 19482 ||| 1739 ||| 
2021 ||| depth privileged scene recognition via dual attention hallucination. ||| 927 ||| 5948 ||| 5088 ||| 
2021 ||| attend and guide (ag-net): a keypoints-driven attention-based deep network for image recognition. ||| 18075 ||| 6384 ||| 7263 ||| 7264 ||| 6383 ||| 
2020 ||| an efficient fire detection method based on multiscale feature extraction, implicit deep supervision and channel attention mechanism. ||| 40145 ||| 40146 ||| 10572 ||| 
2021 ||| mlda-net: multi-level dual attention-based network for self-supervised monocular depth estimation. ||| 18979 ||| 3337 ||| 18707 ||| 2240 ||| 23616 ||| 7449 ||| 23618 ||| 
2021 ||| multi-scale spatial attention-guided monocular depth estimation with semantic enhancement. ||| 40147 ||| 5790 ||| 40148 ||| 
2020 ||| forecasting future action sequences with attention: a new approach to weakly supervised action forecasting. ||| 38050 ||| 33076 ||| 
2022 ||| structure-aware positional transformer for visible-infrared person re-identification. ||| 40149 ||| 17926 ||| 35028 ||| 35027 ||| 860 ||| 17661 ||| 
2020 ||| learning recurrent 3d attention for video-based person re-identification. ||| 1917 ||| 1920 ||| 19774 ||| 1921 ||| 
2021 ||| end-to-end learnt image compression via non-local attention optimization and improved context modeling. ||| 5664 ||| 19055 ||| 19057 ||| 19056 ||| 38555 ||| 10409 ||| 
2021 ||| person re-identification with reinforced attribute attention selection. ||| 5086 ||| 5948 ||| 5088 ||| 
2021 ||| self-attention context network: addressing the threat of adversarial attacks for hyperspectral image classification. ||| 17756 ||| 17757 ||| 17760 ||| 
2020 ||| no-reference image quality assessment: an attention driven approach. ||| 7202 ||| 2142 ||| 7204 ||| 
2022 ||| two-branch attention network via efficient semantic coupling for one-shot learning. ||| 5536 ||| 40150 ||| 17695 ||| 18248 ||| 444 ||| 
2020 ||| a multi-scale spatial-temporal attention model for person re-identification in videos. ||| 781 ||| 40151 ||| 38072 ||| 40152 ||| 19820 ||| 2398 ||| 
2021 ||| multi-relation attention network for image patch matching. ||| 40153 ||| 6827 ||| 1329 ||| 40154 ||| 40155 ||| 8849 ||| 6829 ||| 400 ||| 
2021 ||| dense attention fluid network for salient object detection in optical remote sensing images. ||| 38463 ||| 35817 ||| 13621 ||| 1904 ||| 19941 ||| 8626 ||| 4400 ||| 19728 ||| 
2022 ||| meta pid attention network for flexible and efficient real-world noisy image denoising. ||| 40156 ||| 40157 ||| 40158 ||| 5746 ||| 
2020 ||| cascaded attention guidance network for single rainy image restoration. ||| 18691 ||| 18753 ||| 18394 ||| 
2020 ||| learning a deep dual attention network for video super-resolution. ||| 4398 ||| 4399 ||| 4400 ||| 
2020 ||| region attention networks for pose and occlusion robust facial expression recognition. ||| 333 ||| 8769 ||| 8771 ||| 11244 ||| 2149 ||| 
2022 ||| remote sensing scene classification via multi-branch local attention network. ||| 380 ||| 40159 ||| 17623 ||| 5755 ||| 382 ||| 40160 ||| 
2021 ||| mask-guided attention network and occlusion-sensitive hard example mining for occluded pedestrian detection. ||| 1824 ||| 2279 ||| 2308 ||| 1971 ||| 1972 ||| 1932 ||| 
2021 ||| dpanet: depth potentiality-aware gated attention network for rgb-d salient object detection. ||| 32367 ||| 35817 ||| 40161 ||| 13825 ||| 
2020 ||| attention-aware multi-task convolutional neural networks. ||| 40162 ||| 22668 ||| 17724 ||| 
2018 ||| skeleton-based human action recognition with global context-aware attention lstm networks. ||| 1235 ||| 8608 ||| 18938 ||| 40163 ||| 11620 ||| 
2021 ||| dan: deep-attention network for 3d shape recognition. ||| 19742 ||| 3226 ||| 40164 ||| 820 ||| 
2021 ||| spatial-angular attention network for light field reconstruction. ||| 1712 ||| 19139 ||| 1716 ||| 1715 ||| 34222 ||| 
2019 ||| cam-rnn: co-attention model based rnn for video captioning. ||| 22078 ||| 6922 ||| 8002 ||| 
2021 ||| coanet: connectivity attention network for road extraction from satellite imagery. ||| 10758 ||| 40165 ||| 40166 ||| 1904 ||| 
2020 ||| hierarchical u-shape attention network for salient object detection. ||| 2354 ||| 2357 ||| 40167 ||| 6436 ||| 7393 ||| 40168 ||| 14779 ||| 
2021 ||| accurate and fast image denoising via attention guided scaling. ||| 1730 ||| 2232 ||| 2233 ||| 11802 ||| 40169 ||| 1734 ||| 
2022 ||| weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification. ||| 40170 ||| 40171 ||| 17757 ||| 17760 ||| 
2020 ||| ha-ccn: hierarchical attention-based crowd counting network. ||| 19135 ||| 18609 ||| 
2020 ||| improving the harmony of the composite image by spatial-separated attention module. ||| 17856 ||| 17857 ||| 
2019 ||| discriminative feature learning with foreground attention for person re-identification. ||| 2354 ||| 2357 ||| 8850 ||| 32491 ||| 11549 ||| 14779 ||| 
2017 ||| visual attention modeling for stereoscopic video: a benchmark and computational model. ||| 19941 ||| 1460 ||| 4807 ||| 19853 ||| 11610 ||| 11611 ||| 
2018 ||| deep visual attention prediction. ||| 2444 ||| 2445 ||| 
2021 ||| learning to discover multi-class attentional regions for multi-label image recognition. ||| 33324 ||| 7913 ||| 
2020 ||| compositional attention networks with two-stream fusion for video question answering. ||| 34911 ||| 1754 ||| 1753 ||| 1756 ||| 
2020 ||| attribute-guided attention for referring expression generation and comprehension. ||| 2139 ||| 1160 ||| 10429 ||| 7143 ||| 
2021 ||| aan-face: attention augmented networks for face recognition. ||| 2322 ||| 2323 ||| 
2021 ||| uncertainty guided multi-scale attention network for raindrop removal from a single image. ||| 40172 ||| 40173 ||| 8850 ||| 2018 ||| 
2022 ||| high-resolution depth maps imaging via attention-based hierarchical multi-modal fusion. ||| 12055 ||| 12058 ||| 12056 ||| 32538 ||| 32539 ||| 32540 ||| 
2020 ||| reverse attention-based residual network for salient object detection. ||| 8601 ||| 8602 ||| 8603 ||| 1700 ||| 8604 ||| 1734 ||| 
2019 ||| occlusion aware facial expression recognition using cnn with attention mechanism. ||| 2969 ||| 38926 ||| 1916 ||| 1788 ||| 
2022 ||| health status diagnosis of distribution transformers based on big data mining. ||| 7400 ||| 40174 ||| 31997 ||| 40175 ||| 
2021 ||| predicting the next location: a self-attention and recurrent neural network model with temporal context. ||| 5069 ||| 10812 ||| 10813 ||| 5068 ||| 
2021 ||| the three a's of wearable and ubiquitous computing: activity, affect, and attention. ||| 37793 ||| 
2021 ||| eeg-based auditory attention detection and its possible future applications for passive bci. ||| 40176 ||| 40177 ||| 40178 ||| 3882 ||| 
2018 ||| data-driven charging strategy of pevs under transformer aging risk. ||| 40179 ||| 4811 ||| 8214 ||| 40180 ||| 40181 ||| 
2019 ||| port-controlled phasor hamiltonian modeling and ida-pbc control of solid-state transformer. ||| 25070 ||| 40182 ||| 40183 ||| 25071 ||| 40184 ||| 40185 ||| 
2018 ||| short-term traffic speed forecasting based on attention convolutional neural network for arterials. ||| 40186 ||| 40187 ||| 40188 ||| 
2022 ||| spatiotemporal gated graph attention network for urban traffic flow prediction based on license plate recognition data. ||| 40189 ||| 40190 ||| 
2018 ||| visual attention-based access: granting access based on users' joint attention on shared workspaces. ||| 28 ||| 16126 ||| 40191 ||| 40192 ||| 
2020 ||| giobalfusion: a global attentional deep learning framework for multisensor information fusion. ||| 17168 ||| 3366 ||| 40193 ||| 20491 ||| 40194 ||| 3365 ||| 17171 ||| 
2018 ||| mindid: person identification from brain waves through attention-based recurrent neural network. ||| 586 ||| 771 ||| 34395 ||| 37686 ||| 774 ||| 770 ||| 
2021 ||| exploration of person-independent bcis for internal and external attention-detection in augmented reality. ||| 13860 ||| 13862 ||| 
2019 ||| classifying attention types with thermal imaging and eye tracking. ||| 16127 ||| 40195 ||| 4564 ||| 4566 ||| 40196 ||| 4569 ||| 8348 ||| 40197 ||| 40198 ||| 
2020 ||| mail: multi-scale attention-guided indoor localization using geomagnetic sequences. ||| 18712 ||| 17669 ||| 9027 ||| 9053 ||| 29074 ||| 9010 ||| 
2018 ||| a survey of attention management systems in ubiquitous computing environments. ||| 36705 ||| 36706 ||| 36707 ||| 36708 ||| 36709 ||| 36710 ||| 22996 ||| 
2021 ||| opitrack: a wearable-based clinical opioid use tracker with temporal convolutional attention networks. ||| 40199 ||| 40200 ||| 40201 ||| 40202 ||| 40203 ||| 40204 ||| 
2021 ||| to see or not to see: exploring inattentional blindness for the design of unobtrusive interfaces in shared public places. ||| 40205 ||| 40206 ||| 40207 ||| 40208 ||| 
2021 ||| memx: an attention-aware smart eyewear system for personalized moment auto-capture. ||| 38640 ||| 38641 ||| 36833 ||| 28703 ||| 38642 ||| 38643 ||| 38644 ||| 2961 ||| 2962 ||| 26933 ||| 
2021 ||| multi-head spatio-temporal attention mechanism for urban anomaly event prediction. ||| 24895 ||| 13408 ||| 9053 ||| 
2019 ||| decentralized attention-based personalized human mobility prediction. ||| 40209 ||| 40210 ||| 40211 ||| 40212 ||| 40213 ||| 
2020 ||| segment boundary detection directed attention for online end-to-end speech recognition. ||| 4462 ||| 4395 ||| 3198 ||| 12372 ||| 
2019 ||| a new joint ctc-attention-based speech recognition model with multi-level multi-head attention. ||| 40214 ||| 40215 ||| 40077 ||| 
2021 ||| neural network-based non-intrusive speech quality assessment using attention pooling function. ||| 8764 ||| 4550 ||| 40216 ||| 5743 ||| 
2021 ||| adversarial joint training with self-attention mechanism for robust end-to-end speech recognition. ||| 23273 ||| 40217 ||| 40218 ||| 23270 ||| 23271 ||| 23274 ||| 5695 ||| 
2017 ||| the effects of attention monitoring with eeg biofeedback on university students' attention and self-efficacy: the case of anti-phishing instructional materials. ||| 6180 ||| 40219 ||| 
2017 ||| the indirect relationship of media multitasking self-efficacy on learning performance within the personal learning environment: implications from the mechanism of perceived attention problems and self-regulation strategies. ||| 40220 ||| 
2018 ||| exploring effects of discussion on visual attention, learning performance, and perceptions of students learning with str-support. ||| 40221 ||| 40222 ||| 40223 ||| 
2021 ||| the learning behaviours of dropouts in moocs: a collective attention network perspective. ||| 21013 ||| 4160 ||| 39696 ||| 
2017 ||| seeing the instructor's face and gaze in demonstration video examples affects attention allocation but not learning. ||| 40224 ||| 40225 ||| 
2019 ||| who is better adapted in learning online within the personal learning environment? relating gender differences in cognitive attention networks to digital distraction. ||| 40220 ||| 40226 ||| 
2020 ||| does visual attention to the instructor in online video affect learning and learner perceptions? an eye-tracking analysis. ||| 15830 ||| 40227 ||| 40228 ||| 
2020 ||| electronic storybook design, kindergartners' visual attention, and print awareness: an eye-tracking investigation. ||| 40229 ||| 40230 ||| 40231 ||| 40232 ||| 
2021 ||| generative chemical transformer: neural machine learning of molecular geometric structures from chemical language via attention. ||| 32443 ||| 32444 ||| 32445 ||| 
2020 ||| signal-3l 3.0: improving signal peptide prediction through combining attention deep learning with window-based scoring. ||| 40233 ||| 33558 ||| 40234 ||| 
2021 ||| hidra: hierarchical network for drug response prediction with attention. ||| 40235 ||| 40236 ||| 
2019 ||| deepchemstable: chemical stability prediction with an attention-based graph convolution network. ||| 40237 ||| 40238 ||| 40239 ||| 40240 ||| 8976 ||| 2013 ||| 
2019 ||| identifying structure-property relationships through smiles syntax analysis with self-attention mechanism. ||| 16736 ||| 40238 ||| 16595 ||| 2013 ||| 
2020 ||| predicting binding from screening assays with transformer network embeddings. ||| 40241 ||| 40242 ||| 40243 ||| 40244 ||| 
2020 ||| predicting retrosynthetic reactions using self-corrected transformer neural networks. ||| 16736 ||| 23503 ||| 32626 ||| 2013 ||| 16595 ||| 
2021 ||| msa-regularized protein sequence transformer toward predicting genome-wide chemical-protein interactions: application to gpcrome deorphanization. ||| 476 ||| 40245 ||| 40246 ||| 2324 ||| 40247 ||| 12384 ||| 
2021 ||| valid, plausible, and diverse retrosynthesis using tied two-way transformers with latent variables. ||| 40248 ||| 40249 ||| 40250 ||| 40251 ||| 40252 ||| 
2021 ||| molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. ||| 36781 ||| 36782 ||| 36783 ||| 40253 ||| 40254 ||| 40255 ||| 36784 ||| 34181 ||| 
2020 ||| predicting the feasibility of copper(i)-catalyzed alkyne-azide cycloaddition reactions using a recurrent neural network with a self-attention mechanism. ||| 40256 ||| 40257 ||| 40258 ||| 16736 ||| 40259 ||| 20189 ||| 2013 ||| 
2022 ||| protein interaction network reconstruction with a structural gated attention deep model by incorporating network structure information. ||| 40260 ||| 40261 ||| 16602 ||| 40262 ||| 40263 ||| 
2021 ||| transferable multilevel attention neural network for accurate prediction of quantum chemistry properties via multitask learning. ||| 40264 ||| 33717 ||| 40265 ||| 40266 ||| 28214 ||| 8722 ||| 3646 ||| 
2021 ||| applying transformer insulation using weibull extended distribution based on progressive censoring scheme. ||| 40267 ||| 40268 ||| 40269 ||| 40270 ||| 
2017 ||| big data and the attention economy: big data (ubiquity symposium). ||| 40271 ||| 
2021 ||| expression eeg multimodal emotion recognition method based on the bidirectional lstm and attention mechanism. ||| 40272 ||| 40273 ||| 
2020 ||| the eeg-based attention analysis in multimedia m-learning. ||| 40274 ||| 2039 ||| 40275 ||| 
2020 ||| acnnt3: attention-cnn framework for prediction of sequence-based bacterial type iii secreted effectors. ||| 4634 ||| 40276 ||| 40277 ||| 40278 ||| 
2018 ||| a new approach for advertising ctr prediction based on deep neural network via attention mechanism. ||| 31519 ||| 9082 ||| 40279 ||| 40280 ||| 
2020 ||| attention optimization method for eeg via the tgam. ||| 2280 ||| 13410 ||| 
2018 ||| fpan: fine-grained and progressive attention localization network for data retrieval. ||| 37633 ||| 9705 ||| 8718 ||| 40281 ||| 35144 ||| 35145 ||| 
2020 ||| guided soft attention network for classification of breast cancer histopathology images. ||| 40282 ||| 40283 ||| 40284 ||| 40285 ||| 
2020 ||| global pixel transformers for virtual staining of microscopy images. ||| 1199 ||| 6928 ||| 25412 ||| 23491 ||| 
2021 ||| automated skin lesion segmentation via an adaptive dual attention module. ||| 22260 ||| 40286 ||| 40287 ||| 22263 ||| 8636 ||| 
2020 ||| attention by selection: a deep selective attention approach to breast cancer classification. ||| 15599 ||| 15600 ||| 15601 ||| 15602 ||| 15603 ||| 15604 ||| 15605 ||| 13429 ||| 15606 ||| 
2021 ||| ca-net: comprehensive attention convolutional neural networks for explainable medical image segmentation. ||| 27343 ||| 15623 ||| 20159 ||| 1858 ||| 37751 ||| 37752 ||| 7111 ||| 14874 ||| 27462 ||| 15555 ||| 
2021 ||| lcanet: learnable connected attention network for human identification using dental images. ||| 40288 ||| 40289 ||| 40290 ||| 40291 ||| 37314 ||| 40292 ||| 37313 ||| 340 ||| 
2019 ||| attention to lesion: lesion-aware convolutional neural network for retinal optical coherence tomography image classification. ||| 6749 ||| 31736 ||| 36004 ||| 40293 ||| 40294 ||| 40295 ||| 
2020 ||| learning an attention model for robust 2-d/3-d registration using point-to-plane correspondences. ||| 40296 ||| 8349 ||| 40297 ||| 40298 ||| 6818 ||| 
2021 ||| cabnet: category attention block for imbalanced diabetic retinopathy grading. ||| 40299 ||| 4003 ||| 7015 ||| 333 ||| 4056 ||| 
2021 ||| contour transformer network for one-shot segmentation of anatomical structures. ||| 8159 ||| 19070 ||| 27531 ||| 32908 ||| 27492 ||| 32909 ||| 307 ||| 705 ||| 27491 ||| 32910 ||| 32911 ||| 
2021 ||| ala-net: adaptive lesion-aware attention network for 3d colorectal tumor segmentation. ||| 40300 ||| 40301 ||| 40302 ||| 40303 ||| 40304 ||| 40305 ||| 22570 ||| 40306 ||| 13541 ||| 
2021 ||| hierarchical temporal attention network for thyroid nodule recognition using dynamic ceus imaging. ||| 15615 ||| 604 ||| 15617 ||| 15619 ||| 15620 ||| 
2021 ||| 3d multi-attention guided multi-task learning network for automatic gastric tumor segmentation and lymph node classification. ||| 40307 ||| 40308 ||| 31206 ||| 8636 ||| 5476 ||| 18416 ||| 3072 ||| 40309 ||| 40310 ||| 6582 ||| 
2021 ||| deep relation transformer for diagnosing glaucoma with optical coherence tomography and visual field function. ||| 40311 ||| 35698 ||| 9028 ||| 40312 ||| 8783 ||| 40313 ||| 2149 ||| 
2020 ||| canet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading. ||| 38522 ||| 8634 ||| 27802 ||| 978 ||| 1739 ||| 8637 ||| 
2020 ||| accurate screening of covid-19 using attention-based deep 3d multiple instance learning. ||| 40314 ||| 22266 ||| 40315 ||| 19051 ||| 40316 ||| 30640 ||| 40317 ||| 781 ||| 
2021 ||| self-supervised attention mechanism for pediatric bone age assessment with efficient weak annotation. ||| 18070 ||| 18071 ||| 17860 ||| 
2020 ||| prior-attention residual learning for more discriminative covid-19 screening in ct images. ||| 1224 ||| 24236 ||| 40318 ||| 24240 ||| 40319 ||| 40320 ||| 12334 ||| 4811 ||| 24239 ||| 
2020 ||| anatomical attention guided deep networks for roi segmentation of brain mr images. ||| 19819 ||| 5279 ||| 15620 ||| 27373 ||| 
2020 ||| dual-sampling attention network for diagnosis of covid-19 from community acquired pneumonia. ||| 32718 ||| 15645 ||| 36664 ||| 36665 ||| 1235 ||| 36666 ||| 36667 ||| 36668 ||| 5101 ||| 9705 ||| 26582 ||| 36669 ||| 9191 ||| 36670 ||| 27813 ||| 27538 ||| 577 ||| 18051 ||| 
2021 ||| dual attention multi-instance deep learning for alzheimer's disease diagnosis with structural mri. ||| 37235 ||| 19819 ||| 40321 ||| 37085 ||| 15620 ||| 
2021 ||| mama net: multi-scale attention memory autoencoder network for anomaly detection. ||| 28489 ||| 4600 ||| 28490 ||| 35738 ||| 29141 ||| 28491 ||| 
2020 ||| attention-diffusion-bilinear neural network for brain network analysis. ||| 40321 ||| 27723 ||| 3279 ||| 15620 ||| 
2020 ||| zoom in lesions for better diagnosis: attention guided deformation network for wce image classification. ||| 16744 ||| 25547 ||| 21829 ||| 
2019 ||| tetris: template transformer networks for image segmentation with shape priors. ||| 27945 ||| 39881 ||| 40322 ||| 27844 ||| 27948 ||| 
2020 ||| attentionboost: learning what to attend for gland segmentation in histopathological images by boosting fully convolutional networks. ||| 39252 ||| 39253 ||| 39254 ||| 
2021 ||| learning to segment from scribbles using multi-scale adversarial attention gates. ||| 35319 ||| 35320 ||| 13369 ||| 
2021 ||| nhbs-net: a feature fusion attention network for ultrasound neonatal hip bone segmentation. ||| 40323 ||| 40324 ||| 14037 ||| 40325 ||| 977 ||| 24143 ||| 3750 ||| 40326 ||| 18051 ||| 
2022 ||| semi-supervised segmentation of radiation-induced pulmonary fibrosis from lung ct scans with multi-scale guided dense attention. ||| 15623 ||| 33636 ||| 33637 ||| 33638 ||| 33639 ||| 30726 ||| 33640 ||| 1749 ||| 32188 ||| 15555 ||| 
2022 ||| global-local transformer for brain age estimation. ||| 31319 ||| 31327 ||| 27851 ||| 
2021 ||| limited view tomographic reconstruction using a cascaded residual dense spatial-channel attention network with projection data fidelity layer. ||| 21551 ||| 27778 ||| 15551 ||| 38116 ||| 
2021 ||| diagnostic regions attention network (dra-net) for histopathology wsi recommendation and retrieval. ||| 40327 ||| 6717 ||| 40328 ||| 15657 ||| 6716 ||| 40329 ||| 40330 ||| 40331 ||| 
2021 ||| learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction. ||| 247 ||| 36365 ||| 27607 ||| 8906 ||| 31317 ||| 8660 ||| 
2019 ||| attention residual learning for skin lesion classification. ||| 27886 ||| 27885 ||| 9199 ||| 6335 ||| 
2019 ||| learning where to see: a novel attention model for automated immunohistochemical scoring. ||| 15 ||| 34697 ||| 
2021 ||| multi-modal retinal image classification with modality-specific attention network. ||| 40332 ||| 40333 ||| 6749 ||| 40334 ||| 
2020 ||| sacnn: self-attention convolutional neural network for low-dose ct denoising with self-supervised perceptual loss network. ||| 5860 ||| 40335 ||| 11405 ||| 40336 ||| 7204 ||| 
2021 ||| learning hierarchical attention for weakly-supervised chest x-ray abnormality localization and diagnosis. ||| 32718 ||| 1745 ||| 1744 ||| 2441 ||| 15645 ||| 27596 ||| 577 ||| 15532 ||| 
2021 ||| artifact and detail attention generative adversarial networks for low-dose ct denoising. ||| 7605 ||| 40337 ||| 40338 ||| 40339 ||| 40340 ||| 40341 ||| 
2020 ||| a large-scale database and a cnn model for attention-based glaucoma detection. ||| 19373 ||| 18741 ||| 19375 ||| 438 ||| 12608 ||| 19374 ||| 18740 ||| 40342 ||| 40343 ||| 
2020 ||| chordiogram image descriptor based on visual attention model for image retrieval. ||| 40344 ||| 40345 ||| 40346 ||| 
2021 ||| the principle of intelligent switch composition and algorithm of the built-in electronic voltage transformer. ||| 40347 ||| 40348 ||| 40349 ||| 1448 ||| 40350 ||| 40351 ||| 
2021 ||| key n -gram extractions and analyses of different registers based on attention network. ||| 887 ||| 5439 ||| 1253 ||| 40352 ||| 17417 ||| 
2022 ||| neural acoustic-phonetic approach for speaker verification with phonetic attention mask. ||| 14464 ||| 13581 ||| 14562 ||| 12494 ||| 
2020 ||| canet: concatenated attention neural network for image restoration. ||| 28857 ||| 25264 ||| 34031 ||| 28858 ||| 
2021 ||| macro: multi-attention convolutional recurrent model for subject-independent erp detection. ||| 40353 ||| 40354 ||| 40355 ||| 40356 ||| 40357 ||| 
2018 ||| end-to-end feature integration for correlation filter tracking with channel attention. ||| 29491 ||| 40358 ||| 40359 ||| 7408 ||| 
2019 ||| multi-level dual-attention based cnn for macular optical coherence tomography classification. ||| 28701 ||| 2651 ||| 16408 ||| 
2019 ||| a skip attention mechanism for monaural singing voice separation. ||| 40360 ||| 40361 ||| 40362 ||| 40363 ||| 11418 ||| 
2021 ||| ata: attentional non-linear activation function approximation for vlsi-based neural networks. ||| 40364 ||| 40365 ||| 40366 ||| 
2021 ||| graphtte: travel time estimation based on attention-spatiotemporal graphs. ||| 3304 ||| 40367 ||| 40368 ||| 2067 ||| 
2020 ||| memory attention: robust alignment using gating mechanism for end-to-end speech synthesis. ||| 40369 ||| 34479 ||| 40370 ||| 10966 ||| 
2021 ||| interactive multimodal attention network for emotion recognition in conversation. ||| 40371 ||| 40372 ||| 40373 ||| 19742 ||| 
2020 ||| spectro-temporal attention-based voice activity detection. ||| 40374 ||| 40375 ||| 8559 ||| 8560 ||| 
2022 ||| a nested u-net with self-attention and dense connectivity for monaural speech enhancement. ||| 40376 ||| 40377 ||| 40378 ||| 
2021 ||| wavelet multi-level attention capsule network for texture classification. ||| 40379 ||| 40380 ||| 4634 ||| 
2020 ||| interlayer selective attention network for robust personalized wake-up word detection. ||| 40381 ||| 40382 ||| 14585 ||| 14397 ||| 
2022 ||| biattnnet: bilateral attention for improving real-time semantic segmentation. ||| 40383 ||| 13824 ||| 40384 ||| 
2022 ||| cooperative light-field image super-resolution based on multi-modality embedding and fusion with frequency attention. ||| 40385 ||| 40386 ||| 40387 ||| 40388 ||| 
2022 ||| heterogeneous attention nested u-shaped network for blur detection. ||| 40389 ||| 40390 ||| 40391 ||| 6116 ||| 7419 ||| 
2020 ||| a stereo attention module for stereo image super-resolution. ||| 19307 ||| 19139 ||| 19138 ||| 40392 ||| 19143 ||| 7271 ||| 
2022 ||| hybrid autoregressive and non-autoregressive transformer models for speech recognition. ||| 12241 ||| 12242 ||| 12041 ||| 3364 ||| 12244 ||| 
2021 ||| a deep feature fusion network based on multiple attention mechanisms for joint iris-periocular biometric recognition. ||| 40393 ||| 40394 ||| 782 ||| 
2021 ||| inheritance attention matrix-based universal adversarial perturbations on vision transformers. ||| 40395 ||| 40396 ||| 25030 ||| 40397 ||| 40398 ||| 
2021 ||| hanme: hierarchical attention network for singing melody extraction. ||| 4520 ||| 4112 ||| 5250 ||| 3337 ||| 
2022 ||| rgtransformer: region-graph transformer for image representation and few-shot classification. ||| 2632 ||| 40399 ||| 5755 ||| 
2018 ||| voice activity detection using an adaptive context attention model. ||| 32429 ||| 40400 ||| 
2022 ||| mtt: multi-scale temporal transformer for skeleton-based action recognition. ||| 27439 ||| 40401 ||| 24565 ||| 
2021 ||| non-autoregressive transformer for speech recognition. ||| 12579 ||| 3549 ||| 12581 ||| 12582 ||| 12580 ||| 12583 ||| 
2019 ||| visual attention network for low-dose ct. ||| 37312 ||| 37313 ||| 37314 ||| 37315 ||| 8534 ||| 340 ||| 
2021 ||| pixel-attention cnn with color correlation loss for color image denoising. ||| 19949 ||| 11420 ||| 40402 ||| 7785 ||| 
2021 ||| disentangling 3d/4d facial affect recognition with faster multi-view transformer. ||| 40403 ||| 33479 ||| 33480 ||| 
2022 ||| apan: across-scale progressive attention network for single image deraining. ||| 3304 ||| 11802 ||| 17601 ||| 40404 ||| 40405 ||| 
2018 ||| 3-d convolutional recurrent neural networks with attention model for speech emotion recognition. ||| 40406 ||| 40407 ||| 6271 ||| 14048 ||| 
2021 ||| dilated-scale-aware category-attention convnet for multi-class object counting. ||| 7804 ||| 36739 ||| 36821 ||| 40408 ||| 1484 ||| 
2022 ||| light field image super-resolution with transformers. ||| 39623 ||| 19139 ||| 19138 ||| 19142 ||| 23020 ||| 
2021 ||| improving super-resolution performance using meta-attention layers. ||| 32747 ||| 32748 ||| 32749 ||| 32750 ||| 32751 ||| 
2020 ||| dually connected deraining net using pixel-wise attention. ||| 40409 ||| 40410 ||| 3304 ||| 40405 ||| 
2019 ||| three-stream network with bidirectional self-attention for action recognition in extreme low resolution videos. ||| 2392 ||| 2242 ||| 2243 ||| 2244 ||| 
2021 ||| graph attention networks adjusted bi-lstm for video summarization. ||| 40411 ||| 3049 ||| 9243 ||| 40412 ||| 5888 ||| 
2022 ||| actionness-guided transformer for anchor-free temporal action localization. ||| 40413 ||| 2477 ||| 1222 ||| 2398 ||| 
2022 ||| hashformer: vision transformer based deep hashing for image retrieval. ||| 4003 ||| 1770 ||| 40414 ||| 40415 ||| 
2020 ||| bi-directional seed attention network for interactive image segmentation. ||| 40416 ||| 18219 ||| 
2019 ||| a convolutional recurrent attention model for subject-independent eeg signal analysis. ||| 773 ||| 771 ||| 770 ||| 40417 ||| 
2020 ||| curriculum enhanced supervised attention network for person re-identification. ||| 22444 ||| 22575 ||| 3091 ||| 22449 ||| 
2021 ||| ga-net: global attention network for point cloud semantic segmentation. ||| 36975 ||| 36976 ||| 
2021 ||| transrppg: remote photoplethysmography transformer for 3d mask face presentation attack detection. ||| 33001 ||| 33479 ||| 1703 ||| 33480 ||| 
2021 ||| a unimodal reinforced transformer with time squeeze fusion for multimodal sentiment analysis. ||| 40418 ||| 40419 ||| 5746 ||| 
2021 ||| multi-attention network for unsupervised video object segmentation. ||| 40420 ||| 40421 ||| 40422 ||| 
2020 ||| adaptively leverage unlabeled tracklets based on part attention model for few-example re-id. ||| 40423 ||| 11220 ||| 11221 ||| 
2022 ||| dropdim: a regularization method for transformer networks. ||| 3386 ||| 40077 ||| 40424 ||| 40425 ||| 
2021 ||| self-guided body part alignment with relation transformers for occluded person re-identification. ||| 40426 ||| 40427 ||| 40428 ||| 31373 ||| 17688 ||| 
2021 ||| channel and space attention neural network for image denoising. ||| 9149 ||| 28729 ||| 472 ||| 
2021 ||| complemental attention multi-feature fusion network for fine-grained classification. ||| 30857 ||| 40429 ||| 28648 ||| 438 ||| 6799 ||| 
2021 ||| manet: multi-scale attention network for correspondence learning. ||| 40430 ||| 40431 ||| 189 ||| 40432 ||| 
2021 ||| a convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement. ||| 40376 ||| 40377 ||| 40378 ||| 
2018 ||| residual lstm attention network for object tracking. ||| 40433 ||| 19147 ||| 
2020 ||| corrections to "three-stream network with bidirectional self-attention for action recognition in extreme low resolution videos". ||| 2392 ||| 2242 ||| 2243 ||| 2244 ||| 
2020 ||| image editing via segmentation guided self-attention network. ||| 5086 ||| 40434 ||| 14707 ||| 22577 ||| 5088 ||| 
2018 ||| rescoring of n-best hypotheses using top-down selective attention for automatic speech recognition. ||| 12050 ||| 40435 ||| 40436 ||| 40437 ||| 38593 ||| 
2022 ||| acoustic word embedding based on multi-head attention quadruplet network. ||| 40438 ||| 4646 ||| 40439 ||| 40440 ||| 
2021 ||| a two-stage spatiotemporal attention convolution network for continuous dimensional emotion recognition from facial video. ||| 5888 ||| 40441 ||| 5885 ||| 6879 ||| 5890 ||| 
2020 ||| attention aggregation encoder-decoder network framework for stereo matching. ||| 28689 ||| 19921 ||| 40442 ||| 2304 ||| 
2017 ||| redesign of the attention process of patients with rheumatologic diseases: assessing the performance with analytic hierarchy process. ||| 40443 ||| 40444 ||| 40445 ||| 7033 ||| 
2021 ||| simultaneous remote monitoring of transformers' ambient parameters by using iot. ||| 40446 ||| 40447 ||| 40448 ||| 
2021 ||| add: attention-based deepfake detection approach. ||| 40449 ||| 40450 ||| 
2022 ||| the currency of the attentional economy: the uses and abuses of attention in our world. ||| 40451 ||| 
2022 ||| a deep learning approach for classifying vulnerability descriptions using self attention based neural network. ||| 40452 ||| 40453 ||| 40454 ||| 
2022 ||| nonlocal convolutional block attention module vnet for gliomas automatic segmentation. ||| 40455 ||| 9990 ||| 40456 ||| 40457 ||| 40458 ||| 40459 ||| 
2021 ||| automatic covid-19 ct segmentation using u-net integrated spatial and channel attention mechanism. ||| 15607 ||| 2693 ||| 15610 ||| 15608 ||| 
2020 ||| spatio-temporal context based recurrent visual attention model for lymph node detection. ||| 40460 ||| 40461 ||| 
2022 ||| breast cancer histopathological image classification using attention high-order deep network. ||| 16910 ||| 11988 ||| 15534 ||| 2304 ||| 
2021 ||| deep learning super-resolution electron microscopy based on deep residual attention network. ||| 23367 ||| 40462 ||| 18588 ||| 40463 ||| 
2022 ||| att2resnet: a deep attention-based approach for melanoma skin cancer classification. ||| 40464 ||| 40465 ||| 40466 ||| 
2022 ||| adopting attention-mechanisms for neural logic rule layers. ||| 40467 ||| 34386 ||| 40468 ||| 
2021 ||| exploring the distribution regularities of user attention and sentiment toward product aspects in online reviews. ||| 40469 ||| 3192 ||| 40470 ||| 
2019 ||| rumour verification through recurring information and an inner-attention mechanism. ||| 33978 ||| 40471 ||| 26435 ||| 14980 ||| 
2019 ||| collective attention patterns under controlled conditions. ||| 40472 ||| 40473 ||| 40474 ||| 40475 ||| 
2022 ||| predicate transformer semantics for hybrid systems. ||| 38793 ||| 38794 ||| 38795 ||| 
2018 ||| a multi-criteria computer package for power transformer fault detection and diagnosis. ||| 40476 ||| 40477 ||| 
2022 ||| eigat: incorporating global information in local attention for knowledge representation learning. ||| 3473 ||| 40478 ||| 3474 ||| 40479 ||| 34920 ||| 3475 ||| 3476 ||| 3477 ||| 
2021 ||| coarse-to-fine: a dual-view attention network for click-through rate prediction. ||| 2009 ||| 40480 ||| 40481 ||| 836 ||| 
2022 ||| holistic graph neural networks based on a global-based attention mechanism. ||| 40482 ||| 40483 ||| 40484 ||| 
2022 ||| ham-net: predictive business process monitoring with a hierarchical attention mechanism. ||| 5986 ||| 5987 ||| 5989 ||| 5988 ||| 
2022 ||| association rules enhanced knowledge graph attention network. ||| 2778 ||| 8447 ||| 38168 ||| 
2022 ||| self-attention neural architecture search for semantic image segmentation. ||| 40485 ||| 18063 ||| 6288 ||| 40486 ||| 2628 ||| 40487 ||| 
2021 ||| spatio-attention embedded recurrent neural network for air quality prediction. ||| 17209 ||| 40488 ||| 15227 ||| 
2018 ||| domain attention model for multi-domain sentiment classification. ||| 10472 ||| 10471 ||| 3755 ||| 4792 ||| 2795 ||| 
2021 ||| channel-spatial attention-based pan-sharpening of very high-resolution satellite images. ||| 40489 ||| 40490 ||| 
2021 ||| attentional memory network with correlation-based embedding for time-aware poi recommendation. ||| 40491 ||| 4072 ||| 4073 ||| 4071 ||| 4067 ||| 
2021 ||| an end-to-end atrial fibrillation detection by a novel residual-based temporal attention convolutional neural network with exponential nonlinearity loss. ||| 22355 ||| 2054 ||| 15621 ||| 
2022 ||| explainable attention guided adversarial deep network for 3d radiotherapy dose distribution prediction. ||| 40492 ||| 40493 ||| 40190 ||| 40494 ||| 18047 ||| 27721 ||| 25649 ||| 25648 ||| 247 ||| 
2021 ||| history-based attention in seq2seq model for multi-label text classification. ||| 40495 ||| 1329 ||| 5332 ||| 40496 ||| 2619 ||| 28727 ||| 
2022 ||| phrase dependency relational graph attention network for aspect-based sentiment analysis. ||| 887 ||| 883 ||| 1253 ||| 40352 ||| 888 ||| 
2021 ||| pose transfer generation with semantic parsing attention network for person re-identification. ||| 33336 ||| 6534 ||| 40497 ||| 33550 ||| 8260 ||| 
2022 ||| gated attention fusion network for multimodal sentiment classification. ||| 40498 ||| 1305 ||| 40499 ||| 40500 ||| 
2021 ||| dnnattention: a deep neural network and attention based architecture for cross project defect number prediction. ||| 40501 ||| 40502 ||| 
2019 ||| ea-lstm: evolutionary attention-based lstm for time series prediction. ||| 39285 ||| 6507 ||| 39286 ||| 5252 ||| 4400 ||| 
2020 ||| deep learning models and datasets for aspect term sentiment classification: implementing holistic recurrent attention on target-dependent memories. ||| 40503 ||| 40504 ||| 40505 ||| 
2021 ||| positionless aspect based sentiment analysis using attention mechanism. ||| 40506 ||| 40507 ||| 14258 ||| 14257 ||| 
2021 ||| iit-gat: instance-level image transformation via unsupervised generative attention networks with disentangled representations. ||| 40508 ||| 40509 ||| 28607 ||| 2018 ||| 8850 ||| 
2022 ||| mols-net: multi-organ and lesion segmentation network based on sequence feature pyramid and attention mechanism for aortic dissection diagnosis. ||| 34204 ||| 40510 ||| 40511 ||| 40512 ||| 40513 ||| 
2022 ||| multi-view informed attention-based model for irony and satire detection in spanish variants. ||| 40514 ||| 10522 ||| 852 ||| 40515 ||| 
2020 ||| multi-scale generative adversarial inpainting network based on cross-layer attention transfer mechanism. ||| 40508 ||| 32936 ||| 2018 ||| 8850 ||| 
2020 ||| relation classification via knowledge graph enhanced transformer encoder. ||| 40516 ||| 40517 ||| 37478 ||| 978 ||| 37481 ||| 
2021 ||| lgattnet: automatic micro-expression detection using dual-stream local and global attentions. ||| 40518 ||| 40519 ||| 18483 ||| 40520 ||| 11333 ||| 40521 ||| 
2020 ||| visual question answering via combining inferential attention and semantic space mapping. ||| 4477 ||| 5320 ||| 19722 ||| 40522 ||| 19724 ||| 843 ||| 
2021 ||| st-lbagan: spatio-temporal learnable bidirectional attention generative adversarial networks for missing traffic data imputation. ||| 40523 ||| 40078 ||| 40524 ||| 17578 ||| 1705 ||| 
2021 ||| dual-graph convolutional network based on band attention and sparse constraint for hyperspectral band selection. ||| 6738 ||| 6739 ||| 9455 ||| 397 ||| 30317 ||| 30442 ||| 400 ||| 
2021 ||| deep transformer modeling via grouping skip connection for neural machine translation. ||| 26007 ||| 3564 ||| 1254 ||| 
2021 ||| see, hear, read: leveraging multimodality with guided attention for abstractive text summarization. ||| 37867 ||| 13313 ||| 37868 ||| 3835 ||| 
2020 ||| an attention-guided and prior-embedded approach with multi-task learning for shadow detection. ||| 40525 ||| 8440 ||| 40526 ||| 24356 ||| 40527 ||| 
2021 ||| light-weight uav object tracking network based on strategy gradient and attention mechanism. ||| 29369 ||| 29370 ||| 29371 ||| 40528 ||| 952 ||| 
2022 ||| multi-view self-attention networks. ||| 3288 ||| 3037 ||| 3039 ||| 3040 ||| 
2022 ||| elementary discourse units with sparse attention for multi-label emotion classification. ||| 1107 ||| 15249 ||| 
2020 ||| siamatt: siamese attention network for visual tracking. ||| 5492 ||| 40529 ||| 40530 ||| 40531 ||| 
2020 ||| effective person re-identification by self-attention model guided feature learning. ||| 438 ||| 5061 ||| 5062 ||| 
2022 ||| an explainable recommendation framework based on an improved knowledge graph attention network with massive volumes of side information. ||| 40532 ||| 40533 ||| 40534 ||| 
2021 ||| hackrl: reinforcement learning with hierarchical attention for cross-graph knowledge fusion and collaborative reasoning. ||| 40535 ||| 5957 ||| 40536 ||| 40537 ||| 3002 ||| 40538 ||| 1796 ||| 11354 ||| 
2020 ||| dam: transformer-based relation detection for question answering over knowledge base. ||| 40539 ||| 40540 ||| 
2022 ||| double-branch dehazing network based on self-calibrated attentional convolution. ||| 40541 ||| 40542 ||| 5062 ||| 7828 ||| 
2021 ||| enhancing transformer-based language models with commonsense representations for knowledge-driven machine comprehension. ||| 40543 ||| 40544 ||| 11804 ||| 40545 ||| 29902 ||| 40546 ||| 
2022 ||| video sentiment analysis with bimodal information-augmented multi-head attention. ||| 39066 ||| 39067 ||| 12503 ||| 39068 ||| 40547 ||| 40548 ||| 39069 ||| 39070 ||| 
2020 ||| hasvrec: a modularized hierarchical attention-based scholarly venue recommender system. ||| 31981 ||| 7364 ||| 31983 ||| 
2021 ||| hierarchical attention link prediction neural network. ||| 1176 ||| 1178 ||| 40549 ||| 
2021 ||| attention uncovers task-relevant semantics in emotional narrative understanding. ||| 20975 ||| 20974 ||| 20976 ||| 
2018 ||| audio classification using attention-augmented convolutional neural network. ||| 2280 ||| 40550 ||| 31205 ||| 
2021 ||| joint knowledge-powered topic level attention for a convolutional text summarization model. ||| 522 ||| 523 ||| 524 ||| 
2021 ||| hierarchical social similarity-guided model with dual-mode attention for session-based recommendation. ||| 40551 ||| 2764 ||| 11099 ||| 336 ||| 40552 ||| 40553 ||| 
2022 ||| method and dataset entity mining in scientific literature: a cnn + bilstm model with self-attention. ||| 34910 ||| 1422 ||| 15249 ||| 34911 ||| 1419 ||| 885 ||| 1129 ||| 34912 ||| 34913 ||| 
2021 ||| tagat: type-aware graph attention networks for reasoning over knowledge graphs. ||| 40554 ||| 31572 ||| 40555 ||| 40556 ||| 40557 ||| 
2021 ||| dtdr-alstm: extracting dynamic time-delays to reconstruct multivariate data for improving attention-based lstm industrial time series prediction models. ||| 40558 ||| 2760 ||| 40559 ||| 21903 ||| 40560 ||| 1199 ||| 
2021 ||| soft-self and hard-cross graph attention network for knowledge graph entity alignment. ||| 40561 ||| 40562 ||| 40563 ||| 6170 ||| 
2021 ||| tau: transferable attention u-net for optic disc and cup segmentation. ||| 24349 ||| 18015 ||| 4646 ||| 40564 ||| 19486 ||| 15243 ||| 
2020 ||| tsasnet: tooth segmentation on dental panoramic x-ray images by two-stage attention segmentation network. ||| 3226 ||| 14647 ||| 11223 ||| 1305 ||| 40565 ||| 2320 ||| 8850 ||| 
2020 ||| multi-view factorization machines for mobile app recommendation based on hierarchical attention. ||| 11632 ||| 1429 ||| 12373 ||| 3891 ||| 1094 ||| 1236 ||| 
2019 ||| topic-enhanced emotional conversation generation with attention mechanism. ||| 40566 ||| 40567 ||| 11707 ||| 11710 ||| 
2021 ||| dual attention-based method for occluded person re-identification. ||| 40568 ||| 30624 ||| 40569 ||| 
2021 ||| iterative graph attention memory network for cross-modal retrieval. ||| 40570 ||| 40571 ||| 40572 ||| 40573 ||| 
2020 ||| hagerec: hierarchical attention graph convolutional network incorporating knowledge graph for explainable recommendation. ||| 40574 ||| 825 ||| 
2022 ||| le-gan: unsupervised low-light image enhancement network using attention module and identity invariant loss. ||| 1714 ||| 40575 ||| 16206 ||| 8678 ||| 
2020 ||| hybrid time-aligned and context attention for time series prediction. ||| 40576 ||| 1166 ||| 8463 ||| 
2022 ||| dynamic graph convolutional autoencoder with node-attribute-wise attention for kidney and tumor segmentation from ct volumes. ||| 24119 ||| 421 ||| 22348 ||| 24117 ||| 10536 ||| 24118 ||| 14786 ||| 
2020 ||| predicting taxi demands via an attention-based convolutional recurrent neural network. ||| 15198 ||| 15197 ||| 11190 ||| 1505 ||| 
2020 ||| multi-domain modeling of atrial fibrillation detection with twin attentional convolutional long short-term memory neural networks. ||| 28527 ||| 28530 ||| 40577 ||| 40578 ||| 28533 ||| 
2020 ||| sparse attention based separable dilated convolutional neural network for targeted sentiment analysis. ||| 28695 ||| 4754 ||| 28698 ||| 28697 ||| 
2021 ||| hits-based attentional neural model for abstractive summarization. ||| 29552 ||| 40579 ||| 40580 ||| 29553 ||| 40581 ||| 
2020 ||| cross-modal recipe retrieval via parallel- and cross-attention networks learning. ||| 40582 ||| 40583 ||| 29375 ||| 9631 ||| 
2021 ||| a novel transfer diagnosis method under unbalanced sample based on discrete-peak joint attention enhancement mechanism. ||| 3096 ||| 40584 ||| 40585 ||| 40586 ||| 40587 ||| 40588 ||| 
2021 ||| fgcan: filter-based gated contextual attention network for event detection. ||| 3648 ||| 8356 ||| 8207 ||| 1288 ||| 
2022 ||| predrann: the spatiotemporal attention convolution recurrent neural network for precipitation nowcasting. ||| 30481 ||| 40589 ||| 40590 ||| 25166 ||| 7466 ||| 
2020 ||| lightweight multi-scale residual networks with attention for image super-resolution. ||| 5791 ||| 29587 ||| 40591 ||| 40592 ||| 
2022 ||| self-attention-based adaptive remaining useful life prediction for igbt with monte carlo dropout. ||| 28531 ||| 28530 ||| 40593 ||| 40594 ||| 40577 ||| 28533 ||| 
2021 ||| inphynet: leveraging attention-based multitask recurrent networks for multi-label physics text classification. ||| 40595 ||| 15048 ||| 40596 ||| 3835 ||| 
2021 ||| eaa-net: a novel edge assisted attention network for single image dehazing. ||| 1589 ||| 40597 ||| 6861 ||| 40172 ||| 40598 ||| 40599 ||| 35497 ||| 
2021 ||| reasoning like humans: on dynamic attention prior in image captioning. ||| 16446 ||| 10525 ||| 315 ||| 19850 ||| 13676 ||| 
2022 ||| joint-attention feature fusion network and dual-adaptive nms for object detection. ||| 17877 ||| 40600 ||| 40510 ||| 34204 ||| 18134 ||| 
2021 ||| knowledge-enhanced recommendation using item embedding and path attention. ||| 40601 ||| 728 ||| 40602 ||| 8974 ||| 8978 ||| 
2021 ||| amff: a new attention-based multi-feature fusion method for intention recognition. ||| 5010 ||| 21206 ||| 
2021 ||| graph neural networks with multiple kernel ensemble attention. ||| 40603 ||| 11333 ||| 
2022 ||| diagnosis of alzheimer's disease via an attention-based multi-scale convolutional neural network. ||| 2801 ||| 40604 ||| 28938 ||| 40605 ||| 29073 ||| 29074 ||| 
2021 ||| channel and spatial attention based deep object co-segmentation. ||| 11126 ||| 40606 ||| 18933 ||| 40607 ||| 40608 ||| 1913 ||| 
2021 ||| jam: joint attention model for next event recommendation in event-based social networks. ||| 25167 ||| 40609 ||| 29203 ||| 25170 ||| 40610 ||| 40611 ||| 
2022 ||| publication classification prediction via citation attention fusion based on dynamic relations. ||| 40612 ||| 40613 ||| 40614 ||| 40615 ||| 
2019 ||| acnn-fm: a novel recommender with attention-based convolutional neural network and factorization machines. ||| 8427 ||| 22375 ||| 29329 ||| 40616 ||| 13738 ||| 22376 ||| 31174 ||| 
2022 ||| discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer. ||| 33186 ||| 17654 ||| 3836 ||| 3835 ||| 
2021 ||| personalized sentiment classification of customer reviews via an interactive attributes attention model. ||| 3312 ||| 3313 ||| 3315 ||| 
2021 ||| retrieval-enhanced adversarial training with dynamic memory-augmented attention for image paragraph captioning. ||| 11640 ||| 1081 ||| 3199 ||| 1082 ||| 15906 ||| 11643 ||| 
2020 ||| direction-sensitive relation extraction using bi-sdp attention model. ||| 40617 ||| 13742 ||| 40618 ||| 13744 ||| 13743 ||| 
2020 ||| siamese attentional keypoint network for high performance visual tracking. ||| 2170 ||| 33956 ||| 2355 ||| 31970 ||| 31546 ||| 2349 ||| 
2021 ||| prior-knowledge and attention based meta-learning for few-shot learning. ||| 39926 ||| 40619 ||| 36298 ||| 39927 ||| 13413 ||| 40620 ||| 19282 ||| 337 ||| 
2020 ||| attention-aware scoring learning for person re-identification. ||| 31633 ||| 31631 ||| 31630 ||| 31629 ||| 31632 ||| 
2021 ||| hierarchical accumulation network with grid attention for image super-resolution. ||| 6070 ||| 40621 ||| 
2019 ||| visual-textual sentiment classification with bi-directional multi-level attention networks. ||| 24731 ||| 19722 ||| 5320 ||| 18202 ||| 9991 ||| 843 ||| 19723 ||| 
2021 ||| kernel multi-attention neural network for knowledge graph embedding. ||| 40622 ||| 28880 ||| 932 ||| 28881 ||| 
2021 ||| multiscale fused network with additive channel-spatial attention for image segmentation. ||| 40623 ||| 29586 ||| 29587 ||| 40591 ||| 40592 ||| 729 ||| 
2021 ||| gated multi-attention representation in reinforcement learning. ||| 40624 ||| 40625 ||| 5074 ||| 
2021 ||| asrnn: a recurrent neural network with an attention model for sequence labeling. ||| 677 ||| 30754 ||| 40626 ||| 40627 ||| 
2021 ||| learning hyperbolic attention-based embeddings for link prediction in knowledge graphs. ||| 29533 ||| 40628 ||| 29532 ||| 40629 ||| 15562 ||| 
2022 ||| gar-net: a graph attention reasoning network for conversation understanding. ||| 623 ||| 19484 ||| 40630 ||| 40631 ||| 40632 ||| 622 ||| 
2021 ||| co-attention networks based on aspect and context for aspect-level sentiment analysis. ||| 40633 ||| 40634 ||| 33964 ||| 13249 ||| 
2020 ||| adrl: an attention-based deep reinforcement learning framework for knowledge graph reasoning. ||| 6627 ||| 40635 ||| 4470 ||| 
2020 ||| decab-lstm: deep contextualized attentional bidirectional lstm for cancer hallmark classification. ||| 40636 ||| 4323 ||| 40637 ||| 40638 ||| 
2021 ||| quantum probability-inspired graph attention network for modeling complex text interaction. ||| 40639 ||| 20658 ||| 20660 ||| 
2020 ||| recurrent neural network with pooling operation and attention mechanism for sentiment analysis: a multi-task learning approach. ||| 11156 ||| 40640 ||| 40641 ||| 24822 ||| 40642 ||| 3477 ||| 
2021 ||| a novel network with multiple attention mechanisms for aspect-level sentiment analysis. ||| 32920 ||| 40643 ||| 16834 ||| 1419 ||| 
2020 ||| crga: homographic pun detection with a contextualized-representation: gated attention network. ||| 8973 ||| 8974 ||| 8967 ||| 8975 ||| 8976 ||| 8978 ||| 
2022 ||| multi-label modality enhanced attention based self-supervised deep cross-modal hashing. ||| 40644 ||| 5126 ||| 40645 ||| 40646 ||| 
2022 ||| graph transformer network with temporal kernel attention for skeleton-based action recognition. ||| 40647 ||| 3386 ||| 436 ||| 40648 ||| 
2022 ||| 3d hierarchical dual-attention fully convolutional networks with hybrid losses for diverse glioma segmentation. ||| 40649 ||| 40650 ||| 247 ||| 17238 ||| 40651 ||| 
2020 ||| attention deep neural network for lane marking detection. ||| 40652 ||| 17665 ||| 38991 ||| 40653 ||| 
2022 ||| semi-supervised npc segmentation with uncertainty and attention guided consistency. ||| 40654 ||| 17180 ||| 40493 ||| 40494 ||| 27722 ||| 27721 ||| 25649 ||| 25648 ||| 247 ||| 
2021 ||| accurate and explainable recommendation via hierarchical attention network oriented towards crowd intelligence. ||| 497 ||| 40655 ||| 40656 ||| 170 ||| 11105 ||| 4121 ||| 
2019 ||| context-aware emotion cause analysis with multi-attention-based neural network. ||| 1309 ||| 1311 ||| 1312 ||| 8426 ||| 
2022 ||| mixhead: breaking the low-rank bottleneck in multi-head attention language models. ||| 19052 ||| 40657 ||| 40658 ||| 16563 ||| 40659 ||| 18512 ||| 
2021 ||| usevis: visual analytics of attention-based neural embedding in information retrieval. ||| 40660 ||| 28008 ||| 40661 ||| 40662 ||| 28010 ||| 40663 ||| 
2020 ||| a deep learning method based on an attention mechanism for wireless network traffic prediction. ||| 765 ||| 40664 ||| 8762 ||| 40665 ||| 
2022 ||| small target recognition using dynamic time warping and visual attention. ||| 25030 ||| 40666 ||| 40667 ||| 
2022 ||| an attention enhanced cross-modal image-sound mutual generation model for birds. ||| 40112 ||| 12668 ||| 40668 ||| 40669 ||| 
2021 ||| predicting stance polarity and intensity in cyber argumentation with deep bidirectional transformers. ||| 40670 ||| 40671 ||| 40672 ||| 
2020 ||| bert-caps: a transformer-based capsule network for tweet act classification. ||| 864 ||| 40673 ||| 404 ||| 405 ||| 
2022 ||| the deep features and attention mechanism-based method to dish healthcare under social iot systems: an empirical study with a hand-deep local-global net. ||| 10817 ||| 40674 ||| 10815 ||| 28686 ||| 23336 ||| 40675 ||| 
2020 ||| amnn: attention-based multimodal neural network model for hashtag recommendation. ||| 5101 ||| 5097 ||| 5098 ||| 234 ||| 5100 ||| 40676 ||| 40677 ||| 
2021 ||| two-level attention model of representation learning for fraud detection. ||| 40678 ||| 24638 ||| 29455 ||| 40679 ||| 
2021 ||| two-phase multidocument summarization through content-attention-based subtopic detection. ||| 40680 ||| 40681 ||| 40682 ||| 40683 ||| 
2021 ||| memory augmented hierarchical attention network for next point-of-interest recommendation. ||| 24100 ||| 7596 ||| 18565 ||| 9837 ||| 40684 ||| 31563 ||| 
2020 ||| robust and efficient wls-based dynamic state estimation considering transformer core saturation. ||| 40685 ||| 40686 ||| 
2020 ||| hitanomaly: hierarchical transformers for anomaly detection in system log. ||| 3499 ||| 1199 ||| 40687 ||| 40688 ||| 40689 ||| 40690 ||| 40691 ||| 
2021 ||| neural and attentional factorization machine-based web api recommendation for mashup development. ||| 1568 ||| 162 ||| 30515 ||| 1569 ||| 1353 ||| 1570 ||| 
2021 ||| deepcc: multi-agent deep reinforcement learning congestion control for multi-path tcp based on self-attention. ||| 6186 ||| 40692 ||| 6188 ||| 6189 ||| 6191 ||| 26667 ||| 40693 ||| 39780 ||| 
2021 ||| log sequence anomaly detection based on local information extraction and globally sparse transformer model. ||| 40694 ||| 40695 ||| 40696 ||| 35832 ||| 40697 ||| 
2021 ||| one spatio-temporal sharpening attention mechanism for light-weight yolo models based on sharpening spatial attention. ||| 40698 ||| 1774 ||| 40699 ||| 40700 ||| 21195 ||| 
2021 ||| residual spatial and channel attention networks for single image dehazing. ||| 17840 ||| 40701 ||| 8952 ||| 40702 ||| 7204 ||| 
2021 ||| super-resolution network with information distillation and multi-scale attention for medical ct image. ||| 40703 ||| 24482 ||| 30501 ||| 40704 ||| 
2020 ||| attention-deficit/hyperactivity disorder (adhd): integrating the moxo-dcpt with an eye tracker enhances diagnostic precision. ||| 40705 ||| 40706 ||| 40707 ||| 40708 ||| 
2020 ||| eeg-based emotion classification using long short-term memory network with attention mechanism. ||| 40709 ||| 40710 ||| 
2021 ||| fac-net: feedback attention network based on context encoder network for skin lesion segmentation. ||| 40711 ||| 4719 ||| 12637 ||| 12639 ||| 
2020 ||| study of the influence of winding and sensor design on ultra-high frequency partial discharge signals in power transformers. ||| 40712 ||| 40713 ||| 40714 ||| 
2020 ||| an online tea fixation state monitoring algorithm based on image energy attention mechanism and supervised clustering (ieamsc). ||| 40715 ||| 3313 ||| 40716 ||| 40717 ||| 
2020 ||| end-to-end deep learning architecture for continuous blood pressure estimation using attention mechanism. ||| 40718 ||| 31085 ||| 40719 ||| 40720 ||| 40721 ||| 40722 ||| 40723 ||| 17290 ||| 
2020 ||| performance problems of non-toroidal shaped current transformers. ||| 10924 ||| 852 ||| 40724 ||| 40725 ||| 40726 ||| 10314 ||| 40727 ||| 10926 ||| 2698 ||| 10925 ||| 
2021 ||| ddostc: a transformer-based network attack detection hybrid mechanism in sdn. ||| 40728 ||| 3337 ||| 
2019 ||| design and performance test of transformer winding optical fibre composite wire based on raman scattering. ||| 40729 ||| 40730 ||| 5904 ||| 40731 ||| 
2020 ||| extreme low-light image enhancement for surveillance cameras using attention u-net. ||| 40732 ||| 40733 ||| 
2022 ||| attention autoencoder for generative latent representational learning in anomaly detection. ||| 40734 ||| 40735 ||| 40736 ||| 40737 ||| 33811 ||| 33813 ||| 
2020 ||| detection of computer graphics using attention-based dual-branch convolutional neural network from fused color components. ||| 40738 ||| 34631 ||| 244 ||| 40739 ||| 
2022 ||| a graph neural network with spatio-temporal attention for multi-sources time series data: an application to frost forecast. ||| 40740 ||| 40741 ||| 40742 ||| 40743 ||| 
2019 ||| a smart context-aware hazard attention system to help people with peripheral vision loss. ||| 40744 ||| 40745 ||| 40746 ||| 40747 ||| 
2022 ||| wearable sensor-based human activity recognition with transformer model. ||| 40748 ||| 40749 ||| 12225 ||| 40750 ||| 13478 ||| 40751 ||| 40752 ||| 40753 ||| 
2021 ||| how geometry affects sensitivity of a differential transformer for contactless characterization of liquids. ||| 40754 ||| 40755 ||| 40756 ||| 
2021 ||| the impact of attention mechanisms on speech emotion recognition. ||| 40757 ||| 40758 ||| 40759 ||| 40760 ||| 40761 ||| 40762 ||| 
2021 ||| bendductor - transformer steel magnetomechanical force sensor. ||| 40763 ||| 40764 ||| 40765 ||| 40766 ||| 
2020 ||| pyramid inter-attention for high dynamic range imaging. ||| 40767 ||| 40768 ||| 40769 ||| 40770 ||| 40771 ||| 1515 ||| 
2020 ||| spatial-spectral feature refinement for hyperspectral image classification based on attention-dense 3d-2d-cnn. ||| 390 ||| 40772 ||| 40773 ||| 40774 ||| 
2020 ||| are inductive current transformers performance really affected by actual distorted network conditions? an experimental case study. ||| 5590 ||| 5591 ||| 40775 ||| 40776 ||| 5627 ||| 
2020 ||| learning attention representation with a multi-scale cnn for gear fault diagnosis under different working conditions. ||| 40777 ||| 10418 ||| 40778 ||| 40779 ||| 
2021 ||| posture detection of individual pigs based on lightweight convolution neural networks and efficient channel-wise attention. ||| 40780 ||| 40781 ||| 40782 ||| 40783 ||| 
2021 ||| a simple method for compensating harmonic distortion in current transformers: experimental validation. ||| 5633 ||| 5635 ||| 5636 ||| 
2022 ||| effect of auditory discrimination therapy on attentional processes of tinnitus patients. ||| 40784 ||| 40785 ||| 3882 ||| 40786 ||| 40787 ||| 40788 ||| 40789 ||| 40790 ||| 4046 ||| 40791 ||| 40792 ||| 
2019 ||| static and dynamic evaluation of a winding deformation fbg sensor for power transformer applications. ||| 40793 ||| 40794 ||| 40795 ||| 40796 ||| 40797 ||| 40798 ||| 1994 ||| 40799 ||| 
2022 ||| deep modular bilinear attention network for visual question answering. ||| 40800 ||| 40801 ||| 40802 ||| 
2021 ||| combining implicit and explicit feature extraction for eye tracking: attention classification using a heterogeneous input. ||| 13860 ||| 13862 ||| 
2020 ||| on the long-period accuracy behavior of inductive and low-power instrument transformers. ||| 5590 ||| 40775 ||| 5591 ||| 5627 ||| 
2020 ||| toward the standardization of limits to offset and noise in electronic instrument transformers. ||| 5590 ||| 5591 ||| 5627 ||| 
2021 ||| joint soft-hard attention for self-supervised monocular depth estimation. ||| 28762 ||| 40803 ||| 40804 ||| 40805 ||| 40806 ||| 
2021 ||| data-driven anomaly detection in high-voltage transformer bushings with lstm auto-encoder. ||| 40807 ||| 40808 ||| 40809 ||| 40810 ||| 11384 ||| 
2021 ||| deep-emotion: facial expression recognition using attentional convolutional network. ||| 17204 ||| 35273 ||| 35272 ||| 
2021 ||| comparison of mathematical methods for compensating a current signal under current transformers saturation conditions. ||| 40811 ||| 40812 ||| 40813 ||| 40814 ||| 40815 ||| 40816 ||| 40817 ||| 5250 ||| 
2020 ||| validity evaluation method based on data driving for on-line monitoring data of transformer under dc-bias. ||| 40818 ||| 40819 ||| 40820 ||| 40821 ||| 
2020 ||| gps trajectory completion using end-to-end bidirectional convolutional recurrent encoder-decoder architecture with attention mechanism. ||| 40822 ||| 22700 ||| 18202 ||| 40823 ||| 40824 ||| 40825 ||| 
2021 ||| vehicle trajectory prediction with lane stream attention-based lstms and road geometry linearization. ||| 40826 ||| 40827 ||| 34333 ||| 40828 ||| 
2018 ||| a lightweight convolutional neural network based on visual attention for sar image target classification. ||| 40829 ||| 40830 ||| 40831 ||| 40832 ||| 
2021 ||| weed classification using explainable multi-resolution slot attention. ||| 40833 ||| 7111 ||| 40834 ||| 40835 ||| 40836 ||| 22118 ||| 40837 ||| 
2022 ||| multi-task model for esophageal lesion analysis using endoscopic images: classification with image retrieval and segmentation with attention. ||| 40838 ||| 40839 ||| 40840 ||| 28975 ||| 28974 ||| 
2020 ||| towards attention-based convolutional long short-term memory for travel time prediction of bus journeys. ||| 40841 ||| 603 ||| 8111 ||| 40842 ||| 
2019 ||| a convolution component-based method with attention mechanism for travel-time prediction. ||| 40843 ||| 40844 ||| 40845 ||| 39281 ||| 
2021 ||| mask attention-srgan for mobile sensing networks. ||| 40846 ||| 40847 ||| 40222 ||| 
2021 ||| towards precise interpretation of oil transformers via novel combined techniques based on dga and partial discharge sensors. ||| 40848 ||| 40849 ||| 40850 ||| 40851 ||| 40852 ||| 16220 ||| 40853 ||| 
2020 ||| fabrication of thermal conductivity detector based on mems for monitoring dissolved gases in power transformer. ||| 40854 ||| 40855 ||| 27452 ||| 40856 ||| 13237 ||| 
2022 ||| acceleration of magnetic resonance fingerprinting reconstruction using denoising and self-attention pyramidal convolutional neural network. ||| 40857 ||| 40858 ||| 40859 ||| 17425 ||| 40860 ||| 40861 ||| 40862 ||| 40863 ||| 40864 ||| 40865 ||| 
2021 ||| attention-based multi-scale convolutional neural network (a+mcnn) for multi-class classification in road images. ||| 40866 ||| 40867 ||| 
2021 ||| transformers and generative adversarial networks for liveness detection in multitarget fingerprint sensors. ||| 40868 ||| 6726 ||| 40869 ||| 
2021 ||| short-circuited turn fault diagnosis in transformers by using vibration signals, statistical time features, and support vector machines on fpga. ||| 40870 ||| 40871 ||| 40872 ||| 40873 ||| 40874 ||| 40875 ||| 
2022 ||| correlating personal resourcefulness and psychomotor skills: an analysis of stress, visual attention and technical metrics. ||| 40876 ||| 40877 ||| 3419 ||| 40878 ||| 40879 ||| 8048 ||| 40880 ||| 40881 ||| 40882 ||| 852 ||| 40883 ||| 40884 ||| 40881 ||| 40885 ||| 11933 ||| 40886 ||| 
2017 ||| modelling and optimization of four-segment shielding coils of current transformers. ||| 40887 ||| 7700 ||| 13930 ||| 40888 ||| 8440 ||| 28380 ||| 40889 ||| 
2020 ||| attention based cnn-convlstm for pedestrian attribute recognition. ||| 438 ||| 28684 ||| 28685 ||| 28686 ||| 
2020 ||| coastal land cover classification of high-resolution remote sensing images using attention-driven context encoding network. ||| 40890 ||| 1216 ||| 6703 ||| 40891 ||| 31285 ||| 40892 ||| 
2020 ||| improving object tracking by added noise and channel attention. ||| 40893 ||| 7987 ||| 40894 ||| 40895 ||| 7989 ||| 
2021 ||| luminance-degradation compensation based on multistream self-attention to address thin-film transistor-organic light emitting diode burn-in. ||| 40896 ||| 40897 ||| 10985 ||| 
2020 ||| global temperature sensing for an operating power transformer based on raman scattering. ||| 40729 ||| 40898 ||| 17464 ||| 40731 ||| 
2020 ||| global and local attention-based free-form image inpainting. ||| 40899 ||| 40900 ||| 
2021 ||| optical fiber sensors for structural monitoring in power transformers. ||| 40901 ||| 2871 ||| 40902 ||| 40903 ||| 40904 ||| 2712 ||| 40905 ||| 40906 ||| 40907 ||| 7111 ||| 40908 ||| 40909 ||| 7033 ||| 
2021 ||| emotion detection for social robots based on nlp transformers and an emotion ontology. ||| 40910 ||| 852 ||| 40911 ||| 40912 ||| 40913 ||| 40914 ||| 40915 ||| 
2019 ||| a sensor system for detecting and localizing partial discharges in power transformers with improved immunity to interferences. ||| 40916 ||| 40917 ||| 16413 ||| 40918 ||| 40919 ||| 40920 ||| 40921 ||| 40922 ||| 40923 ||| 
2021 ||| graph attention feature fusion network for als point cloud classification. ||| 1132 ||| 40924 ||| 40925 ||| 
2021 ||| social robots for evaluating attention state in older adults. ||| 18187 ||| 40926 ||| 40927 ||| 40928 ||| 40929 ||| 6375 ||| 
2021 ||| an attention-based multilayer gru model for multistep-ahead short-term load forecasting. ||| 40930 ||| 40931 ||| 31118 ||| 31119 ||| 
2021 ||| instance sequence queries for video instance segmentation with transformers. ||| 40932 ||| 40933 ||| 
2020 ||| bidirectional attention for text-dependent speaker verification. ||| 40934 ||| 40935 ||| 2982 ||| 4894 ||| 
2022 ||| efficient spatiotemporal attention network for remote heart rate variability analysis. ||| 40936 ||| 40937 ||| 40938 ||| 40939 ||| 
2020 ||| hybrid attention network for language-based person search. ||| 438 ||| 28684 ||| 28686 ||| 
2020 ||| spatial-semantic and temporal attention mechanism-based online multi-object tracking. ||| 40940 ||| 29370 ||| 952 ||| 40528 ||| 13569 ||| 
2021 ||| transformation of transient overvoltages by inductive voltage transformers. ||| 40941 ||| 40942 ||| 
2021 ||| multi-modal adaptive fusion transformer network for the estimation of depression level. ||| 7725 ||| 40943 ||| 40944 ||| 40945 ||| 1550 ||| 40946 ||| 1528 ||| 
2019 ||| deep attention models for human tracking using rgbd. ||| 40947 ||| 40948 ||| 40949 ||| 40950 ||| 40951 ||| 
2020 ||| multi-modal explicit sparse attention networks for visual question answering. ||| 40952 ||| 40953 ||| 
2021 ||| a lightweight 1-d convolution augmented transformer with metric learning for hyperspectral image classification. ||| 3768 ||| 4552 ||| 40954 ||| 16550 ||| 40955 ||| 
2021 ||| improving ponzi scheme contract detection using multi-channel textcnn and transformer. ||| 31468 ||| 40956 ||| 5972 ||| 40957 ||| 11707 ||| 40958 ||| 
2021 ||| korean grammatical error correction based on transformer with copying mechanisms and grammatical noise implantation methods. ||| 40959 ||| 40960 ||| 40961 ||| 40962 ||| 
2020 ||| robust building extraction for high spatial resolution remote sensing images with self-attention network. ||| 20328 ||| 40963 ||| 30273 ||| 40964 ||| 40965 ||| 30272 ||| 380 ||| 382 ||| 
2020 ||| radar emitter signal recognition based on one-dimensional convolutional neural network with attention mechanism. ||| 411 ||| 40966 ||| 3675 ||| 40967 ||| 40968 ||| 40969 ||| 
2020 ||| an effective dense co-attention networks for visual question answering. ||| 40970 ||| 40953 ||| 
2020 ||| hybrid network with attention mechanism for detection and location of myocardial infarction based on 12-lead electrocardiogram signals. ||| 40971 ||| 40972 ||| 40973 ||| 40974 ||| 40975 ||| 40976 ||| 
2021 ||| dynamic hand gesture recognition in in-vehicle environment based on fmcw radar and transformer. ||| 40977 ||| 17644 ||| 40978 ||| 40979 ||| 40980 ||| 5021 ||| 241 ||| 
2018 ||| novel simulation technique of electromagnetic wave propagation in the ultra high frequency range within power transformers. ||| 40981 ||| 40714 ||| 
2019 ||| a dynamic part-attention model for person re-identification. ||| 40982 ||| 40983 ||| 40984 ||| 40985 ||| 
2021 ||| attention-guided network with densely connected convolution for skin lesion segmentation. ||| 5304 ||| 5366 ||| 40986 ||| 25100 ||| 40987 ||| 
2022 ||| vehicle interaction behavior prediction with self-attention. ||| 30338 ||| 40988 ||| 40989 ||| 40990 ||| 40991 ||| 
2019 ||| a classification method for select defects in power transformers based on the acoustic signals. ||| 40992 ||| 40993 ||| 
2021 ||| research on on-line detection method of transformer winding deformation based on vfto. ||| 40994 ||| 40995 ||| 40996 ||| 4111 ||| 40997 ||| 40998 ||| 40999 ||| 41000 ||| 
2021 ||| nra-net - neg-region attention network for salient object detection with gaze tracking. ||| 41001 ||| 41002 ||| 21488 ||| 
2020 ||| distribution transformer parameters detection based on low-frequency noise, machine learning methods, and evolutionary algorithm. ||| 41003 ||| 41004 ||| 41005 ||| 
2021 ||| analysis of the influence of the breaking radiation magnetic field of a 10 kv intelligent circuit breaker on an electronic transformer. ||| 41006 ||| 41007 ||| 7087 ||| 41008 ||| 5523 ||| 
2021 ||| msst-rt: multi-stream spatial-temporal relative transformer for skeleton-based action recognition. ||| 113 ||| 41009 ||| 11420 ||| 
2021 ||| esophagus segmentation in ct images via spatial attention network and staple algorithm. ||| 41010 ||| 2841 ||| 2838 ||| 2839 ||| 41011 ||| 41012 ||| 
2021 ||| edge-enhanced with feedback attention network for image super-resolution. ||| 41013 ||| 41014 ||| 
2021 ||| improved action recognition with separable spatio-temporal attention using alternative skeletal and video pre-processing. ||| 41015 ||| 2600 ||| 41016 ||| 41017 ||| 
2022 ||| epileptic-net: an improved epileptic seizure detection system using dense convolutional block with attention network from eeg. ||| 41018 ||| 41019 ||| 41020 ||| 
2021 ||| moment-to-moment continuous attention fluctuation monitoring through consumer-grade eeg device. ||| 41021 ||| 41022 ||| 41023 ||| 41024 ||| 41025 ||| 
2021 ||| radar transformer: an object classification network based on 4d mmw imaging radar. ||| 17644 ||| 40977 ||| 11504 ||| 2081 ||| 39537 ||| 40979 ||| 
2021 ||| implementation of an online auditory attention detection model with electroencephalography in a dichotomous listening experiment. ||| 41026 ||| 41027 ||| 41028 ||| 
2018 ||| machine learning-based sensor data modeling methods for power transformer phm. ||| 31908 ||| 31906 ||| 31911 ||| 31910 ||| 31909 ||| 
2022 ||| lstnet: a reference-based learning spectral transformer network for spectral super-resolution. ||| 41029 ||| 41030 ||| 41031 ||| 41032 ||| 595 ||| 
2021 ||| robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion. ||| 34087 ||| 41033 ||| 34091 ||| 
2020 ||| sgdan - a spatio-temporal graph dual-attention neural network for quantified flight delay prediction. ||| 6042 ||| 41034 ||| 6043 ||| 6044 ||| 41035 ||| 41036 ||| 41037 ||| 
2021 ||| a novel transformers fault diagnosis method based on probabilistic neural network and bio-inspired optimizer. ||| 41038 ||| 31906 ||| 41039 ||| 2884 ||| 
2022 ||| dan-superpoint: self-supervised feature point detection algorithm with dual attention network. ||| 41040 ||| 4470 ||| 41041 ||| 15242 ||| 41042 ||| 41043 ||| 
2020 ||| image deblurring using multi-stream bottom-top-bottom attention network and global information-based fusion and reconstruction network. ||| 11215 ||| 28924 ||| 28926 ||| 
2021 ||| attention-based joint training of noise suppression and sound event detection for noise-robust classification. ||| 41044 ||| 10985 ||| 
2021 ||| super-resolution generative adversarial network based on the dual dimension attention mechanism for biometric image super-resolution. ||| 40846 ||| 40222 ||| 41045 ||| 40847 ||| 
2019 ||| dissolved gas analysis equipment for online monitoring of transformer oil: a review. ||| 41046 ||| 41047 ||| 41048 ||| 41049 ||| 41050 ||| 41051 ||| 
2020 ||| multi-branch convolutional neural network for automatic sleep stage classification with embedded stage refinement and residual attention channel fusion. ||| 41052 ||| 41053 ||| 32817 ||| 
2021 ||| generalized deep learning eeg models for cross-participant and cross-task detection of the vigilance decrement in sustained attention tasks. ||| 41054 ||| 41055 ||| 41056 ||| 41057 ||| 
2021 ||| skeleton-based emotion recognition based on two-stream self-attention enhanced spatial-temporal graph convolutional network. ||| 41058 ||| 39466 ||| 14590 ||| 20474 ||| 
2021 ||| vitt: vision transformer tracker. ||| 41059 ||| 41060 ||| 41061 ||| 41062 ||| 41063 ||| 
2018 ||| social image captioning: exploring visual attention and user attention. ||| 18638 ||| 18636 ||| 18637 ||| 18634 ||| 41064 ||| 18635 ||| 
2020 ||| low-power voltage transformer smart frequency modeling and output prediction up to 2.5 khz, using sinc-response approach. ||| 5626 ||| 5590 ||| 5591 ||| 5627 ||| 
2022 ||| power transformer voltages classification with acoustic signal in various noisy environments. ||| 41065 ||| 41066 ||| 
2021 ||| ar3d: attention residual 3d network for human action recognition. ||| 41067 ||| 41068 ||| 41069 ||| 11702 ||| 41070 ||| 
2021 ||| high accurate environmental sound classification: sub-spectrogram segmentation versus temporal-frequency attention mechanism. ||| 6565 ||| 6566 ||| 6567 ||| 6564 ||| 
2021 ||| single- and cross-modality near duplicate image pairs detection via spatial transformer comparing cnn. ||| 340 ||| 41071 ||| 949 ||| 10075 ||| 
2020 ||| adst: forecasting metro flow using attention-based deep spatial-temporal networks with multi-task learning. ||| 41072 ||| 32242 ||| 1371 ||| 18829 ||| 41073 ||| 41074 ||| 41075 ||| 
2021 ||| unsupervised trademark retrieval method based on attention mechanism. ||| 41076 ||| 41077 ||| 37766 ||| 41078 ||| 
2021 ||| gourmetnet: food segmentation using multi-scale waterfall features with spatial and channel attention. ||| 41079 ||| 41080 ||| 41081 ||| 
2021 ||| panchromatic image super-resolution via self attention-augmented wasserstein generative adversarial network. ||| 2853 ||| 41082 ||| 31496 ||| 41083 ||| 2858 ||| 
2021 ||| adaptive attention memory graph convolutional networks for skeleton-based action recognition. ||| 16941 ||| 452 ||| 22437 ||| 41084 ||| 27439 ||| 41085 ||| 
2021 ||| stac: spatial-temporal attention on compensation information for activity recognition in fpv. ||| 3289 ||| 26771 ||| 41086 ||| 41087 ||| 41088 ||| 
2022 ||| a comparative analysis applied to the partial discharges identification in dry-type transformers by hall and acoustic emission sensors. ||| 41089 ||| 41090 ||| 41091 ||| 41092 ||| 41093 ||| 3369 ||| 41094 ||| 
2022 ||| feature-based sentimental analysis on public attention towards covid-19 using cuda-sadbm classification model. ||| 41095 ||| 41096 ||| 41097 ||| 41098 ||| 
2022 ||| deterioration level estimation based on convolutional neural network using confidence-aware attention mechanism for infrastructure inspection. ||| 7611 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| color index of transformer oil: a low-cost measurement approach using ultraviolet-blue laser. ||| 21269 ||| 21268 ||| 41099 ||| 21267 ||| 41100 ||| 21272 ||| 41101 ||| 
2022 ||| convolutional blur attention network for cell nuclei segmentation. ||| 41102 ||| 4501 ||| 7585 ||| 4502 ||| 
2020 ||| wavelet-like transform to optimize the order of an autoregressive neural network model to predict the dissolved gas concentration in power transformer oil from sensor data. ||| 41103 ||| 41104 ||| 41105 ||| 41106 ||| 7111 ||| 41107 ||| 41108 ||| 41109 ||| 41110 ||| 41111 ||| 504 ||| 41112 ||| 
2019 ||| spatio-temporal attention model for foreground detection in cross-scene surveillance videos. ||| 11467 ||| 41113 ||| 41114 ||| 30434 ||| 
2021 ||| design of insulation tape tension control system of transformer winding machine based on fuzzy pid. ||| 41115 ||| 41116 ||| 41117 ||| 
2020 ||| calibration procedure to test the effects of multiple influence quantities on low-power voltage transformers. ||| 5590 ||| 5591 ||| 5627 ||| 
2022 ||| deep learning post-filtering using multi-head attention and multiresolution feature fusion for image and intra-video quality enhancement. ||| 41118 ||| 41119 ||| 
2018 ||| transformerless ultrasonic ranging system with the feature of intrinsic safety for explosive environment. ||| 3906 ||| 41120 ||| 41121 ||| 5089 ||| 1251 ||| 41122 ||| 952 ||| 41123 ||| 
2021 ||| a bci based alerting system for attention recovery of uav operators. ||| 41124 ||| 11906 ||| 41125 ||| 41126 ||| 
2021 ||| attention networks for the quality enhancement of light field images. ||| 41118 ||| 41119 ||| 
2020 ||| real-time monitoring for hydraulic states based on convolutional bidirectional lstm with attention mechanism. ||| 41127 ||| 41128 ||| 
2021 ||| vehicle destination prediction using bidirectional lstm with attention mechanism. ||| 41129 ||| 9472 ||| 41130 ||| 41131 ||| 3419 ||| 41132 ||| 
2021 ||| remote sensing image dataset expansion based on generative adversarial networks with modified shuffle attention. ||| 3147 ||| 13714 ||| 41133 ||| 
2021 ||| hybrid attention cascade network for facial expression recognition. ||| 41134 ||| 41135 ||| 2932 ||| 4321 ||| 
2019 ||| bio-inspired phm model for diagnostics of faults in power transformers using dissolved gas-in-oil data. ||| 31911 ||| 31906 ||| 31908 ||| 31910 ||| 41136 ||| 
2020 ||| xfinger-net: pixel-wise segmentation method for partially defective fingerprint based on attention gates and u-net. ||| 41137 ||| 41138 ||| 31848 ||| 41139 ||| 41140 ||| 41141 ||| 
2020 ||| time series multiple channel convolutional neural network with attention-based long short-term memory for predicting bearing remaining useful life. ||| 41142 ||| 41143 ||| 41144 ||| 
2021 ||| real-time semantic segmentation with dual encoder and self-attention mechanism for autonomous driving. ||| 41145 ||| 41146 ||| 41147 ||| 41148 ||| 
2021 ||| automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze. ||| 41149 ||| 5004 ||| 
2021 ||| a fast stereo matching network with multi-cross attention. ||| 9476 ||| 8952 ||| 2342 ||| 26762 ||| 30457 ||| 30459 ||| 
2020 ||| closed-loop attention restoration theory for virtual reality-based attentional engagement enhancement. ||| 858 ||| 41150 ||| 41151 ||| 41152 ||| 
2020 ||| principles of charge estimation methods using high-frequency current transformer sensors in partial discharge measurements. ||| 41153 ||| 41154 ||| 18405 ||| 41155 ||| 
2021 ||| energy load forecasting using a dual-stage attention-based recurrent neural network. ||| 41156 ||| 41157 ||| 41158 ||| 
2021 ||| attention fusion for one-stage multispectral pedestrian detection. ||| 28937 ||| 447 ||| 21207 ||| 41159 ||| 41160 ||| 
2021 ||| a driver's visual attention prediction using optical flow. ||| 15467 ||| 39373 ||| 
2019 ||| design and development of a bio-inspired uhf sensor for partial discharge detection in power transformers. ||| 41161 ||| 41162 ||| 41163 ||| 41164 ||| 41165 ||| 41166 ||| 41167 ||| 
2021 ||| adversarial learning with bidirectional attention for visual question answering. ||| 29657 ||| 41168 ||| 41169 ||| 
2020 ||| tensor-based emotional category classification via visual attention-based heterogeneous cnn feature fusion. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2021 ||| automatic lung segmentation on chest x-rays using self-attention deep neural network. ||| 41170 ||| 41171 ||| 
2021 ||| learning region-based attention network for traffic sign recognition. ||| 41172 ||| 41173 ||| 17609 ||| 
2021 ||| multi-scale squeeze u-segnet with multi global attention for brain mri segmentation. ||| 41174 ||| 41175 ||| 41176 ||| 
2022 ||| maff-net: multi-attention guided feature fusion network for change detection in remote sensing images. ||| 41177 ||| 6065 ||| 41178 ||| 41179 ||| 
2022 ||| a cascade attention based facial expression recognition network by fusing multi-scale spatio-temporal features. ||| 41134 ||| 41180 ||| 2932 ||| 4321 ||| 41181 ||| 
2018 ||| attention-based recurrent temporal restricted boltzmann machine for radar high resolution range profile sequence recognition. ||| 2341 ||| 41182 ||| 41183 ||| 41184 ||| 2008 ||| 
2021 ||| small object detection in traffic scenes based on attention feature fusion. ||| 40989 ||| 41185 ||| 30338 ||| 12388 ||| 40991 ||| 
2019 ||| uhf partial discharge location in power transformers via solution of the maxwell equations in a computational environment. ||| 41161 ||| 41167 ||| 41164 ||| 41165 ||| 41162 ||| 41163 ||| 
2021 ||| patch attention layer of embedding handcrafted features in cnn for facial expression recognition. ||| 41186 ||| 41187 ||| 41188 ||| 41189 ||| 41190 ||| 41191 ||| 10330 ||| 
2017 ||| design and experimental study of a current transformer with a stacked pcb based on b-dot. ||| 23439 ||| 41192 ||| 1321 ||| 41193 ||| 
2020 ||| time series forecasting and classification models based on recurrent with attention mechanism and generative adversarial networks. ||| 30959 ||| 14007 ||| 41194 ||| 12600 ||| 
2021 ||| attention-based context aware network for semantic comprehension of aerial scenery. ||| 41195 ||| 41196 ||| 41197 ||| 41198 ||| 41199 ||| 41200 ||| 
2020 ||| two-stream attention network for pain recognition from video sequences. ||| 41201 ||| 41202 ||| 20353 ||| 
2022 ||| heart rate measurement based on 3d central difference convolution with attention mechanism. ||| 40939 ||| 41203 ||| 40936 ||| 40938 ||| 
2021 ||| erratum: rodrigo-mor et al. principles of charge estimation methods using high-frequency current transformer sensors in partial discharge measurements. sensors 2020, 20, 2520. ||| 41153 ||| 41154 ||| 18405 ||| 41155 ||| 
2020 ||| multi-scale feature integrated attention-based rotation network for object detection in vhr aerial images. ||| 2320 ||| 41204 ||| 41205 ||| 28548 ||| 5845 ||| 
2020 ||| unsupervised dark-channel attention-guided cyclegan for single-image dehazing. ||| 41206 ||| 41207 ||| 37313 ||| 15367 ||| 
2020 ||| learning soft mask based feature fusion with channel and spatial attention for robust visual object tracking. ||| 40893 ||| 7987 ||| 7989 ||| 
2022 ||| application of thermography and adversarial reconstruction anomaly detection in power cast-resin transformer. ||| 41208 ||| 41209 ||| 
2022 ||| deep learning and transformer approaches for uav-based wildfire detection and segmentation. ||| 30417 ||| 24563 ||| 30419 ||| 30420 ||| 
2019 ||| traffic speed prediction: an attention-based method. ||| 41210 ||| 41211 ||| 10149 ||| 5785 ||| 
2022 ||| attention-based deep recurrent neural network to forecast the temperature behavior of an electric arc furnace side-wall. ||| 41212 ||| 41213 ||| 41214 ||| 41215 ||| 41216 ||| 3157 ||| 41217 ||| 41218 ||| 41219 ||| 
2018 ||| uv-vis spectroscopy: a new approach for assessing the color index of transformer insulating oil. ||| 21267 ||| 21268 ||| 21272 ||| 41220 ||| 41221 ||| 41222 ||| 21273 ||| 41223 ||| 
2018 ||| characterizing focused attention and working memory using eeg. ||| 2896 ||| 41224 ||| 41225 ||| 2897 ||| 2898 ||| 
2019 ||| classification of low frequency signals emitted by power transformers using sensors and machine learning methods. ||| 41003 ||| 41004 ||| 41005 ||| 
2021 ||| an approach to steady-state power transformer modeling considering direct current resistance test measurements. ||| 41226 ||| 3419 ||| 277 ||| 41227 ||| 
2022 ||| lane mark detection with pre-aligned spatial-temporal attention. ||| 41228 ||| 18614 ||| 
2021 ||| attention-guided image captioning through word information. ||| 41229 ||| 41230 ||| 18507 ||| 
2022 ||| novel video surveillance-based fire and smoke classification using attentional feature map in capsule networks. ||| 41231 ||| 41232 ||| 41233 ||| 41234 ||| 
2021 ||| double ghost convolution attention mechanism network: a framework for hyperspectral reconstruction of a single rgb image. ||| 41235 ||| 34113 ||| 
2019 ||| fusionatt: deep fusional attention networks for multi-channel biomedical signals. ||| 2296 ||| 6123 ||| 
2021 ||| automatic pixel-level pavement crack recognition using a deep feature aggregation segmentation network with a scse attention mechanism module. ||| 41236 ||| 41237 ||| 41238 ||| 41239 ||| 858 ||| 
2021 ||| dual branch attention network for person re-identification. ||| 41240 ||| 4719 ||| 12637 ||| 12639 ||| 
2021 ||| a two-stage multistep-ahead electricity load forecasting scheme based on lightgbm and attention-bilstm. ||| 41241 ||| 31119 ||| 
2020 ||| partial discharge localization using time reversal: application to power transformers. ||| 41242 ||| 41243 ||| 41244 ||| 41245 ||| 29195 ||| 28295 ||| 41246 ||| 
2022 ||| cross encoder-decoder transformer with global-local visual extractor for medical image captioning. ||| 41247 ||| 41248 ||| 41249 ||| 8363 ||| 8366 ||| 
2019 ||| development of acoustic emission sensor optimized for partial discharge monitoring in power transformers. ||| 41250 ||| 
2022 ||| fast panoptic segmentation with soft attention embeddings. ||| 7403 ||| 7404 ||| 
2018 ||| simultaneous ship detection and orientation estimation in sar images based on attention module and angle regression. ||| 41251 ||| 41252 ||| 40458 ||| 
2021 ||| a transformer-based neural machine translation model for arabic dialects that utilizes subword units. ||| 41253 ||| 7548 ||| 41254 ||| 
2019 ||| anti-interference deep visual identification method for fault localization of transformer using a winding model. ||| 41255 ||| 22230 ||| 41256 ||| 4600 ||| 41257 ||| 
2018 ||| action recognition by an attention-aware temporal weighted convolutional neural network. ||| 6436 ||| 14777 ||| 6437 ||| 35168 ||| 8674 ||| 14779 ||| 
2021 ||| optical voltage transformer based on fbg-pzt for power quality measurement. ||| 41258 ||| 8500 ||| 41259 ||| 
2020 ||| fusing visual attention cnn and bag of visual words for cross-corpus speech emotion recognition. ||| 41260 ||| 41261 ||| 
2021 ||| harp: hierarchical attention oriented region-based processing for high-performance computation in vision sensor. ||| 14200 ||| 14199 ||| 14201 ||| 
2021 ||| dual memory lstm with dual attention neural network for spatiotemporal prediction. ||| 14710 ||| 41262 ||| 
2021 ||| a clustering method of case-involved news by combining topic network and multi-head attention mechanism. ||| 41263 ||| 41264 ||| 2950 ||| 28286 ||| 2952 ||| 
2021 ||| monocular depth estimation with joint attention feature distillation and wavelet-based loss function. ||| 10572 ||| 41265 ||| 41266 ||| 41267 ||| 
2021 ||| computer-aided diagnosis of alzheimer's disease through weak supervision deep learning framework with attention mechanism. ||| 11182 ||| 6824 ||| 
2021 ||| an efficient anomaly recognition framework using an attention residual lstm in surveillance videos. ||| 41268 ||| 41269 ||| 41270 ||| 41271 ||| 28959 ||| 
2019 ||| a novel differential high-frequency current transformer sensor for series arc fault detection. ||| 41272 ||| 41273 ||| 41274 ||| 1215 ||| 
2021 ||| blood pressure morphology assessment from photoplethysmogram and demographic information using deep learning with attention mechanism. ||| 41275 ||| 41276 ||| 7111 ||| 41277 ||| 41278 ||| 
2021 ||| larnet: real-time detection of facial micro expression using lossless attention residual network. ||| 29836 ||| 29837 ||| 41279 ||| 29839 ||| 29841 ||| 41280 ||| 41281 ||| 
2019 ||| uncertainty analysis of a test bed for calibrating voltage transformers vs. temperature. ||| 5590 ||| 5591 ||| 5627 ||| 5626 ||| 
2020 ||| a dual-attention recurrent neural network method for deep cone thickener underflow concentration prediction. ||| 41282 ||| 823 ||| 8976 ||| 41283 ||| 
2021 ||| cross-attention fusion based spatial-temporal multi-graph convolutional network for traffic flow prediction. ||| 35595 ||| 41284 ||| 41285 ||| 41286 ||| 41287 ||| 
2021 ||| multi-scale capsule attention network and joint distributed optimal transport for bearing fault diagnosis under different working loads. ||| 41288 ||| 41289 ||| 41290 ||| 40634 ||| 41291 ||| 
2021 ||| multi-u-net: residual module under multisensory field and attention mechanism based optimized u-net for vhr image semantic segmentation. ||| 41292 ||| 41293 ||| 41294 ||| 41295 ||| 40310 ||| 
2021 ||| age and gender recognition using a convolutional neural network with a specially designed multi-attention module through speech spectrograms. ||| 41296 ||| 29312 ||| 41297 ||| 29313 ||| 
2021 ||| a lightweight attention-based cnn model for efficient gait recognition with wearable imu sensors. ||| 41298 ||| 12300 ||| 11494 ||| 41299 ||| 
2020 ||| end-to-end automatic pronunciation error detection based on improved hybrid ctc/attention architecture. ||| 4864 ||| 645 ||| 123 ||| 41300 ||| 41301 ||| 41302 ||| 41303 ||| 41304 ||| 
2020 ||| object tracking in rgb-t videos using modal-aware attention network and competitive learning. ||| 4600 ||| 241 ||| 28786 ||| 875 ||| 
2020 ||| high-resolution neural network for driver visual attention prediction. ||| 15467 ||| 39373 ||| 
2018 ||| a feasibility study of transformer winding temperature and strain detection based on distributed optical fibre sensors. ||| 40729 ||| 5904 ||| 40731 ||| 41305 ||| 8859 ||| 17464 ||| 40730 ||| 41306 ||| 
2021 ||| hadf-crowd: a hierarchical attention-based dense feature extraction network for single-image crowd counting. ||| 41307 ||| 35309 ||| 41308 ||| 
2020 ||| transformer winding deformation detection based on botdr and rotdr. ||| 41309 ||| 40729 ||| 17464 ||| 41310 ||| 41311 ||| 41312 ||| 40731 ||| 
2019 ||| real-time visual tracking with variational structure attention network. ||| 41313 ||| 41314 ||| 41315 ||| 41316 ||| 
2021 ||| numerical and experimental evaluation and heat transfer characteristics of a soft magnetic transformer built from laminated steel plates. ||| 41317 ||| 3369 ||| 41318 ||| 3882 ||| 41319 ||| 41320 ||| 8648 ||| 41321 ||| 
2021 ||| detecting attention levels in adhd children with a video game and the measurement of brain activity with a single-channel bci headset. ||| 41322 ||| 41323 ||| 41324 ||| 41325 ||| 28162 ||| 41326 ||| 8048 ||| 41327 ||| 41328 ||| 
2021 ||| relation-based deep attention network with hybrid memory for one-shot person re-identification. ||| 41329 ||| 764 ||| 5084 ||| 41330 ||| 
2021 ||| quantitative analysis of metallographic image using attention-aware deep neural networks. ||| 41331 ||| 41332 ||| 41333 ||| 41334 ||| 41335 ||| 41336 ||| 113 ||| 41337 ||| 
2021 ||| tunneling magnetoresistance dc current transformer for ion beam diagnostics. ||| 41338 ||| 41339 ||| 41340 ||| 41341 ||| 41342 ||| 
2020 ||| attpnet: attention-based deep neural network for 3d point set analysis. ||| 41343 ||| 41344 ||| 875 ||| 13676 ||| 11333 ||| 
2019 ||| integrated on-chip transformers: recent progress in the design, layout, modeling and fabrication. ||| 41345 ||| 41346 ||| 
2019 ||| an lstm-based method with attention mechanism for travel time prediction. ||| 40843 ||| 40844 ||| 40845 ||| 39281 ||| 
2021 ||| modeling capacitive low-power voltage transformer behavior over temperature and frequency. ||| 5590 ||| 41347 ||| 5640 ||| 5591 ||| 5627 ||| 
2021 ||| prediction of head movement in 360-degree videos using attention model. ||| 25360 ||| 41348 ||| 39619 ||| 
2021 ||| cross attention squeeze excitation network (case-net) for whole body fetal mri segmentation. ||| 41349 ||| 41350 ||| 41351 ||| 41352 ||| 41353 ||| 41354 ||| 41355 ||| 41356 ||| 41357 ||| 41358 ||| 41359 ||| 41360 ||| 
2021 ||| face manipulation detection based on supervised multi-feature fusion attention network. ||| 41361 ||| 41362 ||| 2532 ||| 41363 ||| 41364 ||| 41365 ||| 
2021 ||| dual crisscross attention module for road extraction from remote sensing images. ||| 5794 ||| 41366 ||| 17991 ||| 10812 ||| 
2020 ||| adversarial networks for scale feature-attention spectral image reconstruction from a single rgb. ||| 4968 ||| 41367 ||| 
2020 ||| skin lesion classification using densely connected convolutional networks with attention residual learning. ||| 839 ||| 6595 ||| 21688 ||| 41368 ||| 10405 ||| 
2022 ||| a two-stage approach to important area detection in gathering place using a novel multi-input attention network. ||| 41369 ||| 41370 ||| 41371 ||| 
2021 ||| visual attention and color cues for 6d pose estimation on occluded scenarios using rgb-d data. ||| 41372 ||| 41373 ||| 41374 ||| 
2020 ||| few-shot personalized saliency prediction based on adaptive image selection considering object and visual attention. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2020 ||| multi-modality emotion recognition model with gat-based multi-head inter-modality attention. ||| 20470 ||| 39466 ||| 14590 ||| 20474 ||| 
2021 ||| attention-based 3d human pose sequence refinement network. ||| 41375 ||| 41376 ||| 
2020 ||| spatial attention fusion for obstacle detection using mmwave radar and vision sensor. ||| 41377 ||| 2341 ||| 2532 ||| 11968 ||| 41378 ||| 259 ||| 41379 ||| 
2022 ||| spatial-temporal convolutional transformer network for multivariate time series forecasting. ||| 5278 ||| 39350 ||| 3433 ||| 41380 ||| 
2021 ||| far-net: feature-wise attention-based relation network for multilabel jujube defect classification. ||| 41381 ||| 30276 ||| 41382 ||| 41383 ||| 41384 ||| 
2019 ||| ship detection for optical remote sensing images based on visual attention enhanced network. ||| 41385 ||| 41386 ||| 12373 ||| 41387 ||| 41388 ||| 
2021 ||| capformer: pedestrian crossing action prediction using transformer. ||| 36503 ||| 41389 ||| 14327 ||| 15452 ||| 41390 ||| 41391 ||| 41392 ||| 15449 ||| 15450 ||| 15453 ||| 15454 ||| 
2021 ||| attendaffectnet-emotion prediction of movie viewers using multimodal fusion with self-attention. ||| 20374 ||| 20375 ||| 20376 ||| 936 ||| 
2022 ||| eye-tracker study of influence of affective disruptive content on user's visual attention and emotional state. ||| 14840 ||| 41393 ||| 18307 ||| 18308 ||| 
2021 ||| gated graph attention network for cancer prediction. ||| 22959 ||| 3694 ||| 22958 ||| 31777 ||| 
2021 ||| computer vision-based bridge damage detection using deep convolutional networks with expectation maximum attention module. ||| 41236 ||| 41239 ||| 41237 ||| 41238 ||| 858 ||| 
2020 ||| attention-based automated feature extraction for malware analysis. ||| 41394 ||| 41395 ||| 41396 ||| 41397 ||| 5572 ||| 
2021 ||| an attention-enhanced multi-scale and dual sign language recognition network based on a graph convolution network. ||| 41398 ||| 41399 ||| 
2021 ||| attention based bidirectional convolutional lstm for high-resolution radio tomographic imaging. ||| 41400 ||| 12672 ||| 7457 ||| 41401 ||| 
2021 ||| synthesis design on wideband single-ended and differential dual-band filtering impedance transformer. ||| 41402 ||| 29960 ||| 14713 ||| 5723 ||| 41403 ||| 
2021 ||| all-pass network and transformer based sige bicmos phase shifter for multi-band arrays. ||| 41404 ||| 41405 ||| 41406 ||| 41407 ||| 
2021 ||| a 28-ghz doherty power amplifier with a compact transformer-based quadrature hybrid in 65-nm cmos. ||| 41408 ||| 4287 ||| 25621 ||| 
2021 ||| doubly-tuned transformer networks: a tutorial. ||| 29996 ||| 29995 ||| 
2021 ||| a 30-36 ghz passive hybrid phase shifter with a transformer-based high-resolution reflect-type phase shifting technique. ||| 41409 ||| 3072 ||| 41410 ||| 25248 ||| 
2021 ||| a wideband cmos frequency quadrupler with transformer-based tail feedback loop. ||| 41411 ||| 41412 ||| 41413 ||| 41414 ||| 41415 ||| 41416 ||| 41417 ||| 41418 ||| 
2022 ||| ensemble learning with attention-based multiple instance pooling for classification of spt. ||| 41419 ||| 1340 ||| 30796 ||| 
2021 ||| transformer secondary voltage based resonant frequency tracking for llc converter. ||| 41420 ||| 41421 ||| 41422 ||| 41423 ||| 
2021 ||| -order transformer-based injection-locked frequency divider with 87.1% locking range in 40-nm cmos. ||| 41424 ||| 41425 ||| 41426 ||| 41427 ||| 32161 ||| 
2021 ||| a mmc-based multiport power electronic transformer with shared medium-frequency transformer. ||| 22002 ||| 22003 ||| 22004 ||| 29987 ||| 38356 ||| 
2017 ||| social sensors based online attention computing of public safety events. ||| 19110 ||| 41428 ||| 4600 ||| 8162 ||| 41429 ||| 41430 ||| 9352 ||| 6259 ||| 
2021 ||| is your document novel? let attention guide you. an attention-based model for document-level novelty detection. ||| 41431 ||| 41432 ||| 165 ||| 405 ||| 41433 ||| 41434 ||| 
2021 ||| an analysis of the impact of attentional momentum effect on driver's ability of awareness during night-time driving. ||| 41435 ||| 41436 ||| 41437 ||| 41438 ||| 
2021 ||| attention-based multi-modal sentiment analysis and emotion detection in conversation using rnn. ||| 31712 ||| 31713 ||| 31714 ||| 
2021 ||| self-attention implicit function networks for 3d dental data completion. ||| 41439 ||| 41440 ||| 14066 ||| 11116 ||| 19461 ||| 
2021 ||| attentional markov model for human mobility prediction. ||| 41441 ||| 2969 ||| 8866 ||| 39780 ||| 
2021 ||| attention-weighted federated deep reinforcement learning for device-to-device assisted heterogeneous collaborative edge caching. ||| 12608 ||| 13485 ||| 3995 ||| 41442 ||| 41443 ||| 13486 ||| 
2022 ||| deep reinforcement learning with communication transformer for adaptive live streaming in wireless edge networks. ||| 41444 ||| 41445 ||| 41446 ||| 
2021 ||| siabr: a structured intra-attention bidirectional recurrent deep learning method for ultra-accurate terahertz indoor localization. ||| 41447 ||| 4722 ||| 41448 ||| 12824 ||| 
2019 ||| single-phase common mode transformer-less soft-switching grid-connected inverter with eliminated leakage current. ||| 41449 ||| 41450 ||| 15382 ||| 
2019 ||| a new structure of single-phase two-stage hybrid transformerless multilevel pv inverter. ||| 41451 ||| 41452 ||| 41453 ||| 
2021 ||| a k-band high-gain power amplifier with slow-wave transmission-line transformer in 130-nm rf cmos. ||| 41454 ||| 41455 ||| 41456 ||| 1371 ||| 41457 ||| 41458 ||| 41459 ||| 
2022 ||| a new design of transformerless, non-isolated, high step-up dc-dc converter with hybrid fuzzy logic mppt controller. ||| 41460 ||| 41461 ||| 
2021 ||| non-isolated single-phase inverter based on an autotransformer for low-power applications. ||| 41462 ||| 3419 ||| 41463 ||| 41464 ||| 41465 ||| 41466 ||| 
2021 ||| multi-input transformer-less four-wire microinverter with distributed mppt for pv systems. ||| 41467 ||| 41468 ||| 41469 ||| 
2019 ||| modified method for transformer magnetizing characteristic computation and point-on-wave control switching for inrush current mitigation. ||| 41470 ||| 41471 ||| 41472 ||| 
2019 ||| analysis, modeling, and implementation of a new transformerless semi-quadratic buck-boost dc/dc converter. ||| 41473 ||| 22213 ||| 41474 ||| 41475 ||| 
2018 ||| generalized analytical formulae to compute electrical characteristics of a homogenous ladder network of the transformer winding. ||| 41476 ||| 41477 ||| 
2021 ||| design of high-isolation and low-loss single pole double throw switch based on the triple-coupled transformer for ultra-wideband phased array systems. ||| 846 ||| 41478 ||| 18644 ||| 5189 ||| 41479 ||| 
2017 ||| high-efficiency transformerless buck-boost dc-dc converter. ||| 41480 ||| 41481 ||| 
2021 ||| multi-input multi-phase transformerless large voltage conversion ratio dc/dc converter. ||| 41482 ||| 41483 ||| 41450 ||| 41484 ||| 
2021 ||| design and optimization of 3-kw inductive power transfer charging system with compact asymmetric loosely coupled transformer for special applications. ||| 41485 ||| 20327 ||| 2747 ||| 41486 ||| 41487 ||| 41488 ||| 
2018 ||| llc resonant converter utilizing a step-gap transformer structure for holdup time improvement. ||| 41489 ||| 41490 ||| 41491 ||| 
2020 ||| modelling characteristics of the impulse transformer in a wide frequency range. ||| 15494 ||| 15495 ||| 41492 ||| 
2018 ||| tuning the program transformers from cc to pdl. ||| 41493 ||| 41494 ||| 41495 ||| 41496 ||| 41497 ||| 41498 ||| 
2021 ||| mobile app-based chatbot to deliver cognitive behavioral therapy and psychoeducation for adults with attention deficit: a development and feasibility/usability study. ||| 41499 ||| 29806 ||| 41500 ||| 41501 ||| 41502 ||| 29807 ||| 
2022 ||| automatic snomed ct coding of chinese clinical terms via attention-based semantic matching. ||| 41503 ||| 41504 ||| 41505 ||| 31159 ||| 10331 ||| 
2022 ||| explainable icd multi-label classification of ehrs in spanish with convolutional attention. ||| 41506 ||| 13979 ||| 41507 ||| 3419 ||| 13983 ||| 13981 ||| 2600 ||| 
2020 ||| attention-deficit/ hyperactivity disorder mobile apps: a systematic review. ||| 41508 ||| 41509 ||| 41510 ||| 
2017 ||| robotic autonomous behavior selection using episodic memory and attention system. ||| 8709 ||| 41511 ||| 484 ||| 41512 ||| 41513 ||| 
2021 ||| improving novelty detection by self-supervised learning and channel attention mechanism. ||| 13192 ||| 13194 ||| 41514 ||| 32430 ||| 
2022 ||| an end-to-end deep context gate convolutional visual odometry system based on lightweight attention mechanism. ||| 2377 ||| 7300 ||| 41515 ||| 40994 ||| 
2021 ||| a dual deep neural network with phrase structure and attention mechanism for sentiment analysis. ||| 26469 ||| 41516 ||| 26470 ||| 41517 ||| 41518 ||| 
2020 ||| prediction of attentional focus from respiration with simple feed-forward and time delay neural networks. ||| 41519 ||| 41520 ||| 41521 ||| 41522 ||| 41523 ||| 
2022 ||| correction to: combining functional near-infrared spectroscopy and eeg measurements for the diagnosis of attention-deficit hyperactivity disorder. ||| 41524 ||| 15096 ||| 41525 ||| 41526 ||| 41527 ||| 41528 ||| 41529 ||| 41530 ||| 41531 ||| 41532 ||| 41533 ||| 
2020 ||| efficient human motion recovery using bidirectional attention network. ||| 23417 ||| 23418 ||| 13432 ||| 23419 ||| 
2022 ||| a topic-based multi-channel attention model under hybrid mode for image caption. ||| 41534 ||| 41535 ||| 
2021 ||| automatic identification of commodity label images using lightweight attention network. ||| 29532 ||| 29533 ||| 41536 ||| 15562 ||| 29534 ||| 
2021 ||| an attention-based cnn-lstm model for subjectivity detection in opinion-mining. ||| 41537 ||| 41538 ||| 41539 ||| 
2022 ||| dual attention granularity network for vehicle re-identification. ||| 29162 ||| 30596 ||| 19333 ||| 41540 ||| 41541 ||| 13196 ||| 
2019 ||| context-aware attention network for image recognition. ||| 16451 ||| 5439 ||| 41542 ||| 
2020 ||| crhasum: extractive text summarization with contextualized-representation hierarchical-attention summarization network. ||| 8973 ||| 8974 ||| 8967 ||| 8975 ||| 29909 ||| 8976 ||| 8977 ||| 8978 ||| 
2020 ||| a self-attention-based destruction and construction learning fine-grained image classification method for retail product recognition. ||| 14007 ||| 41543 ||| 41544 ||| 41545 ||| 41546 ||| 
2022 ||| traditional chinese medicine entity relation extraction based on cnn with segment attention. ||| 41547 ||| 12144 ||| 41548 ||| 6718 ||| 41549 ||| 
2021 ||| dilated causal convolution with multi-head self attention for sensor human activity recognition. ||| 41550 ||| 32890 ||| 41551 ||| 41552 ||| 14286 ||| 
2020 ||| multi-scale attention vehicle re-identification. ||| 17624 ||| 41553 ||| 41554 ||| 17623 ||| 5755 ||| 382 ||| 
2020 ||| combining functional near-infrared spectroscopy and eeg measurements for the diagnosis of attention-deficit hyperactivity disorder. ||| 41524 ||| 15096 ||| 41525 ||| 41526 ||| 41527 ||| 41528 ||| 41529 ||| 41530 ||| 41531 ||| 41532 ||| 41533 ||| 
2021 ||| cda-lstm: an evolutionary convolution-based dual-attention lstm for univariate time series prediction. ||| 41555 ||| 41556 ||| 6898 ||| 41557 ||| 41558 ||| 
2022 ||| triple-layer attention mechanism-based network embedding approach for anchor link identification across social networks. ||| 41559 ||| 41560 ||| 41561 ||| 41562 ||| 
2020 ||| a transformer-based approach to irony and sarcasm detection. ||| 32933 ||| 32934 ||| 32935 ||| 
2020 ||| deep joint two-stream wasserstein auto-encoder and selective attention alignment for unsupervised domain adaptation. ||| 6318 ||| 6915 ||| 31025 ||| 41563 ||| 41564 ||| 
2022 ||| aenet: an attention-enabled neural architecture for fake news detection using contextual features. ||| 41565 ||| 41566 ||| 41567 ||| 41568 ||| 15049 ||| 
2020 ||| fabnet: feature attention-based network for simultaneous segmentation of microvessels and nerves in routine histology images of oral cancer. ||| 41569 ||| 41570 ||| 41571 ||| 41572 ||| 41573 ||| 41574 ||| 34697 ||| 
2021 ||| a deep embedding model for knowledge graph completion based on attention mechanism. ||| 33028 ||| 41575 ||| 8092 ||| 1724 ||| 4080 ||| 1169 ||| 
2021 ||| transformer guided geometry model for flow-based unsupervised visual odometry. ||| 5199 ||| 20002 ||| 1703 ||| 29034 ||| 2365 ||| 20005 ||| 
2021 ||| han-regru: hierarchical attention network with residual gated recurrent unit for emotion recognition in conversation. ||| 41576 ||| 8349 ||| 41577 ||| 8974 ||| 
2021 ||| boosting attention fusion generative adversarial network for image denoising. ||| 41578 ||| 41579 ||| 41580 ||| 
2022 ||| deep semantic hashing with dual attention for cross-modal retrieval. ||| 41581 ||| 41582 ||| 41583 ||| 41584 ||| 23377 ||| 
2021 ||| adaptive feature fusion with attention mechanism for multi-scale target detection. ||| 41585 ||| 41586 ||| 41587 ||| 41588 ||| 
2020 ||| electronic word-of-mouth effects on studio performance leveraging attention-based model. ||| 1305 ||| 26780 ||| 41589 ||| 41590 ||| 41591 ||| 3725 ||| 21794 ||| 41592 ||| 
2020 ||| multi-granularity bidirectional attention stream machine comprehension method for emotion cause extraction. ||| 8973 ||| 8974 ||| 8967 ||| 8975 ||| 29909 ||| 8976 ||| 8978 ||| 728 ||| 
2020 ||| spatiotemporal saliency-based multi-stream networks with attention-aware lstm for action recognition. ||| 2801 ||| 21600 ||| 19437 ||| 41593 ||| 29274 ||| 
2020 ||| single-column cnn for crowd counting with pixel-wise attention mechanism. ||| 41594 ||| 30333 ||| 41595 ||| 41596 ||| 30335 ||| 27687 ||| 
2022 ||| discriminative attention-augmented feature learning for facial expression recognition in the wild. ||| 41597 ||| 41598 ||| 41599 ||| 41600 ||| 
2021 ||| character-based handwritten text transcription with attention networks. ||| 33689 ||| 33690 ||| 
2020 ||| single-image super-resolution with multilevel residual attention network. ||| 5323 ||| 737 ||| 
2022 ||| adaptive kernel selection network with attention constraint for surgical instrument classification. ||| 815 ||| 19850 ||| 13809 ||| 19851 ||| 16872 ||| 817 ||| 19289 ||| 
2020 ||| path-based reasoning with constrained type attention for knowledge graph completion. ||| 18181 ||| 390 ||| 18179 ||| 4296 ||| 1079 ||| 1081 ||| 1082 ||| 
2022 ||| hierarchical attention network for attributed community detection of joint representation. ||| 41601 ||| 262 ||| 31052 ||| 264 ||| 
2022 ||| multi-view dual attention network for 3d object recognition. ||| 41235 ||| 41602 ||| 128 ||| 
2022 ||| laanet: lightweight attention-guided asymmetric network for real-time semantic segmentation. ||| 21150 ||| 41603 ||| 41604 ||| 41605 ||| 
2021 ||| local-aware spatio-temporal attention network with multi-stage feature fusion for human action recognition. ||| 815 ||| 4635 ||| 816 ||| 822 ||| 19851 ||| 11988 ||| 817 ||| 
2022 ||| modeling and experimental validation of dry-type transformers with multiobjective swarm intelligence-based optimization algorithms for industrial application. ||| 41606 ||| 41607 ||| 41608 ||| 41609 ||| 41610 ||| 41611 ||| 41612 ||| 41613 ||| 
2020 ||| application of improved genetic algorithm in ultrasonic location of transformer partial discharge. ||| 41614 ||| 11358 ||| 41615 ||| 
2020 ||| dkd-dad: a novel framework with discriminative kinematic descriptor and deep attention-pooled descriptor for action recognition. ||| 30619 ||| 41616 ||| 18044 ||| 6805 ||| 41617 ||| 
2020 ||| interactive knowledge-enhanced attention network for answer selection. ||| 41618 ||| 9677 ||| 1081 ||| 
2021 ||| improve relation extraction with dual attention-guided graph convolutional networks. ||| 264 ||| 20282 ||| 973 ||| 631 ||| 613 ||| 262 ||| 
2019 ||| in-air handwritten english word recognition using attention recurrent translator. ||| 41619 ||| 19506 ||| 
2022 ||| temporal attention augmented transformer hawkes process. ||| 414 ||| 415 ||| 416 ||| 417 ||| 
2021 ||| a structure distinguishable graph attention network for knowledge base completion. ||| 5841 ||| 5840 ||| 4289 ||| 41620 ||| 
2020 ||| recurrent neural network with attention mechanism for language model. ||| 41621 ||| 41622 ||| 41623 ||| 41624 ||| 
2021 ||| residual attention convolutional autoencoder for feature learning and fault detection in nonlinear industrial processes. ||| 5902 ||| 28346 ||| 41625 ||| 
2020 ||| tilegan: category-oriented attention-based high-quality tiled clothes generation from dressed person. ||| 19482 ||| 30748 ||| 5519 ||| 5543 ||| 
2021 ||| graph convolutional networks with attention for multi-label weather recognition. ||| 41626 ||| 31417 ||| 5278 ||| 41627 ||| 13750 ||| 
2021 ||| frequency-amplitude coupling: a new approach for decoding of attended features in covert visual attention task. ||| 41628 ||| 41629 ||| 41630 ||| 
2019 ||| dilated residual attention network for load disaggregation. ||| 41631 ||| 41632 ||| 41633 ||| 25099 ||| 181 ||| 
2021 ||| application of solely self-attention mechanism in csi-fingerprinting-based indoor localization. ||| 41634 ||| 41635 ||| 
2021 ||| dgfau-net: global feature attention upsampling network for medical image segmentation. ||| 29271 ||| 41636 ||| 41637 ||| 41638 ||| 
2021 ||| two-branch encoding and iterative attention decoding network for semantic segmentation. ||| 29308 ||| 1254 ||| 8622 ||| 8750 ||| 
2022 ||| gsta: gated spatial-temporal attention approach for travel time prediction. ||| 41639 ||| 41640 ||| 38827 ||| 
2021 ||| improving neural machine translation using gated state network and focal adaptive attention networtk. ||| 13781 ||| 13782 ||| 41641 ||| 6828 ||| 13783 ||| 
2020 ||| kganet: a knowledge graph attention network for enhancing natural language inference. ||| 8467 ||| 41642 ||| 22629 ||| 
2021 ||| automatic lumbar spinal mri image segmentation with a multi-scale attention network. ||| 41643 ||| 41588 ||| 2054 ||| 41644 ||| 41645 ||| 41646 ||| 41647 ||| 40729 ||| 
2021 ||| dual-attention network with multitask learning for multistep short-term speed prediction on expressways. ||| 166 ||| 167 ||| 1894 ||| 
2021 ||| dm-ctsa: a discriminative multi-focused and complementary temporal/spatial attention framework for action recognition. ||| 30619 ||| 41648 ||| 41649 ||| 41650 ||| 41616 ||| 
2020 ||| arbitrary-oriented object detection via dense feature fusion and attention model for remote sensing super-resolution image. ||| 41651 ||| 41652 ||| 29274 ||| 41653 ||| 41654 ||| 9576 ||| 41655 ||| 2233 ||| 
2020 ||| deep refinement: capsule network with attention mechanism-based system for text classification. ||| 957 ||| 41656 ||| 41657 ||| 41658 ||| 41659 ||| 
2021 ||| a multimodal fake news detection model based on crossmodal attention residual and multichannel convolutional neural networks. ||| 41660 ||| 16316 ||| 25191 ||| 411 ||| 
2021 ||| adversarial network integrating dual attention and sparse representation for semi-supervised semantic segmentation. ||| 41661 ||| 41662 ||| 9645 ||| 
2020 ||| aldonar: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalized domain ontology and a regularized neural attention model. ||| 18373 ||| 14072 ||| 
2020 ||| a comparative study of outfit recommendation methods with a focus on attention-based fusion. ||| 33850 ||| 10702 ||| 
2022 ||| a convolutional attention network for unifying general and sequential recommenders. ||| 41663 ||| 41664 ||| 41665 ||| 41666 ||| 41667 ||| 41668 ||| 
2022 ||| fine-grained citation count prediction via a transformer-based model with among-attention mechanism. ||| 41669 ||| 18572 ||| 40470 ||| 3131 ||| 41670 ||| 2182 ||| 
2019 ||| attention-based long short-term memory network using sentiment lexicon embedding for aspect-level sentiment analysis in korean. ||| 40504 ||| 40503 ||| 40505 ||| 
2019 ||| aspect-based sentiment analysis with alternating coattention networks. ||| 497 ||| 41671 ||| 170 ||| 29376 ||| 
2020 ||| mgat: multimodal graph attention network for recommendation. ||| 35378 ||| 41672 ||| 1894 ||| 1063 ||| 35376 ||| 3605 ||| 
2022 ||| multi-attribute adaptive aggregation transformer for vehicle re-identification. ||| 19750 ||| 41673 ||| 41674 ||| 41675 ||| 41676 ||| 
2020 ||| multi-modal fusion with multi-level attention for visual dialog. ||| 41677 ||| 3304 ||| 7648 ||| 
2022 ||| group event recommendation based on graph multi-head attention network combining explicit and implicit information. ||| 25167 ||| 25168 ||| 25170 ||| 40611 ||| 
2022 ||| research of chinese intangible cultural heritage knowledge graph construction and attribute value extraction with graph attention network. ||| 41678 ||| 1371 ||| 
2020 ||| exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition. ||| 242 ||| 12309 ||| 41679 ||| 645 ||| 
2019 ||| tdam: a topic-dependent attention model for sentiment analysis. ||| 3661 ||| 3662 ||| 3664 ||| 
2020 ||| dynamic attention-based explainable recommendation with textual and visual fusion. ||| 10572 ||| 41680 ||| 41681 ||| 
2020 ||| image caption generation with dual attention mechanism. ||| 9105 ||| 41682 ||| 41683 ||| 41684 ||| 11284 ||| 
2020 ||| hierarchical neural query suggestion with an attention mechanism. ||| 1045 ||| 1046 ||| 1047 ||| 1048 ||| 
2020 ||| video question answering via grounded cross-attention network learning. ||| 7651 ||| 13412 ||| 7650 ||| 41685 ||| 17723 ||| 1937 ||| 7652 ||| 
2022 ||| bert-smap: paying attention to essential terms in passage ranking beyond bert. ||| 41686 ||| 41687 ||| 5125 ||| 41688 ||| 11677 ||| 13179 ||| 
2020 ||| transformer based contextualization of pre-trained word embeddings for irony detection in twitter. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 16474 ||| 41689 ||| 16476 ||| 
2022 ||| combining non-sampling and self-attention for sequential recommendation. ||| 41690 ||| 41691 ||| 41692 ||| 41693 ||| 23436 ||| 
2021 ||| deepgrp: engineering a software tool for predicting genomic repetitive elements using recurrent neural networks with attention. ||| 41694 ||| 41695 ||| 
2020 ||| mediation criteria for interactive serious games aimed at improving learning in children with attention deficit hyperactivity disorder (adhd). ||| 41696 ||| 41697 ||| 41698 ||| 41699 ||| 
2021 ||| joint attention behaviour in remote collaborative problem solving: exploring different attentional levels in dyadic interaction. ||| 41700 ||| 41701 ||| 41702 ||| 41703 ||| 16413 ||| 41704 ||| 41705 ||| 41706 ||| 
2021 ||| correction to: mediation criteria for interactive serious games aimed at improving learning in children with attention deficit hyperactivity disorder (adhd). ||| 41696 ||| 41697 ||| 41698 ||| 41699 ||| 
2021 ||| attention: there is an inconsistency between android permissions and application metadata! ||| 41707 ||| 48 ||| 41708 ||| 
2021 ||| temporal pyramid attention-based spatiotemporal fusion model for parkinson's disease diagnosis from gait data. ||| 41709 ||| 17601 ||| 40405 ||| 
2020 ||| a systematic study of inner-attention-based sentence representations in multilingual neural machine translation. ||| 23764 ||| 23765 ||| 2698 ||| 23766 ||| 23768 ||| 4194 ||| 23767 ||| 
2021 ||| technological troubleshooting based on sentence embedding with deep transformers. ||| 41710 ||| 41711 ||| 41712 ||| 
2021 ||| remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. ||| 39262 ||| 41713 ||| 2527 ||| 31135 ||| 
2021 ||| implementation of a novel algorithm of wheelset and axle box concurrent fault identification based on an efficient neural network with the attention mechanism. ||| 41714 ||| 41715 ||| 1955 ||| 41716 ||| 
2019 ||| guidelines to design tangible tabletop activities for children with attention deficit hyperactivity disorder. ||| 41717 ||| 41718 ||| 41697 ||| 41719 ||| 41698 ||| 25636 ||| 
2020 ||| self-tracking while doing sport: comfort, motivation, attention and lifestyle of athletes using personal informatics tools. ||| 41720 ||| 41721 ||| 
2022 ||| the influence of audio effects and attention on the perceived duration of interaction. ||| 41722 ||| 1451 ||| 20990 ||| 
2019 ||| the mere presence of an attentive and emotionally responsive virtual character influences focus of attention and perceived stress. ||| 16529 ||| 41723 ||| 41724 ||| 16530 ||| 41725 ||| 41726 ||| 41727 ||| 16534 ||| 
2018 ||| attention allocation for human multi-robot control: cognitive analysis based on behavior data and hidden states. ||| 41728 ||| 41729 ||| 41730 ||| 41731 ||| 4601 ||| 4602 ||| 
2018 ||| collective attention and active consumer participation in community energy systems. ||| 41732 ||| 41733 ||| 
2020 ||| attention-based activation pruning to reduce data movement in real-time ai: a case-study on local motion planning in autonomous vehicles. ||| 41734 ||| 41735 ||| 548 ||| 
2021 ||| transformerless three-level flying-capacitor step-up pv micro-inverter without electrolytic capacitors. ||| 41736 ||| 41737 ||| 41738 ||| 
2019 ||| visual attention-aware omnidirectional video streaming using optimal tiles for virtual reality. ||| 1595 ||| 21740 ||| 21741 ||| 1597 ||| 
2020 ||| anchor-free object detection with mask attention. ||| 41739 ||| 41740 ||| 31954 ||| 
2017 ||| predicting students' attention in the classroom from kinect facial and body features. ||| 3017 ||| 41741 ||| 
2021 ||| steganographic visual story with mutual-perceived joint attention. ||| 41742 ||| 25029 ||| 25030 ||| 
2020 ||| learning attention for object tracking with adversarial learning network. ||| 6560 ||| 41743 ||| 35042 ||| 41744 ||| 
2021 ||| retinal vessel segmentation with constrained-based nonnegative matrix factorization and 3d modified attention u-net. ||| 882 ||| 41745 ||| 
2020 ||| collaboration or battle between minds? an attention training game through collaborative and competitive reinforcement. ||| 41746 ||| 
2019 ||| the influence of graphical elements on user's attention and control on a neurofeedback-based game. ||| 6692 ||| 6693 ||| 
2022 ||| multi-label classification of legislative contents with hierarchical label attention networks. ||| 41747 ||| 5335 ||| 41748 ||| 41749 ||| 41750 ||| 
2021 ||| paying attention to video object pattern understanding. ||| 2444 ||| 2445 ||| 19316 ||| 3303 ||| 2163 ||| 
2020 ||| neural machine translation with deep attention. ||| 3180 ||| 3181 ||| 3182 ||| 
2022 ||| covariance attention for semantic segmentation. ||| 6635 ||| 6634 ||| 6636 ||| 41751 ||| 
2018 ||| two-stream transformer networks for video-based face alignment. ||| 5170 ||| 1920 ||| 41752 ||| 1921 ||| 
2022 ||| visual grounding via accumulated attention. ||| 19281 ||| 6417 ||| 1827 ||| 10072 ||| 10069 ||| 6413 ||| 
2019 ||| focal visual-text attention for memex question answering. ||| 18702 ||| 18703 ||| 12717 ||| 9303 ||| 14441 ||| 18704 ||| 
2017 ||| aligning where to see and what to tell: image captioning with region-based attention and scene-specific contexts. ||| 12746 ||| 41753 ||| 367 ||| 17735 ||| 12748 ||| 
2019 ||| predicting the driver's focus of attention: the dr(eye)ve project. ||| 34755 ||| 34756 ||| 34757 ||| 34758 ||| 13611 ||| 
2021 ||| attention-based dropout layer for weakly supervised single object localization and semantic segmentation. ||| 2090 ||| 41754 ||| 18694 ||| 
2022 ||| parallax attention for unsupervised stereo correspondence learning. ||| 19138 ||| 7271 ||| 19139 ||| 19140 ||| 19141 ||| 19308 ||| 19143 ||| 
2022 ||| zero-shot video object segmentation with co-attention siamese networks. ||| 19316 ||| 2444 ||| 2445 ||| 9384 ||| 2166 ||| 
2022 ||| heterogeneous graph attention network for unsupervised multiple-target domain adaptation. ||| 5157 ||| 2306 ||| 41755 ||| 1756 ||| 
2019 ||| aster: an attentional scene text recognizer with flexible rectification. ||| 18898 ||| 41756 ||| 2219 ||| 37872 ||| 7426 ||| 17429 ||| 
2019 ||| a deep network solution for attention and aesthetics aware photo cropping. ||| 2444 ||| 2445 ||| 2163 ||| 
2020 ||| guided attention inference network. ||| 2232 ||| 1744 ||| 1746 ||| 19330 ||| 1734 ||| 
2020 ||| hierarchical lstms with adaptive attention for visual captioning. ||| 1039 ||| 17738 ||| 9576 ||| 1040 ||| 
2022 ||| purely attention based local feature integration for video classification. ||| 18016 ||| 3774 ||| 1759 ||| 136 ||| 41757 ||| 2531 ||| 2190 ||| 
2018 ||| tracking gaze and visual focus of attention of people involved in social interaction. ||| 34335 ||| 41758 ||| 34338 ||| 
2022 ||| mra-net: improving vqa via multi-modal relation attention network. ||| 275 ||| 11466 ||| 369 ||| 19044 ||| 1040 ||| 
2020 ||| gravitational laws of focus of attention. ||| 896 ||| 897 ||| 898 ||| 
2022 ||| universal adversarial attack on attention and the resulting dataset damagenet. ||| 37333 ||| 37855 ||| 37856 ||| 1132 ||| 11266 ||| 
2020 ||| leader-based multi-scale attention deep architecture for person re-identification. ||| 41759 ||| 6365 ||| 2198 ||| 17842 ||| 4970 ||| 
2017 ||| windowing-based threshold technique to play the simple breakout game at neutral attention level. ||| 41760 ||| 21843 ||| 
2021 ||| review helpfulness evaluation and recommendation based on an attention model of customer expectation. ||| 15071 ||| 15072 ||| 15073 ||| 15074 ||| 
2021 ||| unsupervised spatial-awareness attention-based and multi-scale domain adaption network for point cloud classification. ||| 7128 ||| 6496 ||| 6497 ||| 1828 ||| 3072 ||| 41761 ||| 
2019 ||| a compact transformer-combined polar/quadrature reconfigurable digital power amplifier in 28-nm logic lp cmos. ||| 25236 ||| 25238 ||| 25237 ||| 41053 ||| 25239 ||| 5935 ||| 25268 ||| 22422 ||| 
2020 ||| highly integrated zvs flyback converter ics with pulse transformer to optimize usb power delivery for fast-charging mobile devices. ||| 41762 ||| 41763 ||| 41764 ||| 41765 ||| 41766 ||| 41767 ||| 41768 ||| 41769 ||| 41770 ||| 
2017 ||| on the design of wideband transformer-based fourth order matching networks for e-band receivers in 28-nm cmos. ||| 41771 ||| 22392 ||| 
2022 ||| an 85-ghz power amplifier utilizing a transformer-based power combiner operating beyond the self-resonance frequency. ||| 41772 ||| 41773 ||| 
2019 ||| a compact dual-band digital polar doherty power amplifier using parallel-combining transformer. ||| 25236 ||| 25237 ||| 25238 ||| 25239 ||| 25240 ||| 22422 ||| 
2017 ||| a 7.9-ghz transformer-feedback quadrature oscillator with a noise-shifting coupling network. ||| 41774 ||| 29967 ||| 
2020 ||| a broadband switched-transformer digital power amplifier for deep back-off efficiency enhancement. ||| 25236 ||| 5935 ||| 25237 ||| 25267 ||| 25240 ||| 25268 ||| 22422 ||| 
2018 ||| analysis and design of ultra-wideband mm-wave injection-locked frequency dividers using transformer-based high-order resonators. ||| 41775 ||| 41776 ||| 41414 ||| 41416 ||| 41418 ||| 
2021 ||| s/dec, recurrent attention in-memory processor for keyword spotting. ||| 5767 ||| 5768 ||| 5769 ||| 5770 ||| 
2021 ||| millimeter-wave sige radiometer front end with transformer-based dicke switch and on-chip calibration noise source. ||| 41777 ||| 41778 ||| 41779 ||| 41780 ||| 41781 ||| 25225 ||| 
2019 |||  1-w isolated power transfer system using fully integrated transformer with magnetic core. ||| 5765 ||| 25273 ||| 5764 ||| 25274 ||| 12417 ||| 25275 ||| 41782 ||| 5766 ||| 
2021 ||| quadrature switched/floated capacitor power amplifier with reconfigurable self-coupling canceling transformer for deep back-off efficiency enhancement. ||| 25276 ||| 25277 ||| 25278 ||| 
2020 ||| highly linear high-power 802.11ac/ax wlan sige hbt power amplifiers with a compact 2nd-harmonic-shorted four-way transformer and a thermally compensating dynamic bias circuit. ||| 25220 ||| 41783 ||| 25225 ||| 
2019 ||| an 82-107.6-ghz integer-n adpll employing a dco with split transformer and dual-path switched-capacitor ladder and a clock-skew-sampling delta-sigma tdc. ||| 25227 ||| 29967 ||| 
2021 ||| eeg-based emotion recognition via capsule network with channel-wise attention and lstm models. ||| 41784 ||| 41785 ||| 41786 ||| 41787 ||| 
2019 ||| reinforcement adaptation of an attention-based neural natural language generator for spoken dialogue systems. ||| 14435 ||| 14436 ||| 2693 ||| 14437 ||| 14438 ||| 14439 ||| 
2021 ||| rational, emotional, and attentional models for recommender systems. ||| 13531 ||| 13532 ||| 13533 ||| 13534 ||| 3419 ||| 13535 ||| 4252 ||| 
2020 ||| air pollution forecasting based on attention-based lstm neural network and ensemble learning. ||| 41788 ||| 38059 ||| 17194 ||| 41789 ||| 
2022 ||| weight attention layer-based document classification incorporating information gain. ||| 33385 ||| 41790 ||| 11552 ||| 
2022 ||| attention deep learning-based large-scale learning classifier for cassava leaf disease classification. ||| 1534 ||| 41791 ||| 41792 ||| 
2018 ||| modelling a smart environment for nonintrusive analysis of attention in the workplace. ||| 8432 ||| 227 ||| 41793 ||| 8434 ||| 8435 ||| 
2021 ||| cogcn: combining co-attention with graph convolutional network for entity linking with knowledge graphs. ||| 41794 ||| 1287 ||| 1288 ||| 41795 ||| 
2021 ||| an attention-driven videogame based on steady-state motion visual evoked potentials. ||| 41796 ||| 41797 ||| 41798 ||| 41799 ||| 
2021 ||| resnet-attention model for human authentication using ecg signals. ||| 41800 ||| 41801 ||| 20424 ||| 41802 ||| 
2020 ||| a secure mhealth application for attention deficit and hyperactivity disorder. ||| 41803 ||| 41804 ||| 2600 ||| 41805 ||| 41806 ||| 3419 ||| 41807 ||| 41808 ||| 
2019 ||| adaptive attention annotation model: optimizing the prediction path through dependency fusion. ||| 41809 ||| 3114 ||| 5182 ||| 5181 ||| 15770 ||| 41810 ||| 41811 ||| 41812 ||| 
2021 ||| s2san: a sentence-to-sentence attention network for sentiment analysis of online reviews. ||| 2885 ||| 38180 ||| 41813 ||| 
2021 ||| improving fake news detection with domain-adversarial and graph-attention neural network. ||| 41814 ||| 6837 ||| 41815 ||| 36447 ||| 2349 ||| 
2021 ||| process data properties matter: introducing gated convolutional neural networks (gcnn) and key-value-predict attention networks (kvp) for next event prediction with deep learning. ||| 41816 ||| 41817 ||| 41818 ||| 41819 ||| 
2018 ||| grounding by attention simulation in peripersonal space: pupils dilate to pinch grip but not big size nominal classifier. ||| 41820 ||| 41821 ||| 
2021 ||| curious objects: how visual complexity guides attention and engagement. ||| 41822 ||| 41823 ||| 
2022 ||| effect of repeated exposure to the visual environment on young children's attention. ||| 12844 ||| 41824 ||| 12953 ||| 12954 ||| 12846 ||| 
2018 ||| dividing attention between tasks: testing whether explicit payoff functions elicit optimal dual-task performance. ||| 41825 ||| 41826 ||| 41827 ||| 15404 ||| 
2020 ||| the role of attention in word recognition: results from ob1-reader. ||| 41828 ||| 41829 ||| 41830 ||| 41831 ||| 41832 ||| 
2021 ||| how reliably do eye parameters indicate internal versus external attentional focus? ||| 26915 ||| 41833 ||| 13862 ||| 26914 ||| 41834 ||| 33598 ||| 26909 ||| 
2018 ||| functional equivalence of sleep loss and time on task effects in sustained attention. ||| 41835 ||| 41836 ||| 
2019 ||| the cognitive architecture of perceived animacy: intention, attention, and memory. ||| 41837 ||| 41838 ||| 41839 ||| 41840 ||| 33090 ||| 
2018 ||| linguistic and perceptual mapping in spatial representations: an attentional account. ||| 41841 ||| 7111 ||| 852 ||| 41842 ||| 41843 ||| 3882 ||| 16145 ||| 41844 ||| 
2019 ||| degree of language experience modulates visual attention to visible speech and iconic gestures during clear and degraded speech comprehension. ||| 41845 ||| 41846 ||| 41847 ||| 41848 ||| 41849 ||| 
2021 ||| attention does not affect the speed of subjective time, but whether temporal information guides performance: a large-scale study of intrinsically motivated timers in a real-time strategy game. ||| 41850 ||| 41851 ||| 
2017 ||| automatic mechanisms for social attention are culturally penetrable. ||| 41852 ||| 41853 ||| 41854 ||| 41855 ||| 
2021 ||| diverse dance synthesis via keyframes with transformer controllers. ||| 41856 ||| 4815 ||| 41857 ||| 41858 ||| 
2018 ||| visual attention for rendered 3d shapes. ||| 11609 ||| 15325 ||| 15326 ||| 41859 ||| 41860 ||| 41861 ||| 
2020 ||| scga-net: skip connections global attention network for image restoration. ||| 12667 ||| 12035 ||| 12668 ||| 12669 ||| 
2017 ||| flicker observer effect: guiding attention through high frequency flicker in images. ||| 41862 ||| 41863 ||| 41864 ||| 
2018 ||| visual attention feature (vaf) : a novel strategy for visual tracking based on cloud platform in intelligent surveillance systems. ||| 41865 ||| 9455 ||| 41623 ||| 28956 ||| 
2020 ||| detection of ship targets in photoelectric images based on an improved recurrent attention convolutional neural network. ||| 41866 ||| 41867 ||| 14036 ||| 41868 ||| 
2018 ||| attention-mechanism-based tracking method for intelligent internet of vehicles. ||| 35143 ||| 9705 ||| 8718 ||| 35144 ||| 35145 ||| 
2020 ||| network attack detection and visual payload labeling technology based on seq2seq architecture with attention mechanism. ||| 41869 ||| 13925 ||| 28370 ||| 41870 ||| 31872 ||| 
2021 ||| fusion of attentional and traditional convolutional networks for facial expression recognition. ||| 41871 ||| 41872 ||| 
2017 ||| self-rehabilitation of acquired brain injury patients including neglect and attention deficit disorder with a tablet game in a clinical setting. ||| 41873 ||| 41874 ||| 41875 ||| 41876 ||| 41877 ||| 22118 ||| 
2020 ||| location of ambulance bases for the attention of traffic accidents in mexico city. ||| 41878 ||| 41879 ||| 41880 ||| 3419 ||| 41881 ||| 
2021 ||| evaluation of in-service power transformer health condition for inspection, repair, and replacement (irr) maintenance planning in electric utilities. ||| 41882 ||| 41883 ||| 
2017 ||| differential relay reliability enhancement using fourth harmonic for a large power transformer. ||| 41884 ||| 41885 ||| 
2020 ||| parallel voltage sag compensator without an injection transformer. ||| 41886 ||| 41887 ||| 41888 ||| 
2021 ||| transformers for future medicinal chemists. ||| 41889 ||| 5335 ||| 
2021 ||| multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. ||| 41890 ||| 39784 ||| 41891 ||| 4523 ||| 41892 ||| 41893 ||| 39783 ||| 41894 ||| 2760 ||| 41895 ||| 41896 ||| 5250 ||| 41897 ||| 
2021 ||| global voxel transformer networks for augmented microscopy. ||| 25412 ||| 34221 ||| 23491 ||| 
2021 ||| modality attention and sampling enables deep learning with heterogeneous marker combinations in fluorescence microscopy. ||| 34881 ||| 34882 ||| 34883 ||| 34884 ||| 34885 ||| 3157 ||| 34886 ||| 41898 ||| 
2021 ||| mapping the space of chemical reactions using attention-based neural networks. ||| 34543 ||| 34544 ||| 34545 ||| 34546 ||| 34547 ||| 34548 ||| 34549 ||| 
2021 ||| 'attention, attention, exploring minds acknowledge digital structure!' the shift to digital humanities has happened, so what should information scientists do in response? ||| 41899 ||| 
2021 ||| attention based multi-agent intrusion detection systems using reinforcement learning. ||| 41900 ||| 2797 ||| 41901 ||| 2798 ||| 
2019 ||| attention with structure regularization for action recognition. ||| 41902 ||| 5348 ||| 41903 ||| 41904 ||| 
2019 ||| triple attention network for sentimental visual question answering. ||| 41905 ||| 14649 ||| 41906 ||| 41907 ||| 15278 ||| 
2019 ||| visual skeleton and reparative attention for part-of-speech image captioning system. ||| 8967 ||| 5746 ||| 
2021 ||| weakly supervised action segmentation with effective use of attention and self-attention. ||| 38050 ||| 33076 ||| 
2021 ||| multimodal attention networks for low-level vision-and-language navigation. ||| 37295 ||| 19003 ||| 19001 ||| 37296 ||| 13611 ||| 
2019 ||| drau: dual recurrent attention units for visual question answering. ||| 33054 ||| 33055 ||| 
2019 ||| residual attention unit for action recognition. ||| 41908 ||| 5746 ||| 41909 ||| 41910 ||| 
2020 ||| the synergy of double attention: combine sentence-level and word-level attention for image captioning. ||| 612 ||| 264 ||| 613 ||| 262 ||| 
2021 ||| skeleton-based action recognition via spatial and temporal transformer networks. ||| 20096 ||| 7307 ||| 7310 ||| 
2020 ||| cascade multi-head attention networks for action recognition. ||| 7820 ||| 8769 ||| 2149 ||| 
2017 ||| human attention in visual question answering: do humans and deep networks look at the same regions? ||| 12024 ||| 8569 ||| 22837 ||| 8567 ||| 8564 ||| 
2020 ||| an attention recurrent model for human cooperation detection. ||| 41911 ||| 3882 ||| 41912 ||| 41913 ||| 41914 ||| 41915 ||| 41916 ||| 
2020 ||| pyramid channel-based feature attention network for image dehazing. ||| 28779 ||| 128 ||| 6067 ||| 41917 ||| 17104 ||| 
2021 ||| main: multi-attention instance network for video segmentation. ||| 41918 ||| 41919 ||| 41920 ||| 10907 ||| 41921 ||| 41922 ||| 41923 ||| 9163 ||| 41924 ||| 10318 ||| 35812 ||| 
2021 ||| multi-scale attention network for image inpainting. ||| 41925 ||| 4399 ||| 4400 ||| 
2020 ||| ghost removal via channel attention in exposure fusion. ||| 19015 ||| 1241 ||| 1053 ||| 19865 ||| 41926 ||| 19017 ||| 31055 ||| 1107 ||| 19866 ||| 10075 ||| 
2020 ||| multi-attention fusion modeling for sentiment analysis of educational big data. ||| 41927 ||| 3666 ||| 22747 ||| 980 ||| 
2022 ||| magan: unsupervised low-light image enhancement guided by mixed-attention. ||| 41928 ||| 170 ||| 497 ||| 41929 ||| 41930 ||| 
2020 ||| a semi-supervised attention model for identifying authentic sneakers. ||| 11466 ||| 9591 ||| 41931 ||| 17587 ||| 41932 ||| 9592 ||| 
2021 ||| attention-aware heterogeneous graph neural network. ||| 41933 ||| 41934 ||| 
2018 ||| relation classification via recurrent neural network with attention and tensor layers. ||| 41935 ||| 41936 ||| 6496 ||| 3072 ||| 
2020 ||| ambiguous representations of semilattices, imperfect information, and predicate transformers. ||| 41937 ||| 41938 ||| 
2018 ||| external bridging and internal bonding: unlocking the generative resources of member time and attention spent in online communities. ||| 41939 ||| 41940 ||| 4396 ||| 
2019 ||| managerial attention alteration in integrated product-service development. ||| 41941 ||| 
2019 ||| human nature is not a machine: on liberty, attention engineering, and learning analytics. ||| 41942 ||| 
2020 ||| group password strength meter based on attention mechanism. ||| 41943 ||| 41944 ||| 2985 ||| 41945 ||| 41946 ||| 41947 ||| 
2019 ||| deepfocus: deep encoding brainwaves and emotions with multi-scenario behavior analytics for human attention enhancement. ||| 29395 ||| 5541 ||| 3049 ||| 2969 ||| 8976 ||| 41948 ||| 
2020 ||| spotlight on ai! - why everyone should pay attention now! ||| 41949 ||| 2906 ||| 41950 ||| 
2019 ||| a survey of attention deficit hyperactivity disorder identification using psychophysiological data. ||| 41951 ||| 41952 ||| 41953 ||| 41954 ||| 41955 ||| 
2021 ||| mitigating sentimental bias via a polar attention mechanism. ||| 15490 ||| 34913 ||| 15246 ||| 41956 ||| 15249 ||| 
2019 ||| hical self-attention networks. ||| 23769 ||| 41957 ||| 31073 ||| 41958 ||| 31076 ||| 31077 ||| 41959 ||| 41960 ||| 41961 ||| 31078 ||| 31082 ||| 23771 ||| 23770 ||| 
2020 ||| deep learning with attention supervision for automated motion artefact detection in quality control of cardiac t1-mapping. ||| 817 ||| 41962 ||| 41963 ||| 41964 ||| 41965 ||| 41966 ||| 41967 ||| 41968 ||| 41969 ||| 
2021 ||| fully-channel regional attention network for disease-location recognition with tongue images. ||| 29559 ||| 29558 ||| 29557 ||| 24634 ||| 29560 ||| 775 ||| 41970 ||| 38551 ||| 
2021 ||| multiple instance convolutional neural network with modality-based attention and contextual multi-instance learning pooling layer for effective differentiation between borderline and malignant epithelial ovarian tumors. ||| 41971 ||| 33093 ||| 3248 ||| 41972 ||| 26649 ||| 38646 ||| 41973 ||| 41974 ||| 13676 ||| 
2021 ||| health issue identification in social media based on multi-task hierarchical neural networks with topic attention. ||| 3663 ||| 41975 ||| 41976 ||| 
2022 ||| resattengan: simultaneous segmentation of multiple spinal structures on axial lumbar mri image using residual attention and adversarial learning. ||| 41977 ||| 41978 ||| 9283 ||| 27856 ||| 
2020 ||| fully convolutional attention network for biomedical image segmentation. ||| 29745 ||| 29079 ||| 29080 ||| 31221 ||| 41979 ||| 
2021 ||| an attention-based weakly supervised framework for spitzoid melanocytic lesion diagnosis in whole slide images. ||| 16460 ||| 34795 ||| 34796 ||| 34797 ||| 11927 ||| 34798 ||| 25710 ||| 34799 ||| 3369 ||| 34800 ||| 34801 ||| 34802 ||| 
2021 ||| tp-ddi: transformer-based pipeline for the extraction of drug-drug interactions. ||| 13154 ||| 13155 ||| 
2019 ||| recurrent neural networks with segment attention and entity description for relation extraction from clinical texts. ||| 22351 ||| 41980 ||| 41981 ||| 41982 ||| 
2021 ||| liver segmentation in abdominal ct images via auto-context neural network and self-supervised contour attention. ||| 34518 ||| 34519 ||| 39072 ||| 41983 ||| 34520 ||| 34521 ||| 
2022 ||| enhancing dynamic ecg heartbeat classification with lightweight transformer model. ||| 16881 ||| 41984 ||| 30112 ||| 41985 ||| 41986 ||| 301 ||| 
2020 ||| ecg-based multi-class arrhythmia detection using spatio-temporal attention-based convolutional recurrent neural network. ||| 875 ||| 41987 ||| 4213 ||| 6007 ||| 181 ||| 2184 ||| 
2019 ||| identifying user profile by incorporating self-attention mechanism based on csdn data set. ||| 41988 ||| 41989 ||| 41990 ||| 41991 ||| 41992 ||| 12623 ||| 17798 ||| 27992 ||| 
2021 ||| bidirectional guided attention network for 3-d semantic detection of remote sensing images. ||| 4392 ||| 4393 ||| 41993 ||| 2240 ||| 41994 ||| 
2022 ||| lightweight spectral-spatial attention network for hyperspectral image classification. ||| 13194 ||| 41995 ||| 41996 ||| 20185 ||| 30377 ||| 
2022 ||| a relation-augmented embedded graph attention network for remote sensing object detection. ||| 41997 ||| 41998 ||| 41999 ||| 11284 ||| 42000 ||| 23018 ||| 
2021 ||| hyperspectral image denoising using a 3-d attention denoising network. ||| 6919 ||| 42001 ||| 42002 ||| 42003 ||| 17760 ||| 
2022 ||| multiattention network for semantic segmentation of fine-resolution remote sensing images. ||| 8207 ||| 30454 ||| 13362 ||| 30455 ||| 32217 ||| 30557 ||| 42004 ||| 
2022 ||| cloud detection method using cnn based on cascaded feature attention and channel attention. ||| 875 ||| 1035 ||| 1341 ||| 4061 ||| 6701 ||| 
2020 ||| hyperspectral image super-resolution by band attention through adversarial learning. ||| 6699 ||| 30625 ||| 1717 ||| 6700 ||| 6701 ||| 2240 ||| 6720 ||| 
2022 ||| semantic segmentation with attention mechanism for remote sensing images. ||| 1872 ||| 42005 ||| 42006 ||| 6735 ||| 
2019 ||| visual attention-driven hyperspectral image classification. ||| 42007 ||| 42008 ||| 42009 ||| 6750 ||| 5536 ||| 
2022 ||| feedback attention-based dense cnn for hyperspectral image classification. ||| 288 ||| 42010 ||| 42011 ||| 42012 ||| 42013 ||| 
2021 ||| aru-net: reduction of atmospheric phase screen in sar interferometry using attention-based deep residual u-net. ||| 42014 ||| 35959 ||| 42015 ||| 42016 ||| 
2019 ||| scene classification with recurrent attention of vhr remote sensing images. ||| 6627 ||| 6921 ||| 8849 ||| 6922 ||| 
2022 ||| attention-based 3-d seismic fault segmentation training by a few 2-d slice labels. ||| 42017 ||| 42018 ||| 42019 ||| 6821 ||| 42020 ||| 
2022 ||| attention-aware dynamic self-aggregation network for satellite image time series classification. ||| 781 ||| 30284 ||| 42021 ||| 989 ||| 42022 ||| 42023 ||| 42024 ||| 30286 ||| 
2022 ||| transformer-based multistage enhancement for remote sensing image super-resolution. ||| 42025 ||| 6654 ||| 42026 ||| 
2022 ||| db-blendmask: decomposed attention and balanced blendmask for instance segmentation of high-resolution remote sensing images. ||| 42027 ||| 42028 ||| 42029 ||| 42030 ||| 42031 ||| 
2022 ||| bdanet: multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images. ||| 32436 ||| 2226 ||| 2229 ||| 2230 ||| 32437 ||| 32438 ||| 17199 ||| 6720 ||| 
2020 ||| scene-adaptive remote sensing image super-resolution using a multiscale attention network. ||| 6588 ||| 42032 ||| 4634 ||| 42033 ||| 42034 ||| 
2021 ||| attention-based adaptive spectral-spatial kernel resnet for hyperspectral image classification. ||| 33815 ||| 42035 ||| 11224 ||| 35959 ||| 
2022 ||| solo-to-collaborative dual-attention network for one-shot object detection in remote sensing images. ||| 41682 ||| 42036 ||| 42037 ||| 2365 ||| 2278 ||| 2414 ||| 
2020 ||| learning to pay attention on spectral domain: a spectral attention module-based convolutional network for hyperspectral image classification. ||| 16130 ||| 16131 ||| 
2021 ||| hyperspectral image classification based on 3-d octave convolution with spatial-spectral attention network. ||| 6617 ||| 42038 ||| 397 ||| 9552 ||| 30203 ||| 5743 ||| 400 ||| 
2022 ||| feature-grouped network with spectral-spatial connected attention for hyperspectral image classification. ||| 42039 ||| 29586 ||| 29587 ||| 
2022 ||| multilayer global spectral-spatial attention network for wetland hyperspectral image classification. ||| 42040 ||| 42041 ||| 42042 ||| 42043 ||| 36004 ||| 
2022 ||| multiframe video satellite image super-resolution via attention-based residual learning. ||| 42044 ||| 5536 ||| 5071 ||| 42045 ||| 42046 ||| 
2022 ||| mixed loss graph attention network for few-shot sar target classification. ||| 42047 ||| 42048 ||| 1052 ||| 6924 ||| 
2022 ||| rrnet: relational reasoning network with parallel multiscale attention for salient object detection in optical remote sensing images. ||| 35817 ||| 35818 ||| 6749 ||| 5536 ||| 4400 ||| 19728 ||| 
2022 ||| a multilevel encoder-decoder attention network for change detection in hyperspectral images. ||| 6870 ||| 42049 ||| 42050 ||| 6701 ||| 6843 ||| 
2021 ||| scene-driven multitask parallel attention network for building extraction in high-resolution remote sensing images. ||| 42051 ||| 6919 ||| 17757 ||| 17760 ||| 30558 ||| 42052 ||| 
2022 ||| deep multiscale siamese network with parallel convolutional structure and self-attention for change detection. ||| 42053 ||| 12666 ||| 42054 ||| 42055 ||| 23018 ||| 
2022 ||| effective pan-sharpening with transformer and invertible neural network. ||| 42056 ||| 19660 ||| 23312 ||| 34444 ||| 41987 ||| 42057 ||| 
2022 ||| attention and hybrid loss guided deep learning for consecutively missing seismic data reconstruction. ||| 42058 ||| 6752 ||| 
2021 ||| collaborative attention-based heterogeneous gated fusion network for land cover classification. ||| 6821 ||| 6820 ||| 6822 ||| 765 ||| 42059 ||| 
2021 ||| hyperspectral target detection with roi feature transformation and multiscale spectral attention. ||| 6869 ||| 6699 ||| 6868 ||| 42060 ||| 6701 ||| 
2022 ||| cross-attention spectral-spatial network for hyperspectral image classification. ||| 5492 ||| 7725 ||| 21747 ||| 8002 ||| 
2021 ||| attention multibranch convolutional neural network for hyperspectral image classification based on adaptive region search. ||| 6738 ||| 42061 ||| 30442 ||| 28536 ||| 4634 ||| 400 ||| 397 ||| 
2021 ||| optical remote sensing image change detection based on attention mechanism and image difference. ||| 42062 ||| 30553 ||| 5189 ||| 10063 ||| 
2022 ||| multiple attention-guided capsule networks for hyperspectral image classification. ||| 42008 ||| 42063 ||| 42064 ||| 42007 ||| 
2019 ||| remote sensing image superresolution using deep residual channel attention. ||| 42007 ||| 14327 ||| 1867 ||| 42065 ||| 42008 ||| 42009 ||| 6750 ||| 
2022 ||| a multiscale self-attention deep clustering for change detection in sar images. ||| 42066 ||| 30389 ||| 400 ||| 5743 ||| 3535 ||| 
2021 ||| lanet: local attention embedding to improve the semantic segmentation of remote sensing images. ||| 35958 ||| 435 ||| 35959 ||| 
2022 ||| end-to-end method with transformer for 3-d detection of oil tank from single sar image. ||| 5264 ||| 30216 ||| 30215 ||| 42067 ||| 42068 ||| 42069 ||| 42070 ||| 30469 ||| 
2021 ||| ship detection in large-scale sar images via spatial shuffle-group enhance attention. ||| 6712 ||| 6711 ||| 42071 ||| 6713 ||| 5233 ||| 
2022 ||| h2an: hierarchical homogeneity-attention network for hyperspectral image classification. ||| 11504 ||| 11505 ||| 11506 ||| 3034 ||| 11508 ||| 
2022 ||| distance weight-graph attention model-based high-resolution remote sensing urban functional zone identification. ||| 42072 ||| 42073 ||| 42074 ||| 42075 ||| 17149 ||| 42076 ||| 42077 ||| 
2022 ||| self-attention deep image prior network for unsupervised 3-d seismic data enhancement. ||| 42078 ||| 42079 ||| 42080 ||| 42081 ||| 42082 ||| 42083 ||| 42084 ||| 
2020 ||| hyperspectral pansharpening using deep prior and dual attention residual network. ||| 6868 ||| 6699 ||| 6701 ||| 8718 ||| 42085 ||| 8849 ||| 
2022 ||| net: spectral-spatial-semantic network for hyperspectral image classification with the multiway attention mechanism. ||| 42086 ||| 42087 ||| 42088 ||| 21748 ||| 
2022 ||| remote sensing image change detection with transformers. ||| 2424 ||| 39901 ||| 6654 ||| 
2022 ||| remote sensing image super-resolution via dual-resolution network based on connected attention mechanism. ||| 397 ||| 42089 ||| 15298 ||| 42090 ||| 6617 ||| 30378 ||| 400 ||| 
2022 ||| clustering feature constraint multiscale attention network for shadow extraction from remote sensing images. ||| 30505 ||| 42091 ||| 796 ||| 42092 ||| 25978 ||| 41270 ||| 28959 ||| 
2022 ||| single-image super-resolution for remote sensing images using a deep generative adversarial network with local and global attention mechanisms. ||| 42093 ||| 7111 ||| 42094 ||| 729 ||| 30430 ||| 42095 ||| 42096 ||| 42097 ||| 30431 ||| 
2021 ||| a spectral grouping and attention-driven residual dense network for hyperspectral image super-resolution. ||| 42098 ||| 4634 ||| 42032 ||| 
2021 ||| attention-based second-order pooling network for hyperspectral image classification. ||| 30287 ||| 42099 ||| 21184 ||| 30284 ||| 
2022 ||| attention-based multiscale residual adaptation network for cross-scene classification. ||| 42100 ||| 17757 ||| 17760 ||| 9889 ||| 
2022 ||| exploring vision transformers for polarimetric sar image classification. ||| 39691 ||| 39692 ||| 39693 ||| 
2021 ||| hyperspectral image classification with attention-aided cnns. ||| 34040 ||| 34041 ||| 6625 ||| 34042 ||| 34043 ||| 
2022 ||| high-resolution remote sensing image captioning based on structured attention. ||| 8164 ||| 6654 ||| 2185 ||| 
2022 ||| ground-based remote sensing cloud classification via context graph attention network. ||| 13805 ||| 42101 ||| 19052 ||| 42102 ||| 28433 ||| 
2022 ||| a spectral and spatial attention network for change detection in hyperspectral images. ||| 34992 ||| 30282 ||| 30379 ||| 30283 ||| 42103 ||| 3399 ||| 42104 ||| 1042 ||| 
2020 ||| hsi-bert: hyperspectral image classification using the bidirectional encoder representation from transformers. ||| 42105 ||| 42106 ||| 42107 ||| 42108 ||| 3337 ||| 
2020 ||| sound active attention framework for remote sensing image captioning. ||| 8002 ||| 5911 ||| 42109 ||| 
2022 ||| mrddanet: a multiscale residual dense dual attention network for sar image denoising. ||| 3854 ||| 1379 ||| 6642 ||| 8838 ||| 8841 ||| 31628 ||| 
2022 ||| a deformable attention network for high-resolution remote sensing images semantic segmentation. ||| 42110 ||| 42111 ||| 42112 ||| 38700 ||| 
2017 ||| robust infrared maritime target detection based on visual attention and spatiotemporal filtering. ||| 30600 ||| 379 ||| 15191 ||| 30601 ||| 
2021 ||| residual spectral-spatial attention network for hyperspectral image classification. ||| 42113 ||| 400 ||| 5743 ||| 36641 ||| 42114 ||| 
2021 ||| nas-guided lightweight multiscale attention fusion network for hyperspectral image classification. ||| 42114 ||| 42115 ||| 42116 ||| 41679 ||| 42113 ||| 36641 ||| 400 ||| 
2022 ||| multiple attention siamese network for high-resolution image change detection. ||| 42117 ||| 42118 ||| 214 ||| 42119 ||| 
2022 ||| radar hrrp target recognition model based on a stacked cnn-bi-rnn with attention mechanism. ||| 42120 ||| 42121 ||| 42122 ||| 31786 ||| 19958 ||| 9337 ||| 42123 ||| 17308 ||| 
2021 ||| recurrent thrifty attention network for remote sensing scene recognition. ||| 42124 ||| 1751 ||| 42125 ||| 
2022 ||| spectral super-resolution of multispectral images using spatial-spectral residual attention network. ||| 42109 ||| 42126 ||| 8002 ||| 
2021 ||| multiscale cnn with autoencoder regularization joint contextual attention network for sar image classification. ||| 42127 ||| 6829 ||| 400 ||| 
2022 ||| spectralformer: rethinking hyperspectral image classification with transformers. ||| 8848 ||| 39780 ||| 8847 ||| 30238 ||| 30240 ||| 6750 ||| 8849 ||| 
2021 ||| attention symbiotic neural network for hyperspectral image refined classification based on relative water content retrieval. ||| 42128 ||| 23018 ||| 12666 ||| 
2022 ||| super-resolution-based change detection network with stacked attention module for images with different resolutions. ||| 6918 ||| 6919 ||| 32692 ||| 32693 ||| 29235 ||| 17760 ||| 
2021 ||| attention-aware pseudo-3-d convolutional neural network for hyperspectral image classification. ||| 27413 ||| 16130 ||| 16131 ||| 32540 ||| 2188 ||| 
2022 ||| multilabel aerial image classification with a concept attention graph neural network. ||| 42129 ||| 27413 ||| 2932 ||| 2188 ||| 42130 ||| 
2022 ||| spectral-spatial transformer network for hyperspectral image classification: a factorized architecture search framework. ||| 19191 ||| 949 ||| 42131 ||| 19197 ||| 14892 ||| 
2022 ||| a deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection. ||| 6919 ||| 6918 ||| 13516 ||| 29235 ||| 2355 ||| 17760 ||| 
2021 ||| hybrid 2-d-3-d deep residual attentional network with structure tensor constraints for spectral super-resolution of rgb images. ||| 6699 ||| 6698 ||| 6700 ||| 6843 ||| 42132 ||| 1717 ||| 6701 ||| 
2022 ||| msacon: mining spatial attention-based contextual information for road extraction. ||| 42133 ||| 2424 ||| 31228 ||| 5536 ||| 
2022 ||| composite sequential network with poa attention for polsar image analysis. ||| 11453 ||| 5858 ||| 30687 ||| 37287 ||| 30686 ||| 
2020 ||| attention gans: unsupervised deep feature learning for aerial scene classification. ||| 9394 ||| 1737 ||| 42134 ||| 
2019 ||| multisource region attention network for fine-grained object recognition in remote sensing imagery. ||| 6902 ||| 35099 ||| 35100 ||| 
2022 ||| hybrid multiple attention network for semantic segmentation in aerial images. ||| 39968 ||| 10525 ||| 36631 ||| 30448 ||| 42135 ||| 12746 ||| 
2022 ||| ssa-siamnet: spectral-spatial-wise attention-based siamese network for hyperspectral image change detection. ||| 42136 ||| 30377 ||| 42137 ||| 42004 ||| 
2022 ||| ccanet: class-constraint coarse-to-fine attentional deep network for subdecimeter aerial image semantic segmentation. ||| 42138 ||| 42139 ||| 42140 ||| 42141 ||| 30511 ||| 
2022 ||| gated recurrent multiattention network for vhr remote sensing image classification. ||| 38367 ||| 7271 ||| 19142 ||| 19138 ||| 19139 ||| 19143 ||| 
2021 ||| addcnn: an attention-based deep dilated convolutional neural network for seismic facies analysis with interpretable spatial-spectral maps. ||| 19327 ||| 42142 ||| 19326 ||| 42143 ||| 
2022 ||| dynamic-hierarchical attention distillation with synergetic instance selection for land cover classification using missing heterogeneity images. ||| 6821 ||| 6820 ||| 6822 ||| 30332 ||| 
2021 ||| unsupervised pansharpening based on self-attention mechanism. ||| 32331 ||| 32332 ||| 7421 ||| 30439 ||| 
2020 ||| roi extraction based on multiview learning and attention mechanism for unbalanced remote sensing data set. ||| 11423 ||| 11503 ||| 9678 ||| 
2022 ||| attentional feature refinement and alignment network for aircraft detection in sar imagery. ||| 10646 ||| 80 ||| 29989 ||| 31528 ||| 30332 ||| 2014 ||| 
2021 ||| correction to "scene-driven multitask parallel attention network for building extraction in high-resolution remote sensing images". ||| 42051 ||| 6919 ||| 17757 ||| 17760 ||| 30558 ||| 42052 ||| 
2022 ||| end-to-end recognition of similar space cone-cylinder targets based on complex-valued coordinate attention networks. ||| 42144 ||| 31971 ||| 33700 ||| 31923 ||| 241 ||| 
2022 ||| cnn cloud detection algorithm based on channel and spatial attention and probabilistic upsampling for remote sensing image. ||| 875 ||| 4061 ||| 1341 ||| 1035 ||| 6701 ||| 
2022 ||| an attention-based hypocenter estimator for earthquake localization. ||| 42145 ||| 17201 ||| 42146 ||| 42147 ||| 
2019 ||| dense attention pyramid networks for multi-scale ship detection in sar images. ||| 6712 ||| 6721 ||| 6713 ||| 42071 ||| 
2022 ||| spectral partitioning residual network with spatial attention mechanism for hyperspectral image classification. ||| 397 ||| 42148 ||| 6617 ||| 6738 ||| 400 ||| 
2022 ||| global visual feature and linguistic state guided attention for remote sensing image captioning. ||| 30447 ||| 19850 ||| 30449 ||| 13676 ||| 12746 ||| 10525 ||| 
2022 ||| multistage dual-attention guided fusion network for hyperspectral pansharpening. ||| 42149 ||| 42150 ||| 
2020 ||| spectral-spatial attention network for hyperspectral image classification. ||| 7725 ||| 42109 ||| 8002 ||| 7655 ||| 
2022 ||| adaptive hash attention and lower triangular network for hyperspectral image classification. ||| 30334 ||| 30333 ||| 30335 ||| 27687 ||| 14650 ||| 6891 ||| 
2022 ||| vision transformer: an excellent teacher for guiding small networks in remote sensing image scene classification. ||| 15984 ||| 15986 ||| 15985 ||| 
2018 ||| radar and rain gauge merging-based precipitation estimation via geographical-temporal attention continuous conditional random field. ||| 42151 ||| 42152 ||| 35277 ||| 42153 ||| 
2021 ||| compact band weighting module based on attention-driven for hyperspectral image classification. ||| 1976 ||| 42154 ||| 2259 ||| 42155 ||| 42156 ||| 42157 ||| 
2018 ||| visual attention-based target detection and discrimination for high-resolution sar images in complex scenes. ||| 42158 ||| 26787 ||| 989 ||| 3501 ||| 2355 ||| 42159 ||| 42160 ||| 
2022 ||| hyperspectral image classification using attention-based bidirectional long short-term memory network. ||| 6719 ||| 42161 ||| 2530 ||| 42162 ||| 6720 ||| 
2022 ||| high-order markov random field as attention network for high-resolution remote-sensing image compression. ||| 42163 ||| 42164 ||| 42165 ||| 
2022 ||| multi-direction networks with attentional spectral prior for hyperspectral image classification. ||| 42060 ||| 6699 ||| 6701 ||| 6700 ||| 42166 ||| 6869 ||| 6720 ||| 
2022 ||| hyperspectral image classification based on deep attention graph convolutional network. ||| 18503 ||| 42167 ||| 13452 ||| 400 ||| 1562 ||| 42168 ||| 
2022 ||| nlrnet: an efficient nonlocal attention resnet for pansharpening. ||| 30188 ||| 2424 ||| 30190 ||| 20432 ||| 
2022 ||| spectral-spatial self-attention networks for hyperspectral image classification. ||| 28926 ||| 42169 ||| 38700 ||| 42170 ||| 42171 ||| 10070 ||| 42172 ||| 42173 ||| 
2022 ||| water retrieval embedded attention network with multiscale receptive fields for hyperspectral image refined classification. ||| 42128 ||| 23018 ||| 12666 ||| 
2021 ||| attentional local contrast networks for infrared small target detection. ||| 7144 ||| 7147 ||| 6289 ||| 7148 ||| 
2022 ||| map-net: sar and optical image matching via image-based convolutional network with attention mechanism and spatial pyramid aggregated pooling. ||| 42174 ||| 42175 ||| 17760 ||| 42141 ||| 30511 ||| 
2022 ||| center-boundary dual attention for oriented object detection in remote sensing images. ||| 9455 ||| 2037 ||| 1700 ||| 42176 ||| 
2021 ||| scattering enhanced attention pyramid network for aircraft detection in sar images. ||| 42177 ||| 30336 ||| 29359 ||| 
2022 ||| mutual attention inception network for remote sensing visual question answering. ||| 42109 ||| 5911 ||| 42178 ||| 8002 ||| 
2022 ||| transformer-based regression network for pansharpening remote sensing images. ||| 42179 ||| 31473 ||| 42180 ||| 
2022 ||| recurrent attention and semantic gate for remote sensing image captioning. ||| 10031 ||| 397 ||| 37721 ||| 399 ||| 398 ||| 6617 ||| 400 ||| 
2021 ||| remote sensing image super-resolution via mixed high-order attention network. ||| 19794 ||| 155 ||| 19793 ||| 1040 ||| 
2022 ||| spectral feature fusion networks with dual attention for hyperspectral image classification. ||| 9390 ||| 2523 ||| 42181 ||| 
2022 ||| unsupervised domain adaptation for semantic segmentation of high-resolution remote sensing imagery driven by category-certainty attention. ||| 1037 ||| 42182 ||| 42183 ||| 8110 ||| 340 ||| 42184 ||| 
2022 ||| attention-based multistage fusion network for remote sensing image pansharpening. ||| 42185 ||| 31473 ||| 42180 ||| 
2021 ||| attention and working memory. ||| 42186 ||| 28656 ||| 
2021 ||| computer models of saliency alone fail to predict subjective visual attention to landmarks during observed navigation. ||| 42187 ||| 42188 ||| 11561 ||| 42189 ||| 42190 ||| 42191 ||| 
2019 ||| classifying suspicious content in tor darknet through semantic attention keypoint filtering. ||| 30727 ||| 30728 ||| 30731 ||| 30732 ||| 2253 ||| 30729 ||| 30730 ||| 
2021 ||| neutron: an attention-based neural decompiler. ||| 38474 ||| 8539 ||| 42192 ||| 472 ||| 
2020 ||| a dga domain names detection modeling method based on integrating an attention mechanism and deep neural network. ||| 6305 ||| 6306 ||| 6974 ||| 6307 ||| 
2022 ||| application of the crow search algorithm to the problem of the parametric estimation in transformers considering voltage and current measures. ||| 42193 ||| 23167 ||| 3369 ||| 42194 ||| 6235 ||| 42195 ||| 
2021 ||| black-hole optimization applied to the parametric estimation in distribution transformers considering voltage and current measures. ||| 42196 ||| 3419 ||| 42197 ||| 42195 ||| 42198 ||| 
2019 ||| the application of ant colony algorithms to improving the operation of traction rectifier transformers. ||| 42199 ||| 42200 ||| 42201 ||| 
2021 ||| fine-grained cross-modal retrieval for cultural items with focal attention and hierarchical encodings. ||| 42202 ||| 33850 ||| 7814 ||| 10702 ||| 
2021 ||| crowd counting with segmentation attention convolutional neural network. ||| 40091 ||| 42203 ||| 
2021 ||| towards accurate coronary artery calcium segmentation with multi-scale attention mechanism. ||| 16739 ||| 42204 ||| 42205 ||| 13794 ||| 
2020 ||| multi-head mutual-attention cyclegan for unpaired image-to-image translation. ||| 2066 ||| 42206 ||| 920 ||| 
2019 ||| spatial non-local attention for thoracic disease diagnosis and visualisation in weakly supervised learning. ||| 42207 ||| 9741 ||| 35277 ||| 
2021 ||| hard exudate segmentation in retinal image with attention mechanism. ||| 17608 ||| 17609 ||| 1305 ||| 35867 ||| 
2021 ||| ambcr: low-light image enhancement via attention guided multi-branch construction and retinex theory. ||| 29626 ||| 18530 ||| 42208 ||| 42209 ||| 42210 ||| 
2022 ||| esa-cyclegan: edge feature and self-attention based cycle-consistent generative adversarial network for style transfer. ||| 1052 ||| 24787 ||| 29056 ||| 
2020 ||| pavement crack detection network based on pyramid structure and attention mechanism. ||| 11589 ||| 28498 ||| 11592 ||| 
2022 ||| deep coordinate attention network for single image super-resolution. ||| 28799 ||| 42211 ||| 42212 ||| 
2017 ||| ir small target detection based on human visual attention using pulsed discrete cosine transform. ||| 42213 ||| 42214 ||| 42215 ||| 
2020 ||| dual attention convolutional network for action recognition. ||| 42216 ||| 42217 ||| 16591 ||| 42218 ||| 1505 ||| 
2021 ||| a multi-class covid-19 segmentation network with pyramid attention and edge loss in ct images. ||| 42219 ||| 1107 ||| 38229 ||| 42220 ||| 12500 ||| 19025 ||| 
2021 ||| alzheimer's disease diagnosis based on the visual attention model and equal-distance ring shape context features. ||| 42221 ||| 42222 ||| 42223 ||| 42224 ||| 
2021 ||| part-level attention networks for cross-domain person re-identification. ||| 42225 ||| 42226 ||| 42227 ||| 42228 ||| 42229 ||| 8012 ||| 8880 ||| 40045 ||| 42230 ||| 9576 ||| 
2020 ||| object counting method based on dual attention network. ||| 40525 ||| 8440 ||| 40526 ||| 
2021 ||| uda-net: densely attention network for underwater image enhancement. ||| 438 ||| 19629 ||| 
2021 ||| multi-dimensional weighted cross-attention network in crowded scenes. ||| 42231 ||| 27160 ||| 42232 ||| 42233 ||| 42234 ||| 42235 ||| 
2021 ||| efficient recurrent attention network for remote sensing scene classification. ||| 42236 ||| 17759 ||| 
2022 ||| learning spatial self-attention information for visual tracking. ||| 42237 ||| 42238 ||| 37077 ||| 42239 ||| 42240 ||| 
2020 ||| robust image hashing with visual attention model and invariant moments. ||| 42241 ||| 42242 ||| 17857 ||| 42243 ||| 42244 ||| 42245 ||| 
2022 ||| ensemble cross-stage partial attention network for image classification. ||| 13541 ||| 14155 ||| 
2021 ||| ca-pmg: channel attention and progressive multi-granularity training network for fine-grained visual classification. ||| 42246 ||| 13442 ||| 42247 ||| 42248 ||| 42249 ||| 34992 ||| 
2021 ||| a discriminative self-attention cycle gan for face super-resolution and recognition. ||| 13455 ||| 42250 ||| 42251 ||| 28786 ||| 29187 ||| 
2020 ||| multi-focus image fusion with siamese self-attention network. ||| 42252 ||| 42253 ||| 42254 ||| 42255 ||| 42256 ||| 
2021 ||| leveraging attention-based visual clue extraction for image classification. ||| 42257 ||| 42258 ||| 9464 ||| 35196 ||| 4639 ||| 
2021 ||| a keypoint-based object detection method with wide dual-path backbone network and attention modules. ||| 30835 ||| 42259 ||| 42260 ||| 
2021 ||| multiple object tracking based on multi-task learning with strip attention. ||| 42261 ||| 989 ||| 471 ||| 6488 ||| 42262 ||| 10075 ||| 
2021 ||| salient target detection in hyperspectral image based on visual attention. ||| 42263 ||| 42264 ||| 24151 ||| 42265 ||| 3675 ||| 1052 ||| 
2021 ||| attention-based video object segmentation algorithm. ||| 8539 ||| 42266 ||| 41448 ||| 23327 ||| 
2021 ||| bilateral attention network for semantic segmentation. ||| 21960 ||| 42267 ||| 9018 ||| 42268 ||| 
2022 ||| augmented global attention network for image super-resolution. ||| 248 ||| 42269 ||| 3114 ||| 
2022 ||| image quality enhancement using hybrid attention networks. ||| 24708 ||| 42270 ||| 20922 ||| 
2020 ||| flow driven attention network for video salient object detection. ||| 6924 ||| 6624 ||| 6625 ||| 2323 ||| 
2021 ||| paralleled attention modules and adaptive focal loss for siamese visual tracking. ||| 42271 ||| 24565 ||| 27439 ||| 9745 ||| 
2020 ||| mpa-net: multi-path attention stereo matching network. ||| 28722 ||| 42272 ||| 13255 ||| 11819 ||| 
2021 ||| detail texture detection based on yolov4-tiny combined with attention mechanism and bicubic interpolation. ||| 42273 ||| 42274 ||| 42275 ||| 
2021 ||| visual-attention gan for interior sketch colourisation. ||| 42276 ||| 42277 ||| 42278 ||| 42279 ||| 781 ||| 
2020 ||| image dehazing with uneven illumination prior by dense residual channel attention network. ||| 42280 ||| 42281 ||| 29555 ||| 42282 ||| 
2021 ||| a dual-attention v-network for pulmonary lobe segmentation in ct scans. ||| 13810 ||| 42283 ||| 21191 ||| 29707 ||| 19634 ||| 24677 ||| 24676 ||| 42284 ||| 42285 ||| 
2022 ||| mam: a multipath attention mechanism for image recognition. ||| 3386 ||| 42286 ||| 42287 ||| 42288 ||| 436 ||| 42289 ||| 
2020 ||| e2-capsule neural networks for facial expression recognition using au-aware attention. ||| 6567 ||| 35721 ||| 17561 ||| 
2022 ||| underwater image enhancement via lbp-based attention residual network. ||| 42290 ||| 31473 ||| 42180 ||| 
2021 ||| look into my "virtual" eyes: what dynamic virtual agents add to the realistic study of joint attention. ||| 42291 ||| 8382 ||| 42292 ||| 42293 ||| 
2021 ||| computer- assessment of attention and memory utilizing ecologically valid distractions: a scoping review. ||| 42294 ||| 42295 ||| 42296 ||| 
2022 ||| measuring attentional distraction in children with adhd using virtual reality technology with eye-tracking. ||| 42297 ||| 42298 ||| 42299 ||| 42300 ||| 
2022 ||| automatic extraction of orchards from remote sensing image based on category attention mechanism. ||| 23840 ||| 42301 ||| 42302 ||| 42303 ||| 
2022 ||| cultivated land segmentation of remote sensing image based on pspnet of attention mechanism. ||| 42301 ||| 42303 ||| 23840 ||| 4754 ||| 
2017 ||| a video summarization approach based on the emulation of bottom-up mechanisms of visual attention. ||| 42304 ||| 277 ||| 42305 ||| 42306 ||| 42307 ||| 42308 ||| 42309 ||| 
2021 ||| leveraging contextual embeddings and self-attention neural networks with bi-attention for sentiment analysis. ||| 11924 ||| 11923 ||| 11925 ||| 
2021 ||| caesar: context-aware explanation based on supervised attention for service recommendations. ||| 3034 ||| 3036 ||| 8875 ||| 
2021 ||| cgspn : cascading gated self-attention and phrase-attention network for sentence modeling. ||| 42310 ||| 4477 ||| 
2017 ||| there is no agency without attention. ||| 2666 ||| 2668 ||| 
2021 ||| social explorative attention based recommendation for content distribution platforms. ||| 38632 ||| 15201 ||| 38633 ||| 38634 ||| 894 ||| 11145 ||| 
2021 ||| attention based adversarially regularized learning for network embedding. ||| 16680 ||| 42311 ||| 42312 ||| 
2018 ||| eeg indices correlate with sustained attention performance in patients affected by diffuse axonal injury. ||| 4626 ||| 42313 ||| 42314 ||| 42315 ||| 4627 ||| 
2022 ||| identifying risk factors of intracerebral hemorrhage stability using explainable attention model. ||| 42316 ||| 21821 ||| 19132 ||| 24672 ||| 42317 ||| 42318 ||| 42319 ||| 42320 ||| 42321 ||| 21822 ||| 
2019 ||| evaluation of divided attention using different stimulation models in event-related potentials. ||| 41533 ||| 41524 ||| 15096 ||| 41525 ||| 41527 ||| 
2021 ||| tumor type detection in brain mr images of the deep model developed using hypercolumn technique, attention modules, and residual blocks. ||| 42322 ||| 21408 ||| 42323 ||| 42324 ||| 42325 ||| 
2021 ||| attention monitoring for synchronous distance learning. ||| 42326 ||| 42327 ||| 41916 ||| 42328 ||| 42329 ||| 
2020 ||| transformer fault diagnosis method using iot based monitoring system and ensemble machine learning. ||| 42330 ||| 22230 ||| 42331 ||| 42332 ||| 8838 ||| 42333 ||| 
2020 ||| attention-based sentiment analysis using convolutional and recurrent neural network. ||| 42334 ||| 42335 ||| 29031 ||| 42336 ||| 42337 ||| 5652 ||| 
2020 ||| ransomware classification using patch-based cnn and self-attention network on embedded n-grams of opcodes. ||| 2747 ||| 42338 ||| 2744 ||| 41623 ||| 17249 ||| 2854 ||| 
2021 ||| automatic segmentation of bioabsorbable vascular stents in intravascular optical coherence images using weakly supervised attention network. ||| 17472 ||| 16821 ||| 42339 ||| 30932 ||| 42340 ||| 31995 ||| 
2020 ||| transformer based deep intelligent contextual embedding for twitter sentiment analysis. ||| 5237 ||| 41666 ||| 42341 ||| 42342 ||| 
2018 ||| iot-based students interaction framework using attention-scoring assessment in elearning. ||| 42343 ||| 42344 ||| 42345 ||| 42346 ||| 42347 ||| 42348 ||| 42349 ||| 42350 ||| 
2019 ||| a general ai-defined attention network for predicting cdn performance. ||| 19472 ||| 42351 ||| 42352 ||| 13795 ||| 42353 ||| 42354 ||| 42355 ||| 
2021 ||| scalable multi-channel dilated cnn-bilstm model with attention mechanism for chinese textual sentiment analysis. ||| 28695 ||| 42356 ||| 28698 ||| 
2021 ||| multi-channel, convolutional attention based neural model for automated diagnostic coding of unstructured patient discharge summaries. ||| 8339 ||| 42357 ||| 42358 ||| 42359 ||| 
2021 ||| human action recognition using attention based lstm network with dilated cnn features. ||| 28956 ||| 29312 ||| 41269 ||| 42360 ||| 28957 ||| 42361 ||| 42362 ||| 29277 ||| 
2021 ||| abcdm: an attention-based bidirectional cnn-rnn deep model for sentiment analysis. ||| 42363 ||| 42364 ||| 42365 ||| 893 ||| 6127 ||| 
2020 ||| multi-entity sentiment analysis using self-attention based hierarchical dilated convolutional neural network. ||| 28695 ||| 4754 ||| 28698 ||| 
2022 ||| reliable customer analysis using federated learning and exploring deep-attention edge intelligence. ||| 676 ||| 678 ||| 677 ||| 
2021 ||| graph-cat: graph co-attention networks via local and global attribute augmentations. ||| 8967 ||| 42366 ||| 8972 ||| 8969 ||| 
2022 ||| acmf: an attention collaborative extended matrix factorization based model for mooc course service via a heterogeneous view. ||| 8171 ||| 8172 ||| 8173 ||| 1556 ||| 
2022 ||| eandc: an explainable attention network based deep adaptive clustering model for mental health treatment. ||| 676 ||| 678 ||| 40627 ||| 677 ||| 
2020 ||| simultaneous left atrium anatomy and scar segmentations via deep learning in multiview information with attention. ||| 6005 ||| 1785 ||| 27334 ||| 27856 ||| 27633 ||| 27634 ||| 27636 ||| 27635 ||| 37185 ||| 37186 ||| 4060 ||| 25716 ||| 27637 ||| 27335 ||| 27639 ||| 27638 ||| 
2022 ||| patch attention network with generative adversarial model for semi-supervised binocular disparity prediction. ||| 4392 ||| 4393 ||| 2240 ||| 42367 ||| 
2018 ||| visual attention prediction for images with leading line structure. ||| 42368 ||| 42369 ||| 42370 ||| 
2020 ||| fine-grained action recognition using multi-view attentions. ||| 42371 ||| 17728 ||| 
2021 ||| (sarn)spatial-wise attention residual network for image super-resolution. ||| 42372 ||| 42373 ||| 42374 ||| 42375 ||| 
2020 ||| 3d rans: 3d residual attention networks for action recognition. ||| 42376 ||| 42377 ||| 
2020 ||| sta-gcn: two-stream graph convolutional network with spatial-temporal attention for hand gesture recognition. ||| 781 ||| 42378 ||| 2343 ||| 42379 ||| 42380 ||| 42381 ||| 
2022 ||| attention to fine-grained information: hierarchical multi-scale network for retinal vessel segmentation. ||| 42382 ||| 42383 ||| 2182 ||| 
2022 ||| contour-aware semantic segmentation network with spatial attention mechanism for medical image. ||| 42384 ||| 16714 ||| 11185 ||| 
2020 ||| feature-attention module for context-aware image-to-image translation. ||| 18503 ||| 9862 ||| 6796 ||| 
2021 ||| multi-level progressive parallel attention guided salient object detection for rgb-d images. ||| 19662 ||| 42385 ||| 42386 ||| 17444 ||| 
2020 ||| traffic transformer: capturing the continuity and periodicity of time series for traffic forecasting. ||| 28176 ||| 28174 ||| 28173 ||| 28175 ||| 8473 ||| 
2021 ||| do children with reading difficulties benefit from instructional game supports? exploring children's attention and understanding of feedback. ||| 42387 ||| 42388 ||| 42389 ||| 42390 ||| 42391 ||| 42392 ||| 
2017 ||| assessing the attention levels of students by using a novel attention aware system based on brainwave signals. ||| 21100 ||| 30129 ||| 42393 ||| 
2021 ||| design of online monitoring system for distribution transformer based on cloud side end collaboration of internet of things. ||| 42394 ||| 42395 ||| 42396 ||| 42397 ||| 42398 ||| 42399 ||| 
2022 ||| an sar target detector based on gradient harmonized mechanism and attention mechanism. ||| 30171 ||| 26787 ||| 3501 ||| 
2022 ||| remote sensing image generation based on attention mechanism and vae-msgan for roi extraction. ||| 11503 ||| 40647 ||| 
2022 ||| classfication of hyperspectral image with attention mechanism-based dual-path convolutional network. ||| 29589 ||| 15985 ||| 42400 ||| 
2022 ||| hybrid attention networks for flow and pressure forecasting in water distribution systems. ||| 33854 ||| 39531 ||| 39532 ||| 39533 ||| 
2020 ||| synthetic aperture radar scene classification using multiview cross correlation attention network. ||| 42401 ||| 7147 ||| 5845 ||| 
2022 ||| multiscale building extraction with refined attention pyramid networks. ||| 42402 ||| 42403 ||| 41559 ||| 1785 ||| 42404 ||| 42405 ||| 
2022 ||| erratum to "convolutional neural network with attention mechanism for sar automatic target recognition". ||| 1251 ||| 42406 ||| 42407 ||| 42408 ||| 29965 ||| 42409 ||| 
2022 ||| multidimensional attention learning for vhr remote sensing imagery recognition. ||| 2904 ||| 42410 ||| 5485 ||| 5484 ||| 
2021 ||| pyramid attention dilated network for aircraft detection in sar images. ||| 10646 ||| 80 ||| 42411 ||| 30332 ||| 
2022 ||| graph sample and aggregate-attention network for hyperspectral image classification. ||| 42412 ||| 42413 ||| 42414 ||| 42415 ||| 42416 ||| 
2021 ||| attention-based convolutional neural network for earthquake event classification. ||| 42417 ||| 42418 ||| 42419 ||| 42420 ||| 8560 ||| 
2022 ||| attention-driven graph convolution network for remote sensing image retrieval. ||| 6864 ||| 6865 ||| 6866 ||| 6867 ||| 
2021 ||| joint spatial-spectral attention network for hyperspectral image classification. ||| 3034 ||| 11508 ||| 38700 ||| 11504 ||| 11617 ||| 
2022 ||| separable attention network in single- and mixed-precision floating point for land-cover classification of remote sensing images. ||| 42421 ||| 42007 ||| 42422 ||| 33815 ||| 42423 ||| 6750 ||| 
2022 ||| markcapsnet: road marking extraction from aerial images using self-attention-guided capsule network. ||| 30423 ||| 42424 ||| 859 ||| 1224 ||| 42425 ||| 42426 ||| 42427 ||| 42428 ||| 5082 ||| 
2022 ||| context residual attention network for remote sensing scene classification. ||| 14771 ||| 42429 ||| 14773 ||| 42430 ||| 397 ||| 42431 ||| 
2022 ||| maenet: multiple attention encoder-decoder network for farmland segmentation of remote sensing images. ||| 42432 ||| 2487 ||| 42433 ||| 1589 ||| 42434 ||| 340 ||| 
2022 ||| point transformer for shape classification and retrieval of urban roof point clouds. ||| 39786 ||| 39787 ||| 39788 ||| 
2022 ||| the application of semisupervised attentional generative adversarial networks in desert seismic data denoising. ||| 6898 ||| 42435 ||| 25321 ||| 42436 ||| 
2021 ||| multibranch spatial-channel attention for semantic labeling of very high-resolution remote sensing images. ||| 11617 ||| 11508 ||| 11505 ||| 38700 ||| 
2020 ||| an adaptive scale sea surface temperature predicting method based on deep learning with attention mechanism. ||| 13542 ||| 42437 ||| 11676 ||| 15189 ||| 
2022 ||| dual-triple attention network for hyperspectral image classification using limited training samples. ||| 13194 ||| 42438 ||| 42439 ||| 20185 ||| 30377 ||| 
2022 ||| terrain segmentation in polarimetric sar images using dual-attention fusion network. ||| 42440 ||| 42441 ||| 42442 ||| 13676 ||| 10525 ||| 
2022 ||| dbranet: road extraction by dual-branch encoder and regional attention decoder. ||| 6569 ||| 42443 ||| 5755 ||| 382 ||| 42444 ||| 20329 ||| 
2022 ||| ssa-net: spatial scale attention network for image-based geo-localization. ||| 42445 ||| 42446 ||| 42447 ||| 34021 ||| 42448 ||| 42449 ||| 10075 ||| 
2022 ||| a mutual guidance attention-based multi-level fusion network for hyperspectral and lidar classification. ||| 42450 ||| 42451 ||| 42050 ||| 6870 ||| 42452 ||| 
2022 ||| context-aware attentional graph u-net for hyperspectral image classification. ||| 42453 ||| 30337 ||| 30339 ||| 30340 ||| 37981 ||| 
2022 ||| a global-local spectral weight network based on attention for hyperspectral band selection. ||| 42454 ||| 42455 ||| 42456 ||| 42457 ||| 20653 ||| 
2022 ||| hyperspectral image classification with multiattention fusion network. ||| 42458 ||| 42459 ||| 42460 ||| 3337 ||| 42461 ||| 42462 ||| 22373 ||| 
2022 ||| spectral-spatial residual graph attention network for hyperspectral image classification. ||| 15984 ||| 3226 ||| 30839 ||| 11223 ||| 15985 ||| 
2022 ||| csafnet: channel similarity attention fusion network for multispectral pansharpening. ||| 42463 ||| 36247 ||| 42464 ||| 
2021 ||| remote sensing image scene classification based on an enhanced attention module. ||| 19112 ||| 42465 ||| 42466 ||| 595 ||| 36661 ||| 
2022 ||| improving deep learning-based cloud detection for satellite images with attention mechanism. ||| 254 ||| 5322 ||| 42467 ||| 18443 ||| 42125 ||| 
2020 ||| apdc-net: attention pooling-based convolutional network for aerial scene classification. ||| 27337 ||| 42468 ||| 14048 ||| 42469 ||| 42470 ||| 5723 ||| 
2022 ||| radar-based human activity recognition with 1-d dense attention network. ||| 42471 ||| 33020 ||| 42472 ||| 
2022 ||| semantic segmentation of remote sensing image based on regional self-attention mechanism. ||| 6909 ||| 6203 ||| 820 ||| 6654 ||| 40328 ||| 
2022 ||| contrasting yolov5, transformer, and efficientdet detectors for crop circle detection in desert. ||| 42473 ||| 42474 ||| 6726 ||| 6727 ||| 42475 ||| 42476 ||| 
2022 ||| semantic segmentation of remote-sensing images based on multiscale feature fusion and attention refinement. ||| 10812 ||| 6496 ||| 6497 ||| 42477 ||| 1828 ||| 3072 ||| 42478 ||| 
2022 ||| remote-sensing image captioning based on multilayer aggregated transformer. ||| 11442 ||| 8164 ||| 6654 ||| 
2022 ||| development of a dual-attention u-net model for sea ice and open water classification on sar images. ||| 42479 ||| 14108 ||| 24700 ||| 42480 ||| 
2022 ||| attention-based polarimetric feature selection convolutional network for polsar image classification. ||| 39691 ||| 39692 ||| 42481 ||| 39693 ||| 
2022 ||| consecutively missing seismic data interpolation based on coordinate attention unet. ||| 42482 ||| 6752 ||| 6754 ||| 25869 ||| 
2022 ||| global context parallel attention for anchor-free instance segmentation in remote sensing images. ||| 20231 ||| 42483 ||| 
2022 ||| land cover classification of multispectral lidar data with an efficient self-attention capsule network. ||| 30423 ||| 859 ||| 30424 ||| 42427 ||| 42484 ||| 42485 ||| 21148 ||| 19197 ||| 
2021 ||| self-attention-based deep feature fusion for remote sensing scene classification. ||| 42486 ||| 6749 ||| 711 ||| 6748 ||| 
2021 ||| scattnet: semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images. ||| 12801 ||| 39078 ||| 3036 ||| 39079 ||| 39080 ||| 39081 ||| 
2022 ||| sea surface height prediction with deep learning based on attention mechanism. ||| 2046 ||| 42487 ||| 3279 ||| 15189 ||| 
2021 ||| semisupervised classification for hyperspectral images using graph attention networks. ||| 42488 ||| 379 ||| 42489 ||| 42490 ||| 
2022 ||| adaptive cross-attention-driven spatial-spectral graph convolutional network for hyperspectral image classification. ||| 42491 ||| 31730 ||| 31731 ||| 42492 ||| 6720 ||| 
2022 ||| end-to-end multilevel hybrid attention framework for hyperspectral image classification. ||| 42493 ||| 26651 ||| 42494 ||| 42495 ||| 
2022 ||| synthetic data augmentation using multiscale attention cyclegan for aircraft detection in remote sensing images. ||| 42496 ||| 382 ||| 1235 ||| 
2022 ||| ground-based cloud detection using multiscale attention convolutional neural network. ||| 19052 ||| 42497 ||| 13805 ||| 30788 ||| 42102 ||| 
2021 ||| darecnet-bs: unsupervised dual-attention reconstruction network for hyperspectral band selection. ||| 33815 ||| 42498 ||| 11224 ||| 42499 ||| 
2020 ||| combining multilevel features for remote sensing image scene classification with attention model. ||| 6764 ||| 6514 ||| 6765 ||| 6766 ||| 6767 ||| 
2022 ||| local fusion attention network for semantic segmentation of building facade point clouds. ||| 17597 ||| 17595 ||| 5287 ||| 17599 ||| 3691 ||| 
2022 ||| a shadow detection algorithm based on multiscale spatial attention mechanism for aerial remote sensing images. ||| 42500 ||| 12666 ||| 42501 ||| 23018 ||| 
2022 ||| a spatiotemporal attention model for severe precipitation estimation. ||| 778 ||| 2885 ||| 42502 ||| 42503 ||| 15790 ||| 
2022 ||| u-shaped attention connection network for remote-sensing image super-resolution. ||| 30222 ||| 30223 ||| 27259 ||| 11493 ||| 27258 ||| 
2020 ||| learning to match ground camera image and uav 3-d model-rendered image based on siamese network with attention mechanism. ||| 17595 ||| 3691 ||| 17596 ||| 42504 ||| 42505 ||| 17598 ||| 5690 ||| 42506 ||| 19197 ||| 
2022 ||| self-supervised convolutional neural network via spectral attention module for hyperspectral image classification. ||| 15985 ||| 42400 ||| 29589 ||| 
2022 ||| improving remote sensing image captioning by combining grid features and transformer. ||| 42507 ||| 2885 ||| 8608 ||| 15790 ||| 42508 ||| 6810 ||| 
2022 ||| remote sensing image classification based on a cross-attention mechanism and graph convolution. ||| 29239 ||| 42509 ||| 
2022 ||| block multi-dimensional attention for road segmentation in remote sensing imagery. ||| 42510 ||| 30239 ||| 
2022 ||| ship segmentation via encoder-decoder network with global attention in high-resolution sar images. ||| 28832 ||| 6914 ||| 42511 ||| 1060 ||| 42512 ||| 
2021 ||| region-merging method with texture pattern attention for sar image segmentation. ||| 42513 ||| 42514 ||| 42515 ||| 
2020 ||| multi-scale spatial and channel-wise attention for improving object detection in remote sensing imagery. ||| 1037 ||| 12654 ||| 42182 ||| 22972 ||| 42184 ||| 
2022 ||| multistage attention resu-net for semantic segmentation of fine-resolution remote sensing images. ||| 8207 ||| 30454 ||| 30455 ||| 32217 ||| 13362 ||| 
2020 ||| local attention networks for occluded airplane detection in remote sensing images. ||| 42516 ||| 2185 ||| 6654 ||| 28824 ||| 28823 ||| 
2022 ||| spectral-spatial graph attention network for semisupervised hyperspectral image classification. ||| 42517 ||| 1371 ||| 42518 ||| 
2022 ||| sscan: a spatial-spectral cross attention network for hyperspectral image denoising. ||| 5514 ||| 32830 ||| 18085 ||| 20135 ||| 19756 ||| 
2020 ||| roof classification from 3-d lidar point clouds using multiview cnn with self-attention. ||| 39786 ||| 42519 ||| 39788 ||| 
2022 ||| multiscale feature learning by transformer for building extraction from satellite images. ||| 19066 ||| 16132 ||| 34286 ||| 34285 ||| 42520 ||| 16133 ||| 
2022 ||| hyperspectral image super-resolution based on multiscale mixed attention network fusion. ||| 42041 ||| 42521 ||| 42522 ||| 32128 ||| 
2021 ||| band selection of hyperspectral images using attention-based autoencoders. ||| 42523 ||| 7646 ||| 42524 ||| 2614 ||| 42525 ||| 
2022 ||| automatic extraction of layover from insar imagery based on multilayer feature fusion attention mechanism. ||| 30154 ||| 30150 ||| 30151 ||| 30350 ||| 30153 ||| 30348 ||| 30148 ||| 
2022 ||| da2net: distraction-attention-driven adversarial network for robust remote sensing image scene classification. ||| 11453 ||| 30686 ||| 37287 ||| 42526 ||| 5858 ||| 
2022 ||| visible-assisted infrared image super-resolution based on spatial attention residual network. ||| 42527 ||| 42108 ||| 3337 ||| 10000 ||| 
2022 ||| when cnns meet vision transformer: a joint framework for remote sensing scene classification. ||| 15986 ||| 15984 ||| 15985 ||| 
2022 ||| a novel transformer based semantic segmentation scheme for fine-resolution remote sensing images. ||| 30557 ||| 8207 ||| 30455 ||| 13362 ||| 30560 ||| 37179 ||| 
2021 ||| wide-context attention network for remote sensing image retrieval. ||| 42528 ||| 42529 ||| 42530 ||| 42531 ||| 
2019 ||| optimized input for cnn-based hyperspectral image classification using spatial transformer network. ||| 10812 ||| 30615 ||| 
2022 ||| unsupervised hyperspectral pansharpening by ratio estimation and residual attention network. ||| 42532 ||| 42533 ||| 41856 ||| 
2019 ||| attentional information fusion networks for cross-scene power line detection. ||| 185 ||| 2427 ||| 1930 ||| 1931 ||| 
2019 ||| convolutional attention in ensemble with knowledge transferred for remote sensing image classification. ||| 42534 ||| 18146 ||| 42535 ||| 7240 ||| 
2022 ||| attention mask-based network with simple color annotation for uav vehicle re-identification. ||| 42536 ||| 42537 ||| 42538 ||| 35571 ||| 
2022 ||| scaf-net: scene context attention-based fusion network for vehicle detection in aerial imagery. ||| 27971 ||| 42539 ||| 42540 ||| 6749 ||| 16131 ||| 
2019 ||| multiscale visual attention networks for object detection in vhr remote sensing images. ||| 5187 ||| 3239 ||| 11634 ||| 1086 ||| 42541 ||| 
2022 ||| sar target recognition based on efficient fully convolutional attention block cnn. ||| 8207 ||| 42542 ||| 8349 ||| 17687 ||| 25692 ||| 
2022 ||| convolutional neural network with attention mechanism for sar automatic target recognition. ||| 1251 ||| 42406 ||| 42407 ||| 42408 ||| 29965 ||| 42543 ||| 
2022 ||| ed-drap: encoder-decoder deep residual attention prediction network for radar echoes. ||| 42544 ||| 42545 ||| 42546 ||| 14181 ||| 42547 ||| 
2022 ||| channel attention-based temporal convolutional network for satellite image time series classification. ||| 42022 ||| 30284 ||| 30426 ||| 989 ||| 42548 ||| 42549 ||| 
2022 ||| target detection based on edge-aware and cross-coupling attention for sar images. ||| 11503 ||| 40070 ||| 40073 ||| 
2021 ||| cooperative spectral-spatial attention dense network for hyperspectral image classification. ||| 28123 ||| 28122 ||| 28124 ||| 28125 ||| 42550 ||| 42551 ||| 
2022 ||| a lightweight detector based on attention mechanism for aluminum strip surface defect detection. ||| 42552 ||| 42553 ||| 42554 ||| 42555 ||| 42556 ||| 42557 ||| 
2018 ||| multi-view pedestrian captioning with an attention topic cnn model. ||| 14059 ||| 11579 ||| 2077 ||| 42558 ||| 
2021 ||| a double-layer attention based adversarial network for partial transfer learning in machinery fault diagnosis. ||| 42559 ||| 42560 ||| 42561 ||| 42562 ||| 18731 ||| 34858 ||| 
2022 ||| a feature fusion enhanced multiscale cnn with attention mechanism for spot-welding surface appearance recognition. ||| 42563 ||| 2760 ||| 42564 ||| 42565 ||| 25344 ||| 42566 ||| 
2019 ||| coarse-to-fine document localization in natural scene image with regional attention and recursive corner refinement. ||| 42567 ||| 10922 ||| 22351 ||| 12692 ||| 
2021 ||| eaml: ensemble self-attention-based mutual learning network for document image classification. ||| 42568 ||| 33000 ||| 17370 ||| 17371 ||| 10907 ||| 23154 ||| 23155 ||| 
2022 ||| $\hbox {tg}^2$: text-guided transformer gan for restoring document readability and perceived quality. ||| 42569 ||| 42570 ||| 
2020 ||| ma-crnn: a multi-scale attention crnn for chinese text line recognition in natural scenes. ||| 42571 ||| 2969 ||| 42572 ||| 42573 ||| 1371 ||| 42574 ||| 
2020 ||| identify vulnerability fix commits automatically using hierarchical attention network. ||| 42575 ||| 9673 ||| 42576 ||| 42577 ||| 42578 ||| 
2021 ||| leveraging attention-based deep neural networks for security vetting of android applications. ||| 42579 ||| 42580 ||| 42581 ||| 42582 ||| 
2019 ||| triplet loss with channel attention for person re-identification. ||| 20075 ||| 42583 ||| 42584 ||| 6107 ||| 
2019 ||| robust target tracking algorithm based on superpixel visual attention mechanism: robust target tracking algorithm. ||| 42585 ||| 31693 ||| 42586 ||| 42587 ||| 
2021 ||| deep scaffold hopping with multimodal transformer neural networks. ||| 16736 ||| 42588 ||| 42589 ||| 4108 ||| 42590 ||| 16595 ||| 
2020 ||| a self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility. ||| 38262 ||| 42591 ||| 42592 ||| 42593 ||| 4784 ||| 1696 ||| 
2020 ||| building attention and edge message passing neural networks for bioactivity and physical-chemical property prediction. ||| 4105 ||| 4106 ||| 504 ||| 4107 ||| 4108 ||| 
2020 ||| transformer-cnn: swiss knife for qsar modeling and interpretation. ||| 4123 ||| 4124 ||| 4125 ||| 
2019 ||| biotransformer: a comprehensive computational tool for small molecule metabolism prediction and metabolite identification. ||| 42594 ||| 42595 ||| 42596 ||| 42597 ||| 42598 ||| 42599 ||| 
2021 ||| decimer 1.0: deep learning for chemical image recognition using transformers. ||| 42600 ||| 42601 ||| 42602 ||| 
2021 ||| constrained versus unconstrained rational inattention. ||| 42603 ||| 
2021 ||| champ versus chump: viewing an opponent's face engages attention but not reward systems. ||| 42604 ||| 42605 ||| 26128 ||| 42606 ||| 26127 ||| 
2022 ||| precise learning of source code contextual semantics via hierarchical dependence structure and graph attention networks. ||| 36481 ||| 2760 ||| 2064 ||| 36482 ||| 1390 ||| 
2022 ||| prhan: automated pull request description generation based on hybrid attention network. ||| 32134 ||| 6514 ||| 32135 ||| 14788 ||| 42607 ||| 42608 ||| 
2020 ||| forecasting currency exchange rate time series with fireworks-algorithm-based higher order neural network with special attention to training data enrichment. ||| 42609 ||| 42610 ||| 42611 ||| 
2020 ||| resistance and resolution: attentional dynamics in discourse. ||| 42612 ||| 42613 ||| 
2018 ||| effects of online synchronous instruction with an attention monitoring and alarm mechanism on sustained attention and learning performance. ||| 21100 ||| 30129 ||| 
2019 ||| improving effectiveness of learners' review of video lectures by using an attention-based video lecture review mechanism based on brainwave signals. ||| 21121 ||| 21100 ||| 
2018 ||| stop daydreaming, pay attention. ||| 42614 ||| 
2018 ||| effects of using a second-screen application on attention, learning, and user experience in an educational content. ||| 42615 ||| 42616 ||| 42617 ||| 
2017 ||| enhancing learning performance, attention, and meditation using a speech-to-text recognition application: evidence from multiple data sources. ||| 40223 ||| 42618 ||| 17328 ||| 
2020 ||| attendance and attention. ||| 42614 ||| 
2017 ||| strategic pricing with rational inattention to quality. ||| 22397 ||| 
2017 ||| texting and walking: a controlled field study of crossing behaviours and inattentional blindness in taiwan. ||| 42619 ||| 42620 ||| 42621 ||| 
2018 ||| attention to esports advertisement: effects of ad animation and in-game dynamics on viewers' visual attention. ||| 42622 ||| 42623 ||| 42624 ||| 42625 ||| 
2018 ||| improving periocular recognition by explicit attention to critical regions in deep neural network. ||| 42626 ||| 42627 ||| 
2020 ||| attention-based two-stream convolutional networks for face spoofing detection. ||| 42628 ||| 18063 ||| 337 ||| 28830 ||| 18064 ||| 338 ||| 
2020 ||| target-specific siamese attention network for real-time object tracking. ||| 42629 ||| 11331 ||| 11330 ||| 42630 ||| 42631 ||| 
2022 ||| person re-identification by context-aware part attention and multi-head collaborative learning. ||| 42632 ||| 17926 ||| 42633 ||| 13676 ||| 2445 ||| 
2019 ||| joint intensity transformer network for gait recognition robust against clothing and carrying status. ||| 2008 ||| 28811 ||| 28810 ||| 28812 ||| 42634 ||| 
2021 ||| depth as attention for face representation learning. ||| 20087 ||| 12091 ||| 20088 ||| 20089 ||| 
2020 ||| person re-identification using spatial and layer-wise attention. ||| 18838 ||| 18839 ||| 8379 ||| 18840 ||| 8034 ||| 
2020 ||| towards complete and accurate iris segmentation using deep multi-task attention network for non-cooperative iris recognition. ||| 18588 ||| 42635 ||| 42636 ||| 18590 ||| 17945 ||| 
2021 ||| end-to-end domain adaptive attention network for cross-domain person re-identification. ||| 33861 ||| 11374 ||| 11330 ||| 11331 ||| 
2021 ||| deepsbd: a deep neural network model with attention mechanism for socialbot detection. ||| 8476 ||| 8477 ||| 8478 ||| 
2021 ||| multi-scale gradients self-attention residual learning for face photo-sketch transformation. ||| 42637 ||| 42638 ||| 28491 ||| 23489 ||| 23352 ||| 
2021 ||| minutiae attention network with reciprocal distance loss for contactless to contact-based fingerprint identification. ||| 42639 ||| 42627 ||| 
2020 ||| dependency-aware attention control for image set-based face recognition. ||| 8525 ||| 19896 ||| 5379 ||| 8526 ||| 
2021 ||| dsa-face: diverse and sparse attentions for face recognition robust to pose variation and occlusion. ||| 2322 ||| 2323 ||| 
2022 ||| self-adversarial training incorporating forgery attention for image forgery localization. ||| 35583 ||| 35584 ||| 6502 ||| 35585 ||| 
2020 ||| fused behavior recognition model based on attention mechanism. ||| 1207 ||| 1840 ||| 816 ||| 7676 ||| 817 ||| 
2017 ||| social media attention increases article visits: an investigation on article-level referral data of peerj. ||| 35217 ||| 35218 ||| 35219 ||| 35220 ||| 
2018 ||| distracted reading: acts of attention in the age of the internet. ||| 42640 ||| 
2018 ||| from distracted to distributed attention: expanded learning through social media, augmented reality, remixing, and activist geocaching. ||| 42641 ||| 
2018 ||| three-dimensional attention-based deep ranking model for video highlight detection. ||| 42642 ||| 42643 ||| 42644 ||| 19721 ||| 2304 ||| 18733 ||| 
2022 ||| human action recognition by discriminative feature pooling and video segment attention model. ||| 24981 ||| 24982 ||| 8854 ||| 24983 ||| 33632 ||| 
2018 ||| gla: global-local attention for image description. ||| 5449 ||| 18089 ||| 17860 ||| 18090 ||| 2398 ||| 
2021 ||| sal: selection and attention losses for weakly supervised semantic segmentation. ||| 11464 ||| 20235 ||| 2740 ||| 42645 ||| 
2019 ||| learning attentional recurrent neural network for visual tracking. ||| 19816 ||| 1280 ||| 18793 ||| 7912 ||| 
2020 ||| spatio-temporal attention networks for action recognition and detection. ||| 5536 ||| 17695 ||| 42646 ||| 7266 ||| 9576 ||| 437 ||| 
2019 ||| comic: toward a compact image captioning model with attention. ||| 33863 ||| 7852 ||| 27311 ||| 
2021 ||| hybrid-attention enhanced two-stream fusion network for video venue prediction. ||| 42647 ||| 19601 ||| 9631 ||| 19491 ||| 
2021 ||| image-text multimodal emotion classification via multi-view attentional network. ||| 42648 ||| 1311 ||| 1312 ||| 8426 ||| 
2021 ||| stacked u-shape network with channel-wise attention for salient object detection. ||| 32430 ||| 42649 ||| 6625 ||| 11355 ||| 
2022 ||| cola-net: collaborative attention network for image restoration. ||| 12195 ||| 12196 ||| 37834 ||| 23192 ||| 37835 ||| 
2020 ||| convolutional networks with channel and stips attention model for action recognition in videos. ||| 42650 ||| 9442 ||| 31541 ||| 
2020 ||| corrections to "stat: spatial-temporal attention mechanism for video captioning". ||| 17648 ||| 19427 ||| 42651 ||| 18994 ||| 42652 ||| 17860 ||| 37898 ||| 
2021 ||| beyond vision: a multimodal recurrent attention convolutional neural network for unified image aesthetic prediction tasks. ||| 38080 ||| 6621 ||| 32419 ||| 42653 ||| 4634 ||| 
2017 ||| videowhisper: toward discriminative unsupervised video feature learning with attention-based recurrent neural networks. ||| 20430 ||| 2484 ||| 5946 ||| 444 ||| 3605 ||| 
2017 ||| diversified visual attention networks for fine-grained object classification. ||| 7873 ||| 19413 ||| 1685 ||| 19585 ||| 1728 ||| 
2022 ||| cross parallax attention network for stereo image super-resolution. ||| 42654 ||| 42655 ||| 4482 ||| 42656 ||| 
2021 ||| an attention-based unsupervised adversarial model for movie review spam detection. ||| 5519 ||| 34992 ||| 29455 ||| 30379 ||| 
2019 ||| high-quality image captioning with fine-grained and semantic-guided visual attention. ||| 601 ||| 603 ||| 602 ||| 604 ||| 
2021 ||| a coarse-to-fine facial landmark detection method based on self-attention mechanism. ||| 42657 ||| 13200 ||| 13201 ||| 1932 ||| 42658 ||| 
2020 ||| a cuboid cnn model with an attention mechanism for skeleton-based action recognition. ||| 42659 ||| 20027 ||| 42660 ||| 15003 ||| 17788 ||| 
2021 ||| br$^2$net: defocus blur detection via a bidirectional channel attention residual refining network. ||| 19988 ||| 19990 ||| 30894 ||| 1703 ||| 
2021 ||| transformer encoder with multi-modal multi-head attention for continuous affect recognition. ||| 1159 ||| 4441 ||| 42661 ||| 
2021 ||| mvanet: multi-task guided multi-view attention network for chinese food recognition. ||| 42662 ||| 29558 ||| 29559 ||| 29557 ||| 24634 ||| 38169 ||| 
2019 ||| where-and-when to look: deep siamese attention networks for video-based person re-identification. ||| 17580 ||| 602 ||| 619 ||| 9889 ||| 
2022 ||| e-commerce storytelling recommendation using attentional domain-transfer network and adversarial pre-training. ||| 19474 ||| 9002 ||| 8709 ||| 4881 ||| 19672 ||| 8710 ||| 1807 ||| 
2020 ||| stnreid: deep convolutional networks with pairwise spatial transformer networks for partial person re-identification. ||| 1702 ||| 1706 ||| 33747 ||| 1460 ||| 
2022 ||| multi-localized sensitive autoencoder-attention-lstm for skeleton-based action recognition. ||| 32001 ||| 1042 ||| 13179 ||| 
2019 ||| attention-based multiview re-observation fusion network for skeletal action recognition. ||| 42663 ||| 7662 ||| 1758 ||| 42664 ||| 
2021 ||| apse: attention-aware polarity-sensitive embedding for emotion-based image retrieval. ||| 1829 ||| 1831 ||| 1833 ||| 1830 ||| 1832 ||| 1834 ||| 
2021 ||| pfan++: bi-directional image-text retrieval with position focused attention network. ||| 23435 ||| 2792 ||| 42665 ||| 23436 ||| 19727 ||| 811 ||| 8481 ||| 8482 ||| 
2021 ||| understanding more about human and machine attention in deep neural networks. ||| 23332 ||| 1969 ||| 42666 ||| 23335 ||| 2445 ||| 1932 ||| 
2020 ||| gaim: graph attention interaction model for collective activity recognition. ||| 22574 ||| 4151 ||| 22571 ||| 22572 ||| 22573 ||| 6459 ||| 
2020 ||| deep multi-kernel convolutional lstm networks and an attention-based mechanism for videos. ||| 39549 ||| 1387 ||| 
2020 ||| relation attention for temporal action localization. ||| 42667 ||| 2190 ||| 42668 ||| 1263 ||| 6414 ||| 6413 ||| 
2017 ||| video captioning with attention-based lstm and semantic consistency. ||| 1039 ||| 3391 ||| 2484 ||| 9579 ||| 1040 ||| 
2020 ||| oriented spatial transformer network for pedestrian detection using fish-eye camera. ||| 42669 ||| 19774 ||| 7662 ||| 23576 ||| 1780 ||| 
2022 ||| infrared action detection in the dark via cross-stream attention mechanism. ||| 9645 ||| 11223 ||| 42670 ||| 208 ||| 8850 ||| 
2021 ||| spatial pyramid attention for deep convolutional neural networks. ||| 19807 ||| 19806 ||| 19808 ||| 19809 ||| 19810 ||| 6415 ||| 19811 ||| 17441 ||| 19812 ||| 
2021 ||| attentionfgan: infrared and visible image fusion using attention-based generative adversarial networks. ||| 4807 ||| 28534 ||| 28409 ||| 28535 ||| 11225 ||| 
2020 ||| pay attention to the activations: a modular attention mechanism for fine-grained image recognition. ||| 8668 ||| 6235 ||| 37414 ||| 8670 ||| 8669 ||| 8671 ||| 8672 ||| 8048 ||| 
2021 ||| character detection in animated movies using multi-style adaptation and visual attention. ||| 42671 ||| 42672 ||| 23816 ||| 23815 ||| 42673 ||| 
2022 ||| sam: modeling scene, object and action with semantics attention modules for video recognition. ||| 9535 ||| 7314 ||| 17842 ||| 
2022 ||| focus your attention: a focal attention for multimodal learning. ||| 2290 ||| 5963 ||| 18733 ||| 19713 ||| 379 ||| 17860 ||| 
2021 ||| improving driver gaze prediction with reinforced attention. ||| 42674 ||| 18507 ||| 4214 ||| 3337 ||| 8571 ||| 
2021 ||| attention-based deep reinforcement learning for virtual cinematography of 360$^{\circ}$ videos. ||| 38608 ||| 18741 ||| 19374 ||| 38529 ||| 
2020 ||| learning normal patterns via adversarial attention-based autoencoder for abnormal event detection in videos. ||| 42675 ||| 42676 ||| 42677 ||| 42678 ||| 42679 ||| 
2019 ||| attend and imagine: multi-label image classification with visual attention and recurrent neural networks. ||| 10069 ||| 6417 ||| 10072 ||| 1827 ||| 6413 ||| 
2020 ||| tamper-proofing video with hierarchical attention autoencoder hashing on blockchain. ||| 2286 ||| 42680 ||| 2288 ||| 42681 ||| 42682 ||| 42683 ||| 42684 ||| 42685 ||| 42686 ||| 42687 ||| 
2021 ||| adversarial disentanglement spectrum variations and cross-modality attention networks for nir-vis face recognition. ||| 42688 ||| 5746 ||| 
2019 ||| unified spatio-temporal attention networks for action recognition in videos. ||| 6552 ||| 19117 ||| 18938 ||| 2165 ||| 19345 ||| 
2019 ||| decoupled spatial neural attention for weakly supervised semantic segmentation. ||| 32297 ||| 1677 ||| 1691 ||| 32298 ||| 6335 ||| 11620 ||| 
2022 ||| lag-net: multi-granularity network for person re-identification via local attention system. ||| 42689 ||| 42690 ||| 633 ||| 42691 ||| 382 ||| 42692 ||| 42693 ||| 
2022 ||| fine-grained attention and feature-sharing generative adversarial networks for single image super-resolution. ||| 36694 ||| 36695 ||| 17231 ||| 36696 ||| 36697 ||| 42694 ||| 36698 ||| 
2021 ||| ld-man: layout-driven multimodal attention network for online news sentiment recognition. ||| 18013 ||| 4646 ||| 18015 ||| 42695 ||| 1834 ||| 15243 ||| 
2021 ||| emotion attention-aware collaborative deep reinforcement learning for image cropping. ||| 11272 ||| 42696 ||| 19755 ||| 
2020 ||| frame augmented alternating attention network for video question answering. ||| 42697 ||| 17723 ||| 37895 ||| 1937 ||| 2258 ||| 7654 ||| 
2020 ||| hierarchical attention network for visually-aware food recommendation. ||| 42698 ||| 34623 ||| 1063 ||| 688 ||| 42699 ||| 717 ||| 39153 ||| 3605 ||| 
2021 ||| self-adaptive neural module transformer for visual question answering. ||| 42700 ||| 9630 ||| 22328 ||| 2484 ||| 2489 ||| 2490 ||| 
2022 ||| spatial-temporal action localization with hierarchical self-attention. ||| 2242 ||| 2243 ||| 2244 ||| 
2021 ||| heterogeneous community question answering via social-aware multi-modal co-attention convolutional matching. ||| 844 ||| 1174 ||| 1173 ||| 1175 ||| 
2020 ||| bidirectional attention-recognition model for fine-grained object classification. ||| 18070 ||| 18071 ||| 19820 ||| 18073 ||| 5308 ||| 17860 ||| 
2022 ||| image co-saliency detection and instance co-segmentation using attention graph clustering based graph convolutional network. ||| 19238 ||| 19237 ||| 19239 ||| 1748 ||| 6625 ||| 34041 ||| 
2021 ||| iptv channel zapping recommendation with attention mechanism. ||| 42701 ||| 42702 ||| 42703 ||| 36992 ||| 4297 ||| 42704 ||| 
2021 ||| caa-net: conditional atrous cnns with attention for explainable device-robust acoustic scene classification. ||| 12762 ||| 12618 ||| 12763 ||| 12619 ||| 648 ||| 649 ||| 
2019 ||| pre-attention and spatial dependency driven no-reference image quality assessment. ||| 42705 ||| 42706 ||| 19840 ||| 
2018 ||| content-attention representation by factorized action-scene network for action recognition. ||| 42707 ||| 42677 ||| 42708 ||| 42679 ||| 
2020 ||| stat: spatial-temporal attention mechanism for video captioning. ||| 17648 ||| 19427 ||| 42651 ||| 18994 ||| 42652 ||| 17860 ||| 37898 ||| 
2021 ||| spa-gan: spatial attention gan for image-to-image translation. ||| 15277 ||| 32977 ||| 15278 ||| 32978 ||| 
2020 ||| the effects of video games in memory and attention. ||| 42709 ||| 42710 ||| 28656 ||| 
2018 ||| temporal reference, attentional modulation, and crossmodal assimilation. ||| 42711 ||| 42712 ||| 
2019 ||| analysis of biased competition and cooperation for attention in the cerebral cortex. ||| 42713 ||| 42714 ||| 
2020 ||| biosignal-based attention monitoring to support nuclear operator safety-relevant tasks. ||| 42715 ||| 42716 ||| 42717 ||| 42718 ||| 
2020 ||| attention in psychology, neuroscience, and machine learning. ||| 42719 ||| 
2021 ||| corrigendum: attention in psychology, neuroscience, and machine learning. ||| 42719 ||| 
2018 ||| multi-timescale memory dynamics extend task repertoire in a reinforcement learning network with attention-gated memory. ||| 34989 ||| 34990 ||| 34991 ||| 
2017 ||| a computational model for the automatic diagnosis of attention deficit hyperactivity disorder based on functional brain volume. ||| 42720 ||| 42721 ||| 28849 ||| 42722 ||| 42723 ||| 
2018 ||| effect of stimulus contrast and visual attention on spike-gamma phase relationship in macaque primary visual cortex. ||| 42724 ||| 42725 ||| 
2021 ||| toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. ||| 42726 ||| 42727 ||| 42728 ||| 42729 ||| 42730 ||| 42731 ||| 42732 ||| 42733 ||| 42734 ||| 42735 ||| 42736 ||| 
2019 ||| a computational model of attention control in multi-attribute, context-dependent decision making. ||| 42737 ||| 16094 ||| 42738 ||| 
2020 ||| integrating breakdown detection into dialogue systems to improve knowledge management: encoding temporal utterances with memory attention. ||| 42739 ||| 42740 ||| 42741 ||| 42742 ||| 36444 ||| 
2021 ||| ppanet: point-wise pyramid attention network for semantic segmentation. ||| 29519 ||| 26700 ||| 42743 ||| 17472 ||| 42744 ||| 42745 ||| 29520 ||| 42746 ||| 
2020 ||| multimodal fusion method based on self-attention mechanism. ||| 42747 ||| 42748 ||| 11796 ||| 18639 ||| 42749 ||| 42750 ||| 
2021 ||| joint generative image deblurring aided by edge attention prior and dynamic kernel selection. ||| 6563 ||| 10064 ||| 13241 ||| 42751 ||| 
2022 ||| remote sensing image fusion algorithm based on two-stream fusion network and residual channel attention mechanism. ||| 18632 ||| 42752 ||| 42753 ||| 42754 ||| 8976 ||| 28414 ||| 42755 ||| 
2020 ||| leveraging social relationship-based graph attention model for group event recommendation. ||| 25167 ||| 25168 ||| 
2021 ||| san-gal: spatial attention network guided by attribute label for person re-identification. ||| 24092 ||| 2420 ||| 24099 ||| 42756 ||| 18122 ||| 24093 ||| 
2021 ||| dual-level attention based on a heterogeneous graph convolution network for aspect-based sentiment classification. ||| 3559 ||| 479 ||| 162 ||| 5091 ||| 5090 ||| 5089 ||| 
2020 ||| automatic image captioning based on resnet50 and lstm with soft attention. ||| 6068 ||| 42757 ||| 42758 ||| 42759 ||| 6072 ||| 
2021 ||| attention mechanism-based cnn-lstm model for wind turbine fault prediction using ssn ontology annotation. ||| 12120 ||| 42760 ||| 42761 ||| 42762 ||| 42763 ||| 42764 ||| 
2021 ||| eawnet: an edge attention-wise objector for real-time visual internet of things. ||| 6563 ||| 10064 ||| 13241 ||| 42751 ||| 
2017 ||| a semi-supervised inattention detection method using biological signal. ||| 41126 ||| 11906 ||| 41125 ||| 
2021 ||| aspect-based capsule network with mutual attention for recommendations. ||| 42765 ||| 34317 ||| 42766 ||| 42767 ||| 
2020 ||| recursive multi-signal temporal fusions with attention mechanism improves emg feature extraction. ||| 24253 ||| 42768 ||| 42769 ||| 42770 ||| 
2021 ||| reverse graph self-attention for target-directed atomic importance estimation. ||| 42771 ||| 42772 ||| 
2020 ||| attention-guided cnn for image denoising. ||| 42773 ||| 1125 ||| 27710 ||| 2018 ||| 14043 ||| 2519 ||| 
2021 ||| end-to-end keyword search system based on attention mechanism and energy scorer for low resource languages. ||| 14604 ||| 11779 ||| 
2019 ||| neural dynamics of spreading attentional labels in mental contour tracing. ||| 42774 ||| 42775 ||| 
2020 ||| learning cascade attention for fine-grained image classification. ||| 42776 ||| 42777 ||| 25908 ||| 42778 ||| 
2020 ||| on the localness modeling for the self-attention based end-to-end speech synthesis. ||| 12571 ||| 12572 ||| 4529 ||| 42779 ||| 42780 ||| 4530 ||| 12384 ||| 3808 ||| 
2022 ||| multi-level attention pooling for graph neural networks: unifying graph representations with multiple localities. ||| 4420 ||| 4421 ||| 4427 ||| 
2020 ||| sequential vessel segmentation via deep channel attention network. ||| 39023 ||| 39024 ||| 39025 ||| 39026 ||| 39027 ||| 39028 ||| 39029 ||| 
2018 |||  attention: an lstm framework for human trajectory prediction and abnormal event detection. ||| 12567 ||| 11374 ||| 11330 ||| 11331 ||| 
2022 ||| transformers for modeling physical systems. ||| 32928 ||| 32929 ||| 
2020 ||| neuromodulated attention and goal-driven perception in uncertain domains. ||| 32844 ||| 32842 ||| 32846 ||| 32845 ||| 
2021 ||| bilateral attention decoder: a lightweight decoder for real-time semantic segmentation. ||| 42781 ||| 1321 ||| 2230 ||| 42782 ||| 6863 ||| 
2022 ||| cross-attention-map-based regularization for adversarial domain adaptation. ||| 42783 ||| 42784 ||| 10992 ||| 42785 ||| 42786 ||| 
2021 ||| unsupervised foveal vision neural architecture with top-down attention. ||| 850 ||| 34144 ||| 34145 ||| 852 ||| 853 ||| 854 ||| 
2021 ||| low-shot transfer with attention for highly imbalanced cursive character recognition. ||| 42787 ||| 42788 ||| 488 ||| 
2021 ||| improved deep cnns based on nonlinear hybrid attention module for image classification. ||| 40009 ||| 31523 ||| 40011 ||| 24625 ||| 
2020 ||| cnn-mhsa: a convolutional neural network and multi-head self-attention combined approach for detecting phishing websites. ||| 2744 ||| 12140 ||| 2745 ||| 3614 ||| 2748 ||| 
2018 ||| distant supervision for neural relation extraction integrated with word attention and property features. ||| 24812 ||| 42789 ||| 24832 ||| 42790 ||| 42791 ||| 
2021 ||| optimal attention tuning in a neuro-computational model of the visual cortex-basal ganglia-prefrontal cortex loop. ||| 42792 ||| 42793 ||| 42794 ||| 
2021 ||| multi-scale attention convolutional neural network for time series classification. ||| 5110 ||| 42795 ||| 
2021 ||| tgan: a simple model update strategy for visual tracking via template-guidance attention network. ||| 5492 ||| 31722 ||| 42796 ||| 42797 ||| 
2021 ||| hiam: a hierarchical attention based model for knowledge graph multi-hop reasoning. ||| 24651 ||| 17917 ||| 9019 ||| 5295 ||| 
2021 ||| stacked debert: all attention in incomplete data for text classification. ||| 32850 ||| 488 ||| 
2021 ||| graph embedding clustering: graph attention auto-encoder with cluster-specificity distribution. ||| 42798 ||| 33093 ||| 42799 ||| 2278 ||| 6621 ||| 
2021 ||| combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition. ||| 645 ||| 25762 ||| 11983 ||| 647 ||| 12310 ||| 12041 ||| 648 ||| 649 ||| 
2018 ||| deep neural network for traffic sign recognition systems: an analysis of spatial transformers and stochastic optimisation methods. ||| 42800 ||| 3419 ||| 42801 ||| 42064 ||| 42802 ||| 
2022 ||| utrad: anomaly detection and localization with u-transformer. ||| 36848 ||| 42803 ||| 40645 ||| 42804 ||| 42805 ||| 
2021 ||| h-vectors: improving the robustness in utterance-level speaker embeddings using a hierarchical attention model. ||| 12086 ||| 12087 ||| 8233 ||| 
2020 ||| mgat: multi-view graph attention networks. ||| 29455 ||| 42806 ||| 34992 ||| 42807 ||| 42808 ||| 
2021 ||| d-mona: a dilated mixed-order non-local attention network for speaker and language recognition. ||| 14246 ||| 12357 ||| 24091 ||| 12104 ||| 
2022 ||| generalized attention-weighted reinforcement learning. ||| 37891 ||| 37892 ||| 
2018 ||| distant supervision for relation extraction with hierarchical selective attention. ||| 24204 ||| 5350 ||| 42809 ||| 42810 ||| 5308 ||| 728 ||| 
2022 ||| dual global enhanced transformer for image captioning. ||| 22649 ||| 264 ||| 613 ||| 262 ||| 
2020 ||| high tissue contrast image synthesis via multistage attention-gan: application to segmenting brain mr scans. ||| 18068 ||| 5476 ||| 6582 ||| 
2021 ||| self-selective attention using correlation between instances for distant supervision relation extraction. ||| 42811 ||| 42812 ||| 42813 ||| 18232 ||| 42814 ||| 
2021 ||| igagcn: information geometry and attention-based spatiotemporal graph convolutional networks for traffic flow prediction. ||| 42815 ||| 42816 ||| 683 ||| 42817 ||| 42818 ||| 42819 ||| 4003 ||| 
2021 ||| sam-gan: self-attention supporting multi-stage generative adversarial networks for text-to-image synthesis. ||| 29271 ||| 42820 ||| 5010 ||| 42821 ||| 
2021 ||| tumor attention networks: better feature selection, better tumor segmentation. ||| 42822 ||| 42823 ||| 41667 ||| 42824 ||| 42825 ||| 
2019 ||| attention inspired network: steep learning curve in an invariant pattern recognition model. ||| 42826 ||| 42827 ||| 
2020 ||| amd-gan: attention encoder and multi-branch structure based generative adversarial networks for fundus disease detection from scanning laser ophthalmoscopy images. ||| 27716 ||| 27715 ||| 27717 ||| 18435 ||| 15668 ||| 440 ||| 31510 ||| 31207 ||| 15670 ||| 6582 ||| 
2021 ||| uncertainty-aware fusion of probabilistic classifiers for improved transformer diagnostics. ||| 29252 ||| 29257 ||| 29253 ||| 29254 ||| 29255 ||| 29256 ||| 
2021 ||| self-attention-based temporary curiosity in reinforcement learning exploration. ||| 42828 ||| 35048 ||| 7875 ||| 
2018 ||| pricing when customers have limited attention. ||| 31705 ||| 31706 ||| 31707 ||| 31708 ||| 
2018 ||| maxing out globally: individualism, investor attention, and the cross section of expected stock returns. ||| 42829 ||| 15434 ||| 
2022 ||| inattention in contract markets: evidence from a consolidation of options in telecom. ||| 648 ||| 42830 ||| 42831 ||| 42832 ||| 42833 ||| 42834 ||| 
2021 ||| asset management and financial conglomerates: attention through stellar funds. ||| 42835 ||| 
2020 ||| dynamic attention behavior under return predictability. ||| 42836 ||| 42837 ||| 
2021 ||| intertemporal choices are causally influenced by fluctuations in visual attention. ||| 42838 ||| 
2017 ||| the value of nothing: asymmetric attention to opportunity costs drives intertemporal decision making. ||| 42839 ||| 42840 ||| 42841 ||| 
2017 ||| the comovement of investor attention. ||| 42842 ||| 42843 ||| 42844 ||| 42845 ||| 
2021 ||| cater to thy client: analyst responsiveness to institutional investor attention. ||| 42846 ||| 42847 ||| 42848 ||| 42849 ||| 
2021 ||| asymmetric attention and stock returns. ||| 42850 ||| 42851 ||| 42852 ||| 
2021 ||| competition for attention in online social networks: implications for seeding strategies. ||| 42853 ||| 42854 ||| 42855 ||| 
2018 ||| promoting change from the outside: directing managerial attention in the implementation of environmental improvements. ||| 42856 ||| 42857 ||| 42858 ||| 
2018 ||| category encoders: a scikit-learn-contrib package of transformers for encoding categorical data. ||| 42859 ||| 42860 ||| 42861 ||| 42862 ||| 
2021 ||| tx$^2$: transformer explainability and exploration. ||| 42863 ||| 42864 ||| 
2021 ||| underwater image restoration and enhancement via residual two-fold attention networks. ||| 42865 ||| 42866 ||| 42867 ||| 42868 ||| 42869 ||| 189 ||| 
2019 ||| attention pooling-based bidirectional gated recurrent units model for sentimental classification. ||| 5815 ||| 5811 ||| 17556 ||| 22041 ||| 42870 ||| 42871 ||| 26781 ||| 
2021 ||| deep learning-based short-term load forecasting for transformers in distribution grid. ||| 42872 ||| 764 ||| 
2020 ||| sequential prediction of glycosylated hemoglobin based on long short-term memory with self-attention mechanism. ||| 42873 ||| 42874 ||| 42875 ||| 42876 ||| 19144 ||| 42877 ||| 42878 ||| 
2021 ||| a classification model of legal consulting questions based on multi-attention prototypical networks. ||| 42879 ||| 42880 ||| 42881 ||| 42882 ||| 42883 ||| 
2021 ||| forecasting teleconsultation demand with an ensemble attention-based bidirectional long short-term memory model. ||| 42884 ||| 42885 ||| 42886 ||| 
2021 ||| restoring attentional resources with nature: a replication study of berto's (2005) paradigm including commentary from dr. rita berto. ||| 42887 ||| 42888 ||| 42889 ||| 42890 ||| 
2019 ||| sustained attention to science: a tribute to the life and scholarship of joel warm. ||| 42891 ||| 42892 ||| 
2018 ||| auditory task irrelevance: a basis for inattentional deafness. ||| 42893 ||| 42894 ||| 42895 ||| 42896 ||| 
2017 ||| task engagement and attentional resources. ||| 42897 ||| 42898 ||| 42899 ||| 
2020 ||| classification of attentional tunneling through behavioral indices. ||| 42900 ||| 42901 ||| 
2020 ||| artificial optic flow guides visual attention in a driving scene. ||| 42902 ||| 42903 ||| 42904 ||| 7851 ||| 
2020 ||| attentional demand as a function of contextual factors in different traffic scenarios. ||| 42905 ||| 42906 ||| 5335 ||| 42907 ||| 42908 ||| 
2017 ||| minimum required attention: a human-centered approach to driver inattention. ||| 42908 ||| 42906 ||| 5335 ||| 
2018 ||| measuring and mitigating the costs of attentional switches in active network monitoring for cybersecurity. ||| 42900 ||| 42909 ||| 42901 ||| 39036 ||| 42910 ||| 14807 ||| 
2020 ||| shoulder muscular fatigue from static posture concurrently reduces cognitive attentional resources. ||| 42911 ||| 42912 ||| 42913 ||| 21042 ||| 
2018 ||| allocating attention to detect motorcycles: the role of inattentional blindness. ||| 42914 ||| 42915 ||| 42916 ||| 
2019 ||| the abbreviated vigilance task and its attentional contributors. ||| 42888 ||| 42890 ||| 
2017 ||| measuring memory and attention to preview in motion. ||| 42917 ||| 42918 ||| 42919 ||| 
2017 ||| a closed-loop model of operator visual attention, situation awareness, and performance across automation mode transitions. ||| 42920 ||| 42921 ||| 42922 ||| 42923 ||| 
2017 ||| measuring sustained attention and perceived workload. ||| 42924 ||| 42925 ||| 42926 ||| 
2019 ||| the effect of partial automation on driver attention: a naturalistic driving study. ||| 42927 ||| 42928 ||| 
2020 ||| allocation of driver attention for varying in-vehicle system modalities. ||| 7015 ||| 42929 ||| 
2021 ||| drivers' spatio-temporal attentional distributions are influenced by vehicle dynamics and displayed point of view. ||| 42930 ||| 42919 ||| 42931 ||| 42917 ||| 42932 ||| 42933 ||| 
2019 ||| spatial attention model based target detection for aerial robotic systems. ||| 5475 ||| 15716 ||| 35716 ||| 42934 ||| 5810 ||| 
2021 ||| transformers aftermath: current research and rising trends. ||| 42935 ||| 42936 ||| 42937 ||| 42938 ||| 42939 ||| 42940 ||| 42941 ||| 42942 ||| 42943 ||| 42944 ||| 5335 ||| 42945 ||| 42946 ||| 
2018 ||| attention guided u-net for accurate iris segmentation. ||| 42947 ||| 42948 ||| 30828 ||| 22494 ||| 21801 ||| 42949 ||| 
2022 ||| attention integrated hierarchical networks for no-reference image quality assessment. ||| 11247 ||| 11248 ||| 
2021 ||| attention-based contextual interaction asymmetric network for rgb-d saliency prediction. ||| 15909 ||| 15908 ||| 31370 ||| 31372 ||| 
2021 ||| sign language recognition based on global-local attention. ||| 42950 ||| 31971 ||| 
2020 ||| cross-level reinforced attention network for person re-identification. ||| 24565 ||| 17546 ||| 27439 ||| 42951 ||| 42952 ||| 
2019 ||| deep spatial attention hashing network for image retrieval. ||| 42953 ||| 1796 ||| 42954 ||| 26814 ||| 1780 ||| 42955 ||| 
2020 ||| ir saliency detection via a gcf-sb visual attention framework. ||| 42956 ||| 42957 ||| 210 ||| 42958 ||| 42959 ||| 7676 ||| 42960 ||| 41041 ||| 
2019 ||| two-level attention with two-stage multi-task learning for facial emotion recognition. ||| 5885 ||| 5886 ||| 5887 ||| 5888 ||| 5889 ||| 5890 ||| 
2019 ||| weighted correlation filters guidance with spatial-temporal attention for online multi-object tracking. ||| 42961 ||| 31828 ||| 42962 ||| 28290 ||| 
2020 ||| spatial-temporal saliency action mask attention network for action recognition. ||| 24565 ||| 42963 ||| 27439 ||| 
2021 ||| spatial self-attention network with self-attention distillation for fine-grained image recognition. ||| 42964 ||| 3293 ||| 16446 ||| 40737 ||| 41430 ||| 
2021 ||| attention-guided image captioning with adaptive global and local feature fusion. ||| 8372 ||| 42965 ||| 42966 ||| 8374 ||| 15854 ||| 17661 ||| 
2022 ||| dca-cyclegan: unsupervised single image dehazing using dark channel attention optimized cyclegan. ||| 42967 ||| 42968 ||| 32014 ||| 6524 ||| 
2021 ||| sequential alignment attention model for scene text recognition. ||| 15016 ||| 42969 ||| 19785 ||| 18857 ||| 19914 ||| 18858 ||| 17695 ||| 
2021 ||| stable self-attention adversarial learning for semi-supervised semantic image segmentation. ||| 720 ||| 264 ||| 613 ||| 262 ||| 
2020 ||| exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-identification. ||| 42970 ||| 42971 ||| 42972 ||| 42973 ||| 42974 ||| 
2019 ||| a multiscale dilated dense convolutional network for saliency prediction with instance-level attention competition. ||| 1705 ||| 42975 ||| 21748 ||| 42976 ||| 
2021 ||| image super-resolution based on deep neural network of multiple attention mechanism. ||| 7676 ||| 42977 ||| 29236 ||| 30969 ||| 
2021 ||| attention guided feature pyramid network for crowd counting. ||| 42978 ||| 19874 ||| 42979 ||| 
2021 ||| a bottom-up and top-down human visual attention approach for hyperspectral anomaly detection. ||| 42980 ||| 42981 ||| 
2021 ||| residual attention-based tracking-by-detection network with attention-driven data augmentation. ||| 42982 ||| 29611 ||| 42983 ||| 7436 ||| 42984 ||| 
2019 ||| multimodal activity recognition with local block cnn and attention-based spatial weighted cnn. ||| 42985 ||| 42986 ||| 9149 ||| 1754 ||| 7688 ||| 
2021 ||| multi-view motion modelled deep attention networks (m2da-net) for video based sign language recognition. ||| 42987 ||| 42988 ||| 42989 ||| 
2020 ||| local relation network with multilevel attention for visual question answering. ||| 534 ||| 42990 ||| 42991 ||| 535 ||| 
2019 ||| an extensive evaluation of deep featuresof convolutional neural networks for saliency prediction of human visual attention. ||| 42992 ||| 6173 ||| 
2021 ||| gaze estimation via bilinear pooling-based attention networks. ||| 42993 ||| 976 ||| 42994 ||| 42995 ||| 42996 ||| 35167 ||| 
2021 ||| shape transformer nets: generating viewpoint-invariant 3d shapes from a single image. ||| 42997 ||| 42998 ||| 12169 ||| 
2021 ||| single image deblurring with cross-layer feature fusion and consecutive attention. ||| 20152 ||| 5298 ||| 16821 ||| 5300 ||| 
2021 ||| multi-scale attention network for image super-resolution. ||| 1052 ||| 20051 ||| 42999 ||| 43000 ||| 43001 ||| 
2022 ||| cross-layer progressive attention bilinear fusion method for fine-grained visual classification. ||| 43002 ||| 30102 ||| 43003 ||| 43004 ||| 11973 ||| 43005 ||| 
2017 ||| exploring visual attention using random walks based eye tracking protocols. ||| 43006 ||| 9061 ||| 
2021 ||| blind image quality assessment with channel attention based deep residual network and extended largevis dimensionality reduction. ||| 43007 ||| 28786 ||| 29187 ||| 875 ||| 444 ||| 
2021 ||| mranet: multi-atrous residual attention network for stereo image super-resolution. ||| 43008 ||| 40341 ||| 43009 ||| 43010 ||| 43011 ||| 
2022 ||| attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing. ||| 9337 ||| 43012 ||| 43013 ||| 
2021 ||| multiple attention encoded cascade r-cnn for scene text detection. ||| 4285 ||| 43014 ||| 43015 ||| 
2021 ||| role-based attention in deep reinforcement learning for games. ||| 2019 ||| 4552 ||| 20325 ||| 43016 ||| 
2021 ||| aff-dehazing: attention-based feature fusion network for low-light image dehazing. ||| 11689 ||| 14026 ||| 14037 ||| 977 ||| 14039 ||| 32700 ||| 
2019 ||| classification and diagnosis of thyroid carcinoma using reinforcement residual network with visual attention mechanisms in ultrasound images. ||| 43017 ||| 
2019 ||| top-down attention recurrent vlad encoding for action recognition in videos. ||| 9832 ||| 9833 ||| 
2019 ||| focusing attention in populations of semi-autonomously operating sensing nodes. ||| 43018 ||| 43019 ||| 43020 ||| 43021 ||| 
2020 ||| at first sight: robots' subtle eye movement parameters affect human attentional engagement, spontaneous attunement and perceived human-likeness. ||| 12832 ||| 12834 ||| 12833 ||| 43022 ||| 10259 ||| 
2017 ||| a customized attention-based long short-term memory network for distant supervised relation extraction. ||| 30860 ||| 30861 ||| 30862 ||| 3248 ||| 24121 ||| 
2021 ||| real-time decoding of attentional states using closed-loop eeg neurofeedback. ||| 43023 ||| 43024 ||| 43025 ||| 43026 ||| 
2022 ||| spatial attention enhances crowded stimulus encoding across modeled receptive fields by increasing redundancy of feature representations. ||| 43027 ||| 43028 ||| 43029 ||| 
2021 ||| emergence of content-agnostic information processing by a robot using active inference, visual attention, working memory, and planning. ||| 43030 ||| 43031 ||| 25497 ||| 25498 ||| 25499 ||| 
2021 ||| flexible working memory through selective gating and attentional tagging. ||| 43032 ||| 9145 ||| 9146 ||| 43033 ||| 
2020 ||| fine-grained 3d-attention prototypes for few-shot learning. ||| 22883 ||| 1235 ||| 11423 ||| 22884 ||| 39296 ||| 
2020 ||| semantic vector learning using pretrained transformers in natural language understanding. ||| 43034 ||| 
2017 ||| computational visual attention models. ||| 43035 ||| 43036 ||| 
2021 ||| mutualrec: joint friend and item recommendations with mutualistic attentional graph neural networks. ||| 8488 ||| 30589 ||| 43037 ||| 771 ||| 5791 ||| 
2020 ||| a novel syntax-aware automatic graphics code generation with attention-based deep neural network. ||| 43038 ||| 43039 ||| 14647 ||| 25737 ||| 43040 ||| 43041 ||| 
2022 ||| transformers for gui testing: a plausible solution to automated test case generation and flaky tests. ||| 43042 ||| 43043 ||| 43044 ||| 
2017 ||| are they paying attention? a model-based method to identify individuals' mental states. ||| 43045 ||| 43046 ||| 43047 ||| 
2021 ||| ai ethics: a long history and a recent burst of attention. ||| 43048 ||| 43049 ||| 43050 ||| 43051 ||| 43052 ||| 
2020 ||| design of dry-type transformer temperature controller based on internet of things. ||| 43053 ||| 43054 ||| 43055 ||| 43056 ||| 
2021 ||| multi-level spatial attention network for image data segmentation. ||| 786 ||| 43057 ||| 43058 ||| 
2021 ||| an alzheimer's disease identification and classification model based on the convolutional neural network with attention mechanisms. ||| 43059 ||| 
2021 ||| medical big data analysis with attention and large margin loss model for skin lesion application. ||| 839 ||| 6050 ||| 21688 ||| 6595 ||| 43060 ||| 43061 ||| 10405 ||| 
2021 ||| multimodal emotion recognition from art using sequential co-attention. ||| 20219 ||| 43062 ||| 20220 ||| 20221 ||| 13992 ||| 
2020 ||| attention-based fully gated cnn-bgru for russian handwritten text. ||| 38388 ||| 38389 ||| 38390 ||| 
2021 ||| skeleton-based attention mask for pedestrian attribute recognition network. ||| 43063 ||| 43064 ||| 
2021 ||| to grasp the world at a glance: the role of attention in visual and semantic associative processing. ||| 43065 ||| 
2018 ||| macrobase: prioritizing attention in fast data. ||| 43066 ||| 9953 ||| 43067 ||| 9954 ||| 15148 ||| 15149 ||| 9955 ||| 9956 ||| 
2019 ||| multistep flow prediction on car-sharing systems: a multi-graph convolutional neural network with attention mechanism. ||| 25651 ||| 25650 ||| 20085 ||| 25652 ||| 25653 ||| 25654 ||| 25310 ||| 
2020 ||| enabling reliability-driven optimization selection with gate graph attention neural network. ||| 18469 ||| 43068 ||| 43069 ||| 39818 ||| 14510 ||| 
2020 ||| split attention pointer network for source code language modeling. ||| 43070 ||| 43071 ||| 
2021 ||| an effective method of evaluating pension service quality using multi-dimension attention convolutional neural networks. ||| 43072 ||| 30907 ||| 43073 ||| 43074 ||| 43075 ||| 
2020 ||| opinion dynamics with topological gossiping: asynchronous updates under limited attention. ||| 2981 ||| 2980 ||| 
2018 ||| transformer semantics. ||| 38795 ||| 
2020 ||| a deep attention-based ensemble network for real-time face hallucination. ||| 15559 ||| 43076 ||| 31120 ||| 43077 ||| 43078 ||| 17965 ||| 43079 ||| 43080 ||| 16597 ||| 
2022 ||| event camera simulator design for modeling attention-based inference architectures. ||| 14199 ||| 39868 ||| 14200 ||| 14201 ||| 
2021 ||| efficient unsupervised monocular depth estimation using attention guided generative adversarial network. ||| 43081 ||| 19284 ||| 43082 ||| 2230 ||| 
2021 ||| two-pathway attention network for real-time facial expression recognition. ||| 37798 ||| 43083 ||| 43084 ||| 19109 ||| 43085 ||| 43086 ||| 
2021 ||| an improved one-stage pedestrian detection method based on multi-scale attention feature extraction. ||| 9547 ||| 43087 ||| 43088 ||| 31358 ||| 31361 ||| 
2021 ||| fast contour detection with supervised attention learning. ||| 37091 ||| 28347 ||| 
2019 ||| graph convolutional network with sequential attention for goal-oriented dialogue systems. ||| 43089 ||| 3328 ||| 
2021 ||| augmenting transformers with knn-based composite memory for dialog. ||| 23898 ||| 32971 ||| 9772 ||| 32972 ||| 32973 ||| 
2021 ||| recursive non-autoregressive graph-to-graph transformer for dependency parsing with iterative refinement. ||| 26685 ||| 3672 ||| 
2021 ||| sparse, dense, and attentional representations for text retrieval. ||| 4764 ||| 13316 ||| 3185 ||| 37389 ||| 
2020 ||| theoretical limitations of self-attention in neural sequence models. ||| 35699 ||| 
2019 ||| attention-passing models for robust and data-efficient end-to-end speech translation. ||| 3516 ||| 3067 ||| 11470 ||| 3518 ||| 
2018 ||| video captioning with multi-faceted attention. ||| 18016 ||| 2190 ||| 3774 ||| 
2021 ||| efficient content-based sparse attention with routing transformers. ||| 39395 ||| 39396 ||| 2466 ||| 39397 ||| 
2021 ||| extractive opinion summarization in quantized transformer spaces. ||| 36953 ||| 3713 ||| 36954 ||| 36955 ||| 3408 ||| 
2020 ||| amr-to-text generation with graph transformer. ||| 3120 ||| 3121 ||| 43090 ||| 
2017 ||| overcoming language variation in sentiment analysis with social attention. ||| 208 ||| 13316 ||| 
2018 ||| attentive convolution: equipping cnns with rnn-style attention mechanisms. ||| 11631 ||| 11716 ||| 11717 ||| 
2021 ||| decoupling the role of data, attention, and losses in multimodal transformers. ||| 3717 ||| 33867 ||| 33868 ||| 1993 ||| 3718 ||| 
2020 ||| target-guided structured attention network for target-dependent sentiment analysis. ||| 1422 ||| 1177 ||| 4968 ||| 43091 ||| 43092 ||| 
2021 ||| editor: an edit-based transformer with repositioning for neural machine translation with soft lexical constraints. ||| 34853 ||| 34854 ||| 
2018 ||| developing joint attention for children with autism in robot-enhanced therapy. ||| 17051 ||| 43093 ||| 43094 ||| 43095 ||| 41510 ||| 
2021 ||| does context matter? effects of robot appearance and reliability on social attention differs based on lifelikeness of gaze task. ||| 43096 ||| 43097 ||| 4995 ||| 
2018 ||| directing attention through gaze hints improves task solving in human-humanoid interaction. ||| 43098 ||| 43099 ||| 43100 ||| 43101 ||| 43102 ||| 43103 ||| 43104 ||| 
2021 ||| personalized robot interventions for autistic children: an automated methodology for attention assessment. ||| 43105 ||| 20880 ||| 20879 ||| 20881 ||| 43106 ||| 
2021 ||| analysis of attention in child-robot interaction among children diagnosed with cognitive impairment. ||| 43107 ||| 43108 ||| 43109 ||| 15034 ||| 43110 ||| 
2017 ||| role of speaker cues in attention inference. ||| 43111 ||| 43112 ||| 43113 ||| 
2017 ||| the attention schema theory: a foundation for engineering artificial consciousness. ||| 43114 ||| 
2020 ||| repetitive robot behavior impacts perception of intentionality and gaze-related attentional orienting. ||| 43096 ||| 10259 ||| 
2017 ||| probabilistic mapping of human visual attention from head pose estimation. ||| 43115 ||| 43116 ||| 43117 ||| 33532 ||| 
2021 ||| attention enhancement for exoskeleton-assisted hand rehabilitation using fingertip haptic stimulation. ||| 12646 ||| 27834 ||| 43118 ||| 9837 ||| 43119 ||| 43120 ||| 43121 ||| 11745 ||| 23710 ||| 43122 ||| 
2020 ||| improving ct image tumor segmentation through deep supervision and attentional gates. ||| 43123 ||| 20220 ||| 43124 ||| 43125 ||| 43126 ||| 43127 ||| 4250 ||| 4251 ||| 4252 ||| 
2021 ||| mind the eyes: artificial agents' eye movements modulate attentional engagement and anthropomorphic attribution. ||| 12832 ||| 12834 ||| 12833 ||| 10259 ||| 
2018 ||| debugging translations of transformer-based neural machine translation systems. ||| 37277 ||| 43128 ||| 
2021 ||| power transmission line anomaly detection scheme based on cnn-transformer model. ||| 4160 ||| 43129 ||| 
2021 ||| research on lstm+attention model of infant cry classification. ||| 43130 ||| 43131 ||| 43132 ||| 36219 ||| 
2020 ||| crowd counting network with self-attention distillation. ||| 43133 ||| 1052 ||| 43134 ||| 43135 ||| 
2020 ||| visual attention-based comparative study on disaster detection from social media images. ||| 20112 ||| 7376 ||| 7375 ||| 43136 ||| 
2021 ||| investigating and modeling the web elements' visual feature influence on free-viewing attention. ||| 24559 ||| 24560 ||| 24561 ||| 
2020 ||| herg-att: self-attention-based deep neural network for predicting herg blockers. ||| 43137 ||| 40236 ||| 
2021 ||| able: attention based learning for enzyme classification. ||| 43138 ||| 4155 ||| 
2020 ||| attention-based generative adversarial network for semi-supervised image classification. ||| 11589 ||| 43139 ||| 6927 ||| 43140 ||| 11592 ||| 
2020 ||| hierarchical temporal fusion of multi-grained attention features for video question answering. ||| 7649 ||| 7650 ||| 7651 ||| 9570 ||| 1937 ||| 1306 ||| 18948 ||| 7652 ||| 
2020 ||| semantic image segmentation with improved position attention and feature fusion. ||| 29308 ||| 43141 ||| 8622 ||| 
2021 ||| multi-object tracking method based on efficient channel attention and switchable atrous convolution. ||| 11589 ||| 43142 ||| 43143 ||| 43144 ||| 6927 ||| 
2019 ||| adaptive syncretic attention for constrained image captioning. ||| 8967 ||| 5746 ||| 
2021 ||| attention-based deep gated fully convolutional end-to-end architectures for time series classification. ||| 43145 ||| 31572 ||| 43146 ||| 
2019 ||| image captioning with text-based visual attention. ||| 43147 ||| 5746 ||| 
2022 ||| scale-insensitive object detection via attention feature pyramid transformer network. ||| 3535 ||| 2781 ||| 41263 ||| 43148 ||| 43149 ||| 
2019 ||| image caption with endogenous-exogenous attention. ||| 30559 ||| 5746 ||| 43147 ||| 
2021 ||| refine for semantic segmentation based on parallel convolutional network with attention model. ||| 27172 ||| 27171 ||| 1371 ||| 
2020 ||| refocused attention: long short-term rewards guided video captioning. ||| 43150 ||| 33136 ||| 43151 ||| 1317 ||| 
2021 ||| parsbert: transformer-based model for persian language understanding. ||| 32587 ||| 32588 ||| 32589 ||| 32590 ||| 
2019 ||| image captioning with bidirectional semantic attention-based guiding of long short-term memory. ||| 26280 ||| 43152 ||| 19819 ||| 26468 ||| 13688 ||| 13692 ||| 
2022 ||| an improved attention and hybrid optimization technique for visual question answering. ||| 28692 ||| 28693 ||| 
2021 ||| self-supervised monocular trained depth estimation using triplet attention and funnel activation. ||| 11589 ||| 43140 ||| 43143 ||| 43144 ||| 6927 ||| 
2022 ||| proposal-based graph attention networks for workflow detection. ||| 1254 ||| 43153 ||| 43154 ||| 1037 ||| 
2020 ||| traffic sign recognition in harsh environment using attention based convolutional pooling neural network. ||| 43155 ||| 43156 ||| 43157 ||| 43158 ||| 
2021 ||| multilevel attention models for drug target binding affinity prediction. ||| 43159 ||| 
2020 ||| deep dual-stream network with scale context selection attention module for semantic segmentation. ||| 41563 ||| 2591 ||| 6318 ||| 6915 ||| 23650 ||| 31025 ||| 
2021 ||| exposing deepfake videos using attention based convolutional lstm network. ||| 43160 ||| 43161 ||| 6076 ||| 19742 ||| 
2019 ||| image captioning using region-based attention joint with time-varying attention. ||| 6317 ||| 5746 ||| 
2021 ||| siamese pre-trained transformer encoder for knowledge base completion. ||| 30628 ||| 1241 ||| 800 ||| 
2017 ||| capturing temporal structures for video captioning by spatio-temporal contexts and channel attention mechanism. ||| 19890 ||| 3337 ||| 8726 ||| 
2020 ||| bi-directional lstm model with symptoms-frequency position attention for question answering system in medical domain. ||| 43162 ||| 5831 ||| 29885 ||| 43163 ||| 43164 ||| 
2020 ||| 3d model retrieval using bipartite graph matching based on attention. ||| 7138 ||| 920 ||| 43165 ||| 43166 ||| 43167 ||| 43168 ||| 
2020 ||| visual sentiment prediction with attribute augmentation and multi-attention mechanism. ||| 43169 ||| 40667 ||| 40666 ||| 
2021 ||| attention refined network for human pose estimation. ||| 19946 ||| 43170 ||| 3049 ||| 
2021 ||| image captioning with dense fusion connection and improved stacked attention module. ||| 29308 ||| 20370 ||| 8622 ||| 
2019 ||| deep captioning with attention-based visual concept transfer mechanism for enriching description. ||| 41909 ||| 5746 ||| 
2019 ||| multi-task character-level attentional networks for medical concept normalization. ||| 43171 ||| 28420 ||| 43172 ||| 35276 ||| 35277 ||| 
2020 ||| multi-layer attention based cnn for target-dependent sentiment classification. ||| 43173 ||| 43174 ||| 2279 ||| 2278 ||| 
2020 ||| attentive semantic and perceptual faces completion using self-attention generative adversarial networks. ||| 28896 ||| 29149 ||| 29376 ||| 
2021 ||| high gain and low noise wideband folded-switching mixer employing transformer and complimentary cs/cg hybrid topology. ||| 43175 ||| 43176 ||| 43177 ||| 43178 ||| 
2021 ||| automatic mandible segmentation from ct image using 3d fully convolutional neural network based on denseaspp and attention gates. ||| 43179 ||| 12424 ||| 43180 ||| 43181 ||| 43182 ||| 43183 ||| 986 ||| 
2022 ||| gca-net: global context attention network for intestinal wall vascular segmentation. ||| 4417 ||| 43184 ||| 543 ||| 43185 ||| 43186 ||| 43187 ||| 
2021 ||| lightweight pyramid network with spatial attention mechanism for accurate retinal vessel segmentation. ||| 43188 ||| 43189 ||| 43190 ||| 43191 ||| 38454 ||| 
2019 ||| memory-efficient 2.5d convolutional transformer networks for multi-modal deformable registration with weak label supervision applied to whole-heart ct and mri scans. ||| 43192 ||| 43193 ||| 43194 ||| 27351 ||| 
2021 ||| sha-mtl: soft and hard attention multi-task learning for automated breast cancer ultrasound image segmentation and classification. ||| 43195 ||| 43196 ||| 40315 ||| 43197 ||| 43198 ||| 22266 ||| 
2021 ||| duda-net: a double u-shaped dilated attention network for automatic infection area segmentation in covid-19 lung ct images. ||| 473 ||| 11446 ||| 43199 ||| 43200 ||| 13221 ||| 43201 ||| 43202 ||| 
2021 ||| 3d axial-attention for lung nodule classification. ||| 36676 ||| 36677 ||| 35952 ||| 
2021 ||| neural linguistic steganalysis via multi-head self-attention. ||| 43203 ||| 43204 ||| 1558 ||| 43205 ||| 
2019 ||| power transformer fault severity estimation based on dissolved gas analysis and energy of fault formation technique. ||| 43206 ||| 43207 ||| 10341 ||| 
2021 ||| non-local attention based cnn model for aspect extraction. ||| 43208 ||| 43209 ||| 43210 ||| 711 ||| 
2021 ||| residual network for deep reinforcement learning with attention mechanism. ||| 28143 ||| 28144 ||| 
2022 ||| an explainable multi-modal hierarchical attention model for developing phishing threat intelligence. ||| 43211 ||| 43212 ||| 43213 ||| 43214 ||| 
2021 ||| datingsec: detecting malicious accounts in dating apps using a content-based attention network. ||| 43215 ||| 43216 ||| 8335 ||| 1420 ||| 398 ||| 2758 ||| 
2021 ||| monte carlo denoising via auxiliary feature guided self-attention. ||| 43217 ||| 42666 ||| 7175 ||| 43218 ||| 6645 ||| 43219 ||| 
2019 ||| deepremaster: temporal source-reference attention networks for comprehensive video enhancement. ||| 20045 ||| 15920 ||| 
2021 ||| transflower: probabilistic autoregressive dance generation with multimodal attention. ||| 38625 ||| 2600 ||| 3203 ||| 21673 ||| 38626 ||| 36973 ||| 38627 ||| 
2020 ||| a multimodal interlocutor-modulated attentional blstm for classifying autism subgroups during clinical interviews. ||| 12346 ||| 14400 ||| 12347 ||| 
2020 ||| oidc-net: omnidirectional image distortion correction via coarse-to-fine region attention. ||| 43220 ||| 43221 ||| 4400 ||| 31739 ||| 15770 ||| 
2021 ||| lightweight tensor attention-driven convlstm neural network for hyperspectral image classification. ||| 31731 ||| 31730 ||| 43222 ||| 10525 ||| 6720 ||| 6750 ||| 
2021 ||| attention-based neural networks for chroma intra prediction in video coding. ||| 11480 ||| 11481 ||| 11482 ||| 11483 ||| 1675 ||| 11484 ||| 
2020 ||| gan-generated image detection with self-attention mechanism against gan generator defect. ||| 43223 ||| 43224 ||| 43225 ||| 3617 ||| 
2017 ||| hybrid ctc/attention architecture for end-to-end speech recognition. ||| 3549 ||| 2508 ||| 12177 ||| 2511 ||| 8223 ||| 
2020 ||| where is the model looking at? - concentrate and explain the network attention. ||| 19479 ||| 19478 ||| 602 ||| 21132 ||| 30467 ||| 7697 ||| 21133 ||| 
2020 ||| automatic assessment of depression from speech via a hierarchical attention transfer network and attention autoencoders. ||| 645 ||| 12309 ||| 11983 ||| 23792 ||| 647 ||| 12310 ||| 12041 ||| 648 ||| 649 ||| 
2020 ||| statistical modeling of visual attention of junior and senior anesthesiologists during the induction of general anesthesia in real and simulated cases. ||| 43226 ||| 43227 ||| 43228 ||| 
2021 ||| sequential weakly labeled multiactivity localization and recognition on wearable sensors using recurrent attention networks. ||| 11361 ||| 532 ||| 241 ||| 
2019 ||| electroencephalographic phase-amplitude coupling in simulated driving with varying modality-specific attentional demand. ||| 43229 ||| 43230 ||| 43231 ||| 43232 ||| 43233 ||| 7074 ||| 
2022 ||| eeg-based auditory attention detection via frequency and channel neural attention. ||| 14575 ||| 14576 ||| 14578 ||| 12494 ||| 
2022 ||| binocular feature fusion and spatial attention mechanism based gaze tracking. ||| 43234 ||| 43235 ||| 9440 ||| 
2020 ||| an ego-vision system for discovering human joint attention. ||| 6419 ||| 7893 ||| 7894 ||| 
2018 ||| design, development, and evaluation of a noninvasive autonomous robot-mediated joint attention intervention system for young children with asd. ||| 4613 ||| 15201 ||| 43236 ||| 43237 ||| 4617 ||| 4618 ||| 
2019 ||| drivers' attentional instability on a winding roadway. ||| 42917 ||| 42919 ||| 43238 ||| 42931 ||| 42930 ||| 42932 ||| 42933 ||| 
2018 ||| eye tracking the visual attention of nurses interpreting simulated vital signs scenarios: mining metrics to discriminate between performance level. ||| 43239 ||| 43240 ||| 43241 ||| 43242 ||| 43243 ||| 43244 ||| 
2021 ||| risk assessment algorithm for power transformer fleets based on condition and strategic importance. ||| 43245 ||| 3369 ||| 43246 ||| 43247 ||| 
2021 ||| a domain adaptive person re-identification based on dual attention mechanism and camstyle transfer. ||| 43248 ||| 30267 ||| 30268 ||| 43249 ||| 43250 ||| 1206 ||| 
2021 ||| sequential recommendation through graph neural networks and transformer encoder with degree encoding. ||| 43251 ||| 43252 ||| 43253 ||| 390 ||| 43254 ||| 9643 ||| 9641 ||| 
2020 ||| mdan-unet: multi-scale and dual attention enhanced nested u-net architecture for segmentation of optical coherence tomography images. ||| 26165 ||| 43255 ||| 43256 ||| 
2021 ||| closed-loop cognitive-driven gain control of competing sounds using auditory attention decoding. ||| 1490 ||| 24387 ||| 43257 ||| 43258 ||| 1495 ||| 
2021 ||| a multinomial dga classifier for incipient fault detection in oil-impregnated power transformers. ||| 43259 ||| 43260 ||| 43261 ||| 
2021 ||| pm2.5 concentration prediction based on cnn-bilstm and attention mechanism. ||| 14692 ||| 43262 ||| 33123 ||| 43263 ||| 
2021 ||| an alternative explanation for attribute framing and spillover effects in multidimensional supplier evaluation and supplier termination: focusing on asymmetries in attention. ||| 43264 ||| 
2018 ||| recommendation of site commissioning tests for rapid recovery transformers with an installation time less than 30 hours. ||| 43265 ||| 43266 ||| 43267 ||| 43268 ||| 43269 ||| 43270 ||| 
2022 ||| online dissolved gas analysis used for transformers - possibilities, experiences, and limitations. ||| 43271 ||| 43272 ||| 13840 ||| 43273 ||| 43274 ||| 
2018 ||| measurement of thermal behavior of an ester-filled power transformer at ultra-low temperatures. ||| 43275 ||| 43276 ||| 
2020 ||| field experience of small quasi-dc bias on power transformers. ||| 43277 ||| 43278 ||| 43279 ||| 43276 ||| 43280 ||| 43281 ||| 
2018 ||| adhesives for bonding transformerboard: partial discharge and ageing behaviour. ||| 43282 ||| 3831 ||| 43283 ||| 43284 ||| 43285 ||| 43286 ||| 43287 ||| 43288 ||| 
2018 ||| geomagnetically induced currents modelling and monitoring transformer neutral currents in austria. ||| 43289 ||| 43279 ||| 43290 ||| 
2020 ||| verification of withstand capability for very fast transients of a 200 mva, 500 kv gsu transformer by modelling and testing. ||| 43291 ||| 43292 ||| 
2020 ||| evaluation of dynamic loading capability for optimal loading strategies of power transformers. ||| 43293 ||| 43294 ||| 43295 ||| 43296 ||| 43297 ||| 43298 ||| 43299 ||| 
2022 ||| mlfc-net: a multi-level feature combination attention model for remote sensing scene classification. ||| 43300 ||| 23348 ||| 43301 ||| 
2021 ||| dual-input attention network for automatic identification of detritus from river sands. ||| 43302 ||| 778 ||| 43303 ||| 43304 ||| 43305 ||| 
2022 ||| semantic segmentation of high-resolution remote sensing images based on a class feature attention mechanism fused with deeplabv3+. ||| 43306 ||| 43307 ||| 43308 ||| 43309 ||| 43310 ||| 43311 ||| 
2021 ||| strip pooling channel spatial attention network for the segmentation of cloud and cloud shadow. ||| 43312 ||| 41631 ||| 31341 ||| 
2019 ||| improving bug detection via context-based code representation learning and attention-based neural networks. ||| 1329 ||| 38823 ||| 43313 ||| 43314 ||| 
2019 ||| a predicate transformer semantics for effects (functional pearl). ||| 32365 ||| 43315 ||| 
2020 ||| bio-semantic relation extraction with attention-based external knowledge reinforcement. ||| 43316 ||| 43317 ||| 24646 ||| 397 ||| 399 ||| 
2022 ||| ggatlda: lncrna-disease association prediction based on graph-level graph attention network. ||| 1052 ||| 38595 ||| 
2021 ||| binding affinity prediction for protein-ligand complex using deep attention mechanism based on intermolecular interactions. ||| 43318 ||| 43319 ||| 29577 ||| 43320 ||| 
2018 ||| biomedical event extraction based on gru integrating attention mechanism. ||| 16631 ||| 16870 ||| 43321 ||| 8349 ||| 
2019 ||| chemical-induced disease relation extraction via attention-based distant supervision. ||| 43322 ||| 43323 ||| 21126 ||| 3088 ||| 
2020 ||| incorporating representation learning and multihead attention to improve biomedical cross-sentence n-ary relation extraction. ||| 6481 ||| 8349 ||| 43324 ||| 398 ||| 8974 ||| 16590 ||| 
2019 ||| biomedical word sense disambiguation with bidirectional long short-term memory and attention-based neural networks. ||| 43325 ||| 4866 ||| 4868 ||| 13702 ||| 
2022 ||| a-prot: protein structure modeling using msa transformer. ||| 43326 ||| 43327 ||| 43328 ||| 
2020 ||| hybrid attentional memory network for computational drug repositioning. ||| 16680 ||| 36333 ||| 36334 ||| 36335 ||| 
2021 ||| identifying pirna targets on mrnas in c. elegans using a deep multi-head attention network. ||| 43329 ||| 43330 ||| 17201 ||| 43331 ||| 43332 ||| 
2021 ||| gat-li: a graph attention network based learning and interpreting method for functional brain network classification. ||| 823 ||| 43333 ||| 824 ||| 825 ||| 977 ||| 
2021 ||| an efficient scrna-seq dropout imputation method using graph attention network. ||| 43334 ||| 23489 ||| 43335 ||| 
2021 ||| attentionddi: siamese attention-based deep learning method for drug-drug interaction predictions. ||| 35896 ||| 33294 ||| 35897 ||| 26645 ||| 
2022 ||| lerna: transformer architectures for configuring error correction tools for short- and long-read genome sequencing. ||| 33277 ||| 33278 ||| 33279 ||| 23545 ||| 33280 ||| 33281 ||| 
2021 ||| mathla: a robust framework for hla-peptide binding prediction integrating bidirectional lstm and multiple head attention mechanism. ||| 43336 ||| 8349 ||| 43337 ||| 9149 ||| 43338 ||| 14552 ||| 5902 ||| 43339 ||| 
2020 ||| biomedical document triage using a hierarchical attention-based capsule network. ||| 8349 ||| 43340 ||| 43341 ||| 8974 ||| 16590 ||| 43324 ||| 
2017 ||| an attention-based effective neural model for drug-drug interactions extraction. ||| 16828 ||| 8974 ||| 5015 ||| 16551 ||| 43342 ||| 16566 ||| 16590 ||| 8349 ||| 
2021 ||| jlan: medical code prediction via joint learning attention networks and denoising mechanism. ||| 43343 ||| 16566 ||| 43344 ||| 43345 ||| 16567 ||| 16568 ||| 
2020 ||| assistant diagnosis with chinese electronic medical records based on cnn and bilstm with phrase-level and word-level attentions. ||| 6669 ||| 24119 ||| 43346 ||| 24117 ||| 
2019 ||| attention-based recurrent neural network for influenza epidemic prediction. ||| 7647 ||| 16797 ||| 16798 ||| 16799 ||| 16800 ||| 16801 ||| 13805 ||| 16802 ||| 40581 ||| 43347 ||| 16803 ||| 
2019 ||| a hybrid self-attention deep learning framework for multivariate sleep stage classification. ||| 2296 ||| 6123 ||| 1071 ||| 6121 ||| 18559 ||| 18560 ||| 1630 ||| 
2019 ||| adverse drug reaction detection via a multihop self-attention mechanism. ||| 29314 ||| 8974 ||| 29315 ||| 8967 ||| 728 ||| 16590 ||| 8349 ||| 16566 ||| 
2019 ||| attention mechanism enhanced lstm with residual architecture and its application for protein-protein interaction residue pairs prediction. ||| 43348 ||| 43349 ||| 
2019 ||| relation extraction between bacteria and biotopes from biomedical texts with attention mechanisms and domain-specific contextual representations. ||| 43350 ||| 43351 ||| 15786 ||| 
2021 ||| deepgrn: prediction of transcription factor binding site across cell-types using attention-based deep neural networks. ||| 2230 ||| 23455 ||| 43352 ||| 1007 ||| 43353 ||| 33725 ||| 
2022 ||| lmms reloaded: transformer-based sense embeddings for disambiguation and beyond. ||| 9976 ||| 9977 ||| 32232 ||| 32233 ||| 852 ||| 24953 ||| 
2021 ||| enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning. ||| 3182 ||| 3652 ||| 26005 ||| 3653 ||| 3654 ||| 3655 ||| 3181 ||| 3656 ||| 2166 ||| 
2020 ||| protoattend: attention-based prototypical learning. ||| 36554 ||| 36555 ||| 36557 ||| 
2021 ||| attention is turing-complete. ||| 43354 ||| 2600 ||| 43355 ||| 43356 ||| 
2020 ||| exploring the limits of transfer learning with a unified text-to-text transformer. ||| 3338 ||| 9132 ||| 4822 ||| 37278 ||| 13146 ||| 26716 ||| 26720 ||| 3337 ||| 22749 ||| 
2018 ||| attention to news and its dissemination on twitter: a survey. ||| 43357 ||| 43358 ||| 
2021 ||| an attention network based on feature sequences for cross-domain sentiment classification. ||| 15305 ||| 11141 ||| 43359 ||| 15304 ||| 
2021 ||| attention mechanism based lstm in classification of stressed speech under workload. ||| 18446 ||| 43360 ||| 43361 ||| 43362 ||| 7957 ||| 8525 ||| 
2020 ||| knowledge-embodied attention for distantly supervised relation extraction. ||| 43363 ||| 43364 ||| 43365 ||| 43366 ||| 
2022 ||| temporal link prediction in directed networks based on self-attention mechanism. ||| 20820 ||| 20819 ||| 34689 ||| 20818 ||| 17546 ||| 
2019 ||| an attention-gated convolutional neural network for sentence classification. ||| 1305 ||| 33942 ||| 36154 ||| 36155 ||| 8154 ||| 27886 ||| 
2019 ||| synergistic attention u-net for sublingual vein segmentation. ||| 30853 ||| 30854 ||| 30855 ||| 30856 ||| 24118 ||| 
2018 ||| word polarity attention in sentiment analysis. ||| 43367 ||| 21113 ||| 
2021 ||| choice modeling using dot-product attention mechanism. ||| 43368 ||| 21784 ||| 20474 ||| 
2021 ||| a hybrid ai approach for supporting clinical diagnosis of attention deficit hyperactivity disorder (adhd) in adults. ||| 43369 ||| 43370 ||| 43371 ||| 43372 ||| 
2019 ||| neural attention with character embeddings for hay fever detection from twitter. ||| 43373 ||| 43374 ||| 43375 ||| 300 ||| 301 ||| 
2019 ||| attention for web directory advertisements: a top-down or bottom-up process? ||| 43376 ||| 43377 ||| 43378 ||| 24193 ||| 
2021 ||| mobile application user experience checklist: a tool to assess attention to core ux principles. ||| 43379 ||| 43380 ||| 43381 ||| 
2021 ||| effect of imperfect information and action automation on attentional allocation. ||| 43382 ||| 43383 ||| 3766 ||| 43384 ||| 33928 ||| 43385 ||| 7559 ||| 43386 ||| 43387 ||| 
2021 ||| attention: theory, principles, models and applications. ||| 43388 ||| 
2021 ||| comparing youth engagement on the attentiontrip to the child attention network test. ||| 43389 ||| 43390 ||| 43391 ||| 
2017 ||| theory-based models of attention in visual workspaces. ||| 43392 ||| 43393 ||| 43388 ||| 
2021 ||| when preschoolers use tablets: the effect of educational serious games on children's attention development. ||| 26165 ||| 43394 ||| 43395 ||| 21653 ||| 5743 ||| 
2018 ||| using an eye-tracking approach to explore gender differences in visual attention and shopping attitudes in an online shopping environment. ||| 43396 ||| 43397 ||| 
2019 ||| a comparison of engagement between the attention network test and a videogame-like version, called the attentiontrip. ||| 43391 ||| 43398 ||| 43399 ||| 
2019 ||| an sic-driven modular step-up converter with soft-switched module having 1: 1 turns ratio multiphase transformer for wind systems. ||| 43400 ||| 43401 ||| 
2020 ||| a high step up sepic-based converter based on partly interleaved transformer. ||| 43402 ||| 6078 ||| 21310 ||| 21313 ||| 
2018 ||| shielding technique for planar matrix transformers to suppress common-mode emi noise and improve efficiency. ||| 43403 ||| 43404 ||| 16895 ||| 43405 ||| 
2017 ||| a multilevel transformerless inverter employing ground connection between pv negative terminal and grid neutral point. ||| 43406 ||| 43407 ||| 
2018 ||| transformer-free, off-the-shelf electrical interface for low-voltage dc energy harvesting. ||| 17370 ||| 43408 ||| 43409 ||| 43410 ||| 43411 ||| 
2020 ||| scscn: a separated channel-spatial convolution net with attention for single-view reconstruction. ||| 6863 ||| 3386 ||| 12062 ||| 2146 ||| 
2017 ||| a single-phase transformerless inverter with charge pump circuit concept for grid-tied pv applications. ||| 43412 ||| 41450 ||| 43413 ||| 22216 ||| 15382 ||| 28295 ||| 
2021 ||| novel virtual-ground single-stage single-inductor transformerless buck-boost inverter. ||| 43414 ||| 43415 ||| 43416 ||| 
2018 ||| esi: a novel three-phase inverter with leakage current attenuation for transformerless pv systems. ||| 21702 ||| 25530 ||| 43417 ||| 
2021 ||| reversible wideband hybrid model of two-winding transformer including the core nonlinearity and emtp implementation. ||| 28549 ||| 43418 ||| 19774 ||| 28551 ||| 43419 ||| 43420 ||| 
2021 ||| multiscale convolutional attention network for predicting remaining useful life of machinery. ||| 890 ||| 43421 ||| 43422 ||| 9026 ||| 
2017 ||| high-efficiency high-power-density llc converter with an integrated planar matrix transformer for high-output current applications. ||| 43403 ||| 43405 ||| 16895 ||| 
2022 ||| air-core-transformer-based solid-state fault-current limiter for bidirectional hvdc systems. ||| 19774 ||| 30105 ||| 28549 ||| 26583 ||| 28551 ||| 17677 ||| 
2019 ||| adaptive power transformer lifetime predictions through machine learning and uncertainty modeling in nuclear power plants. ||| 29252 ||| 29254 ||| 29253 ||| 29255 ||| 29256 ||| 29257 ||| 
2021 ||| front-end bidirectional symmetric bipolar outputs llc dc-transformer (dcx) for a half bridge class-d audio amplifier. ||| 43423 ||| 43424 ||| 43425 ||| 1858 ||| 41485 ||| 
2020 ||| dc voltage control strategy of three-terminal medium-voltage power electronic transformer-based soft normally open points. ||| 43426 ||| 43427 ||| 6070 ||| 43428 ||| 43429 ||| 43430 ||| 
2020 ||| efficiency optimization of dc solid-state transformer for photovoltaic power systems. ||| 43431 ||| 43432 ||| 43433 ||| 25530 ||| 43434 ||| 
2018 ||| an improved dc solid state transformer based on switched capacitor and multiple-phase-shift shoot-through modulation for integration of lvdc energy storage system and mvdc distribution grid. ||| 14178 ||| 43435 ||| 43436 ||| 43437 ||| 
2022 ||| an advanced wideband model and a novel multitype insulation monitoring strategy for vsc-connected transformers based on common-mode impedance response. ||| 22008 ||| 22009 ||| 22010 ||| 
2018 ||| concurrent voltage control and dispatch of active distribution networks by means of smart transformer and storage. ||| 16201 ||| 25087 ||| 25088 ||| 25089 ||| 10868 ||| 
2021 ||| attention recurrent neural network-based severity estimation method for interturn short-circuit fault in permanent magnet synchronous machines. ||| 43438 ||| 43439 ||| 43440 ||| 43441 ||| 39410 ||| 
2020 ||| three-step switching frequency selection criteria for the generalized cllc-type dc transformer in hybrid ac-dc microgrid. ||| 43442 ||| 1340 ||| 2855 ||| 
2020 ||| improving current transformer-based energy extraction from ac power lines by manipulating magnetic field. ||| 23544 ||| 40367 ||| 43443 ||| 43444 ||| 43445 ||| 22602 ||| 43446 ||| 
2017 ||| current transformer saturation detection using morphological gradient and morphological decomposition and its hardware implementation. ||| 43447 ||| 43448 ||| 43449 ||| 43450 ||| 43451 ||| 
2021 ||| a novel interleaved high step-up converter with built-in transformer voltage multiplier cell. ||| 43452 ||| 43453 ||| 43454 ||| 
2017 ||| an improved zero-current-switching single-phase transformerless pv h6 inverter with switching loss-free. ||| 43455 ||| 254 ||| 43456 ||| 
2017 ||| analysis and design of an electronic on-load tap changer distribution transformer for automatic voltage regulation. ||| 43457 ||| 43458 ||| 43459 ||| 43460 ||| 43461 ||| 43462 ||| 43463 ||| 
2022 ||| an mvdc-based meshed hybrid microgrid enabled using smart transformers. ||| 22021 ||| 16200 ||| 10868 ||| 
2020 ||| protection of single-phase fault at the transformer valve side of fb-mmc-based bipolar hvdc systems. ||| 683 ||| 19339 ||| 29210 ||| 43464 ||| 43465 ||| 43466 ||| 
2017 ||| modulation technique for single-phase transformerless photovoltaic inverters with reactive power capability. ||| 43467 ||| 43468 ||| 43469 ||| 30008 ||| 43470 ||| 
2019 ||| modeling and hierarchical structure based model predictive control of cascaded flying capacitor bridge multilevel converter for active front-end rectifier in solid-state transformer. ||| 43471 ||| 43472 ||| 43473 ||| 
2017 ||| uk-derived transformerless common-grounded pv microinverter in ccm. ||| 43474 ||| 43475 ||| 
2017 ||| highly efficient and reliable sic-based dc-dc converter for smart transformer. ||| 43476 ||| 21934 ||| 10868 ||| 
2017 ||| multilevel mvdc link strategy of high-frequency-link dc transformer based on switched capacitor for mvdc power distribution. ||| 3906 ||| 14178 ||| 29206 ||| 43435 ||| 43436 ||| 43437 ||| 
2017 ||| current-feed single-switch forward resonant dc transformer (dcx) with secondary diode-clamping. ||| 26963 ||| 43477 ||| 21995 ||| 
2022 ||| partial-power post-regulated llc resonant dc transformer. ||| 43478 ||| 43479 ||| 43480 ||| 
2019 ||| comparative analysis of multiple active bridge converters configurations in modular smart transformer. ||| 43476 ||| 22069 ||| 21934 ||| 10868 ||| 
2020 ||| multitransformer primary-side regulated flyback converter for supplying isolated igbt and mosfet drivers. ||| 43481 ||| 43482 ||| 43483 ||| 
2022 ||| input current step-doubling for autotransformer-based 12-pulse rectifier using two auxiliary diodes. ||| 43484 ||| 43485 ||| 43486 ||| 43487 ||| 43488 ||| 
2020 ||| a multiphysics design and optimization method for air-core planar transformers in high-frequency llc resonant converters. ||| 43489 ||| 43490 ||| 43491 ||| 
2021 ||| solid-state transformer based fast charging station for various categories of electric vehicles with batteries of vastly different ratings. ||| 21948 ||| 10852 ||| 
2018 ||| analysis and stabilization of a smart transformer-fed grid. ||| 22166 ||| 21934 ||| 10868 ||| 
2022 ||| temporal attention convolutional neural network for estimation of icing probability on wind turbine blades. ||| 6560 ||| 41869 ||| 29902 ||| 29121 ||| 29124 ||| 13196 ||| 
2017 ||| time-varying and constant switching frequency-based sliding-mode control methods for transformerless dvr employing half-bridge vsi. ||| 10932 ||| 43492 ||| 
2018 ||| modular transformer-based regenerative-cascaded multicell converter for drives with multilevel voltage operation at both input and output sides. ||| 43493 ||| 43494 ||| 10852 ||| 
2020 ||| model predictive control for dual active bridge in naval dc microgrids supplying pulsed power loads featuring fast transition and online transformer current minimization. ||| 43495 ||| 25053 ||| 41907 ||| 43496 ||| 43497 ||| 
2020 ||| graph-theory-based modeling and control for system-level optimization of smart transformers. ||| 10868 ||| 43498 ||| 22077 ||| 
2018 ||| intermediate voltage variation-based interconnecting transformer design for voltage and phase angle control with coupled field fea studies. ||| 43499 ||| 43500 ||| 43501 ||| 
2020 ||| a new single-phase transformerless grid-connected inverter with boosting ability and common ground feature. ||| 43453 ||| 43413 ||| 41450 ||| 
2018 ||| improved fault-tolerant method and control strategy based on reverse charging for the power electronic traction transformer. ||| 43502 ||| 43503 ||| 
2019 ||| robust circuit parameters design for the cllc-type dc transformer in the hybrid ac-dc microgrid. ||| 43442 ||| 1340 ||| 43504 ||| 43505 ||| 5845 ||| 43506 ||| 43507 ||| 43508 ||| 
2017 ||| isolated ac-dc converter using medium frequency transformer for off-shore wind turbine dc collection grid. ||| 43509 ||| 43510 ||| 43511 ||| 43512 ||| 43513 ||| 43514 ||| 
2019 ||| neutral current minimization control for solid state transformers under unbalanced loads in distribution systems. ||| 21979 ||| 15490 ||| 22014 ||| 21983 ||| 
2021 ||| noniterative design of litz-wire high-frequency gapped-transformer (lw-hfgt) for llc converters based on optimal core-geometry factor model (okgm). ||| 43515 ||| 1052 ||| 
2019 ||| zero-voltage switching operation of transformer class-e inverter at any coupling coefficient. ||| 43516 ||| 43517 ||| 43518 ||| 43519 ||| 
2020 ||| cascaded transformer multilevel inverters with asymmetrical turns ratios based on npc. ||| 43520 ||| 43521 ||| 43522 ||| 43523 ||| 43524 ||| 43525 ||| 
2020 ||| condition monitoring of transformer breather using a capacitive moisture sensor. ||| 43526 ||| 43527 ||| 28435 ||| 43528 ||| 
2018 ||| a bidirectional bridge modular switched-capacitor-based power electronics transformer. ||| 43529 ||| 43530 ||| 43531 ||| 5935 ||| 
2019 ||| design and implementation of a new transformerless bidirectional dc-dc converter with wide conversion ratios. ||| 43532 ||| 43533 ||| 43534 ||| 
2018 ||| an llc-lc type bidirectional control strategy for an llc resonant converter in power electronic traction transformer. ||| 43535 ||| 43502 ||| 43536 ||| 43503 ||| 602 ||| 43537 ||| 
2017 ||| a single-phase buck matrix converter with high-frequency transformer isolation and reduced switch count. ||| 29137 ||| 43415 ||| 43538 ||| 
2022 ||| health management of dry-type transformer based on broad learning system. ||| 22951 ||| 33769 ||| 43539 ||| 22952 ||| 20242 ||| 31746 ||| 
2018 ||| leakage current attenuation of a three-phase cascaded inverter for transformerless grid-connected pv systems. ||| 21702 ||| 43540 ||| 2824 ||| 43541 ||| 43542 ||| 
2020 ||| a novel high-voltage dc transformer based on diode-clamped modular multilevel converters with voltage self-balancing capability. ||| 43543 ||| 43544 ||| 30352 ||| 43545 ||| 5189 ||| 43546 ||| 43547 ||| 
2020 ||| novel transformerless buck-boost inverters without leakage current. ||| 43538 ||| 43548 ||| 43549 ||| 3807 ||| 43550 ||| 43551 ||| 
2022 ||| cascaded transformers-based multilevel inverters with npc. ||| 43552 ||| 43520 ||| 43521 ||| 43522 ||| 43523 ||| 43525 ||| 
2021 ||| transformerless ups system based on the half-bridge hybrid switched-capacitor operating as ac-dc and dc-dc converter. ||| 43553 ||| 43554 ||| 43555 ||| 43556 ||| 43557 ||| 43558 ||| 24391 ||| 24392 ||| 43559 ||| 
2017 ||| active harmonic reduction using dc-side current injection applied in a novel large current rectifier based on fork-connected autotransformer. ||| 43560 ||| 43561 ||| 38086 ||| 43562 ||| 
2018 ||| a new soft-switching configuration and its application in transformerless photovoltaic grid-connected inverters. ||| 43563 ||| 254 ||| 369 ||| 5287 ||| 
2018 ||| combination analysis and switching method of a cascaded h-bridge multilevel inverter based on transformers with the different turns ratio for increasing the voltage level. ||| 43564 ||| 43565 ||| 43566 ||| 30008 ||| 
2018 ||| a novel dc current injection suppression method for three-phase grid-connected inverter without the isolation transformer. ||| 29395 ||| 25060 ||| 6514 ||| 43567 ||| 43568 ||| 43569 ||| 
2022 ||| a single-phase five-level transformer-less pv inverter for leakage current reduction. ||| 43570 ||| 43571 ||| 43572 ||| 43573 ||| 43574 ||| 43575 ||| 
2019 ||| an improved partially interleaved transformer structure for high-voltage high-frequency multiple-output applications. ||| 22078 ||| 22079 ||| 43576 ||| 22080 ||| 22082 ||| 
2021 ||| design and adaptive control of matrix transformer based indirect converter for large-capacity circuit breaker testing application. ||| 43577 ||| 43578 ||| 43579 ||| 43580 ||| 12137 ||| 
2021 ||| a new harmonic mitigation system with double balanced impedance filtering power transformer for multistage distribution network. ||| 11478 ||| 43581 ||| 43582 ||| 2969 ||| 43583 ||| 
2018 ||| measurement of the no-load characteristics of single-phase transformer using an improved low-frequency method. ||| 189 ||| 43584 ||| 43585 ||| 43586 ||| 5463 ||| 
2019 ||| single-phase transformer-based hf-isolated impedance source inverters with voltage clamping techniques. ||| 22150 ||| 29136 ||| 22152 ||| 43587 ||| 43588 ||| 
2020 ||| modular hybrid-full-bridge dc transformer with full-process matching switching strategy for mvdc power distribution application. ||| 43435 ||| 41810 ||| 43589 ||| 43590 ||| 43591 ||| 43592 ||| 39766 ||| 
2021 ||| component-level thermo-electromagnetic nonlinear transient finite element modeling of solid-state transformer for dc grid studies. ||| 43593 ||| 10572 ||| 43594 ||| 
2020 ||| a transformer integrated filtering system for power quality improvement of industrial dc supply system. ||| 43582 ||| 2969 ||| 43595 ||| 43581 ||| 
2021 ||| machine remaining useful life prediction via an attention-based deep learning approach. ||| 17993 ||| 16597 ||| 8164 ||| 43596 ||| 29157 ||| 3488 ||| 
2020 ||| macroscopic-microscopic attention in lstm networks based on fusion features for gear remaining life prediction. ||| 28392 ||| 18175 ||| 43597 ||| 43598 ||| 
2021 ||| deep learning with spatiotemporal attention-based lstm for industrial soft sensor model development. ||| 43599 ||| 1556 ||| 43600 ||| 43601 ||| 43602 ||| 
2020 ||| impedance modeling and dc bus voltage stability assessment of a solid-state-transformer-enabled hybrid ac-dc grid considering bidirectional power flow. ||| 43603 ||| 43604 ||| 4175 ||| 
2021 ||| a common grounded type dual-mode five-level transformerless inverter for photovoltaic applications. ||| 43605 ||| 30006 ||| 43606 ||| 144 ||| 43607 ||| 43608 ||| 43609 ||| 43610 ||| 22216 ||| 43611 ||| 
2022 ||| data-driven estimation of driver attention using calibration-free eye gaze and scene features. ||| 43612 ||| 37375 ||| 43613 ||| 1124 ||| 37682 ||| 
2017 ||| integration of large photovoltaic and wind system by means of smart transformer. ||| 10867 ||| 22006 ||| 43614 ||| 10868 ||| 
2019 ||| visual-attention-based pixel dimming technique for oled displays of mobile devices. ||| 43615 ||| 43616 ||| 19453 ||| 
2020 ||| experimental evaluation of transformer internal fault detection based on v-i characteristics. ||| 28560 ||| 43584 ||| 43617 ||| 43618 ||| 43619 ||| 39573 ||| 28557 ||| 
2020 ||| a new testing method for the dielectric response of oil-immersed transformer. ||| 28327 ||| 28326 ||| 7261 ||| 43620 ||| 37978 ||| 43621 ||| 15544 ||| 
2022 ||| transformer-less boost converter with reduced voltage stress for high voltage step-up applications. ||| 43622 ||| 10861 ||| 43623 ||| 22152 ||| 43624 ||| 
2018 ||| lifetime-based power routing in parallel converters for smart transformer application. ||| 22077 ||| 43498 ||| 21934 ||| 10868 ||| 
2021 ||| a novel suppression method for grounding transformer against earth current from urban rail transit. ||| 43625 ||| 21289 ||| 43626 ||| 43627 ||| 2182 ||| 9087 ||| 43628 ||| 43629 ||| 
2018 ||| three-phase lines to single-phase coil planar contactless power transformer. ||| 43630 ||| 43631 ||| 43632 ||| 43633 ||| 43634 ||| 
2021 ||| a novel leakage-current-based online insulation monitoring strategy for converter transformers using common-mode and differential-mode harmonics in vsc system. ||| 22008 ||| 22010 ||| 
2017 ||| a new fxlms algorithm with offline and online secondary-path modeling scheme for active noise control of power transformers. ||| 23395 ||| 43635 ||| 2982 ||| 254 ||| 
2021 ||| a transformerless dc-dc modular multilevel converter for hybrid interconnections in hvdc. ||| 43636 ||| 22197 ||| 
2019 ||| high step-up transformerless inverter for ac module applications with active power decoupling. ||| 43637 ||| 43638 ||| 43639 ||| 
2017 ||| series voltage regulator for a distribution transformer to compensate voltage sag/swell. ||| 43640 ||| 43641 ||| 43642 ||| 43643 ||| 
2022 ||| fast grnn-based method for distinguishing inrush currents in power transformers. ||| 29132 ||| 29133 ||| 29134 ||| 29135 ||| 43644 ||| 43497 ||| 
2017 ||| optimal design and implementation of high-voltage high-power silicon steel core medium-frequency transformer. ||| 43645 ||| 21289 ||| 2182 ||| 43646 ||| 43647 ||| 43648 ||| 29071 ||| 43649 ||| 
2017 ||| a zero-voltage-transition heric-type transformerless photovoltaic grid-connected inverter. ||| 43455 ||| 254 ||| 43456 ||| 
2021 ||| a modular multilevel converter (mmc) based solid-state transformer (sst) topology with simplified energy conversion process and magnetic integration. ||| 43650 ||| 16833 ||| 43651 ||| 
2018 ||| a grid-connected single-phase transformerless inverter controlling two solar pv arrays operating under different atmospheric conditions. ||| 43652 ||| 43653 ||| 43654 ||| 
2018 ||| proposal of a photovoltaic ac-module with a single-stage transformerless grid-connected boost microinverter. ||| 43655 ||| 43656 ||| 43657 ||| 43658 ||| 43659 ||| 43660 ||| 43661 ||| 43662 ||| 
2021 ||| six-switch step-up common-grounded five-level inverter with switched-capacitor cell for transformerless grid-tied pv applications. ||| 43609 ||| 30006 ||| 43453 ||| 22216 ||| 
2018 ||| sizing and soc management of a smart-transformer-based energy storage system. ||| 16200 ||| 10867 ||| 21934 ||| 10868 ||| 
2021 ||| an ultrawide output range ${llc}$ resonant converter based on adjustable turns ratio transformer and reconfigurable bridge. ||| 43663 ||| 3091 ||| 
2021 ||| a doubly grounded transformerless pv grid-connected inverter without shoot-through problem. ||| 10919 ||| 26816 ||| 
2022 ||| extended-state-observer based model predictive control of a hybrid modular dc transformer. ||| 17750 ||| 22193 ||| 22190 ||| 22192 ||| 22191 ||| 12938 ||| 2885 ||| 
2017 ||| a novel structure for single-switch nonisolated transformerless buck-boost dc-dc converter. ||| 41480 ||| 41481 ||| 
2018 ||| dual-transformer-based dab converter with wide zvs range for wide voltage conversion gain application. ||| 43664 ||| 43665 ||| 43666 ||| 43545 ||| 
2017 ||| active balancing of li-ion battery cells using transformer as energy carrier. ||| 43667 ||| 7936 ||| 43668 ||| 43669 ||| 
2019 ||| nsga-ii+fem based loss optimization of three-phase transformer. ||| 43670 ||| 43671 ||| 
2021 ||| structured protection measures for better use of nanocrystalline cores in air-cooled medium-frequency transformer for induction heating. ||| 43672 ||| 
2019 ||| simultaneous common-mode resonance circulating current and leakage current suppression for transformerless three-level t-type pv inverter system. ||| 5378 ||| 43673 ||| 43674 ||| 43675 ||| 43676 ||| 43677 ||| 
2020 ||| tunable high-power multilayer piezoelectric transformer. ||| 5710 ||| 43678 ||| 43679 ||| 43680 ||| 43681 ||| 
2018 ||| analytical modeling and implementation of a coaxially wound transformer with integrated filter inductance for isolated soft-switching dc-dc converters. ||| 43682 ||| 22067 ||| 
2019 ||| a transformerless hybrid modular multilevel dc-dc converter with dc fault ride-through capability. ||| 6490 ||| 3337 ||| 2101 ||| 43683 ||| 7111 ||| 
2018 ||| detection of power transformer winding deformation using improved fra based on binary morphology and extreme point variation. ||| 43684 ||| 43584 ||| 43618 ||| 43685 ||| 
2017 ||| 48-v voltage regulator module with pcb winding matrix transformer for future data centers. ||| 43686 ||| 43403 ||| 43405 ||| 16895 ||| 
2017 ||| performance and design analysis on round-shaped transformers applied in rectifier systems. ||| 43687 ||| 11318 ||| 43182 ||| 43688 ||| 43689 ||| 
2018 ||| transformerless line-interactive ups with low ground leakage current. ||| 41886 ||| 41887 ||| 
2017 ||| design and implementation of an amorphous high-frequency transformer coupling multiple converters in a smart microgrid. ||| 43690 ||| 43691 ||| 43692 ||| 40194 ||| 43693 ||| 21171 ||| 
2020 ||| quarter-turn transformer design and optimization for high power density 1-mhz llc resonant converter. ||| 43694 ||| 43695 ||| 2230 ||| 43696 ||| 43697 ||| 43698 ||| 43699 ||| 
2019 ||| a push-pull modular-multilevel-converter-based low step-up ratio dc transformer. ||| 43700 ||| 43701 ||| 43702 ||| 5157 ||| 6832 ||| 
2020 ||| voltage-adjustable capacitor isolated solid-state transformer. ||| 43546 ||| 43544 ||| 30352 ||| 43703 ||| 43543 ||| 
2020 ||| a novel scalar pwm method to reduce leakage current in three-phase two-level transformerless grid-connected vsis. ||| 43704 ||| 43705 ||| 16446 ||| 43706 ||| 43707 ||| 
2017 ||| on the effect of disorder on stray capacitance of transformer winding in high-voltage power supplies. ||| 43708 ||| 43709 ||| 
2022 ||| design of symmetrical cllc-resonant dc transformer considering voltage transfer ratio and cascaded system stability. ||| 43710 ||| 1340 ||| 42482 ||| 25062 ||| 43562 ||| 
2018 ||| input filter for a power electronics transformer in a railway traction application. ||| 43711 ||| 3882 ||| 43712 ||| 43713 ||| 
2017 ||| corrections to "multilevel mvdc link strategy of high-frequency-link dc transformer based on switched capacitor for mvdc power distribution". ||| 3906 ||| 14178 ||| 29206 ||| 43435 ||| 43436 ||| 43437 ||| 
2018 ||| a modified phase disposition pulse width modulation to suppress the leakage current for the transformerless cascaded h-bridge inverters. ||| 6033 ||| 5189 ||| 43714 ||| 43715 ||| 
2020 ||| double-ended active-clamp forward converter with low dc offset current of transformer. ||| 43716 ||| 43717 ||| 43718 ||| 43719 ||| 43720 ||| 
2018 ||| online condition monitoring of onboard traction transformer core based on core-loss calculation model. ||| 43721 ||| 43722 ||| 43723 ||| 43724 ||| 43725 ||| 
2019 ||| current-fed isolated lcc-t resonant converter with zcs and improved transformer utilization. ||| 43726 ||| 43727 ||| 
2017 ||| a modular multilevel dc-link front-to-front dc solid-state transformer based on high-frequency dual active phase shift for hvdc grid integration. ||| 43435 ||| 14178 ||| 43436 ||| 43437 ||| 
2019 ||| flicker-free single-switch quadratic boost led driver compatible with electronic transformers. ||| 43728 ||| 43252 ||| 43729 ||| 41488 ||| 
2021 ||| the dual-mode combined control strategy for centralized photovoltaic grid-connected inverters based on double-split transformers. ||| 765 ||| 9535 ||| 43730 ||| 43731 ||| 9028 ||| 
2022 ||| improved zvs range for three-phase dual-active-bridge converter with wye-extended-delta transformer. ||| 43732 ||| 43733 ||| 3369 ||| 43734 ||| 43735 ||| 
2021 ||| leakage current suppression of single-phase five-level inverter for transformerless photovoltaic system. ||| 43570 ||| 43571 ||| 43736 ||| 43573 ||| 43572 ||| 43574 ||| 43575 ||| 
2018 ||| a balance transformer-integrated rpfc for railway power system pq improvement with low-design capacity. ||| 43595 ||| 4417 ||| 2969 ||| 43737 ||| 29647 ||| 43738 ||| 43739 ||| 43740 ||| 43581 ||| 
2018 ||| maintaining continuous zvs operation of a dual active bridge by reduced coupling transformers. ||| 43741 ||| 43742 ||| 43743 ||| 43744 ||| 
2021 ||| exploration of a novel hts thin-film device combined with roles of transformer and overcurrent limiter. ||| 42236 ||| 3906 ||| 43745 ||| 43746 ||| 
2017 ||| a resonant zvzcs dc-dc converter with two uneven transformers for an mvdc collection system of offshore wind farms. ||| 22004 ||| 22003 ||| 43747 ||| 43748 ||| 43749 ||| 27852 ||| 30258 ||| 7481 ||| 
2022 ||| active power backflow control strategy for cascaded photovoltaic solid-state transformer during low-voltage ride through. ||| 43750 ||| 22223 ||| 
2018 ||| common-ground-type transformerless inverters for single-phase solar photovoltaic systems. ||| 30006 ||| 22216 ||| 
2019 ||| a new design and analysis of round-shaped transformers supplying diode and controllable rectifiers. ||| 43751 ||| 
2021 ||| smart transformer-enabled meshed hybrid distribution grid. ||| 43752 ||| 21978 ||| 16200 ||| 10868 ||| 
2021 ||| an estimation method for real-time thermal capacity of traction transformers under unbalanced loads. ||| 28326 ||| 37978 ||| 586 ||| 1073 ||| 15544 ||| 28327 ||| 
2020 ||| magnetic-integrated multipulse rectifier transformer with a tight impedance equalizing strategy for power quality improvement of dc traction power supply system. ||| 43738 ||| 2969 ||| 43753 ||| 29647 ||| 43739 ||| 43754 ||| 43581 ||| 
2021 ||| quasi-clamped zsi with two transformers. ||| 22150 ||| 43755 ||| 29137 ||| 29138 ||| 
2020 ||| a new transformerless inverter with leakage current limiting and voltage boosting capabilities for grid-connected pv applications. ||| 43756 ||| 
2018 ||| smart transformer-fed variable frequency distribution grid. ||| 22166 ||| 22006 ||| 21934 ||| 10868 ||| 
2022 ||| speed adaptability assessment of railway balise transmission module using a deep-adaptive-attention-based encoder-decoder network. ||| 43757 ||| 43758 ||| 43759 ||| 43760 ||| 
2019 ||| dc decoupling-based three-phase three-level transformerless pv inverter topology for minimization of leakage current. ||| 43761 ||| 43762 ||| 
2022 ||| divergence distance based index for discriminating inrush and internal fault currents in power transformers. ||| 43763 ||| 43644 ||| 
2020 ||| multifunction transformer with adjustable magnetic stage based on nanocomposite semihard magnets. ||| 29177 ||| 2008 ||| 43764 ||| 
2020 ||| a novel high step-up high efficiency interleaved dc-dc converter with coupled inductor and built-in transformer for renewable energy systems. ||| 43452 ||| 43765 ||| 43453 ||| 
2017 ||| integrating two stages as a common-mode transformerless photovoltaic converter. ||| 10882 ||| 2698 ||| 43766 ||| 2698 ||| 43767 ||| 43768 ||| 10314 ||| 43769 ||| 43770 ||| 2698 ||| 1881 ||| 43771 ||| 
2021 ||| multitime scale frequency regulation of a general resonant dc transformer in a hybrid ac/dc microgrid. ||| 43442 ||| 1340 ||| 43772 ||| 
2022 ||| hybrid high voltage gain transformerless dc-dc converter. ||| 2871 ||| 43773 ||| 43774 ||| 43775 ||| 43776 ||| 
2021 ||| transformer current ringing in dual active bridge converters. ||| 43777 ||| 43778 ||| 22216 ||| 43779 ||| 
2021 ||| analysis and implementation of a single-stage transformer-less converter with high step-down voltage gain for voltage regulator modules. ||| 43780 ||| 43781 ||| 43782 ||| 16597 ||| 
2020 ||| an efficient convolutional neural network model based on object-level attention mechanism for casting defect detection on radiography images. ||| 24498 ||| 29050 ||| 
2020 ||| a coupled inductor-based buck-boost type grid connected transformerless pv inverter having the ability to control two subarrays simultaneously. ||| 43652 ||| 43654 ||| 
2019 ||| hybrid dc-ac zonal microgrid enabled by solid-state transformer and centralized esd integration. ||| 43783 ||| 43784 ||| 43785 ||| 
2022 ||| a novel double-stacked autoencoder for power transformers dga signals with an imbalanced data structure. ||| 43786 ||| 41925 ||| 43787 ||| 40181 ||| 
2017 ||| dual-transformer-based asymmetrical triple-port active bridge (dt-atab) isolated dc-dc converter. ||| 43788 ||| 43407 ||| 43789 ||| 
2019 ||| an interleaved high step-up converter with coupled inductor and built-in transformer voltage multiplier cell techniques. ||| 43452 ||| 43453 ||| 43413 ||| 15382 ||| 41450 ||| 
2022 ||| four-port, single-stage, multidirectional ac-ac converter for solid-state transformer applications. ||| 43790 ||| 43791 ||| 41466 ||| 43792 ||| 43793 ||| 3419 ||| 43794 ||| 
2020 ||| emoji-based sentiment analysis using attention networks. ||| 43795 ||| 3289 ||| 9028 ||| 43796 ||| 3725 ||| 
2022 ||| denigrate comment detection in low-resource hindi language using attention-based residual networks. ||| 43797 ||| 43798 ||| 
2019 ||| chinese-catalan: a neural machine translation approach based on pivoting and attention mechanisms. ||| 3466 ||| 3465 ||| 11843 ||| 852 ||| 3467 ||| 
2021 ||| exploration of effective attention strategies for neural automatic post-editing with transformer. ||| 21392 ||| 21391 ||| 36420 ||| 43799 ||| 43800 ||| 4941 ||| 
2021 ||| sentiment analysis using xlm-r transformer and zero-shot transfer learning on resource-poor indian language. ||| 958 ||| 29277 ||| 
2021 ||| geogat: graph model based on attention mechanism for geographic text classification. ||| 30337 ||| 37980 ||| 30339 ||| 37981 ||| 
2020 ||| joint model of entity recognition and relation extraction with self-attention mechanism. ||| 9105 ||| 43801 ||| 1178 ||| 3725 ||| 
2020 ||| chinese short text classification with mutual-attention convolutional neural networks. ||| 43802 ||| 728 ||| 43803 ||| 43804 ||| 5051 ||| 
2021 ||| a multi-classification sentiment analysis model of chinese short text based on gated linear units and attention mechanism. ||| 10330 ||| 2424 ||| 43805 ||| 
2021 ||| hybridization between neural computing and nature-inspired algorithms for a sentence similarity model based on the attention mechanism. ||| 31560 ||| 43806 ||| 43807 ||| 35728 ||| 
2021 ||| two-channel attention mechanism fusion model of stock price prediction based on cnn-lstm. ||| 917 ||| 43808 ||| 43809 ||| 
2021 ||| bi-directional long short-term memory model with semantic positional attention for the question answering system. ||| 43162 ||| 5831 ||| 29885 ||| 43163 ||| 43164 ||| 
2020 ||| attention mechanism for uyghur personal pronouns resolution. ||| 29749 ||| 29080 ||| 29079 ||| 43810 ||| 
2022 ||| a transformer-based approach to multilingual fake news detection in low-resource languages. ||| 43811 ||| 43812 ||| 43813 ||| 165 ||| 
2019 ||| pos tag-enhanced coarse-to-fine attention for neural machine translation. ||| 26532 ||| 3182 ||| 43814 ||| 3043 ||| 1305 ||| 18155 ||| 
2021 ||| emotion cause detection with enhanced-representation attention convolutional-context network. ||| 8973 ||| 8974 ||| 8967 ||| 8975 ||| 29909 ||| 8976 ||| 8978 ||| 
2021 ||| cross-modality co-attention networks for visual question answering. ||| 40953 ||| 43815 ||| 43816 ||| 43817 ||| 
2021 ||| abml: attention-based multi-task learning for jointly humor recognition and pun detection. ||| 31452 ||| 728 ||| 8974 ||| 8967 ||| 
2019 ||| a soft-computing-based approach to artificial visual attention using human eye-fixation paradigm: toward a human-like skill in robot vision. ||| 31533 ||| 31534 ||| 31535 ||| 31537 ||| 
2021 ||| pyramidal convolution attention generative adversarial network with data augmentation for image denoising. ||| 41578 ||| 43818 ||| 43819 ||| 43820 ||| 8207 ||| 
2022 ||| spatial-temporal attention fusion for traffic speed prediction. ||| 43821 ||| 43822 ||| 1349 ||| 
2020 ||| public information, heterogeneous attention and market instability. ||| 43823 ||| 43824 ||| 22485 ||| 43825 ||| 
2018 ||| recognizing the human attention state using cardiac pulse from the noncontact and automatic-based measurements. ||| 43826 ||| 19828 ||| 43827 ||| 35728 ||| 3337 ||| 43828 ||| 
2021 ||| multi-attention embedded network for salient object detection. ||| 16694 ||| 43829 ||| 43830 ||| 3402 ||| 
2017 ||| chaos-assisted multiobjective evolutionary algorithm to the design of transformer. ||| 43831 ||| 43832 ||| 43833 ||| 43834 ||| 43835 ||| 
2021 ||| cascading residual-residual attention generative adversarial network for image super resolution. ||| 43836 ||| 43837 ||| 3768 ||| 43838 ||| 
2022 ||| image super-resolution reconstruction based on generative adversarial network model with feedback and attention mechanisms. ||| 11973 ||| 9889 ||| 43839 ||| 968 ||| 19843 ||| 16690 ||| 30102 ||| 
2020 ||| a model of co-saliency based audio attention. ||| 28840 ||| 12319 ||| 30765 ||| 
2017 ||| prediction of visual attention with deep cnn on artificially degraded videos for studies of attention of patients with dementia. ||| 16 ||| 2701 ||| 43840 ||| 43841 ||| 2710 ||| 
2018 ||| hybrid convolutional neural networks and optical flow for video visual attention prediction. ||| 17589 ||| 17974 ||| 1751 ||| 369 ||| 
2020 ||| 3d model retrieval based on multi-view attentional convolutional neural network. ||| 19713 ||| 43842 ||| 43843 ||| 19742 ||| 
2019 ||| tracking students' visual attention on manga-based interactive e-book while reading: an eye-movement approach. ||| 17329 ||| 17330 ||| 43844 ||| 43845 ||| 
2022 ||| text multi-label learning method based on label-aware attention and semantic dependency. ||| 43846 ||| 43847 ||| 28965 ||| 43848 ||| 43849 ||| 
2021 ||| medical image segmentation algorithm based on multilayer boundary perception-self attention deep learning model. ||| 43850 ||| 30697 ||| 
2021 ||| video multimodal emotion recognition based on bi-gru and attention fusion. ||| 43851 ||| 43852 ||| 43853 ||| 43854 ||| 26814 ||| 43855 ||| 
2022 ||| distracted driving detection based on the improved centernet with attention mechanism. ||| 43856 ||| 43857 ||| 43858 ||| 43859 ||| 43860 ||| 
2020 ||| gatecap: gated spatial and semantic attention model for image captioning. ||| 30736 ||| 588 ||| 586 ||| 589 ||| 
2022 ||| fine-grained histopathological cell segmentation through residual attention with prior embedding. ||| 43861 ||| 43862 ||| 43863 ||| 43864 ||| 
2022 ||| multi-label image recognition with attentive transformer-localizer module. ||| 43865 ||| 2313 ||| 2312 ||| 13422 ||| 2315 ||| 
2021 ||| attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional lstm. ||| 31712 ||| 31713 ||| 31714 ||| 
2018 ||| cross-modal recipe retrieval with stacked attention model. ||| 14648 ||| 43866 ||| 19498 ||| 
2020 ||| fine-grained facial image-to-image translation with an attention based pipeline generative adversarial framework. ||| 10646 ||| 43867 ||| 1589 ||| 2626 ||| 43868 ||| 43869 ||| 2625 ||| 8292 ||| 2627 ||| 
2021 ||| using recurrent neural network structure with enhanced multi-head self-attention for sentiment analysis. ||| 43870 ||| 43871 ||| 5463 ||| 
2022 ||| attentional networks for music generation. ||| 38724 ||| 38725 ||| 17364 ||| 38726 ||| 38727 ||| 38728 ||| 
2022 ||| self-attention mechanism in person re-identification models. ||| 6661 ||| 17342 ||| 43872 ||| 40032 ||| 43873 ||| 43874 ||| 
2021 ||| underwater target detection with an attention mechanism and improved scale. ||| 43875 ||| 29080 ||| 29079 ||| 43876 ||| 40031 ||| 
2018 ||| saturation-aware human attention region of interest algorithm for efficient video compression. ||| 43877 ||| 4449 ||| 
2020 ||| remote sensing image caption generation via transformer and reinforcement learning. ||| 43878 ||| 3072 ||| 6496 ||| 6497 ||| 
2018 ||| weakly supervised detection with decoupled attention-based deep representation. ||| 43879 ||| 19112 ||| 19113 ||| 
2020 ||| image attention retargeting using defocus map and bilateral filter. ||| 43880 ||| 43881 ||| 43882 ||| 43883 ||| 
2021 ||| cascaded atrous dual attention u-net for tumor segmentation. ||| 11296 ||| 23882 ||| 43884 ||| 43885 ||| 11298 ||| 11299 ||| 
2021 ||| visual attention model based dual watermarking for simultaneous image copyright protection and authentication. ||| 43886 ||| 
2019 ||| video attention prediction using gaze saliency. ||| 12787 ||| 43887 ||| 31348 ||| 43888 ||| 
2021 ||| scale channel attention network for image segmentation. ||| 43889 ||| 43890 ||| 11270 ||| 43891 ||| 689 ||| 
2020 ||| a novel attention-guided jnd model for improving robust image watermarking. ||| 1224 ||| 43892 ||| 
2022 ||| split-attention effects in multimedia learning environments: eye-tracking and eeg analysis. ||| 43893 ||| 43894 ||| 43895 ||| 43896 ||| 13310 ||| 43897 ||| 
2018 ||| video-based learners' observed attention estimates for lecture learning gain evaluation. ||| 43898 ||| 3017 ||| 41741 ||| 
2021 ||| asts: attention based spatio-temporal sequential framework for movie trailer genre classification. ||| 43899 ||| 43900 ||| 438 ||| 43901 ||| 
2021 ||| tell and guess: cooperative learning for natural image caption generation with hierarchical refined attention. ||| 42697 ||| 17723 ||| 43902 ||| 7652 ||| 7654 ||| 
2020 ||| image captions: global-local and joint signals attention model (gl-jsam). ||| 43903 ||| 4438 ||| 
2021 ||| merta: micro-expression recognition with ternary attentions. ||| 40523 ||| 43904 ||| 43905 ||| 1099 ||| 215 ||| 
2017 ||| anomaly detection based on spatio-temporal sparse representation and visual attention analysis. ||| 5187 ||| 37800 ||| 2504 ||| 
2019 ||| an attention mechanism based convolutional lstm network for video action recognition. ||| 19851 ||| 43906 ||| 43907 ||| 19819 ||| 
2020 ||| spatio-temporal sru with global context-aware attention for 3d human action recognition. ||| 43908 ||| 43909 ||| 43910 ||| 43911 ||| 
2021 ||| attention-based dual context aggregation for image semantic segmentation. ||| 43912 ||| 43913 ||| 43914 ||| 11241 ||| 
2022 ||| two-stream adaptive-attentional subgraph convolution networks for skeleton-based action recognition. ||| 43915 ||| 43916 ||| 43917 ||| 43918 ||| 43919 ||| 43920 ||| 
2019 ||| combining sun-based visual attention model and saliency contour detection algorithm for apple image segmentation. ||| 43921 ||| 43922 ||| 43923 ||| 748 ||| 43924 ||| 
2022 ||| lightweight adaptive enhanced attention network for image super-resolution. ||| 1052 ||| 43001 ||| 43925 ||| 20051 ||| 43926 ||| 
2021 ||| sketchformer: transformer-based approach for sketch recognition using vector images. ||| 10210 ||| 10207 ||| 10208 ||| 10209 ||| 
2020 ||| regional bit allocation with visual attention and distortion sensitivity. ||| 51 ||| 52 ||| 
2020 ||| combining an information-maximization-based attention mechanism and illumination invariance theory for the recognition of green apples in natural scenes. ||| 43927 ||| 43928 ||| 43929 ||| 43922 ||| 43930 ||| 43923 ||| 43931 ||| 
2019 ||| attention-based multi-modal fusion for improved real estate appraisal: a case study in los angeles. ||| 43932 ||| 43933 ||| 921 ||| 43934 ||| 
2020 ||| bi-directional attention comparison for semantic sentence matching. ||| 43935 ||| 43936 ||| 43937 ||| 43938 ||| 43939 ||| 43940 ||| 
2020 ||| two stages double attention convolutional neural network for crowd counting. ||| 43941 ||| 42968 ||| 32014 ||| 43942 ||| 
2020 ||| human motion prediction based on attention mechanism. ||| 43943 ||| 43944 ||| 43945 ||| 
2020 ||| local and global aligned spatiotemporal attention network for video-based person re-identification. ||| 40089 ||| 6633 ||| 43946 ||| 43947 ||| 6632 ||| 6631 ||| 
2018 ||| looking deeper and transferring attention for image captioning. ||| 11318 ||| 7656 ||| 35022 ||| 11319 ||| 
2020 ||| object-aware semantics of attention for image captioning. ||| 30736 ||| 588 ||| 586 ||| 43948 ||| 589 ||| 
2020 ||| deep attentional fine-grained similarity network with adversarial learning for cross-modal retrieval. ||| 4260 ||| 737 ||| 
2020 ||| an attentional spatial temporal graph convolutional network with co-occurrence feature learning for action recognition. ||| 18956 ||| 43949 ||| 3889 ||| 43950 ||| 
2019 ||| self-attention recurrent network for saliency detection. ||| 37535 ||| 6281 ||| 6279 ||| 
2020 ||| association between slides-format and major's contents: effects on perceived attention and significant learning. ||| 43951 ||| 43952 ||| 43953 ||| 43954 ||| 
2021 ||| tardb-net: triple-attention guided residual dense and bilstm networks for hyperspectral image classification. ||| 29239 ||| 43955 ||| 42509 ||| 43956 ||| 43957 ||| 
2019 ||| word-to-region attention network for visual question answering. ||| 275 ||| 11466 ||| 19590 ||| 13410 ||| 1038 ||| 18992 ||| 9579 ||| 
2021 ||| weakly supervised fine-grained recognition based on spatial-channel aware attention filters. ||| 13749 ||| 5278 ||| 31417 ||| 13750 ||| 379 ||| 
2020 ||| unsupervised densely attention network for infrared and visible image fusion. ||| 438 ||| 43958 ||| 30857 ||| 28648 ||| 
2020 ||| pain-attentive network: a deep spatio-temporal attention model for pain estimation. ||| 7393 ||| 43959 ||| 43960 ||| 43961 ||| 
2022 ||| mask attention-guided graph convolution layer for weakly supervised temporal action detection. ||| 43962 ||| 43963 ||| 43964 ||| 43965 ||| 43966 ||| 
2020 ||| a lane detection network based on ibn and attention. ||| 6281 ||| 43967 ||| 43968 ||| 37535 ||| 7830 ||| 
2022 ||| lightweight multi-scale aggregated residual attention networks for image super-resolution. ||| 43969 ||| 5790 ||| 40148 ||| 
2022 ||| assamese news image caption generation using attention mechanism. ||| 43970 ||| 30963 ||| 
2021 ||| an ensemble multi-scale residual attention network (emra-net) for image dehazing. ||| 43958 ||| 42968 ||| 43942 ||| 
2021 ||| enhanced ssd with interactive multi-scale attention features for object detection. ||| 43971 ||| 43972 ||| 
2021 ||| attacks on state-of-the-art face recognition using attentional adversarial attack generative network. ||| 12169 ||| 31718 ||| 31719 ||| 
2020 ||| improving person re-identification via attribute-identity representation and visual attention mechanism. ||| 43973 ||| 38122 ||| 17572 ||| 43974 ||| 
2021 ||| automated detection of retinopathy of prematurity by deep attention network. ||| 6582 ||| 27717 ||| 15534 ||| 15667 ||| 15668 ||| 15669 ||| 5476 ||| 31207 ||| 15670 ||| 
2022 ||| bit-wise attention deep complementary supervised hashing for image retrieval. ||| 32001 ||| 29240 ||| 43975 ||| 1341 ||| 
2021 ||| semantic segmentation of brain tumor with nested residual attention networks. ||| 43976 ||| 17433 ||| 6174 ||| 
2020 ||| pedestrian object detection with fusion of visual attention mechanism and semantic computation. ||| 3996 ||| 43977 ||| 43978 ||| 
2022 ||| continuous digital zoom with cross attention for dual camera system. ||| 27118 ||| 6721 ||| 43979 ||| 43980 ||| 43981 ||| 
2021 ||| self-attention-based neural networks for refining the overlength product titles. ||| 32065 ||| 32066 ||| 32067 ||| 9920 ||| 11154 ||| 
2021 ||| mri enhancement based on visual-attention by adaptive contrast adjustment and image fusion. ||| 8473 ||| 32490 ||| 24526 ||| 6278 ||| 
2021 ||| multiple attention networks for stereo matching. ||| 43982 ||| 43983 ||| 43984 ||| 
2020 ||| person re-identification based on multi-level feature complementarity of cross-attention with part metric learning. ||| 43985 ||| 6984 ||| 17857 ||| 5479 ||| 
2020 ||| multi-level feature learning with attention for person re-identification. ||| 43986 ||| 43987 ||| 2424 ||| 43988 ||| 3311 ||| 
2021 ||| adversarial erasing attention for fine-grained image classification. ||| 6764 ||| 6765 ||| 6514 ||| 6766 ||| 6767 ||| 
2020 ||| low-sample size remote sensing image recognition based on a multihead attention integration network. ||| 43989 ||| 43990 ||| 43991 ||| 
2021 ||| correction to: attention-based multimodal contextual fusion for sentiment and emotion classification using bidirectional lstm. ||| 31712 ||| 31713 ||| 31714 ||| 
2021 ||| mask-guided dual attention-aware network for visible-infrared person re-identification. ||| 35028 ||| 43992 ||| 43993 ||| 860 ||| 35027 ||| 40149 ||| 
2021 ||| action recognition in still images using a multi-attention guided network with weakly supervised saliency detection. ||| 43994 ||| 43995 ||| 43996 ||| 
2021 ||| attention-based encoder-decoder networks for workflow recognition. ||| 1254 ||| 43153 ||| 43154 ||| 1037 ||| 
2021 ||| computational attention model for children, adults and the elderly. ||| 20386 ||| 20388 ||| 20387 ||| 
2018 ||| fine-grained attention for image caption generation. ||| 43997 ||| 
2019 ||| 3d convolution network and siamese-attention mechanism for expression recognition. ||| 43998 ||| 2990 ||| 2487 ||| 
2020 ||| a part-based attention network for person re-identification. ||| 6766 ||| 6765 ||| 6514 ||| 6764 ||| 6767 ||| 
2017 ||| a car-face region-based image retrieval method with attention of sift features. ||| 43999 ||| 6711 ||| 4287 ||| 2045 ||| 204 ||| 
2021 ||| attention cutting and padding learning for fine-grained image recognition. ||| 1212 ||| 44000 ||| 44001 ||| 44002 ||| 44003 ||| 1702 ||| 
2020 ||| a target response adaptive correlation filter tracker with spatial attention. ||| 44004 ||| 4807 ||| 17757 ||| 25333 ||| 44005 ||| 
2019 ||| a novel hybrid image fusion method based on integer lifting wavelet and discrete cosine transformer for visual sensor networks. ||| 44006 ||| 44007 ||| 44008 ||| 44009 ||| 
2020 ||| dnn-based speech enhancement with self-attention on feature dimension. ||| 44010 ||| 28645 ||| 17104 ||| 
2021 ||| correction to: mri enhancement based on visual-attention by adaptive contrast adjustment and image fusion. ||| 8473 ||| 32490 ||| 24526 ||| 6278 ||| 
2018 ||| complex event detection via attention-based video representation and classification. ||| 19112 ||| 44011 ||| 19113 ||| 
2020 ||| attention-based convolutional neural network for deep face recognition. ||| 975 ||| 44012 ||| 14730 ||| 976 ||| 977 ||| 
2021 ||| spam review detection using self attention based cnn and bi-directional lstm. ||| 44013 ||| 44014 ||| 44015 ||| 
2018 ||| a visual attention-based keyword extraction for document classification. ||| 610 ||| 31545 ||| 18548 ||| 
2017 ||| leveraging visual attention and neural activity for stereoscopic 3d visual comfort assessment. ||| 44016 ||| 44017 ||| 44018 ||| 5950 ||| 44019 ||| 
2019 ||| multiple attention fully convolutional network for automated ventricle segmentation in cardiac magnetic resonance imaging. ||| 44020 ||| 44021 ||| 27971 ||| 38646 ||| 38454 ||| 
2019 ||| brain function network analysis of children with attention-deficit/hyperactivity disorder based on adaptive sparse representation method. ||| 44022 ||| 44023 ||| 44024 ||| 29002 ||| 9447 ||| 
2020 ||| analysis of electroencephalography signals on the contents of cognitive function game: attention and memory. ||| 44025 ||| 44026 ||| 44027 ||| 44028 ||| 44029 ||| 44030 ||| 
2021 ||| unsupervised deep learning network with self-attention mechanism for non-rigid registration of 3d brain mr images. ||| 44031 ||| 44032 ||| 34520 ||| 34521 ||| 
2019 ||| application of deep convolutional neural networks in attention-deficit/hyperactivity disorder classification: data augmentation and convolutional neural network transfer learning. ||| 41692 ||| 44033 ||| 
2019 ||| contextual predictability and phonetic attention. ||| 44034 ||| 
2018 ||| perceptual attention as the locus of transfer to nonnative speech perception. ||| 44035 ||| 
2020 ||| time-compressed audio on attention, meditation, cognitive load, and learning. ||| 44036 ||| 44037 ||| 44038 ||| 44039 ||| 39837 ||| 44040 ||| 
2018 ||| is group polling better? an investigation of the effect of individual and group polling strategies on students' academic performance, anxiety, and attention. ||| 6180 ||| 6185 ||| 40219 ||| 44041 ||| 6182 ||| 
2018 ||| application of a gamified interactive response system to enhance the intrinsic and extrinsic motivation, student engagement, and attention of english learners. ||| 6180 ||| 44042 ||| 
2017 ||| effects of attention cueing on learning speech organ operation through mobile phones. ||| 44043 ||| 
2018 ||| a votable concept mapping approach to promoting students' attentional behavior: an analysis of sequential behavioral patterns and brainwave data. ||| 6180 ||| 6181 ||| 6182 ||| 6183 ||| 6184 ||| 6185 ||| 
2021 ||| analysis and prediction of "ai + education" attention based on baidu index - taking guizhou province as an example. ||| 44044 ||| 44045 ||| 44046 ||| 
2020 ||| basn - learning steganography with a binary attention mechanism. ||| 29448 ||| 29451 ||| 11466 ||| 42216 ||| 
2018 ||| a bi-directional lstm-cnn model with attention for aspect-level text classification. ||| 9090 ||| 44047 ||| 10820 ||| 44048 ||| 1008 ||| 
2019 ||| a multi-attention network for aspect-level sentiment analysis. ||| 44049 ||| 15504 ||| 
2021 ||| the effects of the content elements of online banner ads on visual attention: evidence from an-eye-tracking study. ||| 44050 ||| 44051 ||| 44052 ||| 
2021 ||| person re-identification based on attention mechanism and context information fusion. ||| 44053 ||| 44054 ||| 44055 ||| 
2020 ||| hierarchical gated recurrent unit with semantic attention for event prediction. ||| 44056 ||| 44057 ||| 
2019 ||| an improved approach for text sentiment classification based on a deep neural network via a sentiment attention mechanism. ||| 44058 ||| 20373 ||| 44049 ||| 2361 ||| 
2021 ||| knowledge-graph-based drug repositioning against covid-19 by graph convolutional network with attention mechanism. ||| 44059 ||| 44060 ||| 44061 ||| 44062 ||| 44063 ||| 
2020 ||| learning a hierarchical global attention for image classification. ||| 28266 ||| 44064 ||| 44065 ||| 28268 ||| 
2021 ||| linking phubbing behavior to self-reported attentional failures and media multitasking. ||| 44066 ||| 44067 ||| 
2021 ||| video captioning based on channel soft attention and semantic reconstructor. ||| 44055 ||| 44068 ||| 
2018 ||| chinese event extraction based on attention and semantic features: a bidirectional circular neural network. ||| 4104 ||| 44069 ||| 
2019 ||| dynamic group recommendation based on the attention mechanism. ||| 44070 ||| 44071 ||| 42033 ||| 692 ||| 44072 ||| 
2021 ||| rumor detection based on attention cnn and time series of context information. ||| 38218 ||| 44073 ||| 
2019 ||| combined self-attention mechanism for chinese named entity recognition in military. ||| 44074 ||| 44075 ||| 44076 ||| 44077 ||| 
2021 ||| a classification method for academic resources based on a graph attention network. ||| 11676 ||| 44078 ||| 44079 ||| 44080 ||| 
2019 ||| feature fusion text classification model combining cnn and bigru with multi-attention mechanism. ||| 44081 ||| 9082 ||| 9083 ||| 13435 ||| 
2020 ||| unsteady multi-element time series analysis and prediction based on spatial-temporal attention and error forecast fusion. ||| 44082 ||| 15189 ||| 
2022 ||| da-gan: dual attention generative adversarial network for cross-modal retrieval. ||| 44083 ||| 978 ||| 15661 ||| 44084 ||| 
2019 ||| object detection network based on feature fusion and attention mechanism. ||| 4646 ||| 44085 ||| 34725 ||| 44086 ||| 
2020 ||| paranoid transformer: reading narrative of madness as computational approach to creativity. ||| 13373 ||| 3430 ||| 13374 ||| 
2021 ||| dual-sampling attention pooling for graph neural networks on 3d mesh. ||| 44087 ||| 44088 ||| 484 ||| 19358 ||| 44089 ||| 
2022 ||| dcacnet: dual context aggregation and attention-guided cross deconvolution network for medical image segmentation. ||| 31221 ||| 29079 ||| 29080 ||| 6174 ||| 29745 ||| 31219 ||| 31217 ||| 44090 ||| 
2019 ||| computer aided diagnosis system for multiple sclerosis disease based on phase to amplitude coupling in covert visual attention. ||| 41629 ||| 41628 ||| 41630 ||| 
2022 ||| multi-scale information with attention integration for classification of liver fibrosis in b-mode us image. ||| 44091 ||| 19066 ||| 44092 ||| 44093 ||| 29989 ||| 44094 ||| 44095 ||| 
2021 ||| xecgnet: fine-tuning attention map within convolutional neural network to improve detection and explainability of concurrent cardiac arrhythmias. ||| 44096 ||| 44097 ||| 44098 ||| 
2021 ||| deep attention branch networks for skin lesion classification. ||| 44099 ||| 44100 ||| 44101 ||| 44102 ||| 42527 ||| 44103 ||| 44104 ||| 44105 ||| 
2021 ||| gradual back-projection residual attention network for magnetic resonance image super-resolution. ||| 44106 ||| 42766 ||| 34317 ||| 
2018 ||| attentional bias in mdd: erp components analysis and classification using a dot-probe task. ||| 15710 ||| 44107 ||| 23377 ||| 11188 ||| 44108 ||| 44109 ||| 44110 ||| 44111 ||| 20031 ||| 6271 ||| 40070 ||| 
2020 ||| hybrid attention for automatic segmentation of whole fetal head in prenatal ultrasound volumes. ||| 7676 ||| 8012 ||| 9149 ||| 35562 ||| 35563 ||| 35564 ||| 16933 ||| 8637 ||| 20535 ||| 
2021 ||| social media data analytics for outbreak risk communication: public attention on the "new normal" during the covid-19 pandemic in indonesia. ||| 44112 ||| 44113 ||| 44114 ||| 44115 ||| 44116 ||| 
2020 ||| avnet: a retinal artery/vein classification network with category-attention weighted fusion. ||| 40564 ||| 44117 ||| 19684 ||| 44118 ||| 4003 ||| 333 ||| 
2021 ||| bseresu-net: an attention-based before-activation residual u-net for retinal vessel segmentation. ||| 951 ||| 44119 ||| 
2021 ||| automated ecg classification using a non-local convolutional block attention module. ||| 44120 ||| 18305 ||| 44121 ||| 44122 ||| 30097 ||| 44123 ||| 16915 ||| 
2020 ||| skeletal bone age prediction based on a deep residual network with spatial transformer. ||| 44124 ||| 44125 ||| 
2021 ||| virus identification in electron microscopy images by residual mixed attention network. ||| 44126 ||| 5250 ||| 39210 ||| 30428 ||| 3880 ||| 44127 ||| 5252 ||| 
2022 ||| classification of renal biopsy direct immunofluorescence image using multiple attention convolutional neural network. ||| 1166 ||| 765 ||| 44128 ||| 44129 ||| 5187 ||| 44130 ||| 42545 ||| 2562 ||| 
2021 ||| super-resolution of pneumocystis carinii pneumonia ct via self-attention gan. ||| 44131 ||| 44132 ||| 44133 ||| 44134 ||| 44135 ||| 44136 ||| 44137 ||| 1223 ||| 10646 ||| 
2021 ||| time-series deep survival prediction for hemodialysis patients using an attention-based bi-gru network. ||| 44138 ||| 36631 ||| 44139 ||| 44140 ||| 3750 ||| 44141 ||| 31060 ||| 
2022 ||| a cnn-transformer hybrid approach for decoding visual neural activity into text. ||| 39696 ||| 399 ||| 44142 ||| 44143 ||| 31736 ||| 44144 ||| 44145 ||| 19823 ||| 44146 ||| 471 ||| 44147 ||| 
2022 ||| automatic location scheme of anatomical landmarks in 3d head mri based on the scale attention hourglass network. ||| 44148 ||| 44149 ||| 27849 ||| 44150 ||| 44151 ||| 44152 ||| 44153 ||| 27850 ||| 44154 ||| 
2018 ||| visuospatial working memory assessment using a digital tablet in adolescents with attention deficit hyperactivity disorder. ||| 44155 ||| 44156 ||| 44157 ||| 44158 ||| 44159 ||| 44160 ||| 44161 ||| 
2017 ||| reliability and validity of ds-adhd: a decision support system on attention deficit hyperactivity disorders. ||| 44162 ||| 44163 ||| 44164 ||| 44165 ||| 44166 ||| 44167 ||| 
2021 ||| automated detection of conduct disorder and attention deficit hyperactivity disorder using decomposition and nonlinear techniques with eeg signals. ||| 44168 ||| 44169 ||| 44170 ||| 44171 ||| 44172 ||| 44173 ||| 6127 ||| 44174 ||| 
2020 ||| classification of breast density categories based on se-attention neural networks. ||| 44175 ||| 44176 ||| 44177 ||| 44178 ||| 1199 ||| 4600 ||| 
2019 ||| extracting chemical-protein interactions from biomedical literature via granular attention based recurrent neural networks. ||| 16633 ||| 16631 ||| 15301 ||| 1305 ||| 20809 ||| 
2021 ||| multiscale attention guided u-net architecture for cardiac segmentation in short-axis mri images. ||| 17554 ||| 17555 ||| 479 ||| 9199 ||| 10075 ||| 
2020 ||| self-attention based recurrent convolutional neural network for disease prediction using healthcare data. ||| 42334 ||| 42335 ||| 44179 ||| 42336 ||| 5652 ||| 
2020 ||| semi-supervised segmentation of lesion from breast ultrasound images with attentional generative adversarial network. ||| 44180 ||| 44181 ||| 35562 ||| 11634 ||| 44182 ||| 44183 ||| 1302 ||| 16906 ||| 39696 ||| 
2021 ||| clcu-net: cross-level connected u-shaped network with selective feature aggregation attention module for brain tumor segmentation. ||| 44184 ||| 44185 ||| 44186 ||| 44187 ||| 
2018 ||| classification of auditory selective attention using spatial coherence and modular attention index. ||| 44188 ||| 44189 ||| 44190 ||| 44191 ||| 
2017 ||| instructor presence in instructional video: effects on visual attention, recall, and perceived learning. ||| 15830 ||| 40227 ||| 
2018 ||| the effect of cellphones on attention and learning: the influences of time, distraction, and nomophobia. ||| 44192 ||| 44193 ||| 44194 ||| 44195 ||| 44196 ||| 
2018 ||| the female gaze: content composition and slot position in personalized banner ads, and how they influence visual attention in online shoppers. ||| 23726 ||| 
2017 ||| effects of an integrated physiological signal-based attention-promoting and english listening system on students' learning performance and behavioral patterns. ||| 44197 ||| 44198 ||| 44199 ||| 
2017 ||| effectiveness, attention, and recall of human and artificial voices in an advertising story. prosody influence and functions of voices. ||| 44200 ||| 
2019 ||| narcissistic adolescents' attention-seeking following social rejection: links with social media disclosure, problematic social media use, and smartphone stress. ||| 44201 ||| 44202 ||| 44203 ||| 44204 ||| 
2022 ||| share of attention: exploring the allocation of user attention to consumer applications. ||| 44205 ||| 44206 ||| 
2019 ||| how attention level and cognitive style affect learning in a mooc environment? based on the perspective of brainwave analysis. ||| 44207 ||| 44208 ||| 44209 ||| 
2021 ||| mobile use induces local attentional precedence and is associated with limited socio-cognitive skills in preschoolers. ||| 44210 ||| 44211 ||| 3882 ||| 44212 ||| 44213 ||| 44214 ||| 44215 ||| 44216 ||| 44217 ||| 44218 ||| 44219 ||| 44220 ||| 44221 ||| 44222 ||| 44223 ||| 15326 ||| 44224 ||| 44225 ||| 
2018 ||| does game rules work as a game changer? analyzing the effect of rule orientation on brand attention and memory in advergames. ||| 44226 ||| 44227 ||| 44228 ||| 
2022 ||| predicting reposting latency of news content in social media: a focus on issue attention, temporal usage pattern, and information redundancy. ||| 44229 ||| 44230 ||| 44231 ||| 
2020 ||| effects of spatial distance on the effectiveness of mental and physical integration strategies in learning from split-attention examples. ||| 648 ||| 44232 ||| 44233 ||| 44234 ||| 
2020 ||| enhanced memory-driven attentional capture in action video game players. ||| 1618 ||| 44235 ||| 44236 ||| 44237 ||| 41378 ||| 1065 ||| 
2020 ||| the looking glass selfie: instagram use frequency predicts visual attention to high-anxiety body regions in young women. ||| 44238 ||| 
2019 ||| comparative analysis of advertising attention to facebook social network: evidence from eye-movement data. ||| 17329 ||| 17330 ||| 
2019 ||| accidentally attentive: comparing visual, close-ended, and open-ended measures of attention on social media. ||| 44239 ||| 44240 ||| 44241 ||| 44242 ||| 
2019 ||| electronic performance monitoring and sustained attention: social facilitation for modern applications. ||| 44243 ||| 42892 ||| 
2020 ||| how can you persuade me online? the impact of goal-driven motivations on attention to online information. ||| 44244 ||| 44245 ||| 44246 ||| 
2020 ||| human-computer interaction based joint attention cues: implications on functional and physiological measures for children with autism spectrum disorder. ||| 4679 ||| 4681 ||| 
2018 ||| my friend likes this brand: do ads with social context attract more attention on social networking sites? ||| 44247 ||| 44248 ||| 44249 ||| 44250 ||| 44251 ||| 3049 ||| 
2018 ||| effects of instructor presence in video modeling examples on attention and learning. ||| 40224 ||| 44252 ||| 40225 ||| 
2018 ||| using time pressure and note-taking to prevent digital distraction behavior and enhance online search performance: perspectives from the load theory of attention and cognitive control. ||| 40220 ||| 44253 ||| 
2017 ||| mobile attachment: separation from the mobile phone induces physiological and behavioural stress and attentional bias to separation-related stimuli. ||| 44210 ||| 44221 ||| 44222 ||| 15326 ||| 44224 ||| 44225 ||| 
2020 ||| testing the feasibility of a media multitasking self-regulation intervention for students: behaviour change, attention, and self-perception. ||| 44254 ||| 44255 ||| 44256 ||| 
2020 ||| the influence of spatial distance and signaling on the split-attention effect. ||| 44257 ||| 44233 ||| 648 ||| 44232 ||| 
2017 ||| switching on or switching off? everyday computer use as a predictor of sustained attention and cognitive reflection. ||| 44258 ||| 
2017 ||| effects of cognition demand, mode of interactivity and brand anthropomorphism on gamers' brand attention and memory in advergames. ||| 44226 ||| 44227 ||| 
2018 ||| the influence of peer accountability on attention during gameplay. ||| 44259 ||| 44260 ||| 
2020 ||| investigating the attentional bias and information processing mechanism of mobile phone addicts towards emotional information. ||| 44261 ||| 44262 ||| 44263 ||| 44264 ||| 44265 ||| 44266 ||| 44267 ||| 44268 ||| 
2017 ||| constructing perceptual common ground between human and robot through joint attention. ||| 44269 ||| 25913 ||| 
2020 ||| humanoid robots and autistic children: a review on technological tools to assess social attention and engagement. ||| 43105 ||| 20880 ||| 20881 ||| 44270 ||| 44271 ||| 
2019 ||| automatic carotid artery detection using attention layer region-based convolution neural network. ||| 13818 ||| 25198 ||| 16645 ||| 40458 ||| 16644 ||| 8775 ||| 44272 ||| 
2019 ||| fault diagnosis of a transformer based on polynomial neural networks. ||| 44273 ||| 39001 ||| 44274 ||| 44275 ||| 
2019 ||| the impact of attention heterogeneity on stock market in the era of big data. ||| 44276 ||| 44277 ||| 
2019 ||| fault diagnosis for oil-filled transformers using voting based extreme learning machine. ||| 44278 ||| 44279 ||| 
2021 ||| multimodal encoders and decoders with gate attention for visual question answering. ||| 44280 ||| 40953 ||| 
2020 ||| a recommendations model with multiaspect awareness and hierarchical user-product attention mechanisms. ||| 44281 ||| 44282 ||| 6474 ||| 14939 ||| 
2021 ||| a novel distant target region detection method using hybrid saliency-based attention model under complex textures. ||| 44283 ||| 30026 ||| 
2020 ||| semi-supervised trajectory understanding with poi attention for end-to-end trip recommendation. ||| 9010 ||| 44284 ||| 9011 ||| 44285 ||| 9012 ||| 
2021 ||| deepran: attention-based bilstm and crf for ransomware early detection and classification. ||| 44286 ||| 12760 ||| 
2020 ||| how do small and medium-sized game companies use analytics? an attention-based view of game analytics. ||| 44287 ||| 44288 ||| 18816 ||| 15736 ||| 44289 ||| 
2021 ||| popular news are relevant news! how investor attention affects algorithmic decision-making and decision support in financial markets. ||| 44290 ||| 44291 ||| 44292 ||| 
2020 ||| attention-based recurrent neural network for multistep-ahead prediction of process performance. ||| 32977 ||| 15277 ||| 15278 ||| 44293 ||| 
2021 ||| face-based attention recognition model for children with autism spectrum disorder. ||| 4346 ||| 4347 ||| 4348 ||| 11828 ||| 
2019 ||| deepsnp: an end-to-end deep neural network with attention-based localization for breakpoint detection in single-nucleotide polymorphism array genomic data. ||| 35700 ||| 35701 ||| 35702 ||| 35703 ||| 35704 ||| 35706 ||| 35707 ||| 35708 ||| 35709 ||| 11898 ||| 35710 ||| 
2018 ||| a computing model of selective attention for service robot based on spatial data fusion. ||| 31759 ||| 31760 ||| 
2021 ||| dncp: an attention-based deep learning approach enhanced with attractiveness and timeliness of news for online news click prediction. ||| 44294 ||| 5999 ||| 44295 ||| 44296 ||| 
2021 ||| solving multi-agent routing problems using deep attention mechanisms. ||| 44297 ||| 44298 ||| 44299 ||| 34796 ||| 44300 ||| 44301 ||| 
2021 ||| an attention-based deep learning framework for trip destination prediction of sharing bike. ||| 1160 ||| 42413 ||| 44302 ||| 42130 ||| 3402 ||| 5474 ||| 
2021 ||| detecting driver behavior using stacked long short term memory network with attention layer. ||| 25892 ||| 25893 ||| 
2022 ||| heterogeneous attentions for solving pickup and delivery problem via deep reinforcement learning. ||| 30672 ||| 18265 ||| 18245 ||| 35039 ||| 18266 ||| 1134 ||| 
2021 ||| anomaly detection in automated vehicles using multistage attention-based convolutional neural network. ||| 44303 ||| 29831 ||| 44304 ||| 44305 ||| 44306 ||| 
2018 ||| multi-level contextual rnns with attention model for scene labeling. ||| 7245 ||| 44307 ||| 44308 ||| 2163 ||| 
2021 ||| detecting anomalies in intelligent vehicle charging and station power supply systems with multi-head attention models. ||| 14115 ||| 254 ||| 44309 ||| 1160 ||| 
2021 ||| detecting text in scene and traffic guide panels with attention anchor mechanism. ||| 44310 ||| 17416 ||| 748 ||| 14747 ||| 44311 ||| 44312 ||| 5051 ||| 
2020 ||| fast pedestrian detection with attention-enhanced multi-scale rpn and soft-cascaded decision trees. ||| 12273 ||| 11220 ||| 11221 ||| 
2020 ||| driver inattention detection in the context of next-generation autonomous vehicles design: a survey. ||| 44313 ||| 44314 ||| 44315 ||| 
2021 ||| a spatial-temporal attention approach for traffic prediction. ||| 18147 ||| 44316 ||| 38827 ||| 44317 ||| 621 ||| 
2019 ||| traffic sign detection using a multi-scale recurrent attention network. ||| 29275 ||| 44318 ||| 12038 ||| 44319 ||| 1801 ||| 
2022 ||| efficient resource allocation for multi-beam satellite-terrestrial vehicular networks: a multi-agent actor-critic method with attention mechanism. ||| 44320 ||| 44321 ||| 44322 ||| 44323 ||| 17433 ||| 13486 ||| 
2022 ||| a multi-scale attributes attention model for transport mode identification. ||| 9758 ||| 9759 ||| 9761 ||| 44324 ||| 44325 ||| 
2020 ||| how do drivers allocate their potential attention? driving fixation prediction via convolutional neural networks. ||| 19821 ||| 19823 ||| 5435 ||| 44326 ||| 7201 ||| 
2021 ||| using channel-wise attention for deep cnn based real-time semantic segmentation with class-aware edge information. ||| 44327 ||| 28820 ||| 6374 ||| 6375 ||| 
2021 ||| a hybrid deep learning model with attention-based conv-lstm networks for short-term traffic flow prediction. ||| 16300 ||| 16299 ||| 16301 ||| 44328 ||| 
2022 ||| subcycle waveform modeling of traffic intersections using recurrent attention networks. ||| 23584 ||| 23583 ||| 23585 ||| 23586 ||| 
2019 ||| a reference model for driver attention in automation: glance behavior changes during lateral and longitudinal assistance. ||| 44329 ||| 44330 ||| 44331 ||| 
2021 ||| a robust attentional framework for license plate recognition in the wild. ||| 6570 ||| 5845 ||| 4175 ||| 5189 ||| 6335 ||| 10075 ||| 
2020 ||| attention-based deep ensemble net for large-scale online taxi-hailing demand prediction. ||| 1305 ||| 3232 ||| 44332 ||| 12749 ||| 
2020 ||| simultaneous end-to-end vehicle and license plate detection with multi-branch attention neural network. ||| 17394 ||| 14747 ||| 44333 ||| 17396 ||| 5051 ||| 
2018 ||| training tips for the transformer model. ||| 21388 ||| 21421 ||| 
2017 ||| visualizing neural machine translation attention and confidence. ||| 37277 ||| 21403 ||| 21421 ||| 
2017 ||| generating alignments using target foresight in attention-based neural machine translation. ||| 44334 ||| 44335 ||| 3454 ||| 
2020 ||| every layer counts: multi-layer multi-head attention for neural machine translation. ||| 7548 ||| 7549 ||| 7550 ||| 7551 ||| 
2021 ||| spatiotemporal attention enhanced features fusion network for action recognition. ||| 42952 ||| 24565 ||| 27439 ||| 20238 ||| 
2022 ||| domain adaptive attention-based dropout for one-shot person re-identification. ||| 2819 ||| 2821 ||| 
2021 ||| hierarchical multi-attention networks for document classification. ||| 44336 ||| 44337 ||| 44338 ||| 527 ||| 528 ||| 
2021 ||| dlsa: dual-learning based on self-attention for rating prediction. ||| 44339 ||| 44340 ||| 44341 ||| 42140 ||| 25715 ||| 25716 ||| 
2021 ||| learning to capture contrast in sarcasm with contextual dual-view attention network. ||| 31452 ||| 8974 ||| 728 ||| 8967 ||| 8977 ||| 
2022 ||| mixed attention hourglass network for robust face alignment. ||| 44342 ||| 44343 ||| 13439 ||| 20871 ||| 36836 ||| 
2021 ||| a hybrid of xgboost and aspect-based review mining with attention neural network for user preference prediction. ||| 44344 ||| 41788 ||| 44345 ||| 
2019 ||| word-character attention model for chinese text classification. ||| 44346 ||| 44347 ||| 5107 ||| 44348 ||| 
2021 ||| chinese medical relation extraction based on multi-hop self-attention mechanism. ||| 29314 ||| 8974 ||| 44349 ||| 29315 ||| 29316 ||| 728 ||| 
2021 ||| attention-based context aggregation network for monocular depth estimation. ||| 39962 ||| 39963 ||| 39964 ||| 44350 ||| 
2021 ||| a novel self-attention deep subspace clustering. ||| 44351 ||| 44352 ||| 44353 ||| 
2021 ||| sequence and graph structure co-awareness via gating mechanism and self-attention for session-based recommendation. ||| 44354 ||| 1052 ||| 10830 ||| 
2019 ||| attention-based argumentation mining. ||| 22672 ||| 22673 ||| 22674 ||| 22675 ||| 22676 ||| 22677 ||| 
2019 ||| bio-inspired visual attention process using spiking neural networks controlling a camera. ||| 3369 ||| 44355 ||| 15325 ||| 15326 ||| 44356 ||| 44357 ||| 
2019 ||| an embodied agent learning affordances with intrinsic motivations and solving extrinsic tasks with attention and one-step planning. ||| 29021 ||| 44358 ||| 44359 ||| 44360 ||| 
2019 ||| attention based visual analysis for fast grasp planning with a multi-fingered robotic hand. ||| 34782 ||| 2236 ||| 5736 ||| 1168 ||| 12748 ||| 18083 ||| 
2021 ||| a biological inspired cognitive framework for memory-based multi-sensory joint attention in human-robot interactive tasks. ||| 44361 ||| 44362 ||| 44363 ||| 44364 ||| 24067 ||| 
2021 ||| dual attention triplet hashing network for image retrieval. ||| 44365 ||| 44366 ||| 44367 ||| 
2022 ||| attention-guided multi-scale feature fusion network for low-light image enhancement. ||| 44368 ||| 31473 ||| 42180 ||| 44369 ||| 
2021 ||| client-server approach for managing visual attention, integrated in a cognitive architecture for a social robot. ||| 44370 ||| 3882 ||| 44371 ||| 7111 ||| 4577 ||| 4578 ||| 44372 ||| 44373 ||| 44374 ||| 
2021 ||| progressive multi-scale vision transformer for facial action unit detection. ||| 44375 ||| 44376 ||| 
2021 ||| multi-head attention-based long short-term memory for depression detection from speech. ||| 10646 ||| 30881 ||| 28012 ||| 254 ||| 31027 ||| 17104 ||| 
2021 ||| the acquisition of culturally patterned attention styles under active inference. ||| 44377 ||| 44378 ||| 44379 ||| 44380 ||| 44381 ||| 32156 ||| 3831 ||| 44382 ||| 
2019 ||| an attention-controlled hand exoskeleton for the rehabilitation of finger extension and flexion using a rigid-soft combined mechanism. ||| 12646 ||| 6186 ||| 44383 ||| 44384 ||| 27834 ||| 44385 ||| 23710 ||| 11745 ||| 44386 ||| 
2022 ||| query selector-efficient transformer with sparse attention. ||| 37640 ||| 20399 ||| 37641 ||| 37642 ||| 37643 ||| 
2017 ||| evolutionary multi-objective fault diagnosis of power transformers. ||| 29334 ||| 29335 ||| 29336 ||| 29337 ||| 
2017 ||| estimation of transformer parameters from nameplate data by imperialist competitive and gravitational search algorithms. ||| 44387 ||| 44388 ||| 44389 ||| 
2018 ||| multi objective evolutionary algorithm for designing energy efficient distribution transformers. ||| 43831 ||| 43832 ||| 43833 ||| 44390 ||| 44391 ||| 
2022 ||| an anisotropic non-local attention network for image segmentation. ||| 44392 ||| 44393 ||| 30496 ||| 30831 ||| 44394 ||| 
2019 ||| a spatiotemporal attention-based resc3d model for large-scale gesture recognition. ||| 13441 ||| 13442 ||| 44395 ||| 44396 ||| 2303 ||| 
2018 ||| end-to-end temporal attention extraction and human action recognition. ||| 6735 ||| 44397 ||| 24350 ||| 27118 ||| 241 ||| 44398 ||| 
2021 ||| fae-gan: facial attribute editing with multi-scale attention normalization. ||| 44399 ||| 44400 ||| 10000 ||| 19066 ||| 4550 ||| 6030 ||| 
2021 ||| fpanet: feature-enhanced position attention network for semantic segmentation. ||| 44401 ||| 29059 ||| 29058 ||| 5067 ||| 6415 ||| 44402 ||| 
2022 ||| rca-iunet: a residual cross-spatial attention-guided inception u-net model for tumor segmentation in breast ultrasound imaging. ||| 12821 ||| 12823 ||| 
2021 ||| sa-singan: self-attention for single-image generation adversarial networks. ||| 5250 ||| 44403 ||| 44404 ||| 44405 ||| 44406 ||| 44407 ||| 
2021 ||| lesion-aware attention with neural support vector machine for retinopathy diagnosis. ||| 30062 ||| 44408 ||| 
2021 ||| virtual animals as diegetic attention guidance mechanisms in 360-degree experiences. ||| 44409 ||| 44410 ||| 44411 ||| 44412 ||| 44413 ||| 44414 ||| 44415 ||| 44416 ||| 
2019 ||| veram: view-enhanced recurrent attention model for 3d shape classification. ||| 39158 ||| 32118 ||| 2349 ||| 37268 ||| 5723 ||| 
2021 ||| attention flows: analyzing and comparing attention mechanisms in language models. ||| 39166 ||| 39167 ||| 39168 ||| 
2022 ||| visqa: x-raying vision and language reasoning in transformers. ||| 36568 ||| 36569 ||| 36570 ||| 36571 ||| 36572 ||| 7969 ||| 
2020 ||| toward cognitive load inference for attention management in ubiquitous systems. ||| 22996 ||| 22994 ||| 36705 ||| 36710 ||| 44417 ||| 
2020 ||| how far are we from quantifying visual attention in mobile hci? ||| 15349 ||| 15350 ||| 15351 ||| 8348 ||| 
2020 ||| attention paid versus paying attention in pervasive computing. ||| 44418 ||| 38893 ||| 
2021 ||| robust finger vein recognition based on deep cnn with spatial attention and bias field correction. ||| 4641 ||| 4642 ||| 
2021 ||| cpman: change point detection approach in time series based on the prediction of multi-stage attention networks. ||| 14188 ||| 15769 ||| 15770 ||| 
2019 ||| modelling speaker-dependent auditory attention using a spiking neural network with temporal coding and supervised learning. ||| 44419 ||| 5350 ||| 728 ||| 
2019 ||| safont: automatic font synthesis using self-attention mechanisms. ||| 44420 ||| 17344 ||| 44421 ||| 17342 ||| 
2019 ||| multi-lingual attention based multi-intent detection in dialogue system. ||| 163 ||| 27260 ||| 165 ||| 405 ||| 
2019 ||| a feedback attention network for classification and visualization of thyroid nodules on ultrasound images. ||| 14759 ||| 16837 ||| 16771 ||| 14761 ||| 5950 ||| 25999 ||| 16772 ||| 8479 ||| 44422 ||| 16773 ||| 16774 ||| 
2019 ||| a match-transformer framework for modeling diverse relevance patterns in ad-hoc retrieval. ||| 44423 ||| 989 ||| 44424 ||| 5076 ||| 44425 ||| 44426 ||| 19591 ||| 5075 ||| 
2019 ||| dissect sliced-rnn in multi-attention view. ||| 44427 ||| 44428 ||| 286 ||| 
2019 ||| pyramnet: point cloud pyramid attention network and graph embedding module for classification and segmentation. ||| 39522 ||| 7015 ||| 
2021 ||| learning to construct nested polar codes: an attention-based set-to-element model. ||| 438 ||| 44429 ||| 44430 ||| 37198 ||| 44431 ||| 
2021 ||| self-attention-based real-time signal detector for communication systems with unknown channel models. ||| 1207 ||| 21803 ||| 
2022 ||| graph attention network-based single-pixel compressive direction of arrival estimation. ||| 13478 ||| 13479 ||| 39617 ||| 2101 ||| 44432 ||| 
2021 ||| automatic modulation recognition based on adaptive attention mechanism and resnext wsl model. ||| 15895 ||| 15896 ||| 10415 ||| 44433 ||| 7676 ||| 
2021 ||| multi-head attention based popularity prediction caching in social content-centric networking with mobile edge computing. ||| 1832 ||| 44434 ||| 44435 ||| 44436 ||| 44437 ||| 44438 ||| 44439 ||| 44440 ||| 
2020 ||| spatial-temporal attention-convolution network for citywide cellular traffic prediction. ||| 43503 ||| 44441 ||| 44442 ||| 44443 ||| 44444 ||| 
2020 ||| radar emitter classification with attention-based multi-rnns. ||| 44445 ||| 44446 ||| 44447 ||| 44448 ||| 
2021 ||| st-tran: spatial-temporal transformer for cellular traffic prediction. ||| 44449 ||| 33204 ||| 42995 ||| 
2021 ||| a self-attention-based i/q imbalance estimator for beyond 5g communication systems. ||| 44450 ||| 44451 ||| 44452 ||| 
2021 ||| attention-based multilevel co-occurrence graph convolutional lstm for 3-d action recognition. ||| 44453 ||| 23374 ||| 44454 ||| 17840 ||| 13701 ||| 16724 ||| 23377 ||| 
2021 ||| hierarchical-attention-based defense method for load frequency control system against dos attack. ||| 44455 ||| 28795 ||| 44456 ||| 
2022 ||| environmental sound classification via time-frequency attention and framewise self-attention-based deep neural networks. ||| 11346 ||| 12647 ||| 
2019 ||| deep model for store site recommendation with attentional spatial embeddings. ||| 9337 ||| 16806 ||| 44457 ||| 875 ||| 44458 ||| 44459 ||| 44460 ||| 775 ||| 44461 ||| 771 ||| 
2021 ||| targeted attention attack on deep learning models in road sign recognition. ||| 33140 ||| 11493 ||| 33141 ||| 683 ||| 1756 ||| 
2020 ||| multimodal alignment and attention-based person search via natural language description. ||| 2276 ||| 30745 ||| 
2018 ||| attention-in-attention networks for surveillance video understanding in internet of things. ||| 7957 ||| 19713 ||| 19742 ||| 19743 ||| 
2022 ||| smart diagnosis: deep learning boosted driver inattention detection and abnormal driving prediction. ||| 44462 ||| 32992 ||| 44463 ||| 774 ||| 
2021 ||| an adaptive neurofeedback method for attention regulation based on the internet of things. ||| 20032 ||| 340 ||| 8932 ||| 12196 ||| 23377 ||| 16724 ||| 
2019 ||| an attention mechanism inspired selective sensing framework for physical-cyber mapping in internet of things. ||| 44464 ||| 44465 ||| 44466 ||| 44467 ||| 44468 ||| 
2021 ||| multimodality sentiment analysis in social internet of things based on hierarchical attentions and csat-tcn with mbm network. ||| 44469 ||| 44470 ||| 3137 ||| 44471 ||| 633 ||| 44472 ||| 43826 ||| 
2021 ||| temperature forecasting for stored grain: a deep spatiotemporal attention approach. ||| 1499 ||| 1500 ||| 1501 ||| 1502 ||| 1503 ||| 
2021 ||| distributed attention-based temporal convolutional network for remaining useful life prediction. ||| 3198 ||| 44473 ||| 31541 ||| 44474 ||| 28412 ||| 44475 ||| 
2020 ||| man: mutual attention neural networks model for aspect-level sentiment classification in siot. ||| 35885 ||| 44476 ||| 17694 ||| 44477 ||| 44478 ||| 
2020 ||| a novel iot-perceptive human activity recognition (har) approach using multihead convolutional attention. ||| 44479 ||| 44480 ||| 44481 ||| 9028 ||| 44482 ||| 
2021 ||| efficient covid-19 segmentation from ct slices exploiting semantic segmentation with integrated attention mechanism. ||| 44483 ||| 44484 ||| 44485 ||| 42324 ||| 42325 ||| 44486 ||| 59 ||| 
2021 ||| fetal ultrasound image segmentation for automatic head circumference biometry using deeply supervised attention-gated v-net. ||| 38979 ||| 44487 ||| 1358 ||| 44488 ||| 44489 ||| 
2020 ||| the invasiveness classification of ground-glass nodules using 3d attention network and hrct. ||| 44490 ||| 44491 ||| 44492 ||| 44493 ||| 36811 ||| 31576 ||| 
2021 ||| evolutionary deep attention convolutional neural networks for 2d and 3d medical image segmentation. ||| 13262 ||| 13263 ||| 13264 ||| 
2022 ||| high accuracy offering attention mechanisms based deep learning approach using cnn/bi-lstm for sentiment analysis. ||| 44494 ||| 44495 ||| 
2021 ||| double attention u-net for brain tumor mr image segmentation. ||| 10057 ||| 44496 ||| 
2021 ||| research on clothing patterns generation based on multi-scales self-attention improved generative adversarial network. ||| 44497 ||| 44498 ||| 
2022 ||| auto-attentional mechanism in multi-domain convolutional neural networks for improving object tracking. ||| 44499 ||| 
2020 ||| loss analysis from capacitance between windings in multilayer transformer and loss improvement by winding layer layout considering working voltage. ||| 44500 ||| 44501 ||| 44502 ||| 44503 ||| 
2018 ||| the use of new technologies to improve attention in neurodevelopmental disabilities: new educational scenarios for the enhancement of differences. ||| 44504 ||| 29796 ||| 29797 ||| 
2021 ||| focus within or on others: the impact of reviewers' attentional focus on review helpfulness. ||| 21655 ||| 21656 ||| 14048 ||| 
2020 ||| content growth and attention contagion in information networks: addressing information poverty on wikipedia. ||| 457 ||| 44505 ||| 44506 ||| 
2017 ||| cosearch attention and stock return predictability in supply chains. ||| 44507 ||| 22319 ||| 44508 ||| 44509 ||| 
2021 ||| fake news, investor attention, and market reaction. ||| 44510 ||| 44511 ||| 44512 ||| 44513 ||| 
2020 ||| 0.3 v 15-ghz band vco ics with novel transformer-based harmonic tuned tanks in 45-nm soi cmos. ||| 4231 ||| 44514 ||| 44515 ||| 
2021 ||| cross-modal retrieval with dual multi-angle self-attention. ||| 1178 ||| 28688 ||| 9689 ||| 580 ||| 6514 ||| 17845 ||| 
2021 ||| hierarchical attention model for personalized tag recommendation. ||| 44516 ||| 44517 ||| 43214 ||| 44518 ||| 15205 ||| 
2021 ||| impact of using bidirectional encoder representations from transformers (bert) models for arabic dialogue acts identification. ||| 44519 ||| 10486 ||| 44520 ||| 
2020 ||| sclerasegnet: an attention assisted u-net model for accurate sclera segmentation. ||| 18588 ||| 42636 ||| 17944 ||| 18590 ||| 2824 ||| 17945 ||| 
2020 ||| detecting multi-scale faces using attention-based feature fusion and smoothed context enhancement. ||| 6389 ||| 8704 ||| 8705 ||| 
2021 ||| attention-based spatial-temporal multi-scale network for face anti-spoofing. ||| 16828 ||| 44521 ||| 44522 ||| 3854 ||| 
2020 ||| evaluation and visualization of driver inattention rating from facial features. ||| 44523 ||| 44524 ||| 44525 ||| 44526 ||| 
2022 ||| arface: attention-aware and regularization for face recognition with reinforcement learning. ||| 30190 ||| 44527 ||| 44528 ||| 44529 ||| 44530 ||| 29239 ||| 5187 ||| 40031 ||| 
2020 ||| interpretable spatio-temporal attention lstm model for flood forecasting. ||| 18655 ||| 18656 ||| 4287 ||| 44531 ||| 18658 ||| 
2020 ||| stock movement predictive network via incorporative attention mechanisms based on tweet and historical prices. ||| 44532 ||| 44533 ||| 42948 ||| 42949 ||| 
2021 ||| knowledge augmented transformer for adversarial multidomain multiclassification multimodal fake news detection. ||| 41660 ||| 16316 ||| 25191 ||| 411 ||| 
2021 ||| nesting spatiotemporal attention networks for action recognition. ||| 44534 ||| 18980 ||| 14779 ||| 
2021 ||| gated graph neural attention networks for abstractive summarization. ||| 44535 ||| 7688 ||| 25201 ||| 9021 ||| 
2021 ||| a cognitive brain model for multimodal sentiment analysis based on attention neural networks. ||| 44536 ||| 19919 ||| 6187 ||| 6621 ||| 
2020 ||| bidirectional lstm with self-attention mechanism and multi-channel features for sentiment classification. ||| 31469 ||| 29661 ||| 2074 ||| 2950 ||| 
2020 ||| multi-level feature fusion based locality-constrained spatial transformer network for video crowd counting. ||| 19824 ||| 19827 ||| 4807 ||| 8700 ||| 44537 ||| 19828 ||| 
2021 ||| tlsan: time-aware long- and short-term attention network for next-item recommendation. ||| 38565 ||| 38566 ||| 1565 ||| 
2021 ||| masg-gan: a multi-view attention superpixel-guided generative adversarial network for efficient and simultaneous histopathology image segmentation and classification. ||| 44538 ||| 3114 ||| 6648 ||| 15846 ||| 
2021 ||| distant supervised relation extraction with position feature attention and selective bag attention. ||| 43307 ||| 44539 ||| 
2018 ||| hierarchical attention-based multimodal fusion for video captioning. ||| 18635 ||| 18634 ||| 18636 ||| 41064 ||| 19113 ||| 18638 ||| 
2021 ||| a time-frequency channel attention and vectorization network for automatic depression level prediction. ||| 12759 ||| 2304 ||| 12041 ||| 25762 ||| 
2019 ||| stacked u-shape networks with channel-wise attention for image super-resolution. ||| 44540 ||| 6030 ||| 42485 ||| 
2021 ||| image super-resolution based on residually dense distilled attention network. ||| 44541 ||| 44542 ||| 9458 ||| 23436 ||| 
2020 ||| knowledge attention sandwich neural network for text classification. ||| 22380 ||| 44543 ||| 44544 ||| 44545 ||| 1420 ||| 44546 ||| 
2021 ||| urca-gan: upsample residual channel-wise attention generative adversarial network for image-to-image translation. ||| 44547 ||| 44548 ||| 44549 ||| 21361 ||| 44550 ||| 
2021 ||| multi-scale stacking attention pooling for remote sensing scene classification. ||| 27337 ||| 14048 ||| 42468 ||| 
2021 ||| ransp: ranking attention network for saliency prediction on omnidirectional images. ||| 19902 ||| 19903 ||| 11343 ||| 19906 ||| 16821 ||| 19907 ||| 6516 ||| 8532 ||| 
2020 ||| video salient object detection via spatiotemporal attention neural networks. ||| 44551 ||| 44552 ||| 18061 ||| 1390 ||| 2514 ||| 
2021 ||| a visual place recognition approach using learnable feature map filtering and graph attention networks. ||| 44553 ||| 11276 ||| 44554 ||| 44555 ||| 44556 ||| 44557 ||| 
2021 ||| pdanet: pyramid density-aware attention based network for accurate crowd counting. ||| 39196 ||| 17341 ||| 10330 ||| 44558 ||| 30826 ||| 5123 ||| 
2021 ||| multi-branch guided attention network for irregular text recognition. ||| 778 ||| 779 ||| 
2021 ||| group multi-scale attention pyramid network for traffic sign detection. ||| 44559 ||| 44560 ||| 19855 ||| 44561 ||| 
2022 ||| relaxnet: residual efficient learning and attention expected fusion network for real-time semantic segmentation. ||| 3888 ||| 7485 ||| 44562 ||| 2306 ||| 44563 ||| 
2021 ||| sa-capsgan: using capsule networks with embedded self-attention for generative adversarial network. ||| 44564 ||| 44352 ||| 44565 ||| 44566 ||| 
2021 ||| conciseness is better: recurrent attention lstm model for document-level sentiment analysis. ||| 3312 ||| 3313 ||| 3315 ||| 
2021 ||| attention adjacency matrix based graph convolutional networks for skeleton-based action recognition. ||| 11745 ||| 13442 ||| 42249 ||| 44567 ||| 20669 ||| 44568 ||| 10235 ||| 
2018 ||| a visual attention based roi detection method for facial expression recognition. ||| 44569 ||| 39963 ||| 2821 ||| 
2021 ||| image super-resolution using multi-granularity perception and pyramid attention networks. ||| 2054 ||| 44570 ||| 44571 ||| 44572 ||| 6797 ||| 3890 ||| 
2021 ||| multi-attention based deep neural network with hybrid features for dynamic sequential facial expression recognition. ||| 443 ||| 44573 ||| 5890 ||| 
2022 ||| semi-supervised classification via full-graph attention neural networks. ||| 20341 ||| 44574 ||| 44575 ||| 
2021 ||| multi-stage attention spatial-temporal graph networks for traffic prediction. ||| 44576 ||| 44317 ||| 44577 ||| 38827 ||| 44316 ||| 621 ||| 
2021 ||| htda: hierarchical time-based directional attention network for sequential user behavior modeling. ||| 44578 ||| 6514 ||| 9689 ||| 
2021 ||| fine-grained question-answer sentiment classification with hierarchical graph attention network. ||| 44579 ||| 43061 ||| 1438 ||| 19695 ||| 
2021 ||| person image generation with attention-based injection network. ||| 33336 ||| 6534 ||| 40497 ||| 33550 ||| 8260 ||| 
2018 ||| a two-level attention-based interaction model for multi-person activity recognition. ||| 22574 ||| 22572 ||| 4151 ||| 22573 ||| 6459 ||| 
2021 ||| visual affordance detection using an efficient attention convolutional neural network. ||| 21637 ||| 21638 ||| 12969 ||| 
2022 ||| instance-level context attention network for instance segmentation. ||| 805 ||| 7778 ||| 7780 ||| 44580 ||| 7779 ||| 7781 ||| 7782 ||| 
2019 ||| daa: dual lstms with adaptive attention for image captioning. ||| 16616 ||| 44581 ||| 4134 ||| 39562 ||| 5536 ||| 44582 ||| 
2021 ||| adversarial learning with collaborative attention for facial makeup removal. ||| 44583 ||| 1107 ||| 10075 ||| 
2020 ||| adaptive attention-aware network for unsupervised person re-identification. ||| 13750 ||| 31417 ||| 5278 ||| 41626 ||| 41627 ||| 
2021 ||| tsrgan: real-world text image super-resolution based on adversarial learning and triplet attention. ||| 44584 ||| 1107 ||| 44585 ||| 44586 ||| 
2021 ||| residual attention graph convolutional network for web services classification. ||| 8838 ||| 22351 ||| 8333 ||| 
2020 ||| polar coordinate sampling-based segmentation of overlapping cervical cells using attention u-net and random walk. ||| 14048 ||| 41745 ||| 44586 ||| 
2021 ||| attention-based sequence to sequence model for machine remaining useful life prediction. ||| 38299 ||| 17993 ||| 16597 ||| 16599 ||| 29157 ||| 3488 ||| 
2021 ||| crowd counting based on attention-guided multi-scale fusion networks. ||| 1099 ||| 44587 ||| 44588 ||| 31584 ||| 44589 ||| 
2022 ||| stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms. ||| 44532 ||| 44533 ||| 42948 ||| 42949 ||| 
2020 ||| deep multi-view residual attention network for crowd flows prediction. ||| 6928 ||| 1150 ||| 31485 ||| 1151 ||| 
2021 ||| a review on the attention mechanism of deep learning. ||| 44590 ||| 2817 ||| 13435 ||| 
2021 ||| d-mmt: a concise decoder-only multi-modal transformer for abstractive summarization in videos. ||| 44591 ||| 10525 ||| 44592 ||| 19850 ||| 21132 ||| 
2019 ||| hybrid attention for chinese character-level neural machine translation. ||| 6832 ||| 5110 ||| 5937 ||| 5270 ||| 728 ||| 
2017 ||| the reward-attention circuit model: nicotine's influence on attentional focus and consequences on attention deficit hyperactivity disorder. ||| 44593 ||| 227 ||| 44594 ||| 44595 ||| 
2018 ||| period-aware content attention rnns for time series forecasting with missing values. ||| 5127 ||| 5128 ||| 5129 ||| 5130 ||| 5131 ||| 5132 ||| 
2020 ||| deep attention user-based collaborative filtering for recommendation. ||| 1037 ||| 44596 ||| 25715 ||| 44339 ||| 25716 ||| 
2020 ||| position-aware context attention for session-based recommendation. ||| 1272 ||| 1273 ||| 1274 ||| 21243 ||| 1275 ||| 
2021 ||| automatic fluid segmentation in retinal optical coherence tomography images using attention based deep learning. ||| 10405 ||| 44597 ||| 4646 ||| 8709 ||| 6595 ||| 
2021 ||| iae-clustergan: a new inverse autoencoder for generative adversarial attention clustering network. ||| 44598 ||| 16747 ||| 16750 ||| 2614 ||| 44599 ||| 
2020 ||| att-moe: attention-based mixture of experts for nuclear and cytoplasmic segmentation. ||| 44600 ||| 15512 ||| 44601 ||| 
2021 ||| molecular graph enhanced transformer for retrosynthesis prediction. ||| 2743 ||| 2744 ||| 1262 ||| 1261 ||| 1265 ||| 9346 ||| 
2021 ||| exploiting vector attention and context prior for ultrasound image segmentation. ||| 42075 ||| 44602 ||| 44603 ||| 44604 ||| 28896 ||| 14909 ||| 30900 ||| 
2021 ||| twilbert: pre-trained deep bidirectional transformers for spanish twitter. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 16474 ||| 41689 ||| 16476 ||| 
2021 ||| lsi-lstm: an attention-aware lstm for real-time driving destination prediction by considering location semantics and location importance of trajectory points. ||| 44605 ||| 44606 ||| 23024 ||| 44607 ||| 44608 ||| 44609 ||| 44610 ||| 44611 ||| 44612 ||| 
2019 ||| aela-dlstms: attention-enabled and location-aware double lstms for aspect-level sentiment classification. ||| 8356 ||| 8357 ||| 44613 ||| 8207 ||| 8359 ||| 
2021 ||| unpaired salient object translation via spatial attention prior. ||| 44614 ||| 44615 ||| 3386 ||| 33769 ||| 19845 ||| 4297 ||| 
2021 ||| visual content-enhanced sequential recommendation with feature-level attention. ||| 44616 ||| 44617 ||| 18213 ||| 
2019 ||| ta-cnn: two-way attention models in deep convolutional neural network for plant recognition. ||| 42776 ||| 44618 ||| 44619 ||| 44620 ||| 44621 ||| 25908 ||| 42778 ||| 
2021 ||| a recurrent video quality enhancement framework with multi-granularity frame-fusion and frame difference based attention. ||| 44622 ||| 44623 ||| 44624 ||| 19755 ||| 
2020 ||| human action recognition using convolutional lstm and fully-connected lstm with different attentions. ||| 28698 ||| 44625 ||| 28695 ||| 28699 ||| 
2022 ||| hybrid attention-based transformer block model for distant supervision relation extraction. ||| 34625 ||| 34626 ||| 25504 ||| 34627 ||| 
2021 ||| exploring multi-scale deformable context and channel-wise attention for salient object detection. ||| 1199 ||| 44626 ||| 44627 ||| 44628 ||| 44629 ||| 3034 ||| 817 ||| 
2018 ||| textual sentiment analysis via three different attention convolutional neural networks and cross-modality consistent regression. ||| 44630 ||| 9243 ||| 28695 ||| 
2021 ||| learning graph attention-aware knowledge graph embedding. ||| 399 ||| 33715 ||| 44631 ||| 18005 ||| 9407 ||| 718 ||| 215 ||| 
2020 ||| intelligent prognostics of machining tools based on adaptive variational mode decomposition and deep learning method with attention mechanism. ||| 13745 ||| 13747 ||| 44632 ||| 13746 ||| 28378 ||| 
2020 ||| dynamic attention network for semantic segmentation. ||| 2258 ||| 17396 ||| 6633 ||| 43947 ||| 44633 ||| 44634 ||| 
2018 ||| feature-enhanced attention network for target-dependent sentiment classification. ||| 1081 ||| 9677 ||| 986 ||| 44635 ||| 1082 ||| 18181 ||| 
2021 ||| graph transformer networks based text representation. ||| 29551 ||| 29552 ||| 29553 ||| 29554 ||| 
2021 ||| review-based hierarchical attention cooperative neural networks for recommendation. ||| 40498 ||| 44636 ||| 40499 ||| 44637 ||| 
2022 ||| combining dynamic local context focus and dependency cluster attention for aspect-level sentiment classification. ||| 44638 ||| 44639 ||| 44640 ||| 44641 ||| 44642 ||| 38206 ||| 
2019 ||| electrode regulating system modeling in electrical smelting furnace using recurrent neural network with attention mechanism. ||| 29340 ||| 29339 ||| 44643 ||| 44644 ||| 29338 ||| 
2021 ||| efficient attention based deep fusion cnn for smoke detection in fog environment. ||| 23028 ||| 23025 ||| 44645 ||| 4719 ||| 2755 ||| 
2021 ||| context-aware self-attention networks for natural language processing. ||| 3037 ||| 3038 ||| 3039 ||| 4882 ||| 3041 ||| 
2020 ||| tean: timeliness enhanced attention network for session-based recommendation. ||| 44646 ||| 44647 ||| 44648 ||| 44649 ||| 
2019 ||| densely convolutional attention network for image super-resolution. ||| 44650 ||| 32419 ||| 32417 ||| 44651 ||| 40042 ||| 
2020 ||| fine-grained vehicle type detection and recognition based on dense attention network. ||| 44652 ||| 44653 ||| 
2021 ||| enhancing two-view correspondence learning by local-global self-attention. ||| 44654 ||| 189 ||| 44655 ||| 44656 ||| 44657 ||| 44658 ||| 44659 ||| 
2021 ||| csart: channel and spatial attention-guided residual learning for real-time object tracking. ||| 11236 ||| 11235 ||| 11238 ||| 44660 ||| 
2020 ||| scale-aware spatial pyramid pooling with both encoder-mask and scale-attention for semantic segmentation. ||| 6924 ||| 23206 ||| 23207 ||| 
2020 ||| attention mechanism-based cnn for facial expression recognition. ||| 4807 ||| 44661 ||| 12816 ||| 23188 ||| 9440 ||| 
2020 ||| fixed pattern noise reduction for infrared images based on cascade residual attention cnn. ||| 39570 ||| 39571 ||| 39572 ||| 474 ||| 35073 ||| 
2022 ||| tcct: tightly-coupled convolutional transformer on time series forecasting. ||| 30312 ||| 32234 ||| 
2021 ||| wrtre: weighted relative position transformer for joint entity and relation extraction. ||| 16828 ||| 1419 ||| 44662 ||| 6922 ||| 
2020 ||| aggregating diverse deep attention networks for large-scale plant species identification. ||| 44663 ||| 44664 ||| 44665 ||| 44666 ||| 30201 ||| 1755 ||| 
2020 ||| micro-attention for micro-expression recognition. ||| 5663 ||| 3626 ||| 39323 ||| 5664 ||| 
2020 ||| a model with length-variable attention for spoken language understanding. ||| 1128 ||| 3477 ||| 31138 ||| 44667 ||| 44668 ||| 2736 ||| 
2020 ||| combining attention-based bidirectional gated recurrent neural network and two-dimensional convolutional neural network for document-level sentiment classification. ||| 28721 ||| 44669 ||| 44670 ||| 10980 ||| 
2018 ||| learning better discourse representation for implicit discourse relation recognition via attention networks. ||| 3180 ||| 3181 ||| 3182 ||| 1254 ||| 
2020 ||| robust visual tracking with channel attention and focal loss. ||| 29491 ||| 40358 ||| 40359 ||| 44671 ||| 7408 ||| 
2017 ||| toward an audiovisual attention model for multimodal video content. ||| 44672 ||| 41861 ||| 44673 ||| 
2022 ||| heterogeneous graph embedding by aggregating meta-path and meta-structure through attention mechanism. ||| 41034 ||| 6044 ||| 6043 ||| 
2019 ||| densely connected attentional pyramid residual network for human pose estimation. ||| 29275 ||| 6595 ||| 44674 ||| 44675 ||| 
2019 ||| social recommendation based on users' attention and preference. ||| 1060 ||| 13181 ||| 44676 ||| 6729 ||| 31428 ||| 
2021 ||| aspect term extraction for opinion mining using a hierarchical self-attention network. ||| 44677 ||| 44678 ||| 44679 ||| 44680 ||| 44681 ||| 44682 ||| 
2021 ||| gated pe-nl-ma: a multi-modal attention based network for video understanding. ||| 44683 ||| 44684 ||| 
2021 ||| jwsaa: joint weak saliency and attention aware for person re-identification. ||| 40031 ||| 44685 ||| 44686 ||| 30190 ||| 
2021 ||| summary-aware attention for social media short text abstractive summarization. ||| 520 ||| 521 ||| 
2018 ||| salient object detection via multi-scale attention cnn. ||| 31721 ||| 31722 ||| 28491 ||| 
2020 ||| occluded offline handwritten chinese character inpainting via generative adversarial network and self-attention mechanism. ||| 35237 ||| 33204 ||| 369 ||| 
2020 ||| infrared head pose estimation with multi-scales feature fusion on the irhp database for human attention recognition. ||| 4079 ||| 1894 ||| 781 ||| 44687 ||| 25496 ||| 
2021 ||| multi-scale attention u-net for segmenting clinical target volume in graves' ophthalmopathy. ||| 25532 ||| 23502 ||| 241 ||| 44688 ||| 31205 ||| 
2020 ||| a hierarchical temporal attention-based lstm encoder-decoder model for individual mobility prediction. ||| 44608 ||| 44605 ||| 44689 ||| 44607 ||| 44690 ||| 44691 ||| 44606 ||| 44609 ||| 44612 ||| 44692 ||| 
2022 ||| multi-agent reinforcement learning by the actor-critic model with an attention interface. ||| 44693 ||| 44694 ||| 44695 ||| 44696 ||| 4041 ||| 
2017 ||| integration of touch attention mechanisms to improve the robotic haptic exploration of surfaces. ||| 20529 ||| 1994 ||| 44697 ||| 44698 ||| 44699 ||| 
2020 ||| face sketch-to-photo transformation with multi-scale self-attention gan. ||| 44700 ||| 44701 ||| 5077 ||| 
2021 ||| soft sensor based on extreme gradient boosting and bidirectional converted gates long short-term memory self-attention network. ||| 44702 ||| 34627 ||| 44703 ||| 44704 ||| 
2020 ||| adaptive embedding gate for attention-based scene text recognition. ||| 17682 ||| 17680 ||| 17681 ||| 6559 ||| 17418 ||| 
2021 ||| attention-based full slice brain ct image diagnosis with explanations. ||| 44705 ||| 17433 ||| 44706 ||| 21897 ||| 44707 ||| 
2018 ||| fine-grained attention mechanism for neural machine translation. ||| 36068 ||| 3008 ||| 9196 ||| 
2022 ||| multi actor hierarchical attention critic with rnn-based feature extraction. ||| 24567 ||| 44708 ||| 24566 ||| 25827 ||| 24569 ||| 15823 ||| 24568 ||| 41330 ||| 5082 ||| 
2020 ||| abstractive social media text summarization using selective reinforced seq2seq attention model. ||| 44535 ||| 7688 ||| 44709 ||| 
2020 ||| a two-stage attention aware method for train bearing shed oil inspection based on convolutional neural networks. ||| 44710 ||| 29149 ||| 2058 ||| 29376 ||| 11514 ||| 1242 ||| 
2020 ||| directional attention weaving for text-grounded conversational question answering. ||| 40543 ||| 40544 ||| 11804 ||| 40545 ||| 29902 ||| 
2019 ||| scar: spatial-/channel-wise attention regression networks for crowd counting. ||| 31724 ||| 6627 ||| 6650 ||| 
2017 ||| salient region detection via locally smoothed label propagation: with application to attention driven image abstraction. ||| 42277 ||| 32700 ||| 44711 ||| 
2020 ||| attention-guided dual spatial-temporal non-local network for video super-resolution. ||| 4217 ||| 10075 ||| 
2021 ||| a transfer approach with attention reptile method and long-term generation mechanism for few-shot traffic prediction. ||| 31484 ||| 1150 ||| 31485 ||| 24048 ||| 
2021 ||| exploiting dependency information to improve biomedical event detection via gated polar attention mechanism. ||| 16631 ||| 44712 ||| 
2021 ||| filter gate network based on multi-head attention for aspect-level sentiment classification. ||| 44713 ||| 9082 ||| 
2022 ||| plam: a plug-in module for flexible graph attention learning. ||| 30237 ||| 35048 ||| 29459 ||| 4719 ||| 7875 ||| 
2022 ||| joint usage of global and local attentions in hourglass network for human pose estimation. ||| 44714 ||| 1754 ||| 12196 ||| 
2019 ||| multi-resolution attention convolutional neural network for crowd counting. ||| 17566 ||| 17567 ||| 17568 ||| 11620 ||| 
2021 ||| automatic depression recognition using cnn with attention mechanism from videos. ||| 44715 ||| 30529 ||| 44716 ||| 
2021 ||| contextual similarity-based multi-level second-order attention network for semi-supervised few-shot learning. ||| 44717 ||| 36432 ||| 706 ||| 1796 ||| 44718 ||| 
2020 ||| adcm: attention dropout convolutional module. ||| 44719 ||| 2853 ||| 6145 ||| 33550 ||| 
2020 ||| stochastic region pooling: make attention more expressive. ||| 29557 ||| 29558 ||| 29559 ||| 29560 ||| 38169 ||| 
2021 ||| micro-expression action unit detection with spatial and channel attention. ||| 44720 ||| 34686 ||| 33480 ||| 
2021 ||| streamer action recognition in live video with spatial-temporal attention and deep dictionary learning. ||| 28785 ||| 875 ||| 44721 ||| 
2020 ||| question-led object attention for visual question answering. ||| 1039 ||| 25484 ||| 9579 ||| 155 ||| 9576 ||| 
2018 ||| boosting image sentiment analysis with visual attention. ||| 44722 ||| 19117 ||| 44723 ||| 2165 ||| 
2020 ||| xnet: task-specific attentional domain adaptation for satellite-to-aerial scene. ||| 27413 ||| 44724 ||| 2189 ||| 44725 ||| 
2021 ||| target relational attention-oriented knowledge graph reasoning. ||| 44726 ||| 25764 ||| 10833 ||| 44727 ||| 472 ||| 6718 ||| 
2020 ||| short-term traffic speed forecasting based on graph attention temporal convolutional networks. ||| 28242 ||| 30636 ||| 
2021 ||| mask-guided contrastive attention and two-stream metric co-learning for person re-identification. ||| 6147 ||| 44728 ||| 4648 ||| 10429 ||| 
2022 ||| non-tumorous facial pigmentation classification based on multi-view convolutional neural network with attention mechanism. ||| 28857 ||| 848 ||| 28858 ||| 5439 ||| 846 ||| 
2020 ||| unsupervised depth estimation from monocular videos with hybrid geometric-refined loss and contextual attention. ||| 19936 ||| 19935 ||| 8482 ||| 11962 ||| 
2020 ||| a non-parametric softmax for improving neural attention in time-series forecasting. ||| 44729 ||| 30475 ||| 30330 ||| 
2022 ||| remove and recover: deep end-to-end two-stage attention network for single-shot heavy rain removal. ||| 44730 ||| 44731 ||| 44732 ||| 43158 ||| 
2021 ||| visual relationship detection with recurrent attention and negative sampling. ||| 3279 ||| 44733 ||| 15003 ||| 968 ||| 44734 ||| 24823 ||| 
2021 ||| deep saliency detection via spatial-wise dilated convolutional attention. ||| 44735 ||| 6645 ||| 44736 ||| 
2021 ||| exploring attention mechanisms based on summary information for end-to-end automatic speech recognition. ||| 12105 ||| 12106 ||| 12107 ||| 
2019 ||| optical flow estimation using channel attention mechanism and dilated convolutional neural networks. ||| 11590 ||| 11589 ||| 11591 ||| 6927 ||| 11592 ||| 
2020 ||| crowd counting with crowd attention convolutional neural network. ||| 40091 ||| 44737 ||| 42203 ||| 
2021 ||| attention based convolutional recurrent neural network for environmental sound classification. ||| 6563 ||| 6564 ||| 6566 ||| 6565 ||| 6567 ||| 
2022 ||| ccaffmnet: dual-spectral semantic segmentation network with channel-coordinate attention feature fusion module. ||| 44738 ||| 14307 ||| 19209 ||| 44739 ||| 
2022 ||| progressively real-time video salient object detection via cascaded fully convolutional networks with motion attention. ||| 44740 ||| 949 ||| 29608 ||| 36426 ||| 
2022 |||  attention network for video-based person re-identification. ||| 837 ||| 30779 ||| 19869 ||| 44741 ||| 30780 ||| 
2022 ||| enhancing structure modeling for relation extraction with fine-grained gating and co-attention. ||| 3128 ||| 3754 ||| 2795 ||| 
2019 ||| vd-san: visual-densely semantic attention network for image caption generation. ||| 38071 ||| 11466 ||| 18898 ||| 17429 ||| 
2022 ||| mpan: multi-parallel attention network for session-based recommendation. ||| 44742 ||| 11190 ||| 11188 ||| 11189 ||| 24842 ||| 
2022 ||| segdq: segmentation assisted multi-object tracking with dynamic query-based transformers. ||| 44743 ||| 44744 ||| 44745 ||| 44746 ||| 44747 ||| 5957 ||| 11354 ||| 
2021 ||| attention-aware concentrated network for saliency prediction. ||| 44748 ||| 4480 ||| 4482 ||| 18842 ||| 15003 ||| 
2021 ||| a novel transferability attention neural network model for eeg emotion recognition. ||| 438 ||| 37267 ||| 136 ||| 21748 ||| 540 ||| 
2021 ||| bidirectional gated temporal convolution with attention for text classification. ||| 44749 ||| 293 ||| 14570 ||| 5790 ||| 44750 ||| 
2018 ||| image captioning with triple-attention and stack parallel lstm. ||| 11392 ||| 44751 ||| 2058 ||| 44752 ||| 44753 ||| 44754 ||| 
2020 ||| scene image retrieval with siamese spatial attention pooling. ||| 44755 ||| 737 ||| 
2021 ||| detector-tracker integration framework and attention mechanism for multi-object tracking. ||| 44756 ||| 44757 ||| 44758 ||| 44759 ||| 
2018 ||| attention based collaborative filtering. ||| 44760 ||| 13783 ||| 44761 ||| 12003 ||| 
2021 ||| da-dsunet: dual attention-based dense su-net for automatic head-and-neck tumor segmentation in mri images. ||| 44762 ||| 27721 ||| 44763 ||| 10233 ||| 40493 ||| 40494 ||| 25649 ||| 25648 ||| 27723 ||| 247 ||| 
2021 ||| on the diversity of multi-head attention. ||| 595 ||| 3309 ||| 3041 ||| 597 ||| 
2020 ||| portfolio trading system of digital currencies: a deep reinforcement learning with multidimensional attention gating mechanism. ||| 29874 ||| 42455 ||| 41631 ||| 10107 ||| 41633 ||| 
2017 ||| emotion-modulated attention improves expression recognition: a deep learning model. ||| 2656 ||| 2655 ||| 14416 ||| 1017 ||| 
2021 ||| incorporating sentimental trend into gated mechanism based transformer network for story ending generation. ||| 44764 ||| 44765 ||| 40640 ||| 11156 ||| 44766 ||| 44767 ||| 3477 ||| 
2020 ||| attention-based face alignment: a solution to speed/accuracy trade-off. ||| 30559 ||| 44768 ||| 44769 ||| 
2019 ||| single image super-resolution via multi-scale residual channel attention network. ||| 29587 ||| 5791 ||| 
2022 ||| acort: a compact object relation transformer for parameter efficient image captioning. ||| 33863 ||| 35038 ||| 7852 ||| 27311 ||| 
2021 ||| accelerated masked transformer for dense video captioning. ||| 1753 ||| 44770 ||| 
2020 ||| food det: detecting foods in refrigerator with supervised transformer network. ||| 19481 ||| 7662 ||| 11290 ||| 2077 ||| 2080 ||| 
2020 ||| the scale-invariant space for attention layer in neural network. ||| 7400 ||| 44771 ||| 44772 ||| 
2021 ||| long- and short-term self-attention network for sequential recommendation. ||| 23316 ||| 23793 ||| 659 ||| 3476 ||| 18142 ||| 9588 ||| 6475 ||| 
2020 ||| sarpnet: shape attention regional proposal network for lidar-based 3d object detection. ||| 44773 ||| 31005 ||| 1460 ||| 44774 ||| 18260 ||| 
2021 ||| residual attention and other aspects module for aspect-based sentiment analysis. ||| 25100 ||| 44775 ||| 44776 ||| 4213 ||| 44777 ||| 882 ||| 44778 ||| 44779 ||| 
2022 ||| scan: a shared causal attention network for adverse drug reactions detection in tweets. ||| 44780 ||| 38160 ||| 44781 ||| 44782 ||| 7436 ||| 
2020 ||| aspect-based sentiment classification with multi-attention network. ||| 44783 ||| 41692 ||| 7895 ||| 44784 ||| 
2020 ||| modeling bottom-up and top-down attention with a neurodynamic model of v1. ||| 1864 ||| 1869 ||| 
2020 ||| fused gru with semantic-temporal attention for video captioning. ||| 1039 ||| 44785 ||| 9576 ||| 1305 ||| 
2019 ||| a hierarchical attention model for rating prediction by leveraging user and product reviews. ||| 40279 ||| 9082 ||| 31519 ||| 40280 ||| 44786 ||| 
2019 ||| eeg model stability and online decoding of attentional demand during gait using gamma band features. ||| 44787 ||| 3419 ||| 44788 ||| 10318 ||| 44789 ||| 44790 ||| 852 ||| 25037 ||| 44791 ||| 3882 ||| 
2019 ||| self-attention based speaker recognition using cluster-range loss. ||| 44792 ||| 44793 ||| 44794 ||| 
2021 ||| self-attention feature fusion network for semantic segmentation. ||| 16317 ||| 9018 ||| 21960 ||| 42268 ||| 44795 ||| 
2020 ||| tag recommendation by text classification with attention-based capsule network. ||| 18181 ||| 44796 ||| 1081 ||| 44797 ||| 
2021 ||| self-supervised video object segmentation using integration-augmented attention. ||| 44798 ||| 16872 ||| 44794 ||| 
2020 ||| saanet: siamese action-units attention network for improving dynamic facial expression recognition. ||| 19587 ||| 32718 ||| 31937 ||| 12300 ||| 44799 ||| 44800 ||| 
2021 ||| label-guided attention distillation for lane segmentation. ||| 11477 ||| 32323 ||| 
2021 ||| adaptive multi-level feature fusion and attention-based network for arbitrary-oriented object detection in remote sensing imagery. ||| 44801 ||| 44802 ||| 17568 ||| 2332 ||| 44803 ||| 
2021 ||| enhancing social recommendation via two-level graph attentional networks. ||| 261 ||| 262 ||| 263 ||| 264 ||| 10800 ||| 
2021 ||| single image super-resolution with attention-based densely connected module. ||| 5166 ||| 4151 ||| 6610 ||| 6459 ||| 6611 ||| 6612 ||| 
2020 ||| learning from discrete gaussian label distribution and spatial channel-aware residual attention for head pose estimation. ||| 340 ||| 42645 ||| 18111 ||| 15367 ||| 
2022 ||| ecanet: explicit cyclic attention-based network for video saliency prediction. ||| 7322 ||| 44804 ||| 44805 ||| 
2021 ||| context-aware positional representation for self-attention networks. ||| 3112 ||| 3049 ||| 4907 ||| 4908 ||| 
2020 ||| traffic scene semantic segmentation using self-attention mechanism and bi-directional gru to correlate context. ||| 44806 ||| 44807 ||| 4807 ||| 19919 ||| 44808 ||| 
2022 ||| co-attention network with label embedding for text classification. ||| 44809 ||| 44810 ||| 44811 ||| 38239 ||| 
2020 ||| gssa: pay attention to graph feature importance for gcn via statistical self-attention. ||| 44812 ||| 602 ||| 44813 ||| 44814 ||| 977 ||| 907 ||| 
2018 ||| lattice-to-sequence attentional neural machine translation models. ||| 11746 ||| 3182 ||| 44815 ||| 18155 ||| 18156 ||| 
2021 ||| local and correlation attention learning for subtle facial expression recognition. ||| 6649 ||| 6650 ||| 42109 ||| 8002 ||| 
2021 ||| multi-level dictionary learning for fine-grained images categorization with attention model. ||| 6764 ||| 44816 ||| 5937 ||| 6514 ||| 19316 ||| 
2021 ||| a method of traffic police detection based on attention mechanism in natural scene. ||| 25208 ||| 44817 ||| 44818 ||| 44819 ||| 
2022 ||| multiscale face recognition in cluttered backgrounds based on visual attention. ||| 6286 ||| 44820 ||| 44821 ||| 44822 ||| 44823 ||| 1899 ||| 3503 ||| 44824 ||| 44825 ||| 
2020 ||| two-branch fusion network with attention map for crowd counting. ||| 8310 ||| 781 ||| 44826 ||| 44827 ||| 
2021 ||| gapointnet: graph attention based point neural network for exploiting local feature of point cloud. ||| 36661 ||| 36662 ||| 36663 ||| 
2020 ||| bilstm with multi-polarity orthogonal attention for implicit sentiment analysis. ||| 44828 ||| 44829 ||| 44830 ||| 31956 ||| 44831 ||| 
2022 ||| kaicd: a knowledge attention-based deep learning framework for automatic icd coding. ||| 27551 ||| 38226 ||| 31173 ||| 5253 ||| 16687 ||| 12646 ||| 
2020 ||| attention-based bidirectional gated recurrent unit neural networks for well logs prediction and lithology identification. ||| 44832 ||| 44833 ||| 44834 ||| 
2020 ||| deep attention based music genre classification. ||| 882 ||| 44835 ||| 5168 ||| 44836 ||| 1305 ||| 5172 ||| 
2020 ||| alstm: an attention-based long short-term memory framework for knowledge base reasoning. ||| 6627 ||| 40635 ||| 
2020 ||| aplnet: attention-enhanced progressive learning network. ||| 4600 ||| 44837 ||| 44838 ||| 11354 ||| 
2021 ||| residual attention-based multi-scale script identification in scene text images. ||| 44839 ||| 44840 ||| 15534 ||| 12368 ||| 44841 ||| 4776 ||| 
2021 ||| element graph-augmented abstractive summarization for legal public opinion news with graph transformer. ||| 28286 ||| 2950 ||| 2952 ||| 44842 ||| 44843 ||| 
2021 ||| attention-aware conditional generative adversarial networks for facial age synthesis. ||| 44844 ||| 13415 ||| 30753 ||| 6721 ||| 
2021 ||| towards accurate rgb-d saliency detection with complementary attention and adaptive integration. ||| 31460 ||| 39909 ||| 27820 ||| 17092 ||| 4055 ||| 423 ||| 
2019 ||| end-to-end semantic-aware object retrieval based on region-wise attention. ||| 2527 ||| 44845 ||| 44846 ||| 
2021 ||| automatic segmentation of gross target volume of nasopharynx cancer using ensemble of multiscale deep neural networks with spatial attention. ||| 38310 ||| 15622 ||| 27343 ||| 38311 ||| 38312 ||| 38313 ||| 15623 ||| 
2021 ||| lrdnet: a lightweight and efficient network with refined dual attention decorder for real-time semantic segmentation. ||| 44847 ||| 44848 ||| 25727 ||| 44849 ||| 44850 ||| 28352 ||| 
2021 ||| attm-cnn: attention and metric learning based cnn for pornography, age and child sexual abuse (csa) detection in images. ||| 44851 ||| 2253 ||| 30729 ||| 30730 ||| 30728 ||| 30727 ||| 
2020 ||| radc-net: a residual attention based convolution network for aerial scene classification. ||| 27337 ||| 42468 ||| 14048 ||| 42470 ||| 5723 ||| 
2022 ||| skeleton-based abnormal gait recognition with spatio-temporal attention enhanced gait-structural graph convolutional networks. ||| 44852 ||| 9442 ||| 42650 ||| 44853 ||| 
2021 ||| daeanet: dual auto-encoder attention network for depth map super-resolution. ||| 44854 ||| 44855 ||| 44856 ||| 44857 ||| 2377 ||| 44858 ||| 44859 ||| 11225 ||| 
2022 ||| ganlda: graph attention network for lncrna-disease associations prediction. ||| 44860 ||| 44861 ||| 44862 ||| 21414 ||| 1130 ||| 524 ||| 
2018 ||| deep transformer: a framework for 2d text image rectification from planar transformations. ||| 44863 ||| 38483 ||| 12748 ||| 
2020 ||| joint multi-level attentional model for emotion detection and emotion-cause pair extraction. ||| 435 ||| 3725 ||| 3726 ||| 
2021 ||| adaptive attention augmentor for weakly supervised object localization. ||| 44864 ||| 447 ||| 
2021 ||| ttpp: temporal transformer with progressive prediction for efficient action anticipation. ||| 8948 ||| 8769 ||| 6830 ||| 2149 ||| 2343 ||| 
2019 ||| dual residual attention module network for single image super resolution. ||| 6620 ||| 44865 ||| 6621 ||| 6622 ||| 
2019 ||| recurrent convolutional video captioning with global and local attention. ||| 14156 ||| 22668 ||| 17724 ||| 
2022 ||| point cloud recognition based on lightweight embeddable attention module. ||| 44866 ||| 6496 ||| 6497 ||| 1828 ||| 42477 ||| 
2020 ||| joint image deblurring and super-resolution with attention dual supervised network. ||| 19794 ||| 19795 ||| 155 ||| 
2021 ||| mafnet: multi-style attention fusion network for salient object detection. ||| 44805 ||| 44867 ||| 44804 ||| 12371 ||| 44868 ||| 
2020 ||| unsupervised domain adaptation with self-attention for post-disaster building damage detection. ||| 44869 ||| 2389 ||| 40559 ||| 6595 ||| 44870 ||| 1199 ||| 
2020 ||| a holistic representation guided attention network for scene text recognition. ||| 12169 ||| 5845 ||| 4175 ||| 5189 ||| 10075 ||| 
2021 ||| palmprint orientation field recovery via attention-based generative adversarial network. ||| 3072 ||| 44871 ||| 
2022 ||| hierarchical multimodal transformer to summarize videos. ||| 22078 ||| 34992 ||| 6922 ||| 
2020 ||| dcgsa: a global self-attention network with dilated convolution for crowd density map generating. ||| 12214 ||| 12213 ||| 1780 ||| 2496 ||| 44872 ||| 
2021 ||| on combining acoustic and modulation spectrograms in an attention lstm-based system for speech intelligibility level classification. ||| 14389 ||| 14390 ||| 3882 ||| 14391 ||| 
2021 ||| multi-scale graph attention subspace clustering network. ||| 6669 ||| 4090 ||| 44873 ||| 44874 ||| 1382 ||| 44875 ||| 
2021 ||| attention based multilayer feature fusion convolutional neural network for unsupervised monocular depth estimation. ||| 44876 ||| 247 ||| 44877 ||| 44878 ||| 
2021 ||| ast-gnn: an attention-based spatio-temporal graph neural network for interaction-aware pedestrian trajectory prediction. ||| 2736 ||| 5154 ||| 5155 ||| 5156 ||| 5157 ||| 5158 ||| 
2022 ||| automatic and accurate segmentation of peripherally inserted central catheter (picc) from chest x-rays using multi-stage attention-guided learning. ||| 13818 ||| 16642 ||| 44879 ||| 44880 ||| 35885 ||| 5030 ||| 16645 ||| 44881 ||| 8775 ||| 16644 ||| 44882 ||| 44883 ||| 
2021 ||| residual scale attention network for arbitrary scale image super-resolution. ||| 1714 ||| 9375 ||| 6514 ||| 44884 ||| 
2020 ||| exploiting geographical-temporal awareness attention for next point-of-interest recommendation. ||| 23877 ||| 6191 ||| 23878 ||| 5039 ||| 6187 ||| 
2021 ||| fusion layer attention for image-text matching. ||| 44885 ||| 4719 ||| 35048 ||| 7875 ||| 10067 ||| 12637 ||| 44886 ||| 12638 ||| 
2020 ||| tnam: a tag-aware neural attention model for top-n recommendation. ||| 670 ||| 31429 ||| 671 ||| 21560 ||| 672 ||| 
2017 ||| attention region detection based on closure prior in layered bit planes. ||| 976 ||| 9378 ||| 44887 ||| 1037 ||| 44888 ||| 44889 ||| 5204 ||| 
2020 ||| cross-modal image fusion guided by subjective visual attention. ||| 33619 ||| 10058 ||| 10075 ||| 
2021 ||| a comparative study of language transformers for video question answering. ||| 44890 ||| 3344 ||| 3346 ||| 3345 ||| 1876 ||| 44891 ||| 
2020 ||| multivariate time series forecasting via attention-based encoder-decoder framework. ||| 980 ||| 979 ||| 3666 ||| 44892 ||| 
2019 ||| topical co-attention networks for hashtag recommendation on microblogs. ||| 438 ||| 3311 ||| 44893 ||| 800 ||| 
2021 ||| anisotropic angle distribution learning for head pose estimation and attention understanding in human-computer interaction. ||| 4079 ||| 44894 ||| 44687 ||| 25496 ||| 
2020 ||| attention augmentation with multi-residual in bidirectional lstm. ||| 6718 ||| 12274 ||| 12277 ||| 12273 ||| 12276 ||| 
2020 ||| multistage attention network for multivariate time series prediction. ||| 844 ||| 1213 ||| 
2021 ||| scsa-net: presentation of two-view reliable correspondence learning via spatial-channel self-attention. ||| 189 ||| 40432 ||| 44654 ||| 44895 ||| 44656 ||| 44659 ||| 
2021 ||| harmonious attention network for person re-identification via complementarity between groups and individuals. ||| 6469 ||| 1007 ||| 6468 ||| 44896 ||| 
2022 ||| towards more effective prm-based crowd counting via a multi-resolution fusion and attention network. ||| 388 ||| 392 ||| 
2021 ||| a lightweight multi-scale channel attention network for image super-resolution. ||| 29460 ||| 44897 ||| 29461 ||| 29592 ||| 29458 ||| 
2021 ||| pairwise attention network for cross-domain image recognition. ||| 13208 ||| 44898 ||| 44899 ||| 571 ||| 
2020 ||| multi-attention guided feature fusion network for salient object detection. ||| 44900 ||| 2038 ||| 1700 ||| 
2020 ||| spatial attention based visual semantic learning for action recognition in still images. ||| 44901 ||| 42109 ||| 8002 ||| 7655 ||| 
2019 ||| extension of reward-attention circuit model: alcohol's influence on attentional focus and consequences on autism spectrum disorder. ||| 44593 ||| 227 ||| 
2020 ||| multi-attention deep reinforcement learning and re-ranking for vehicle re-identification. ||| 16550 ||| 2445 ||| 44838 ||| 
2021 ||| automatic ultrasound image report generation with adaptive multimodal attention mechanism. ||| 22963 ||| 2478 ||| 22964 ||| 16446 ||| 2479 ||| 27741 ||| 
2020 ||| epan: effective parts attention network for scene text recognition. ||| 17417 ||| 44902 ||| 6559 ||| 17418 ||| 
2021 ||| facial image inpainting using attention-based multi-level generative network. ||| 3114 ||| 2835 ||| 
2020 ||| wavelet-based residual attention network for image super-resolution. ||| 44903 ||| 44904 ||| 295 ||| 31025 ||| 
2021 ||| multi-view spectral graph convolution with consistent edge attention for molecular modeling. ||| 805 ||| 44905 ||| 27919 ||| 44906 ||| 44907 ||| 806 ||| 
2020 ||| recurrent reverse attention guided residual learning for saliency object detection. ||| 19238 ||| 44908 ||| 19237 ||| 6625 ||| 
2022 ||| adapted transformer network for news recommendation. ||| 44909 ||| 44910 ||| 9599 ||| 4166 ||| 
2020 ||| a trajectory-based attention model for sequential impurity detection. ||| 19912 ||| 24143 ||| 19910 ||| 44911 ||| 44912 ||| 44913 ||| 
2021 ||| adaptive multi-scale dual attention network for semantic segmentation. ||| 44914 ||| 44915 ||| 6898 ||| 44916 ||| 
2020 ||| sa-nli: a supervised attention based framework for natural language inference. ||| 10524 ||| 44592 ||| 19850 ||| 21132 ||| 10525 ||| 
2022 ||| jac-net: joint learning with adaptive exploration and concise attention for unsupervised domain adaptive person re-identification. ||| 29119 ||| 44917 ||| 44918 ||| 5250 ||| 
2021 ||| self-supervised attention flow for dialogue state tracking. ||| 44919 ||| 44920 ||| 1717 ||| 1115 ||| 
2021 ||| environment sound classification using an attention-based residual neural network. ||| 991 ||| 20320 ||| 
2021 ||| attention-based interpolation network for video deblurring. ||| 28779 ||| 28825 ||| 128 ||| 17217 ||| 17104 ||| 
2021 ||| self-attention guided representation learning for image-text matching. ||| 44921 ||| 4646 ||| 2038 ||| 1700 ||| 
2020 ||| point clouds learning with attention-based graph convolution networks. ||| 35331 ||| 35332 ||| 19855 ||| 
2020 ||| a window-based self-attention approach for sentence encoding. ||| 44922 ||| 38460 ||| 44923 ||| 5250 ||| 
2021 ||| single image haze removal via attention-based transmission estimation and classification fusion network. ||| 11501 ||| 11503 ||| 18157 ||| 
2021 ||| geometric attentional dynamic graph convolutional neural networks for point cloud analysis. ||| 3642 ||| 189 ||| 44924 ||| 28442 ||| 44925 ||| 2249 ||| 
2021 ||| spatiotemporal and frequential cascaded attention networks for speech emotion recognition. ||| 44926 ||| 4480 ||| 44927 ||| 18842 ||| 44928 ||| 4482 ||| 
2021 ||| mscan: multimodal self-and-collaborative attention network for image aesthetic prediction tasks. ||| 38080 ||| 6621 ||| 42653 ||| 32419 ||| 
2021 ||| predicting short-term next-active-object through visual attention and hand position. ||| 40110 ||| 15491 ||| 10064 ||| 15492 ||| 14779 ||| 
2021 ||| automatic whole slide pathology image diagnosis framework via unit stochastic selection and attention fusion. ||| 27493 ||| 16809 ||| 17961 ||| 17965 ||| 44929 ||| 
2022 ||| an accurate box localization method based on rotated-rpn with weighted edge attention for bin picking. ||| 44930 ||| 12828 ||| 8207 ||| 9570 ||| 6810 ||| 2628 ||| 
2022 ||| cephalometric landmark detection via global and local encoders and patch-wise attentions. ||| 44931 ||| 34518 ||| 34521 ||| 
2021 ||| attcry: attention-based neural network model for protein crystallization prediction. ||| 37728 ||| 44932 ||| 33797 ||| 14048 ||| 
2020 ||| dilated residual networks with multi-level attention for speaker verification. ||| 44933 ||| 10421 ||| 44934 ||| 4620 ||| 44935 ||| 
2020 ||| attention shake siamese network with auxiliary relocation branch for visual object tracking. ||| 1224 ||| 31404 ||| 31405 ||| 38670 ||| 11400 ||| 
2021 ||| joint structured pruning and dense knowledge distillation for efficient transformer model compression. ||| 26475 ||| 22668 ||| 17724 ||| 
2022 ||| global-guided asymmetric attention network for image-text matching. ||| 44936 ||| 31968 ||| 44937 ||| 15544 ||| 11023 ||| 
2022 ||| contrastive predictive coding with transformer for video representation learning. ||| 575 ||| 44938 ||| 9039 ||| 17665 ||| 44939 ||| 44940 ||| 1310 ||| 
2021 ||| attention-based label consistency for semi-supervised deep learning based image classification. ||| 5394 ||| 5395 ||| 44941 ||| 
2021 ||| a joint object detection and semantic segmentation model with cross-attention and inner-attention mechanisms. ||| 15491 ||| 44942 ||| 40110 ||| 10064 ||| 44943 ||| 24255 ||| 14779 ||| 
2020 ||| a cross-modal multi-granularity attention network for rgb-ir person re-identification. ||| 860 ||| 44944 ||| 35028 ||| 577 ||| 35027 ||| 40149 ||| 
2019 ||| structure-aware person search with self-attention and online instance aggregation matching. ||| 44945 ||| 1828 ||| 6497 ||| 6496 ||| 10072 ||| 7760 ||| 
2019 ||| bidirectional lstm with attention mechanism and convolutional layer for text classification. ||| 14570 ||| 44946 ||| 
2021 ||| video person re-identification with global statistic pooling and self-attention distillation. ||| 42633 ||| 6456 ||| 2445 ||| 
2021 ||| i2net: mining intra-video and inter-video attention for temporal action localization. ||| 781 ||| 6460 ||| 44947 ||| 6457 ||| 3266 ||| 
2021 ||| multi-object tracking with hard-soft attention network and group-based cost minimization. ||| 44743 ||| 27687 ||| 44744 ||| 44948 ||| 11354 ||| 
2020 ||| artan: align reviews with topics in attention network for rating prediction. ||| 11186 ||| 5244 ||| 11187 ||| 
2020 ||| triple attention network for video segmentation. ||| 29275 ||| 20779 ||| 14512 ||| 44949 ||| 44950 ||| 19437 ||| 
2021 ||| gsa-gan: global spatial attention generative adversarial networks. ||| 44951 ||| 44952 ||| 15854 ||| 
2021 ||| multiple perspective attention based on double bilstm for aspect and sentiment pair extract. ||| 44953 ||| 44829 ||| 438 ||| 31956 ||| 31957 ||| 3488 ||| 
2020 ||| attentional coarse-and-fine generative adversarial networks for image inpainting. ||| 44954 ||| 2740 ||| 18830 ||| 602 ||| 
2019 ||| a hierarchical contextual attention-based network for sequential recommendation. ||| 44955 ||| 1075 ||| 4648 ||| 10429 ||| 
2022 ||| a 2.5d semantic segmentation of the pancreas using attention guided dual context embedded u-net. ||| 23467 ||| 44956 ||| 44957 ||| 44958 ||| 44959 ||| 44960 ||| 44961 ||| 340 ||| 
2020 ||| self-constraining and attention-based hashing network for bit-scalable cross-modal retrieval. ||| 4598 ||| 40644 ||| 40646 ||| 5126 ||| 
2021 ||| tagattention: mobile object tracing with zero appearance knowledge by vision-rfid fusion. ||| 14158 ||| 28098 ||| 28096 ||| 8534 ||| 28097 ||| 28099 ||| 18226 ||| 
2022 ||| sasa: source-aware self-attention for ip hijack detection. ||| 44962 ||| 44963 ||| 
2017 ||| scalable verification of networks with packet transformers using atomic predicates. ||| 44964 ||| 44965 ||| 
2019 ||| wikipedia and cryptocurrencies: interplay between collective attention and market performance. ||| 36015 ||| 36016 ||| 36017 ||| 
2022 ||| cellvgae: an unsupervised scrna-seq analysis workflow with graph attention networks. ||| 44966 ||| 44967 ||| 44968 ||| 44969 ||| 44970 ||| 23955 ||| 
2022 ||| cpg transformer for imputation of single-cell methylomes. ||| 44971 ||| 30941 ||| 44972 ||| 30942 ||| 
2019 ||| acme: pan-specific peptide-mhc class i binding prediction through attention-based deep neural networks. ||| 5331 ||| 35556 ||| 44973 ||| 44974 ||| 6469 ||| 44975 ||| 27509 ||| 44976 ||| 44977 ||| 44978 ||| 
2021 ||| lbert: lexically aware transformer-based bidirectional encoder representation model for learning universal bio-entity relations. ||| 44979 ||| 44980 ||| 44981 ||| 
2021 ||| crephan: cross-species prediction of enhancers by using hierarchical attention networks. ||| 44982 ||| 44983 ||| 11466 ||| 
2021 ||| on biases of attention in scientific discovery. ||| 44984 ||| 36001 ||| 14803 ||| 
2020 ||| tempel: time-series mutation prediction of influenza a viruses via attention-based recurrent neural networks. ||| 44985 ||| 44986 ||| 44987 ||| 9472 ||| 16599 ||| 
2021 ||| titan: t-cell receptor specificity prediction with bimodal attention networks. ||| 39897 ||| 32811 ||| 10907 ||| 32815 ||| 32816 ||| 4046 ||| 
2022 ||| mire2e: a full end-to-end deep model based on transformers for prediction of pre-mirnas. ||| 44988 ||| 44989 ||| 44990 ||| 44991 ||| 
2021 ||| bert4bitter: a bidirectional encoder representations from transformers (bert)-based model for improving the prediction of bitter peptides. ||| 44992 ||| 44993 ||| 44994 ||| 44995 ||| 44996 ||| 
2020 ||| identifying enhancer-promoter interactions with neural network based on pre-trained dna vectors and attention mechanism. ||| 44997 ||| 16808 ||| 39630 ||| 16878 ||| 
2020 ||| cancer subtype classification and modeling by pathway attention and propagation. ||| 44998 ||| 44999 ||| 45000 ||| 45001 ||| 45002 ||| 
2022 ||| hyperattentiondti: improving drug-protein interaction prediction by sequence-based deep learning with attention mechanism. ||| 16615 ||| 45003 ||| 11162 ||| 1130 ||| 
2020 ||| saint: self-attention augmented inception-inside-inception network improves protein secondary structure prediction. ||| 45004 ||| 45005 ||| 45006 ||| 45007 ||| 
2021 ||| fragat: a fragment-oriented multi-scale graph attention model for molecular property prediction. ||| 45008 ||| 45009 ||| 2613 ||| 
2021 ||| graph contextualized attention network for predicting synthetic lethality in human cancers. ||| 16596 ||| 16597 ||| 4297 ||| 6837 ||| 16599 ||| 16600 ||| 3488 ||| 
2018 ||| an attention-based bilstm-crf approach to document-level chemical named entity recognition. ||| 5015 ||| 16590 ||| 24634 ||| 16591 ||| 3279 ||| 8974 ||| 8349 ||| 
2021 ||| learning embedding features based on multisense-scaled attention architecture to improve the predictive performance of anticancer peptides. ||| 45010 ||| 3906 ||| 9695 ||| 39631 ||| 39630 ||| 
2021 ||| moltrans: molecular interaction transformer for drug-target interaction prediction. ||| 33007 ||| 1070 ||| 38397 ||| 23323 ||| 
2019 ||| deephint: understanding hiv-1 integration via deep learning with attention. ||| 44973 ||| 32699 ||| 22609 ||| 6852 ||| 45011 ||| 4100 ||| 45012 ||| 241 ||| 44978 ||| 
2020 ||| transformercpi: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. ||| 45013 ||| 45014 ||| 45015 ||| 45016 ||| 1782 ||| 45017 ||| 45018 ||| 45019 ||| 45020 ||| 45021 ||| 45022 ||| 
2021 ||| lbert: lexically-aware transformers based bidirectional encoder representation model for learning universal bio-entity relations. ||| 44979 ||| 44980 ||| 44981 ||| 
2021 ||| dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. ||| 45023 ||| 45024 ||| 37420 ||| 45025 ||| 
2021 ||| polypharmacy side-effect prediction with enhanced interpretability based on graph feature attention network. ||| 45026 ||| 45027 ||| 45028 ||| 
2020 ||| mhcattnnet: predicting mhc-peptide bindings for mhc alleles classes i and ii using an attention-based deep neural model. ||| 45029 ||| 45030 ||| 20464 ||| 45031 ||| 
2021 ||| tale: transformer-based protein function annotation with joint sequence-label embedding. ||| 1767 ||| 8831 ||| 
2022 ||| deepidp-2l: protein intrinsically disordered region prediction by combining convolutional attention network and hierarchical attention network. ||| 45032 ||| 45033 ||| 2304 ||| 
2021 ||| halcyon: an accurate basecaller exploiting an encoder-decoder model with monotonic attention. ||| 45034 ||| 45035 ||| 45036 ||| 45037 ||| 45038 ||| 
2021 ||| bert-gt: cross-sentence n-ary relation extraction with bert and graph transformer. ||| 32507 ||| 21587 ||| 
2022 ||| stonkgs: a sophisticated transformer trained on biomedical text and knowledge graphs. ||| 45039 ||| 45040 ||| 45041 ||| 45042 ||| 45043 ||| 45044 ||| 45045 ||| 6787 ||| 45046 ||| 45047 ||| 10314 ||| 
2019 ||| annofly: annotating drosophila embryonic images based on an attention-enhanced rnn model. ||| 11466 ||| 45048 ||| 45049 ||| 40234 ||| 
2021 ||| in silico prediction of in vitro protein liquid-liquid phase separation experiments outcomes with multi-head neural attention. ||| 45050 ||| 45051 ||| 45052 ||| 45053 ||| 45054 ||| 45055 ||| 45056 ||| 45057 ||| 45058 ||| 
2021 ||| ids-attention: an efficient algorithm for intrusion detection systems using attention mechanism. ||| 45059 ||| 45060 ||| 45061 ||| 45062 ||| 
2022 ||| image captioning model using attention and object features to mimic human image understanding. ||| 45063 ||| 45064 ||| 10486 ||| 
2020 ||| deep anomaly detection through visual attention in surveillance videos. ||| 45065 ||| 45066 ||| 45067 ||| 45068 ||| 
2021 ||| enhanced credit card fraud detection based on attention mechanism and lstm deep model. ||| 45069 ||| 45060 ||| 45070 ||| 45071 ||| 
2020 ||| argument annotation and analysis using deep learning with attention mechanism in bahasa indonesia. ||| 22672 ||| 22673 ||| 22674 ||| 22675 ||| 22676 ||| 22677 ||| 
2021 ||| incsa-unet incsa-unet: spatial attention inception unet for aerial images segmentation. ||| 45072 ||| 
2021 ||| pvrar: point-view relation neural network embedded with both attention mechanism and radon transform for 3d shape recognition. ||| 1921 ||| 45073 ||| 45074 ||| 
2019 ||| pay attention and you won't lose it: a deep learning approach to sequence imputation. ||| 45075 ||| 25977 ||| 45076 ||| 45077 ||| 
2022 ||| attention enhanced capsule network for text classification by encoding syntactic dependency trees with graph convolutional neural network. ||| 45078 ||| 1052 ||| 
2021 ||| seedsortnet: a rapid and highly effificient lightweight cnn based on visual attention for seed sorting. ||| 14165 ||| 25757 ||| 20184 ||| 20183 ||| 40925 ||| 
2021 ||| video captioning with stacked attention and semantic hard pull. ||| 45079 ||| 45080 ||| 45081 ||| 45082 ||| 45083 ||| 
2021 ||| st-afn: a spatial-temporal attention based fusion network for lane-level traffic flow prediction. ||| 10149 ||| 45084 ||| 45085 ||| 10150 ||| 
2022 ||| self-supervised recurrent depth estimation with attention mechanisms. ||| 23190 ||| 45086 ||| 7880 ||| 45087 ||| 
2021 ||| stochastic attentions and context learning for person re-identification. ||| 45088 ||| 45089 ||| 45090 ||| 
2021 ||| sva-ssd: saliency visual attention single shot detector for building detection in low contrast high-resolution satellite images. ||| 45091 ||| 45092 ||| 
2020 ||| online supervised attention-based recurrent depth estimation from monocular video. ||| 45093 ||| 23190 ||| 
2020 ||| being the center of attention: a person-context cnn framework for personality recognition. ||| 39199 ||| 39200 ||| 11061 ||| 
2018 ||| estimating collective attention toward a public display. ||| 26109 ||| 45094 ||| 45095 ||| 45096 ||| 45097 ||| 45098 ||| 45099 ||| 45100 ||| 
2020 ||| learning multi-agent communication with double attentional deep reinforcement learning. ||| 3917 ||| 3918 ||| 3919 ||| 3920 ||| 45101 ||| 
2022 ||| embedded attention network for semantic segmentation. ||| 45102 ||| 45103 ||| 6288 ||| 2628 ||| 45104 ||| 9472 ||| 
2021 ||| real-time semantic segmentation with fast attention. ||| 18937 ||| 37938 ||| 37939 ||| 18722 ||| 18766 ||| 7223 ||| 7958 ||| 
2022 ||| look closer: bridging egocentric and third-person views with transformers for robotic manipulation. ||| 33985 ||| 33986 ||| 33987 ||| 14750 ||| 1117 ||| 
2020 ||| probabilistic crowd gan: multimodal pedestrian trajectory prediction using a graph vehicle-pedestrian attention network. ||| 21778 ||| 21777 ||| 21779 ||| 21781 ||| 34380 ||| 21782 ||| 
2022 ||| 3crossnet: cross-level cross-scale cross-attention network for point cloud representation. ||| 36120 ||| 36121 ||| 11126 ||| 515 ||| 
2020 ||| robot navigation in crowds by graph convolutional networks with attention learned from human gaze. ||| 1303 ||| 21804 ||| 5407 ||| 124 ||| 
2022 ||| g3doa: generalizable 3d descriptor with overlap attention for point cloud registration. ||| 45105 ||| 45106 ||| 23576 ||| 19774 ||| 
2022 ||| q-attention: enabling efficient learning for vision-based robotic manipulation. ||| 38149 ||| 18801 ||| 
2020 ||| may i draw your attention? initial lessons from the large-scale generative mark maker. ||| 45107 ||| 45108 ||| 45109 ||| 
2022 ||| tracker meets night: a transformer enhancer for uav tracking. ||| 2606 ||| 2605 ||| 2604 ||| 30894 ||| 45110 ||| 2607 ||| 
2021 ||| adversarial inverse reinforcement learning with self-attention dynamics model. ||| 33251 ||| 25404 ||| 45111 ||| 33253 ||| 2373 ||| 
2022 ||| hierarchical point cloud encoding and decoding with lightweight self-attention based model. ||| 34817 ||| 3386 ||| 11321 ||| 3387 ||| 
2020 ||| alleviating the burden of labeling: sentence generation by attention branch encoder-decoder network. ||| 39737 ||| 22285 ||| 726 ||| 723 ||| 724 ||| 725 ||| 12308 ||| 
2021 ||| 4d attention: comprehensive framework for spatio-temporal gaze mapping. ||| 39086 ||| 39087 ||| 39088 ||| 39089 ||| 
2019 ||| using human attention to address human-robot motion. ||| 17 ||| 45112 ||| 45113 ||| 
2021 ||| esa-vlad: a lightweight network based on second-order attention and netvlad for loop closure detection. ||| 2377 ||| 41515 ||| 45114 ||| 40994 ||| 7300 ||| 45115 ||| 
2022 ||| passive bimanual skills learning from demonstration with motion graph attention networks. ||| 45116 ||| 45117 ||| 28510 ||| 45118 ||| 7084 ||| 
2021 ||| case relation transformer: a crossmodal language generation model for fetching instructions. ||| 34051 ||| 726 ||| 
2021 ||| target-dependent uniter: a transformer-based multimodal language comprehension model for domestic service robots. ||| 37331 ||| 726 ||| 
2021 ||| fov-net: field-of-view extrapolation using self-attention and uncertainty. ||| 45119 ||| 45120 ||| 19030 ||| 7814 ||| 
2020 ||| a multimodal target-source classifier with attention branches to understand ambiguous instructions for fetching daily objects. ||| 22285 ||| 726 ||| 12308 ||| 
2021 ||| multi-gat: a graphical attention-based hierarchical multimodal representation learning approach for human activity recognition. ||| 25554 ||| 25555 ||| 
2020 ||| self-attention based visual-tactile fusion learning for predicting grasp outcomes. ||| 45121 ||| 3049 ||| 45122 ||| 45123 ||| 2039 ||| 
2018 ||| learning context flexible attention model for long-term visual place recognition. ||| 45124 ||| 6334 ||| 45125 ||| 27693 ||| 45126 ||| 
2021 ||| message-aware graph attention networks for large-scale multi-robot path planning. ||| 35093 ||| 35094 ||| 6474 ||| 35095 ||| 
2022 ||| realtime global attention network for semantic segmentation. ||| 36956 ||| 7952 ||| 
2021 ||| crossmap transformer: a crossmodal masked path transformer using double back-translation for vision-and-language navigation. ||| 22285 ||| 726 ||| 12308 ||| 
2019 ||| sliding-window temporal attention based deep learning system for robust sensor modality fusion for ugv navigation. ||| 45127 ||| 45128 ||| 45129 ||| 45130 ||| 
2022 ||| plate: visually-grounded planning with transformers in procedural tasks. ||| 33251 ||| 33252 ||| 33253 ||| 33254 ||| 2373 ||| 33255 ||| 
2021 ||| attentional graph neural network for parking-slot detection. ||| 33742 ||| 33743 ||| 17199 ||| 33744 ||| 33745 ||| 33746 ||| 
2021 ||| towards an interpretable deep driving network by attentional bottleneck. ||| 2041 ||| 19298 ||| 
2021 ||| keypoint matching for point cloud registration using multiplex dynamic graph attention networks. ||| 45131 ||| 35801 ||| 45132 ||| 28696 ||| 13431 ||| 35800 ||| 
2020 ||| learning scheduling policies for multi-robot coordination with graph attention networks. ||| 22678 ||| 3929 ||| 
2022 ||| pq-transformer: jointly parsing 3d objects and layouts from point clouds. ||| 17682 ||| 34641 ||| 34642 ||| 34643 ||| 
2020 ||| spatio-temporal deformable 3d convnets with attention for action recognition. ||| 5536 ||| 17695 ||| 7266 ||| 18142 ||| 
2022 ||| two-stage aware attentional siamese network for visual tracking. ||| 45133 ||| 45134 ||| 45135 ||| 1822 ||| 45136 ||| 45137 ||| 
2021 ||| exploring global diverse attention via pairwise temporal relation for video summarization. ||| 977 ||| 32942 ||| 32943 ||| 1722 ||| 32944 ||| 1932 ||| 
2022 ||| defect attention template generation cyclegan for weakly supervised surface defect segmentation. ||| 45138 ||| 6502 ||| 2219 ||| 45139 ||| 45140 ||| 
2020 ||| metric learning-based kernel transformer with triplets and label constraints for feature fusion. ||| 8853 ||| 45141 ||| 8854 ||| 17571 ||| 33041 ||| 45142 ||| 
2021 ||| hypergraph convolution and hypergraph attention. ||| 2083 ||| 36230 ||| 2160 ||| 
2021 ||| divergent-convergent attention for image captioning. ||| 40133 ||| 45143 ||| 38080 ||| 
2022 ||| sparse attention block: aggregating contextual information for object detection. ||| 37347 ||| 1754 ||| 44723 ||| 
2022 ||| deep co-supervision and attention fusion strategy for automatic covid-19 lung infection segmentation on ct images. ||| 34481 ||| 34482 ||| 34483 ||| 34484 ||| 34485 ||| 15608 ||| 
2022 ||| robust face alignment by dual-attentional spatial-aware capsule networks. ||| 45144 ||| 4807 ||| 17757 ||| 378 ||| 13439 ||| 44005 ||| 
2021 ||| joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks. ||| 13234 ||| 17440 ||| 17441 ||| 779 ||| 
2019 ||| attention guided deep audio-face fusion for efficient speaker naming. ||| 189 ||| 2799 ||| 2163 ||| 9552 ||| 
2020 ||| point attention network for semantic segmentation of 3d point clouds. ||| 38349 ||| 1166 ||| 38350 ||| 38351 ||| 38352 ||| 
2020 ||| video semantic segmentation via feature propagation with holistic attention. ||| 45145 ||| 45146 ||| 6456 ||| 45147 ||| 
2020 ||| semantic segmentation using stride spatial pyramid pooling and dual attention decoder. ||| 42781 ||| 6863 ||| 
2019 ||| deep multi-path convolutional neural network joint with salient region attention for facial expression recognition. ||| 45148 ||| 5746 ||| 45149 ||| 
2022 ||| multi-attention augmented network for single image super-resolution. ||| 23353 ||| 5538 ||| 45150 ||| 
2022 ||| action transformer: a self-attention model for short-time pose-based human action recognition. ||| 30173 ||| 38905 ||| 30602 ||| 38906 ||| 30175 ||| 
2021 ||| oaenet: oriented attention ensemble for accurate facial expression recognition. ||| 33731 ||| 45151 ||| 19236 ||| 33733 ||| 
2021 ||| visual attention dehazing network with multi-level features refinement and fusion. ||| 42280 ||| 45152 ||| 29555 ||| 39526 ||| 
2019 ||| multi attention module for visual tracking. ||| 2384 ||| 2385 ||| 45153 ||| 952 ||| 5908 ||| 1700 ||| 
2022 ||| contrastive attention network with dense field estimation for face completion. ||| 9442 ||| 20137 ||| 18593 ||| 18592 ||| 6660 ||| 6935 ||| 
2021 ||| deep multi-task learning with relational attention for business success prediction. ||| 25309 ||| 25310 ||| 18143 ||| 23306 ||| 9588 ||| 9592 ||| 
2020 ||| deep imitator: handwriting calligraphy imitation via deep attention networks. ||| 45154 ||| 12041 ||| 22256 ||| 12241 ||| 14675 ||| 12243 ||| 
2019 ||| attention-based convolutional neural network and long short-term memory for short-term detection of mood disorders based on elicited speech responses. ||| 12238 ||| 4388 ||| 12237 ||| 
2020 ||| multi-head enhanced self-attention network for novelty detection. ||| 17979 ||| 45155 ||| 30895 ||| 3239 ||| 45156 ||| 
2021 ||| generalized pyramid co-attention with learnable aggregation net for video question answering. ||| 1039 ||| 25192 ||| 17738 ||| 17649 ||| 660 ||| 17650 ||| 
2021 ||| dual self-attention with co-attention networks for visual question answering. ||| 4477 ||| 5320 ||| 45157 ||| 9991 ||| 19722 ||| 29355 ||| 843 ||| 
2020 ||| learning to infer human attention in daily activities. ||| 15491 ||| 18981 ||| 45158 ||| 31008 ||| 18980 ||| 18982 ||| 14779 ||| 
2020 ||| structured self-attention architecture for graph-level representation learning. ||| 45159 ||| 34992 ||| 29455 ||| 30282 ||| 1705 ||| 
2021 ||| stroke constrained attention network for online handwritten mathematical expression recognition. ||| 20135 ||| 1010 ||| 4489 ||| 379 ||| 33123 ||| 
2022 ||| dla-net: learning dual local attention features for semantic segmentation of large-scale building facade point clouds. ||| 17597 ||| 17595 ||| 17599 ||| 5287 ||| 33873 ||| 33874 ||| 3691 ||| 
2022 ||| segmentation information with attention integration for classification of breast tumor in ultrasound image. ||| 45160 ||| 44095 ||| 6922 ||| 
2020 ||| graph convolutional network with structure pooling and joint-wise channel attention for action recognition. ||| 32326 ||| 45161 ||| 8837 ||| 8838 ||| 4600 ||| 45162 ||| 8841 ||| 
2022 ||| canet: co-attention network for rgb-d semantic segmentation. ||| 2736 ||| 6344 ||| 5158 ||| 5157 ||| 6345 ||| 45163 ||| 
2021 ||| region-based dropout with attention prior for weakly supervised object localization. ||| 2090 ||| 2088 ||| 2087 ||| 9523 ||| 2091 ||| 18694 ||| 
2022 ||| learning multiscale hierarchical attention for video summarization. ||| 45164 ||| 1920 ||| 45165 ||| 1921 ||| 
2021 ||| multiple attentional pyramid networks for chinese herbal recognition. ||| 38169 ||| 29558 ||| 29559 ||| 29557 ||| 29560 ||| 38461 ||| 38551 ||| 
2019 ||| script identification in natural scene image and video frames using an attention based convolutional-lstm network. ||| 1968 ||| 27724 ||| 39929 ||| 39928 ||| 36140 ||| 30717 ||| 
2022 ||| contour-enhanced attention cnn for ct-based covid-19 segmentation. ||| 28967 ||| 45166 ||| 29343 ||| 45167 ||| 
2021 ||| deep ancient roman republican coin classification via feature fusion and attention. ||| 35683 ||| 2474 ||| 35684 ||| 7408 ||| 
2019 ||| blind image quality assessment via learnable attention-based pooling. ||| 2250 ||| 45168 ||| 2252 ||| 2255 ||| 
2021 ||| linguistically-aware attention for reducing the semantic gap in vision-language tasks. ||| 33094 ||| 1909 ||| 33095 ||| 1910 ||| 
2021 ||| ggac: multi-relational image gated gcn with attention convolutional binary neural tree for identifying disease with chest x-rays. ||| 40523 ||| 40078 ||| 40070 ||| 1705 ||| 
2019 ||| aem: attentional ensemble model for personalized classifier weight learning. ||| 45169 ||| 45170 ||| 45171 ||| 
2022 ||| multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification. ||| 13538 ||| 13537 ||| 13540 ||| 45172 ||| 14776 ||| 
2022 ||| a tri-attention fusion guided multi-modal segmentation network. ||| 15607 ||| 15608 ||| 35135 ||| 2693 ||| 15610 ||| 
2021 ||| a nested u-shape network with multi-scale upsample attention for robust retinal vascular segmentation. ||| 45173 ||| 7496 ||| 45174 ||| 5379 ||| 
2022 ||| ae-net: fine-grained sketch-based image retrieval via attention-enhanced network. ||| 45175 ||| 45176 ||| 45177 ||| 9689 ||| 580 ||| 6514 ||| 17845 ||| 
2019 ||| moran: a multi-object rectified attention network for scene text recognition. ||| 17418 ||| 6559 ||| 44902 ||| 
2020 ||| a novel image-dehazing network with a parallel attention block. ||| 42280 ||| 29555 ||| 39526 ||| 
2021 ||| view-invariant action recognition via unsupervised attention transfer (uant). ||| 18992 ||| 11466 ||| 1040 ||| 19502 ||| 
2018 ||| unsupervised image saliency detection with gestalt-laws guided optimization and visual attention based refinement. ||| 45178 ||| 10070 ||| 42169 ||| 45179 ||| 2414 ||| 6922 ||| 45180 ||| 45181 ||| 
2020 ||| attention and boundary guided salient object detection. ||| 6645 ||| 28738 ||| 45182 ||| 
2021 ||| lightweight dynamic conditional gan with pyramid attention for text-to-image synthesis. ||| 1039 ||| 45183 ||| 1306 ||| 155 ||| 1040 ||| 
2021 ||| mvdrnet: multi-view diabetic retinopathy detection by combining dcnns and attention mechanisms. ||| 45184 ||| 30930 ||| 1125 ||| 45185 ||| 45186 ||| 45187 ||| 45188 ||| 45189 ||| 30929 ||| 
2020 ||| self-attention driven adversarial similarity learning network. ||| 32028 ||| 5543 ||| 32029 ||| 7237 ||| 30823 ||| 444 ||| 
2021 ||| covid-19 detection from x-ray images using multi-kernel-size spatial-channel attention network. ||| 45190 ||| 45191 ||| 45192 ||| 19838 ||| 
2021 ||| weakly-supervised temporal attention 3d network for human action recognition. ||| 5572 ||| 19339 ||| 45193 ||| 2835 ||| 45194 ||| 
2021 ||| channel attention in lidar-camera fusion for lane line segmentation. ||| 3613 ||| 23629 ||| 13676 ||| 45195 ||| 5536 ||| 
2019 ||| deep gated attention networks for large-scale street-level scene segmentation. ||| 19691 ||| 683 ||| 31091 ||| 26605 ||| 1700 ||| 
2019 ||| attention driven person re-identification. ||| 1856 ||| 2379 ||| 7406 ||| 11404 ||| 11405 ||| 7204 ||| 
2022 ||| a unified deep sparse graph attention network for scene graph generation. ||| 2736 ||| 45196 ||| 19856 ||| 1796 ||| 13240 ||| 
2020 ||| learning visual relationship and context-aware attention for image captioning. ||| 18101 ||| 1160 ||| 10429 ||| 5018 ||| 31031 ||| 17803 ||| 
2020 ||| long video question answering: a matching-guided attention model. ||| 6390 ||| 4648 ||| 10429 ||| 
2021 ||| position-aware self-attention based neural sequence labeling. ||| 5474 ||| 35103 ||| 34045 ||| 11710 ||| 12300 ||| 35104 ||| 
2017 ||| age and gender recognition in the wild with deep attention. ||| 8668 ||| 6235 ||| 8670 ||| 8669 ||| 8671 ||| 8672 ||| 8048 ||| 
2021 ||| image super-resolution via channel attention and spatial graph convolutional network. ||| 6070 ||| 40621 ||| 
2021 ||| attention-shift based deep neural network for fine-grained visual categorization. ||| 17807 ||| 4135 ||| 21748 ||| 
2021 ||| acn: occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks. ||| 45197 ||| 2178 ||| 
2022 ||| relation-aware dynamic attributed graph attention network for stocks recommendation. ||| 45198 ||| 40367 ||| 45199 ||| 22233 ||| 22961 ||| 21378 ||| 
2020 ||| generative attention adversarial classification network for unsupervised domain adaptation. ||| 45200 ||| 5746 ||| 
2022 ||| balanced single-shot object detection using cross-context attention-guided network. ||| 45201 ||| 45202 ||| 580 ||| 9689 ||| 15990 ||| 45203 ||| 3137 ||| 17845 ||| 
2022 ||| a multimodal attention fusion network with a dynamic vocabulary for textvqa. ||| 20171 ||| 1010 ||| 45204 ||| 749 ||| 45205 ||| 45206 ||| 45207 ||| 4489 ||| 4463 ||| 
2020 ||| multistage attention network for image inpainting. ||| 6003 ||| 45208 ||| 23467 ||| 45209 ||| 17758 ||| 
2022 ||| visual vs internal attention mechanisms in deep neural networks for image classification and object detection. ||| 2695 ||| 2701 ||| 2702 ||| 2703 ||| 2697 ||| 2698 ||| 2699 ||| 2700 ||| 
2021 ||| stan: a sequential transformation attention-based network for scene text recognition. ||| 17419 ||| 17418 ||| 6559 ||| 9780 ||| 
2021 ||| memf: multi-level-attention embedding and multi-layer-feature fusion model for person re-identification. ||| 28546 ||| 31006 ||| 31005 ||| 2747 ||| 45210 ||| 
2021 ||| multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network. ||| 24984 ||| 20835 ||| 
2022 ||| channel attention image steganography with generative adversarial networks. ||| 45211 ||| 45212 ||| 45213 ||| 33523 ||| 13451 ||| 
2017 ||| information cascades in feed-based networks of users with limited attention. ||| 45214 ||| 45215 ||| 45216 ||| 23280 ||| 45217 ||| 45218 ||| 
2021 ||| canintelliids: detecting in-vehicle intrusion attacks on a controller area network using cnn and attention-based gru. ||| 44303 ||| 44304 ||| 44305 ||| 29213 ||| 45219 ||| 
2017 ||| visualization of brain activation during attention-demanding tasks using cognitive signal processing. ||| 45220 ||| 45221 ||| 45222 ||| 45223 ||| 45224 ||| 
2021 ||| time-wise attention aided convolutional neural network for data-driven cellular traffic prediction. ||| 45225 ||| 45226 ||| 45227 ||| 20499 ||| 
2019 ||| placement delivery array design via attention-based sequence-to-sequence model with deep neural network. ||| 37394 ||| 37395 ||| 28953 ||| 12186 ||| 37396 ||| 
2022 ||| semantic communication with adaptive universal transformer. ||| 34204 ||| 6967 ||| 6968 ||| 34205 ||| 675 ||| 
2021 ||| channelattention: utilizing attention layers for accurate massive mimo channel feedback. ||| 44451 ||| 44452 ||| 
2021 ||| saldr: joint self-attention learning and dense refine for massive mimo csi feedback with multiple compression ratio. ||| 40210 ||| 1224 ||| 5894 ||| 45228 ||| 45229 ||| 45230 ||| 45231 ||| 
2020 ||| chinese text classification based on attention mechanism and feature-enhanced fusion neural network. ||| 45232 ||| 45233 ||| 18497 ||| 45234 ||| 45235 ||| 45236 ||| 45237 ||| 
2021 ||| air pollution prediction based on factory-aware attentional lstm neural network. ||| 41788 ||| 45238 ||| 45239 ||| 45240 ||| 
2021 ||| inferbert: a transformer-based causal inference framework for enhancing pharmacovigilance. ||| 45241 ||| 6278 ||| 13691 ||| 45242 ||| 45243 ||| 
2021 ||| what does a language-and-vision transformer see: the impact of semantic information on visual representations. ||| 45244 ||| 8507 ||| 
2021 ||| improving adversarial robustness via attention and adversarial logit pairing. ||| 23983 ||| 36802 ||| 3478 ||| 36803 ||| 1406 ||| 
2021 ||| bi-gru model based on pooling and attention for text classification. ||| 45245 ||| 45246 ||| 
2020 ||| sub-word attention mechanism and ensemble learning-based semantic annotation for heterogeneous networks. ||| 1166 ||| 45247 ||| 45248 ||| 14570 ||| 45249 ||| 14114 ||| 181 ||| 
2017 ||| survey of recent advances in 3d visual attention for robotics. ||| 45250 ||| 45251 ||| 45252 ||| 
2021 ||| predicting user visual attention in virtual reality with a deep learning model. ||| 14784 ||| 45253 ||| 45254 ||| 4104 ||| 45255 ||| 45256 ||| 
2021 ||| sustained inattentional blindness in virtual reality and under conventional laboratory conditions. ||| 45257 ||| 16324 ||| 45258 ||| 45259 ||| 45260 ||| 
2022 ||| the predictive role of body image and anti-fat attitudes on attentional bias toward body area in haptic virtual reality environment. ||| 45261 ||| 45262 ||| 2693 ||| 45263 ||| 
2021 ||| correction to: predicting user visual attention in virtual reality with a deep learning model. ||| 14784 ||| 45253 ||| 45254 ||| 4104 ||| 45264 ||| 45256 ||| 
2022 ||| leveraging eye tracking to understand children's attention during game-based, tangible robotics activities. ||| 45265 ||| 45266 ||| 45267 ||| 45268 ||| 
2018 ||| image captioning with affective guiding and selective attention. ||| 45269 ||| 5746 ||| 8967 ||| 
2021 ||| guessuneed: recommending courses via neural attention network and course prerequisite relation embeddings. ||| 45270 ||| 45271 ||| 242 ||| 9631 ||| 
2020 ||| ab-lstm: attention-based bidirectional lstm model for scene text detection. ||| 45272 ||| 1806 ||| 1807 ||| 
2020 ||| an end-to-end attention-based neural model for complementary clothing matching. ||| 45273 ||| 9674 ||| 9631 ||| 34787 ||| 9547 ||| 
2020 ||| multichannel attention refinement for video question answering. ||| 7654 ||| 19739 ||| 40238 ||| 45274 ||| 1306 ||| 1937 ||| 7652 ||| 
2018 ||| paying more attention to saliency: image captioning with saliency and context attention. ||| 19001 ||| 19003 ||| 4700 ||| 13611 ||| 
2019 ||| visual content recognition by exploiting semantic feature map with attention and multi-task learning. ||| 35606 ||| 336 ||| 7314 ||| 43436 ||| 17842 ||| 
2020 ||| enforcing affinity feature learning through self-attention for person re-identification. ||| 13741 ||| 13742 ||| 13743 ||| 13744 ||| 31182 ||| 
2020 ||| attention-based modality-gated networks for image-text sentiment analysis. ||| 19722 ||| 45275 ||| 45276 ||| 843 ||| 
2020 ||| recurrent attention network with reinforced generator for visual dialog. ||| 19018 ||| 1974 ||| 208 ||| 2258 ||| 
2020 ||| meta-path augmented sequential recommendation with contextual co-attention network. ||| 19392 ||| 1174 ||| 1173 ||| 19393 ||| 1175 ||| 
2021 ||| bi-directional co-attention network for image captioning. ||| 28801 ||| 6317 ||| 5746 ||| 
2018 ||| image captioning via semantic guidance attention and consensus selection strategy. ||| 13795 ||| 5746 ||| 2342 ||| 
2022 ||| sadnet: semi-supervised single image dehazing method based on an attention mechanism. ||| 45277 ||| 42204 ||| 45278 ||| 2885 ||| 45279 ||| 13794 ||| 
2020 ||| kernel attention network for single image super-resolution. ||| 19794 ||| 155 ||| 1040 ||| 
2020 ||| constrained lstm and residual attention for image captioning. ||| 8967 ||| 5746 ||| 45280 ||| 45281 ||| 
2019 ||| learning click-based deep structure-preserving embeddings with visual attention. ||| 19118 ||| 19116 ||| 19117 ||| 1775 ||| 19345 ||| 2165 ||| 
2019 ||| video question answering via knowledge-based progressive spatial-temporal attention network. ||| 9703 ||| 1306 ||| 7650 ||| 4634 ||| 7652 ||| 7654 ||| 
2019 ||| convolutional attention networks for scene text recognition. ||| 18071 ||| 19702 ||| 8710 ||| 45282 ||| 185 ||| 17860 ||| 
2019 ||| cmhne: attention-aware collaborative multimodal heterogeneous network embedding. ||| 844 ||| 1174 ||| 1173 ||| 13834 ||| 1175 ||| 
2020 ||| exploring disorder-aware attention for clinical event extraction. ||| 3062 ||| 45283 ||| 165 ||| 404 ||| 405 ||| 
2019 ||| multi-source multi-level attention networks for visual question answering. ||| 19344 ||| 1699 ||| 45284 ||| 2165 ||| 
2019 ||| spatiotemporal-textual co-attention network for video question answering. ||| 8710 ||| 9614 ||| 45285 ||| 17860 ||| 
2020 ||| joint stacked hourglass network and salient region attention refinement for robust face alignment. ||| 5745 ||| 5746 ||| 45286 ||| 
2020 ||| spatio-temporal deep residual network with hierarchical attentions for video event recognition. ||| 29055 ||| 4183 ||| 4180 ||| 45287 ||| 45288 ||| 
2019 ||| pseudo-3d attention transfer network with content-aware strategy for image captioning. ||| 13795 ||| 5746 ||| 8967 ||| 
2022 ||| age-invariant face recognition by multi-feature fusionand decomposition with self-attention. ||| 17648 ||| 45289 ||| 13824 ||| 45290 ||| 8474 ||| 24823 ||| 28442 ||| 28441 ||| 28440 ||| 
2021 ||| fine-grained visual textual alignment for cross-modal retrieval using transformer encoders. ||| 2689 ||| 2690 ||| 20143 ||| 2691 ||| 2692 ||| 2693 ||| 2694 ||| 
2021 ||| adaptive attention-based high-level semantic introduction for image caption. ||| 45291 ||| 43759 ||| 
2019 ||| image captioning with visual-semantic double attention. ||| 43147 ||| 5746 ||| 
2019 ||| moving foreground-aware visual attention and key volume mining for human action recognition. ||| 41909 ||| 5746 ||| 45281 ||| 
2020 ||| image captioning with a joint attention mechanism by visual concept samples. ||| 5332 ||| 241 ||| 40496 ||| 2619 ||| 28727 ||| 
2021 ||| part-wise spatio-temporal attention driven cnn-based 3d human action recognition. ||| 36190 ||| 13827 ||| 45292 ||| 
2022 ||| robust unsupervised gaze calibration using conversation and manipulation attention priors. ||| 59 ||| 19280 ||| 8050 ||| 
2020 ||| tagging reading comprehension materials with document extraction attention networks. ||| 534 ||| 45293 ||| 42990 ||| 25395 ||| 45294 ||| 45295 ||| 
2020 ||| virtual reality based joint attention task platform for children with autism. ||| 4679 ||| 4681 ||| 
2020 ||| attention-based relation and context modeling for point cloud semantic segmentation. ||| 45296 ||| 44402 ||| 16764 ||| 7300 ||| 
2018 ||| exploring visual attention and saliency modeling for task-based visual analysis. ||| 45297 ||| 41863 ||| 41864 ||| 45298 ||| 28040 ||| 
2022 ||| tg-net: reconstruct visual wood texture with semantic attention. ||| 41206 ||| 45299 ||| 12652 ||| 45300 ||| 
2020 ||| anu-net: attention-based nested u-net to exploit full resolution features for medical image segmentation. ||| 399 ||| 11395 ||| 5110 ||| 9999 ||| 16640 ||| 11396 ||| 9028 ||| 
2021 ||| single image deraining via detail-guided efficient channel attention network. ||| 1238 ||| 376 ||| 471 ||| 30488 ||| 45301 ||| 5141 ||| 
2021 ||| cma: cross-modal attention for 6d object pose estimation. ||| 17556 ||| 17557 ||| 45302 ||| 6388 ||| 33268 ||| 
2021 ||| text-conditioned transformer for automatic pronunciation error detection. ||| 45303 ||| 45304 ||| 45305 ||| 
2020 ||| masked multi-head self-attention for causal speech enhancement. ||| 14553 ||| 45306 ||| 
2020 ||| anet: combining bidirectional lstm and self-attention for end-to-end learning of task-oriented dialogue system. ||| 45307 ||| 45308 ||| 45309 ||| 
2021 ||| toward comprehensive user and item representations via three-tier attention network. ||| 4166 ||| 696 ||| 8197 ||| 45310 ||| 3755 ||| 697 ||| 
2020 ||| a price-per-attention auction scheme using mouse cursor information. ||| 9659 ||| 34387 ||| 34388 ||| 9660 ||| 
2021 ||| hgat: heterogeneous graph attention networks for semi-supervised short text classification. ||| 26431 ||| 25190 ||| 1373 ||| 9021 ||| 3488 ||| 9631 ||| 
2022 ||| cha: categorical hierarchy-based attention for next poi recommendation. ||| 45311 ||| 45312 ||| 633 ||| 45313 ||| 45314 ||| 
2020 ||| an attention-based deep relevance model for few-shot document filtering. ||| 45315 ||| 1400 ||| 5067 ||| 45316 ||| 45317 ||| 8987 ||| 
2019 ||| from question to text: question-oriented feature attention for answer selection. ||| 688 ||| 32005 ||| 9631 ||| 34045 ||| 9675 ||| 
2022 ||| on the study of transformers for query suggestion. ||| 45318 ||| 45319 ||| 45320 ||| 3141 ||| 
2019 ||| emotion and attention: audiovisual models for group-level skin response recognition in short movies. ||| 45321 ||| 45322 ||| 8483 ||| 8484 ||| 3419 ||| 4048 ||| 4049 ||| 4046 ||| 8485 ||| 8486 ||| 3419 ||| 14327 ||| 14328 ||| 
2020 ||| correlation alignment with attention mechanism for unsupervised domain adaptation. ||| 19629 ||| 45323 ||| 
2021 ||| transnet: shift invariant transformer network for power attack. ||| 45324 ||| 45325 ||| 45326 ||| 45327 ||| 
2022 ||| transformer encoder-based crypto-ransomware detection for low-power embedded processors. ||| 45328 ||| 45329 ||| 45330 ||| 45331 ||| 45332 ||| 
2018 ||| 5g-transformer: slicing and orchestrating transport networks for industry verticals. ||| 45333 ||| 2259 ||| 45334 ||| 45335 ||| 45336 ||| 45337 ||| 45338 ||| 45339 ||| 45340 ||| 45341 ||| 7101 ||| 45342 ||| 45343 ||| 
2019 ||| causal discovery with attention-based convolutional neural networks. ||| 45344 ||| 45345 ||| 45346 ||| 
2021 ||| a combined short time fourier transform and image classification transformer model for rolling element bearings fault diagnosis in electric motors. ||| 45347 ||| 28086 ||| 45348 ||| 45349 ||| 
2021 ||| attention deep model with multi-scale deep supervision for person re-identification. ||| 8976 ||| 1589 ||| 5782 ||| 45350 ||| 33400 ||| 
2021 ||| graph attention network-based multi-agent reinforcement learning for slicing resource management in dense cellular network. ||| 6966 ||| 6967 ||| 16875 ||| 37180 ||| 6968 ||| 675 ||| 
2019 ||| a single-switch transformerless dc-dc converter with universal input voltage for fuel cell vehicles: analysis and design. ||| 43532 ||| 43533 ||| 43534 ||| 
2021 ||| hsta: a hierarchical spatio-temporal attention model for trajectory prediction. ||| 45351 ||| 1382 ||| 45352 ||| 22154 ||| 45353 ||| 45354 ||| 14153 ||| 
2021 ||| using appearance to predict pedestrian trajectories through disparity-guided attention and convolutional lstm. ||| 6729 ||| 45355 ||| 45356 ||| 45357 ||| 15487 ||| 
2017 ||| a modularization method for battery equalizers using multiwinding transformers. ||| 45358 ||| 45359 ||| 43674 ||| 45360 ||| 1834 ||| 45361 ||| 
2021 ||| environment-attention network for vehicle trajectory prediction. ||| 45362 ||| 18804 ||| 33864 ||| 9570 ||| 25267 ||| 15453 ||| 15454 ||| 22037 ||| 
2021 ||| fii-centernet: an anchor-free detector with foreground attention for traffic object detection. ||| 45363 ||| 45364 ||| 45365 ||| 4600 ||| 45366 ||| 23579 ||| 11354 ||| 
2019 ||| common semantic representation method based on object attention and adversarial learning for cross-modal data in iov. ||| 7690 ||| 7688 ||| 45367 ||| 6389 ||| 45368 ||| 7687 ||| 45369 ||| 
2019 ||| design considerations of efficiency enhanced llc pev charger using reconfigurable transformer. ||| 3091 ||| 45370 ||| 43663 ||| 
2021 ||| anomaly detection for in-vehicle network using cnn-lstm with attention mechanism. ||| 45371 ||| 45372 ||| 45276 ||| 45373 ||| 45374 ||| 
2021 ||| residual attention network-based confidence estimation algorithm for non-holonomic constraint in gnss/ins integrated navigation system. ||| 45375 ||| 32242 ||| 18829 ||| 8968 ||| 45376 ||| 45377 ||| 9695 ||| 
2021 ||| hyperspectral image classification based on dual-branch spectral multiscale attention network. ||| 30375 ||| 30376 ||| 45378 ||| 28341 ||| 30377 ||| 
2020 ||| hierarchical attention and bilinear fusion for remote sensing image scene classification. ||| 45379 ||| 45380 ||| 22359 ||| 31742 ||| 45381 ||| 45382 ||| 
2021 ||| center attention network for hyperspectral image classification. ||| 42517 ||| 45383 ||| 1371 ||| 42518 ||| 
2021 ||| densely connected multiscale attention network for hyperspectral image classification. ||| 45384 ||| 45385 ||| 45386 ||| 45387 ||| 
2021 ||| attention_fpnet: two-branch remote sensing image pansharpening network based on attention feature fusion. ||| 45388 ||| 30102 ||| 17677 ||| 9570 ||| 45389 ||| 45390 ||| 45391 ||| 1235 ||| 
2022 ||| transformer-driven semantic relation inference for multilabel classification of high-resolution remote sensing images. ||| 45392 ||| 45393 ||| 45394 ||| 45395 ||| 333 ||| 45396 ||| 
2021 ||| attention consistent network for remote sensing scene classification. ||| 6617 ||| 45397 ||| 397 ||| 5743 ||| 30203 ||| 400 ||| 
2020 ||| multimodal bilinear fusion network with second-order attention-based channel selection for land cover classification. ||| 6821 ||| 6820 ||| 6822 ||| 765 ||| 30332 ||| 
2021 ||| multiview attention cnn-lstm network for sar automatic target recognition. ||| 45398 ||| 7971 ||| 45399 ||| 45400 ||| 16591 ||| 5233 ||| 
2021 ||| cfcanet: a complete frequency channel attention network for sar image scene classification. ||| 5235 ||| 1235 ||| 1647 ||| 382 ||| 13930 ||| 
2020 ||| attention-based domain adaptation using residual network for hyperspectral image classification. ||| 45401 ||| 6720 ||| 45402 ||| 12229 ||| 7442 ||| 6742 ||| 11546 ||| 6743 ||| 
2020 ||| deep collaborative attention network for hyperspectral image classification by combining 2-d cnn and 3-d cnn. ||| 8358 ||| 45403 ||| 45404 ||| 27907 ||| 17615 ||| 
2021 ||| nonlocal band attention network for hyperspectral image band selection. ||| 45405 ||| 28122 ||| 28124 ||| 28125 ||| 45406 ||| 
2020 ||| using an attention-based lstm encoder-decoder network for near real-time disturbance detection. ||| 6650 ||| 10545 ||| 45407 ||| 45408 ||| 45409 ||| 411 ||| 25764 ||| 
2021 ||| mha-net: multipath hybrid attention network for building footprint extraction from high-resolution remote sensing imagery. ||| 45410 ||| 44085 ||| 
2022 ||| tr-misr: multiimage super-resolution based on feature fusion with transformers. ||| 45411 ||| 1340 ||| 45412 ||| 45413 ||| 4259 ||| 2255 ||| 
2021 ||| integrating gate and attention modules for high-resolution image semantic segmentation. ||| 45414 ||| 4498 ||| 45415 ||| 45416 ||| 
2021 ||| lightweight oriented object detection using multiscale context and enhanced channel attention in remote sensing images. ||| 45417 ||| 13930 ||| 45418 ||| 45419 ||| 45420 ||| 14274 ||| 
2021 ||| siamese spectral attention with channel consistency for hyperspectral image classification. ||| 18638 ||| 45421 ||| 45422 ||| 30638 ||| 18635 ||| 
2020 ||| deep prototypical networks with hybrid residual attention for hyperspectral image classification. ||| 42060 ||| 6699 ||| 6701 ||| 6700 ||| 6869 ||| 15723 ||| 6720 ||| 
2021 ||| attention-gate-based encoder-decoder network for automatical building extraction. ||| 45423 ||| 6919 ||| 5536 ||| 
2021 ||| dspcanet: dual-channel scale-aware segmentation network with position and channel attentions for high-resolution aerial images. ||| 45424 ||| 31730 ||| 31731 ||| 45425 ||| 
2021 ||| stransfuse: fusing swin transformer and convolutional neural network for remote sensing image semantic segmentation. ||| 45390 ||| 17677 ||| 45426 ||| 9570 ||| 45389 ||| 45427 ||| 30102 ||| 
2021 ||| litescanet: an efficient lightweight network based on spectral and channel-wise attention for hyperspectral image classification. ||| 45428 ||| 45429 ||| 45430 ||| 19697 ||| 
2021 ||| agcdetnet: an attention-guided network for building change detection in high-resolution remote sensing images. ||| 22841 ||| 11390 ||| 
2021 ||| spectral-spatial attention feature extraction for hyperspectral image classification based on generative adversarial network. ||| 45431 ||| 45432 ||| 45433 ||| 8156 ||| 
2021 ||| semsdnet: a multiscale dense network with attention for remote sensing scene classification. ||| 1321 ||| 3535 ||| 6798 ||| 45434 ||| 
2021 ||| contextual sa-attention convolutional lstm for precipitation nowcasting: a spatiotemporal sequence forecasting view. ||| 45435 ||| 45436 ||| 1371 ||| 45437 ||| 45438 ||| 45439 ||| 
2021 ||| csds: end-to-end aerial scenes classification with depthwise separable convolution and an attention mechanism. ||| 40695 ||| 570 ||| 16648 ||| 571 ||| 
2021 ||| road extraction using a dual attention dilated-linknet based on satellite images and floating vehicle trajectory data. ||| 45440 ||| 6187 ||| 45441 ||| 45442 ||| 27160 ||| 45443 ||| 30212 ||| 45444 ||| 
2019 ||| high-resolution aerial images semantic segmentation using deep fully convolutional network with channel attention mechanism. ||| 45445 ||| 45446 ||| 45447 ||| 6780 ||| 45448 ||| 
2020 ||| an augmentation attention mechanism for high-spatial-resolution remote sensing image scene classification. ||| 45449 ||| 6704 ||| 14543 ||| 6703 ||| 
2021 ||| a multiscale dual-branch feature fusion and attention network for hyperspectral images classification. ||| 45384 ||| 45450 ||| 5009 ||| 45387 ||| 
2022 ||| gcsanet: a global context spatial attention deep learning network for remote sensing scene classification. ||| 6798 ||| 45451 ||| 45452 ||| 45453 ||| 45454 ||| 6703 ||| 
2021 ||| few-shot object detection with self-adaptive attention network for remote sensing images. ||| 35569 ||| 42538 ||| 35570 ||| 35571 ||| 
2022 ||| attention-based octave network for hyperspectral image denoising. ||| 45455 ||| 45456 ||| 45457 ||| 6749 ||| 340 ||| 
2021 ||| attention multisource fusion-based deep few-shot learning for hyperspectral image classification. ||| 42128 ||| 23018 ||| 12666 ||| 
2021 ||| sentinel-3 super-resolution based on dense multireceptive channel attention. ||| 45458 ||| 14327 ||| 1867 ||| 42065 ||| 45459 ||| 45460 ||| 
2021 ||| dapnet: a double self-attention convolutional network for point cloud semantic labeling. ||| 3036 ||| 45461 ||| 36809 ||| 36811 ||| 36812 ||| 21518 ||| 12801 ||| 
2020 ||| attention receptive pyramid network for ship detection in sar images. ||| 10646 ||| 80 ||| 45462 ||| 30332 ||| 
2021 ||| multimodal representation learning and set attention for lwir in-scene atmospheric compensation. ||| 45463 ||| 45464 ||| 41055 ||| 41056 ||| 45465 ||| 45466 ||| 
2021 ||| multilabel remote sensing image annotation with multiscale attention and label correlation. ||| 1858 ||| 45467 ||| 471 ||| 
2022 ||| relation-attention networks for remote sensing scene classification. ||| 398 ||| 45468 ||| 45469 ||| 30434 ||| 
2021 ||| selective kernel res-attention unet: deep learning for generating decorrelation mask with applications to tandem-x interferograms. ||| 336 ||| 30559 ||| 45470 ||| 45471 ||| 
2022 ||| homo-heterogenous transformer learning framework for rs scene classification. ||| 30203 ||| 45472 ||| 6617 ||| 397 ||| 5743 ||| 400 ||| 
2021 ||| attention-guided label refinement network for semantic segmentation of very high resolution aerial orthoimages. ||| 45473 ||| 40924 ||| 2903 ||| 45474 ||| 
2021 ||| cs-hsnet: a cross-siamese change detection network based on hierarchical-split attention. ||| 45475 ||| 989 ||| 
2021 ||| attention-based tri-unet for remote sensing image pan-sharpening. ||| 42185 ||| 31473 ||| 42180 ||| 
2021 ||| sa-jstn: self-attention joint spatiotemporal network for temperature forecasting. ||| 45476 ||| 45477 ||| 44118 ||| 4003 ||| 29628 ||| 
2021 ||| self-supervised pretraining of transformers for satellite image time series classification. ||| 6650 ||| 10545 ||| 
2020 ||| channel-attention-based densenet network for remote sensing image scene classification. ||| 45452 ||| 6798 ||| 14543 ||| 45453 ||| 6703 ||| 
2021 ||| cross-layer attention network for small object detection in remote sensing imagery. ||| 6852 ||| 21507 ||| 30441 ||| 45478 ||| 400 ||| 30442 ||| 
2020 ||| a cnn-transformer hybrid approach for crop classification using multitemporal multisensor images. ||| 45479 ||| 45480 ||| 45481 ||| 
2020 ||| 3-d channel and spatial attention based multiscale spatial-spectral residual network for hyperspectral image classification. ||| 22952 ||| 92 ||| 3656 ||| 17613 ||| 45482 ||| 
2022 ||| sar image despeckling using continuous attention module. ||| 45483 ||| 45484 ||| 
2021 ||| hresnetam: hierarchical residual network with attention mechanism for hyperspectral image classification. ||| 45485 ||| 45486 ||| 3072 ||| 45487 ||| 45488 ||| 
2021 ||| visual attention and background subtraction with adaptive weight for hyperspectral anomaly detection. ||| 45489 ||| 45490 ||| 45491 ||| 2856 ||| 17464 ||| 2858 ||| 
2021 ||| da-roadnet: a dual-attention network for road extraction from high resolution satellite imagery. ||| 30571 ||| 30572 ||| 30570 ||| 45492 ||| 30575 ||| 
2022 ||| multiscale densely connected attention network for hyperspectral image classification. ||| 398 ||| 45493 ||| 
2021 ||| umag-net: a new unsupervised multiattention-guided network for hyperspectral and multispectral image fusion. ||| 3854 ||| 45494 ||| 9262 ||| 8838 ||| 8841 ||| 30796 ||| 
2021 ||| remote sensing image super-resolution via residual aggregation and split attentional fusion network. ||| 9570 ||| 17677 ||| 45426 ||| 30102 ||| 45427 ||| 45388 ||| 
2020 ||| a novel attention fully convolutional network method for synthetic aperture radar image segmentation. ||| 45495 ||| 4176 ||| 45496 ||| 1224 ||| 30475 ||| 30434 ||| 
2021 ||| a multiscale attention network for remote sensing scene images classification. ||| 16821 ||| 45497 ||| 7700 ||| 17472 ||| 45498 ||| 45499 ||| 9262 ||| 
2021 ||| attention-based object detection with saliency loss in remote sensing images. ||| 45500 ||| 45501 ||| 45502 ||| 45503 ||| 
2020 ||| video-based person re-identification with parallel spatial-temporal attention module. ||| 27439 ||| 42951 ||| 24565 ||| 28534 ||| 
2021 ||| a-darts: attention-guided differentiable architecture search for lung nodule classification. ||| 45504 ||| 45505 ||| 1796 ||| 220 ||| 1305 ||| 38673 ||| 
2020 ||| aligned attention for common multimodal embeddings. ||| 18714 ||| 45506 ||| 18719 ||| 
2018 ||| dual-level attention-aware network for temporal emotion segmentation. ||| 534 ||| 536 ||| 45507 ||| 532 ||| 535 ||| 
2021 ||| action recognition for sports video analysis using part-attention spatio-temporal graph convolutional network. ||| 45508 ||| 45509 ||| 
2019 ||| carf-net: cnn attention and rnn fusion network for video-based person reidentification. ||| 45510 ||| 11604 ||| 45511 ||| 19019 ||| 
2021 ||| deep progressive attention for person re-identification. ||| 20244 ||| 20246 ||| 5067 ||| 
2021 ||| weighted feature fusion and attention mechanism for object detection. ||| 45512 ||| 31404 ||| 31405 ||| 
2020 ||| attn-eh aln: complex text-to-image generation with attention-enhancing adversarial learning networks. ||| 45513 ||| 45514 ||| 124 ||| 45515 ||| 
2019 ||| widget detection network: widget detection in mobile screenshot with region-based attention networks. ||| 11275 ||| 45516 ||| 
2021 ||| facial expression recognition based on facial part attention mechanism. ||| 45517 ||| 45518 ||| 45519 ||| 45520 ||| 45521 ||| 
2021 ||| tamnet: two attention modules-based network on facial expression recognition under uncertainty. ||| 155 ||| 11359 ||| 
2021 ||| boundary-enhanced attention-aware network for detecting salient objects in rgb-depth images. ||| 45522 ||| 31370 ||| 
2019 ||| spatiotemporal information deep fusion network with frame attention mechanism for video action recognition. ||| 45523 ||| 45524 ||| 
2021 ||| person re-identification based on attention clustering and long short-term memory network. ||| 1224 ||| 45525 ||| 45526 ||| 
2019 ||| attention module-based spatial-temporal graph convolutional networks for skeleton-based action recognition. ||| 45527 ||| 144 ||| 19919 ||| 45528 ||| 2278 ||| 
2021 ||| siampat: siamese point attention networks for robust visual tracking. ||| 14383 ||| 40619 ||| 45529 ||| 
2020 ||| spatial-temporal graph attention networks for skeleton-based action recognition. ||| 34779 ||| 40634 ||| 45530 ||| 13249 ||| 45531 ||| 
2021 ||| progressive multi-scale attention network for compression artifact reduction. ||| 45532 ||| 2170 ||| 45533 ||| 45534 ||| 
2021 ||| graph attention mechanism with global contextual information for multi-label image recognition. ||| 45535 ||| 19230 ||| 19228 ||| 45536 ||| 45537 ||| 45538 ||| 
2020 ||| dpsa: dense pixelwise spatial attention network for hatching egg fertility detection. ||| 45539 ||| 45540 ||| 45541 ||| 45542 ||| 
2021 ||| channel and spatial attention-based siamese network for visual object tracking. ||| 45543 ||| 45544 ||| 45545 ||| 44552 ||| 2514 ||| 
2020 ||| img-net: inner-cross-modal attentional multigranular network for description-based person re-identification. ||| 45546 ||| 45547 ||| 45548 ||| 10406 ||| 45549 ||| 8674 ||| 
2021 ||| dual attention and part drop network for person reidentification. ||| 29039 ||| 45550 ||| 45150 ||| 45551 ||| 6632 ||| 
2021 ||| lightweight facial expression recognition method based on attention mechanism and key region fusion. ||| 45527 ||| 45552 ||| 19919 ||| 45553 ||| 45528 ||| 2278 ||| 
2021 ||| adaptive scene-aware deep attention network for remote sensing image compression. ||| 45554 ||| 14570 ||| 31481 ||| 45555 ||| 45556 ||| 45557 ||| 
2020 ||| crowd counting via an inverse attention residual network. ||| 31380 ||| 31379 ||| 45558 ||| 45559 ||| 31383 ||| 
2021 ||| monodepthplus: self-supervised monocular depth estimation using soft-attention and learnable outlier-masking. ||| 1796 ||| 12169 ||| 
2021 ||| abnormal event detection algorithm based on dual attention future frame prediction and gap fusion discrimination. ||| 35884 ||| 44915 ||| 
2021 ||| violence behavior recognition of two-cascade temporal shift module with attention mechanism. ||| 45560 ||| 2969 ||| 45561 ||| 45562 ||| 
2021 ||| super-resolution of compressed images using enhanced attention network. ||| 45563 ||| 45555 ||| 31481 ||| 45556 ||| 45564 ||| 
2021 ||| thermal imaging pedestrian detection algorithm based on attention guidance and local cross-level network. ||| 45565 ||| 1575 ||| 45566 ||| 45567 ||| 
2019 ||| an attention-based spiking neural network for unsupervised spike-sorting. ||| 45568 ||| 45569 ||| 
2019 ||| lateral inhibition organizes beta attentional modulation in the primary visual cortex. ||| 45570 ||| 24418 ||| 24419 ||| 45571 ||| 45572 ||| 
2021 ||| the influence of visual attention on the performance of a novel tactile p300 brain-computer interface with cheeks-stim paradigm. ||| 45573 ||| 10406 ||| 45574 ||| 45575 ||| 45576 ||| 45577 ||| 
2021 ||| graph attention network with focal loss for seizure detection on electroencephalography signals. ||| 45578 ||| 45579 ||| 45580 ||| 13438 ||| 45581 ||| 31518 ||| 
2021 ||| multi-task learning with multi-view weighted fusion attention for artery-specific calcification analysis. ||| 16905 ||| 6005 ||| 700 ||| 4060 ||| 45582 ||| 25716 ||| 27335 ||| 34593 ||| 29277 ||| 
2019 ||| hierarchical multi-modal fusion fcn with attention model for rgb-d tracking. ||| 45583 ||| 45584 ||| 45585 ||| 30907 ||| 45586 ||| 32950 ||| 
2021 ||| a defense method based on attention mechanism against traffic sign adversarial samples. ||| 18472 ||| 2747 ||| 9472 ||| 45587 ||| 45588 ||| 45589 ||| 45590 ||| 45276 ||| 
2020 ||| a dual-branch attention fusion deep network for multiresolution remote-sensing image classification. ||| 9335 ||| 30389 ||| 3535 ||| 400 ||| 36641 ||| 6829 ||| 
2021 ||| pay attention to doctor-patient dialogues: multi-modal knowledge graph attention image-text embedding for covid-19 diagnosis. ||| 11352 ||| 11351 ||| 11353 ||| 45591 ||| 45592 ||| 45593 ||| 11354 ||| 
2020 ||| granger causality-based information fusion applied to electrical measurements from power transformers. ||| 45594 ||| 45595 ||| 2600 ||| 45596 ||| 45597 ||| 45598 ||| 10910 ||| 45599 ||| 3369 ||| 45600 ||| 45601 ||| 45602 ||| 45603 ||| 45604 ||| 3882 ||| 45605 ||| 45606 ||| 45607 ||| 45608 ||| 45609 ||| 45610 ||| 45611 ||| 14913 ||| 
2019 ||| cross-modality interactive attention network for multispectral pedestrian detection. ||| 2037 ||| 45612 ||| 13412 ||| 5157 ||| 44836 ||| 4776 ||| 30475 ||| 
2022 ||| a fusion spatial attention approach for few-shot learning. ||| 45613 ||| 45614 ||| 45615 ||| 45616 ||| 45617 ||| 45618 ||| 
2020 ||| multi-class arrhythmia detection from 12-lead varied-length ecg using attention-based time-incremental convolutional neural network. ||| 45619 ||| 20027 ||| 45620 ||| 45621 ||| 11494 ||| 
2020 ||| multimodal feature fusion by relational reasoning and attention for visual question answering. ||| 1273 ||| 8160 ||| 45622 ||| 43153 ||| 7095 ||| 
2022 ||| pesa-net: permutation-equivariant split attention network for correspondence learning. ||| 45623 ||| 40432 ||| 45624 ||| 39630 ||| 28779 ||| 
2021 ||| multimodal feature-wise co-attention method for visual question answering. ||| 13776 ||| 29395 ||| 43076 ||| 41651 ||| 17650 ||| 43078 ||| 
2021 ||| transient finite element method for computing and analyzing the effect of harmonics on hysteresis and eddy current loss of distribution transformer. ||| 45625 ||| 45626 ||| 45627 ||| 
2022 ||| coupled field magnetostatic analysis for free buckling in double layer helical winding of a distribution transformer. ||| 45625 ||| 45626 ||| 45627 ||| 
2017 ||| an attentional model for autonomous mobile robots. ||| 5421 ||| 5419 ||| 227 ||| 45628 ||| 
2020 ||| a hierarchical attention model for ctr prediction based on user interest. ||| 31519 ||| 45629 ||| 17239 ||| 40279 ||| 40280 ||| 
2018 ||| adequate planning of shunt power capacitors involving transformer capacity release benefit. ||| 45630 ||| 45631 ||| 45632 ||| 
2017 ||| visual focus of attention estimation using eye center localization. ||| 12818 ||| 45633 ||| 29162 ||| 13196 ||| 12819 ||| 
2021 ||| discrimination of internal faults and other transients in an interconnected system with power transformers and phase angle regulators. ||| 11774 ||| 11776 ||| 39792 ||| 
2020 ||| mind: mind networked device architecture for attention-gated ambient assisted living systems. ||| 45634 ||| 45635 ||| 45636 ||| 
2020 ||| control of parallel ultc transformers in active distribution systems. ||| 45637 ||| 45638 ||| 16213 ||| 
2017 ||| conaim: a conscious attention-based integrated model for human-like robots. ||| 5419 ||| 227 ||| 5421 ||| 45628 ||| 
2020 ||| neural attention model with keyword memory for abstractive document summarization. ||| 3866 ||| 3867 ||| 3868 ||| 
2021 ||| influence of promotion mode on purchase decision based on multilevel psychological distance dimension of visual attention model and data mining. ||| 45639 ||| 45640 ||| 45641 ||| 
2021 ||| a gastric cancer recognition algorithm on gastric pathological sections based on multistage attention-densenet. ||| 1748 ||| 45642 ||| 10875 ||| 45643 ||| 25932 ||| 45644 ||| 
2021 ||| conditional pre-trained attention based chinese question generation. ||| 1166 ||| 45645 ||| 45646 ||| 3337 ||| 35380 ||| 
2021 ||| image super-resolution with parallel convolution attention network. ||| 45647 ||| 43086 ||| 45648 ||| 2320 ||| 45649 ||| 45650 ||| 
2020 ||| various syncretic co-attention network for multimodal sentiment analysis. ||| 455 ||| 9090 ||| 30923 ||| 30628 ||| 31805 ||| 
2021 ||| tatt-bilstm: web service classification with topical attention-based bilstm. ||| 1568 ||| 30515 ||| 162 ||| 2965 ||| 1569 ||| 10805 ||| 45651 ||| 
2021 ||| social rumor detection based on multilayer transformer encoding blocks. ||| 45652 ||| 45653 ||| 
2021 ||| sev-net: residual network embedded with attention mechanism for plant disease severity detection. ||| 29610 ||| 29612 ||| 9579 ||| 31372 ||| 31370 ||| 
2021 ||| multiscale channel attention network for infrared and visible image fusion. ||| 45525 ||| 43085 ||| 45654 ||| 19109 ||| 45649 ||| 43086 ||| 
2021 ||| flexible scene text recognition based on dual attention mechanism. ||| 9454 ||| 45655 ||| 45656 ||| 45657 ||| 
2021 ||| web service classification based on information gain theory and bidirectional long short-term memory with attention mechanism. ||| 10805 ||| 162 ||| 1569 ||| 160 ||| 
2021 ||| calculation and analysis of dc magnetic bias current of urban main transformer under the action of stray current. ||| 40682 ||| 45658 ||| 3034 ||| 
2021 ||| remote sensing data detection based on multiscale fusion and attention mechanism. ||| 45659 ||| 45660 ||| 45661 ||| 
2021 ||| rdmmfet: representation of dense multimodality fusion encoder based on transformer. ||| 181 ||| 40953 ||| 45662 ||| 
2021 ||| facial expression recognition method combined with attention mechanism. ||| 8009 ||| 45663 ||| 45664 ||| 5098 ||| 340 ||| 
2021 ||| air quality prediction based on a spatiotemporal attention mechanism. ||| 45665 ||| 45666 ||| 45667 ||| 36003 ||| 45668 ||| 45669 ||| 
2021 ||| research on uyghur-chinese neural machine translation based on the transformer at multistrategy segmentation granularity. ||| 45670 ||| 45671 ||| 45672 ||| 
2021 ||| multiscale dense cross-attention mechanism with covariance pooling for hyperspectral image scene classification. ||| 40030 ||| 40031 ||| 29239 ||| 45673 ||| 
2021 ||| music feature classification based on recurrent neural networks with channel attention mechanism. ||| 45674 ||| 
2021 ||| joint source-target encoding with pervasive attention. ||| 23095 ||| 3510 ||| 2353 ||| 
2021 ||| joint extraction of entities and relations based on character graph convolutional network and multi-head self-attention mechanism. ||| 1388 ||| 29079 ||| 29080 ||| 29081 ||| 
2020 ||| visualization of user's attention on objects in 3d environment using only eye tracking glasses. ||| 45675 ||| 26096 ||| 45676 ||| 
2022 ||| attention-based spatial-temporal neural network for accurate phase recognition in minimally invasive surgery: feasibility and efficiency verification. ||| 45677 ||| 11593 ||| 45678 ||| 4398 ||| 
2019 ||| attention and anticipation in fast visual-inertial navigation. ||| 21805 ||| 21806 ||| 
2020 ||| learning manipulation skills via hierarchical spatial attention. ||| 22281 ||| 22282 ||| 
2019 ||| pressure characteristics of a novel double rotor hydraulic transformer. ||| 45679 ||| 45680 ||| 
2020 ||| variable speed digital hydraulic transformer-based servo drive. ||| 45681 ||| 
2019 ||| the state monitoring method of electronic voltage transformer based on l-m algorithm. ||| 45682 ||| 
2021 ||| mashup tag completion with attention-based topic model. ||| 160 ||| 161 ||| 17209 ||| 45683 ||| 
2019 ||| 28-ghz cmos vco with capacitive splitting and transformer feedback techniques for 5g communication. ||| 45684 ||| 45685 ||| 45686 ||| 28037 ||| 45687 ||| 
2020 ||| a 3.15-mw +16.0-dbm iip3 22-db cg inductively source degenerated balun-lna mixer with integrated transformer-based gate inductor and im2 injection technique. ||| 45688 ||| 10780 ||| 45689 ||| 8471 ||| 45690 ||| 
2017 ||| a 0.9-5.8-ghz software-defined receiver rf front-end with transformer-based current-gain boosting and harmonic rejection calibration. ||| 29965 ||| 45691 ||| 45692 ||| 29966 ||| 45693 ||| 45694 ||| 29967 ||| 
2020 |||  full-span differential vector modulator phase rotator with transformer-based poly-phase quadrature network. ||| 45695 ||| 45696 ||| 300 ||| 
2021 ||| han-bsvd: a hierarchical attention network for binary software vulnerability detection. ||| 45697 ||| 18232 ||| 42812 ||| 8426 ||| 
2019 ||| attention-based convolutional approach for misinformation identification from massive and noisy microblog posts. ||| 32817 ||| 1073 ||| 1075 ||| 10429 ||| 17803 ||| 
2020 ||| adsad: an unsupervised attention-based discrete sequence anomaly detection framework for network security analysis. ||| 5283 ||| 5284 ||| 5285 ||| 
2021 ||| finefool: a novel dnn object contour attack on image recognition based on the attention perturbation adversarial technique. ||| 21005 ||| 21007 ||| 9592 ||| 45698 ||| 45699 ||| 45700 ||| 9003 ||| 
2021 ||| automatically predicting cyber attack preference with attributed heterogeneous attention networks and transductive learning. ||| 1418 ||| 12569 ||| 45701 ||| 1717 ||| 45702 ||| 9407 ||| 11629 ||| 
2021 ||| doubigru-a: software defect detection algorithm based on attention mechanism and double bigru. ||| 45703 ||| 45704 ||| 45705 ||| 
2021 ||| phishing websites detection via cnn and multi-head self-attention on imbalanced datasets. ||| 2744 ||| 42338 ||| 12140 ||| 2747 ||| 2745 ||| 3477 ||| 2748 ||| 
2019 ||| neural malware analysis with attention mechanism. ||| 15119 ||| 15120 ||| 15121 ||| 15122 ||| 15123 ||| 
2021 ||| : interpretable malware detector using galaxy transformer. ||| 39308 ||| 29479 ||| 39309 ||| 39310 ||| 
2021 ||| low pass filter design with improved stop-band suppression and synthesis with transformer-free ladders. ||| 45706 ||| 45707 ||| 45708 ||| 45709 ||| 
2018 ||| low-power design for dc current transformer using class-d compensating amplifier. ||| 45710 ||| 45711 ||| 45712 ||| 45713 ||| 
2017 ||| serious games in k-12 education: benefits and impacts on students with attention, memory and developmental disabilities. ||| 45714 ||| 28656 ||| 45715 ||| 45716 ||| 
2019 ||| multisensory integration and exogenous spatial attention: a time-window-of-integration analysis. ||| 45717 ||| 45718 ||| 
2020 ||| short-term smartphone app-based focused attention meditation diminishes cognitive flexibility. ||| 45719 ||| 45720 ||| 
2019 ||| modulation of event-related potentials of visual discrimination by meditation training and sustained attention. ||| 45721 ||| 45722 ||| 45723 ||| 45724 ||| 45725 ||| 45726 ||| 45727 ||| 
2017 ||| distinct frontoparietal networks underlying attentional effort and cognitive control. ||| 45728 ||| 45729 ||| 45730 ||| 
2021 ||| attentional templates are sharpened through differential signal enhancement, not differential allocation of attention. ||| 45731 ||| 45732 ||| 
2021 ||| steady-state visually evoked potentials and feature-based attention: preregistered null results and a focused review of methodological considerations. ||| 45733 ||| 45734 ||| 45735 ||| 45736 ||| 
2022 ||| diversion of attention leads to conflict between concurrently attended stimuli, not delayed orienting to the object of interest. ||| 45737 ||| 45738 ||| 45739 ||| 45740 ||| 
2020 ||| attentional access to multiple target objects in visual search. ||| 45741 ||| 45742 ||| 
2020 ||| deconstructing reorienting of attention: cue predictiveness modulates the inhibition of the no-target side and the hemispheric distribution of the p1 response to invalid targets. ||| 45743 ||| 45744 ||| 45745 ||| 45746 ||| 45747 ||| 45748 ||| 12865 ||| 45749 ||| 
2022 ||| no evidence of attentional modulation of the neural response to the temporal fine structure of continuous musical pieces. ||| 45750 ||| 45751 ||| 45752 ||| 45753 ||| 
2022 ||| learning at variable attentional load requires cooperation of working memory, meta-learning, and attention-augmented reinforcement learning. ||| 45754 ||| 45755 ||| 45756 ||| 
2019 ||| attentional facilitation of constituent features of an object does not spread automatically along object-defining cortical boundaries. ||| 45757 ||| 45758 ||| 45759 ||| 3831 ||| 
2019 ||| neural dynamics of cognitive control over working memory capture of attention. ||| 45760 ||| 45761 ||| 45762 ||| 45763 ||| 
2017 ||| "nonspatial" attentional deficits interact with spatial position in neglect. ||| 45764 ||| 45765 ||| 45766 ||| 
2018 ||| prefrontal modulation of visual processing and sustained attention in aging, a tdcs-eeg coregistration approach. ||| 5335 ||| 45767 ||| 45768 ||| 45769 ||| 45770 ||| 45771 ||| 45772 ||| 41521 ||| 41523 ||| 
2021 ||| stimulus-induced alpha suppression tracks the difficulty of attentional selection, not visual working memory storage. ||| 45773 ||| 45774 ||| 45775 ||| 
2021 ||| no effect of transcranial direct current stimulation over left dorsolateral prefrontal cortex on temporal attention. ||| 45776 ||| 45777 ||| 
2020 ||| alpha-band activity tracks the zoom lens of attention. ||| 45778 ||| 45779 ||| 45780 ||| 
2019 ||| neural correlates of enhanced visual attentional control in action video game players: an event-related potential study. ||| 45781 ||| 45782 ||| 45783 ||| 45784 ||| 45785 ||| 45786 ||| 
2017 ||| power and phase of alpha oscillations reveal an interaction between spatial and temporal visual attention. ||| 45787 ||| 45788 ||| 
2021 ||| shifting attention in feature space: fast facilitation of the to-be-attended feature is followed by slow inhibition of the to-be-ignored feature. ||| 45789 ||| 45759 ||| 3831 ||| 
2021 ||| sustained attention and spatial attention distinctly influence long-term memory encoding. ||| 45790 ||| 45791 ||| 45792 ||| 45780 ||| 
2017 ||| enhancing spatial attention and working memory in younger and older adults. ||| 45793 ||| 45794 ||| 45795 ||| 45796 ||| 45797 ||| 
2017 ||| feature-selective attention in frontoparietal cortex: multivoxel codes adjust to prioritize task-relevant information. ||| 45798 ||| 45799 ||| 45800 ||| 45801 ||| 
2018 ||| pupillary correlates of fluctuations in sustained attention. ||| 45802 ||| 45803 ||| 45804 ||| 
2017 ||| the pivotal role of the right parietal lobe in temporal attention. ||| 45805 ||| 45806 ||| 45807 ||| 45808 ||| 45809 ||| 45810 ||| 45811 ||| 45812 ||| 45813 ||| 
2017 ||| fluctuations of attentional networks and default mode network during the resting state reflect variations in cognitive states: evidence from a novel resting-state experience sampling method. ||| 45814 ||| 45815 ||| 45816 ||| 15325 ||| 15326 ||| 45817 ||| 45818 ||| 
2021 ||| overlapping neuronal population responses in the human parietal cortex during visuospatial attention and arithmetic processing. ||| 31999 ||| 45819 ||| 45820 ||| 45821 ||| 6415 ||| 45822 ||| 
2019 ||| taking attention out of context: frontopolar transcranial magnetic stimulation abolishes the formation of new context memories in visual search. ||| 45823 ||| 45824 ||| 45825 ||| 45826 ||| 3831 ||| 45827 ||| 
2018 ||| attending to what and where: background connectivity integrates categorical and spatial attention. ||| 45828 ||| 45829 ||| 45830 ||| 
2020 ||| unraveling the relation between eeg correlates of attentional orienting and sound localization performance: a diffusion model approach. ||| 45831 ||| 45832 ||| 45833 ||| 45834 ||| 4194 ||| 45835 ||| 45836 ||| 45837 ||| 
2021 ||| self-reported mind wandering and response time variability differentiate prestimulus electroencephalogram microstate dynamics during a sustained attention task. ||| 45721 ||| 45838 ||| 45839 ||| 
2018 ||| connectome-based models predict separable components of attention in novel individuals. ||| 45840 ||| 45841 ||| 45842 ||| 45843 ||| 45844 ||| 
2020 ||| neural mechanisms of strategic adaptation in attentional flexibility. ||| 45845 ||| 45846 ||| 45762 ||| 
2017 ||| dopamine alters the fidelity of working memory representations according to attentional demands. ||| 45847 ||| 45848 ||| 45849 ||| 45850 ||| 45851 ||| 
2017 ||| global enhancement but local suppression in feature-based attention. ||| 45852 ||| 7111 ||| 45853 ||| 45759 ||| 3831 ||| 
2017 ||| hierarchies of attention and experimental designs: effects of spatial and intermodal attention revisited. ||| 45854 ||| 45855 ||| 
2020 ||| resting-state functional connectivity of the right temporoparietal junction relates to belief updating and reorienting during spatial attention. ||| 45856 ||| 45857 ||| 45858 ||| 45859 ||| 45860 ||| 
2022 ||| spatial and feature-selective attention have distinct, interacting effects on population-level tuning. ||| 45861 ||| 45862 ||| 45801 ||| 
2019 ||| the functional consequences of social attention for memory-guided attention orienting and anticipatory neural dynamics. ||| 45863 ||| 45864 ||| 26406 ||| 45865 ||| 45866 ||| 45867 ||| 
2019 ||| tuning attention to object categories: spatially global effects of attention to faces in visual processing. ||| 45868 ||| 45869 ||| 45870 ||| 45871 ||| 
2018 ||| dynamics of feature-based attentional selection during color-shape conjunction search. ||| 25425 ||| 45872 ||| 45873 ||| 42299 ||| 
2017 ||| comparing the effects of 10-hz repetitive tms on tasks of visual stm and attention. ||| 45874 ||| 45875 ||| 45876 ||| 45877 ||| 
2019 ||| attentional modulation of visual spatial integration: psychophysical evidence supported by population coding modeling. ||| 45878 ||| 45879 ||| 45880 ||| 
2019 ||| spatially specific attention mechanisms are sensitive to competition during visual search. ||| 45881 ||| 45882 ||| 45883 ||| 
2018 ||| polarity-dependent effects of biparietal transcranial direct current stimulation on the interplay between target location and distractor saliency in visual attention. ||| 45884 ||| 45885 ||| 42299 ||| 24140 ||| 
2017 ||| soap opera: self as object and agent in prioritizing attention. ||| 45886 ||| 45887 ||| 
2018 ||| auditory attention causes gain enhancement and frequency sharpening at successive stages of cortical processing - evidence from human electroencephalography. ||| 45888 ||| 45889 ||| 
2019 ||| dissociating reward- and attention-driven biasing of global feature-based selection in human visual cortex. ||| 45890 ||| 45891 ||| 45892 ||| 45893 ||| 45894 ||| 45895 ||| 45896 ||| 45897 ||| 
2019 ||| brain and cognitive mechanisms of top-down attentional control in a multisensory world: benefits of electrical neuroimaging. ||| 3900 ||| 45898 ||| 45899 ||| 45900 ||| 45901 ||| 
2017 ||| attention to distinct goal-relevant features differentially guides semantic knowledge retrieval. ||| 45902 ||| 45903 ||| 
2020 ||| slow endogenous fluctuations in cortical fmri signals correlate with reduced performance in a visual detection task and are suppressed by spatial attention. ||| 45904 ||| 45905 ||| 43029 ||| 
2017 ||| rebalancing spatial attention: endogenous orienting may partially overcome the left visual field bias in rapid serial visual presentation. ||| 45906 ||| 45907 ||| 45908 ||| 
2020 ||| spatial attention and temporal expectation exert differential effects on visual and auditory discrimination. ||| 45909 ||| 45910 ||| 45911 ||| 45912 ||| 45913 ||| 
2019 ||| testing the possibility of model-based pavlovian control of attention to threat. ||| 45914 ||| 45915 ||| 45916 ||| 
2020 ||| contextual modulation of emotional distraction: attentional capture and motivational significance. ||| 45917 ||| 45918 ||| 45919 ||| 45920 ||| 
2017 ||| magnocellular bias in exogenous attention to biologically salient stimuli as revealed by manipulating their luminosity and color. ||| 45921 ||| 45922 ||| 10907 ||| 45923 ||| 45924 ||| 45925 ||| 45926 ||| 10314 ||| 45927 ||| 10907 ||| 45928 ||| 45929 ||| 
2020 ||| tracking the effects of top-down attention on word discrimination using frequency-tagged neuromagnetic responses. ||| 45930 ||| 45931 ||| 45932 ||| 45933 ||| 45934 ||| 45935 ||| 45936 ||| 45937 ||| 45938 ||| 45939 ||| 
2022 ||| leveraging spiking deep neural networks to understand the neural mechanisms underlying selective attention. ||| 45940 ||| 45941 ||| 45942 ||| 45777 ||| 9145 ||| 45943 ||| 
2018 ||| selective attention to faces in a rapid visual stream: hemispheric differences in enhancement and suppression of category-selective neural activity. ||| 45944 ||| 45945 ||| 45946 ||| 45947 ||| 
2020 ||| attention for speaking: prestimulus motor-cortical alpha power predicts picture naming latencies. ||| 45948 ||| 45949 ||| 45950 ||| 
2021 ||| hemisphere-specific parietal contributions to the interplay between working memory and attention. ||| 45951 ||| 45952 ||| 45953 ||| 45762 ||| 
2018 ||| visual working memory load disrupts template-guided attentional selection during visual search. ||| 45741 ||| 45742 ||| 
2018 ||| contralateral delay activity indexes working memory storage, not the current focus of spatial attention. ||| 45778 ||| 45779 ||| 45792 ||| 45780 ||| 
2021 ||| motivational salience guides attention to valuable and threatening stimuli: evidence from behavior and functional magnetic resonance imaging. ||| 45954 ||| 45955 ||| 45956 ||| 45957 ||| 45958 ||| 
2018 ||| attentional fluctuations influence the neural fidelity and connectivity of stimulus representations. ||| 45959 ||| 12972 ||| 12974 ||| 
2021 ||| caffeine boosts preparatory attention for reward-related stimulus information. ||| 45960 ||| 45961 ||| 45763 ||| 45962 ||| 
2018 ||| rapid improvement on a temporal attention task within a single session of high-frequency transcranial random noise stimulation. ||| 45807 ||| 45963 ||| 45813 ||| 
2020 ||| distinct neural mechanisms meet challenges in dynamic visual attention due to either load or object spacing. ||| 45964 ||| 45965 ||| 45966 ||| 45967 ||| 45968 ||| 
2021 ||| the microstructure of attentional control in the dorsal attention network. ||| 45969 ||| 45970 ||| 45971 ||| 45972 ||| 45973 ||| 45974 ||| 45975 ||| 
2018 ||| the dorsal attention network reflects both encoding load and top-down control during working memory. ||| 45818 ||| 15325 ||| 15326 ||| 45817 ||| 45976 ||| 45977 ||| 45978 ||| 
2021 ||| attention biases competition for visual representation via dissociable influences from frontal and parietal cortex. ||| 45979 ||| 45980 ||| 45981 ||| 45982 ||| 45983 ||| 45984 ||| 45985 ||| 45877 ||| 
2018 ||| independent attention mechanisms control the activation of tactile and visual working memory representations. ||| 45986 ||| 45742 ||| 
2018 ||| spatial attention enhances the neural representation of invisible signals embedded in noise. ||| 45987 ||| 45988 ||| 
2017 ||| preparatory encoding of the fine scale of human spatial attention. ||| 45796 ||| 45989 ||| 45793 ||| 45990 ||| 45991 ||| 45992 ||| 45993 ||| 45994 ||| 45995 ||| 45797 ||| 
2017 ||| moving beyond attentional biases: shifting the interhemispheric balance between left and right posterior parietal cortex modulates attentional control processes. ||| 45996 ||| 45997 ||| 45998 ||| 45999 ||| 46000 ||| 
2019 ||| hemifield-specific correlations between cue-related blood oxygen level dependent activity in bilateral nodes of the dorsal attention network and attentional benefits in a spatial orienting paradigm. ||| 46001 ||| 45996 ||| 46002 ||| 46003 ||| 46000 ||| 
2020 ||| enhanced attention using head-mounted virtual reality. ||| 858 ||| 45794 ||| 46004 ||| 6954 ||| 46005 ||| 45797 ||| 
2020 ||| voluntary control of task selection does not eliminate the impact of selection history on attention. ||| 46006 ||| 46007 ||| 46008 ||| 
2020 ||| the neural consequences of attentional prioritization of internal representations in visual working memory. ||| 45981 ||| 45979 ||| 45877 ||| 
2021 ||| dynamic interplay between reward and voluntary attention determines stimulus processing in visual cortex. ||| 46009 ||| 46010 ||| 12894 ||| 7111 ||| 45853 ||| 
2018 ||| spatially selective alpha oscillations reveal moment-by-moment trade-offs between working memory and attention. ||| 46011 ||| 46012 ||| 45876 ||| 46013 ||| 43033 ||| 45780 ||| 
2019 ||| top-down attention is limited within but not between feature dimensions. ||| 46014 ||| 46015 ||| 7111 ||| 45853 ||| 
2019 ||| causal evidence for the role of neuronal oscillations in top-down and bottom-up attention. ||| 46016 ||| 46017 ||| 46018 ||| 46019 ||| 46020 ||| 
2019 ||| attentional weighting in the face processing network: a magnetic response image-guided magnetoencephalography study using multiple cyclic entrainments. ||| 46021 ||| 46022 ||| 
2020 ||| involuntary orienting and conflict resolution during auditory attention: the role of ventral and dorsal streams. ||| 46023 ||| 46024 ||| 46025 ||| 46026 ||| 
2021 ||| probing the neural systems underlying flexible dimensional attention. ||| 13055 ||| 46027 ||| 46028 ||| 46029 ||| 12934 ||| 
2018 ||| cross-frequency phase-amplitude coupling as a mechanism for temporal orienting of attention in childhood. ||| 46030 ||| 46031 ||| 45867 ||| 
2017 ||| intermodal attention shifts in multimodal working memory. ||| 45986 ||| 46032 ||| 45742 ||| 
2021 ||| a direct comparison of spatial attention and stimulus-response compatibility between mice and humans. ||| 46033 ||| 46034 ||| 9146 ||| 46035 ||| 
2017 ||| neural representation of working memory content is modulated by visual attentional demand. ||| 45951 ||| 46036 ||| 45762 ||| 
2020 ||| does closing the eyes enhance auditory attention? eye closure increases attentional alpha-power modulation but not listening performance. ||| 46037 ||| 46038 ||| 46039 ||| 45911 ||| 
2021 ||| mosaic convolution-attention network for demosaicing multispectral filter array images. ||| 46040 ||| 3266 ||| 30529 ||| 46041 ||| 30692 ||| 6460 ||| 
2020 ||| cagan: a cycle-consistent generative adversarial network with attention for low-dose ct imaging. ||| 46042 ||| 31123 ||| 46043 ||| 46044 ||| 28992 ||| 41291 ||| 31124 ||| 189 ||| 11467 ||| 30931 ||| 27703 ||| 
2022 ||| rational inattention and public signals. ||| 46045 ||| 
2021 ||| dynamic choice under familiarity-based attention. ||| 46046 ||| 
2018 ||| salience and limited attention. ||| 46047 ||| 
2021 ||| automatic semicircular canal segmentation of ct volumes using improved 3d u-net with attention mechanism. ||| 46048 ||| 46049 ||| 46050 ||| 42496 ||| 46051 ||| 5071 ||| 46052 ||| 4297 ||| 46053 ||| 875 ||| 
2021 ||| an attention mechanism oriented hybrid cnn-rnn deep learning architecture of container terminal liner handling conditions prediction. ||| 6502 ||| 46054 ||| 
2021 ||| fraudulent news headline detection with attention mechanism. ||| 46055 ||| 41943 ||| 41945 ||| 
2020 ||| change detection of remote sensing images based on attention mechanism. ||| 9570 ||| 31138 ||| 3675 ||| 46056 ||| 
2020 ||| inherent importance of early visual features in attraction of human attention. ||| 46057 ||| 46058 ||| 46059 ||| 46060 ||| 
2020 ||| extracting parallel sentences from nonparallel corpora using parallel hierarchical attention network. ||| 46061 ||| 25530 ||| 46062 ||| 
2021 ||| deep learning based on hierarchical self-attention for finance distress prediction incorporating text. ||| 46063 ||| 46064 ||| 46065 ||| 3337 ||| 
2021 ||| social recommendation system based on hypergraph attention network. ||| 6056 ||| 6055 ||| 6054 ||| 
2021 ||| remaining useful life estimation of aircraft engines using a joint deep learning model based on tcnn and transformer. ||| 46066 ||| 46067 ||| 46068 ||| 
2021 ||| simultaneous pickup and delivery traveling salesman problem considering the express lockers using attention route planning network. ||| 484 ||| 46069 ||| 46070 ||| 7965 ||| 13099 ||| 
2021 ||| analysis of volleyball video intelligent description technology based on computer memory network and attention mechanism. ||| 46071 ||| 
2021 ||| multiscale convolutional neural networks with attention for plant species recognition. ||| 12699 ||| 46072 ||| 26929 ||| 
2021 ||| egat: extended graph attention network for pedestrian trajectory prediction. ||| 46073 ||| 4477 ||| 4175 ||| 46074 ||| 
2019 ||| dual cnn for relation extraction with knowledge-based attention and word embeddings. ||| 5536 ||| 31567 ||| 46075 ||| 46076 ||| 
2021 ||| afi-net: attention-guided feature integration network for rgbd saliency detection. ||| 46077 ||| 46078 ||| 19250 ||| 46079 ||| 46080 ||| 46081 ||| 46082 ||| 
2017 ||| fuzzy classification of high resolution remote sensing scenes using visual attention features. ||| 46083 ||| 46084 ||| 46085 ||| 
2021 ||| utilizing entity-based gated convolution and multilevel sentence attention to improve distantly supervised relation extraction. ||| 5180 ||| 5181 ||| 5182 ||| 
2021 ||| intelligent malaysian sign language translation system using convolutional-based attention module with residual network. ||| 46086 ||| 46087 ||| 46088 ||| 40824 ||| 46089 ||| 46090 ||| 
2021 ||| attention-based temporal encoding network with background-independent motion mask for action recognition. ||| 46091 ||| 46092 ||| 46093 ||| 46094 ||| 46095 ||| 46096 ||| 
2019 ||| leveraging contextual sentences for text classification by using a neural attention model. ||| 8220 ||| 46097 ||| 
2021 ||| a novel time-incremental end-to-end shared neural network with attention-based feature fusion for multiclass motor imagery recognition. ||| 46098 ||| 46099 ||| 46100 ||| 46101 ||| 46102 ||| 
2022 ||| a pavement crack detection method based on multiscale attention and hfs. ||| 46103 ||| 13103 ||| 5167 ||| 24334 ||| 46104 ||| 46105 ||| 
2021 ||| hybrid lstm self-attention mechanism model for forecasting the reform of scientific research in morocco. ||| 46106 ||| 46107 ||| 46108 ||| 46109 ||| 46110 ||| 46111 ||| 
2019 ||| eeg alpha power is modulated by attentional changes during cognitive tasks and virtual reality immersion. ||| 46112 ||| 46113 ||| 46114 ||| 46115 ||| 46116 ||| 
2019 ||| a stacked bilstm neural network based on coattention mechanism for question answering. ||| 959 ||| 46117 ||| 46118 ||| 46119 ||| 
2019 ||| optimized complex network method (ocnm) for improving accuracy of measuring human attention in single-electrode neurofeedback system. ||| 46120 ||| 781 ||| 764 ||| 31428 ||| 46121 ||| 
2019 ||| attention-based personalized encoder-decoder model for local citation recommendation. ||| 29553 ||| 46122 ||| 29552 ||| 7895 ||| 
2021 ||| research on volleyball video intelligent description technology combining the long-term and short-term memory network and attention mechanism. ||| 46123 ||| 46124 ||| 5538 ||| 46125 ||| 38314 ||| 
2019 ||| context attention heterogeneous network embedding. ||| 19245 ||| 46126 ||| 2487 ||| 46127 ||| 811 ||| 
2021 ||| dar-net: dense attentional residual network for vehicle detection in aerial images. ||| 46128 ||| 379 ||| 
2021 ||| medical text classification using hybrid deep learning models with multihead attention. ||| 28280 ||| 28282 ||| 
2021 ||| a generative adversarial network fused with dual-attention mechanism and its application in multitarget image fine segmentation. ||| 24823 ||| 40522 ||| 46129 ||| 46130 ||| 14036 ||| 
2021 ||| identification of navel orange diseases and pests based on the fusion of densenet and self-attention mechanism. ||| 46131 ||| 46132 ||| 
2021 ||| hierarchical attention-based multimodal fusion network for video emotion recognition. ||| 24050 ||| 46133 ||| 46134 ||| 
2020 ||| effects of visual attention on tactile p300 bci. ||| 46135 ||| 10406 ||| 46136 ||| 46137 ||| 46138 ||| 45577 ||| 
2017 ||| complexity analysis of resting-state fmri in adult patients with attention deficit hyperactivity disorder: brain entropy. ||| 2101 ||| 46139 ||| 46140 ||| 
2021 ||| a multi-rnn research topic prediction model based on spatial attention and semantic consistency-based scientific influence modeling. ||| 16944 ||| 7688 ||| 46141 ||| 7689 ||| 7690 ||| 6389 ||| 5858 ||| 13659 ||| 
2020 ||| interactive dual attention network for text sentiment classification. ||| 46142 ||| 46143 ||| 46144 ||| 
2021 ||| dual-path attention compensation u-net for stroke lesion segmentation. ||| 46145 ||| 9665 ||| 46146 ||| 46147 ||| 
2021 ||| a multitask learning model with multiperspective attention and its application in recommendation. ||| 46148 ||| 31138 ||| 46149 ||| 
2022 ||| research on real-time face key point detection algorithm based on attention mechanism. ||| 46150 ||| 15490 ||| 
2020 ||| a knowledge-fusion ranking system with an attention network for making assignment recommendations. ||| 7706 ||| 46151 ||| 46152 ||| 1460 ||| 46153 ||| 7707 ||| 
2021 ||| a study of two-way short- and long-term memory network intelligent computing iot model-assisted home education attention mechanism. ||| 46154 ||| 
2021 ||| a multiattention-based supervised feature selection method for multivariate time series. ||| 5919 ||| 46155 ||| 46156 ||| 17794 ||| 
2021 ||| multicomponent spatial-temporal graph attention convolution networks for traffic prediction with spatially sparse data. ||| 46157 ||| 46158 ||| 46159 ||| 20133 ||| 46160 ||| 5538 ||| 
2020 ||| using cnn and channel attention mechanism to identify driver's distracted behavior. ||| 46161 ||| 10980 ||| 46162 ||| 46163 ||| 46164 ||| 46165 ||| 46166 ||| 46167 ||| 
2018 ||| attention decrease detection based on video analysis in e-learning. ||| 46168 ||| 
2020 ||| detecting aging substation transformers by audio signal with deep neural network. ||| 4245 ||| 46169 ||| 11333 ||| 46170 ||| 7778 ||| 4297 ||| 
2021 ||| partial discharge pattern recognition of transformers based on the gray-level co-occurrence matrix of optimal parameters. ||| 46171 ||| 29009 ||| 46172 ||| 46173 ||| 46174 ||| 1975 ||| 
2020 ||| research and application of generator protection based on fiber optical current transformer. ||| 1785 ||| 46175 ||| 333 ||| 
2020 ||| hybrid attention distribution and factorized embedding matrix in image captioning. ||| 8349 ||| 6738 ||| 
2020 ||| multi-scale attention generative adversarial networks for video frame interpolation. ||| 46176 ||| 46177 ||| 
2019 ||| r-trans: rnn transformer network for chinese machine reading comprehension. ||| 46178 ||| 13776 ||| 1340 ||| 1341 ||| 
2019 ||| person re-identification based on two-stream network with attention and pose features. ||| 46179 ||| 42985 ||| 
2022 ||| heterogeneous attention concentration link prediction algorithm for attracting customer flow in online brand community. ||| 29497 ||| 46180 ||| 17308 ||| 5743 ||| 28498 ||| 20370 ||| 29499 ||| 
2020 ||| image inpainting based on inside-outside attention and wavelet decomposition. ||| 46181 ||| 46182 ||| 46183 ||| 
2019 ||| road marking segmentation based on siamese attention module and maximum stable external region. ||| 16905 ||| 46184 ||| 46185 ||| 46186 ||| 44717 ||| 
2019 ||| captionnet: automatic end-to-end siamese difference captioning model with attention. ||| 46187 ||| 40735 ||| 46188 ||| 46189 ||| 40736 ||| 46190 ||| 
2021 ||| real-time semantic segmentation of remote sensing images based on bilateral attention refined network. ||| 46191 ||| 46192 ||| 46193 ||| 46194 ||| 46195 ||| 2735 ||| 46196 ||| 
2020 ||| capacity enhancement of a radial distribution grid using smart transformer. ||| 21978 ||| 46197 ||| 16200 ||| 
2021 ||| csi-ianet: an inception attention network for human-human interaction recognition based on csi signal. ||| 46198 ||| 46199 ||| 46200 ||| 
2020 ||| short-term load forecasting using recurrent neural networks with input attention mechanism and hidden connection mechanism. ||| 46201 ||| 46202 ||| 16726 ||| 
2019 ||| attention-based character-word hybrid neural networks with semantic and structural information for identifying of urgent posts in mooc discussion forums. ||| 31643 ||| 31641 ||| 46203 ||| 5194 ||| 4287 ||| 
2019 ||| boosting arabic named-entity recognition with multi-attention layer. ||| 46204 ||| 46205 ||| 46206 ||| 
2020 ||| hybrid attention densely connected ensemble framework for lesion segmentation from magnetic resonance images. ||| 16917 ||| 5858 ||| 5213 ||| 42521 ||| 16918 ||| 
2019 ||| lstm-crf neural network with gated self attention for chinese ner. ||| 46207 ||| 46208 ||| 46209 ||| 46210 ||| 27538 ||| 3049 ||| 
2020 ||| separable attention capsule network for signal classification. ||| 46211 ||| 46212 ||| 749 ||| 36641 ||| 214 ||| 
2019 ||| an indoor temperature prediction framework based on hierarchical attention gated recurrent unit model for energy efficient buildings. ||| 46213 ||| 46214 ||| 46215 ||| 3694 ||| 38147 ||| 46216 ||| 
2018 ||| a 62-90 ghz high linearity and low noise cmos mixer using transformer-coupling cascode topology. ||| 46217 ||| 46218 ||| 37195 ||| 46219 ||| 46220 ||| 41416 ||| 41414 ||| 41418 ||| 
2020 ||| an intrusion detection model with hierarchical attention mechanism. ||| 748 ||| 1305 ||| 7666 ||| 430 ||| 
2019 ||| aspect based sentiment analysis with feature enhanced attention cnn-bilstm. ||| 14060 ||| 46221 ||| 20373 ||| 46222 ||| 46223 ||| 
2022 ||| cmsea: compound model scaling with efficient attention for fine-grained image classification. ||| 46224 ||| 46225 ||| 
2021 ||| etdnet: an efficient transformer deraining model. ||| 46226 ||| 46227 ||| 398 ||| 6379 ||| 46228 ||| 10192 ||| 
2021 ||| a duality based quasi-steady-state model of three-phase five-limb sen transformer. ||| 29527 ||| 3166 ||| 11742 ||| 6796 ||| 
2020 ||| fast trajectory prediction method with attention enhanced sru. ||| 42093 ||| 46229 ||| 241 ||| 46230 ||| 46231 ||| 46232 ||| 
2019 ||| cascaded conditional generative adversarial networks with multi-scale attention fusion for automated bi-ventricle segmentation in cardiac mri. ||| 11275 ||| 789 ||| 41984 ||| 46233 ||| 46234 ||| 34508 ||| 23440 ||| 
2020 ||| a novel clothing attribute representation network-based self-attention mechanism. ||| 46235 ||| 46236 ||| 46237 ||| 
2022 ||| reliable estimation for health index of transformer oil based on novel combined predictive maintenance techniques. ||| 40850 ||| 40851 ||| 46238 ||| 40849 ||| 40848 ||| 40852 ||| 16220 ||| 40853 ||| 
2019 ||| mscoa: multi-step co-attention model for multi-label classification. ||| 46239 ||| 35137 ||| 46240 ||| 46241 ||| 46242 ||| 
2020 ||| aspect-context interactive attention representation for aspect-level sentiment classification. ||| 46243 ||| 438 ||| 44829 ||| 31957 ||| 3488 ||| 31956 ||| 
2019 ||| hran: hybrid residual attention network for single image super-resolution. ||| 8742 ||| 38969 ||| 8747 ||| 
2022 ||| spatio-temporal self-attention network for fire detection and segmentation in video surveillance. ||| 23882 ||| 46244 ||| 46245 ||| 30972 ||| 46246 ||| 28956 ||| 11299 ||| 
2019 ||| webshell detection based on the word attention mechanism. ||| 46247 ||| 46248 ||| 46249 ||| 24731 ||| 46250 ||| 7259 ||| 
2019 ||| inter-basket and intra-basket adaptive attention network for next basket recommendation. ||| 46251 ||| 659 ||| 11122 ||| 660 ||| 6475 ||| 11116 ||| 
2021 ||| bayesian deep neural network to compensate for current transformer saturation. ||| 46252 ||| 46253 ||| 46254 ||| 46255 ||| 
2021 ||| attention-modulated triplet network for face sketch recognition. ||| 46256 ||| 36696 ||| 46257 ||| 
2021 ||| understanding the influence of power transformer faults on the frequency response signature using simulation analysis and statistical indicators. ||| 46258 ||| 46259 ||| 46260 ||| 46261 ||| 46262 ||| 46263 ||| 28557 ||| 
2019 ||| high gain transformer-less double-duty-triple-mode dc/dc converter for dc microgrid. ||| 10861 ||| 43623 ||| 22152 ||| 10860 ||| 46264 ||| 46265 ||| 
2021 ||| recognizing pests in field-based images by combining spatial and channel attention mechanism. ||| 46266 ||| 46267 ||| 765 ||| 46268 ||| 46269 ||| 46270 ||| 
2020 ||| spatial attention based real-time object detection network for internet of things devices. ||| 46271 ||| 17444 ||| 46272 ||| 46273 ||| 
2020 ||| multi-frame blind restoration for image of space target with frc and branch-attention. ||| 46274 ||| 46275 ||| 46276 ||| 
2019 ||| multi-dimensional residual dense attention network for stereo matching. ||| 46277 ||| 46278 ||| 46279 ||| 46280 ||| 46281 ||| 10140 ||| 
2020 ||| face super-resolution reconstruction based on self-attention residual network. ||| 31378 ||| 31379 ||| 46282 ||| 46283 ||| 31383 ||| 31398 ||| 
2022 ||| respiratory sound classification: from fluid-solid coupling analysis to feature-band attention. ||| 46284 ||| 14500 ||| 46285 ||| 14499 ||| 1556 ||| 
2022 ||| an object detection algorithm for rotary-wing uav based on awin transformer. ||| 46286 ||| 46287 ||| 11705 ||| 
2019 ||| rapid transformer health state recognition through canopy cluster-merging of dissolved gas data in high-dimensional space. ||| 46288 ||| 989 ||| 46289 ||| 38608 ||| 46290 ||| 46291 ||| 
2021 ||| artificial intelligence-based power transformer health index for handling data uncertainty. ||| 23177 ||| 23178 ||| 23179 ||| 28557 ||| 
2019 ||| human action recognition in unconstrained trimmed videos using residual attention network and joints path signature. ||| 46292 ||| 6559 ||| 46293 ||| 46294 ||| 
2021 ||| a residual-attention offline handwritten chinese text recognition based on fully convolutional neural networks. ||| 46295 ||| 46296 ||| 40538 ||| 27856 ||| 
2018 ||| attention-based memory network for text sentiment classification. ||| 5752 ||| 3888 ||| 46297 ||| 
2022 ||| an attention-based spatiotemporal ggnn for next poi recommendation. ||| 46298 ||| 46299 ||| 46300 ||| 6415 ||| 
2021 ||| breast cancer histopathology image super-resolution using wide-attention gan with improved wasserstein gradient penalty and perceptual loss. ||| 46301 ||| 
2019 ||| two-stage short-term load forecasting for power transformers under different substation operating conditions. ||| 11023 ||| 46302 ||| 15703 ||| 46303 ||| 46304 ||| 
2019 ||| a hybrid bidirectional recurrent convolutional neural network attention-based model for text classification. ||| 44812 ||| 46305 ||| 
2021 ||| an object detection method combining multi-level feature fusion and region channel attention. ||| 46306 ||| 46307 ||| 16299 ||| 
2021 ||| triple-stage attention-based multiple parallel connection hybrid neural network model for conditional time series forecasting. ||| 31803 ||| 10154 ||| 
2021 ||| a skill-based visual attention model for cloud gaming. ||| 22705 ||| 22703 ||| 46308 ||| 22702 ||| 10381 ||| 
2020 ||| a dual-transformer-based bidirectional dc-dc converter of using blocking capacitor for wide zvs range. ||| 46309 ||| 3534 ||| 46310 ||| 46311 ||| 
2021 ||| deep learning model for house price prediction using heterogeneous data analysis along with joint self-attention mechanism. ||| 46312 ||| 46313 ||| 46314 ||| 46315 ||| 46316 ||| 
2018 ||| exploring new backbone and attention module for semantic segmentation in street scenes. ||| 46317 ||| 46318 ||| 46319 ||| 46320 ||| 
2021 ||| from local to global: efficient dual attention mechanism for single image super-resolution. ||| 3803 ||| 42150 ||| 
2019 ||| vibro-acoustic methods in the condition assessment of power transformers: a survey. ||| 46321 ||| 46322 ||| 46323 ||| 
2021 ||| electroencephalogram-based attention level classification using convolution attention memory neural network. ||| 20850 ||| 20851 ||| 20852 ||| 
2018 ||| attention alignment multimodal lstm for fine-gained common space learning. ||| 37633 ||| 9705 ||| 8718 ||| 
2021 ||| siamese attentional cascade keypoints network for visual object tracking. ||| 25007 ||| 46324 ||| 46325 ||| 46326 ||| 3558 ||| 46327 ||| 
2020 ||| an accurate ensemble forecasting approach for highly dynamic cloud workload with vmd and r-transformer. ||| 46328 ||| 4991 ||| 3433 ||| 46329 ||| 46330 ||| 
2021 ||| a human-robot interaction system calculating visual focus of human's attention level. ||| 26271 ||| 46331 ||| 26272 ||| 46332 ||| 46333 ||| 46334 ||| 
2022 ||| analysis and comparison of series resonant converter with embedded filters for high power density dcx of solid-state transformer. ||| 21956 ||| 21957 ||| 21958 ||| 21959 ||| 
2020 ||| attention-based convolutional lstm for describing video. ||| 46335 ||| 46336 ||| 46337 ||| 46338 ||| 46339 ||| 
2020 ||| multiscale residual attention network for distinguishing stationary humans and common animals under through-wall condition using ultra-wideband radar. ||| 46340 ||| 46341 ||| 822 ||| 46342 ||| 46343 ||| 5972 ||| 885 ||| 46344 ||| 46345 ||| 1420 ||| 
2019 ||| harsam: a hybrid model for recommendation supported by self-attention mechanism. ||| 29271 ||| 10839 ||| 5010 ||| 
2022 ||| an attention-based predictive agent for static and dynamic environments. ||| 46346 ||| 46347 ||| 46348 ||| 
2021 ||| enhancing local dependencies for transformer-based text-to-speech via hybrid lightweight convolution. ||| 7700 ||| 10327 ||| 44794 ||| 
2022 ||| feature attention parallel aggregation network for single image haze removal. ||| 46349 ||| 46350 ||| 210 ||| 
2021 ||| stackda: a stacked dual attention neural network for multivariate time-series forecasting. ||| 46351 ||| 29576 ||| 29577 ||| 
2020 ||| nonlinearity of magnetic core in evaluation of current and phase errors of transformation of higher harmonics of distorted current by inductive current transformers. ||| 40941 ||| 46352 ||| 
2019 ||| sentiment analysis of text based on bidirectional lstm with multi-head attention. ||| 37199 ||| 5384 ||| 19801 ||| 
2020 ||| arbitrary shape natural scene text detection method based on soft attention mechanism and dilated convolution. ||| 5289 ||| 46353 ||| 26931 ||| 46354 ||| 7417 ||| 
2020 ||| geospatial contextual attention mechanism for automatic and fast airport detection in sar imagery. ||| 30348 ||| 30150 ||| 30152 ||| 30151 ||| 46355 ||| 30349 ||| 
2021 ||| hierarchical graph attention based multi-view convolutional neural network for 3d object recognition. ||| 33823 ||| 46356 ||| 46357 ||| 5280 ||| 46358 ||| 
2021 ||| a deep learning based light-weight face mask detector with residual context attention and gaussian heatmap to fight against covid-19. ||| 46359 ||| 46360 ||| 46361 ||| 
2021 ||| image inpainting with learnable edge-attention maps. ||| 46362 ||| 46363 ||| 41235 ||| 46364 ||| 
2019 ||| analysis of electrodermal activity signal collected during visual attention oriented tasks. ||| 46365 ||| 46366 ||| 
2020 ||| hilbert id considering multi-window feature extraction for transformer deep vision fault positioning. ||| 41256 ||| 22230 ||| 46367 ||| 41257 ||| 46368 ||| 41255 ||| 
2019 ||| global-local attention network for aerial scene classification. ||| 44816 ||| 6764 ||| 19316 ||| 13767 ||| 13766 ||| 45396 ||| 
2020 ||| state of the art of solid-state transformers: advanced topologies, implementation issues, recent progress and improvements. ||| 41100 ||| 21268 ||| 46369 ||| 46370 ||| 46371 ||| 21262 ||| 22216 ||| 
2022 ||| an interleaved high step-up dc-dc converter based on integration of coupled inductor and built-in-transformer with switched-capacitor cells for renewable energy applications. ||| 22026 ||| 22027 ||| 22028 ||| 22029 ||| 
2022 ||| lianet: layer interactive attention network for rgb-d salient object detection. ||| 46372 ||| 4719 ||| 12638 ||| 46373 ||| 
2020 ||| electrophysiological evidence of attentional avoidance in sub-clinical individuals with obsessive-compulsive symptoms. ||| 46374 ||| 46375 ||| 46376 ||| 46377 ||| 46378 ||| 
2019 ||| numerical study on natural convective heat transfer of nanofluids in disc-type transformer windings. ||| 46379 ||| 46380 ||| 46381 ||| 46382 ||| 46383 ||| 
2020 ||| a stochastic attention cnn model for rumor stance classification. ||| 46384 ||| 28583 ||| 41936 ||| 
2022 ||| medical image segmentation using transformer networks. ||| 27583 ||| 35562 ||| 27585 ||| 
2021 ||| inversion detection of transformer transient hot spot temperature. ||| 46385 ||| 46386 ||| 615 ||| 46387 ||| 
2022 ||| transformer fault diagnosis based on multi-class adaboost algorithm. ||| 46388 ||| 46389 ||| 46390 ||| 46391 ||| 
2020 ||| a hybrid method with adaptive sub-series clustering and attention-based stacked residual lstms for multivariate time series forecasting. ||| 28721 ||| 46392 ||| 46393 ||| 
2020 ||| attention-based sign language recognition network utilizing keyframe sampling and skeletal features. ||| 26955 ||| 46394 ||| 4438 ||| 
2020 ||| non-local attention and densely-connected convolutional neural networks for malignancy suspiciousness classification of gastric ulcer. ||| 16673 ||| 46395 ||| 46396 ||| 46397 ||| 20216 ||| 
2020 ||| learning attention-enhanced spatiotemporal representation for action recognition. ||| 19559 ||| 19560 ||| 19561 ||| 2625 ||| 2626 ||| 43869 ||| 2627 ||| 
2022 ||| masked face recognition with mask transfer and self-attention under the covid-19 pandemic. ||| 5475 ||| 14489 ||| 46398 ||| 23649 ||| 
2019 ||| self-attention network for session-based recommendation with streaming data input. ||| 46399 ||| 46400 ||| 46401 ||| 16655 ||| 
2020 ||| an attention-based user preference matching network for recommender system. ||| 46402 ||| 46403 ||| 3756 ||| 
2019 ||| a novel unbalance compensation method for distribution solid-state transformer based on reduced order generalized integrator. ||| 46404 ||| 46405 ||| 46406 ||| 46407 ||| 46408 ||| 28557 ||| 
2020 ||| sdban: salient object detection using bilateral attention network with dice coefficient loss. ||| 46409 ||| 46410 ||| 41316 ||| 
2018 ||| a fine-grained spatial-temporal attention model for video captioning. ||| 19713 ||| 46411 ||| 19473 ||| 19743 ||| 19019 ||| 
2021 ||| bi-level attention model with topic information for classification. ||| 4166 ||| 46412 ||| 
2020 ||| fault prediction for power transformer using optical spectrum of transformer oil and data mining analysis. ||| 46413 ||| 46414 ||| 21268 ||| 46415 ||| 21267 ||| 46416 ||| 21272 ||| 41223 ||| 21273 ||| 
2021 ||| local structural aware heterogeneous information network embedding based on relational self-attention graph neural network. ||| 455 ||| 46417 ||| 458 ||| 46418 ||| 459 ||| 
2019 ||| a novel transformerless current source inverter for leakage current reduction. ||| 21702 ||| 27514 ||| 29162 ||| 46419 ||| 46420 ||| 
2019 ||| entity disambiguation leveraging multi-perspective attention. ||| 46421 ||| 10525 ||| 44592 ||| 19850 ||| 
2019 ||| attention-based convolutional and recurrent neural networks for driving behavior recognition using smartphone sensor data. ||| 1796 ||| 44718 ||| 706 ||| 46422 ||| 36432 ||| 46423 ||| 44717 ||| 3337 ||| 
2021 ||| automated artifact retouching in morphed images with attention maps. ||| 13609 ||| 46424 ||| 46425 ||| 46426 ||| 
2021 ||| a hybrid isolated bidirectional dc/dc solid-state transformer for dc distribution network. ||| 3906 ||| 29517 ||| 46427 ||| 46428 ||| 43869 ||| 46429 ||| 6553 ||| 
2019 ||| entity alignment algorithm based on dual-attention and incremental learning mechanism. ||| 11466 ||| 46430 ||| 46431 ||| 46432 ||| 46433 ||| 
2021 ||| a lightweight multiscale attention semantic segmentation algorithm for detecting laser welding defects on safety vent of power battery. ||| 46434 ||| 46435 ||| 46054 ||| 46436 ||| 46437 ||| 46438 ||| 254 ||| 
2020 ||| short text embedding autoencoders with attention-based neighborhood preservation. ||| 15703 ||| 46439 ||| 46440 ||| 
2020 ||| an improved attention-based spatiotemporal-stream model for action recognition in videos. ||| 26764 ||| 46441 ||| 19075 ||| 40415 ||| 18083 ||| 
2021 ||| semantic-sca: semantic structure image inpainting with the spatial-channel attention. ||| 6295 ||| 1446 ||| 46442 ||| 
2019 ||| alternative dielectric fluids for transformer insulation system: progress, challenges, and future prospects. ||| 46443 ||| 4338 ||| 46444 ||| 46445 ||| 46446 ||| 46447 ||| 
2020 ||| the n270 in facial s1-s2 paradigm as a biomarker for children with attention-deficit/hyperactivity disorder. ||| 46448 ||| 497 ||| 28688 ||| 
2021 ||| a super resolution algorithm based on attention mechanism and srgan network. ||| 46449 ||| 46450 ||| 
2019 ||| characteristics of tin oxide chromatographic detector for dissolved gases analysis of transformer oil. ||| 46451 ||| 6474 ||| 46452 ||| 46453 ||| 46454 ||| 46455 ||| 46456 ||| 
2019 ||| a method for hot spot temperature prediction of a 10 kv oil-immersed transformer. ||| 46386 ||| 46385 ||| 615 ||| 46387 ||| 46457 ||| 46458 ||| 46459 ||| 
2020 ||| a novel attention cooperative framework for automatic modulation recognition. ||| 46460 ||| 2349 ||| 46461 ||| 46462 ||| 46463 ||| 
2020 ||| speech emotion recognition using 3d convolutions and attention-based sliding recurrent networks with auditory front-ends. ||| 46464 ||| 46465 ||| 46466 ||| 40363 ||| 5095 ||| 46467 ||| 
2020 ||| a natural interaction method of multi-sensory channels for virtual assembly system of power transformer control cabinet. ||| 46468 ||| 46469 ||| 46470 ||| 46471 ||| 46472 ||| 
2020 ||| attention-based radar pri modulation recognition with recurrent neural networks. ||| 44445 ||| 44446 ||| 46473 ||| 
2021 ||| hierarchical self-attention embedded neural network with dense connection for remote-sensing image semantic segmentation. ||| 46474 ||| 633 ||| 46475 ||| 4003 ||| 46476 ||| 27771 ||| 46477 ||| 46478 ||| 
2022 ||| medt: using multimodal encoding-decoding network as in transformer for multimodal sentiment analysis. ||| 31844 ||| 31845 ||| 3248 ||| 46479 ||| 
2019 ||| whisper to normal speech conversion using sequence-to-sequence mapping model with auditory attention. ||| 46480 ||| 46481 ||| 46482 ||| 25471 ||| 540 ||| 
2020 ||| ultra-short-term photovoltaic power prediction based on self-attention mechanism and multi-task learning. ||| 46483 ||| 4807 ||| 35903 ||| 
2021 ||| method for internal fault testing of instrument transformers with sectioned active parts. ||| 46484 ||| 46485 ||| 46486 ||| 46487 ||| 
2020 ||| contextual attention refinement network for real-time semantic segmentation. ||| 46488 ||| 31651 ||| 46489 ||| 46490 ||| 
2019 ||| a dual-attention-based stock price trend prediction model with dual features. ||| 46491 ||| 25737 ||| 43041 ||| 
2021 ||| transformer based language identification for malayalam-english code-mixed text. ||| 46492 ||| 46493 ||| 
2019 ||| scientific literature summarization using document structure and hierarchical attention model. ||| 46494 ||| 46495 ||| 46496 ||| 
2020 ||| comparison of ageing characteristics of superior insulating fluids with mineral oil for power transformer application. ||| 46497 ||| 46498 ||| 46499 ||| 46500 ||| 
2020 ||| just another attention network for remaining useful life prediction of rolling element bearings. ||| 46501 ||| 46502 ||| 7965 ||| 46503 ||| 46504 ||| 
2021 ||| deeplpc-mhanet: multi-head self-attention for augmented kalman filter-based speech enhancement. ||| 46505 ||| 14553 ||| 45306 ||| 
2020 ||| transformer condition monitoring based on load-varied vibration response and gru neural networks. ||| 28495 ||| 28395 ||| 46506 ||| 
2021 ||| 800 kv converter transformer bushing employing nano-hexagonal boron nitride paper using fem. ||| 46507 ||| 46508 ||| 46509 ||| 46510 ||| 
2020 ||| integrating hierarchical attentions for future subevent prediction. ||| 25190 ||| 
2019 ||| self-attention-based bilstm model for short text fine-grained sentiment classification. ||| 11745 ||| 9283 ||| 46511 ||| 46512 ||| 30819 ||| 
2019 ||| hpgat: high-order proximity informed graph attention network. ||| 1293 ||| 46513 ||| 12671 ||| 46514 ||| 46515 ||| 
2017 ||| transient electromagnetic disturbance induced on the ports of intelligent component of electronic instrument transformer due to switching operations in 500 kv gis substations. ||| 46516 ||| 46517 ||| 46518 ||| 46519 ||| 46520 ||| 
2020 ||| hybrid attention-based prototypical network for unfamiliar restaurant food image few-shot recognition. ||| 46521 ||| 35378 ||| 35376 ||| 35377 ||| 683 ||| 35375 ||| 
2019 ||| recognizing food places in egocentric photo-streams using multi-scale atrous convolutional networks and self-attention mechanism. ||| 39105 ||| 39108 ||| 46522 ||| 46523 ||| 46524 ||| 39104 ||| 46525 ||| 39107 ||| 
2019 ||| strawberry verticillium wilt detection network based on multi-task learning and attention. ||| 44547 ||| 16642 ||| 44548 ||| 11333 ||| 
2020 ||| enhanced network representation learning with community aware and relational attention. ||| 46526 ||| 26764 ||| 46527 ||| 46528 ||| 
2020 ||| double attention for multi-label image classification. ||| 46529 ||| 5067 ||| 46530 ||| 46531 ||| 
2022 ||| automatic requirements classification based on graph attention network. ||| 858 ||| 46532 ||| 12646 ||| 29507 ||| 
2020 ||| a convolutional attention residual network for stereo matching. ||| 46533 ||| 46534 ||| 46535 ||| 46536 ||| 44895 ||| 29074 ||| 
2020 ||| attention-based dense decoding network for monocular depth estimation. ||| 14759 ||| 35297 ||| 5950 ||| 35514 ||| 6049 ||| 
2019 ||| a numerical-based attention method for stock market prediction with dual information. ||| 46537 ||| 13160 ||| 
2020 ||| the image super-resolution algorithm based on the dense space attention network. ||| 46538 ||| 46539 ||| 
2021 ||| improving bert with self-supervised attention. ||| 24962 ||| 38569 ||| 22726 ||| 18501 ||| 
2019 ||| a hierarchical attention fused descriptor for 3d point matching. ||| 46279 ||| 46278 ||| 46540 ||| 46277 ||| 46281 ||| 10140 ||| 
2021 ||| diversified semantic attention model for fine-grained entity typing. ||| 44348 ||| 44346 ||| 46541 ||| 44347 ||| 
2020 ||| flexible design scheme for a simple dual-band ultra-high impedance transformer and its application in a balun. ||| 46542 ||| 46543 ||| 46544 ||| 
2020 ||| on some imperative ieee standards for usage of natural ester liquids in transformers. ||| 46443 ||| 4338 ||| 46545 ||| 
2019 ||| transient current similarity-based protection for interconnecting transformers in wind farms. ||| 46546 ||| 43570 ||| 46547 ||| 
2022 ||| route-based proactive content caching using self-attention in hierarchical federated learning. ||| 46548 ||| 20629 ||| 46549 ||| 
2020 ||| colorectal tumor segmentation of ct scans based on a convolutional neural network with an attention mechanism. ||| 46550 ||| 46551 ||| 32066 ||| 46552 ||| 42277 ||| 46553 ||| 10405 ||| 41616 ||| 46554 ||| 46555 ||| 
2019 ||| interactive multi-head attention networks for aspect-level sentiment classification. ||| 44049 ||| 15504 ||| 46556 ||| 46222 ||| 20373 ||| 
2021 ||| student program classification using gated graph attention neural network. ||| 35263 ||| 46557 ||| 35264 ||| 6643 ||| 
2020 ||| efficient visual tracking with stacked channel-spatial attention learning. ||| 46558 ||| 40893 ||| 7989 ||| 
2019 ||| spatial transformer generative adversarial network for robust image super-resolution. ||| 20195 ||| 20196 ||| 19755 ||| 
2021 ||| dealing with data uncertainty for transformer insulation system health index. ||| 23178 ||| 23179 ||| 28557 ||| 
2021 ||| multi-view attention-guided multiple instance detection network for interpretable breast cancer histopathological image diagnosis. ||| 46559 ||| 46560 ||| 46561 ||| 3725 ||| 46562 ||| 
2022 ||| improving clustering-based forecasting of aggregated distribution transformer loadings with gradient boosting and feature selection. ||| 46563 ||| 46564 ||| 46565 ||| 46566 ||| 
2021 ||| combining context-aware embeddings and an attentional deep learning model for arabic affect analysis on twitter. ||| 46567 ||| 46568 ||| 
2020 ||| aidan: an attention-guided dual-path network for pediatric echocardiography segmentation. ||| 46569 ||| 31330 ||| 46570 ||| 46571 ||| 31206 ||| 6575 ||| 27943 ||| 6582 ||| 5476 ||| 
2020 ||| attention network for non-uniform deblurring. ||| 46572 ||| 9395 ||| 46573 ||| 
2019 ||| joint attention mechanism for person re-identification. ||| 28649 ||| 28648 ||| 46574 ||| 46575 ||| 16785 ||| 390 ||| 
2019 ||| a new online temperature compensation technique for electronic instrument transformers. ||| 28556 ||| 46576 ||| 28557 ||| 4086 ||| 6514 ||| 
2019 ||| an interpretable disease onset predictive model using crossover attention mechanism from electronic health records. ||| 5819 ||| 9998 ||| 9695 ||| 4175 ||| 46577 ||| 
2019 ||| automated heartbeat classification exploiting convolutional neural network with channel-wise attention. ||| 46578 ||| 46579 ||| 46580 ||| 46581 ||| 46582 ||| 
2020 ||| agricultural pest super-resolution and identification with attention enhanced residual and dense fusion generative and adversarial network. ||| 46583 ||| 38546 ||| 46584 ||| 46585 ||| 
2020 ||| using lumped element equivalent network model to derive analytical equations for interpretation of transformer frequency responses. ||| 46586 ||| 16208 ||| 18366 ||| 
2019 ||| fault diagnosis of power transformers with membership degree. ||| 46587 ||| 46588 ||| 9705 ||| 
2022 ||| hann: hybrid attention neural network for detecting covid-19 related rumors. ||| 46589 ||| 46590 ||| 46591 ||| 46592 ||| 46593 ||| 
2020 ||| no load simulation and downscaled experiment of uhv single-phase autotransformer under dc bias. ||| 8838 ||| 24897 ||| 46594 ||| 41616 ||| 
2021 ||| directed eeg functional connectivity features to reveal different attention indexes using hierarchical clustering. ||| 46595 ||| 46596 ||| 
2019 ||| spatial-temporal graph attention networks: a deep learning approach for traffic forecasting. ||| 4144 ||| 46597 ||| 1199 ||| 
2021 ||| synthetic aperture radar sar image target recognition algorithm based on attention mechanism. ||| 46598 ||| 4385 ||| 46599 ||| 41559 ||| 
2019 ||| intelligent localization of transformer internal degradations combining deep convolutional neural networks and image segmentation. ||| 41255 ||| 22230 ||| 42331 ||| 46600 ||| 41257 ||| 4600 ||| 
2021 ||| a partial discharge localization method in transformers based on linear conversion and density peak clustering. ||| 46601 ||| 22230 ||| 46602 ||| 46603 ||| 40333 ||| 46604 ||| 
2021 ||| wind power forecasting using attention-based recurrent neural networks: a comparative study. ||| 46605 ||| 46606 ||| 46607 ||| 
2019 ||| automatic prostate zonal segmentation using fully convolutional network with feature pyramid attention. ||| 39367 ||| 39375 ||| 6005 ||| 39368 ||| 39369 ||| 39370 ||| 39371 ||| 39372 ||| 39373 ||| 39374 ||| 
2020 ||| attention based multi-layer fusion of multispectral images for pedestrian detection. ||| 40307 ||| 46608 ||| 46609 ||| 46610 ||| 
2021 ||| dpit: detecting defects of photovoltaic solar cells with image transformers. ||| 46611 ||| 5525 ||| 46612 ||| 9999 ||| 952 ||| 38972 ||| 
2019 ||| attention-based deep learning model for predicting collaborations between different research affiliations. ||| 18425 ||| 46613 ||| 45270 ||| 25530 ||| 46614 ||| 46615 ||| 
2020 ||| a data-driven approach for collision risk early warning in vessel encounter situations using attention-bilstm. ||| 11423 ||| 46616 ||| 7676 ||| 291 ||| 46617 ||| 46618 ||| 
2022 ||| a deep neural network using double self-attention mechanism for als point cloud segmentation. ||| 46619 ||| 5936 ||| 9458 ||| 
2019 ||| accf: learning attentional conformity for collaborative filtering. ||| 46620 ||| 24835 ||| 46621 ||| 728 ||| 9635 ||| 1160 ||| 
2019 ||| remote sensing image change detection based on information transmission and attention mechanism. ||| 46622 ||| 46623 ||| 46624 ||| 46625 ||| 
2021 ||| incorporating relative position information in transformer-based sign language recognition and translation. ||| 46626 ||| 46627 ||| 46628 ||| 
2020 ||| multimodal data processing framework for smart city: a positional-attention based deep learning approach. ||| 21631 ||| 46629 ||| 21632 ||| 6514 ||| 
2019 ||| a deep learning approach for credit scoring of peer-to-peer lending using attention mechanism lstm. ||| 46630 ||| 46631 ||| 46632 ||| 46633 ||| 
2020 ||| comparative study of full and reduced feature scenarios for health index computation of power transformers. ||| 46634 ||| 46635 ||| 
2022 ||| neural network with hierarchical attention mechanism for contextual topic dialogue generation. ||| 443 ||| 46636 ||| 
2020 ||| hierarchical attention-based astronaut gesture recognition: a dataset and cnn model. ||| 46637 ||| 46638 ||| 36117 ||| 
2020 ||| crop leaf disease image super-resolution and identification with dual attention and topology fusion generative adversarial network. ||| 46583 ||| 38546 ||| 46584 ||| 46585 ||| 
2020 ||| capsule networks with word-attention dynamic routing for cultural relics relation extraction. ||| 1254 ||| 30494 ||| 
2021 ||| amsaseg: an attention-based multi-scale atrous convolutional neural network for real-time object segmentation from 3d point cloud. ||| 46639 ||| 41307 ||| 41308 ||| 
2020 ||| a modular multiple dc transformer based dc transmission system for pmsg based offshore wind farm integration. ||| 46640 ||| 44985 ||| 46641 ||| 46642 ||| 
2019 ||| exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition. ||| 645 ||| 12309 ||| 14269 ||| 11983 ||| 647 ||| 12762 ||| 648 ||| 649 ||| 
2020 ||| a general traffic flow prediction approach based on spatial-temporal graph attention. ||| 22466 ||| 22467 ||| 22468 ||| 46643 ||| 46644 ||| 
2021 ||| stereo feature learning based on attention and geometry for absolute hand pose estimation in egocentric stereo views. ||| 46645 ||| 46646 ||| 46647 ||| 46648 ||| 
2018 ||| matching descriptions to spatial entities using a siamese hierarchical attention network. ||| 343 ||| 29965 ||| 46649 ||| 46650 ||| 30572 ||| 
2020 ||| a pornographic images recognition model based on deep one-class classification with visual attention mechanism. ||| 46651 ||| 40350 ||| 32196 ||| 46062 ||| 15844 ||| 46652 ||| 
2019 ||| integrating an attention mechanism and convolution collaborative filtering for document context-aware rating prediction. ||| 46653 ||| 46654 ||| 46655 ||| 46656 ||| 46657 ||| 
2020 ||| effect of temperature on space charge distribution in two layers of transformer oil and impregnated pressboard under dc voltage. ||| 4797 ||| 46658 ||| 46659 ||| 46660 ||| 46661 ||| 
2022 ||| vehicle re-identification based on global relational attention and multi-granularity feature learning. ||| 6862 ||| 46662 ||| 46663 ||| 46664 ||| 46665 ||| 
2017 ||| towards automatic real-time estimation of observed learner's attention using psychophysiological and affective signals: the touch-typing study case. ||| 46666 ||| 46667 ||| 46668 ||| 41741 ||| 
2018 ||| full-bridge llc resonant converter with series-parallel connected transformers for electric vehicle on-board charger. ||| 46669 ||| 16679 ||| 5790 ||| 46670 ||| 
2021 ||| using lstm neural network based on improved pso and attention mechanism for predicting the effluent cod in a wastewater treatment plant. ||| 189 ||| 46671 ||| 5107 ||| 46672 ||| 
2019 ||| visual attention guided pixel-wise just noticeable difference model. ||| 46673 ||| 32593 ||| 8260 ||| 32592 ||| 6553 ||| 40131 ||| 
2022 ||| pulmonary nodule detection using 3-d residual u-net oriented context-guided attention and multi-branch classification network. ||| 46674 ||| 46675 ||| 46676 ||| 46677 ||| 13105 ||| 
2020 ||| a new modular multilevel ac/ac converter using hf transformer. ||| 46678 ||| 46679 ||| 46680 ||| 46681 ||| 46682 ||| 46683 ||| 46684 ||| 
2021 ||| a modified generative adversarial network using spatial and channel-wise attention for cs-mri reconstruction. ||| 21160 ||| 34858 ||| 34859 ||| 
2019 ||| ahcnet: an application of attention mechanism and hybrid connection for liver tumor segmentation in ct volumes. ||| 29706 ||| 29705 ||| 46685 ||| 46686 ||| 
2019 ||| natural answer generation with attention over instances. ||| 46687 ||| 1420 ||| 
2018 ||| enhancing pv penetration in lv networks using reactive power control and on load tap changer with existing transformers. ||| 46688 ||| 46689 ||| 
2021 ||| mixed high-order non-local attention network for single image super-resolution. ||| 248 ||| 42269 ||| 46690 ||| 46691 ||| 249 ||| 
2019 ||| complex impedance transformers based on allowed and forbidden regions. ||| 46692 ||| 46693 ||| 
2019 ||| numerical and experimental investigation of temperature distribution for oil-immersed transformer winding based on dimensionless least-squares and upwind finite element method. ||| 14570 ||| 4613 ||| 46694 ||| 46695 ||| 46696 ||| 1556 ||| 
2021 ||| deep neural networks using residual fast-slow refined highway and global atomic spatial attention for action recognition and detection. ||| 21732 ||| 21733 ||| 
2021 ||| classification of remote sensing images using efficientnet-b3 cnn model with attention. ||| 46697 ||| 46698 ||| 6726 ||| 46699 ||| 30374 ||| 
2019 ||| design and analysis of pwm inverter for 100kva solid state transformer in a distribution system. ||| 46700 ||| 46701 ||| 
2020 ||| stability assessment of voltage control strategies for smart transformer-fed distribution grid. ||| 22166 ||| 10868 ||| 369 ||| 5287 ||| 
2019 ||| diagnosing transformer winding deformation faults based on the analysis of binary image obtained from fra signature. ||| 43684 ||| 43584 ||| 31931 ||| 43618 ||| 46702 ||| 43685 ||| 
2019 ||| isolation transformer for 3-port 3-phase dual-active bridge converters in medium voltage level. ||| 43682 ||| 22067 ||| 
2020 ||| a single-phase line-interactive ups system for transformer-coupled loading conditions. ||| 46703 ||| 46704 ||| 
2019 ||| energy harvesting-based smart transportation mode detection system via attention-based lstm. ||| 46705 ||| 46706 ||| 23367 ||| 46707 ||| 17433 ||| 21244 ||| 
2020 ||| attention-based pose sequence machine for 3d hand pose estimation. ||| 46708 ||| 46709 ||| 46710 ||| 40589 ||| 29190 ||| 
2021 ||| wide-range digital-analog mixed calibration technology of a dc instrument transformer test set. ||| 46711 ||| 46712 ||| 46713 ||| 28460 ||| 6924 ||| 5042 ||| 46714 ||| 
2020 ||| a novel curve database for moisture evaluation of transformer oil-immersed cellulose insulation using fds and exponential decay model. ||| 28373 ||| 46715 ||| 28372 ||| 18177 ||| 7079 ||| 
2019 ||| unsupervised object-level image-to-image translation using positional attention bi-flow generative network. ||| 46716 ||| 28839 ||| 5746 ||| 
2018 ||| dc-transformer modelling, analysis and comparison of the experimental investigation of a non-inverting and non-isolated nx multilevel boost converter (nx mbc) for low to high dc voltage applications. ||| 22152 ||| 10861 ||| 43623 ||| 10860 ||| 
2021 ||| a new transformerless ultra high gain dc-dc converter for dc microgrid application. ||| 35117 ||| 46717 ||| 46718 ||| 46719 ||| 46720 ||| 46721 ||| 46722 ||| 21925 ||| 46723 ||| 46724 ||| 
2021 ||| document-level neural tts using curriculum learning and attention masking. ||| 46725 ||| 10985 ||| 
2021 ||| ladnet: an ultra-lightweight and efficient dilated residual network with light-attention module. ||| 46726 ||| 11390 ||| 46727 ||| 46728 ||| 
2019 ||| a session-based customer preference learning method by using the gated recurrent units with attention function. ||| 46729 ||| 46730 ||| 
2020 ||| gate: graph-attention augmented temporal neural network for medication recommendation. ||| 46731 ||| 10971 ||| 449 ||| 
2021 ||| atpgnn: reconstruction of neighborhood in graph neural networks with attention-based topological patterns. ||| 9096 ||| 46732 ||| 46733 ||| 46734 ||| 46735 ||| 11162 ||| 12300 ||| 31717 ||| 
2019 ||| weighted optical flow prediction and attention model for object tracking. ||| 16750 ||| 46736 ||| 46737 ||| 
2020 ||| a partitioning-stacking prediction fusion network based on an improved attention u-net for stroke lesion segmentation. ||| 46145 ||| 9665 ||| 46738 ||| 46739 ||| 46740 ||| 
2020 ||| web application attack detection based on attention and gated convolution networks. ||| 46741 ||| 46249 ||| 24731 ||| 46248 ||| 43701 ||| 46250 ||| 
2020 ||| design of high efficiency controller for wide input range dc-dc piezoelectric transformer converter. ||| 46742 ||| 17260 ||| 
2019 ||| attentional pattern classification for automatic dementia detection. ||| 46743 ||| 46744 ||| 46745 ||| 46746 ||| 46747 ||| 
2020 ||| multiple-aspect attentional graph neural networks for online social network user localization. ||| 9013 ||| 46748 ||| 19553 ||| 30646 ||| 9010 ||| 
2021 ||| multi-attention ghost residual fusion network for image classification. ||| 46749 ||| 46750 ||| 46751 ||| 46752 ||| 46753 ||| 
2020 ||| a text normalization method for speech synthesis based on local attention mechanism. ||| 41549 ||| 46754 ||| 46755 ||| 
2021 ||| dielectric response of the oil-paper insulation system in nanofluid-based transformers. ||| 46756 ||| 46757 ||| 24419 ||| 46758 ||| 3419 ||| 46759 ||| 
2020 ||| a calculation method to adjust the short-circuit impedance of a transformer. ||| 24630 ||| 46760 ||| 46761 ||| 46762 ||| 46763 ||| 43738 ||| 2969 ||| 
2022 ||| attention retrieval model for entity relation extraction from biological literature. ||| 46764 ||| 46765 ||| 46766 ||| 46767 ||| 46768 ||| 
2020 ||| sarcasm detection using multi-head attention based bidirectional lstm. ||| 44677 ||| 44679 ||| 44678 ||| 44682 ||| 44681 ||| 
2019 ||| interpretability analysis of heartbeat classification based on heartbeat activity's global sequence features and bilstm-attention neural network. ||| 46769 ||| 30950 ||| 46770 ||| 18735 ||| 20186 ||| 
2020 ||| adecnn: an improved model for aspect-level sentiment analysis based on deformable cnn and attention. ||| 1921 ||| 46771 ||| 46772 ||| 
2021 ||| an alternating training method of attention-based adapters for visual explanation of multi-domain satellite images. ||| 46773 ||| 46774 ||| 46775 ||| 46776 ||| 46777 ||| 
2021 ||| marn: multi-scale attention retinex network for low-light image enhancement. ||| 1340 ||| 46778 ||| 
2020 ||| hagn: hierarchical attention guided network for crowd counting. ||| 46779 ||| 46780 ||| 46781 ||| 
2020 ||| common-mode stability test and design guidelines for a transformer-based push-pull power amplifier. ||| 41772 ||| 41773 ||| 
2021 ||| transformer oil diagnosis based on a capacitive sensor frequency response analysis. ||| 852 ||| 46782 ||| 46783 ||| 852 ||| 40724 ||| 40725 ||| 40726 ||| 10314 ||| 10924 ||| 
2018 ||| lexicon-enhanced lstm with attention for general sentiment analysis. ||| 4186 ||| 4185 ||| 17433 ||| 46784 ||| 30878 ||| 
2021 ||| three-phase transformer inrush current reduction strategy based on prefluxing and controlled switching. ||| 46785 ||| 21337 ||| 2855 ||| 46786 ||| 46787 ||| 46788 ||| 
2021 ||| a two-stage multiscale residual attention network for light guide plate defect detection. ||| 46789 ||| 14504 ||| 46790 ||| 
2020 ||| a location-velocity-temporal attention lstm model for pedestrian trajectory prediction. ||| 7322 ||| 151 ||| 152 ||| 
2019 ||| ms-pointer network: abstractive text summary based on multi-head self-attention. ||| 42177 ||| 46791 ||| 29202 ||| 46792 ||| 
2021 ||| pamsgan: pyramid attention mechanism-oriented symmetry generative adversarial network for motion image deblurring. ||| 46793 ||| 
2021 ||| multi-gate attention network for image captioning. ||| 28801 ||| 46794 ||| 5746 ||| 5017 ||| 46795 ||| 
2020 ||| attention-based siamese region proposals network for visual tracking. ||| 1704 ||| 2760 ||| 46796 ||| 46797 ||| 46798 ||| 
2021 ||| lattegan: visually guided language attention for multi-turn text-conditioned image manipulation. ||| 1981 ||| 1983 ||| 1982 ||| 726 ||| 1985 ||| 
2020 ||| hybrid grey wolf optimizer for transformer fault diagnosis using dissolved gases considering uncertainty in measurements. ||| 46799 ||| 46238 ||| 46635 ||| 
2020 ||| the dynamical interplay of collective attention, awareness and epidemics spreading in the multiplex social networks during covid-19. ||| 46800 ||| 46801 ||| 46802 ||| 46803 ||| 
2019 ||| modeling and analyzing the influence of multi-information coexistence on attention. ||| 43503 ||| 46804 ||| 21653 ||| 46805 ||| 
2019 ||| attention-based dense point cloud reconstruction from a single image. ||| 5017 ||| 46806 ||| 46807 ||| 19838 ||| 28756 ||| 
2021 ||| naem: noisy attention exploration module for deep reinforcement learning. ||| 46808 ||| 46809 ||| 46810 ||| 46811 ||| 46812 ||| 
2019 ||| second-order response transform attention network for image classification. ||| 11988 ||| 46813 ||| 11986 ||| 11987 ||| 2304 ||| 817 ||| 19289 ||| 
2021 ||| transformerless high step-up dc-dc converter with low voltage stress for fuel cells. ||| 2378 ||| 22223 ||| 46814 ||| 
2022 ||| agnet: attention guided sparse depth completion using convolutional neural networks. ||| 46815 ||| 2835 ||| 
2020 ||| improved self-organizing map clustering of power transformer dissolved gas analysis using inputs pre-processing. ||| 46816 ||| 46817 ||| 46818 ||| 
2019 ||| assessment of hydraulic network models in predicting reverse flows in od cooled disc type transformer windings. ||| 586 ||| 16208 ||| 
2019 ||| real-time crop recognition in transplanted fields with prominent weed growth: a visual-attention-based approach. ||| 5192 ||| 46819 ||| 46820 ||| 46821 ||| 43966 ||| 46822 ||| 
2020 ||| attention-based lstm network for rotatory machine remaining useful life prediction. ||| 3386 ||| 817 ||| 28507 ||| 46823 ||| 7854 ||| 
2019 ||| a semi-supervised synthetic aperture radar (sar) image recognition algorithm based on an attention mechanism and bias-variance decomposition. ||| 4176 ||| 24999 ||| 1224 ||| 30475 ||| 30434 ||| 
2021 ||| improved attention mechanism and residual network for remote sensing image scene classification. ||| 46824 ||| 46825 ||| 17030 ||| 46826 ||| 10409 ||| 46827 ||| 
2020 ||| tser: a two-stage character segmentation network with two-stream attention and edge refinement. ||| 46828 ||| 3888 ||| 46829 ||| 46830 ||| 46831 ||| 
2022 ||| an abnormal traffic detection model combined biindrnn with global attention. ||| 46832 ||| 46833 ||| 46834 ||| 12371 ||| 46835 ||| 
2021 ||| remaining useful life estimation combining two-step maximal information coefficient and temporal convolutional network with attention mechanism. ||| 46836 ||| 46837 ||| 46838 ||| 46839 ||| 46840 ||| 
2020 ||| the effects of visual stimuli on attention in children with autism spectrum disorder: an eye-tracking study. ||| 4346 ||| 4347 ||| 4348 ||| 26062 ||| 11828 ||| 
2020 ||| supersaliency: a novel pipeline for predicting smooth pursuit-based attention improves generalisability of video saliency. ||| 36923 ||| 36924 ||| 
2021 ||| a method of steel bar image segmentation based on multi-attention u-net. ||| 46841 ||| 46842 ||| 46843 ||| 46844 ||| 
2020 ||| automatic modulation classification scheme based on lstm with random erasing and attention mechanism. ||| 46845 ||| 5279 ||| 3888 ||| 17120 ||| 46846 ||| 
2020 ||| multi-scale feature channel attention generative adversarial network for face sketch synthesis. ||| 46847 ||| 46848 ||| 46849 ||| 39932 ||| 968 ||| 
2020 ||| low light image enhancement network with attention mechanism and retinex model. ||| 471 ||| 32744 ||| 1858 ||| 
2021 ||| an efficient approach with application of linear and nonlinear models for evaluation of power transformer health index. ||| 46850 ||| 46851 ||| 46852 ||| 
2020 ||| real-time ultrasound image despeckling using mixed-attention mechanism based residual unet. ||| 46853 ||| 28926 ||| 
2020 ||| col-gan: plausible and collision-less trajectory prediction by attention-based gan. ||| 46157 ||| 46854 ||| 28244 ||| 20133 ||| 
2021 ||| abnormal detection of electricity consumption of user based on particle swarm optimization and long short term memory with the attention mechanism. ||| 46855 ||| 3279 ||| 46856 ||| 954 ||| 46857 ||| 5474 ||| 
2019 ||| i-vals: visual attention localization for mobile service computing. ||| 46858 ||| 692 ||| 8207 ||| 33758 ||| 
2020 ||| convolutional-neural-network-based partial discharge diagnosis for power transformer using uhf sensor. ||| 46859 ||| 46860 ||| 46861 ||| 46862 ||| 46863 ||| 
2020 ||| towards a deep attention-based sequential recommender system. ||| 41663 ||| 5988 ||| 41665 ||| 41667 ||| 23427 ||| 
2019 ||| dielectric and thermal performance up-gradation of transformer oil using valuable nano-particles. ||| 46864 ||| 46865 ||| 46866 ||| 46867 ||| 46868 ||| 46869 ||| 
2019 ||| a multi-layer dual attention deep learning model with refined word embeddings for aspect-based sentiment analysis. ||| 46870 ||| 46871 ||| 46872 ||| 46873 ||| 46874 ||| 46875 ||| 46876 ||| 
2019 ||| attention dense-u-net for automatic breast mass segmentation in digital mammogram. ||| 40157 ||| 41067 ||| 46877 ||| 46878 ||| 
2022 ||| a dual-staged attention based conversion-gated long short term memory for multivariable time series prediction. ||| 46879 ||| 28299 ||| 
2020 ||| single image reflection removal via attention model and sn-gan. ||| 41082 ||| 45490 ||| 2853 ||| 46880 ||| 2858 ||| 
2018 ||| development of power electronic distribution transformer based on adaptive pi controller. ||| 46881 ||| 46882 ||| 46883 ||| 46884 ||| 46885 ||| 46886 ||| 
2021 ||| an efficient video coding system with an adaptive overfitted multi-scale attention network. ||| 5111 ||| 46887 ||| 44794 ||| 3034 ||| 46888 ||| 6843 ||| 6701 ||| 
2019 ||| wide or narrow? a visual attention inspired model for view-type classification. ||| 7849 ||| 7848 ||| 7850 ||| 7851 ||| 
2020 ||| influence of the load on the impulse frequency response approach based diagnosis of transformer's inter-turn short-circuit. ||| 46889 ||| 46890 ||| 46891 ||| 
2020 ||| deep learning for load forecasting: sequence to sequence recurrent neural networks with attention. ||| 46892 ||| 46893 ||| 
2022 ||| efficient lightweight attention network for face recognition. ||| 989 ||| 34444 ||| 10572 ||| 46894 ||| 
2019 ||| a novel deep recurrent belief network model for trend prediction of transformer dga data. ||| 46288 ||| 12382 ||| 989 ||| 46290 ||| 37325 ||| 
2021 ||| multi-attention generative adversarial network for multivariate time series prediction. ||| 12524 ||| 17303 ||| 17316 ||| 3212 ||| 14133 ||| 17317 ||| 
2021 ||| study on how expert and novice pilots can distribute their visual attention to improve flight performance. ||| 46895 ||| 46896 ||| 8293 ||| 46897 ||| 46898 ||| 46899 ||| 46900 ||| 
2020 ||| a novel channel and temporal-wise attention in convolutional networks for multivariate time series classification. ||| 6560 ||| 46901 ||| 29121 ||| 13196 ||| 29124 ||| 
2021 ||| crowd density estimation by using attention based capsule network and multi-column cnn. ||| 46902 ||| 13310 ||| 46903 ||| 
2020 ||| next basket recommendation model based on attribute-aware multi-level attention. ||| 15198 ||| 46904 ||| 46905 ||| 
2021 ||| scep - a new image dimensional emotion recognition model based on spatial and channel-wise attention mechanisms. ||| 1717 ||| 46906 ||| 46907 ||| 46908 ||| 46909 ||| 46910 ||| 
2021 ||| attention-based design and user decisions on information sharing: a thematic literature review. ||| 26083 ||| 26084 ||| 11483 ||| 
2020 ||| attention-based adaptive memory network for recommendation with review and rating. ||| 683 ||| 22439 ||| 46911 ||| 4550 ||| 41623 ||| 
2021 ||| a serial-parallel self-attention network joint with multi-scale dilated convolution. ||| 46912 ||| 46913 ||| 46914 ||| 46915 ||| 46916 ||| 
2021 ||| a method for fans' potential malfunction detection of onaf transformer using top-oil temperature monitoring. ||| 37978 ||| 46917 ||| 7261 ||| 22106 ||| 46918 ||| 
2022 ||| transformer network for remaining useful life prediction of lithium-ion batteries. ||| 46919 ||| 46920 ||| 46921 ||| 
2020 ||| saliency guided self-attention network for weakly and semi-supervised semantic segmentation. ||| 38760 ||| 38761 ||| 
2020 ||| classification of selective attention within steady-state somatosensory evoked potentials from dry electrodes using mutual information-based spatio-spectral feature selection. ||| 16087 ||| 16088 ||| 16089 ||| 46922 ||| 16090 ||| 
2020 ||| a novel analysis approach for dual-frequency parallel transmission-line transformer with complex terminal loads. ||| 46923 ||| 1117 ||| 46924 ||| 46925 ||| 46926 ||| 
2019 ||| sfa: small faces attention face detector. ||| 32489 ||| 32490 ||| 8473 ||| 24526 ||| 
2020 ||| an efficient procedure for temperature calculation of high current leads in large power transformers. ||| 46927 ||| 46928 ||| 3369 ||| 46929 ||| 46930 ||| 
2019 ||| impact of load ramping on power transformer dissolved gas analysis. ||| 46931 ||| 42083 ||| 46932 ||| 46933 ||| 1371 ||| 28557 ||| 43685 ||| 
2018 ||| transformer fault condition prognosis using vibration signals over cloud environment. ||| 28312 ||| 28309 ||| 46934 ||| 
2020 ||| rrgccan: re-ranking via graph convolution channel attention network for person re-identification. ||| 46935 ||| 29608 ||| 24102 ||| 45350 ||| 43807 ||| 
2019 ||| k-reciprocal harmonious attention network for video-based person re-identification. ||| 20270 ||| 19399 ||| 19401 ||| 12300 ||| 5474 ||| 44800 ||| 46936 ||| 
2021 |||  arrangement transformer under single-line-to-ground fault. ||| 46937 ||| 22107 ||| 13339 ||| 46938 ||| 5981 ||| 
2021 ||| parameter-free attention in fmri decoding. ||| 40621 ||| 46939 ||| 46940 ||| 46941 ||| 
2019 ||| multi-task learning for authorship attribution via topic approximation and competitive attention. ||| 17827 ||| 18731 ||| 26529 ||| 
2020 ||| image restoration via deep memory-based latent attention network. ||| 45532 ||| 2170 ||| 46942 ||| 46943 ||| 45533 ||| 45534 ||| 
2022 ||| improved yolov4 based on attention mechanism for ship detection in sar images. ||| 46944 ||| 3138 ||| 46945 ||| 46946 ||| 
2019 ||| hierarchical attention and knowledge matching networks with information enhancement for end-to-end task-oriented dialog systems. ||| 46947 ||| 1780 ||| 46948 ||| 46949 ||| 46950 ||| 
2019 ||| short text understanding combining text conceptualization and transformer embedding. ||| 5536 ||| 31567 ||| 46075 ||| 46076 ||| 
2020 ||| fret: functional reinforced transformer with bert for code summarization. ||| 46951 ||| 46952 ||| 46953 ||| 5024 ||| 5025 ||| 
2020 ||| compilation optimization pass selection using gate graph attention neural network for reliability improvement. ||| 18469 ||| 43068 ||| 43069 ||| 39818 ||| 14510 ||| 5445 ||| 
2021 ||| chord conditioned melody generation with transformer based decoders. ||| 11903 ||| 11902 ||| 46954 ||| 11904 ||| 11906 ||| 
2021 ||| biomedical text similarity evaluation using attention mechanism and siamese neural network. ||| 43342 ||| 46955 ||| 46956 ||| 
2020 ||| pran: progressive residual attention network for super resolution. ||| 46957 ||| 4807 ||| 17090 ||| 46958 ||| 
2021 ||| attention to wi-fi diversity: resource management in wlans with heterogeneous aps. ||| 46959 ||| 852 ||| 46960 ||| 46961 ||| 21740 ||| 1867 ||| 46962 ||| 852 ||| 46963 ||| 46964 ||| 46965 ||| 29780 ||| 46966 ||| 
2022 ||| transformerless quadruple high step-up dc/dc converter using coupled inductors. ||| 46967 ||| 46968 ||| 46969 ||| 46970 ||| 
2019 ||| top-n-targets-balanced recommendation based on attentional sequence-to-sequence learning. ||| 46971 ||| 46972 ||| 46973 ||| 46974 ||| 
2019 ||| sarcasm detection using soft attention-based bidirectional long short-term memory model with convolution network. ||| 46975 ||| 958 ||| 46976 ||| 46977 ||| 46978 ||| 46979 ||| 
2022 ||| linear arrhenius-weibull model for power transformer thermal stress assessment. ||| 16210 ||| 16212 ||| 46980 ||| 
2021 ||| attention meets perturbations: robust and interpretable attention with adversarial training. ||| 25316 ||| 25317 ||| 
2020 ||| hovering control of submersible transformer inspection robot based on asmbc method. ||| 46981 ||| 46982 ||| 12815 ||| 9440 ||| 
2019 ||| syntax-directed hybrid attention network for aspect-level sentiment analysis. ||| 7629 ||| 21132 ||| 46983 ||| 10525 ||| 3279 ||| 31816 ||| 
2022 ||| simple and effective multimodal learning based on pre-trained transformer models. ||| 46984 ||| 46985 ||| 46986 ||| 
2020 ||| real-time speech enhancement algorithm based on attention lstm. ||| 28645 ||| 46987 ||| 28654 ||| 46988 ||| 44010 ||| 
2022 ||| attention-based applications in extended reality to support autistic users: a systematic review. ||| 15393 ||| 21060 ||| 15395 ||| 
2021 ||| multi-perspective attention network for fast temporal moment localization. ||| 46989 ||| 29482 ||| 
2018 ||| variation of discharge characteristics with temperature in moving transformer oil contaminated by metallic particles. ||| 46990 ||| 46991 ||| 46992 ||| 37870 ||| 46993 ||| 
2020 ||| lightweight single image super-resolution with multi-scale spatial attention networks. ||| 33951 ||| 32238 ||| 
2019 ||| electric field distribution characteristics and space charge motion process in transformer oil under impulse voltage. ||| 46288 ||| 46994 ||| 28035 ||| 46995 ||| 46996 ||| 46997 ||| 46290 ||| 
2020 ||| exploring multi-level attention and semantic relationship for remote sensing image captioning. ||| 46998 ||| 6922 ||| 6627 ||| 
2020 ||| robot-assisted joint attention: a comparative study between children with autism spectrum disorder and typically developing children in interaction with nao. ||| 46999 ||| 47000 ||| 47001 ||| 47002 ||| 47003 ||| 47004 ||| 47005 ||| 
2018 ||| improved method to obtain the online impulse frequency response signature of a power transformer by multi scale complex cwt. ||| 43684 ||| 31931 ||| 43584 ||| 47006 ||| 47007 ||| 47008 ||| 43685 ||| 
2021 ||| a 25.1 dbm 25.9-db gain 25.4% pae x-band power amplifier utilizing voltage combining transformer in 65-nm cmos. ||| 41772 ||| 41773 ||| 
2020 ||| forecasting stock prices using a hybrid deep learning model integrating attention mechanism, multi-layer perceptron, and bidirectional long-short term memory neural network. ||| 12760 ||| 19548 ||| 47009 ||| 
2018 ||| semi-empirical model for precise analysis of copper losses in high-frequency transformers. ||| 12719 ||| 1556 ||| 
2019 ||| an attention enhanced bidirectional lstm for early forest fire smoke recognition. ||| 14181 ||| 2320 ||| 47010 ||| 7659 ||| 
2020 ||| zero-shot visual recognition via semantic attention-based compare network. ||| 13515 ||| 47011 ||| 6511 ||| 14710 ||| 
2019 ||| an attention-based bilstm-crf model for chinese clinic named entity recognition. ||| 47012 ||| 47013 ||| 47014 ||| 19749 ||| 1419 ||| 
2019 ||| a memory term reduction approach for digital pre-distortion using the attention mechanism. ||| 29620 ||| 47015 ||| 47016 ||| 47017 ||| 47018 ||| 
2020 ||| aafm: adaptive attention fusion mechanism for crowd counting. ||| 46779 ||| 47019 ||| 46781 ||| 
2020 ||| hot-spot temperature forecasting of the instrument transformer using an artificial neural network. ||| 47020 ||| 47021 ||| 3882 ||| 47022 ||| 47023 ||| 47024 ||| 47025 ||| 
2020 ||| an efficient and accurate ddpg-based recurrent attention model for object localization. ||| 47026 ||| 
2018 ||| determination of core losses in open-core power voltage transformers. ||| 46485 ||| 47027 ||| 47028 ||| 
2021 ||| esophageal tumor segmentation in ct images using a dilated dense attention unet (ddaunet). ||| 35854 ||| 35855 ||| 47029 ||| 47030 ||| 47031 ||| 47032 ||| 47033 ||| 35859 ||| 
2019 ||| deep attention neural network for multi-label classification in unmanned aerial vehicle imagery. ||| 47034 ||| 6726 ||| 46699 ||| 47035 ||| 40869 ||| 
2022 ||| satsal: a multi-level self-attention based architecture for visual saliency prediction. ||| 11575 ||| 11574 ||| 47036 ||| 11576 ||| 47037 ||| 47038 ||| 7847 ||| 4713 ||| 
2019 ||| distant supervision for relation extraction via piecewise attention and bag-level contextual inference. ||| 4801 ||| 47039 ||| 4800 ||| 4802 ||| 47040 ||| 4803 ||| 
2021 ||| appearance guidance attention for multi-object tracking. ||| 20639 ||| 4120 ||| 47041 ||| 47042 ||| 47043 ||| 
2021 ||| semi-attentionae: an integrated model for graph representation learning. ||| 47044 ||| 602 ||| 47045 ||| 26093 ||| 
2020 ||| a concrete dam deformation prediction method based on lstm with attention mechanism. ||| 47046 ||| 47047 ||| 47048 ||| 39144 ||| 24519 ||| 47049 ||| 1717 ||| 
2020 ||| graph attention networks with local structure awareness for knowledge graph completion. ||| 41620 ||| 5840 ||| 13744 ||| 
2020 ||| text summarization method based on double attention pointer network. ||| 264 ||| 40499 ||| 631 ||| 613 ||| 262 ||| 
2019 ||| attention-guided coarse-to-fine network for 2d face alignment in the wild. ||| 189 ||| 25695 ||| 25471 ||| 25697 ||| 
2021 ||| multi-horizon electricity load and price forecasting using an interpretable multi-head self-attention and eemd-based framework. ||| 47050 ||| 47051 ||| 
2019 ||| multi-attention and incorporating background information model for chest x-ray image report generation. ||| 17578 ||| 47052 ||| 7804 ||| 43807 ||| 
2020 ||| human action performance using deep neuro-fuzzy recurrent attention model. ||| 24589 ||| 13077 ||| 47053 ||| 24591 ||| 
2021 ||| polysemy needs attention: short-text topic discovery with global and multi-sense information. ||| 47054 ||| 13171 ||| 340 ||| 27710 ||| 
2021 ||| constructing bi-order-transformer-crf with neural cosine similarity function for power metering entity recognition. ||| 47055 ||| 38234 ||| 47056 ||| 47057 ||| 4417 ||| 47058 ||| 
2020 ||| hierarchical transformer encoder with structured representation for abstract reasoning. ||| 32200 ||| 32201 ||| 
2020 ||| channel transformer network. ||| 47059 ||| 12786 ||| 47060 ||| 
2020 ||| novel fault location method for power systems based on attention mechanism and double structure gru neural network. ||| 2532 ||| 47061 ||| 47062 ||| 29446 ||| 47063 ||| 47064 ||| 
2019 ||| multiple object tracking with attention to appearance, structure, motion and size. ||| 47065 ||| 12273 ||| 47066 ||| 
2019 ||| a fault diagnosis model of power transformers based on dissolved gas analysis features selection and improved krill herd algorithm optimized support vector machine. ||| 18177 ||| 633 ||| 29216 ||| 47067 ||| 28373 ||| 28375 ||| 47068 ||| 4816 ||| 
2020 ||| group activity recognition by using effective multiple modality relation representation with temporal-spatial attention. ||| 47069 ||| 47070 ||| 6626 ||| 47071 ||| 952 ||| 3534 ||| 
2021 ||| named entity recognition in electric power metering domain based on attention mechanism. ||| 47055 ||| 14785 ||| 398 ||| 47058 ||| 47072 ||| 4417 ||| 47056 ||| 47057 ||| 
2020 ||| a hybrid bert model that incorporates label semantics via adjustive attention for multi-label text classification. ||| 47073 ||| 20250 ||| 5463 ||| 28104 ||| 
2020 ||| detection of 2fal furanic compound in transformer oil using optical spectroscopy method and verification using morse oscillation theory. ||| 46415 ||| 21268 ||| 21267 ||| 47074 ||| 21273 ||| 
2021 ||| single-trial decoding of motion direction during visual attention from local field potential signals. ||| 47075 ||| 47076 ||| 41630 ||| 
2019 ||| a bi-attention adversarial network for prostate cancer segmentation. ||| 16821 ||| 16825 ||| 47077 ||| 16823 ||| 24049 ||| 47078 ||| 5298 ||| 16826 ||| 5300 ||| 
2020 ||| a discriminative dual-stream model with a novel sustained attention mechanism for skeleton-based human action recognition. ||| 47079 ||| 47080 ||| 47081 ||| 1748 ||| 1748 ||| 
2019 ||| scale pyramid attention for single shot multibox detector. ||| 4796 ||| 220 ||| 47082 ||| 47083 ||| 38972 ||| 34405 ||| 
2020 ||| probabilistic health index-based apparent age estimation for power transformers. ||| 47084 ||| 47085 ||| 47086 ||| 14066 ||| 47087 ||| 
2020 ||| tblc-rattention: a deep neural network model for recognizing the emotional tendency of chinese medical comment. ||| 47088 ||| 47089 ||| 47090 ||| 47091 ||| 47092 ||| 6133 ||| 
2020 ||| visual attention, mental stress and gender: a study using physiological signals. ||| 46365 ||| 47093 ||| 46366 ||| 47094 ||| 
2020 ||| attentional generative adversarial networks with representativeness and diversity for generating text to realistic image. ||| 47095 ||| 218 ||| 
2019 ||| transmission lines positive sequence parameters estimation and instrument transformers calibration based on pmu measurement error model. ||| 5187 ||| 33837 ||| 33838 ||| 33839 ||| 
2021 ||| a transformer fault diagnosis method based on parameters optimization of hybrid kernel extreme learning machine. ||| 46388 ||| 46390 ||| 47096 ||| 46389 ||| 
2020 ||| two-level progressive attention convolutional network for fine-grained image recognition. ||| 17676 ||| 8952 ||| 1241 ||| 30457 ||| 30458 ||| 
2021 ||| a dynamic spatial-temporal attention-based gru model with healthy features for state-of-health estimation of lithium-ion batteries. ||| 47097 ||| 47098 ||| 
2021 ||| fraud detection in online product review systems via heterogeneous graph transformer. ||| 47099 ||| 47100 ||| 29209 ||| 
2020 ||| sentiment classification based on part-of-speech and self-attention mechanism. ||| 47101 ||| 47102 ||| 47103 ||| 
2019 ||| influential factors and correction method of furfural content in transformer oil. ||| 47104 ||| 31972 ||| 3574 ||| 47105 ||| 
2021 ||| light field image super-resolution via mutual attention guidance. ||| 5166 ||| 4151 ||| 
2019 ||| a deep neural network based on an attention mechanism for sar ship detection in multiscale and complex scenarios. ||| 2230 ||| 25699 ||| 25698 ||| 25700 ||| 400 ||| 
2020 ||| high accuracy individual identification model of crested ibis (nipponia nippon) based on autoencoder with self-attention. ||| 47106 ||| 13171 ||| 47107 ||| 29460 ||| 
2019 ||| the experimental and cfd research on the pressure reduction process of the double rotor hydraulic transformer. ||| 45679 ||| 45680 ||| 
2021 ||| unpaired stain style transfer using invertible neural networks based on channel attention and long-range residual. ||| 47108 ||| 47109 ||| 19129 ||| 47110 ||| 47111 ||| 47112 ||| 47113 ||| 47114 ||| 47115 ||| 47116 ||| 1216 ||| 
2019 ||| classifying transformer winding deformation fault types and degrees using fra based on support vector machine. ||| 47117 ||| 43684 ||| 31931 ||| 43584 ||| 43618 ||| 43685 ||| 
2020 ||| experimental research on lightning disturbance characteristics of 10-kv fusion voltage transformer intelligent component acquisition port. ||| 6627 ||| 47118 ||| 47119 ||| 47120 ||| 47121 ||| 47122 ||| 46713 ||| 
2020 ||| measuring time-sensitive and topic-specific influence in social networks with lstm and self-attention. ||| 9681 ||| 4385 ||| 802 ||| 4873 ||| 9684 ||| 1160 ||| 
2021 ||| generative adversarial networks for abnormal event detection in videos based on self-attention mechanism. ||| 47123 ||| 47124 ||| 18632 ||| 31091 ||| 47125 ||| 
2019 ||| stream-flow forecasting based on dynamic spatio-temporal attention. ||| 4287 ||| 47126 ||| 47127 ||| 
2019 ||| ultra-compact and wideband v(u)hf 3-db power dividers consisting of novel asymmetric impedance transformers. ||| 46692 ||| 46693 ||| 
2019 ||| short-term load forecasting based on deep learning for end-user transformer subject to volatile electric heating loads. ||| 29160 ||| 47128 ||| 47129 ||| 47130 ||| 47131 ||| 47132 ||| 
2021 ||| the impact of geomagnetically produced negative-sequence harmonics on power transformers. ||| 47133 ||| 47134 ||| 
2021 ||| lightweight channel attention and multiscale feature fusion discrimination for remote sensing scene classification. ||| 47135 ||| 1037 ||| 47136 ||| 47137 ||| 17179 ||| 29235 ||| 47138 ||| 19025 ||| 
2020 ||| a three-phase constant common-mode voltage inverter with triple voltage boost for transformerless photovoltaic system. ||| 47139 ||| 46420 ||| 47140 ||| 47141 ||| 47142 ||| 47143 ||| 47144 ||| 
2020 ||| action recognition using attention-joints graph convolutional neural networks. ||| 46292 ||| 47145 ||| 47146 ||| 46294 ||| 
2020 ||| transfer2depth: dual attention network with transfer learning for monocular depth estimation. ||| 43615 ||| 47147 ||| 40123 ||| 47148 ||| 
2021 ||| chicken image segmentation via multi-scale attention-based deep convolutional neural network. ||| 3337 ||| 8488 ||| 18979 ||| 47149 ||| 47150 ||| 4648 ||| 47151 ||| 
2019 ||| improved distant supervised model in tibetan relation extraction using elmo and attention. ||| 47152 ||| 47153 ||| 25012 ||| 47154 ||| 47155 ||| 
2021 ||| few-shot scene classification with multi-attention deepemd network in remote sensing. ||| 47156 ||| 47157 ||| 1556 ||| 47158 ||| 
2021 ||| spatial-temporal genetic-based attention networks for short-term photovoltaic power forecasting. ||| 41678 ||| 4150 ||| 5525 ||| 46611 ||| 46612 ||| 
2020 ||| an empirical study on the influence of internet attention on the performance of individual stocks in the securities market under the environment of big data. ||| 47159 ||| 47160 ||| 5439 ||| 
2021 ||| relatable clothing: soft-attention mechanism for detecting worn/unworn objects. ||| 27245 ||| 27248 ||| 
2021 ||| accent for visible and infrared registration (avir): attention block for increasing patch matching rate through edge emphasis. ||| 47161 ||| 30518 ||| 30519 ||| 
2020 ||| color transfer with salient features mapping via attention maps between images. ||| 47162 ||| 47163 ||| 
2019 ||| lstm-attention-embedding model-based day-ahead prediction of photovoltaic power output using bayesian optimization. ||| 47164 ||| 6502 ||| 47165 ||| 
2019 ||| a novel power transformer condition monitoring system based on wide-band measurement of core earth signals and correlation analysis with multi-source data. ||| 47166 ||| 14166 ||| 47167 ||| 14163 ||| 
2018 ||| transformerless common-mode current-source inverter grid-connected for pv applications. ||| 47168 ||| 11005 ||| 47169 ||| 47170 ||| 47171 ||| 21702 ||| 10882 ||| 2698 ||| 47172 ||| 
2020 ||| attention guided encoder-decoder network with multi-scale context aggregation for land cover segmentation. ||| 15217 ||| 47173 ||| 35716 ||| 5810 ||| 17444 ||| 
2019 ||| attention-based dual-source spatiotemporal neural network for lightning forecast. ||| 38286 ||| 17117 ||| 17116 ||| 479 ||| 47174 ||| 47175 ||| 47176 ||| 47177 ||| 47178 ||| 
2020 ||| bi-modal learning with channel-wise attention for multi-label image classification. ||| 3675 ||| 26814 ||| 31144 ||| 31138 ||| 
2019 ||| improving human pose estimation with self-attention generative adversarial networks. ||| 19946 ||| 19945 ||| 3049 ||| 2740 ||| 19947 ||| 
2020 ||| a novel hybrid spatial-temporal attention-lstm model for heat load prediction. ||| 47179 ||| 38147 ||| 46214 ||| 46213 ||| 47180 ||| 
2022 ||| multi-size object detection in large scene remote sensing images under dual attention mechanism. ||| 47181 ||| 47182 ||| 40528 ||| 47183 ||| 47184 ||| 47185 ||| 
2020 ||| fdta: fully convolutional scene text detection with text attention. ||| 47186 ||| 47187 ||| 47188 ||| 
2019 ||| aesgru: an attention-based temporal correlation approach for end-to-end machine health perception. ||| 29198 ||| 2019 ||| 47189 ||| 1796 ||| 29201 ||| 
2021 ||| a novel dynamic attack on classical ciphers using an attention-based lstm encoder-decoder model. ||| 47190 ||| 47191 ||| 47192 ||| 47193 ||| 
2020 ||| ma-net: a multi-scale attention network for liver and tumor segmentation. ||| 47194 ||| 47195 ||| 185 ||| 47196 ||| 
2020 ||| the research on additional errors of voltage transformer connected in series. ||| 28379 ||| 28380 ||| 28381 ||| 
2021 ||| impact of optimal control of distributed generation converters in smart transformer based meshed hybrid distribution network. ||| 16200 ||| 22194 ||| 22195 ||| 10868 ||| 
2019 ||| phishing email detection using improved rcnn model with multilevel vectors and attention mechanism. ||| 31872 ||| 3761 ||| 31874 ||| 10225 ||| 6070 ||| 
2019 ||| a packet-length-adjustable attention model based on bytes embedding using flow-wgan for smart cybersecurity. ||| 47197 ||| 46972 ||| 47198 ||| 
2021 ||| optimal sizing of energy storage system to reduce impacts of transportation electrification on power distribution transformers integrated with photovoltaic. ||| 47199 ||| 47200 ||| 47201 ||| 47202 ||| 47203 ||| 28558 ||| 
2020 ||| adversarial erasing attention for person re-identification in camera networks under complex environments. ||| 13805 ||| 47204 ||| 47205 ||| 19052 ||| 28433 ||| 
2020 ||| text sentiment orientation analysis based on multi-channel cnn and bidirectional gru with attention mechanism. ||| 47206 ||| 47207 ||| 47208 ||| 47209 ||| 47210 ||| 47211 ||| 
2020 ||| locally adaptive channel attention-based network for denoising images. ||| 47212 ||| 2471 ||| 
2020 ||| adaptive modulation strategy for modular multilevel high-frequency dc transformer in dc distribution networks. ||| 3906 ||| 47213 ||| 46451 ||| 46429 ||| 47214 ||| 46428 ||| 47215 ||| 6553 ||| 
2020 ||| real-time surgical tool detection in minimally invasive surgery based on attention-guided convolutional neural network. ||| 45677 ||| 11593 ||| 47216 ||| 17568 ||| 
2021 ||| transformers for clinical coding in spanish. ||| 17518 ||| 17519 ||| 3419 ||| 852 ||| 17520 ||| 17521 ||| 17522 ||| 17523 ||| 
2020 ||| a graph attention model for dictionary-guided named entity recognition. ||| 43795 ||| 43796 ||| 9028 ||| 3725 ||| 
2019 ||| an emotion-embedded visual attention model for dimensional emotion context learning. ||| 47217 ||| 14649 ||| 41907 ||| 41906 ||| 17591 ||| 
2019 ||| motion based inference of social circles via self-attention and contextualized embedding. ||| 9013 ||| 5743 ||| 9010 ||| 9011 ||| 9012 ||| 
2019 ||| identifying implicit polarity of events by using an attention-based neural network model. ||| 352 ||| 26781 ||| 3725 ||| 
2021 ||| efficient audio-visual speech enhancement using deep u-net with early fusion of audio and video information and rnn attention blocks. ||| 47218 ||| 19147 ||| 47219 ||| 
2022 ||| homogeneous learning: self-attention decentralized deep learning. ||| 33572 ||| 33573 ||| 
2021 ||| optimal data selection rule mining for transformer condition assessment. ||| 989 ||| 46288 ||| 47220 ||| 46290 ||| 46289 ||| 46291 ||| 37325 ||| 
2020 ||| an efficient adaptive attention neural network for social recommendation. ||| 8490 ||| 8491 ||| 8492 ||| 
2021 ||| temperature rise test and thermal-fluid coupling simulation of an oil-immersed autotransformer under dc bias. ||| 41616 ||| 24897 ||| 47221 ||| 47222 ||| 47223 ||| 
2019 ||| single-phase common-ground-type transformerless pv grid-connected inverters. ||| 
2019 ||| temporal attention networks for multitemporal multisensor crop classification. ||| 45479 ||| 45480 ||| 45481 ||| 
2019 ||| quantitative formula of blink rates-pupillometry for attention level detection in supervised machine learning. ||| 47224 ||| 46596 ||| 
2019 ||| multilayer dense attention model for image caption. ||| 47225 ||| 30692 ||| 1704 ||| 47226 ||| 47227 ||| 
2019 |||  nanoparticles and water on transformer oil electrical performance. ||| 47228 ||| 3906 ||| 43745 ||| 47229 ||| 
2022 ||| household energy consumption prediction using the stationary wavelet transform and transformers. ||| 47230 ||| 1402 ||| 47231 ||| 
2020 ||| joint attention mechanisms for monocular depth estimation with multi-scale convolutions and adaptive weight adjustment. ||| 10572 ||| 41265 ||| 41266 ||| 41267 ||| 
2020 ||| multimodal emotion recognition with transformer-based self supervised feature fusion. ||| 47232 ||| 47233 ||| 47234 ||| 47235 ||| 
2021 ||| scale-aware transformers for diagnosing melanocytic lesions. ||| 1572 ||| 24016 ||| 47236 ||| 47237 ||| 47238 ||| 47239 ||| 38394 ||| 38395 ||| 
2019 ||| an economic operation analysis method of transformer based on clustering. ||| 29532 ||| 15562 ||| 29534 ||| 
2020 ||| hyperspectral band selection using attention-based convolutional neural networks. ||| 32227 ||| 32228 ||| 32229 ||| 13472 ||| 
2021 ||| progressive guided fusion network with multi-modal and multi-scale attention for rgb-d salient object detection. ||| 20171 ||| 45134 ||| 47240 ||| 1822 ||| 45137 ||| 47241 ||| 47242 ||| 47243 ||| 
2021 ||| domain adaptation deep attention network for automatic logo detection and recognition in google street view. ||| 47244 ||| 40123 ||| 29465 ||| 47245 ||| 47246 ||| 47247 ||| 
2021 ||| modeling and optimization of semantic segmentation for track bed foreign object based on attention mechanism. ||| 47248 ||| 47249 ||| 47250 ||| 2588 ||| 31409 ||| 2045 ||| 
2020 ||| one-dimensional deep attention convolution network (odacn) for signals classification. ||| 36641 ||| 749 ||| 47251 ||| 47252 ||| 214 ||| 
2019 ||| r-stan: residual spatial-temporal attention network for action recognition. ||| 47253 ||| 47254 ||| 47255 ||| 
2020 ||| combining multi-perspective attention mechanism with convolutional networks for monaural speech enhancement. ||| 7955 ||| 12514 ||| 47256 ||| 12515 ||| 596 ||| 8922 ||| 
2021 ||| escaping the gradient vanishing: periodic alternatives of softmax in attention mechanism. ||| 36110 ||| 968 ||| 2304 ||| 
2021 ||| overview and partial discharge analysis of power transformers: a literature review. ||| 47257 ||| 10933 ||| 10934 ||| 
2020 ||| investigating a new approach for moisture assessment of transformer insulation system. ||| 6514 ||| 2039 ||| 10922 ||| 28557 ||| 47258 ||| 47259 ||| 47260 ||| 
2020 ||| single-phase grid-tied transformerless inverter of zero leakage current for pv system. ||| 46416 ||| 47261 ||| 47262 ||| 46414 ||| 47263 ||| 
2021 ||| poat-net: parallel offset-attention assisted transformer for 3d object detection for autonomous driving. ||| 47264 ||| 1238 ||| 47265 ||| 
2021 ||| analysis and design of an integrated magnetics planar transformer for high power density llc resonant converter. ||| 47266 ||| 47267 ||| 
2022 ||| design and emulation of physics-centric cyberattacks on an electrical power transformer. ||| 47268 ||| 47269 ||| 47270 ||| 
2020 ||| using features specifically: an efficient network for scene segmentation based on dedicated attention mechanisms. ||| 33737 ||| 18779 ||| 4634 ||| 33738 ||| 33739 ||| 
2021 ||| attention mechanism cloud detection with modified fcn for infrared remote sensing images. ||| 2652 ||| 5378 ||| 189 ||| 47271 ||| 47272 ||| 47273 ||| 
2022 ||| temperature segment compensation method of dissipation factor for insulation diagnosis in converter transformer bushing. ||| 47274 ||| 47275 ||| 23905 ||| 47276 ||| 47277 ||| 47278 ||| 
2020 ||| easa: entity alignment algorithm based on semantic aggregation and attribute attention. ||| 47279 ||| 6259 ||| 
2019 ||| attention-based dual-scale cnn in-loop filter for versatile video coding. ||| 47280 ||| 47281 ||| 41977 ||| 47282 ||| 
2022 ||| energy management strategy of ac/dc hybrid microgrid based on solid-state transformer. ||| 46404 ||| 47283 ||| 46406 ||| 28557 ||| 46408 ||| 47284 ||| 
2020 ||| improved relativistic cycle-consistent gan with dilated residual network and multi-attention for speech enhancement. ||| 4383 ||| 4382 ||| 47285 ||| 1341 ||| 4385 ||| 
2021 ||| an accurate method for leakage inductance calculation of shell-type multi core-segment transformers with circular windings. ||| 47286 ||| 47287 ||| 47288 ||| 47289 ||| 
2019 ||| water hazard detection using conditional generative adversarial network with mixture reflection attention units. ||| 1052 ||| 2054 ||| 
2019 ||| transformer based memory network for sentiment analysis of web comments. ||| 1871 ||| 25832 ||| 47290 ||| 1254 ||| 
2020 ||| disease-pertinent knowledge extraction in online health communities using gru based on a double attention mechanism. ||| 47291 ||| 47292 ||| 2855 ||| 
2021 ||| comprehensive analysis of winding electromagnetic force and deformation during no-load closing and short-circuiting of power transformers. ||| 47293 ||| 47294 ||| 11173 ||| 47295 ||| 
2019 ||| adding prior knowledge in hierarchical attention neural network for cross domain sentiment classification. ||| 47296 ||| 1780 ||| 
2021 ||| point transformer. ||| 23622 ||| 23623 ||| 23624 ||| 
2018 ||| a cascade coupled convolutional neural network guided visual attention method for ship detection from sar images. ||| 47297 ||| 47298 ||| 35395 ||| 47299 ||| 
2019 ||| an adaptive multi-robot therapy for improving joint attention and imitation of asd children. ||| 17043 ||| 17044 ||| 47300 ||| 17045 ||| 17046 ||| 47301 ||| 47302 ||| 47303 ||| 47304 ||| 
2021 ||| pyramid co-attention compare network for few-shot segmentation. ||| 15562 ||| 47305 ||| 47306 ||| 47307 ||| 
2021 ||| inter-dimensional correlations aggregated attention network for action recognition. ||| 47308 ||| 47309 ||| 47310 ||| 
2019 ||| multi-channel cnn based inner-attention for compound sentence relation classification. ||| 47311 ||| 8177 ||| 47312 ||| 438 ||| 
2020 ||| multimodal attention network for continuous-time emotion recognition using video and eeg signals. ||| 47313 ||| 47314 ||| 21489 ||| 
2021 ||| hydro-dynamic model and low-speed stability analysis of hydraulic transformer. ||| 47315 ||| 47316 ||| 47317 ||| 979 ||| 47318 ||| 
2020 ||| scene classification of remote sensing images based on saliency dual attention residual network. ||| 47319 ||| 47320 ||| 47158 ||| 
2021 ||| mvan: multi-view attention networks for fake news detection on social media. ||| 47321 ||| 47322 ||| 47323 ||| 
2021 ||| image-based scam detection method using an attention capsule network. ||| 47324 ||| 38177 ||| 14419 ||| 1371 ||| 47325 ||| 
2020 ||| cross-lingual image caption generation based on visual attention model. ||| 379 ||| 47326 ||| 2251 ||| 47327 ||| 602 ||| 640 ||| 
2020 ||| normalization for fds of transformer insulation considering the synergistic effect generated by temperature and moisture. ||| 28372 ||| 47328 ||| 28451 ||| 28373 ||| 18177 ||| 46715 ||| 
2020 ||| parameter estimation of electric power transformers using coyote optimization algorithm with experimental verification. ||| 47329 ||| 47330 ||| 45631 ||| 47331 ||| 47332 ||| 
2021 ||| dgattgan: cooperative up-sampling based dual generator attentional gan on text-to-image synthesis. ||| 14048 ||| 41745 ||| 47333 ||| 11109 ||| 
2019 ||| spatial-temporal attention-based human dynamics retrospection. ||| 23428 ||| 3156 ||| 
2021 ||| traffic sign detection and recognition using multi-scale fusion and prime sample attention. ||| 47334 ||| 47335 ||| 471 ||| 
2022 ||| fuzzy based condition monitoring tool for real-time analysis of synthetic ester fluid as transformer insulant. ||| 47336 ||| 28314 ||| 47337 ||| 9851 ||| 
2020 ||| da-capnet: dual attention deep learning based on u-net for nailfold capillary segmentation. ||| 40720 ||| 40718 ||| 17290 ||| 
2019 ||| transformer-based neural network for answer selection in question answering. ||| 47338 ||| 23695 ||| 1047 ||| 47339 ||| 
2019 ||| bandwidth enhancement of gan mmic doherty power amplifiers using broadband transformer-based load modulation network. ||| 29985 ||| 29964 ||| 30703 ||| 
2021 ||| exploring category attention for open set domain adaptation. ||| 47340 ||| 
2020 ||| the evolution of trapping parameters on three-layer oil-paper of partial discharge degradation for on-board traction transformers. ||| 47341 ||| 3815 ||| 3666 ||| 39985 ||| 47085 ||| 
2021 ||| multi-scale attention network for diabetic retinopathy classification. ||| 47342 ||| 47343 ||| 
2020 ||| an efficient voxel-based segmentation algorithm based on hierarchical clustering to extract lidar power equipment data in transformer substations. ||| 47344 ||| 47345 ||| 47346 ||| 47347 ||| 47348 ||| 47349 ||| 47350 ||| 
2021 ||| attention-based bi-directional long-short term memory network for earthquake prediction. ||| 27031 ||| 27030 ||| 27033 ||| 47351 ||| 27035 ||| 27036 ||| 47352 ||| 47353 ||| 
2020 ||| use of alternative fluids in very high-power transformers: experimental and numerical thermal studies. ||| 47354 ||| 47355 ||| 47356 ||| 47357 ||| 47358 ||| 10314 ||| 504 ||| 47359 ||| 47360 ||| 
2019 ||| a single attention-based combination of cnn and rnn for relation classification. ||| 5150 ||| 4600 ||| 47361 ||| 47362 ||| 47363 ||| 
2020 ||| recurrent attention dense network for single image de-raining. ||| 47364 ||| 47365 ||| 2323 ||| 47366 ||| 47367 ||| 1160 ||| 946 ||| 
2020 ||| convolutional neural network-based pavement crack segmentation using pyramid attention network. ||| 696 ||| 47368 ||| 
2019 ||| inferring drivers' visual focus attention through head-mounted inertial sensors. ||| 852 ||| 47369 ||| 2600 ||| 47370 ||| 6235 ||| 47371 ||| 8396 ||| 47372 ||| 47373 ||| 3882 ||| 41807 ||| 47374 ||| 
2022 ||| channel attention gan trained with enhanced dataset for single-image shadow removal. ||| 47375 ||| 21708 ||| 
2020 ||| fine-grained age estimation with multi-attention network. ||| 47376 ||| 619 ||| 43889 ||| 47377 ||| 47378 ||| 
2019 ||| group recommendation via self-attention and collaborative metric learning model. ||| 18952 ||| 47379 ||| 47380 ||| 
2021 ||| mtsan: multi-task semantic attention network for adas applications. ||| 47381 ||| 47382 ||| 47383 ||| 15643 ||| 
2020 ||| temporal-frequency attention-based human activity recognition using commercial wifi devices. ||| 45152 ||| 47384 ||| 27521 ||| 47385 ||| 
2020 ||| video synopsis based on attention mechanism and local transparent processing. ||| 44053 ||| 47386 ||| 44068 ||| 47387 ||| 47388 ||| 
2020 ||| context-aware cross-attention for skeleton-based human action recognition. ||| 47389 ||| 47390 ||| 7826 ||| 47391 ||| 340 ||| 
2019 ||| deep feature fusion by competitive attention for pedestrian detection. ||| 47392 ||| 254 ||| 426 ||| 429 ||| 47393 ||| 
2019 ||| combining the attention network and semantic representation for chinese verb metaphor identification. ||| 8977 ||| 8974 ||| 47394 ||| 47395 ||| 21178 ||| 
2020 ||| video alignment using bi-directional attention flow in a multi-stage learning model. ||| 16365 ||| 16366 ||| 16367 ||| 
2021 ||| analytical calculation of magnetic field in fractional-slot windings linear phase-shifting transformer based on exact subdomain model. ||| 47396 ||| 47397 ||| 34493 ||| 47398 ||| 
2019 ||| how to make attention mechanisms more practical in malware classification. ||| 9442 ||| 47399 ||| 47400 ||| 46575 ||| 47401 ||| 24987 ||| 47402 ||| 
2021 ||| an average voltage approach to control energy storage device and tap changing transformers under high distributed generation. ||| 47403 ||| 47404 ||| 47405 ||| 
2021 ||| haze relevant feature attention network for single image dehazing. ||| 17840 ||| 218 ||| 8952 ||| 40702 ||| 7204 ||| 
2019 ||| attention gan-based method for designing intelligent making system. ||| 47406 ||| 47407 ||| 25099 ||| 47408 ||| 781 ||| 45228 ||| 45231 ||| 
2019 ||| correction to "a novel unbalance compensation method for distribution solid-state transformer based on reduced order generalized integrator". ||| 46404 ||| 46405 ||| 46406 ||| 46407 ||| 46408 ||| 28557 ||| 
2019 ||| residual attention convolutional network for online visual tracking. ||| 47409 ||| 6701 ||| 47410 ||| 
2021 ||| investigation on design of novel step-up 18-pulse auto-transformer rectifier. ||| 46935 ||| 1978 ||| 7830 ||| 
2019 ||| multi-scale visual attention deep convolutional neural network for multi-focus image fusion. ||| 39571 ||| 47411 ||| 39570 ||| 39572 ||| 
2020 ||| collaborative filtering recommendation algorithm based on attention gru and adversarial learning. ||| 47412 ||| 47413 ||| 2487 ||| 
2020 ||| adversarial attention-based variational graph autoencoder. ||| 6054 ||| 6055 ||| 47414 ||| 
2020 ||| da-net: pedestrian detection using dense connected block and attention modules. ||| 47415 ||| 47082 ||| 7700 ||| 220 ||| 
2020 ||| split-attention multiframe alignment network for image restoration. ||| 47416 ||| 47417 ||| 43980 ||| 43979 ||| 6721 ||| 
2020 ||| multimodal encoder-decoder attention networks for visual question answering. ||| 47418 ||| 40953 ||| 1224 ||| 
2020 ||| skin lesion segmentation based on multi-scale attention convolutional neural network. ||| 5366 ||| 40986 ||| 5304 ||| 31347 ||| 
2022 ||| a novel attention-based multi-modal modeling technique on mixed type data for improving tft-lcd repair process. ||| 1199 ||| 47419 ||| 47420 ||| 
2019 ||| sentiment-aware deep recommender system with neural attention networks. ||| 47421 ||| 47422 ||| 
2021 ||| multi-domain aspect extraction using bidirectional encoder representations from transformers. ||| 47423 ||| 47424 ||| 47425 ||| 
2020 ||| stgat: spatial-temporal graph attention networks for traffic flow forecasting. ||| 31410 ||| 31405 ||| 31411 ||| 47426 ||| 12196 ||| 3131 ||| 
2019 ||| facial landmark detection via attention-adaptive deep network. ||| 47427 ||| 47428 ||| 47429 ||| 291 ||| 
2021 ||| tactical decision-making for autonomous driving using dueling double deep q network with double attention. ||| 27204 ||| 27205 ||| 27206 ||| 47430 ||| 47431 ||| 
2020 ||| target tracking method based on adaptive structured sparse representation with attention. ||| 5894 ||| 47432 ||| 3386 ||| 47433 ||| 
2020 ||| improving whole-heart ct image segmentation by attention mechanism. ||| 1160 ||| 47434 ||| 47435 ||| 1125 ||| 20424 ||| 
2020 ||| wavenet with cross-attention for audiovisual speech recognition. ||| 1341 ||| 4176 ||| 3226 ||| 47436 ||| 
2020 ||| fpgan: an fpga accelerator for graph attention networks with software and hardware co-optimization. ||| 1504 ||| 1505 ||| 1506 ||| 
2021 ||| diagnosis of inter-turn shorts of loaded transformer under various load currents and power factors; impulse voltage-based frequency response approach. ||| 46889 ||| 25256 ||| 46890 ||| 47437 ||| 46891 ||| 
2019 ||| convolution-based neural attention with applications to sentiment classification. ||| 21175 ||| 3662 ||| 3664 ||| 15906 ||| 28037 ||| 
2020 ||| study on the operators' attention of different areas in university laboratories based on eye movement tracking technology. ||| 47438 ||| 47439 ||| 47440 ||| 47441 ||| 
2019 ||| improving distantly supervised relation classification with attention and semantic weight. ||| 47442 ||| 651 ||| 6931 ||| 
2020 ||| aedmts: an attention-based encoder-decoder framework for multi-sensory time series analytic. ||| 32837 ||| 47443 ||| 32838 ||| 19919 ||| 47444 ||| 
2020 ||| a dimension reduction method used in detecting errors of distribution transformer connectivity. ||| 47445 ||| 32436 ||| 47446 ||| 47447 ||| 1856 ||| 47448 ||| 47449 ||| 
2021 ||| ultrasonic logging image denoising based on cnn and feature attention. ||| 47450 ||| 47451 ||| 47452 ||| 47453 ||| 47454 ||| 47455 ||| 
2021 ||| ddanet: dual-path depth-aware attention network for fingerspelling recognition using rgb-d images. ||| 47456 ||| 47457 ||| 47458 ||| 47459 ||| 
2020 ||| modeling and stability analysis of a smart transformer-fed grid. ||| 22166 ||| 10868 ||| 369 ||| 5287 ||| 
2020 ||| salient object detection using recurrent guidance network with hierarchical attention features. ||| 47460 ||| 47461 ||| 47462 ||| 
2019 ||| a novel measuring method of interfacial tension of transformer oil combined pso optimized svm and multi frequency ultrasonic technology. ||| 47463 ||| 47006 ||| 38646 ||| 43684 ||| 
2020 ||| investigation on the parasitic capacitance of high frequency and high voltage transformers of multi-section windings. ||| 47464 ||| 21487 ||| 14108 ||| 47465 ||| 11124 ||| 
2022 ||| a power transformer fault diagnosis method-based hybrid improved seagull optimization algorithm and support vector machine. ||| 47466 ||| 47467 ||| 340 ||| 47468 ||| 31744 ||| 
2021 ||| multi-head self-attention transformation networks for aspect-based sentiment analysis. ||| 32065 ||| 47469 ||| 42675 ||| 32067 ||| 
2021 ||| attention span prediction using head-pose estimation with deep neural networks. ||| 47470 ||| 47471 ||| 47472 ||| 47473 ||| 47474 ||| 47475 ||| 
2019 ||| no-reference stereoscopic image quality assessment based on visual attention and perception. ||| 47476 ||| 2320 ||| 43892 ||| 1224 ||| 4213 ||| 720 ||| 47477 ||| 
2020 ||| ca-gan: class-condition attention gan for underwater image enhancement. ||| 4550 ||| 977 ||| 47478 ||| 47479 ||| 44088 ||| 47480 ||| 47481 ||| 
2019 ||| a single-phase transformer-based cascaded asymmetric multilevel inverter with balanced power distribution. ||| 47482 ||| 47483 ||| 6024 ||| 47484 ||| 2600 ||| 47485 ||| 40741 ||| 47486 ||| 
2019 ||| msnet: multi-head self-attention network for distantly supervised relation extraction. ||| 47487 ||| 1151 ||| 1152 ||| 31485 ||| 
2020 ||| lightweight prediction and boundary attention-based semantic segmentation for road scene understanding. ||| 7934 ||| 37566 ||| 7938 ||| 
2020 ||| capacitor voltage ripple minimization of a modular three-phase ac/dc power electronics transformer with four-winding power channel. ||| 29285 ||| 47488 ||| 47489 ||| 47490 ||| 25530 ||| 3691 ||| 
2020 ||| hierarchical graph transformer-based deep learning model for large-scale multi-label text classification. ||| 9641 ||| 47491 ||| 47492 ||| 29251 ||| 47493 ||| 47494 ||| 27562 ||| 47495 ||| 29086 ||| 47496 ||| 
2020 ||| s2n2: an interpretive semantic structure attention neural network for trajectory classification. ||| 7706 ||| 47497 ||| 47498 ||| 47499 ||| 7707 ||| 
2020 ||| multi-attention network for stereo matching. ||| 13255 ||| 47500 ||| 11819 ||| 28722 ||| 42272 ||| 47501 ||| 
2020 ||| classification and biomarker exploration of autism spectrum disorders based on recurrent attention model. ||| 47026 ||| 11453 ||| 
2020 ||| speech emotion recognition by combining a unified first-order attention network with data balance. ||| 1216 ||| 47502 ||| 47503 ||| 28840 ||| 
2020 ||| msba: multiple scales, branches and attention network with bag of tricks for person re-identification. ||| 47504 ||| 47505 ||| 24989 ||| 33746 ||| 47506 ||| 16550 ||| 47507 ||| 
2021 ||| soft thresholding attention network for adaptive feature denoising in sar ship detection. ||| 3049 ||| 47508 ||| 47509 ||| 47510 ||| 47511 ||| 47512 ||| 
2020 ||| influence of phase-shifted square wave modulation on medium frequency transformer in a mmc based sst. ||| 47513 ||| 47514 ||| 4175 ||| 
2019 ||| single image rain removal via cascading attention aggregation network on challenging weather conditions. ||| 47515 ||| 47516 ||| 31350 ||| 
2020 ||| effective exploitation of posterior information for attention-based speech recognition. ||| 1248 ||| 4462 ||| 3198 ||| 12372 ||| 12357 ||| 
2022 ||| qos prediction of web services based on a two-level heterogeneous graph attention network. ||| 47517 ||| 47518 ||| 8859 ||| 47519 ||| 
2020 ||| a discriminative deep model with feature fusion and temporal attention for human action recognition. ||| 12814 ||| 12815 ||| 2334 ||| 47520 ||| 47521 ||| 23188 ||| 9440 ||| 
2020 ||| multi-modal memory enhancement attention network for image-text matching. ||| 2276 ||| 47522 ||| 2277 ||| 46054 ||| 
2020 ||| image retrieval using a deep attention-based hash. ||| 47523 ||| 47524 ||| 47525 ||| 47526 ||| 47527 ||| 1234 ||| 47528 ||| 
2021 ||| upper airway segmentation based on the attention mechanism of weak feature regions. ||| 293 ||| 882 ||| 13930 ||| 47241 ||| 47529 ||| 
2018 ||| an input-series-output-series modular multilevel dc transformer with inter-module arithmetic phase interleaving control to reduce dc ripples. ||| 47530 ||| 43749 ||| 47531 ||| 27852 ||| 
2019 ||| a hierarchical bidirectional gru model with attention for eeg-based emotion classification. ||| 47532 ||| 47533 ||| 47534 ||| 
2019 ||| line impedance measurement to improve power systems protection of the gautrain 25 kv autotransformer traction power supply system. ||| 47535 ||| 47536 ||| 47537 ||| 47538 ||| 
2020 ||| a novel csi feedback approach for massive mimo using lstm-attention cnn. ||| 6721 ||| 2899 ||| 21180 ||| 19958 ||| 14165 ||| 
2021 ||| health analysis of transformer winding insulation through thermal monitoring and fast fourier transform (fft) power spectrum. ||| 42345 ||| 47539 ||| 47540 ||| 47541 ||| 47542 ||| 47543 ||| 47544 ||| 
2020 ||| evaluation of sentiment analysis in finance: from lexicons to transformers. ||| 47545 ||| 47546 ||| 47547 ||| 47548 ||| 47549 ||| 
2020 ||| experimental and numerical study on stray loss in laminated magnetic shielding under 3-d ac-dc hybrid excitations for hvdc transformers. ||| 47550 ||| 47551 ||| 47552 ||| 47553 ||| 47554 ||| 9694 ||| 
2020 ||| echobert: a transformer-based approach for behavior detection in echograms. ||| 2712 ||| 47555 ||| 2713 ||| 26057 ||| 
2020 ||| weakly supervised learning for land cover mapping of satellite image time series via attention-based cnn. ||| 6789 ||| 47556 ||| 47557 ||| 31198 ||| 
2019 ||| combining part-of-speech tags and self-attention mechanism for simile recognition. ||| 32039 ||| 11156 ||| 18844 ||| 47558 ||| 25347 ||| 
2018 ||| attention-based relation extraction with bidirectional gated recurrent unit and highway network in the analysis of geological data. ||| 31142 ||| 47559 ||| 4201 ||| 47560 ||| 47561 ||| 
2020 ||| interactive rule attention network for aspect-level sentiment analysis. ||| 5017 ||| 46222 ||| 47562 ||| 47563 ||| 32728 ||| 
2020 ||| recognition of teachers' facial expression intensity based on convolutional neural network and attention mechanism. ||| 47564 ||| 2019 ||| 17228 ||| 47565 ||| 
2019 ||| deep cnns with self-attention for speaker identification. ||| 47566 ||| 47567 ||| 4190 ||| 
2018 ||| comprehensive power losses model for electronic power transformer. ||| 47568 ||| 47569 ||| 47570 ||| 46054 ||| 47571 ||| 47572 ||| 23617 ||| 
2021 ||| high-resolution pelvic mri reconstruction using a generative adversarial network with attention and cyclic loss. ||| 21160 ||| 34858 ||| 6522 ||| 34859 ||| 6005 ||| 
2021 ||| adaptive dynamic meta-heuristics for feature selection and classification in diagnostic accuracy of transformer faults. ||| 46634 ||| 47573 ||| 47574 ||| 47575 ||| 47576 ||| 
2020 ||| reducing the impacts of electric vehicle charging on power distribution transformers. ||| 47199 ||| 47200 ||| 47201 ||| 47577 ||| 28558 ||| 
2020 ||| stacked residual recurrent neural networks with cross-layer attention for text classification. ||| 47578 ||| 47579 ||| 47580 ||| 16280 ||| 399 ||| 
2020 ||| solid-state transformer for power distribution grid based on a hybrid switched-capacitor llc-src converter: analysis, design, and experimentation. ||| 47581 ||| 47582 ||| 47583 ||| 47584 ||| 
2021 ||| effect of iron/titania-based nanomaterials on the dielectric properties of mineral oil, natural and synthetic esters as transformers insulating fluid. ||| 47585 ||| 21925 ||| 47586 ||| 46722 ||| 
2021 ||| stack attention-pruning aggregates multiscale graph convolution networks for hyperspectral remote sensing image classification. ||| 28865 ||| 2747 ||| 47587 ||| 47588 ||| 43847 ||| 
2021 ||| hierarchically structured network with attention convolution and embedding propagation for imbalanced few-shot learning. ||| 6579 ||| 1108 ||| 47589 ||| 47590 ||| 
2020 ||| scalar coupling constant prediction using graph embedding local attention encoder. ||| 33936 ||| 33937 ||| 12196 ||| 27377 ||| 
2019 ||| dominance in visual space of asd children using multi-robot joint attention integrated distributed imitation system. ||| 17044 ||| 17045 ||| 17043 ||| 47302 ||| 47303 ||| 
2019 ||| inversion of oil-immersed paper resistivity in transformer based on dielectric loss factor. ||| 46385 ||| 46459 ||| 47591 ||| 11796 ||| 36631 ||| 46386 ||| 
2019 ||| fully convolutional captionnet: siamese difference captioning attention model. ||| 46187 ||| 47592 ||| 40735 ||| 40736 ||| 40737 ||| 47593 ||| 
2021 ||| gigapixel histopathological image analysis using attention-based neural networks. ||| 35790 ||| 35791 ||| 35792 ||| 35793 ||| 
2019 ||| densely connected convolutional networks with attention lstm for crowd flows prediction. ||| 3337 ||| 19619 ||| 47401 ||| 189 ||| 47594 ||| 46575 ||| 
2019 ||| image super-resolution using aggregated residual transformation networks with spatial attention. ||| 47595 ||| 11236 ||| 
2020 ||| a hierarchical structured multi-head attention network for multi-turn response generation. ||| 47596 ||| 12342 ||| 37652 ||| 47597 ||| 
2020 ||| semi-supervised breast histological image classification by node-attention graph transfer network. ||| 47598 ||| 20536 ||| 1190 ||| 
2019 ||| time+user dual attention based sentiment prediction for multiple social network texts with time series. ||| 3034 ||| 47599 ||| 19920 ||| 47600 ||| 
2020 ||| hybrid power quality compensation system for electric railway supplied by the hypotenuse of a scott transformer. ||| 47601 ||| 47602 ||| 47603 ||| 22485 ||| 4807 ||| 
2020 ||| probabilistic matrix factorization recommendation of self-attention mechanism convolutional neural networks with item auxiliary information. ||| 47604 ||| 3691 ||| 
2019 ||| vaa: visual aligning attention model for remote sensing image captioning. ||| 30447 ||| 19850 ||| 30448 ||| 30449 ||| 13676 ||| 10525 ||| 
2020 ||| transformer based multi-grained attention network for aspect-based sentiment analysis. ||| 5322 ||| 47605 ||| 40266 ||| 47606 ||| 47607 ||| 
2018 ||| automatic generation of news comments based on gated attention neural networks. ||| 793 ||| 1160 ||| 47608 ||| 41623 ||| 
2022 ||| attention mechanism-based light-field view synthesis. ||| 11300 ||| 11301 ||| 11302 ||| 11303 ||| 7111 ||| 11304 ||| 11305 ||| 
2020 ||| hierarchical attentional factorization machines for expert recommendation in community question answering. ||| 47609 ||| 2961 ||| 11105 ||| 11106 ||| 2962 ||| 
2020 ||| knowledge graph embedding via graph attenuated attention networks. ||| 3049 ||| 47610 ||| 47611 ||| 47612 ||| 1254 ||| 
2020 ||| human-centric emotion estimation based on correlation maximization considering changes with time in visual attention and brain activity. ||| 7607 ||| 7608 ||| 7609 ||| 7610 ||| 
2019 ||| prediction of natural gas consumption for city-level dhs based on attention gru: a case study for a northern chinese city. ||| 46214 ||| 46213 ||| 19208 ||| 38147 ||| 47180 ||| 3694 ||| 
2022 ||| roberta-lstm: a hybrid model for sentiment analysis with transformer and recurrent neural network. ||| 47613 ||| 47614 ||| 47615 ||| 47616 ||| 
2020 ||| precious metal price prediction based on deep regularization self-attention regression. ||| 29145 ||| 47617 ||| 47618 ||| 1371 ||| 47619 ||| 47620 ||| 29146 ||| 
2019 ||| query is gan: scene retrieval with attentional text-to-image generative adversarial network. ||| 47621 ||| 47622 ||| 7609 ||| 7610 ||| 
2019 ||| a visuo-haptic attention training game with dynamic adjustment of difficulty. ||| 47623 ||| 47624 ||| 47625 ||| 705 ||| 
2021 ||| multi-grained attention representation with albert for aspect-level sentiment classification. ||| 47626 ||| 47627 ||| 602 ||| 47628 ||| 
2020 ||| mscnn-am: a multi-scale convolutional neural network with attention mechanisms for retinal vessel segmentation. ||| 47629 ||| 47630 ||| 398 ||| 
2021 ||| state identification of transformer under dc bias based on wavelet singular entropy. ||| 1235 ||| 10873 ||| 33411 ||| 10874 ||| 47631 ||| 47632 ||| 47633 ||| 40820 ||| 
2020 ||| generative text summary based on enhanced semantic attention and gain-benefit gate. ||| 41293 ||| 438 ||| 47634 ||| 47635 ||| 
2019 ||| a very deep spatial transformer towards robust single image super-resolution. ||| 19755 ||| 20195 ||| 20196 ||| 
2019 ||| pre-alignment guided attention for improving training efficiency and model stability in end-to-end speech synthesis. ||| 47636 ||| 47637 ||| 12571 ||| 42779 ||| 12384 ||| 
2021 ||| unified transformer multi-task learning for intent classification with entity recognition. ||| 47638 ||| 47639 ||| 47640 ||| 47641 ||| 47642 ||| 39961 ||| 47643 ||| 
2019 ||| parallax-based spatial and channel attention for stereo image super-resolution. ||| 47644 ||| 47645 ||| 
2020 ||| vision-based fall event detection in complex background using attention guided bi-directional lstm. ||| 20639 ||| 47646 ||| 4754 ||| 39689 ||| 39690 ||| 
2020 ||| ensemble learning with attention-integrated convolutional recurrent neural network for imbalanced speech emotion recognition. ||| 47647 ||| 6475 ||| 47648 ||| 11658 ||| 46474 ||| 
2021 ||| deep learning based mineral image classification combined with visual attention mechanism. ||| 1305 ||| 47649 ||| 19332 ||| 3279 ||| 47650 ||| 
2022 ||| structured fusion attention network for image super-resolution reconstruction. ||| 47651 ||| 47652 ||| 47653 ||| 15696 ||| 47654 ||| 
2021 ||| dynamic thermal model for power transformers. ||| 42345 ||| 47539 ||| 47540 ||| 47542 ||| 47655 ||| 47543 ||| 47427 ||| 47544 ||| 
2019 ||| study on characteristic parameters of short-circuit impedance for a four-winding inductive filtering transformer in power system supplying nonlinear loads. ||| 43583 ||| 47656 ||| 47657 ||| 43581 ||| 
2021 ||| generative adversarial networks with attention mechanisms at every scale. ||| 47658 ||| 47659 ||| 
2020 ||| robust qrs detection using high-resolution wavelet packet decomposition and time-attention convolutional neural network. ||| 46580 ||| 46578 ||| 46579 ||| 46581 ||| 46582 ||| 
2021 ||| breakdown voltage of transformer oil containing cellulose particle contamination with and without bridge formation under lightning impulse stress. ||| 47660 ||| 47661 ||| 47662 ||| 47663 ||| 47664 ||| 
2020 ||| adaptive inattentional framework for video object detection with reward-conditional training. ||| 47665 ||| 47666 ||| 47667 ||| 47668 ||| 
2020 ||| dairy goat image generation based on improved-self-attention generative adversarial networks. ||| 17464 ||| 47669 ||| 
2019 ||| gisca: gradient-inductive segmentation network with contextual attention for scene text detection. ||| 455 ||| 4430 ||| 5892 ||| 859 ||| 
2021 ||| an air target tactical intention recognition model based on bidirectional gru with attention mechanism. ||| 311 ||| 47670 ||| 17687 ||| 8608 ||| 
2018 ||| cross-modal multistep fusion network with co-attention for visual question answering. ||| 47671 ||| 13807 ||| 1341 ||| 1340 ||| 
2019 ||| coronary arteries segmentation based on 3d fcn with attention gate and level set function. ||| 47672 ||| 30831 ||| 30832 ||| 29202 ||| 30833 ||| 47673 ||| 
2018 ||| estimation of perceptual surface property using deep networks with attention models. ||| 47674 ||| 47675 ||| 47676 ||| 47677 ||| 
2020 ||| prior attention enhanced convolutional neural network based automatic segmentation of organs at risk for head and neck cancer radiotherapy. ||| 47678 ||| 47679 ||| 24342 ||| 42809 ||| 47680 ||| 27776 ||| 47681 ||| 2903 ||| 47682 ||| 4151 ||| 
2019 ||| an automatic scale-adaptive approach with attention mechanism-based crowd spatial information for crowd counting. ||| 40526 ||| 8440 ||| 47683 ||| 43917 ||| 
2019 ||| short-term photovoltaic power forecasting based on long short term memory neural network and attention mechanism. ||| 47684 ||| 11443 ||| 47685 ||| 13809 ||| 2379 ||| 8836 ||| 
2018 ||| influence of high voltage dc transmission on measuring accuracy of current transformers. ||| 47686 ||| 47687 ||| 47688 ||| 
2020 ||| an attention guided semi-supervised learning mechanism to detect electricity frauds in the distribution systems. ||| 47689 ||| 47690 ||| 47691 ||| 47692 ||| 47693 ||| 47694 ||| 
2020 ||| a semi-supervised autoencoder with an auxiliary task (saat) for power transformer fault diagnosis using dissolved gas analysis. ||| 47695 ||| 47696 ||| 47697 ||| 47698 ||| 47699 ||| 47700 ||| 47701 ||| 47702 ||| 
2020 ||| validity evaluation of transformer dga online monitoring data in grid edge systems. ||| 29101 ||| 47703 ||| 47704 ||| 47705 ||| 47706 ||| 1241 ||| 
2019 ||| multi-gram cnn-based self-attention model for relation classification. ||| 4152 ||| 30823 ||| 10971 ||| 13203 ||| 785 ||| 12169 ||| 16741 ||| 13205 ||| 
2020 ||| modeling and characterization of series connected hybrid transformers for low-profile power converters. ||| 47707 ||| 47708 ||| 47709 ||| 
2021 ||| impact of geomagnetically induced currents on high voltage transformers in malaysian power network and its mitigation. ||| 47710 ||| 47711 ||| 47712 ||| 47713 ||| 
2020 ||| an end to end framework with adaptive spatio-temporal attention module for human action recognition. ||| 47714 ||| 9442 ||| 42650 ||| 31541 ||| 
2020 ||| deep high-resolution network with double attention residual blocks for human pose estimation. ||| 47715 ||| 47716 ||| 47717 ||| 47718 ||| 
2020 ||| a novel approach for analyzing entity linking between words and entities for a knowledge base using an attention-based bilinear joint learning and weighted summation model. ||| 47719 ||| 10816 ||| 10815 ||| 
2020 ||| enhanced switching pattern to improve cell balancing performance in active cell balancing circuit using multi-winding transformer. ||| 47720 ||| 47721 ||| 47722 ||| 47723 ||| 47724 ||| 
2020 ||| a cascaded r-cnn with multiscale attention and imbalanced samples for traffic sign detection. ||| 5365 ||| 47725 ||| 47726 ||| 47727 ||| 3313 ||| 
2020 ||| multi-head self-attention-based deep clustering for single-channel speech separation. ||| 46207 ||| 47728 ||| 47729 ||| 247 ||| 
2020 ||| mpan: multi-part attention network for point cloud based 3d shape retrieval. ||| 47730 ||| 47731 ||| 3226 ||| 6281 ||| 19742 ||| 
2019 ||| attention-guided network for semantic video segmentation. ||| 27542 ||| 47732 ||| 11391 ||| 20171 ||| 2058 ||| 
2019 ||| long document classification from local word glimpses via recurrent attention learning. ||| 532 ||| 47733 ||| 11307 ||| 47734 ||| 3890 ||| 
2022 ||| on the effects of lamination artificial faults in a 15 kva three-phase transformer core. ||| 47735 ||| 47736 ||| 47737 ||| 47738 ||| 
2020 ||| remaining useful life estimation of bldc motor considering voltage degradation and attention-based neural network. ||| 47739 ||| 47740 ||| 
2019 ||| investigation of attention deficit/hyperactivity disorder assessment using electro interstitial scan based on chronoamperometry technique. ||| 47741 ||| 47742 ||| 47743 ||| 47744 ||| 
2019 ||| dualattn-gan: text to image synthesis with dual attentional generative adversarial network. ||| 47745 ||| 133 ||| 47746 ||| 136 ||| 47747 ||| 135 ||| 47748 ||| 
2021 ||| modulation and control of a dc-ac converter with high-frequency link transformer for grid-connected applications. ||| 47749 ||| 47750 ||| 22152 ||| 47751 ||| 47752 ||| 47753 ||| 
2021 ||| sss-ae: anomaly detection using self-attention based sequence-to-sequence auto-encoder in smd assembly machine sound. ||| 47754 ||| 47755 ||| 47756 ||| 
2020 ||| attention-based sequence learning model for travel time estimation. ||| 10918 ||| 5033 ||| 5034 ||| 47757 ||| 
2019 ||| hyperspectral image classification with pre-activation residual attention network. ||| 45384 ||| 24057 ||| 47758 ||| 45387 ||| 
2020 ||| an adaptive multiscale fusion network based on regional attention for remote sensing images. ||| 46195 ||| 47759 ||| 46194 ||| 2735 ||| 46191 ||| 
2021 ||| sal-hmax: an enhanced hmax model in conjunction with a visual attention mechanism to improve object recognition task. ||| 47760 ||| 47761 ||| 47762 ||| 
2020 ||| research on current limiting method used for short circuit fault current of resonant dc transformer based on inverted displacement phase control. ||| 47763 ||| 5157 ||| 43700 ||| 7605 ||| 
2021 ||| the impact of rotor torques for the pressure ratio characteristics of the double rotor hydraulic transformer. ||| 45680 ||| 47764 ||| 47765 ||| 45679 ||| 
2021 ||| emotion wheel attention-based emotion distribution learning. ||| 47766 ||| 47767 ||| 47768 ||| 47769 ||| 
2020 ||| non-locally up-down convolutional attention network for remote sensing image super-resolution. ||| 2054 ||| 6797 ||| 44570 ||| 44571 ||| 44572 ||| 
2019 ||| multi-label image classification by feature attention network. ||| 47770 ||| 21340 ||| 44800 ||| 25908 ||| 
2019 ||| prostate mr image segmentation with self-attention adversarial training based on wasserstein distance. ||| 11700 ||| 47771 ||| 748 ||| 47772 ||| 17757 ||| 
2018 ||| air core transformer winding disk deformation: a precise study on mutual inductance variation and its influence on frequency response spectrum. ||| 28312 ||| 46934 ||| 47773 ||| 47774 ||| 
2020 ||| application of duality-based equivalent circuits for modeling multilimb transformers using alternative input parameters. ||| 47775 ||| 47776 ||| 47777 ||| 47778 ||| 47779 ||| 
2021 ||| attention-based convolution skip bidirectional long short-term memory network for speech emotion recognition. ||| 47780 ||| 47781 ||| 47782 ||| 
2021 ||| foreground feature attention module based on unsupervised saliency detector for few-shot learning. ||| 47783 ||| 47784 ||| 47785 ||| 47786 ||| 
2021 ||| modeling global spatial-temporal graph attention network for traffic prediction. ||| 36003 ||| 45667 ||| 47787 ||| 45668 ||| 
2019 ||| routine test analysis in power transformers by using firefly algorithm and computer program. ||| 47788 ||| 
2020 ||| flagging implausible inspection reports of distribution transformers via anomaly detection. ||| 47789 ||| 47790 ||| 47791 ||| 
2021 ||| a transformer-less voltage equalizer for energy storage cells based on double-tiered multi-stacked converters. ||| 47792 ||| 47793 ||| 47794 ||| 37845 ||| 17179 ||| 47795 ||| 47796 ||| 3906 ||| 
2021 ||| mobile service traffic classification based on joint deep learning with attention mechanism. ||| 47797 ||| 6964 ||| 6965 ||| 47798 ||| 
2020 ||| weakly supervised local-global attention network for facial expression recognition. ||| 36552 ||| 44737 ||| 42203 ||| 
2020 ||| residual flux density measurement method of single-phase transformer core based on time constant. ||| 28474 ||| 28476 ||| 2622 ||| 28477 ||| 28475 ||| 
2020 ||| dynamic phasor modeling of various multipulse rectifiers and a vsi fed by 18-pulse asymmetrical autotransformer rectifier unit for fast transient analysis. ||| 47799 ||| 47800 ||| 47062 ||| 
2019 ||| r-transformer network based on position and self-attention mechanism for aspect-level sentiment classification. ||| 44713 ||| 9082 ||| 31519 ||| 
2019 ||| named entity recognition from biomedical texts using a fusion attention-based bilstm-crf. ||| 16567 ||| 47801 ||| 16565 ||| 7084 ||| 47802 ||| 47803 ||| 16568 ||| 
2019 ||| multi-attention object detection model in remote sensing images based on multi-scale. ||| 16769 ||| 3304 ||| 14761 ||| 5950 ||| 47804 ||| 8479 ||| 16773 ||| 16774 ||| 
2020 ||| a zero-sequence steerable cbpwm strategy for eliminating zero-sequence current of dual-inverter fed open-end winding transformer based pv grid-tied system with common dc bus. ||| 47805 ||| 9535 ||| 47806 ||| 
2021 ||| spacetransformers: language modeling for space systems. ||| 47807 ||| 47808 ||| 47809 ||| 
2021 ||| deep neural networks using capsule networks and skeleton-based attentions for action recognition. ||| 21732 ||| 21733 ||| 
2020 ||| recognizing the hrrp by combining cnn and birnn with attention mechanism. ||| 31787 ||| 9283 ||| 47810 ||| 47811 ||| 31788 ||| 31789 ||| 
2019 ||| tree-structured neural networks with topic attention for social emotion classification. ||| 8903 ||| 8904 ||| 47812 ||| 
2022 ||| translating melody to chord: structured and flexible harmonization of melody with transformer. ||| 47813 ||| 47814 ||| 47815 ||| 47816 ||| 
2021 ||| enhanced dense space attention network for super-resolution construction from single input image. ||| 47817 ||| 47818 ||| 47819 ||| 
2020 ||| multi-layer transformer aggregation encoder for answer generation. ||| 47820 ||| 3888 ||| 47821 ||| 
2019 ||| prediction of transformers conditions and lifetime using furan compounds analysis. ||| 47822 ||| 47823 ||| 47824 ||| 47825 ||| 
2020 ||| embedding encoder-decoder with attention mechanism for monaural speech enhancement. ||| 7955 ||| 47256 ||| 12514 ||| 44069 ||| 8922 ||| 
2019 ||| cross-domain sentiment classification with bidirectional contextualized transformer language models. ||| 47826 ||| 4634 ||| 47827 ||| 
2020 ||| pa-gan: a patch-attention based aggregation network for face recognition in surveillance. ||| 124 ||| 47828 ||| 3750 ||| 47829 ||| 
2019 ||| individual dc voltage balance control for cascaded h-bridge electronic power transformer with separated dc-link topology. ||| 344 ||| 47830 ||| 47831 ||| 21288 ||| 47832 ||| 21289 ||| 2182 ||| 
2019 ||| a decentralized optimal operation of ac/dc hybrid microgrids equipped with power electronic transformer. ||| 47833 ||| 6514 ||| 47834 ||| 47835 ||| 47836 ||| 
2021 ||| cross-domain fault diagnosis of rotating machinery using discriminative feature attention network. ||| 47837 ||| 47838 ||| 15282 ||| 
2019 ||| pam: pyramid attention mechanism based on contextual reasoning. ||| 47839 ||| 47840 ||| 47841 ||| 47842 ||| 47843 ||| 47844 ||| 1251 ||| 
2020 ||| sr-hgat: symmetric relations based heterogeneous graph attention network. ||| 2778 ||| 8447 ||| 38168 ||| 
2019 ||| a novel maintenance decision making model of power transformers based on reliability and economy assessment. ||| 47845 ||| 29216 ||| 18177 ||| 47846 ||| 29891 ||| 47847 ||| 47848 ||| 15544 ||| 
2021 ||| residual vector capsule: improving capsule by pose attention. ||| 13410 ||| 47849 ||| 
2020 ||| syntactic edge-enhanced graph convolutional networks for aspect-level sentiment classification with interactive attention. ||| 4286 ||| 11710 ||| 
2019 ||| abne: an attention-based network embedding for user alignment across social networks. ||| 2014 ||| 47850 ||| 47851 ||| 47852 ||| 844 ||| 47853 ||| 
2020 ||| hierarchical multi-granularity attention- based hybrid neural network for text classification. ||| 10978 ||| 34361 ||| 34362 ||| 34363 ||| 34364 ||| 
2020 ||| power control of a modular three-port solid-state transformer with three-phase unbalance regulation capabilities. ||| 47854 ||| 47855 ||| 47856 ||| 27534 ||| 47857 ||| 
2021 ||| ast-mtl: an attention-based multi-task learning strategy for traffic forecasting. ||| 47858 ||| 47859 ||| 47860 ||| 
2021 ||| intelligent classifiers in distinguishing transformer faults using frequency response analysis. ||| 47861 ||| 47862 ||| 47863 ||| 
2020 ||| a model of text-enhanced knowledge graph representation learning with mutual attention. ||| 21183 ||| 6275 ||| 32004 ||| 47864 ||| 7965 ||| 
2020 ||| exploring the correlation between attention and cognitive load through association rule mining by using a brainwave sensing headband. ||| 17328 ||| 17326 ||| 17325 ||| 47865 ||| 
2020 ||| a sparse transformer-based approach for image captioning. ||| 44055 ||| 47387 ||| 44053 ||| 44068 ||| 47386 ||| 
2020 ||| transient oscillation suppression method of modular multilevel dc transformer. ||| 47866 ||| 47867 ||| 47868 ||| 27852 ||| 10877 ||| 47869 ||| 
2019 ||| ce-heat: an aspect-level sentiment classification approach with collaborative extraction hierarchical attention network. ||| 5089 ||| 162 ||| 5090 ||| 5091 ||| 
2019 ||| comparison of pd and breakdown characteristics induced by metal particles and bubbles in flowing transformer oil. ||| 46992 ||| 46991 ||| 46990 ||| 37870 ||| 
2018 ||| novel approach for optimizing the transformer's critical power limit. ||| 47870 ||| 47871 ||| 47872 ||| 47873 ||| 47874 ||| 47875 ||| 
2021 ||| an accurate analytical method for leakage inductance calculation of shell-type transformers with rectangular windings. ||| 47286 ||| 47287 ||| 47288 ||| 47289 ||| 
2020 ||| an interpretable visual attention plug-in for convolutions. ||| 40123 ||| 47876 ||| 47877 ||| 29465 ||| 
2021 ||| multi-level multi-modal cross-attention network for fake news detection. ||| 47878 ||| 13435 ||| 9548 ||| 47879 ||| 1174 ||| 
2020 ||| aspect-based fashion recommendation with attention mechanism. ||| 47880 ||| 47881 ||| 
2020 ||| attention mask r-cnn for ship detection and segmentation from remote sensing images. ||| 44547 ||| 47882 ||| 44548 ||| 47883 ||| 44550 ||| 
2021 ||| power transformer fault diagnosis based on dga using a convolutional neural network with noise in measurements. ||| 46635 ||| 47884 ||| 46238 ||| 
2019 ||| cross-media body-part attention network for image-to-video person re-identification. ||| 47885 ||| 7957 ||| 25471 ||| 
2020 ||| stack-vs: stacked visual-semantic attention for image caption generation. ||| 34044 ||| 5474 ||| 34045 ||| 4297 ||| 16696 ||| 
2021 ||| adversarial networks with circular attention mechanism for fine-grained domain adaptation. ||| 12281 ||| 12282 ||| 
2020 ||| a novel end-to-end corporate credit rating model based on self-attention mechanism. ||| 36540 ||| 47886 ||| 
2021 ||| attentional behavior of children with asd in response to robotic agents. ||| 17044 ||| 47887 ||| 21785 ||| 20474 ||| 47303 ||| 17043 ||| 17045 ||| 
2019 ||| two-level attention model based video action recognition network. ||| 43943 ||| 41179 ||| 43945 ||| 
2021 ||| targeted aspect-based multimodal sentiment analysis: an attention capsule extraction and multi-head fusion network. ||| 526 ||| 33188 ||| 47888 ||| 33189 ||| 33190 ||| 33191 ||| 525 ||| 300 ||| 
2019 ||| detecting hypernymy relations between medical compound entities using a hybrid-attention based bi-gru-capsnet model. ||| 16882 ||| 16883 ||| 6627 ||| 5036 ||| 10977 ||| 
2020 ||| conditionally learn to pay attention for sequential visual task. ||| 532 ||| 31831 ||| 241 ||| 31830 ||| 
2020 ||| csanet: channel and spatial mixed attention cnn for pedestrian detection. ||| 47889 ||| 47890 ||| 816 ||| 7676 ||| 47891 ||| 817 ||| 19289 ||| 
2019 ||| a lightweight moving vehicle classification system through attention-based method and deep learning. ||| 45065 ||| 45066 ||| 45067 ||| 
2019 ||| enhancing attention-based lstm with position context for aspect-level sentiment classification. ||| 47892 ||| 8697 ||| 41172 ||| 
2021 ||| discrete-state event-driven numerical prototyping of megawatt solid-state transformers and ac/dc hybrid microgrids. ||| 47893 ||| 47894 ||| 47895 ||| 47896 ||| 
2020 ||| attention-aware joint location constraint hashing for multi-label image retrieval. ||| 47897 ||| 28299 ||| 47898 ||| 42972 ||| 42761 ||| 
2020 ||| metro passenger flow prediction model using attention-based neural network. ||| 13171 ||| 47899 ||| 47900 ||| 
2020 ||| object tracking based on channel attention. ||| 46737 ||| 47901 ||| 
2021 ||| study of the impregnation of power-transformer cellulosic materials with dielectric ester oils. ||| 47902 ||| 47903 ||| 3882 ||| 47356 ||| 47904 ||| 10314 ||| 47905 ||| 47357 ||| 
2021 ||| fault diagnosis of oil-immersed power transformer based on difference-mutation brain storm optimized catboost model. ||| 47906 ||| 47907 ||| 9472 ||| 523 ||| 47908 ||| 47909 ||| 47910 ||| 
2020 ||| self-attention-masking semantic decomposition and segmentation for facial attribute manipulation. ||| 47911 ||| 47912 ||| 5192 ||| 47913 ||| 2854 ||| 47914 ||| 
2019 ||| attention-based denseunet network with adversarial training for skin lesion segmentation. ||| 28955 ||| 16904 ||| 1207 ||| 16895 ||| 47915 ||| 
2020 ||| multi-adversarial partial transfer learning with object-level attention mechanism for unsupervised remote sensing scene classification. ||| 3675 ||| 31138 ||| 26814 ||| 189 ||| 46149 ||| 
2021 ||| regenerative active electronic load with current, voltage and frequency control for power transformer testing. ||| 47916 ||| 47917 ||| 47918 ||| 47919 ||| 47920 ||| 47921 ||| 
2020 ||| design and analysis of tension control system for transformer insulation layer winding. ||| 47922 ||| 47923 ||| 14492 ||| 41115 ||| 31874 ||| 
2021 ||| sca-net: a spatial and channel attention network for medical image segmentation. ||| 47924 ||| 47925 ||| 
2019 ||| improvements to the construction of bubble inception formulae for use with transformer insulation. ||| 47926 ||| 16208 ||| 1073 ||| 47927 ||| 47928 ||| 47929 ||| 
2021 ||| efficient attention pyramid network for semantic segmentation. ||| 47930 ||| 47931 ||| 47932 ||| 
2018 ||| novel upper-limb rehabilitation system based on attention technology for post-stroke patients: a preliminary study. ||| 47933 ||| 47934 ||| 47935 ||| 
2020 ||| parameters identification and application of equivalent circuit at low frequency of oil-paper insulation in transformer. ||| 46459 ||| 46385 ||| 
2019 ||| a new technique to estimate the degree of polymerization of insulation paper using multiple aging parameters of transformer oil. ||| 5103 ||| 47936 ||| 28557 ||| 42083 ||| 46932 ||| 47937 ||| 
2019 ||| aagan: enhanced single image dehazing with attention-to-attention generative adversarial network. ||| 3497 ||| 5806 ||| 47938 ||| 4811 ||| 47939 ||| 
2021 ||| moon impact crater detection using nested attention mechanism based unet++. ||| 30183 ||| 10330 ||| 47940 ||| 
2021 ||| transanomaly: video anomaly detection using video vision transformer. ||| 47941 ||| 47942 ||| 18425 ||| 7400 ||| 47943 ||| 
2020 ||| load noise prediction of high-voltage transformers by equation applying 3-d emcn. ||| 47944 ||| 47945 ||| 47946 ||| 47947 ||| 47948 ||| 
2021 ||| millimeter wave path loss modeling for 5g communications using deep learning with dilated convolution and attention. ||| 1860 ||| 19475 ||| 47949 ||| 47950 ||| 
2022 ||| social media popularity prediction based on multi-modal self-attention mechanisms. ||| 47951 ||| 47952 ||| 47953 ||| 2212 ||| 11299 ||| 
2021 ||| a prediction model of hot spot temperature for split-windings traction transformer considering the load characteristics. ||| 18177 ||| 47954 ||| 28372 ||| 25099 ||| 47955 ||| 781 ||| 47956 ||| 47957 ||| 28373 ||| 
2020 ||| eye-contact game using mixed reality for the treatment of children with attention deficit hyperactivity disorder. ||| 47958 ||| 47959 ||| 47960 ||| 47961 ||| 47962 ||| 47963 ||| 
2020 ||| three-phase flexible transformer based on bipolar direct ac/ac chopper and its control strategy. ||| 32453 ||| 47964 ||| 47965 ||| 47966 ||| 47967 ||| 46952 ||| 
2020 ||| a gated recurrent unit network model for predicting open channel flow in coal mines based on attention mechanisms. ||| 47968 ||| 17797 ||| 204 ||| 47969 ||| 
2019 ||| transformer incipient hybrid fault diagnosis based on solar-powered rfid sensor and optimized dbn approach. ||| 128 ||| 22230 ||| 28321 ||| 8838 ||| 
2021 ||| a deep learning model based on bert and sentence transformer for semantic keyphrase extraction on big social data. ||| 47970 ||| 47971 ||| 47972 ||| 47973 ||| 47974 ||| 
2022 ||| spatial attention guided residual attention network for hyperspectral image classification. ||| 11240 ||| 11241 ||| 
2020 ||| cephann: a multi-head attention network for cephalometric landmark detection. ||| 40303 ||| 40304 ||| 5287 ||| 22570 ||| 10888 ||| 13541 ||| 
2019 ||| the location of partial discharge sources inside power transformers based on tdoa database with uhf sensors. ||| 14154 ||| 14155 ||| 47975 ||| 3676 ||| 47976 ||| 47977 ||| 47978 ||| 47979 ||| 
2020 ||| global spatio-temporal attention for action recognition based on 3d human skeleton data. ||| 937 ||| 938 ||| 47980 ||| 941 ||| 942 ||| 
2022 ||| super-resolution reconstruction of 3t-like images from 0.35t mri using a hybrid attention residual network. ||| 47981 ||| 47982 ||| 47983 ||| 47984 ||| 47985 ||| 47986 ||| 38454 ||| 
2019 ||| fca-net: adversarial learning for skin lesion segmentation based on multi-scale features and factorized channel attention. ||| 32875 ||| 39106 ||| 39108 ||| 46522 ||| 47987 ||| 47988 ||| 47989 ||| 47990 ||| 39107 ||| 
2020 ||| hierarchical attention-based fusion for image caption with multi-grained rewards. ||| 18635 ||| 47991 ||| 47992 ||| 18634 ||| 18638 ||| 
2019 ||| co-attention network with question type for visual question answering. ||| 497 ||| 47993 ||| 170 ||| 40655 ||| 29376 ||| 
2021 ||| recipebowl: a cooking recommender for ingredients and recipes using set transformer. ||| 47994 ||| 47995 ||| 47996 ||| 47997 ||| 9250 ||| 
2020 ||| multi-attention-based capsule network for uyghur personal pronouns resolution. ||| 29749 ||| 29080 ||| 29079 ||| 43810 ||| 
2021 ||| experimental validation of a method of drying cellulose insulation in distribution transformers using circulating synthetic ester. ||| 47998 ||| 47999 ||| 41250 ||| 48000 ||| 48001 ||| 48002 ||| 
2020 ||| frequency domain spectroscopy for non-uniformly distributed moisture detection in transformer bushings. ||| 47274 ||| 47275 ||| 36890 ||| 
2021 ||| tracking attention of social media event by hidden markov model-cases from sina weibo. ||| 48003 ||| 48004 ||| 25400 ||| 
2019 ||| thermal modelling of a power transformer disc type winding immersed in mineral and ester-based oils using network models and cfd. ||| 47355 ||| 48005 ||| 504 ||| 47359 ||| 47360 ||| 47905 ||| 
2021 ||| research on variable frequency transformer: a smart power transmission technology. ||| 48006 ||| 48007 ||| 48008 ||| 48009 ||| 
2021 ||| boosting inertial-based human activity recognition with transformers. ||| 2260 ||| 48010 ||| 
2020 ||| a lexicon-enhanced attention network for aspect-level sentiment analysis. ||| 48011 ||| 5830 ||| 5829 ||| 5831 ||| 48012 ||| 48013 ||| 
2021 ||| running status diagnosis of onboard traction transformers based on kernel principal component analysis and fuzzy clustering. ||| 48014 ||| 47084 ||| 47086 ||| 
2021 ||| efficient attention fusion network in wavelet domain for demoireing. ||| 48015 ||| 48016 ||| 4719 ||| 48017 ||| 
2020 ||| inattentional blindness for redirected walking using dynamic foveated rendering. ||| 37858 ||| 37859 ||| 
2020 ||| siamese cascaded region proposal networks with channel-interconnection-spatial attention for visual tracking. ||| 48018 ||| 48019 ||| 43603 ||| 48020 ||| 
2021 ||| an integrated transformer design with a center-core air-gap for dab converters. ||| 48021 ||| 48022 ||| 48023 ||| 48024 ||| 
2020 ||| enhanced visual attention-guided deep neural networks for image classification. ||| 43615 ||| 48025 ||| 48026 ||| 40126 ||| 
2022 ||| unrestricted attention may not be all you need-masked attention mechanism focuses better on relevant parts in aspect-based sentiment analysis. ||| 48027 ||| 48028 ||| 48029 ||| 
2021 ||| ragat: relation aware graph attention network for knowledge graph completion. ||| 48030 ||| 48031 ||| 48032 ||| 48033 ||| 
2022 ||| lotr: face landmark localization using localization transformer. ||| 33099 ||| 33100 ||| 33101 ||| 33102 ||| 33103 ||| 33104 ||| 33105 ||| 33106 ||| 33107 ||| 
2019 ||| pedestrian heading estimation based on spatial transformer networks and hierarchical lstm. ||| 45377 ||| 32242 ||| 48034 ||| 11295 ||| 18829 ||| 4648 ||| 44324 ||| 
2020 ||| channel-attention u-net: channel attention mechanism for semantic segmentation of esophagus and esophageal cancer. ||| 6984 ||| 48035 ||| 48036 ||| 30133 ||| 5479 ||| 27850 ||| 27849 ||| 25471 ||| 
2021 ||| effects of spike voltages coupling with high dv/dt square wave on dielectric loss and electric-thermal field of high-frequency transformer. ||| 48037 ||| 48038 ||| 5439 ||| 398 ||| 46932 ||| 
2020 ||| an improved loss-separation method for transformer core loss calculation and its experimental verification. ||| 48039 ||| 5744 ||| 5536 ||| 5672 ||| 
2021 ||| study on operation parameter characteristics of induction filter distribution transformer in low-voltage distribution network. ||| 1190 ||| 48040 ||| 48041 ||| 48042 ||| 
2019 ||| hybrid-frequency cascaded full-bridge solid-state transformer. ||| 48043 ||| 48044 ||| 48045 ||| 48046 ||| 48047 ||| 
2020 ||| towards understanding attention-based speech recognition models. ||| 40214 ||| 40077 ||| 
2019 ||| infrared and visible image fusion using detail enhanced channel attention network. ||| 48048 ||| 42373 ||| 42374 ||| 
2020 ||| feature pyramid attention model and multi-label focal loss for pedestrian attribute recognition. ||| 11494 ||| 48049 ||| 24092 ||| 48050 ||| 242 ||| 24093 ||| 
2020 ||| a data mining approach for transformer failure rate modeling based on daily oil chromatographic data. ||| 471 ||| 315 ||| 19828 ||| 48051 ||| 48052 ||| 24979 ||| 5228 ||| 48053 ||| 48054 ||| 48055 ||| 48056 ||| 
2019 ||| digital image steganalysis based on visual attention and deep reinforcement learning. ||| 48057 ||| 48058 ||| 36426 ||| 48059 ||| 147 ||| 45190 ||| 
2021 ||| amr-net: arbitrary-oriented ship detection using attention module, multi-scale feature fusion and rotation pseudo-label. ||| 27551 ||| 7700 ||| 47082 ||| 220 ||| 
2019 ||| sequential image-based attention network for inferring force estimation without haptic sensor. ||| 37571 ||| 48060 ||| 37572 ||| 37573 ||| 37574 ||| 37575 ||| 
2020 ||| sequence generation network based on hierarchical attention for multi-charge prediction. ||| 48061 ||| 48062 ||| 48063 ||| 46242 ||| 46239 ||| 35137 ||| 
2020 ||| dielectric performance of magneto-nanofluids for advancing oil-immersed power transformer. ||| 47257 ||| 48064 ||| 47586 ||| 10933 ||| 10934 ||| 
2020 ||| a remote-sensing image pan-sharpening method based on multi-scale channel attention residual network. ||| 633 ||| 29359 ||| 46476 ||| 27771 ||| 48065 ||| 48066 ||| 48067 ||| 
2022 ||| joint learning with bert-gcn and multi-attention for event text classification and event assignment. ||| 48068 ||| 38128 ||| 1216 ||| 
2020 ||| a deep multi-attention driven approach for multi-label remote sensing image classification. ||| 6902 ||| 6903 ||| 6904 ||| 
2017 ||| 3d cnn based automatic diagnosis of attention deficit hyperactivity disorder using functional and structural mri. ||| 2982 ||| 2983 ||| 16696 ||| 2984 ||| 2188 ||| 
2019 ||| temperature analysis in power transformer windings using created artificial bee algorithm and computer program. ||| 47788 ||| 
2020 ||| attention guided u-net with atrous convolution for accurate retinal vessels segmentation. ||| 48069 ||| 41576 ||| 48070 ||| 48071 ||| 
2021 ||| estimation of life cycle of distribution transformer in context to furan content formation, pollution index, and dielectric strength. ||| 48072 ||| 48073 ||| 48074 ||| 48075 ||| 48076 ||| 48077 ||| 48078 ||| 
2020 ||| improving graph convolutional networks based on relation-aware attention for end-to-end relation extraction. ||| 48079 ||| 47275 ||| 48080 ||| 48081 ||| 48082 ||| 38871 ||| 
2021 ||| the nlp cookbook: modern recipes for transformer based deep learning architectures. ||| 33303 ||| 33304 ||| 
2020 ||| multiple attention network for facial expression recognition. ||| 48083 ||| 30100 ||| 48084 ||| 48085 ||| 
2020 ||| bdars_capsnet: bi-directional attention routing sausage capsule network. ||| 40031 ||| 48086 ||| 44686 ||| 48087 ||| 48088 ||| 44527 ||| 48089 ||| 
2021 ||| multi-head attentional point cloud classification and segmentation using strictly rotation-invariant representations. ||| 40379 ||| 48090 ||| 40380 ||| 48091 ||| 
2021 ||| a deep learning approach for robust detection of bots in twitter using transformers. ||| 48092 ||| 48093 ||| 26875 ||| 48094 ||| 48095 ||| 48096 ||| 48097 ||| 10314 ||| 48098 ||| 48099 ||| 42064 ||| 
2021 ||| sentiment analysis of review text based on bigru-attention and hybrid cnn. ||| 17662 ||| 48100 ||| 48101 ||| 
2021 ||| hunt for unseen intrusion: multi-head self-attention neural detector. ||| 48102 ||| 48103 ||| 48104 ||| 48105 ||| 48106 ||| 48107 ||| 48108 ||| 
2020 ||| long short-term memory with attention mechanism for state of charge estimation of lithium-ion batteries. ||| 48109 ||| 48110 ||| 
2019 ||| video captioning with adaptive attention and mixed loss optimization. ||| 30721 ||| 30722 ||| 
2021 ||| pa-mvsnet: sparse-to-dense multi-view stereo with pyramid attention. ||| 19919 ||| 11232 ||| 48111 ||| 48112 ||| 
2022 ||| multi-modality reconstruction attention and difference enhancement network for brain mri image segmentation. ||| 48113 ||| 9337 ||| 48114 ||| 44392 ||| 
2020 ||| coupled rain streak and background estimation via separable element-wise attention. ||| 48115 ||| 35359 ||| 8636 ||| 8538 ||| 35360 ||| 19152 ||| 
2021 ||| forecasting copper electrorefining cathode rejection by means of recurrent neural networks with attention mechanism. ||| 48116 ||| 48117 ||| 48118 ||| 10318 ||| 48119 ||| 48120 ||| 
2021 ||| multi-head self-attention for 3d point cloud classification. ||| 48121 ||| 48122 ||| 48123 ||| 48124 ||| 
2018 ||| a modified single-phase transformerless y-source pv grid-connected inverter. ||| 48125 ||| 48126 ||| 48127 ||| 1160 ||| 21313 ||| 
2021 ||| enhancements of attention-based bidirectional lstm for hybrid automatic text summarization. ||| 48128 ||| 48129 ||| 48130 ||| 48131 ||| 2078 ||| 48132 ||| 48133 ||| 
2020 ||| pgcn-tca: pseudo graph convolutional network with temporal and channel-wise attention for skeleton-based action recognition. ||| 48134 ||| 48135 ||| 48136 ||| 48137 ||| 48138 ||| 10140 ||| 
2020 ||| improving the performance of convolutional neural network for the segmentation of optic disc in fundus images using attention gates and conditional random fields. ||| 48139 ||| 48140 ||| 48141 ||| 48142 ||| 
2021 ||| sentiment analysis using multi-head attention capsules with multi-channel cnn and bidirectional gru. ||| 47206 ||| 48143 ||| 48144 ||| 5860 ||| 48145 ||| 48146 ||| 14316 ||| 
2020 ||| the performance analysis of signal recognition using attention based cnn method. ||| 48147 ||| 9283 ||| 48148 ||| 26659 ||| 1349 ||| 
2021 ||| enhancing diagnostic accuracy of transformer faults using teaching-learning-based optimization. ||| 46634 ||| 40852 ||| 16220 ||| 40853 ||| 
2021 ||| controllable and editable neural story plot generation via control-and-edit transformer. ||| 13706 ||| 48149 ||| 17798 ||| 2424 ||| 
2020 ||| self-attention network with joint loss for remote sensing image scene classification. ||| 48150 ||| 48151 ||| 13824 ||| 48152 ||| 48153 ||| 
2019 ||| calculation of hot spot temperature of transformer bushing considering current fluctuation. ||| 48154 ||| 46385 ||| 46457 ||| 48155 ||| 48156 ||| 48157 ||| 
2019 ||| bi-level attention model for sentiment analysis of short texts. ||| 683 ||| 48158 ||| 36609 ||| 
2019 ||| topological transient models of three-phase, three-legged transformer. ||| 16613 ||| 48159 ||| 48160 ||| 48161 ||| 48162 ||| 48163 ||| 48164 ||| 
2020 ||| extended application for the impulse-based frequency response analysis: preliminary diagnosis of partial discharges in transformer. ||| 48165 ||| 46890 ||| 46891 ||| 
2018 ||| understanding multimodal popularity prediction of social media videos with self-attention. ||| 19334 ||| 19335 ||| 
2021 ||| highly efficient hbt power amplifier using high-q single- and two-winding transformer with imd3 cancellation. ||| 30874 ||| 48166 ||| 30876 ||| 30877 ||| 
2020 ||| deep attention and multi-scale networks for accurate remote sensing image segmentation. ||| 5501 ||| 5498 ||| 5502 ||| 20216 ||| 16673 ||| 
2020 ||| a fusion model-based label embedding and self-interaction attention for text classification. ||| 48167 ||| 20373 ||| 46222 ||| 46556 ||| 44049 ||| 
2020 ||| optimal area-product model (oapm) based non-iterative analytical design methodology for litz-wired high-frequency gapped- transformer (lhfgt) in llc converters. ||| 43515 ||| 1052 ||| 
2020 ||| rating prediction based on merge-cnn and concise attention review mining. ||| 48168 ||| 45239 ||| 41788 ||| 48169 ||| 
2021 ||| short text sentiment analysis based on multi-channel cnn with multi-head attention mechanism. ||| 48170 ||| 47206 ||| 
2019 ||| theory and design of impedance matching network utilizing a lossy on-chip transformer. ||| 41772 ||| 41773 ||| 
2019 ||| attention embedded spatio-temporal network for video salient object detection. ||| 21868 ||| 48171 ||| 1800 ||| 13930 ||| 2315 ||| 
2021 ||| accuracy improvement of power transformer faults diagnostic using knn classifier with decision tree principle. ||| 47738 ||| 48172 ||| 48173 ||| 48174 ||| 46634 ||| 
2020 ||| a methodology for diagnosing faults in oil-immersed power transformers based on minimizing the maintenance cost. ||| 48175 ||| 48176 ||| 48177 ||| 
2020 ||| multi-level context aggregation network with channel-wise attention for salient object detection. ||| 48178 ||| 48179 ||| 2395 ||| 782 ||| 
2019 ||| exploiting transfer learning with attention for in-domain top-n recommendation. ||| 48180 ||| 4600 ||| 
2021 ||| single-shot detection based on cyclic attention. ||| 48181 ||| 48182 ||| 43957 ||| 
2020 ||| multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network. ||| 48183 ||| 2838 ||| 2841 ||| 5696 ||| 
2019 ||| electro-thermal fault diagnosis method of rapo vegetable oil transformer based on characteristic gas and ratio criterion. ||| 48184 ||| 48185 ||| 48186 ||| 48187 ||| 
2019 ||| recurrent models of visual co-attention for person re-identification. ||| 48188 ||| 48189 ||| 48190 ||| 19075 ||| 
2019 ||| recent progress and challenges in transformer oil nanofluid development: a review on thermal and electrical properties. ||| 48191 ||| 48192 ||| 48193 ||| 48194 ||| 48195 ||| 48196 ||| 
2021 ||| metric-based attention feature learning for video action recognition. ||| 48197 ||| 48198 ||| 48199 ||| 21489 ||| 
2019 ||| identification and location of pd defects in medium voltage underground power cables using high frequency current transformer. ||| 47692 ||| 48200 ||| 48201 ||| 48202 ||| 48203 ||| 48204 ||| 48205 ||| 
2021 ||| trilateral attention network for real-time cardiac region segmentation. ||| 34554 ||| 48206 ||| 34555 ||| 27631 ||| 
2021 ||| person gender classification on rgb-d data with self-joint attention. ||| 48207 ||| 36702 ||| 44699 ||| 48208 ||| 
2020 ||| thai spelling correction and word normalization on social text using a two-stage pipeline with neural contextual attention. ||| 48209 ||| 48210 ||| 48211 ||| 
2021 ||| evaluating the inter-resonance characteristics of various power transformer winding designs. ||| 48212 ||| 48213 ||| 23179 ||| 
2021 ||| single-image snow removal based on an attention mechanism and a generative adversarial network. ||| 48214 ||| 41285 ||| 1132 ||| 28236 ||| 
2019 ||| generating emotional controllable response based on multi-task and dual attention framework. ||| 785 ||| 48215 ||| 1382 ||| 
2019 ||| performance analysis and threshold quantization of transformer differential protection under sampled value packets loss/delay. ||| 48216 ||| 9407 ||| 48217 ||| 11464 ||| 48218 ||| 
2020 ||| first-principles calculations of gas-sensing properties of pd clusters decorated alnnts to dissolved gases in transformer oil. ||| 47007 ||| 48219 ||| 47008 ||| 48220 ||| 577 ||| 48221 ||| 
2019 ||| multi-scale context attention network for stereo matching. ||| 28722 ||| 48222 ||| 11819 ||| 
2020 ||| multi-element hierarchical attention capsule network for stock prediction. ||| 48223 ||| 8974 ||| 8967 ||| 728 ||| 48224 ||| 
2020 ||| semantic segmentation of marine remote sensing based on a cross direction attention mechanism. ||| 29358 ||| 41361 ||| 48225 ||| 48226 ||| 48227 ||| 
2021 ||| complex-valued channel attention and application in ego-velocity estimation with automotive radar. ||| 48228 ||| 48229 ||| 48230 ||| 8576 ||| 
2020 ||| constrained image splicing detection and localization with attention-aware encoder-decoder and atrous convolution. ||| 48231 ||| 25452 ||| 
2019 ||| system-level efficiency evaluation of isolated dc/dc converters in power electronics transformers for medium-voltage dc systems. ||| 12459 ||| 25352 ||| 48232 ||| 29107 ||| 29106 ||| 
2021 ||| two-stage attention over lstm with bayesian optimization for day-ahead solar power forecasting. ||| 42345 ||| 48233 ||| 48234 ||| 48235 ||| 
2020 ||| a discriminative person re-identification model with global-local attention and adaptive weighted rank list loss. ||| 48236 ||| 4719 ||| 12639 ||| 12638 ||| 
2019 ||| channel attention networks for image translation. ||| 48237 ||| 7873 ||| 19066 ||| 48238 ||| 5068 ||| 
2021 ||| active vision-based attention monitoring system for non-distracted driving. ||| 48239 ||| 4885 ||| 48240 ||| 48241 ||| 6089 ||| 48242 ||| 4886 ||| 
2021 ||| study on the sound radiation efficiency of a typical distribution transformer. ||| 20800 ||| 48243 ||| 48244 ||| 48245 ||| 48246 ||| 
2019 ||| automatic pavement crack detection and classification using multiscale feature attention network. ||| 48247 ||| 48248 ||| 48249 ||| 6053 ||| 
2020 ||| roi-attention vectorized cnn model for static facial expression recognition. ||| 443 ||| 40102 ||| 48250 ||| 
2022 ||| classification of diabetic retinopathy severity based on gca attention mechanism. ||| 48251 ||| 48252 ||| 48253 ||| 48254 ||| 524 ||| 
2020 ||| enhanced multi-channel feature synthesis for hand gesture recognition based on cnn with a channel and spatial attention mechanism. ||| 31793 ||| 241 ||| 48255 ||| 48256 ||| 48257 ||| 
2020 ||| sentiment analysis model based on self-attention and character-level embedding. ||| 47412 ||| 48258 ||| 2487 ||| 
2021 ||| joint image dehazing and super-resolution: closed shared source residual attention fusion network. ||| 48259 ||| 48260 ||| 48261 ||| 
2019 ||| self residual attention network for deep face recognition. ||| 975 ||| 44012 ||| 23870 ||| 14730 ||| 976 ||| 977 ||| 
2021 ||| boundary-adaptive encoder with attention method for chinese sign language recognition. ||| 48262 ||| 4438 ||| 
2020 ||| multiple features fusion attention mechanism enhanced deep knowledge tracing for student performance prediction. ||| 8709 ||| 48263 ||| 1796 ||| 48264 ||| 48265 ||| 5669 ||| 
2022 ||| frequency characteristics of power transformer for isolated dc-dc converter. ||| 48266 ||| 48267 ||| 
2019 ||| deep attention-guided hashing. ||| 37478 ||| 37479 ||| 37480 ||| 37481 ||| 
2021 ||| a novel deep learning-based multilevel parallel attention neural (mpan) model for multidomain arabic sentiment analysis. ||| 48268 ||| 48269 ||| 30475 ||| 
2020 ||| attention gate resu-net for automatic mri brain tumor segmentation. ||| 11988 ||| 48270 ||| 48271 ||| 815 ||| 2304 ||| 
2019 ||| marking key segment of program input via attention mechanism. ||| 9535 ||| 9537 ||| 48272 ||| 9540 ||| 9541 ||| 
2019 ||| to identify tree species with highly similar leaves based on a novel attention mechanism for cnn. ||| 48273 ||| 42870 ||| 48274 ||| 
2021 ||| a spam transformer model for sms spam detection. ||| 48275 ||| 23995 ||| 23996 ||| 
2020 ||| power of attention in mooc dropout prediction. ||| 48276 ||| 48277 ||| 31572 ||| 19130 ||| 
2019 ||| a magnetic integration half-turn planar transformer and its analysis for llc resonant dc-dc converters. ||| 18040 ||| 48278 ||| 48279 ||| 3248 ||| 48280 ||| 48281 ||| 
2019 ||| unsupervised region attention network for person re-identification. ||| 837 ||| 44741 ||| 19869 ||| 
2020 ||| attention-block deep learning based features fusion in wearable social sensor for mental wellbeing evaluations. ||| 48282 ||| 48283 ||| 48284 ||| 48285 ||| 48286 ||| 41552 ||| 
2018 ||| an attention-based word-level interaction model for knowledge base relation detection. ||| 26439 ||| 3894 ||| 13230 ||| 21132 ||| 4398 ||| 12746 ||| 3279 ||| 31816 ||| 
2020 ||| monitoring method on loosened state and deformational fault of transformer winding based on vibration and reactance information. ||| 48287 ||| 13148 ||| 48288 ||| 
2021 ||| deep spatiotemporal attention network for fine particle matter 2.5 concentration prediction with causality analysis. ||| 48289 ||| 
2018 ||| optimal wind farm cable routing: modeling branches and offshore transformer modules. ||| 48290 ||| 48291 ||| 
2021 ||| power transformer fault diagnosis system based on internet of things. ||| 48292 ||| 5439 ||| 48293 ||| 48294 ||| 48295 ||| 5264 ||| 48296 ||| 
2021 ||| network security situation prediction based on feature separation and dual attention mechanism. ||| 48297 ||| 48298 ||| 48299 ||| 46562 ||| 
2020 ||| few-shot relation classification by context attention-based prototypical networks with bert. ||| 5840 ||| 10225 ||| 11126 ||| 5841 ||| 48300 ||| 
2021 ||| three-phase distribution transformer connections modeling based on matrix operation method by phase-coordinates. ||| 15698 ||| 15699 ||| 15700 ||| 
2019 ||| a novel approach to workload prediction using attention-based lstm encoder-decoder network in cloud environment. ||| 9090 ||| 10820 ||| 48301 ||| 10817 ||| 
2021 ||| spatio-temporal self-attention weighted vlad neural network for action recognition. ||| 11498 ||| 11499 ||| 312 ||| 18040 ||| 48302 ||| 2320 ||| 
2021 ||| neural architecture search for convolutional neural networks with attention. ||| 981 ||| 982 ||| 983 ||| 
2021 ||| haif: a hierarchical attention-based model of filtering invalid webpage. ||| 48303 ||| 48304 ||| 48305 ||| 29033 ||| 
2020 ||| sentence-embedding and similarity via hybrid bidirectional-lstm and cnn utilizing weighted-pooling attention. ||| 1417 ||| 48306 ||| 48307 ||| 48308 ||| 48309 ||| 5890 ||| 
2020 ||| an attention-based gru network for anomaly detection from system logs. ||| 48310 ||| 33942 ||| 48311 ||| 
2018 ||| saliency priority of individual bottom-up attributes in designing visual attention models. ||| 48312 ||| 48313 ||| 
2022 ||| ahrnn: attention-based hybrid robust neural network for emotion recognition. ||| 3617 ||| 2304 ||| 12041 ||| 25761 ||| 14675 ||| 48314 ||| 
2017 ||| a role for attentional reorienting during approximate multiplication and division. ||| 48315 ||| 48316 ||| 3369 ||| 48317 ||| 
2017 ||| dividing attention increases operational momentum. ||| 48318 ||| 48319 ||| 
2022 ||| graph attention spatial-temporal network with collaborative global-local learning for citywide mobile traffic prediction. ||| 15891 ||| 9645 ||| 5021 ||| 4520 ||| 15893 ||| 
2019 ||| wifi csi based passive human activity recognition using attention based blstm. ||| 17993 ||| 17992 ||| 48320 ||| 18245 ||| 17991 ||| 
2022 ||| attention-based gait recognition and walking direction estimation in wi-fi networks. ||| 6781 ||| 2334 ||| 29395 ||| 35986 ||| 34962 ||| 
2020 ||| a transformer-based model for multi-track music generation. ||| 48321 ||| 128 ||| 48322 ||| 48323 ||| 48324 ||| 33308 ||| 33034 ||| 
2020 ||| attention-based multimodal neural network for automatic evaluation of press conferences. ||| 7752 ||| 7753 ||| 7754 ||| 7755 ||| 7756 ||| 
2020 ||| evolutionary large-scale multiobjective optimization for ratio error estimation of voltage transformers. ||| 25681 ||| 25504 ||| 48325 ||| 11478 ||| 9604 ||| 25682 ||| 
2021 ||| efficient evolutionary search of attention convolutional networks via sampled training and node inheritance. ||| 39818 ||| 34626 ||| 25504 ||| 34627 ||| 
2021 ||| robust multimodal representation learning with evolutionary adversarial attention networks. ||| 19722 ||| 30755 ||| 48326 ||| 
2021 ||| the impact of increase in minimum wages on consumer perceptions of service: a transformer model of online restaurant reviews. ||| 48327 ||| 48328 ||| 48329 ||| 
2017 ||| prominent attributes under limited attention. ||| 2422 ||| 48330 ||| 
2021 ||| measuring competition for attention in social media: national women's soccer league players on twitter. ||| 48331 ||| 48332 ||| 
2020 ||| a new time-frequency attention tensor network for language identification. ||| 14246 ||| 12357 ||| 8298 ||| 
2019 ||| a 190 ghz vco with transformer-based push-push frequency doubler in 40 nm cmos. ||| 48333 ||| 48334 ||| 6252 ||| 5775 ||| 
2020 ||| synthetic transformer design using commercially available active components. ||| 48335 ||| 48336 ||| 15350 ||| 48337 ||| 48338 ||| 
2020 ||| attention and feature selection for automatic speech emotion recognition using utterance and syllable-level prosodic features. ||| 48339 ||| 48340 ||| 48341 ||| 
2021 ||| a dual-band transformer-coupled notch filter mixer for 2.45-/5.2-ghz wlan application. ||| 48342 ||| 10780 ||| 45688 ||| 
2021 ||| classification of flower image based on attention mechanism and multi-loss attention network. ||| 47906 ||| 48343 ||| 48344 ||| 
2019 ||| removal notice to "equipping recurrent neural network with cnn-style attention mechanisms for sentiment analysis of network reviews" [comput. commun. (2019) 98-106]. ||| 42334 ||| 42335 ||| 13171 ||| 48345 ||| 48346 ||| 9472 ||| 48347 ||| 48348 ||| 
2019 ||| removed: equipping recurrent neural network with cnn-style attention mechanisms for sentiment analysis of network reviews. ||| 
2022 ||| attention-based federated incremental learning for traffic classification in the internet of things. ||| 48349 ||| 12029 ||| 48350 ||| 47149 ||| 48351 ||| 
2021 ||| mask-rcnn with spatial attention for pedestrian segmentation in cyber-physical systems. ||| 14907 ||| 48352 ||| 
2022 ||| face detection algorithm based on improved tinyyolov3 and attention mechanism. ||| 46150 ||| 15490 ||| 
2021 ||| transformer text recognition with deep learning algorithm. ||| 48353 ||| 48354 ||| 48355 ||| 48356 ||| 48357 ||| 48358 ||| 
2021 ||| fine-grained semantic image synthesis with object-attention generative adversarial network. ||| 214 ||| 17572 ||| 48359 ||| 38122 ||| 128 ||| 48360 ||| 
2021 ||| temporal hierarchical graph attention network for traffic prediction. ||| 335 ||| 332 ||| 48361 ||| 334 ||| 48362 ||| 48363 ||| 48364 ||| 48365 ||| 
2018 ||| concept and attention-based cnn for question retrieval in multi-view learning. ||| 28954 ||| 18515 ||| 18517 ||| 1406 ||| 48366 ||| 7826 ||| 6559 ||| 
2018 ||| characterizing user skills from application usage traces with hierarchical attention recurrent networks. ||| 48367 ||| 8761 ||| 8763 ||| 23973 ||| 48368 ||| 
2020 ||| domain-attention conditional wasserstein distance for multi-source domain adaptation. ||| 48369 ||| 27846 ||| 48370 ||| 1827 ||| 
2020 ||| an attention-based rumor detection model with tree-structured recursive neural networks. ||| 3646 ||| 1310 ||| 1313 ||| 3647 ||| 
2021 ||| gtae: graph transformer-based auto-encoders for linguistic-constrained text style transfer. ||| 19224 ||| 10418 ||| 36947 ||| 1686 ||| 36948 ||| 2315 ||| 
2021 ||| mvgan: multi-view graph attention network for social event detection. ||| 45367 ||| 7688 ||| 44265 ||| 7690 ||| 7689 ||| 
2021 ||| an attentive survey of attention models. ||| 37841 ||| 9765 ||| 37842 ||| 37843 ||| 
2021 ||| multi-stage fusion and multi-source attention network for multi-modal remote sensing image segmentation. ||| 6497 ||| 6496 ||| 48371 ||| 48372 ||| 31439 ||| 1828 ||| 
2018 ||| mining significant microblogs for misinformation identification: an attention-based approach. ||| 1073 ||| 32817 ||| 1075 ||| 10429 ||| 
2020 ||| a discriminative convolutional neural network with context-aware attention. ||| 48373 ||| 23438 ||| 5089 ||| 688 ||| 32005 ||| 
2018 ||| opposing effects of memory-driven and stimulus-driven attention on distractor perception. ||| 48374 ||| 
2020 ||| examining the roles of working memory and visual attention in multiple object tracking expertise. ||| 48375 ||| 48376 ||| 48377 ||| 48378 ||| 
2017 ||| the role of stimulus predictability in the allocation of attentional resources: an eye-tracking study. ||| 48379 ||| 2713 ||| 48380 ||| 48381 ||| 
2017 ||| the effects of multiphasic prepulses on automatic and attention-modulated prepulse inhibition. ||| 48382 ||| 48383 ||| 
2017 ||| erratum to: the effects of multiphasic prepulses on automatic and attention-modulated prepulse inhibition. ||| 48382 ||| 48383 ||| 
2019 ||| effects of attentional behaviours on infant visual preferences and object choice. ||| 48384 ||| 48385 ||| 48386 ||| 48387 ||| 
2022 ||| do attentional focus instructions affect real-time reinvestment during level-ground walking in older adults? ||| 48388 ||| 48389 ||| 
2017 ||| attention to body-parts varies with visual preference and verb-effector associations. ||| 48390 ||| 48391 ||| 48392 ||| 
2017 ||| a computational framework for attentional object discovery in rgb-d videos. ||| 27043 ||| 15007 ||| 46758 ||| 3419 ||| 48393 ||| 5736 ||| 
2020 ||| cue-target onset asynchrony modulates interaction between exogenous attention and audiovisual integration. ||| 48394 ||| 48395 ||| 48396 ||| 48397 ||| 
2017 ||| dysfunctional personality traits in adolescence: effects on alerting, orienting and executive control of attention. ||| 48398 ||| 48399 ||| 48400 ||| 48401 ||| 48402 ||| 48403 ||| 48404 ||| 
2019 ||| climate change images produce an attentional bias associated with pro-environmental disposition. ||| 48405 ||| 48406 ||| 48407 ||| 
2020 ||| assessing orienting of attention to understand the time course of mental calculation. ||| 48408 ||| 48409 ||| 48410 ||| 48411 ||| 
2017 ||| flow and quiet eye: the role of attentional control in flow experience. ||| 48375 ||| 48378 ||| 48376 ||| 
2021 ||| hungry for colours? attentional bias for food crucially depends on perceptual information. ||| 48412 ||| 48413 ||| 48414 ||| 48415 ||| 
2017 ||| the appeal of the devil's eye: social evaluation affects social attention. ||| 48416 ||| 48417 ||| 48418 ||| 48419 ||| 48420 ||| 48421 ||| 
2020 ||| prediction diversity and selective attention in the wisdom of crowds. ||| 33295 ||| 852 ||| 33296 ||| 
2021 ||| the online attention game for digital identity education: an exploratory study. ||| 25445 ||| 25446 ||| 25447 ||| 25448 ||| 25449 ||| 25450 ||| 
2017 ||| competition for attention in the digital age: the case of single releases in the recorded music industry. ||| 48422 ||| 48423 ||| 48424 ||| 
2021 ||| multi-field synergy manipulating soft polymeric hydrogel transformers. ||| 48425 ||| 538 ||| 48426 ||| 48427 ||| 48428 ||| 48429 ||| 48430 ||| 8557 ||| 48431 ||| 1978 ||| 
2021 ||| an end-to-end framework for remaining useful life prediction of rolling bearing based on feature pre-extraction mechanism and deep adaptive transformer model. ||| 48432 ||| 30003 ||| 48433 ||| 48434 ||| 48435 ||| 
2021 ||| biomedical ontology matching through attention-based bidirectional long short-term memory network. ||| 48436 ||| 48437 ||| 1134 ||| 48438 ||| 
2021 ||| an improved steganography without embedding based on attention gan. ||| 34749 ||| 48057 ||| 48059 ||| 48439 ||| 5860 ||| 10212 ||| 
2021 ||| from edge data to recommendation: a double attention-based deformable convolutional network. ||| 9779 ||| 20822 ||| 7058 ||| 20823 ||| 20824 ||| 48440 ||| 
2020 ||| a hybrid recommender method based on multiple dimension attention analysis. ||| 48441 ||| 48442 ||| 24054 ||| 24056 ||| 43503 ||| 41692 ||| 44481 ||| 10974 ||| 
2022 ||| detection of spam reviews through a hierarchical attention architecture with n-gram cnn and bi-lstm. ||| 17606 ||| 1052 ||| 48443 ||| 48444 ||| 
2022 ||| multi-label legal document classification: a deep learning-based approach with label-attention and domain-specific pre-training. ||| 48445 ||| 11851 ||| 48446 ||| 48447 ||| 
2019 ||| an attention-augmented deep architecture for hard drive status monitoring in large-scale storage systems. ||| 430 ||| 48448 ||| 1429 ||| 48449 ||| 1094 ||| 
2019 ||| anthropomorphisms in multimedia learning: attract attention but do not enhance learning? ||| 48450 ||| 48451 ||| 40751 ||| 48452 ||| 48453 ||| 48454 ||| 
2021 ||| learning immunology in a game: learning outcomes, the use of player characters, immersion experiences and visual attention distributions. ||| 48455 ||| 48456 ||| 48457 ||| 
2020 ||| effects of prior knowledge and joint attention on learning from eye movement modelling examples. ||| 48458 ||| 48459 ||| 48460 ||| 48461 ||| 48462 ||| 40225 ||| 
2021 ||| using eye movement modelling examples to guide visual attention and foster cognitive performance: a meta-analysis. ||| 48463 ||| 48464 ||| 48465 ||| 48466 ||| 48467 ||| 48468 ||| 
2017 ||| the effects of cognitive capacity and gaming expertise on attention and comprehension. ||| 48469 ||| 48470 ||| 
2021 ||| attention-driven read-aloud technology increases reading comprehension in children with reading disabilities. ||| 36897 ||| 36898 ||| 36899 ||| 9808 ||| 36900 ||| 
2021 ||| unpacking the black-box of students' visual attention in mathematics and english classrooms: empirical evidence using mini-video recording gadgets. ||| 48471 ||| 48472 ||| 48473 ||| 48474 ||| 
2018 ||| attention to the model's face when learning from video modeling examples in adolescents with and without autism spectrum disorder. ||| 40224 ||| 48475 ||| 40225 ||| 
2018 ||| high frequency transformer design and optimization using bio-inspired algorithms. ||| 48476 ||| 48477 ||| 
2021 ||| automatic diagnosis of attention deficit hyperactivity disorder using machine learning. ||| 43370 ||| 43372 ||| 43371 ||| 43369 ||| 48478 ||| 
2018 ||| real-world plant species identification based on deep convolutional neural networks and visual attention. ||| 37777 ||| 6535 ||| 37779 ||| 37778 ||| 
2021 ||| electric signal synchronization as a behavioural strategy to generate social attention in small groups of mormyrid weakly electric fish and a mobile fish robot. ||| 48479 ||| 48480 ||| 48481 ||| 
2017 ||| an insect-inspired model for visual binding ii: functional analysis and visual attention. ||| 48482 ||| 48483 ||| 
2021 ||| multi-lingual character handwriting framework based on an integrated deep learning based sequence-to-sequence attention model. ||| 48484 ||| 48485 ||| 48486 ||| 48487 ||| 30475 ||| 48488 ||| 
2021 ||| a new model of transformer operation state evaluation based on analytic hierarchy process and association rule mining. ||| 48489 ||| 48490 ||| 22231 ||| 
2021 ||| self-attention based sentiment analysis with effective embedding techniques. ||| 48491 ||| 48492 ||| 
2020 ||| the dilemma of user engagement in privacy notices: effects of interaction modes and habituation on user attention. ||| 48493 ||| 48494 ||| 48495 ||| 48496 ||| 3450 ||| 
2019 ||| multiparameter-based fuzzy logic health index assessment for oil-immersed power transformers. ||| 43206 ||| 48497 ||| 
2022 ||| rose: real one-stage effort to detect the fingerprint singular point based on multi-scale spatial attention. ||| 35638 ||| 35639 ||| 35640 ||| 35641 ||| 18613 ||| 33169 ||| 
2021 ||| image super-resolution network based on a multi-branch attention mechanism. ||| 7676 ||| 48498 ||| 29236 ||| 30969 ||| 
2021 ||| lightweight attention convolutional neural network through network slimming for robust facial expression recognition. ||| 41576 ||| 48499 ||| 48500 ||| 31730 ||| 
2022 ||| dadan: dual-path attention with distribution analysis network for text-image matching. ||| 11109 ||| 41745 ||| 47333 ||| 14048 ||| 
2021 ||| symmetric pyramid attention convolutional neural network for moving object detection. ||| 48501 ||| 48502 ||| 48503 ||| 43218 ||| 48504 ||| 
2021 ||| weighted least square design technique for hilbert transformer using fractional derivative. ||| 48505 ||| 28253 ||| 48506 ||| 48507 ||| 48508 ||| 
2022 ||| global attention-assisted representation learning for vehicle re-identification. ||| 48509 ||| 29033 ||| 23577 ||| 
2022 ||| multi-level attention network: application to brain tumor classification. ||| 30062 ||| 44408 ||| 
2021 ||| view transform graph attention recurrent networks for skeleton-based action recognition. ||| 34779 ||| 40634 ||| 45531 ||| 13249 ||| 
2020 ||| correction to: high-speed tracking based on multi-cf filters and attention mechanism. ||| 48510 ||| 3049 ||| 48511 ||| 48512 ||| 2529 ||| 48513 ||| 48514 ||| 220 ||| 
2021 ||| fully convolutional network with attention modules for semantic segmentation. ||| 29058 ||| 16648 ||| 
2021 ||| temporal attention learning for action quality assessment in sports video. ||| 48515 ||| 48516 ||| 48517 ||| 
2021 ||| pose estimation at night in infrared images using a lightweight multi-stage attention network. ||| 48518 ||| 48519 ||| 48520 ||| 43786 ||| 
2021 ||| cascade-guided multi-scale attention network for crowd counting. ||| 43964 ||| 43963 ||| 43962 ||| 43966 ||| 
2022 ||| attention-guided multi-path cross-cnn for underwater image super-resolution. ||| 2349 ||| 48521 ||| 48522 ||| 48523 ||| 48524 ||| 
2022 ||| stratified attention dense network for image super-resolution. ||| 1428 ||| 34166 ||| 31406 ||| 48525 ||| 48526 ||| 
2021 ||| high-speed tracking based on multi-cf filters and attention mechanism. ||| 48510 ||| 3049 ||| 48511 ||| 48512 ||| 2529 ||| 48513 ||| 48514 ||| 220 ||| 
2021 ||| application of an attention u-net incorporating transfer learning for optic disc and cup segmentation. ||| 48527 ||| 48528 ||| 764 ||| 48529 ||| 48530 ||| 48531 ||| 
2021 ||| compound-attention network with original feature injection for visual question and answering. ||| 18635 ||| 811 ||| 17585 ||| 13795 ||| 48532 ||| 47991 ||| 
2022 ||| derainattentiongan: unsupervised single-image deraining using attention-guided generative adversarial networks. ||| 48533 ||| 45457 ||| 48534 ||| 48535 ||| 
2021 ||| regression loss in transformer-based supervised neural machine translation. ||| 41043 ||| 48536 ||| 
2018 ||| text classification research with attention-based recurrent neural networks. ||| 48537 ||| 5278 ||| 
2021 ||| efficient building extraction for high spatial resolution images based on dual attention network. ||| 48538 ||| 48539 ||| 13692 ||| 749 ||| 
2020 ||| research in the attention economy. ||| 48540 ||| 48541 ||| 48542 ||| 
2021 ||| enhancing sustained attention. ||| 13992 ||| 48543 ||| 48544 ||| 26114 ||| 6787 ||| 48545 ||| 48546 ||| 26112 ||| 3882 ||| 26113 ||| 48547 ||| 48548 ||| 
2021 ||| data fusion analysis for attention-deficit hyperactivity disorder emotion recognition with thermal image and internet of things devices. ||| 48549 ||| 48550 ||| 48551 ||| 48552 ||| 41621 ||| 
2021 ||| cross-group or within-group attention flow? exploring the amplification process among elite users and social media publics in sina weibo. ||| 48553 ||| 48554 ||| 
2021 ||| sound design inducing attention in the context of audiovisual immersive environments. ||| 48555 ||| 48556 ||| 48557 ||| 48558 ||| 
2021 ||| serious games for basic learning mechanisms: reinforcing mexican children's gross motor skills and attention. ||| 48559 ||| 48560 ||| 48561 ||| 48562 ||| 42064 ||| 48563 ||| 22999 ||| 48564 ||| 41131 ||| 3419 ||| 28153 ||| 
2022 ||| cagat: centrality-adjusted graph attention network for active scientific talent discovery. ||| 48565 ||| 48566 ||| 4061 ||| 48567 ||| 
2017 ||| web page attentional priority model. ||| 17079 ||| 
2020 ||| effects of working memory, attention, and expertise on pilots' situation awareness. ||| 48568 ||| 48569 ||| 48570 ||| 
2017 ||| predictive aids can lead to sustained attention decrements in the detection of non-routine critical events in event monitoring. ||| 48571 ||| 48572 ||| 
2021 ||| exploring the crossing behaviours and visual attention allocation of children in primary school in an outdoor road environment. ||| 48573 ||| 5039 ||| 48574 ||| 48575 ||| 48576 ||| 48577 ||| 
2019 ||| a new abstraction framework for affine transformers. ||| 13190 ||| 13191 ||| 
2020 ||| extremely low-resource text simplification with pre-trained transformer language model. ||| 13591 ||| 13592 ||| 
2017 ||| collaborative game-based learning with motion-sensing technology: analyzing students' motivation, attention, and relaxation levels. ||| 48578 ||| 48579 ||| 48580 ||| 48581 ||| 6180 ||| 
2021 ||| a weakly supervised learning method based on attention fusion for covid-19 segmentation in ct images. ||| 48582 ||| 48583 ||| 
2022 ||| small object detection combining attention mechanism and a novel fpn. ||| 18844 ||| 48584 ||| 2932 ||| 48585 ||| 48586 ||| 
2022 ||| equivalent circuit modelling of a three-phase to seven-phase transformer using pso and ga. ||| 48587 ||| 28677 ||| 22152 ||| 48588 ||| 48589 ||| 48590 ||| 
2019 ||| siamese hierarchical attention networks for extractive summarization. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 29690 ||| 29688 ||| 29689 ||| 29687 ||| 16474 ||| 41689 ||| 
2021 ||| neighborhood aggregation based graph attention networks for open-world knowledge graph reasoning. ||| 986 ||| 48591 ||| 18529 ||| 
2022 ||| research on named entity recognition of chinese electronic medical records based on multi-head attention mechanism and character-word information fusion. ||| 48592 ||| 48593 ||| 32035 ||| 48594 ||| 42107 ||| 
2018 ||| transformer fault diagnosis method based on graph theory and rough set. ||| 19508 ||| 6281 ||| 48595 ||| 
2022 ||| camgan: combining attention mechanism generative adversarial networks for cartoon face style transfer. ||| 6514 ||| 29080 ||| 29079 ||| 
2022 ||| lexical attention and aspect-oriented graph convolutional networks for aspect-based sentiment analysis. ||| 48596 ||| 48597 ||| 48598 ||| 
2020 ||| multi-head attention model for aspect level sentiment analysis. ||| 48599 ||| 35690 ||| 
2021 ||| nighttime vehicle detection based on direction attention network and bayes corner localization. ||| 48600 ||| 348 ||| 48601 ||| 45117 ||| 
2021 ||| towards corpus and model: hierarchical structured-attention-based features for indonesian named entity recognition. ||| 48602 ||| 35182 ||| 48603 ||| 6938 ||| 
2022 ||| joint intent detection and slot filling with wheel-graph attention networks. ||| 34339 ||| 34340 ||| 34341 ||| 
2019 ||| power transformers internal fault diagnosis based on deep convolutional neural networks. ||| 29133 ||| 29132 ||| 29134 ||| 29135 ||| 
2021 ||| deep context interaction network based on attention mechanism for click-through rate prediction. ||| 48604 ||| 48605 ||| 48606 ||| 48607 ||| 48608 ||| 
2020 ||| a fuzzy based system for target search using top-down visual attention. ||| 48609 ||| 48610 ||| 48611 ||| 
2022 ||| optimization of apt attack detection based on a model combining attention and deep learning. ||| 48612 ||| 48613 ||| 
2021 ||| the enhanced deep plug-and-play super-resolution algorithm with residual channel attention networks. ||| 48614 ||| 48615 ||| 40372 ||| 48616 ||| 250 ||| 
2021 ||| fusion of part-of-speech vectors and attention mechanisms for cross-domain sentiment analysis. ||| 711 ||| 44842 ||| 48617 ||| 254 ||| 43209 ||| 
2021 ||| non-diacritized arabic speech recognition based on cnn-lstm and attention-based models. ||| 48618 ||| 48619 ||| 48620 ||| 48621 ||| 
2022 ||| aspect-level sentiment analysis for based on joint aspect and position hierarchy attention mechanism network. ||| 43208 ||| 48622 ||| 27951 ||| 44842 ||| 6805 ||| 2952 ||| 48623 ||| 
2021 ||| sentiment classification based on dependency-relationship embedding and attention mechanism. ||| 37478 ||| 48624 ||| 45270 ||| 242 ||| 
2020 ||| summarization for online reviews based on hierarchical attention network. ||| 48625 ||| 48626 ||| 48627 ||| 48628 ||| 
2022 ||| fmagan: fusing multiple attention and generative adversarial network to enhance underwater image. ||| 48629 ||| 29080 ||| 29079 ||| 30068 ||| 
2020 ||| attention-based lstm, gru and cnn for short text classification. ||| 48630 ||| 48631 ||| 48632 ||| 6553 ||| 48633 ||| 
2022 ||| end-to-end driving model based on deep learning and attention mechanism. ||| 48634 ||| 15696 ||| 48635 ||| 8217 ||| 48636 ||| 
2021 ||| normalization and deep learning based attention deficit hyperactivity disorder classification. ||| 27298 ||| 48637 ||| 
2021 ||| truncated attention mechanism and cascade loss for cross-modal person re-identification. ||| 48638 ||| 48639 ||| 29119 ||| 48640 ||| 21769 ||| 48641 ||| 
2019 ||| feature attention based detection model for medical text. ||| 48642 ||| 41172 ||| 44710 ||| 27130 ||| 
2021 ||| composite load modeling by spatial-temporal deep attention network based on wide-area monitoring systems. ||| 48643 ||| 48644 ||| 48645 ||| 
2020 ||| bidirectional indrnn malicious webpages detection algorithm based on convolutional neural network and attention mechanism. ||| 48646 ||| 29079 ||| 29080 ||| 48647 ||| 48648 ||| 48649 ||| 48650 ||| 
2019 ||| bi-direction hierarchical lstm with spatial-temporal attention for action recognition. ||| 6533 ||| 1796 ||| 13240 ||| 19856 ||| 
2018 ||| attention based english to punjabi neural machine translation. ||| 48651 ||| 48652 ||| 16471 ||| 
2018 ||| priority assessment model of on-line monitoring devices investment for power transformers. ||| 48653 ||| 48654 ||| 48655 ||| 48656 ||| 48657 ||| 48658 ||| 
2022 ||| hsrec: hierarchical self-attention incorporating knowledge graph for sequential recommendation. ||| 40574 ||| 825 ||| 
2020 ||| an intelligent approach for the evaluation of transformers in a power distribution project. ||| 48659 ||| 48660 ||| 
2021 ||| correlation analysis of law-related news combining bidirectional attention flow of news title and body. ||| 9472 ||| 2950 ||| 41263 ||| 28286 ||| 2951 ||| 
2022 ||| an object detection network based on yolov4 and improved spatial attention mechanism. ||| 48661 ||| 29079 ||| 29080 ||| 14271 ||| 3613 ||| 
2021 ||| interval fuzzy probability method for power transformer multiple fault diagnosis. ||| 48662 ||| 48663 ||| 37313 ||| 17729 ||| 
2022 ||| attention deeplabv3 model and its application into gear pitting measurement. ||| 28391 ||| 28392 ||| 9103 ||| 
2020 ||| guided attention mechanism: training network more efficiently. ||| 42278 ||| 42277 ||| 42276 ||| 48664 ||| 42279 ||| 
2019 ||| dual attention based fine-grained leukocyte recognition for imbalanced microscopic images. ||| 32942 ||| 48665 ||| 40569 ||| 36714 ||| 48666 ||| 48667 ||| 
2021 ||| long-distance contextual attention network for skin disease segmentation. ||| 30068 ||| 29079 ||| 29080 ||| 30066 ||| 48668 ||| 48629 ||| 
2020 ||| extractive summarization using siamese hierarchical transformer encoders. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 29690 ||| 29688 ||| 29689 ||| 29687 ||| 16474 ||| 41689 ||| 
2020 ||| self-attention for twitter sentiment analysis in spanish. ||| 852 ||| 16472 ||| 16473 ||| 8048 ||| 16474 ||| 41689 ||| 16476 ||| 
2021 ||| extracting relational facts based on hybrid syntax-guided transformer and pointer network. ||| 48669 ||| 1907 ||| 31481 ||| 31482 ||| 390 ||| 
2018 ||| eeg lecture on recommended activities for the induction of attention and concentration mental states on e-learning students. ||| 48670 ||| 48671 ||| 48672 ||| 48673 ||| 2253 ||| 48674 ||| 48675 ||| 852 ||| 48676 ||| 
2019 ||| android malware analysis and detection based on attention-cnn-lstm. ||| 48677 ||| 3232 ||| 1717 ||| 48678 ||| 48679 ||| 48680 ||| 
2019 ||| inattentional blindness factors that make people vulnerable to security threats in social networking sites. ||| 48681 ||| 
2021 ||| an attention-based spatiotemporal lstm network for next poi recommendation. ||| 48682 ||| 31130 ||| 14544 ||| 44898 ||| 
2019 ||| advancing acoustic-to-word ctc model with attention and mixed-units. ||| 12178 ||| 12179 ||| 14324 ||| 8164 ||| 12033 ||| 
2021 ||| detection of multiple steganography methods in compressed speech based on code element embedding, bi-lstm and cnn with attention mechanisms. ||| 40145 ||| 23439 ||| 10572 ||| 48683 ||| 40146 ||| 
2020 ||| improving self-attention networks with sequential relations. ||| 48684 ||| 4845 ||| 17791 ||| 18284 ||| 4847 ||| 
2021 ||| preordering encoding on transformer for translation. ||| 48685 ||| 3346 ||| 48686 ||| 
2020 ||| towards better word alignment in transformer. ||| 18286 ||| 48687 ||| 17793 ||| 84 ||| 3289 ||| 3471 ||| 3468 ||| 1254 ||| 
2019 ||| weakly labelled audioset tagging with attention neural networks. ||| 12618 ||| 13518 ||| 1125 ||| 33060 ||| 11418 ||| 12619 ||| 
2019 ||| attention with sparsity regularization for neural machine translation and summarization. ||| 3044 ||| 13249 ||| 1013 ||| 11690 ||| 
2018 ||| a hierarchy-to-sequence attentional neural machine translation model. ||| 3182 ||| 3043 ||| 3181 ||| 1305 ||| 3428 ||| 11745 ||| 
2020 ||| audio object classification using distributed beliefs and attention. ||| 12118 ||| 12119 ||| 
2020 ||| multi-instrument automatic music transcription with self-attention-based instance segmentation. ||| 40865 ||| 4497 ||| 11900 ||| 
2021 ||| deformable self-attention for text classification. ||| 3219 ||| 3221 ||| 3220 ||| 48688 ||| 3222 ||| 
2018 ||| cross-language neural dialog state tracker for large ontologies using hierarchical attention. ||| 48689 ||| 48690 ||| 48691 ||| 48692 ||| 
2020 ||| end-to-end post-filter for speech separation with deep attention fusion features. ||| 14675 ||| 12041 ||| 2304 ||| 12242 ||| 12244 ||| 14380 ||| 
2019 ||| global-local mutual attention model for text classification. ||| 3219 ||| 48688 ||| 32000 ||| 48693 ||| 32001 ||| 
2020 ||| monaural speech dereverberation using temporal convolutional networks with self attention. ||| 10646 ||| 14725 ||| 48694 ||| 6514 ||| 
2021 ||| audio-based piano performance evaluation for beginners with convolutional neural network and attention mechanism. ||| 36232 ||| 48695 ||| 48696 ||| 48697 ||| 765 ||| 
2021 ||| many-to-many voice transformer network. ||| 12538 ||| 14450 ||| 12537 ||| 12539 ||| 12540 ||| 12130 ||| 
2020 ||| online hybrid ctc/attention end-to-end automatic speech recognition architecture. ||| 12401 ||| 12101 ||| 12104 ||| 8298 ||| 
2021 ||| investigating typed syntactic dependencies for targeted sentiment classification using graph attention neural network. ||| 6273 ||| 33714 ||| 3289 ||| 
2020 ||| swings and roundabouts: attention-structure interaction effect in deep semantic matching. ||| 48698 ||| 20664 ||| 
2020 ||| fast query-by-example speech search using attention-based deep binary embeddings. ||| 48699 ||| 12384 ||| 14280 ||| 48700 ||| 12203 ||| 
2021 ||| contrastive information extraction with generative transformer. ||| 17987 ||| 17986 ||| 17988 ||| 12432 ||| 17989 ||| 21576 ||| 9686 ||| 17990 ||| 
2020 ||| sound event detection of weakly labelled data with cnn-transformer and automatic threshold optimization. ||| 12618 ||| 1125 ||| 11418 ||| 12619 ||| 
2021 ||| grtr: generative-retrieval transformers for data-efficient dialogue domain adaptation. ||| 12593 ||| 12594 ||| 12595 ||| 12596 ||| 
2021 ||| end-to-end recurrent cross-modality attention for video dialogue. ||| 33282 ||| 33283 ||| 33284 ||| 28053 ||| 
2021 ||| keyword search using attention-based end-to-end asr and frame-synchronous phoneme alignments. ||| 12102 ||| 12101 ||| 12401 ||| 14321 ||| 12104 ||| 8298 ||| 
2020 ||| attention-based response generation using parallel double q-learning for dialog policy decision in a conversational system. ||| 12237 ||| 4388 ||| 28075 ||| 
2021 ||| bridging text and video: a universal multimodal transformer for audio-visual scene-aware dialog. ||| 3073 ||| 34037 ||| 3442 ||| 3076 ||| 1921 ||| 
2021 ||| a joint model for named entity recognition with sentence-level entity type attentions. ||| 43796 ||| 11725 ||| 43795 ||| 48701 ||| 
2020 ||| knowledge guided capsule attention network for aspect-based sentiment analysis. ||| 7468 ||| 25166 ||| 143 ||| 7467 ||| 48702 ||| 7466 ||| 
2020 ||| adversarial learning for multi-task sequence labeling with attention mechanism. ||| 3906 ||| 920 ||| 48703 ||| 4829 ||| 5229 ||| 
2020 ||| cognitive-driven binaural beamforming using eeg-based auditory attention decoding. ||| 1490 ||| 1495 ||| 
2019 ||| adversarial regularization for attention based end-to-end robust speech recognition. ||| 12395 ||| 787 ||| 12384 ||| 48704 ||| 
2020 ||| a two-stage transformer-based approach for variable-length abstractive summarization. ||| 12237 ||| 4388 ||| 48705 ||| 
2020 ||| entity-sensitive attention and fusion network for entity-level multimodal sentiment classification. ||| 3827 ||| 800 ||| 3828 ||| 
2021 ||| deep selective memory network with selective attention and inter-aspect modeling for aspect level sentiment classification. ||| 18124 ||| 5395 ||| 6528 ||| 
2022 ||| automatic math word problem generation with topic-expression co-attention mechanism and reinforcement learning. ||| 48706 ||| 336 ||| 3273 ||| 
2019 ||| speech emotion classification using attention-based lstm. ||| 28654 ||| 28645 ||| 30881 ||| 48707 ||| 48708 ||| 648 ||| 649 ||| 
2021 ||| tera: self-supervised learning of transformer encoder representation for speech. ||| 12722 ||| 39834 ||| 12644 ||| 
2021 ||| hierarchical neighbor propagation with bidirectional graph attention network for relation prediction. ||| 11707 ||| 11708 ||| 3888 ||| 11710 ||| 9606 ||| 
2020 ||| audio replay spoof attack detection by joint segment-based linear filter bank feature extraction and attention-enhanced densenet-bilstm network. ||| 48709 ||| 17857 ||| 
2021 ||| medical term and status generation from chinese clinical dialogue with multi-granularity transformer. ||| 48710 ||| 48711 ||| 48712 ||| 13249 ||| 11689 ||| 11690 ||| 
2021 ||| dense cnn with self-attention for time-domain speech enhancement. ||| 36383 ||| 14725 ||| 
2021 ||| information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition. ||| 12403 ||| 1010 ||| 1008 ||| 13930 ||| 35435 ||| 12404 ||| 
2021 ||| nt context: softmax-free attention for online encoder-decoder speech recognition. ||| 34478 ||| 14603 ||| 34479 ||| 34480 ||| 10966 ||| 
2020 ||| how to teach dnns to pay attention to the visual modality in speech recognition. ||| 14640 ||| 14641 ||| 14642 ||| 
2021 ||| randomly wired network based on roberta and dialog history attention for response selection. ||| 48713 ||| 48714 ||| 48715 ||| 
2021 ||| attending from foresight: a novel attention mechanism for neural machine translation. ||| 3047 ||| 3048 ||| 3041 ||| 19879 ||| 4882 ||| 21829 ||| 
2021 ||| ctnet: conversational transformer network for emotion recognition. ||| 6227 ||| 2304 ||| 12041 ||| 
2019 ||| low-rank and locality constrained self-attention for sequence modeling. ||| 4967 ||| 3272 ||| 4970 ||| 1770 ||| 
2020 ||| investigating self-attention network for chinese word segmentation. ||| 37292 ||| 3289 ||| 
2019 ||| sparse self-attention lstm for sentiment lexicon construction. ||| 48716 ||| 33376 ||| 25999 ||| 37599 ||| 
2022 ||| s-vectors and tesa: speaker embeddings and a speaker authenticator based on transformer encoder. ||| 48717 ||| 12074 ||| 48718 ||| 
2021 ||| speech enhancement via attention masking network (seamnet): an end-to-end system for joint suppression of noise and reverberation. ||| 48719 ||| 5335 ||| 48720 ||| 
2022 ||| aggregating frame-level information in the spectral domain with self-attention for speaker embedding. ||| 48721 ||| 25738 ||| 
2021 ||| target speaker verification with selective auditory attention for single and multi-talker speech. ||| 12565 ||| 13946 ||| 37902 ||| 12494 ||| 
2019 ||| interdisciplinary scholarly communication: an exploratory study for the field of joint attention. ||| 922 ||| 23315 ||| 40470 ||| 48722 ||| 4997 ||| 48723 ||| 48724 ||| 
2021 ||| production profiles in brazilian science, with special attention to social sciences and humanities. ||| 48725 ||| 48726 ||| 
2018 ||| and now for something completely different: the congruence of the altmetric attention score's structure between different article groups. ||| 48727 ||| 48728 ||| 48729 ||| 
2018 ||| competition between academic journals for scholars' attention: the 'nature effect' in scholarly communication. ||| 48730 ||| 3419 ||| 48731 ||| 4251 ||| 4252 ||| 48732 ||| 
2020 ||| communities of attention networks: introducing qualitative and conversational perspectives for altmetrics. ||| 33940 ||| 
2020 ||| an altmetric attention advantage for open access books in the humanities and social sciences. ||| 48733 ||| 
2018 ||| the market of academic attention. ||| 48734 ||| 48735 ||| 
2018 ||| allegation of scientific misconduct increases twitter attention. ||| 18403 ||| 18401 ||| 
2018 ||| why do some research articles receive more online attention and higher altmetrics? reasons for online success according to the authors. ||| 18417 ||| 18418 ||| 
2020 ||| modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models. ||| 48736 ||| 48737 ||| 
2020 ||| does presence of social media plugins in a journal website result in higher social media attention of its research publications? ||| 32873 ||| 32874 ||| 32875 ||| 
2021 ||| news media attention in climate action: latent topics and open access. ||| 48738 ||| 48739 ||| 
2018 ||| understanding the formation of interdisciplinary research from the perspective of keyword evolution: a case study on joint attention. ||| 922 ||| 40470 ||| 23315 ||| 48740 ||| 28285 ||| 4997 ||| 917 ||| 
2020 ||| evaluating the relationship between the academic and social impact of open access books based on citation behaviors and social media attention. ||| 48741 ||| 48742 ||| 
2020 ||| the impact of preprints in library and information science: an analysis of citations, usage and social attention indicators. ||| 18413 ||| 18414 ||| 18415 ||| 18416 ||| 
2021 ||| do research articles with more readable abstracts receive higher online attention? evidence from science. ||| 48743 ||| 48744 ||| 48745 ||| 48746 ||| 48747 ||| 
2021 ||| design of a novel topology of transformer-less cuk inverter for isolated load applications. ||| 48748 ||| 48749 ||| 
2021 ||| optimized active power management in solar pv-fed transformerless grid-connected system for rural electrified microgrid. ||| 48750 ||| 48751 ||| 48752 ||| 
2021 ||| low-mismatch and low-loss transformer-based spdt switch with switching load technique for 5g applications. ||| 48753 ||| 48754 ||| 48755 ||| 48756 ||| 1894 ||| 
2018 ||| a voltage multiplier based high voltage gain transformerless buck-boost dc-dc converter. ||| 41481 ||| 41480 ||| 48757 ||| 
2020 ||| a low-loss and high-isolation transformer-based mm-wave spdt with integrated fan-out wafer level packaging. ||| 48753 ||| 48755 ||| 48756 ||| 48758 ||| 48754 ||| 
2020 ||| content-based attention network for person image generation. ||| 48759 ||| 48760 ||| 19066 ||| 42485 ||| 6030 ||| 
2019 ||| design of 85 ghz high power gain sige buffer with transformer matching network. ||| 48761 ||| 781 ||| 11391 ||| 
2017 ||| modified gorski-popiel technique and synthetic floating transformer circuit using minimum components. ||| 48338 ||| 48762 ||| 48763 ||| 48337 ||| 
2017 ||| spreading one's tweets: how can journalists gain attention for their tweeted news? ||| 43357 ||| 33545 ||| 43358 ||| 
2017 ||| why pay attention to paths in the practice of environmental modelling? ||| 48764 ||| 48765 ||| 48766 ||| 5335 ||| 2713 ||| 26269 ||| 
2021 ||| an attention u-net model for detection of fine-scale hydrologic streamlines. ||| 36809 ||| 36812 ||| 48767 ||| 12504 ||| 48768 ||| 48769 ||| 48770 ||| 48771 ||| 3036 ||| 48772 ||| 48773 ||| 
2021 ||| a serious-gamification blueprint towards a normalized attention. ||| 8287 ||| 
2022 ||| trg-datt: the target relational graph and double attention network based sentiment analysis and prediction for supporting decision making. ||| 48774 ||| 48775 ||| 10817 ||| 28684 ||| 5474 ||| 
2018 ||| price shock detection with an influence-based model of social attention. ||| 48776 ||| 1302 ||| 48777 ||| 9592 ||| 
2019 ||| the time-course of component processes of selective attention. ||| 48778 ||| 48779 ||| 48780 ||| 
2021 ||| how does the brain navigate knowledge of social relations? testing for shared neural mechanisms for shifting attention in space and social knowledge. ||| 48781 ||| 48782 ||| 48783 ||| 
2017 ||| interactions between the default network and dorsal attention network vary across default subsystems, time, and cognitive states. ||| 48784 ||| 48785 ||| 48786 ||| 48787 ||| 8097 ||| 48788 ||| 48789 ||| 
2021 ||| organization of directed functional connectivity among nodes of ventral attention network reveals the common network mechanisms underlying saliency processing across distinct spatial and spatio-temporal scales. ||| 48790 ||| 48791 ||| 48792 ||| 
2018 ||| decoding selective attention to context memory: an aging study. ||| 48793 ||| 48794 ||| 48795 ||| 48796 ||| 48797 ||| 
2018 ||| cortical summation and attentional modulation of combined chromatic and luminance signals. ||| 48798 ||| 7111 ||| 45853 ||| 
2017 ||| dissociated roles of the parietal and frontal cortices in the scope and control of attention during visual working memory. ||| 48799 ||| 48800 ||| 2058 ||| 5724 ||| 48801 ||| 48802 ||| 48803 ||| 
2020 ||| theta and alpha oscillations as signatures of internal and external attention to delayed intentions: a magnetoencephalography (meg) study. ||| 48804 ||| 48805 ||| 48806 ||| 48807 ||| 48808 ||| 48809 ||| 48810 ||| 
2021 ||| the "when" and "where" of the interplay between attentional capture and response inhibition during a go/nogo variant. ||| 48811 ||| 48812 ||| 48813 ||| 48814 ||| 48815 ||| 
2019 ||| attention modulates event-related spectral power in multisensory self-motion perception. ||| 48816 ||| 48817 ||| 48818 ||| 48819 ||| 48820 ||| 
2018 ||| connectome-based predictive modeling of attention: comparing different functional connectivity features and prediction methods across datasets. ||| 48821 ||| 45840 ||| 45841 ||| 13776 ||| 48822 ||| 45842 ||| 45843 ||| 45844 ||| 
2019 ||| dynamic functional connectivity during task performance and rest predicts individual differences in attention across studies. ||| 48823 ||| 48821 ||| 45840 ||| 13776 ||| 48822 ||| 45842 ||| 45843 ||| 45844 ||| 
2021 ||| dimension-selective attention and dimensional salience modulate cortical tracking of acoustic dimensions. ||| 48824 ||| 48825 ||| 48826 ||| 
2022 ||| selective attention involves a feature-specific sequential release from inhibitory gating. ||| 48827 ||| 48828 ||| 48829 ||| 
2018 ||| anticipating the good and the bad: a study on the neural correlates of bivalent emotion anticipation and their malleability via attentional deployment. ||| 48830 ||| 48831 ||| 48832 ||| 48833 ||| 48834 ||| 48835 ||| 48836 ||| 48837 ||| 48838 ||| 
2021 ||| towards understanding how we pay attention in naturalistic visual search settings. ||| 45898 ||| 45899 ||| 45900 ||| 45901 ||| 3900 ||| 
2021 ||| anatomical and functional coupling between the dorsal and ventral attention networks. ||| 48839 ||| 6630 ||| 2259 ||| 48840 ||| 48841 ||| 48842 ||| 48843 ||| 48844 ||| 
2017 ||| spatio-temporal dynamics of attentional selection stages during multiple object tracking. ||| 48845 ||| 45897 ||| 45896 ||| 
2019 ||| spatial attention underpins social word learning in the right fronto-parietal network. ||| 48846 ||| 48847 ||| 
2018 ||| changes in alpha activity reveal that social opinion modulates attention allocation during face processing. ||| 48848 ||| 48849 ||| 48850 ||| 48851 ||| 
2020 ||| too little, too late, and in the wrong place: alpha band activity does not reflect an active mechanism of selective attention. ||| 48852 ||| 48853 ||| 7111 ||| 45853 ||| 
2019 ||| decoding of selective attention to continuous speech from the human auditory brainstem response. ||| 45750 ||| 48854 ||| 48855 ||| 48856 ||| 45753 ||| 
2017 ||| attentional selection of multiple objects in the human visual system. ||| 48857 ||| 48858 ||| 48859 ||| 48860 ||| 
2019 ||| large-scale network interactions involved in dividing attention between the external environment and internal thoughts to pursue two distinct goals. ||| 48861 ||| 48862 ||| 48863 ||| 48864 ||| 
2018 ||| spatial attention modulates visual gamma oscillations across the human ventral stream. ||| 48865 ||| 48866 ||| 
2021 ||| lateralized alpha activity and slow potential shifts over visual cortex track the time course of both endogenous and exogenous orienting of attention. ||| 48867 ||| 45868 ||| 45869 ||| 
2019 ||| rapid sensory gain with emotional distracters precedes attentional deployment from a foreground task. ||| 48868 ||| 48869 ||| 45758 ||| 45759 ||| 3831 ||| 
2018 ||| adaptive flexibility of the within-hand attentional gradient in touch: an meg study. ||| 48870 ||| 48871 ||| 48872 ||| 
2021 ||| callosal anisotropy predicts attentional network changes after parietal inhibitory stimulation. ||| 48873 ||| 48874 ||| 48875 ||| 48876 ||| 48877 ||| 48878 ||| 48879 ||| 
2022 ||| divided attention at retrieval does not influence neural correlates of recollection in young or older adults. ||| 48880 ||| 48881 ||| 48882 ||| 48883 ||| 
2018 ||| neural mechanisms of divided feature-selective attention to colour. ||| 48798 ||| 48884 ||| 45785 ||| 45759 ||| 3831 ||| 7111 ||| 45853 ||| 
2020 ||| effects of auditory selective attention on neural phase: individual differences and short-term training. ||| 48885 ||| 48825 ||| 48886 ||| 
2019 ||| impoverished auditory cues limit engagement of brain networks controlling spatial selective attention. ||| 48887 ||| 48888 ||| 12775 ||| 23275 ||| 
2021 ||| can expectation suppression be explained by reduced attention to predictable stimuli? ||| 48889 ||| 48890 ||| 
2020 ||| psilocybin acutely alters the functional connectivity of the claustrum with brain networks that support perception, memory, and attention. ||| 48891 ||| 48892 ||| 48893 ||| 48894 ||| 48895 ||| 
2020 ||| evidence accumulation during perceptual decision-making is sensitive to the dynamics of attentional selection. ||| 48896 ||| 45988 ||| 
2019 ||| focus of attention modulates the heartbeat evoked potential. ||| 48897 ||| 48898 ||| 48899 ||| 48900 ||| 48901 ||| 48902 ||| 
2017 ||| deciding where to attend: large-scale network mechanisms underlying attention and intention revealed by graph-theoretic analysis. ||| 45971 ||| 7082 ||| 48903 ||| 48904 ||| 45975 ||| 45974 ||| 
2018 ||| tracking behavioral and neural fluctuations during sustained attention: a robust replication and extension. ||| 48905 ||| 45959 ||| 48906 ||| 12972 ||| 12974 ||| 
2017 ||| rapid top-down control over template-guided attention shifts to multiple objects. ||| 46032 ||| 48907 ||| 43033 ||| 45742 ||| 
2021 ||| orienting auditory attention in time: lateralized alpha power reflects spatio-temporal filtering. ||| 46037 ||| 46038 ||| 48908 ||| 45911 ||| 
2021 ||| the pulse: transient fmri signal increases in subcortical arousal systems during transitions in attention. ||| 5204 ||| 48909 ||| 48910 ||| 48911 ||| 48912 ||| 48913 ||| 16833 ||| 44147 ||| 48914 ||| 
2018 ||| the involvement of alpha oscillations in voluntary attention directed towards encoding episodic memories. ||| 48915 ||| 48916 ||| 48917 ||| 
2022 ||| reward-driven modulation of spatial attention in the human frontal eye-field. ||| 48918 ||| 48919 ||| 48920 ||| 48921 ||| 
2021 ||| brain state-based detection of attentional fluctuations and their modulation. ||| 48922 ||| 45959 ||| 48863 ||| 48923 ||| 12974 ||| 
2017 ||| loss of lateral prefrontal cortex control in food-directed attention and goal-directed food choice in obesity. ||| 48924 ||| 48925 ||| 48926 ||| 48927 ||| 48928 ||| 48929 ||| 48930 ||| 
2019 ||| selective effects of acute low-grade inflammation on human visual attention. ||| 48931 ||| 48932 ||| 48933 ||| 48934 ||| 48935 ||| 48936 ||| 48937 ||| 48849 ||| 
2020 ||| cholinergic white matter pathways make a stronger contribution to attention and memory in normal aging than cerebrovascular health and nucleus basalis of meynert. ||| 48938 ||| 48939 ||| 48940 ||| 48941 ||| 48942 ||| 48943 ||| 48944 ||| 43126 ||| 48945 ||| 48946 ||| 852 ||| 48947 ||| 48948 ||| 48949 ||| 48950 ||| 
2018 ||| how acute stress may enhance subsequent memory for threat stimuli outside the focus of attention: dlpfc-amygdala decoupling. ||| 9915 ||| 48951 ||| 1867 ||| 10314 ||| 48952 ||| 48953 ||| 9472 ||| 42277 ||| 48954 ||| 
2018 ||| a functional connectivity-based neuromarker of sustained attention generalizes to predict recall in a reading task. ||| 48955 ||| 48956 ||| 48957 ||| 48958 ||| 45840 ||| 48959 ||| 48960 ||| 
2018 ||| cortical tracking of multiple streams outside the focus of attention in naturalistic auditory scenes. ||| 48961 ||| 48962 ||| 48963 ||| 48964 ||| 
2019 ||| desynchronizing to be faster? perceptual- and attentional-modulation of brain rhythms at the sub-millisecond scale. ||| 48965 ||| 42954 ||| 48872 ||| 
2017 ||| functional modular architecture underlying attentional control in aging. ||| 48966 ||| 48967 ||| 48968 ||| 48969 ||| 48970 ||| 48971 ||| 
2021 ||| auditory steady-state responses during and after a stimulus: cortical sources, and the influence of attention and musicality. ||| 48972 ||| 10195 ||| 48973 ||| 7111 ||| 48974 ||| 3882 ||| 48975 ||| 
2017 ||| network-targeted cerebellar transcranial magnetic stimulation improves attentional control. ||| 12974 ||| 48976 ||| 48977 ||| 12972 ||| 45980 ||| 48978 ||| 48979 ||| 
2019 ||| the ebb and flow of attention: between-subject variation in intrinsic connectivity and cognition associated with the dynamics of ongoing experience. ||| 48980 ||| 48981 ||| 48982 ||| 48983 ||| 48984 ||| 48985 ||| 
2021 ||| spectral signature of attentional reorienting in the human brain. ||| 48986 ||| 48987 ||| 48988 ||| 48989 ||| 48990 ||| 48991 ||| 
2017 ||| audio-visual synchrony and spatial attention enhance processing of dynamic visual stimulation independently and in parallel: a frequency-tagging study. ||| 48992 ||| 48993 ||| 48994 ||| 48995 ||| 6787 ||| 45759 ||| 3831 ||| 
2020 ||| spatiotemporal dynamics of attentional orienting and reorienting revealed by fast optical imaging in occipital and parietal cortices. ||| 48996 ||| 48997 ||| 48998 ||| 48999 ||| 49000 ||| 49001 ||| 49002 ||| 
2017 ||| a temporal dependency account of attentional inhibition in oculomotor control. ||| 49003 ||| 49004 ||| 49005 ||| 
2018 ||| anticipatory neural dynamics of spatial-temporal orienting of attention in younger and older adults. ||| 49006 ||| 49007 ||| 49008 ||| 49009 ||| 45864 ||| 45866 ||| 
2018 ||| quenching of spontaneous fluctuations by attention in human visual cortex. ||| 49010 ||| 49011 ||| 49012 ||| 49013 ||| 
2021 ||| the neural substrates of subliminal attentional bias and reduced inhibition in individuals with a higher bmi: a vbm and resting state connectivity study. ||| 49014 ||| 49015 ||| 49016 ||| 49017 ||| 49018 ||| 
2019 ||| alpha and alpha-beta phase synchronization mediate the recruitment of the visuospatial attention network through the superior longitudinal fasciculus. ||| 49019 ||| 49020 ||| 49021 ||| 48989 ||| 49022 ||| 49023 ||| 49024 ||| 
2018 ||| real-time decoding of covert attention in higher-order visual areas. ||| 49025 ||| 49026 ||| 49027 ||| 49028 ||| 49029 ||| 14886 ||| 
2019 ||| connectome-based models predict attentional control in aging adults. ||| 49030 ||| 49031 ||| 45840 ||| 49032 ||| 
2020 ||| connectome-based neurofeedback: a pilot study to improve sustained attention. ||| 45842 ||| 49033 ||| 49034 ||| 49035 ||| 45843 ||| 45844 ||| 45840 ||| 
2022 ||| distinct neural-behavioral correspondence within face processing and attention networks for the composite face effect. ||| 49036 ||| 49037 ||| 42277 ||| 49038 ||| 49039 ||| 49040 ||| 49041 ||| 
2017 ||| attentional capture in visual search: capture and post-capture dynamics revealed by eeg. ||| 49042 ||| 49043 ||| 49044 ||| 17425 ||| 45826 ||| 3831 ||| 
2021 ||| children with attention-deficit/hyperactivity disorder spend more time in hyperconnected network states and less time in segregated network states as revealed by dynamic connectivity analysis. ||| 49045 ||| 49046 ||| 49047 ||| 49048 ||| 49049 ||| 49050 ||| 49051 ||| 
2019 ||| decoding attentional states for neurofeedback: mindfulness vs. wandering thoughts. ||| 49052 ||| 49053 ||| 49054 ||| 49055 ||| 49056 ||| 49057 ||| 
2017 ||| gamma-band activity reflects attentional guidance by facial expression. ||| 49058 ||| 49059 ||| 49060 ||| 49061 ||| 49062 ||| 
2017 ||| linking dopaminergic reward signals to the development of attentional bias: a positron emission tomographic study. ||| 45958 ||| 49063 ||| 49064 ||| 49065 ||| 49066 ||| 49067 ||| 49068 ||| 
2021 ||| neural correlates of visual attention during risky decision evidence integration. ||| 49069 ||| 49070 ||| 49071 ||| 49072 ||| 
2020 ||| combining fmri during resting state and an attention bias task in children. ||| 49073 ||| 49074 ||| 49075 ||| 49076 ||| 49077 ||| 49078 ||| 49079 ||| 49080 ||| 
2017 ||| metacognition of attention during tactile discrimination. ||| 49081 ||| 49082 ||| 49083 ||| 48975 ||| 
2020 ||| a real-time marker of object-based attention in the human brain. a possible component of a "gate-keeping mechanism" performing late attentional selection in the ventro-lateral prefrontal cortex. ||| 49084 ||| 49085 ||| 49086 ||| 49087 ||| 49088 ||| 49089 ||| 49090 ||| 49091 ||| 49092 ||| 49093 ||| 49094 ||| 49095 ||| 49096 ||| 
2018 ||| neural circuitry underlying sustained attention in healthy adolescents and in adhd symptomatology. ||| 49097 ||| 49098 ||| 49099 ||| 49100 ||| 49101 ||| 49102 ||| 49103 ||| 49104 ||| 49105 ||| 49106 ||| 49107 ||| 49108 ||| 18510 ||| 49109 ||| 49110 ||| 49111 ||| 17449 ||| 49112 ||| 49113 ||| 49114 ||| 49115 ||| 
2019 ||| self-directed down-regulation of auditory cortex activity mediated by real-time fmri neurofeedback augments attentional processes, resting cerebral perfusion, and auditory activation. ||| 49116 ||| 49117 ||| 49118 ||| 49119 ||| 49120 ||| 49121 ||| 49122 ||| 
2021 ||| common functional brain networks between attention deficit and disruptive behaviors in youth. ||| 49123 ||| 14048 ||| 49124 ||| 31239 ||| 31240 ||| 
2018 ||| distinct phase-amplitude couplings distinguish cognitive processes in human attention. ||| 49125 ||| 49126 ||| 49127 ||| 49128 ||| 49129 ||| 49130 ||| 48990 ||| 49131 ||| 49132 ||| 
2017 ||| attentional processes, not implicit mentalizing, mediate performance in a perspective-taking task: evidence from stimulation of the temporoparietal junction. ||| 49133 ||| 49134 ||| 49135 ||| 49136 ||| 
2017 ||| sensory-biased attention networks in human lateral frontal cortex revealed by intrinsic functional connectivity. ||| 49137 ||| 49138 ||| 49139 ||| 49140 ||| 
2018 ||| influence of talker discontinuity on cortical dynamics of auditory spatial attention. ||| 49141 ||| 12775 ||| 49142 ||| 
2020 ||| nested oscillations and brain connectivity during sequential stages of feature-based attention. ||| 48827 ||| 48828 ||| 48829 ||| 
2019 ||| selective spatial attention involves two alpha-band components associated with distinct spatiotemporal and functional characteristics. ||| 49143 ||| 11318 ||| 48189 ||| 
2017 ||| temporal orienting precedes intersensory attention and has opposing effects on early evoked brain activity. ||| 29405 ||| 49144 ||| 49145 ||| 49146 ||| 
2019 ||| auditory cortical generators of the frequency following response are modulated by intermodal attention. ||| 49147 ||| 49148 ||| 
2017 ||| measuring the effects of attention to individual fingertips in somatosensory cortex using ultra-high field (7t) fmri. ||| 49149 ||| 49150 ||| 49151 ||| 49152 ||| 
2019 ||| eye movements explain decodability during perception and cued attention in meg. ||| 49153 ||| 49154 ||| 49155 ||| 49156 ||| 49157 ||| 
2021 ||| fronto-temporal brain activity and connectivity track implicit attention to positive and negative social words in a novel socio-emotional stroop task. ||| 49158 ||| 49159 ||| 49160 ||| 49161 ||| 
2020 ||| gradients of functional organization in posterior parietal cortex revealed by visual attention, visual short-term memory, and intrinsic functional connectivity. ||| 49162 ||| 49163 ||| 24262 ||| 49137 ||| 49140 ||| 
2020 ||| multi-spectral oscillatory dynamics serving directed and divided attention. ||| 49164 ||| 49165 ||| 49166 ||| 49167 ||| 49168 ||| 
2022 ||| attention enhances category representations across the brain with strengthened residual correlations to ventral temporal cortex. ||| 49169 ||| 49170 ||| 49171 ||| 49172 ||| 49173 ||| 
2019 ||| rapid adaptive adjustments of selective attention following errors revealed by the time course of steady-state visual evoked potentials. ||| 49174 ||| 7111 ||| 45853 ||| 
2020 ||| fetal brain age estimation and anomaly detection using attention-based deep ensembles with uncertainty. ||| 49175 ||| 49176 ||| 49177 ||| 49178 ||| 49179 ||| 31194 ||| 44125 ||| 340 ||| 49180 ||| 216 ||| 
2019 ||| theory of visual attention thalamic model for visual short-term memory capacity and top-down control: evidence from a thalamo-cortical structural connectivity analysis. ||| 49181 ||| 49182 ||| 49183 ||| 49184 ||| 49185 ||| 49186 ||| 45826 ||| 3831 ||| 49187 ||| 49188 ||| 
2021 ||| frontal cortical regions associated with attention connect more strongly to central than peripheral v1. ||| 49189 ||| 49190 ||| 49191 ||| 49192 ||| 
2019 ||| bidirectional signal exchanges and their mechanisms during joint attention interaction - a hyperscanning fmri study. ||| 49193 ||| 49194 ||| 49195 ||| 49196 ||| 49197 ||| 49198 ||| 49199 ||| 
2020 ||| attentional modulation of the auditory steady-state response across the cortex. ||| 48972 ||| 49200 ||| 10195 ||| 48973 ||| 7111 ||| 48974 ||| 3882 ||| 48975 ||| 
2020 ||| spatial attention enhances cortical tracking of quasi-rhythmic visual stimuli. ||| 49201 ||| 48993 ||| 49202 ||| 46022 ||| 
2018 ||| cortical depth dependent population receptive field attraction by spatial attention in human v1. ||| 49203 ||| 49204 ||| 49205 ||| 49206 ||| 49207 ||| 49208 ||| 
2021 ||| multivariate analysis of eeg activity indexes contingent attentional capture. ||| 49209 ||| 49210 ||| 45876 ||| 46013 ||| 45780 ||| 
2021 ||| breaking down the cocktail party: attentional modulation of cerebral audiovisual speech processing. ||| 49211 ||| 49212 ||| 49213 ||| 49214 ||| 49215 ||| 49216 ||| 49217 ||| 
2017 ||| expectation violation and attention to pain jointly modulate neural gain in somatosensory cortex. ||| 49218 ||| 49219 ||| 49220 ||| 49221 ||| 49222 ||| 49223 ||| 
2020 ||| topographic specificity of alpha power during auditory spatial attention. ||| 48887 ||| 48888 ||| 12775 ||| 
2021 ||| attention reinforces human corticofugal system to aid speech perception in noise. ||| 49224 ||| 49225 ||| 
2021 ||| parallel cortical-brainstem pathways to attentional analgesia. ||| 49226 ||| 49227 ||| 49228 ||| 9531 ||| 49229 ||| 49230 ||| 49231 ||| 
2021 ||| decreased emotional reactivity after 3-month socio-affective but not attention- or meta-cognitive-based mental training: a randomized, controlled, longitudinal fmri study. ||| 49232 ||| 49233 ||| 49234 ||| 49235 ||| 
2017 ||| spatiotemporal oscillatory dynamics of visual selective attention during a flanker task. ||| 49236 ||| 49165 ||| 49237 ||| 49238 ||| 49168 ||| 
2021 ||| attentional modulation of neural entrainment to sound streams in children with and without adhd. ||| 48885 ||| 48825 ||| 12848 ||| 48886 ||| 
2019 ||| a division of labor between power and phase coherence in encoding attention to stimulus streams. ||| 49239 ||| 49240 ||| 
2019 ||| lateral prefrontal anodal transcranial direct current stimulation augments resolution of auditory perceptual-attentional conflicts. ||| 49241 ||| 10712 ||| 49242 ||| 49243 ||| 45720 ||| 49244 ||| 
2019 ||| age-related changes in attention control and their relationship with gait performance in older adults with high risk of falls. ||| 49245 ||| 5335 ||| 49246 ||| 49247 ||| 48921 ||| 
2018 ||| frequency-specific attentional modulation in human primary auditory cortex and midbrain. ||| 48962 ||| 49248 ||| 48963 ||| 49249 ||| 49250 ||| 48964 ||| 49251 ||| 
2019 ||| prism adaptation enhances decoupling between the default mode network and the attentional networks. ||| 49252 ||| 49253 ||| 49254 ||| 49255 ||| 
2021 ||| multi-band meg signatures of bold connectivity reorganization during visuospatial attention. ||| 49256 ||| 48986 ||| 48988 ||| 48987 ||| 49257 ||| 48991 ||| 48990 ||| 
2018 ||| lateral prefrontal cortex lesion impairs regulation of internally and externally directed attention. ||| 49258 ||| 49259 ||| 49260 ||| 49261 ||| 49262 ||| 
2019 ||| attention differentially modulates the amplitude of resonance frequencies in the visual cortex. ||| 49263 ||| 49264 ||| 4130 ||| 
2021 ||| dika-nets: domain-invariant knowledge-guided attention networks for brain skull stripping of early developing macaques. ||| 27586 ||| 27587 ||| 27588 ||| 27589 ||| 17773 ||| 27747 ||| 49265 ||| 1052 ||| 18051 ||| 9472 ||| 858 ||| 
2021 ||| integration and segregation across large-scale intrinsic brain networks as a marker of sustained attention and task-unrelated thought. ||| 49266 ||| 48863 ||| 48922 ||| 49267 ||| 49268 ||| 48923 ||| 12974 ||| 
2020 ||| processing speed and attention training modifies autonomic flexibility: a mechanistic intervention study. ||| 49269 ||| 49270 ||| 49271 ||| 49272 ||| 49273 ||| 49274 ||| 49275 ||| 
2020 ||| overlapping attentional networks yield divergent behavioral predictions across tasks: neuromarkers for diffuse and focused attention? ||| 49276 ||| 49277 ||| 49278 ||| 49279 ||| 49280 ||| 49281 ||| 45840 ||| 49282 ||| 49283 ||| 
2017 ||| distinct roles of theta and alpha oscillations in the involuntary capture of goal-directed attention. ||| 49284 ||| 49285 ||| 49286 ||| 45988 ||| 
2020 ||| network-based fmri-neurofeedback training of sustained attention. ||| 49287 ||| 49288 ||| 49289 ||| 49290 ||| 49291 ||| 49016 ||| 49028 ||| 49292 ||| 
2020 ||| global effects of feature-based attention depend on surprise. ||| 45987 ||| 49293 ||| 45988 ||| 
2019 ||| using machine learning-based lesion behavior mapping to identify anatomical networks of cognitive dysfunction: spatial neglect and attention. ||| 49294 ||| 49295 ||| 49296 ||| 45765 ||| 45766 ||| 
2018 ||| high-alpha band synchronization across frontal, parietal and visual cortex mediates behavioral and neuronal effects of visuospatial attention. ||| 49297 ||| 49298 ||| 49299 ||| 
2020 ||| attention modulates the gating of primary somatosensory oscillations. ||| 49165 ||| 49168 ||| 
2020 ||| relating alpha power modulations to competing visuospatial attention theories. ||| 49300 ||| 45996 ||| 49301 ||| 45997 ||| 49302 ||| 46000 ||| 
2019 ||| homozygous lamc3 mutation links to structural and functional changes in visual attention networks. ||| 49303 ||| 49304 ||| 49305 ||| 49190 ||| 49306 ||| 49307 ||| 49308 ||| 49309 ||| 49310 ||| 49311 ||| 
2020 ||| parasympathetic arousal-related cortical activity is associated with attention during cognitive task performance. ||| 49312 ||| 49313 ||| 49314 ||| 49315 ||| 49316 ||| 49317 ||| 
2020 ||| isometric exercise facilitates attention to salient events in women via the noradrenergic system. ||| 49318 ||| 49319 ||| 49320 ||| 49321 ||| 49322 ||| 49323 ||| 49324 ||| 49325 ||| 
2017 ||| attention reorganizes connectivity across networks in a frequency specific manner. ||| 49326 ||| 49327 ||| 49328 ||| 49329 ||| 
2019 ||| diminished pre-stimulus alpha-lateralization suggests compromised self-initiated attentional control of auditory processing in old age. ||| 49330 ||| 49331 ||| 49244 ||| 49243 ||| 49332 ||| 
2017 ||| age-related reduction of hemispheric lateralisation for spatial attention: an eeg study. ||| 49333 ||| 49334 ||| 49335 ||| 49336 ||| 
2017 ||| internal and external attention and the default mode network. ||| 49337 ||| 49338 ||| 49339 ||| 49340 ||| 49341 ||| 
2017 ||| involuntary orienting of attention to a sound desynchronizes the occipital alpha rhythm and improves visual perception. ||| 15220 ||| 45868 ||| 45869 ||| 2871 ||| 49342 ||| 4046 ||| 45740 ||| 45785 ||| 
2019 ||| default mode and visual network activity in an attention task: direct measurement with intracranial eeg. ||| 49343 ||| 49344 ||| 49345 ||| 49346 ||| 48909 ||| 49347 ||| 4143 ||| 49348 ||| 49349 ||| 48914 ||| 
2021 ||| learning with self-attention for rental market spatial dynamics in the atlanta metropolitan area. ||| 49350 ||| 49351 ||| 
2021 ||| a stream prediction model based on attention-lstm. ||| 47126 ||| 49352 ||| 47127 ||| 49353 ||| 
2021 ||| integrated image defogging network based on improved atmospheric scattering model and attention feature fusion. ||| 49354 ||| 49355 ||| 49356 ||| 49357 ||| 
2021 ||| latte: lstm self-attention based anomaly detection in e mbedded automotive platforms. ||| 7490 ||| 7489 ||| 7491 ||| 
2021 ||| algorithm-hardware co-design of attention mechanism on fpga devices. ||| 29522 ||| 49358 ||| 49359 ||| 49360 ||| 49361 ||| 
2020 ||| meta-supervision for attention using counterfactual estimation. ||| 18541 ||| 18542 ||| 3716 ||| 
2019 ||| semantic based autoencoder-attention 3d reconstruction network. ||| 11960 ||| 11963 ||| 11962 ||| 30664 ||| 48323 ||| 4385 ||| 
2022 ||| edvam: a 3d eye-tracking dataset for visual attention modeling in a virtual museum. ||| 14781 ||| 14782 ||| 14783 ||| 14784 ||| 14785 ||| 27738 ||| 
2017 ||| attention-based encoder-decoder model for answer selection in question answering. ||| 5064 ||| 49362 ||| 5066 ||| 49363 ||| 10833 ||| 
2021 ||| video summarization with a graph convolutional attention network. ||| 977 ||| 31931 ||| 32944 ||| 
2019 ||| attention shifting during child - robot interaction: a preliminary clinical study for children with autism spectrum disorder. ||| 49364 ||| 49365 ||| 49366 ||| 49367 ||| 49368 ||| 19220 ||| 49369 ||| 49370 ||| 49371 ||| 49372 ||| 49373 ||| 49374 ||| 49375 ||| 
2020 ||| analysis and design of transformer-based cmos ultra-wideband millimeter-wave circuits for wireless applications: a review. ||| 41411 ||| 41418 ||| 
2022 ||| attention-based graph resnet with focal loss for epileptic seizure detection. ||| 45580 ||| 45578 ||| 45579 ||| 49376 ||| 49377 ||| 49378 ||| 49379 ||| 
2021 ||| applying attention-based models for detecting cognitive processes and mental health conditions. ||| 49380 ||| 49381 ||| 49382 ||| 49383 ||| 49384 ||| 7440 ||| 
2019 ||| improving user attribute classification with text and social network attention. ||| 49385 ||| 8967 ||| 728 ||| 8349 ||| 8974 ||| 
2019 ||| unsupervised object transfiguration with attention. ||| 10068 ||| 10069 ||| 33970 ||| 673 ||| 10071 ||| 10072 ||| 
2019 ||| multi-region risk-sensitive cognitive ensembler for accurate detection of attention-deficit/hyperactivity disorder. ||| 49386 ||| 561 ||| 24290 ||| 49387 ||| 49388 ||| 49389 ||| 
2022 ||| sentic computing for aspect-based opinion summarization using multi-head attention with feature pooled pointer generator network. ||| 958 ||| 49390 ||| 38623 ||| 49391 ||| 
2022 ||| design and deployment of an image polarity detector with visual attention. ||| 192 ||| 193 ||| 194 ||| 195 ||| 196 ||| 
2018 ||| attentional bias pattern recognition in spiking neural networks from spatio-temporal eeg data. ||| 49392 ||| 49393 ||| 28236 ||| 
2021 ||| modeling articulatory rehearsal in an attention-based model of working memory. ||| 3766 ||| 49394 ||| 49395 ||| 49396 ||| 
2021 ||| real-time lane detection by using biologically inspired attention mechanism to learn contextual information. ||| 2037 ||| 49397 ||| 15553 ||| 6271 ||| 13181 ||| 
2021 ||| hana: hierarchical attention network assembling for semantic segmentation. ||| 683 ||| 9741 ||| 49398 ||| 
2022 ||| cat-bigru: convolution and attention with bi-directional gated recurrent unit for self-deprecating sarcasm detection. ||| 49399 ||| 8478 ||| 
2022 ||| to ban or not to ban: bayesian attention networks for reliable hate speech detection. ||| 49400 ||| 10213 ||| 49401 ||| 24948 ||| 
2021 ||| attention-augmented machine memory. ||| 10061 ||| 2817 ||| 10062 ||| 10063 ||| 4776 ||| 
2021 ||| a convolutional stacked bidirectional lstm with a multiplicative attention mechanism for aspect category and sentiment detection. ||| 27243 ||| 27244 ||| 893 ||| 
2021 ||| electroencephalography-based auditory attention decoding: toward neurosteered hearing devices. ||| 8253 ||| 49402 ||| 49403 ||| 49404 ||| 49405 ||| 49406 ||| 24293 ||| 8254 ||| 8255 ||| 
2021 ||| word and graph attention networks for semi-supervised classification. ||| 875 ||| 49407 ||| 49408 ||| 22968 ||| 49409 ||| 
2021 ||| proaid: path-based reasoning for self-attentional disease prediction. ||| 10331 ||| 9695 ||| 49410 ||| 49411 ||| 
2022 ||| e2eet: from pipeline to end-to-end entity typing via transformer-based embeddings. ||| 34737 ||| 683 ||| 
2020 ||| exploiting review embedding and user attention for item recommendation. ||| 49412 ||| 11273 ||| 9645 ||| 49413 ||| 49414 ||| 
2021 ||| a relative position attention network for aspect-based sentiment analysis. ||| 25100 ||| 44775 ||| 4213 ||| 44777 ||| 882 ||| 44778 ||| 
2021 ||| convolutional recurrent neural network with attention for vietnamese speech to text problem in the operating room. ||| 49415 ||| 49416 ||| 49417 ||| 49418 ||| 49419 ||| 
2017 ||| adequacy assessment of power distribution network with large fleets of phevs considering condition-dependent transformer faults. ||| 49420 ||| 4259 ||| 
2019 ||| implementation of bidirectional resonant dc transformer in hybrid ac/dc micro-grid. ||| 43442 ||| 43507 ||| 49421 ||| 5845 ||| 49422 ||| 
2017 ||| a novel wireless multifunctional electronic current transformer based on zigbee-based communication. ||| 23236 ||| 49423 ||| 49424 ||| 49425 ||| 
2021 ||| wide-band current transformers for traveling-waves-based protection applications. ||| 28305 ||| 49426 ||| 28308 ||| 28307 ||| 28422 ||| 
2018 ||| parallel operation of transformers with on load tap changer and photovoltaic systems with reactive power control. ||| 49427 ||| 49428 ||| 49429 ||| 
2017 ||| optimized reactive power supports using transformer tap stagger in distribution networks. ||| 16206 ||| 16207 ||| 
2018 ||| a smart iec 61850 merging unit for impending fault detection in transformers. ||| 49430 ||| 49431 ||| 15900 ||| 49432 ||| 49433 ||| 25793 ||| 49434 ||| 
2018 ||| multiple resonances mitigation of paralleled inverters in a solid-state transformer (sst) enabled ac microgrid. ||| 43603 ||| 43604 ||| 4175 ||| 
2019 ||| technical and economic impact of pv-bess charging station on transformer life: a case study. ||| 16211 ||| 16212 ||| 
2018 ||| a novel association rule mining method of big data for power transformers state parameters based on probabilistic graph model. ||| 10889 ||| 49435 ||| 10892 ||| 18159 ||| 
2019 ||| applicability of solid-state transformers in today's and future distribution grids. ||| 49436 ||| 49437 ||| 
2021 ||| optimal dispatch with transformer dynamic thermal rating in adns incorporating high pv penetration. ||| 49438 ||| 9149 ||| 49439 ||| 
2018 ||| tracking transformer tap position in real-time distribution network power flow applications. ||| 49440 ||| 49441 ||| 49442 ||| 
2021 ||| online detection of inter-turn winding faults in single-phase distribution transformers using smart meter data. ||| 22305 ||| 22306 ||| 22307 ||| 21281 ||| 
2018 ||| a reconstruction of the wams-detected transformer sympathetic inrush phenomenon. ||| 49443 ||| 49444 ||| 
2020 ||| detection of hidden transformer tap change command attacks in transmission networks. ||| 16259 ||| 16260 ||| 
2020 ||| distribution network marginal costs: enhanced ac opf including transformer degradation. ||| 49445 ||| 49446 ||| 
2019 ||| real-time primary frequency regulation using load power control by smart transformers. ||| 22006 ||| 21934 ||| 10868 ||| 49447 ||| 
2020 ||| synchrophasor-based condition monitoring of instrument transformers using clustering approach. ||| 20021 ||| 49448 ||| 49449 ||| 
2018 ||| optimal penetration of home energy management systems in distribution networks considering transformer aging. ||| 49450 ||| 49451 ||| 49452 ||| 
2018 ||| load control using sensitivity identification by means of smart transformer. ||| 22006 ||| 21934 ||| 10868 ||| 49447 ||| 
2021 ||| synchronization of low voltage grids fed by smart and conventional transformers. ||| 49453 ||| 22006 ||| 49454 ||| 49455 ||| 10868 ||| 49456 ||| 
2017 ||| co-optimization of distribution transformer aging and energy arbitrage using electric vehicles. ||| 49451 ||| 49450 ||| 49452 ||| 
2017 ||| a single-phase on-line ups system for multiple load transformers. ||| 46703 ||| 49457 ||| 
2017 ||| a flexible two-section transmission-line transformer design approach for complex source and real load impedances. ||| 1117 ||| 49458 ||| 46924 ||| 
2017 ||| design and testing of a novel rotary transformer for rotary ultrasonic machining. ||| 49459 ||| 46938 ||| 11145 ||| 49460 ||| 
2018 ||| a ka band cmos lo distribution buffer using transformer-based three-way power divider. ||| 49461 ||| 49462 ||| 18731 ||| 49463 ||| 49464 ||| 49465 ||| 
2017 ||| a tunable transformer-based cmos directional coupler for uhf rfid readers. ||| 49466 ||| 49467 ||| 22431 ||| 49468 ||| 49469 ||| 22432 ||| 
2018 ||| fully-integrated linear cmos power amplifier with proportional series combining transformer for s-band applications. ||| 49470 ||| 6502 ||| 46471 ||| 49471 ||| 11361 ||| 49472 ||| 1305 ||| 
2017 ||| characteristic analysis of relatively high speed, loosely coupled rotating excitation transformers in hev and ev drive motor excitation systems. ||| 1223 ||| 12824 ||| 
2020 ||| equivalent circuit of interleaved air-core toroidal transformer derived from analogy with coupled transmission lines. ||| 22440 ||| 22442 ||| 
2021 ||| a 21.6dbm cmos power amplifier using a compact high-k output transformer for x-band application. ||| 49473 ||| 49474 ||| 49475 ||| 6821 ||| 49476 ||| 49477 ||| 49478 ||| 33134 ||| 2779 ||| 
2017 ||| a three-phase off-line ups system for transformer coupled loads. ||| 46703 ||| 49479 ||| 49480 ||| 49457 ||| 
2019 ||| an equivalent lumped circuit model for on-chip helical transformers [ieice electronics express vol. 15 (2018) no. 3 pp. 20170818]. ||| 49481 ||| 27256 ||| 49482 ||| 49483 ||| 
2018 ||| an equivalent lumped circuit model for on-chip helical transformers. ||| 49481 ||| 27256 ||| 49482 ||| 49483 ||| 
2018 ||| a 1-13 ghz cmos low-noise amplifier using compact transformer-based inter-stage networks. ||| 49484 ||| 49485 ||| 41773 ||| 
2019 ||| to the attention of mobile software developers: guess what, test your app! ||| 36590 ||| 36591 ||| 13151 ||| 
2019 ||| racmf: robust attention convolutional matrix factorization for rating prediction. ||| 44639 ||| 49486 ||| 49487 ||| 49488 ||| 1254 ||| 
2021 ||| a new method of hybrid time window embedding with transformer-based traffic data classification in iot-networked environment. ||| 16439 ||| 49489 ||| 16440 ||| 
2022 ||| travel order quantity prediction via attention-based bidirectional lstm networks. ||| 20341 ||| 44574 ||| 44575 ||| 
2020 ||| a parallel computing-based deep attention model for named entity recognition. ||| 34446 ||| 360 ||| 4149 ||| 49490 ||| 49491 ||| 
2021 ||| a new method of abnormal behavior detection using lstm network with temporal attention mechanism. ||| 49492 ||| 49493 ||| 
2020 ||| pulmonary nodule image super-resolution using multi-scale deep residual channel attention network with joint optimization. ||| 49494 ||| 8969 ||| 42366 ||| 49495 ||| 49496 ||| 49497 ||| 
2018 ||| a camera-based attention level assessment tool designed for classroom usage. ||| 49498 ||| 49499 ||| 
2021 ||| a novel attention fusion network-based framework to ensemble the predictions of cnns for lymph node metastasis detection. ||| 49500 ||| 49501 ||| 49502 ||| 49503 ||| 49504 ||| 
2022 ||| improving the readability and saliency of abstractive text summarization using combination of deep neural networks equipped with auxiliary attention mechanism. ||| 49505 ||| 49506 ||| 49507 ||| 
2017 ||| deep neural network with attention model for scene text recognition. ||| 13240 ||| 49508 ||| 47461 ||| 1101 ||| 1796 ||| 
2021 ||| hybrid attention mechanism for few-shot relational learning of knowledge graphs. ||| 49509 ||| 49510 ||| 49511 ||| 2932 ||| 
2020 ||| skeleton-based attention-aware spatial-temporal model for action detection and recognition. ||| 15752 ||| 45547 ||| 49512 ||| 8674 ||| 
2022 ||| parallax-based second-order mixed attention for stereo image super-resolution. ||| 47644 ||| 47645 ||| 
2020 ||| dual attention module and multi-label based fully convolutional network for crowd counting. ||| 44915 ||| 10875 ||| 1748 ||| 49513 ||| 
2019 ||| crowd counting using a self-attention multi-scale cascaded network. ||| 8440 ||| 40525 ||| 40526 ||| 
2019 ||| attention-based spatial-temporal hierarchical convlstm network for action recognition in videos. ||| 40016 ||| 28853 ||| 9900 ||| 1272 ||| 
2021 ||| l4net: an anchor-free generic object detector with attention mechanism for autonomous driving. ||| 49514 ||| 38122 ||| 49515 ||| 32292 ||| 
2021 ||| a tri-attention enhanced graph convolutional network for skeleton-based action recognition. ||| 49516 ||| 49517 ||| 1903 ||| 
2022 ||| dpanet: dual pooling-aggregated attention network for fish segmentation. ||| 9900 ||| 49518 ||| 9898 ||| 
2022 ||| multi-stream adaptive spatial-temporal attention graph convolutional network for skeleton-based action recognition. ||| 49519 ||| 49520 ||| 49521 ||| 49522 ||| 
2021 ||| standing out in a networked communication context: toward a network contingency model of public attention. ||| 49523 ||| 49524 ||| 
2019 ||| selective attention in the news feed: an eye-tracking study on the perception and selection of political news posts on facebook. ||| 49525 ||| 49526 ||| 49527 ||| 10712 ||| 49528 ||| 
2018 ||| attention and amplification in the hybrid media system: the composition and activity of donald trump's twitter following during the 2016 presidential election. ||| 21872 ||| 49529 ||| 307 ||| 49530 ||| 
2021 ||| going with the flow: nudging attention online. ||| 49531 ||| 49532 ||| 49533 ||| 
2021 ||| book review: subprime attention crisis: advertising and the time bomb at the heart of the internet. ||| 49534 ||| 
2021 ||| identification of three-dimensional defect topology in concrete structures based on self-attention network using hammering response data. ||| 49535 ||| 49536 ||| 49537 ||| 49538 ||| 49539 ||| 
2018 ||| a new deep spatial transformer convolutional neural network for image saliency detection. ||| 48599 ||| 35690 ||| 49540 ||| 
2021 ||| few-shot text classification by leveraging bi-directional attention and cross-class knowledge. ||| 49541 ||| 27178 ||| 1160 ||| 49542 ||| 49543 ||| 
2020 ||| faclstm: convlstm with focused attention for scene text recognition. ||| 17340 ||| 17343 ||| 17341 ||| 5123 ||| 2730 ||| 17344 ||| 17342 ||| 
2021 ||| task-wise attention guided part complementary learning for few-shot image classification. ||| 42037 ||| 42511 ||| 49544 ||| 2414 ||| 
2017 ||| 45-ghz and 60-ghz 90 nm cmos power amplifiers with a fully symmetrical 8-way transformer power combiner. ||| 46219 ||| 49545 ||| 49546 ||| 29454 ||| 41414 ||| 49547 ||| 1235 ||| 41418 ||| 
2020 ||| hybrid first and second order attention unet for building segmentation in remote sensing images. ||| 6748 ||| 6749 ||| 6750 ||| 
2021 ||| text information aggregation with centrality attention. ||| 38353 ||| 3816 ||| 38354 ||| 4967 ||| 3272 ||| 3273 ||| 
2019 ||| haptics-mediated approaches for enhancing sustained attention: framework and challenges. ||| 47624 ||| 14710 ||| 49548 ||| 14909 ||| 47625 ||| 
2019 ||| irregular scene text detection via attention guided border labeling. ||| 1037 ||| 49549 ||| 49550 ||| 49551 ||| 3760 ||| 
2020 ||| multi-attention based cross-domain beauty product image retrieval. ||| 19956 ||| 5902 ||| 49552 ||| 49553 ||| 19434 ||| 
2021 ||| dual-axial self-attention network for text classification. ||| 29911 ||| 3272 ||| 49554 ||| 49555 ||| 27234 ||| 
2021 ||| dual attention autoencoder for all-weather outdoor lighting estimation. ||| 49556 ||| 8718 ||| 49557 ||| 49558 ||| 49559 ||| 49560 ||| 8722 ||| 
2019 ||| arpnet: attention region proposal network for 3d object detection. ||| 44773 ||| 1460 ||| 44774 ||| 
2017 ||| common patterns of online collective attention flow. ||| 2969 ||| 49561 ||| 817 ||| 39696 ||| 2972 ||| 
2021 ||| pay attention to raw traces: a deep learning architecture for end-to-end profiling attacks. ||| 1459 ||| 1460 ||| 49562 ||| 1461 ||| 49563 ||| 
2019 ||| channel-wise attention model-based fire and rating level detection in video. ||| 4285 ||| 49564 ||| 30714 ||| 49565 ||| 49566 ||| 173 ||| 
2018 ||| temporal enhanced sentence-level attention model for hashtag recommendation. ||| 9547 ||| 717 ||| 32004 ||| 49567 ||| 49568 ||| 
2020 ||| affective computing study of attention recognition for the 3d guide system. ||| 49569 ||| 49570 ||| 49571 ||| 
2021 ||| agg: a novel intelligent network traffic prediction method based on joint attention and gcn-gru. ||| 49572 ||| 49573 ||| 2884 ||| 49574 ||| 
2021 ||| spatial-channel attention-based class activation mapping for interpreting cnn-based image classification models. ||| 40068 ||| 25768 ||| 40077 ||| 4215 ||| 49575 ||| 49576 ||| 
2021 ||| f3snet: a four-step strategy for qim steganalysis of compressed speech based on hierarchical attention network. ||| 34961 ||| 2334 ||| 49577 ||| 34962 ||| 
2021 ||| r2au-net: attention recurrent residual convolutional neural network for multimodal medical image segmentation. ||| 9892 ||| 9893 ||| 9894 ||| 
2021 ||| attention-guided digital adversarial patches on visual detection. ||| 49578 ||| 40273 ||| 49579 ||| 49580 ||| 
2021 ||| a hierarchical approach for advanced persistent threat detection with attention-based graph neural networks. ||| 49581 ||| 1287 ||| 49582 ||| 1422 ||| 10841 ||| 
2021 ||| hierarchical attention graph embedding networks for binary code similarity against compilation diversity. ||| 247 ||| 49583 ||| 31874 ||| 49584 ||| 40738 ||| 
2020 ||| slam: a malware detection method based on sliding local attention mechanism. ||| 1785 ||| 47399 ||| 9442 ||| 47400 ||| 46250 ||| 8009 ||| 46575 ||| 
2022 ||| rumor detection with bidirectional graph attention networks. ||| 31906 ||| 6432 ||| 46134 ||| 
2021 ||| social network spam detection based on albert and combination of bi-lstm with self-attention. ||| 49585 ||| 49586 ||| 1235 ||| 
2021 ||| compressed wavelet tensor attention capsule network. ||| 49587 ||| 49588 ||| 4385 ||| 15003 ||| 3676 ||| 
2021 ||| an enhanced visual attention siamese network that updates template features online. ||| 49589 ||| 49590 ||| 1073 ||| 49591 ||| 
2021 ||| eeg correlates of sustained attention variability during discrete multi-finger force control tasks. ||| 47623 ||| 49592 ||| 49593 ||| 47625 ||| 705 ||| 47624 ||| 
2021 ||| high generalization performance structured self-attention model for knapsack problem. ||| 49594 ||| 49595 ||| 49596 ||| 
2021 ||| an unsupervised approach for fault diagnosis of power transformers. ||| 49597 ||| 49598 ||| 49599 ||| 7033 ||| 8396 ||| 49600 ||| 227 ||| 49601 ||| 49602 ||| 49603 ||| 
2019 ||| attentional multilabel learning over graphs: a message passing approach. ||| 37707 ||| 31109 ||| 37708 ||| 31110 ||| 
2019 ||| dynamic attention-integrated neural network for session-based news recommendation. ||| 41680 ||| 10572 ||| 41681 ||| 
2019 ||| temporal pattern attention for multivariate time series forecasting. ||| 39519 ||| 39520 ||| 12644 ||| 
2020 ||| convolutional multi-head self-attention on memory for aspect sentiment classification. ||| 10476 ||| 7679 ||| 880 ||| 
2022 ||| highway lane change decision-making via attention-based deep reinforcement learning. ||| 26760 ||| 1014 ||| 29026 ||| 
2021 ||| global-attention-based neural networks for vision language intelligence. ||| 16927 ||| 49604 ||| 49605 ||| 31717 ||| 
2019 ||| hierarchical visual attention model for saliency detection inspired by avian visual pathways. ||| 5885 ||| 49606 ||| 
2021 ||| mu-gan: facial attribute editing based on multi-attention mechanism. ||| 19919 ||| 38939 ||| 24484 ||| 24485 ||| 28869 ||| 
2020 ||| a recurrent attention and interaction model for pedestrian trajectory prediction. ||| 27687 ||| 44743 ||| 44948 ||| 11354 ||| 
2020 ||| a spatial-temporal attention model for human trajectory prediction. ||| 17563 ||| 1012 ||| 49607 ||| 29026 ||| 
2021 ||| a transformer-based architecture for fake news classification. ||| 49608 ||| 49609 ||| 49610 ||| 48652 ||| 
2022 ||| abusive bangla comments detection on facebook using transformer-based deep learning models. ||| 9499 ||| 9501 ||| 9500 ||| 
2022 ||| caviar-ws-based han: conditional autoregressive value at risk-water sailfish-based hierarchical attention network for emotion classification in covid-19 text review data. ||| 49611 ||| 49612 ||| 49613 ||| 
2020 ||| a mathematical model of local and global attention in natural scene viewing. ||| 49614 ||| 49615 ||| 49616 ||| 49617 ||| 49618 ||| 49619 ||| 
2021 ||| learning, visualizing and exploring 16s rrna structure using an attention-based deep neural network. ||| 49620 ||| 49621 ||| 49622 ||| 49623 ||| 49624 ||| 49625 ||| 
2020 ||| the impact of news exposure on collective attention in the united states during the 2016 zika epidemic. ||| 49626 ||| 3369 ||| 49627 ||| 49628 ||| 49629 ||| 
2020 ||| flocking in complex environments - attention trade-offs in collective information processing. ||| 49630 ||| 49631 ||| 49632 ||| 
2020 ||| insight into the protein solubility driving forces with neural attention. ||| 45050 ||| 45051 ||| 49633 ||| 45056 ||| 
2021 ||| rational inattention and tonic dopamine. ||| 49634 ||| 49635 ||| 49636 ||| 
2019 ||| deep attention networks reveal the rules of collective motion in zebrafish. ||| 49637 ||| 49638 ||| 49639 ||| 49640 ||| 
2019 ||| prediction of off-target specificity and cell-specific fitness of crispr-cas system using attention boosted deep learning and network-based gene feature. ||| 8922 ||| 18012 ||| 12384 ||| 
2021 ||| aim: a network model of attention in auditory cortex. ||| 49641 ||| 49642 ||| 
2021 ||| analysis of spiking synchrony in visual cortex reveals distinct types of top-down modulation signals for spatial and object-based attention. ||| 4414 ||| 13722 ||| 59 ||| 49643 ||| 13725 ||| 
2019 ||| a computational account of threat-related attentional bias. ||| 49644 ||| 49645 ||| 49646 ||| 49647 ||| 
2021 ||| attention-enhanced gradual machine learning for entity resolution. ||| 35571 ||| 22590 ||| 22588 ||| 49648 ||| 
2021 ||| transformer-based models for automatic identification of argument relations: a cross-domain evaluation. ||| 36825 ||| 852 ||| 36827 ||| 49649 ||| 36828 ||| 36829 ||| 
2021 ||| tsnet: three-stream self-attention network for rgb-d indoor semantic segmentation. ||| 31370 ||| 49650 ||| 31372 ||| 31374 ||| 
2018 ||| a defense of ad blocking and consumer inattention. ||| 49651 ||| 49652 ||| 
2021 ||| the ethics of inattention: revitalising civil inattention as a privacy-protecting mechanism in public spaces. ||| 49653 ||| 49654 ||| 
2020 ||| the effects of long-term child-robot interaction on the attention and the engagement of children with autism. ||| 49655 ||| 49656 ||| 49657 ||| 49658 ||| 49659 ||| 49660 ||| 49661 ||| 49662 ||| 49663 ||| 49664 ||| 43099 ||| 
2018 ||| deep learning systems for estimating visual attention in robot-assisted therapy of children with autism and intellectual disability. ||| 49665 ||| 49666 ||| 49667 ||| 49668 ||| 49669 ||| 
2021 ||| topic classification of electric vehicle consumer experiences with transformer-based deep learning. ||| 49670 ||| 49671 ||| 49672 ||| 49673 ||| 
2020 ||| two is better than one. improved attention guiding in ar by combining techniques. ||| 14926 ||| 14927 ||| 14928 ||| 
2021 ||| rsanet: towards real-time object detection with residual semantic-guided attention feature pyramid network. ||| 11215 ||| 5894 ||| 10107 ||| 11214 ||| 19801 ||| 18802 ||| 
2020 ||| a topic attention mechanism and factorization machines based mobile application recommendation method. ||| 1569 ||| 927 ||| 162 ||| 1592 ||| 
2021 ||| tprune: efficient transformer pruning for mobile devices. ||| 49674 ||| 39864 ||| 13659 ||| 9877 ||| 7488 ||| 
2020 ||| the plausibility of using unmanned aerial vehicles as a serious game for dealing with attention deficit-hyperactivity disorder. ||| 49675 ||| 11005 ||| 852 ||| 49676 ||| 49677 ||| 49678 ||| 49679 ||| 
2021 ||| complementary interactions between classical and top-down driven inhibitory mechanisms of attention. ||| 49680 ||| 49681 ||| 49682 ||| 
2018 ||| interactions between astrocytes and the reward-attention circuit: a model for attention focusing in the presence of nicotine. ||| 44593 ||| 227 ||| 44594 ||| 44595 ||| 
2019 ||| a mathematical model of the interaction between bottom-up and top-down attention controllers in response to a target and a distractor in human beings. ||| 49683 ||| 49684 ||| 49685 ||| 
2020 ||| a bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes. ||| 9118 ||| 9119 ||| 9120 ||| 9121 ||| 504 ||| 9122 ||| 
2019 ||| an eye-tracking attention based model for abstractive text headline. ||| 40616 ||| 22375 ||| 13738 ||| 8427 ||| 31174 ||| 
2017 ||| public attention, social media, and the edward snowden saga. ||| 49686 ||| 49687 ||| 
2019 ||| 'coherent clusters' or 'fuzzy zones' - understanding attention and structure in online political participation. ||| 49688 ||| 
2021 ||| ava: a financial service chatbot based on deep bidirectional transformers. ||| 32325 ||| 32326 ||| 32327 ||| 
2021 ||| the role of working memory and attention in older workers' learning. ||| 49689 ||| 49690 ||| 28656 ||| 
2019 ||| entity recognition in chinese clinical text using attention-based cnn-lstm-crf. ||| 5231 ||| 1117 ||| 18517 ||| 12391 ||| 
2021 ||| transformers-sklearn: a toolkit for medical language understanding with transformer-based models. ||| 49691 ||| 49692 ||| 49693 ||| 9600 ||| 
2019 ||| a deep learning model incorporating part of speech and self-matching attention for named entity recognition of chinese electronic medical records. ||| 49694 ||| 825 ||| 823 ||| 
2020 ||| autodiscern: rating the quality of online health information with hierarchical encoder attention-based neural networks. ||| 33293 ||| 33294 ||| 26645 ||| 
2020 ||| an interpretable risk prediction model for healthcare with pattern attention. ||| 49695 ||| 3747 ||| 16280 ||| 3750 ||| 
2021 ||| u-net combined with multi-scale attention mechanism for liver segmentation in ct images. ||| 49696 ||| 49697 ||| 49698 ||| 49699 ||| 49700 ||| 48755 ||| 49701 ||| 1341 ||| 49702 ||| 
2020 ||| effective attention-based network for syndrome differentiation of aids. ||| 49703 ||| 5392 ||| 49704 ||| 49705 ||| 8349 ||| 49706 ||| 4400 ||| 
2021 ||| automatic detection of actionable radiology reports using bidirectional encoder representations from transformers. ||| 49707 ||| 49708 ||| 49709 ||| 49710 ||| 49711 ||| 49712 ||| 49713 ||| 49714 ||| 49715 ||| 
2019 ||| an attention-based deep learning model for clinical named entity recognition of chinese electronic medical records. ||| 21577 ||| 6644 ||| 21578 ||| 49716 ||| 49717 ||| 49718 ||| 
2019 ||| attention-based deep residual learning network for entity relation extraction in chinese emrs. ||| 5302 ||| 18724 ||| 9472 ||| 49719 ||| 
2020 ||| multi-modality self-attention aware deep network for 3d biomedical segmentation. ||| 6543 ||| 12438 ||| 49720 ||| 12500 ||| 
2020 ||| interpretable clinical prediction via attention-based neural network. ||| 49721 ||| 8305 ||| 49722 ||| 10331 ||| 49723 ||| 31160 ||| 
2021 ||| constrained transformer network for ecg signal processing and arrhythmia classification. ||| 44061 ||| 49724 ||| 519 ||| 49725 ||| 11110 ||| 
2021 ||| transformer-based deep neural network language models for alzheimer's disease risk assessment from targeted speech. ||| 49726 ||| 49727 ||| 49728 ||| 
2022 ||| predicting mirna-disease associations via layer attention graph convolutional network model. ||| 43007 ||| 49729 ||| 49730 ||| 49731 ||| 
2021 ||| a bci video game using neurofeedback improves the attention of children with autism. ||| 49732 ||| 49733 ||| 28153 ||| 
2021 ||| empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum. ||| 15939 ||| 23671 ||| 49734 ||| 49735 ||| 23672 ||| 15940 ||| 15941 ||| 
2018 ||| leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups. ||| 25709 ||| 45267 ||| 7111 ||| 49736 ||| 49737 ||| 49738 ||| 49739 ||| 
2019 ||| unsupervised learning of depth estimation based on attention model and global pose optimization. ||| 49740 ||| 30832 ||| 30831 ||| 5061 ||| 49741 ||| 40542 ||| 30833 ||| 
2019 ||| vehicle joint make and model recognition with multiscale attention windows. ||| 24876 ||| 24877 ||| 49742 ||| 24879 ||| 24878 ||| 
2020 ||| comprehensive feature fusion mechanism for video-based person re-identification via significance-aware attention. ||| 6469 ||| 1007 ||| 44896 ||| 
2018 ||| modeling visual and word-conditional semantic attention for image captioning. ||| 18635 ||| 18634 ||| 18636 ||| 19113 ||| 18638 ||| 
2020 ||| feature refinement for image-based driver action recognition via multi-scale attention convolutional neural network. ||| 28797 ||| 28798 ||| 7659 ||| 
2019 ||| spatio-temporal attention mechanisms based model for collective activity recognition. ||| 22574 ||| 22572 ||| 4151 ||| 22573 ||| 6459 ||| 
2021 ||| saliency4asd: challenge, dataset and tools for visual attention modeling for autism spectrum disorder. ||| 12581 ||| 33910 ||| 26875 ||| 49743 ||| 6516 ||| 11611 ||| 
2018 ||| multiple rotation symmetry group detection via saliency-based visual attention and frieze expansion pattern. ||| 49744 ||| 49745 ||| 49746 ||| 49747 ||| 49748 ||| 
2020 ||| image captioning using densenet network and adaptive attention. ||| 40056 ||| 40058 ||| 29073 ||| 40059 ||| 29074 ||| 
2021 ||| cdadnet: context-guided dense attentional dilated network for crowd counting. ||| 45547 ||| 49749 ||| 49750 ||| 47633 ||| 49751 ||| 8674 ||| 49752 ||| 
2022 ||| perceiving informative key-points: a self-attention approach for person search. ||| 19347 ||| 49753 ||| 5107 ||| 
2021 ||| visual attention prediction for autism spectrum disorder with hierarchical semantic fusion. ||| 19941 ||| 42485 ||| 49754 ||| 43879 ||| 19942 ||| 49755 ||| 
2020 ||| saliency detection in human crowd images of different density levels using attention mechanism. ||| 49756 ||| 49757 ||| 49758 ||| 
2019 ||| fusion global and local deep representations with neural attention for aesthetic quality assessment. ||| 38080 ||| 6621 ||| 32419 ||| 5253 ||| 42653 ||| 
2021 ||| siamda: dual attention siamese network for real-time visual tracking. ||| 6546 ||| 6547 ||| 6548 ||| 6549 ||| 6488 ||| 
2022 ||| multi-scale visual attention for attribute disambiguation in zero-shot learning. ||| 26660 ||| 9283 ||| 49759 ||| 3386 ||| 49760 ||| 49761 ||| 49762 ||| 31788 ||| 
2018 ||| hierarchical multi-scale attention networks for action recognition. ||| 13 ||| 11226 ||| 11227 ||| 11228 ||| 
2021 ||| from semantic to spatial awareness: vehicle reidentification with multiple attention mechanisms. ||| 49763 ||| 2146 ||| 813 ||| 5964 ||| 
2021 ||| feature-guided spatial attention upsampling for real-time stereo matching network. ||| 49764 ||| 49765 ||| 38512 ||| 
2022 ||| transformer models in the home improvement domain. ||| 2923 ||| 2925 ||| 
2021 ||| attention-based context boosted cyberbullying detection in social media. ||| 49766 ||| 5988 ||| 
2021 ||| attention, consciousness, and linguistic cooperation with ai. ||| 49767 ||| 
2022 ||| a design of global workspace model with attention: simulations of attentional blink and lag-1 sparing. ||| 49768 ||| 49769 ||| 13070 ||| 
2020 ||| attention and consciousness in intentional action: steps toward rich artificial agency. ||| 2666 ||| 2668 ||| 
2019 ||| activism via attention: interpretable spatiotemporal learning to forecast protest activities. ||| 49770 ||| 49771 ||| 49772 ||| 49773 ||| 13659 ||| 
2021 ||| attention dynamics on the chinese social media sina weibo during the covid-19 pandemic. ||| 29310 ||| 4194 ||| 33971 ||| 11944 ||| 
2017 ||| measuring and monitoring collective attention during shocking events. ||| 49774 ||| 49771 ||| 
2021 ||| industrial process monitoring and fault diagnosis based on temporal attention augmented deep network. ||| 49775 ||| 49776 ||| 18671 ||| 49777 ||| 
2021 ||| audio and video bimodal emotion recognition in social networks based on improved alexnet network and attention mechanism. ||| 6796 ||| 31430 ||| 
2019 ||| two-dimensional attention-based lstm model for stock index prediction. ||| 49778 ||| 49779 ||| 
2021 ||| priming effect of colour on aiding the attentional reorientation in sequential presentations of temporal data visualization: evidence from eye-tracking. ||| 49780 ||| 49781 ||| 18952 ||| 49782 ||| 23870 ||| 
2020 ||| massive mimo csi reconstruction using cnn-lstm and attention mechanism. ||| 28698 ||| 49783 ||| 28695 ||| 28699 ||| 
2021 ||| siamese tracking combing frequency channel attention with adaptive template. ||| 49784 ||| 49785 ||| 49786 ||| 49787 ||| 49788 ||| 
2021 ||| glacier classification from sentinel-2 imagery using spatial-spectral attention convolutional model. ||| 49789 ||| 38069 ||| 49790 ||| 49791 ||| 49792 ||| 30179 ||| 49793 ||| 49794 ||| 
2021 ||| ads-net: an attention-based deeply supervised network for remote sensing image change detection. ||| 49795 ||| 35561 ||| 49796 ||| 49797 ||| 49798 ||| 49799 ||| 
2021 ||| image super-resolution with dense-sampling residual channel-spatial attention networks for multi-temporal remote sensing image classification. ||| 49800 ||| 49801 ||| 49802 ||| 
2022 ||| an attention-based u-net for detecting deforestation within satellite sensor imagery. ||| 49803 ||| 13362 ||| 
2021 ||| a deep learning framework under attention mechanism for wheat yield estimation using remotely sensed indices in the guanzhong plain, pr china. ||| 49804 ||| 49805 ||| 49806 ||| 49807 ||| 49808 ||| 49809 ||| 43764 ||| 
2021 ||| suacdnet: attentional change detection network based on siamese u-shaped structure. ||| 27412 ||| 41631 ||| 49810 ||| 49811 ||| 31341 ||| 
2021 ||| hierarchical semantic segmentation of urban scene point clouds via group proposal and graph attention network. ||| 49812 ||| 4394 ||| 2063 ||| 181 ||| 6417 ||| 507 ||| 
2021 ||| dsa-net: a novel deeply supervised attention-guided network for building change detection in high-resolution remote sensing images. ||| 19549 ||| 32830 ||| 18085 ||| 49813 ||| 
2021 ||| deep color calibration for uav imagery in crop monitoring using semantic style transfer with local to global attention. ||| 49814 ||| 49815 ||| 49816 ||| 49817 ||| 49818 ||| 49819 ||| 49820 ||| 18589 ||| 49821 ||| 49822 ||| 
2022 ||| intrusion detection model using temporal convolutional network blend into attention mechanism. ||| 21701 ||| 17907 ||| 28937 ||| 633 ||| 
2019 ||| how academia and society pay attention to climate changes: a bibliometric and altmetric analysis. ||| 49823 ||| 49824 ||| 49825 ||| 49826 ||| 49827 ||| 
2021 ||| abstractive review summarization based on improved attention mechanism with pointer generator network model. ||| 49828 ||| 49829 ||| 
2020 ||| preliminary study of a separative shared control scheme focusing on control-authority and attention allocation for multi-limb disaster response robots. ||| 15486 ||| 49830 ||| 49831 ||| 49832 ||| 15489 ||| 
2020 ||| can a humanoid robot continue to draw attention in an office environment? ||| 49833 ||| 49834 ||| 49835 ||| 49836 ||| 49837 ||| 21785 ||| 20474 ||| 
2019 ||| a human behavior model of multi-agent attention based on actor-observer switching for asynchronous motion tasks with limited field of view. ||| 49838 ||| 49839 ||| 49840 ||| 
2021 ||| adaptive neuro-fuzzy-based attention deficit/hyperactivity disorder diagnostic system. ||| 49841 ||| 49842 ||| 49843 ||| 49844 ||| 
2020 ||| identifying depression in tweets using cnn-deep and bilstm with attention model. ||| 49845 ||| 49846 ||| 49847 ||| 49848 ||| 
2019 ||| the effect of task on visual attention in interactive virtual environments. ||| 49849 ||| 49850 ||| 49851 ||| 49852 ||| 
2017 ||| gaze data for the analysis of attention in feature films. ||| 49853 ||| 49854 ||| 
2021 ||| does what we see shape history? examining workload history as a function of performance and ambient/focal visual attention. ||| 27074 ||| 49855 ||| 27076 ||| 
2021 ||| lapformer: surgical tool detection in laparoscopic surgical video using transformer architecture. ||| 49856 ||| 
2022 ||| temporal convolutional networks and transformers for classifying the sleep stage in awake or asleep using pulse oximetry signals. ||| 37634 ||| 37635 ||| 29722 ||| 37636 ||| 
2021 ||| stan: spatio-temporal attention network for pandemic prediction using real-world evidence. ||| 18564 ||| 34790 ||| 38396 ||| 38397 ||| 38398 ||| 34791 ||| 23323 ||| 1070 ||| 
2018 ||| hierarchical attention networks for information extraction from cancer pathology reports. ||| 23769 ||| 49857 ||| 41957 ||| 31077 ||| 41959 ||| 41960 ||| 23771 ||| 23770 ||| 
2021 ||| multimodal, multitask, multiattention (m3) deep learning detection of reticular pseudodrusen: toward automated and accessible classification of age-related macular degeneration. ||| 36929 ||| 36930 ||| 36931 ||| 21585 ||| 36932 ||| 3882 ||| 36933 ||| 36934 ||| 36935 ||| 36936 ||| 36937 ||| 36938 ||| 36939 ||| 36940 ||| 36941 ||| 19768 ||| 36942 ||| 21587 ||| 
2020 ||| clinical concept extraction using transformers. ||| 13408 ||| 2989 ||| 34086 ||| 12067 ||| 
2019 ||| algorithm to detect pediatric provider attention to high bmi and associated medical risk. ||| 49858 ||| 49859 ||| 49860 ||| 
2020 ||| unified medical language system resources improve sieve-based generation and bidirectional encoder representations from transformers (bert)-based ranking for concept normalization. ||| 34840 ||| 49861 ||| 494 ||| 49862 ||| 104 ||| 3357 ||| 
2020 ||| global channel attention networks for intracranial vessel segmentation. ||| 439 ||| 441 ||| 3091 ||| 49863 ||| 49864 ||| 49865 ||| 49866 ||| 
2019 ||| rianet: recurrent interleaved attention network for cardiac mri segmentation. ||| 27919 ||| 27918 ||| 27921 ||| 27920 ||| 49867 ||| 16614 ||| 8637 ||| 
2022 ||| emotion recognition from eeg based on multi-task learning with capsule network and attention mechanism. ||| 28409 ||| 379 ||| 49868 ||| 16550 ||| 49869 ||| 49870 ||| 2184 ||| 
2022 ||| safe medicine recommendation via star interactive enhanced-based transformer model. ||| 29554 ||| 29552 ||| 29553 ||| 29551 ||| 
2021 ||| paired-unpaired unsupervised attention guided gan with transfer learning for bidirectional brain mr-ct synthesis. ||| 49871 ||| 49872 ||| 49873 ||| 49874 ||| 49875 ||| 
2020 ||| han-ecg: an interpretable atrial fibrillation detection model using hierarchical attention networks. ||| 6124 ||| 6125 ||| 6127 ||| 
2021 ||| a novel m-segnet with global attention cnn architecture for automatic segmentation of brain mri. ||| 49876 ||| 41176 ||| 
2022 ||| anatomically constrained squeeze-and-excitation graph attention network for cortical surface parcellation. ||| 5851 ||| 49877 ||| 49878 ||| 2519 ||| 5854 ||| 1160 ||| 
2021 ||| mca-dn: multi-path convolution leveraged attention deep network for salvageable tissue detection in ischemic stroke from multi-parametric mri. ||| 49879 ||| 49880 ||| 20440 ||| 
2020 ||| massd: multi-scale attention single shot detector for surgical instruments. ||| 49881 ||| 43739 ||| 49882 ||| 49883 ||| 38782 ||| 
2021 ||| autism spectrum disorder diagnosis using graph attention network based on spatial-constrained sparse functional brain networks. ||| 49884 ||| 49878 ||| 49877 ||| 49885 ||| 5851 ||| 
2021 ||| ta-net: triple attention network for medical image segmentation. ||| 438 ||| 13171 ||| 439 ||| 440 ||| 441 ||| 
2021 ||| d2a u-net: automatic segmentation of covid-19 ct slices based on dual attention and hybrid dilated convolution. ||| 9055 ||| 989 ||| 33449 ||| 33446 ||| 33451 ||| 14908 ||| 33869 ||| 33870 ||| 33453 ||| 
2017 ||| diagnosis of attention deficit hyperactivity disorder using imaging and signal processing techniques. ||| 49886 ||| 49887 ||| 6127 ||| 49888 ||| 49889 ||| 
2021 ||| a depthwise separable dense convolutional network with convolution block attention module for covid-19 diagnosis on ct scans. ||| 1719 ||| 49890 ||| 49891 ||| 49892 ||| 
2021 ||| densely connected attention network for diagnosing covid-19 based on chest ct. ||| 32066 ||| 49893 ||| 49894 ||| 
2022 ||| a multiscale double-branch residual attention network for anatomical-functional medical image fusion. ||| 20432 ||| 49895 ||| 11391 ||| 49896 ||| 28934 ||| 49897 ||| 
2022 ||| automated classification of attention deficit hyperactivity disorder and conduct disorder using entropy features with ecg signals. ||| 49898 ||| 44169 ||| 44170 ||| 49899 ||| 44168 ||| 44173 ||| 49900 ||| 6127 ||| 44174 ||| 
2021 ||| trp-bert: discrimination of transient receptor potential (trp) channels using contextual representations from deep bidirectional transformer based on bert. ||| 49901 ||| 49902 ||| 
2022 ||| deepmgt-dti: transformer network incorporating multilayer graph information for drug-target interaction prediction. ||| 49724 ||| 49903 ||| 44061 ||| 11110 ||| 
2022 ||| is the aspect ratio of cells important in deep learning? a robust comparison of deep learning methods for multi-scale cytopathology cell image classification: from convolutional neural networks to visual transformers. ||| 32663 ||| 399 ||| 13789 ||| 4100 ||| 33561 ||| 49904 ||| 8841 ||| 32662 ||| 13788 ||| 34508 ||| 15956 ||| 
2022 ||| attention-based 3d cnn with residual connections for efficient ecg-based covid-19 detection. ||| 49905 ||| 44486 ||| 59 ||| 49900 ||| 6127 ||| 
2021 ||| caspianet++: a multidimensional channel-spatial asymmetric attention network with noisy student curriculum learning paradigm for brain tumor segmentation. ||| 35949 ||| 35950 ||| 35951 ||| 35952 ||| 
2021 ||| automatic consecutive context perceived transformer gan for serial sectioning image blind inpainting. ||| 3279 ||| 49906 ||| 49907 ||| 1134 ||| 49908 ||| 49909 ||| 49910 ||| 
2021 ||| caagp: rethinking channel attention with adaptive global pooling for liver tumor segmentation. ||| 1460 ||| 28951 ||| 37396 ||| 28953 ||| 
2022 ||| il-mcam: an interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach. ||| 32662 ||| 399 ||| 5378 ||| 13789 ||| 8841 ||| 13786 ||| 32663 ||| 13788 ||| 33561 ||| 49911 ||| 15956 ||| 
2019 ||| binary tree-like network with two-path fusion attention feature for cervical cell nucleus segmentation. ||| 18083 ||| 49912 ||| 49913 ||| 49914 ||| 49915 ||| 49916 ||| 
2021 ||| attention-embedded complementary-stream cnn for false positive reduction in pulmonary nodule detection. ||| 49917 ||| 49918 ||| 49919 ||| 49920 ||| 49921 ||| 37969 ||| 37970 ||| 
2021 ||| hybrid dilation and attention residual u-net for medical image segmentation. ||| 49922 ||| 49923 ||| 49924 ||| 
2021 ||| fad-bert: improved prediction of fad binding sites using pre-training of deep bidirectional transformers. ||| 49925 ||| 49926 ||| 49927 ||| 49902 ||| 
2019 ||| document-level attention-based bilstm-crf incorporating disease dictionary for disease named entity recognition. ||| 5723 ||| 4267 ||| 49928 ||| 6627 ||| 4270 ||| 
2021 ||| focus u-net: a novel dual attention-gated cnn for polyp segmentation during colonoscopy. ||| 34228 ||| 34230 ||| 34231 ||| 34232 ||| 34229 ||| 
2022 ||| development of adaptive human-computer interaction games to evaluate attention. ||| 49929 ||| 49930 ||| 
2020 ||| game-based promotion of motivation and attention for socio-emotional training in autism. ||| 49931 ||| 49932 ||| 49933 ||| 49934 ||| 49935 ||| 
2021 ||| an end-to-end heterogeneous graph attention network for mycobacterium tuberculosis drug-resistance prediction. ||| 11466 ||| 49936 ||| 49937 ||| 3995 ||| 49938 ||| 49939 ||| 49940 ||| 
2021 ||| saresnet: self-attention residual network for predicting dna-protein binding. ||| 49941 ||| 9337 ||| 49942 ||| 49943 ||| 
2021 ||| deep drug-target binding affinity prediction with multiple attention blocks. ||| 49944 ||| 49945 ||| 49946 ||| 49947 ||| 49605 ||| 
2021 ||| recognizing binding sites of poorly characterized rna-binding proteins on circular rnas using attention siamese network. ||| 49948 ||| 33558 ||| 11466 ||| 40234 ||| 
2022 ||| fusiondta: attention-based feature polymerizer and knowledge distillation for drug-target binding affinity prediction. ||| 49949 ||| 49950 ||| 43838 ||| 
2021 ||| deepatt: a hybrid category attention neural network for identifying functional effects of dna sequences. ||| 2404 ||| 49951 ||| 16836 ||| 49952 ||| 35640 ||| 
2022 ||| deepdds: deep graph neural network with attention mechanism to predict synergistic drug combinations. ||| 36906 ||| 36907 ||| 35065 ||| 16602 ||| 17677 ||| 
2021 ||| a transformer architecture based on bert and 2d convolutional neural network to identify dna enhancers from sequence information. ||| 49927 ||| 49925 ||| 49926 ||| 49902 ||| 
2021 ||| attentional multi-level representation encoding based on convolutional and variance autoencoders for lncrna-disease association prediction. ||| 30996 ||| 421 ||| 24117 ||| 24119 ||| 
2022 ||| gvdti: graph convolutional and variational autoencoders with attribute-level attention for drug-protein interaction prediction. ||| 24119 ||| 49953 ||| 421 ||| 24117 ||| 24118 ||| 
2021 ||| a novel graph attention model for predicting frequencies of drug-side effects from multi-view data. ||| 45003 ||| 11162 ||| 16618 ||| 1130 ||| 
2022 ||| accurate protein function prediction via graph attention networks with predicted structure information. ||| 49954 ||| 13675 ||| 
2022 ||| multi-channel graph attention autoencoders for disease-related lncrnas prediction. ||| 30996 ||| 41549 ||| 247 ||| 764 ||| 24119 ||| 30995 ||| 49955 ||| 
2022 ||| kgancda: predicting circrna-disease associations based on knowledge graph attention network. ||| 44860 ||| 49956 ||| 44862 ||| 49957 ||| 3888 ||| 16855 ||| 524 ||| 
2021 ||| leveraging the attention mechanism to improve the identification of dna n6-methyladenine sites. ||| 4646 ||| 9337 ||| 922 ||| 2735 ||| 49958 ||| 49942 ||| 49943 ||| 
2022 ||| identifying drug-target interactions via heterogeneous graph attention networks combined with cross-modal similarities. ||| 18703 ||| 46728 ||| 7400 ||| 49959 ||| 49960 ||| 49961 ||| 
2021 ||| erratum to: evotuning protocols for transformer-based variant effect prediction on multi-domain proteins. ||| 49962 ||| 49963 ||| 
2022 ||| egret: edge aggregated graph attention networks and transfer learning improve protein-protein interaction site prediction. ||| 45005 ||| 45007 ||| 
2021 ||| explainability in transformer models for functional genomics. ||| 30941 ||| 44972 ||| 30942 ||| 
2021 ||| predicting human microbe-disease associations via graph attention networks with inductive matrix completion. ||| 16596 ||| 16600 ||| 9472 ||| 18749 ||| 
2021 ||| multi-view multichannel attention graph convolutional network for mirna-disease association prediction. ||| 49964 ||| 16600 ||| 16832 ||| 16926 ||| 
2022 ||| a novel convolution attention model for predicting transcription factor binding sites by combination of sequence and shape. ||| 13553 ||| 13547 ||| 49965 ||| 263 ||| 49966 ||| 49967 ||| 25648 ||| 49952 ||| 
2021 ||| a spatial-temporal gated attention module for molecular property prediction based on molecular geometry. ||| 49968 ||| 17636 ||| 28593 ||| 49969 ||| 16808 ||| 
2022 ||| predicting mirna-disease associations based on graph random propagation network and attention network. ||| 49970 ||| 49971 ||| 26943 ||| 49972 ||| 15201 ||| 
2022 ||| adapt-kcr: a novel deep learning framework for accurate prediction of lysine crotonylation sites based on learning embedding features and attention architecture. ||| 49973 ||| 49974 ||| 49975 ||| 49976 ||| 23577 ||| 49977 ||| 
2022 ||| maresnet: predicting transcription factor binding sites by combining multi-scale bottom-up and top-down attention and residual network. ||| 49978 ||| 49941 ||| 49979 ||| 922 ||| 49942 ||| 49943 ||| 
2021 ||| atse: a peptide toxicity predictor by exploiting structural and evolutionary information based on graph neural network and attention mechanism. ||| 49980 ||| 49981 ||| 19129 ||| 49982 ||| 39630 ||| 
2022 ||| mdf-sa-ddi: predicting drug-drug interaction events based on multi-source drug fusion, multi-source feature fusion and transformer self-attention mechanism. ||| 49983 ||| 49984 ||| 49985 ||| 49986 ||| 49987 ||| 49988 ||| 49989 ||| 49990 ||| 49991 ||| 45378 ||| 49992 ||| 
2022 ||| stable-abppred: a stacked ensemble predictor based on bilstm and attention mechanism for accelerated discovery of antibacterial peptides. ||| 49993 ||| 49994 ||| 49995 ||| 29279 ||| 49996 ||| 
2022 ||| amde: a novel attention-mechanism-based multidimensional feature encoder for drug-drug interaction prediction. ||| 32966 ||| 4646 ||| 20159 ||| 7237 ||| 12038 ||| 49997 ||| 49998 ||| 3882 ||| 
2021 ||| evotuning protocols for transformer-based variant effect prediction on multi-domain proteins. ||| 49962 ||| 49963 ||| 
2021 ||| predicting drug-disease associations through layer attention graph convolutional network. ||| 49999 ||| 50000 ||| 50001 ||| 50002 ||| 13735 ||| 
2022 ||| heterogeneous graph attention network based on meta-paths for lncrna-disease association prediction. ||| 50003 ||| 50004 ||| 49961 ||| 
2022 ||| dsgat: predicting frequencies of drug side effects by graph attention networks. ||| 50005 ||| 50006 ||| 10137 ||| 5439 ||| 4715 ||| 3997 ||| 5744 ||| 
2022 ||| alphafold2-aware protein-dna binding site prediction using graph transformer. ||| 50007 ||| 35986 ||| 23503 ||| 16736 ||| 50008 ||| 16595 ||| 
2019 ||| parsing heterogeneity in autism spectrum disorder and attention-deficit/hyperactivity disorder with individual connectome mapping. ||| 50009 ||| 50010 ||| 50011 ||| 49049 ||| 50012 ||| 50013 ||| 
2017 ||| structural and functional abnormalities in children with attention-deficit/hyperactivity disorder: a focus on subgenual anterior cingulate cortex. ||| 50014 ||| 50015 ||| 27543 ||| 50016 ||| 33139 ||| 
2019 ||| functional connectivity of attention, visual, and language networks during audio, illustrated, and animated stories in preschool-age children. ||| 50017 ||| 50018 ||| 50019 ||| 50020 ||| 50021 ||| 
2021 ||| directed flow of beta band communication during reorienting of attention within the dorsal attention network. ||| 48986 ||| 50022 ||| 48991 ||| 48990 ||| 50023 ||| 
2018 ||| resting-state network functional connectivity patterns associated with the mindful attention awareness scale. ||| 50024 ||| 50025 ||| 50026 ||| 
2019 ||| integration and segregation of the brain relate to stability of performance in children and adolescents with varied levels of inattention and impulsivity. ||| 50027 ||| 50028 ||| 
2021 ||| topological aberrance of structural brain network provides quantitative substrates of post-traumatic brain injury attention deficits in children. ||| 455 ||| 50029 ||| 1744 ||| 50030 ||| 50031 ||| 50032 ||| 50033 ||| 31691 ||| 33139 ||| 
2021 ||| a review of the default mode network in autism spectrum disorders and attention deficit hyperactivity disorder. ||| 50034 ||| 50035 ||| 50036 ||| 50037 ||| 50038 ||| 
2019 ||| electroencephalography functional networks reveal global effects of methylphenidate in youth with attention deficit/hyperactivity disorder. ||| 50039 ||| 50040 ||| 50041 ||| 50042 ||| 50043 ||| 50044 ||| 
2021 ||| aggregation-and-attention network for brain tumor segmentation. ||| 50045 ||| 11628 ||| 41188 ||| 
2021 ||| ve adults with attention-deficit hyperactivity disorder during fmri tasks of motor inhibition and cognitive switching. ||| 50046 ||| 50047 ||| 50048 ||| 50049 ||| 50050 ||| 50051 ||| 50052 ||| 50053 ||| 50054 ||| 
2020 ||| context-aware attention network for human emotion recognition in video. ||| 24050 ||| 46134 ||| 
2018 ||| anomaly detection in moving crowds through spatiotemporal autoencoding and additional attention. ||| 50055 ||| 50056 ||| 6656 ||| 9447 ||| 
2020 ||| stock index prices prediction via temporal pattern attention and long-short-term memory. ||| 50057 ||| 50058 ||| 50059 ||| 50060 ||| 
2021 ||| improving power losses and thermal management in switch mode power converters using multiple transformers. ||| 50061 ||| 50062 ||| 50063 ||| 
2021 ||| local-constraint transformer network for stock movement prediction. ||| 50064 ||| 
2020 ||| a short text conversation generation model combining bert and context attention mechanism. ||| 15201 ||| 12720 ||| 4470 ||| 
2021 ||| rpt: relational pre-trained transformer is almost all you need towards democratizing data preparation. ||| 37169 ||| 37170 ||| 37171 ||| 37172 ||| 23842 ||| 4094 ||| 15148 ||| 37174 ||| 
2017 ||| asap: prioritizing attention via time series smoothing. ||| 9955 ||| 9953 ||| 
2021 ||| fear of missing out (fomo) among undergraduate students in relation to attention distraction and learning disengagement in lectures. ||| 50065 ||| 50066 ||| 
2017 ||| design optimization of distribution transformers with nature-inspired metaheuristics: a comparative analysis. ||| 50067 ||| 50068 ||| 
2020 ||| analysis of acoustic sensor placement for pd location in power transformer. ||| 50069 ||| 47662 ||| 50070 ||| 50071 ||| 50072 ||| 50073 ||| 
2019 ||| real-time implementation of electronic power transformer based on intelligent controller. ||| 50074 ||| 50075 ||| 7442 ||| 50076 ||| 50077 ||| 50078 ||| 
2017 ||| modeling and simulation of 2.5 mva sf6-gas-insulated transformer. ||| 50079 ||| 58 ||| 50080 ||| 50081 ||| 
2018 ||| application of acf-wavelet feature extraction for classification of some artificial pd models of power transformer. ||| 50082 ||| 
2017 ||| calculation of creepage discharge safety factors against the tangential component of electric fields in the insulation structure of power transformers. ||| 50083 ||| 
2021 ||| exploring the attention process differentiation of attention deficit hyperactivity disorder (adhd) symptomatic adults using artificial intelligence on electroencephalography (eeg) signals. ||| 50084 ||| 50085 ||| 50086 ||| 50087 ||| 
2019 ||| investigation of control of power flow by using phase shifting transformers: turkey case study. ||| 50088 ||| 50089 ||| 59 ||| 50090 ||| 
2018 ||| transformer incipient fault diagnosis on the basis of energy-weighted dga using an artificial neural network. ||| 50091 ||| 28315 ||| 28435 ||| 
2019 ||| a coordinated dc voltage control strategy for cascaded solid state transformer with star configuration. ||| 10877 ||| 47867 ||| 9688 ||| 10876 ||| 27852 ||| 
2018 ||| sf6 gas-insulated 50-kva distribution transformer design. ||| 50079 ||| 58 ||| 50080 ||| 5827 ||| 71 ||| 
2018 ||| average modeling and evaluation of 18-pulse autotransformer rectifier unit without interphase transformers. ||| 50092 ||| 50093 ||| 50094 ||| 50095 ||| 50096 ||| 50097 ||| 
2017 ||| discrete design optimization of distribution transformers with guaranteed optimum convergence using the cuckoo search algorithm. ||| 50067 ||| 50068 ||| 
2020 ||| attention mechanism based semi-supervised multi-gain image fusion. ||| 50098 ||| 273 ||| 50099 ||| 50100 ||| 50101 ||| 
2021 ||| attention-inspired artificial neural networks for speech processing: a systematic review. ||| 50102 ||| 50103 ||| 852 ||| 50104 ||| 21533 ||| 50105 ||| 50106 ||| 
2020 ||| unsupervised hashing with gradient attention. ||| 46373 ||| 4719 ||| 12637 ||| 12638 ||| 12639 ||| 
2020 ||| dual attention-guided multiscale dynamic aggregate graph convolutional networks for skeleton-based human action recognition. ||| 50107 ||| 50108 ||| 
2020 ||| automatic classification system of arrhythmias using 12-lead ecgs with a deep neural network based on an attention mechanism. ||| 50109 ||| 20226 ||| 44178 ||| 49270 ||| 50110 ||| 
2022 ||| attention optimized deep generative adversarial network for removing uneven dense haze. ||| 50111 ||| 50112 ||| 50113 ||| 50114 ||| 
2020 ||| better understanding: stylized image captioning with style attention and adversarial training. ||| 42765 ||| 8922 ||| 42767 ||| 
2021 ||| bidirectional symmetry network with dual-field cyclic attention for multi-temporal aerial remote sensing image registration. ||| 3503 ||| 336 ||| 50115 ||| 1207 ||| 
2021 ||| attention to a moment in time impairs episodic distinctiveness during rapid serial visual presentation. ||| 50116 ||| 50117 ||| 50118 ||| 
2021 ||| transfer detection of yolo to focus cnn's attention on nude regions for adult content detection. ||| 50119 ||| 50120 ||| 50121 ||| 50122 ||| 50123 ||| 50124 ||| 6346 ||| 
2021 ||| attention modulated multiple object tracking with motion enhancement and dual correlation. ||| 22358 ||| 50125 ||| 3402 ||| 13816 ||| 
2019 ||| entity linking via symmetrical attention-based neural network and entity structural features. ||| 50126 ||| 50127 ||| 50128 ||| 50129 ||| 49542 ||| 
2021 ||| deep convolutional symmetric encoder - decoder neural networks to predict students' visual attention. ||| 50130 ||| 50131 ||| 50132 ||| 50133 ||| 
2021 ||| hotspot temperature prediction of dry-type transformers based on particle filter optimization with support vector regression. ||| 29009 ||| 46172 ||| 10057 ||| 50134 ||| 48653 ||| 50135 ||| 46173 ||| 1975 ||| 
2019 ||| semantic relation classification via bidirectional lstm networks with entity-aware attention using latent entity typing. ||| 34423 ||| 34424 ||| 34425 ||| 
2018 ||| sequential dual attention: coarse-to-fine-grained hierarchical generation for image captioning. ||| 50136 ||| 3129 ||| 50137 ||| 50138 ||| 50139 ||| 
2021 ||| deep deterministic policy gradient algorithm based on convolutional block attention for autonomous driving. ||| 46207 ||| 47729 ||| 50140 ||| 50141 ||| 
2021 ||| an ensemble of global and local-attention based convolutional neural networks for covid-19 diagnosis on chest x-ray images. ||| 50142 ||| 50143 ||| 50144 ||| 50145 ||| 50146 ||| 
2021 ||| a multi-scale residual attention network for retinal vessel segmentation. ||| 5366 ||| 50147 ||| 25100 ||| 50148 ||| 
2021 ||| automatic lung segmentation algorithm on chest x-ray images based on fusion variational auto-encoder and three-terminal attention mechanism. ||| 50149 ||| 41367 ||| 
2019 ||| attention bilinear pooling for fine-grained classification. ||| 50150 ||| 1796 ||| 50151 ||| 
2022 ||| tcan-ids: intrusion detection system for internet of vehicle using temporal convolutional attention network. ||| 50152 ||| 5723 ||| 50153 ||| 50154 ||| 
2020 ||| convolutional attention network with maximizing mutual information for fine-grained image classification. ||| 50151 ||| 2736 ||| 13240 ||| 1101 ||| 1796 ||| 
2020 ||| an improved deep mutual-attention learning model for person re-identification. ||| 50155 ||| 50156 ||| 50157 ||| 
2018 ||| research on electronic voltage transformer for big data background. ||| 28556 ||| 10409 ||| 15833 ||| 4086 ||| 
2020 ||| joint entity-relation extraction via improved graph attention networks. ||| 50158 ||| 23545 ||| 2143 ||| 
2022 ||| adaptive memory-controlled self-attention for polyphonic sound event detection. ||| 6145 ||| 50159 ||| 50160 ||| 50161 ||| 
2021 ||| bi-sanet - bilateral network with scale attention for retinal vessel segmentation. ||| 5366 ||| 50147 ||| 40987 ||| 50162 ||| 
2020 ||| attention-based lstm with filter mechanism for entity relation classification. ||| 46207 ||| 27538 ||| 46209 ||| 
2019 ||| symmetry encoder-decoder network with attention mechanism for fast video object segmentation. ||| 50163 ||| 5815 ||| 3879 ||| 50164 ||| 
2020 ||| aresu-net: attention residual u-net for brain tumor segmentation. ||| 11988 ||| 50165 ||| 50166 ||| 2304 ||| 
2021 ||| mre: a military relation extraction model based on bigru and multi-head attention. ||| 50167 ||| 50168 ||| 50169 ||| 28102 ||| 50170 ||| 50171 ||| 
2021 ||| dual attention network for pitch estimation of monophonic music. ||| 50172 ||| 12385 ||| 24815 ||| 
2019 ||| a crucial role of attention in lateralisation of sound processing? ||| 50173 ||| 50174 ||| 25710 ||| 50175 ||| 50176 ||| 50177 ||| 50178 ||| 50179 ||| 
2021 ||| contributions of the right prefrontal and parietal cortices to the attentional blink: a tdcs study. ||| 50180 ||| 50181 ||| 50182 ||| 50183 ||| 50184 ||| 45743 ||| 50185 ||| 
2018 ||| unsupervised multi-object detection for video surveillance using memory-based recurrent attention networks. ||| 50186 ||| 50187 ||| 
2020 ||| anticipatory defocusing of attention and contextual response priming but no role of aesthetic appreciation in simple symmetry judgments when switching between tasks. ||| 50188 ||| 12529 ||| 50189 ||| 50190 ||| 50191 ||| 
2021 ||| neurofunctional symmetries and asymmetries during voluntary out-of- and within-body vivid imagery concurrent with orienting attention and visuospatial detection. ||| 50192 ||| 50193 ||| 50194 ||| 50195 ||| 50196 ||| 50197 ||| 
2018 ||| neural relation classification using selective attention and symmetrical directional instances. ||| 50127 ||| 1717 ||| 50198 ||| 50129 ||| 49542 ||| 
2020 ||| cloud detection for satellite imagery using attention-based u-net convolutional neural network. ||| 50199 ||| 50200 ||| 50201 ||| 38558 ||| 
2021 ||| improving sentiment classification of restaurant reviews with attention-based bi-gru neural network. ||| 50202 ||| 8967 ||| 50203 ||| 
2022 ||| a vibration similarity model of converter transformers and its verification method. ||| 1371 ||| 254 ||| 50204 ||| 20923 ||| 2982 ||| 
2021 ||| mrda-mgfsnet: network based on a multi-rate dilated attention mechanism and multi-granularity feature sharer for image-based butterflies fine-grained classification. ||| 29237 ||| 29238 ||| 29239 ||| 29240 ||| 29241 ||| 29242 ||| 29243 ||| 29244 ||| 
2018 ||| practical feedback method for mobile cpr support systems considering noise and user's attention. ||| 50205 ||| 50206 ||| 
2021 ||| epitope prediction of antigen protein using attention-based lstm network. ||| 50207 ||| 50208 ||| 50209 ||| 50210 ||| 50211 ||| 50212 ||| 50213 ||| 
2018 ||| investors' attention and overpricing of ipo: an empirical study on china's growth enterprise market. ||| 50214 ||| 50215 ||| 17979 ||| 
2020 ||| temporal continuity of visual attention for future gaze prediction in immersive virtual reality. ||| 50216 ||| 4417 ||| 50217 ||| 
2021 ||| self-attention transfer networks for speech emotion recognition. ||| 645 ||| 12309 ||| 11983 ||| 647 ||| 50218 ||| 12310 ||| 12041 ||| 648 ||| 649 ||| 
2021 ||| fcca: hybrid code representation for functional clone detection using attention networks. ||| 8596 ||| 3893 ||| 3891 ||| 50219 ||| 3894 ||| 
2020 ||| spare assessment of distribution power transformers considering the issues of redundancy and mus capability. ||| 50220 ||| 50221 ||| 
2020 ||| hierarchical analysis of loops with relaxed abstract transformers. ||| 50222 ||| 50223 ||| 50224 ||| 430 ||| 
2020 ||| attention-based word-level contextual feature extraction and cross-modality fusion for sentiment analysis and emotion classification. ||| 31712 ||| 31713 ||| 31714 ||| 
2017 ||| do researchers pay attention to publication subsidies? ||| 50225 ||| 
2020 ||| the power of social cues in the battle for attention: evidence from an online platform for scholarly commentary. ||| 50226 ||| 50227 ||| 50228 ||| 
2020 ||| comparison of citations and attention of cover and non-cover papers. ||| 50229 ||| 50230 ||| 
2018 ||| supervised search result diversification via subtopic attention. ||| 1376 ||| 1375 ||| 3504 ||| 1377 ||| 9692 ||| 1378 ||| 
2021 ||| co-attention memory network for multimodal microblog's hashtag recommendation. ||| 9693 ||| 3272 ||| 336 ||| 11723 ||| 17842 ||| 3273 ||| 
2022 ||| g-inspector: recurrent attention model on graph. ||| 50231 ||| 50232 ||| 25418 ||| 42031 ||| 
2021 ||| hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification. ||| 9407 ||| 215 ||| 18202 ||| 26422 ||| 18200 ||| 32595 ||| 1717 ||| 1094 ||| 9988 ||| 
2020 ||| a hierarchical attention model for social contextual image recommendation. ||| 15205 ||| 1207 ||| 5946 ||| 50233 ||| 9574 ||| 444 ||| 
2017 ||| hierarchical contextual attention recurrent neural network for map query suggestion. ||| 27183 ||| 7652 ||| 2258 ||| 50234 ||| 2814 ||| 9396 ||| 1088 ||| 
2021 ||| neural attention frameworks for explainable recommendation. ||| 50235 ||| 1305 ||| 768 ||| 50236 ||| 50237 ||| 
2021 ||| multi-level attention networks for multi-step citywide passenger demands prediction. ||| 22919 ||| 8920 ||| 17777 ||| 44742 ||| 11190 ||| 
2021 ||| fall detection with wearable sensors: a hierarchical attention-based convolutional neural network approach. ||| 50238 ||| 43211 ||| 50239 ||| 50240 ||| 50241 ||| 23548 ||| 
2018 ||| attention adjustment, renewal, and equilibrium seeking in online search: an eye-tracking approach. ||| 50242 ||| 50243 ||| 50244 ||| 50245 ||| 
2019 ||| a possible framework for attention-based politics: a field for research. ||| 50246 ||| 
2021 ||| sparse deep lstms with convolutional attention for human action recognition. ||| 50247 ||| 50248 ||| 50249 ||| 
2020 ||| mobile interface attentional priority model. ||| 17079 ||| 17080 ||| 
2020 ||| contextual stroke classification in online handwritten documents with edge graph attention networks. ||| 13234 ||| 17440 ||| 17441 ||| 779 ||| 
2022 ||| macularnet: towards fully automated attention-based deep cnn for macular disease classification. ||| 28701 ||| 2651 ||| 16408 ||| 
2021 ||| exploring multi-task multi-lingual learning of transformer models for hate speech and offensive speech identification in social media. ||| 8410 ||| 8411 ||| 8412 ||| 
2021 ||| learning accurate integer transformer machine-translation models. ||| 32541 ||| 
2021 ||| deep learning-based image retrieval system with clustering on attention-based representations. ||| 50250 ||| 50251 ||| 50252 ||| 
2020 ||| extracting opinion targets using attention-based neural model. ||| 50253 ||| 50254 ||| 10480 ||| 
2022 ||| bornon: bengali image captioning with transformer-based deep learning approach. ||| 32448 ||| 32449 ||| 32450 ||| 32451 ||| 32452 ||| 
2021 ||| vision-to-language tasks based on attributes and attention mechanism. ||| 6922 ||| 39841 ||| 8002 ||| 
2022 ||| chinese image caption generation via visual attention and topic modeling. ||| 9105 ||| 41683 ||| 41682 ||| 50255 ||| 41684 ||| 
2021 ||| asif-net: attention steered interweave fusion network for rgb-d salient object detection. ||| 13621 ||| 35817 ||| 19728 ||| 19426 ||| 4056 ||| 39747 ||| 50256 ||| 13825 ||| 
2020 ||| embedding attention and residual network for accurate salient object detection. ||| 8601 ||| 8603 ||| 8602 ||| 8604 ||| 
2020 ||| temporally identity-aware ssd with attentional lstm. ||| 5832 ||| 25559 ||| 25558 ||| 
2022 ||| complementarity-aware attention network for salient object detection. ||| 32430 ||| 42649 ||| 6625 ||| 13194 ||| 30960 ||| 
2021 ||| bio-inspired representation learning for visual attention prediction. ||| 6650 ||| 35911 ||| 8002 ||| 
2022 ||| sman: stacked multimodal attention network for cross-modal image-text retrieval. ||| 2276 ||| 2277 ||| 2278 ||| 2279 ||| 
2019 ||| deep attention-based spatially recursive networks for fine-grained visual recognition. ||| 17580 ||| 602 ||| 9889 ||| 619 ||| 
2020 ||| alignment-supervised bidimensional attention-based recursive autoencoders for bilingual phrase representation. ||| 3180 ||| 3181 ||| 3182 ||| 50257 ||| 
2020 ||| visual object tracking by hierarchical attention siamese network. ||| 2445 ||| 33342 ||| 50258 ||| 1932 ||| 
2019 ||| describing video with attention-based bidirectional lstm. ||| 19590 ||| 11466 ||| 1038 ||| 13410 ||| 1040 ||| 6922 ||| 
2021 ||| attention-based convolutional neural network for bangla sentiment analysis. ||| 50259 ||| 50260 ||| 
2021 ||| pretrained transformers for text ranking: bert and beyond ||| 3009 ||| 3006 ||| 9664 ||| 
2019 ||| a deep architecture for chinese semantic matching with pairwise comparisons and attention-pooling. ||| 43935 ||| 43936 ||| 43937 ||| 43938 ||| 43939 ||| 43940 ||| 
2020 ||| exploring the relationship between attention and awareness. neurophenomenology of the centroencephalic space of functional integration. ||| 50261 ||| 50262 ||| 20935 ||| 50263 ||| 50264 ||| 
2017 ||| an ecog-based bci based on auditory attention to natural speech. ||| 50265 ||| 50266 ||| 50267 ||| 4194 ||| 50268 ||| 50269 ||| 50270 ||| 
2019 ||| online adaptive synchronous bci system with attention variations. ||| 5557 ||| 5558 ||| 5112 ||| 5559 ||| 5560 ||| 
2021 ||| few-shot knowledge reasoning: an attention mechanism based method. ||| 25763 ||| 10833 ||| 25764 ||| 
2020 ||| subtle visual attention guidance in vr. ||| 23666 ||| 23670 ||| 
